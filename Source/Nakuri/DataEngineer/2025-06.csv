title,company,experience,salary,locations,description,industry,department,employment_type,skills,scraped_at
Data Engineer - Python/SQL,Wipro,3 - 5 years,Not Disclosed,['Bengaluru'],"Role Purpose\nThe purpose of this role is to design, test and maintain software programs for operating systems or applications which needs to be deployed at a client end and ensure its meet 100% quality assurance parameters\n\n\n\nDo\n1. Instrumental in understanding the requirements and design of the product/ software\nDevelop software solutions by studying information needs, studying systems flow, data usage and work processes\nInvestigating problem areas followed by the software development life cycle\nFacilitate root cause analysis of the system issues and problem statement\nIdentify ideas to improve system performance and impact availability\nAnalyze client requirements and convert requirements to feasible design\nCollaborate with functional teams or systems analysts who carry out the detailed investigation into software requirements\nConferring with project managers to obtain information on software capabilities\n\n\n\n2. Perform coding and ensure optimal software/ module development\nDetermine operational feasibility by evaluating analysis, problem definition, requirements, software development and proposed software\nDevelop and automate processes for software validation by setting up and designing test cases/scenarios/usage cases, and executing these cases\nModifying software to fix errors, adapt it to new hardware, improve its performance, or upgrade interfaces.\nAnalyzing information to recommend and plan the installation of new systems or modifications of an existing system\nEnsuring that code is error free or has no bugs and test failure\nPreparing reports on programming project specifications, activities and status\nEnsure all the codes are raised as per the norm defined for project / program / account with clear description and replication patterns\nCompile timely, comprehensive and accurate documentation and reports as requested\nCoordinating with the team on daily project status and progress and documenting it\nProviding feedback on usability and serviceability, trace the result to quality risk and report it to concerned stakeholders\n\n\n\n3. Status Reporting and Customer Focus on an ongoing basis with respect to project and its execution\nCapturing all the requirements and clarifications from the client for better quality work\nTaking feedback on the regular basis to ensure smooth and on time delivery\nParticipating in continuing education and training to remain current on best practices, learn new programming languages, and better assist other team members.\nConsulting with engineering staff to evaluate software-hardware interfaces and develop specifications and performance requirements\nDocument and demonstrate solutions by developing documentation, flowcharts, layouts, diagrams, charts, code comments and clear code\nDocumenting very necessary details and reports in a formal way for proper understanding of software from client proposal to implementation\nEnsure good quality of interaction with customer w.r.t. e-mail content, fault report tracking, voice calls, business etiquette etc\nTimely Response to customer requests and no instances of complaints either internally or externally\n\n\n\nDeliver\n\nNo.Performance ParameterMeasure\n1.Continuous Integration, Deployment & Monitoring of Software100% error free on boarding & implementation, throughput %, Adherence to the schedule/ release plan\n2.Quality & CSATOn-Time Delivery, Manage software, Troubleshoot queries,Customer experience, completion of assigned certifications for skill upgradation\n3.MIS & Reporting100% on time MIS & report generation\nMandatory Skills: Python for Insights.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Python', 'module development', 'Data Engineering', 'software development', 'software programs', 'SQL']",2025-06-12 13:40:38
Data Engineer,AMERICAN EXPRESS,2 - 4 years,13-17 Lacs P.A.,"['Gurugram', 'Delhi / NCR']","Role & responsibilities\nUnderstanding business use cases and be able to convert to technical design\nPart of a cross-disciplinary team, working closely with other data engineers, software engineers, data scientists, data managers and business partners.\nYou will be designing scalable, testable and maintainable data pipelines\nIdentify areas for data governance improvements and help to resolve data quality problems through the appropriate choice of error detection and correction, process control and improvement, or process design changes",,,,"['Spark', 'SQL', 'Python', 'Hadoop', 'Big Data']",2025-06-12 13:40:40
Data Engineer,Visa,10 - 13 years,Not Disclosed,['Bengaluru'],"Translate business requirements and source system understanding into technical solutions using Opensource Tech Stack, Big Data, Java.\nWork with business partners directly to seek clarity on requirements.\nDefine solutions in terms of components, modules, and algorithms.\nDesign, develop, document, and implement new programs and subprograms, as we'll as enhancements, modifications and corrections to existing software.\nCreate technical documentation and procedures for installation and maintenance.\nWrite Unit Tests covering known use cases using appropriate tools.\nIntegrate test frameworks in the development process.\nWork with operations to get the solutions deployed.\nTake ownership of production deployment of code.\nCome up with Coding and Design best practices.\nThrive in a self-motivated, internal-innovation driven environment.\nAdapt quickly to new application knowledge and changes.\nThis is a hybrid position. Expectation of days in office will be confirmed by your Hiring Manager.\n\n\nBachelor degree in Computer Science, Electrical Engineering, Information Systems or other technical discipline.\nMinimum of 1 plus years of software development experience in Hadoop using Spark, Scala, Hive.\nExpertise in Object Oriented Programming Language Java, Python.\nExperience using CI CD Process, version control and bug tracking tools.\nResult-oriented with strong analytical and problem-solving skills.\nExperience with automation of job execution, validation and comparison of data files on Hadoop Environment at the field level.\nExperience in working with a small team and being a team player.\nStrong communication skills with proven ability to present complex ideas and document them in a clear and concise way.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Electrical engineering', 'Automation', 'Version control', 'Coding', 'Analytical', 'SCALA', 'Object oriented programming', 'Python', 'Technical documentation']",2025-06-12 13:40:43
Data Engineer,PwC India,4 - 8 years,Not Disclosed,['Bengaluru'],"If Interested please fill the below application link : https://forms.office.com/r/Zc8wDfEGEH\n\n\nResponsibilities:\nDeliver projects integrating data flows within and across technology systems.\nLead data modeling sessions with end user groups, project stakeholders, and technology teams to produce logical and physical data models.",,,,"['Pyspark', 'Aws Cloud', 'Azure Cloud', 'Python']",2025-06-12 13:40:46
Data Engineer,AMERICAN EXPRESS,3 - 8 years,Not Disclosed,['Chennai'],"You Lead the Way. We've Got Your Back.\n\nWith the right backing, people and businesses have the power to progress in incredible ways. When you join Team Amex, you become part of a global and diverse community of colleagues with an unwavering commitment to back our customers, communities and each other. Here, youll learn and grow as we help you create a career journey thats unique and meaningful to you with benefits, programs, and flexibility that support you personally and professionally.\nAt American Express, you’ll be recognized for your contributions, leadership, and impact—every colleague has the opportunity to share in the company’s success. Together, we’ll win as a team, striving to uphold our company values and powerful backing promise to provide the world’s best customer experience every day. And we’ll do it with the utmost integrity, and in an environment where everyone is seen, heard and feels like they belong. As part of our diverse tech team, you can architect, code and ship software that makes us an essential part of our customers’ digital lives. Here, you can work alongside talented engineers in an open, supportive, inclusive environment where your voice is valued, and you make your own decisions on what tech to use to solve challenging problems. Amex offers a range of opportunities to work with the latest technologies and encourages you to back the broader engineering community through open source. And because we understand the importance of keeping your skills fresh and relevant, we give you dedicated time to invest in your professional development. Find your place in technology on #TeamAmex.",,,,"['Data Engineering', 'GCP', 'Airflow', 'Pyspark', 'Bigquery', 'Hadoop', 'Big Data', 'SQL', 'Python', 'Backend Development']",2025-06-12 13:40:48
Data Engineer,HCLTech,11 - 14 years,16-27.5 Lacs P.A.,"['Hyderabad', 'Chennai', 'Bengaluru']","Roles and Responsibilities\nDesign, develop, and maintain large-scale data pipelines using Azure Data Factory (ADF) to extract, transform, and load data from various sources into Snowflake Data Warehouse.\nDevelop complex SQL queries to optimize database performance and troubleshoot issues in Snowflake tables.\nCollaborate with cross-functional teams to gather requirements for reporting needs and design scalable solutions using Power BI.\nEnsure high-quality data modeling by creating logical and physical models for large datasets.\nTroubleshoot technical issues related to ETL processes, data quality, and performance tuning.",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['Azure Data Factory', 'Power Bi', 'Snowflake', 'Azure Data Lake', 'SQL', 'Data Modeling', 'Data Warehousing']",2025-06-12 13:40:51
Data Engineer,HARMAN,5 - 10 years,Not Disclosed,['Bengaluru'],"-Strong analytical thinking and problem-solving skills, with the ability to translate complex data into actionable insights\n-Excellent communication skills, with the ability to effectively convey complex findings to both technical and non-technical stakeholders.\nCandidate to work form SRIB Bangalore with 3 days working from office is mandatory\n  What You Will Do",,,,"['Digital media', 'CTV', 'Analytical', 'Machine learning', 'Agile', 'Data processing', 'Automotive', 'Python']",2025-06-12 13:40:54
Data Engineer - Hadoop,Wipro,5 - 8 years,Not Disclosed,['Bengaluru'],"Wipro Limited (NYSEWIT, BSE507685, NSEWIPRO) is a leading technology services and consulting company focused on building innovative solutions that address clients’ most complex digital transformation needs. Leveraging our holistic portfolio of capabilities in consulting, design, engineering, and operations, we help clients realize their boldest ambitions and build future-ready, sustainable businesses. With over 230,000 employees and business partners across 65 countries, we deliver on the promise of helping our customers, colleagues, and communities thrive in an ever-changing world. For additional information, visit us at www.wipro.com.\n About The Role  _x000D_\n\n \n\nRole Purpose  \nThe purpose of the role is to support process delivery by ensuring daily performance of the Production Specialists, resolve technical escalations and develop technical capability within the Production Specialists.\n\n\n ? _x000D_\n\n \n\nDo  \n\n\nOversee and support process by reviewing daily transactions on performance parameters\nReview performance dashboard and the scores for the team\nSupport the team in improving performance parameters by providing technical support and process guidance\nRecord, track, and document all queries received, problem-solving steps taken and total successful and unsuccessful resolutions\nEnsure standard processes and procedures are followed to resolve all client queries\nResolve client queries as per the SLA’s defined in the contract\nDevelop understanding of process/ product for the team members to facilitate better client interaction and troubleshooting\nDocument and analyze call logs to spot most occurring trends to prevent future problems\nIdentify red flags and escalate serious client issues to Team leader in cases of untimely resolution\nEnsure all product information and disclosures are given to clients before and after the call/email requests\nAvoids legal challenges by monitoring compliance with service agreements\n\n\n ? _x000D_\n\n\nHandle technical escalations through effective diagnosis and troubleshooting of client queries\nManage and resolve technical roadblocks/ escalations as per SLA and quality requirements\nIf unable to resolve the issues, timely escalate the issues to TA & SES\nProvide product support and resolution to clients by performing a question diagnosis while guiding users through step-by-step solutions\nTroubleshoot all client queries in a user-friendly, courteous and professional manner\nOffer alternative solutions to clients (where appropriate) with the objective of retaining customers’ and clients’ business\nOrganize ideas and effectively communicate oral messages appropriate to listeners and situations\nFollow up and make scheduled call backs to customers to record feedback and ensure compliance to contract SLA’s\n\n\n ? _x000D_\n\n\nBuild people capability to ensure operational excellence and maintain superior customer service levels of the existing account/client\nMentor and guide Production Specialists on improving technical knowledge\nCollate trainings to be conducted as triage to bridge the skill gaps identified through interviews with the Production Specialist\nDevelop and conduct trainings (Triages) within products for production specialist as per target\nInform client about the triages being conducted\nUndertake product trainings to stay current with product features, changes and updates\nEnroll in product specific and any other trainings per client requirements/recommendations\nIdentify and document most common problems and recommend appropriate resolutions to the team\nUpdate job knowledge by participating in self learning opportunities and maintaining personal networks\n\n\n ? _x000D_\n\nDeliver\nNoPerformance ParameterMeasure1ProcessNo. of cases resolved per day, compliance to process and quality standards, meeting process level SLAs, Pulse score, Customer feedback, NSAT/ ESAT2Team ManagementProductivity, efficiency, absenteeism3Capability developmentTriages completed, Technical Test performance\n\nMandatory\n\nSkills:\nHadoop_x000D_.\n\nExperience5-8 Years_x000D_.\n\nReinvent your world. We are building a modern Wipro. We are an end-to-end digital transformation partner with the boldest ambitions. To realize them, we need people inspired by reinvention. Of yourself, your career, and your skills. We want to see the constant evolution of our business and our industry. It has always been in our DNA - as the world around us changes, so do we. Join a business powered by purpose and a place that empowers you to design your own reinvention. Come to Wipro. Realize your ambitions. Applications from people with disabilities are explicitly welcome.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['hive', 'data engineering', 'spark', 'troubleshooting', 'hadoop', 'cloudera', 'python', 'scala', 'big data analytics', 'oozie', 'airflow', 'pyspark', 'data warehousing', 'apache pig', 'machine learning', 'sql', 'mapreduce', 'sqoop', 'big data', 'aws', 'etl', 'hbase']",2025-06-12 13:40:56
Data Engineer,"NTT DATA, Inc.",1 - 4 years,Not Disclosed,['Bengaluru'],"Req ID: 321498\n\nWe are currently seeking a Data Engineer to join our team in Bangalore, Karntaka (IN-KA), India (IN).\n\nJob Duties""¢ Work closely with Lead Data Engineer to understand business requirements, analyse and translate these requirements into technical specifications and solution design.\n""¢ Work closely with Data modeller to ensure data models support the solution design\n""¢ Develop , test and fix ETL code using Snowflake, Fivetran, SQL, Stored proc.\n""¢ Analysis of the data and ETL for defects/service tickets (for solution in production ) raised and service tickets.\n""¢ Develop documentation and artefacts to support projects\n\nMinimum Skills Required""¢ ADF\n""¢ Fivetran (orchestration & integration)\n""¢ SQL\n""¢ Snowflake DWH",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['snowflake', 'data engineering', 'sql', 'oracle adf', 'etl', 'hive', 'python', 'data analysis', 'oracle', 'informatica powercenter', 'amazon redshift', 'talend', 'data warehousing', 'power bi', 'plsql', 'tableau', 'data modeling', 'spark', 'etl tool', 'technical specifications', 'hadoop', 'informatica', 'unix']",2025-06-12 13:40:59
Data Engineer,IBM,2 - 4 years,Not Disclosed,['Bengaluru'],"Ingest new data from relational and non-relational source database systems into our warehouse. Connect data from various sources.\n\nIntegrate data from external sources to warehouse by building facts and dimensions based on the EPM data model requirements.\n\n\n\nAutomate data exchange and processing through serverless data pipelines.\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nExperience in data analysis and integration.\nExperience in data building and consuming fact and dimension tables.\nExperience in automating data integration through data pipelines.\nExperience with object-oriented programing languages such as Python.\nExperience with structured data processing languages such as SQL and Spark SQL.\nExperience with REST APIs and JSON\nExperience in IBM Cloud data processing services such as IBM Code Engine, IBM Event Streams (Apache Kafka).\nStrong understanding of Datawarehouse concepts and various data warehouse architectures\n\n\nPreferred technical and professional experience\nExperience with IBM Cloud architecture\nExperience with DevOps.\nKnowledge of Agile development methodologies\nExperience with building containerized applications and running them in serverless environments on the Cloud such as IBM Code Engine, Kubernetes, or Satellite.\nExperience with IBM Cognitive Enterprise Data Platform and CodeHub.\nExperience with data integration tools such as IBM DataStage or Informatica",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'data warehousing', 'sql', 'spark', 'data warehousing concepts', 'hive', 'kubernetes', 'rest', 'data analysis', 'ibm cloud', 'datastage', 'data processing', 'data engineering', 'apache', 'cloud architecture', 'cognitive', 'devops', 'ibm datastage', 'kafka', 'json', 'agile', 'hadoop', 'informatica']",2025-06-12 13:41:01
Hadoop Data Engineer,Info Origin,0 - 2 years,Not Disclosed,['Gurugram'],"Job Overview:\nWe are looking for experienced Data Engineers proficient in Hadoop, Hive, Python, SQL, and Pyspark/Spark to join our dynamic team. Candidates will be responsible for designing, developing, and maintaining scalable big data solutions.\nKey Responsibilities:\nDevelop and optimize data pipelines for large-scale data processing.\nWork with structured and unstructured datasets to derive actionable insights.\nCollaborate with cross-functional teams to enhance data-driven decision-making.\nEnsure the performance, scalability, and reliability of data architectures.\nImplement best practices for data security and governance.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['hive', 'Scalability', 'data security', 'spark', 'Hadoop', 'Data processing', 'big data', 'SQL', 'Python']",2025-06-12 13:41:04
Data Engineer,DXC Technology,3 - 5 years,Not Disclosed,['Bengaluru'],"Job Description:\nSr. Snowflake, DBT, SQL Developer:\nWe are looking for an experienced senior Backend developer (Snowflake, DBT and SQL) The person should have proven hands-on experience in Snowflake, DBT and SQL. Azure and DBT is always an added advantage as Nestle is using DBT for Transformation.\nThe overall experience of 7+ years in SQL and with minimum 3-5 years of experience in Snowflake.\nExperience with SnowFlake data warehouse\nExperience with Data Ingestion into SnowFlake such as Snowpipe and DBT\nDBT Development: Design, develop, and maintain DBT models, transformations, and SQL code to build efficient data pipelines for analytics and reporting.\nGood understanding of SnowFlake architecture and processing\nHands-on experience in Snowflake Cloud Development\nExperience in writing complicated SQLs, analyzing query performance, query tuning, database indexes partitions, and stored procedure development.\nImplementing ETL pipelines within and outside of a data warehouse using Python and Snowflakes Snow SQL\nQuerying Snowflake using SQL\ngood knowledge of SQL language and data warehousing concepts,\no Experience with technologies such as SQL Server 2008, as well as with newer ones like SSIS and stored procedures\no Designing database tables and structures.\no Creating views, functions, and stored procedures.\no Writing optimized SQL queries for integration with other applications.\no Creating database triggers for use in automation.\no Maintaining data quality and overseeing database security.\no Exceptional experience developing codes, testing for quality assurance, administering RDBMS, and monitoring of database\no Strong knowledge and experience with relational databases and database administration (indexes, optimization, etc. )\no Querying Snowflake using SQL, Experience with SQL and PLSQL, SQL query tuning, database performance tuning and data warehousing concepts, Knowledge of DBT and Azure (ADF) is desirable.\nRecruitment fraud is a scheme in which fictitious job opportunities are offered to job seekers typically through online services, such as false websites, or through unsolicited emails claiming to be from the company. These emails may request recipients to provide personal information or to make payments as part of their illegitimate recruiting process. DXC does not make offers of employment via social media networks and DXC never asks for any money or payments from applicants at any point in the recruitment process, nor ask a job seeker to purchase IT or other equipment on our behalf. More information on employment scams is available here .",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Performance tuning', 'Automation', 'Manager Quality Assurance', 'RDBMS', 'Database administration', 'Stored procedures', 'SSIS', 'Analytics', 'Monitoring', 'Python']",2025-06-12 13:41:06
Big Data Developer/Data Engineer,Grid Dynamics,5 - 10 years,Not Disclosed,['Bengaluru'],"Role & responsibilities\nExperience: 5 - 8 years\nEmployment Type: Full-Time\n\nJob Summary:\nWe are looking for a highly skilled Scala and Spark Developer to join our data engineering team. The ideal candidate will have strong experience in building scalable data processing solutions using Apache Spark and writing robust, high-performance applications in Scala. You will work closely with data scientists, data analysts, and product teams to design, develop, and optimize large-scale data pipelines and ETL workflows.\n\nKey Responsibilities:\nDevelop and maintain scalable data processing pipelines using Apache Spark and Scala.\nWork on batch and real-time data processing using Spark (RDD/DataFrame/Dataset).\nWrite efficient and maintainable code following best practices and coding standards.\nCollaborate with cross-functional teams to understand data requirements and implement solutions.\nOptimize performance of Spark jobs and troubleshoot data-related issues.\nIntegrate data from multiple sources and ensure data quality and consistency.\nParticipate in design reviews, code reviews, and provide technical leadership when needed.\nContribute to data modeling, schema design, and architecture discussions.\nRequired Skills:\nStrong programming skills in Scala.\nExpertise in Apache Spark (Core, SQL, Streaming).\nHands-on experience with distributed computing and large-scale data processing.\nExperience with data formats like Parquet, Avro, ORC, and JSON.\nGood understanding of functional programming concepts.\nFamiliarity with data ingestion tools (Kafka, Flume, Sqoop, etc.).\nExperience working with Hadoop ecosystem (HDFS, Hive, YARN, etc.) is a plus.\nStrong SQL skills and experience working with relational and NoSQL databases.\nExperience with version control tools like Git.\nPreferred Qualifications:\nBachelor's or Masters degree in Computer Science, Engineering, or related field.\nExperience with cloud platforms like AWS, Azure, or GCP (especially EMR, Databricks, etc.).\nKnowledge of containerization (Docker, Kubernetes) is a plus.\nFamiliarity with CI/CD tools and DevOps practices.ndidate profile",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Scala', 'Pyspark', 'Spark']",2025-06-12 13:41:08
Data Engineer,"NTT DATA, Inc.",4 - 9 years,Not Disclosed,['Chennai'],"Req ID: 324631\n\nWe are currently seeking a Data Engineer to join our team in Chennai, Tamil Ndu (IN-TN), India (IN).\n\nKey Responsibilities:\n\nDesign and implement tailored data solutions to meet customer needs and use cases, spanning from streaming to data lakes, analytics, and beyond within a dynamically evolving technical stack.\n\nProvide thought leadership by recommending the most appropriate technologies and solutions for a given use case, covering the entire spectrum from the application layer to infrastructure.\n\nDemonstrate proficiency in coding skills, utilizing languages such as Python, Java, and Scala to efficiently move solutions into production while prioritizing performance, security, scalability, and robust data integrations.\n\nCollaborate seamlessly across diverse technical stacks, including Cloudera, Databricks, Snowflake, and AWS.\n\nDevelop and deliver detailed presentations to effectively communicate complex technical concepts.\n\nGenerate comprehensive solution documentation, including sequence diagrams, class hierarchies, logical system views, etc.\n\nAdhere to Agile practices throughout the solution development process.\n\nDesign, build, and deploy databases and data stores to support organizational requirements.\n\nBasic Qualifications:\n\n4+ years of experience supporting Software Engineering, Data Engineering, or Data Analytics projects.\n\n2+ years of experience leading a team supporting data related projects to develop end-to-end technical solutions.\n\nExperience with Informatica, Python, Databricks, Azure Data Engineer\n\nAbility to travel at least 25%.""\n\nPreferred\n\nSkills:\n\n\nDemonstrate production experience in core data platforms such as Snowflake, Databricks, AWS, Azure, GCP, Hadoop, and more.\n\nPossess hands-on knowledge of Cloud and Distributed Data Storage, including expertise in HDFS, S3, ADLS, GCS, Kudu, ElasticSearch/Solr, Cassandra, or other NoSQL storage systems.\n\nExhibit a strong understanding of Data integration technologies, encompassing Informatica, Spark, Kafka, eventing/streaming, Streamsets, NiFi, AWS Data Migration Services, Azure DataFactory, Google DataProc.\n\nShowcase professional written and verbal communication skills to effectively convey complex technical concepts.\n\nUndergraduate or Graduate degree preferred",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['azure data lake', 'nosql', 'elastic search', 'cassandra', 'solr', 'core data', 'cloudera', 'python', 'data analytics', 'scala', 'kudu', 'microsoft azure', 'data engineering', 'data bricks', 'gen', 'java', 'apache nifi', 'spark', 'gcp', 'kafka', 'software engineering', 'hadoop', 'aws', 'data integration']",2025-06-12 13:41:11
Data Engineer,"NTT DATA, Inc.",4 - 9 years,Not Disclosed,['Chennai'],"Req ID: 324632\n\nWe are currently seeking a Data Engineer to join our team in Chennai, Tamil Ndu (IN-TN), India (IN).\n\nKey Responsibilities:\n\nDesign and implement tailored data solutions to meet customer needs and use cases, spanning from streaming to data lakes, analytics, and beyond within a dynamically evolving technical stack.\n\nProvide thought leadership by recommending the most appropriate technologies and solutions for a given use case, covering the entire spectrum from the application layer to infrastructure.\n\nDemonstrate proficiency in coding skills, utilizing languages such as Python, Java, and Scala to efficiently move solutions into production while prioritizing performance, security, scalability, and robust data integrations.\n\nCollaborate seamlessly across diverse technical stacks, including Cloudera, Databricks, Snowflake, and AWS.\n\nDevelop and deliver detailed presentations to effectively communicate complex technical concepts.\n\nGenerate comprehensive solution documentation, including sequence diagrams, class hierarchies, logical system views, etc.\n\nAdhere to Agile practices throughout the solution development process.\n\nDesign, build, and deploy databases and data stores to support organizational requirements.\n\nBasic Qualifications:\n\n4+ years of experience supporting Software Engineering, Data Engineering, or Data Analytics projects.\n\n2+ years of experience leading a team supporting data related projects to develop end-to-end technical solutions.\n\nExperience with Informatica, Python, Databricks, Azure Data Engineer\n\nAbility to travel at least 25%.""\n\nPreferred\n\nSkills:\n\n\nDemonstrate production experience in core data platforms such as Snowflake, Databricks, AWS, Azure, GCP, Hadoop, and more.\n\nPossess hands-on knowledge of Cloud and Distributed Data Storage, including expertise in HDFS, S3, ADLS, GCS, Kudu, ElasticSearch/Solr, Cassandra, or other NoSQL storage systems.\n\nExhibit a strong understanding of Data integration technologies, encompassing Informatica, Spark, Kafka, eventing/streaming, Streamsets, NiFi, AWS Data Migration Services, Azure DataFactory, Google DataProc.\n\nShowcase professional written and verbal communication skills to effectively convey complex technical concepts.\n\nUndergraduate or Graduate degree preferred",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['azure data lake', 'nosql', 'elastic search', 'cassandra', 'solr', 'core data', 'cloudera', 'python', 'data analytics', 'scala', 'kudu', 'microsoft azure', 'data engineering', 'data bricks', 'gen', 'java', 'apache nifi', 'spark', 'gcp', 'kafka', 'software engineering', 'hadoop', 'aws', 'data integration']",2025-06-12 13:41:13
Starburst Data Engineer/ Architect,Wipro,5 - 8 years,Not Disclosed,['Bengaluru'],"Starburst Data Engineer/Architect \nExpertise in Starburst and policy management like Ranger or equivalent.\nIn-depth knowledge of data modelling principles and techniques, including relational and dimensional.\nExcellent problem solving skills and the ability to troubleshoot and debug complex data related issues.\nStrong awareness of data tools and platforms like: Starburst, Snowflakes, Databricks and programming languages like SQL.\nIn-depth knowledge of data management principles, methodologies, and best practices with excellent analytical, problem-solving and decision making skills.\nDevelop, implement and maintain database systems using SQL.\nWrite complex SQL queries for integration with applications.\nDevelop and maintain data models (Conceptual, physical and logical) to meet organisational needs.\n\n\n ? \n\nDo\n\n1. Managing the technical scope of the project in line with the requirements at all stages\n\na. Gather information from various sources (data warehouses, database, data integration and modelling) and interpret patterns and trends\n\nb. Develop record management process and policies\n\nc. Build and maintain relationships at all levels within the client base and understand their requirements.\n\nd. Providing sales data, proposals, data insights and account reviews to the client base\n\ne. Identify areas to increase efficiency and automation of processes\n\nf. Set up and maintain automated data processes\n\ng. Identify, evaluate and implement external services and tools to support data validation and cleansing.\n\nh. Produce and track key performance indicators\n\n\n\n2. Analyze the data sets and provide adequate information\n\na. Liaise with internal and external clients to fully understand data content\n\nb. Design and carry out surveys and analyze survey data as per the customer requirement\n\nc. Analyze and interpret complex data sets relating to customer??s business and prepare reports for internal and external audiences using business analytics reporting tools\n\nd. Create data dashboards, graphs and visualization to showcase business performance and also provide sector and competitor benchmarking\n\ne. Mine and analyze large datasets, draw valid inferences and present them successfully to management using a reporting tool\n\nf. Develop predictive models and share insights with the clients as per their requirement\n\n ? \n\nDeliver\nNoPerformance ParameterMeasure1.Analyses data sets and provide relevant information to the clientNo. Of automation done, On-Time Delivery, CSAT score, Zero customer escalation, data accuracy\n\n\n ? \n\n ? \nMandatory\n\nSkills:\nStartburst.\n\nExperience5-8 Years.\n\nReinvent your world. We are building a modern Wipro. We are an end-to-end digital transformation partner with the boldest ambitions. To realize them, we need people inspired by reinvention. Of yourself, your career, and your skills. We want to see the constant evolution of our business and our industry. It has always been in our DNA - as the world around us changes, so do we. Join a business powered by purpose and a place that empowers you to design your own reinvention. Come to Wipro. Realize your ambitions. Applications from people with disabilities are explicitly welcome.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data management', 'sql', 'data bricks', 'data modeling', 'policy management', 'hive', 'snowflake', 'python', 'data mining', 'data warehousing', 'power bi', 'dbms', 'data architecture', 'sql server', 'plsql', 'tableau', 'unix shell scripting', 'spark', 'hadoop', 'etl', 'ssis', 'data integration', 'informatica']",2025-06-12 13:41:16
Data Engineer,"NTT DATA, Inc.",4 - 9 years,Not Disclosed,['Pune'],"Req ID: 324653\n\nWe are currently seeking a Data Engineer to join our team in Pune, Mahrshtra (IN-MH), India (IN).\n\nKey Responsibilities:\n\nDesign and implement tailored data solutions to meet customer needs and use cases, spanning from streaming to data lakes, analytics, and beyond within a dynamically evolving technical stack.\n\nProvide thought leadership by recommending the most appropriate technologies and solutions for a given use case, covering the entire spectrum from the application layer to infrastructure.\n\nDemonstrate proficiency in coding skills, utilizing languages such as Python, Java, and Scala to efficiently move solutions into production while prioritizing performance, security, scalability, and robust data integrations.\n\nCollaborate seamlessly across diverse technical stacks, including Cloudera, Databricks, Snowflake, and AWS.\n\nDevelop and deliver detailed presentations to effectively communicate complex technical concepts.\n\nGenerate comprehensive solution documentation, including sequence diagrams, class hierarchies, logical system views, etc.\n\nAdhere to Agile practices throughout the solution development process.\n\nDesign, build, and deploy databases and data stores to support organizational requirements.\n\nBasic Qualifications:\n\n4+ years of experience supporting Software Engineering, Data Engineering, or Data Analytics projects.\n\n2+ years of experience leading a team supporting data related projects to develop end-to-end technical solutions.\n\nExperience with Informatica, Python, Databricks, Azure Data Engineer\n\nAbility to travel at least 25%.""\n\nPreferred\n\nSkills:\n\n\nDemonstrate production experience in core data platforms such as Snowflake, Databricks, AWS, Azure, GCP, Hadoop, and more.\n\nPossess hands-on knowledge of Cloud and Distributed Data Storage, including expertise in HDFS, S3, ADLS, GCS, Kudu, ElasticSearch/Solr, Cassandra, or other NoSQL storage systems.\n\nExhibit a strong understanding of Data integration technologies, encompassing Informatica, Spark, Kafka, eventing/streaming, Streamsets, NiFi, AWS Data Migration Services, Azure DataFactory, Google DataProc.\n\nShowcase professional written and verbal communication skills to effectively convey complex technical concepts.\n\nUndergraduate or Graduate degree preferred",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['azure data lake', 'nosql', 'elastic search', 'cassandra', 'solr', 'core data', 'cloudera', 'python', 'data analytics', 'scala', 'kudu', 'microsoft azure', 'data engineering', 'data bricks', 'gen', 'java', 'apache nifi', 'spark', 'gcp', 'kafka', 'software engineering', 'hadoop', 'aws', 'data integration']",2025-06-12 13:41:18
Data Engineer,"NTT DATA, Inc.",4 - 9 years,Not Disclosed,['Pune'],"Req ID: 324609\n\nWe are currently seeking a Data Engineer to join our team in Pune, Mahrshtra (IN-MH), India (IN).\n\nKey Responsibilities:\n\nDesign and implement tailored data solutions to meet customer needs and use cases, spanning from streaming to data lakes, analytics, and beyond within a dynamically evolving technical stack.\n\nProvide thought leadership by recommending the most appropriate technologies and solutions for a given use case, covering the entire spectrum from the application layer to infrastructure.\n\nDemonstrate proficiency in coding skills, utilizing languages such as Python, Java, and Scala to efficiently move solutions into production while prioritizing performance, security, scalability, and robust data integrations.\n\nCollaborate seamlessly across diverse technical stacks, including Cloudera, Databricks, Snowflake, and AWS.\n\nDevelop and deliver detailed presentations to effectively communicate complex technical concepts.\n\nGenerate comprehensive solution documentation, including sequence diagrams, class hierarchies, logical system views, etc.\n\nAdhere to Agile practices throughout the solution development process.\n\nDesign, build, and deploy databases and data stores to support organizational requirements.\n\nBasic Qualifications:\n\n4+ years of experience supporting Software Engineering, Data Engineering, or Data Analytics projects.\n\n2+ years of experience leading a team supporting data related projects to develop end-to-end technical solutions.\n\nExperience with Informatica, Python, Databricks, Azure Data Engineer\n\nAbility to travel at least 25%.""\n\nPreferred\n\nSkills:\n\n\nDemonstrate production experience in core data platforms such as Snowflake, Databricks, AWS, Azure, GCP, Hadoop, and more.\n\nPossess hands-on knowledge of Cloud and Distributed Data Storage, including expertise in HDFS, S3, ADLS, GCS, Kudu, ElasticSearch/Solr, Cassandra, or other NoSQL storage systems.\n\nExhibit a strong understanding of Data integration technologies, encompassing Informatica, Spark, Kafka, eventing/streaming, Streamsets, NiFi, AWS Data Migration Services, Azure DataFactory, Google DataProc.\n\nShowcase professional written and verbal communication skills to effectively convey complex technical concepts.\n\nUndergraduate or Graduate degree preferred",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['azure data lake', 'nosql', 'elastic search', 'cassandra', 'solr', 'core data', 'cloudera', 'python', 'data analytics', 'scala', 'kudu', 'microsoft azure', 'data engineering', 'data bricks', 'gen', 'java', 'apache nifi', 'spark', 'gcp', 'kafka', 'software engineering', 'hadoop', 'aws', 'data integration']",2025-06-12 13:41:21
Data Engineer - Scala,Capco,5 - 8 years,Not Disclosed,['Bengaluru'],"The Senior Data Engineer will be responsible for designing, developing, and maintaining scalable data pipelines and building out new API integrations to support continuing increases in data volume and complexity. They will collaborate with analytics and business teams to improve data models that feed business intelligence tools, increasing data accessibility and fostering data-driven decision making across the organization.\nResponsibilities:\nDesign, construct, install, test and maintain highly scalable data management systems & Data Pipeline.\nEnsure systems meet business requirements and industry practices.\nBuild high-performance algorithms, prototypes, predictive models, and proof of concepts.\nResearch opportunities for data acquisition and new uses for existing data.\nDevelop data set processes for data modeling, mining and production.\nIntegrate new data management technologies and software engineering tools into existing structures.\nCreate custom software components and analytics applications.\nInstall and update disaster recovery procedures.\nCollaborate with data architects, modelers, and IT team members on project goals.\nProvide senior level technical consulting to peer data engineers during data application design and development for highly complex and critical data projects.\nQualifications:\nBachelors degree in computer science, Engineering, or related field, or equivalent work experience.\nProven 5-8 years of experience as a Senior Data Engineer or similar role.\nExperience with big data tools: Hadoop, Spark, Kafka, Ansible, chef, Terraform, Airflow, and Protobuf RPC etc.\nExpert level SQL skills for data manipulation (DML) and validation (DB2).\nExperience with data pipeline and workflow management tools.\nExperience with object-oriented/object function scripting languages: Python, Java, Go lang etc.\nStrong problem solving and analytical skills.\nExcellent verbal communication skills.\nGood interpersonal skills.\nAbility to provide technical leadership for the team.",Industry Type: Banking,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Data management', 'Db2', 'Data modeling', 'Consulting', 'Disaster recovery', 'Business intelligence', 'Analytics', 'SQL', 'Python']",2025-06-12 13:41:23
Data Engineer (C2H),First Mile Consulting,4 - 8 years,Not Disclosed,"['Hyderabad', 'Pune', 'Chennai', 'Bengaluru']","Very strong in python, pyspark and SQL. Good experience in any cloud . They use AWS but any cloud experience is ok. They will train on other things but if candidates have experience with ETL (like AWS Airflow), datalakes like Snowflake",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Temporary/Contractual","['Pyspark', 'Cloud', 'Data engineer', 'Python', 'Sql', 'Airflow']",2025-06-12 13:41:26
Data Engineer,Lenskart,1 - 4 years,Not Disclosed,['Bengaluru'],"Key Responsibilities\nBuild and maintain scalable ETL/ELT data pipelines using Python and cloud-native tools.\nDesign and optimize data models and queries on Google BigQuery for analytical workloads.\nDevelop, schedule, and monitor workflows using orchestration tools like Apache Airflow or Cloud Composer.\nIngest and integrate data from multiple structured and semi-structured sources, including MySQL, MongoDB, APIs, and cloud storage.",,,,"['GCP', 'Bigquery', 'MySQL', 'MongoDB', 'Python']",2025-06-12 13:41:28
Data Engineer,Xenonstack,2 - 5 years,Not Disclosed,['Mohali( Phase 8B Mohali )'],"At XenonStack, We committed to become the Most Value Driven Cloud Native, Platform Engineering and Decision Driven Analytics Company. Our Consulting Services and Solutions towards the Neural Company and its Key Drivers.\nXenonStacks DataOps team is looking for a Data Engineer who will be responsible for employing techniques to create and sustain structures that allow for the analysis of data while remaining familiar with dominant programming and deployment strategies in the field.\nYou should demonstrate flexibility, creativity, and the capacity to receive and utilize constructive criticism. The ideal candidate should be highly skilled in all aspects of Python, Java/Scala, SQL and analytical skills.\nJob Responsibilities:\nDevelop, construct, test and maintain Data Platform Architectures\nAlign Data Architecture with business requirements\nLiaising with co-workers and clients to elucidate the requirements for each task.\nScalable and High Performant Data Platform Infrastructure that allows big data to be accessed and analysed quickly by BI & AI Teams.\nReformulating existing frameworks to optimize their functioning.\nTransforming Raw Data into InSights for manipulation by Data Scientists.\nEnsuring that your work remains backed up and readily accessible to relevant co-workers.\nRemaining up-to-date with industry standards and technological advancements that will improve the quality of your outputs.\nRequirements:\nTechnical Requirements\nExperience of Python, Java/Scala\nGreat Statistical / SQL based Analytical Skills\nExperience of Data Analytics Architectural Design Patterns for Batch, Event Driven and Real-Time Analytics Use Cases\nUnderstanding of Data warehousing, ETL tools, machine learning, Data EPIs\nExcellent in Algorithms and Data Systems\nUnderstanding of Distributed System for Data Processing and Analytics\nFamiliarity with Popular Data Analytics Framework like Hadoop , Spark , Delta Lake , Time Series / Analytical Stores Stores.\nProfessional Attributes:\nExcellent communication skills & Attention to detail.\nAnalytical mind and problem-solving Aptitude with Strong Organizational skills & Visual Thinking.\nBenefits:\nDiscover the benefits of joining our team:\nDynamic and purposeful work culture in a people-oriented organization contributing to multi-million-dollar projects with guaranteed job security.\nOpen, authentic, and transparent communication fostering a warm work environment.\nRegular constructive feedback and exposure to diverse technologies.\nRecognition and rewards for exceptional performance achievements.\nAccess to certification courses & Skill Sessions to develop continually and refine your skills.\nAdditional allowances for team members assigned to specific projects.\nSpecial skill allowances to acknowledge and compensate for unique expertise.\nComprehensive medical insurance policy for your health and well-being.\nTo Learn more about the company -\nWebsite - http://www.xenonstack.com/",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Hadoop', 'Spark', 'ETL', 'Python', 'SQL', 'Java', 'Data Processing', 'Machine Learning']",2025-06-12 13:41:30
Data Engineer - Databricks,KPI Partners,2 - 5 years,Not Disclosed,"['Hyderabad', 'Pune', 'Bengaluru']","About KPI Partners.\nKPI Partners is a leading provider of data analytics solutions, dedicated to helping organizations transform data into actionable insights. Our innovative approach combines advanced technology with expert consulting, allowing businesses to leverage their data for improved performance and decision-making.\n\nJob Description.\nWe are seeking a skilled and motivated Data Engineer with experience in Databricks to join our dynamic team. The ideal candidate will be responsible for designing, building, and maintaining scalable data pipelines and data processing solutions that support our analytics initiatives. You will collaborate closely with data scientists, analysts, and other engineers to ensure the consistent flow of high-quality data across our platforms.",,,,"['python', 'data analytics', 'analytical', 'scala', 'pyspark', 'microsoft azure', 'data warehousing', 'data pipeline', 'data architecture', 'data engineering', 'sql', 'data bricks', 'cloud', 'analytics', 'data quality', 'data modeling', 'gcp', 'teamwork', 'integration', 'aws', 'etl', 'programming', 'communication skills', 'etl scripts']",2025-06-12 13:41:33
Data Engineer,Luxoft,5 - 8 years,Not Disclosed,['Pune'],"Help Group Enterprise Architecture team to develop our suite of EA tools and workbenches\nWork in the development team to support the development of portfolio health insights\nBuild data applications from cloud infrastructure to visualization layer\nProduce clear and commented code\nProduce clear and comprehensive documentation\nPlay an active role with technology support teams and ensure deliverables are completed or escalated on time\nProvide support on any related presentations, communications, and trainings\nBe a team player, working across the organization with skills to indirectly manage and influence\nBe a self-starter willing to inform and educate others\nSkills\nMust have\nB.Sc./M.Sc. degree in computing or similar\n5-8+ years experience as a Data Engineer, ideally in a large corporate environment\nIn-depth knowledge of SQL and data modelling/data processing\nStrong experience working with Microsoft Azure\nExperience with visualisation tools like PowerBI (or Tableau, QlikView or similar)\nExperience working with Git, JIRA, GitLab\nStrong flair for data analytics\nStrong flair for IT architecture and IT architecture metrics\nExcellent stakeholder interaction and communication skills\nUnderstanding of performance implications when making design decisions to deliver performant and maintainable software.\nExcellent end-to-end SDLC process understanding.\nProven track record of delivering complex data apps on tight timelines\nFluent in English both written and spoken.\nPassionate about development with focus on data and cloud\nAnalytical and logical, with strong problem solving skills\nA team player, comfortable with taking the lead on complex tasks\nAn excellent communicator who is adept in, handling ambiguity and communicating with both technical and non-technical audiences\nComfortable with working in cross-functional global teams to effect change\nPassionate about learning and developing your hard and soft professional skills\nNice to have\nExperience working in the financial industry\nExperience in complex metrics design and reporting\nExperience in using artificial intelligence for data analytics\nOther\nLanguages\nEnglish: C1 Advanced\nSeniority\nSenior\nRefer a Friend\nPositive work environments and stellar reputations attract and retain top talent. Find out why Luxoft stands apart from the rest.\nRecommend a friend\nRelated jobs View all vacancies\nSenior Power BI Developer\nBI Engineering\nIndia\nBengaluru\nSenior Power BI Developer\nBI Engineering\nIndia\nChennai\nSenior Power BI Developer\nBI Engineering\nIndia\nGurugram\nPune, India\nReq. VR-114797\nBI Engineering\nBCM Industry\n02/06/2025\nReq. VR-114797\nApply for Data Engineer in Pune\n*",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['GIT', 'Enterprise architecture', 'Analytical', 'Artificial Intelligence', 'Data processing', 'Data analytics', 'QlikView', 'JIRA', 'SDLC', 'SQL']",2025-06-12 13:41:35
Data Engineer _Technology Lead,Broadridge,6 - 10 years,Not Disclosed,['Bengaluru'],"Key Responsibilities:\nAnalyzes and solve problems using technical experience, judgment and precedents\nProvides informal guidance to new team members\nExplains complex information to others in straightforward situations\n1. Data Engineering and Modelling:\nDesign & Develop Scalable Data Pipelines: Leverage AWS technologies to design, develop, and manage end-to-end data pipelines with services like .",,,,"['Star Schema', 'Snowflake', 'AWS', 'Apache Airflow']",2025-06-12 13:41:37
Data Engineer,Prohr Strategies,9 - 11 years,Not Disclosed,['Bengaluru'],"Hands-on Data Engineer with strong Databricks expertise in Git/DevOps integration, Unity Catalog governance, and performance tuning of data transformation workloads. Skilled in optimizing pipelines and ensuring secure, efficient data operations.",Industry Type: Emerging Technologies (AI/ML),Department: Data Science & Analytics,"Employment Type: Full Time, Temporary/Contractual","['Data Transformation', 'GIT', 'Azure Databricks', 'Databricks', 'Devops', 'Data Engineering', 'Governance', 'Catalog', 'Code Versioning Tools']",2025-06-12 13:41:39
Consultant - Data Engineer,Affine Analytics,5 - 8 years,Not Disclosed,['Bengaluru'],"We are looking for a highly skilled API & Pixel Tracking Integration Engineer to lead the development and deployment of server-side tracking and attribution solutions across multiple platforms. The ideal candidate brings deep expertise in CAPI integrations (Meta, Google, and other platforms), secure data handling using cryptographic techniques, and experience working within privacy-first environments like Azure Clean Rooms.\nThis role requires strong hands-on experience in Azure cloud services, OCI (Oracle Cloud Infrastructure), and marketing technology stacks including Adobe Tag Management and Pixel Management. You will work closely with engineering, analytics, and marketing teams to deliver scalable, compliant, and secure data tracking solutions that drive business insights and performance.",,,,"['Python', 'Azure Cloud technologies', 'Azure Data Factory', 'Adobe Tag Management', 'Azure security', 'ADF', 'ADLS', 'Azure Functions', 'Key Vault']",2025-06-12 13:41:41
Big Data Engineer - Hadoop,Info Origin Technologies Pvt Ltd,3 - 7 years,Not Disclosed,"['Hyderabad', 'Gurugram']","Role: Hadoop Data Engineer\nLocation: Gurgaon / Hyderabad\nWork Mode: Hybrid\nEmployment Type: Full-Time\nInterview Mode: First Video then In Person\nJob Description\nJob Overview:\nWe are looking for experienced Data Engineers proficient in Hadoop, Hive, Python, SQL, and Pyspark/Spark to join our dynamic team. Candidates will be responsible for designing, developing, and maintaining scalable big data solutions.\nKey Responsibilities:\nDevelop and optimize data pipelines for large-scale data processing.\nWork with structured and unstructured datasets to derive actionable insights.\nCollaborate with cross-functional teams to enhance data-driven decision-making.\nEnsure the performance, scalability, and reliability of data architectures.\nImplement best practices for data security and governance.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Hive', 'Hadoop', 'Pyspark', 'Big Data', 'Python', 'SQL']",2025-06-12 13:41:43
Senior Data Engineer - Azure,Blend360 India,3 - 6 years,Not Disclosed,['Hyderabad'],"Job Description\nAs a Data Engineer, your role is to spearhead the data engineering teams and elevate the team to the next level! You will be responsible for laying out the architecture of the new project as well as selecting the tech stack associated with it. You will plan out the development cycles deploying AGILE if possible and create the foundations for good data stewardship with our new data products!\nYou will also set up a solid code framework that needs to be built to purpose yet have enough flexibility to adapt to new business use cases tough but rewarding challenge!\n\nResponsibilities\nCollaborate with several stakeholders to deeply understand the needs of data practitioners to deliver at scale\nLead Data Engineers to define, build and maintain Data Platform\nWork on building Data Lake in Azure Fabric processing data from multiple sources\nMigrating existing data store from Azure Synapse to Azure Fabric\nImplement data governance and access control\nDrive development effort End-to-End for on-time delivery of high-quality solutions that conform to requirements, conform to the architectural vision, and comply with all applicable standards.\nPresent technical solutions, capabilities, considerations, and features in business terms. Effectively communicate status, issues, and risks in a precise and timely manner.\nFurther develop critical initiatives, such as Data Discovery, Data Lineage and Data Quality\nLeading team and Mentor junior resources\nHelp your team members grow in their role and achieve their career aspirations\nBuild data systems, pipelines, analytical tools and programs\nConduct complex data analysis and report on results\n\n\nQualifications\n3+ Years of Experience as a data engineer or similar role in Azure Synapses, ADF or\nrelevant exp in Azure Fabric\nDegree in Computer Science, Data Science, Mathematics, IT, or similar fiel",Industry Type: Advertising & Marketing,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Data analysis', 'Data modeling', 'Analytical', 'Agile', 'data governance', 'Data quality', 'Data mining', 'SQL', 'Python']",2025-06-12 13:41:45
Microsoft Fabrics Data Engineer,Swits Digital,5 - 10 years,Not Disclosed,['Bengaluru'],"Job TItle: Microsoft Fabric Data Engineer\nLocation: Bangalore\nJob Type: Conract (24 Months)\nJob Description:\nWe are seeking a highly skilled and experienced Microsoft Fabric Data Engineer/Architect to design, develop, and maintain robust, scalable, and secure data solutions within the Microsoft Fabric ecosystem. This role will leverage the full suite of Microsoft Azure data services, including Azure Data Bricks, Azure Data Factory, and Azure Data Lake, to build end-to-end data pipelines, data warehouses, and data lakehouses that enable advanced analytics and business intelligence.\nRequired Skills & Qualifications:\nBachelors degree in Computer Science, Engineering, or a related field.\n5+ years of experience in data architecture and engineering, with a strong focus on Microsoft Azure data platforms.\nProven hands-on expertise with Microsoft Fabric and its components, including:\nOneLake\nData Factory (Pipelines, Dataflows Gen2)\nSynapse Analytics (Data Warehousing, SQL analytics endpoint)\nLakehouses and Warehouses\nNotebooks (PySpark)\nExtensive experience with Azure Data Bricks, including Spark development (PySpark, Scala, SQL).\nStrong proficiency in Azure Data Factory for building and orchestrating ETL/ELT pipelines.\nDeep understanding and experience with Azure Data Lake Storage Gen2.\nProficiency in SQL (T-SQL, Spark SQL), Python, and/or other relevant scripting languages.\nSolid understanding of data warehousing concepts, dimensional modeling, and data lakehouse architectures.\nExperience with data governance principles and tools (e.g., Microsoft Purview).\nFamiliarity with CI/CD practices, version control (Git), and DevOps for data pipelines.\nExcellent problem-solving, analytical, and communication skills.\nAbility to work independently and collaboratively in a fast-paced, agile environment.\nPreferred Qualifications:\nMicrosoft certifications in Azure Data Engineering (e.g., DP-203, DP-600: Microsoft Fabric Analytics Engineer Associate).\nExperience with Power BI for data visualization and reporting.\nFamiliarity with real-time analytics and streaming data processing.\nExposure to machine learning workflows and integrating ML models with data solutions",Industry Type: Recruitment / Staffing,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['GIT', 'Analytical', 'microsoft azure', 'data visualization', 'microsoft', 'Business intelligence', 'Data warehousing', 'Analytics', 'Data architecture', 'Python']",2025-06-12 13:41:48
Data Engineer,Nemetschek,5 - 10 years,Not Disclosed,"['Hyderabad', 'Bengaluru', 'Mumbai (All Areas)']","Role & responsibilities\n5+ years in software development, with a focus on data-intensive applications, cloud solutions, and scalable data architectures.\nDevelopment experience in GoLang for building scalable and efficient data applications.\nExperience with Snowflake, Redshift, or similar data platforms including architecture, data modeling, performance optimization, and integrations.\nExperience designing and building data lakes and data warehouses, ensuring data integrity, scalability, and performance.\nProficient in developing and managing ETL pipelines, using modern tools and techniques to transform, load, and integrate data efficiently.\nExperience with high-volume event streams (such as Kafka, Kinesis) and near real-time data processing solutions for fast and accurate reporting.\nHands-on experience with Terraform for automating infrastructure deployment and configuration management in cloud environments.\nExperience with containerization technologies (Docker, Kubernetes) and orchestration.\nSolid grasp of database fundamentals (SQL, NoSQL, data modeling, performance tuning)\nExperience with CI/CD pipelines and automation tools for testing, deployment, and continuous improvement.\nExperience working in AWS cloud environments, specifically with big data solutions and serverless architectures\nAbility to mentor and guide junior engineers, fostering a culture of learning and innovation\nStrong communication skills to articulate technical concepts clearly to non-technical stakeholders.\nWHAT WE OFFER\nA young, dynamic, and innovation-oriented environment\nA wide variety of projects within different industries\nA very open and informal culture where knowledge sharing, and employee development are key.\nRoom for personal initiative, development, and growth\nRealistic career opportunities\nCompetitive package and fringe benefits.\n\nPreferred candidate profile",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Golang', 'Snowflake', 'Javascript', 'ETL', 'AWS']",2025-06-12 13:41:50
Senior Data Engineer,Qualcomm,5 - 10 years,Not Disclosed,['Hyderabad'],"Job Area: Information Technology Group, Information Technology Group > IT Data Engineer\n\nGeneral Summary:\n\nDeveloper will play an integral role in the PTEIT Machine Learning Data Engineering team. Design, develop and support data pipelines in a hybrid cloud environment to enable advanced analytics. Design, develop and support CI/CD of data pipelines and services. - 5+ years of experience with Python or equivalent programming using OOPS, Data Structures and Algorithms - Develop new services in AWS using server-less and container-based services. - 3+ years of hands-on experience with AWS Suite of services (EC2, IAM, S3, CDK, Glue, Athena, Lambda, RedShift, Snowflake, RDS) - 3+ years of expertise in scheduling data flows using Apache Airflow - 3+ years of strong data modelling (Functional, Logical and Physical) and data architecture experience in Data Lake and/or Data Warehouse - 3+ years of experience with SQL databases - 3+ years of experience with CI/CD and DevOps using Jenkins - 3+ years of experience with Event driven architecture specially on Change Data Capture - 3+ years of Experience in Apache Spark, SQL, Redshift (or) Big Query (or) Snowflake, Databricks - Deep understanding building the efficient data pipelines with data observability, data quality, schema drift, alerting and monitoring. - Good understanding of the Data Catalogs, Data Governance, Compliance, Security, Data sharing - Experience in building the reusable services across the data processing systems. - Should have the ability to work and contribute beyond defined responsibilities - Excellent communication and inter-personal skills with deep problem-solving skills.\n\nMinimum Qualifications:\n3+ years of IT-related work experience with a Bachelor's degree in Computer Engineering, Computer Science, Information Systems or a related field.\nOR\n5+ years of IT-related work experience without a Bachelors degree.\n\n2+ years of any combination of academic or work experience with programming (e.g., Java, Python).\n1+ year of any combination of academic or work experience with SQL or NoSQL Databases.\n1+ year of any combination of academic or work experience with Data Structures and algorithms.\n5 years of Industry experience and minimum 3 years experience in Data Engineering development with highly reputed organizations- Proficiency in Python and AWS- Excellent problem-solving skills- Deep understanding of data structures and algorithms- Proven experience in building cloud native software preferably with AWS suit of services- Proven experience in design and develop data models using RDBMS (Oracle, MySQL, etc.)\n\nDesirable - Exposure or experience in other cloud platforms (Azure and GCP) - Experience working on internals of large-scale distributed systems and databases such as Hadoop, Spark - Working experience on Data Lakehouse platforms (One House, Databricks Lakehouse) - Working experience on Data Lakehouse File Formats (Delta Lake, Iceberg, Hudi)\n\nBachelor's or Master's degree in Computer Science, Software Engineering, or a related field.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['algorithms', 'python', 'data quality', 'data structures', 'aws', 'schema', 'continuous integration', 'glue', 'amazon redshift', 'event driven architecture', 'ci/cd', 'data engineering', 'sql', 'alerts', 'java', 'data modeling', 'spark', 'devops', 'data flow', 'nosql databases', 'sql database']",2025-06-12 13:41:52
Data Engineer with Neo4j,Luxoft,3 - 5 years,Not Disclosed,['Gurugram'],"Graph Data Modeling & Implementation.\nDesign and implement complex graph data models using Cypher and Neo4j best practices.\nLeverage APOC procedures, custom plugins, and advanced graph algorithms to solve domain-specific problems.\nOversee integration of Neo4j with other enterprise systems, microservices, and data platforms.\nDevelop and maintain APIs and services in Java, Python, or JavaScript to interact with the graph database.\nMentor junior developers and review code to maintain high-quality standards.\nEstablish guidelines for performance tuning, scalability, security, and disaster recovery in Neo4j environments.\nWork with data scientists, analysts, and business stakeholders to translate complex requirements into graph-based solutions.\nSkills\nMust have\n12+ years in software/data engineering, with at least 3-5 years hands-on experience with Neo4j.\nLead the technical strategy, architecture, and delivery of Neo4j-based solutions.\nDesign, model, and implement complex graph data structures using Cypher and Neo4j best practices.\nGuide the integration of Neo4j with other data platforms and microservices.\nCollaborate with cross-functional teams to understand business needs and translate them into graph-based models.\nMentor junior developers and ensure code quality through reviews and best practices.\nDefine and enforce performance tuning, security standards, and disaster recovery strategies for Neo4j.\nStay up-to-date with emerging technologies in the graph database and data engineering space.\nStrong proficiency in Cypher query language, graph modeling, and data visualization tools (e.g., Bloom, Neo4j Browser).\nSolid background in Java, Python, or JavaScript and experience integrating Neo4j with these languages.\nExperience with APOC procedures, Neo4j plugins, and query optimization.\nFamiliarity with cloud platforms (AWS) and containerization tools (Docker, Kubernetes).\nProven experience leading engineering teams or projects.\nExcellent problem-solving and communication skills.\nNice to have\nN/A\nOther\nLanguages\nEnglish: C1 Advanced\nSeniority\nSenior\nRefer a Friend\nPositive work environments and stellar reputations attract and retain top talent. Find out why Luxoft stands apart from the rest.\nRecommend a friend\nRelated jobs View all vacancies\nSenior Flexera Data Analyst\nData Science\nIndia\nBengaluru\nSenior Flexera Data Analyst\nData Science\nIndia\nChennai\nData Scientist\nData Science\nIndia\nBengaluru\nGurugram, India\nReq. VR-114556\nData Science\nBCM Industry\n23/05/2025\nReq. VR-114556\nApply for Data Engineer with Neo4j in Gurugram\n*",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Performance tuning', 'query optimization', 'neo4j', 'data science', 'Data modeling', 'Disaster recovery', 'Javascript', 'Data structures', 'data visualization', 'Python']",2025-06-12 13:41:54
Data Engineer with Neo4j,Luxoft,3 - 5 years,Not Disclosed,['Chennai'],"Graph Data Modeling & Implementation.\nDesign and implement complex graph data models using Cypher and Neo4j best practices.\nLeverage APOC procedures, custom plugins, and advanced graph algorithms to solve domain-specific problems.\nOversee integration of Neo4j with other enterprise systems, microservices, and data platforms.\nDevelop and maintain APIs and services in Java, Python, or JavaScript to interact with the graph database.\nMentor junior developers and review code to maintain high-quality standards.\nEstablish guidelines for performance tuning, scalability, security, and disaster recovery in Neo4j environments.\nWork with data scientists, analysts, and business stakeholders to translate complex requirements into graph-based solutions.\nSkills\nMust have\n12+ years in software/data engineering, with at least 3-5 years hands-on experience with Neo4j.\nLead the technical strategy, architecture, and delivery of Neo4j-based solutions.\nDesign, model, and implement complex graph data structures using Cypher and Neo4j best practices.\nGuide the integration of Neo4j with other data platforms and microservices.\nCollaborate with cross-functional teams to understand business needs and translate them into graph-based models.\nMentor junior developers and ensure code quality through reviews and best practices.\nDefine and enforce performance tuning, security standards, and disaster recovery strategies for Neo4j.\nStay up-to-date with emerging technologies in the graph database and data engineering space.\nStrong proficiency in Cypher query language, graph modeling, and data visualization tools (e.g., Bloom, Neo4j Browser).\nSolid background in Java, Python, or JavaScript and experience integrating Neo4j with these languages.\nExperience with APOC procedures, Neo4j plugins, and query optimization.\nFamiliarity with cloud platforms (AWS) and containerization tools (Docker, Kubernetes).\nProven experience leading engineering teams or projects.\nExcellent problem-solving and communication skills.\nNice to have\nN/A\nOther\nLanguages\nEnglish: C1 Advanced\nSeniority\nSenior\nRefer a Friend\nPositive work environments and stellar reputations attract and retain top talent. Find out why Luxoft stands apart from the rest.\nRecommend a friend\nRelated jobs View all vacancies\nSenior Flexera Data Analyst\nData Science\nIndia\nGurugram\nSenior Flexera Data Analyst\nData Science\nIndia\nBengaluru\nData Scientist\nData Science\nIndia\nBengaluru\nChennai, India\nReq. VR-114556\nData Science\nBCM Industry\n23/05/2025\nReq. VR-114556\nApply for Data Engineer with Neo4j in Chennai\n*",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Performance tuning', 'query optimization', 'neo4j', 'data science', 'Data modeling', 'Disaster recovery', 'Javascript', 'Data structures', 'data visualization', 'Python']",2025-06-12 13:41:57
Data Engineer with Neo4j,Luxoft,3 - 5 years,Not Disclosed,['Bengaluru'],"Graph Data Modeling & Implementation.\nDesign and implement complex graph data models using Cypher and Neo4j best practices.\nLeverage APOC procedures, custom plugins, and advanced graph algorithms to solve domain-specific problems.\nOversee integration of Neo4j with other enterprise systems, microservices, and data platforms.\nDevelop and maintain APIs and services in Java, Python, or JavaScript to interact with the graph database.\nMentor junior developers and review code to maintain high-quality standards.\nEstablish guidelines for performance tuning, scalability, security, and disaster recovery in Neo4j environments.\nWork with data scientists, analysts, and business stakeholders to translate complex requirements into graph-based solutions.\nSkills\nMust have\n12+ years in software/data engineering, with at least 3-5 years hands-on experience with Neo4j.\nLead the technical strategy, architecture, and delivery of Neo4j-based solutions.\nDesign, model, and implement complex graph data structures using Cypher and Neo4j best practices.\nGuide the integration of Neo4j with other data platforms and microservices.\nCollaborate with cross-functional teams to understand business needs and translate them into graph-based models.\nMentor junior developers and ensure code quality through reviews and best practices.\nDefine and enforce performance tuning, security standards, and disaster recovery strategies for Neo4j.\nStay up-to-date with emerging technologies in the graph database and data engineering space.\nStrong proficiency in Cypher query language, graph modeling, and data visualization tools (e.g., Bloom, Neo4j Browser).\nSolid background in Java, Python, or JavaScript and experience integrating Neo4j with these languages.\nExperience with APOC procedures, Neo4j plugins, and query optimization.\nFamiliarity with cloud platforms (AWS) and containerization tools (Docker, Kubernetes).\nProven experience leading engineering teams or projects.\nExcellent problem-solving and communication skills.\nNice to have\nN/A\nOther\nLanguages\nEnglish: C1 Advanced\nSeniority\nSenior\nRefer a Friend\nPositive work environments and stellar reputations attract and retain top talent. Find out why Luxoft stands apart from the rest.\nRecommend a friend\nRelated jobs View all vacancies\nSenior Flexera Data Analyst\nData Science\nIndia\nGurugram\nSenior Flexera Data Analyst\nData Science\nIndia\nChennai\nBusiness Analyst\nData Science\nPoland\nRemote Poland\nBengaluru, India\nReq. VR-114556\nData Science\nBCM Industry\n23/05/2025\nReq. VR-114556\nApply for Data Engineer with Neo4j in Bengaluru\n*",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Performance tuning', 'query optimization', 'neo4j', 'data science', 'Data modeling', 'Disaster recovery', 'Javascript', 'Data structures', 'data visualization', 'Python']",2025-06-12 13:42:00
Senior Data Engineer - AWS,Blend360 India,5 - 10 years,Not Disclosed,['Hyderabad'],"Job Description\nWe are looking for an experienced Senior Data Engineer with a strong foundation in Python, SQL, and Spark , and hands-on expertise in AWS, Databricks . In this role, you will build and maintain scalable data pipelines and architecture to support analytics, data science, and business intelligence initiatives. You ll work closely with cross-functional teams to drive data reliability, quality, and performance.\nResponsibilities:\nDesign, develop, and optimize scalable data pipelines using Databricks in AWS such as Glue, S3, Lambda, EMR, Databricks notebooks, workflows and jobs.\nBuilding data lake in WS Databricks.\nBuild and maintain robust ETL/ELT workflows using Python and SQL to handle structured and semi-structured data.\nDevelop distributed data processing solutions using Apache Spark or PySpark .\nPartner with data scientists and analysts to provide high-quality, accessible, and well-structured data.\nEnsure data quality, governance, security, and compliance across pipelines and data stores.\nMonitor, troubleshoot, and improve the performance of data systems and pipelines.\nParticipate in code reviews and help establish engineering best practices.\nMentor junior data engineers and support their technical development.\n\n\nQualifications\nRequirements\nBachelors or masters degree in computer science, Engineering, or a related field.\n5+ years of hands-on experience in data",Industry Type: Advertising & Marketing,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Automation', 'Version control', 'GIT', 'Workflow', 'Data quality', 'Business intelligence', 'Analytics', 'SQL', 'Python']",2025-06-12 13:42:03
Data Engineer,Amakshar Technology,4 - 7 years,Not Disclosed,"['Mumbai', 'Pune', 'Chennai', 'Bengaluru']","Job Category: IT\nJob Type: Full Time\nJob Location: Bangalore Chennai Mumbai Pune\nLocation- Mumbai, Pune, Bangalore, Chennai\nExperience- 5+\nData Engineer: Expertise in Python Language is MUST. SQL (should be able to write complex SQL Queries) is MUST Data Lake Development experience. Orchestration (Apache Airflow is preferred). Spark and Hive: Optimization of Spark/PySpark and Hive apps is MUST Trino/(AWS Athena) (Good to have) Snowflake (good to have). Data Quality (good to have). File Storage (S3 is good to have)\nKind Note: Please apply or share your resume only if it matches the above criteria",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['hive', 'SQL queries', 'orchestration', 'spark', 'Data quality', 'Apache', 'AWS', 'Python']",2025-06-12 13:42:06
Consultant - Data Engineer (with Fabric),Affine Analytics,5 - 8 years,Not Disclosed,['Bengaluru'],"We are looking for a highly skilled API & Pixel Tracking Integration Engineer to lead the development and deployment of server-side tracking and attribution solutions across multiple platforms. The ideal candidate brings deep expertise in CAPI integrations (Meta, Google, and other platforms), secure data handling using cryptographic techniques, and experience working within privacy-first environments like Azure Clean Rooms.\nThis role requires strong hands-on experience in C# development, Azure cloud services, OCI (Oracle Cloud Infrastructure), and marketing technology stacks including Adobe Tag Management and Pixel Management. You will work closely with engineering, analytics, and marketing teams to deliver scalable, compliant, and secure data tracking solutions that drive business insights and performance.",,,,"['Adobe Tag Management', 'Data Engineering', 'Azure Data Factory', 'Azure security', 'ADF', 'ADLS', 'Azure Functions', 'Meta CAPI', 'Google Enhanced Conversions', 'Key Vault', 'Cosmos DB']",2025-06-12 13:42:08
Senior Data Engineer,Fractal Analytics,8 - 10 years,Not Disclosed,['Mumbai'],"Job Description:\nAs a Backend (Java) Engineer, you would be part of the team consisting of Scrum Master, Cloud Engineers, AI/ML Engineers, and UI/UX Engineers to build end-to-end Data to Decision Systems.\nMandatory:\n8+ years of demonstrable experience designing, building, and working as a Java Developer for enterprise web applications\nIdeally, this would include the following:\no Expert-level proficiency with Java\no Expert-level proficiency with SpringBoot\nFamiliarity with common databases (RDBMS such as MySQL & NoSQL such as MongoDB) and data warehousing concepts (OLAP, OLTP)\nUnderstanding of REST concepts and building/interacting with REST APIs\nDeep understanding of core backend concepts:\no Develop and design RESTful services and APIs\no Develop functional databases, applications, and servers to support websites on the back end\no Performance optimization and multithreading concepts\no Experience with deploying and maintaining high traffic infrastructure (performance testing is a plus)\nIn addition, the ideal candidate would have great problem-solving skills, and familiarity with code versioning tools such as GitHub",,,,"['Backend', 'Multithreading', 'RDBMS', 'MySQL', 'Performance testing', 'OLAP', 'Scrum', 'MongoDB', 'Apache', 'OLTP']",2025-06-12 13:42:11
Data Engineer,IT Services Company,2 - 3 years,6-7 Lacs P.A.,['Pune'],"Data Engineer\nJob Description :\nJash Data Sciences: Letting Data Speak!\nDo you love solving real-world data problems with the latest and best techniques? And having fun while solving them in a team! Then come and join our high-energy team of passionate data people. Jash Data Sciences is the right place for you.\nWe are a cutting-edge Data Sciences and Data Engineering startup based in Pune, India. We believe in continuous learning and evolving together. And we let the data speak!\nWhat will you be doing?\nYou will be discovering trends in the data sets and developing algorithms to transform\nraw data for further analytics\nCreate Data Pipelines to bring in data from various sources, with different formats,\ntransform it, and finally load it to the target database.\nImplement ETL/ ELT processes in the cloud using tools like AirFlow, Glue, Stitch, Cloud\nData Fusion, and DataFlow.\nDesign and implement Data Lake, Data Warehouse, and Data Marts in AWS, GCP, or\nAzure using Redshift, BigQuery, PostgreSQL, etc.\nCreating efficient SQL queries and understanding query execution plans for tuning\nqueries on engines like PostgreSQL.\nPerformance tuning of OLAP/ OLTP databases by creating indices, tables, and views.\nWrite Python scripts for the orchestration of data pipelines\nHave thoughtful discussions with customers to understand their data engineering\nrequirements. Break complex requirements into smaller tasks for execution.\nWhat do we need from you?\nStrong Python coding skills with basic knowledge of algorithms/data structures and\ntheir application.\nStrong understanding of Data Engineering concepts including ETL, ELT, Data Lake, Data\nWarehousing, and Data Pipelines.\nExperience designing and implementing Data Lakes, Data Warehouses, and Data Marts\nthat support terabytes of scale data.\nA track record of implementing Data Pipelines on public cloud environments\n(AWS/GCP/Azure) is highly desirable\nA clear understanding of Database concepts like indexing, query performance\noptimization, views, and various types of schemas.\nHands-on SQL programming experience with knowledge of windowing functions,\nsubqueries, and various types of joins.\nExperience working with Big Data technologies like PySpark/ Hadoop\nA good team player with the ability to communicate with clarity\nShow us your git repo/ blog!\nQualification\n1-2 years of experience working on Data Engineering projects for Data Engineer I\n2-5 years of experience working on Data Engineering projects for Data Engineer II\n1-5 years of Hands-on Python programming experience\nBachelors/Masters' degree in Computer Science is good to have\nCourses or Certifications in the area of Data Engineering will be given a higher preference.\nCandidates who have demonstrated a drive for learning and keeping up to date with technology by continuing to do various courses/self-learning will be given high preference.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Airflow', 'Elt', 'Data Mart', 'Data Pipeline', 'ETL', 'Pyspark', 'Hadoop', 'Data Bricks', 'SQL', 'Data Fusion', 'Glue', 'GCP', 'Data Flow', 'Data Warehousing', 'Azzure', 'AWS']",2025-06-12 13:42:13
Cloud Data Engineer,PwC India,3 - 8 years,Not Disclosed,"['Gurugram', 'Bengaluru']","Job Description:\n\nWe are seeking skilled and dynamic Cloud Data Engineers specializing in AWS, Azure, Databricks. The ideal candidate will have a strong background in data engineering, with a focus on data ingestion, transformation, and warehousing. They should also possess excellent knowledge of PySpark or Spark, and a proven ability to optimize performance in Spark job executions.\n\nKey Responsibilities:",,,,"['AWS OR Azure', 'Azure Data Engineer OR AWS Data Engineer', 'Azure', 'AWS']",2025-06-12 13:42:15
Data Engineer,XL India Business Services Pvt. Ltd,1 - 7 years,Not Disclosed,['Gurugram'],"Senior Engineer, Data Modeling Gurgaon/Bangalore, India AXA XL recognizes data and information as critical business assets, both in terms of managing risk and enabling new business opportunities\n\nThis data should not only be high quality, but also actionable - enabling AXA XL s executive leadership team to maximize benefits and facilitate sustained industrious advantage\n\nOur Chief Data Office also known as our Innovation, Data Intelligence & Analytics team (IDA) is focused on driving innovation through optimizing how we leverage data to drive strategy and create a new business model - disrupting the insurance market\n\nAs we develop an enterprise-wide data and digital strategy that moves us toward greater focus on the use of data and data-driven insights, we are seeking a Data Engineer\n\nThe role will support the team s efforts towards creating, enhancing, and stabilizing the Enterprise data lake through the development of the data pipelines\n\nThis role requires a person who is a team player and can work well with team members from other disciplines to deliver data in an efficient and strategic manner\n\nWhat you ll be doing What will your essential responsibilities include? Act as a data engineering expert and partner to Global Technology and data consumers in controlling complexity and cost of the data platform, whilst enabling performance, governance, and maintainability of the estate\n\nUnderstand current and future data consumption patterns, architecture (granular level), partner with Architects to make sure optimal design of data layers\n\nApply best practices in Data architecture\n\nFor example, balance between materialization and virtualization, optimal level of de-normalization, caching and partitioning strategies, choice of storage and querying technology, performance tuning\n\nLeading and hands-on execution of research into new technologies\n\nFormulating frameworks for assessment of new technology vs business benefit, implications for data consumers\n\nAct as a best practice expert, blueprint creator of ways of working such as testing, logging, CI/CD, observability, release, enabling rapid growth in data inventory and utilization of Data Science Platform\n\nDesign prototypes and work in a fast-paced iterative solution delivery model\n\nDesign, Develop and maintain ETL pipelines using Py spark in Azure Databricks using delta tables\n\nUse Harness for deployment pipeline\n\nMonitor Performance of ETL Jobs, resolve any issue that arose and improve the performance metrics as needed\n\nDiagnose system performance issue related to data processing and implement solution to address them\n\nCollaborate with other teams to make sure successful integration of data pipelines into larger system architecture requirement\n\nMaintain integrity and quality across all pipelines and environments\n\nUnderstand and follow secure coding practice to make sure code is not vulnerable\n\nYou will report to the Application Manager\n\nWhat you will BRING We re looking for someone who has these abilities and skills: Required Skills and Abilities: Effective Communication skills\n\nBachelor s degree in computer science, Mathematics, Statistics, Finance, related technical field, or equivalent work experience\n\nRelevant years of extensive work experience in various data engineering & modeling techniques (relational, data warehouse, semi-structured, etc), application development, advanced data querying skills\n\nRelevant years of programming experience using Databricks\n\nRelevant years of experience using Microsoft Azure suite of products (ADF, synapse and ADLS)\n\nSolid knowledge on network and firewall concepts\n\nSolid experience writing, optimizing and analyzing SQL\n\nRelevant years of experience with Python\n\nAbility to break complex data requirements and architect solutions into achievable targets\n\nRobust familiarity with Software Development Life Cycle (SDLC) processes and workflow, especially Agile\n\nExperience using Harness\n\nTechnical lead responsible for both individual and team deliveries\n\nDesired Skills and Abilities: Worked in big data migration projects\n\nWorked on performance tuning both at database and big data platforms\n\nAbility to interpret complex data requirements and architect solutions\n\nDistinctive problem-solving and analytical skills combined with robust business acumen\n\nExcellent basics on parquet files and delta files\n\nEffective Knowledge of Azure cloud computing platform\n\nFamiliarity with Reporting software - Power BI is a plus\n\nFamiliarity with DBT is a plus\n\nPassion for data and experience working within a data-driven organization\n\nYou care about what you do, and what we do\n\nWho WE are AXA XL, the P&C and specialty risk division of AXA, is known for solving complex risks\n\nFor mid-sized companies, multinationals and even some inspirational individuals we don t just provide re/insurance, we reinvent it\n\nHow? By combining a comprehensive and efficient capital platform, data-driven insights, leading technology, and the best talent in an agile and inclusive workspace, empowered to deliver top client service across all our lines of business property, casualty, professional, financial lines and specialty\n\nWith an innovative and flexible approach to risk solutions, we partner with those who move the world forward\n\nLearn more at axaxl\n\ncom What we OFFER Inclusion AXA XL is committed to equal employment opportunity and will consider applicants regardless of gender, sexual orientation, age, ethnicity and origins, marital status, religion, disability, or any other protected characteristic\n\nAt AXA XL, we know that an inclusive culture and a diverse workforce enable business growth and are critical to our success\n\nThat s why we have made a strategic commitment to attract, develop, advance and retain the most diverse workforce possible, and create an inclusive culture where everyone can bring their full selves to work and can reach their highest potential\n\nIt s about helping one another and our business to move forward and succeed\n\nFive Business Resource Groups focused on gender, LGBTQ+, ethnicity and origins, disability and inclusion with 20 Chapters around the globe Robust support for Flexible Working Arrangements Enhanced family friendly leave benefits Named to the Diversity Best Practices Index Signatory to the UK Women in Finance Charter Learn more at axaxl\n\ncom / about-us / inclusion-and-diversity\n\nAXA XL is an Equal Opportunity Employer\n\nTotal Rewards AXA XL s Reward program is designed to take care of what matters most to you, covering the full picture of your health, wellbeing, lifestyle and financial security\n\nIt provides dynamic compensation and personalized, inclusive benefits that evolve as you do\n\nWe re committed to rewarding your contribution for the long term, so you can be your best self today and look forward to the future with confidence\n\nSustainability At AXA XL, Sustainability is integral to our business strategy\n\nIn an ever-changing world, AXA XL protects what matters most for our clients and communities\n\nWe know that sustainability is at the root of a more resilient future\n\nOur 2023-26 Sustainability strategy, called Roots of resilience , focuses on protecting natural ecosystems, addressing climate change, and embedding sustainable practices across our operations\n\nOur Pillars: Valuing nature: How we impact nature affects how nature impacts us\n\nResilient ecosystems - the foundation of a sustainable planet and society - are essential to our future\n\nWe re committed to protecting and restoring nature - from mangrove forests to the bees in our backyard - by increasing biodiversity awareness and inspiring clients and colleagues to put nature at the heart of their plans\n\nAddressing climate change: The effects of a changing climate are far reaching and significant\n\nUnpredictable weather, increasing temperatures, and rising sea levels cause both social inequalities and environmental disruption\n\nWere building a net zero strategy, developing insurance products and services, and mobilizing to advance thought leadership and investment in societal-led solutions\n\nIntegrating ESG: All companies have a role to play in building a more resilient future\n\nIncorporating ESG considerations into our internal processes and practices builds resilience from the roots of our business\n\nWe re training our colleagues, engaging our external partners, and evolving our sustainability governance and reporting\n\nAXA Hearts in Action: We have established volunteering and charitable giving programs to help colleagues support causes that matter most to them, known as AXA XL s Hearts in Action programs\n\nThese include our Matching Gifts program, Volunteering Leave, and our annual volunteering day - the Global Day of Giving\n\nFor more information, please see axaxl\n\ncom/sustainability",Industry Type: Insurance,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Performance tuning', 'Data modeling', 'Coding', 'Agile', 'Workflow', 'Application development', 'SDLC', 'SQL', 'Python', 'Firewall']",2025-06-12 13:42:18
Data Engineer,Infoobjects Inc.,3 - 6 years,Not Disclosed,['Jaipur'],"Role & responsibilities:\nDesign, develop, and maintain robust ETL/ELT pipelines to ingest and process data from multiple sources.\nBuild and maintain scalable and reliable data warehouses, data lakes, and data marts.\nCollaborate with data scientists, analysts, and business stakeholders to understand data needs and deliver solutions.\nEnsure data quality, integrity, and security across all data systems.\nOptimize data pipeline performance and troubleshoot issues in a timely manner.\nImplement data governance and best practices in data management.\nAutomate data validation, monitoring, and reporting processes.\n\n\n\nPreferred candidate profile:\nBachelor's or Masters degree in Computer Science, Engineering, Information Systems, or related field.\nProven experience (X+ years) as a Data Engineer or similar role.\nStrong programming skills in Python, Java, or Scala.\nProficiency with SQL and working knowledge of relational databases (e.g., PostgreSQL, MySQL).\nHands-on experience with big data technologies (e.g., Spark, Hadoop).\nFamiliarity with cloud platforms such as AWS, GCP, or Azure (e.g., S3, Redshift, BigQuery, Data Factory).\nExperience with orchestration tools like Airflow or Prefect.\nKnowledge of data modeling, warehousing, and architecture design principles.\nStrong problem-solving skills and attention to detail.\n\nPerks and benefits\nFree Meals\nPF and Gratuity\nMedical and Term Insurance",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SCALA', 'Kafka', 'AWS', 'Python', 'Pyspark', 'Java', 'Postgresql', 'Hadoop', 'Spark', 'ETL', 'SQL']",2025-06-12 13:42:20
Data Engineer II,Amazon,3 - 8 years,Not Disclosed,['Bengaluru'],"Amazon strives to be the worlds most customer-centric company, where customers can research and purchase anything they might want online\nWe set big goals and are looking for people who can help us reach and exceed them\nThe CPT Data Engineering & Analytics (DEA) team builds and maintains critical data infrastructure that enhances seller experience and protects the privacy of Amazon business partners throughout their lifecycle\nWe are looking for a strong Data Engineer to join our team\n\nThe Data Engineer I will work with well-defined requirements to develop and maintain data pipelines that help internal teams gather required insights for business decisions timely and accurately\nYou will collaborate with a team of Data Scientists, Business Analysts and other Engineers to build solutions that reduce investigation defects and assess the health of our Operations business while ensuring data quality and regulatory compliance\n\nThe ideal candidate must be passionate about building reliable data infrastructure, detail-oriented, and driven to help protect Amazons customers and business partners\nThey will be an individual contributor who works effectively with guidance from senior team members to successfully implement data solutions\nThe candidate must be proficient in SQL and at least one scripting language (e\ng\nPython, Perl, Scala), with strong understanding of data management fundamentals and distributed systems concepts\n\n\nBuild and optimize physical data models and data pipelines for simple datasets\nWrite secure, stable, testable, maintainable code with minimal defects\nTroubleshoot existing datasets and maintain data quality\nParticipate in team design, scoping, and prioritization discussions\nDocument solutions to ensure ease of use and maintainability\nHandle data in accordance with Amazon policies and security requirements Masters degree in computer science, engineering, analytics, mathematics, statistics, IT or equivalent\n3+ years of data engineering experience\nExperience with SQL\nExperience with data modeling, warehousing and building ETL pipelines\nKnowledge of distributed systems concepts from data storage and compute perspective\nAbility to work effectively in a team environment Experience with AWS technologies like Redshift, S3, AWS Glue, EMR, Kinesis, FireHose, Lambda, and IAM roles and permissions\nFamiliarity with big data technologies (Hadoop, Spark, etc\n)\nKnowledge of data security and privacy best practices\nStrong problem-solving and analytical skills\nExcellent written and verbal communication skills",Industry Type: Internet,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Data management', 'Data modeling', 'data security', 'Perl', 'Data quality', 'Distribution system', 'Analytics', 'SQL', 'Python']",2025-06-12 13:42:23
Data Engineer,Amgen Inc,1 - 6 years,Not Disclosed,['Hyderabad'],"Role Description:\nAs part of the cybersecurity organization, In this vital role you will be responsible for designing, building, and maintaining data infrastructure to support data-driven decision-making. This role involves working with large datasets, developing reports, executing data governance initiatives, and ensuring data is accessible, reliable, and efficiently managed. The role sits at the intersection of data infrastructure and business insight delivery, requiring the Data Engineer to design and build robust data pipelines while also translating data into meaningful visualizations for stakeholders across the organization. The ideal candidate has strong technical skills, experience with big data technologies, and a deep understanding of data architecture, ETL processes, and cybersecurity data frameworks.\nRoles & Responsibilities:\nDesign, develop, and maintain data solutions for data generation, collection, and processing.\nBe a key team member that assists in design and development of the data pipeline.\nBuild data pipelines and ensure data quality by implementing ETL processes to migrate and deploy data across systems.\nDevelop and maintain interactive dashboards and reports using tools like Tableau, ensuring data accuracy and usability\nSchedule and manage workflows the ensure pipelines run on schedule and are monitored for failures.\nCollaborate with multi-functional teams to understand data requirements and design solutions that meet business needs.\nDevelop and maintain data models, data dictionaries, and other documentation to ensure data accuracy and consistency.\nImplement data security and privacy measures to protect sensitive data.\nLeverage cloud platforms (AWS preferred) to build scalable and efficient data solutions.\nCollaborate and communicate effectively with product teams.\nCollaborate with data scientists to develop pipelines that meet dynamic business needs.\nShare and discuss findings with team members practicing SAFe Agile delivery model.\n\n\nBasic Qualifications:\nMasters degree and 1 to 3 years of experience of Computer Science, IT or related field experience OR\nBachelors degree and 3 to 5 years of Computer Science, IT or related field experience OR\nDiploma and 7 to 9 years of Computer Science, IT or related field experience\nPreferred Qualifications:\nHands on experience with data practices, technologies, and platforms, such as Databricks, Python, GitLab, LucidChart, etc.\nHands-on experience with data visualization and dashboarding toolsTableau, Power BI, or similar is a plus\nProficiency in data analysis tools (e.g. SQL) and experience with data sourcing tools\nExcellent problem-solving skills and the ability to work with large, complex datasets\nUnderstanding of data governance frameworks, tools, and best practices\nKnowledge of and experience with data standards (FAIR) and protection regulations and compliance requirements (e.g., GDPR, CCPA)\n\nGood-to-Have Skills:\nExperience with ETL tools and various Python packages related to data processing, machine learning model development\nStrong understanding of data modeling, data warehousing, and data integration concepts\nKnowledge of Python/R, Databricks, cloud data platforms\nExperience working in Product team's environment\nExperience working in an Agile environment\n\nProfessional Certifications:\nAWS Certified Data Engineer preferred\nDatabricks Certificate preferred\n\nSoft Skills:\nInitiative to explore alternate technology and approaches to solving problems\nSkilled in breaking down problems, documenting problem statements, and estimating efforts\nExcellent analytical and troubleshooting skills\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation\nAbility to handle multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data engineering', 'data analysis', 'data modeling', 'analysis tools', 'data warehousing', 'troubleshooting', 'data architecture', 'data integration', 'etl process']",2025-06-12 13:42:25
Data Engineer,Luxoft,5 - 10 years,Not Disclosed,['Pune'],"Are you passionate about data and analytics? Are you keen to be part of the journey to modernize a data warehouse/ analytics suite of application(s). Do you take pride in the quality of software delivered for each development iteration?\nWere looking for someone like that to join us and\nbe a part of a high-performing team on a high-profile project.\nsolve challenging problems in an elegant way\nmaster state-of-the-art technologies\nbuild a highly responsive and fast updating application in an Agile & Lean environment\napply best development practices and effectively utilize technologies\nwork across the full delivery cycle to ensure high-quality delivery\nwrite high-quality code and adhere to coding standards\nwork collaboratively with diverse team(s) of technologists\nYou are:\nCurious and collaborative, comfortable working independently, as well as in a team\nFocused on delivery to the business\nStrong in analytical skills. For example, the candidate must understand the key dependencies among existing systems in terms of the flow of data among them. It is essential that the candidate learns to understand the big picture of how IB industry/business functions.\nAble to quickly absorb new terminology and business requirements\nAlready strong in analytical tools, technologies, platforms, etc. The candidate must also demonstrate a strong desire for learning and self-improvement.\nOpen to learning home-grown technologies, support current state infrastructure and help drive future state migrations. imaginative and creative with newer technologies\nAble to accurately and pragmatically estimate the development effort required for specific objectives\nYou will have the opportunity to work under minimal supervision to understand local and global system requirements, design and implement the required functionality/bug fixes/enhancements. You will be responsible for components that are developed across the whole team and deployed globally.\nYou will also have the opportunity to provide third-line support to the applications global user community, which will include assisting dedicated support staff and liaising with the members of other development teams directly, some of which will be local and some remote.\nSkills\nMust have\nA bachelors or masters degree, preferably in Information Technology or a related field (computer science, mathematics, etc.), focusing on data engineering.\n5+ years of relevant experience as a data engineer in Big Data is required.\nStrong Knowledge of programming languages (Python / Scala) and Big Data technologies (Spark, Databricks or equivalent) is required.\nStrong experience in executing complex data analysis and running complex SQL/Spark queries.\nStrong experience in building complex data transformations in SQL/Spark.\nStrong knowledge of Database technologies is required.\nStrong knowledge of Azure Cloud is advantageous.\nGood understanding and experience with Agile methodologies and delivery.\nStrong communication skills with the ability to build partnerships with stakeholders.\nStrong analytical, data management and problem-solving skills.\nNice to have\nExperience working on the QlikView tool\nUnderstanding of QlikView scripting and data model\nOther\nLanguages\nEnglish: C1 Advanced\nSeniority\nSenior\nRefer a Friend\nPositive work environments and stellar reputations attract and retain top talent. Find out why Luxoft stands apart from the rest.\nRecommend a friend\nRelated jobs View all vacancies\nBig Data Engineer (Scala/Java/Python)\nBigData Development\nUnited States of America\nStamford, US\nBig Data Engineer (Scala/Java/Python)\nBigData Development\nUnited States of America\nWeehawken\nData Engineer - PostgreSQL\nBigData Development\nPoland\nRemote Poland\nPune, India\nReq. VR-114879\nBigData Development\nBCM Industry\n05/06/2025\nReq. VR-114879\nApply for Data Engineer in Pune\n*",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Data analysis', 'Data management', 'Coding', 'Postgresql', 'Agile', 'Information technology', 'Analytics', 'SQL', 'Python']",2025-06-12 13:42:27
Data Engineer,Amgen Inc,1 - 6 years,Not Disclosed,['Hyderabad'],"ABOUT THE ROLE\nRole Description:\nAs part of the cybersecurity organization, the Data Engineer is responsible for designing, building, and maintaining data infrastructure to support data-driven decision-making. This role involves working with large datasets, developing reports, executing data governance initiatives, and ensuring data is accessible, reliable, and efficiently managed. The ideal candidate has strong technical skills, experience with big data technologies, and a deep understanding of data architecture, ETL processes, and cybersecurity data frameworks.\nRoles & Responsibilities:\nDesign, develop, and maintain data solutions for data generation, collection, and processing.\nBe a key team member that assists in design and development of the data pipeline.\nCreate data pipelines and ensure data quality by implementing ETL processes to migrate and deploy data across systems.\nSchedule and manage workflows the ensure pipelines run on schedule and are monitored for failures.\nCollaborate with cross-functional teams to understand data requirements and design solutions that meet business needs.\nDevelop and maintain data models, data dictionaries, and other documentation to ensure data accuracy and consistency.\nImplement data security and privacy measures to protect sensitive data.\nLeverage cloud platforms (AWS preferred) to build scalable and efficient data solutions.\nCollaborate and communicate effectively with product teams.\nCollaborate with data scientists to develop pipelines that meet dynamic business needs.\nShare and discuss findings with team members practicing SAFe Agile delivery model.\nFunctional Skills:\nBasic Qualifications:\nMasters degree and 1 to 3 years of Computer Science, IT or related field experience OR\nBachelors degree and 3 to 5 years of Computer Science, IT or related field experience OR\nDiploma and 7 to 9 years of Computer Science, IT or related field experience\nPreferred Qualifications:\nHands on experience with data practices, technologies, and platforms, such as Databricks, Python, Gitlab, LucidChart,etc.\nProficiency in data analysis tools (e.g. SQL) and experience with data sourcing tools\nExcellent problem-solving skills and the ability to work with large, complex datasets\nUnderstanding of data governance frameworks, tools, and best practices\nKnowledge of and experience with data standards (FAIR) and protection regulations and compliance requirements (e.g., GDPR, CCPA)\nGood-to-Have Skills:\nExperience with ETL tools and various Python packages related to data processing, machine learning model development\nStrong understanding of data modeling, data warehousing, and data integration concepts\nKnowledge of Python/R, Databricks, cloud data platforms\nExperience working in Product team's environment\nExperience working in an Agile environment\nProfessional Certifications:\nAWS Certified Data Engineer preferred\nDatabricks Certificate preferred\nSoft Skills:\nInitiative to explore alternate technology and approaches to solving problems\nSkilled in breaking down problems, documenting problem statements, and estimating efforts\nExcellent analytical and troubleshooting skills\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation\nAbility to manage multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data engineering', 'data security', 'Agile', 'cloud data platforms', 'Databricks', 'data governance frameworks', 'ETL', 'AWS', 'SQL', 'Python']",2025-06-12 13:42:30
Data Engineer - SSIS - 5+ Years - Gurugram (Hybrid),Crescendo Global,5 - 10 years,Not Disclosed,['Gurugram'],"Data Engineer - SSIS - 5+ Years - Gurugram (Hybrid)\n\nAre you a skilled Data Engineer with expertise in SSIS and 5+ years of experience? Do you have a passion for analytics and want to work in a hybrid setup in Gurugram? Our client is seeking a talented individual to join their team and contribute to their data engineering projects.\n\nLocation : Gurugram (Hybrid)\n\nYour Future EmployerOur client is a leading organization in the analytics domain, known for fostering an inclusive and diverse work environment. They are committed to providing their employees with opportunities for growth and development.\n\nResponsibilities\nDesign, develop, and maintain data pipelines using SSIS for efficient data processing\nCollaborate with cross-functional teams to understand data requirements and provide effective data solutions\nOptimize data pipelines for performance and scalability\nEnsure data quality and integrity throughout the data engineering process\n\nRequirements\n5+ years of experience in data engineering with a strong focus on SSIS\nProficiency in data warehousing concepts and ETL processes\nHands-on experience with SQL databases and data modeling4.Strong analytical and problem-solving skills\nBachelor's degree in Computer Science, Engineering, or related field\n\nWhat's in it for you : In this role, you will have the opportunity to work on challenging projects and enhance your expertise in data engineering. The organization offers a competitive compensation package and a supportive work environment where your contributions are valued.\n\nReach us : If you feel this opportunity is well aligned with your career progression plans, please feel free to reach me with your updated profile at rohit.kumar@crescendogroup.in\n\nDisclaimer : Crescendo Global specializes in Senior to C-level niche recruitment. We are passionate about empowering job seekers and employers with an engaging memorable job search and leadership hiring experience. Crescendo Global does not discriminate on the basis of race, religion, color, origin, gender, sexual orientation, age, marital status, veteran status or disability status.\n\nNote : We receive a lot of applications on a daily basis so it becomes a bit difficult for us to get back to each candidate. Please assume that your profile has not been shortlisted in case you don't hear back from us in 1 week. Your patience is highly appreciated.\n\nScammers can misuse Crescendo Globals name for fake job offers. We never ask for money, purchases, or system upgrades. Verify all opportunities at www.crescendo-global.com and report fraud immediately. Stay alert!\n\nProfile keywords : Data Engineer, SSIS, Data Warehousing, ETL, SQL, Analytics",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Engineering', 'SSIS', 'SQL', 'Analytics']",2025-06-12 13:42:32
Data Engineer,Konrad Group,3 - 7 years,15-30 Lacs P.A.,['Gurugram( Sector 42 Gurgaon )'],"Who We Are\n\nKonrad is a next generation digital consultancy. We are dedicated to solving complex business problems for our global clients with creative and forward-thinking solutions. Our employees enjoy a culture built on innovation and a commitment to creating best-in-class digital products in use by hundreds of millions of consumers around the world. We hire exceptionally smart, analytical, and hard working people who are lifelong learners.\nAbout The Role\nAs a Data Engineer youll be tasked with designing, building, and maintaining scalable data platforms and pipelines. Your deep knowledge of data platforms such as Azure Fabric, Databricks, and Snowflake will be essential as you collaborate closely with data analysts, scientists, and other engineers to ensure reliable, secure, and efficient data solutions.\n\nWhat Youll Do\n\nDesign, build, and manage robust data pipelines and data architectures.\nImplement solutions leveraging platforms such as Azure Fabric, Databricks, and Snowflake.\nOptimize data workflows, ensuring reliability, scalability, and performance.\nCollaborate with internal stakeholders to understand data needs and deliver tailored solutions.\nEnsure data security and compliance with industry standards and best practices.\nPerform data modelling, data extraction, transformation, and loading (ETL/ELT).\nIdentify and recommend innovative solutions to enhance data quality and analytics capabilities.\n\nQualifications\n\nBachelors degree or higher in Computer Science, Data Engineering, Information Technology, or a related field.\nAt least 3 years of professional experience as a Data Engineer or similar role.\nProficiency in data platforms such as Azure Fabric, Databricks, and Snowflake.\nHands-on experience with data pipeline tools, cloud services, and storage solutions.\nStrong programming skills in SQL, Python, or related languages.\nExperience with big data technologies and concepts (Spark, Hadoop, Kafka).\nExcellent analytical, troubleshooting, and problem-solving skills.\nAbility to effectively communicate technical concepts clearly to non-technical stakeholders.\nAdvanced English\n\nNice to have\n\nCertifications related to Azure Data Engineering, Databricks, or Snowflake.\nFamiliarity with DevOps practices and CI/CD pipelines.\n\nPerks and Benefits\n\nComprehensive Health & Wellness Benefits Package \nSocials, Outings & Retreats\nCulture of Learning & Development\nFlexible Working Hours\nWork from Home Flexibility\nService Recognition Programs\n\nKonrad is committed to maintaining a diverse work environment and is proud to be an equal opportunity employer. All qualified applicants, regardless of race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status will receive consideration for employment. If you have any accessibility requirements or concerns regarding the hiring process or employment with us, please notify us so we can provide suitable accommodation.\nWhile we sincerely appreciate all applications, only those candidates selected for an interview will be contacted.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Hadoop', 'Azure Data Factory', 'Azure Databricks', 'Spark', 'Fabric', 'Python']",2025-06-12 13:42:34
Data Engineer,Atyeti,2 - 4 years,Not Disclosed,['Pune'],"Role & responsibilities\n\nDevelop and Maintain Data Pipelines: Design, develop, and manage scalable ETL pipelines to process large datasets using PySpark, Databricks, and other big data technologies.\nData Integration and Transformation: Work with various structured and unstructured data sources to build efficient data workflows and integrate them into a central data warehouse.\nCollaborate with Data Scientists & Analysts: Work closely with the data science and business intelligence teams to ensure the right data is available for advanced analytics, machine learning, and reporting.",,,,"['Azure Synapse', 'Pyspark', 'ETL', 'Python']",2025-06-12 13:42:37
Data Engineer,zocket,4 - 5 years,Not Disclosed,['Chennai'],"About Zocket: AI Assistant for Marketers\nZocket empowers businesses to scale social media advertising seamlessly with advanced AI . From search to conversions, Zocket automates the entire workflow effortlessly.\n\nHeadquartered in Chennai and San Francisco , with 500+ customers across 25+ countries.\n\nOur GenAI Product Suite:\nCreative Studio: Make ad creatives 10x faster, 20x better\n\nAudience Assistant: Precision targeting, platform-wide\n\nSnoop AI: Competitive & industry insights in seconds\n\nInsights AI: 360 performance dashboard across channels\n\nGrowth Infra: Whitelisted infra for unbounded scale\n\nTrusted and Recognized:\nAWS GenAI Accelerator 2024 (Top 80 global startups)\n\nGoogle for Startups (GFSA Class VIII Ai-first)\n\nNASSCOM Gen AI Foundry\n\n4.9 Trustpilot | #1 Product of the Day & Week on Product Hunt\n\n\nKey Responsibilities\n\nDesign, build, and maintain robust ETL pipelines to process large-scale structured and unstructured data.\n\nWork with GraphQL databases and Vector DBs (e.g., Pinecone, Chroma) to power intelligent, AI-driven features.\n\nBuild and optimize scalable ML pipelines and contribute to model lifecycle management .\n\nImplement real-time and batch data streaming solutions using Kafka and Spark .\n\nWork with Airflow to schedule and orchestrate data workflows.\n\nDevelop efficient data models in PostgreSQL and ensure high-performance data access.\n\nIntegrate and optimize Elasticsearch for fast search and analytics.\n\nEnsure high data quality and reliability through monitoring and automation.\n\nCollaborate with Product, Data Science, and Engineering teams to deliver scalable data solutions.\n\nOwn the data infrastructure on AWS - from data lake to warehouse and analytics stack.\n\nDrive internal data tooling improvements and support analytics for business decision-making.\n\n\nKey Requirements\n\n4-5 years of experience as a Data Engineer or in a similar backend/data-intensive role.\n\nStrong programming experience with Python and familiarity with ML model development workflows .\n\nHands-on experience with GraphQL and Vector DBs (Pinecone, Chroma).\n\nProficiency in PostgreSQL and working knowledge of NoSQL and search systems like Elasticsearch .\n\nSolid understanding of data engineering on AWS (EC2, S3, RDS, Redshift, etc.).\n\nExperience with Apache Kafka , Apache Spark , and Airflow is essential.\n\nFamiliarity with modern ML Ops and data science integration is a plus.\n\nStrong analytical thinking and problem-solving skills.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Automation', 'Backend', 'data science', 'Analytical', 'Social media', 'Workflow', 'Data quality', 'AWS', 'Analytics', 'Monitoring']",2025-06-12 13:42:39
Data Engineer,Trantor,5 - 10 years,Not Disclosed,[],"We are looking for a skilled and motivated Data Engineer with deep expertise in GCP,\nBigQuery, Apache Airflow to join our data platform team. The ideal candidate should have hands-on experience building scalable data pipelines, automating workflows, migrating large-scale datasets, and optimizing distributed systems. The candidate should have experience with building Web APIs using Python. This role will play a key part in designing and maintaining robust data engineering solutions across cloud and on-prem environments.\nKey Responsibilities\nBigQuery & Cloud Data Pipelines:\nDesign and implement scalable ETL pipelines for ingesting large-scale datasets.\nBuild solutions for efficient querying of tables in BigQuery.\nAutomated scheduled data ingestion using Google Cloud services and scheduled\nApache Airflow DAGs",,,,"['Airflow', 'Etl Pipelines', 'GCP', 'Bigquery', 'Python', 'SFTP', 'ETL', 'SQL']",2025-06-12 13:42:41
Data Engineer,Amgen Inc,1 - 3 years,Not Disclosed,['Hyderabad'],"What you will do\nIn this vital role you will be responsible to design, develop, and optimize data pipelines, data integration frameworks, and metadata-driven architectures that enable seamless data access and analytics. This role prefers deep expertise in big data processing, distributed computing, data modeling, and governance frameworks to support self-service analytics, AI-driven insights, and enterprise-wide data management.\nDesign, develop, and maintain complex ETL/ELT data pipelines in Databricks using PySpark, Scala, and SQL to process large-scale datasets\nUnderstand the biotech/pharma or related domains & build highly efficient data pipelines to migrate and deploy complex data across systems\nDesign and Implement solutions to enable unified data access, governance, and interoperability across hybrid cloud environments\nIngest and transform structured and unstructured data from databases (PostgreSQL, MySQL, SQL Server, MongoDB etc.), APIs, logs, event streams, images, pdf, and third-party platforms\nEnsuring data integrity, accuracy, and consistency through rigorous quality checks and monitoring\nExpert in data quality, data validation and verification frameworks\nInnovate, explore and implement new tools and technologies to enhance efficient data processing\nProactively identify and implement opportunities to automate tasks and develop reusable frameworks\nWork in an Agile and Scaled Agile (SAFe) environment, collaborating with cross-functional teams, product owners, and Scrum Masters to deliver incremental value\nUse JIRA, Confluence, and Agile DevOps tools to manage sprints, backlogs, and user stories\nSupport continuous improvement, test automation, and DevOps practices in the data engineering lifecycle\nCollaborate and communicate effectively with the product teams, with cross-functional teams to understand business requirements and translate them into technical solutions\nWhat we expect of you\nWe are all different, yet we all use our unique contributions to serve patients. We are looking for highly motivated expert Data Engineer who can own the design & development of complex data pipelines, solutions and frameworks.\nBasic Qualifications:\nMasters degree and 1 to 3 years of Computer Science, IT or related field experience OR\nBachelors degree and 3 to 5 years of Computer Science, IT or related field experience OR\nDiploma and 7 to 9 years of Computer Science, IT or related field experience\nHands-on experience in data engineering technologies such as Databricks, PySpark, SparkSQL Apache Spark, AWS, Python, SQL, and Scaled Agile methodologies\nProficiency in workflow orchestration, performance tuning on big data processing\nStrong understanding of AWS services\nAbility to quickly learn, adapt and apply new technologies\nStrong problem-solving and analytical skills\nExcellent communication and teamwork skills\nExperience with Scaled Agile Framework (SAFe), Agile delivery practices, and DevOps practices\nPreferred Qualifications:\nAWS Certified Data Engineer preferred\nDatabricks Certificate preferred\nScaled Agile SAFe certification preferred\nData Engineering experience in Biotechnology or pharma industry\nExperience in writing APIs to make the data available to the consumers\nExperienced with SQL/NOSQL database, vector database for large language models\nExperienced with data modeling and performance tuning for both OLAP and OLTP databases\nExperienced with software engineering best-practices, including but not limited to version control (Git, Subversion, etc.), CI/CD (Jenkins, Maven etc.), automated unit testing, and DevOps\nSoft Skills:\nExcellent analytical and troubleshooting skills.\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nTeam-oriented, with a focus on achieving team goals.\nAbility to learn quickly, be organized and detail oriented.\nStrong presentation and public speaking skills.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data engineering', 'Maven', 'data validation', 'PySpark', 'Scala', 'APIs', 'SQL Server', 'SQL', 'Jenkins', 'Git', 'MySQL', 'troubleshooting', 'MongoDB', 'ETL']",2025-06-12 13:42:44
Data Engineer,Cloud Angles Digital Transformation,3 - 5 years,Not Disclosed,['Noida'],"Essential Functions/Responsibilities/Duties\n•       Work closely with Senior Business Intelligence engineer and BI architect to understand the schema objects and build BI reports and Dashboards\n•       Participation in sprint refinement, planning, and kick-off to understand the Agile process and Sprint priorities\n•       Develop necessary transformations and aggregate tables required for the reporting\\Dashboard needs\n•       Understand the Schema layer in MicroStrategy and business requirements\n•       Develop complex reports and Dashboards in MicroStrategy\n•       Investigate and troubleshoot issues with Dashboard and reports\n•       Proactively researching new technologies and proposing improvements to processes and tech stack\n•       Create test cases and scenarios to validate the dashboards and maintain data accuracy\nEducation and Experience\n•       3 years of experience in Business Intelligence and Data warehousing\n•       3+ years of experience in MicroStrategy Reports and Dashboard development\n•       2 years of experience in SQL\n•       Bachelors or masters degree in IT or Computer Science or ECE.\n•       Nice to have – Any MicroStrategy certifications\nRequired Knowledge, Skills, and Abilities\n•       Good in writing complex SQL, including aggregate functions, subqueries and complex date calculations and able to teach these concepts to others.\n•       Detail oriented and able to examine data and code for quality and accuracy.\n•       Self-Starter – taking initiative when inefficiencies or opportunities are seen.\n•       Good understanding of modern relational and non-relational models and differences between them\n•       Good understanding of Datawarehouse concepts, snowflake & star schema architecture and SCD concepts\n•       Good understanding of MicroStrategy Schema objects\n•       Develop Public objects such as metrics, filters, prompts, derived objects, custom groups and consolidations in MicroStrategy\n•       Develop complex reports and dashboards using OLAP and MTDI cubes\n•       Create complex dashboards with data blending\n•       Understand VLDB settings and report optimization\n•       Understand security filters and connection mappings in MSTR\nWork Environment\nAt Personify Health, we value and celebrate diversity and are committed to creating an inclusive environment for all employees. We believe in creating teams made up of individuals with various backgrounds, experiences, and perspectives. Diversity inspires innovation and collaboration and challenges us to produce better solutions. But more than this, diversity is our strength and a catalyst in our ability to change lives for the good. \nPhysical Requirements\n•       Constantly operates a computer and other office productivity machinery, such as copy machine, computer printer, calculator, etc.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Microstrategy', 'SQL', 'Dashboards']",2025-06-12 13:42:46
Data Engineer,Centrilogic,15 - 20 years,Not Disclosed,['Hyderabad'],"Data Engineer\n\nPurpose:\n\nOver 15 years, we have become a premier global provider of multi-cloud management, cloud-native application development solutions, and strategic end-to-end digital transformation services.\nHeadquartered in Canada and with regional headquarters in the U.S. and the United Kingdom, Centrilogic delivers smart, streamlined solutions to clients worldwide.\n\nWe are looking for a passionate and experienced Data Engineer to work with our other 70 Software, Data and DevOps engineers to guide and assist our clients data modernization journey.\n\nOur team works with companies with ambitious missions - clients who are creating new, innovative products, often in uncharted markets. We work as embedded members and leaders of our clients development and data teams. We bring experienced senior engineers, leading-edge technologies and mindsets, and creative thinking. We show our clients how to move to the modern frameworks of data infrastructures and processing, and we help them reach their full potential with the power of data.\n\nIn this role, youll be the day-to-day primary point of contact with our clients to modernize their data infrastructures, architecture, and pipelines.\n\nPrincipal Responsibilities:\n\nConsulting clients on cloud-first strategies for core bet-the-company data initiatives\nProviding thought leadership on both process and technical matters\nBecoming a real champion and trusted advisor to our clients on all facets of Data Engineering\nDesigning, developing, deploying, and supporting the modernization and transformation of our client s end-to-end data strategy, including infrastructure, collection, transmission, processing, and analytics\nMentoring and educating clients teams to keep them up to speed with the latest approaches, tools and skills, and setting them up for continued success post-delivery\n\nRequired Experience and Skills:\n\nMust have either Microsoft Certified Azure Data Engineer Associate or Fabric Data Engineer Associate certification.\nMust have experience working in a consulting or contracting capacity on large data management and modernization programs.\nExperience with SQL Servers, data engineering, on platforms such as Azure Data Factory, Databricks, Data Lake, and Synapse.\nStrong knowledge and demonstrated experience with Delta Lake and Lakehouse Architecture.\nStrong knowledge of securing Azure environment, such as RBAC, Key Vault, and Azure Security Center.\nStrong knowledge of Kafka and Spark and extensive experience using them in a production environment.\nStrong and demonstrable experience as DBA in large-scale MS SQL environments deployed in Azure.\nStrong problem-solving skills, with the ability to get to the route of an issue quickly.\nStrong knowledge of Scala or Python.\nStrong knowledge of Linux administration and networking.\nScripting skills and Infrastructure as Code (IaC) experience using PowerShell, Bash, and ARM templates.\nUnderstanding of security and corporate governance issues related with cloud-first data architecture, as well as accepted industry solutions.\nExperience in enabling continuous delivery for development teams using scripted cloud provisioning and automated tooling.\nExperience working with Agile development methodology that is fit for purpose.\nSound business judgment and demonstrated leadership",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['MS SQL', 'Networking', 'Data management', 'Powershell', 'Consulting', 'Application development', 'microsoft', 'Analytics', 'Python', 'Data architecture']",2025-06-12 13:42:48
Data Engineer,Databeat,3 - 7 years,Not Disclosed,['Hyderabad( Rai Durg )'],"Experience Required: 3+ years\n\nTechnical knowledge: AWS, Python, SQL, S3, EC2, Glue, Athena, Lambda, DynamoDB, RedShift, Step Functions, Cloud Formation, CI/CD Pipelines, Github, EMR, RDS,AWS Lake Formation, GitLab, Jenkins and AWS CodePipeline.\n\n\n\nRole Summary: As a Senior Data Engineer,with over 3 years of expertise in Python, PySpark, SQL to design, develop and optimize complex data pipelines, support data modeling, and contribute to the architecture that supports big data processing and analytics to cutting-edge cloud solutions that drive business growth. You will lead the design and implementation of scalable, high-performance data solutions on AWS and mentor junior team members.This role demands a deep understanding of AWS services, big data tools, and complex architectures to support large-scale data processing and advanced analytics.\nKey Responsibilities:\nDesign and develop robust, scalable data pipelines using AWS services, Python, PySpark, and SQL that integrate seamlessly with the broader data and product ecosystem.\nLead the migration of legacy data warehouses and data marts to AWS cloud-based data lake and data warehouse solutions.\nOptimize data processing and storage for performance and cost.\nImplement data security and compliance best practices, in collaboration with the IT security team.\nBuild flexible and scalable systems to handle the growing demands of real-time analytics and big data processing.\nWork closely with data scientists and analysts to support their data needs and assist in building complex queries and data analysis pipelines.\nCollaborate with cross-functional teams to understand their data needs and translate them into technical requirements.\nContinuously evaluate new technologies and AWS services to enhance data capabilities and performance.\nCreate and maintain comprehensive documentation of data pipelines, architectures, and workflows.\nParticipate in code reviews and ensure that all solutions are aligned to pre-defined architectural specifications.\nPresent findings to executive leadership and recommend data-driven strategies for business growth.\nCommunicate effectively with different levels of management to gather use cases/requirements and provide designs that cater to those stakeholders.\nHandle clients in multiple industries at the same time, balancing their unique needs.\nProvide mentoring and guidance to junior data engineers and team members.\n\n\n\nRequirements:\n3+ years of experience in a data engineering role with a strong focus on AWS, Python, PySpark, Hive, and SQL.\nProven experience in designing and delivering large-scale data warehousing and data processing solutions.\nLead the design and implementation of complex, scalable data pipelines using AWS services such as S3, EC2, EMR, RDS, Redshift, Glue, Lambda, Athena, and AWS Lake Formation.\nBachelor's or Masters degree in Computer Science, Engineering, or a related technical field.\nDeep knowledge of big data technologies and ETL tools, such as Apache Spark, PySpark, Hadoop, Kafka, and Spark Streaming.\nImplement data architecture patterns, including event-driven pipelines, Lambda architectures, and data lakes.\nIncorporate modern tools like Databricks, Airflow, and Terraform for orchestration and infrastructure as code.\nImplement CI/CD using GitLab, Jenkins, and AWS CodePipeline.\nEnsure data security, governance, and compliance by leveraging tools such as IAM, KMS, and AWS CloudTrail.\nMentor junior engineers, fostering a culture of continuous learning and improvement.\nExcellent problem-solving and analytical skills, with a strategic mindset.\nStrong communication and leadership skills, with the ability to influence stakeholders at all levels.\nAbility to work independently as well as part of a team in a fast-paced environment.\nAdvanced data visualization skills and the ability to present complex data in a clear and concise manner.\nExcellent communication skills, both written and verbal, to collaborate effectively across teams and levels.\n\nPreferred Skills:\nExperience with Databricks, Snowflake, and machine learning pipelines.\nExposure to real-time data streaming technologies and architectures.\nFamiliarity with containerization and serverless computing (Docker, Kubernetes, AWS Lambda).",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'Aws Glue', 'SQL', 'Data Pipeline', 'Python', 'Amazon Ec2', 'Data Engineering', 'Data Bricks', 'Aws Lambda', 'Amazon Redshift', 'Azure Cloud', 'Data Lake', 'Data Modeling', 'Athena']",2025-06-12 13:42:51
Data Engineer 4,Comcast,5 - 11 years,Not Disclosed,['Chennai'],".\nResponsible for designing, building and overseeing the deployment and operation of technology architecture, solutions and software to capture, manage, store and utilize structured and unstructured data from internal and external sources. Establishes and builds processes and structures based on business and technical requirements to channel data from multiple inputs, route appropriately and store using any combination of distributed (cloud) structures, local databases, and other applicable storage forms as required. Develops technical tools and programming that leverage artificial intelligence, machine learning and big-data techniques to cleanse, organize and transform data and to maintain, defend and update data structures and integrity on an automated basis. Creates and establishes design standards and assurance processes for software, systems and applications development to ensure compatibility and operability of data connections, flows and storage requirements. Reviews internal and external business and product requirements for data operations and activity and suggests changes and upgrades to systems and storage to accommodate ongoing needs. Work with data modelers/analysts to understand the business problems they are trying to solve then create or augment data assets to feed their analysis. Integrates knowledge of business and functional priorities. Acts as a key contributor in a complex and crucial environment. May lead teams or projects and shares expertise.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBachelors Degree\nWhile possessing the stated degree is preferred, Comcast also may consider applicants who hold some combination of coursework and experience, or who have extensive related professional experience.\n7-10 Years\nComcast is proud to be an equal opportunity workplace. We will consider all qualified applicants for employment without regard to race, color, religion, age, sex, sexual orientation, gender identity, national origin, disability, veteran status, genetic information, or any other basis protected by applicable law.",,,,"['Engineering services', 'Assurance', 'Process optimization', 'MySQL', 'Machine learning', 'Data structures', 'Data quality', 'Troubleshooting', 'Downstream', 'Python']",2025-06-12 13:42:53
Data Engineer,Fortune India 500 Chemicals Firm,12 - 18 years,Not Disclosed,['Mumbai (All Areas)'],"Skills:\nData Management: Expertise in data warehousing, SQL/NoSQL, cloud platforms (AWS, Azure, GCP)\nETL Tools: Proficient in Informatica, Talend, Azure Data Factory\nModelling: Strong in dimensional modelling, star/snowflake schema\nGovernance & Compliance: Knowledge of GDPR, HIPAA, data governance frameworks\nLanguages: T-SQL, PL/SQL\nSoft Skills: Effective communicator, strong analytical and problem-solving skills\nKey Responsibilities:\nArchitecture: Designed scalable, high-performance data warehouse architectures and data models\nETL & Integration: Led ETL design/development for structured/unstructured data across platforms\nGovernance: Defined data quality standards and collaborated on data governance policy implementation\nCollaboration: Interfaced with BI, data science, and business teams to align data strategies\nPerformance & Security: Optimized queries/ETL jobs and ensured data security and compliance\nDocumentation: Maintained standards and documentation for architecture, ETL, and workflows",Industry Type: Chemicals,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Warehousing', 'GCP', 'Snowflake', 'Microsoft Azure', 'Dimensional Modeling', 'Data Modeling', 'ETL', 'AWS']",2025-06-12 13:42:56
Data Engineer- MS Fabric,InfoCepts,5 - 9 years,Not Disclosed,['India'],"Position: Data Engineer – MS Fabric\n  Purpose of the Position: As an MS Fabric Data engineer you will be responsible for designing, implementing, and managing scalable data pipelines. Strong experience in implementation and management of lake House using MS Fabric Azure Tech stack (ADLS Gen2, ADF, Azure SQL) .\nProficiency in data integration techniques, ETL processes and data pipeline architectures. Well versed in Data Quality rules, principles and implementation.\n",,,,"['components', 'data', 'scala', 'delta', 'pyspark', 'data warehousing', 'rules', 'azure data factory', 'sql', 'parquet', 'analytics', 'sql azure', 'spark', 'oracle adf', 'data pipeline architecture', 'etl', 'python', 'azure synapse', 'microsoft azure', 'power bi', 'data bricks', 'data quality', 'system', 't', 'fabric', 'data integration', 'etl process']",2025-06-12 13:42:58
Cloud Data Engineer,PwC India,5 - 8 years,10-20 Lacs P.A.,['Bengaluru'],"Role & responsibilities\nStrong hands-on experience with multi cloud (AWS, Azure, GCP)  services such as GCP BigQuery, Dataform AWS Redshift, \nProficient in PySpark and SQL for building scalable data processing pipelines\nKnowledge of utilizing serverless technologies like AWS Lambda, and Google Cloud Functions \nExperience in orchestration frameworks like Apache Airflow, Kubernetes, and Jenkins to manage and orchestrate data pipelines",,,,"['Pyspark', 'Python', 'SQL', 'Datafactory', 'GCP', 'Microsoft Azure', 'AWS', 'Data Bricks']",2025-06-12 13:43:00
Data Engineer - ETL/ Python,Meritus Management Service,5 - 7 years,10-14 Lacs P.A.,['Indore'],"Focus on Python, you'll play a crucial role in designing, developing, and maintaining data pipelines and ETL processes. Python to manage large datasets, automate data workflows, and ensure data accuracy and efficiency across our organization.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pandas', 'MySQL', 'Sqlalchemy', 'Numpy', 'Python', 'Azure Synapse', 'Postgresql', 'Etl Process', 'SQL']",2025-06-12 13:43:03
"Data Engineer Openings at Advantum Health, Hyderabad",Advantum Health,3 - 5 years,Not Disclosed,['Hyderabad'],"Data Engineer openings at Advantum Health Pvt Ltd, Hyderabad.\nOverview:\nWe are looking for a Data Engineer to build and optimize robust data pipelines that support AI and RCM analytics. This role involves integrating structured and unstructured data from diverse healthcare systems into scalable, AI-ready datasets.\nKey Responsibilities:\nDesign, implement, and optimize data pipelines for ingesting and transforming healthcare and RCM data.\nBuild data marts and warehouses to support analytics and machine learning.\nEnsure data quality, lineage, and governance across AI use cases.\nIntegrate data from EMRs, billing platforms, claims databases, and third-party APIs.\nSupport data infrastructure in a HIPAA-compliant cloud environment.\nQualifications:\nBachelors in Computer Science, Data Engineering, or related field.\n3+ years of experience with ETL/ELT pipelines using tools like Apache Airflow, dbt, or Azure Data Factory.\nStrong SQL and Python skills.\nExperience with healthcare data standards (HL7, FHIR, X12) preferred.\nFamiliarity with data lake house architectures and AI integration best practices\nPh: 9177078628\nEmail id: jobs@advantumhealth.com\nAddress: Advantum Health Private Limited, Cyber gateway, Block C, 4th floor Hitech City, Hyderabad.\nDo follow us on LinkedIn, Facebook, Instagram, YouTube and Threads\nAdvantum Health LinkedIn Page:\nhttps://lnkd.in/gVcQAXK3\n\nAdvantum Health Facebook Page:\nhttps://lnkd.in/g7ARQ378\n\nAdvantum Health Instagram Page:\nhttps://lnkd.in/gtQnB_Gc\n\nAdvantum Health India YouTube link:\nhttps://lnkd.in/g_AxPaPp\n\nAdvantum Health Threads link:\nhttps://lnkd.in/gyq73iQ6",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Data Factory', 'SQL', 'Python', 'Airflow', 'ETL', 'Elt']",2025-06-12 13:43:06
Data Engineer,Conversehr Business Solutions,4 - 7 years,15-30 Lacs P.A.,['Hyderabad'],"What are the ongoing responsibilities of Data Engineer responsible for?\nWe are building a growing Data and AI team. You will play a critical role in the efforts to centralize structured and unstructured data for the firm. We seek a candidate with skills in data modeling, data management and data governance, and can contribute first-hand towards firms data strategy. The ideal candidate is a self-starter with a strong technical foundation, a collaborative mindset, and the ability to navigate complex data challenges #ASSOCIATE\nWhat ideal qualifications, skills & experience would help someone to be successful?\nBachelors degree in computer science or computer applications; or equivalent experience in lieu of degree with 3 years of industry experience.\nStrong expertise in data modeling and data management concepts. Experience in implementing master data management is preferred.\nSound knowledge on Snowflake and data warehousing techniques.\nExperience in building, optimizing, and maintaining data pipelines and data management frameworks to support business needs.\nProficiency in at least one programming language, preferably python.\nCollaborate with cross-functional teams to translate business needs into scalable data and AI-driven solutions.\nTake ownership of projects from ideation to production, operating in a startup-like culture within an enterprise environment. Excellent communication, collaboration, and ownership mindset.\nFoundational Knowledge of API development and integration.\nKnowledge of Tableau, Alteryx is good-to-have.\nWork Shift Timings - 2:00 PM - 11:00 PM IST",Industry Type: Financial Services (Asset Management),Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Snowflake', 'Master Data Management', 'Python', 'Etl Pipelines', 'Alteryx', 'ai', 'Data Modeling', 'Tableau', 'ETL']",2025-06-12 13:43:08
Azure Data Engineer,Wipro,5 - 8 years,Not Disclosed,['Bengaluru'],"Role Purpose\nThe purpose of the role is to support process delivery by ensuring daily performance of the Production Specialists, resolve technical escalations and develop technical capability within the Production Specialists.\n\nDo\nOversee and support process by reviewing daily transactions on performance parameters\nReview performance dashboard and the scores for the team\nSupport the team in improving performance parameters by providing technical support and process guidance\nRecord, track, and document all queries received, problem-solving steps taken and total successful and unsuccessful resolutions\nEnsure standard processes and procedures are followed to resolve all client queries\nResolve client queries as per the SLAs defined in the contract\nDevelop understanding of process/ product for the team members to facilitate better client interaction and troubleshooting\nDocument and analyze call logs to spot most occurring trends to prevent future problems\nIdentify red flags and escalate serious client issues to Team leader in cases of untimely resolution\nEnsure all product information and disclosures are given to clients before and after the call/email requests\nAvoids legal challenges by monitoring compliance with service agreements\n\nHandle technical escalations through effective diagnosis and troubleshooting of client queries\nManage and resolve technical roadblocks/ escalations as per SLA and quality requirements\nIf unable to resolve the issues, timely escalate the issues to TA & SES\nProvide product support and resolution to clients by performing a question diagnosis while guiding users through step-by-step solutions\nTroubleshoot all client queries in a user-friendly, courteous and professional manner\nOffer alternative solutions to clients (where appropriate) with the objective of retaining customers and clients business\nOrganize ideas and effectively communicate oral messages appropriate to listeners and situations\nFollow up and make scheduled call backs to customers to record feedback and ensure compliance to contract SLAs\nBuild people capability to ensure operational excellence and maintain superior customer service levels of the existing account/client\nMentor and guide Production Specialists on improving technical knowledge\nCollate trainings to be conducted as triage to bridge the skill gaps identified through interviews with the Production Specialist\nDevelop and conduct trainings (Triages) within products for production specialist as per target\nInform client about the triages being conducted\nUndertake product trainings to stay current with product features, changes and updates\nEnroll in product specific and any other trainings per client requirements/recommendations\nIdentify and document most common problems and recommend appropriate resolutions to the team\nUpdate job knowledge by participating in self learning opportunities and maintaining personal networks\n\nDeliver\n\nNoPerformance ParameterMeasure1ProcessNo. of cases resolved per day, compliance to process and quality standards, meeting process level SLAs, Pulse score, Customer feedback, NSAT/ ESAT2Team ManagementProductivity, efficiency, absenteeism3Capability developmentTriages completed, Technical Test performance\nMandatory Skills: Azure Data Factory. Experience: 5-8 Years.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure', 'azure databricks', 'azure data lake', 'ssas', 'ssrs', 'microsoft azure', 'azure data factory', 'ssis', 'msbi', 'sql server', 'sql']",2025-06-12 13:43:10
AWS Data Engineer,Infosys,5 - 9 years,Not Disclosed,"['Hyderabad', 'Pune', 'Bengaluru']","Significant 5 to 9 years of experience in designing and implementing scalable data engineering solutions on AWS.\nStrong proficiency in Python programming language.\nExpertise in serverless architecture and AWS services such as Lambda, Glue, Redshift, Kinesis, SNS, SQS, and CloudFormation.\nExperience with Infrastructure as Code (IaC) using AWS CDK for defining and provisioning AWS resources.\nProven leadership skills with the ability to mentor and guide junior team members.\nExcellent understanding of data modeling concepts and experience with tools like ERStudio.\nStrong communication and collaboration skills, with the ability to work effectively in a cross-functional team environment.\nExperience with Apache Airflow for orchestrating data pipelines is a plus.\nKnowledge of Data Lakehouse, dbt, or Apache Hudi data format is a plus.\nRoles and Responsibilities\nDesign, develop, test, deploy, and maintain large-scale data pipelines using AWS services such as S3, Glue, Lambda, Redshift.\nCollaborate with cross-functional teams to gather requirements and design solutions that meet business needs.\nDesired Candidate Profile\n5-9 years of experience in an IT industry setting with expertise in Python programming language (Pyspark).\nStrong understanding of AWS ecosystem including S3, Glue, Lambda, Redshift.\nBachelor's degree in Any Specialization (B.Tech/B.E.).",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Aws Glue', 'Pyspark', 'Aws Lambda', 'Amazon Redshift', 'Athena', 'Python']",2025-06-12 13:43:12
Senior Data Engineer,Grid Dynamics,8 - 13 years,15-25 Lacs P.A.,['Bengaluru'],"We are looking for an enthusiastic and technology-proficient Big Data Engineer, who is eager to participate in the design and implementation of a top-notch Big Data solution to be deployed at massive scale.\nOur customer is one of the world's largest technology companies based in Silicon Valley with operations all over the world. On this project we are working on the bleeding-edge of Big Data technology to develop high performance data analytics platform, which handles petabytes datasets.\nEssential functions\nParticipate in design and development of Big Data analytical applications.\nDesign, support and continuously enhance the project code base, continuous integration pipeline, etc.\nWrite complex ETL processes and frameworks for analytics and data management.\nImplement large-scale near real-time streaming data processing pipelines.\nWork inside the team of industry experts on the cutting edge Big Data technologies to develop solutions for deployment at massive scale.\nQualifications\nStrong coding experience with Scala, Spark,Hive, Hadoop.\nIn-depth knowledge of Hadoop and Spark, experience with data mining and stream processing technologies (Kafka, Spark Streaming, Akka Streams).\nUnderstanding of the best practices in data quality and quality engineering.\nExperience with version control systems, Git in particular.\nDesire and ability for quick learning of new tools and technologies.\nWould be a plus\nKnowledge of Unix-based operating systems (bash/ssh/ps/grep etc.).\nExperience with Github-based development processes.\nExperience with JVM build systems (SBT, Maven, Gradle).\nWe offer\nOpportunity to work on bleeding-edge projects\nWork with a highly motivated and dedicated team\nCompetitive salary\nFlexible schedule\nBenefits package - medical insurance, sports\nCorporate social events\nProfessional development opportunities\nWell-equipped office\nAbout us\nGrid Dynamics (NASDAQ: GDYN) is a leading provider of technology consulting, platform and product engineering, AI, and advanced analytics services. Fusing technical vision with business acumen, we solve the most pressing technical challenges and enable positive business outcomes for enterprise companies undergoing business transformation. A key differentiator for Grid Dynamics is our 8 years of experience and leadership in enterprise AI, supported by profound expertise and ongoing investment in data, analytics, cloud & DevOps, application modernization and customer experience. Founded in 2006, Grid Dynamics is headquartered in Silicon Valley with offices across the Americas, Europe, and India.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Hive', 'SCALA', 'Hadoop', 'Big Data', 'Spark']",2025-06-12 13:43:14
Big Data Engineer,Rarr Technologies,6 - 8 years,Not Disclosed,['Bengaluru'],Job description\n\nProven experience working with data pipelines ETL BI regardless of the technology\nProven experience working with AWS including at least 3 of RedShift S3 EMR Cloud Formation DynamoDB RDS lambda\nBig Data technologies and distributed systems one of Spark Presto or Hive\nPython language scripting and object oriented\nFluency in SQL for data warehousing RedShift in particular is a plus\nGood understanding on data warehousing and Data modelling concepts\nFamiliar with GIT Linux CICD pipelines is a plus\nStrong systems process orientation with demonstrated analytical thinking organization skills and problem solving skills\nAbility to self manage prioritize and execute tasks in a demanding environment\nStrong consultancy orientation and experience with the ability to form collaborative\nproductive working relationships across diverse teams and cultures is a must\nWillingness and ability to train and teach others\nAbility to facilitate meetings and follow up with resulting action items,Industry Type: IT Services & Consulting,Department: Other,"Employment Type: Full Time, Permanent","['python scripting', 'Big Data Technologies', 'ETL', 'AWS']",2025-06-12 13:43:17
AWS Data Engineer,Infosys,5 - 9 years,Not Disclosed,"['Hyderabad', 'Pune', 'Bengaluru']","AWS Data Engineer\n\nTo Apply, use the below link:\nhttps://career.infosys.com/jobdesc?jobReferenceCode=INFSYS-EXTERNAL-210775&rc=0\n\nJOB Profile:\nSignificant 5 to 9 years of experience in designing and implementing scalable data engineering solutions on AWS.\nStrong proficiency in Python programming language.\nExpertise in serverless architecture and AWS services such as Lambda, Glue, Redshift, Kinesis, SNS, SQS, and CloudFormation.\nExperience with Infrastructure as Code (IaC) using AWS CDK for defining and provisioning AWS resources.\nProven leadership skills with the ability to mentor and guide junior team members.\nExcellent understanding of data modeling concepts and experience with tools like ERStudio.\nStrong communication and collaboration skills, with the ability to work effectively in a cross-functional team environment.\nExperience with Apache Airflow for orchestrating data pipelines is a plus.\nKnowledge of Data Lakehouse, dbt, or Apache Hudi data format is a plus.\n\n\nRoles and Responsibilities\nDesign, develop, test, deploy, and maintain large-scale data pipelines using AWS services such as S3, Glue, Lambda, Redshift.\nCollaborate with cross-functional teams to gather requirements and design solutions that meet business needs.\nDesired Candidate Profile\n5-9 years of experience in an IT industry setting with expertise in Python programming language (Pyspark).\nStrong understanding of AWS ecosystem including S3, Glue, Lambda, Redshift.\nBachelor's degree in Any Specialization (B.Tech/B.E.).",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Aws Glue', 'AWS Data Engineer', 'Pyspark', 'Aws Lambda', 'Redshift Aws', 'Python']",2025-06-12 13:43:19
Azure Data Engineer,Infosys,5 - 9 years,Not Disclosed,"['Hyderabad', 'Pune', 'Bengaluru']","Job description\nWe are looking for Azure Data Engineer's resources having minimum 5 to 9 years of Experience.\n\nRole & responsibilities\nBlend of technical expertise with 5 to 9 year of experience, analytical problem-solving, and collaboration with cross-functional teams. Design and implement Azure data engineering solutions (Ingestion & Curation)\nCreate and maintain Azure data solutions including Azure SQL Database, Azure Data Lake, and Azure Blob Storage.\nDesign, implement, and maintain data pipelines for data ingestion, processing, and transformation in Azure.\nUtilizing Azure Data Factory or comparable technologies, create and maintain ETL (Extract, Transform, Load) operations\nUse Azure Data Factory and Databricks to assemble large, complex data sets\nImplementing data validation and cleansing procedures will ensure the quality, integrity, and dependability of the data.\nEnsure data quality / security and compliance.\nOptimize Azure SQL databases for efficient query performance.\nCollaborate with data engineers, and other stakeholders to understand requirements and translate them into scalable and reliable data platform architectures.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Databricks', 'Azure Data Factory', 'Azure Synapse', 'Pyspark', 'Azure Data Lake']",2025-06-12 13:43:21
Data Engineer,Reputed Client,5 - 10 years,18-25 Lacs P.A.,[],"Data Engineer\n(Python, PySpark, SQL and Spark SQL)\n\nExperience - 5-10 Years\nMandate Skills: Python, PySpark, SQL and SparkSQL\nWorking Hours: 11:00 am to 8 pm\n\n(Candidate has to be flexible. 4-hour overlap with US business hours)\n\nSalary : 1.50 LPM to 2 LPM + Tax (Rate is not fixed, negotiable depending upon the candidate feedback)\n\nRemote / Hybrid (3 Days in a week WFO) (Pune, Bangalore, Noida, Mumbai, Hyderabad)\n\nNOTE: Need candidates within these cities, they have to collect assets from the office / need to be available for meetings - if they are working remotely)\n\nIt's a 6 months (C2H role).",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Temporary/Contractual","['Python', 'PySpark', 'Spark', 'SQL']",2025-06-12 13:43:24
Data Engineer,Meritus Management Service,4 - 9 years,9-18 Lacs P.A.,"['Pune', 'Gurugram']","The first Data Engineer specializes in traditional ETL with SAS DI and Big Data (Hadoop, Hive). The second is more versatile, skilled in modern data engineering with Python, MongoDB, and real-time processing.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Etl Pipelines', 'Big Data', 'Informatica', 'SAS DI', 'SQL', 'Hive', 'Hadoop', 'Talend', 'ETL Tool', 'Python']",2025-06-12 13:43:26
Architect (Data Engineering),Amgen Inc,9 - 12 years,Not Disclosed,['Hyderabad'],"ABOUT THE ROLE\n\nRole Description:\n\nWe are seeking a Data Solutions Architect with deep expertise in Biotech/Pharma to design, implement, and optimize scalable and high-performance data solutions that support enterprise analytics, AI-driven insights, and digital transformation initiatives. This role will focus on data strategy, architecture, governance, security, and operational efficiency, ensuring seamless data integration across modern cloud platforms. The ideal candidate will work closely with engineering teams, business stakeholders, and leadership to establish a future-ready data ecosystem, balancing performance, cost-efficiency, security, and usability. This position requires expertise in modern cloud-based data architectures, data engineering best practices, and Scaled Agile methodologies.\n\nRoles & Responsibilities:\nDesign and implement scalable, modular, and future-proof data architectures that initiatives in enterprise.\nDevelop enterprise-wide data frameworks that enable governed, secure, and accessible data across various business domains.\nDefine data modeling strategies to support structured and unstructured data, ensuring efficiency, consistency, and usability across analytical platforms.\nLead the development of high-performance data pipelines for batch and real-time data processing, integrating APIs, streaming sources, transactional systems, and external data platforms.\nOptimize query performance, indexing, caching, and storage strategies to enhance scalability, cost efficiency, and analytical capabilities.\nEstablish data interoperability frameworks that enable seamless integration across multiple data sources and platforms.\nDrive data governance strategies, ensuring security, compliance, access controls, and lineage tracking are embedded into enterprise data solutions.\nImplement DataOps best practices, including CI/CD for data pipelines, automated monitoring, and proactive issue resolution, to improve operational efficiency.\nLead Scaled Agile (SAFe) practices, facilitating Program Increment (PI) Planning, Sprint Planning, and Agile ceremonies, ensuring iterative delivery of enterprise data capabilities.\nCollaborate with business stakeholders, product teams, and technology leaders to align data architecture strategies with organizational goals.\nAct as a trusted advisor on emerging data technologies and trends, ensuring that the enterprise adopts cutting-edge data solutions that provide competitive advantage and long-term scalability.\nMust-Have\n\nSkills:\nExperience in data architecture, enterprise data management, and cloud-based analytics solutions.\nWell versed in domain of Biotech/Pharma industry and has been instrumental in solving complex problems for them using data strategy.\nExpertise in Databricks, cloud-native data platforms, and distributed computing frameworks.\nStrong proficiency in modern data modeling techniques, including dimensional modeling, NoSQL, and data virtualization.\nExperience designing high-performance ETL/ELT pipelines and real-time data processing solutions.\nDeep understanding of data governance, security, metadata management, and access control frameworks.\nHands-on experience with CI/CD for data solutions, DataOps automation, and infrastructure as code (IaC).\nProven ability to collaborate with cross-functional teams, including business executives, data engineers, and analytics teams, to drive successful data initiatives.\nStrong problem-solving, strategic thinking, and technical leadership skills.\nExperienced with SQL/NOSQL database, vector database for large language models\nExperienced with data modeling and performance tuning for both OLAP and OLTP databases\nExperienced with Apache Spark, Apache Airflow\nExperienced with software engineering best-practices, including but not limited to version control (Git, Subversion, etc.), CI/CD (Jenkins, Maven etc.), automated unit testing, and Dev Ops\nGood-to-Have\n\nSkills:\nExperience with Data Mesh architectures and federated data governance models.\nCertification in cloud data platforms or enterprise architecture frameworks.\nKnowledge of AI/ML pipeline integration within enterprise data architectures.\nFamiliarity with BI & analytics platforms for enabling self-service analytics and enterprise reporting.\nEducation and Professional Certifications\n9 to 12 years of experience in Computer Science, IT or related field\nAWS Certified Data Engineer preferred\nDatabricks Certificate preferred\nSoft\n\nSkills:\nExcellent analytical and troubleshooting skills.\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nTeam-oriented, with a focus on achieving team goals.\nAbility to learn quickly, be organized and detail oriented.\nStrong presentation and public speaking skills.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'continuous integration', 'technical leadership', 'metadata management', 'presentation skills', 'ci/cd', 'distributed computing', 'sql', 'data bricks', 'git', 'data modeling', 'spark', 'devops', 'data governance', 'jenkins', 'troubleshooting', 'access control', 'etl']",2025-06-12 13:43:29
Data Engineer III,Expedia Group,5 - 10 years,Not Disclosed,['Bengaluru'],"Why Join Us?\nTo shape the future of travel, people must come first. Guided by our Values and Leadership Agreements, we foster an open culture where everyone belongs, differences are celebrated and know that when one of us wins, we all win.\nWe provide a full benefits package, including exciting travel perks, generous time-off, parental leave, a flexible work model (with some pretty cool offices), and career development resources, all to fuel our employees passion for travel and ensure a rewarding career journey. We re building a more open world. Join us.\nData Engineer III\nIntroduction to the Team\nExpedia Technology teams partner with our Product teams to create innovative products, services, and tools to deliver high-quality experiences for travelers, partners, and our employees. A singular technology platform powered by data and machine learning provides secure, differentiated, and personalized experiences that drive loyalty and traveler satisfaction.\nExpedia Group is seeking a skilled and motivated Data Engineer III to join our Finance Business Intelligence team supporting the Product & Technology Finance organization. In this role, you will help drive data infrastructure and analytics solutions that support strategic financial planning, reporting, and operational decision-making across the Global Finance community. You ll work closely with Finance and Technology partners to ensure data accuracy, accessibility, and usability in support of Expedia s business objectives.\nAs a Data Engineer III, you have strong experience working with a variety of datasets, data environments, tools, and analytical techniques. You enjoy a fun, collaborative and stimulating team environment. Successful candidates should be able to own projects end-to-end, including identifying problems and solutions, building and maintain data pipelines and dashboards, distilling key insights and communicate to stakeholders.\nIn this role, you will:\nDevelop new and improve existing end to end Business Intelligence products (data pipelines, Tableau dashboards, and Machine Learning predictive forecasting models).\nDrive internal efficiencies through streamline code/documentation/Tableau development to maintain high data integrity.\nTroubleshoot and resolve production issues with the team products (automation opportunities, optimizations, back-end data issues, data reconciliations).\nProactively reach out to subject matter experts /stakeholders and collaborate to solve problems.\nRespond to ad hoc data requests and conduct analysis to provide valuable insights to stakeholders.\nCollaborate and coordinate with team members/stakeholders to translate complex data into meaningful insights, that improve the analytical capabilities of the business.\nApply knowledge of database design to support migration of data pipelines from on prem to cloud environment (including data extraction, ingestion, processing of large data sets)\nSupport dashboard development on cloud environment to enable self-service reporting.\nCommunicate clearly on current work status and design considerations\nThink broadly and comprehend the how, why, and what behind data architecture designs\nExperience & Qualifications:\nBachelor s in Computer Science, Mathematics, Statistics, Information Systems, or related field\n5+ years experience in a Data Analyst, Data Engineer or Business Analyst role\nProven expertise in SQL, with practical experience utilizing query engines including SQL Server, Starburst, Trino, Querybook and data science tools such as Python/R, SparkSQL.\nProficient visualization skills (Tableau, Looker, or similar) and excel modeling/report automation.\nExceptional understanding of relational and dimensional datasets, data warehouse and data mining and applies database design principles to solve data requirements\nExperience building robust data extract, load and transform (ELT) processes, that source data from multiple databases.\nDemonstrated record of defining and executing key analysis and solving problems with minimal supervision.\nDynamic individual contributor who consistently enhances operational playbooks to address business problems.\n3+ year working in a hybrid environment that uses both on-premise and cloud technologies is preferred.\nExperience working in an environment that manipulates large datasets on the cloud platform preferred.\nBackground in analytics, finance or a comparable reporting and analytics role preferred.\nAccommodation requests\nIf you need assistance with any part of the application or recruiting process due to a disability, or other physical or mental health conditions, please reach out to our Recruiting Accommodations Team through the Accommodation Request .\nWe are proud to be named as a Best Place to Work on Glassdoor in 2024 and be recognized for award-winning culture by organizations like Forbes, TIME, Disability:IN, and others.\nExpedia Groups family of brands includes: Brand Expedia , Hotels.com , Expedia Partner Solutions, Vrbo , trivago , Orbitz , Travelocity , Hotwire , Wotif , ebookers , CheapTickets , Expedia Group Media Solutions, Expedia Local Expert , CarRentals.com , and Expedia Cruises . 2024 Expedia, Inc. All rights reserved. Trademarks and logos are the property of their respective owners. . Never provide sensitive, personal information to someone unless you re confident who the recipient is. Expedia Group does not extend job offers via email or any other messaging tools to individuals with whom we have not made prior contact. Our email domain is @expediagroup.com. The official website to find and apply for job openings at Expedia Group is careers.expediagroup.com/jobs .\nExpedia is committed to creating an inclusive work environment with a diverse workforce. All qualified applicants will receive consideration for employment without regard to race, religion, gender, sexual orientation, national origin, disability or age.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Automation', 'Database design', 'Machine learning', 'Business intelligence', 'Data mining', 'Analytics', 'SQL', 'Python', 'Data architecture']",2025-06-12 13:43:32
Azure Data Engineer,Hexaware Technologies,6 - 10 years,Not Disclosed,"['Chennai', 'Bengaluru']","Hi\n\nWork Location : Chennai AND Bangalore\nWork location : Imm - 30 days\n\nPrimary: Azure Databricks,ADF, Pyspark SQL",,,,"['Pyspark', 'Azure Databricks', 'SQL', 'Azure Data Factory']",2025-06-12 13:43:34
Data Engineer,Society Managers,3 - 5 years,Not Disclosed,['Mumbai (All Areas)'],"We are seeking a skilled and driven SDE-II (Data Engineering) to join our dynamic team. In this role, you will design, develop, and maintain scalable data pipelines, working with large, complex datasets. Youll collaborate closely with cross-functional teams to gather data requirements and contribute to the architecture of our data systems, leveraging your expertise in tools like Databricks, Spark, and SQL.\n\nRoles and responsibilities\nData Pipeline Development: Design,build, and maintain scalable data pipelines using Databricks, Python,and Spark.\nData Processing & Transformation: Handle large, complex datasets to ensure efficient data processing and transformations.\nCollaboration: Work with cross-functional teams to gather, understand, and implement data requirements.\nSQL & ETL: Write and optimize SQL queries for data extraction, transformation, and loading (ETL) processes.\nData Quality & Security: Ensure data accuracy, integrity, and security across all stages of the data lifecycle.\nSystem Design & Architecture: Contribute to the design and architecture of scalable data systems and solutions.\nRequired Skills and Qualification\nExperience: 3+ years of experience in data engineering or a related field.\nDatabricks & Spark: Strong expertise in Databricks and distributed data processing with Spark.\nProgramming: Proficiency in Python for data engineering tasks.\nSQL Optimization: Solid experience in writing and optimizing complex SQL queries.\nData Systems Knowledge: Hands-on experience with large-scale data systems and tools.\nDomain Knowledge: Familiarity with Capital Market/Private Equity is a plus (relaxation may apply).\nData Visualization: Experience with Tableau for creating insightful data visualizations and reports.\nPreferred skills\nCloud Platforms: Familiarity with cloud services like AWS, Azure, or GCP.\nData Warehousing & ETL:Experience with data warehousing concepts and ETL processes.\nAnalytical Skills: Strong problem-solving and analytical capabilities Analytics Tools: Hands-on experience with tools like Amplitude, PostHog, Google Analytics, or Mixpanel.\nAdditional Tools: Knowledge of Python for web scraping and frameworks like Django (good to have).",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'Microsoft Azure', 'Data Bricks', 'Spark', 'ETL', 'Python', 'SQL']",2025-06-12 13:43:36
Big Data Engineer,Apexon,11 - 16 years,Not Disclosed,['Bengaluru'],"We enable #HumanFirstDigital\n\nJob Summary:\nWe are looking for a highly experienced and strategic Data Engineer to drive the design, development, and optimization of our enterprise data platform. This role requires deep technical expertise in AWS, StreamSets, and Snowflake, along with solid experience in Kubernetes, Apache Airflow, and unit testing. The ideal candidate will lead a team of data engineers and play a key role in delivering scalable, secure, and high-performance data solutions for both historical and incremental data loads.\nKey Responsibilities:\nLead the architecture, design, and implementation of end-to-end data pipelines using StreamSets and Snowflake.\nOversee the development of scalable ETL/ELT processes for historical data migration and incremental data ingestion.\nGuide the team in leveraging AWS services (S3, Lambda, Glue, IAM, etc.) to build cloud-native data solutions.\nProvide technical leadership in deploying and managing containerized applications using Kubernetes.\nDefine and implement workflow orchestration strategies using Apache Airflow.\nEstablish best practices for unit testing, code quality, and data validation.\nCollaborate with data architects, analysts, and business stakeholders to align data solutions with business goals.\nMentor junior engineers and foster a culture of continuous improvement and innovation.\nMonitor and optimize data workflows for performance, scalability, and cost-efficiency.\nRequired Skills & Qualifications:\nHigh proficiency in AWS, including hands-on experience with core services (S3, Lambda, Glue, IAM, CloudWatch).\nExpert-level experience with StreamSets, including Data Collector, Transformer, and Control Hub.\nStrong Snowflake expertise, including data modeling, SnowSQL, and performance tuning.\nMedium-level experience with Kubernetes, including container orchestration and deployment.\nWorking knowledge of Apache Airflow for workflow scheduling and monitoring.\nExperience with unit testing frameworks and practices in data engineering.\nProven experience in building and managing ETL pipelines for both batch and real-time data.\nStrong command of SQL and scripting languages such as Python or Shell.\nExperience with CI/CD pipelines and version control tools (e.g., Git, Jenkins).\nPreferred Qualifications:\nAWS certification (e.g., AWS Certified Data Analytics, Solutions Architect).\nExperience with data governance, security, and compliance frameworks.\nFamiliarity with Agile methodologies and tools like Jira and Confluence.\nPrior experience in a leadership or mentoring role within a data engineering team.\nOur Commitment to Diversity & Inclusion:\nOur Perks and Benefits:\nOur benefits and rewards program has been thoughtfully designed to recognize your skills and contributions, elevate your learning/upskilling experience and provide care and support for you and your loved ones. As an Apexon Associate, you get continuous skill-based development, opportunities for career advancement, and access to comprehensive health and well-being benefits and assistance.\nWe also offer:\no Group Health Insurance covering family of 4\no Term Insurance and Accident Insurance\no Paid Holidays & Earned Leaves\no Paid Parental LeaveoLearning & Career Development\no Employee Wellness\nJob Location : Bengaluru, India",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Performance tuning', 'Data modeling', 'Agile', 'Wellness', 'Workflow', 'Healthcare', 'Unit testing', 'Apache', 'SQL', 'Python']",2025-06-12 13:43:38
Big Data Engineer,Client of Hiresquad Resources,5 - 8 years,22.5-30 Lacs P.A.,"['Noida', 'Hyderabad', 'Bengaluru']","Role: Data Engineer\nExp: 5 to 8 Years\nLocation: Bangalore, Noida, and Hyderabad (Hybrid, weekly 2 Days office must)\nNP: Immediate to 15 Days (Try to find only immediate joiners)\n\n\nNote:\nCandidate must have experience in Python, Kafka Streams, Pyspark, and Azure Databricks.\nNot looking for candidates who have only Exp in Pyspark and not in Python.\n\n\nJob Title: SSE Kafka, Python, and Azure Databricks (Healthcare Data Project)\nExperience:  5 to 8 years\n\nRole Overview:\nWe are looking for a highly skilled with expertise in Kafka, Python, and Azure Databricks (preferred) to drive our healthcare data engineering projects. The ideal candidate will have deep experience in real-time data streaming, cloud-based data platforms, and large-scale data processing. This role requires strong technical leadership, problem-solving abilities, and the ability to collaborate with cross-functional teams.\n\nKey Responsibilities:\nLead the design, development, and implementation of real-time data pipelines using Kafka, Python, and Azure Databricks.\nArchitect scalable data streaming and processing solutions to support healthcare data workflows.\nDevelop, optimize, and maintain ETL/ELT pipelines for structured and unstructured healthcare data.\nEnsure data integrity, security, and compliance with healthcare regulations (HIPAA, HITRUST, etc.).\nCollaborate with data engineers, analysts, and business stakeholders to understand requirements and translate them into technical solutions.\nTroubleshoot and optimize Kafka streaming applications, Python scripts, and Databricks workflows.\nMentor junior engineers, conduct code reviews, and ensure best practices in data engineering.\nStay updated with the latest cloud technologies, big data frameworks, and industry trends.\n\n\nRequired Skills & Qualifications:\n4+ years of experience in data engineering, with strong proficiency in Kafka and Python.\nExpertise in Kafka Streams, Kafka Connect, and Schema Registry for real-time data processing.\nExperience with Azure Databricks (or willingness to learn and adopt it quickly).\nHands-on experience with cloud platforms (Azure preferred, AWS or GCP is a plus).\nProficiency in SQL, NoSQL databases, and data modeling for big data processing.\nKnowledge of containerization (Docker, Kubernetes) and CI/CD pipelines for data applications.\nExperience working with healthcare data (EHR, claims, HL7, FHIR, etc.) is a plus.\nStrong analytical skills, problem-solving mindset, and ability to lead complex data projects.\nExcellent communication and stakeholder management skills.\n\n\n\nEmail: Sam@hiresquad.in",Industry Type: Medical Services / Hospital,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Kafka', 'Azure Databricks', 'Python', 'Etl Pipelines', 'Pyspark', 'spark architecture', 'Data Engineering', 'opps concepts', 'Data Streaming', 'Medallion Architecture', 'python scripts', 'schema registry', 'SQL Database', 'Nosql Databases', 'spark tuning', 'Kafka Streams', 'kafka connect']",2025-06-12 13:43:41
Lead Big Data Engineer - Python & Spark,Hubnex,7 - 12 years,Not Disclosed,['Bengaluru'],"Job Title: Lead Big Data Engineer - Python & Spark\nLocation: Gurgaon, India\nExperience: 7+ Years\nEmployment Type: Full-Time | Onsite\nDepartment: Data Engineering\nAbout Hubnex Labs:\nHubnex Labs is a forward-looking IT consulting and software services company, building next-generation data platforms, AI systems, and enterprise-grade applications. We re looking for a Lead Big Data Engineer to drive the development and deployment of high-performance data processing solutions.\nRole Overview:\nAs a Lead Big Data Engineer , you will be responsible for architecting and implementing scalable big data solutions using Spark, Python, Hive, and related technologies. You will mentor a team of developers and work closely with cross-functional stakeholders to ensure on-time, error-free software delivery.\nKey Responsibilities:\nLead and mentor a team of Python and big data developers to deliver robust data-driven applications\nDesign, develop, and maintain scalable data processing pipelines using Spark (Scala/PySpark), Hive, and Hadoop ecosystems\nWrite efficient, reusable, and well-documented code in Python, SQL, and shell scripting\nOptimize Spark applications for performance and scalability; tune existing Hadoop-based systems\nCollaborate with QA, DevOps, and business teams to ensure high-quality software delivery\nPerform code reviews , enforce coding standards, and contribute to overall architectural decisions\nActively participate in daily Scrum meetings , sprint planning, retrospectives, and release cycles\nTroubleshoot complex issues across the full data pipeline\nRequired Qualifications:\n7+ years of hands-on experience in software development, with a strong background in big data frameworks\nDeep expertise in Hadoop ecosystem including HDFS, Hive, HQL, Spark (Scala/PySpark), Sqoop\n4+ years of programming experience using Python , SQL , and Unix shell scripting\nProven experience in leading development teams and delivering enterprise-level solutions\nStrong grasp of Spark architecture , performance tuning, and data frame APIs\nExcellent understanding of database concepts , data structures , and distributed systems\nBachelors degree in Computer Science , Engineering , or a related field\nExceptional communication, problem-solving, and leadership skills\nPreferred Skills:\nExperience with CI/CD pipelines for data projects\nFamiliarity with cloud-based data platforms (AWS EMR, GCP DataProc, or Azure HDInsight)\nWorking knowledge of Kafka , Airflow , or other data orchestration tools\nWhy Join Hubnex Labs?\nWork on mission-critical big data solutions that impact businesses globally\nBe part of an innovative, agile, and supportive team\nLeadership opportunities and exposure to latest technologies in AI and cloud computing\nCompetitive salary, performance incentives, and professional growth support",Industry Type: Internet,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Performance tuning', 'Cloud computing', 'Coding', 'Agile', 'Data structures', 'Scrum', 'Unix shell scripting', 'SQL', 'Python']",2025-06-12 13:43:43
"Senior Data Engineer (Snowflake, DBT)",Allegis Global Solutions (AGS),5 - 10 years,Not Disclosed,[],"Senior Data Engineer (Snowflake, DBT, Azure)\nJob Location: Hyderabad / Bangalore / Chennai / Kolkata / Noida/ Gurgaon / Pune / Indore / Mumbai\n\nJob Details\nTechnical Expertise:\nStrong proficiency in Snowflake architecture, including data sharing, partitioning, clustering, and materialized views.\nAdvanced experience with DBT for data transformations and workflow management.\nExpertise in Azure services, including Azure Data Factory, Azure Data Lake, Azure Synapse, and Azure Functions.\nData Engineering:\nProficiency in SQL, Python, or other relevant programming languages.\nStrong understanding of data modeling concepts, including star schema and normalization.\nHands-on experience with ETL/ELT pipelines and data integration tools.\n\nPreferred Qualifications:\nCertifications in Snowflake, DBT, or Azure Data Engineering.\nFamiliar with data visualization tools like Power BI or Tableau.\nKnowledge of CI/CD pipelines and DevOps practices for data workflows.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Snowflake', 'Data Build Tool', 'Azure']",2025-06-12 13:43:46
Senior Data Engineer,Epsilon,5 - 9 years,Not Disclosed,['Bengaluru'],"This position in the Engineering team under the Digital Experience organization. We drive the first mile of the customer experience through personalization of offers and content. We are currently on the lookout for a smart, highly driven engineer.\nYou will be part of a team that is focused on building & managing solutions, pipelines using marketing technology stacks. You will also be expected to Identify and implement improvements including for optimizing data delivery and automate processes/pipelines.\nThe incumbent is also expected to partner with various stakeholders, bring scientific rigor to design and develop high quality solutions.\nCandidate must have excellent verbal and written communication skills and be comfortable working in an entrepreneurial, startup environment within a larger company.\nClick here to view how Epsilon transforms marketing with 1 View, 1 Vision and 1 Voice.\n\nBrief Description of Role:\nExperience with both structured and unstructured data\nExperience working on AdTech or MarTech technologies.\nExperience in relational and non-relational databases and SQL (NoSQL is a plus).\nUnderstanding of Data Modeling, Data Catalog concepts and tools\nAbility to deal with data imperfections such as missing values, outliers, inconsistent formatting, etc.\nCollaborate with other members of the team to ensure high quality deliverables\nLearning and implementing the latest design patterns in data engineering\n\nData Management\nExperience with both structured and unstructured data\nExperience building Data and CI/CD pipelines\nExperience working on AdTech or MarTech technologies is added advantage\nExperience in relational and non-relational databases and SQL (NoSQL is a plus).\nHands on experience building ETL workflows/pipelines on large volumes of data\nGood understanding of Data Modeling, Data Warehouse, Data Catalog concepts and tools\nAble to identify, join, explore, and examine data from multiple disparate sources and formats\nAbility to reduce large quantities of unstructured or formless data and get it into a form in which it can be analyzed\nAbility to deal with data imperfections such as missing values, outliers, inconsistent formatting, etc.\nDevelopment\nAbility to write code in programming languages such as Python and shell script on Linux\nFamiliarity with development methodology such as Agile/Scrum\nLove to learn new technologies, keep abreast of the latest technologies within the cloud architecture, and drive your organization to adapt to emerging best practices\nGood knowledge of working in UNIX/LINUX systems\nQualifications\nBachelors degree in computer science with 5+ years of similar experience\nTech Stack: Python, SQL, Scripting language (preferably JavaScript)\nExperience or knowledge on Adobe Experience Platform (RT-CDP/AEP)\nExperience working in Cloud Platforms (GCP or AWS)\nFamiliarity with automated unit/integration test frameworks\nGood written and spoken communication skills, team player.\nStrong analytic thought process and ability to interpret findings",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'Data Engineering', 'Data Bricks', 'Python', 'SQL', 'Azure Aws', 'AWS']",2025-06-12 13:43:48
Data Engineer III,Expedia Group,6 - 11 years,Not Disclosed,['Gurugram'],"Why Join Us?\nTo shape the future of travel, people must come first. Guided by our Values and Leadership Agreements, we foster an open culture where everyone belongs, differences are celebrated and know that when one of us wins, we all win.\nWe provide a full benefits package, including exciting travel perks, generous time-off, parental leave, a flexible work model (with some pretty cool offices), and career development resources, all to fuel our employees passion for travel and ensure a rewarding career journey. We re building a more open world. Join us.\nData Engineer III\nExpedia Group s CTO Enablement team is looking for a highly motivated Data Engineer III to lead the design, delivery, and stewardship of business-critical data infrastructure that powers our Capitalization program and Business Operations functions . This role is at the intersection of finance, strategy, and engineering , where data precision and operational rigor directly support the company s financial integrity and execution effectiveness.\nYou will collaborate with stakeholders across Finance, BizOps, and Technology to build scalable data solutions that ensure capitalization accuracy, enable deep operational analytics, and streamline financial and business reporting at scale.\nWhat you will do:\nDesign, build, and maintain high-scale data pipelines and transformation logic to support CapEx/OpEx classification, capitalization tracking, and operational data modeling.\nDeliver clean, well-documented, governed datasets that drive finance reporting, strategic planning, and key operational dashboards.\nPartner with cross-functional teams (Finance, Engineering, Strategy) to translate business and compliance requirements into technical solutions.\nLead the development of data models and ETL processes to support performance monitoring, workforce utilization, project financials, and business KPIs.\nEstablish and enforce data quality, lineage, and access control standards to ensure trust in business-critical data.\nProactively identify and resolve data reliability issues related to financial close processes, budget tracking, and capitalization rules.\nServe as a technical advisor to BizOps and Finance stakeholders, recommending improvements in tooling, architecture, and process automation.\nMentor other engineers and contribute to the growth of a high-performance data team culture.\nWho you are:\n6+ years of experience in data engineering , analytics engineering , or data infrastructure roles with a focus on operational and financial data.\nExpertise in SQL and Python , and experience with data pipeline orchestration tools such as Airflow , dbt , or equivalent.\nStrong understanding of cloud-based data platforms (e.g., Snowflake, BigQuery, Redshift, or Databricks).\nDeep familiarity with capitalization standards , CapEx/OpEx distinction, and operational reporting in a tech-driven environment.\nDemonstrated ability to build scalable, reliable ETL/ELT workflows that serve diverse analytical and reporting needs.\nExperience working cross-functionally in complex organizations with multiple stakeholder groups.\nPassion for operational excellence, data governance, and driving actionable business insights from data.\nPreferred qualifications:\n- Experience supporting BizOps , FP&A , or Product Finance teams with data tooling and reporting.\n- Familiarity with BI platforms like Looker , Power BI , or Tableau .\n- Exposure to agile delivery frameworks and enterprise-level operational rhythms.\nAccommodation requests\nIf you need assistance with any part of the application or recruiting process due to a disability, or other physical or mental health conditions, please reach out to our Recruiting Accommodations Team through the Accommodation Request .\nWe are proud to be named as a Best Place to Work on Glassdoor in 2024 and be recognized for award-winning culture by organizations like Forbes, TIME, Disability:IN, and others.\nExpedia Groups family of brands includes: Brand Expedia , Hotels.com , Expedia Partner Solutions, Vrbo , trivago , Orbitz , Travelocity , Hotwire , Wotif , ebookers , CheapTickets , Expedia Group Media Solutions, Expedia Local Expert , CarRentals.com , and Expedia Cruises . 2024 Expedia, Inc. All rights reserved. Trademarks and logos are the property of their respective owners. . Never provide sensitive, personal information to someone unless you re confident who the recipient is. Expedia Group does not extend job offers via email or any other messaging tools to individuals with whom we have not made prior contact. Our email domain is @expediagroup.com. The official website to find and apply for job openings at Expedia Group is careers.expediagroup.com/jobs .\nExpedia is committed to creating an inclusive work environment with a diverse workforce. All qualified applicants will receive consideration for employment without regard to race, religion, gender, sexual orientation, national origin, disability or age.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Operational excellence', 'Data modeling', 'Talent acquisition', 'Analytical', 'Strategic planning', 'Data quality', 'Operations', 'Analytics', 'SQL', 'Business operations']",2025-06-12 13:43:51
Senior Data Engineer,Wavicle Data Solutions,6 - 11 years,15-25 Lacs P.A.,"['Chennai', 'Coimbatore', 'Bengaluru']","Hi Professionals,\n\nWe are looking for Senior Data Engineer for Permanent Role\n\nWork Location: Hybrid Chennai, Coimbatore or Bangalore\n\nExperience: 6 to 12 Years\n\nNotice Period: 0 TO 15 Days or Immediate Joiner.\n\nSkills:\n1. Python\n2. Pyspark\n3. SQL\n4. AWS\n5. GCP\n6. MLOps\n\nInterested can send your resume to gowtham.veerasamy@wavicledata.com.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'GCP', 'AWS', 'Ml']",2025-06-12 13:43:53
Cloud Data Engineer - GCP,Synechron,2 - 3 years,Not Disclosed,"['Hyderabad', 'Gachibowli']","Job Summary\nSynechron is seeking a highly motivated and skilled Senior Cloud Data Engineer GCP to join our cloud solutions team. In this role, you will collaborate closely with clients and internal stakeholders to design, implement, and manage scalable, secure, and high-performance cloud-based data solutions on Google Cloud Platform (GCP). You will leverage your technical expertise to ensure the integrity, security, and efficiency of cloud data architectures, enabling the organization to derive maximum value from cloud data assets. This role contributes directly to our mission of delivering innovative digital transformation solutions and supports the organizations strategic objectives of scalable and sustainable cloud infrastructure.\nSoftware Requirements\nRequired Skills:\nProficiency with Google Cloud Platform (GCP) services (Compute Engine, Cloud Storage, BigQuery, Cloud Pub/Sub, Dataflow, etc.)\nBasic scripting skills with Python, Bash, or similar languages\nFamiliarity with virtualization and cloud networking concepts\nUnderstanding of cloud security best practices and compliance standards\nExperience with infrastructure as code tools (e.g., Terraform, Deployment Manager)\nStrong knowledge of data management, data pipelines, and ETL processes\nPreferred Skills:\nExperience with other cloud platforms (AWS, Azure)\nKnowledge of SQL and NoSQL databases\nFamiliarity with containerization (Docker, GKE)\nExperience with data visualization tools\nOverall Responsibilities\nDesign, implement, and operate cloud data solutions that are secure, scalable, and optimized for performance\nCollaborate with clients and internal teams to identify infrastructure and data architecture requirements\nManage and monitor cloud infrastructure and ensure operational reliability\nResolve technical issues related to cloud data workflows and storage solutions\nParticipate in project planning, timelines, and technical documentation\nContribute to best practices and continuous improvement initiatives within the organization\nEducate and support clients in adopting cloud data services and best practices\nTechnical Skills (By Category)\nProgramming Languages:\nEssential: Python, Bash scripts\nPreferred: SQL, Java, or other data processing languages\nDatabases & Data Management:\nEssential: BigQuery, Cloud SQL, Cloud Spanner, Cloud Storage\nPreferred: NoSQL databases like Firestore, MongoDB\nCloud Technologies:\nEssential: Google Cloud Platform core services (Compute, Storage, BigQuery, Dataflow, Pub/Sub)\nPreferred: Cloud monitoring, logging, and security tools\nFrameworks & Libraries:\nEssential: Data pipeline frameworks, Cloud SDKs, APIs\nPreferred: Apache Beam, Data Studio\nDevelopment Tools & Methodologies:\nEssential: Infrastructure as Code (Terraform, Deployment Manager)\nPreferred: CI/CD tools (Jenkins, Cloud Build)\nSecurity Protocols:\nEssential: IAM policies, data encryption, network security best practices\nPreferred: Compliance frameworks such as GDPR, HIPAA\nExperience Requirements\n2-3 years of experience in cloud data engineering, cloud infrastructure, or related roles\nHands-on experience with GCP is preferred; experience with AWS or Azure is a plus\nBackground in designing and managing cloud data pipelines, storage, and security solutions\nProven ability to deliver scalable data solutions in cloud environments\nExperience working with cross-functional teams on cloud deployments\nAlternative experience pathways: academic projects, certifications, or relevant internships demonstrating cloud data skills\nDay-to-Day Activities\nDevelop and deploy cloud data pipelines, databases, and analytics solutions\nCollaborate with clients and team members to plan and implement infrastructure architecture\nPerform routine monitoring, maintenance, and performance tuning of cloud data systems\nTroubleshoot technical issues affecting data workflows and resolve performance bottlenecks\nDocument system configurations, processes, and best practices\nEngage in continuous learning on new cloud features and data management tools\nParticipate in project meetings, code reviews, and knowledge sharing sessions\nQualifications\nBachelors or Masters degree in computer science, engineering, information technology, or a related field\nRelevant certifications (e.g., Google Cloud Professional Data Engineer, Cloud Architect) are preferred\nTraining in cloud security, data management, or infrastructure design is advantageous\nCommitment to professional development and staying updated with emerging cloud technologies\nProfessional Competencies\nCritical thinking and problem-solving skills to resolve complex cloud architecture challenges\nAbility to work collaboratively with multidisciplinary teams and clients\nStrong communication skills for technical documentation and stakeholder engagement\nAdaptability to evolving cloud technologies and project priorities\nOrganized with a focus on quality and detail-oriented delivery\nProactive learner with a passion for innovation in cloud data solutions\nAbility to manage multiple tasks effectively and prioritize in a fast-paced environment",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['GCP', 'Jenkins', 'Java', 'NoSQL', 'Bash scripts', 'Data Studio', 'Data Management', 'CI/CD', 'Apache Beam', 'MongoDB', 'Cloud Build']",2025-06-12 13:43:55
Senior Data Engineer,SPAN.IO,5 - 10 years,Not Disclosed,['Bengaluru( Indira Nagar )'],"Senior Data Engineer\n\nOur Mission\n\nSPAN is enabling electrification for all\nWe are a mission-driven company designing, building, and deploying products that electrify the built environment, reduce carbon emissions, and slow the effects of climate change.\nDecarbonization is the process to reduce or remove greenhouse gas emissions, especially carbon dioxide, from entering our atmosphere.\nElectrification is the process of replacing fossil fuel appliances that run on gas or oil with all-electric upgrades for a cleaner way to power our lives.\n\nAt SPAN, we believe in:\nEnabling homes and vehicles powered by clean energy\nMaking electrification upgrades possible\nBuilding more resilient homes with reliable backup\nDesigning a flexible and distributed electrical grid\n\nThe Role\nAs a Data Engineer you would be working to design, build, test and create infrastructure necessary for real time analytics and batch analytics pipelines. You will work with multiple teams within the org to provide analysis, insights on the data. You will also be involved in writing ETL processes that support data ingestion. You will also guide and enforce best practices for data management, governance and security. You will build infrastructure to monitor these data pipelines / ETL jobs / tasks and create tooling/infrastructure for providing visibility into these.\n\nResponsibilities\nWe are looking for a Data Engineer with passion for building data pipelines, working with product, data science and business intelligence teams and delivering great solutions. As a part of the team you:-\nAcquire deep business understanding on how SPAN data flows from IoT device to cloud through the system and build scalable and optimized data solutions that impact many stakeholders.\nBe an advocate for data quality and excellence of our platform.\nBuild tools that help streamline the management and operation of our data ecosystem.\nEnsure best practices and standards in our data ecosystem are shared across teams.\nWork with teams within the company to build close relationships with our partners to understand the value our platform can bring and how we can make it better.\nImprove data discovery by creating data exploration processes and promoting adoption of data sources across the company.\nHave a desire to write tools and applications to automate work rather than do everything by hand.\nAssist internal teams in building out data logging, alerting and monitoring for their applications\nAre passionate about CI/CD process.\nDesign, develop and establish KPIs to monitor analysis and provide strategic insights to drive growth and performance.\n\nAbout You\n\nRequired Qualifications\nBachelor's Degree in a quantitative discipline: computer science, statistics, operations research, informatics, engineering, applied mathematics, economics, etc.\n5+ years of relevant work experience in data engineering, business intelligence, research or related fields.\nExpert level production-grade, programming experience in at least one of these languages (Python, Kotlin, or other JVM based languages)\nExperience in writing clean, concise and well structured code in one of the above languages.\nExperience working with Infrastructure-as-code tools: Pulumi, Terraform, etc.\nExperience working with CI/CD systems: Circle-CI, Github Actions, Argo-CD, etc.\nExperience managing data engineering infrastructure through Docker and Kubernetes\nExperience working with latency data processing solutions like Flink, Prefect, AWS Kinesis, Kafka, Spark Stream processing etc.\nExperience with SQL/Relational databases, OLAP databases like Snowflake.\nExperience working in AWS: S3, Glue, Athena, MSK, EMR, ECR etc.\n\nBonus Qualifications\nExperience with the Energy industry\nExperience with building IoT and/or hardware products\nUnderstanding of electrical systems and residential loads\nExperience with data visualization using Tableau.\nExperience in Data loading tools like FiveTran as well as data debugging tools such as DataDog\n\nLife at SPAN\nOur Bengaluru team plays a pivotal role in SPANs continued growth and expansion. Together, were driving engineering, product development, and operational excellence to shape the future of home energy solutions.\nAs part of our team in India, youll have the opportunity to collaborate closely with our teams in the US and across the globe. This international collaboration fosters innovation, learning, and growth, while helping us achieve our bold mission of electrifying homes and advancing clean energy solutions worldwide.\nOur in-office culture offers the chance for dynamic interactions and hands-on teamwork, making SPAN a truly collaborative environment where every team members contribution matters.\nOur climate-focused culture is driven by a team of forward-thinkers, engineers, and problem-solvers who push boundaries every day.\nDo mission-driven work: Every role at SPAN directly advances clean energy adoption.\nBring powerful ideas to life: We encourage diverse ideas and perspectives to drive stronger products.\nNurture an innovation-first mindset: We encourage big thinking and bold action.\nDeliver exceptional customer value: We value hard work, and the ability to deliver exceptional customer value.\n\nBenefits at SPAN India\nGenerous paid leave\nComprehensive Insurance & Health Benefits\nCentrally located office in Bengaluru with easy access to public transit, dining, and city amenities\n\nInterested in joining our team? Apply today and well be in touch with the next steps!",Industry Type: Electronics Manufacturing,"Department: Production, Manufacturing & Engineering","Employment Type: Full Time, Permanent","['Terraform', 'Snowflake', 'AWS', 'Python', 'SQL', 'Java', 'Apache Flink', 'Kotlin']",2025-06-12 13:43:57
Sr Data Engineer,Lowes Services India Private limited,5 - 10 years,Not Disclosed,['Bengaluru'],"We are seeking a seasoned Senior Data Engineer to join our Marketing Data Platform team. This role is pivotal in designing, building, and optimizing scalable data pipelines and infrastructure that support our marketing analytics and customer engagement strategies. The ideal candidate will have extensive experience with big data technologies, cloud platforms, and a strong understanding of marketing data dynamics.\n\nData Pipeline Development & Optimization\nDesign, develop, and maintain robust ETL/ELT pipelines using Apache PySpark on GCP services like Dataproc and Cloud Composer.\nEnsure data pipelines are scalable, efficient, and reliable to handle large volumes of marketing data.\nData Warehousing & Modeling\nImplement and manage data warehousing solutions using BigQuery, ensuring optimal performance and cost-efficiency.\nDevelop and maintain data models that support marketing analytics and reporting needs.\nCollaboration & Stakeholder Engagement\nWork closely with marketing analysts, data scientists, and cross-functional teams to understand data requirements and deliver solutions that drive business insights.\nTranslate complex business requirements into technical specifications and data architecture.\nData Quality & Governance\nImplement data quality checks and monitoring to ensure the accuracy and integrity of marketing data.\nAdhere to data governance policies and ensure compliance with data privacy regulations.\nContinuous Improvement & Innovation\nStay abreast of emerging technologies and industry trends in data engineering and marketing analytics.\nPropose and implement improvements to existing data processes and infrastructure\n  Years of Experience\n5 Years in Data Engineer space\n  Education Qualification & Certifications\nB.Tech or MCA\n  Experience\nProven experience with Apache PySpark, GCP (including Dataproc, BigQuery, Cloud Composer), and data pipeline orchestration.\nTechnical Skills\nProficiency in SQL and Python.\nExperience with data modeling, ETL/ELT processes, and data warehousing concepts.",Industry Type: Retail,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['orchestration', 'Data modeling', 'data governance', 'Data quality', 'Apache', 'Continuous improvement', 'Monitoring', 'SQL', 'Python', 'Data architecture']",2025-06-12 13:43:59
Lead AWS Glue Data Engineer,Allegis Group,8 - 13 years,Not Disclosed,[],"Lead AWS Glue Data Engineer\nJob Location : Hyderabad / Bangalore / Chennai / Noida/ Gurgaon / Pune / Indore / Mumbai/ Kolkata\n\nWe are seeking a skilled Lead AWS Data Engineer with 8+ years of strong programming and SQL skills to join our team. The ideal candidate will have hands-on experience with AWS Data Analytics services and a basic understanding of general AWS services. Additionally, prior experience with Oracle and Postgres databases and secondary skills in Python and Azure DevOps will be an advantage.\n\nKey Responsibilities:\nDesign, develop, and optimize data pipelines using AWS Data Analytics services such as RDS, DMS, Glue, Lambda, Redshift, and Athena.\nImplement data migration and transformation processes using AWS DMS and Glue.\nWork with SQL (Oracle & Postgres) to query, manipulate, and analyse large datasets.\nDevelop and maintain ETL/ELT workflows for data ingestion and transformation.\nUtilize AWS services like S3, IAM, CloudWatch, and VPC to ensure secure and efficient data operations.\nWrite clean and efficient Python scripts for automation and data processing.\nCollaborate with DevOps teams using Azure DevOps for CI/CD pipelines and infrastructure management.\nMonitor and troubleshoot data workflows to ensure high availability and performance.\n\nPreferred Qualifications:\nAWS certifications in Data Analytics, Solutions Architect, or DevOps.\nExperience with data warehousing concepts and data lake implementations.\nHands-on experience with Infrastructure as Code (IaC) tools like Terraform or CloudFormation.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['RDS', 'Glue', 'DMS', 'Lambda', 'Redshift', 'Athena']",2025-06-12 13:44:02
Azure Data Engineer,HTC Global Services,4 - 8 years,Not Disclosed,['Bengaluru( Murugeshpalya )'],"Job Summary:\nWe are looking for a highly skilled Azure Data Engineer with experience in building and managing scalable data pipelines using Azure Data Factory, Synapse, and Databricks. The ideal candidate should be proficient in big data tools and Azure services, with strong programming knowledge and a solid understanding of data architecture and cloud platforms.\n\nKey Responsibilities:",,,,"['Power Bi', 'Azure Databricks', 'Azure Data Factory', 'Synapse', 'Python', 'Java', 'Scala', 'Kafka', 'big data tools', 'SQL', 'EventHub', 'Azure cloud services', 'Spark']",2025-06-12 13:44:04
Azure Cloud Data Engineering Consultant,Optum,7 - 10 years,17-27.5 Lacs P.A.,['Gurugram'],"Primary Responsibilities:\nDesign and develop applications and services running on Azure, with a strong emphasis on Azure Databricks, ensuring optimal performance, scalability, and security.\nBuild and maintain data pipelines using Azure Databricks and other Azure data integration tools.\nWrite, read, and debug Spark, Scala, and Python code to process and analyze large datasets.\nWrite extensive query in SQL and Snowflake\nImplement security and access control measures and regularly audit Azure platform and infrastructure to ensure compliance.\nCreate, understand, and validate design and estimated effort for given module/task, and be able to justify it.\nPossess solid troubleshooting skills and perform troubleshooting of issues in different technologies and environments.\nImplement and adhere to best engineering practices like design, unit testing, functional testing automation, continuous integration, and delivery.\nMaintain code quality by writing clean, maintainable, and testable code.\nMonitor performance and optimize resources to ensure cost-effectiveness and high availability.\nDefine and document best practices and strategies regarding application deployment and infrastructure maintenance.\nProvide technical support and consultation for infrastructure questions.\nHelp develop, manage, and monitor continuous integration and delivery systems.\nTake accountability and ownership of features and teamwork.\nComply with the terms and conditions of the employment contract, company policies and procedures, and any directives.\nRequired Qualifications:\nB.Tech/MCA (Minimum 16 years of formal education)\nOverall 7+ years of experience.\nMinimum of 3 years of experience in Azure (ADF), Databricks and DevOps.\n5 years of experience in writing advanced level SQL.\n2-3 years of experience in writing, reading, and debugging Spark, Scala, and Python code.\n3 or more years of experience in architecting, designing, developing, and implementing cloud solutions on Azure.\nProficiency in programming languages and scripting tools.\nUnderstanding of cloud data storage and database technologies such as SQL and NoSQL.\nProven ability to collaborate with multidisciplinary teams of business analysts, developers, data scientists, and subject-matter experts.\nFamiliarity with DevOps practices and tools, such as continuous integration and continuous deployment (CI/CD) and Teraform.\nProven proactive approach to spotting problems, areas for improvement, and performance bottlenecks.\nProven excellent communication, writing, and presentation skills.\nExperience in interacting with international customers to gather requirements and convert them into solutions using relevant skills.\nPreferred Qualifications:\nKnowledge of AI/ML or LLM (GenAI).\nKnowledge of US Healthcare domain and experience with healthcare data.\nExperience and skills with Snowflake.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Data Factory', 'Azure Databricks', 'ETL', 'SQL', 'Python', 'Airflow', 'Pyspark', 'Snowflake', 'SCALA', 'Spark', 'Data Bricks']",2025-06-12 13:44:06
Senior GCP Data Engineer,Swits Digital,6 - 9 years,Not Disclosed,['Bengaluru'],"Job Title: Senior GCP Data Engineer\nLocation: Chennai, Bangalore, Hyderabad\nExperience: 6-9 Years\nJob Summary:\nWe are seeking a GCP Data & Cloud Engineer with strong expertise in Google Cloud Platform services, including BigQuery, Cloud Run, Cloud Storage , and Pub/Sub . The ideal candidate will have deep experience in SQL coding , data pipeline development, and deploying cloud-native solutions.\nKey Responsibilities:\nDesign, implement, and optimize scalable data pipelines and services using GCP\nBuild and manage cloud-native applications deployed via Cloud Run\nDevelop complex and performance-optimized SQL queries for analytics and data transformation\nManage and automate data storage, retrieval, and archival using Cloud Storage\nImplement event-driven architectures using Google Pub/Sub\nWork with large datasets in BigQuery , including ETL/ELT design and query optimization\nEnsure security, monitoring, and compliance of cloud-based systems\nCollaborate with data analysts, engineers, and product teams to deliver end-to-end cloud solutions\nRequired Skills & Experience:\n3+ years of experience working with Google Cloud Platform (GCP)\nStrong proficiency in SQL coding , query tuning, and handling complex data transformations\nHands-on experience with:\nBigQuery\nCloud Run\nCloud Storage\nPub/Sub\nUnderstanding of data pipeline and ETL/ELT workflows in cloud environments\nFamiliarity with containerized services and CI/CD pipelines\nExperience in scripting languages (e.g., Python, Shell) is a plus\nStrong analytical and problem-solving skills",Industry Type: Recruitment / Staffing,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['SUB', 'query optimization', 'GCP', 'Analytical', 'Cloud', 'query', 'cloud storage', 'Security monitoring', 'SQL coding', 'Python']",2025-06-12 13:44:09
Lead Data Engineer,Conduent,8 - 13 years,Not Disclosed,['Noida'],"Job Overview \n\nWe are looking for a Data Engineer who will be part of our Analytics Practice and will be expected to actively work in a multi-disciplinary fast paced environment. This role requires a broad range of skills and the ability to step into different roles depending on the size and scope of the project; its primary responsibility is the acquisition, transformation, loading and processing of data from a multitude of disparate data sources, including structured and unstructured data for advanced analytics and machine learning in a big data environment.\n\n\n Responsibilities: \nEngineer a modern data pipeline to collect, organize, and process data from disparate sources.\nPerforms data management tasks, such as conduct data profiling, assess data quality, and write SQL queries to extract and integrate data\nDevelop efficient data collection systems and sound strategies for getting quality data from different sources\nConsume and analyze data from the data pool to support inference, prediction and recommendation of actionable insights to support business growth.\nDesign and develop ETL processes using tools and scripting. Troubleshoot and debug ETL processes. Performance tuning and opitimization of the ETL processes.\nProvide support to new of existing applications while recommending best practices and leading projects to implement new functionality.\nCollaborate in design reviews and code reviews to ensure standards are met. Recommend new standards for visualizations.\nLearn and develop new ETL techniques as required to keep up with the contemporary technologies.\nReviews the solution requirements and architecture to ensure selection of appropriate technology, efficient use of resources and integration of multiple systems and technology.\nSupport presentations to Customers and Partners\nAdvising on new technology trends and possible adoption to maintain competitive advantage\n\n\n Experience Needed: \n8+ years of related experience is required.\nA BS or Masters degree in Computer Science or related technical discipline is required\nETL experience with data integration to support data marts, extracts and reporting\nExperience connecting to varied data sources\nExcellent SQL coding experience with performance optimization for data queries.\nUnderstands different data models like normalized, de-normalied, stars, and snowflake models. Worked with transactional, temporarl, time series, and structured and unstructured data.\nExperience on Azure Data Factory and Azure Synapse Analytics\nWorked in big data environments, cloud data stores, different RDBMS and OLAP solutions.\nExperience in cloud-based ETL development processes.\nExperience in deployment and maintenance of ETL Jobs.\nIs familiar with the principles and practices involved in development and maintenance of software solutions and architectures and in service delivery.\nHas strong technical background and remains evergreen with technology and industry developments.\nAt least 3 years of demonstrated success in software engineering, release engineering, and/or configuration management.\nHighly skilled in scripting languages like PowerShell.\nSubstantial experience in the implementation and exectuion fo CI/CD processes.\n\n\n Additional  \nDemonstrated ability to have successfully completed multiple, complex technical projects\nPrior experience with application delivery using an Onshore/Offshore model\nExperience with business processes across multiple Master data domains in a services based company\nDemonstrates a rational and organized approach to the tasks undertaken and an awareness of the need to achieve quality.\nDemonstrates high standards of professional behavior in dealings with clients, colleagues and staff.\nIs able to make sound and far reaching decisions alone on major issues and to take full responsibility for them on a technical basis.\nStrong written communication skills. Is effective and persuasive in both written and oral communication.\nExperience with gathering end user requirements and writing technical documentation\nTime management and multitasking skills to effectively meet deadlines under time-to-market pressure",Industry Type: BPM / BPO,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sql coding', 'sql', 'configuration management', 'software engineering', 'release engineering', 'continuous integration', 'rdbms', 'sql queries', 'performance tuning', 'azure synapse', 'ci/cd', 'azure data factory', 'machine learning', 'data engineering', 'powershell', 'olap', 'etl', 'big data']",2025-06-12 13:44:11
Senior Data Engineer,The Main Stage Productions,4 - 6 years,Not Disclosed,['Bengaluru'],"Design and implement cloud-native data architectures on AWS, including data lakes, data warehouses, and streaming pipelines using services like S3, Glue, Redshift, Athena, EMR, Lake Formation, and Kinesis.\nDevelop and orchestrate ETL/ELT pipelines\n\nRequired Candidate profile\nParticipate in pre-sales and consulting activities such as:\nEngaging with clients to gather requirements and propose AWS-based data engineering solutions.\nSupporting RFPs/RFIs, technical proposals",Industry Type: Advertising & Marketing,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['AWS Glue', 'GitHub Actions', 'PySpark', 'Scala', 'CodePipeline', 'Step Functions', 'data engineering']",2025-06-12 13:44:13
Collibra Data Governance Engineer,Allegis Group,6 - 11 years,Not Disclosed,[],"Collibra Data Governance Engineer\nJob Location : Hyderabad / Bangalore / Chennai / Noida/ Gurgaon / Pune / Indore / Mumbai/ Kolkata\nRequired Skills\n5+ years of experience in data governance and/or metadata management.\nHands-on experience with Collibra Data Governance Center (Collibra DGC), including workflow configuration, cataloging, and operating model customization.\nStrong knowledge of metadata management, data lineage, and data quality principles.\nHands-on experience with Snowflake\nFamiliarity with data integration tools and AWS cloud platform\nExperience with SQL and working knowledge of relational databases.\nUnderstanding of data privacy regulations (e.g., GDPR, CCPA) and compliance frameworks.\nPreferred Skills\nCertifications such as Collibra Certified Solution Architect.\nExperience integrating Collibra with tools like Snowflake, Tableau or other BI/analytics platforms.\nExposure to DataOps, MDM (Master Data Management), and data governance frameworks like DAMA-DMBOK.\nStrong communication and stakeholder management skills.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Collibra', 'Metadata', 'Data Governance']",2025-06-12 13:44:15
Senior Software Engineer - Data Engineering,Zendesk,2 - 6 years,Not Disclosed,['Bengaluru'],"Design, build, and maintain data quality systems and pipelines.\nWork with tools such as Snowflake, Docker/Kubernetes, and Kafka to enable scalable, observable data movement.\nCollaborate cross-functionally to close skill gaps in DQ and data platform tooling.\nContribute to building internal tooling that supports schema validation, data experimentation, and automated checks.\nCollaborate cross-functionally with data producers, analytics engineers, platform teams, and business stakeholders.\nOwn the reliability, scalability, and performance of ingestion systems deployed on AWS\nArchitect and build core components of our real-time ingestion platform using Kafka, Snowpipe Streaming.\nChampion software engineering excellence including testing, observability, CI/CD, and automation\nDrive the development of platform tools that ensure data quality, observability, and lineage through Protobuf-based schema management..\nParticipate in the implementation of ingestion best practices and reusable frameworks across data and software engineering teams.\nCore Skills:\nSolid programming experience (preferably in Java )\nExperience with distributed data systems ( Kafka, Snowflake )\nFamiliarity with Data Quality tooling and concepts\nGood working knowledge of SQL (especially for diagnostics and DQ workflows)\nExperience with containerization (Docker, Kubernetes )\nStrong debugging, observability, and pipeline reliability practices\n\nWhat You Bring:\nA systems mindset with strong software engineering fundamentals.\nPassion for building resilient, high-throughput, real-time platforms.\nAbility to influence technical direction across teams and drive alignment.\nStrong communication and mentoring skills.\nA bias toward automation, continuous improvement, and platform thinking.\n\nNice to Haves:\nExperience with GenAI tools or supporting ML/AI data workflows\nFamiliarity with cloud-native data platforms (e.g., AWS, GCP)\nExposure to dbt or ELT frameworks",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Automation', 'GCP', 'Quality systems', 'Debugging', 'Data quality', 'Customer service', 'Continuous improvement', 'Monitoring', 'Analytics', 'SQL']",2025-06-12 13:44:17
Senior Data Engineer,Talentien Global Solutions,4 - 8 years,12-18 Lacs P.A.,"['Hyderabad', 'Chennai', 'Coimbatore']","We are seeking a skilled and motivated Data Engineer to join our dynamic team. The ideal candidate will have experience in designing, developing, and maintaining scalable data pipelines and architectures using Hadoop, PySpark, ETL processes, and Cloud technologies.\n\nResponsibilities:\nDesign, develop, and maintain data pipelines for processing large-scale datasets.\nBuild efficient ETL workflows to transform and integrate data from multiple sources.\nDevelop and optimize Hadoop and PySpark applications for data processing.\nEnsure data quality, governance, and security standards are met across systems.\nImplement and manage Cloud-based data solutions (AWS, Azure, or GCP).\nCollaborate with data scientists and analysts to support business intelligence initiatives.\nTroubleshoot performance issues and optimize query executions in big data environments.\nStay updated with industry trends and advancements in big data and cloud technologies.\nRequired Skills:\nStrong programming skills in Python, Scala, or Java.\nHands-on experience with Hadoop ecosystem (HDFS, Hive, Spark, etc.).\nExpertise in PySpark for distributed data processing.\nProficiency in ETL tools and workflows (SSIS, Apache Nifi, or custom pipelines).\nExperience with Cloud platforms (AWS, Azure, GCP) and their data-related services.\nKnowledge of SQL and NoSQL databases.\nFamiliarity with data warehousing concepts and data modeling techniques.\nStrong analytical and problem-solving skills.\n\nInterested can reach us at +91 7305206696/ saranyadevib@talentien.com",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'Data Engineering', 'Hadoop', 'Spark', 'ETL', 'Airflow', 'Etl Pipelines', 'Big Data', 'EMR', 'Gcp Cloud', 'Data Bricks', 'Azure Cloud', 'Data Pipeline', 'SCALA', 'Snowflake', 'Data Lake', 'Data Warehousing', 'Data Modeling', 'AWS', 'Python']",2025-06-12 13:44:20
Lead Data Engineer - Azure,Blend360 India,7 - 12 years,Not Disclosed,['Hyderabad'],"Job Description\nAs a Sr Data Engineer, your role is to spearhead the data engineering teams and elevate the team to the next level! You will be responsible for laying out the architecture of the new project as well as selecting the tech stack associated with it. You will plan out the development cycles deploying AGILE if possible and create the foundations for good data stewardship with our new data products!\nYou will also set up a solid code framework that needs to be built to purpose yet have enough flexibility to adapt to new business use cases tough but rewarding challenge!\n\nResponsibilities\nCollaborate with several stakeholders to deeply understand the needs of data practitioners to deliver at scale\nLead Data Engineers to define, build and maintain Data Platform\nWork on building Data Lake in Azure Fabric processing data from multiple sources\nMigrating existing data store from Azure Synapse to Azure Fabric\nImplement data governance and access control\nDrive development effort End-to-End for on-time delivery of high-quality solutions that conform to requirements, conform to the architectural vision, and comply with all applicable standards.\nPresent technical solutions, capabilities, considerations, and features in business terms. Effectively communicate status, issues, and risks in a precise and timely manner.\nFurther develop critical initiatives, such as Data Discovery, Data Lineage and Data Quality\nLeading team and Mentor junior resources\nHelp your team members grow in their role and achieve their career aspirations\nBuild data systems, pipelines, analytical tools and programs\nConduct complex data analysis and report on results\n\n\nQualifications\n7+ Years of Experience as a data engineer or similar role in Azure Synapses, ADF or\nrelevant exp in Azure Fabric\nDegree in Computer Science, Data Science, Mathematics, IT, or similar fiel",Industry Type: Advertising & Marketing,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Data analysis', 'Data modeling', 'Analytical', 'Agile', 'data governance', 'Data quality', 'Data mining', 'SQL', 'Python']",2025-06-12 13:44:22
"Lead Engineer, Data Engineering (J2EE/Angular/React/React Full Stack)",S&P Global Market Intelligence,10 - 15 years,Not Disclosed,"['Mumbai', 'Maharastra']","About the Role:\nGrade Level (for internal use): 11\nThe Team\nYou will be an expert contributor and part of the Rating Organizations Data Services Product Engineering Team. This team, who has a broad and expert knowledge on Ratings organizations critical data domains, technology stacks and architectural patterns, fosters knowledge sharing and collaboration that results in a unified strategy. All Data Services team members provide leadership, innovation, timely delivery, and the ability to articulate business value. Be a part of a unique opportunity to build and evolve S&P Ratings next gen analytics platform.\nResponsibilities:\nArchitect, design, and implement innovative software solutions to enhance S&P Ratings' cloud-based analytics platform.\nMentor a team of engineers (as required), fostering a culture of trust, continuous growth, and collaborative problem-solving.\nCollaborate with business partners to understand requirements, ensuring technical solutions align with business goals.\nManage and improve existing software solutions, ensuring high performance and scalability.\nParticipate actively in all Agile scrum ceremonies, contributing to the continuous improvement of team processes.\nProduce comprehensive technical design documents and conduct technical walkthroughs.\nExperience & Qualifications:\nBachelors degree in computer science, Information Systems, Engineering, equivalent or more is required\nProficient with software development lifecycle (SDLC) methodologies like Agile, Test-driven development\n10+ years of experience with 4+ years designing/developing enterprise products, modern tech stacks and data platforms\n4+ years of hands-on experience contributing to application architecture & designs, proven software/enterprise integration design patterns and full-stack knowledge including modern distributed front end and back-end technology stacks\n5+ years full stack development experience in modern web development technologies, Java/J2EE, UI frameworks like Angular, React, SQL, Oracle, NoSQL Databases like MongoDB\nExperience designing transactional/data warehouse/data lake and data integrations with Big data eco system leveraging AWS cloud technologies\nThorough understanding of distributed computing\nPassionate, smart, and articulate developer\nQuality first mindset with a strong background and experience with developing products for a global audience at scale\nExcellent analytical thinking, interpersonal, oral and written communication skills with strong ability to influence both IT and business partners\nSuperior knowledge of system architecture, object-oriented design, and design patterns.\nGood work ethic, self-starter, and results-oriented\nExcellent communication skills are essential, with strong verbal and writing proficiencies\nExp. with Delta Lake systems like Databricks using AWS cloud technologies and PySpark is a plus\nAdditional Preferred Qualifications:\nExperience working AWS\nExperience with SAFe Agile Framework\nBachelor's/PG degree in Computer Science, Information Systems or equivalent.\nHands-on experience contributing to application architecture & designs, proven software/enterprise integration design principles\nAbility to prioritize and manage work to critical project timelines in a fast-paced environment\nExcellent Analytical and communication skills are essential, with strong verbal and writing proficiencies\nAbility to train and mentor",Industry Type: Banking,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'AWS cloud technologies', 'PySpark', 'J2EE', 'React Full Stack', 'Databricks', 'React', 'Angular']",2025-06-12 13:44:24
"Senior Data Engineer - Airflow, PLSQL",Relanto Global,5 - 10 years,Not Disclosed,['Bengaluru'],"PositionSenior Data Engineer - Airflow, PLSQL \n\n Experience5+ Years \n\n LocationBangalore/Hyderabad/Pune \n\n\n\nSeeking a Senior Data Engineer with strong expertise in Apache Airflow and Oracle PL/SQL, along with working experience in Snowflake and Agile methodologies. The ideal candidate will also take up Scrum Master responsibilities and lead a data engineering scrum team to deliver robust, scalable data solutions.\n\n\n Key Responsibilities: \nDesign, develop, and maintain scalable data pipelines using Apache Airflow.\nWrite and optimize complex PL/SQL queries, procedures, and packages on Oracle databases.\nCollaborate with cross-functional teams to design efficient data models and integration workflows.\nWork with Snowflake for data warehousing and analytics use cases.\nOwn the delivery of sprint goals, backlog grooming, and facilitation of agile ceremonies as the Scrum Master.\nMonitor pipeline health and troubleshoot production data issues proactively.\nEnsure code quality, documentation, and best practices across the team.\nMentor junior data engineers and promote a culture of continuous improvement.\n\n\n Required Skills and Qualifications: \n5+ years of experience as a Data Engineer in enterprise environments.\nStrong expertise in  Apache Airflow  for orchestrating workflows.\nExpert in  Oracle PL/SQL  - stored procedures, performance tuning, debugging.\nHands-on experience with  Snowflake  - data modeling, SQL, optimization.\nWorking knowledge of version control (Git) and CI/CD practices.\nPrior experience or certification as a  Scrum Master  is highly desirable.\nStrong analytical and problem-solving skills with attention to detail.\nExcellent communication and leadership skills.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sql', 'plsql', 'stored procedures', 'oracle pl', 'performance tuning', 'hive', 'continuous integration', 'ci/cd', 'data warehousing', 'pyspark', 'git', 'apache', 'data modeling', 'spark', 'debugging', 'hadoop', 'big data', 'snowflake', 'python', 'oracle', 'sql queries', 'airflow', 'data engineering', 'agile', 'sqoop']",2025-06-12 13:44:27
"Senior Data Engineer Databricks, ADF, PySpark",Suzva Software Technologies,6 - 11 years,Not Disclosed,"['Mumbai', 'Delhi / NCR', 'Bengaluru']","Senior Data Engineer (Remote, Contract 6 Months) Databricks, ADF, and PySpark.\nWe are hiring a Senior Data Engineer for a 6-month remote contract position. The ideal candidate is highly skilled in building scalable data pipelines and working within the Azure cloud ecosystem, especially Databricks, ADF, and PySpark. You'll work closely with cross-functional teams to deliver enterprise-level data engineering solutions.\n\nKeyResponsibilities\nBuild scalable ETL pipelines and implement robust data solutions in Azure.\n\nManage and orchestrate workflows using ADF, Databricks, ADLS Gen2, and Key Vaults.\n\nDesign and maintain secure and efficient data lake architecture.\n\nWork with stakeholders to gather data requirements and translate them into technical specs.\n\nImplement CI/CD pipelines for seamless data deployment using Azure DevOps.\n\nMonitor data quality, performance bottlenecks, and scalability issues.\n\nWrite clean, organized, reusable PySpark code in an Agile environment.\n\nDocument pipelines, architectures, and best practices for reuse.\n\nMustHaveSkills\nExperience: 6+ years in Data Engineering\n\nTech Stack: SQL, Python, PySpark, Spark, Azure Databricks, ADF, ADLS Gen2, Azure DevOps, Key Vaults\n\nCore Expertise: Data Warehousing, ETL, Data Pipelines, Data Modelling, Data Governance\n\nAgile, SDLC, Containerization (Docker), Clean coding practices\n\nGoodToHaveSkills\nEvent Hubs, Logic Apps\n\nPower BI\n\nStrong logic building and competitive programming background\n\nLocation : - Remote,Mumbai, Delhi / NCR, Bengaluru , Kolkata, Chennai, Hyderabad, Ahmedabad, Pune",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Databricks', 'ADF', 'PySpark', 'ADLS Gen2', 'Azure Databricks', 'Key Vaults', 'Spark', 'Azure DevOps', 'SQL', 'Python']",2025-06-12 13:44:29
Data Engineer IV - Big Data / Spark,Sadup Soft,5 - 7 years,Not Disclosed,['Chennai'],"Must have skills :\n\n- Minimum of 5-7 years of experience in software development, with a focus on Java and infrastructure tools.\n\n- Min 6+ years of experience as a Data Engineer.\n\n- Good Experience in handling Big Data Spark, Hive SQL, BigQuery, SQL.\n\n- Candidate worked on cloud platforms and GCP would be an added advantage.\n\n- Good understanding of Hadoop based ecosystem including hard sequel, HDFS would be very essential.\n\n- Very good professional knowledge of PySpark or using Scala\n\nResponsibilities :\n\n- Collaborate with cross-functional teams such as Data Scientists, Product Partners and Partner Team Developers to identify opportunities for Big Data, Query ( Spark, Hive SQL, BigQuery, SQL ) tuning opportunities that can be solved using machine learning and generative AI.\n\n- Write clean, high-performance, high-quality, maintainable code.\n\n- Design and develop Big Data Engineering Solutions Applications for above ensuring scalability, efficiency, and maintainability of such solutions.\n\nRequirements :\n\n- A Bachelor or Master's degree in Computer Science or a related field.\n\n- Proven experience working as a Big Data & MLOps Engineer, with a focus on Spark, Scala Spark or PySpark, Spark SQL, BigQuery, Python, Google Cloud,.\n\n- Deep understanding and experience in tuning Dataproc, BigQuery, Spark Applications.\n\n- Solid knowledge of software engineering best practices, including version control systems (e.g Git), code reviews, and testing methodologies.\n\n- Strong communication skills to effectively collaborate and present findings to both technical and non-technical stakeholders.\n\n- Proven ability to adapt and learn new technologies and frameworks quickly.\n\n- A proactive mindset with a passion for continuous learning and research.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Big Data', 'Data Engineering', 'BigQuery', 'GCP', 'Spark', 'Machine Learning', 'Python', 'SQL']",2025-06-12 13:44:31
"Senior Data Engineer II, Business Intelligence & Reporting",XL India Business Services Pvt. Ltd,3 - 7 years,Not Disclosed,['Gurugram'],"Senior Data Engineer II, Business Intelligence & Reporting Gurgaon, Haryana, India AXA XL recognizes digital, data, and information assets are critical for the business, both in terms of managing risk and enabling new business opportunities\n\nThis data should not only be high quality, but also actionable - enabling AXA XL s executive leadership team to maximize benefits and facilitate sustained dynamic advantage\n\nOur Data, Intelligence & Analytics function is focused on driving innovation by optimizing how we leverage digital, data, and AI to drive strategy and differentiate ourselves from the competition\n\nAs we develop an enterprise-wide data and digital strategy that moves us toward a greater focus on the use of data and strengthening our digital, AI capabilities, we are seeking a Deputy Manager, BI and Reporting\n\nIn this role, you will support/manage BI & reporting\n\nWhat you ll be DOING Your essential responsibilities include: BI & Reporting Management: Oversee and support Business Intelligence (BI) and Reporting products, ensuring their effectiveness and alignment with organizational goals\n\nStakeholder Engagement: Manage Business as Usual (BAU) activities for BI and Reporting, fostering effective communication and relationships with stakeholders to understand their needs and expectations\n\nModel Integration: Energize and synergize various Business Intelligence models and reporting systems to enhance data insights and reporting capabilities\n\nStrategic Initiative Support: Collaborate with the Data Intelligence and Analytics (DIA) team on various strategic initiatives, enabling the development of BI and Reporting functions and related capabilities\n\nTalent Development: Foster the growth of BI and Reporting talent across AXA XL by promoting an inclusive and diverse environment that encourages the utilization and value creation of our strategic digital, data, and analytics assets\n\nCustomer-Centric Culture: Instill a customer-first mindset within the team, prioritizing exceptional service for business stakeholders and ensuring their needs are met\n\nTeam Development: Contribute to the enhancement of the Business Intelligence teams tools, skills, and culture, driving positive impacts on team performance and outcomes\n\nYou will report to Senior Manager, Business Intelligence & Reporting\n\nWhat you will BRING At AXA XL, we view individuals holistically through their People, Business, and Technical Skills\n\nWe re interested in what you bring, how you think, and your potential for growth\n\nWe value diverse backgrounds and perspectives, recognizing that each person contributes uniquely to our teams success\n\nWe value relevant education and experience in a related field\n\nAdditionally, we encourage candidates with diverse educational backgrounds or equivalent experience to apply\n\nHere are some of the key skills important for the role: People Skills Customer Centricity: Brings a collaborative spirit, a can-do attitude, and a Customer First mindset, ensuring that stakeholder needs are prioritized\n\nCross-Functional Collaboration: Ability to communicate effectively with teams, peers, and stakeholders across the globe, fostering collaboration and understanding\n\nAble to help and guide team members on technical issues, fostering their development and promoting self-directed problem-solving\n\nGrowth Mindset: Passion for digital, data, and AI, along with a commitment to personal and team development in a digital and data-driven organization\n\nResilience: Ability to lead a project or team, demonstrating adaptability and leadership under various circumstances\n\nAnalytical & Strategic Mindset: Ability to analyze data effectively and develop strategic insights that drive decision-making and improve business outcomes\n\nPerformance Excellence: Commitment to delivering high-quality results and continuously improving processes and performance metrics within the team\n\nBUSINESS Skills Business & Insurance Acumen: Ability to showcase relevant industry knowledge supporting multiple specialty areas of Data and Analytics\n\nStakeholder Management: Ability to manage stakeholders effectively, understanding their needs and ensuring clear communication and support\n\nSimplifies Complexity: Ability to distill complex data concepts and analyses into clear, actionable insights for stakeholders\n\nEnsuring that technical information is accessible, enabling informed decision-making and fostering collaboration between technical and non-technical teams\n\nTECHNICAL Skills Data Visualization: Experience with end-user BI tools like Power BI, enabling effective presentation and visualization of data insights\n\nReporting Tools: Proficincy in SQL, Advanced Excel, MS Access, and VBA, allowing for effective data manipulation and reporting\n\nData Analytics: Ability to help and guide team members on technical issues, fostering skill development within the team to self-directedly manage data analytics tasks",Industry Type: Insurance,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Stakeholder Engagement', 'MS Access', 'Analytical', 'Agile', 'digital strategy', 'Business strategy', 'data visualization', 'Stakeholder management', 'Reporting tools', 'SQL']",2025-06-12 13:44:33
IT Manager - Data Engineering & Analytics,ZS,12 - 15 years,Not Disclosed,['Pune'],"IT MANAGER, DATA ENGINEERING AND ANALYTICS will lead a team of data engineers and analysts responsible for designing, developing, and maintaining robust data systems and integrations. This role is critical for ensuring the smooth collection, transformation, integration and visualization of data, making it easily accessible for analytics and decision-making across the organization. The Manager will collaborate closely with analysts, developers, business leaders and other stakeholders to ensure that the data infrastructure meets business needs and is scalable, reliable, and efficient.\n",,,,"['Data modeling', 'Project management', 'Analytical', 'Financial planning', 'Management consulting', 'Data quality', 'Troubleshooting', 'Stakeholder management', 'Analytics', 'SQL']",2025-06-12 13:44:36
Data Lineage Engineers,Altimetrik,5 - 10 years,15-30 Lacs P.A.,"['Pune', 'Chennai', 'Bengaluru']","Role & responsibilities\nSkill need :  Data Lineage with Ab-initio - Metadata hub\n\nSkills & Experience:\nExpertise in mHub or similar tools, data pipelines, and cloud platforms.\nProficiency in Python, Oracle, SQL, Java, and ETL tools.\n5-10 years of experience in data engineering and governance.",,,,"['Ab-initio', 'Metadata hub', 'Python', 'mhub', 'ETL', 'Oracle', 'SQL']",2025-06-12 13:44:38
Senior Data Engineer (Identity),Kargo,6 - 11 years,Not Disclosed,[],"Success takes all kinds. Diversity describes our workforce. Inclusion defines our culture. We do not discriminate on the basis of race, color, religion, sex, sexual orientation, gender identity, marital status, age, national origin, protected veteran status, disability or other legally protected status. Individuals with disabilities are provided reasonable accommodation to participate in the job application process, perform essential job functions, and receive other benefits and privileges of employment.\nTitle: Senior Data Engineer\nJob Type: Permanent\n\nJob Location: Remote\nThe Opportunity\nAt Kargo, we are rapidly evolving our data infrastructure and capabilities to address challenges of data scale, new methodologies for onboarding and targeting, and rigorous privacy standards. Were looking for an experienced Senior Data Engineer to join our team, focusing on hands-on implementation, creative problem-solving, and exploring new technical approaches. Youll work collaboratively with our technical leads and peers, actively enhancing and scaling the data processes that drive powerful targeting systems.\nThe Daily To-Do\nIndependently implement, optimize, and maintain robust ETL/ELT pipelines using Python, Airflow, Spark, Iceberg, Snowflake, Aerospike, Docker, Kubernetes (EKS), AWS, and real-time streaming technologies like Kafka and Flink.\nEngage proactively in collaborative design and brainstorming sessions, contributing technical insights and innovative ideas for solving complex data engineering challenges.\nSupport the definition and implementation of robust testing strategies, and guide the team in adopting disciplined CI/CD practices using ArgoCD to enable efficient and reliable deployments.\nMonitor and optimize data systems and infrastructure to ensure operational reliability, performance efficiency, and cost-effectiveness.\nActively contribute to onboarding new datasets, enhancing targeting capabilities, and exploring modern privacy-compliant methodologies.\nMaintain thorough documentation of technical implementations, operational procedures, and best practices for effective knowledge sharing and onboarding.\nQualifications:\nStrong expertise in implementing, maintaining, and optimizing large-scale data systems with minimal oversight.\nDeep proficiency in Python, Spark, and Iceberg, with a clear understanding of data structuring for efficiency and performance.\nExperience with Airflow for building robust data workflows is strongly preferred.\nExtensive DevOps experience, particularly with AWS (including EKS), Docker, Kubernetes, CI/CD automation using ArgoCD, and monitoring via Prometheus.\nFamiliarity with Snowflake, including writing and optimizing SQL queries and understanding Snowflakes performance and cost dynamics.\nComfort with Agile methodologies, including regular use of Jira and Confluence for task management and documentation.\nProven ability to independently drive implementation and problem-solving, turning ambiguity into clearly defined actions.\nExcellent communication skills to effectively engage in discussions with technical teams and stakeholders.\nFamiliarity with identity, privacy, and targeting methodologies in AdTech is required.\nFollow Our Lead\nBig Picture: kargo.com\nThe Latest: Instagram ( @kargomobile ) and LinkedIn ( Kargo )",Industry Type: Advertising & Marketing,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SQL queries', 'Automation', 'CTV', 'spark', 'Agile', 'JIRA', 'Operations', 'Cost', 'AWS', 'Python']",2025-06-12 13:44:40
Sr Data Engineering Manager,Amgen Inc,12 - 15 years,Not Disclosed,['Hyderabad'],"ABOUT THE ROLE\nRole Description:\nWe are seeking a Senior Data Engineering Manager with a strong background in Regulatory or Integrated Product Teams within the Biotech or Pharmaceutical domain. This role will lead the end-to-end data strategy and execution for regulatory product submissions, lifecycle management, and compliance reporting, ensuring timely and accurate delivery of regulatory data assets across global markets.You will be embedded in a cross-functional Regulatory Integrated Product Team (IPT) and serve as the data and technology lead, driving integration between scientific, regulatory, and engineering functions to support submission-ready data and regulatory intelligence solutions.\nRoles & Responsibilities:\nFunctional Skills:\nLead the engineering strategy and implementation for end-to-end regulatory operations, including data ingestion, transformation, integration, and delivery across regulatory systems.\nServe as the data engineering SME in the Integrated Product Team (IPT) to support regulatory submissions, agency interactions, and lifecycle updates.\nCollaborate with global regulatory affairs, clinical, CMC, quality, safety, and IT teams to gather submission data requirements and translate them into data engineering solutions.\nManage and oversee the development of data pipelines, data models, and metadata frameworks that support submission data standards (e.g., eCTD, IDMP, SPL, xEVMPD).\nEnable integration and reporting across regulatory information management systems (RIMS), EDMS, clinical trial systems, and lab data platforms.\nImplement data governance, lineage, validation, and audit trails for regulatory data workflows, ensuring GxP and regulatory compliance.\nGuide the development of automation solutions, dashboards, and analytics that improve visibility into submission timelines, data quality, and regulatory KPIs.\nEnsure interoperability between regulatory data platforms and enterprise data lakes or lakehouses for cross-functional reporting and insights.\nCollaborate with IT, data governance, and enterprise architecture teams to ensure alignment with overall data strategy and compliance frameworks.\nDrive innovation by evaluating emerging technologies in data engineering, graph data, knowledge management, and AI for regulatory intelligence.\nLead, mentor, and coach a small team of data engineers and analysts, fostering a culture of excellence, innovation, and delivery.\nDrive Agile and Scaled Agile (SAFe) methodologies, managing sprint backlogs, prioritization, and iterative improvements to enhance team velocity and project delivery.\nStay up-to-date with emerging data technologies, industry trends, and best practices, ensuring the organization leverages the latest innovations in data engineering and architecture.\nMust-Have Skills:\n812 years of experience in data engineering or data architecture, with 3+ years in a senior or managerial capacity, preferably within the biotech or pharmaceutical industry.\nProven experience supporting regulatory functions, including submissions, tracking, and reporting for FDA, EMA, and other global authorities.\nExperience with ETL/ELT tools, data pipelines, and cloud-based data platforms (e.g., Databricks, AWS, Azure, or GCP).\nFamiliarity with regulatory standards and data models such as eCTD, IDMP, HL7, CDISC, and xEVMPD.\nDeep understanding of GxP data compliance, audit requirements, and regulatory submission processes.\nExperience with tools like Power BI, Tableau, or Qlik for regulatory dashboarding and visualization is a plus.\nStrong project management, stakeholder communication, and leadership skills, especially in matrixed, cross-functional environments.\nAbility to translate technical capabilities into regulatory and business outcomes.Prepare team members for stakeholder discussions by helping assess data costs, access requirements, dependencies, and availability for business scenarios.\nGood-to-Have Skills:\nPrior experience working on integrated product teams or regulatory transformation programs.\nKnowledge of Regulatory Information Management Systems (RIMS), Veeva Vault RIM, or Master Data Management (MDM) in regulated environments.\nFamiliarity with Agile/SAFe methodologies and DevOps/DataOps best practices.\nEducation and Professional Certifications\n12 to 15 years of experience in Computer Science, IT or related field\nScaled Agile SAFe certification preferred\nProject Management certifications preferred\nSoft Skills:\nExcellent analytical and troubleshooting skills\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation\nAbility to manage multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals\nStrong presentation and public speaking skills",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'engineering strategy', 'DevOps', 'Project Management', 'DataOps', 'Agile', 'data strategy']",2025-06-12 13:44:42
Senior Data Engineer (Data Architect),Adastra Corp,8 - 13 years,Not Disclosed,[],"Join our innovative team and architect the future of data solutions on Azure, Synapse, and Databricks!\nSenior Data Engineer (Data Architect)\nAdditional Details:\nNotice Period: 30 days (maximum)\nLocation: Remote\nAbout the Role\nDesign and implement scalable data pipelines, data warehouses, and data lakes that drive business growth. Collaborate with stakeholders to deliver data-driven insights and shape the data landscape.\nRequirements\n8+ years of experience in data engineering and data architecture\nStrong expertise in Azure services (Synapse Analytics, Databricks, Storage, Active Directory)\nProven experience in designing and implementing data pipelines, data warehouses, and data lakes\nStrong understanding of data governance, data quality, and data security\nExperience with infrastructure design and implementation, including DevOps practices and tools",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Azure', 'DESIGN', 'Architecture', 'Synapse Analytics']",2025-06-12 13:44:44
"Walk-In Senior Data Engineer - DataStage, Azure & Power BI",Net Connect,6 - 10 years,5-11 Lacs P.A.,['Hyderabad( Madhapur )'],"Greetings from NCG!\n\nWe have a opening for Snowflake Developer role in Hyderabad office!\nBelow JD for your reference\n\nJob Description:\n\nWe are hiring an experienced Senior Data Engineer with strong expertise in IBM DataStage, , and . The ideal candidate will be responsible for end-to-end data integration, transformation, and reporting solutions that drive business decisions.",,,,"['Azure Data Factory', 'Datastage', 'Etl Datastage']",2025-06-12 13:44:46
"Sr. Data Engineer, R&D Data Catalyst Team",Amgen Inc,7 - 9 years,Not Disclosed,['Hyderabad'],"The R&D Data Catalyst Team is responsible for buildingData Searching, Cohort Building, and Knowledge Management tools that provide the Amgen scientific community with visibility to Amgens wealth of human datasets, projects and study histories, and knowledge over various scientific findings. These solutions are pivotal tools in Amgens goal to accelerate the speed of discovery, and speed to market of advanced precision medications.\nThe Data Engineer will be responsible for the end-to-end development of an enterprise analytics and data mastering solution leveraging Databricks and Power BI. This role requiresexpertise in both data architecture and analytics, with the ability to create scalable, reliable, and high-performing enterprise solutions that research cohort-building and advanced research pipeline.The ideal candidate will have experience creating and surfacing large unifiedrepositories of human data, based on integrations from multiple repositories and solutions, and be exceptionally skilled with data analysis and profiling.\nYou will collaborate closely with stakeholders, product team members, and related IT teams, to design and implement data models, integrate data from various sources, and ensure best practices for data governance and security. The ideal candidate will have a strong background in data warehousing, ETL, Databricks, Power BI, and enterprise data mastering.\nRoles & Responsibilities:\nDesign and build scalable enterprise analytics solutions using Databricks, Power BI, and other modern data tools.\nLeverage data virtualization, ETL, and semantic layers to balance need for unification, performance, and data transformation with goal to reduce data proliferation\nBreak down features into work that aligns with the architectural direction runway\nParticipate hands-on in pilots and proofs-of-concept for new patterns\nCreate robust documentation from data analysis and profiling, and proposed designs and data logic\nDevelop advanced sql queries to profile, and unify data\nDevelop data processing code in sql, along with semantic views to prepare data for reporting\nDevelop PowerBI Models and reporting packages\nDesign robust data models, and processing layers, that support both analytical processing and operational reporting needs.\nDesign and develop solutions based on best practices for data governance, security, and compliance within Databricks and Power BI environments.\nEnsure the integration of data systems with other enterprise applications, creating seamless data flows across platforms.\nDevelop and maintain Power BI solutions, ensuring data models and reports are optimized for performance and scalability.\nCollaborate with stakeholders to define data requirements, functional specifications, and project goals.\nContinuously evaluate and adopt new technologies and methodologies to enhance the architecture and performance of data solutions.\nBasic Qualifications and Experience:\nMasters degree with 1 to 3years of experience in Data Engineering OR\nBachelors degree with 4 to 5 years of experience in Data Engineering\nDiploma and 7 to 9 years of experience in Data Engineering.\nFunctional Skills:\nMust-Have Skills:\nMinimum of 3 years of hands-on experience with BI solutions (Preferrable Power BI or Business Objects) including report development, dashboard creation, and optimization.\nMinimum of 3years of hands-on experience building Change-data-capture (CDC) ETL pipelines, data warehouse design and build, and enterprise-level data management.\nHands-on experience with Databricks, including data engineering, optimization, and analytics workloads.\nDeep understanding of Power BI, including model design, DAX, and Power Query.\nProven experience designing and implementing data mastering solutions and data governance frameworks.\nExpertise in cloud platforms (AWS), data lakes, and data warehouses.\nStrong knowledge of ETL processes, data pipelines, and integration technologies.\nStrong communication and collaboration skills to work with cross-functional teams and senior leadership.\nAbility to assess business needs and design solutions that align with organizational goals.\nExceptional hands-on capabilities with data profiling, data transformation, data mastering\nSuccess in mentoring and training team members\nGood-to-Have Skills:\nExperience in developing differentiated and deliverable solutions\nExperience with human data, ideally human healthcare data\nFamiliarity with laboratory testing, patient data from clinical care, HL7, FHIR, and/or clinical trial data management\nProfessional Certifications (please mention if the certification is preferred or mandatory for the role):\nITIL Foundation or other relevant certifications (preferred)\nSAFe Agile Practitioner (6.0)\nMicrosoft Certified: Data Analyst Associate (Power BI) or related certification.\nDatabricks Certified Professional or similar certification.\nSoft Skills:\nExcellent analytical and troubleshooting skills\nDeep intellectual curiosity\nHighest degree of initiative and self-motivation\nStrong verbal and written communication skills, including presentation to varied audiences of complex technical/business topics\nConfidence technical leader\nAbility to work effectively with global, virtual teams, specifically including leveraging of tools and artifacts to assure clear and efficient collaboration across time zones\nAbility to manage multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals\nStrong problem solving, analytical skills;\nAbility to learn quickly and retain and synthesize complex information from diverse sources",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Engineering', 'data analysis', 'ETL processes', 'DAX', 'Business Objects', 'data warehouse design', 'ETL', 'PowerBI Models', 'AWS', 'Power Query']",2025-06-12 13:44:49
Data Engineering - Senior Developer with Salesforce,Job Delights,5 - 10 years,25-27.5 Lacs P.A.,"['Hyderabad', 'Chennai', 'Bengaluru']","Data Engineering with SQL, Python, ETL & SalesForce Marketing Cloud (Must)",Industry Type: Software Product,Department: Other,"Employment Type: Full Time, Permanent","['Salesforce Marketing Cloud', 'SQL', 'Marketing Cloud', 'Salesforce', 'Python']",2025-06-12 13:44:51
Data Engineer - SAS Migration,Crisil,2 - 4 years,Not Disclosed,['Mumbai'],"The SAS to Databricks Migration Developer will be responsible for migrating existing SAS code, data processes, and workflows to the Databricks platform\n\nThis role requires expertise in both SAS and Databricks, with a focus on converting SAS logic into scalable PySpark and Python code\n\nThe developer will design, implement, and optimize data pipelines, ensuring seamless integration and functionality within the Databricks environment\n\nCollaboration with various teams is essential to understand data requirements and deliver solutions that meet business needs",Industry Type: Financial Services,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['hive', 'scala', 'pyspark', 'data warehousing', 'data migration', 'azure data factory', 'sql', 'sql azure', 'java', 'spark', 'mysql', 'hadoop', 'big data', 'etl', 'python', 'sas', 'microsoft azure', 'power bi', 'machine learning', 'sql server', 'data bricks', 'migration', 'sqoop', 'aws', 'ssis']",2025-06-12 13:44:53
"Data Engineering : Sr Software Engineer, Tech Lead & Sr Tech Lead",Reflion Tech,7 - 12 years,22.5-37.5 Lacs P.A.,"['Mumbai( Ghansoli )', 'Navi Mumbai', 'Mumbai (All Areas)']","Hiring: Data Engineering Senior Software Engineer / Tech Lead / Senior Tech Lead\n\n- Hybrid (3 Days from office) | Shift: 2 PM 11 PM IST\n- Experience: 5 to 12+ years (based on role & grade)\n\nOpen Grades/Roles:\nSenior Software Engineer: 58 Years\nTech Lead: 7–10 Years\nSenior Tech Lead: 10–12+ Years\n\nJob Description – Data Engineering Team\n\nCore Responsibilities (Common to All Levels):\n\nDesign, build and optimize ETL/ELT pipelines using tools like Pentaho, Talend, or similar\nWork on traditional databases (PostgreSQL, MSSQL, Oracle) and MPP/modern systems (Vertica, Redshift, BigQuery, MongoDB)\nCollaborate cross-functionally with BI, Finance, Sales, and Marketing teams to define data needs\nParticipate in data modeling (ER/DW/Star schema), data quality checks, and data integration\nImplement solutions involving messaging systems (Kafka), REST APIs, and scheduler tools (Airflow, Autosys, Control-M)\nEnsure code versioning and documentation standards are followed (Git/Bitbucket)\n\nAdditional Responsibilities by Grade\n\nSenior Software Engineer (5–8 Yrs):\nFocus on hands-on development of ETL pipelines, data models, and data inventory\nAssist in architecture discussions and POCs\nGood to have: Tableau/Cognos, Python/Perl scripting, GCP exposure\n\nTech Lead (7–10 Yrs):\nLead mid-sized data projects and small teams\nDecide on ETL strategy (Push Down/Push Up) and performance tuning\nStrong working knowledge of orchestration tools, resource management, and agile delivery\n\nSenior Tech Lead (10–12+ Yrs):\nDrive data architecture, infrastructure decisions, and internal framework enhancements\nOversee large-scale data ingestion, profiling, and reconciliation across systems\nMentoring junior leads and owning stakeholder delivery end-to-end\nAdvantageous: Experience with AdTech/Marketing data, Hadoop ecosystem (Hive, Spark, Sqoop)\n\n- Must-Have Skills (All Levels):\n\nETL Tools: Pentaho / Talend / SSIS / Informatica\nDatabases: PostgreSQL, Oracle, MSSQL, Vertica / Redshift / BigQuery\nOrchestration: Airflow / Autosys / Control-M / JAMS\nModeling: Dimensional Modeling, ER Diagrams\nScripting: Python or Perl (Preferred)\nAgile Environment, Git-based Version Control\nStrong Communication and Documentation",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'SQL', 'ETL', 'Orchestration', 'Postgresql', 'Peri', 'Informatica', 'ETL Tool', 'SSIS', 'Elt', 'Modeling', 'MongoDB', 'Data Architecture', 'Talend', 'Pentaho', 'Python']",2025-06-12 13:44:56
"Senior data engineer - Python, Pyspark, AWS - 5+ years Gurgaon",One of the largest insurance providers.,5 - 10 years,Not Disclosed,['Gurugram'],"Senior data engineer - Python, Pyspark, AWS - 5+ years Gurgaon\n\nSummary: An excellent opportunity for someone having a minimum of five years of experience with expertise in building data pipelines. A person must have experience in Python, Pyspark and AWS.\n\nLocation- Gurgaon (Hybrid)\n\nYour Future Employer- One of the largest insurance providers.\n\nResponsibilities-\nTo design, develop, and maintain large-scale data pipelines that can handle large datasets from multiple sources.\nReal-time data replication and batch processing of data using distributed computing platforms like Spark, Kafka, etc.\nTo optimize the performance of data processing jobs and ensure system scalability and reliability.\nTo collaborate with DevOps teams to manage infrastructure, including cloud environments like AWS.\nTo collaborate with data scientists, analysts, and business stakeholders to develop tools and platforms that enable advanced analytics and reporting.\n\nRequirements-\nHands-on experience with AWS services such as S3, DMS, Lambda, EMR, Glue, Redshift, RDS (Postgres) Athena, Kinesics, etc.\nExpertise in data modeling and knowledge of modern file and table formats.\nProficiency in programming languages such as Python, PySpark, and SQL/PLSQL for implementing data pipelines and ETL processes.\nExperience data architecting or deploying Cloud/Virtualization solutions (Like Data Lake, EDW, Mart ) in the enterprise.\nCloud/hybrid cloud (preferably AWS) solution for data strategy for Data lake, BI and Analytics.\nWhat is in for you-\nA stimulating working environment with equal employment opportunities.\nGrowing of skills while working with industry leaders and top brands.\nA meritocratic culture with great career progression.\n\nReach us- If you feel that you are the right fit for the role please share your updated CV at randhawa.harmeen@crescendogroup.in\n\nDisclaimer- Crescendo Global specializes in Senior to C-level niche recruitment. We are passionate about empowering job seekers and employers with an engaging memorable job search and leadership hiring experience. Crescendo Global does not discriminate on the basis of race, religion, color, origin, gender, sexual orientation, age, marital status, veteran status, or disability status.",Industry Type: Insurance,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Pyspark', 'Data Pipeline', 'AWS', 'Data Ingestion', 'Data Engineering', 'Data Processing']",2025-06-12 13:44:58
Linux Kernel Engineer Senior For Data Center SoC,Qualcomm,2 - 7 years,Not Disclosed,['Bengaluru'],"Job Area: Engineering Group, Engineering Group > Software Engineering\n\nGeneral Summary:\n\nAs a Senior Software Engineer, you will play a pivotal role in designing, developing, optimizing, and commercializing software solutions for Qualcomms next-generation data center platforms. You will collaborate closely with cross-functional teams to advance critical technologies such as virtualization, memory management, scheduling, and the Linux Kernel.\n\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 2+ years of Software Engineering or related work experience.\nOR\nMaster's degree in Engineering, Information Systems, Computer Science, or related field and 1+ year of Software Engineering or related work experience.\nOR\nPhD in Engineering, Information Systems, Computer Science, or related field.\n\n2+ years of academic or work experience with Programming Language such as C, C++, Java, Python, etc.\nCollaborate within the team and across teams to design, develop, and release our software, tooling, and practices to meet community standards and internal and external requirements.\nBring up platform solutions across the Qualcomm chipset portfolio.\nTriage software build, tooling, packaging, functional, or stability failures.\nGuide and support development teams inside and outside the Linux organization, focusing on Linux userspace software functionality, integration, and maintenance.\nWork with development and product teams as necessary for issue resolution.\n\n\nPreferred Qualifications:\nMaster's Degree in Engineering, Information Systems, Computer Science, or a related field.\nStrong background in Computer Science and software fundamentals.\nWorking knowledge of C, C++, and proficiency in scripting languages (Bash, Python, etc.).\nExperience using git/gerrit.\nStrong understanding of the Linux kernel, configuration techniques like ACPI and device tree, system services, and various components that make up a Linux distribution.\nExperience with Linux distributions such as Debian, Ubuntu, RedHat, Yocto, etc.\nFamiliarity with package managers and their workings is crucial.\nFamiliarity with CI/CD tools.\nProven ability and interest in debugging complex compute and data center systems.\nStrong ability to solve problems in a non-linear fashion.\nQuick learner; able to grasp concepts with only basic training and the initiative to ask questions and investigate new areas and concepts as needed.\nPrior experience with Qualcomm software platforms is a plus.\nMature interpersonal skills with an ability to collaboratively work within the team and with many varied teams to resolve problems spanning many disciplines.\nProven ability to work in a dynamic, multi-tasked environment.\nExcellent written and verbal communication skills are required.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['computer science', 'linux', 'software engineering', 'scripting languages', 'linux kernel', 'continuous integration', 'c++', 'redhat linux', 'ci/cd', 'gerrit', 'git', 'java', 'yocto', 'embedded systems', 'debian', 'html', 'mysql', 'python', 'c', 'ubuntu', 'javascript', 'data center', 'embedded c', 'bash', 'aws']",2025-06-12 13:45:00
Senior Azure Data Engineer,Cloud Angles Digital Transformation,8 - 12 years,Not Disclosed,['Hyderabad'],"Job Summary:\nWe are seeking a highly skilled Data Engineer with expertise in leveraging Data Lake architecture and the Azure cloud platform to develop, deploy, and optimise data-driven solutions. . You will play a pivotal role in transforming raw data into actionable insights, supporting strategic decision-making across the organisation.\nResponsibilities\nDesign and implement scalable data science solutions using Azure Data Lake, Azure Data Bricks, Azure Data Factory and related Azure services.\nDevelop, train, and deploy machine learning models to address business challenges.\nCollaborate with data engineering teams to optimise data pipelines and ensure seamless data integration within Azure cloud infrastructure.\nConduct exploratory data analysis (EDA) to identify trends, patterns, and insights.\nBuild predictive and prescriptive models to support decision-making processes.\nExpertise in developing end-to-end Machine learning lifecycle utilizing crisp-DM which includes of data collection, cleansing, visualization, preprocessing, model development, model validation and model retraining\nProficient in building and implementing RAG systems that enhance the accuracy and relevance of model outputs by integrating retrieval mechanisms with generative models.\nEnsure data security, compliance, and governance within the Azure cloud ecosystem.\nMonitor and optimise model performance and scalability in production environments.\nPrepare clear and concise documentation for developed models and workflows.\nSkills Required:\nGood experience in using Pyspark, Python, MLops (Optional), ML flow (Optional), Azure Data Lake Storage. Unity Catalog\nWorked and utilized data from various RDBMS like MYSQL, SQL Server, Postgres and NoSQL databases like MongoDB, Cassandra, Redis and graph DB like Neo4j, Grakn.\nProven experience as a Data Engineer with a strong focus on Azure cloud platform and Data Lake architecture.\nProficiency in Python, Pyspark,\nHands-on experience with Azure services such as Azure Data Lake, Azure Synapse Analytics, Azure Machine Learning, Azure Databricks, and Azure Functions.\nStrong knowledge of SQL and experience in querying large datasets from Data Lakes.\nFamiliarity with data engineering tools and frameworks for data ingestion and transformation in Azure.\nExperience with version control systems (e.g., Git) and CI/CD pipelines for machine learning projects.\nExcellent problem-solving skills and the ability to work collaboratively in a team environment.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Data Factory', 'Azure Data Engineering', 'Azure Databricks', 'Pyspark', 'Azure Data Lake', 'Python']",2025-06-12 13:45:03
Data Streaming Engineer,Data Streaming Engineer,5 - 10 years,Not Disclosed,"['Pune', 'Chennai', 'Bengaluru']","Hello Candidates,\n\nWe are Hiring !!\n\nJob Position - Data Streaming Engineer\nExperience - 5+ years\nLocation - Mumbai, Pune , Chennai , Bangalore\nWork mode - Hybrid ( 3 days WFO)\n\nJOB DESCRIPTION\n\nRequest for Data Streaming Engineer Data Streaming @ offshore :\n• Flink , Python Language.\n• Data Lake Systems. (OLAP Systems).\n• SQL (should be able to write complex SQL Queries)\n• Orchestration (Apache Airflow is preferred).\n• Hadoop (Spark and Hive: Optimization of Spark and Hive apps).\n• Snowflake (good to have).\n• Data Quality (good to have).\n• File Storage (S3 is good to have)\n\nNOTE - Candidates can share their resume on - shrutia.talentsketchers@gmail.com",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Flink', 'Apache Airflow', 'Data Quality', 'Hadoop', 'Snowflake', 'Data Lake', 'orchastration', 'Python', 'SQL']",2025-06-12 13:45:05
Senior Data Engineer,Amgen Inc,3 - 8 years,Not Disclosed,['Hyderabad'],"Role Description:\nWe are looking for highly motivated expert Senior Data Engineer who can own the design & development of complex data pipelines, solutions and frameworks. The ideal candidate will be responsible to design, develop, and optimize data pipelines, data integration frameworks, and metadata-driven architectures that enable seamless data access and analytics. This role prefers deep expertise in big data processing, distributed computing, data modeling, and governance frameworks to support self-service analytics, AI-driven insights, and enterprise-wide data management.\nRoles & Responsibilities:\nDesign, develop, and maintain scalable ETL/ELT pipelines to support structured, semi-structured, and unstructured data processing across the Enterprise Data Fabric.\nImplement real-time and batch data processing solutions, integrating data from multiple sources into a unified, governed data fabric architecture.\nOptimize big data processing frameworks using Apache Spark, Hadoop, or similar distributed computing technologies to ensure high availability and cost efficiency.\nWork with metadata management and data lineage tracking tools to enable enterprise-wide data discovery and governance.\nEnsure data security, compliance, and role-based access control (RBAC) across data environments.\nOptimize query performance, indexing strategies, partitioning, and caching for large-scale data sets.\nDevelop CI/CD pipelines for automated data pipeline deployments, version control, and monitoring.\nImplement data virtualization techniques to provide seamless access to data across multiple storage systems.\nCollaborate with cross-functional teams, including data architects, business analysts, and DevOps teams, to align data engineering strategies with enterprise goals.\nStay up to date with emerging data technologies and best practices, ensuring continuous improvement of Enterprise Data Fabric architectures.\nMust-Have Skills:\nHands-on experience in data engineering technologies such as Databricks, PySpark, SparkSQL Apache Spark, AWS, Python, SQL, and Scaled Agile methodologies.\nProficiency in workflow orchestration, performance tuning on big data processing.\nStrong understanding of AWS services\nExperience with Data Fabric, Data Mesh, or similar enterprise-wide data architectures.\nAbility to quickly learn, adapt and apply new technologies\nStrong problem-solving and analytical skills\nExcellent communication and teamwork skills\nExperience with Scaled Agile Framework (SAFe), Agile delivery practices, and DevOps practices.\nGood-to-Have Skills:\nGood to have deep expertise in Biotech & Pharma industries\nExperience in writing APIs to make the data available to the consumers\nExperienced with SQL/NOSQL database, vector database for large language models\nExperienced with data modeling and performance tuning for both OLAP and OLTP databases\nExperienced with software engineering best-practices, including but not limited to version control (Git, Subversion, etc.), CI/CD (Jenkins, Maven etc.), automated unit testing, and DevOps\nEducation and Professional Certifications\nMasters degree and 3 to 4 + years of Computer Science, IT or related field experience\nOR\nBachelors degree and 5 to 8 + years of Computer Science, IT or related field experience\nAWS Certified Data Engineer preferred\nDatabricks Certificate preferred\nScaled Agile SAFe certification preferred\nSoft Skills:\nExcellent analytical and troubleshooting skills.\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nTeam-oriented, with a focus on achieving team goals.\nAbility to learn quickly, be organized and detail oriented.\nStrong presentation and public speaking skills.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Git', 'PySpark', 'CI/CD', 'Databricks', 'ETL', 'NOSQL', 'AWS', 'data integration', 'SQL', 'Apache Spark', 'Python']",2025-06-12 13:45:07
Consultant - Lead Data Engineer,Affine Analytics,5 - 8 years,Not Disclosed,['Bengaluru'],"Strong experience with Python, SQL, pySpark, AWS Glue. Good to have - Shell Scripting, Kafka\nGood knowledge of DevOps pipeline usage (Jenkins, Bitbucket, EKS, Lightspeed)\nExperience of AWS tools (AWS S3, EC2, Athena, Redshift, Glue, EMR, Lambda, RDS, Kinesis, DynamoDB, QuickSight etc.).\nOrchestration using Airflow\nGood to have - Streaming technologies and processing engines, Kinesis, Kafka, Pub/Sub and Spark Streaming\nGood debugging skills",,,,"['Python', 'RDS', 'Shell Scripting', 'Kafka', 'AWS Glue', 'DynamoDB', 'Lightspeed', 'EMR', 'EKS', 'pySpark', 'Redshift', 'SQL', 'Jenkins', 'QuickSight', 'Glue', 'EC2', 'Kinesis', 'AWS S3', 'Bitbucket', 'Athena', 'Lambda']",2025-06-12 13:45:10
Senior Data Engineer,Amgen Inc,3 - 7 years,Not Disclosed,['Hyderabad'],"Role Description:\nWe are looking for highly motivated expert Senior Data Engineer who can own the design & development of complex data pipelines, solutions and frameworks. The ideal candidate will be responsible to design, develop, and optimize data pipelines, data integration frameworks, and metadata-driven architectures that enable seamless data access and analytics. This role prefers deep expertise in big data processing, distributed computing, data modeling, and governance frameworks to support self-service analytics, AI-driven insights, and enterprise-wide data management.\nRoles & Responsibilities:\nDesign, develop, and maintain scalable ETL/ELT pipelines to support structured, semi-structured, and unstructured data processing across the Enterprise Data Fabric.\nImplement real-time and batch data processing solutions, integrating data from multiple sources into a unified, governed data fabric architecture.\nOptimize big data processing frameworks using Apache Spark, Hadoop, or similar distributed computing technologies to ensure high availability and cost efficiency.\nWork with metadata management and data lineage tracking tools to enable enterprise-wide data discovery and governance.\nEnsure data security, compliance, and role-based access control (RBAC) across data environments.\nOptimize query performance, indexing strategies, partitioning, and caching for large-scale data sets.\nDevelop CI/CD pipelines for automated data pipeline deployments, version control, and monitoring.\nImplement data virtualization techniques to provide seamless access to data across multiple storage systems.\nCollaborate with cross-functional teams, including data architects, business analysts, and DevOps teams, to align data engineering strategies with enterprise goals.\nStay up to date with emerging data technologies and best practices, ensuring continuous improvement of Enterprise Data Fabric architectures.\nMust-Have Skills:\nHands-on experience in data engineering technologies such as Databricks, PySpark, SparkSQL Apache Spark, AWS, Python, SQL, and Scaled Agile methodologies.\nProficiency in workflow orchestration, performance tuning on big data processing.\nStrong understanding of AWS services\nExperience with Data Fabric, Data Mesh, or similar enterprise-wide data architectures.\nAbility to quickly learn, adapt and apply new technologies\nStrong problem-solving and analytical skills\nExcellent communication and teamwork skills\nExperience with Scaled Agile Framework (SAFe), Agile delivery practices, and DevOps practices.\nGood-to-Have Skills:\nGood to have deep expertise in Biotech & Pharma industries\nExperience in writing APIs to make the data available to the consumers\nExperienced with SQL/NOSQL database, vector database for large language models\nExperienced with data modeling and performance tuning for both OLAP and OLTP databases\nExperienced with software engineering best-practices, including but not limited to version control (Git, Subversion, etc.), CI/CD (Jenkins, Maven etc.), automated unit testing, and Dev Ops\nEducation and Professional Certifications\n9 to 12 years of Computer Science, IT or related field experience\nAWS Certified Data Engineer preferred\nDatabricks Certificate preferred\nScaled Agile SAFe certification preferred\nSoft Skills:\nExcellent analytical and troubleshooting skills.\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nTeam-oriented, with a focus on achieving team goals.\nAbility to learn quickly, be organized and detail oriented.\nStrong presentation and public speaking skills.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Maven', 'SparkSQL Apache Spark', 'PySpark', 'Subversion', 'OLAP', 'Scaled Agile methodologies', 'SQL', 'Scaled Agile Framework', 'Jenkins', 'NOSQL database', 'Git', 'Databricks', 'Data Fabric', 'Data Mesh', 'AWS', 'Python']",2025-06-12 13:45:12
Senior Data Engineer,Amgen Inc,9 - 12 years,Not Disclosed,['Hyderabad'],"Role Description:\nWe are looking for highly motivated expert Senior Data Engineer who can own the design & development of complex data pipelines, solutions and frameworks. The ideal candidate will be responsible to design, develop, and optimize data pipelines, data integration frameworks, and metadata-driven architectures that enable seamless data access and analytics. This role prefers deep expertise in big data processing, distributed computing, data modeling, and governance frameworks to support self-service analytics, AI-driven insights, and enterprise-wide data management.\nRoles & Responsibilities:\nDesign, develop, and maintain scalable ETL/ELT pipelines to support structured, semi-structured, and unstructured data processing across the Enterprise Data Fabric.\nImplement real-time and batch data processing solutions, integrating data from multiple sources into a unified, governed data fabric architecture.\nOptimize big data processing frameworks using Apache Spark, Hadoop, or similar distributed computing technologies to ensure high availability and cost efficiency.\nWork with metadata management and data lineage tracking tools to enable enterprise-wide data discovery and governance.\nEnsure data security, compliance, and role-based access control (RBAC) across data environments.\nOptimize query performance, indexing strategies, partitioning, and caching for large-scale data sets.\nDevelop CI/CD pipelines for automated data pipeline deployments, version control, and monitoring.\nImplement data virtualization techniques to provide seamless access to data across multiple storage systems.\nCollaborate with cross-functional teams, including data architects, business analysts, and DevOps teams, to align data engineering strategies with enterprise goals.\nStay up to date with emerging data technologies and best practices, ensuring continuous improvement of Enterprise Data Fabric architectures.\nMust-Have Skills:\nHands-on experience in data engineering technologies such as Databricks, PySpark, SparkSQL Apache Spark, AWS, Python, SQL, and Scaled Agile methodologies.\nProficiency in workflow orchestration, performance tuning on big data processing.\nStrong understanding of AWS services\nExperience with Data Fabric, Data Mesh, or similar enterprise-wide data architectures.\nAbility to quickly learn, adapt and apply new technologies\nStrong problem-solving and analytical skills\nExcellent communication and teamwork skills\nExperience with Scaled Agile Framework (SAFe), Agile delivery practices, and DevOps practices.\nGood-to-Have Skills:\nGood to have deep expertise in Biotech & Pharma industries\nExperience in writing APIs to make the data available to the consumers\nExperienced with SQL/NOSQL database, vector database for large language models\nExperienced with data modeling and performance tuning for both OLAP and OLTP databases\nExperienced with software engineering best-practices, including but not limited to version control (Git, Subversion, etc.), CI/CD (Jenkins, Maven etc.), automated unit testing, and Dev Ops\nEducation and Professional Certifications\n9 to 12 years of Computer Science, IT or related field experience\nAWS Certified Data Engineer preferred\nDatabricks Certificate preferred\nScaled Agile SAFe certification preferred\nSoft Skills:\nExcellent analytical and troubleshooting skills.\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nTeam-oriented, with a focus on achieving team goals.\nAbility to learn quickly, be organized and detail oriented.\nStrong presentation and public speaking skills.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data engineering', 'performance tuning', 'data security', 'data processing', 'Hadoop', 'Apache Spark', 'SQL', 'CI/CD', 'troubleshooting', 'big data', 'aws', 'ETL', 'Python']",2025-06-12 13:45:15
Senior Data Engineer,Amgen Inc,3 - 7 years,Not Disclosed,['Hyderabad'],"What you will do\nRole Description:\nWe are seeking a Senior Data Engineer with expertise in Graph Data technologies to join our data engineering team and contribute to the development of scalable, high-performance data pipelines and advanced data models that power next-generation applications and analytics. This role combines core data engineering skills with specialized knowledge in graph data structures, graph databases, and relationship-centric data modeling, enabling the organization to leverage connected data for deep insights, pattern detection, and advanced analytics use cases. The ideal candidate will have a strong background in data architecture, big data processing, and Graph technologies and will work closely with data scientists, analysts, architects, and business stakeholders to design and deliver graph-based data engineering solutions.\nRoles & Responsibilities:\nDesign, build, and maintain robust data pipelines using Databricks (Spark, Delta Lake, PySpark) for complex graph data processing workflows.\nOwn the implementation of graph-based data models, capturing complex relationships and hierarchies across domains.\nBuild and optimize Graph Databases such as Stardog, Neo4j, Marklogic or similar to support query performance, scalability, and reliability.\nImplement graph query logic using SPARQL, Cypher, Gremlin, or GSQL, depending on platform requirements.\nCollaborate with data architects to integrate graph data with existing data lakes, warehouses, and lakehouse architectures.\nWork closely with data scientists and analysts to enable graph analytics, link analysis, recommendation systems, and fraud detection use cases.\nDevelop metadata-driven pipelines and lineage tracking for graph and relational data processing.\nEnsure data quality, governance, and security standards are met across all graph data initiatives.\nMentor junior engineers and contribute to data engineering best practices, especially around graph-centric patterns and technologies.\nStay up to date with the latest developments in graph technology, graph ML, and network analytics.\nWhat we expect of you\nMust-Have Skills:\nHands-on experience in Databricks, including PySpark, Delta Lake, and notebook-based development.\nHands-on experience with graph database platforms such as Stardog, Neo4j, Marklogic etc.\nStrong understanding of graph theory, graph modeling, and traversal algorithms\nProficiency in workflow orchestration, performance tuning on big data processing\nStrong understanding of AWS services\nAbility to quickly learn, adapt and apply new technologies with strong problem-solving and analytical skills\nExcellent collaboration and communication skills, with experience working with Scaled Agile Framework (SAFe), Agile delivery practices, and DevOps practices.\nGood-to-Have Skills:\nGood to have deep expertise in Biotech & Pharma industries\nExperience in writing APIs to make the data available to the consumers\nExperienced with SQL/NOSQL database, vector database for large language models\nExperienced with data modeling and performance tuning for both OLAP and OLTP databases\nExperienced with software engineering best-practices, including but not limited to version control (Git, Subversion, etc.), CI/CD (Jenkins, Maven etc.), automated unit testing, and Dev Ops\nEducation and Professional Certifications\nMasters degree and 3 to 4 + years of Computer Science, IT or related field experience\nBachelors degree and 5 to 8 + years of Computer Science, IT or related field experience\nAWS Certified Data Engineer preferred\nDatabricks Certificate preferred\nScaled Agile SAFe certification preferred\nSoft Skills:\nExcellent analytical and troubleshooting skills.\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nTeam-oriented, with a focus on achieving team goals.\nAbility to learn quickly, be organized and detail oriented.\nStrong presentation and public speaking skills.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'SPARQL', 'Maven', 'PySpark', 'GSQL', 'Subversion', 'AWS services', 'Stardog', 'Cypher', 'SAFe', 'Jenkins', 'DevOps', 'Git', 'Neo4j', 'Delta Lake', 'Graph Databases', 'Spark', 'Marklogic', 'Gremlin']",2025-06-12 13:45:17
Immediate Joiner- Data Engineer,Healthedge,1 - 4 years,Not Disclosed,['Bengaluru'],"Data Engineer\nYou will be working with agile cross functional software development teams developing cutting age software to solve a significant problem in the Provider Data Management space. This hire will have experience building large scale complex data systems involving multiple cross functional data sets and teams. The ideal candidate will be excited about working on new product development, is comfortable pushing the envelope and challenging the status quo, sets high standards for him/herself and the team, and works well with ambiguity.\nWhat you will do:\nBuild data pipelines to assemble large, complex sets of data that meet non-functional and functional business requirements.\nWork closely with data architect, SMEs and other technology partners to develop & execute data architecture and product roadmap.\nBuild analytical tools to utilize the data pipeline, providing actionable insight into key business performance including operational efficiency and business metrics.\nWork with stakeholders including the leadership, product, customer teams to support their data infrastructure needs while assisting with data-related technical issues.\nAct as a subject matter expert to other team members for technical guidance, solution design and best practices within the customer organization.\nKeep current on big data and data visualization technology trends, evaluate, work on proof-of-concept and make recommendations on cloud technologies.\nWhat you bring:\n2+ years of data engineering experience working in partnership with large data sets (preferably terabyte scale)\nExperience in building data pipelines using any of the ETL tools such as Glue, ADF, Notebooks, Stored Procedures, SQL/Python constructs or similar.\nDeep experience working with industry standard RDBMS such Postgres, SQL Server, Oracle, MySQL etc. and any of the analytical cloud databases such as Big Query, Redshift, Snowflake or similar\nAdvanced SQL expertise and solid programming experience with Python and/or Spark\nExperience working with orchestration tools such as Airflow and building complex dependency workflows.\nExperience, developing and implementing Data Warehouse or Data Lake Architectures, OLAP technologies, data modeling with star/snowflake-schemas to enable analytics & reporting.\nGreat problem-solving capabilities, troubleshooting data issues and experience in stabilizing big data systems.\nExcellent communication and presentation skills as youll be regularly interacting with stakeholders and engineering leadership.\nBachelors or master's in quantitative disciplines such as Computer Science, Computer Engineering, Analytics, Mathematics, Statistics, Information Systems, or other scientific fields.\nBonus points:\nHands-on deep experience with cloud data migration, and experience working with analytic platforms like Fabric, Databricks on the cloud.\nCertification in one of the cloud platforms (AWS/GCP/Azure)\nExperience or demonstrated understanding with real-time data streaming tools like Kafka, Kinesis or any similar tools.",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['ETL', 'SQL', 'Pyspark', 'Cloud', 'Python']",2025-06-12 13:45:19
Senior Data Engineer,Conversehr Business Solutions,7 - 12 years,30-45 Lacs P.A.,['Hyderabad'],"What is the Data team responsible for?\nAs a Senior Data Engineer, youll be a key member of the Data & AI team. This team is responsible for designing and delivering data engineering, analytics, and generative AI solutions that drive meaningful business impact. Were looking for a pragmatic, results-driven problem solver who thrives in a fast-paced environment and is passionate about building solutions on a scale #MID_SENIOR_LEVEL\nThe ideal candidate has a strong technical foundation, a collaborative mindset, and the ability to navigate complex challenges. You should be comfortable working in a fast-moving, startup-like environment within an established enterprise, and should bring strong skill sets to adapt to new solutions fast. You will play a pivotal role in optimizing data infrastructure, enabling data-driven decision-making and integrating AI across the organization.\nWhat is the Lead Software Engineer (Senior Data Engineer) responsible for?\nServe as a hands-on technical lead, driving project execution and delivery in our growing data team based in the Hyderabad office.\nCollaborate closely with the U.S.-based team and cross-functional stakeholders to understand business needs and deliver scalable solutions.\nLead the initiative to build firmwide data models and master data management solutions for structured data (in Snowflake) and manage unstructured data using vector embeddings.\nBuild, maintain, and optimize robust data pipelines and frameworks to support business intelligence and operational workflows.\nDevelop dashboards and data visualizations that support strategic business decisions.\nStay current with emerging trends in data engineering and help implement best practices within the team.\nMentor and support junior engineers, fostering a culture of learning and technical excellence.\nWhat ideal qualifications, skills & experience would help someone to be successful?\nBachelors or master’s degree in computer science, data science, engineering, or a related field.\n7+ years of experience in data engineering including 3+ years in a technical leadership role.\nStrong SQL skills and hands-on experience with modern data pipeline technologies (e.g., Spark, Flink).\nDeep expertise in the Snowflake ecosystem, including data modeling, data warehousing, and master data management.\nProficiency in at least one programming language - Python preferred.\nExperience with Tableau and Alteryx is a plus.\nSelf-starter with a passion for learning new tools and technologies.\nStrong communication skills and a collaborative, ownership-driven mindset.\nWork Shift Timings - 2:00 PM - 11:00 PM IST",Industry Type: Investment Banking / Venture Capital / Private Equity,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Engineering', 'Snowflake', 'Python', 'flink', 'Data Pipeline', 'Spark', 'SQL']",2025-06-12 13:45:21
Data Engineer KL-BL,Puresoftware,5 - 12 years,Not Disclosed,['Bengaluru'],"Core Competences Required and Desired Attributes:\nBachelors degree in computer science, Information Technology, or a related field.\nProficiency in Azure Data Factory, Azure Databricks and Unity Catalog, Azure SQL Database, and other Azure data services.\nStrong programming skills in SQL, Python and PySpark languages.\nExperience in the Asset Management domain would be preferable.\nStrong proficiency in data analysis and data modelling, with the ability to extract insights from complex data sets.\nHands-on experience in Power BI, including creating custom visuals, DAX expressions, and data modelling.\nFamiliarity with Azure Analysis Services, data modelling techniques, and optimization.\nExperience with data quality and data governance frameworks with an ability to debug, fine tune and optimise large scale data processing jobs.\nStrong analytical and problem-solving skills, with a keen eye for detail.\nExcellent communication and interpersonal skills, with the ability to work collaboratively in a team environment.\nProactive and self-motivated, with the ability to manage multiple tasks and deliver high-quality results within deadlines.",Industry Type: Management Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data analysis', 'Interpersonal skills', 'Data modeling', 'Analytical', 'data governance', 'Data quality', 'Asset management', 'Information technology', 'SQL', 'Python']",2025-06-12 13:45:23
Data Engineering Specialist,Overture Rede,10 - 15 years,Not Disclosed,"['Mumbai', 'Hyderabad', 'Gurugram', 'Bengaluru']","Job Title: Sales Excellence COE Data Engineering Specialist\nLocations: Mumbai / Bangalore / Gurgaon / Hyderabad\nExperience: 1012 Years Level: Team Lead / Specialist (Level 9)\n\nJob Role\nLead data engineering efforts to support sales insights through scalable pipelines, statistical modeling, and ML workflows across cloud platforms.\n\nRequired Skills\nProficiency in Python\nAdvanced SQL (Views, Functions, Procedures)\nExperience with Google Cloud Platform (GCP) ML workflow setup\nStrong in Data Modeling and ETL Development\nExcel skills including VBA, Power Pivot, Cube Functions\nSolid understanding of Sales Processes\n\n\n",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Statistical modeling', 'Sales', 'Excel', 'VBA', 'Data modeling', 'GCP', 'Cloud', 'Workflow', 'SQL', 'Python']",2025-06-12 13:45:26
Data Engineer II - Marketplace (Experimentation Track),Booking Holdings,5 - 10 years,Not Disclosed,['Bengaluru'],"We are looking for a Data Engineer to join our team and help us to improve the platform that supports one of the best experimentation tools in the world.\nYou will work side by side with other data engineers and site reliability engineers to improve the reliability, scalability, maintenance and operations of all the data products that are part of the experimentation tool at Booking.com.\nYour day to day work includes but is not limited to: maintenance and operations of data pipelines and products that handles data at big scale; the development of capabilities for monitoring, alerting, testing and troubleshooting of the data ecosystem of the experiment platform; and the delivery of data products that produce metrics for experimentation at scale. You will collaborate with colleagues in Amsterdam to achieve results the right way. This will include engineering managers, product managers, engineers and data scientists.\nKey Responsibilities and Duties\nTake ownership of multiple data pipelines and products and provide innovative solutions to reduce the operational workload required to maintain them\nRapidly developing next-generation scalable, flexible, and high-performance data pipelines.\nContribute to the development of data platform capabilities such as testing, monitoring, debugging and alerting to improve the development environment of data products\nSolve issues with data and data pipelines, prioritizing based on customer impact.\nEnd-to-end ownership of data quality in complex datasets and data pipelines.\nExperiment with new tools and technologies, driving innovative engineering solutions to meet business requirements regarding performance, scaling, and data quality.\nProvide self-organizing tools that help the analytics community discover data, assess quality, explore usage, and find peers with relevant expertise.\nServe as the main point of contact for technical and business stakeholders regarding data engineering issues, such as pipeline failures and data quality concerns\nRole requirements\nMinimum 5 years of hands-on experience in data engineering as a Data Engineer or as a Software Engineer developing data pipelines and products.\nBachelors degree in Computer Science, Computer or Electrical Engineering, Mathematics, or a related field or 5 years of progressively responsible experience in the specialty as equivalent\nSolid experience in at least one programming language. We use Java and Python\nExperience building production data pipelines in the cloud, setting up data-lakes and server-less solutions\nHands-on experience with schema design and data modeling\nExperience designing systems E2E and knowledge of basic concepts (lb, db, caching, NoSQL, etc)\nKnowledge of Flink, CDC, Kafka, Airflow, Snowflake, DBT or equivalent tools\nPractical experience building data platform capabilities like testing, alerting, monitoring, debugging, security\nExperience working with big data.\nExperience working with teams located in different timezones is a plus\nExperience with experimentation, statistics and A/B testing is a plus",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Airflow', 'Java', 'CDC', 'NoSQL', 'Snowflake', 'DBT', 'Kafka', 'Python']",2025-06-12 13:45:28
GCP Data Engineer,TVS Next,3 - 5 years,Not Disclosed,['Bengaluru'],"What you’ll be doing:\nAssist in developing machine learning models based on project requirements\nWork with datasets by preprocessing, selecting appropriate data representations, and ensuring data quality.\nPerforming statistical analysis and fine-tuning using test results.\nSupport training and retraining of ML systems as needed.\nHelp build data pipelines for collecting and processing data efficiently.",,,,"['kubernetes', 'pyspark', 'data pipeline', 'sql', 'docker', 'cloud', 'tensorflow', 'java', 'spark', 'gcp', 'pytorch', 'bigquery', 'programming', 'ml', 'cloud sql', 'cd', 'python', 'airflow', 'cloud spanner', 'cloud pubsub', 'application engine', 'machine learning', 'apache flink', 'data engineering', 'dataproc', 'kafka', 'cloud storage', 'terraform', 'bigtable']",2025-06-12 13:45:31
Azure Data Engineer,Big4,7 - 12 years,18-30 Lacs P.A.,['Bengaluru'],"Urgently Hiring for Senior Azure Data Engineer\n\nJob Location- Bangalore\nMinimum exp - Total 7+yrs with min 4 years relevant exp\n\nKeywords Databricks, Pyspark, SCALA, SQL, Live / Streaming data, batch processing data\n\nShare CV siddhi.pandey@adecco.com\nOR Call 6366783349\n\nRoles and Responsibilities:\nThe Data Engineer will work on data engineering projects for various business units, focusing on delivery of complex data management solutions by leveraging industry best practices. They work with the project team to build the most efficient data pipelines and data management solutions that make data easily available for consuming applications and analytical solutions. A Data engineer is expected to possess strong technical skills\n\nKey Characteristics\nTechnology champion who constantly pursues skill enhancement and has inherent curiosity to understand work from multiple dimensions\nInterest and passion in Big Data technologies and appreciates the value that can be brought in with an effective data management solution\nHas worked on real data challenges and handled high volume, velocity, and variety of data.\nExcellent analytical & problem-solving skills, willingness to take ownership and resolve technical challenges.\nContributes to community building initiatives like CoE, CoP.\nMandatory skills:\nAzure - Master\nELT - Skill\nData Modeling - Skill\nData Integration & Ingestion - Skill\nData Manipulation and Processing - Skill\nGITHUB, Action, Azure DevOps - Skill\nData factory, Databricks, SQL DB, Synapse, Stream Analytics, Glue, Airflow, Kinesis, Redshift, SonarQube, PyTest - Skill\nOptional skills:\nExperience in project management, running a scrum team.\nExperience working with BPC, Planning.\nExposure to working with external technical ecosystem.\nMKDocs documentation\n\nShare CV siddhi.pandey@adecco.com\nOR Call 6366783349",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['databricks', 'Azure Synapse', 'Pyspark', 'Stream Analytics', 'SCALA', 'SQL Azure', 'Data Bricks', 'SQL']",2025-06-12 13:45:33
Tech. PM - Data Engineering-Data Analytics@ Gurgaon/Blore_Urgent,A global leader in delivering innovative...,5 - 10 years,Not Disclosed,"['Gurugram', 'Bengaluru']","Job Title - Technical Project Manager\n\nLocation - Gurgaon/ Bangalore\n\nNature of Job - Permanent\n\nDepartment - data analytics\n\nWhat you will be doing\n\n\nDemonstrated client servicing and business analytics skills with at least 5 - 9 years of experience as data engineer, BI developer, data analyst, technical project manager, program manager etc.\nTechnical project management- drive BRD, project scope, resource allocation, team\ncoordination, stakeholder communication, UAT, Prod fix, change requests, project governance\nSound knowledge of banking industry (payments, retail operations, fraud etc.)\nStrong ETL experience or experienced Teradata developer\nManaging team of business analysts, BI developers, ETL developers to ensure that projects are completed on time\nResponsible for providing thought leadership and technical advice on business issues\nDesign methodological frameworks and solutions.\n\n\nWhat were looking for\n\n\nBachelors/masters degree in computer science/data science/AI/statistics, Certification in Gen AI. Masters degree Preferred.\nManage multiple projects, at a time, from inception to delivery\nSuperior problem-solving, analytical, and quantitative skills\nEntrepreneurial mindset, coupled with a “can do” attitude\nDemonstrated ability to collaborate with cross-functional, cross-border teams and coach / mentor colleagues.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Technical Project Manager', 'Data Engineering', 'multiple projects', 'Technical project management', 'Data Analytics', 'project scope', 'ETL Pipeline', 'team coordination', 'resource allocation', 'Prod fix', 'drive BRD', 'program manager', 'Big data']",2025-06-12 13:45:35
Data Engineering Manager,NOVARTIS,6 - 8 years,Not Disclosed,['Hyderabad'],"Summary\nWe are seeking a highly skilled and motivated GCP Data Engineering Manager to join our dynamic team. As a Data Engineering manager specializing in Google Cloud Platform (GCP), you will play a crucial role in designing, implementing, and maintaining scalable data pipelines and\nsystems. You will leverage your expertise in Google Big Query, SQL, Python, and analytical skills to drive data-driven decision-making processes and support various business functions.\nAbout the Role\nKey Responsibilities:\nData Pipeline Development: Design, develop, and maintain robust data pipelines using GCP services like Dataflow, Dataproc, ensuring high performance and scalability.\nGoogle Big Query Expertise: Utilize your hands-on experience with Google Big Query to manage and optimize data storage, retrieval, and processing.\nSQL Proficiency: Write and optimize complex SQL queries to transform and analyze large datasets, ensuring data accuracy and integrity.\nPython Programming: Develop and maintain Python scripts for data processing, automation, and integration with other systems and tools.\nData Integration: Collaborate with data analysts, and other stakeholders to integrate data from various sources, ensuring seamless data flow and consistency.\nData Quality and Governance: Implement data quality checks, validation processes, and governance frameworks to maintain high data standards.\nPerformance Tuning: Monitor and optimize the performance of data pipelines, queries, and storage solutions to ensure efficient data processing.\nDocumentation: Create comprehensive documentation for data pipelines, processes, and best practices to facilitate knowledge sharing and team collaboration.\nMinimum Qualifications:\nProven experience (minimum 6 - 8 yrs) in Data Engineer, with significant hands-on experience in Google Cloud Platform (GCP) and Google Big Query.\nProficiency in SQL for data transformation, analysis and performance optimization.\nStrong programming skills in Python, with experience in developing data processing scripts and automation.\nProven analytical skills with the ability to interpret complex data and provide actionable insights.\nExcellent problem-solving abilities and attention to detail.\nStrong communication and collaboration skills, with the ability to work effectively in a team enviro\nDesired Skills :\nExperience with Google Analytics data and understanding of digital marketing data.\nFamiliarity with other GCP services such as Cloud Storage, Dataflow, Pub/Sub, and Dataproc.\nKnowledge of data visualization tools such as Looker, Tableau, or Data Studio.\nExperience with machine learning frameworks and libraries.\nWhy Novartis: Helping people with disease and their families takes more than innovative science. It takes a community of smart, passionate people like you. Collaborating, supporting and inspiring each other. Combining to achieve breakthroughs that change patients lives. Ready to create a brighter future together? https://www. novartis. com / about / strategy / people-and-culture\nJoin our Novartis Network: Not the right Novartis role for you? Sign up to our talent community to stay connected and learn about suitable career opportunities as soon as they come up: https://talentnetwork. novartis. com/network\nBenefits and Rewards: Read our handbook to learn about all the ways we ll help you thrive personally and professionally:",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Performance tuning', 'Automation', 'Google Analytics', 'Machine learning', 'Data processing', 'Data quality', 'data visualization', 'Digital marketing', 'SQL', 'Python']",2025-06-12 13:45:38
GCP Data Engineer,Swits Digital,4 - 6 years,Not Disclosed,['Bengaluru'],"Job Title: GCP Data Engineer\nLocation: Chennai, Bangalore, Hyderabad\nExperience: 4-6 Years\nJob Summary:\nWe are seeking a GCP Data & Cloud Engineer with strong expertise in Google Cloud Platform services, including BigQuery, Cloud Run, Cloud Storage , and Pub/Sub . The ideal candidate will have deep experience in SQL coding , data pipeline development, and deploying cloud-native solutions.\nKey Responsibilities:\nDesign, implement, and optimize scalable data pipelines and services using GCP\nBuild and manage cloud-native applications deployed via Cloud Run\nDevelop complex and performance-optimized SQL queries for analytics and data transformation\nManage and automate data storage, retrieval, and archival using Cloud Storage\nImplement event-driven architectures using Google Pub/Sub\nWork with large datasets in BigQuery , including ETL/ELT design and query optimization\nEnsure security, monitoring, and compliance of cloud-based systems\nCollaborate with data analysts, engineers, and product teams to deliver end-to-end cloud solutions\nRequired Skills & Experience:\n4 years of experience working with Google Cloud Platform (GCP)\nStrong proficiency in SQL coding , query tuning, and handling complex data transformations\nHands-on experience with:\nBigQuery\nCloud Run\nCloud Storage\nPub/Sub\nUnderstanding of data pipeline and ETL/ELT workflows in cloud environments\nFamiliarity with containerized services and CI/CD pipelines\nExperience in scripting languages (e.g., Python, Shell) is a plus\nStrong analytical and problem-solving skills",Industry Type: Recruitment / Staffing,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['SUB', 'query optimization', 'GCP', 'Analytical', 'Cloud', 'query', 'cloud storage', 'Analytics', 'SQL coding', 'Python']",2025-06-12 13:45:40
Senior Data Engineer,Jeavio,5 - 10 years,Not Disclosed,[],"We are seeking an experienced Senior Data Engineer to join our team. The ideal candidate will have a strong background in data engineering and AWS infrastructure, with hands-on experience in building and maintaining data pipelines and the necessary infrastructure components. The role will involve using a mix of data engineering tools and AWS services to design, build, and optimize data architecture.\n\nKey Responsibilities:\nDesign, develop, and maintain data pipelines using Airflow and AWS services.\nImplement and manage data warehousing solutions with Databricks and PostgreSQL.\nAutomate tasks using GIT / Jenkins.\nDevelop and optimize ETL processes, leveraging AWS services like S3, Lambda, AppFlow, and DMS.\nCreate and maintain visual dashboards and reports using Looker.\nCollaborate with cross-functional teams to ensure smooth integration of infrastructure components.\nEnsure the scalability, reliability, and performance of data platforms.\nWork with Jenkins for infrastructure automation.\n\nTechnical and functional areas of expertise:\nWorking as a senior individual contributor on a data intensive project\nStrong experience in building high performance, resilient & secure data processing pipelines preferably using Python based stack.\nExtensive experience in building data intensive applications with a deep understanding of querying and modeling with relational databases preferably on time-series data.\nIntermediate proficiency in AWS services (S3, Airflow)\nProficiency in Python and PySpark\nProficiency with ThoughtSpot or Databricks.\nIntermediate proficiency in database scripting (SQL)\nBasic experience with Jenkins for task automation\n\nNice to Have :\nIntermediate proficiency in data analytics tools (Power BI / Tableau / Looker / ThoughSpot)\nExperience working with AWS Lambda, Glue, AppFlow, and other AWS transfer services.\nExposure to PySpark and data automation tools like Jenkins or CircleCI.\nFamiliarity with Terraform for infrastructure-as-code.\nExperience in data quality testing to ensure the accuracy and reliability of data pipelines.\nProven experience working directly with U.S. client stakeholders.\nAbility to work independently and take the lead on tasks.\n\nEducation and experience:\nBachelors or masters in computer science or related fields.\n5+ years of experience\n\nStack/Skills needed:\nDatabricks\nPostgreSQL\nPython & Pyspark\nAWS Stack\nPower BI / Tableau / Looker / ThoughSpot\nFamiliarity with GIT and/or CI/CD tools",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Pyspark', 'Data Engineering', 'AWS', 'Data Bricks', 'Python', 'Etl Pipelines', 'Airflow', 'Database Scripting', 'Postgresql', 'Looker', 'SQL']",2025-06-12 13:45:42
Data Engineer (AWS),Neoware Technology Solutions,4 - 10 years,Not Disclosed,"['Chennai', 'Bengaluru']","Data Engineer (AWS) - Neoware Technology Solutions Private Limited Data Engineer (AWS)\nRequirements\n4 - 10 years of hands-on experience in designing, developing and implementing data engineering solutions.\nStrong SQL development skills, including performance tuning and query optimization.\nGood understanding of data concepts.\nProficiency in Python and a solid understanding of programming concepts.\nHands-on experience with PySpark or Spark Scala for building data pipelines.\nUnderstanding of streaming data pipelines for near real-time analytics.\nExperience with Azure services including Data Factory, Functions, Databricks, Synapse Analytics, Event Hub, Stream Analytics and Data Lake Storage.\nFamiliarity with at least one NoSQL database.\nKnowledge of modern data architecture patterns and industry trends in data engineering.\nUnderstanding of data governance concepts for data platforms and analytical solutions.\nExperience with Git for managing version control for source code.\nExperience with DevOps processes, including experience implementing CI/CD pipelines for data engineering solutions.\nStrong analytical and problem-solving skills.\nExcellent communication and teamwork skills.\nResponsibilities\nAzure Certifications related to Data Engineering are highly preferred.\nExperience with Amazon AppFlow, EKS, API Gateway, NoSQL database services.\nStrong understanding and experience with BI/visualization tools like Power BI.\nChennai, Bangalore\nFull time\n4+ years\nOther positions Principal Architect (Data and Cloud) Development",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Performance tuning', 'Version control', 'GIT', 'NoSQL', 'query optimization', 'Analytical', 'data governance', 'Analytics', 'Python', 'Data architecture']",2025-06-12 13:45:45
Gcp Data Engineer,Saama Technologies,3 - 8 years,Not Disclosed,"['Pune', 'Chennai', 'Coimbatore']","We are looking for immediate joiners only.\nPosition: GCP Data Engineer\nWe are seeking a skilled and experienced GCP Data Engineer to join our dynamic team. The ideal candidate will have a strong background in Google Cloud Platform (GCP), BigQuery, Dataform, and data warehouse concepts. Experience with Airflow/Cloud Composer and cloud computing knowledge will be a significant advantage.\nResponsibilities:\n- Designing, developing, and maintaining data pipelines and workflows on the Google Cloud Platform.",,,,"['Pyspark', 'GCP', 'Python', 'SQL', 'Google Cloud Platforms']",2025-06-12 13:45:48
Azure Data Engineer,JRD Systems,7 - 12 years,Not Disclosed,"['Hyderabad', 'Bengaluru']","Cloud Data Engineer\n\nThe Cloud Data Engineer will be responsible for developing the data lake platform and all applications on Azure cloud. Proficiency in data engineering, data modeling, SQL, and Python programming is essential. The Data Engineer will provide design and development solutions for applications in the cloud.\nEssential Job Functions:\nUnderstand requirements and collaborate with the team to design and deliver projects.\nDesign and implement data lake house projects within Azure.\nDevelop application lifecycle utilizing Microsoft Azure technologies.\nParticipate in design, planning, and necessary documentation.\nEngage in Agile ceremonies including daily standups, scrum, retrospectives, demos, and code reviews.\nHands-on experience with Python/SQL development and Azure data pipelines.\nCollaborate with the team to develop and deliver cross-functional products.\nKey Skills:\na. Data Engineering and SQL\nb. Python\nc. PySpark\nd. Azure Data Lake and ADF\ne. Databricks\nf. CI/CD\ng. Strong communication\nOther Responsibilities:\nDocument and maintain project artifacts.\nMaintain comprehensive knowledge of industry standards, methodologies, processes, and best practices.\nComplete training as required for Privacy, Code of Conduct, etc.\nPromptly report any known or suspected loss, theft, or unauthorized disclosure or use of PI to the General Counsel/Chief Compliance Officer or Chief Information Officer.\nAdhere to the company's compliance program.\nSafeguard the company's intellectual property, information, and assets.\nOther duties as assigned.\nMinimum Qualifications and Job Requirements:\nBachelor's degree in Computer Science.\n7 years of hands-on experience in designing and developing distributed data pipelines.\n5 years of hands-on experience in Azure data service technologies.\n5 years of hands-on experience in Python, SQL, Object-oriented programming, ETL, and unit testing.\nExperience with data integration with APIs, Web services, Queues.\nExperience with Azure DevOps and CI/CD as well as agile tools and processes including JIRA, Confluence.\n*Required: Azure data engineering associate and databricks data engineering certification",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'Delta Table', 'Azure Databricks', 'SQL', 'Python', 'SCALA', 'Big Data', 'Kafka', 'Azure Data Lake', 'Spark', 'ETL', 'Data Bricks']",2025-06-12 13:45:50
Azure Data Engineer ( Azure Databricks),Apex One,4 - 8 years,Not Disclosed,"['Hyderabad', 'Bengaluru']","Job Summary\nWe are seeking a skilled Azure Data Engineer with 4 years of overall experience, including at least 2 years of hands-on experience with Azure Databricks (Must). The ideal candidate will have strong expertise in building and maintaining scalable data pipelines and working across cloud-based data platforms.\nKey Responsibilities\nDesign, develop, and optimize large-scale data pipelines using Azure Data Factory, Azure Databricks, and Azure Synapse.\nImplement data lake solutions and work with structured and unstructured datasets in Azure Data Lake Storage (ADLS).\nCollaborate with data scientists, analysts, and engineering teams to design and deliver end-to-end data solutions.\nDevelop ETL/ELT processes and integrate data from multiple sources.\nMonitor, debug, and optimize workflows for performance and cost-efficiency.\nEnsure data governance, quality, and security best practices are maintained.\nMust-Have Skills\n4+ years of total experience in data engineering.\n2+ years of experience with Azure Databricks (PySpark, Notebooks, Delta Lake).\nStrong experience with Azure Data Factory, Azure SQL, and ADLS.\nProficient in writing SQL queries and Python/Scala scripting.\nUnderstanding of CI/CD pipelines and version control systems (e.g., Git).\nSolid grasp of data modeling and warehousing concepts.",Industry Type: Management Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure', 'Azure Data Factory', 'SQL queries', 'PySpark', 'Delta Lake', 'Azure Databricks', 'Notebooks', 'Azure SQL']",2025-06-12 13:45:52
Senior Data Engineer - Azure,Blend360 India,6 - 11 years,Not Disclosed,['Hyderabad'],"As a Data Engineer, your role is to spearhead the data engineering teams and elevate the team to the next level! You will be responsible for laying out the architecture of the new project as well as selecting the tech stack associated with it. You will plan out the development cycles deploying AGILE if possible and create the foundations for good data stewardship with our new data products!\nYou will also set up a solid code framework that needs to be built to purpose yet have enough flexibility to adapt to new business use cases tough but rewarding challenge!\n\nResponsibilities\nCollaborate with several stakeholders to deeply understand the needs of data practitioners to deliver at scale\nLead Data Engineers to define, build and maintain Data Platform\nWork on building Data Lake in Azure Fabric processing data from multiple sources\nMigrating existing data store from Azure Synapse to Azure Fabric\nImplement data governance and access control\nDrive development effort End-to-End for on-time delivery of high-quality solutions that conform to requirements, conform to the architectural vision, and comply with all applicable standards.\nPresent technical solutions, capabilities, considerations, and features in business terms. Effectively communicate status, issues, and risks in a precise and timely manner.\nFurther develop critical initiatives, such as Data Discovery, Data Lineage and Data Quality\nLeading team and Mentor junior resources\nHelp your team members grow in their role and achieve their career aspirations\nBuild data systems, pipelines, analytical tools and programs\nConduct complex data analysis and report on results\n\n\n3+ Years of Experience as a data engineer or similar role in Azure Synapses, ADF or\nrelevant exp in Azure Fabric\nDegree in Computer Science, Data Science, Mathematics, IT, or similar field\nMust have experience e",Industry Type: Industrial Automation,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Data analysis', 'Data modeling', 'Analytical', 'Agile', 'data governance', 'Data quality', 'Data mining', 'SQL', 'Python']",2025-06-12 13:45:54
Senior Data Engineer : 7+ Years,Jayam Solutions Pvt Ltd - CMMI Level III Company,5 - 9 years,Not Disclosed,['Hyderabad( Madhapur )'],"Job Description:\nPosition: Sr.Data Engineer\nExperience: Minimum 7 years\nLocation: Hyderabad\nJob Summary:\n\nWhat Youll Do\n\nDesign and build efficient, reusable, and reliable data architecture leveraging technologies like Apache Flink, Spark, Beam and Redis to support large-scale, real-time, and batch data processing.\nParticipate in architecture and system design discussions, ensuring alignment with business objectives and technology strategy, and advocating for best practices in distributed data systems.\nIndependently perform hands-on development and coding of data applications and pipelines using Java, Scala, and Python, including unit testing and code reviews.\nMonitor key product and data pipeline metrics, identify root causes of anomalies, and provide actionable insights to senior management on data and business health.\nMaintain and optimize existing datalake infrastructure, lead migrations to lakehouse architectures, and automate deployment of data pipelines and machine learning feature engineering requests.\nAcquire and integrate data from primary and secondary sources, maintaining robust databases and data systems to support operational and exploratory analytics.\nEngage with internal stakeholders (business teams, product owners, data scientists) to define priorities, refine processes, and act as a point of contact for resolving stakeholder issues.\nDrive continuous improvement by establishing and promoting technical standards, enhancing productivity, monitoring, tooling, and adopting industry best practices.\n\nWhat Youll Bring\n\nBachelors degree or higher in Computer Science, Engineering, or a quantitative discipline, or equivalent professional experience demonstrating exceptional ability.\n7+ years of work experience in data engineering and platform engineering, with a proven track record in designing and building scalable data architectures.\nExtensive hands-on experience with modern data stacks, including datalake, lakehouse, streaming data (Flink, Spark), and AWS or equivalent cloud platforms.\nCloud - AWS\nApache Flink/Spark , Redis\nDatabase platform- Databricks.\nProficiency in programming languages such as Java, Scala, and Python(Good to have) for data engineering and pipeline development.\nExpertise in distributed data processing and caching technologies, including Apache Flink, Spark, and Redis.\nExperience with workflow orchestration, automation, and DevOps tools (Kubernetes,git,Terraform, CI/CD).\nAbility to perform under pressure, managing competing demands and tight deadlines while maintaining high-quality deliverables.\nStrong passion and curiosity for data, with a commitment to data-driven decision making and continuous learning.\nExceptional attention to detail and professionalism in report and dashboard creation.\nExcellent team player, able to collaborate across diverse functional groups and communicate complex technical concepts clearly.\nOutstanding verbal and written communication skills to effectively manage and articulate the health and integrity of data and systems to stakeholders.\n\nPlease feel free to contact us: 9440806850\nEmail ID : careers@jayamsolutions.com",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Apache Flink', 'Redis', 'Spark', 'Python', 'SCALA', 'Ci/Cd', 'Devops', 'AWS']",2025-06-12 13:45:56
Data Engineer - AWS,Happiest Minds Technologies,6 - 10 years,Not Disclosed,"['Pune', 'Bengaluru']","Role & responsibilities\nEssential Skills: Experience: 6 to 10 yrs\n- Technical Expertise: Proficiency in AWS services such as Amazon S3, Redshift, EMR, Glue, Lambda, and Kinesis. Strong skills in SQL and experience with scripting languages like Python or Java.\n- Data Engineering Experience: Hands on experience in building and maintaining data pipelines, data modeling, and working with big data technologies.\n- Problem-Solving Skills: Ability to analyze complex data issues and develop effective solutions to optimize data processing and storage.",,,,"['Data Engineering', 'Pyspark', 'Aws Lambda', 'Amazon Redshift', 'Aws Glue', 'Athena', 'AWS', 'Python', 'SQL']",2025-06-12 13:45:59
Senior Data Engineer,Neal Analytics,10 - 15 years,Not Disclosed,['Mumbai'],"Its fun to work in a company where people truly BELIEVE in what they are doing!\nWere committed to bringing passion and customer focus to the business.\nJob Description:\nAs a Backend (Java) Engineer, you would be part of the team consisting of Scrum Master, Cloud Engineers, AI/ML Engineers, and UI/UX Engineers to build end-to-end Data to Decision Systems.\nMandatory:\n8+ years of demonstrable experience designing, building, and working as a Java Developer for enterprise web applications\nIdeally, this would include the following:\no Expert-level proficiency with Java\no Expert-level proficiency with SpringBoot\nFamiliarity with common databases (RDBMS such as MySQL & NoSQL such as MongoDB) and data warehousing concepts (OLAP, OLTP)\nUnderstanding of REST concepts and building/interacting with REST APIs\nDeep understanding of core backend concepts:\no Develop and design RESTful services and APIs\no Develop functional databases, applications, and servers to support websites on the back end\no Performance optimization and multithreading concepts\no Experience with deploying and maintaining high traffic infrastructure (performance testing is a plus)\nIn addition, the ideal candidate would have great problem-solving skills, and familiarity with code versioning tools such as GitHub\nGood to have:\nFamiliarity with Microsoft Azure Cloud Services (particularly Azure Web App, Storage and VM), or familiarity with AWS (EC2 containers) or GCP Services.\nExperience with Microservices, Messaging Brokers (e.g., RabbitMQ)\nExperience with fine-tuning reverse proxy engines such as Nginx, Apache HTTPD\nIf you like wild growth and working with happy, enthusiastic over-achievers, youll enjoy your career with us!\nNot the right fit? Let us know youre interested in a future opportunity by clicking Introduce Yourself in the top-right corner of the page or create an account to set up email alerts as new job postings become available that meet your interest!",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Backend', 'Multithreading', 'RDBMS', 'MySQL', 'Performance testing', 'OLAP', 'Scrum', 'MongoDB', 'Apache', 'OLTP']",2025-06-12 13:46:01
Senior Data Engineer - AWS,Blend360 India,6 - 10 years,Not Disclosed,['Hyderabad'],"We are looking for an experienced Senior Data Engineer with a strong foundation in Python, SQL, and Spark , and hands-on expertise in AWS, Databricks . In this role, you will build and maintain scalable data pipelines and architecture to support analytics, data science, and business intelligence initiatives. You ll work closely with cross-functional teams to drive data reliability, quality, and performance.\nResponsibilities:\nDesign, develop, and optimize scalable data pipelines using Databricks in AWS such as Glue, S3, Lambda, EMR, Databricks notebooks, workflows and jobs.\nBuilding data lake in WS Databricks.\nBuild and maintain robust ETL/ELT workflows using Python and SQL to handle structured and semi-structured data.\nDevelop distributed data processing solutions using Apache Spark or PySpark .\nPartner with data scientists and analysts to provide high-quality, accessible, and well-structured data.\nEnsure data quality, governance, security, and compliance across pipelines and data stores.\nMonitor, troubleshoot, and improve the performance of data systems and pipelines.\nParticipate in code reviews and help establish engineering best practices.\nMentor junior data engineers and support their technical development.\n\n\nRequirements\nBachelors or masters degree in computer science, Engineering, or a related field.\n5+ years of hands-on experience in data engineering , with at lea",Industry Type: Industrial Automation,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Automation', 'Version control', 'GIT', 'Workflow', 'Data quality', 'Business intelligence', 'Analytics', 'SQL', 'Python']",2025-06-12 13:46:03
Associate Data Engineer,Amgen Inc,0 - 3 years,Not Disclosed,['Hyderabad'],"What you will do\nIn this vital role We are seeking a Associate Data Engineer to design, build, and maintain scalable data solutions that drive business insights. You will work with large datasets, cloud platforms (AWS preferred), and big data technologies to develop ETL pipelines, ensure data quality, and support data governance initiatives.\nDevelop and maintain data pipelines, ETL/ELT processes, and data integration solutions.\nDesign and implement data models, data dictionaries, and documentation for accuracy and consistency.\nEnsure data security, privacy, and governance standard processes.\nUse Databricks, Apache Spark (PySpark, SparkSQL), AWS, Redshift, for scalable data processing.\nCollaborate with cross-functional teams to understand data needs and deliver actionable insights.\nOptimize data pipeline performance and explore new tools for efficiency.\nFollow best practices in coding, testing, and infrastructure-as-code (CI/CD, version control, automated testing).\n\n\nWhat we expect of you\nWe are all different, yet we all use our unique contributions to serve patients. Strong problem-solving, critical thinking, and communication skills.\nAbility to collaborate effectively in a team setting.\nProficiency in SQL, data analysis tools, and data visualization.\nHands-on experience with big data technologies (Databricks, Apache Spark, AWS, Redshift ).\nExperience with ETL tools, workflow orchestration, and performance tuning for big data.\nBasic Qualifications:\nBachelors degree and 0 to 3 years of experience OR Diploma and 4 to 7 years of experience in Computer science, IT or related field.\nPreferred Qualifications:\nKnowledge of data modeling, warehousing, and graph databases\nExperience with Python, SageMaker, and cloud data platforms.\nAWS Certified Data Engineer or Databricks certification preferred.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'data analysis', 'data modeling', 'data warehousing', 'data visualization', 'Databricks', 'ETL', 'AWS', 'SQL', 'Apache Spark', 'Python']",2025-06-12 13:46:05
Graph Engineer- Data Science,HARMAN,4 - 9 years,Not Disclosed,['Bengaluru'],"Job Description\nIntroduction: Digital Transformation Solutions (DTS)\n.\nExtensive experience in defining, developing, and implementing security software, ideally with a strong embedded firmware development background\nAbout the Role\nThis position offers an opportunity to work in a globally distributed team where you will get a unique opportunity of personal development in a multi-cultural environment. You will also get a challenging environment to develop expertise in the technologies useful in the industry.",,,,"['Computer science', 'Product quality', 'UML', 'XML', 'Relationship', 'Javascript', 'HTML', 'Oracle', 'Automotive', 'Python']",2025-06-12 13:46:07
"Associate Director, Data Science/Software Engineering",ATT Communication Services,10 - 15 years,Not Disclosed,['Bengaluru'],"Associate Director, Data Science/Software Engineering:\nAT&T is one of the leading service providers in the telecommunication sector and propelling it into the data and AI driven era is powered by CDO (Chief Data Office) . CDO is empowering AT&T, through execution, self-service, and as a data and AI center of excellence, to unlock transformative insights and actions that drive value for the company and its customers.\nEmployees in CDO imagine, innovate, and unlock data & AI driven insights and actions that create value for our customers and the enterprise. Part of the work, we govern data collection and use, mitigate for potential bias in machine learning models, and encourage an enterprise culture of responsible AI.\nAT&T s Chief Data Office (CDO) is harnessing data and making AT&T s data assets and ground-breaking AI functionality accessible to employees across the firm. In addition, our talented employees are a significant component that contributes to AT&T s place as the U.S. company with the sixth most AI-related patents. CDO also maintains academic and tech partnerships to cultivate the next generation of experts in statistics and machine learning, statistical computing, data visualization, text mining, time series modelling, data stream and database management, data quality and anomaly detection, data privacy, and more.\nWe are looking for an accomplished and visionary professional for the role of Associate Director, Data Science/Software Engineering to join our team and lead the development of cutting-edge software solutions. This is a hands-on leadership position that requires the fine balance of supervising and leading people while providing significant technical contributions to the projects you will be responsible for. As a key technical leader, you will leverage your expertise in full-stack development, DevOps best practices, Data analysis, AI/ML and Generative AI to lead your team in creating scalable, reliable, and efficient systems.\nThis role demands a strategic thinker and hands-on contributor who can work across multiple teams, drive innovation, and ensure technical excellence. You will be instrumental in shaping the technical roadmap, mentoring teams, and delivering transformative solutions that align with business objectives.\nKey Responsibilities:\nTechnical Leadership:\nDefine and drive the technical vision and architecture for scalable, resilient, and secure full-stack applications utilizing data powered insights.\nLead end-to-end software development projects from concept to deployment and maintenance.\nCollaborate with cross-functional teams to translate business requirements into technical solutions.\nServe as a mentor and technical advisor to engineering teams, fostering a culture of innovation and excellence.\nFull-Stack Development:\nDesign and implement scalable and high-performance web applications using modern front-end and back-end frameworks (e.g., React, Angular, Node.js, Python, Java).\nDevelop modular and reusable APIs (RESTful or GraphQL) with an emphasis on maintainability and performance.\nEnsure seamless integration of front-end and back-end systems while maintaining best practices for UI/UX design.\nOptimize database structures and queries for both relational (e.g., MySQL, PostgreSQL) and non-relational (e.g., MongoDB, DynamoDB) databases.\nDevOps and Automation:\nArchitect and implement CI/CD pipelines to streamline build, test, and deployment processes.\nEnsure seamless deployment and scalability of applications through containerization tools (e.g., Docker) and orchestration platforms (e.g., Kubernetes).\nLeverage infrastructure-as-code solutions (e.g., Terraform, Ansible) to automate infrastructure provisioning and management.\nMonitor application performance, troubleshoot issues, and ensure high availability through tools like Prometheus, Grafana, or New Relic.\nShell Scripting and Automation:\nDevelop and maintain shell scripts to automate routine tasks, system monitoring, and application deployments.\nDebug and troubleshoot production issues using scripting techniques to ensure minimal downtime.\nEnhance system efficiency by automating log analysis, error detection, and reporting.\nStrategic Contribution:\nCollaborate with stakeholders to align technical priorities with business goals.\nEvaluate emerging technologies and tools to recommend and implement solutions that advance the organization s technical capabilities.\nEstablish and enforce software engineering best practices, ensuring robust security, scalability, and maintainability.\nQualifications:\nEducation:\nBachelor s or Master s degree in Computer Science, Software Engineering, or a related field. A Ph.D. is a plus.\nExperience:\n13+ years of experience in software engineering, including hands-on experience with full-stack development and DevOps practices.\nProven track record of delivering large-scale, high-impact software solutions in a leadership capacity.\nTechnical Expertise:\nAdvanced proficiency in front-end frameworks (React, Angular, or Vue.js) and back-end technologies (Node.js, Python, Java, Go, etc.).\nStrong experience with DevOps tools (Jenkins, GitLab CI/CD, Docker, Kubernetes).\nDeep understanding of cloud platforms (AWS, Azure, GCP), including architecture and deployment strategies.\nSolid grasp of database technologies (SQL and NoSQL) and optimization techniques.\nProficiency in writing, debugging, and maintaining shell scripts for automation and system monitoring.\nStrong knowledge of microservices architecture, API gateways, and distributed systems.\nSoft Skills:\nExceptional problem-solving and critical-thinking abilities.\nStrong leadership and mentoring skills, with the ability to inspire and guide teams.\nExcellent communication skills, both written and verbal, to collaborate effectively with technical and non-technical stakeholders.\nStrategic mindset, capable of balancing technical depth with business impact.\nPreferred Qualifications:\nExperience with serverless computing frameworks (e.g., AWS Lambda).\nCertifications in cloud platforms (e.g., AWS Certified Solutions Architect, Azure DevOps Engineer Expert).\nKnowledge of security best practices in software development and DevOps.\n#DataEngineering\nLocation:\nIND:KA:Bengaluru / Innovator Building, Itpb, Whitefield Rd - Adm: Intl Tech Park, Innovator Bldg\nJob ID R-66889 Date posted 05/14/2025",Industry Type: Telecom / ISP,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Automation', 'Data analysis', 'Front end', 'Postgresql', 'MySQL', 'Shell scripting', 'Telecommunication', 'SQL', 'Python']",2025-06-12 13:46:09
Associate Data Engineer,Amgen Inc,0 - 2 years,Not Disclosed,['Hyderabad'],"Role Description:\nWe are looking for an Associate Data Engineer with deep expertise in writing data pipelines to build scalable, high-performance data solutions. The ideal candidate will be responsible for developing, optimizing and maintaining complex data pipelines, integration frameworks, and metadata-driven architectures that enable seamless access and analytics. This role prefers deep understanding of the big data processing, distributed computing, data modeling, and governance frameworks to support self-service analytics, AI-driven insights, and enterprise-wide data management.\nRoles & Responsibilities:\nData Engineer who owns development of complex ETL/ELT data pipelines to process large-scale datasets\nContribute to the design, development, and implementation of data pipelines, ETL/ELT processes, and data integration solutions\nEnsuring data integrity, accuracy, and consistency through rigorous quality checks and monitoring\nExploring and implementing new tools and technologies to enhance ETL platform and performance of the pipelines\nProactively identify and implement opportunities to automate tasks and develop reusable frameworks\nEager to understand the biotech/pharma domains & build highly efficient data pipelines to migrate and deploy complex data across systems\nWork in an Agile and Scaled Agile (SAFe) environment, collaborating with cross-functional teams, product owners, and Scrum Masters to deliver incremental value\nUse JIRA, Confluence, and Agile DevOps tools to manage sprints, backlogs, and user stories.\nSupport continuous improvement, test automation, and DevOps practices in the data engineering lifecycle\nCollaborate and communicate effectively with the product teams, with cross-functional teams to understand business requirements and translate them into technical solutions\nMust-Have Skills:\nExperience in Data Engineering with a focus on Databricks, AWS, Python, SQL, and Scaled Agile methodologies\nProficiency & Strong understanding of data processing and transformation of big data frameworks (Databricks, Apache Spark, Delta Lake, and distributed computing concepts)\nStrong understanding of AWS services and can demonstrate the same\nAbility to quickly learn, adapt and apply new technologies\nStrong problem-solving and analytical skills\nExcellent communication and teamwork skills\nExperience with Scaled Agile Framework (SAFe), Agile delivery, and DevOps practices\nGood-to-Have Skills:\nData Engineering experience in Biotechnology or pharma industry\nExposure to APIs, full stack development\nExperienced with SQL/NOSQL database, vector database for large language models\nExperienced with data modeling and performance tuning for both OLAP and OLTP databases\nExperienced with software engineering best-practices, including but not limited to version control (Git, Subversion, etc.), CI/CD (Jenkins, Maven etc.), automated unit testing, and Dev Ops\nEducation and Professional Certifications\nBachelors degree and 2 to 5 + years of Computer Science, IT or related field experience\nOR\nMasters degree and 1 to 4 + years of Computer Science, IT or related field experience\nAWS Certified Data Engineer preferred\nDatabricks Certificate preferred\nScaled Agile SAFe certification preferred\nSoft Skills:\nExcellent analytical and troubleshooting skills.\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nTeam-oriented, with a focus on achieving team goals.\nAbility to learn quickly, be organized and detail oriented.\nStrong presentation and public speaking skills.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Maven', 'test automation', 'data engineering lifecycle', 'Scaled Agile methodologies', 'JIRA', 'SQL', 'Apache Spark', 'Jenkins', 'Agile DevOps tools', 'ETL platform', 'Confluence', 'Scaled Agile', 'Delta Lake', 'Agile', 'Databricks', 'AWS', 'Python']",2025-06-12 13:46:12
Snowflake Data Engineer,Tredence,3 - 8 years,Not Disclosed,['Bengaluru'],"Role & responsibilities\n\nDesign, build, and maintain scalable data pipelines using DBT and Airflow.\nDevelop and optimize SQL queries and data models in Snowflake.\nImplement ETL/ELT workflows, ensuring data quality, performance, and reliability.\nWork with Python for data processing, automation, and integration tasks.\nHandle JSON data structures for data ingestion, transformation, and APIs.\nLeverage AWS services (e.g., S3, Lambda, Glue, Redshift) for cloud-based data solutions. Collaborate with data analysts, engineers, and business teams to deliver high-quality data products.",,,,"['Snowflake', 'DBT', 'SQL']",2025-06-12 13:46:14
Urgent hiring For Cloud Data Engineer,Wowjobs,7 - 10 years,30-45 Lacs P.A.,"['Hyderabad', 'Chennai', 'Bengaluru']","Role & responsibilities\n*  Design, Build, and Maintain ETL Pipelines: Develop robust, scalable, and efficient ETL workflows to ingest, transform, and load data into distributed data products within the Data Mesh architecture.\n*   Data Transformation with dbt: Use dbt to build modular, reusable transformation workflows that align with the principles of Data Products.\n*   Cloud Expertise: Leverage Google Cloud Platform (GCP) services such as BigQuery, Cloud Storage, Pub/Sub, and Dataflow to implement highly scalable data solutions.\n*   Data Quality & Governance: Enforce strict data quality standards by implementing validation checks, anomaly detection mechanisms, and monitoring frameworks.\n*   Performance Optimization: Continuously optimize ETL pipelines for speed, scalability, and cost efficiency.\n*   Collaboration & Ownership: Work closely with data product owners, BI developers, and stakeholders to understand requirements and deliver on expectations. Take full ownership of your deliverables.\n*   Documentation & Standards: Maintain detailed documentation of ETL workflows, enforce coding standards, and adhere to best practices in data engineering.\n*   Troubleshooting & Issue Resolution: Proactively identify bottlenecks or issues in pipelines and resolve them quickly with minimal disruption.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Python coding', 'Cloud data engineer', 'ETL Workflow']",2025-06-12 13:46:16
AM Client Workflows & Ref Data - Associate - Software Engineering,Goldman Sachs,2 - 5 years,Not Disclosed,['Bengaluru'],"Work with a global team of highly motivated platform engineers and software developers building integrated architectures for secure, scalable infrastructure services serving a diverse set of use cases.\nPartner with colleagues from across technology and risk to ensure an outstanding platform is delivered.\nHelp to provide frictionless integration with the firm s runtime, deployment and SDLC technologies.\nCollaborate on feature design and problem solving.\nHelp to ensure reliability, define, measure, and meet service level objectives.\nQuality coding & integration, testing, release, and demise of software products supporting AWM functions.\nEngage in quality assurance and production troubleshooting.\nHelp to communicate and promote best practices for software engineering across the Asset Management tech stack.\nBasic Qualifications\nA strong grounding in software engineering concepts and implementation of architecture design patterns.\nA good understanding of multiple aspects of software development in microservices architecture, full stack development experience, Identity / access management and technology risk.\nSound SDLC and practices and tooling experience - version control, CI/CD and configuration management tools.\nAbility to communicate technical concepts effectively, both written and orally, as we'll as interpersonal skills required to collaborate effectively with colleagues across diverse technology teams.\nExperience meeting demands for high availability and scalable system requirements.\nAbility to reason about performance, security, and process interactions in complex distributed systems.\nAbility to understand and effectively debug both new and existing software.\nExperience with metrics and monitoring tooling, including the ability to use metrics to rationally derive system health and availability information.\nExperience in auditing and supporting software based on sound SRE principles.\nPreferred Qualifications\n3+ Years of Experience using and/or supporting Java based frameworks & SQL / NOSQL data stores.\nExperience with deploying software to containerized environments - Kubernetes/Docker.\nScripting skills using Python, Shell or bash.\nExperience with Terraform or similar infrastructure-as-code platforms.\nExperience building services using public cloud providers such as AWS, Azure or GCP.",Industry Type: Banking,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Manager Quality Assurance', 'Coding', 'Configuration management', 'Investment banking', 'Asset management', 'Troubleshooting', 'SDLC', 'Monitoring', 'SQL', 'Python']",2025-06-12 13:46:18
Director - Data Engineering,Blend360 India,10 - 15 years,Not Disclosed,['Bengaluru'],"We are seeking a strategic Director of Data & AI Engineering to lead the growth and evolution of our data engineering function. This role will play a pivotal part in designing scalable platforms, enabling advanced AI and ML applications, and building a world-class engineering team. The ideal candidate is both a seasoned technical leader and a visionary strategist who thrives at the intersection of innovation, execution, and impact. You will collaborate with internal and client teams to develop systems and infrastructure that power intelligent products and data-driven decision-making. You will also build and mentor a high-performing team of Data and AI Engineers, foster a modern data culture, and champion best practices for scalable architecture, AI integration, and engineering excellence.\nResponsibilities:\nSolution Design & Engineering Leadership\nArchitect and build scalable, high-performance data and AI pipelines using tools such as Spark, PySpark, SQL, Python, DBT, Airflow, and cloud-native platforms (AWS, GCP, or Azure).\nLead the design of hybrid/on-prem data platforms, incorporating security, governance, and performance optimization.\nStrategic Client & Stakeholder Engagement\nServe as a trusted technical advisor to internal and external stakeholders.\nTranslate complex business needs into practical engineering solutions and oversee the end-to-end delivery lifecycle.\nAI-Driven Productivity & Innovation\nIntroduce and scale AI-driven tools and practices to accelerate development and enhance data quality, resilience, and maintainability.\nChampion the adoption of generative AI and foundation models to enable intelligent automation and insight generation.\nGrowth & Team Leadership\nBuild, lead, and inspire a diverse team of Data Engineers, ML Engineers, and AI Specialists.\nSet a clear vision and goals, provide mentorship, and cultivate a strong engineering culture and standards.\nPlatform and Data Strategy\nLead initiatives to modernize data infrastructure, improve data discoverability, and support real-time analytics and experimentation.\nCollaborate cross-functionally to shape product data strategies and influence the overall AI roadmap.\nPresales & Business Development Support\nPartner with sales and solution teams to craft compelling proposals, technical solutions, and client presentations.\nRepresent the engineering function in client discussions, workshops, and RFP responses to articulate value and differentiation.\nSupport opportunity scoping, estimation, and roadmap planning for prospective engagements.\n\n\n10+ years of experience in data engineering, AI/ML engineering, or product/platform engineering.\nProven track record of leading high-performing teams and managing senior engineers and managers.\nBachelor s or masters degre",Industry Type: Industrial Automation,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Automation', 'GCP', 'Business Development Manager', 'Social media', 'Data quality', 'RFP', 'Analytics', 'SQL', 'Python']",2025-06-12 13:46:20
Sr Data Engineer - Fully Remote & Immediate Opportunity,Zealogics.com,10 - 15 years,Not Disclosed,[],"10 yrs of exp working in cloud-native data (Azure Preferred),Databricks, SQL,PySpark, migrating from Hive Metastore to Unity Catalog, Unity Catalog, implementing Row-Level Security (RLS), metadata-driven ETL design patterns,Databricks certifications",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'Azure', 'Metadata', 'Data Bricks', 'Unity Catalog', 'ETL design', 'SQL']",2025-06-12 13:46:22
Specialist Data/AI Engineering,ATT Communication Services,4 - 9 years,Not Disclosed,['Bengaluru'],"Key Roles and Responsibilities\nDevelop, enhance, and support assigned applications, ensuring seamless functionality and high performance.\nProvide subject matter expertise, acting as a go-to resource for application-specific knowledge.\nCollaborate and communicate effectively with team members, stakeholders, and end-users.\nTroubleshoot issues, monitor performance, and optimize processes for continuous improvement.\nStay current with industry trends and share best practices with the team.\nAdhere to company policies, procedures, and security requirements while maintaining compliance standards.\nFlexible with shifts and occasional weekend support.\nKey Competencies\nFull life-cycle experience on enterprise software development projects.\nExperience in relational databases/ data marts/data warehouses and complex SQL programming.\nExtensive experience in ETL, shell or python scripting, data modelling, analysis, and preparation\nExperience in Unix/Linux system, files systems, shell scripting.\nGood to have knowledge on any cloud platforms like Azure, Databricks etc.\nGood to have experience in BI Reporting tools Power BI\nGood problem-solving and analytical skills used to resolve technical problems.\nMust possess a good understanding of Compliance and Security Standards (preferably US standards).\nAbility to work independently but must be a team player. Should be able to drive business decisions and take ownership of their work.\nExperience in presentation design, development, delivery, and good communication skills to present analytical results and recommendations for action-oriented data driven decisions and associated operational and financial impacts.\nRequired/Desired Skills\nApplication: Databricks\nLanguages: Python, Shell Scripting, PLSQL, SQL\nCloud Technologies: Azure.\nTools: Jupyter Notebooks, SQL Developer, Postman, IDE.\nDatabase: Oracle, Snowflake and SQL Server.\nDevops: Jenkins, Kubernetes and Docker.\nBachelor s degree in computer science, Information Systems or related field.\n4+ years of experience in working in Engineering or Development roles with Compliance Standards\n4+ years of experience building high transaction defensive web applications and Python applications\n2+ years of experience in cloud technologies: AWS, Azure, OpenStack, Ansible, Chef or Terraform\n2+ years of experience in creating application for container services Docker, Kubernetes,\n2+ years of experience in build and CICD technologies: GitHub, Maven, Jenkins, Nexus or Sonar\nProficiency in Unix/Linux command line\n1+ years of experience in building applications using Large Language Models.\n1+ years of experience in building intelligent automation using Power Automate\n1+ years of experience in using Agentic framework\n1+ years of experience in building accurate prompts for AI tools.\nExpert knowledge and experience working with asynchronous message processing, stream processing and event driven computing.\nExperience working within Agile/Scrum/Kanban development team\nFamiliarity with HTML5, JavaScript frameworks, and CSS3\nCertified in Java, Spring or Azure technologies\nExcellent written and verbal communication skills with demonstrated ability to present complex technical information in a clear manner to peers, developers, and senior leaders\nEducation & Qualifications\nUniversity Degree in Computer Science and/or Analytics\nMinimum Experience required: 6-9 years in relational database design & development, ETL development, GenAI\nAdditional Details\nShift timing (if any): 12.30 to 9.30 IST(Bangalore)\nWork mode: Hybrid (3 days mandatory in office)\nLocation: Bangalore\n#DataPlatform\n#DataScience\nJob ID R-61633 Date posted 06/03/2025",Industry Type: Telecom / ISP,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Unix', 'Computer science', 'Automation', 'Linux', 'Database design', 'Shell scripting', 'Javascript', 'Design development', 'SQL', 'Python']",2025-06-12 13:46:25
Senior Data Engineer,Binary Infoways,5 - 9 years,12-19.2 Lacs P.A.,['Hyderabad'],"Responsibilities:\n* Design, develop & maintain data pipelines using Airflow, Python & SQL.\n* Optimize performance through Spark & Splunk analytics.\n* Collaborate with cross-functional teams on big data initiatives.\n* AWS",Industry Type: BPM / BPO,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Airflow', 'Big Data Technologies', 'ETL', 'AWS', 'Python', 'Glue', 'Snowflake', 'Spark', 'Splunk', 'SQL']",2025-06-12 13:46:27
Data Engineer-Having Stratup-Mid-Size company Exp.@ Bangalore_Urgent,"As a leader in this space, we deliver wo...",8 - 13 years,Not Disclosed,['Bengaluru'],"Data Engineer\n\nLocation: Bangalore - Onsite\nExperience: 8 - 15 years\nType: Full-time\n\nRole Overview\n\nWe are seeking an experienced Data Engineer to build and maintain scalable, high-performance data pipelines and infrastructure for our next-generation data platform. The platform ingests and processes real-time and historical data from diverse industrial sources such as airport systems, sensors, cameras, and APIs. You will work closely with AI/ML engineers, data scientists, and DevOps to enable reliable analytics, forecasting, and anomaly detection use cases.\nKey Responsibilities\nDesign and implement real-time (Kafka, Spark/Flink) and batch (Airflow, Spark) pipelines for high-throughput data ingestion, processing, and transformation.\nDevelop data models and manage data lakes and warehouses (Delta Lake, Iceberg, etc) to support both analytical and ML workloads.\nIntegrate data from diverse sources: IoT sensors, databases (SQL/NoSQL), REST APIs, and flat files.\nEnsure pipeline scalability, observability, and data quality through monitoring, alerting, validation, and lineage tracking.\nCollaborate with AI/ML teams to provision clean and ML-ready datasets for training and inference.\nDeploy, optimize, and manage pipelines and data infrastructure across on-premise and hybrid environments.\nParticipate in architectural decisions to ensure resilient, cost-effective, and secure data flows.\nContribute to infrastructure-as-code and automation for data deployment using Terraform, Ansible, or similar tools.\n\n\nQualifications & Required Skills\n\nBachelors or Master’s in Computer Science, Engineering, or related field.\n6+ years in data engineering roles, with at least 2 years handling real-time or streaming pipelines.\nStrong programming skills in Python/Java and SQL.\nExperience with Apache Kafka, Apache Spark, or Apache Flink for real-time and batch processing.\nHands-on with Airflow, dbt, or other orchestration tools.\nFamiliarity with data modeling (OLAP/OLTP), schema evolution, and format handling (Parquet, Avro, ORC).\nExperience with hybrid/on-prem and cloud platforms (AWS/GCP/Azure) deployments.\nProficient in working with data lakes/warehouses like Snowflake, BigQuery, Redshift, or Delta Lake.\nKnowledge of DevOps practices, Docker/Kubernetes, Terraform or Ansible.\nExposure to data observability, data cataloging, and quality tools (e.g., Great Expectations, OpenMetadata).\nGood-to-Have\nExperience with time-series databases (e.g., InfluxDB, TimescaleDB) and sensor data.\nPrior experience in domains such as aviation, manufacturing, or logistics is a plus.\n\nRole & responsibilities\n\n\nPreferred candidate profile",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['aviation', 'Data Modeling', 'Python', 'OLAP', 'Cloud', 'ORC', 'logistics', 'Avro', 'Terraform', 'Snowflake', 'manufacturing', 'AWS', 'Parquet', 'Java', 'Azure', 'BigQuery', 'Data', 'Redshift', 'SQL', 'TimescaleDB', 'GCP', 'InfluxDB', 'dbt', 'Ansible', 'OLTP', 'Kubernetes']",2025-06-12 13:46:30
Data Engineer (Azure),Neoware Technology Solutions,4 - 10 years,Not Disclosed,"['Chennai', 'Bengaluru']","Data Engineer (Azure) - Neoware Technology Solutions Private Limited Data Engineer (Azure)\nRequirements\n4 - 10 years of hands-on experience in designing, developing and implementing data engineering solutions.\nStrong SQL development skills, including performance tuning and query optimization.\nGood understanding of data concepts.\nProficiency in Python and a solid understanding of programming concepts.\nHands-on experience with PySpark or Spark Scala for building data pipelines.\nEnsure data consistency and address ambiguities or inconsistencies across datasets.\nUnderstanding of streaming data pipelines for near real-time analytics.\nExperience with Azure services including Data Factory, Functions, Databricks, Synapse Analytics, Event Hub, Stream Analytics and Data Lake Storage.\nFamiliarity with at least one NoSQL database.\nKnowledge of modern data architecture patterns and industry trends in data engineering.\nUnderstanding of data governance concepts for data platforms and analytical solutions.\nExperience with Git for managing version control for source code.\nExperience with DevOps processes, including experience implementing CI/CD pipelines for data engineering solutions.\nStrong analytical and problem-solving skills.\nExcellent communication and teamwork skills.\nResponsibilities\nAzure Certifications related to Data Engineering are highly preferred.\nExperience with Azure Kubernetes Service (AKS), Container Apps and API Management.\nStrong understanding and experience with BI/visualization tools like Power BI.\nChennai, Bangalore\nFull time\n4+ years\nOther positions\nChennai / Bangalore / Mumbai\n3+ years\nPrincipal Architect (Data and Cloud) Development",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Performance tuning', 'Version control', 'GIT', 'query optimization', 'NoSQL', 'Analytical', 'SCALA', 'Analytics', 'Python', 'Data architecture']",2025-06-12 13:46:33
Azure Data Engineer/Lead/Architect (5 - 20 Years) (Pan India Location),Allegis Group,5 - 10 years,Not Disclosed,[],"Azure Data Engineer/Lead/Architect (5 - 20 Years) (Pan India Location)\nJob Location : Hyderabad / Bangalore / Chennai / Kolkata / Noida/ Gurgaon / Pune / Indore / Mumbai\n\n\n5 -20 years of relevant hands on development experience. And 4+ years as Azure Data Engineering role\nProficient in Azure technologies like ADB, ADF, SQL(capability of writing complex SQL queries), ADB, PySpark, Python, Synapse, Delta Tables, Unity Catalog\nHands on in Python, PySpark or Spark SQL\nHands on in Azure Analytics and DevOps\nTaking part in Proof of Concepts (POCs) and pilot solutions preparation\nAbility to conduct data profiling, cataloguing, and mapping for technical design and construction of technical data flows\nExperience in business processing mapping of data and analytics solutions",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Analytics', 'Azure Data Engineering', 'Azure Databricks', 'Devops', 'Python', 'Azure Data Factory', 'Pyspark', 'Azure', 'Adb']",2025-06-12 13:46:35
Senior Data Engineer,Conviction HR,8 - 10 years,Not Disclosed,"['Kolkata', 'Hyderabad', 'Pune( Malad )']","Must have -Azure Data Factory (Mandatory). Azure Databricks, Pyspark and Python and advance SQL Azure eco-system. 1) Advanced SQL Skills. 2)Data Analysis. 3) Data Models. 4) Python (Desired). 5) Automation - Experience required : 8 to 10 years.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Data Factory', 'Data Engineering', 'Python', 'Azure Databricks', 'Data Modeling', 'Data Bricks', 'SQL']",2025-06-12 13:46:38
Gcp Data Engineer,Royal Cyber,9 - 14 years,Not Disclosed,[],"Minimum 7+ years in data engineering with 5+ years of hands-on experience on GCP.\nProven track record with tools and services like BigQuery, Cloud Composer (Apache Airflow), Cloud Functions, Pub/Sub, Cloud Storage, Dataflow, and IAM/VPC.\nDemonstrated expertise in Apache Spark (batch and streaming), PySpark, and building scalable API integrations.\nAdvanced Airflow skills including custom operators, dynamic DAGs, and workflow performance tuning.\nCertifications\nGoogle Cloud Professional Data Engineer certification preferred.\nKey Skills\nMandatory Technical Skills\nAdvanced Python (PySpark, Pandas, pytest) for automation and data pipelines.\nStrong SQL with experience in window functions, CTEs, partitioning, and optimization.\nProficiency in GCP services including BigQuery, Dataflow, Cloud Composer, Cloud Functions, and Cloud Storage.\nHands-on with Apache Airflow, including dynamic DAGs, retries, and SLA enforcement.\nExpertise in API data ingestion, Postman collections, and REST/GraphQL integration workflows.\nFamiliarity with CI/CD workflows using Git, Jenkins, or Bitbucket.\nExperience with infrastructure security and governance using IAM and VPC.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Part Time, Temporary/Contractual","['GCP', 'Bigquery', 'Google Cloud Platforms', 'Cloud Storage', 'Data Flow']",2025-06-12 13:46:40
"Data Engineer - Snowflake, Azure Data Factory (ADF)",Suzva Software Technologies,0 - 1 years,Not Disclosed,['Mumbai'],"We are seeking an experienced Data Engineer to join our team for a 6-month contract assignment. The ideal candidate will work on data warehouse development, ETL pipelines, and analytics enablement using Snowflake, Azure Data Factory (ADF), dbt, and other tools.\n\nThis role requires strong hands-on experience with data integration platforms, documentation, and pipeline optimizationespecially in cloud environments such as Azure and AWS.\n\n#KeyResponsibilities\nBuild and maintain ETL pipelines using Fivetran, dbt, and Azure Data Factory\n\nMonitor and support production ETL jobs\n\nDevelop and maintain data lineage documentation for all systems\n\nDesign data mapping and documentation to aid QA/UAT testing\n\nEvaluate and recommend modern data integration tools\n\nOptimize shared data workflows and batch schedules\n\nCollaborate with Data Quality Analysts to ensure accuracy and integrity of data flows\n\nParticipate in performance tuning and improvement recommendations\n\nSupport BI/MDM initiatives including Data Vault and Data Lakes\n\n#RequiredSkills\n7+ years of experience in data engineering roles\n\nStrong command of SQL, with 5+ years of hands-on development\n\nDeep experience with Snowflake, Azure Data Factory, dbt\n\nStrong background with ETL tools (Informatica, Talend, ADF, dbt, etc.)\n\nBachelor's in CS, Engineering, Math, or related field\n\nExperience in healthcare domain (working with PHI/PII data)\n\nFamiliarity with scripting/programming (Python, Perl, Java, Linux-based environments)\n\nExcellent communication and documentation skills\n\nExperience with BI tools like Power BI, Cognos, etc.\n\nOrganized, self-starter with strong time-management and critical thinking abilities\n\n#NiceToHave\nExperience with Data Lakes and Data Vaults\n\nQA & UAT alignment with clear development documentation\nMulti-cloud experience (especially Azure, AWS)",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Java', 'Azure', 'Power BI', 'UAT', 'Perl', 'QA', 'Azure Data Factory', 'Linux', 'Cognos', 'Snowflake', 'ETL', 'AWS', 'Python']",2025-06-12 13:46:43
"Data Engineer( Python, AWS, Databricks, EKS, Airflow)",Banking,5 - 9 years,Not Disclosed,['Bengaluru'],"Exprence 5-8 Years\nLocation - Bangalore\nMode C2H\n\nHands on data engineering experience.\nHands on experience with Python programming\nHands-on Experience with AWS & EKS\nWorking knowledge of Unix, Databases, SQL\nWorking Knowledge on Databricks\nWorking Knowledge on Airflow and DBT",Industry Type: Investment Banking / Venture Capital / Private Equity,Department: Engineering - Software & QA,"Employment Type: Full Time, Temporary/Contractual","['Airflow', 'Data Engineering', 'AWS', 'Python', 'SQL', 'Databricks', 'Eks']",2025-06-12 13:46:45
Aws Data Engineer,Hiring for Leading MNC Company!!,8 - 13 years,Not Disclosed,['Bengaluru'],"Warm Greetings from SP Staffing!!\n\nRole:AWS Data Engineer\nExperience Required :8 to 15 yrs\nWork Location :Bangalore\n\nRequired Skills,\n\nTechnical knowledge of data engineering solutions and practices. Implementation of data pipelines using tools like EMR, AWS Glue, AWS Lambda, AWS Step Functions, API Gateway, Athena\nProficient in Python and Spark, with a focus on ETL data processing and data engineering practices.\n\nInterested candidates can send resumes to nandhini.spstaffing@gmail.com",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['ETL', 'AWS', 'Pyspark', 'python', 'EMR', 'Aws Glue', 'Aws Emr', 'AWS Data Engineer', 'Aws Lambda', 'Lakehouse', 'spark', 'Data Engineer', 'Athena', 'gateway']",2025-06-12 13:46:48
Software Engineer (Java + Big Data),Impetus Technologies,5 - 7 years,Not Disclosed,['Chennai'],"Job: Software Developer (Java + Big Data)\nLocation: Indore\nYears of experience: 5-7 years\n\nRequisition Description\n1. Problem solving and analytical skills\n2. Good verbal and written communication skills\n\nRoles and Responsibilities\n\n1. Design and develop high performance, scale-able applications with Java + Bigdata  as minimum required skill .\nJava, Microservices , Spring boot, API ,Bigdata-Hive, Spark, Pyspark\n2. Build and maintain efficient data pipelines to process large volumes of structured and unstructured data.\n3. Develop micro-services, API and distributed systems\n4. Worked on Spark, HDFS, CEPh, Solr/Elastic search, Kafka, Deltalake\n5. Mentor and Guide junior members",Industry Type: IT Services & Consulting,Department: Project & Program Management,"Employment Type: Full Time, Permanent","['Java', 'Hive', 'Big Data', 'Spring Boot', 'Microservices', 'Hdfs', 'Spark Streaming']",2025-06-12 13:46:50
"AI/ML Engineer (Specializing in NLP/ML, Large Data Processing,",Synechron,8 - 10 years,Not Disclosed,"['Pune', 'Hinjewadi']","job requisition idJR1027361\n\nJob Summary\nSynechron seeks a highly skilled AI/ML Engineer specializing in Natural Language Processing (NLP), Large Language Models (LLMs), Foundation Models (FMs), and Generative AI (GenAI). The successful candidate will design, develop, and deploy advanced AI solutions, contributing to innovative projects that transform monolithic systems into scalable microservices integrated with leading cloud platforms such as Azure, Amazon Bedrock, and Google Gemini. This role plays a critical part in advancing Synechrons capabilities in cutting-edge AI technologies, enabling impactful business insights and product innovations.\n\nSoftware\n\nRequired Proficiency:\nPython (core librariesTensorFlow, PyTorch, Hugging Face transformers, etc.)\nCloud platformsAzure, AWS, Google Cloud (familiarity with AI/ML services)\nContainerizationDocker, Kubernetes\nVersion controlGit\nData management toolsSQL, NoSQL databases (e.g., MongoDB)\nModel deployment and MLOps toolsMLflow, CI/CD pipelines, monitoring tools\nPreferred\n\nSkills:\nExperience with cloud-native AI frameworks and SDKs\nFamiliarity with AutoML tools\nAdditional programming languages (e.g., Java, Scala)\nOverall Responsibilities\nDesign, develop, and optimize NLP models, including advanced LLMs and Foundation Models, for diverse business use cases.\nLead the development of large data pipelines for training, fine-tuning, and deploying models on big data platforms.\nArchitect, implement, and maintain scalable AI solutions in line with MLOps best practices.\nTransition legacy monolithic AI systems into modular, microservices-based architectures for scalability and maintainability.\nBuild end-to-end AI applications from scratch, including data ingestion, model training, deployment, and integration.\nImplement retrieval-augmented generation techniques for enhanced context understanding and response accuracy.\nConduct thorough testing, validation, and debugging of AI/ML models and pipelines.\nCollaborate with cross-functional teams to embed AI capabilities into customer-facing and enterprise products.\nSupport ongoing maintenance, monitoring, and scaling of deployed AI systems.\nDocument system designs, workflows, and deployment procedures for compliance and knowledge sharing.\nPerformance Outcomes:\nProduction-ready AI solutions delivering high accuracy and efficiency.\nRobust data pipelines supporting training and inference at scale.\nSeamless integration of AI models with cloud infrastructure.\nEffective collaboration leading to innovative AI product deployment.\nTechnical Skills (By Category)\n\nProgramming Languages:\nEssential: Python (TensorFlow, PyTorch, Hugging Face, etc.)\nPreferred: Java, Scala\nDatabases/Data Management:\nSQL (PostgreSQL, MySQL), NoSQL (MongoDB, DynamoDB)\nCloud Technologies:\nAzure AI, AWS SageMaker, Bedrock, Google Cloud Vertex AI, Gemini\nFrameworks and Libraries:\nTransformers, Keras, scikit-learn, XGBoost, Hugging Face engines\nDevelopment Tools & Methodologies:\nDocker, Kubernetes, Git, CI/CD pipelines (Jenkins, Azure DevOps)\nSecurity & Compliance:\nKnowledge of data security standards and privacy policies (GDPR, HIPAA as applicable)\nExperience\n8 to 10 years of hands-on experience in AI/ML development, especially NLP and Generative AI.\nDemonstrated expertise in designing, fine-tuning, and deploying LLMs, FMs, and GenAI solutions.\nProven ability to develop end-to-end AI applications within cloud environments.\nExperience transforming monolithic architectures into scalable microservices.\nStrong background with big data processing pipelines.\nPrior experience working with cloud-native AI tools and frameworks.\nIndustry experience in finance, healthcare, or technology sectors is advantageous.\nAlternative Experience:\nCandidates with extensive research or academic experience in AI/ML, especially in NLP and large-scale data processing, are eligible if they have practical deployment experience.\n\nDay-to-Day Activities\nDevelop and optimize sophisticated NLP/GenAI models fulfilling business requirements.\nLead data pipeline construction for training and inference workflows.\nCollaborate with data engineers, architects, and product teams to ensure scalable deployment.\nConduct model testing, validation, and performance tuning.\nImplement and monitor model deployment pipelines, troubleshoot issues, and improve system robustness.\nDocument models, pipelines, and deployment procedures for audit and knowledge sharing.\nStay updated with emerging AI/ML trends, integrating best practices into projects.\nPresent findings, progress updates, and technical guidance to stakeholders.\nQualifications\nBachelors degree in Computer Science, Data Science, or related field; Masters or PhD preferred.\nCertifications in AI/ML, Cloud (e.g., AWS, Azure, Google Cloud), or Data Engineering are a plus.\nProven professional experience with advanced NLP and Generative AI solutions.\nCommitment to continuous learning to keep pace with rapidly evolving AI technologies.\nProfessional Competencies\nStrong analytical and problem-solving capabilities.\nExcellent communication skills, capable of translating complex technical concepts.\nCollaborative team player with experience working across global teams.\nAdaptability to rapidly changing project scopes and emerging AI trends.\nInnovation-driven mindset with a focus on delivering impactful solutions.\nTime management skills to prioritize and manage multiple projects effectively.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'data management', 'data processing', 'pipeline', 'big data', 'continuous integration', 'kubernetes', 'deploying models', 'natural language processing', 'ci/cd', 'fms', 'artificial intelligence', 'docker', 'sql', 'microservices', 'tensorflow', 'java', 'pytorch', 'jenkins', 'keras', 'aws']",2025-06-12 13:46:52
Data Engineer - R&D Data Catalyst Team,Amgen Inc,1 - 3 years,Not Disclosed,['Hyderabad'],"What you will do\nIn this vital role, you will be responsible for the end-to-end development of an enterprise analytics and data mastering solution using Databricks and Power BI. This role requires expertise in both data architecture and analytics, with the ability to create scalable, reliable, and impactful enterprise solutions that research cohort-building and advanced research pipeline. The ideal candidate will have experience creating and surfacing large unified repositories of human data, based on integrations from multiple repositories and solutions, and be extraordinarily skilled with data analysis and profiling.\nYou will collaborate closely with key customers, product team members, and related IT teams, to design and implement data models, integrate data from various sources, and ensure best practices for data governance and security. The ideal candidate will have a good background in data warehousing, ETL, Databricks, Power BI, and enterprise data mastering.\nDesign and build scalable enterprise analytics solutions using Databricks, Power BI, and other modern data tools.\nLeverage data virtualization, ETL, and semantic layers to balance need for unification, performance, and data transformation with goal to reduce data proliferation\nBreak down features into work that aligns with the architectural direction runway\nParticipate hands-on in pilots and proofs-of-concept for new patterns\nCreate robust documentation from data analysis and profiling, and proposed designs and data logic\nDevelop advanced sql queries to profile, and unify data\nDevelop data processing code in sql, along with semantic views to prepare data for reporting\nDevelop PowerBI Models and reporting packages\nDesign robust data models, and processing layers, that support both analytical processing and operational reporting needs.\nDesign and develop solutions based on best practices for data governance, security, and compliance within Databricks and Power BI environments.\nEnsure the integration of data systems with other enterprise applications, creating seamless data flows across platforms.\nDevelop and maintain Power BI solutions, ensuring data models and reports are optimized for performance and scalability.\nCollaborate with key customers to define data requirements, functional specifications, and project goals.\nContinuously evaluate and adopt new technologies and methodologies to enhance the architecture and performance of data solutions.\n\n\nWhat we expect of you\nWe are all different, yet we all use our unique contributions to serve patients. The R&D Data Catalyst Team is responsible for building Data Searching, Cohort Building, and Knowledge Management tools that provide the Amgen scientific community with visibility to Amgens wealth of human datasets, projects and study histories, and knowledge over various scientific findings. These solutions are pivotal tools in Amgens goal to accelerate the speed of discovery, and speed to market of advanced precision medications.\nBasic Qualifications:\nMasters degree and 1 to 3 years of Data Engineering experience OR\nBachelors degree and 3 to 5 years of Data Engineering experience OR\nDiploma and 7 to 9 years of Data Engineering experience\nMust Have Skills:\nMinimum of 3 years of hands-on experience with BI solutions (Preferable Power BI or Business Objects) including report development, dashboard creation, and optimization.\nMinimum of 3 years of hands-on experience building Change-data-capture (CDC) ETL pipelines, data warehouse design and build, and enterprise-level data management.\nHands-on experience with Databricks, including data engineering, optimization, and analytics workloads.\nDeep understanding of Power BI, including model design, DAX, and Power Query.\nProven experience designing and implementing data mastering solutions and data governance frameworks.\nExpertise in cloud platforms (AWS), data lakes, and data warehouses.\nStrong knowledge of ETL processes, data pipelines, and integration technologies.\nGood communication and collaboration skills to work with cross-functional teams and senior leadership.\nAbility to assess business needs and design solutions that align with organizational goals.\nExceptional hands-on capabilities with data profiling, data transformation, data mastering\nSuccess in mentoring and training team members\nGood to Have Skills:\nITIL Foundation or other relevant certifications (preferred)\nSAFe Agile Practitioner (6.0)\nMicrosoft Certified: Data Analyst Associate (Power BI) or related certification.\nDatabricks Certified Professional or similar certification.\nSoft Skills:\nExcellent analytical and troubleshooting skills\nDeep intellectual curiosity\nThe highest degree of initiative and self-motivation\nStrong verbal and written communication skills, including presentation to varied audiences of complex technical/business topics\nConfidence technical leader\nAbility to work effectively with global, remote teams, specifically including using of tools and artifacts to assure clear and efficient collaboration across time zones\nAbility to handle multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals\nStrong problem solving, analytical skills;\nAbility to learn quickly and retain and synthesize complex information from diverse sources.",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Engineering', 'data management', 'Power BI', 'data governance', 'data warehousing', 'Databricks', 'ETL', 'AWS']",2025-06-12 13:46:55
DBA DATA Engineer || 12 Lakhs CTC,Robotics Technologies,6 - 10 years,12 Lacs P.A.,['Hyderabad( Banjara hills )'],"Dear Candidate,\nWe are seeking a skilled and experienced DBA Data Engineer to join our growing data team. The ideal candidate will play a key role in designing, implementing, and maintaining our databases and data pipeline architecture. You will collaborate with software engineers, data analysts, and DevOps teams to ensure efficient data flow, data integrity, and optimal database performance across all systems. This role requires a strong foundation in database administration, SQL performance tuning, data modeling, and experience with both on-prem and cloud-based environments.\n\nRequirements:\nBachelors degree in Computer Science, Information Systems, or a related field.\n6+ years of experience in database administration and data engineering.\nProven expertise in RDBMS (Oracle, MySQL, and PostgreSQL) and NoSQL systems (MongoDB, Cassandra).\nExperience managing databases in cloud environments (AWS, Azure, or GCP).\nProficiency in ETL processes and tools (e.g., Apache NiFi, Talend, Informatica, AWS Glue).\nStrong experience with scripting languages such as Python, Bash, or PowerShell.\n\nDBA Data Engineer Roles & Responsibilities:\nDesign and maintain scalable and high-performance database architectures.\nMonitor and optimize database performance using tools like CloudWatch, Oracle Enterprise Manager, pgAdmin, Mongo Compass, or Dynatrace.\nDevelop and manage ETL/ELT pipelines to support business intelligence and analytics.\nEnsure data integrity and security through best practices in backup, recovery, and encryption.\nAutomate regular database maintenance tasks using scripting and scheduled jobs.\nImplement high availability, failover, and disaster recovery strategies.\nConduct performance tuning of queries, stored procedures, indexes, and table structures.\nCollaborate with DevOps to automate database deployments using CI/CD and IaC tools (e.g., Terraform, AWS CloudFormation).\nDesign and implement data models, including star/snowflake schemas for data warehousing.\nDocument data flows, data dictionaries, and database configurations.\nManage user access and security policies using IAM roles or database-native permissions.\nAnalyze existing data systems and propose modernization or migration plans (on-prem to cloud, SQL to NoSQL, etc.).\nUse AWS RDS, Amazon Redshift, Azure SQL Database, or Google BigQuery as needed.\nStay up-to-date with emerging database technologies and make recommendations.\n\nMust-Have Skills:\nDeep knowledge of SQL and database performance tuning.\nHands-on experience with database migrations and replication strategies.\nFamiliarity with data governance, data quality, and compliance frameworks (GDPR, HIPAA, etc.).\nStrong problem-solving and troubleshooting skills.\nExperience with data streaming platforms such as Apache Kafka, AWS Kinesis, or Apache Flink is a plus.\nExperience with data lake and data warehouse architectures.\nExcellent communication and documentation skills.\n\nSoft Skills:\nProblem-Solving: Ability to analyze complex problems and develop effective solutions.\nCommunication Skills: Strong verbal and written communication skills to effectively collaborate with cross-functional teams.\nAnalytical Thinking: Ability to think critically and analytically to solve technical challenges.\nTime Management: Capable of managing multiple tasks and deadlines in a fast-paced environment.\nAdaptability: Ability to quickly learn and adapt to new technologies and methodologies.\n\nInterview Mode : F2F for who are residing in Hyderabad / Zoom for other states\nLocation : 43/A, MLA Colony,Road no 12, Banjara Hills, 500034\nTime : 2 - 4pm",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Database Administration', 'Database Migration', 'Dba Skills', 'High Availability', 'Db Administration', 'Hadr', 'Database Security', 'Disaster Recovery', 'Dr Testing', 'DR', 'Luw', 'Db Upgrade', 'Brtools', 'UDDI', 'Os Migration', 'Dbase', 'DBMS', 'IBM DB2', 'Db Migration', 'Disaster Recovery Planning', 'Db Dba', 'Backup And Recovery']",2025-06-12 13:46:57
Data Engineering Lead,Yotta Techports,10 - 15 years,30-35 Lacs P.A.,['Hyderabad'],"Responsibilities:\nLead and manage an offshore team of data engineers, providing strategic guidance, mentorship, and support to ensure the successful delivery of projects and the development of team members.\nCollaborate closely with onshore stakeholders to understand project requirements, allocate resources efficiently, and ensure alignment with client expectations and project timelines.\nDrive the technical design, implementation, and optimization of data pipelines, ETL processes, and data warehouses, ensuring scalability, performance, and reliability.\nDefine and enforce engineering best practices, coding standards, and data quality standards to maintain high-quality deliverables and mitigate project risks.\nStay abreast of emerging technologies and industry trends in data engineering, and provide recommendations for tooling, process improvements, and skill development.\nAssume a data architect role as needed, leading the design and implementation of data architecture solutions, data modeling, and optimization strategies.\nDemonstrate proficiency in AWS services such as:\nExpertise in cloud data services, including AWS services like Amazon Redshift, Amazon EMR, and AWS Glue, to design and implement scalable data solutions.\nExperience with cloud infrastructure services such as AWS EC2, AWS S3, to optimize data processing and storage.\nKnowledge of cloud security best practices, IAM roles, and encryption mechanisms to ensure data privacy and compliance.\nProficiency in managing or implementing cloud data warehouse solutions, including data modeling, schema design, performance tuning, and optimization techniques.\nDemonstrate proficiency in modern data platforms such as Snowflake and Databricks, including:\nDeep understanding of Snowflake's architecture, capabilities, and best practices for designing and implementing data warehouse solutions.\nHands-on experience with Databricks for data engineering, data processing, and machine learning tasks, leveraging Spark clusters for scalable data processing.\nAbility to optimize Snowflake and Databricks configurations for performance, scalability, and cost-effectiveness.\nManage the offshore team's performance, including resource allocation, performance evaluations, and professional development, to maximize team productivity and morale.\n\nQualifications:\nBachelor's degree in Computer Science, Engineering, or a related field; advanced degree preferred.\n10+ years of experience in data engineering, with a proven track record of leadership and technical expertise in managing complex data projects.\nProficiency in programming languages such as Python, Java, or Scala, as well as expertise in SQL and relational databases (e.g., PostgreSQL, MySQL).\nStrong understanding of distributed computing, cloud technologies (e.g., AWS), and big data frameworks (e.g., Hadoop, Spark).\nExperience with data architecture design, data modeling, and optimization techniques.\nExcellent communication, collaboration, and leadership skills, with the ability to effectively manage remote teams and engage with onshore stakeholders.\nProven ability to adapt to evolving project requirements and effectively prioritize tasks in a fast-paced environment.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Team Handling', 'Snowflake', 'Data Services', 'Cloud Infrastructure', 'Data Bricks']",2025-06-12 13:46:59
DBA DATA Engineer || 12 Lakhs CTC,Robotics Technologies,8 - 9 years,12 Lacs P.A.,['Hyderabad( Banjara hills )'],"Dear Candidate,\nWe are seeking a skilled and experienced DBA Data Engineer to join our growing data team. The ideal candidate will play a key role in designing, implementing, and maintaining our databases and data pipeline architecture. You will collaborate with software engineers, data analysts, and DevOps teams to ensure efficient data flow, data integrity, and optimal database performance across all systems. This role requires a strong foundation in database administration, SQL performance tuning, data modeling, and experience with both on-prem and cloud-based environments.\n\nRequirements:\nBachelors degree in Computer Science, Information Systems, or a related field.\n8+ years of experience in database administration and data engineering.\nProven expertise in RDBMS (Oracle, MySQL, and PostgreSQL) and NoSQL systems (MongoDB, Cassandra).\nExperience managing databases in cloud environments (AWS, Azure, or GCP).\nProficiency in ETL processes and tools (e.g., Apache NiFi, Talend, Informatica, AWS Glue).\nStrong experience with scripting languages such as Python, Bash, or PowerShell.\n\nDBA Data Engineer Roles & Responsibilities:\nDesign and maintain scalable and high-performance database architectures.\nMonitor and optimize database performance using tools like CloudWatch, Oracle Enterprise Manager, pgAdmin, Mongo Compass, or Dynatrace.\nDevelop and manage ETL/ELT pipelines to support business intelligence and analytics.\nEnsure data integrity and security through best practices in backup, recovery, and encryption.\nAutomate regular database maintenance tasks using scripting and scheduled jobs.\nImplement high availability, failover, and disaster recovery strategies.\nConduct performance tuning of queries, stored procedures, indexes, and table structures.\nCollaborate with DevOps to automate database deployments using CI/CD and IaC tools (e.g., Terraform, AWS CloudFormation).\nDesign and implement data models, including star/snowflake schemas for data warehousing.\nDocument data flows, data dictionaries, and database configurations.\nManage user access and security policies using IAM roles or database-native permissions.\nAnalyze existing data systems and propose modernization or migration plans (on-prem to cloud, SQL to NoSQL, etc.).\nUse AWS RDS, Amazon Redshift, Azure SQL Database, or Google BigQuery as needed.\nStay up-to-date with emerging database technologies and make recommendations.\n\nMust-Have Skills:\nDeep knowledge of SQL and database performance tuning.\nHands-on experience with database migrations and replication strategies.\nFamiliarity with data governance, data quality, and compliance frameworks (GDPR, HIPAA, etc.).\nStrong problem-solving and troubleshooting skills.\nExperience with data streaming platforms such as Apache Kafka, AWS Kinesis, or Apache Flink is a plus.\nExperience with data lake and data warehouse architectures.\nExcellent communication and documentation skills.\n\nSoft Skills:\nProblem-Solving: Ability to analyze complex problems and develop effective solutions.\nCommunication Skills: Strong verbal and written communication skills to effectively collaborate with cross-functional teams.\nAnalytical Thinking: Ability to think critically and analytically to solve technical challenges.\nTime Management: Capable of managing multiple tasks and deadlines in a fast-paced environment.\nAdaptability: Ability to quickly learn and adapt to new technologies and methodologies.\n\nInterview Mode : F2F for who are residing in Hyderabad / Zoom for other states\nLocation : 43/A, MLA Colony,Road no 12, Banjara Hills, 500034\nTime : 2 - 4pm",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Database Administration', 'Database Migration', 'Dba Skills', 'High Availability', 'Db Administration', 'Hadr', 'Database Security', 'Disaster Recovery', 'Dr Testing', 'DR', 'Luw', 'Db Upgrade', 'Brtools', 'UDDI', 'Os Migration', 'Dbase', 'DBMS', 'IBM DB2', 'Db Migration', 'Disaster Recovery Planning', 'Db Dba', 'Backup And Recovery']",2025-06-12 13:47:02
MDM Associate Data Engineer,Amgen Inc,1 - 4 years,Not Disclosed,['Hyderabad'],"We are seeking an MDM Associate Data Engineerwith 25 years of experience to support and enhance our enterprise MDM (Master Data Management) platforms using Informatica/Reltio. This role is critical in delivering high-quality master data solutions across the organization, utilizing modern tools like Databricks and AWS to drive insights and ensure data reliability. The ideal candidate will have strong SQL, data profiling, and experience working with cross-functional teams in a pharma environment.To succeed in this role, the candidate must have strong data engineering experience along with MDM knowledge, hence the candidates having only MDM experience are not eligible for this role. Candidate must have data engineering experience on technologies like (SQL, Python, PySpark, Databricks, AWS etc), along with knowledge of MDM (Master Data Management)\nRoles & Responsibilities:\nAnalyze and manage customer master data using Reltio or Informatica MDM solutions.\nPerform advanced SQL queries and data analysis to validate and ensure master data integrity.\nLeverage Python, PySpark, and Databricks for scalable data processing and automation.\nCollaborate with business and data engineering teams for continuous improvement in MDM solutions.\nImplement data stewardship processes and workflows, including approval and DCR mechanisms.\nUtilize AWS cloud services for data storage and compute processes related to MDM.\nContribute to metadata and data modeling activities.\nTrack and manage data issues using tools such as JIRA and document processes in Confluence.\nApply Life Sciences/Pharma industry context to ensure data standards and compliance.\nBasic Qualifications and Experience:\nMasters degree with 1 - 3 years of experience in Business, Engineering, IT or related field OR\nBachelors degree with 2 - 5 years of experience in Business, Engineering, IT or related field OR\nDiploma with 6 - 8 years of experience in Business, Engineering, IT or related field\nFunctional Skills:\nMust-Have Skills:\nAdvanced SQL expertise and data wrangling.\nStrong experience in Python and PySpark for data transformation workflows.\nStrong experience with Databricks and AWS architecture.\nMust have knowledge of MDM, data governance, stewardship, and profiling practices.\nIn addition to above, candidates having experience with Informatica or Reltio MDM platforms will be preferred.\nGood-to-Have Skills:\nExperience with IDQ, data modeling and approval workflow/DCR.\nBackground in Life Sciences/Pharma industries.\nFamiliarity with project tools like JIRA and Confluence.\nStrong grip on data engineering concepts.\nProfessional Certifications:\nAny ETL certification (e.g. Informatica)\nAny Data Analysis certification (SQL, Python, Databricks)\nAny cloud certification (AWS or AZURE)\nSoft Skills:\nStrong analytical abilities to assess and improve master data processes and solutions.\nExcellent verbal and written communication skills, with the ability to convey complex data concepts clearly to technical and non-technical stakeholders.\nEffective problem-solving skills to address data-related issues and implement scalable solutions.\nAbility to work effectively with global, virtual teams",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['MDM', 'PySpark', 'AWS architecture', 'Jira', 'Reltio', 'SQL', 'Informatica MDM', 'data modeling', 'Confluence', 'IDQ', 'Databricks', 'data stewardship processes', 'Python']",2025-06-12 13:47:04
Senior Engineer - Data Science,Sasken Technologies,2 - 5 years,Not Disclosed,['Bengaluru'],"Job Summary\nPerson at this position has gained significant work experience to be able to apply their knowledge effectively and deliver results. Person at this position is also able to demonstrate the ability to analyse and interpret complex problems and improve change or adapt existing methods to solve the problem.\nPerson at this position regularly interacts with interfacing groups / customer on technical issue clarification and resolves the issues. Also participates actively in important project/ work related activities and contributes towards identifying important issues and risks. Reaches out for guidance and advice to ensure high quality of deliverables.\nPerson at this position consistently seek opportunities to enhance their existing skills, acquire more complex skills and work towards enhancing their proficiency level in their field of specialisation.\nWorks under limited supervision of Team Lead/ Project Manager.\n\n\nRoles & Responsibilities\nResponsible for design, coding, testing, bug fixing, documentation and technical support in the assigned area. Responsible for on time delivery while adhering to quality and productivity goals. Responsible for adhering to guidelines and checklists for all deliverable reviews, sending status report to team lead and following relevant organizational processes. Responsible for customer collaboration and interactions and support to customer queries. Expected to enhance technical capabilities by attending trainings, self-study and periodic technical assessments. Expected to participate in technical initiatives related to project and organization and deliver training as per plan and quality.\n\n\nEducation and Experience Required\nEngineering graduate, MCA, etc Experience: 2-5 years\n\nCompetencies Description\nData Science TCB is applicable to one who\n1) Analyses data to arrive at patterns/Insights/models\n2) Come up with models based on the data to provide recommendations, predictive analytics etc\n3) Provides implementation of the models in R, Matlab etc\n4) Can understand and apply machine learning/AI techniques\nPlatforms-\nUnix\nTechnology Standard-\nNA\nTools-\nR, Matlab, Spark Machine Learning, Python-ML, SPSS, SAS\nLanguages-\nR, Perl, Python, Scala\nSpecialization-\nCOGNITIVE ANALYTICS INCLUDING COMPUTER VISION, AI and ML, STATISTICS.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'R', 'SAS', 'Scala', 'Perl', 'SPSS', 'Spark', 'machine learning', 'Python']",2025-06-12 13:47:07
Assoc. Data Engineer - R&D Precision Medicine Team,Amgen Inc,1 - 3 years,Not Disclosed,['Hyderabad'],"What you will do\n\n\nThe R&D Precision Medicine team is responsible for Data Standardization, Data Searching, Cohort Building, and Knowledge Management tools that provide the Amgen scientific community with access to Amgens wealth of human datasets, projects and study histories, and knowledge over various scientific findings. These data include clinical data, omics, and images. These solutions are pivotal tools in Amgens goal to accelerate the speed of discovery, and speed to market of advanced precision medications.\n\nThe Data Engineer will be responsible for full stack development of enterprise analytics and data mastering solutions leveraging Databricks and Power BI. This role requires expertise in both data architecture and analytics, with the ability to create scalable, reliable, and high-performing enterprise solutions that support research cohort-building and advanced AI pipelines. The ideal candidate will have experience creating and surfacing large unified repositories of human data, based on integrations from multiple repositories and solutions, and be exceptionally skilled with data analysis and profiling.\n\nYou will collaborate closely with partners, product team members, and related IT teams, to design and implement data models, integrate data from various sources, and ensure best practices for data governance and security. The ideal candidate will have a solid background in data warehousing, ETL, Databricks, Power BI, and enterprise data mastering.\n\nRoles & Responsibilities\nDesign and build scalable enterprise analytics solutions using Databricks, Power BI, and other modern data management tools.\nLeverage data virtualization, ETL, and semantic layers to balance need for unification, performance, and data transformation with goal to reduce data proliferation\nBreak down features into work that aligns with the architectural direction runway\nParticipate hands-on in pilots and proofs-of-concept for new patterns\nCreate robust documentation from data analysis and profiling, and proposed designs and data logic\nDevelop advanced sql queries to profile, and unify data\nDevelop data processing code in sql, along with semantic views to prepare data for reporting\nDevelop PowerBI Models and reporting packages\nDesign robust data models, and processing layers, that support both analytical processing and operational reporting needs.\nDesign and develop solutions based on best practices for data governance, security, and compliance within Databricks and Power BI environments.\nEnsure the integration of data systems with other enterprise applications, creating seamless data flows across platforms.\nDevelop and maintain Power BI solutions, ensuring data models and reports are optimized for performance and scalability.\nCollaborate with partners to define data requirements, functional specifications, and project goals.\nContinuously evaluate and adopt new technologies and methodologies to enhance the architecture and performance of data solutions.\n\n\n\nWhat we expect of you\nWe are all different, yet we all use our unique contributions to serve patients. The professional we seek is someone with these qualifications.\n\nBasic Qualifications:\nMasters degree with 1 to 3 years of experience in Data Engineering OR\nBachelors degree with 1 to 3 years of experience in Data Engineering\nMust-Have\n\nSkills:\nMinimum of 1 year of hands-on experience with BI solutions (Preferrable Power BI or Business Objects) including report development, dashboard creation, and optimization.\nMinimum of 1 year of hands-on experience building Change-data-capture (CDC) ETL pipelines, data warehouse design and build, and enterprise-level data management.\nHands-on experience with Databricks, including data engineering, optimization, and analytics workloads.\nExperience using cloud platforms (AWS), data lakes, and data warehouses.\nWorking knowledge of ETL processes, data pipelines, and integration technologies.\nGood communication and collaboration skills to work with cross-functional teams and senior leadership.\nAbility to assess business needs and design solutions that align with organizational goals.\nExceptional hands-on capabilities with data profiling and data anlysis\nGood-to-Have\n\nSkills:\nExperience with human data, ideally human healthcare data\nFamiliarity with laboratory testing, patient data from clinical care, HL7, FHIR, and/or clinical trial data management\nProfessional Certifications:\nITIL Foundation or other relevant certifications (preferred)\nSAFe Agile Practitioner (6.0)\nMicrosoft CertifiedData Analyst Associate (Power BI) or related certification.\nDatabricks Certified Professional or similar certification.\nSoft\n\nSkills:\nExcellent analytical and troubleshooting skills\nDeep intellectual curiosity\nHighest degree of initiative and self-motivation\nStrong verbal and written communication skills, including presentation to varied audiences of complex technical/business topics\nConfidence technical leader\nAbility to work effectively with global, virtual teams, specifically including using of tools and artifacts to assure clear and efficient collaboration across time zones\nAbility to manage multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals\nStrong problem solving, analytical skills;\nAbility to learn quickly and retain and synthesize complex information from diverse sources",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Engineering', 'data lakes', 'data pipelines', 'ETL processes', 'AWS', 'data warehouses', 'BI solutions']",2025-06-12 13:47:10
Azure Data Engineer,CODERZON Technologies Pvt Ltd,3 - 8 years,6-18 Lacs P.A.,['Kochi'],"Looking for a Data Engineer with 3+ yrs exp in Azure Data Factory, Synapse, Data Lake, Databricks, SQL, Python, Spark, CI/CD. Preferred: DP-203 cert, real-time data tools (Kafka, Stream Analytics), data governance (Purview), Power BI.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Data Factory', 'Azure Synapse', 'Pyspark', 'Azure Databricks', 'Azure Data Lake', 'SQL Azure', 'Python']",2025-06-12 13:47:12
Azure Data Engineer,Arges Global,2 - 5 years,8-18 Lacs P.A.,['Pune( Baner )'],"Scope of Work:\nCollaborate with the lead Business / Data Analyst to gather and analyse business requirements for data processing and reporting solutions.\nMaintain and run existing Python code, ensuring smooth execution and troubleshooting any issues that arise.\nDevelop new features and enhancements for data processing, ingestion, transformation, and report building.\nImplement best coding practices to improve code quality, maintainability, and efficiency.\nWork within Microsoft Fabric to manage data integration, warehousing, and analytics, ensuring optimal performance and reliability.\nSupport and maintain CI/CD workflows using Git-based deployments or other automated deployment tools, preferably in Fabric.\nDevelop complex business rules and logic in Python to meet functional specifications and reporting needs.\nParticipate in an agile development environment, providing feedback, iterating on improvements, and supporting continuous integration and delivery processes.\nRequirements:\nThis person will be an individual contributor responsible for programming, maintenance support, and troubleshooting tasks related to data movement, processing, ingestion, transformation, and report building.\nAdvanced-level Python developer.\nModerate-level experience in working in Microsoft Fabric environment (at least one and preferably two or more client projects in Fabric).\nWell-versed with understanding of modelling, databases, data warehousing, data integration, and technical elements of business intelligence technologies.\nAbility to understand business requirements and translate them into functional specifications for reporting applications.\nExperience in GIT-based deployments or other CI/CD workflow options, preferably in Fabric.\nStrong verbal and written communication skills.\nAbility to perform in an agile environment where continual development is prioritized.\nWorking experience in the financial industry domain and familiarity with financial accounting terms and statements like general ledger, balance sheet, and profit & loss statements would be a plus.\nAbility to create Power BI dashboards, KPI scorecards, and visual reports would be a plus.\nDegree in Computer Science or Information Systems, along with a good understanding of financial terms or working experience in banking/financial institutions, is preferred.",Industry Type: Financial Services (Asset Management),Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Power Bi', 'Microsoft Azure', 'Python', 'Azure Data Factory', 'Microsoft Fabric', 'Azure Databricks', 'Azure Data Lake']",2025-06-12 13:47:14
Software Engineer Data Privacy,Bajaj Finserv Health,4 - 6 years,Not Disclosed,['Pune( Viman Nagar )'],"Job Description:\nKnowledgeable and proactive Data Privacy engineer to manage privacy and data protection initiatives in compliance with Indian data privacy regulations, including the Digital Personal Data Protection Act, 2023 (DPDP Act).\nThis role will work closely with legal, IT, security, HR, and business teams to implement privacy by design and foster a strong culture of privacy data protection\n\nKey Objectives/Responsibilities of this Role:\nImplement and oversee the data privacy program implementation in accordance with the Privacy Laws and requirements (DPDP Act, 2023 and other applicable Indian laws)\nDocument and maintain records of processing activities\nDraft, review, and update privacy notices, policies, consent mechanisms, and contractual clauses for data processing.\nConduct and review Data Protection Impact Assessments (DPIAs) and help identify and mitigate privacy risks.\nTrain and educate employees on Data privacy requirements and best practices.\nPerform regular compliance audits and assessments across internal departments and third-party vendors.\nDevelop and deliver training and awareness programs to ensure employees understand their responsibilities under the DPDP Act and internal privacy policies.\nHandle and ensure timely response to data principal rights requests (access, correction, erasure, grievance redressal) as per Privacy Law requirements\nCollaborate with legal and IT/security teams to ensure privacy by design and default in new processes, technologies, and products.\nStay updated with latest developments in data protection laws and regulations.\nSupport incident response and breach notification processes as per regulatory timelines and requirements.\nAssist the Data Privacy Officer in preparing reports for internal leadership or regulators, as required.\nServe as a subject matter expert on Indian data privacy regulations and provide guidance to cross-functional teams.\n\nMandatory Skillset & Experience:\nMinimum 4+ years of relevant experience in information security, compliance, or legal roles with focus on data privacy\nIn-depth knowledge of the Data Protection and Privacy Regulations, and awareness of related Indian IT regulations.\nExperienced in using Data protection management tools and softwares\nUnderstanding of global data privacy frameworks (GDPR, etc.) is a plus.\nStrong analytical and problem-solving skills with a practical approach to risk and compliance.\nAbility to communicate complex privacy topics in a clear and business-friendly manner.\nExperience working with privacy compliance tools and risk management systems.\n\nBehavioral Skills:\nSelf-driven and proactive.\nPlan and execute projects in a timely fashion meeting the project timelines\nExperienced in coordinating activities across different teams and stakeholders\nContinuous process improvement / quality assurance experience is a plus\nProactively identify potential data privacy issues and problems in business processes\n\nPreferred Qualification:\nBachelors or Masters degree in law, Technology, Compliance, Risk Management, or a related field.\n\nPreferred Certifications:\nDCPP (Data Protection Certified Professional - India)\nCIPP/A, CIPM (IAPP certifications)\nISO/IEC 27701, ISO 27001 (Good to have)",Industry Type: Medical Services / Hospital,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Data Protection Manager', 'Privacy Regulation', 'Data Privacy Law', 'Data Privacy', 'Gdpr']",2025-06-12 13:47:16
Data Engineering Lead Microsoft Power BI,Client of Techs To Suit,8 - 12 years,25-32.5 Lacs P.A.,"['Indore', 'Hyderabad', 'Ahmedabad']","Poistion - Data Engineering Lead\nExp - 8 to 12 Years\nJob Location: Hyderabad, Ahmedabad, Indore, India.\n\nMust be Able to join in 30 days\nJob Summary:\nAs a Data Engineering Lead, your role will involve designing, developing, and implementing\ninteractive dashboards and reports using data engineering tools. You will work closely with\nstakeholders to gather requirements and translate them into effective data visualizations that\nprovide valuable insights. Additionally, you will be responsible for extracting, transforming, and\nloading data from multiple sources into Power BI, ensuring its accuracy and integrity. Your\nexpertise in Power BI and data analytics will contribute to informed decision-making and\nsupport the organization in driving data-centric strategies and initiatives.\nWe are looking for you!\nAs an ideal candidate for the Data Engineering Lead position, you embody the qualities of a\nteam player with a relentless get-it-done attitude. Your intellectual curiosity and customer\nfocus drive you to continuously seek new ways to add value to your job accomplishments. You\nthrive under pressure, maintaining a positive attitude and understanding that your career is a\njourney. You are willing to make the right choices to support your growth. In addition to your\nexcellent communication skills, both written and verbal, you have a proven ability to create\nvisually compelling designs using tools like Power BI and Tableau that effectively\ncommunicate our core values.\nYou build high-performing, scalable, enterprise-grade applications and teams. Your creativity\nand proactive nature enable you to think differently, find innovative solutions, deliver high-\nquality outputs, and ensure customers remain referenceable. With over eight years of\nexperience in data engineering, you possess a strong sense of self-motivation and take\nownership of your responsibilities. You prefer to work independently with little to no\nsupervision.\nYou are process-oriented, adopt a methodical approach, and demonstrate a quality-first\nmindset. You have led mid to large-size teams and accounts, consistently using constructive\nfeedback mechanisms to improve productivity, accountability, and performance within the\nteam. Your track record showcases your results-driven approach, as you have consistently\ndelivered successful projects with customer case studies published on public platforms.\nOverall, you possess a unique combination of skills, qualities, and experiences that make you\nan ideal fit to lead our data engineering team(s). You value inclusivity and want to join a culture\nthat empowers you to show up as your authentic self.\nYou know that success hinges on commitment, our differences make us stronger, and the\nfinish line is always sweeter when the whole team crosses together. In your role, you shouldbe driving the team using data, data, and more data. You will manage multiple teams, oversee\nagile stories and their statuses, handle escalations and mitigations, plan ahead, identify hiring\nneeds, collaborate with recruitment teams for hiring, enable sales with pre-sales teams, and\nwork closely with development managers/leads for solutioning and delivery statuses, as well\nas architects for technology research and solutions.\nWhat You Will Do:\nAnalyze Business Requirements.\nAnalyze the Data Model and do GAP analysis with Business Requirements and Power\nBI. Design and Model Power BI schema.\nTransformation of Data in Power BI/SQL/ETL Tool.\nCreate DAX Formula, Reports, and Dashboards. Able to write DAX formulas.\nExperience writing SQL Queries and stored procedures.\nDesign effective Power BI solutions based on business requirements.\nManage a team of Power BI developers and guide their work.\nIntegrate data from various sources into Power BI for analysis.\nOptimize performance of reports and dashboards for smooth usage.\nCollaborate with stakeholders to align Power BI projects with goals.\nKnowledge of Data Warehousing(must), Data Engineering is a plus",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Power Bi Dashboards', 'Microsoft Power Bi', 'SQL Queries', 'Azure Databricks', 'Dax', 'Azure Data Factory']",2025-06-12 13:47:19
Data Engineering Manager,Amgen Inc,8 - 12 years,Not Disclosed,['Hyderabad'],"ABOUT THE ROLE\n\n\nRole Description:\n\nWe are seeking a seasoned Engineering Manager (Data Engineering) to lead the end-to-end management of enterprise data assets and operational data workflows. This role is critical in ensuring the availability, quality, consistency, and timeliness of data across platforms and functions, supporting analytics, reporting, compliance, and digital transformation initiatives. You will be responsible for the day-to-day data operations, manage a team of data professionals, and drive process excellence in data intake, transformation, validation, and delivery. You will work closely with cross-functional teams including data engineering, analytics, IT, governance, and business stakeholders to align operational data capabilities with enterprise needs.\n\nRoles & Responsibilities:\nLead and manage the enterprise data operations team, responsible for data ingestion, processing, validation, quality control, and publishing to various downstream systems.\nDefine and implement standard operating procedures for data lifecycle management, ensuring accuracy, completeness, and integrity of critical data assets.\nOversee and continuously improve daily operational workflows, including scheduling, monitoring, and troubleshooting data jobs across cloud and on-premise environments.\nEstablish and track key data operations metrics (SLAs, throughput, latency, data quality, incident resolution) and drive continuous improvements.\nPartner with data engineering and platform teams to optimize pipelines, support new data integrations, and ensure scalability and resilience of operational data flows.\nCollaborate with data governance, compliance, and security teams to maintain regulatory compliance, data privacy, and access controls.\nServe as the primary escalation point for data incidents and outages, ensuring rapid response and root cause analysis.\nBuild strong relationships with business and analytics teams to understand data consumption patterns, prioritize operational needs, and align with business objectives.\nDrive adoption of best practices for documentation, metadata, lineage, and change management across data operations processes.\nMentor and develop a high-performing team of data operations analysts and leads.\nFunctional\n\nSkills:\nMust-Have Skills:\nExperience managing a team of data engineers in biotech/pharma domain companies.\nExperience in designing and maintaining data pipelines and analytics solutions that extract, transform, and load data from multiple source systems.\nDemonstrated hands-on experience with cloud platforms (AWS) and the ability to architect cost-effective and scalable data solutions.\nExperience managing data workflows in cloud environments such as AWS, Azure, or GCP.\nStrong problem-solving skills with the ability to analyze complex data flow issues and implement sustainable solutions.\nWorking knowledge of SQL, Python, or scripting languages for process monitoring and automation.\nExperience collaborating with data engineering, analytics, IT operations, and business teams in a matrixed organization.\nFamiliarity with data governance, metadata management, access control, and regulatory requirements (e.g., GDPR, HIPAA, SOX).\nExcellent leadership, communication, and stakeholder engagement skills.\nWell versed with full stack development & DataOps automation, logging frameworks, and pipeline orchestration tools.\nStrong analytical and problem-solving skills to address complex data challenges.\nEffective communication and interpersonal skills to collaborate with cross-functional teams.\nGood-to-Have\n\nSkills:\nData Engineering Management experience in Biotech/Life Sciences/Pharma\nExperience using graph databases such as Stardog or Marklogic or Neo4J or Allegrograph, etc.\nEducation and Professional Certifications\nDoctorate Degree with 3-5 + years of experience in Computer Science, IT or related field\nOR\nMasters degree with 6 - 8 + years of experience in Computer Science, IT or related field\nOR\nBachelors degree with 10 - 12 + years of experience in Computer Science, IT or related field\nAWS Certified Data Engineer preferred\nDatabricks Certificate preferred\nScaled Agile SAFe certification preferred\nSoft\n\nSkills:\nExcellent analytical and troubleshooting skills\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation\nAbility to manage multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals\nStrong presentation and public speaking skills",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'fullstack development', 'logging framework', 'stakeholder engagement', 'troubleshooting', 'cloud platforms']",2025-06-12 13:47:21
Hadoop Data Engineer,Envision Technology Solutions,3 - 8 years,5-15 Lacs P.A.,"['New Delhi', 'Hyderabad', 'Gurugram']","Primary Skill – Hadoop, Hive, Python, SQL, Pyspark/Spark.\nLocation –Hyderabad / Gurgaon;",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Hadoop', 'Hive', 'Spark', 'Python', 'SQL']",2025-06-12 13:47:24
Manager Data Engineer – Research Data and Analytics,Amgen Inc,4 - 6 years,Not Disclosed,['Hyderabad'],"What you will do\nIn this vital role you will create and develop data lake solutions for scientific data that drive business decisions for Research. You will build scalable and high-performance data engineering solutions for large scientific datasets and collaborate with Research collaborators. You will also provide technical leadership to junior team members. The ideal candidate possesses experience in the pharmaceutical or biotech industry, demonstrates deep technical skills, is proficient with big data technologies, and has a deep understanding of data architecture and ETL processes.\nRoles & Responsibilities:\nLead, manage, and mentor a high-performing team of data engineers\nDesign, develop, and implement data pipelines, ETL processes, and data integration solutions\nTake ownership of data pipeline projects from inception to deployment, manage scope, timelines, and risks\nDevelop and maintain data models for biopharma scientific data, data dictionaries, and other documentation to ensure data accuracy and consistency\nOptimize large datasets for query performance\nCollaborate with global multi-functional teams including research scientists to understand data requirements and design solutions that meet business needs\nImplement data security and privacy measures to protect sensitive data\nLeverage cloud platforms (AWS preferred) to build scalable and efficient data solutions\nCollaborate with Data Architects, Business SMEs, Software Engineers and Data Scientists to design and develop end-to-end data pipelines to meet fast paced business needs across geographic regions\nIdentify and resolve data-related challenges\nAdhere to best practices for coding, testing, and designing reusable code/component\nExplore new tools and technologies that will help to improve ETL platform performance\nParticipate in sprint planning meetings and provide estimations on technical implementation\n\n\nWhat we expect of you\nWe are all different, yet we all use our unique contributions to serve patients. The [vital attribute] professional we seek is a [type of person] with these qualifications.\nBasic Qualifications:\nDoctorate Degree OR\nMasters degree with 4 - 6 years of experience in Computer Science, IT, Computational Chemistry, Computational Biology/Bioinformatics or related field OR\nBachelors degree with 6 - 8 years of experience in Computer Science, IT, Computational Chemistry, Computational Biology/Bioinformatics or related field OR\nDiploma with 10 - 12 years of experience in Computer Science, IT, Computational Chemistry, Computational Biology/Bioinformatics or related field\nPreferred Qualifications:\n3+ years of experience in implementing and supporting biopharma scientific research data analytics (software platforms)\n\n\nFunctional Skills:\nMust-Have Skills:\nProficiency in SQL and Python for data engineering, test automation frameworks (pytest), and scripting tasks\nHands on experience with big data technologies and platforms, such as Databricks, Apache Spark (PySpark, SparkSQL), workflow orchestration, performance tuning on big data processing\nExcellent problem-solving skills and the ability to work with large, complex datasets\nAble to engage with business collaborators and mentor team to develop data pipelines and data models\n\n\nGood-to-Have Skills:\nA passion for tackling complex challenges in drug discovery with technology and data\nGood understanding of data modeling, data warehousing, and data integration concepts\nGood experience using RDBMS (e.g. Oracle, MySQL, SQL server, PostgreSQL)\nKnowledge of cloud data platforms (AWS preferred)\nExperience with data visualization tools (e.g. Dash, Plotly, Spotfire)\nExperience with diagramming and collaboration tools such as Miro, Lucidchart or similar tools for process mapping and brainstorming\nExperience writing and maintaining technical documentation in Confluence\nUnderstanding of data governance frameworks, tools, and best practices\n\n\nProfessional Certifications:\nDatabricks Certified Data Engineer Professional preferred\n\n\nSoft Skills:\nExcellent critical-thinking and problem-solving skills\nGood communication and collaboration skills\nDemonstrated awareness of how to function in a team setting\nDemonstrated presentation skills",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Spotfire', 'PySpark', 'PostgreSQL', 'Plotly', 'SparkSQL', 'SQL server', 'SQL', 'process mapping', 'Dash', 'MySQL', 'ETL', 'Oracle', 'data governance frameworks', 'Python']",2025-06-12 13:47:26
Snowflake Data Engineer,Epam Systems,5 - 10 years,Not Disclosed,['Chennai'],"Key Skills:\nSnowflake (Snow SQL, Snow PLSQL and Snowpark)\nStrong Python\nAirflow/DBT\nAny DevOps tools\nAWS/Azure Cloud Skills\n\nRequirements:\nLooking for engineer for information warehouse\nWarehouse is based on AWS/Azure, DBT, Snowflake.\nStrong programming experience with Python.\nExperience with workflow management tools like Argo/Oozie/Airflow.\nExperience in Snowflake modelling - roles, schema, databases\nExperience in data Modeling (Data Vault).\nExperience in design and development of data transformation pipelines using the DBT framework.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Snowflake', 'Python', 'Azure Cloud', 'AWS', 'SQL']",2025-06-12 13:47:28
Lead Data Engineer,Prolegion,8 - 12 years,Not Disclosed,['Hyderabad'],"Job Summary:\nWe are seeking a highly skilled Lead Data Engineer/Associate Architect to lead the design, implementation, and optimization of scalable data architectures. The ideal candidate will have a deep understanding of data modeling, ETL processes, cloud data solutions, and big data technologies. You will work closely with cross-functional teams to build robust, high-performance data pipelines and infrastructure to enable data-driven decision-making.\n\nExperience: 8 - 12+ years\nWork Location: Hyderabad (Hybrid)\nMandatory skills: Python, SQL, Snowflake\n\nResponsibilities:\nDesign and Develop scalable and resilient data architectures that support business needs, analytics, and AI/ML workloads.\nData Pipeline Development: Design and implement robust ETL/ELT processes to ensure efficient data ingestion, transformation, and storage.\nBig Data & Cloud Solutions: Architect data solutions using cloud platforms like AWS, Azure, or GCP, leveraging services such as Snowflake, Redshift, BigQuery, and Databricks.\nDatabase Optimization: Ensure performance tuning, indexing strategies, and query optimization for relational and NoSQL databases.\nData Governance & Security: Implement best practices for data quality, metadata management, compliance (GDPR, CCPA), and security.\nCollaboration & Leadership: Work closely with data engineers, analysts, and business stakeholders to translate business requirements into scalable solutions.\nTechnology Evaluation: Stay updated with emerging trends, assess new tools and frameworks, and drive innovation in data engineering.\n\nRequired Skills:\nEducation: Bachelors or Masters degree in Computer Science, Data Engineering, or a related field.\nExperience: 8 - 12+ years of experience in data engineering\nCloud Platforms: Strong expertise in AWS data services.\nBig Data Technologies: Experience with Hadoop, Spark, Kafka, and related frameworks.\nDatabases: Hands-on experience with SQL, NoSQL, and columnar databases such as PostgreSQL, MongoDB, Cassandra, and Snowflake.\nProgramming: Proficiency in Python, Scala, or Java for data processing and automation.\nETL Tools: Experience with tools like Apache Airflow, Talend, DBT, or Informatica.\nMachine Learning & AI Integration (Preferred): Understanding of how to architect data solutions for AI/ML applications\n\n,",Industry Type: Defence & Aerospace,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Performance tuning', 'Automation', 'Data modeling', 'Postgresql', 'Informatica', 'Apache', 'Analytics', 'SQL', 'Python']",2025-06-12 13:47:30
Lead Data Engineer - Azure,Blend360 India,7 - 12 years,Not Disclosed,['Hyderabad'],"As a Sr Data Engineer, your role is to spearhead the data engineering teams and elevate the team to the next level! You will be responsible for laying out the architecture of the new project as well as selecting the tech stack associated with it. You will plan out the development cycles deploying AGILE if possible and create the foundations for good data stewardship with our new data products!\nYou will also set up a solid code framework that needs to be built to purpose yet have enough flexibility to adapt to new business use cases tough but rewarding challenge!\n\nResponsibilities\nCollaborate with several stakeholders to deeply understand the needs of data practitioners to deliver at scale\nLead Data Engineers to define, build and maintain Data Platform\nWork on building Data Lake in Azure Fabric processing data from multiple sources\nMigrating existing data store from Azure Synapse to Azure Fabric\nImplement data governance and access control\nDrive development effort End-to-End for on-time delivery of high-quality solutions that conform to requirements, conform to the architectural vision, and comply with all applicable standards.\nPresent technical solutions, capabilities, considerations, and features in business terms. Effectively communicate status, issues, and risks in a precise and timely manner.\nFurther develop critical initiatives, such as Data Discovery, Data Lineage and Data Quality\nLeading team and Mentor junior resources\nHelp your team members grow in their role and achieve their career aspirations\nBuild data systems, pipelines, analytical tools and programs\nConduct complex data analysis and report on results\n\n\n7+ Years of Experience as a data engineer or similar role in Azure Synapses, ADF or\nrelevant exp in Azure Fabric\nDegree in Computer Science, Data Science, Mathematics, IT, or similar field\nMust have experience e",Industry Type: Industrial Automation,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Data analysis', 'Data modeling', 'Analytical', 'Agile', 'data governance', 'Data quality', 'Data mining', 'SQL', 'Python']",2025-06-12 13:47:32
Azure Data Engineer (Azure Databricks must),Fortune India 500 IT Services Firm,5 - 8 years,Not Disclosed,['Hyderabad'],"We are looking for an experienced Azure Data Engineer with strong expertise in Azure Databricks to join our data engineering team.\n\nMandatory skill- Azure Databricks\nExperience- 5 to 8 years\nLocation- Hyderabad\nKey Responsibilities:\nDesign and build data pipelines and ETL/ELT workflows using Azure Databricks and Azure Data Factory\nIngest, clean, transform, and process large datasets from diverse sources (structured and unstructured)\nImplement Delta Lake solutions and optimize Spark jobs for performance and reliability\nIntegrate Azure Databricks with other Azure services including Data Lake Storage, Synapse Analytics, and Event Hubs\n\n\n\nInterested candidates share your CV at himani.girnar@alikethoughts.com with below details\n\nCandidate's name-\nEmail and Alternate Email ID-\nContact and Alternate Contact no-\nTotal exp-\nRelevant experience-\nCurrent Org-\nNotice period-\nCCTC-\nECTC-\nCurrent Location-\nPreferred Location-\nPancard No-",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Azure Databricks', 'Azure Data Factory', 'Pyspark', 'Azure Data Lake', 'SQL']",2025-06-12 13:47:34
Lead Data Engineer (Immediate joiner),Decision Point,4 - 9 years,15-30 Lacs P.A.,"['Gurugram', 'Chennai']","Role & responsibilities\n• Assume ownership of Data Engineering projects from inception to completion.\nImplement fully operational Unified Data Platform solutions in production environments using technologies like Databricks, Snowflake, Azure Synapse etc.\nShowcase proficiency in Data Modelling and Data Architecture\nUtilize modern data transformation tools such as DBT (Data Build Tool) to streamline and automate data pipelines (nice to have).",,,,"['Pyspark', 'Azure Databricks', 'SQL', 'Azure Synapse', 'Python', 'Etl Pipelines', 'Airflow', 'Bigquery', 'Advance Sql', 'Azure Cloud', 'GCP', 'Data Modeling', 'Data Architecture', 'AWS']",2025-06-12 13:47:37
"Associate Engineer, Digital Data Development",XL India Business Services Pvt. Ltd,1 - 4 years,Not Disclosed,['Gurugram'],"Engineer, Digital Data Development Gurgaon/ Bangalore, India AXA XL offers risk transfer and risk management solutions to clients globally\n\nWe offer worldwide capacity, flexible underwriting solutions, a wide variety of client-focused loss prevention services, and a team-based account management approach\n\nAXA XL recognizes data and information as critical business assets, both in terms of managing risk and enabling new business opportunities\n\nThis data should not only be high quality, but also actionable - enabling AXA XL s executive leadership team to maximize benefits and facilitate sustained dynamic advantage\n\nOur Innovation, Data, and Analytics (IDA) organization is focused on driving innovation by optimizing how we leverage data to drive strategy and create a new business model - disrupting the insurance market\n\nThis role is part of the Digital Data Dev Division within the Digital Transformation vertical of IDA\n\nIt will be responsible for different aspects of Data Product development lifecycle activities, including but not limited to Data Production Support, business stakeholders engagement for usage & problem resolutions, Product migrations, and platform/data product rollouts, performance stability & reliability\n\nWhat you ll be DOING What will your essential responsibilities include? Hands-on experience with CI/CD tools: Harness, Azure DevOps\n\nImplement and manage DevSecOps tools and CI/CD pipelines with security controls\n\nAutomate security scanning and compliance checks (SAST, DAST, container scanning, etc)\n\nCollaborate with development, operations, and security teams to embed security best practices\n\nConduct threat modeling, vulnerability assessments, and risk\n\nBuild, Release Management & DevSecOps support for various data solutions owned and managed by IDA organization\n\nExperience with cloud platforms like Azure is preferred\n\nProficiency in scripting languages: Python, Bash, PowerShell\n\nFamiliarity with containerization and orchestration: Docker, Kubernetes, OpenShift\n\nExperience using Tools like Git, JIRA, Confluence etc Knowledge of Artifactory like JFrog / X-Ray\n\nExperience of working with Agile methodologies\n\nGood knowledge of OOP concepts & Microservice-based architecture\n\nAnalyze and mitigate risks (technical or otherwise) about Data Solution build & release delivery timelines\n\nProvide top-class DevSecOps functionalities and support\n\nPartner with the Product & Production Support team(s) as a Data/DevSecOps/Technical SME for migration of re-architected Product/Product functionalities to the new Cloud Platform\n\nDemonstrate proactive communication with Business users, Development, Technology, Production Support, and Delivery Teams, and Senior Management\n\nProvide day-to-day management of the DevSecOps services and ensure smooth operation of the Release pipelines to various Environments\n\nWork in the Follow the Sun support model providing cross-team support coverage across Digital Data Dev division responsibilities\n\nBuild/Setup/Maintain various critical monitoring processes, alerts, and overall health reports (performance and functional) of production, and pre-production environments to be used by the Production Support Teams\n\nWork with Product Teams to build deployment pipelines for various Data Science Products used within IDA/Pricing & Analytics Teams\n\nOversee the development and maintenance of Build & Release Management processes and their documentation\n\nEnsure that all policies, standards, and best practices are followed and kept up to date\n\nTimely and accurate completion of emergency Release pipelines/processes in a manner that is auditable, testable, and maintainable\n\nEnsure any builds are consistent with Solution design, Security recommendations and business specifications\n\nAchieve & maintain the highest business customer confidence and net promoter score (NPS)\n\nGood grasp of Azure fundamentals (Microsoft AZ-900)\n\nRobust understanding of Designing and Implementing DevOps/DevSecOps Solutions (Microsoft AZ-400)\n\nKnowledge of Python or R Programming Language is a plus\n\nYou will report to Senior Delivery Lead\n\nWhat you will BRING We re looking for someone who has these abilities and skills: Required Skills and Abilities: Excellent understanding of DevOps principles with integrated security practices\n\nA minimum of an Undergraduate University Degree in Computer Science or related fields\n\nExtensive experience in data-focused roles (analytics, specialist, or engineer) and one or more areas of Build, Release & Data Management\n\nDistinctive problem-solving and analytical skills combined with robust business acumen\n\nExperience/knowledge of Microservices, Dot Net, R Programming Language, Python, Azure, and Kibana\n\nExperience with SQL, HIVE, ADLS, and Document Databases like Cosmos, SQL Databases & SQL DW Analytics\n\nExperience/Understanding of systems integration, and developer support tools Azure DevOps/DevSecOps, CI/CD pipelines, Release Management, Configuration Management, and Automation\n\nData Engineering background or working experience with ETL and big data platforms (HDInsight / ADLS / Data Bricks) a plus\n\nDesired Skills and Abilities: Demonstrates a level of experience/ability to influence and understand business problems in technical terminology and able to liaise with staff at all levels in the organization\n\nExcellent writing skills, with the ability to create clear requirements, specifications, and documentation for data systems\n\nExperience with multiple software delivery models (Waterfall, Agile, etc) is a plus\n\nPrevious experience leading small teams with a mix of onsite/offshore developers",Industry Type: Insurance,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Automation', 'Production support', 'Configuration management', 'Agile', 'microsoft', 'Risk management', 'Release management', 'Analytics', 'SQL', 'Python']",2025-06-12 13:47:39
ML Engineer/Data Scientist,Altimetrik,6 - 8 years,15-30 Lacs P.A.,"['Hyderabad', 'Chennai', 'Bengaluru']","Role & responsibilities\nData Scientist /ML engineers : ML Engineer with Python, SQL, Machine Learning, Azure skills(Good to have)",Industry Type: IT Services & Consulting,,,"['Machine Learning', 'Python', 'SQL', 'Data Science', 'Ml', 'azure']",2025-06-12 13:47:41
Senior Technical Support Engineer - SQL & Data Analytics,Insightsoftware,6 - 11 years,Not Disclosed,['Hyderabad'],"insightsoftware is a global provider of comprehensive solutions for the Office of the CFO. We believe an actionable business strategy begins and ends with accessible financial data. With solutions across financial planning and analysis (FP&A), accounting, and operations, we transform how teams operate, empowering leaders to make timely and informed decisions. With data at the heart of everything we do, insightsoftware enables automated processes, delivers trusted insights, boosts predictability, and increases productivity. Learn more at insightsoftware.com\nWorking Timings:\nReady to work as per eastern shift timings, (5:30 PM to 2:30 AM IST)\nWork from office - Hyderabad location.(Hybrid)\nResponsibilities\n\nManage large amounts of incoming emails regarding software support\n\nInteract with customers, partners, and internal teams to provide advice and assistance and achieve customer satisfaction.\n\nLogically deduce root cause and find workarounds and solutions to issues\n\nIdentify, analyze, and document product bugs and fixes relating to the Product functionality, databases, application servers, and new technologies for product management and engineering teams\n\nComplete or assist with customer product installs as needed\n\nMeet personal Productivity, Efficiency, and Quality metrics\n\nPrioritize and resolve issues of the highest technical and business severity\n\nHandle customer complaints and provide appropriate solutions and alternatives within time limits; follow up to ensure resolution including identifying workarounds and communicating those to customers\n\nKeep accurate records of customer interactions by documenting them in Salesforce\n\nCommunicate with Product Management and Development Teams in JIRA\n\nQualifications/ Requirements\nTechnical Requirements:\n\nPrevious technical support /Product Support experience.\n\nHigh proficiency in Microsoft Word, Excel, and PowerPoint\nCandidate should have skills/knowledge on SQL (ex: MS SQL Server, MySQL, Postgre SQL),\nGeneral experience in **Data Analytics** (e.g., Data Visualization, BI tools, Data Cubes)\nAbility to understand and troubleshoot complex systems\n\nAbility to extract meaningful information from Customer communications to understand customer intent and identify the customer s technical issue.\n\nBasic Networking knowledge (TCP/IP, DNS, SSL etc.)\n\nUnderstanding of Windows client and server environments\n\nNice to have:\n\nPrevious experience working with Salesforce.\n\nPrevious experience working with JIRA.\n\nBasic technical SQL knowledge.\n\nBasic Oracle and/or SAP knowledge.\n\nExperience with software installations, network operations, and software support\n\nKnowledge of SQL Server (i.e. able to do admin tasks such as backup/restore, understanding SQL/triggers/stored procedures etc.).\n\nUnderstanding and experience with Microsoft IIS.\n\nUnderstanding and experience with SQL Server Analysis Services\n\nWindows general troubleshooting - understanding Event Viewer logs, Windows Installer errors and logs, using the Registry Editor etc.\n\nCommunication skills:\n\nAbility to communicate correctly and clearly with both internal team members and external customers.\n\nNative or equivalent English proficiency\n\nExcellent written communication skills.\n\nValued Traits:\n\nHighly motivated and driven to perform at the highest level.\n\nNatural curiosity and willingness to learn and understand issues\n\nShows pride in producing quality deliverables.\n\nAlways being punctual and professional internally and with customers.\nEducation Requirements:\nBachelor s Degree (CSE, IT) or MCA",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['SAP', 'Networking', 'Financial planning', 'MySQL', 'Windows', 'Business strategy', 'Oracle', 'Technical support', 'Product support', 'Salesforce']",2025-06-12 13:47:43
Cloud SRE - Big Data Platform,ANZ,6 - 11 years,Not Disclosed,['Bengaluru'],"As a Cloud Big Data SRE in our Cloud Infrastructure Team, you ll play a key role in hprimarily lead the engineering and operational activities on an enterprise big data platform (EBD) on GCP. The platform is vast with Petabytes of data mostly retail and commercial bank customers which is used for banks critical customer remediation program and is significant in regulatory compliance. Additionally, the role involves providing platform engineering leadership and mentorship within the EDAE team, significantly influencing the direction and success of ANZs data capabilities on a broad scale.\nBanking is changing and we re changing with it, giving our people great opportunities to try new things, learn and grow. Whatever your role at ANZ, you ll be building your future, while helping to build ours.\nRole Type:Permanent\nRole Location:Bengaluru\n\nWhat will your day look like?\nLead SRE principles adoption in squad through monitoring of reliability of platform and applications, automate operations.\nEnsure services meet defined uptime, performance and latency targets.\nSolving ambiguous and complex data platform engineering problems.\nWork collaboratively within and across teams, Tech Areas and Domains.\nUtilise tools and practices to maintain, verify and deploy solutions in the most efficient ways, we place a high emphasis on Software Fundamentals.\nImplements a culture within the Tribe and the Chapter, encouraging best practices around reviews, quality, and documentation.\nProvide ongoing support for platforms as required e.g. problem and incident management troubleshoot.\nWhat will you bring?\n\nTo grow and be successful in this role, you will ideally bring the following:\nExperienced in managing enterprise-scale big data lake platforms on cloud, with a strong focus on platform engineering, service hosting, and dependency management across distributed systems.\nSpecialize in SRE practices including reliability engineering, observability implementation, proactive monitoring, and incident response operations to ensure platform stability and operational excellence.\nProven experience in devops, automation, manage and version codebases , configurations and infrastructure automation using Terraform.\nProficiency in Python, with experience in a secondary language like Golang or Java.\n6+ years of experience as a Cloud SRE/Engineer (preferably GCP) with applications utilizing services such as:\nBig Data Ecosystem : Hadoop cluster and technologies like Spark, Hive, and HBase deployed on cloud platforms\nWorkflow Orchestration : Airflow (or equivalent managed services) for job scheduling and pipeline orchestration\nContainerization & Orchestration : Kubernetes for scalable deployment and orchestration of containerized workloads\nObservability: Centralized logging and metrics-based monitoring using cloud-native or open-source solutions\nCloud Object Storage: Scalable storage solutions such as GCS, S3, or Azure Blob Storage\nDistributed SQL Stores: Managed relational databases like Cloud SQL, Amazon RDS, or Azure SQL\nIdentity & Access Management: Role-based access control and policy enforcement via IAM across cloud environments\nYou re not expected to have 100% of these skills. At ANZ a growth mindset is at the heart of our culture, so if you have most of these things in your toolbox, we d love to hear from you.\nSo why join us?\nANZ is a place where big things happen as we work together to provide banking and financial services across more than 30 markets. With more than 7,500 people, our Bengaluru team is the banks largest technology, data and operations centre outside Australia. In operation for over 33 years, the centre is critical in delivering the banks strategy and making an impact for our millions of customers around the world. Our Bengaluru team not only drives the transformation initiatives of the bank, it also drives a culture that makes ANZ a great place to be. Were proud that people feel they can be themselves at ANZ and 90 percent of our people feel they belong.\nWe want to continue building a diverse workplace and welcome applications from everyone. Please talk to us about any adjustments you may require to our recruitment process or the role itself. If you are a candidate with a disability or access requirements, let us know how we can provide you with additional support.\nTo find out more about working at ANZ visit https://www.anz.com/careers/ . You can apply for this role by visiting ANZ Careers and searching for reference number 98658\n.\nJob Posting End Date\n18/06/2025 , 11.59pm, (Melbourne Australia)",Industry Type: Banking,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Automation', 'Access management', 'Workflow', 'Incident management', 'Open source', 'Distribution system', 'Monitoring', 'Financial services', 'SQL', 'Python']",2025-06-12 13:47:46
Software Data Operations Engineer (BS+2),MAQ Software,2 - 5 years,Not Disclosed,['Noida'],"MAQ LLC d.b.a MAQ Software hasmultiple openings at Redmond, WA for:\nSoftware Data Operations Engineer (BS+2)\n\nResponsible for gathering & analyzing business requirements from customers. Implement,test and integrate software applications for use by customers. Develop &review cost effective data architecture to ensure appropriateness with currentindustry advances in data management, cloud & user experience. Automateuser test scenarios, debug & fix errors in cloud-based data infrastructure,reporting applications to meet customer needs. Must be able to traveltemporarily to client sites and or relocate throughout the United States.\n\nRequirements:Bachelors Degree or foreign equivalent in Computer Science, ComputerApplications, Computer Information Systems, Information Technology or relatedfield with two years of work experience in job offered, software engineer, systemsanalyst or related job.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Software Data Operations', 'software applications', 'data management', 'Data Operations', 'data architecture', 'data infrastructure']",2025-06-12 13:47:48
Big Data Lead,IQVIA,8 - 13 years,25-40 Lacs P.A.,['Bengaluru'],"Job Title / Primary Skill: Big Data Developer (Lead/Associate Manager)\nManagement Level: G150\nYears of Experience: 8 to 13 years\nJob Location: Bangalore (Hybrid)\nMust Have Skills: Big data, Spark, Scala, SQL, Hadoop Ecosystem.\nEducational Qualification: BE/BTech/ MTech/ MCA, Bachelor or masters degree in Computer Science,\n\nJob Overview\nOverall Experience 8+ years in IT, Software Engineering or relevant discipline.\nDesigns, develops, implements, and updates software systems in accordance with the needs of the organization.\nEvaluates, schedules, and resources development projects; investigates user needs; and documents, tests, and maintains computer programs.\nJob Description:\nWe look for developers to have good knowledge of Scala programming skills and Knowledge of SQL\nTechnical Skills:\nScala, Python -> Scala is often used for Hadoop-based projects, while Python and Scala are choices for Apache Spark-based projects.\nSQL -> Knowledge of SQL (Structured Query Language) is important for querying and manipulating data\nShell Script -> Shell scripts are used for batch processing of data, it can be used for scheduling the jobs and shell scripts are often used for deploying applications\nSpark Scala -> Spark Scala allows you to write Spark applications using the Spark API in Scala\nSpark SQL -> It allows to work with structured data using SQL-like queries and Data Frame APIs.\nWe can execute SQL queries against Data Frames, enabling easy data exploration, transformation, and analysis.\n\nThe typical tasks and responsibilities of a Big Data Developer include:\n1. Data Ingestion: Collecting and importing data from various sources, such as databases, logs, APIs into the Big Data infrastructure.\n2. Data Processing: Designing data pipelines to clean, transform, and prepare raw data for analysis. This often involves using technologies like Apache Hadoop, Apache Spark.\n3. Data Storage: Selecting appropriate data storage technologies like Hadoop Distributed File System (HDFS), HIVE, IMPALA, or cloud-based storage solutions (Snowflake, Databricks).",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SCALA', 'Big Data', 'Spark', 'Hive', 'Apache Pig', 'Hadoop', 'Hadoop Development', 'Mapreduce', 'Hdfs', 'Impala', 'YARN']",2025-06-12 13:47:51
Big Data Developer-STG(P),Infosys,5 - 10 years,Not Disclosed,"['Hyderabad', 'Chennai', 'Bengaluru']","Role & responsibilities\n\nA day in the life of an Infoscion\nAs part of the Infosys delivery team, your primary role would be to ensure effective Design, Development, Validation and Support activities, to assure that our clients are satisfied with the high levels of service in the technology domain.\nYou will gather the requirements and specifications to understand the client requirements in a detailed manner and translate the same into system requirements.\nYou will play a key role in the overall estimation of work requirements to provide the right information on project estimations to Technology Leads and Project Managers.\nYou would be a key contributor to building efficient programs/ systems and if you think you fit right in to help our clients navigate their next in their digital transformation journey, this is the place for you!\nIf you think you fit right in to help our clients navigate their next in their digital transformation journey, this is the place for you!\n\nPrimary skills:Technology->Functional Programming->Scala\n\nAdditional information(Optional)\nKnowledge of design principles and fundamentals of architecture\nUnderstanding of performance engineering\nKnowledge of quality processes and estimation techniques\nBasic understanding of project domain\nAbility to translate functional / nonfunctional requirements to systems requirements\nAbility to design and code complex programs\nAbility to write test cases and scenarios based on the specifications\nGood understanding of SDLC and agile methodologies\nAwareness of latest technologies and trends\nLogical thinking and problem solving skills along with an ability to collaborate\n\n\nPreferred candidate profile",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SCALA', 'Big Data', 'Spark', 'Hive', 'Kafka']",2025-06-12 13:47:54
Big Data Developer - N,Infosys,5 - 10 years,Not Disclosed,"['Hyderabad', 'Pune', 'Bengaluru']","Job description\n\nHiring for Bigdata Developer with experience range 5 to 15 years.\n\nMandatory Skills: Bigdata, Scala, Spark, Hive, Kafka\n\nEducation: BE/B.Tech/MCA/M.Tech/MSc./MSts",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Hive', 'SCALA', 'Big Data', 'Kafka', 'Spark', 'Bigdata Technologies']",2025-06-12 13:47:56
"Quant Data Specialist, Aladdin Financial Engineering - Associate",Primetrace Technologies,3 - 6 years,Not Disclosed,['Gurugram'],"About this role\nAbout Aladdin Financial Engineering (AFE):\nJoin a diverse and collaborative team of over 3 00 modelers and technologists in Aladdin Financial Engineering (AFE) within BlackRock Solutions, the business responsible for the research and development of Aladdin s financial models. This group is also accountable for analytics production, enhancing the infrastructure platform and delivering analytics content to portfolio and risk management professionals (both within BlackRock and across the Aladdin client community). The models developed and supported by AFE span a wide array of financial products covering equities, fixed income, commodities, derivatives, and private markets. AFE provides investment insights that range from an analysis of cash flows on a single bond, to the overall financial risk associated with an entire portfolio, balance sheet, or enterprise.\nRole Description:\nWe are looking for a person to join the Advanced Data Analytics team with AFE Single Security . Advanced Data Analytics is a team of Quantitative Data and Product Specialists, focused on delivering Single Security Data Content, Governance and Product Solutions and Research Platform. The team leverages data, cloud, and emerging technologies in building an innovative data platform, with the focus on business and research use cases in the S ingle S ecurity space. The team uses various statistical/mathematical methodologies to derive insights and generate content to help develop predictive models, clustering, and classification solutions and enable Governance . The team works on Mortgage, Structured & Credit Products.\nWe are looking for a person to help build and expand Data & Analytics Content in the Credit space . The person will be responsible for building, enhancing, and maintaining the Credit Content Suite . The person will work on the below -\nCredit Derived Data Content\nModel & Data Governance\nCredit Model & Analytics\nExperience\nExperience on Scala\nKnowledge of ETL, data curation and analytical jobs using distributed computing framework with Spark\nKnowledge and Experience of working with large enterprise databases like Snowflake, Cassandra & Cloud manged services like Dataproc , Databricks\nKnowledge of financial instruments like Corporate Bonds, Derivatives etc.\nKnowledge of regression methodologies\nAptitude for design and building tools for D ata Governance\nPython knowledge is a plus\nQualifications\nBachelors / masters in computer science with a major in Math, Econ, or related field\n3 - 6 years of relevant experience\nOur benefits\n\n.\nOur hybrid work model\n.\nAbout BlackRock\n.\nThis mission would not be possible without our smartest investment - the one we make in our employees. It s why we re dedicated to creating an environment where our colleagues feel welcomed, valued and supported with networks, benefits and development opportunities to help them thrive.\nFor additional information on BlackRock, please visit @blackrock | Twitter: @blackrock | LinkedIn: www.linkedin.com / company / blackrock\nBlackRock is proud to be an Equal Opportunity Employer. We evaluate qualified applicants without regard to age, disability, family status, gender identity, race, religion, sex, sexual orientation and other protected attributes at law.",Industry Type: Financial Services,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Analytical', 'Fixed income', 'Financial risk', 'Finance', 'Healthcare', 'Data analytics', 'Risk management', 'Analytics', 'Balance Sheet', 'Financial engineering']",2025-06-12 13:47:58
AI Python Data Science Engineer,Probeseven,4 - 5 years,Not Disclosed,['Coimbatore'],"AI Python Data Science Engineer\nHot Openings\nAs a data science and analytics engineer, you will be involved in developing computer visions and data algorithms. Artificial intelligence development and deep machine learning implementations will be part of your development and deployment to the cloud.\n\nExperience for senior positions: 4 - 5+ years\nExperience for junior positions: 2 - 3 years\n\nRequired Tech Skills\nExperience in Python (must have) and/or R language.\nExperience with computer vision algorithms and data science.\nExperience in deep machine learning models.\nWell-versed in data visualization techniques.\nTroubleshoot and resolve code issues.\nCollaborate with data engineers to design and integrate the data sources.\nExperience in handling multiple priorities with Agile development.\nExperience with Git and working in a collaborative and distributive team environment.\nRequired Soft Skills\nExcellent listening, verbal, and written communication skills.\nStrong interpersonal & customer relationship skills.\nStrong analytical, problem solving, and decision-making skills.\nDocumentation skills.\nApply now\nHot Openings\nPHP + Node.js Developers\nFull Time\nTech Development\nExperience 4 - 6+ years",Industry Type: Film / Music / Entertainment,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer vision', 'GIT', 'data science', 'Analytical', 'Artificial Intelligence', 'Machine learning', 'PHP', 'Customer relationship', 'Analytics', 'Python']",2025-06-12 13:48:01
"Sr Validation Engineer, R&D Data Catalyst Team",Amgen Inc,4 - 6 years,Not Disclosed,['Hyderabad'],"Role Description:\nThe Validation and Testing Leadis responsible forleading the testing activities for software applications and solutions that meet business needs and ensuring the availability of critical systems and applications. This roleis for a lead tester with experience testing data management solutions and data analytics products, and with experience with designing and executing testing for GxP validated systems. The role involves working closely with product managers, designers, and other engineers to test high-quality, scalable software solutions.\nRoles & Responsibilities:\nParticipate inrequirementdiscussionsin order to create test scripts.\nBuild test scriptsper implementation project plan by working with various members of the product team and business partners.\nConduct informal and formal testing, consolidate all the findings and coordinate with developer(s) and business partners to resolve all the issues\nPerform regression testing to verify the changes do not negatively impact existing system functionality\nCommunicate potential risks and contingency plans with project management to ensure process compliance with all regulatory and Amgen procedural requirements\nIdentify and resolve technical challenges/bugs effectively\nWork closely with cross-functional teams, including product management, design, and QA, to deliver high-quality software on time\nSupport the creating and implementation of automated testing frameworks to improve efficiency and consistency\nBasic Qualifications and Experience:\nMasters degree with 4 - 6 years of experience in Computer Science, IT or related field OR\nBachelors degree with 6 - 8 years of experience in Computer Science, IT or related field\nDiploma and 10 to 12 years of experience in Computer Science, IT or related field\nFunctional Skills:\nMust-Have Skills:\nKnowledge of GxPsoftware validation processes\nExperience with test management in Jiraand test automation practices\nExpert sql skills for data profiling and creation of test data\nStrong experience testing analytics and data platforms\nGood Problem-solving skills - Identifying and fixing bugs, adapting to changes\nExcellent communication skills - Explaining design decisions, collaborating with teams\nExperienced in Agile methodology\nGood-to-Have Skills:\nExperience in Risk-based Approach to Change Management of Validated GxP Systems\nExperience with cloud-based technologies\nProfessional Certifications (please mention if the certification is preferred or mandatory for the role):\nSAFE for Teams certification (Preferred)\nSoft Skills:\nExcellent analytical and troubleshooting skills\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation\nAbility to manage multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals\nStrong presentation and public speaking skills",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Validation engineering', 'project management', 'data analytics', 'data validation', 'troubleshooting', 'agile', 'change management', 'sql', 'data profiling', 'jira']",2025-06-12 13:48:04
Senior Engineering Manager - Data Operations,Amgen Inc,12 - 15 years,Not Disclosed,['Hyderabad'],"ABOUT THE ROLE\nRole Description:\nWe are seeking a seasoned Senior Engineering Manager(Data Engineering) to lead the end-to-end management of enterprise data assets and operational data workflows. This role is critical in ensuring the availability, quality, consistency, and timeliness of data across platforms and functions, supporting analytics, reporting, compliance, and digital transformation initiatives.As a senior leader in the data organization, you will oversee the day-to-day data operations, manage a team of data professionals, and drive process excellence in data intake, transformation, validation, and delivery. You will work closely with cross-functional teams including data engineering, analytics, IT, governance, and business stakeholders to align operational data capabilities with enterprise needs.\nRoles & Responsibilities:\nLead and manage the enterprise data operations team, responsible for data ingestion, processing, validation, quality control, and publishing to various downstream systems.\nDefine and implement standard operating procedures for data lifecycle management, ensuring availability, accuracy, completeness, and integrity of critical data assets.\nOversee and continuously improve daily operational workflows, including scheduling, monitoring, and troubleshooting data jobs across cloud and on-premise environments.\nEstablish and track key data operations metrics (SLAs, throughput, latency, data quality, incident resolution) and drive continuous improvements.\nPartner with data engineering and platform teams to optimize pipelines, support new data integrations, and ensure scalability and resilience of operational data flows.\nCollaborate with data governance, compliance, and security teams to maintain regulatory compliance, data privacy, and access controls.\nServe as the primary escalation point for data incidents and outages, ensuring rapid response and root cause analysis.\nBuild strong relationships with business and analytics teams to understand data consumption patterns, prioritize operational needs, and align with business objectives.\nDrive adoption of best practices for documentation, metadata, lineage, and change management across data operations processes.\nMentor and develop a high-performing team of data operations analysts and leads.\nFunctional Skills:\nMust-Have Skills:\nExperience managing a team of data engineers in biotech/pharmadomain companies.\nExperience in designing and maintainingdata pipelines and analytics solutions that extract, transform, and load data from multiple source systems.\nDemonstrated hands-on experience with cloud platforms (AWS) and the ability to architect cost-effective and scalable data solutions.\nExperience managing data workflows on Databricks in cloud environments such as AWS, Azure, or GCP.\nStrong problem-solving skills with the ability to analyze complex data flow issues and implement sustainable solutions.\nWorking knowledge of SQL, Python, PySparkor scripting languages for process monitoring and automation.\nExperience collaborating with data engineering, analytics, IT operations, and business teams in a matrixed organization.\nFamiliarity with data governance, metadata management, access control, and regulatory requirements (e.g., GDPR, HIPAA, SOX).\nExcellent leadership, communication, and stakeholder engagement skills.\nWell versed with full stack development& DataOps automation, logging & observability frameworks, and pipeline orchestration tools.\nStrong analytical and problem-solving skills to address complex data challenges.\nEffective communication and interpersonal skills to collaborate with cross-functional teams.\nGood-to-Have Skills:\nData Engineering Management experience in Biotech/Life Sciences/Pharma\nExperience using graph databases such as Stardog or Marklogic or Neo4J or Allegrograph, etc.\nEducation and Professional Certifications\n12 to 15 years of experience in Computer Science, IT or related field\nDatabricks Certificate preferred\nScaled Agile SAFe certification preferred\nExperience in life sciences, healthcare, or other regulated industries with large-scale operational data environments.\nFamiliarity with incident and change management processes (e.g., ITIL).\nSoft Skills:\nExcellent analytical and troubleshooting skills\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation\nAbility to manage multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals\nStrong presentation and public speaking skills.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Operations', 'Azure', 'Data Engineering', 'Neo4J', 'GCP', 'Engineering Management', 'troubleshooting', 'Stardog', 'Marklogic', 'AWS']",2025-06-12 13:48:06
"Sr. Staff Engineer, Data Frameworks",NetSkope Software,10 - 15 years,Not Disclosed,"['Kolkata', 'Mumbai', 'New Delhi', 'Hyderabad', 'Pune', 'Chennai', 'Bengaluru']","As a Sr. Staff Engineer on the Data Engineering Team you'll be working on some of the hardest problems in the field of Data, Cloud and Security with a mission to achieve the highest standards of customer success. You will be building blocks of technology that will define Netskope s future. You will leverage open source Technologies around OLAP, OLTP, Streaming, Big Data and ML models. You will help design, and build an end-to-end system to manage the data and infrastructure used to improve security insights for our global customer base.\nYou will be part of a growing team of renowned industry experts in the exciting space of Data and Cloud Analytics\nYour contributions will have a major impact on our global customer-base and across the industry through our market-leading products\nYou will solve complex, interesting challenges, and improve the depth and breadth of your technical and business skills.\nWhat you will be doing\nConceiving and building services used by Netskope products to validate, transform, load and perform analytics of large amounts of data using distributed systems with cloud scale and reliability.\nHelping other teams architect their applications using services from the Data team wile using best practices and sound designs.\nEvaluating many open source technologies to find the best fit for our needs, and contributing to some of them.\nWorking with the Application Development and Product Management teams to scale their underlying services\nProviding easy-to-use analytics of usage patterns, anticipating capacity issues and helping with long term planning\nLearning about and designing large-scale, reliable enterprise services.\nWorking with great people in a fun, collaborative environment.\nCreating scalable data mining and data analytics frameworks using cutting edge tools and techniques\nRequired skills and experience\n10+ years of industry experience building highly scalable distributed Data systems\nProgramming experience in Python, Java or Golang\nExcellent data structure and algorithm skills\nProven good development practices like automated testing, measuring code coverage.\nProven experience developing complex Data Platforms and Solutions using Technologies like Kafka, Kubernetes, MySql, Hadoop, Big Query and other open source databases\nExperience designing and implementing large, fault-tolerant and distributed systems around columnar data stores.\nExcellent written and verbal communication skills\nBonus points for contributions to the open source community\nEducation\nBSCS or equivalent required, MSCS or equivalent strongly preferred",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Product management', 'data security', 'MySQL', 'OLAP', 'Application development', 'Open source', 'Data mining', 'OLTP', 'Distribution system', 'Python']",2025-06-12 13:48:08
Data Science & AI Engineer,Blue Altair,5 - 8 years,Not Disclosed,['Pune'],"Greetings from Blue Altair!\nJob Overview:\nWe are seeking an experienced and highly skilled Data Science and AI Engineer to join our dynamic team. The ideal candidate will have 5+ years of experience working on cutting-edge data science and AI technologies across various cloud platforms with a strong focus to work on LLMs and SLMs. The role demands a professional capable of performing in a client-facing environment, as well as mentoring and guiding junior team members.\n\nTitle: Consultant/Sr. Consultant - Data Science Engineer\nExperience: 5-8 years\nLocation: Pune/Bangalore (Hybrid)\n\nRoles and responsibilities:\nDevelop, implement, and optimize machine learning models and AI algorithms to solve complex business problems.\nDesign, build, and fine-tune AI models, particularly focusing on LLMs and SLMs, using state-of-the-art techniques and architectures.\nApply advanced techniques in prompt engineering, model fine-tuning, and optimization to tailor models for specific business needs.\nDeploy and manage machine learning models and pipelines on cloud platforms (AWS, GCP, Azure, etc.).\nWork closely with clients to understand their data and AI needs and provide tailored solutions.\nCollaborate with cross-functional teams to integrate AI solutions into broader software architectures.\nMentor junior team members and provide guidance in implementing best practices in data science and AI development.\nStay up-to-date with the latest trends and advancements in data science, AI, and cloud technologies.\nPrepare technical documentation and present insights to both technical and non-technical stakeholders.\n\nRequirement:\n5+ years of experience in data science, machine learning, and AI technologies.\nProven experience working with cloud platforms such as Google Cloud, Microsoft Azure, or AWS.\nExpertise in programming languages such as Python, R, Julia, and AI frameworks like TensorFlow, PyTorch, Scikit-learn, Hugging face Transformers.\nKnowledge of data visualization tools (e.g., Matplotlib, Seaborn, Tableau)\nSolid understanding of data engineering concepts including ETL, data pipelines, and databases (SQL, NoSQL).\nExperience with MLOps practices and deployment of models in production environments.\nFamiliarity with NLP (Natural Language Processing) tasks and working with large-scale datasets.\nHands-on experience with generative AI models like GPT, Gemini, Claude, Mistral etc.\nClient-facing experience with strong communication skills to manage and engage stakeholders.\nStrong problem-solving skills and analytical mindset.\nAbility to work independently and as part of a team and mentor and provide technical leadership to junior team members.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['LLMs', 'Artificial Intelligence', 'MLOps', 'RAG', 'Natural Language Processing', 'Neural Networks', 'LLM', 'Machine Learning', 'AI Models', 'Data Science', 'PyTorch', 'SLM', 'AI Automation']",2025-06-12 13:48:11
Data Analytics Engineer,Automotive Industry,5 - 6 years,Not Disclosed,['Chennai'],"Position: Data Analytics Engineer\nExp: 5 -6 years\nNP: Immediate - 30 days\nQualification: B.tech\nLocation: Chennai Hybrid\nPrimary: Google Cloud Platform ,Python skills and Big Data pipeline\nSecondary: Big Query SQ, coding, testing, implementing, debugging workflows and apps\nKindly share your updated resume to aishwarya_s@onwardgroup.com\nKindly fill the below details\nTotal Exp:\nRelevant Exp:\nNotice Period: CTC:\nECTC:\nIf servicing NP, Last working Day, offered location & CTC:\nAvailable for Video modes interview on Weekdays (Y/N) :\nPAN Number:\nName as Per PAN Card:\nDate of Birth:\nAlternative Contact No:\nReason for Job Change:",Industry Type: Automobile,Department: Engineering - Hardware & Networks,"Employment Type: Full Time, Permanent","['GCP', 'Bigquery', 'Google Cloud Platform', 'Query SQ', 'Python']",2025-06-12 13:48:13
Senior Associate Data Security Engineer,Amgen Inc,1 - 3 years,Not Disclosed,['Hyderabad'],"What you will do\nIn this vital role the Senior Associate Data Security Engineer role will cover Data Loss Prevention (DLP) and Data Security Posture Management (DSPM) technologies. This role will report to the Manager, Data Security. This position will provide essential services that enable us to better pursue our mission.\nSr. Associate Data Security Engineers operate, manage, and improve Amgens DLP and DSPM solutions. In our Data Security team, they will operate data protection security technologies in a rapidly changing global security sector. They will work with other engineers and business units to help craft, build, coordinate, configure, and implement critical preventive and detective security controls related to the protection of Amgen data.\nThis engineer will play a key role in designing, deploying, and maintaining solutions to build our rapidly growing operations.\nRoles & Responsibilities:\nMaintain the service delivery and working order of Amgen data security solutions across Amgens global enterprise\nExecute Amgen service management processes such as Incident Management, change processes, and service improvements for Amgens data security technologies\nAssist in the design and improvement of Amgens data security technologies and solutions. Build scripts for the configuration and the testing of the solution\nManage and perform analysis of escalated DLP events, engage with the business, fulfill legal hold requests, and provide executive reporting\nWork with business domain specialists to collect, analyze, build, tune and automate DLP policy sets\nAnalyze events and logs for suspicious activity and opportunities to improve posture, processes, procedures, and protections.\nConsult to the Incident Response team on investigations\nDevelop automation solutions in increase response times and reduce risk of identified incidents\nParticipate in regular meetings and conference calls with the client, IT, business partners and vendors to help ensure technical coverage for new or existing projects across the business\n\n\nFunctional Skills:\nMust-Have Skills:\nKnowledge of Cloud Access Security Platforms (Elastica, Netskope, SkyHigh, etc)\nUnderstanding of cloud and SAAS environments (AWS, O365, Box, Salesforce, etc)\nSolid experience with potential to grow knowledge in Linux/Windows OS and other infrastructure systems\nExperience with DLP and data protection technologies for a large global enterprise\nDemonstrated understanding on how emerging security technologies and data flows interoperate across complex, multi-cloud systems.\n\nBasic Qualifications:\nMasters degree and 1 to 3 years of experience OR\nBachelors degree and 3 to 5 years of experience OR\nDiploma and 7 to 9 years of experience.\nPreferred Qualifications:\nGood-to-Have Skills:\nComfort with scripting (PowerShell, Python, etc) and expression development (SQL, Regex)\nAbility to develop documentation for Infrastructure Security implementations\nBasic experience with ITIL processes such as Incident, Problem, and configuration management\nExperience in complex enterprise environments with competing business priorities\nProfessional Certifications (please mention if the certification is preferred or mandatory for the role):\nSystems Security Certified Practitioner (SSCP) or Security+\nSANS Certifications\nRelevant vendor-specific certifications\n\n\nSoft Skills:\nEstablished analytical and gap/fit assessment skills.\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nTeam-oriented, with a focus on achieving team goals\nEffective presentation and public speaking skills.",Industry Type: Pharmaceutical & Life Sciences,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Data Security', 'PowerShell', 'configuration management', 'Cloud Access Security', 'Linux', 'Incident management', 'ITIL', 'Python', 'SQL']",2025-06-12 13:48:15
Big Data Developer/ Senior Big Data Developer,Grid Dynamics,5 - 10 years,Not Disclosed,"['Hyderabad', 'Chennai', 'Bengaluru']","About us\nGrid Dynamics (NASDAQ: GDYN) is a leading provider of technology consulting, platform and product engineering, AI, and advanced analytics services. Fusing technical vision with business acumen, we solve the most pressing technical challenges and enable positive business outcomes for enterprise companies undergoing business transformation. A key differentiator for Grid Dynamics is our 8 years of experience and leadership in enterprise AI, supported by profound expertise and ongoing investment in data, analytics, cloud & DevOps, application modernization and customer experience. Founded in 2006, Grid Dynamics is headquartered in Silicon Valley with offices across the Americas, Europe, and India.\n\nRole & responsibilities\nWe are looking for an enthusiastic and technology-proficient Big Data Engineer, who is eager to participate in the design and implementation of a top-notch Big Data solution to be deployed at massive scale.\nOur customer is one of the world's largest technology companies based in Silicon Valley with operations all over the world. On this project we are working on the bleeding-edge of Big Data technology to develop high performance data analytics platform, which handles petabytes datasets.\nEssential functions\nParticipate in design and development of Big Data analytical applications.\nDesign, support and continuously enhance the project code base, continuous integration pipeline, etc.\nWrite complex ETL processes and frameworks for analytics and data management.\nImplement large-scale near real-time streaming data processing pipelines.\nWork inside the team of industry experts on the cutting edge Big Data technologies to develop solutions for deployment at massive scale.\nQualifications\nStrong coding experience with Scala, Spark,Hive, Hadoop.\nIn-depth knowledge of Hadoop and Spark, experience with data mining and stream processing technologies (Kafka, Spark Streaming, Akka Streams).\nUnderstanding of the best practices in data quality and quality engineering.\nExperience with version control systems, Git in particular.\nDesire and ability for quick learning of new tools and technologies.\nWould be a plus\nKnowledge of Unix-based operating systems (bash/ssh/ps/grep etc.).\nExperience with Github-based development processes.\nExperience with JVM build systems (SBT, Maven, Gradle).\nWe offer\nOpportunity to work on bleeding-edge projects\nWork with a highly motivated and dedicated team\nCompetitive salary\nFlexible schedule\nBenefits package - medical insurance, sports\nCorporate social events\nProfessional development opportunities\nWell-equipped office",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SCALA', 'Big Data', 'Spark']",2025-06-12 13:48:17
Avaloq Software Engineer (Data),Luxoft,3 - 5 years,Not Disclosed,[],"Your expertise in Avaloq technologies, database management, and software development combined with a solid understanding of the Avaloq Enterprise-Wide Object Model will be critical in shaping and enhancing our data capabilities. This is a fantastic opportunity to bring your passion for innovation into a fast-paced, dynamic environment where your impact will be tangible.\nSkills\nMust have\nYou will have between 3-5 years experience working as an Avaloq Developer.\nMapping physical data from Avaloq and other platforms to the Enterprise Data Model, ensuring adherence to reference data standards and clear conceptual definitions.\nPerforming data quality assessments, developing strategies to enhance data integrity, and creating Avaloq system functionalities to mitigate the risk of poor data at source.\nSupporting option analysis, data profiling, and interfacing with external rule/result repositories.\nReviewing and optimising Avaloq APDM outcomes, defining treatment strategies, and improving system performance.\nIntegrating the Avaloq APDM with external data minimisation orchestration tools.\nDesigning and delivering data quality improvement solutions, including mass-data manipulation scripts and associated testing and reconciliation processes.\nAnalysing business requirements and developing tailored software solutions.\nSupporting technical analysis and enhancements for Avaloq change requests and incidents.\nCollaborating with stakeholders to build alignment around technology change initiatives.\nBuilding and maintaining synthetic data delivery routines for test environments.\nManaging market data ingestion into the Avaloq Core Platform (ACP) via third-party tools.\nNice to have\nACCP certification\nOther\nLanguages\nEnglish: C2 Proficient\nSeniority\nSenior\nRefer a Friend\nPositive work environments and stellar reputations attract and retain top talent. Find out why Luxoft stands apart from the rest.\nRecommend a friend\nRelated jobs View all vacancies\nData Modelling Expert (with Avaloq experience)\nAvaloq\nIndia\nBengaluru\nAvaloq Technical Lead\nAvaloq\nAustralia\nSydney\nSenior Avaloq Engineer\nAvaloq\nAustralia\nSydney\nRemote India, India\nReq. VR-114137\nAvaloq\nBCM Industry\n21/05/2025\nReq. VR-114137\nApply for Avaloq Software Engineer (Data) in Remote India\n*",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Technical analysis', 'Quality improvement', 'orchestration', 'Data modeling', 'Reconciliation', 'Technical Lead', 'Data quality', 'market data', 'data integrity', 'Software solutions']",2025-06-12 13:48:19
Lead Engineer - Data Science,Sasken Technologies,5 - 8 years,Not Disclosed,['Bengaluru'],"Job Summary\nPerson at this position takes ownership of a module and associated quality and delivery. Person at this position provides instructions, guidance and advice to team members to ensure quality and on time delivery.\nPerson at this position is expected to be able to instruct and review the quality of work done by technical staff.\nPerson at this position should be able to identify key issues and challenges by themselves, prioritize the tasks and deliver results with minimal direction and supervision.\nPerson at this position has the ability to investigate the root cause of the problem and come up alternatives/ solutions based on sound technical foundation gained through in-depth knowledge of technology, standards, tools and processes.\nPerson has the ability to organize and draw connections among ideas and distinguish between those which are implementable.\nPerson demonstrates a degree of flexibility in resolving problems/ issues that atleast to in-depth command of all techniques, processes, tools and standards within the relevant field of specialisation.\n\n\nRoles & Responsibilities\nResponsible for requirement analysis and feasibility study including system level work estimation while considering risk identification and mitigation.\nResponsible for design, coding, testing, bug fixing, documentation and technical support in the assigned area. Responsible for on time delivery while adhering to quality and productivity goals.\nResponsible for traceability of the requirements from design to delivery Code optimization and coverage.\nResponsible for conducting reviews, identifying risks and ownership of quality of deliverables.\nResponsible for identifying training needs of the team.\nExpected to enhance technical capabilities by attending trainings, self-study and periodic technical assessments.\nExpected to participate in technical initiatives related to project and organization and deliver training as per plan and quality.\nExpected to be a technical mentor for junior members.\nPerson may be given additional responsibility of managing people based on discretion of Project Manager.\n\nEducation and Experience Required\nEngineering graduate, MCA, etc Experience: 5-8 years\n\n\nCompetencies Description\nData Science TCB is applicable to one who\n1) Analyses data to arrive at patterns/Insights/models\n2) Come up with models based on the data to provide recommendations, predictive analytics etc\n3) Provides implementation of the models in R, Matlab etc\n4) Can understand and apply machine learning/AI techniques\nPlatforms-\nUnix\nTools-\nR, Matlab, Spark Machine Learning, Python-ML, SPSS, SAS\nLanguages-\nR, Perl, Python, Scala\nSpecialization-\nCOGNITIVE ANALYTICS INCLUDING COMPUTER VISION, AI and ML, STATISTICS",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Unix', 'R', 'SAS', 'Scala', 'Perl', 'SPSS', 'Machine Learning', 'Python']",2025-06-12 13:48:22
Big Data Developer,Techstar Group,7 - 10 years,Not Disclosed,['Hyderabad'],"Responsibilities of the Candidate :\n\n- Be responsible for the design and development of big data solutions. Partner with domain experts, product managers, analysts, and data scientists to develop Big Data pipelines in Hadoop\n\n- Be responsible for moving all legacy workloads to a cloud platform\n\n- Work with data scientists to build Client pipelines using heterogeneous sources and provide engineering services for data PySpark science applications\n\n- Ensure automation through CI/CD across platforms both in cloud and on-premises\n\n- Define needs around maintainability, testability, performance, security, quality, and usability for the data platform\n\n- Drive implementation, consistent patterns, reusable components, and coding standards for data engineering processes\n\n- Convert SAS-based pipelines into languages like PySpark, and Scala to execute on Hadoop and non-Hadoop ecosystems\n\n- Tune Big data applications on Hadoop and non-Hadoop platforms for optimal performance\n\n- Apply an in-depth understanding of how data analytics collectively integrate within the sub-function as well as coordinate and contribute to the objectives of the entire function.\n\n- Produce a detailed analysis of issues where the best course of action is not evident from the information available, but actions must be recommended/taken.\n\n- Assess risk when business decisions are made, demonstrating particular consideration for the firm's reputation and safeguarding Citigroup, its clients, and assets, by driving compliance with applicable laws, rules, and regulations, adhering to Policy, applying sound ethical judgment regarding personal behavior, conduct, and business practices, and escalating, managing and reporting control issues with transparency\n\nRequirements :\n\n- 6+ years of total IT experience\n\n- 3+ years of experience with Hadoop (Cloudera)/big data technologies\n\n- Knowledge of the Hadoop ecosystem and Big Data technologies Hands-on experience with the Hadoop eco-system (HDFS, MapReduce, Hive, Pig, Impala, Spark, Kafka, Kudu, Solr)\n\n- Experience in designing and developing Data Pipelines for Data Ingestion or Transformation using Java Scala or Python.\n\n- Experience with Spark programming (Pyspark, Scala, or Java)\n\n- Hands-on experience with Python/Pyspark/Scala and basic libraries for machine learning is required.\n\n- Proficient in programming in Java or Python with prior Apache Beam/Spark experience a plus.\n\n- Hand on experience in CI/CD, Scheduling and Scripting\n\n- Ensure automation through CI/CD across platforms both in cloud and on-premises\n\n- System level understanding - Data structures, algorithms, distributed storage & compute\n\n- Can-do attitude on solving complex business problems, good interpersonal and teamwork skills",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Big Data', 'Hive', 'Data Engineering', 'Data Pipeline', 'PySpark', 'Hadoop', 'Kafka', 'HDFS', 'Spark', 'Python']",2025-06-12 13:48:25
Advanced Data Science Associate,ZS,0 - 2 years,Not Disclosed,['Bengaluru'],"Develop advanced and efficient statistically effective algorithms that solve problems of high dimensionality .\nUtilize technical skills such as hypothesis testing, machine learning and retrieval processes to apply statistical and data mining techniques to identify trends, create figures, and analyze other relevant information.\nCollaborate with clients and other stakeholders at ZS to integrate and effectively communicate analysis findings.\nContribute to the assessment of emerging datasets and technologies that impact our analytical",,,,"['Text mining', 'Analytical', 'Management consulting', 'Financial planning', 'Machine learning', 'Hypothesis Testing', 'Predictive modeling', 'Data mining', 'big data']",2025-06-12 13:48:27
"Data Analyst, Staff",Qualcomm,4 - 7 years,Not Disclosed,['Bengaluru'],"Job Area: Miscellaneous Group, Miscellaneous Group > Data Analyst\n \n\nQualcomm Overview: \nQualcomm is a company of inventors that unlocked 5G ushering in an age of rapid acceleration in connectivity and new possibilities that will transform industries, create jobs, and enrich lives. But this is just the beginning. It takes inventive minds with diverse skills, backgrounds, and cultures to transform 5Gs potential into world-changing technologies and products. This is the Invention Age - and this is where you come in.\n\nGeneral Summary:\n\nAbout the Team\n\nQualcomm's People Analytics team plays a crucial role in transforming data into strategic workforce insights that drive HR and business decisions. As part of this lean but high-impact team, you will have the opportunity to analyze workforce trends, ensure data accuracy, and collaborate with key stakeholders to enhance our data ecosystem. This role is ideal for a generalist who thrives in a fast-paced, evolving environment""”someone who can independently conduct data analyses, communicate insights effectively, and work cross-functionally to enhance our People Analytics infrastructure.\n\nWhy Join Us\n\n\nEnd-to-End ImpactWork on the full analytics cycle""”from data extraction to insight generation""”driving meaningful HR and business decisions.\n\n\nCollaboration at ScalePartner with HR leaders, IT, and other analysts to ensure seamless data integration and analytics excellence.\n\n\nData-Driven CultureBe a key player in refining our data lake, ensuring data integrity, and influencing data governance efforts.\n\n\nProfessional GrowthGain exposure to multiple areas of people analytics, including analytics, storytelling, and stakeholder engagement.\n\n\nKey Responsibilities\n\n\nPeople Analytics & Insights\nAnalyze HR and workforce data to identify trends, generate insights, and provide recommendations to business and HR leaders.\nDevelop thoughtful insights to support ongoing HR and business decision-making.\nPresent findings in a clear and compelling way to stakeholders at various levels, including senior leadership.\n\n\nData Quality & Governance\nEnsure accuracy, consistency, and completeness of data when pulling from the data lake and other sources.\nIdentify and troubleshoot data inconsistencies, collaborating with IT and other teams to resolve issues.\nDocument and maintain data definitions, sources, and reporting standards to drive consistency across analytics initiatives.\n\n\nCollaboration & Stakeholder Management\nWork closely with other analysts on the team to align methodologies, share best practices, and enhance analytical capabilities.\nAct as a bridge between People Analytics, HR, and IT teams to define and communicate data requirements.\nPartner with IT and data engineering teams to improve data infrastructure and expand available datasets.\n\n\nQualifications\n\nRequired4-7 years experience in a People Analytics focused role\n\n\nAnalytical & Technical Skills\nStrong ability to analyze, interpret, and visualize HR and workforce data to drive insights.\nExperience working with large datasets and ensuring data integrity.\nProficiency in Excel and at least one data visualization tool (e.g., Tableau, Power BI).\n\n\nCommunication & Stakeholder Management\nAbility to communicate data insights effectively to both technical and non-technical audiences.\nStrong documentation skills to define and communicate data requirements clearly.\nExperience collaborating with cross-functional teams, including HR, IT, and business stakeholders.\n\n\nPreferred:\n\n\nTechnical Proficiency\nExperience with SQL, Python, or R for data manipulation and analysis.\nFamiliarity with HR systems (e.g., Workday) and cloud-based data platforms.\n\n\nPeople Analytics Expertise\nPrior experience in HR analytics, workforce planning, or related fields.\nUnderstanding of key HR metrics and workforce trends (e.g., turnover, engagement, diversity analytics).\n\n\nAdditional Information\nThis is an office-based position (4 days a week onsite) with possible locations that may include India and Mexico",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['sql', 'people analytics', 'documentation', 'tableau', 'data integration tools', 'hiring', 'data warehousing', 'data architecture', 'sourcing', 'jquery', 'staffing', 'plsql', 'oracle 10g', 'java', 'etl tool', 'html', 'etl', 'mongodb', 'python', 'oracle', 'power bi', 'hrsd', 'r', 'node.js', 'hr analytics', 'angularjs']",2025-06-12 13:48:30
Analyst - Data Analytics,AMERICAN EXPRESS,0 - 4 years,Not Disclosed,['Gurugram'],The American Express Enterprise Digital Experimentation & Analytics (EDEA) leads the Enterprise Product Analytics and Experimentation charter for Brand & Performance Marketing and Digital Acquisition & Membership experiences as we'll as Enterprise Platforms. The focus of this collaborative team is to drive growth by enabling efficiencies in paid performance channels & evolve our digital experiences with actionable insights & analytics. The team specializes in using data around digital product usage to drive improvements in the acquisition customer experience to deliver higher satisfaction and business value.\n,,,,"['Mining', 'Career development', 'Finance', 'Analytical', 'Data processing', 'Analytics', 'SQL']",2025-06-12 13:48:32
Big Data Engineer_ Info Edge _Noida,Info Edge,1 - 4 years,14-19 Lacs P.A.,['Noida'],"About Info Edge India Ltd.\nInfo Edge: Info Edge (India) Limited (NSE: NAUKRI) is among the leading internet companies in India. Info Edge, Indias premier online classifieds company is fundamentally in the matching business. With a network of 62 offices located in 43 cities throughout India, Info Edge has 5000 plus employees engaged in innovation, product development, integration with mobile and social media, technology and technology updation, research and development, quality assurance, sales, marketing and payment collection.\nThe umbrella brand has an online recruitment classifieds, www.naukri.com– India’s No. 1 Jobsite with over 75% traffic share, a matrimony classifieds, www.jeevansathi.com, a real estate classifieds, www.99acres.com– India’s largest property marketplace and an education classifieds, www.shiksha.com. Find out more about the Company at",,,,"['Data Modeling', 'Python', 'SCALA']",2025-06-12 13:48:35
Data Bricks,PwC India,7 - 12 years,Not Disclosed,['Bengaluru'],"Job Summary:\n\nWe are seeking a talented Data Engineer with strong expertise in Databricks, specifically in Unity Catalog, PySpark, and SQL, to join our data team. Youll play a key role in building secure, scalable data pipelines and implementing robust data governance strategies using Unity Catalog.\n\nKey Responsibilities:",,,,"['DataBricks', 'Data Bricks', 'Pyspark', 'Delta Lake', 'Databricks Engineer', 'Unity Catalog', 'SQL']",2025-06-12 13:48:37
AVP Data Management Analyst,Wells Fargo,2 - 7 years,Not Disclosed,['Bengaluru'],"About this role:\nWells Fargo is seeking a Data Management Analyst\n\nIn this role, you will:\nParticipate in less complex analysis to identify and remediate data quality or integrity issues and to identify and remediate process or control gaps\nAdhere to data governance standards and procedures",,,,"['Data Management', 'Agile Methodology', 'Funds Transfer Pricing', 'Financial Data Mapping', 'Big Data Query Techniques', 'Lineage Tracing', 'Data warehousing', 'Data Governance', 'Jira', 'Market Risks', 'SQL']",2025-06-12 13:48:39
Data Governance Engineers,Meritus Management Service,4 - 9 years,14-17 Lacs P.A.,"['Pune', 'Gurugram']","Define, implement, & enforce data governance policies & standards to ensure data quality, consistency, & compliance across the organization\nCollaborate with data stewards, business users, & IT teams to maintain metadata, lineage, & data catalog tools",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Stewardship', 'Metadata', 'Data Governance', 'Metadata Management', 'Data Lineage', 'Data Modeling', 'SQL']",2025-06-12 13:48:41
Data Analyst - Gurugram,Infosys,5 - 10 years,Not Disclosed,"['Chennai', 'Bengaluru', 'PAN INDIA']","Responsibilities:\nUnderstand architecture requirements and ensure effective design, development, validation, and support activities.\nAnalyze user requirements, envisioning system features and functionality.\nIdentify bottlenecks and bugs, and recommend system solutions by comparing advantages and disadvantages of custom development.\nContribute to team meetings, troubleshooting development and production problems across multiple environments and operating platforms.\nEnsure effective design, development, validation, and support activities for Big Data solutions.\nTechnical and Professional Requirements:\nSkills:\nProficiency in Scala, Spark, Hive, and Kafka.\nIn-depth knowledge of design issues and best practices.\nSolid understanding of object-oriented programming.\nFamiliarity with various design, architectural patterns, and software development processes.\nExperience with both external and embedded databases.\nCreating database schemas that represent and support business processes.\nImplementing automated testing platforms and unit tests.\nPreferred Skills:\nTechnology -> Big Data -> Scala, Spark, Hive, Kafka\nAdditional Responsibilities:\nCompetencies:\nGood verbal and written communication skills.\nAbility to communicate with remote teams effectively.\nHigh flexibility to travel.\nEducational Requirements:Master of Computer Applications, Master of Technology, Master of Engineering, MSc, Bachelor of Technology, Bachelor of Computer Applications, Bachelor of Computer Science, Bachelor of Engineering",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Spark', 'Hive', 'Hadoop', 'Big Data', 'Kafka']",2025-06-12 13:48:44
Data Scientist,Amgen Inc,1 - 6 years,Not Disclosed,['Hyderabad'],"ABOUT THE ROLE\nRole Description:\nThe Data Scientist is responsible for developing and implementing AI-driven solutions to enhance cybersecurity measures within the organization. This role involves leveraging data science techniques to analyze security data, detect threats, and automate security processes. The Data Scientist will work closely with cybersecurity teams to identify data-driven automation opportunities, strengthening the organizations security posture.\nRoles & Responsibilities:\nDevelop analytics to address security concerns, enhancements, and capabilities to improve the organization's security posture.\nCollaborate with Data Engineers to translate security-focused algorithms into effective solutions.\nWork in technical teams in development, deployment, and application of applied analytics, predictive analytics, and prescriptive analytics.\nPerform exploratory and targeted data analyses using descriptive statistics and other methods to identify security patterns and anomalies.\nDesign and implement security-focused analytics pipelines leveraging MLOps practices.\nCollaborate with data engineers on data quality assessment, data cleansing, and the development of security-related data pipelines.\nContribute to data engineering efforts to refine data infrastructure and ensure scalable, efficient security analytics.\nGenerate reports, annotated code, and other projects artifacts to document, archive, and communicate your work and outcomes.\nShare and discuss findings with team members practicing SAFe Agile delivery model.\nFunctional Skills:\nBasic Qualifications:\nMasters degree and 1 to 3 years of experience with one or more analytic software tools or languages (e.g., SAS, SPSS, R, Python) OR\nBachelors degree and 3 to 5 years of experience with one or more analytic software tools or languages (e.g., SAS, SPSS, R, Python) OR\nDiploma and 7 to 9 years of experience with one or more analytic software tools or languages (e.g., SAS, SPSS, R, Python)\nPreferred Qualifications:\nExperience with one or more analytic software tools or languages (e.g., SAS, SPSS, R, Python)\nDemonstrated skill in the use of applied analytics, descriptive statistics, feature extraction and predictive analytics on industrial datasets\nStrong foundation in machine learning algorithms and techniques\nExperience in statistical techniques and hypothesis testing, experience with regression analysis, clustering and classification\nGood-to-Have Skills:\nProficiency in Python and relevant ML libraries (e.g., TensorFlow, PyTorch, Scikit-learn)\nOutstanding analytical and problem-solving skills; Ability to learn quickly; Excellent communication and interpersonal skills\nExperience with data engineering and pipeline development\nExperience in analyzing time-series data for forecasting and trend analysis\nExperience with AWS, Azure, or Google Cloud\nExperience with Databricks platform for data analytics and MLOps\nExperience with Generative AI models (e.g., GPT, DALLE, Stable Diffusion) and their applications in cybersecurity and data analysis\nExperience working in Product team's environment\nExperience working in an Agile environment\nProfessional Certifications:\nAny AWS Developer certification (preferred)\nAny Python and ML certification (preferred)\nAny SAFe Agile certification (preferred)\nSoft Skills:\nInitiative to explore alternate technology and approaches to solving problems\nSkilled in breaking down problems, documenting problem statements, and estimating efforts\nExcellent analytical and troubleshooting skills\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation\nAbility to manage multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals.",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data science', 'R', 'PyTorch', 'SAS', 'predictive analytics', 'Scikit-learn', 'SPSS', 'machine learning', 'data engineering', 'Python', 'TensorFlow']",2025-06-12 13:48:46
"Delivery Head - Infrastructure Engineering, Data Center",Bajaj Allianz General Insurance Company Limited,15 - 20 years,Not Disclosed,['Pune'],"The role requires strong leadership, strategic thinking, and the ability to drive innovation and efficiency within the technology department. It demands extensive experience in leading complex data center infrastructures, focusing on servers, SAN storage, high availability, disaster recovery, and hybrid environments, including data center operations and physical servers (blade and rack). Responsibilities include designing and testing backup strategies, maintaining documentation, ensuring compliance with regulations, and conducting product and vendor evaluations. Collaboration with various IT teams, the security team, and business stakeholders is essential\n",,,,"['process setting', 'document management', 'vmware', 'center', 'microsoft azure', 'itil service management', 'storage', 'shuttering', 'analysis', 'problem management', 'change management', 'cloud', 'data center', 'operations', 'service delivery', 'incident management', 'leadership', 'it infrastructure management', 'itil']",2025-06-12 13:48:48
Data Techology Senior Associate,MSCI Services,4 - 7 years,Not Disclosed,['Pune'],"Overview\nThe Data Technology team at MSCI is responsible for meeting the data requirements across various business areas, including Index, Analytics, and Sustainability. Our team collates data from multiple sources such as vendors (e.g., Bloomberg, Reuters), website acquisitions, and web scraping (e.g., financial news sites, company websites, exchange websites, filings). This data can be in structured or semi-structured formats. We normalize the data, perform quality checks, assign internal identifiers, and release it to downstream applications.\nResponsibilities\nAs data engineers, we build scalable systems to process data in various formats and volumes, ranging from megabytes to terabytes. Our systems perform quality checks, match data across various sources, and release it in multiple formats. We leverage the latest technologies, sources, and tools to process the data. Some of the exciting technologies we work with include Snowflake, Databricks, and Apache Spark.\nQualifications\nCore Java, Spring Boot, Apache Spark, Spring Batch, Python. Exposure to sql databases like Oracle, Mysql, Microsoft Sql is a must. Any experience/knowledge/certification on Cloud technology preferrably Microsoft Azure or Google cloud platform is good to have. Exposures to non sql databases like Neo4j or Document database is again good to have.\n What we offer you\nTransparent compensation schemes and comprehensive employee benefits, tailored to your location, ensuring your financial security, health, and overall wellbeing.\nFlexible working arrangements, advanced technology, and collaborative workspaces.\nA culture of high performance and innovation where we experiment with new ideas and take responsibility for achieving results.\nA global network of talented colleagues, who inspire, support, and share their expertise to innovate and deliver for our clients.\nGlobal Orientation program to kickstart your journey, followed by access to our Learning@MSCI platform, LinkedIn Learning Pro and tailored learning opportunities for ongoing skills development.\nMulti-directional career paths that offer professional growth and development through new challenges, internal mobility and expanded roles.\nWe actively nurture an environment that builds a sense of inclusion belonging and connection, including eight Employee Resource Groups. All Abilities, Asian Support Network, Black Leadership Network, Climate Action Network, Hola! MSCI, Pride & Allies, Women in Tech, and Women’s Leadership Forum.\nAt MSCI we are passionate about what we do, and we are inspired by our purpose – to power better investment decisions. You’ll be part of an industry-leading network of creative, curious, and entrepreneurial pioneers. This is a space where you can challenge yourself, set new standards and perform beyond expectations for yourself, our clients, and our industry.\nMSCI is a leading provider of critical decision support tools and services for the global investment community. With over 50 years of expertise in research, data, and technology, we power better investment decisions by enabling clients to understand and analyze key drivers of risk and return and confidently build more effective portfolios. We create industry-leading research-enhanced solutions that clients use to gain insight into and improve transparency across the investment process.\nMSCI Inc. is an equal opportunity employer. It is the policy of the firm to ensure equal employment opportunity without discrimination or harassment on the basis of race, color, religion, creed, age, sex, gender, gender identity, sexual orientation, national origin, citizenship, disability, marital and civil partnership/union status, pregnancy (including unlawful discrimination on the basis of a legally protected parental leave), veteran status, or any other characteristic protected by law. MSCI is also committed to working with and providing reasonable accommodations to individuals with disabilities. If you are an individual with a disability and would like to request a reasonable accommodation for any part of the application process, please email Disability.Assistance@msci.com and indicate the specifics of the assistance needed. Please note, this e-mail is intended only for individuals who are requesting a reasonable workplace accommodation; it is not intended for other inquiries.\n To all recruitment agencies\nMSCI does not accept unsolicited CVs/Resumes. Please do not forward CVs/Resumes to any MSCI employee, location, or website. MSCI is not responsible for any fees related to unsolicited CVs/Resumes.\n Note on recruitment scams\nWe are aware of recruitment scams where fraudsters impersonating MSCI personnel may try and elicit personal information from job seekers. Read our full note on careers.msci.com",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['hive', 'access', 'scala', 'pyspark', 'data warehousing', 'hibernate', 'research', 'sql', 'analytics', 'spring', 'java', 'spring batch', 'spark', 'gcp', 'mysql', 'html', 'hadoop', 'big data', 'etl', 'snowflake', 'python', 'oracle', 'data analysis', 'microsoft azure', 'power bi', 'sql server', 'javascript', 'data bricks', 'spring boot', 'tableau', 'neo4j', 'aws', 'sql database']",2025-06-12 13:48:51
"Senior Python Developer (Machine Learning,Data Analysis,Visualization)",Synechron,3 - 5 years,Not Disclosed,"['Pune', 'Hinjewadi']","Software Requirements\nRequired Skills:\nProficiency in Python (version 3.6+) with experience in data analysis, manipulation, and scripting\nKnowledge of SQL for data extraction, transformation, and database querying\nExperience with data visualization tools such as PowerBI, Tableau, or QlikView\nFamiliarity with AI and Machine Learning frameworks such as TensorFlow, Keras, PyTorch, or equivalent\nHands-on experience in developing, deploying, and optimizing machine learning models\nPreferred Skills:\nExperience with R for data analysis\nFamiliarity with cloud platforms like AWS, Azure, or GCP for deploying AI solutions\nKnowledge of version control systems such as Git\nOverall Responsibilities\nAnalyze, interpret, and visualize large and complex datasets to extract actionable insights\nDesign, develop, and implement machine learning and AI models for predictive and prescriptive analytics\nCollaborate with cross-functional teams to understand business requirements and translate them into data-driven solutions\nCommunicate findings, insights, and recommendations via reports, dashboards, and presentations to stakeholders\nEvaluate and refine models and algorithms to maximize accuracy, efficiency, and impact\nStay informed on emerging AI, Data Science, and analytics trends and incorporate best practices into projects\nSupport automation efforts, optimize data pipelines, and enhance existing analytical workflows\nContribute to organizational learning by sharing knowledge and mentoring team members\nStrategic objectives:\nDrive innovation through the application of AI and machine learning\nEnable data-driven decision-making across business units\nImprove operational efficiencies and business outcomes\nPerformance outcomes:\nAccurate, robust, and scalable AI models\nHigh-quality insights delivered on time and aligned with business needs\nWell-documented solutions and knowledge-sharing artifacts\nTechnical Skills (By Category)\nProgramming Languages (Essential):\nPython (required); experience with R is a plus\nSQL (required); experience with data manipulation and querying\nData Analysis & Visualization Tools (Essential):\nPowerBI, Tableau, or QlikView\nFrameworks & Libraries (Essential):\nTensorFlow, Keras, PyTorch, or similar frameworks for AI/ML development\nData Management & Databases (Essential):\nRelational databases (e.g., MySQL, PostgreSQL, Oracle)\nData extraction and transformation (ETL processes)\nCloud & Deployment (Preferred):\nExperience deploying models on cloud platforms such as AWS, Azure, GCP\nDevelopment & Version Control (Preferred):\nGit for code versioning\nOther Skills:\nStrong statistical knowledge and experience with data preprocessing, feature engineering\nFamiliarity with agile development methodologies\nExperience Requirements\n3 to 5 years of relevant experience in AI, Data Science, or Data Analytics roles\nProven track record applying machine learning techniques to real-world problems\nExperience working with large datasets and scalable data pipelines\nExperience collaborating with cross-functional teams to deliver analytics-driven solutions\nIndustry experience in finance, healthcare, retail, or similar data-rich sectors is preferred\nAlternative pathways:\nCandidates with extensive AI & ML project experience, strong programming skills, and relevant certifications can be considered with slightly varied years of experience\nDay-to-Day Activities\nCollect, clean, and explore large datasets to identify patterns and insights\nDevelop and tune machine learning models to address business problems\nCollaborate with business analysts, data engineers, and product owners to align technical solutions with organizational goals\nDocument methodologies, code, and analytical findings to ensure reproducibility and knowledge sharing\nCreate dashboards, visualizations, and reports to communicate insights effectively\nEvaluate model performance regularly and optimize models for accuracy and efficiency\nParticipate in team meetings, project planning, and review sessions\nKeep abreast of advancements in AI/ML technologies, tools, and best practices\nQualifications\nBachelors degree in Computer Science, Data Science, Statistics, or related field\nMasters degree or higher in AI, Data Science, or related disciplines is a plus\nProfessional certifications in AI/ML (e.g., TensorFlow Developer, AWS Machine Learning Specialty) are advantageous\nWilling to learn new tools and stay updated with emerging AI trends\nAbility to work independently and collaborate effectively in a dynamic environment\nProfessional Competencies\nAnalytical and problem-solving mindset with a focus on actionable insights\nExcellent verbal and written communication skills for diverse audiences\nStrong interpersonal skills and stakeholder management\nAdaptability to fast-changing technology landscapes\nGrowth mindset with continuous learning enthusiasm\nOrganizational skills to handle multiple projects and priorities simultaneously\nInnovation-driven approach and proactive problem resolution",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Python', 'PostgreSQL', 'MySQL', 'Data Analysis', 'Data Visualization', 'Oracle', 'ETL', 'Machine Learning']",2025-06-12 13:48:54
"Senior Data Scientist (AI/ML, Data Analysis, Cloud (AWS), and Model",Synechron,8 - 13 years,Not Disclosed,['Pune'],"job requisition idJR1027352\n\nJob Summary\nSynechron is seeking an analytical and innovative Senior Data Scientist to support and advance our data-driven initiatives. The ideal candidate will have a solid understanding of data science principles, hands-on experience with AI/ML tools and techniques, and the ability to interpret complex data sets to deliver actionable insights. This role contributes to the organizations strategic decision-making and technology innovation by applying advanced analytics and machine learning models in a collaborative environment.\n\nSoftware\n\nRequired\n\nSkills:\nPython (including libraries such as pandas, scikit-learn, TensorFlow, PyTorch) proficiency in developing and deploying models\nR (optional, but preferred)\nData management tools (SQL, NoSQL databases)\nCloud platforms (preferably AWS or Azure) for data storage and ML deployment\nJupyter Notebooks or similar interactive development environments\nVersion control tools such as Git\nPreferred\n\nSkills:\nBig data technologies (Spark, Hadoop)\nModel deployment tools (MLflow, Docker, Kubernetes)\nData visualization tools (Tableau, Power BI)\nOverall Responsibilities\nAnalyze and interpret large and complex data sets to generate insights for business and technology initiatives.\nAssist in designing, developing, and implementing AI/ML models and algorithms to solve real-world problems.\nCollaborate with cross-functional teams including data engineers, software developers, and business analysts to integrate models into production systems.\nStay current with emerging trends, research, and best practices in AI/ML/Data Science and apply them to ongoing projects.\nDocument methodologies, modeling approaches, and insights clearly for technical and non-technical stakeholders.\nSupport model validation, testing, and performance monitoring to ensure accuracy and reliability.\nContribute to the development of data science workflows and standards within the organization.\nPerformance Outcomes:\nAccurate and reliable data models that support strategic decision-making.\nClear documentation and communication of findings and recommendations.\nEffective collaboration with technical teams to deploy scalable models.\nContinuous adoption of best practices in AI/ML and data management.\nTechnical Skills (By Category)\n\nProgramming Languages:\nEssential: Python (best practices in ML development), SQL\nPreferred: R, Java (for integration purposes)\nDatabases/Data Management:\nSQL databases, NoSQL (MongoDB, Cassandra)\nCloud data storage solutions (AWS S3, Azure Blob Storage)\nCloud Technologies:\nAWS (S3, EC2, SageMaker, Lambda)\nAzure Machine Learning (preferred)\nFrameworks & Libraries:\nTensorFlow, PyTorch, scikit-learn, Keras, XGBoost\nDevelopment Tools & Methodologies:\nJupyter Notebooks, Git, CI/CD pipelines\nAgile and Scrum processes\nSecurity Protocols:\nBest practices in data security and privacy, GDPR compliance\nExperience\n8+ years of professional experience in AI, ML, or Data Science roles.\nProven hands-on experience designing and deploying ML models in real-world scenarios.\nDemonstrated ability to analyze complex data sets and translate findings into business insights.\nPrevious experience working with cloud-based data science solutions is preferred.\nStrong portfolio showcasing data science projects, models developed, and practical impact.\nAlternative Pathways:\nCandidates with extensive research or academic experience in AI/ML can be considered, provided they demonstrate practical application of skills.\n\nDay-to-Day Activities\nConduct data exploration, cleaning, feature engineering, and model development.\nCollaborate with data engineers to prepare data pipelines for model training.\nBuild, validate, and refine machine learning models.\nPresent insights, models, and recommendations to technical and business stakeholders.\nSupport deployment of models into production environments.\nMonitor model performance and iterate to improve effectiveness.\nParticipate in team meetings, project planning, and reviewing progress.\nDocument methodologies and maintain version control of codebase.\nQualifications\nBachelors degree in Computer Science, Mathematics, Statistics, Data Science, or a related field; Masters or PhD highly desirable.\nEvidence of relevant coursework, certifications, or professional training in AI/ML.\nProfessional certifications (e.g., AWS Certified Machine Learning Specialty, Microsoft Certified Data Scientist) are a plus.\nCommitment to ongoing professional development in AI/ML methodologies.\nProfessional Competencies\nStrong analytical and critical thinking to solve complex problems.\nEffective communication skills for technical and non-technical audiences.\nDemonstrated ability to work collaboratively in diverse teams.\nAptitude for learning new tools, techniques, and technologies rapidly.\nInnovation mindset with a focus on applying emerging research.\nStrong organizational skills to manage multiple projects and priorities.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['java', 'data science', 'python', 'deploying models', 'aws', 'continuous integration', 'kubernetes', 'scikit-learn', 'ci/cd', 'artificial intelligence', 'sql', 'docker', 'tensorflow', 'spark', 'pytorch', 'keras', 'hadoop', 'big data', 'mongodb', 'microsoft azure', 'nosql', 'pandas', 'amazon ec2', 'r', 'cassandra', 'agile']",2025-06-12 13:48:56
Data Loss Prevention Engineer,Credent Infotech Solutions Llp,2 - 3 years,Not Disclosed,['Mumbai( kandivali )'],"Daily Monitoring and Investigation\n\nMonitor DLP alerts across email, endpoint, web, and cloud.\nPerform triage to determine false positives, true positives, and actual incidents.\nDocument findings and escalate critical violations per SOPs.\nIncident Response Support\n\nSupport incident response by providing evidence, logs, and context around DLP policy violations.\nCoordinate with IT, HR, and Legal teams for user engagement, awareness, and disciplinary action if necessary.\nParticipate in Root Cause Analysis (RCA) for recurring or high-severity incidents.\nPolicy Tuning and Optimization\n\nAnalyse alert trends and false positive patterns to suggest and implement policy refinements.\nWork with business and security teams to validate policy changes and test updated rulesets before production deployment.\nMaintain documentation of policy changes, rationales, and approvals.\nLifecycle Management\n\nSupport onboarding business units, or geographies into DLP coverage.\nMaintain and update DLP dashboards and reporting structures.\nStakeholder Communication\n\nProvide regular reports to CISO on DLP violations\nInterface with Data Owners, Business Units, and Compliance teams for policy alignment and exception management.",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Endpoint Protection', 'Incident Response', 'Symantec Dlp', 'SIEM', 'Data Classification', 'Risk Assessment', 'Compliance', 'Powershell', 'Information Security', 'Dlp', 'Casb Netskope', 'Data Protection Manager', 'Microsoft Purview']",2025-06-12 13:49:00
Engineer- Data Support,Powerica,3 - 5 years,Not Disclosed,['Mumbai'],Job profile\na. Co-ordinate with project team\nb. Upload BVQ in oracle system\nc. BOQ revision and making entries.\nd. Data Accuracy\ne. Process management.\n\nDiploma in Engineering\nExp 3-5 Years’ experience.,Industry Type: Power,Department: Project & Program Management,"Employment Type: Full Time, Permanent","['Boq Preparation', 'Oracle', 'BVQ']",2025-06-12 13:49:02
Specialist Data Security Engineer,Amgen Inc,4 - 6 years,Not Disclosed,['Hyderabad'],"What you will do\nIn this vital role the Specialist Data Security Engineer covering Data Loss Prevention (DLP) and Cloud Access Security Broker (CASB) technologies. This role will report to the Manager, Data Security. This position will provide essential services that enable us to better pursue our mission.\nSpecialist Data Security Engineers operate, manage, and improve Amgens DLP and Cloud Access Security Broker (CASB) solutions. In our Data Security team, they will identify emerging risks related to changes in cloud technologies, advise management, and develop technical remediations to address those risks. Specialists lead the development of processes and procedures for multiple solutions which enable business units to remediate identify cloud data exposures. They run multiple projects simultaneously to implement and improve the cloud data security protection and use advanced analytics to demonstrate success.\nThis engineer will play a key role in educating and evangelizing to technologists and business leaders the security strategies that both protect and enable business processes related to cloud data handling.\nRoles & Responsibilities:\nDesigns, operates, maintains, and enhances capabilities for the technical systems that ensure protection of data for all Amgen global operations.\nIdentifies new risk areas for data and plans controls to mitigate those risks.\nResearches new technologies, processes, and approaches based on industry practices and recommends future plans for data protection.\nAuthors procedures and guidelines and advises on policies related to data protection requirements and remediation or investigation of violations.\nDevelops and conducts training on data protection technologies for operations staff. Educates business leadership about data risk.\nAdvises other technology groups on data protection strategies and recommends appropriate points of both technical and process integration.\nPartners with the Manager of Data Security to liaise to legal and human resources leadership on violation remediations.\nCollaborates with cloud strategy leaders and business unit leadership to ensure that cloud data protection is incorporated by design into new business projects.\nCollaborates with Cloud Security Engineers to integrate cloud data protection technology into the operations of traditional Data Loss Prevention operations.\n\nBasic Qualifications:\nMasters degree and 4 to 6 years of experience OR\nBachelors degree and 6 to 8 years of experience OR\nDiploma and 10 to 12 years of experience.\nFunctional Skills:\nMust-Have Skills\nFamiliarity with one or more security frameworks, especially in regulated environments.\nProficiency specifying requirements for technical systems, as well as designing, implementing, and operating those systems.\nExpertise in global IT operations, including an understanding of regulatory and cultural differences encountered when dealing with international peers and customers.\nDemonstrated competence maintaining applications on Windows and Linux based operating systems, and basic understanding of one or more programming or scripting languages.\nDemonstrated proficiency with one or more Cloud Access Security Platforms (Elastica, Netskope, SkyHigh,etc)\nTrack record of project management leadership, preferably using Agile methodology.\nDeep knowledge of the principles of Data Protection, including availability, integrity, and confidentiality of data.\n\n\nGood-to-Have Skills:\nProficiency with communications focused on both the development of written technical processes and the ability to convey complex ideas clearly in front of an audience.\nExperience with data analytics focused on building executive reports\nReputation of successfully navigating large enterprise environments, understanding both ITIL driven processes and business relationship building\nAbility to self-direct work on multiple priorities with little to no oversight, based on critical initiatives.\nProfessional Certifications (please mention if the certification is preferred or required for the role):\nSystems Security Certified Practitioner (SSCP) or Security+\nSANS Certifications\nCloud security certifications\nRelevant vendor-specific certifications\n\n\nSoft Skills:\nEstablished analytical and gap/fit assessment skills.\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nTeam-oriented, with a focus on achieving team goals\nEffective presentation and public speaking skills.",Industry Type: Pharmaceutical & Life Sciences,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Data Security Engineering', 'SkyHigh', 'Data Protection', 'Cloud Access Security Platforms', 'Netskope', 'security frameworks', 'Agile methodology', 'IT operations', 'ITIL', 'Elastica']",2025-06-12 13:49:05
Data Science,Global Banking Organization,5 - 10 years,Not Disclosed,['Bengaluru'],"Key Skills: Machine Learning, Data Science, Azure, Python, Hadoop.\nRoles and Responsibilities:\nStrong understanding of Math, Statistics, and the theoretical foundations of Statistical & Machine Learning, including Parametric and Non-parametric models.\nApply advanced data mining techniques to curate, process, and transform raw data into reliable datasets.\nUse various statistical techniques and ML methods to perform predictive modeling/classification for problems related to clients, distribution, sales, client profiles, and segmentation, and provide actionable insights for business decision-making.\nDemonstrate expertise in the full Machine Learning lifecycle--feature engineering, training, validation, scaling, deployment, scoring, monitoring, and feedback loops.\nProficiency in Python visualization libraries such as matplotlib and seaborn.\nExperience with cloud computing infrastructure like Azure, including Machine Learning Studio, Azure Data Factory, Synapse, Python, and PySpark.\nAbility to develop, test, and deploy models on cloud/web platforms.\nExcellent knowledge of Deep Learning Architectures, including Convolutional Neural Networks and Transformer/LLM Foundation Models.\nStrong expertise in supervised and adversarial learning techniques.\nRobust working knowledge of deep learning frameworks such as TensorFlow, Keras, and PyTorch.\nExcellent Python coding skills.\nExperience with version control tools (Git, GitHub/GitLab) and data version control.\nExperience in end-to-end model deployment and productionization.\nDemonstrated proficiency in deploying, scaling, and optimizing ML models in production environments with low latency, high availability, and cost efficiency.\nSkilled in model interpretability and CI/CD for ML using tools like MLflow and Kubeflow, with the ability to implement automated monitoring, logging, and retraining strategies.\nExperience Requirement:\n5-12 years of experience in designing and deploying deep learning and machine learning solutions.\nProven track record of delivering AI/ML solutions in real-world business applications at scale.\nHands-on experience working in cross-functional teams including data engineers, product managers, and business stakeholders.\nExperience mentoring junior data scientists and providing technical leadership within a data science team.\nExperience working with big data tools and environments such as Hadoop, Spark, or Databricks is a plus.\nPrior experience in managing model lifecycle in enterprise production environments including drift detection and retraining pipelines.\nEducation: B.Tech.",Industry Type: Financial Services,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Azure', 'Hadoop.', 'Machine Learning', 'Python']",2025-06-12 13:49:07
Data Center Critical Facilities Engineer (Electrical),Equinix,3 - 4 years,Not Disclosed,['Mumbai'],"Who are we?\nEquinix is the world s digital infrastructure company , operating over 260 data centers across the globe. Digital leaders harness Equinixs trusted platform to bring together and interconnect foundational infrastructure at software speed. Equinix enables organizations to access all the right places, partners and possibilities to scale with agility, speed the launch of digital services, deliver world-class experiences and multiply their value, while supporting their sustainability goals.\nJoining our operations team means that you will be at the forefront of all we do, maintaining critical facilities infrastructure as part of a close-knit team delivering best-in-class service to our data center customers. We embrace diversity in thought and contribution and are committed to providing an equitable work environment that is foundational to our core values as a company and is vital to our success.",,,,"['Capacity management', 'Access control', 'Protection system', 'Compliance', 'Infrastructure', 'Corrective maintenance', 'Site administration', 'Vendor', 'Fire protection', 'Electricals']",2025-06-12 13:49:09
Senior Data Analyst-Azure Data Factory,Lumen Technologies,8 - 12 years,Not Disclosed,['Bengaluru'],"Were looking for a Senior Data Analyst with a strong foundation in Azure-based data engineering and Machine Learning to design, develop, and optimize robust data pipelines, applications, and analytics infrastructure. This role demands deep technical expertise, cross-functional collaboration, and the ability to align data solutions with dynamic business needs.\nKey Responsibilities:\nData Pipeline Development:\nDesign and implement efficient data pipelines using Azure Databricks with PySpark to transform and process large datasets.\nOptimize data workflows for scalability, reliability, and performance.\nApplication Integration:\nCollaborate with cross-functional teams to develop APIs using the .NET Framework for Azure Web Application integration.\nEnsure smooth data exchange between applications and downstream systems.\nData Warehousing and Analytics:\nBuild and manage data warehousing solutions using Synapse Analytics and Azure Data Factory (ADF).\nDevelop and maintain reusable and scalable data models to support business intelligence needs.\nAutomation and Orchestration:\nUtilize Azure Logic Apps, Function Apps, and Azure DevOps to automate workflows and streamline deployments.\nImplement CI/CD pipelines for efficient code deployment and testing.\nInfrastructure Management:\nOversee Azure infrastructure management and maintenance, ensuring a secure and optimized environment.\nProvide support for performance tuning and capacity planning.\nBusiness Alignment:\nGain a deep understanding of AMO data sources and their business implications.\nWork closely with stakeholders to provide customized solutions aligning with business needs.\nBAU Support:\nMonitor and support data engineering workflows and application functionality in BAU mode.\nTroubleshoot and resolve production issues promptly to ensure business continuity.\nTechnical Expertise:\nProficiency in Microsoft SQL for complex data queries and database management.\nAdvanced knowledge of Azure Databricks and PySpark for data engineering and ETL processes.\nExperience with Azure Data Factory (ADF) for orchestrating data workflows.\nExpertise in Azure Synapse Analytics for data integration and analytics.\nProficiency in .NET Framework for API development and integration.\nCloud and DevOps Skills:\nStrong experience in Azure Infrastructure Management and optimization.\nHands-on knowledge of Azure Logic Apps, Function Apps, and Azure DevOps for CI/CD automation.\n""We are an equal opportunity employer committed to fair and ethical hiring practices. We do not charge any fees or accept any form of payment from candidates at any stage of the recruitment process. If anyone claims to offer employment opportunities in our company in exchange for money or any other benefit, please treat it as fraudulent and report it immediately.""\n#LI-BS1",Industry Type: Telecom / ISP,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Performance tuning', 'Automation', 'orchestration', 'Infrastructure management', 'Machine learning', 'Business intelligence', 'Business continuity', 'Analytics', 'Downstream', 'Capacity planning']",2025-06-12 13:49:11
"ETL Developer ( SSIS, Informatica ,Talend, Big Data)Group Manager",Vuram,5 - 10 years,Not Disclosed,['Bengaluru'],"Design, develop, and maintain relational and non-relational database systems.\nDefine analytical architecture including datalakes, lakehouse, data mesh and medallion patterns\nAbility to understand & analyze business requirements and translate them into analytical or relational database designAble to design for non-SQL datastores\nOptimize SQL queries, stored procedures, and database performance.Create and maintain ETL processes for data integration from various sources.\nWork closely with application teams to design database schemas and support integration.Monitor, troubleshoot, and resolve database issues related to performance, storage, and replication.Implement data security, backup, recovery, and disaster recovery procedures.\nEnsure data integrity and enforce best practices in database development.\nParticipate in code reviews and mentor junior developers.Collaborate with business and analytics teams for reporting and data warehousing needs.Must Have: Strong expertise in SQL and PL/SQL\nHands-on experience with at least one RDBMS: SQL Server, Oracle, PostgreSQL, or MySQL\nExperience with NoSQL databases: MongoDB, Cassandra, or Redis (at least one)\nETL Development Tools: SSIS, Informatica, Talend, or equivalent\nExperience in performance tuning and optimization\nDatabase design and modeling tools: Erwin, dbForge, or similar\nCloud platforms: Experience with AWS RDS, Azure SQL, or Google Cloud SQL\nBackup and Recovery Management, High Availability, and Disaster Recovery Planning\nUnderstanding of indexing, partitioning, replication, and sharding\nKnowledge of CI/CD pipelines and DevOps practices for database deployments\nExperience with Big Data technologies (Hadoop, Spark)Experience working in Agile/Scrum environments\n\n\nQualifications\nBachelor s or Master s degree in Computer Science, Information Technology, or a related field.8+ years of relevant experience in database design and development",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Performance tuning', 'RDBMS', 'Database design', 'MySQL', 'PLSQL', 'Informatica', 'Stored procedures', 'Oracle', 'SSIS', 'SQL']",2025-06-12 13:49:14
Advanced Data Science Associate,ZS,0 - 2 years,Not Disclosed,['Pune'],"ZSs Insights & Analytics group partners with clients to design and deliver solutions to help them tackle a broad range of business challenges. Our teams work on multiple projects simultaneously, leveraging advanced data analytics and problem-solving techniques. Our recommendations and solutions are based on rigorous research and analysis underpinned by deep expertise and thought leadership.\nWhat you'll Do\nDevelop advanced and efficient statistically effective algorithms that solve problems of high",,,,"['Text mining', 'Analytical', 'Management consulting', 'Financial planning', 'Machine learning', 'Hypothesis Testing', 'Predictive modeling', 'Data mining', 'big data']",2025-06-12 13:49:16
Advanced Data Science Associate,ZS,0 - 2 years,Not Disclosed,"['Noida', 'Gurugram']","Develop advanced and efficient statistically effective algorithms that solve problems of high dimensionality .\nUtilize technical skills such as hypothesis testing, machine learning and retrieval processes to apply statistical and data mining techniques to identify trends, create figures, and analyze other relevant information.\nCollaborate with clients and other stakeholders at ZS to integrate and effectively communicate analysis findings.\nContribute to the assessment of emerging datasets and technologies that impact our analytical",,,,"['Text mining', 'Analytical', 'Management consulting', 'Financial planning', 'Machine learning', 'Hypothesis Testing', 'Predictive modeling', 'Data mining', 'big data']",2025-06-12 13:49:18
Data & Analytics Specialist,Hoffmann La Roche,5 - 10 years,Not Disclosed,['Pune'],"At Roche you can show up as yourself, embraced for the unique qualities you bring. Our culture encourages personal expression, open dialogue, and genuine connections, where you are valued, accepted and respected for who you are, allowing you to thrive both personally and professionally. This is how we aim to prevent, stop and cure diseases and ensure everyone has access to healthcare today and for generations to come. Join Roche, where every voice matters.\nThe Position The Position\nWe are looking for a Data & Analytics Specialist/ Business Analyst who will join us in the newly setup Integrated Informatics for a journey to drive transformation with data and foster automated and efficient decision making throughout the organisation\nThe Data and Analytics Specialist must be the big-picture thinker who understands the value of data to the organisation, has a strong focus on delivering high value, connecting the dots, investing in right initiatives with reusability at the heart of it.\nIn this position you will be acting as squad lead, have end to end ownership of Product delivery with setting up teams from multiple teams/areas with focus on Lifecycle management of the product\nResponsibilities\nYou will work on various aspects of Analytics Solution Development, Data Management, Governance and Information Architecture including but not limited to:\nCollaborate with business stakeholders to understand their data and analytics needs and develop a product roadmap that aligns with business goals.\nDefine and prioritise product requirements, user stories, and acceptance criteria for data and analytics products and ensure what was agreed gets delivered.\nWork with data engineers and data scientists to develop data pipelines, analytical models, and visualisations that meet business requirements.\nCollaborate with Infrastructure Teams and software developers to ensure that data and analytics products are integrated into existing systems and platforms in a sustainable way that still meets the needs of business to generate the insights necessary to drive their decisions.\nMonitor data and analytics product performance and identify opportunities for improvement.\nStay up-to-date with industry trends and emerging technologies related to data and analytics in the pharmaceutical industry.\nAct as a subject matter expert for data and analytics products and provide guidance to business stakeholders on how to effectively use these products.\nAccountable to Develop and maintain documentation, training materials, and user guides for data and analytics products.\nThe ideal candidate\nBachelors or Masters degree in computer science, information systems, or a related field.\n5+ years of experience in roles such as Senior Data & Analytics Specialist, Data Solutions Lead, Data Architect, or Data Consultant, with a focus on solution design and implementation. Alternatively, 3-4 years of experience in data streams (e.g., Data Science, Data Engineering, Data Governance) combined with a couple of years in Strategic Data Consultancy / Data Product Ownership. Experience in the pharmaceutical or healthcare industry is highly desirable.\nHigh Level understanding of data engineering, data science, Data governance and analytics concepts and technologies.\nExperience working with cross-functional teams, including data engineers, data scientists, and software developers.\nExcellent communication and interpersonal skills.\nStrong analytical and problem-solving skills.\nExperience with agile development methodologies.\nKnowledge of regulatory requirements related to data and analytics in the pharmaceutical industry.\nKnowledge of working with vendor and customer master data for different divisions - Pharmaceuticals, Diagnostic, & Diabetes care.\nUnderstanding of the transparency reporting landscape.\nHands-on experience of working on applications such as Jira, SQL, Postman, SAP GUI, Monday.com, Trello\nProficient in the knowledge of different CRM/Master Data Management systems such as SFDC, Reltio MDM\nUnderstanding data protection laws and consent processes applicable to healthcare professionals and organizations before transparency disclosure.\nWho we are\n.\nBasel is the headquarters of the Roche Group and one of its most important centres of pharmaceutical research. Over 10,700 employees from over 100 countries come together at our Basel/Kaiseraugst site, which is one of Roche`s largest sites. Read more.\nBesides extensive development and training opportunities, we offer flexible working options, 18 weeks of maternity leave and 10 weeks of gender independent partnership leave. Our employees also benefit from multiple services on site such as child-care facilities, medical services, restaurants and cafeterias, as well as various employee events.\nWe believe in the power of diversity and inclusion, and strive to identify and create opportunities that enable all people to bring their unique selves to Roche.\nRoche is an Equal Opportunity Employer.\nWho we are\nA healthier future drives us to innovate. Together, more than 100 000 employees across the globe are dedicated to advance science, ensuring everyone has access to healthcare today and for generations to come. Our efforts result in more than 26 million people treated with our medicines and over 30 billion tests conducted using our Diagnostics products. We empower each other to explore new possibilities, foster creativity, and keep our ambitions high, so we can deliver life-changing healthcare solutions that make a global impact.\n\nLet s build a healthier future, together.\nRoche is an Equal Opportunity Employer.\n""",Industry Type: Biotechnology,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'SAP', 'Diagnostics', 'HP data protector', 'Analytical', 'Healthcare', 'JIRA', 'Analytics', 'SQL', 'CRM']",2025-06-12 13:49:21
Data & Analytics Specialist,Roche Diagnostics,5 - 10 years,Not Disclosed,['Pune'],"At Roche you can show up as yourself, embraced for the unique qualities you bring. Our culture encourages personal expression, open dialogue, and genuine connections, where you are valued, accepted and respected for who you are, allowing you to thrive both personally and professionally. This is how we aim to prevent, stop and cure diseases and ensure everyone has access to healthcare today and for generations to come. Join Roche, where every voice matters.\nThe Position The Position\nWe are looking for a Data & Analytics Specialist/ Business Analyst who will join us in the newly setup Integrated Informatics for a journey to drive transformation with data and foster automated and efficient decision making throughout the organisation\nThe Data and Analytics Specialist must be the big-picture thinker who understands the value of data to the organisation, has a strong focus on delivering high value, connecting the dots, investing in right initiatives with reusability at the heart of it.\nIn this position you will be acting as squad lead, have end to end ownership of Product delivery with setting up teams from multiple teams/areas with focus on Lifecycle management of the product\nResponsibilities\nYou will work on various aspects of Analytics Solution Development, Data Management, Governance and Information Architecture including but not limited to:\nCollaborate with business stakeholders to understand their data and analytics needs and develop a product roadmap that aligns with business goals.\nDefine and prioritise product requirements, user stories, and acceptance criteria for data and analytics products and ensure what was agreed gets delivered.\nWork with data engineers and data scientists to develop data pipelines, analytical models, and visualisations that meet business requirements.\nCollaborate with Infrastructure Teams and software developers to ensure that data and analytics products are integrated into existing systems and platforms in a sustainable way that still meets the needs of business to generate the insights necessary to drive their decisions.\nMonitor data and analytics product performance and identify opportunities for improvement.\nStay up-to-date with industry trends and emerging technologies related to data and analytics in the pharmaceutical industry.\nAct as a subject matter expert for data and analytics products and provide guidance to business stakeholders on how to effectively use these products.\nAccountable to Develop and maintain documentation, training materials, and user guides for data and analytics products.\nThe ideal candidate\nBachelors or Masters degree in computer science, information systems, or a related field.\n5+ years of experience in roles such as Senior Data & Analytics Specialist, Data Solutions Lead, Data Architect, or Data Consultant, with a focus on solution design and implementation. Alternatively, 3-4 years of experience in data streams (e.g., Data Science, Data Engineering, Data Governance) combined with a couple of years in Strategic Data Consultancy / Data Product Ownership. Experience in the pharmaceutical or healthcare industry is highly desirable.\nHigh Level understanding of data engineering, data science, Data governance and analytics concepts and technologies.\nExperience working with cross-functional teams, including data engineers, data scientists, and software developers.\nExcellent communication and interpersonal skills.\nStrong analytical and problem-solving skills.\nExperience with agile development methodologies.\nKnowledge of regulatory requirements related to data and analytics in the pharmaceutical industry.\nKnowledge of working with vendor and customer master data for different divisions - Pharmaceuticals, Diagnostic, & Diabetes care.\nUnderstanding of the transparency reporting landscape.\nHands-on experience of working on applications such as Jira, SQL, Postman, SAP GUI, Monday.com, Trello\nProficient in the knowledge of different CRM/Master Data Management systems such as SFDC, Reltio MDM\nUnderstanding data protection laws and consent processes applicable to healthcare professionals and organizations before transparency disclosure.\nWho we are\nAt Roche, more than 100,000 people across 100 countries are pushing back the frontiers of healthcare. Working together, we ve become one of the world s leading research-focused healthcare groups. Our success is built on innovation, curiosity and diversity.\nBasel is the headquarters of the Roche Group and one of its most important centres of pharmaceutical research. Over 10,700 employees from over 100 countries come together at our Basel/Kaiseraugst site, which is one of Roche`s largest sites. Read more.\nBesides extensive development and training opportunities, we offer flexible working options, 18 weeks of maternity leave and 10 weeks of gender independent partnership leave. Our employees also benefit from multiple services on site such as child-care facilities, medical services, restaurants and cafeterias, as well as various employee events.\nWe believe in the power of diversity and inclusion, and strive to identify and create opportunities that enable all people to bring their unique selves to Roche.\nRoche is an Equal Opportunity Employer.\nWho we are\n.\n\nLet s build a healthier future, together.\nRoche is an Equal Opportunity Employer.\n""",Industry Type: Biotechnology,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'SAP', 'Diagnostics', 'HP data protector', 'Analytical', 'Healthcare', 'JIRA', 'Analytics', 'SQL', 'CRM']",2025-06-12 13:49:23
Data Analyst - L3,Wipro,3 - 5 years,Not Disclosed,['Hyderabad'],"Role Purpose\nThe purpose of this role is to interpret data and turn into information (reports, dashboards, interactive visualizations etc) which can offer ways to improve a business, thus affecting business decisions.\nDo\n1. Managing the technical scope of the project in line with the requirements at all stages\na. Gather information from various sources (data warehouses, database, data integration and modelling) and interpret patterns and trends\nb. Develop record management process and policies\nc. Build and maintain relationships at all levels within the client base and understand their requirements.\nd. Providing sales data, proposals, data insights and account reviews to the client base\ne. Identify areas to increase efficiency and automation of processes\nf. Set up and maintain automated data processes\ng. Identify, evaluate and implement external services and tools to support data validation and cleansing.\nh. Produce and track key performance indicators\n2. Analyze the data sets and provide adequate information\na. Liaise with internal and external clients to fully understand data content\nb. Design and carry out surveys and analyze survey data as per the customer requirement\nc. Analyze and interpret complex data sets relating to customers business and prepare reports for internal and external audiences using business analytics reporting tools\nd. Create data dashboards, graphs and visualization to showcase business performance and also provide sector and competitor benchmarking\ne. Mine and analyze large datasets, draw valid inferences and present them successfully to management using a reporting tool\nf. Develop predictive models and share insights with the clients as per their requirement\n\nDeliver\n\nNoPerformance ParameterMeasure1.Analyses data sets and provide relevant information to the clientNo. Of automation done, On-Time Delivery, CSAT score, Zero customer escalation, data accuracy\n\nMandatory Skills: Business Analyst/ Data Analyst(Media). Experience: 3-5 Years.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data analysis', 'data validation', 'data mining', 'business analysis', 'data warehousing', 'business analytics', 'dbms', 'dashboards', 'sales', 'analytics reporting', 'reporting tools', 'data integration', 'digital transformation']",2025-06-12 13:49:26
Data & Gen AI Specialist,Altimetrik,1 - 4 years,Not Disclosed,['Bengaluru'],"Job Title: Data & GenAI AWS Specialist\nExperience: 1-4 Years\nLocation: Bangalore\nMandatory Qualification: B.E./ B.Tech/ M.Tech/ MS from IIT or IISc ONLY\nJob Overview:\nWe are seeking a seasoned Data & GenAI Specialist with deep expertise in AWS Managed Services (PaaS) to join our innovative team. The ideal candidate will have extensive experience in designing sophisticated, scalable architectures for data pipelines and Generative AI (GenAI) solutions leveraging cloud services.",,,,"['Generative Ai', 'Cloud', 'Data Science', 'Open Source', 'Data Pipeline', 'GCP', 'Azure Cloud', 'Snowflake', 'Machine Learning', 'AWS']",2025-06-12 13:49:28
Data Architect,Neo Aid,12 - 20 years,48-60 Lacs P.A.,['Bengaluru'],"Data Architect\nBangalore (Pune option).\nHybrid, 2-3 days WFO,\nUp to 60 LPA. Needs GCP, Data Engineering, Analytics, Visualization, Modeling. 1\n5-20 yrs exp, end-to-end data pipeline.\nNotice: 60 days.\nArchitecture - recent 1-2 years\n\n\nProvident fund",Industry Type: Recruitment / Staffing,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Visualizing', 'Data Architecture', 'Data Modeling', 'Data Analytics', 'Gcp Cloud', 'ETL']",2025-06-12 13:49:30
Data Analyst - Senior,FedEx,4 - 7 years,Not Disclosed,"['Gurugram', 'Bengaluru']","Role & responsibilities :\n\nAct as a technical expert on complex and specialist subject(s).\nSupport management with the analysis, interpretation and application of complex information, contributing to the achievement of divisional and corporate goals. Supports or leads projects by applying area of expertise.\nLead and implement advanced analytical processes through data/text mining, model development, and prediction to enable informed business decisions.\nApply sound analytical expertise to examine structured and unstructured data from multiple disparate sources to provide insights and recommend high-quality solutions to leadership across levels.\nPlan initiatives from concept to execution with minimal supervision and communicate results to a broad range of audiences. Develops a superior understanding of pricing and revenue management through internal and external sources to creatively solve business problems and lead the team from concept to execution of projects.\nTypically uses data, statistical and quantitative analysis, modeling, and fact-based management to drive decision-making. Provides regular expert consultative advice to senior leadership.\nEffectively shares best practices and fosters knowledge sharing across teams. Provides crossteam and cross-org consultation and supports communities of practice excellence.\n\n\n\nPreferred candidate profile\n\nRelevant experience in analytics/consulting/informatics and statistics\nKey Skills - Data and Business Analytics, Advanced Statistics and Predictive Modelling,\nStakeholder Management, Project Management\nExperience in pricing and revenue management yield management, customer segmentation analytics, revenue impact analytics, etc. is a plus\nExposure to predictive analytics, ML/ AI techniques is an added advantage\nTools - Oracle, SQL Server, Teradata, SAS, Python, Tableau/PowerBI/Spotfire\nGood to have cloud computing, big data, Azure",Industry Type: Courier / Logistics (Logistics Tech),Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Analysis', 'Business Insights', 'Python', 'SQL', 'Power Bi', 'Business Acumen', 'Tableau']",2025-06-12 13:49:32
Data Science Manager,ZS,10 - 15 years,Not Disclosed,"['Pune', 'Bengaluru']","A key enabler of our services is leveraging data in delivering client solutions. The data available about customers is getting richer and the problems that our customers are trying to answer continue to evolve. In our endeavor to stay ahead in providing solutions to these evolving complex problems, ZS has set up an Advanced Data Science which has three major focus areas:\nResearch the evolving datasets and advanced analytical techniques to develop new offerings/solutions\nDeliver client impact by collaboratively implementing these solutions",,,,"['Team management', 'data science', 'Pharma', 'Analytical', 'Management consulting', 'Financial planning', 'Healthcare', 'Project planning', 'Predictive modeling', 'Financial services']",2025-06-12 13:49:35
ETL Developer - Data & Analytics,Canpack India,3 - 5 years,Not Disclosed,['Pune'],"Giorgi Global Holdings, Inc. ( GGH ) is a privately held, diversified consumer products/packaging company with approximately 11,000 employees and operations in 20 countries. GGH consists of four US based companies ( The Giorgi Companies ) and one global packaging company ( CANPACK ).\nGGH has embarked on a transformation journey to become a digital, technology enabled, customer-centric, data and insights-driven organization. This transformation is evolving our business, strategy, core operations and IT solutions.\nAs an ETL Developer, you will be an integral part of our Data and Analytics team, working closely with the ETL Architect and other developers to design, develop, and maintain efficient data integration and transformation solutions. We are looking for a highly skilled ETL Developer with a deep understanding of ETL processes and data warehousing. The ideal candidate is passionate about optimizing data extraction, transformation, and loading workflows, ensuring high performance, accuracy, and scalability to support business intelligence initiatives.\nWhat you will do:\n1. Design, develop, test and maintain ETL processes and data pipelines to support data integration and transformation needs.\n2. Continuously improve ETL performance and reliability through best practices and optimization techniques.\n3. Develop and implement data validation and quality checks to ensure the integrity and consistency of data.\n4. Collaborate with ETL Architect, Data Engineers, and Business Intelligence teams to understand business requirements and translate them into technical solutions.\n5. Monitor, troubleshoot, and resolve ETL job failures, performance bottlenecks, and data discrepancies.\n6. Proactively identify and resolve ETL-related issues, minimizing impact on business operations.\n7. Contribute to documentation, training, and knowledge sharing to enhance team capabilities.\n8. Communicate progress and challenges clearly to both technical and non-technical teams\nEssential Requirements:\nBachelor s or master s degree in information technology, Computer Science, or a related field.\n3-5 years of relevant experience.\nPower-BI, Tabular Editor/Dax Studio, ALM/Github/Azure Devops skills\nExposure to SAP Systems/Modules like SD, MD, etc. to understand functional data.,\nExposure to MS Fbric, MS Azure Synapse Analytics\nCompetencies needed:\n- Hands-on experience with ETL development and data integration for large-scale systems\n- Experience with platforms such as Synapse Analytics, Azure Data Factory, Fabric, Redshift or Databricks\n- A solid understanding of data warehousing and ETL processes\n- Advanced SQL and PL/SQL skills such as query optimization, complex joins, window functions\n- Expertise in Python (pySpark) programming with a focus on data manipulation and analysis\n- Experience with Azure DevOps and CI/CD process\n- Excellent problem-solving and analytical skills\n- Experience in creating post-implementation documentation\n- Strong team collaboration skills\n- Attention to detail and a commitment to quality\nStrong interpersonal skills including analytical thinking, creativity, organizational abilities, high commitment, initiative in task execution, and a fast-learning capability for understanding IT concepts\n\nIf you are a current CANPACK employee, please apply through your Workday account .\nCANPACK Group is an Equal Opportunity Employer and all qualified applicants will receive consideration for employment without regard to race, colour, religion, age, sex, sexual orientation, gender identity, national origin, disability, or any other characteristic protected by law or not related to job requirements, unless such distinction is required by law.",Industry Type: Packaging & Containers,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'SAP', 'Analytical', 'Packaging', 'PLSQL', 'Business intelligence', 'Information technology', 'Analytics', 'Python', 'Data extraction']",2025-06-12 13:49:38
Senior Data Scientist,Epsilon,6 - 9 years,Not Disclosed,['Bengaluru'],"Responsibilities: -\nContribute and build an internal product library that is focused on solving business problems related to prediction & recommendation.\nResearch unfamiliar methodologies, techniques to fine tune existing models in the product suite and, recommend better solutions and/or technologies.\nImprove features of the product to include newer machine learning algorithms in the likes of product recommendation, real time predictions, fraud detection, offer personalization etc\nCollaborate with client teams to on-board data, build models and score predictions.\nParticipate in building automations and standalone applications around machine learning algorithms to enable a One Click solution to getting predictions and recommendations.\nAnalyze large datasets, perform data wrangling operations, apply statistical treatments to filter and fine tune input data, engineer new features and eventually aid the process of building machine learning models.\nRun test cases to tune existing models for performance, check criteria and define thresholds for success by scaling the input data to multifold.\nDemonstrate a basic understanding of different machine learning concepts such as Regression, Classification, Matrix Factorization, K-fold Validations and different algorithms such as Decision Trees, Random Forrest, K-means clustering.\nDemonstrate working knowledge and contribute to building models using deep learning techniques, ensuring robust, scalable and high-performance solutions\nMinimum Qualifications:\nEducation: Master's or PhD in a quantitative discipline (Statistics, Economics, Mathematics, Computer Science) is highly preferred.\nDeep Learning Mastery: Extensive experience with deep learning frameworks (TensorFlow, PyTorch, or Keras) and advanced deep learning projects across various domains, with a focus on multimodal data applications.\nGenerative AI Expertise: Proven experience with generative AI models and techniques, such as RAG, VAEs, Transformers, and applications at scale in content creation or data augmentation.\nProgramming and Big Data: Expert-level proficiency in Python and big data/cloud technologies (Databricks and Spark) with a minimum of 4-5 years of experience.\nRecommender Systems and Real-time Predictions: Expertise in developing sophisticated recommender systems, including the application of real-time prediction frameworks.\nMachine Learning Algorithms: In-depth experience with complex algorithms such as logistic regression, random forest, XGBoost, advanced neural networks, and ensemble methods.\nExperienced with machine learning algorithms such as logistic regression, random forest, XG boost, KNN, SVM, neural network, linear regression, lasso regression and k-means.\nDesirable Qualifications:\nGenerative AI Tools Knowledge: Proficiency with tools and platforms for generative AI (such as OpenAI, Hugging Face Transformers).\nDatabricks and Unity Catalog: Experience leveraging Databricks and Unity Catalog for robust data management, model deployment, and tracking.\nWorking experience in CI/CD tools such as GIT & BitBucket",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Data Engineering', 'Pyspark', 'Azure Aws', 'Generative AI', 'Big Data', 'AWS', 'Data Bricks', 'Deep Learning', 'Python', 'SQL']",2025-06-12 13:49:40
Senior Engineer II,AMERICAN EXPRESS,8 - 13 years,Not Disclosed,['Bengaluru'],"Join Team Amex and lets lead the way together.\nAmerican Express is looking for Senior Engineers to contribute to the company s focus on building products, like @Work, to support our large and global corporate clients. @Work helps our clients manage their Corporate Card and Corporate Purchasing Card programs more efficiently online. From performing everyday administrative tasks and account maintenance, to accessing reports and utilizing reconciliation solutions, @Work enables fast, efficient and effective program management resulting in time and cost savings for our clients.",,,,"['Computer science', 'Administration', 'Career development', 'Maven', 'Finance', 'Reconciliation', 'MySQL', 'Workflow', 'Monitoring', 'SQL']",2025-06-12 13:49:43
Enterprise Data Operations Manager,Pepsico,12 - 17 years,Not Disclosed,['Hyderabad'],"Overview\n\nDeputy Director - Data Engineering\n\nPepsiCo operates in an environment undergoing immense and rapid change. Big-data and digital technologies are driving business transformation that is unlocking new capabilities and business innovations in areas like eCommerce, mobile experiences and IoT. The key to winning in these areas is being able to leverage enterprise data foundations built on PepsiCos global business scale to enable business insights, advanced analytics, and new product development. PepsiCos Data Management and Operations team is tasked with the responsibility of developing quality data collection processes, maintaining the integrity of our data foundations, and enabling business leaders and data scientists across the company to have rapid access to the data they need for decision-making and innovation.\nIncrease awareness about available data and democratize access to it across the company.\nAs a data engineering lead, you will be the key technical expert overseeing PepsiCo's data product build & operations and drive a strong vision for how data engineering can proactively create a positive impact on the business. You'll be empowered to create & lead a strong team of data engineers who build data pipelines into various source systems, rest data on the PepsiCo Data Lake, and enable exploration and access for analytics, visualization, machine learning, and product development efforts across the company. As a member of the data engineering team, you will help lead the development of very large and complex data applications into public cloud environments directly impacting the design, architecture, and implementation of PepsiCo's flagship data products around topics like revenue management, supply chain, manufacturing, and logistics. You will work closely with process owners, product owners and business users. You'll be working in a hybrid environment with in-house, on-premises data sources as well as cloud and remote systems.\nResponsibilities\n\nData engineering lead role for D&Ai data modernization (MDIP)\n\nIdeally Candidate must be flexible to work an alternative schedule either on tradition work week from Monday to Friday; or Tuesday to Saturday or Sunday to Thursday depending upon coverage requirements of the job. The candidate can work with immediate supervisor to change the work schedule on rotational basis depending on the product and project requirements.\nResponsibilities\nManage a team of data engineers and data analysts by delegating project responsibilities and managing their flow of work as well as empowering them to realize their full potential.\nDesign, structure and store data into unified data models and link them together to make the data reusable for downstream products.\nManage and scale data pipelines from internal and external data sources to support new product launches and drive data quality across data products.\nCreate reusable accelerators and solutions to migrate data from legacy data warehouse platforms such as Teradata to Azure Databricks and Azure SQL.\nEnable and accelerate standards-based development prioritizing reuse of code, adopt test-driven development, unit testing and test automation with end-to-end observability of data\nBuild and own the automation and monitoring frameworks that captures metrics and operational KPIs for data pipeline quality, performance and cost.\nCollaborate with internal clients (product teams, sector leads, data science teams) and external partners (SI partners/data providers) to drive solutioning and clarify solution requirements.\nEvolve the architectural capabilities and maturity of the data platform by engaging with enterprise architects to build and support the right domain architecture for each application following well-architected design standards.\nDefine and manage SLAs for data products and processes running in production.\nCreate documentation for learnings and knowledge transfer to internal associates.\nQualifications\n\n12+ years of engineering and data management experience\n\nQualifications\n12+ years of overall technology experience that includes at least 5+ years of hands-on software development, data engineering, and systems architecture.\n8+ years of experience with Data Lakehouse, Data Warehousing, and Data Analytics tools.\n6+ years of experience in SQL optimization and performance tuning on MS SQL Server, Azure SQL or any other popular RDBMS\n6+ years of experience in Python/Pyspark/Scala programming on big data platforms like Databricks\n4+ years in cloud data engineering experience in Azure or AWS.\nFluent with Azure cloud services. Azure Data Engineering certification is a plus.\nExperience with integration of multi cloud services with on-premises technologies.\nExperience with data modelling, data warehousing, and building high-volume ETL/ELT pipelines.\nExperience with data profiling and data quality tools like Great Expectations.\nExperience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets.\nExperience with at least one business intelligence tool such as Power BI or Tableau\nExperience with running and scaling applications on the cloud infrastructure and containerized services like Kubernetes.\nExperience with version control systems like ADO, Github and CI/CD tools for DevOps automation and deployments.\nExperience with Azure Data Factory, Azure Databricks and Azure Machine learning tools.\nExperience with Statistical/ML techniques is a plus.\nExperience with building solutions in the retail or in the supply chain space is a plus.\nUnderstanding of metadata management, data lineage, and data glossaries is a plus.\nBA/BS in Computer Science, Math, Physics, or other technical fields.\nCandidate must be flexible to work an alternative work schedule either on tradition work week from Monday to Friday; or Tuesday to Saturday or Sunday to Thursday depending upon product and project coverage requirements of the job.\nCandidates are expected to be in the office at the assigned location at least 3 days a week and the days at work needs to be coordinated with immediate supervisor\nSkills, Abilities, Knowledge:\nExcellent communication skills, both verbal and written, along with the ability to influence and demonstrate confidence in communications with senior level management.\nProven track record of leading, mentoring data teams.\nStrong change manager. Comfortable with change, especially that which arises through company growth.\nAbility to understand and translate business requirements into data and technical requirements.\nHigh degree of organization and ability to manage multiple, competing projects and priorities simultaneously.\nPositive and flexible attitude to enable adjusting to different needs in an ever-changing environment.\nStrong leadership, organizational and interpersonal skills; comfortable managing trade-offs.\nFoster a team culture of accountability, communication, and self-management.\nProactively drives impact and engagement while bringing others along.\nConsistently attain/exceed individual and team goals.\nAbility to lead others without direct authority in a matrixed environment.\nComfortable working in a hybrid environment with teams consisting of contractors as well as FTEs spread across multiple PepsiCo locations.\nDomain Knowledge in CPG industry with Supply chain/GTM background is preferred.",Industry Type: Beverage,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Pyspark', 'Azure', 'Power BI', 'Github', 'Azure Databricks', 'Tableau', 'ADO', 'Scala programming', 'SQL', 'Azure Data Factory', 'Azure Machine learning', 'Data Lakehouse', 'Azure Data Engineering', 'CI/CD', 'Data Warehousing', 'Data Analytics', 'AWS', 'Python']",2025-06-12 13:49:45
Associate Specialist Data Science,Merck Sharp & Dohme (MSD),2 - 7 years,Not Disclosed,['Pune'],"Primary Responsibilities\nSupport in establishing frameworks to standardize, productize and scale existing and new capabilities / analytical solutions\nImplement the vision, roadmap, and best practices for the Data Science Center of Excellence ( CoE ) to align with business goals\nSupport establishing governance frameworks to measure the value of products, standardize data science methodologies, coding practices, and project workflows\nWork with senior CoE members in development and maintenance of best practices for model and algorithm development and design, deployment, and monitoring across the enterprise functions\nCollaborate with product team on product development incorporating Agile framework and latest industry best practices and norms\nSupport in development of MLOps and ModelOps frameworks to streamline the development-to-deployment product pipeline\nDrive innovation by identifying, evaluating, and implementing cutting-edge data science methodologies based on latest published literature\n\nQualifications\nEducation & Work Experience Requiremen ts:\nMaster s degree (relevant field like Economics, Statistics, Mathematics, Operational Research) with 2+ years work experience.\nBachelor s degree (in Engineering or related field, such as Computer Science, Data Science, Statistics, Business, etc.) with at least 3 + years relevant experience\nPrior experience in research publications in reputed journal is a plus\nSkillset:\nCandidates must have -\nStrong programming skills in languages such as Python or R, and SQL with experience in data manipulation and analysis libraries (e.g., pandas, NumPy, scikit-learn, stats models)\nExperience with data science principles, machine learning (supervised and unsupervised) and GenAI algorithms, test-control analysis, propensity score matching etc.\nExposure to product roadmaps, Agile methodologies and backlog management, ensuring iterative and incremental product improvements\nStrong problem solving, business analysis and quantitative skills\nAbility to effectively communicate proposals to key stakeholders\nCandidates are desired but not mandatory to have -\nExperience and familiarity with underlying concepts such as Patient analytics, MMx etc.\nUnderstanding of Pharma commercial landscape will be a plus\nExperience working with healthcare, financial, or enterprise SaaS products\n  Search Firm Representatives Please Read Carefully\nEmployee Status:\nRegular\nRelocation:\nVISA Sponsorship:\nTravel Requirements:\nFlexible Work Arrangements:\nNot Applicable\nShift:\nValid Driving License:\nHazardous Material(s):\n\nRequired Skills:\nBusiness Intelligence (BI), Database Design, Data Engineering, Data Modeling, Data Science, Data Visualization, Machine Learning, Software Development, Stakeholder Relationship Management, Waterfall Model",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Relationship management', 'Business analysis', 'Coding', 'Pharma', 'Analytical', 'Healthcare', 'Business intelligence', 'Analytics', 'Monitoring', 'SQL']",2025-06-12 13:49:47
Data Scientist,"Sourced Group, an Amdocs Company",4 - 9 years,Not Disclosed,['Gurugram'],"0px> Who are we?\nIn one sentence\nThis is a hands-on position for a motivated and talented innovator. The Data Scientist performs data mining and develops algorithms that provide insight from data.\nWhat will your job look like?\nYou will be responsible for and perform end-top-end data-based research.\nYou will craft data mining solutions to be implemented and executed with alignment to the planned scope and design coverage and needs/uses, demonstrating knowledge and a broad understanding of E2E business processes and requirements.\nYou will define the data analytics research plan, scope and resources required to meet the objectives of his/her area of ownership.\nYou will identify and analyze new data analytic directions and their potential business impact to determine the accurate prioritization of data analytics activities based on business needs and analytics value.\nYou will identify data sources, supervises the data collection process and crafts the data structure in collaboration with data experts (BI or big-data) and subject matter and business experts. Ensures that data used in the data analysis activities are of the highest quality.\nYou will construct data models (algorithms and formulas) for required business needs and predictions.\nYou will present results, including the preparation of patents and white papers and facilitating presentations during conferences.\nAll you need is...\nPh.D. in Computer Science, Mathematics or Statistics\n4 years experience in tasks related to data analytics\nKnowledge of telecommunications and of the subject area being investigated - advantage\nKnowledge in the product (ACC or other) application knowledge and configuration knowledge\nKnowledge in BSS, billing, Telco and the business processes\nFamiliarity in the Telco Networking - mobile, landline, cable TV, Internet\nknowledge in Oracle SQL\nWhy you will love this job:\nYou will ensure timely resolution or critical issue within the agreed SLA. This includes creating a positive customer support experience and build strong relationships through problem understanding, presenting promptly on progress, and handling customers with a professional demeanour.\nYou will be able to demonstrates an understanding of key business drivers and ensures strategic directions are followed and the organization succeeds\nWe are a dynamic, multi-cultural organization that constantly innovates and empowers our employees to grow. Our people our passionate, daring, and phenomenal teammates that stand by each other with a dedication to creating a diverse, inclusive workplace!\nWe offer a wide range of stellar benefits including health, dental, vision, and life insurance as well as paid time off, sick time, and parental leave!\n",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Data analysis', 'Bss', 'Networking', 'Billing', 'Data collection', 'Customer handling', 'Customer support', 'Data mining', 'Amdocs']",2025-06-12 13:49:49
"Sr. Data Analyst – Tableau, SQL, Snowflake",Int9 Solutions,5 - 7 years,Not Disclosed,['Bengaluru'],"We are looking for a skilled Data Analyst with excellent communication skills and deep expertise in SQL, Tableau, and modern data warehousing technologies. This role involves designing data models, building insightful dashboards, ensuring data quality, and extracting meaningful insights from large datasets to support strategic business decisions.\n\nKey Responsibilities:\nWrite advanced SQL queries to retrieve and manipulate data from cloud data warehouses such as Snowflake, Redshift, or BigQuery.\nDesign and develop data models that support analytics and reporting needs.\nBuild dynamic, interactive dashboards and reports using tools like Tableau, Looker, or Domo.\nPerform advanced analytics techniques including cohort analysis, time series analysis, scenario analysis, and predictive analytics.\nValidate data accuracy and perform thorough data QA to ensure high-quality output.\nInvestigate and troubleshoot data issues; perform root cause analysis in collaboration with BI or data engineering teams.\nCommunicate analytical insights clearly and effectively to stakeholders.\n\nRequired Skills & Qualifications:\nExcellent communication skills are mandatory for this role.\n5+ years of experience in data analytics, BI analytics, or BI engineering roles.\nExpert-level skills in SQL, with experience writing complex queries and building views.\nProven experience using data visualization tools like Tableau, Looker, or Domo.\nStrong understanding of data modeling principles and best practices.\nHands-on experience working with cloud data warehouses such as Snowflake, Redshift, BigQuery, SQL Server, or Oracle.\nIntermediate-level proficiency with spreadsheet tools like Excel, Google Sheets, or Power BI, including functions, pivots, and lookups.\nBachelor's or advanced degree in a relevant field such as Data Science, Computer Science, Statistics, Mathematics, or Information Systems.\nAbility to collaborate with cross-functional teams, including BI engineers, to optimize reporting solutions.\nExperience in handling large-scale enterprise data environments.\nFamiliarity with data governance, data cataloging, and metadata management tools (a plus but not required).",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Snowflake', 'Tableau', 'Data Warehousing', 'Data Analytics', 'SQL', 'Scenario Analysis', 'Cohort Analysis', 'Data Modeling', 'Predictive Analysis', 'Redshift']",2025-06-12 13:49:51
"Senior Manager- Middle and Back Office Data Analyst- ISS,",Fidelity International,10 - 15 years,Not Disclosed,"['Gurugram', 'Bengaluru']","Title: Middle and Back Office Data Analyst - ISS Data (Senior Manager)\nDepartment: Technology\nLocation: Bangalore & Gurgaon (hybrid / flexible working permitted)\nReports To: Middle and Back Office Data Product Owner\nLevel: Senior Manager\nWe re proud to have been helping our clients build better financial futures for over 50 years. How have we achieved this? By working together - and supporting each other - all over the world. So, join our [insert name of team/ business area] team and feel like you re part of something bigger.\nAbout your team\nThe Technology function provides IT services that are integral to running an efficient run-the business operating model and providing change-driven solutions to meet outcomes that deliver on our business strategy. These include the development and support of business applications that underpin our revenue, operational, compliance, finance, legal, marketing and customer service functions. The broader organisation incorporates Infrastructure services that the firm relies on to operate on a day-to-day basis including data centre, networks, proximity services, security, voice, incident management and remediation.\nThe ISS Technology group is responsible for providing Technology solutions to the Investment Solutions & Services (ISS) business (which covers Investment Management, Asset Management Operations & Distribution business units globally)\n\nThe ISS Technology team supports and enhances existing applications as well as designs, builds and procures new solutions to meet requirements and enable the evolving business strategy.\nAs part of this group, a dedicated ISS Data Programme team has been mobilised as a key foundational programme to support the execution of the overarching ISS strategy.\nAbout your role\nThe Middle and Back Office Data Analyst role is instrumental in the creation and execution of a future state design for Fund Servicing & Oversight data across Fidelity s key business areas. The successful candidate will have an in- depth knowledge of data domains that represent Middle and Back-office operations and technology.\nThe role will sit within the ISS Delivery Data Analysis chapter and fully aligned to deliver Fidelity s cross functional ISS Data Programme in Technology, and the candidate will leverage their extensive industry knowledge to build a future state platform in collaboration with Business Architecture, Data Architecture, and business stakeholders.\nThe role is to maintain strong relationships with the various business contacts to ensure a superior service to our clients.\nData Product - Requirements Definition and Delivery of Data Outcomes\nAnalysis of data product requirements to enable business outcomes, contributing to the data product roadmap\nCapture both functional and non-functional data requirements considering the data product and consumers perspectives.\nConduct workshops with both the business and tech stakeholders for requirements gathering, elicitation and walk throughs.\nResponsible for the definition of data requirements, epics and stories within the product backlog and providing analysis support throughout the SDLC.\nResponsible for supporting the UAT cycles, attaining business sign off on outcomes being delivered\nData Quality and Integrity:\nDefine data quality use cases for all the required data sets and contribute to the technical frameworks of data quality.\nAlign the functional solution with the best practice data architecture & engineering principles.\nCoordination and Communication:\nExcellent communication skills to influence technology and business stakeholders globally, attaining alignment and sign off on the requirements.\nCoordinate with internal and external stakeholders to communicate data product deliveries and the change impact to the operating model.\nAn advocate for the ISS Data Programme.\nCollaborate closely with Data Governance, Business Architecture, and Data owners etc.\nConduct workshops within the scrum teams and across business teams, effectively document the minutes and drive the actions.\nAbout you\nAt least 10 years of proven experience as a business/technical/data analyst within technology and/or business changes within the financial services /asset management industry.\nMinimum 5 years as a senior business/technical/data analyst adhering to agile methodology, delivering data solutions using industry leading data platforms such as Snowflake, State Street Alpha Data, Refinitiv Eikon, SimCorp Dimension, BlackRock Aladdin, FactSet etc.\nProven experience. of delivering data driven business outcomes using industry leading data platforms such as Snowflake.\nExcellent knowledge of data life cycle that drives Middle and Back Office capabilities such as trade execution, matching, confirmation, trade settlement, record keeping, accounting, fund & cash positions, custody, collaterals/margin movements, corporate actions , derivations and calculations such as holiday handling, portfolio turnover rates, funds of funds look through .\nIn Depth expertise in data and calculations across the investment industry covering the below.\nAsset-specific data: This includes data related to financial instruments reference data like asset specifications, maintenance records, usage history, and depreciation schedules.\nMarket data: This includes data like security prices, exchange rates, index constituents and licensing restrictions on them.\nABOR & IBOR data: This includes calculation engines covering input data sets, calculations and treatment of various instruments for ABOR and IBOR data leveraging platforms such as Simcorp, Neoxam, Invest1, Charles River, Aladdin etc. Knowledge of TPAs, how data can be structured in a unified way from heterogenous structures.\nShould possess Problem Solving, Attention to detail, Critical thinking.\nTechnical Skills: Excellent hands-on SQL, Advanced Excel, Python, ML (optional) and proven experience and knowledge of data solutions.\nKnowledge of data management, data governance, and data engineering practices\nHands on experience on data modelling techniques such as dimensional, data vault etc.\nWillingness to own and drive things, collaboration across business and tech stakeholders.\nFeel rewarded",Industry Type: Financial Services,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['IT services', 'Data analysis', 'Data management', 'Incident management', 'Scrum', 'Customer service', 'Asset management', 'SDLC', 'SQL', 'Python']",2025-06-12 13:49:54
Senior Backend Engineer - Bangalore - Hybrid (Product Based),Trigent Software Solutions,5 - 10 years,25-27.5 Lacs P.A.,['Bengaluru'],"Job Description:\nWe are looking for an experienced Backend Engineer to join our engineering team and contribute to building highly scalable, low-latency, and high-concurrency SaaS applications. The ideal candidate should have deep expertise in Java, cloud technologies (preferably AWS), and experience working with both RDBMS and NoSQL databases. You will be involved in the full software development lifecycle, from design to deployment, ensuring performance, security, and maintainability.\nKey Responsibilities:\nDesign, develop, and maintain high-performance, scalable, and secure backend services\nBuild and maintain RESTful APIs to support client-side applications and services\nWork on transactional and concurrent systems that serve large-scale SaaS platforms\nCollaborate with frontend engineers, architects, and DevOps to build robust cloud-based systems\nEnsure code quality and performance by writing unit/integration tests and performing code reviews\nTroubleshoot, debug, and optimize existing systems for reliability and efficiency\nFollow Agile development practices and participate in daily stand-ups and sprint planning\nMandatory Skills:\n58 years of backend development experience in building SaaS or transactional web applications\nStrong hands-on experience with Java and web application frameworks like Spring, Spring Boot\nExperience working on high-concurrency, low-latency, and high-availability systems\nSolid experience with at least one RDBMS (e.g., PostgreSQL, MySQL) and one NoSQL DB (e.g., MongoDB, Cassandra)\nExpertise in cloud platforms – preferably AWS (e.g., EC2, S3, RDS, Lambda)\nFamiliarity with application servers like Tomcat\nStrong knowledge of system design, data structures, and multithreading\nExperience with RESTful APIs and microservice architectures\nNice to Have:\nKnowledge of Big Data technologies (e.g., Kafka, Hadoop, Spark)\nFamiliarity with containerization tools like Docker and Kubernetes\nExperience with CI/CD tools and cloud deployment pipelines\nExposure to other cloud platforms like GCP or Azure",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Temporary/Contractual","['java', 'RDBMS', 'Saas Product Development', 'Nosql Databases', 'AWS', 'spring', 'tomcat']",2025-06-12 13:49:56
Senior Murex Front Office & Risk Support Engineer,Synechron,5 - 10 years,Not Disclosed,"['Pune', 'Bengaluru', 'Hinjewadi']","Job Summary\nSynechron is seeking an experienced Murex FO & Risk Support Specialist to join our dynamic team. This role is central to maintaining and supporting Murex platform functionalities related to front-office operations and risk management, with a focus on production support.\nThe individual will collaborate closely with business users and IT teams to resolve complex issues, optimize configurations, and ensure the stability of critical trading and risk systems. By providing expert-level support, this role contributes directly to the organizations ability to manage market and credit risks effectively, deliver timely business insights, and uphold operational resilience.\nSoftware Requirements\nRequired Software Proficiency:\nMurex platform (version 3.1 or later) extensive experience in FO & Risk modules from a production support perspective\nSQL and Database querying tools (Oracle, SQL Server) strong experience in data analysis and troubleshooting\nMarket data management tools and configurations within Murex\nIssue tracking and collaboration tools (e.g., JIRA, ServiceNow)\nPreferred Software Skills:\nFamiliarity with scripting languages (Python, Shell scripting) for automation\nVersion control systems (e.g., Git)\nCloud platforms (if applicable to client environment)\nOverall Responsibilities\nProvide second-line support for Murex front-office and risk modules, ensuring operational stability and performance\nAnalyze and troubleshoot issues related to P&L, market risk, credit risk, pricing, simulations, and market data, collaborating with business users to identify root causes\nManage Murex configurations, including GOM (Global Object Model) setups, static data configurations, and cross-asset class risk settings\nLiaise with business stakeholders and IT teams to resolve complex incidents, queries, and configuration challenges\nCollaborate on incident resolution, change management, and service improvement initiatives\nDocument technical procedures, resolutions, and configuration changes for knowledge sharing and audit compliance\nContinuously monitor platform health, performance, and data integrity, proposing proactive solutions for operational risks\nTechnical Skills (By Category)\nProgramming Languages:\nEssential: Murex editing languages, SQL\nPreferred: Python, Shell scripting for automation tasks\nDatabases / Data Management:\nEssential: Oracle, SQL Server experience with schema design, data queries, and performance tuning\nPreferred: Understand market data integration and static data management in Murex\nCloud Technologies:\nOptional/Preferred: Basic understanding of cloud integrations related to trading platforms, if applicable\nFrameworks and Libraries:\nNot applicable, focus on Murex platform and related tools\nDevelopment Tools and Methodologies:\nFamiliarity with ITSM processes, incident & problem management, Agile/Scrum methodologies\nSecurity Protocols:\nUnderstanding of access controls, data confidentiality, and system compliance relevant to financial platforms\nExperience Requirements\nMinimum of 5+ years experience supporting Murex FO & Risk modules in a production environment\nProven experience in analyzing issues related to P&L, market risk, credit risk, and pricing in a trading systems context\nStrong understanding of GOM configurations, static data setups, and cross-asset risk configurations\nExperience working directly with business lines, traders, risk managers, and IT teams\nIndustry background within banking, financial services, or capital markets preferred but not mandatory\nDay-to-Day Activities\nMonitor and support Murex FO & Risk modules, identifying and resolving operational issues\nPerform root-cause analysis on incidents related to market data, P&L, and risk calculations\nFine-tune GOM and static data configurations to optimize system performance and accuracy\nEngage in daily stand-ups, incident reviews, and change implementation meetings\nCollaborate with business users and technical teams to clarify issues and implement fixes\nConduct system audits and maintain detailed logs of support activities and configuration changes\nContribute to continuous improvement initiatives for system stability and efficiency\nQualifications\nBachelors degree or higher in Computer Science, Finance, Information Technology, or related field\nCertifications in Murex support, risk management, or related disciplines are a plus\nPrior experience with Murex platform support in financial markets, trading, or risk management environments\nKnowledge of market practices, instruments, and risk concepts across multiple asset classes\nWillingness to engage in ongoing professional development and staying current with Murex updates and industry trends\nProfessional Competencies\nStrong analytical and problem-solving skills, with a focus on root cause identification\nEffective communication skills to interact clearly with technical and business stakeholders\nAbility to work collaboratively within cross-functional teams and under pressure\nAdaptability to evolving systems, processes, and technology landscapes\nCustomer-centric approach, ensuring timely and quality support delivery\nDemonstrated organizational skills for managing multiple issues and priorities efficiently.",Industry Type: IT Services & Consulting,Department: Risk Management & Compliance,"Employment Type: Full Time, Permanent","['Murex support', 'risk management', 'Risk Support', 'credit risk', 'market risk', 'pricing']",2025-06-12 13:49:58
Senior Flexera Data Analyst,Luxoft,6 - 11 years,Not Disclosed,['Gurugram'],"Internal Data Structures & Modeling\nDesign, maintain, and optimize internal data models and structures within the Flexera environment.\nMap business asset data to Flexeras normalized software models with precision and accuracy.\nEnsure accurate data classification, enrichment, and normalization to support software lifecycle tracking.\nPartner with infrastructure, operations, and IT teams to ingest and reconcile data from various internal systems.\nReporting & Analytics\nDesign and maintain reports and dashboards in Flexera or via external BI tools such as Power BI or Tableau.\nProvide analytical insights on software usage, compliance, licensing, optimization, and risk exposure.\nAutomate recurring reporting processes and ensure timely delivery to business stakeholders.\nWork closely with business users to gather requirements and translate them into meaningful reports and visualizations.\nAutomated Data Feeds & API Integrations\nDevelop and support automated data feeds using Flexera REST/SOAP APIs.\nIntegrate Flexera with enterprise tools (e.g., CMDB, SCCM, ServiceNow, ERP) to ensure reliable and consistent data flow.\nMonitor, troubleshoot, and resolve issues related to data extracts and API communication.\nImplement robust logging, alerting, and exception handling for integration pipelines.\nSkills\nMust have\nMinimum 6+ years of working with Flexera or similar software.\nFlexera Expertise: Strong hands-on experience with Flexera One, FlexNet Manager Suite, or similar tools.\nTechnical Skills:\nProficient in REST/SOAP API development and integration.\nStrong SQL skills and familiarity with data transformation/normalization concepts.\nExperience using reporting tools like Power BI, Tableau, or Excel for data visualization.\nFamiliarity with enterprise systems such as SCCM, ServiceNow, ERP, CMDBs, etc.\nProcess & Problem Solving:\nStrong analytical and troubleshooting skills for data inconsistencies and API failures.\nUnderstanding of license models, software contracts, and compliance requirements.\nNice to have\nSoft Skills: Excellent communication skills to translate technical data into business insights.\nOther\nLanguages\nEnglish: C1 Advanced\nSeniority\nSenior\nRefer a Friend\nPositive work environments and stellar reputations attract and retain top talent. Find out why Luxoft stands apart from the rest.\nRecommend a friend\nRelated jobs View all vacancies\nData Engineer with Neo4j\nData Science\nIndia\nChennai\nData Engineer with Neo4j\nData Science\nIndia\nBengaluru\nData Scientist\nData Science\nIndia\nBengaluru\nGurugram, India\nReq. VR-114544\nData Science\nBCM Industry\n23/05/2025\nReq. VR-114544\nApply for Senior Flexera Data Analyst in Gurugram\n*",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['ERP', 'neo4j', 'Analytical', 'Data structures', 'data visualization', 'SCCM', 'Licensing', 'Analytics', 'Reporting tools', 'SQL']",2025-06-12 13:50:01
"AI/ML TESTING-AI, ML, DEEP LEARNING, DATA MINING",Zensar,2 - 7 years,Not Disclosed,['Pune'],"Zensar Technologies is looking for AI/ML TESTING-AI, ML, DEEP LEARNING, DATA MINING, ANALYTICS AI/ML TESTING-AI, ML, DEEP LEARNING, DATA MINING, ANALYTICS to join our dynamic team and embark on a rewarding career journey\n\nDevelops and executes test plans for AI and machine learning models\n\nValidates model accuracy, fairness, performance, and edge-case behavior\n\nImplements automation tools and creates synthetic test datasets\n\nEnsures compliance with model validation protocols and documentation",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Design engineering', 'deep learning', 'Technology consulting', 'Focus', 'Agile', 'Conceptualization', 'Management', 'Data mining', 'Analytics', 'Testing']",2025-06-12 13:50:03
Business Data Analyst,CGI,5 - 8 years,Not Disclosed,['Hyderabad'],"Business Data Analyst - HealthCare\n\nJob Summary\nWe are seeking an experienced and results-driven Business Data Analyst with 5+ years of hands-on experience in data analytics, visualization, and business insight generation. This role is ideal for someone who thrives at the intersection of business and datatranslating complex data sets into compelling insights, dashboards, and strategies that support decision-making across the organization.\nYou will collaborate closely with stakeholders across departments to identify business needs, design and build analytical solutions, and tell compelling data stories using advanced visualization tools.\nKey Responsibilities\nData Analytics & Insights Analyze large and complex data sets to identify trends, anomalies, and opportunities that help drive business strategy and operational efficiency.\n• Dashboard Development & Data Visualization Design, develop, and maintain interactive dashboards and visual reports using tools like Power BI, Tableau, or Looker to enable data-driven decisions.\n• Business Stakeholder Engagement Collaborate with cross-functional teams to understand business goals, define metrics, and convert ambiguous requirements into concrete analytical deliverables.\n• KPI Definition & Performance Monitoring Define, track, and report key performance indicators (KPIs), ensuring alignment with business objectives and consistent measurement across teams.\n• Data Modeling & Reporting Automation Work with data engineering and BI teams to create scalable, reusable data models and automate recurring reports and analysis processes.\n• Storytelling with Data Communicate findings through clear narratives supported by data visualizations and actionable recommendations to both technical and non-technical audiences.\n• Data Quality & Governance Ensure accuracy, consistency, and integrity of data through validation, testing, and documentation practices.\nRequired Qualifications\nBachelor’s or Master’s degree in Business, Economics, Statistics, Computer Science, Information Systems, or a related field.\n• 5+ years of professional experience in a data analyst or business analyst role with a focus on data visualization and analytics.\n• Proficiency in data visualization tools: Power BI, Tableau, Looker (at least one).\n• Strong experience in SQL and working with relational databases to extract, manipulate, and analyze data.\n• Deep understanding of business processes, KPIs, and analytical methods.\n• Excellent problem-solving skills with attention to detail and accuracy.\n• Strong communication and stakeholder management skills with the ability to explain technical concepts in a clear and business-friendly manner.\n• Experience working in Agile or fast-paced environments.\nPreferred Qualifications\nExperience working with cloud data platforms (e.g., Snowflake, BigQuery, Redshift).\n• Exposure to Python or R for data manipulation and statistical analysis.\n• Knowledge of data warehousing, dimensional modeling, or ELT/ETL processes.\n• Domain experience in Healthcare is a plus.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Bigquery', 'Snowflake', 'Data Warehousing', 'Redshift', 'Python', 'ETL']",2025-06-12 13:50:05
Senior Data Manager/ Lead,Codeforce 360,6 - 8 years,Not Disclosed,['Hyderabad'],"Job Description:\nWe are looking for a highly experienced and dynamic Senior Data Manager / Lead to oversee a team of Data Engineers and Data Scientists. This role demands a strong background in data platforms such as Snowflake and proficiency in Python, combined with excellent people management and project leadership skills. While hands-on experience in the technologies is beneficial, the primary focus of this role is on team leadership, strategic planning, and project delivery .\n\nJob Title : Senior Data Manager / Lead\nLocation: Hyderabad (Work From Office)\nShift Timing: 10AM-7PM\nKey Responsibilities:\nLead, mentor, and manage a team of Data Engineers and Data Scientists.\nOversee the design and implementation of data pipelines and analytics solutions using Snowflake and Python.\nCollaborate with cross-functional teams (product, business, engineering) to align data solutions with business goals.\nEnsure timely delivery of projects, with high quality and performance.\nConduct performance reviews, training plans, and support career development for the team.\nSet priorities, allocate resources, and manage workloads within the data team.\nDrive adoption of best practices in data management, governance, and documentation.\nEvaluate new tools and technologies relevant to data engineering and data science.\n\nRequired Skills & Qualifications:\n6+ years of experience in data-related roles, with at least 23 years in a leadership or management position.\nStrong understanding of Snowflake architecture, performance tuning, data sharing, security, etc.\nSolid knowledge of Python for data engineering or data science tasks.\nExperience in leading data migration, ETL/ELT, and analytics projects.\nAbility to translate business requirements into technical solutions.\nExcellent leadership, communication, and stakeholder management skills.\nExposure to tools like Databricks, Dataiku, Airflow, or similar platforms is a plus.\nBachelors or Master’s degree in Computer Science, Engineering, Mathematics, or a related field.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Snowflake', 'Data Bricks', 'Python', 'Airflow', 'Data Migration', 'Dataiku', 'Data Warehousing', 'ETL', 'ELT', 'SQL']",2025-06-12 13:50:07
Data Scientist,Big Oh Tech,4 - 6 years,Not Disclosed,['Noida'],"Key Responsibilities:\n\nDesign, build, and maintain robust and scalable data pipelines to support analytics and reporting needs.\nManage and optimize data lake architectures, with a focus on Apache Atlas for metadata management, data lineage, and governance.\nIntegrate and curate data from multiple structured and unstructured sources to enable advanced analytics.\nCollaborate with data scientists and business analysts to ensure availability of clean, well-structured data.\nImplement data quality, validation, and monitoring processes across data pipelines.\nDevelop and manage Power BI datasets and data models, supporting dashboard and report creation.\nSupport data cataloging and classification using Apache Atlas for enterprise-wide discoverability and compliance.\nEnsure adherence to data security, privacy, and compliance policies.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['advanced analytics', 'metadata', 'Compliance', 'Business Analyst', 'data security', 'power bi', 'Data quality', 'Management', 'Apache', 'Monitoring']",2025-06-12 13:50:09
Senior/Lead MLops Engineer,Tiger Analytics,7 - 10 years,Not Disclosed,"['Hyderabad', 'Chennai', 'Bengaluru']","JOB DESCRIPTION\n\nSenior MLE / Architect MLE (ML Ops) Chennai / Bangalore / Hyderabad (Hybrid)\n\nWho we are Tiger Analytics is a global leader in AI and analytics, helping Fortune 1000 companies solve their toughest challenges. We offer fullstack AI and analytics services & solutions to empower businesses to achieve real outcomes and value at scale. We are on a mission to push the boundaries of what AI and analytics can do to help enterprises navigate uncertainty and move forward decisively. Our purpose is to provide certainty to shape a better tomorrow. Our team of 4000+ technologists and consultants are based in the US, Canada, the UK, India, Singapore and Australia, working closely with clients across CPG, Retail, Insurance, BFS, Manufacturing, Life Sciences, and Healthcare. Many of our team leaders rank in Top 10 and 40 Under 40 lists, exemplifying our dedication to innovation and excellence. We are a Great Place to Work-Certified (2022-24), recognized by analyst firms such as Forrester, Gartner, HFS, Everest, ISG and others. We have been ranked among the Best and Fastest Growing analytics firms lists by Inc., Financial Times, Economic Times and Analytics India Magazine.",,,,"['MLops', 'Azure', 'Snowflake', 'Deployment', 'Ci/Cd', 'Machine Learning']",2025-06-12 13:50:11
Big Data Developer,Binary Infoways,6 - 10 years,12-20 Lacs P.A.,['Hyderabad'],"AWS (EMR, S3, Glue, Airflow, RDS, Dynamodb, similar)\nCICD (Jenkins or another)\nRelational Databases experience (any)\nNo SQL databases experience (any)\nMicroservices or Domain services or API gateways or similar\nContainers (Docker, K8s, similar)",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'AWS', 'Python', 'Airflow', 'Java', 'Big Data', 'EMR', 'SQL', 'Jenkins', 'Glue', 'SCALA', 'Big Data Technologies', 'Spark']",2025-06-12 13:50:14
"Senior Staff Engineer, QA Automation",Nagarro,10 - 13 years,Not Disclosed,['India'],"We're Nagarro.\nWe are a Digital Product Engineering company that is scaling in a big way! We build products, services, and experiences that inspire, excite, and delight. We work at scale across all devices and digital mediums, and our people exist everywhere in the world (18000+ experts across 38 countries, to be exact). Our work culture is dynamic and non-hierarchical. We're looking for great new colleagues. That's where you come in!\n\nREQUIREMENTS:\nTotal Experience 10+ years.\nStrong working experience in QA with a strong background in both manual and automation testing.\nHands-on experience with Selenium WebDriver, Appium, and Postman.\nSolid understanding of REST API testing and automation using tools like RestAssured.\nProficient in testing frameworks such as TestNG, JUnit, or Cypress.\nStrong experience in automation of mobile and web applications.\nFamiliarity with CI/CD tools like Jenkins, GitLab CI, or equivalent.\nWorking knowledge of bug tracking and test management tools (e.g., JIRA, TestRail).\nExperience with BDD frameworks like Cucumber.\nGood command of scripting or programming in Java, Python, or similar languages is a plus.\nProblem-solving mindset with the ability to tackle complex data engineering challenges.\nStrong communication and teamwork skills, with the ability to mentor and collaborate effectively.\nRESPONSIBILITIES:\nUnderstanding the clients business use cases and technical requirements and be able to convert them in to technical design which elegantly meets the requirements\nMapping decisions with requirements and be able to translate the same to developers.\nIdentifying different solutions and being able to narrow down the best option that meets the clients requirements.\nDefining guidelines and benchmarks for NFR considerations during project implementation\nWriting and reviewing design document explaining overall architecture, framework, and high-level design of the application for the developers.\nReviewing architecture and design on various aspects like extensibility, scalability, security, design patterns, user experience, NFRs, etc., and ensure that all relevant best practices are followed.\nDeveloping and designing the overall solution for defined functional and non-functional requirements; and defining technologies, patterns, and frameworks to materialize it\nUnderstanding and relating technology integration scenarios and applying these learnings in projects\nResolving issues that are raised during code/review, through exhaustive systematic analysis of the root cause, and being able to justify the decision taken.\nCarrying out POCs to make sure that suggested design/technologies meet the requirements",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['API Testing', 'Appium', 'QA Automation', 'Selenium', 'Postman']",2025-06-12 13:50:16
"Senior Staff Engineer, Frontend React",Nagarro,10 - 13 years,Not Disclosed,['India'],"We're Nagarro.\n\nWe are a Digital Product Engineering company that is scaling in a big way! We build products, services, and experiences that inspire, excite, and delight. We work at scale across all devices and digital mediums, and our people exist everywhere in the world (18000+ experts across 38 countries, to be exact). Our work culture is dynamic and non-hierarchical. We're looking for great new colleagues. That's where you come in!\n\nREQUIREMENTS:\nTotal Experience 10+ years.\nHands on working experience in front-end or full-stack development experience, with building production apps in React.js and Next.js.\nHands-on expertise writing unit/integration tests for React components (Jest, React Testing Library, etc.)\nSolid grasp of state-management patterns and libraries (Redux, React Context, Zustand, etc.).\nStrong understanding of RESTful APIs, asynchronous programming (Promises, async/await), and modern build tools (Webpack, Vite, or Turbopack).\nPractical experience with Git, pull-request workflows, and collaborative development tools (GitHub, GitLab, Bitbucket).\nAdvanced proficiency in JavaScript (ES6+) and TypeScript.\nProblem-solving mindset with the ability to tackle complex data engineering challenges. \nStrong communication and teamwork skills, with the ability to mentor and collaborate effectively.\n\nRESPONSIBILITIES:\nWriting and reviewing great quality code\nUnderstanding the clients business use cases and technical requirements and be able to convert them in to technical design which elegantly meets the requirements\nMapping decisions with requirements and be able to translate the same to developers.\nIdentifying different solutions and being able to narrow down the best option that meets the clients requirements.\nDefining guidelines and benchmarks for NFR considerations during project implementation\nWriting and reviewing design document explaining overall architecture, framework, and high-level design of the application for the developers.\nReviewing architecture and design on various aspects like extensibility, scalability, security, design patterns, user experience, NFRs, etc., and ensure that all relevant best practices are followed.\nDeveloping and designing the overall solution for defined functional and non-functional requirements; and defining technologies, patterns, and frameworks to materialize it\nUnderstanding and relating technology integration scenarios and applying these learnings in projects\nResolving issues that are raised during code/review, through exhaustive systematic analysis of the root cause, and being able to justify the decision taken.\nCarrying out POCs to make sure that suggested design/technologies meet the requirements",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Javascript', 'React.Js', 'Nextjs', 'Typescript']",2025-06-12 13:50:18
Associate- Referral - Decision Science / Data Science,Axtria,3 - 5 years,Not Disclosed,['Gurugram'],"Position Summary \n\nThis Requisition is for the Employee Referral Campaign.\n\nWe are seeking high-energy, driven, and innovative Data Scientists to join our Data Science Practice to develop new, specialized capabilities for Axtria, and to accelerate the company’s growth by supporting our clients’ commercial & clinical strategies.\n\n Job Responsibilities \n\nBe an Individual Contributor tothe Data Science team and solve real-world problems using cutting-edge capabilities and emerging technologies.\n\nHelp clients translate the business use cases they are trying to crack into data science solutions. Provide genuine assistance to users by advising them on how to leverage Dataiku DSS to implement data science projects, from design to production.\n\nData Source Configuration, Maintenance, Document and maintain work-instructions.\n\nDeep working onmachine learning frameworks such as TensorFlow, Caffe, Keras, SparkML\n\nExpert knowledge in Statistical and Probabilistic methods such as SVM, Decision-Trees, Clustering\n\nExpert knowledge of python data-science and math packages such as NumPy , Pandas, Sklearn\n\nProficiency in object-oriented languages (Java and/or Kotlin),Python and common machine learning frameworks(TensorFlow, NLTK, Stanford NLP, Ling Pipe etc\n\n\n Education \n\nBachelor Equivalent - Engineering\nMaster's Equivalent - Engineering\n\n Work Experience \n\nData Scientist 3-5 years of relevant experience in advanced statistical and mathematical models and predictive modeling using Python. Experience in the data science space prior relevant experience in Artificial intelligence and machine Learning algorithms for developing scalable models supervised and unsupervised techniques likeNLP and deep Learning Algorithms. Ability to build scalable models using Python, R-Studio, R Shiny, PySpark, Keras, and TensorFlow. Experience in delivering data science projects leveraging cloud infrastructure. Familiarity with cloud technology such as AWS / Azure and knowledge of AWS tools such as S3, EMR, EC2, Redshift, and Glue; viz tools like Tableau and Power BI. Relevant experience in Feature Engineering, Feature Selection, and Model Validation on Big Data. Knowledge of self-service analytics platforms such as Dataiku/ KNIME/ Alteryx will be an added advantage.\n\nML Ops Engineering 3-5 years of experience with MLOps Frameworks like Kubeflow, MLFlow, Data Robot, Airflow, etc., experience with Docker and Kubernetes, OpenShift. Prior experience in end-to-end automated ecosystems including, but not limited to, building data pipelines, developing & deploying scalable models, orchestration, scheduling, automation, and ML operations. Ability to design and implement cloud solutions and ability to build MLOps pipelines on cloud solutions (AWS, MS Azure, or GCP). Programming languages like Python, Go, Ruby, or Bash, a good understanding of Linux, knowledge of frameworks such as Keras, PyTorch, TensorFlow, etc. Ability to understand tools used by data scientists and experience with software development and test automation. Good understanding of advanced AI/ML algorithms & their applications.\n\nGen AI :Minimum of 4-6 years develop, test, and deploy Python based applications on Azure/AWS platforms.Must have basic knowledge on concepts of Generative AI / LLMs / GPT.Deep understanding of architecture and work experience on Web Technologies.Python, SQL hands-on experience.Expertise in any popular python web frameworks e.g. flask, Django etc. Familiarity with frontend technologies like HTML, JavaScript, REACT.Be an Individual Contributor in the Analytics and Development team and solve real-world problems using cutting-edge capabilities and emerging technologies based on LLM/GenAI/GPT.Can interact with client on GenAI related capabilities and use cases.",Industry Type: Analytics / KPO / Research,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'gpm', 'machine learning', 'python data', 'statistics', 'kubernetes', 'microsoft azure', 'numpy', 'javascript', 'sql', 'docker', 'pandas', 'tensorflow', 'java', 'django', 'predictive modeling', 'python web framework', 'mathematical modeling', 'pytorch', 'keras', 'aws', 'flask', 'advanced statistical']",2025-06-12 13:50:20
Sr. Data Scientist-Stratup-Mid-Size companies Exp.@ Bangalore_Urgent,"A leader in this space, we deliver world...",8 - 13 years,Not Disclosed,['Bengaluru'],"Senior Data Scientist\n\nLocation: Onsite Bangalore\nExperience: 8+ years\n\nRole Overview\n\nWe are seeking a Senior Data Scientist with a strong foundation in machine learning, deep learning, and statistical modeling, with the ability to translate complex operational problems into scalable AI/ML solutions. In addition to core data science responsibilities, the role involves building production-ready backends in Python and contributing to end-to-end model lifecycle management. Exposure to computer vision is a plus, especially for industrial use cases like identification, intrusion detection, and anomaly detection.\n\nKey Responsibilities\n\nDevelop, validate, and deploy machine learning and deep learning models for forecasting, classification, anomaly detection, and operational optimization\nBuild backend APIs using Python (FastAPI, Flask) to serve ML/DL models in production environments\nApply advanced computer vision models (e.g., YOLO, Faster R-CNN) to object detection, intrusion detection, and visual monitoring tasks\nTranslate business problems into analytical frameworks and data science solutions\nWork with data engineering and DevOps teams to operationalize and monitor models at scale\nCollaborate with product, domain experts, and engineering teams to iterate on solution design\nContribute to technical documentation, model explainability, and reproducibility practices\n\n\nRequired Skills\n\nStrong proficiency in Python for data science and backend development\nExperience with ML/DL libraries such as scikit-learn, TensorFlow, or PyTorch\nSolid knowledge of time-series modeling, forecasting techniques, and anomaly detection\nExperience building and deploying APIs for model serving (FastAPI, Flask)\nFamiliarity with real-time data pipelines using Kafka, Spark, or similar tools\nStrong understanding of model validation, feature engineering, and performance tuning\nAbility to work with SQL and NoSQL databases, and large-scale datasets\nGood communication skills and stakeholder engagement experience\n\n\nGood to Have\n\nExperience with ML model deployment tools (MLflow, Docker, Airflow)\nUnderstanding of MLOps and continuous model delivery practices\nBackground in aviation, logistics, manufacturing, or other industrial domains\nFamiliarity with edge deployment and optimization of vision models\n\n\nQualifications\n\nMasters or PhD in Data Science, Computer Science, Applied Mathematics, or related field\n7+ years of experience in machine learning and data science, including end-to-end deployment of models in production",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['scikit-learn', 'time-series modeling', 'ML/DL libraries', 'data science', 'Python', 'Airflow', 'Kafka', 'MLflow', 'logistics', 'anomaly detection', 'aviation', 'SQL', 'PyTorch', 'NoSQL', 'MLOps', 'forecasting techniques', 'Docker', 'manufacturing', 'FastAPI', 'Spark', 'TensorFlow', 'Flask']",2025-06-12 13:50:23
Data Science_ Lead,Rishabh Software,8 - 13 years,Not Disclosed,"['Ahmedabad', 'Bengaluru', 'Vadodara']","Job Description\n\nWith excellent analytical and problem-solving skills, you should understand business problems of the customers, translate them into scope of work and technical specifications for developing into Data Science projects. Efficiently utilize cutting edge technologies in AI, Generative AI areas and implement solutions for business problems. Good exposure technology platforms for Data Science, AI, Gen AI, cloud with implementation experience. Ability to provide end to end technical solutions leveraging latest AI, Gen AI tools, frameworks for the business problems. This Job requires the following:",,,,"['Data Science', 'gen ai', 'Computer Vision', 'Machine Learning', 'Deep Learning', 'Tensorflow', 'NLP', 'Artificial Intelligence', 'Dl', 'Python']",2025-06-12 13:50:25
Python Engineer - ML/Big Query - Hyd/Chennai/Bangalore,People staffing Solutions,5 - 10 years,12-20 Lacs P.A.,"['Hyderabad', 'Chennai', 'Bengaluru']","Key Responsibilities:\nDesign, develop, and maintain scalable and optimized ETL pipelines using Python and SQL.\nWork with Google BigQuery and other cloud-based platforms to build data warehousing solutions.\nDevelop and deploy ML models; collaborate with Data Scientists for productionizing models.\nWrite efficient and optimized SQL queries for large-scale data processing.\nBuild APIs using Flask/Django for machine learning and data applications.\nWork with both SQL and NoSQL databases including Elasticsearch.\nImplement data ingestion using batch and streaming technologies.\nEnsure data quality, integrity, and governance across the data lifecycle.\nAutomate and optimize CI/CD pipelines for data solutions.\nCollaborate with cross-functional teams to gather data requirements and deliver solutions.\nTroubleshoot and monitor data pipelines for seamless operations.\nRequired Skills & Qualifications:\nBachelor's or Master's degree in Computer Science, Engineering, or related field.\n5+ years of experience with Python in a data engineering and/or ML context.\nStrong hands-on experience with SQL, BigQuery, and cloud data platforms (preferably GCP).\nPractical knowledge of ML concepts and experience developing ML models.\nProficiency in frameworks such as Flask and Django.\nExperience with NoSQL databases and data streaming technologies.\nSolid understanding of data modeling, warehousing, and ETL frameworks.\nFamiliarity with CI/CD tools and automation best practices.\nExcellent communication, problem-solving, and collaboration skills.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Django', 'Machine Learning', 'Python', 'SQL', 'Pandas', 'Numpy', 'Ml', 'Flask']",2025-06-12 13:50:27
"Senior Staff Engineer, Mobile -Flutter",Nagarro,10 - 13 years,Not Disclosed,['India'],"We're Nagarro.\n\nWe are a Digital Product Engineering company that is scaling in a big way! We build products, services, and experiences that inspire, excite, and delight. We work at scale across all devices and digital mediums, and our people exist everywhere in the world (18000+ experts across 38 countries, to be exact). Our work culture is dynamic and non-hierarchical. We're looking for great new colleagues. That's where you come in!\n\nREQUIREMENTS:\nTotal Experience 10+ years.\nStrong working experience in Dart and Flutter.\nSolid understanding of Mobile App Architecture (MVVM, BLoC, Provider, GetX).\nExperience with local databases like Sqflite or alternatives.\nHands-on experience in unit testing and test automation for Flutter apps.\nProven experience in building and deploying apps to the App Store and Google Play Store.\nFamiliarity with Git, GitHub/GitLab, CI/CD tools (e.g., Jenkins, Bitrise, GitHub Actions).\nDeep knowledge of mobile app lifecycle, design principles, and clean architecture patterns (MVVM, MVC, etc.)\nExpertise in mobile app performance optimization and security best practices.\nExperience in API integration, RESTful services, and handling JSON data.\nProficient in Version Control Systems like Git.\nProblem-solving mindset with the ability to tackle complex data engineering challenges.\nStrong communication and teamwork skills, with the ability to mentor and collaborate effectively.\nRESPONSIBILITIES:\nWriting and reviewing great quality code\nUnderstanding the clients business use cases and technical requirements and be able to convert them in to technical design which elegantly meets the requirements\nMapping decisions with requirements and be able to translate the same to developers.\nIdentifying different solutions and being able to narrow down the best option that meets the clients requirements.\nDefining guidelines and benchmarks for NFR considerations during project implementation\nWriting and reviewing design document explaining overall architecture, framework, and high-level design of the application for the developers.\nReviewing architecture and design on various aspects like extensibility, scalability, security, design patterns, user experience, NFRs, etc., and ensure that all relevant best practices are followed.\nDeveloping and designing the overall solution for defined functional and non-functional requirements; and defining technologies, patterns, and frameworks to materialize it\nUnderstanding and relating technology integration scenarios and applying these learnings in projects\nResolving issues that are raised during code/review, through exhaustive systematic analysis of the root cause, and being able to justify the decision taken.\nCarrying out POCs to make sure that suggested design/technologies meet the requirements",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['GIT', 'Flutter', 'Dart', 'Swift', 'IOS', 'Android']",2025-06-12 13:50:29
MDM Data Analyst / Steward Lead,Gallagher Service Center (GSC),3 - 7 years,Not Disclosed,['Bengaluru'],"Role & responsibilities\n\nThe MDM Analyst / Data Steward works closely with business stakeholders to understand and gather data requirements, develop data models and database designs, and define and implement data standards, policies, and procedures. This role also implements any rules inside of the MDM tool to improve the data, performs deduplication projects to develop golden records, and overall works towards improving the quality of data in the domain assigned.\n\nRequired skills :\nTechnical Skills: Proficiency in MDM tools and technologies such as Informatica MDM, CluedIn, or similar platforms is essential. Familiarity with data modeling, data integration, and data quality control techniques is also important. Experience with data governance platforms like Collibra and Alation can be beneficial1.\nAnalytical Skills: Strong analytical and problem-solving skills are crucial for interpreting and working with large volumes of data. The ability to translate complex business requirements into practical MDM solutions is also necessary.\nData Management: Experience in designing, implementing, and maintaining master data management systems and solutions. This includes conducting data cleansing, data auditing, and data validation activities.\nCommunication and Collaboration: Excellent communication and interpersonal skills to effectively collaborate with business stakeholders, IT teams, and other departments.\nData Governance: In-depth knowledge of data governance, data quality, and data integration principles. The ability to develop and implement data management processes and policies is essential.\nEducational Background: A Bachelor's or Master's degree in Computer Science, Information Systems, Data Science, or a related field is typically required1.\nCertifications: Certification in the MDM domain (e.g., Certified MDM Professional) can be a plus\n\nKey Skills:\nBecome the expert at the assigned domain of data\nUnderstand all source systems feeding into the MDM\nWrite documentation of stewardship for the domain\nDevelop rules and standards for the domain of data\nGenerate measures of improvement to demonstrate to the business the quality of the data\n\nWe are seeking candidates who can join immediately or within a maximum of 30 days' notice.\nMinimum of 3+ years of relevant experience is required.\nCandidates who are willing to relocate to Bangalore or are already based in Bangalore.\nCandidates should be flexible with working UK/US shifts.",Industry Type: Analytics / KPO / Research,Department: Other,"Employment Type: Full Time, Permanent","['Informatica Mdm', 'Data Modeling', 'Data Integration']",2025-06-12 13:50:31
Data Entry Job in Big Pharma Company at Borivali East in Mumbai,Big and Reputed Pharma Company of India ...,1 - 5 years,1.5-2.5 Lacs P.A.,['Mumbai (All Areas)'],"Only 1 Year+ experienced candidate in any field, who know basic Word, Excel and computer.\n\nThis is office job and you need to work on word, excel and email for the company\n\n2 Saturday and all Sunday are Holiday\n\nFor query call at 8000044060\n\nRequired Candidate profile\nOnly 1 Year+ experienced candidate in any field, who know basic Word, Excel and computer.\n\nThis is office job and you need to work on word, excel and email.\n\n2 Saturday and all Sunday are Holiday",Industry Type: Pharmaceutical & Life Sciences,Department: Administration & Facilities,"Employment Type: Full Time, Permanent","['Office Work', 'Back Office', 'Computer', 'Data Entry', 'operation', 'Backend', 'Typing', 'Excel', 'Word', 'Computer Operating', 'MS Office']",2025-06-12 13:50:34
HIH - Data Science Lead Analyst - Evernorth,ManipalCigna Health Insurance,5 - 8 years,Not Disclosed,['Hyderabad'],"Internal Title: Data Science Lead Analyst\nExternal Title: Data Science Lead Analyst\nRole Summary\nAs a member of the Data Science Center of Expertise (DSCOE), the DS Lead Analyst is responsible for leading and enabling Data Science within Cigna Group with demonstrable aptitude in Data Science (i) Technical Skills (ii) Leadership (iii) Scope & Impact (iv) Influence. Please see Qualifications section below for more details.\n\nThe role will support the development and maintenance of machine learning models, with a focus on ensuring that models meet Cigna s requirements for governance and legal compliance. The role will require collaboration with other data scientists and involve work across many lines of business.\nKey Responsibilities:\nAnalyze model performance of new models with specific regards to requirements for legal compliance and governance standards around accuracy and bias;\nPerform periodic analyses of performance of existing models to ensure continued compliance with internal and external standards for accuracy and bias;\nConduct research (i.e. literature review) to understand when bias may be biologically or medically justifiable, and to what degree, for example: finding evidence from literature that heart disease is more prevalent among older populations\nUsing machine learning development tools to mitigate model bias when this is determined to be necessary\nCollaborating with data scientists, business stakeholders, and governance/compliance teams to ensure models meet compliance and governance standards\nQualifications:\nBachelors or Masters/PhD (preferred) in statistics or computer science or equivalent field with 5-8 years of relevant experience\nStrong proficiency in ML, statistics, python or R, SQL, version control (e.g., Git), health care data (e.g., claims, EHR)\nAbility to promote best coding practices, championing a culture of documentation/logging\nThorough understanding of ML lifecycle, including necessary tradeoffs and associated risks\nLeadership in Data Science\nCan own a project end-to-end e.g., scoping, business value estimation, ideation, dev, prod, timeline\nCollaborates and guides junior team members in completion of projects and career development\nWorks cross functionally with technical (e.g., Data Science, Data Engineering) and business (e.g., clinical, marketing, pricing, business analysts) to implement solutions with measurable value\nScope and Impact\nIndependently delivers clear and well-developed presentations for both technical and business audiences\nCreates data science specific project goals associated with project deliverables\nArticulates timeline changes, rationale, and goals to meet deadlines moving forward\nValues diversity, growth mindset, and improving health outcomes of our customers\n\nLevel of Influence\nCommunicate with stakeholders to identify opportunities and possible solutions based on business need\nDraft project charter, timeline, and features/stories\nInfluence matrix-partner leadership\nAbout Evernorth Health Services",Industry Type: Insurance,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Version control', 'Claims', 'data science', 'Legal compliance', 'Coding', 'Pharmacy', 'Machine learning', 'SQL', 'Python']",2025-06-12 13:50:36
"Sr. Data Analyst – Tableau, SQL, Snowflake",Int9 Solutions,5 - 10 years,Not Disclosed,"['Mumbai', 'Delhi / NCR', 'Bengaluru']","We are looking for a skilled Data Analyst with excellent communication skills and deep expertise in SQL, Tableau, and modern data warehousing technologies. This role involves designing data models, building insightful dashboards, ensuring data quality, and extracting meaningful insights from large datasets to support strategic business decisions.\n\nKey Responsibilities:\nWrite advanced SQL queries to retrieve and manipulate data from cloud data warehouses such as Snowflake, Redshift, or BigQuery.\nDesign and develop data models that support analytics and reporting needs.\nBuild dynamic, interactive dashboards and reports using tools like Tableau, Looker, or Domo.\nPerform advanced analytics techniques including cohort analysis, time series analysis, scenario analysis, and predictive analytics.\nValidate data accuracy and perform thorough data QA to ensure high-quality output.\nInvestigate and troubleshoot data issues; perform root cause analysis in collaboration with BI or data engineering teams.\nCommunicate analytical insights clearly and effectively to stakeholders.\n\nRequired Skills & Qualifications:\nExcellent communication skills are mandatory for this role.\n5+ years of experience in data analytics, BI analytics, or BI engineering roles.\nExpert-level skills in SQL, with experience writing complex queries and building views.\nProven experience using data visualization tools like Tableau, Looker, or Domo.\nStrong understanding of data modeling principles and best practices.\nHands-on experience working with cloud data warehouses such as Snowflake, Redshift, BigQuery, SQL Server, or Oracle.\nIntermediate-level proficiency with spreadsheet tools like Excel, Google Sheets, or Power BI, including functions, pivots, and lookups.\nBachelor's or advanced degree in a relevant field such as Data Science, Computer Science, Statistics, Mathematics, or Information Systems.\nAbility to collaborate with cross-functional teams, including BI engineers, to optimize reporting solutions.\nExperience in handling large-scale enterprise data environments.\nFamiliarity with data governance, data cataloging, and metadata management tools (a plus but not required).\nLocation : - Mumbai, Delhi / NCR, Bengaluru , Kolkata, Chennai, Hyderabad, Ahmedabad, Pune, Remote",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Snowflake', 'Tableau', 'SQL', 'BI Tools', 'Scenario Analysis', 'Cohort Analysis', 'Data Warehousing', 'SQL Server', 'Data Modeling', 'Data Analytics', 'Predictive Analysis', 'Redshift']",2025-06-12 13:50:38
Senior Consultanr - AI Cloud Engineer,AstraZeneca India Pvt. Ltd,5 - 10 years,Not Disclosed,['Chennai'],"Job Title: Senior Consultant - AI Cloud Engineer Career Level: D2 Introduction to role:\nAre you ready to tackle some of the most exciting machine-learning challenges in drug discovery? We are seeking a Senior AI Platform Engineer to join our innovative AI platform team, IGNITE. With your expertise in AWS cloud environments, youll design and deploy large-scale production infrastructure that will redefine healthcare and improve the lives of millions worldwide. As part of a close-knit team of technical specialists, youll create tools that support major AI initiatives, from clinical trial data analysis to imaging and Omics. Your role will be pivotal in providing frameworks for data scientists to develop scalable machine learning models safely and robustly. Are you prepared to bridge the gap between science and engineering with your deep expertise?\nAccountabilities:\nDesign, implement, and manage cloud infrastructure on AWS using Infrastructure as Code (IaC) tools such as Terraform or AWS CloudFormation.\nMaintain and enhance CI/CD pipelines using tools like GitHub Actions, AWS CodePipeline, Jenkins, or ArgoCD.\nEnsure platform reliability, scalability, and high availability across development, staging, and production environments.\nAutomate operational tasks, environment provisioning, and deployments using scripting languages such as Python, Bash, or PowerShell.\nEnable and maintain Amazon SageMaker environments for scalable ML model training, hosting, and pipelines.\nIntegrate AWS Bedrock to provide foundation model access for generative AI applications, ensuring security and cost control.\nLead and publish curated infrastructure templates through AWS Service Catalogue to enable consistent and compliant provisioning.\nCollaborate with security and compliance teams to implement best practices around IAM, encryption, logging, monitoring, and cost optimization.\nImplement and manage observability tools like Amazon CloudWatch, Prometheus/Grafana, or ELK for monitoring and alerting.\nSupport container orchestration environments using EKS (Kubernetes), ECS, or Fargate.\nContribute to incident response, post-mortems, and continuous improvement of the platform s operational excellence.\nEssential Skills/Experience:\nBachelor s degree in Computer Science, Engineering, or related field (or equivalent experience).\n5+ years of hands-on experience with AWS cloud services.\nStrong experience with Terraform, AWS CDK, or CloudFormation.\nProficiency in Linux system administration and networking fundamentals.\nSolid understanding of IAM policies, VPC design, security groups, and encryption.\nExperience with Docker and container orchestration using Kubernetes (EKS preferred).\nHands-on experience with CI/CD tools and version control (Git).\nExperience with monitoring, logging, and alerting systems.\nStrong solving skills and ability to work independently or in a team.\nDesirable Skills/Experience:\nAWS Certification (e.g., AWS Certified DevOps Engineer, Solutions Architect - Associate/Professional).\nExperience with serverless technologies like AWS Lambda, Step Functions, and EventBridge.\nExperience supporting machine learning or big data workloads on AWS.\nExperience with SAFe agile principles and practices.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Data analysis', 'Version control', 'Networking', 'Machine learning', 'Agile', 'Healthcare', 'Monitoring', 'Python', 'Recruitment']",2025-06-12 13:50:41
Senior Flexera Data Analyst,Luxoft,6 - 11 years,Not Disclosed,['Bengaluru'],"Internal Data Structures & Modeling\nDesign, maintain, and optimize internal data models and structures within the Flexera environment.\nMap business asset data to Flexeras normalized software models with precision and accuracy.\nEnsure accurate data classification, enrichment, and normalization to support software lifecycle tracking.\nPartner with infrastructure, operations, and IT teams to ingest and reconcile data from various internal systems.\nReporting & Analytics\nDesign and maintain reports and dashboards in Flexera or via external BI tools such as Power BI or Tableau.\nProvide analytical insights on software usage, compliance, licensing, optimization, and risk exposure.\nAutomate recurring reporting processes and ensure timely delivery to business stakeholders.\nWork closely with business users to gather requirements and translate them into meaningful reports and visualizations.\nAutomated Data Feeds & API Integrations\nDevelop and support automated data feeds using Flexera REST/SOAP APIs.\nIntegrate Flexera with enterprise tools (e.g., CMDB, SCCM, ServiceNow, ERP) to ensure reliable and consistent data flow.\nMonitor, troubleshoot, and resolve issues related to data extracts and API communication.\nImplement robust logging, alerting, and exception handling for integration pipelines.\nSkills\nMust have\nMinimum 6+ years of working with Flexera or similar software.\nFlexera Expertise: Strong hands-on experience with Flexera One, FlexNet Manager Suite, or similar tools.\nTechnical Skills:\nProficient in REST/SOAP API development and integration.\nStrong SQL skills and familiarity with data transformation/normalization concepts.\nExperience using reporting tools like Power BI, Tableau, or Excel for data visualization.\nFamiliarity with enterprise systems such as SCCM, ServiceNow, ERP, CMDBs, etc.\nProcess & Problem Solving:\nStrong analytical and troubleshooting skills for data inconsistencies and API failures.\nUnderstanding of license models, software contracts, and compliance requirements.\nNice to have\nSoft Skills: Excellent communication skills to translate technical data into business insights.\nOther\nLanguages\nEnglish: C1 Advanced\nSeniority\nSenior\nRefer a Friend\nPositive work environments and stellar reputations attract and retain top talent. Find out why Luxoft stands apart from the rest.\nRecommend a friend\nRelated jobs View all vacancies\nData Engineer with Neo4j\nData Science\nIndia\nChennai\nData Engineer with Neo4j\nData Science\nIndia\nGurugram\nBusiness Analyst\nData Science\nPoland\nRemote Poland\nBengaluru, India\nReq. VR-114544\nData Science\nBCM Industry\n23/05/2025\nReq. VR-114544\nApply for Senior Flexera Data Analyst in Bengaluru\n*",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['ERP', 'neo4j', 'Analytical', 'Data structures', 'data visualization', 'SCCM', 'Licensing', 'Analytics', 'Reporting tools', 'SQL']",2025-06-12 13:50:43
Senior Flexera Data Analyst,Luxoft,6 - 11 years,Not Disclosed,['Chennai'],"Internal Data Structures & Modeling\nDesign, maintain, and optimize internal data models and structures within the Flexera environment.\nMap business asset data to Flexeras normalized software models with precision and accuracy.\nEnsure accurate data classification, enrichment, and normalization to support software lifecycle tracking.\nPartner with infrastructure, operations, and IT teams to ingest and reconcile data from various internal systems.\nReporting & Analytics\nDesign and maintain reports and dashboards in Flexera or via external BI tools such as Power BI or Tableau.\nProvide analytical insights on software usage, compliance, licensing, optimization, and risk exposure.\nAutomate recurring reporting processes and ensure timely delivery to business stakeholders.\nWork closely with business users to gather requirements and translate them into meaningful reports and visualizations.\nAutomated Data Feeds & API Integrations\nDevelop and support automated data feeds using Flexera REST/SOAP APIs.\nIntegrate Flexera with enterprise tools (e.g., CMDB, SCCM, ServiceNow, ERP) to ensure reliable and consistent data flow.\nMonitor, troubleshoot, and resolve issues related to data extracts and API communication.\nImplement robust logging, alerting, and exception handling for integration pipelines.\nSkills\nMust have\nMinimum 6+ years of working with Flexera or similar software.\nFlexera Expertise: Strong hands-on experience with Flexera One, FlexNet Manager Suite, or similar tools.\nTechnical Skills:\nProficient in REST/SOAP API development and integration.\nStrong SQL skills and familiarity with data transformation/normalization concepts.\nExperience using reporting tools like Power BI, Tableau, or Excel for data visualization.\nFamiliarity with enterprise systems such as SCCM, ServiceNow, ERP, CMDBs, etc.\nProcess & Problem Solving:\nStrong analytical and troubleshooting skills for data inconsistencies and API failures.\nUnderstanding of license models, software contracts, and compliance requirements.\nNice to have\nSoft Skills: Excellent communication skills to translate technical data into business insights.\nOther\nLanguages\nEnglish: C1 Advanced\nSeniority\nSenior\nRefer a Friend\nPositive work environments and stellar reputations attract and retain top talent. Find out why Luxoft stands apart from the rest.\nRecommend a friend\nRelated jobs View all vacancies\nData Engineer with Neo4j\nData Science\nIndia\nGurugram\nData Engineer with Neo4j\nData Science\nIndia\nBengaluru\nData Scientist\nData Science\nIndia\nBengaluru\nChennai, India\nReq. VR-114544\nData Science\nBCM Industry\n23/05/2025\nReq. VR-114544\nApply for Senior Flexera Data Analyst in Chennai\n*",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['ERP', 'neo4j', 'Analytical', 'Data structures', 'data visualization', 'SCCM', 'Licensing', 'Analytics', 'Reporting tools', 'SQL']",2025-06-12 13:50:45
Engineer II,AMERICAN EXPRESS,3 - 8 years,Not Disclosed,['Bengaluru'],"you're a talented, creative, and motivated engineer who loves developing powerful, stable, and intuitive apps and you're excited to work with a team of individuals with that same passion. you've accumulated years of experience, and you're excited about taking your mastery of Cloud, Big Data and Java to a new level. You enjoy challenging projects involving big data sets and are cool under pressure. you're no stranger to fast-paced environments and agile development methodologies in fact, you embrace them. With your strong analytical skills, your unwavering commitment to quality, your excellent technical skills, and your collaborative work ethic, you'll do great things here at American Express.",,,,"['GIT', 'RDBMS', 'Coding', 'Finance', 'MySQL', 'Agile', 'JIRA', 'SQL', 'Python']",2025-06-12 13:50:48
Data Scientist,Devon Software Services,3 - 7 years,12-19 Lacs P.A.,['Bengaluru'],"What you will do\nBuild end-to-end machine learning models to solve business problems in Marketing\nPerform feature engineering and support data engineering to build robust data pipelines on large marketing datasets from different sources\nCollaborate with ML Engineering to build ML Pipelines to Train, Test, Deploy, Serve and Monitor models, Tune Hyperparameters, detect model and data drift and resolve issues\nPresent machine learning models outcomes, and help interpret model predictions to various stakeholders using standard data visualization tools",,,,"['Tensorflow', 'Ai Algorithms', 'Ml Algorithms', 'Machine Learning', 'Python', 'Pytorch', 'Model Development']",2025-06-12 13:50:52
Software Engineer Gen AI,Wells Fargo,2 - 7 years,Not Disclosed,['Bengaluru'],"locationsBengaluru, India\nposted onPosted 4 Days Ago\njob requisition idR-462134\nAbout this role:\nWells Fargo is seeking a Software Engineer.\n\nIn this role, you will:\nParticipate in low to moderately complex initiatives and projects associated with the technology domain, including installation, upgrades, and deployment efforts\nIdentify opportunities for service quality and availability improvements within the technology domain environment\nDesign, code, test, debug, and document for low to moderately complex projects and programs associated with technology domain, including upgrades and deployments\nReview and analyze technical assignments or challenges that are related to low to medium risk deliverables and that require research, evaluation, and selection of alternative technology domains\nPresent recommendations for resolving issues or may escalate issues as needed to meet established service level agreements\nExercise some independent judgment while also developing understanding of given technology domain in reference to security and compliance requirements\nProvide information to technology colleagues, internal partners, and stakeholders\n\nRequired Qualifications:\n2+ years of software engineering experience, or equivalent demonstrated through one or a combination of the following: work experience, training, military experience, education\n\nDesired Qualifications:\nWork as a Generative AI engineer developing enterprise-scale AI applications\nDesign, implement, and optimize LLM-based solutions using state-of-the-art frameworks\nLead Gen AI initiatives focused on developing intelligent agents and conversational systems\nDesign and build robust LLM interfaces and orchestration pipelines\nDevelop evaluation frameworks to measure and improve model performance\nImplement prompt engineering techniques to optimize model outputs\nIntegrate Gen AI capabilities with existing enterprise applications\nBuild and maintain frontend interfaces for AI applications\nStrong proficiency in Python/Java and LLM orchestration frameworks (LangChain, LangGraph)\nBasic Knowledge of model context protocols, RAG architectures, and embedding techniques\nExperience with model evaluation frameworks and metrics for LLM performance\nProficiency in frontend development with React.js for AI applications\nExperience with UI/UX design patterns specific to AI interfaces\nExperience with vector databases and efficient retrieval methods\nKnowledge of prompt engineering techniques and best practices\nExperience with containerization and microservices architecture\nStrong understanding of semantic search and document retrieval systems\nWorking knowledge of both structured and unstructured data processing\nExperience with version control using GitHub and CI/CD pipelines\nExperience working with globally distributed teams in Agile scrums\n\nJob Expectations:\nUnderstanding of enterprise use cases for Generative AI\nKnowledge of responsible AI practices and ethical considerations\nAbility to optimize AI solutions for performance and cost\nWell versed in MLOps concepts for LLM applications\nStaying current with rapidly evolving Gen AI technologies and best practices\nExperience implementing security best practices for AI applications",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Gen AI', 'Java', 'UI design', 'UX design', 'LLM orchestration', 'React.js', 'Python', 'MLOps concepts']",2025-06-12 13:50:54
Software Engineer Staff,Juniper Networks,10 - 15 years,Not Disclosed,['Bengaluru'],"The Infrastructure team under Juniper Apstra owns and evolves the backend infrastructure for Juniper Apstra.\nJoin a team where systems and data engineering converge\nyou'll work at the core of a high-performance, custom-built infrastructure platform while also helping shape the data pipelines that power our next-gen cloud analytics stack. This role offers rare dual exposure: from low-level optimization in performance-critical systems, to hands-on feature development in in-house time-series and graph databases . you'll contribute to building and refining connectivity for these databases to the cloud, driving real-world data flows across hybrid environments. If you're excited by deep infrastructure, data-intensive workloads, and full-stack thinking, this is where your impact multiplies.\n  What the Team Does\n  1. Core Infrastructure (60% of Role)\nMaintains and enhances the backend platform that powers Apstra s configuration, telemetry, and analytics pipeline.\nOwns:\nProduct packaging, upgrades, and deployment orchestration\nHomegrown file-based telemetry database (custom-built, optimized for performance)\nDistributed querying, data merging, system calls, IPC\nPython + C++ hybrid execution, including C-extensions for performance\n2. Data Engineering (40% of Role)\nEnables data synchronization from on-prem Apstra to on JCloud.\nConstructs data pipelines and ETL flows to prepare, transform, and deliver telemetry for downstream cloud analytics consumers.\nOwns cloud enablement and sync logic (eg examples datasets, format normalization, usage signals).\nWhy Candidates Should Join This Team\n  you'll Build Real Infrastructure Not Just Configure It\nThis isn t infra as code it s infrastructure as a product . you'll design, scale, and optimize distributed systems that support real-time, multi-version enterprise network workloads across campus, data center, and WAN fabrics. It s infrastructure that directly powers next-gen cloud analytics.\n  Hybrid Challenge: On-Prem Meets Cloud\nFew roles offer this blend: deep low-level systems work coupled with forward-looking data engineering . you'll help bridge Apstra s on-prem legacy with its evolving cloud-native future , working across environments to modernize and integrate.\n  Deep Ownership + Technical Core\nThis team doesn t just connect APIs. It owns the architecture behind Apstra s analytics and telemetry engines critical infrastructure that underpins Juniper s most advanced networking insights.\n  Custom Database Engineering + Analytics Edge\nyou'll work directly on our in-house telemetry database , focusing on:\nCompression, distributed query execution , and performance tuning\nBuilding feature enhancements for in-house time-series and graph databases\nEnabling cloud connectivity and insights generation for real-world data use cases\nCloud\nyou'll get early exposure to JCloud , Juniper s internal cloud platform. Think AWS-like building blocks serverless functions, ETL pipelines, distributed stores but tuned for networking workloads, security, and high observability.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Performance tuning', 'C++', 'Backend', 'Networking', 'WAN', 'Packaging', 'Distribution system', 'Analytics', 'Downstream', 'Python']",2025-06-12 13:50:56
Sr. Data Analyst,Icims,4 - 9 years,Not Disclosed,['Hyderabad'],"Overview\nThe Senior Data Analyst is responsible for serving as a subject matter expert who can lead efforts to analyze data with the goal of delivering insights that will influence our products and customers. This position will report into the Data Analytics Manager, and will work closely with members of our product and marketing teams, data engineers, and members of our Customer Success organization supporting client outreach efforts. The chief functions of this role will be finding and sharing data-driven insights to deliver value to less technical audiences, and instilling best practices for analytics in the rest of the team.",,,,"['server', 'data', 'vlookup', 'market data', 'data mapping', 'dashboards', 'research', 'sql', 'analytics', 'tables', 'prep', 'pivot', 'data visualization', 'communication skills', 'python', 'data analytics', 'data analysis', 'insights', 'pivot table', 'data engineering', 'graph', 'excel', 'data quality', 'tableau', 'data governance', 'root cause']",2025-06-12 13:51:00
Senior Associate Data Scientist,Amgen Inc,1 - 3 years,Not Disclosed,['Hyderabad'],"What you will do\nIn this vital role you will identify trends, root causes, and potential improvements in our products and processes, ensuring that patient voices are heard and addressed with utmost precision.\nAs the Sr Associate Data Scientist at Amgen, you will be responsible for developing and deploying basic machine learning, operational research, semantic analysis, and statistical methods to uncover structure in large data sets. This role involves creating analytics solutions to address customer needs and opportunities.\nCollect, clean, and manage large datasets related to product performance and patient complaints.\nEnsure data integrity, accuracy, and accessibility for further analysis.\nDevelop and maintain databases and data systems for storing patient complaints and product feedback.\nAnalyze data to identify patterns, trends, and correlations in patient complaints and product issues.\nUse advanced statistical methods and machine learning techniques to uncover insights and root causes.\nDevelop analytics or predictive models to foresee potential product issues and patient concerns to address customer needs and opportunities.\nPrepare comprehensive reports and visualizations to communicate findings to key collaborators.\nPresent insights and recommendations to cross-functional teams, including product development, quality assurance, and customer service.\nCollaborate with regulatory and compliance teams to ensure adherence to healthcare standards and regulations.\nFind opportunities for product enhancements and process improvements based on data analysis.\nWork with product complaint teams to implement changes and monitor their impact.\nStay abreast of industry trends, emerging technologies, and standard methodologies in data science and healthcare analytics.\nEvaluate data to support product complaints.\nWork alongside software developers and software engineers to translate algorithms into commercially viable products and services.\nWork in technical teams in development, deployment, and application of applied analytics, predictive analytics, and prescriptive analytics.\nPerform exploratory and targeted data analyses using descriptive statistics and other methods.\nWork with data engineers on data quality assessment, data cleansing and data analytics\nGenerate reports, annotated code, and other projects artifacts to document, archive, and communicate your work and outcomes.\n\nBasic Qualifications:\nMasters degree and 1 to 3 years of Data Science and with one or more analytic software tools or languages, and data visualization tools experience OR\nBachelors degree and 3 to 5 years of Data Science and with one or more analytic software tools or languages, and data visualization tools experience OR\nDiploma and 7 to 9 years of Data Science and with one or more analytic software tools or languages, and data visualization tools experience\nPreferred Qualifications:\nDemonstrated skill in the use of applied analytics, descriptive statistics, feature extraction and predictive analytics on industrial datasets.\nExperience in statistical techniques and hypothesis testing, experience with regression analysis, clustering and classification.\nExperience in analyzing time-series data for forecasting and trend analysis.\nExperience with Data Bricks platform for data analytics.\nExperience working with healthcare data, including patient complaints, product feedback, and regulatory requirements.",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'data bricks', 'hypothesis testing', 'predictive analytics', 'data visualization', 'machine learning', 'statistics']",2025-06-12 13:51:03
Business Intel Engineer,Amazon,2 - 7 years,Not Disclosed,['Bengaluru'],"Amazon Business Payments and Lending organization is seeking a highly quantitative Business Intelligence Engineer to drive the development of analytics and insights. You will succeed in this role if you are an organized self-starter who can learn new technologies quickly and excel in a fast-paced environment. In this position, you will be a key contributor and sparring partner, developing analytical solutions that global executive management teams and business leaders will use to deep dive into the businesses and define strategies.\n\nOur team offers a unique opportunity to build a new set of analytical experiences from the ground up. You will be part of the team that is focused on payments around the world. The position is based out of India but will interact with global leaders and teams across Europe, Japan, and US. You should be highly analytical, resourceful, customer focused, team oriented, and have an ability to work independently under time constraints to meet deadlines. You will be comfortable thinking big and diving deep. A proven track record in taking on end-to-end ownership and successfully delivering results in a fast-paced, dynamic business environment is strongly preferred.\n\nNote: This role is part of the rekindle program. For more details on rekindle program, please visit\n\n\n\nOwn the design, development, and maintenance of ongoing metrics, reports, analyses, dashboards, etc. to drive key business decisions. Ensure data accuracy by validating data for new and existing tools.\nRetrieve and analyze data using SQL, Excel, and other data management systems and develop reporting and data visualization solutions -using tools like AWS QuickSight and Looker.\nRecognize and adopt best practices in reporting and analysis: data integrity, test design, analysis, validation, and documentation.\nModel data and metadata to support the reporting pipelines and automate the manual reporting solutions.\nUnderstand trends related to business metrics and recommend strategies to stakeholders to help drive business growth. Reporting of key insight trends, using statistical rigor (Hypothesis testing, measuring experiment success defining statistical significance, developing basic regression and forecasting models) to simplify and inform the larger team of noteworthy story lines.\n\nA day in the life\nAnalyze data and find insights to either drive strategic business decisions or to drive incremental signups or revenue.\nDefine and develop business critical metrics and reports across all international business levers, key performance indicators, and financials.\nOwn alignment and standardization of analytical initiatives across the global business teams\nDrive efforts across international business leaders, BI leaders and executive management across Europe, Asia and North America.\nOwn key executive reports and metrics that are consumed by our VPs and Directors\nProvide thought leadership in global business deep dives across a variety of key performance indicators 2+ years of analyzing and interpreting data with Redshift, Oracle, NoSQL etc. experience\nExperience with data visualization using Tableau, Quicksight, or similar tools\nExperience with one or more industry analytics visualization tools (e.g. Excel, Tableau, QuickSight, MicroStrategy, PowerBI) and statistical methods (e.g. t-test, Chi-squared)\nExperience with scripting language (e.g., Python, Java, or R) Masters degree, or Advanced technical degree\nKnowledge of data modeling and data pipeline design\nExperience with statistical analysis, co-relation analysis",,,,"['Microstrategy', 'metadata', 'Data management', 'Data modeling', 'Oracle', 'Business intelligence', 'Forecasting', 'Analytics', 'SQL', 'Python']",2025-06-12 13:51:06
Site Reliability Engineer,Apple,5 - 10 years,Not Disclosed,['Bengaluru'],"The people here at Apple don t just build products they craft the kind of wonder that has revolutionised entire industries\nIt s the diversity of those people and their ideas that encourages the innovation that runs through everything we do, from amazing technology to industry-leading environmental efforts\nImagine what you could do here\nJoin Apple, and help us leave the world better than we found it\nA job at Apple is unlike any other you ve had\nYou will be challenged\nYou will be inspired\nAnd you ll be proud! At Apple, phenomenal ideas have a way of becoming phenomenal products, services, and customer experiences very quickly\nBring passion and dedication to your job, and theres no telling what you could accomplish!The Apple Services Engineering team (ASE) is one of the most exciting examples of Apple s long-held passion for combining art and technology\nThese are the people who power the App Store, Apple TV, Apple Music, Apple Podcasts, and Apple Books\nAnd they do it at an extensive scale, meeting our high expectations with dedication to deliver a huge variety of entertainment in over 35 languages to more than 150 countries\nThese engineers build secure, end-to-end solutions\nThey develop the custom software used to process all the creative work, the tools that providers use to deliver that media, all the server-side systems, and the APIs for many Apple services\nThanks to Apple s unique integration of hardware, software, and services, engineers here partner to get behind a single unified vision\nThat vision always includes a deep commitment to strengthening Apple s privacy policy, one of our core values\nAlthough services are a bigger part of Apple s business than ever before, these teams remain small, and multi-functional, offering greater exposure to the array of opportunities here\nDescription\nThe Service Reliability Engineer (SRE) role in Apple Services Engineering requires a mix of strategic engineering and design along with hands-on, technical work\nThis SRE will configure, tune, and fix multi-tiered systems to achieve optimal application performance, stability and availability\nWe manage jobs as well as applications on bare-metal and cloud computing platforms to deliver data processing for many of Apple s global products\nOur teams work with exabytes of data, petabytes of memory, and tens of thousands of jobs to enable predicable and performant data analytics enabling features in Apple Music, TV+, Appstore and other world class products\nIf you love designing, running systems that will impact millions of users, then this is the place for you!The Main Responsibilities for this position include:- Support Java-based applications & Spark/Flink jobs on Baremetal, AWS & Kubernetes- Ability to understand the application requirements (Performance, Security, Scalability, etc\n) and assess the right services/topology on AWS, Baremetal & Kubernetes- Build automation to enable self-healing systems- Build tools to monitor high performance & alert the low-latency applications- Ability to troubleshoot application-specific, core network, system & performance issues\n- Involvement in challenging and fast paced projects supporting Apples business by delivering innovative solutions\n- Monitor production, staging, test and development environments for a myriad of applications in an agile and dynamic organisation\nBS degree in computer science or equivalent field with 5+ years or MS degree with 3+ years experience, or equivalent.\nAt least 5 years in a Site Reliability Engineering (SRE), DevOps role\n5+ years of running services in a large-scale *nix environment\nUnderstanding of SRE principles and goals along with prior on-call experience\nExtensive experience in managing applications on AWS & Kubernetes\nDeep understanding and experience in one or more of the following - Hadoop, Spark, Flink, Kubernetes, AWS\nPreferred Qualifications\nFast learner with excellent analytical problem solving and interpersonal skills\nExperience supporting Java applications\nExperience with Big Data Technologies\nExperience working with geographically distributed teams and implementing high level projects and migrations\nStrong communication skills and ability to deliver results on time with high quality",Industry Type: Consumer Electronics & Appliances,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Cloud computing', 'Interpersonal skills', 'spark', 'Analytical', 'Agile', 'Data processing', 'core network', 'big data', 'AWS']",2025-06-12 13:51:08
Etl Testing Engineer,Infosys,3 - 8 years,Not Disclosed,"['Kolkata', 'Hyderabad', 'Bengaluru']","Job description,\n\nHiring for ETL testing with experience range 3-10 years\n\nMandatory Skills: ETL Testing\n\nLocation - Bangalore/Hyderabad/Pune/kolkata/Chennai/Bhubaneswar\n\nEducation: BE/B.Tech/MCA/M.Tech/MSc./MS\n\nResponsibilities\nA day in the life of an Infoscion\nAs part of the Infosys consulting team, your primary role would be to actively aid the consulting team in different phases of the project including problem definition, effort estimation, diagnosis, solution generation and design and deployment\nYou will explore the alternatives to the recommended solutions based on research that includes literature surveys, information available in public domains, vendor evaluation information, etc. and build POCs\nYou will create requirement specifications from the business needs, define the to-be-processes and detailed functional designs based on requirements.\nYou will support configuring solution requirements on the products; understand if any issues, diagnose the root-cause of such issues, seek clarifications, and then identify and shortlist solution alternatives\nYou will also contribute to unit-level and organizational initiatives with an objective of providing high quality value adding solutions to customers. If you think you fit right in to help our clients navigate their next in their digital transformation journey, this is the place for you!",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['ETL Testing', 'ETL', 'DWH Testing']",2025-06-12 13:51:11
Software Engineer 4,Juniper Networks,7 - 13 years,Not Disclosed,['Bengaluru'],"you'll work at the core of a high-performance, custom-built infrastructure platform while also helping shape the data pipelines that power our next-gen cloud analytics stack. This role offers rare dual exposure: from low-level optimization in performance-critical systems, to hands-on feature development in in-house time-series and graph databases . you'll contribute to building and refining connectivity for these databases to the cloud, driving real-world data flows across hybrid environments. If you're excited by deep infrastructure, data-intensive workloads, and full-stack thinking, this is where your impact multiplies.\nWhat the Team Does\n1. Core Infrastructure (60% of Role)\nMaintains and enhances the backend platform that powers Apstra s configuration, telemetry, and analytics pipeline.\nOwns:\nProduct packaging, upgrades, and deployment orchestration\nHomegrown file-based telemetry database (custom-built, optimized for performance)\nDistributed querying, data merging, system calls, IPC\nPython + C++ hybrid execution, including C-extensions for performance\n2. Data Engineering (40% of Role)\nEnables data synchronization from on-prem Apstra to on JCloud.\nConstructs data pipelines and ETL flows to prepare, transform, and deliver telemetry for downstream cloud analytics consumers.\nOwns cloud enablement and sync logic (eg examples datasets, format normalization, usage signals).\nWhy Candidates Should Join This Team\nyou'll Build Real Infrastructure Not Just Configure It\nThis isn t infra as code it s infrastructure as a product . you'll design, scale, and optimize distributed systems that support real-time, multi-version enterprise network workloads across campus, data center, and WAN fabrics. It s infrastructure that directly powers next-gen cloud analytics.\nHybrid Challenge: On-Prem Meets Cloud\nFew roles offer this blend: deep low-level systems work coupled with forward-looking data engineering . you'll help bridge Apstra s on-prem legacy with its evolving cloud-native future , working across environments to modernize and integrate.\nDeep Ownership + Technical Core\nThis team doesn t just connect APIs. It owns the architecture behind Apstra s analytics and telemetry engines critical infrastructure that underpins Juniper s most advanced networking insights.\nCustom Database Engineering + Analytics Edge\nyou'll work directly on our in-house telemetry database , focusing on:\nCompression, distributed query execution , and performance tuning\nBuilding feature enhancements for in-house time-series and graph databases\nEnabling cloud connectivity and insights generation for real-world data use cases\nCloud\nyou'll get early exposure to JCloud , Juniper s internal cloud platform. Think AWS-like building blocks serverless functions, ETL pipelines, distributed stores but tuned for networking workloads, security, and high observability.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Performance tuning', 'C++', 'Backend', 'Networking', 'WAN', 'Packaging', 'Distribution system', 'Analytics', 'Downstream', 'Python']",2025-06-12 13:51:13
Software Development Engineer II,Amazon,3 - 8 years,Not Disclosed,['Bengaluru'],"Want to join a team that protects and improves the buyer experience of millions Amazon customers and builds earths most customer-centric sellers daily using innovative technology including machine learning, data mining and big data analytics, cloud computing services, and highly available/scalable distributed systems that support hundreds of millions of transactions across the globe?\n\nWe have an exciting opportunity with the Regulatory Intelligence, Safety, and Compliance (RISC) engineering team, to architect and build next-generation engineering systems to quickly and accurately identify and mitigate product safety issues and potential risks to the customer experience.\n\nAs a Software Development Engineer, you will work with your team of highly skilled software, data, and ML engineers to invent, design, build and manage highly scalable distributed systems that provide availability, scalability and latency guarantees. You will work with your internal customers to balance customer requirements with team requirements and help your team and business evolve, by working with LLMs and large data sets. You will be using the latest AI, AWS and industry technologies to deliver a one-stop risk identification and remediation ecosystem for Amazon, keeping our customers safe and products compliant, building the software creating the world s most trustworthy data set on everything companies and customers need to know related to the safety and compliance of products and chains.\n\nEach and every person buying, selling, or handling Amazon products will be your customer.\n\nAs a member of this growing team, you ll be able to build the groundwork and influence its direction for the years to come. Our work cuts across various disciplines from delivering an awesome user experience via great UI/UX, to building massively scalable backend systems to support the most high-traffic pages on Amazon.com, to analytical and feedback systems which give us data-driven customer insights, to using machine learning and AI to influence recommendations and marketing. If you have a passion for consumer-facing applications, and are obsessed with customer experience, we want you!\n\nIf you d like to make a real-world difference by working hard, having fun, and making history, this is the team for you!\n\n\nIn this role you will:\nHelp define the system architecture, own and implement specific components, and help shape the overall experience\nCollaborate closely with product managers, UX designers, and other SDE team members to help define the scope of the product\nTake responsibility for technical problem solving, creatively meeting product objectives, and developing best practices\nDemonstrate cross-functional resource interaction to accomplish your goals\nWrite high-quality, efficient, testable code in Java and other object-oriented languages\nDesign Amazon-scale tools to facilitate internal business\nBuild highly available, secure, and low-latency systems\nMentor other developers\nFind out what it takes to engineer systems for ""Amazon Scale""\nDesign and build microservices\nOwn and operate the systems that you build based on real-time customer data and demanding service-level agreements\nContribute to planning, design, implementation, testing, operations, and process improvement\n\nA day in the life\nHigh-level designs, cross-team alignment, long-term architectural roadmap and technical strategy, understanding the business domain and proposing solutions to address customer and business problems, helping scope and analyze product requirements, mentorship, reviewing CRs, writing high-quality code to be an example for the team. 3+ years of non-internship professional software development experience\n2+ years of non-internship design or architecture (design patterns, reliability and scaling) of new and existing systems experience\n3+ years of Video Games Industry (supporting title Development, Release, or Live Ops) experience\nExperience programming with at least one software programming language 3+ years of full software development life cycle, including coding standards, code reviews, source control management, build processes, testing, and operations experience\nBachelors degree in computer science or equivalent",,,,"['Computer science', 'System architecture', 'Cloud computing', 'Backend', 'Coding', 'Analytical', 'Machine learning', 'Data mining', 'Internship', 'Distribution system']",2025-06-12 13:51:16
Software Development Engineer II,Amazon,3 - 8 years,Not Disclosed,['Bengaluru'],"Want to join a team that protects and improves the buyer experience of millions Amazon customers and builds earths most customer-centric sellers daily using innovative technology including machine learning, data mining and big data analytics, cloud computing services, and highly available/scalable distributed systems that support hundreds of millions of transactions across the globe?\n\nWe have an exciting opportunity with the Regulatory Intelligence, Safety, and Compliance (RISC) engineering team, to architect and build next-generation engineering systems to quickly and accurately identify and mitigate product safety issues and potential risks to the customer experience.\n\nAs a Software Development Engineer, you will work with your team of highly skilled software, data, and ML engineers to invent, design, build and manage highly scalable distributed systems that provide availability, scalability and latency guarantees. You will work with your internal customers to balance customer requirements with team requirements and help your team and business evolve, by working with LLMs and large data sets. You will be using the latest AI, AWS and industry technologies to deliver a one-stop risk identification and remediation ecosystem for Amazon, keeping our customers safe and products compliant, building the software creating the world s most trustworthy data set on everything companies and customers need to know related to the safety and compliance of products and chains.\n\nEach and every person buying, selling, or handling Amazon products will be your customer.\n\nAs a member of this growing team, you ll be able to build the groundwork and influence its direction for the years to come. Our work cuts across various disciplines from delivering an awesome user experience via great UI/UX, to building massively scalable backend systems to support the most high-traffic pages on Amazon.com, to analytical and feedback systems which give us data-driven customer insights, to using machine learning and AI to influence recommendations and marketing. If you have a passion for consumer-facing applications, and are obsessed with customer experience, we want you!\n\nIf you d like to make a real-world difference by working hard, having fun, and making history, this is the team for you!\n\n\nIn this role you will:\nHelp define the system architecture, own and implement specific components, and help shape the overall experience\nCollaborate closely with product managers, UX designers, and other SDE team members to help define the scope of the product\nTake responsibility for technical problem solving, creatively meeting product objectives, and developing best practices\nDemonstrate cross-functional resource interaction to accomplish your goals\nWrite high-quality, efficient, testable code in Java and other object-oriented languages\nDesign Amazon-scale tools to facilitate internal business\nBuild highly available, secure, and low-latency systems\nMentor other developers\nFind out what it takes to engineer systems for ""Amazon Scale""\nDesign and build microservices\nOwn and operate the systems that you build based on real-time customer data and demanding service-level agreements\nContribute to planning, design, implementation, testing, operations, and process improvement\n\nA day in the life\nHigh-level designs, cross-team alignment, long-term architectural roadmap and technical strategy, understanding the business domain and proposing solutions to address customer and business problems, helping scope and analyze product requirements, mentorship, reviewing CRs, writing high-quality code to be an example for the team. 3+ years of non-internship professional software development experience\n2+ years of non-internship design or architecture (design patterns, reliability and scaling) of new and existing systems experience\n3+ years of Video Games Industry (supporting title Development, Release, or Live Ops) experience\nExperience programming with at least one software programming language 3+ years of full software development life cycle, including coding standards, code reviews, source control management, build processes, testing, and operations experience\nBachelors degree in computer science or equivalent",Industry Type: Internet,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'System architecture', 'Cloud computing', 'Backend', 'Coding', 'Analytical', 'Machine learning', 'Data mining', 'Internship', 'Distribution system']",2025-06-12 13:51:19
"Quality Assurance Engineer I, Ads QA",Amazon,2 - 7 years,Not Disclosed,['Bengaluru'],"Are you looking to join a team that is among the fastest growing organizations at Amazon? Does wearing multiple hats and working in a fast-paced, entrepreneurial environment sound like a good fit? Then consider joining Amazon Ads.\nAmazon Advertising operates at the intersection of e-commerce and advertising, offering a range of digital advertising solutions with the goal of helping customers discover and shop for anything they want to buy. The Amazon Advertising business is growing at a fast pace and this team s mission is to apply technology to accelerate that growth through best-in-class software engineering, data engineering, and business intelligence.\n\nWithin Amazons advertising ecosystem, the QA team serves as the cornerstone of quality assurance focusing on supporting testing for all initiatives that materially change the advertiser experience or involve significant re-architecture of the ad stack. This includes comprehensive testing of all Tier 1 launches, our most critical deployments that directly affect revenue and customer experience. We drive engineering excellence through automation, developing and maintaining tools that enhance developer productivity and solve testing challenges. We evangelize quality related best practices, building mechanisms to track and report them.\n\nWe are hiring experienced Quality Assurance Engineer (QAE) to drive quality excellence in our advertising systems. In this role, you will architect, design and build test suites and frameworks to push our advertising systems to their limits and beyond. You will work with program management, development teams and our QA organization to understand the customer requirements, scope out features and then work side-by-side with the team to ensure our high quality bar is met and raised. Youll be responsible for raising our quality standards through innovative automation solutions and technical leadership, while consistently exceeding delivery expectations.\n\n\nParticipate in the full development life cycle, working within broadly defined parameters, including test plan execution and software quality needs.\nWriting and executing test plans, designing and developing test tools, automation, debugging and reporting code bugs and pushing quality upstream.\nOwn the delivery of an entire software development test suites and frameworks.\nWork closely with the technical leaders to develop the best approach for testing our functionality at scale. You are capable of understanding the interaction between the components in a distributed system in order to ensure they are functioning properly.\nCreate and execute appropriate test strategies and processes that align with business objectives and project timelines. 2+ years of quality assurance engineering experience\nExperience in manual testing\nExperience in automation testing\nExperience designing and planning test conditions, test scripts, and test data sets to ensure appropriate and adequate coverage and control Experience in API & Mobile testing\nExperience with technologies (like Selenium, Junit, TestNG, and other open source tools)\nExperience with at least one modern language such as Java, Python, C++, or C# including object-oriented design",,,,"['C++', 'Manual testing', 'Test scripts', 'Debugging', 'Test planning', 'Selenium', 'software quality', 'Business intelligence', 'Open source', 'Python']",2025-06-12 13:51:21
Principal Architect (Data and Cloud),Neoware Technology Solutions,10 - 15 years,Not Disclosed,"['Chennai', 'Bengaluru']","Principal Architect (Data and Cloud) - Neoware Technology Solutions Private Limited Principal Architect (Data and Cloud)\nRequirements\nMore than 10 years of experience in Technical, Solutioning, and Analytical roles.\n5+ years of experience in building and managing Data Lakes, Data Warehouse, Data Integration, Data Migration and Business Intelligence/Artificial Intelligence solutions on Cloud (GCP/AWS/Azure).\nAbility to understand business requirements, translate them into functional and non-functional areas, define non-functional boundaries in terms of Availability, Scalability, Performance, Security, Resilience etc.\nExperience in architecting, designing, and implementing end to end data pipelines and data integration solutions for varied structured and unstructured data sources and targets.\nExperience of having worked in distributed computing and enterprise environments like Hadoop, GCP/AWS/Azure Cloud.\nWell versed with various Data Integration, and ETL technologies on Cloud like Spark, Pyspark/Scala, Dataflow, DataProc, EMR, etc. on various Cloud.\nExperience of having worked with traditional ETL tools like Informatica / DataStage / OWB / Talend , etc.\nDeep knowledge of one or more Cloud and On-Premise Databases like Cloud SQL, Cloud Spanner, Big Table, RDS, Aurora, DynamoDB, Oracle, Teradata, MySQL, DB2, SQL Server, etc.\nExposure to any of the No-SQL databases like Mongo dB, CouchDB, Cassandra, Graph dB, etc.\nExperience in architecting and designing scalable data warehouse solutions on cloud on Big Query or Redshift.\nExperience in having worked on one or more data integration, storage, and data pipeline tool sets like S3, Cloud Storage, Athena, Glue, Sqoop, Flume, Hive, Kafka, Pub-Sub, Kinesis, Dataflow, DataProc, Airflow, Composer, Spark SQL, Presto, EMRFS, etc.\nPreferred experience of having worked on Machine Learning Frameworks like TensorFlow, Pytorch, etc.\nGood understanding of Cloud solutions for Iaas, PaaS, SaaS, Containers and Microservices Architecture and Design.\nAbility to compare products and tools across technology stacks on Google, AWS, and Azure Cloud.\nGood understanding of BI Reporting and Dashboarding and one or more tool sets associated with it like Looker, Tableau, Power BI, SAP BO, Cognos, Superset, etc.\nUnderstanding of Security features and Policies in one or more Cloud environments like GCP/AWS/Azure.\nExperience of having worked in business transformation projects for movement of On-Premise data solutions to Clouds like GCP/AWS/Azure.\nBe a trusted technical advisor to customers and solutions for complex Cloud & Data related technical challenges.\nBe a thought leader in architecture design and development of cloud data analytics solutions.\nLiaison with internal and external stakeholders to design optimized data analytics solutions.\nPartner with SMEs and Solutions Architects from leading cloud providers to present solutions to customers.\nSupport Sales and GTM teams from a technical perspective in building proposals and SOWs.\nLead discovery and design workshops with potential customers across the globe.\nDesign and deliver thought leadership webinars and tech talks alongside customers and partners.\nResponsibilities\nLead multiple data engagements on GCP Cloud for data lakes, data engineering, data migration, data warehouse, and business intelligence.\nInterface with multiple stakeholders within IT and business to understand the data requirements.\nTake complete responsibility for the successful delivery of all allocated projects on the parameters of Schedule, Quality, and Customer Satisfaction.\nResponsible for design and development of distributed, high volume multi-thread batch, real-time, and event processing systems.\nImplement processes and systems to validate data, monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it.\nWork with the Pre-Sales team on RFP, RFIs and help them by creating solutions for data.\nMentor young Talent within the Team, Define and track their growth parameters.\nContribute to building Assets and Accelerators.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Db2', 'Cognos', 'MySQL', 'Datastage', 'Presales', 'Informatica', 'Oracle', 'Teradata', 'Business intelligence', 'SQL']",2025-06-12 13:51:24
Data Solution Architect,Maveric,13 - 20 years,Not Disclosed,"['Chennai', 'Bengaluru']","Position Overview\nWe are looking for a highly experienced and versatile Solution Architect Data to lead the solution design and delivery of next-generation data solutions for our BFS clients. The ideal candidate will have a strong background in data architecture and engineering, deep domain expertise in financial services, and hands-on experience with cloud-native data platforms and modern data analytics tools. The role will require architecting solutions across Retail, Corporate, Wealth, and Capital Markets, as well as Payments, Lending, and Onboarding journeys. Possession of Data Analytics and Exposure to Data regulatory domain will be of distinct advantage. Hands on experience of AI & Gen AI enabling data related solution will be a distinct advantage for the position.",,,,"['Data Quality', 'Data Engineering', 'Data Governance', 'GenAI']",2025-06-12 13:51:26
"Associate Staff Engineer, Frontend React",Nagarro,5 - 7 years,Not Disclosed,['Bengaluru'],"We're Nagarro.\n\nWe are a Digital Product Engineering company that is scaling in a big way! We build products, services, and experiences that inspire, excite, and delight. We work at scale across all devices and digital mediums, and our people exist everywhere in the world (18000+ experts across 38 countries, to be exact). Our work culture is dynamic and non-hierarchical. We're looking for great new colleagues. That's where you come in!\n\nREQUIREMENTS:\nTotal Experience 5+ years.\nHands on working experience in front-end or full-stack development experience, with building production apps in React.js and Next.js.\nHands-on expertise writing unit/integration tests for React components (Jest, React Testing Library, etc.)\nSolid grasp of state-management patterns and libraries (Redux, React Context, Zustand, etc.).\nStrong understanding of RESTful APIs, asynchronous programming (Promises, async/await), and modern build tools (Webpack, Vite, or Turbopack).\nPractical experience with Git, pull-request workflows, and collaborative development tools (GitHub, GitLab, Bitbucket).\nAdvanced proficiency in JavaScript (ES6+) and TypeScript.\nProblem-solving mindset with the ability to tackle complex data engineering challenges. \nStrong communication and teamwork skills, with the ability to mentor and collaborate effectively.\n\nRESPONSIBILITIES:\nWriting and reviewing great quality code\nUnderstanding the clients business use cases and technical requirements and be able to convert them in to technical design which elegantly meets the requirements\nMapping decisions with requirements and be able to translate the same to developers.\nIdentifying different solutions and being able to narrow down the best option that meets the clients requirements.\nDefining guidelines and benchmarks for NFR considerations during project implementation\nWriting and reviewing design document explaining overall architecture, framework, and high-level design of the application for the developers.\nReviewing architecture and design on various aspects like extensibility, scalability, security, design patterns, user experience, NFRs, etc., and ensure that all relevant best practices are followed.\nDeveloping and designing the overall solution for defined functional and non-functional requirements; and defining technologies, patterns, and frameworks to materialize it\nUnderstanding and relating technology integration scenarios and applying these learnings in projects\nResolving issues that are raised during code/review, through exhaustive systematic analysis of the root cause, and being able to justify the decision taken.\nCarrying out POCs to make sure that suggested design/technologies meet the requirements.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Typescript', 'Javascript', 'React.Js']",2025-06-12 13:51:28
"Software Development Engineer II, RISC",Amazon,3 - 8 years,Not Disclosed,['Bengaluru'],"Want to join a team that protects and improves the buyer experience of millions Amazon customers and builds earths most customer-centric sellers daily using innovative technology including machine learning, data mining and big data analytics, cloud computing services, and highly available/scalable distributed systems that support hundreds of millions of transactions across the globe?\n\nWe have an exciting opportunity with the Regulatory Intelligence, Safety, and Compliance (RISC) engineering team, to architect and build next-generation engineering systems to quickly and accurately identify and mitigate product safety issues and potential risks to the customer experience.\n\nAs a Software Development Engineer, you will work with your team of highly skilled software, data, and ML engineers to invent, design, build and manage highly scalable distributed systems that provide availability, scalability and latency guarantees. You will work with your internal customers to balance customer requirements with team requirements and help your team and business evolve, by working with LLMs and large data sets. You will be using the latest AI, AWS and industry technologies to deliver a one-stop risk identification and remediation ecosystem for Amazon, keeping our customers safe and products compliant, building the software creating the world s most trustworthy data set on everything companies and customers need to know related to the safety and compliance of products and chains.\n\nEach and every person buying, selling, or handling Amazon products will be your customer.\n\nAs a member of this growing team, you ll be able to build the groundwork and influence its direction for the years to come. Our work cuts across various disciplines from delivering an awesome user experience via great UI/UX, to building massively scalable backend systems to support the most high-traffic pages on Amazon.com, to analytical and feedback systems which give us data-driven customer insights, to using machine learning and AI to influence recommendations and marketing. If you have a passion for consumer-facing applications, and are obsessed with customer experience, we want you!\n\nIf you d like to make a real-world difference by working hard, having fun, and making history, this is the team for you!\n\n\nIn this role you will:\nHelp define the system architecture, own and implement specific components, and help shape the overall experience\nCollaborate closely with product managers, UX designers, and other SDE team members to help define the scope of the product\nTake responsibility for technical problem solving, creatively meeting product objectives, and developing best practices\nDemonstrate cross-functional resource interaction to accomplish your goals\nWrite high-quality, efficient, testable code in Java and other object-oriented languages\nDesign Amazon-scale tools to facilitate internal business\nBuild highly available, secure, and low-latency systems\nMentor other developers\nFind out what it takes to engineer systems for ""Amazon Scale""\nDesign and build microservices\nOwn and operate the systems that you build based on real-time customer data and demanding service-level agreements\nContribute to planning, design, implementation, testing, operations, and process improvement\n\nA day in the life\nHigh-level designs, cross-team alignment, long-term architectural roadmap and technical strategy, understanding the business domain and proposing solutions to address customer and business problems, helping scope and analyze product requirements, mentorship, reviewing CRs, writing high-quality code to be an example for the team. 3+ years of non-internship professional software development experience\n2+ years of non-internship design or architecture (design patterns, reliability and scaling) of new and existing systems experience\n3+ years of Video Games Industry (supporting title Development, Release, or Live Ops) experience\nExperience programming with at least one software programming language 3+ years of full software development life cycle, including coding standards, code reviews, source control management, build processes, testing, and operations experience\nBachelors degree in computer science or equivalent",,,,"['Computer science', 'System architecture', 'Cloud computing', 'Backend', 'Coding', 'Analytical', 'Machine learning', 'Data mining', 'Internship', 'Distribution system']",2025-06-12 13:51:31
Senior Financial Data Analyst,Simcorp,4 - 5 years,Not Disclosed,['Noida'],"Financial Analyst WHAT MAKES US, US Join some of the most innovative thinkers in FinTech as we lead the evolution of financial technology. If you are an innovative, curious, collaborative person who embraces challenges and wants to grow, learn and pursue outcomes with our prestigious financial clients, say Hello to SimCorp! At its foundation, SimCorp is guided by our values caring, customer success-driven, collaborative, curious, and courageous. Our people-centered organization focuses on skills development, relationship building, and client success. We take pride in cultivating an environment where all team members can grow, feel heard, valued, and empowered. If you like what we re saying, keep reading!\nWHY THIS ROLE IS IMPORTANT TO US\nThe Financial Data Operator is responsible to perform the collection, composition, control and distribution of market and master data for financial instruments (Equities, Funds, Fixed Income, ABTS/MBS, OTS Derivatives, etc.) for various SimCorp clients and in accordance of the effective SLA agreements.\nFurthermore, this role is responsible for answering client questions and conduct all necessary data analyses of financial instruments data to resolve service delivery incidents to continue service delivery. The role is also responsible to adhere to all relevant operational risk as well as data governance and quality frameworks.\nEventually, this role also requires demonstrating very client-focused mindset, substantial know- how of financial instruments (such as Equities, Fixed Income, ABS/MBS, etc.) and provide coaching to other members.\nWHAT YOU WILL BE RESPONSIBLE FOR\nPerforms all daily service deliverables in terms of collecting, composing, controlling, and distributing financial instrument data according to effective client SLAs\nExecution of all quality checks part of the service scope and strict adherence to existing runbook(s) as well as data quality and governance frameworks and conduct first data analysis in case of unexpected data behavior\nResolve all data questions, service requests and requested audit support raised by clients in a timely and professional manner to ensure customer satisfaction and SLA compliance\nPerform all necessary tasks to comply existing operational risk frameworks (e.g., Sarbanes- Oxley Act (SOX), Risk and Control Engine (RACE) etc.)\nEfficiently support and contribute to continuous improvement of operational processes (with predominant focus on manual processes, high-risk areas), data quality checks and system functionality\nWork with local/regional clients to identify specific requirements, special data treatment or any other client demands which need to be delivered as part of the service scope\nExperience working cross-organizationally with both Business and Technology groups.\nPerform continuous know-how exchange between the different Data Operations teams in terms of processes, incidents, documentation, or other open topics to avoid know-how silos/gaps and assure service level consistency\nMonitor and report any kind of issues along the data supply chain including but not limited to interface issues, missing data files or interrupted business processes and trigger the necessary resolution processes to ensure service delivery continuation\nMaintain documentation in terms of business processes, functional descriptions, operational runbooks, or other manuals to ensure information transparency and enable know-how transfers\nWHAT WE VALUE\nFor the Financial Analyst position, we value\nMUST HAVE:\nExperience with data vendor feeds (Bloomberg, IDC, Reuters, etc.) and display products, 4- 5 years\nDeep knowledge of traditional and non-traditional financial instruments and markets including structured securities, Swaps, especially complex instruments like ABS/MBS, index linked bonds, and syndicated loans.\nBachelor s degree or equivalent in finance or engineering\nSolving master and reference data issues based on exception handling, 4-5 years\nExperience of data integration on any EDM platform, 4-5 years\nApplying operational data management and data governance, 2-3 years\nProcess design and engineering experience, 2-3 years\nExperience with service request systems or any other similar ticketing tool, like HPALM, Service Now Salesforce, etc., 4-5 years\nGOOD TO HAVE:\nAbility to troubleshoot technical glitches in existing data process and coordinate with Technology team to resolve.\nExperience in developing process automation, improvements, and streamlining using tools like KNIME, Alteryx, Excel VBA with scripting on programming language such as Python, PowerShell including intermediate knowledge of SQL",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Supply chain', 'Loans', 'Data analysis', 'Ticketing', 'Data management', 'Operational risk', 'Bloomberg', 'Fixed income', 'SQL', 'Auditing']",2025-06-12 13:51:33
Azure Data Bricks (4-15 Yrs) - Bangalore,Happiest Minds Technologies,4 - 9 years,Not Disclosed,['Bengaluru'],"Hi,\n\nGreetings from Happiest Minds Technologies\n\nCurrently we are hiring for below positions and looking for immediate joiners.\n1. Azure Databricks Bangalore 5 to 10 Yrs - Bangalore\nAs a Senior Azure Data Engineer, you will leverage Azure technologies to drive data transformation, analytics, and machine learning. You will design scalable Databricks data pipelines using PySpark, transforming raw data into actionable insights. Your role includes building, deploying, and maintaining machine learning models using MLlib or TensorFlow while optimizing cloud data integration from Azure Blob Storage, Data Lake, and SQL/NoSQL sources. You will execute large-scale data processing using Spark Pools, fine-tuning configurations for efficiency. The ideal candidate holds a Bachelors or Masters in Computer Science, Data Science, or a related field, with 7+ years in data engineering and 3+ years specializing in Azure Databricks, PySpark, and Spark Pools. Proficiency in Python PySpark, Pandas, NumPy, SciPy, Spark SQL, DataFrames, RDDs, Delta Lake, Databricks Notebooks, and MLflow is required, along with hands-on experience in Azure Data Lake, Blob Storage, and Synapse Analytics.",,,,"['Pyspark', 'Azure', 'Data Bricks', 'sql', 'ETL']",2025-06-12 13:51:35
App Dev & Support Engineer III,Conduent,6 - 11 years,Not Disclosed,['Bengaluru'],"Responsibilities\nDesign and develop highly scalable web-based applications based on business needs.\nDesign and customize software for client use with the aim of optimizing operational efficiency.\nA deep understanding of, and ability to use and explain all aspects of application integration in .NET and data integration with SQL Server and associated technologies and standards\nStrong background in building and operating SAAS platforms using the Microsoft technology stack with modern services-based architectures.\nAbility to recommend and configure Azure subscriptions and establish connectivity\nWork with IT teams to setup new application architecture requirements\nCoordinate releases with Quality Assurance Team and implement SDLC workflows and better source code integration.\nImplement build process and continuous build integration with Unit Testing framework.\nDevelop and maintain a thorough understanding of business needs from both technical and business perspectives\nAssist and mentor junior team members to enforce development guidelines.\nTake technical ownership of products and provide support with quick turnaround.\nEffectively prioritize and execute tasks in a high-pressure environment\n\n\nQualifications / Experience\nBachelor\\u2019s/master\\u2019s degree in computer science / computer engineering\nMinimum of 6+ years\\u2019 experience in building enterprise scale windows and web application using Microsoft .NET technologies.\n5+ years of experience in C#, ASP.NET MVC and.Net Core Web API\n1+ years of experience in Angular 2 or higher\nExperience in any of the following are also desirableBootstrap, Knockout, entity framework, nhibernate, Subversion, Linq, Asynchronous Module Definition (such as requirejs)\nIn depth knowledge on design patterns and unit testing frameworks.\nExperience with Agile application development.\nSQL Server development, performance tuning (SQL Server 2014/2016) and troubleshooting\nAbility to work with a sense of urgency and attention to detail\nExcellent oral and written communication skills.",Industry Type: BPM / BPO,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sql server', 'application integration', 'design patterns', '.net', 'data integration', 'c#', 'unit testing framework', 'web application', 'performance tuning', 'unit testing', 'scale', 'knockoutjs', 'net mvc', 'angular', 'linq', 'saas applications', 'asp.net', 'web api', 'mvc', 'asp', 'microsoft net']",2025-06-12 13:51:38
Data Scientist,Ltimindtree,8 - 13 years,Not Disclosed,"['Chennai', 'Bengaluru', 'Mumbai (All Areas)']",We are looking for an experienced AI ML Developers experience in data science specializing in machine learning python statistical modelling and big data technologies pyspark sql.\n\nThe ideal candidate will have a strong background in developing and deploying machine learning models optimizing ML pipelines and handling largescale structured and unstructured data to drive business impact.\n\nDeep understanding of supervised and unsupervised learning including regression classification Multiclass classification clustering and NLP Proficiency in statistical analysis AB testing and causal inference techniques Experience with model deployment and MLOps in cloud environments AWS GCP \n\nKey Responsibilities\n\nDevelop and deploy machine learning models and predictive analytics solutions for business impact\nWork with largescale structured and unstructured data to extract insights and build scalable models\nDesign implement and optimize ML pipelines for realtime and batch processing\nCollaborate with engineering product and business stakeholders to translate business problems into data science solutions\nApply statistical modeling AB testing and causal inference techniques to evaluate business performance\nApply machine learning and statistical techniques for audience segmentation helping to identify patterns and optimise business strategies\nDrive research and innovation by staying updated with cuttingedge MLAI advancements and incorporating them into our solutions\nOptimize data science models for performance scalability and interpretability in production environments\nMentor junior data scientists and contribute to best practices in data science and engineering,Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Machine Learning', 'Aiml', 'Ab Testing']",2025-06-12 13:51:40
Ai Ml Engineer,Optum,5 - 10 years,Not Disclosed,['Noida'],"Optum is a global organization that delivers care, aided by technology to help millions of people live healthier lives. The work you do with our team will directly improve health outcomes by connecting people with the care, pharmacy benefits, data and resources they need to feel their best. Here, you will find a culture guided by inclusion, talented peers, comprehensive benefits and career development opportunities. Come make an impact on the communities we serve as you help us advance health optimization on a global scale. Join us to start Caring. Connecting. Growing together.  \nAI Engineer is tasked with the design, development, and deployment of advanced generative AI models and systems. This position requires close collaboration with data scientists, product managers, and other stakeholders to integrate generative AI solutions into existing products and develop new innovative features. Proficiency in the Agentic AI framework is vital for coordinating multiple autonomous AI agents to accomplish complex tasks.\n\nPrimary Responsibilities:\nImplement Generative AI Models: Develop sophisticated generative AI algorithms and models to create new data samples, patterns, or content based on existing data or inputs\nData Processing: Collaborate with stakeholders to preprocess, analyze, and interpret extensive datasets\nModel Deployment: Deploy generative AI models into production environments, ensuring scalability and robustness\nOptimization: Conduct model testing, validation, and optimization to enhance performance\nIntegration: Work with cross-functional teams to seamlessly integrate generative AI solutions into products\nResearch: Stay current with the latest advancements in generative AI technologies and practices\nAgentic AI Framework: Utilize the Agentic AI framework to coordinate multiple AI agents for the completion of complex tasks\nMentorship: Provide mentorship to junior team members and offer technical guidance\nComply with the terms and conditions of the employment contract, company policies and procedures, and any and all directives (such as, but not limited to, transfer and/or re-assignment to different work locations, change in teams and/or work shifts, policies in regards to flexibility of work benefits and/or work environment, alternative work arrangements, and other decisions that may arise due to the changing business environment). The Company may adopt, vary or rescind these policies and directives in its absolute discretion and without any limitation (implied or otherwise) on its ability to do so\n\nRequired Qualifications:\nBachelor's or Master's degree in Computer Science, Engineering, or a related field\n5+ years of experience in software engineering with a focus on AI/ML\nExperience with data preprocessing and analysis\nKnowledge of the Agentic AI framework and its application in AI systems\nProficiency in machine learning frameworks such as TensorFlow and PyTorch\nSolid programming skills in Python, Java, or C++\nFamiliarity with cloud platforms (e.g., AWS, Google Cloud, Azure)\nProven excellent problem-solving abilities and algorithmic thinking\nProven solid communication and teamwork skills\n\nPreferred Qualifications:\nExperience with data processing\nKnowledge of version control systems like Git\nUnderstanding of Generative AI, associated technologies and frameworks like RAG, agents etc.",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Agentic Ai', 'Gen AI', 'Cloud', 'RAG', 'LLM']",2025-06-12 13:51:43
"Business Intelligence Engineer, RBS ARTS",Amazon,5 - 10 years,Not Disclosed,['Chennai'],"An candidate will be a self-starter who is passionate about discovering and solving complicated problems, learning complex systems, working with numbers, and organizing and communicating data and reports. You will be detail-oriented and organized, capable of handling multiple projects at once, and capable of dealing with ambiguity and rapidly changing priorities. You will have expertise in process optimizations and systems thinking and will be required to engage directly with multiple internal teams to drive business projects/automation for the RBS team. Candidates must be successful both as individual contributors and in a team environment, and must be customer-centric. Our environment is fast-paced and requires someone who is flexible, detail-oriented, and comfortable working in a deadline-driven work environment. Responsibilities Include Works across team(s) and Ops organization at country, regional and/or cross regional level to drive improvements and enables to implement solutions for customer, cost savings in process workflow, systems configuration and performance metrics.\n\nBasic Qualifications\nBachelors degree in Computer Science, Information Technology, or a related field\nProficiency in automation using Python\nExcellent oral and written communication skills\nExperience with SQL, ETL processes, or data transformation\n\nPreferred Qualifications\nExperience with scripting and automation tools\nFamiliarity with Infrastructure as Code (IaC) tools such as AWS CDK\nKnowledge of AWS services such as SQS, SNS, CloudWatch and DynamoDB\nUnderstanding of DevOps practices, including CI/CD pipelines and monitoring solutions\nUnderstanding of cloud services, serverless architecture, and systems integration\n\n\nAs a Business Intelligence Engineer in the team, you will collaborate closely with business partners, architect, design, implement, and BI projects & Automations.\n\nResponsibilities:\n\nDesign, development and ongoing operations of scalable, performant data warehouse (Redshift) tables, data pipelines, reports and dashboards.\nDevelopment of moderately to highly complex data processing jobs using appropriate technologies (eg SQL, Python, Spark, AWS Lambda, etc)\nDevelopment of dashboards and reports.\nCollaborating with stakeholders to understand business domains, requirements, and expectations. Additionally, working with owners of data source systems to understand capabilities and limitations.\nDeliver minimally to moderately complex data analysis; collaborating as needed with Data Science as complexity increases.\nActively manage the timeline and deliverables of projects, anticipate risks and resolve issues.\nAdopt best practices in reporting and analysis: data integrity, test design, analysis, validation, and documentation.\nInternal job description\n\nRetail Business Service, ARTS is a growing team that supports the Retail Efficiency and Paid Services business and tech teams. There is ample growth opportunity in this role for someone who exhibits Ownership and Insist on the Highest Standards, and has strong engineering and operational best practices experience.\n\nBasic qualifications:\n\n5+ years of relevant professional experience in business intelligence, analytics, statistics, data engineering, data science or related field.\nExperience with Data modeling, SQL, ETL, Data Warehousing and Data Lakes.\nStrong experience with engineering and operations best practices (version control, data quality/testing, monitoring, etc)\nExpert-level SQL.\nProficiency with one or more general purpose programming languages (eg Python, Java, Scala, etc)\nKnowledge of AWS products such as Redshift, Quicksight, and Lambda.\nExcellent verbal/written communication & data presentation skills, including ability to succinctly summarize key findings and effectively communicate with both business and technical teams.\n\nPreferred qualifications:\n\nExperience with data-specific programming languages/packages such as R or Python Pandas.\nExperience with AWS solutions such as EC2, DynamoDB, S3, and EMR.\nKnowledge of machine learning techniques and concepts. 3+ years of analyzing and interpreting data with Redshift, Oracle, NoSQL etc experience\nExperience with data visualization using Tableau, Quicksight, or similar tools\nExperience with data modeling, warehousing and building ETL pipelines\nExperience in Statistical Analysis packages such as R, SAS and Matlab\nExperience using SQL to pull data from a database or data warehouse and scripting experience (Python) to process data for modeling Experience with AWS solutions such as EC2, DynamoDB, S3, and Redshift\nExperience in data mining, ETL, etc and using databases in a business environment with large-scale, complex datasets",,,,"['SAS', 'Data modeling', 'Oracle', 'Business intelligence', 'MATLAB', 'Information technology', 'Analytics', 'SQL', 'Python']",2025-06-12 13:51:45
ETL Developer,Wipro,5 - 8 years,Not Disclosed,['Bengaluru'],"Role Purpose\nThe purpose of this role is to design, test and maintain software programs for operating systems or applications which needs to be deployed at a client end and ensure its meet 100% quality assurance parameters\n\nResponsibilities:\nDesign and implement the data modeling, data ingestion and data processing for various datasets\nDesign, develop and maintain ETL Framework for various new data source\nDevelop data ingestion using AWS Glue/ EMR, data pipeline using PySpark, Python and Databricks.\nBuild orchestration workflow using Airflow & databricks Job workflow\nDevelop and execute adhoc data ingestion to support business analytics.\nProactively interact with vendors for any questions and report the status accordingly\nExplore and evaluate the tools/service to support business requirement\nAbility to learn to create a data-driven culture and impactful data strategies.\nAptitude towards learning new technologies and solving complex problem.\nQualifications:\nMinimum of bachelors degree. Preferably in Computer Science, Information system, Information technology.\nMinimum 5 years of experience on cloud platforms such as AWS, Azure, GCP.\nMinimum 5 year of experience in Amazon Web Services like VPC, S3, EC2, Redshift, RDS, EMR, Athena, IAM, Glue, DMS, Data pipeline & API, Lambda, etc.\nMinimum of 5 years of experience in ETL and data engineering using Python, AWS Glue, AWS EMR /PySpark and Airflow for orchestration.\nMinimum 2 years of experience in Databricks including unity catalog, data engineering Job workflow orchestration and dashboard generation based on business requirements\nMinimum 5 years of experience in SQL, Python, and source control such as Bitbucket, CICD for code deployment.\nExperience in PostgreSQL, SQL Server, MySQL & Oracle databases.\nExperience in MPP such as AWS Redshift, AWS EMR, Databricks SQL warehouse & compute cluster.\nExperience in distributed programming with Python, Unix Scripting, MPP, RDBMS databases for data integration\nExperience building distributed high-performance systems using Spark/PySpark, AWS Glue and developing applications for loading/streaming data into Databricks SQL warehouse & Redshift.\nExperience in Agile methodology\nProven skills to write technical specifications for data extraction and good quality code.\nExperience with big data processing techniques using Sqoop, Spark, hive is additional plus\nExperience in data visualization tools including PowerBI, Tableau.\nNice to have experience in UI using Python Flask framework anglular\n\n\nMandatory Skills: Python for Insights. Experience: 5-8 Years.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['ETL', 'data bricks', 'aws glue', 'amazon ec2', 'python', 'spark', 'glue', 'amazon redshift', 'cloud platforms', 'aws', 'data engineering', 'sql']",2025-06-12 13:51:48
Data Scientist,Ltimindtree,6 - 11 years,Not Disclosed,['Bengaluru'],"F2F Weekend Drive - Bangalore- 14th June - DS Gen AI\n\nJob description\n\nWe are having a F2F weekend drive for the requirement of a Data Scientist + Gen AI at our LTIM Bangalore Whitefield office.\nDate - 14th June 2025\nExperience - 6+ Years\nMandatory Skills - Data Science, Gen AI, Python, RAG and Azure/AWS, AI/ML, NLPt\n\nLocation - LTIMindtree Bangalore Whitefield Office\n\nSecondary - (Any) Machine Learning, Deep Learning, ChatGPT, Langchain, Prompt, vector stores, RAG, llama, Computer vision, Deep learning, Machine learning, OCR, Transformer, regression, forecasting, classification, hyper parameter tunning, MLOps, Inference, Model training, Model Deployment\nGeneric JD-\nMore than 6 years of experience in Data Engineering, Data Science and AI / ML domain\nExcellent understanding of machine learning techniques and algorithms, such as GPTs, CNN, RNN, k-NN, Naive Bayes, SVM, Decision Forests, etc.\nExperience using business intelligence tools (e.g. Tableau, PowerBI) and data frameworks (e.g. Hadoop)\nExperience in Cloud native skills.\nKnowledge of SQL and Python; familiarity with Scala, Java or C++ is an asset\nAnalytical mind and business acumen and Strong math skills (e.g. statistics, algebra)\nExperience with common data science toolkits, such as TensorFlow, KERAs, PyTorch, PANDAs, Microsoft CNTK, NumPy etc. Deep expertise in at least one of these is highly desirable.\nExperience with NLP, NLG and Large Language Models like BERT, LLaMa, LaMDA, GPT, BLOOM, PaLM, DALL-E, etc.\nGreat communication and presentation skills. Should have experience in working in a fast-paced team culture.\nExperience with AIML and Big Data technologies like AWS SageMaker, Azure Cognitive Services, Google Colab, Jupyter Notebook, Hadoop, PySpark, HIVE, AWS EMR etc.\nExperience with NoSQL databases, such as MongoDB, Cassandra, HBase, Vector databases\nGood understanding of applied statistics skills, such as distributions, statistical testing, regression, etc.\nShould be a data-oriented person with analytical mind and business acumen.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Artificial Intelligence', 'Machine Learning', 'Python']",2025-06-12 13:51:51
AI Engineer,HCLTech,10 - 14 years,Not Disclosed,['Noida'],"Seniority: Senior\nDescription & Requirements\nPosition Summary\nThe Senior AI Engineer with GenAI expertise is responsible for developing advanced technical solutions, integrating cutting-edge generative AI technologies. This role requires a deep understanding of modern technical and cloud-native practices, AI, DevOps, and machine learning technologies, particularly in generative models. You will support a wide range of customers through the Ideation to MVP journey, showcasing leadership and decision-making abilities while tackling complex challenges.",,,,"['AI engineering', 'VMware', 'Java', 'Azure', 'Data engineering', 'AI models', 'Node.js', 'NLP', 'Azure AKS', 'Machine Learning Operations', 'AWS', 'Kubernetes', 'Python']",2025-06-12 13:51:53
Data Analyst-Having Stratup-Mid-Size companies Exp.@ Bangalore_Urgent,"A leader in this space, we deliver world...",8 - 13 years,Not Disclosed,['Bengaluru'],"Data Analyst\n\nLocation: Bangalore\nExperience: 8 - 15 Yrs\nType: Full-time\n\nRole Overview\n\nWe are seeking a skilled Data Analyst to support our platform powering operational intelligence across airports and similar sectors. The ideal candidate will have experience working with time-series datasets and operational information to uncover trends, anomalies, and actionable insights. This role will work closely with data engineers, ML teams, and domain experts to turn raw data into meaningful intelligence for business and operations stakeholders.\n\nKey Responsibilities\n\nAnalyze time-series and sensor data from various sources\nDevelop and maintain dashboards, reports, and visualizations to communicate key metrics and trends.\nCorrelate data from multiple systems (vision, weather, flight schedules, etc) to provide holistic insights.\nCollaborate with AI/ML teams to support model validation and interpret AI-driven alerts (e.g., anomalies, intrusion detection).\nPrepare and clean datasets for analysis and modeling; ensure data quality and consistency.\nWork with stakeholders to understand reporting needs and deliver business-oriented outputs.\n\n\nQualifications & Required Skills\n\nBachelors or Masters degree in Data Science, Statistics, Computer Science, Engineering, or a related field.\n5+ years of experience in a data analyst role, ideally in a technical/industrial domain.\nStrong SQL skills and proficiency with BI/reporting tools (e.g., Power BI, Tableau, Grafana).\nHands-on experience analyzing structured and semi-structured data (JSON, CSV, time-series).\nProficiency in Python or R for data manipulation and exploratory analysis.\nUnderstanding of time-series databases or streaming data (e.g., InfluxDB, Kafka, Kinesis).\nSolid grasp of statistical analysis and anomaly detection methods.\nExperience working with data from industrial systems or large-scale physical infrastructure.\n\n\nGood-to-Have Skills\n\nDomain experience in airports, smart infrastructure, transportation, or logistics.\nFamiliarity with data platforms (Snowflake, BigQuery, Custom-built using open-source).\nExposure to tools like Airflow, Jupyter Notebooks and data quality frameworks.\nBasic understanding of AI/ML workflows and data preparation requirements.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Kafka', 'SQL', 'airports', 'InfluxDB', 'Airflow', 'structured Data', 'time-series', 'JSON', 'Tableau', 'Grafana', 'R', 'AI/ML', 'Kinesis', 'Snowflake', 'time-series databases', 'Data Preparation', 'Python', 'smart infrastructure', 'BigQuery', 'streaming data', 'Power BI', 'CSV', 'transportation', 'logistic', 'reporting tools']",2025-06-12 13:51:55
"Senior Engineer, Application Development",S&P Global Market Intelligence,5 - 8 years,Not Disclosed,['Hyderabad'],"Grade Level (for internal use):\n10\nMarket Intelligence\nThe Role: Senior Full Stack Developer\nGrade level :10\nThe Team: You will work with a team of intelligent, ambitious, and hard-working software professionals. The team is responsible for the architecture, design, development, quality, and maintenance of the next-generation financial data web platform. Other responsibilities include transforming product requirements into technical design and implementation. You will be expected to participate in the design review process, write high-quality code, and work with a dedicated team of QA Analysts, and Infrastructure Teams\nThe Impact: Market Intelligence is seeking a Software Developer to create software design, development, and maintenance for data processing applications. This person would be part of a development team that manages and supports the internal & external applications that is supporting the business portfolio. This role expects a candidate to handle any data processing, big data application development. We have teams made up of people that learn how to work effectively together while working with the larger group of developers on our\nplatform.\nWhats in it for you:\nOpportunity to contribute to the development of a world-class Platform Engineering team .\nEngage in a highly technical, hands-on role designed to elevate team capabilities and foster continuous skill enhancement.\nBe part of a fast-paced, agile environment that processes massive volumes of dataideal for advancing your software development and data engineering expertise while working with a modern tech stack.\nContribute to the development and support of Tier-1, business-critical applications that are central to operations.\nGain exposure to and work with cutting-edge technologies including AWS Cloud , EMR and Apache NiFi .\nGrow your career within a globally distributed team , with clear opportunities for advancement and skill development.\nResponsibilities:\nDesign and develop applications, components, and common services based on development models, languages and tools, including unit testing, performance testing and monitoring and implementation\nSupport business and technology teams as necessary during design, development and delivery to ensure scalable and robust solutions\nBuild data-intensive applications and services to support and enhance fundamental financials in appropriate technologies.( C#, .Net Core, Databricsk, Spark ,Python, Scala, NIFI , SQL)\nBuild data modeling, achieve performance tuning and apply data architecture concepts\nDevelop applications adhering to secure coding practices and industry-standard coding guidelines, ensuring compliance with security best practices (e.g., OWASP) and internal governance policies.\nImplement and maintain CI/CD pipelines to streamline build, test, and deployment processes; develop comprehensive unit test cases and ensure code quality\nProvide operations support to resolve issues proactively and with utmost urgency\nEffectively manage time and multiple tasks\nCommunicate effectively, especially written with the business and other technical groups\nWhat Were Looking For:\nBasic Qualifications:\nBachelorsMasters Degree in Computer Science, Information Systems or equivalent.\nMinimum 5 to 8 years of strong hand-development experience in C#, .Net Core, Cloud Native, MS SQL Server backend development. Proficiency with Object Oriented Programming.\nAdvance SQL programming skills\nPreferred experience or familiarity with tools and technologies such as Odata, Grafana, Kibana, Big Data platforms, Apache Kafka, GitHub, AWS EMR, Terraform, and emerging areas like AI/ML and GitHub Copilot.\nHighly recommended skillset in Databricks, SPARK, Scalatechnologies.\nUnderstanding of database performance tuning in large datasets\nAbility to manage multiple priorities efficiently and effectively within specific timeframes\nExcellent logical, analytical and communication skills are essential, with strong verbal and writing proficiencies\nKnowledge of Fundamentals, or financial industry highly preferred.\nExperience in conducting application design and code reviews\nProficiency with following technologies:\nObject-oriented programming\nPrograming Languages (C#, .Net Core)\nCloud Computing\nDatabase systems (SQL, MS SQL)\nNice to have: No-SQL (Databricks, Spark, Scala, python), Scripting (Bash, Scala, Perl, Powershell)\nPreferred Qualifications:\nHands-on experience with cloud computing platforms including AWS , Azure , or Google Cloud Platform (GCP) .\nProficient in working with Snowflake and Databricks for cloud-based data analytics and processing.\nBenefits:\nHealth & Wellness: Health care coverage designed for the mind and body.\nContinuous Learning: Access a wealth of resources to grow your career and learn valuable new skills.\nInvest in Your Future: Secure your financial future through competitive pay, retirement planning, a continuing education program with a company-matched student loan contribution, and financial wellness programs.\nFamily Friendly Perks: Its not just about you. S&P Global has perks for your partners and little ones, too, with some best-in class benefits for families.\nBeyond the Basics: From retail discounts to referral incentive awardssmall perks can make a big difference.",Industry Type: Banking,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['GitHub Copilot', 'AI/ML', 'Kibana', 'python', 'GitHub', 'Scala', 'AWS EMR', 'Grafana', 'Odata', 'Big Data platforms', 'Terraform', 'Apache Kafka', 'Databricks', 'Spark']",2025-06-12 13:51:58
Senior Software Engineer - Python Developer,FactSet,5 - 10 years,Not Disclosed,['Hyderabad'],"FactSet creates flexible, open data and software solutions for over 200,000 investment professionals worldwide, providing instant access to financial data and analytics that investors use to make crucial decisions.\nAt FactSet, our values are the foundation of everything we do. They express how we act and operate , serve as a compass in our decision-making, and play a big role in how we treat each other, our clients, and our communities. We believe that the best ideas can come from anyone, anywhere, at any time, and that curiosity is the key to anticipating our clients needs and exceeding their expectations.",,,,"['Computer science', 'C++', 'Data analysis', 'GCP', 'Analytical', 'Machine learning', 'Technical leadership', 'Monitoring', 'SQL', 'Python']",2025-06-12 13:52:00
Senior Software Engineer,Dynamic Yield,5 - 8 years,Not Disclosed,['Pune'],"Our Purpose\nTitle and Summary\nSenior Software Engineer\nWho is Mastercard?\nMastercard is a global technology company in the payments industry. Our mission is to connect and power an inclusive, digital economy that benefits everyone, everywhere by making transactions safe, simple, smart, and accessible. Using secure data and networks, partnerships and passion, our innovations and solutions help individuals, financial institutions, governments, and businesses realize their greatest potential.\nOur decency quotient, or DQ, drives our culture and everything we do inside and outside of our company. With connections across more than 210 countries and territories, we are building a sustainable world that unlocks priceless possibilities for all.\n\nOur Team:\n\nThe AI & DPE team is responsible for product management and innovative product development of products and services to address the evolving risk and security needs of all of Mastercard s various customer segments. Platform Services team in AI & DPE focuses on defining the strategic direction for underlying platforms to enable the successful implementation of real-time, data-driven innovative products and services focused on network, security, fraud, digital identity and authentication. The team is responsible to look across all the products/services across AI & DPE and drive efficiency, re-usability and increase speed to market for our products and services.\n\nThe candidate for this position will focus on Data Unification across different data assets, enabling a single unified view of data from multiple sources and support the development of new innovative data driven cyber products, services and actionable insights.\n\nThe Role:\nWe are seeking a Senior Software Engineer who will:\n\nPerform data ingestion, aggregation, processing to drive and enable relevant insights from available data sets.\nPartner with various teams (i.e., Product Manager, Data Science, Platform Strategy, Technology) on data needs/requirements in order to deliver data solutions that generate business value.\nManipulate and analyze complex, high-volume, high-dimensionality data from varying sources using a variety of tools and data analysis techniques.\nIdentify innovative ideas and deliver proof of concepts, prototypes to deliver against the existing and future needs and propose new products, services and enhancements.\nIntegrate & Unify new data assets which increase the value proposition for our customers and enhance our existing solutions and services.\nAnalyse large volumes of transaction and product data to generate insights and actionable recommendations to drive business growth\nCollect and synthesize feedback from clients, development, product and sales teams for new solutions or product enhancements.\nApply knowledge of metrics, measurements, and benchmarking to complex and demanding solutions.\n\nAll about You\nMinimum 5-8 years of relevant experience.\nGood understanding of programming language preferably PySpark and Big Data technologies.\nExperience with Enterprise Business Intelligence Platform/Data platform.\nStrong SQL and higher-level programming languages with solid knowledge of data mining, machine learning algorithms and tools\nExperience with data integration tools - ETL/ELT tools (i.e. Apache NiFi, Azure Data Factory, Databricks)\nExposure to collecting and/or working with data including standardizing, summarizing, offering initial observations and highlighting inconsistencies.\nStrong understanding of the application of analytical methods and data visualization to support business decisions.\nAbility to understand complex operational systems and analytics/business intelligence tools for the delivery of information products and analytical offerings to a large, global user base.\nAble to work in a fast-paced, deadline-driven environment as part of a team and as an individual contributor\nAbility to easily move between business, analytical, and technical teams and articulate solution requirements for each group",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Product management', 'Data analysis', 'Information security', 'Analytical', 'Network security', 'Data mining', 'Business intelligence', 'Operations', 'Analytics', 'SQL']",2025-06-12 13:52:02
MDM Associate Data Steward,Amgen Inc,0 - 3 years,Not Disclosed,['Hyderabad'],"ABOUT THE ROLE\n\nRole Description\n\nWe are seeking an MDM Associate Data Steward who will be responsible for ensuring the accuracy, completeness, and reliability of master data across critical business domains such as Customer, Product, Affiliations, and Payer. This role involves actively managing and curating master data through robust data stewardship processes, comprehensive data cataloging, and data governance frameworks utilizing Informatica or Reltio MDM platforms. Additionally, the incumbent will perform advanced data analysis, data validation, and data transformation tasks through SQL queries and Python scripts to enable informed, data-driven business decisions. The role emphasizes cross-functional collaboration with various teams, including Data Engineering, Commercial, Medical, Compliance, and IT, to align data management activities with organizational goals and compliance standards.\n\nRoles & Responsibilities\nResponsible for master data stewardship, ensuring data accuracy and integrity across key master data domains (e.g., Customer, Product, Affiliations).\nConduct advanced data profiling, cataloging, and reconciliation activities using Informatica or Reltio MDM platforms.\nManage the reconciliation of potential matches, ensuring accurate resolution of data discrepancies and preventing duplicate data entries.\nEffectively manage Data Change Request (DCR) processes, including reviewing, approving, and documenting data updates in compliance with established procedures and SLAs.\nExecute and optimize SQL queries for validation and analysis of master data.\nPerform basic Python for data transformation, quality checks, and automation.\nCollaborate effectively with cross-functional teams including Data Engineering, Commercial, Medical, Compliance, and IT to fulfill data requirements.\nSupport user acceptance testing (UAT) and system integration tests for MDM related system updates.\nImplement data governance processes ensuring compliance with enterprise standards, policies, and frameworks.\nDocument and maintain accurate SOPs, Data Catalogs, Playbooks, and SLAs.\nIdentify and implement process improvements to enhance data stewardship and analytic capabilities.\nPerform regular audits and monitoring to maintain high data quality and integrity.\nBasic Qualifications and Experience\nMasters degree with 1 - 3 years of experience in Business, Engineering, IT or related fieldOR\nBachelors degree with 2 - 5 years of experience in Business, Engineering, IT or related fieldOR\nDiploma with 6 - 8 years of experience in Business, Engineering, IT or related field\nFunctional\n\nSkills:\nMust-Have Skills:\nDirect experience in data stewardship, data profiling, and master data management.\nHands-on experience with Informatica or Reltio MDM platforms.\nProficiency in SQL for data analysis and querying.\nKnowledge of data cataloging techniques and tools.\nBasic proficiency in Python scripting for data processing.\nGood-to-Have\n\nSkills:\nExperience with PySpark and Databricks for large-scale data processing.\nBackground in the pharmaceutical, healthcare, or life sciences industries.\nFamiliarity with AWS or other cloud-based data solutions.\nStrong project management and agile workflow familiarity (e.g., using Jira, Confluence).\nUnderstanding of regulatory compliance related to data protection (GDPR, CCPA).\nProfessional Certifications\nAny ETL certification ( e.g. Informatica)\nAny Data Analysis certification (SQL)\nAny cloud certification (AWS or AZURE)\nSoft\n\nSkills:\nStrong analytical abilities to assess and improve master data processes and solutions.\nExcellent verbal and written communication skills, with the ability to convey complex data concepts clearly to technical and non-technical stakeholders.\nEffective problem-solving skills to address data-related issues and implement scalable solutions.\nAbility to work effectively with global, virtual teams",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data management', 'python', 'project management', 'data analysis', 'data stewardship', 'agile database', 'data processing', 'sql', 'data profiling']",2025-06-12 13:52:05
Data & Analytics Subject Matter Expert,Trianz,10 - 15 years,Not Disclosed,"['Hyderabad', 'Chennai', 'Bengaluru']","Role Overview\nWe are looking for a Data & Analytics Subject Matter Expert with deep expertise in Data Engineering, Business Intelligence (BI), and AWS cloud ecosystems . This role demands strategic thinking, hands-on execution, and collaboration across technical and business teams to deliver impactful data-driven solutions.\nKey Responsibilities\n1. Data Architecture & Engineering\nDesign and implement scalable, high-performance data solutions on AWS.\nBuild robust data pipelines, ETL/ELT workflows, and data lake architectures.\nEnforce data quality, security, and governance practices.\n2. Business Intelligence & Insights\nDevelop interactive dashboards and visualizations using Power BI, Tableau, or QuickSight.\nDefine data models and KPIs to support data-driven decision-making.\nCollaborate with business teams to extract insights that drive action.\n3. Cloud & Advanced Analytics\nDeploy data warehousing solutions using Redshift, Glue, S3, Athena, and other AWS services.\nOptimize storage and processing strategies for performance and cost-efficiency.\nExplore AI/ML integrations for predictive and advanced analytics (preferred).\n4. Collaboration & Best Practices\nPartner with cross-functional teams (engineering, data science, business) to align on data needs.\nChampion best practices in data governance, compliance, and architecture.\nTranslate business requirements into scalable technical solutions.\nRequired Qualifications\nEducation\nBachelor s or Master s in Computer Science, Information Technology, Data Science, or related discipline.\nExperience\n10+ years of experience in data engineering, BI, and analytics domains.\nProven experience with AWS data tools and modern data architectures.\nTechnical Skills\nStrong command of AWS services: Redshift, Glue, S3, Athena, Lambda, Kinesis.\nProficient in SQL, Python, or Scala.\nExperience building and maintaining ETL/ELT workflows and data models.\nExpertise in BI tools like Power BI, Tableau, QuickSight, or Looker.\nFamiliarity with AI/ML models and frameworks is a plus.\nCertifications\nPreferred: AWS Certified Data Analytics - Specialty.\nAdditional certifications in AWS, data engineering, or analytics are a plus.\nWhy Join Trianz\nJoin a high-growth, innovation-led firm delivering transformation at scale.\nCollaborate with global teams on cutting-edge cloud and analytics projects.\nEnjoy a competitive compensation structure and clear career progression pathways.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['power bi', 'Data analytics', 'Subject Matter Expert', 'Business intelligence', 'AWS', 'Information technology', 'Analytics', 'SQL', 'Data architecture', 'Python']",2025-06-12 13:52:07
Data Architecture,Top B2B MNC in Management Consulting Dom...,5 - 8 years,Not Disclosed,['Bengaluru'],"About the Company\nGreetings from Teamware Solutions a division of Quantum Leap Consulting Pvt. Ltd\n\nAbout the Role\nWe are hiring a Data Architecture\n\nLocation: Bangalore\nWork Model: Hybrid\nExperience: 5-9 Years\nNotice Period: Immediate to 15 Days\n\nJob Description:\nData Architecture, Data Governance, Data Modeling\n\nAdditional Information:\nMandatory Skills: Data Architecture, Data Governance, Data Modeling\nNice to have skills Certification in Data Engineering\nInterview Mode Virtual Interview\nminimum 5 yrs relevant experience and maximum 9 yrs for this requirement. Someone with more experience in building PySpark data streaming jobs on Azure Databricks\nwho have done real projects, have expertise, and hands-on experience also\nAlso, Data governance and data modeling experience with a minimum of 4 years is mandatory\nCommunication should be excellent\n\n\nPlease let me know if you are interested in this position and send me your resumes to netra.s@twsol.com",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Data Architecture', 'Data Modeling', 'Data Governance', 'Data Engineering']",2025-06-12 13:52:09
Senior Software Engineer,Mastercard,5 - 8 years,Not Disclosed,['Pune'],"Senior Software Engineer\n?\n\n\n\n\nThe AI & DPE team is responsible for product management and innovative product development of products and services to address the evolving risk and security needs of all of Mastercard s various customer segments. Platform Services team in AI & DPE focuses on defining the strategic direction for underlying platforms to enable the successful implementation of real-time, data-driven innovative products and services focused on network, security, fraud, digital identity and authentication. The team is responsible to look across all the products/services across AI & DPE and drive efficiency, re-usability and increase speed to market for our products and services.\n\nThe candidate for this position will focus on Data Unification across different data assets, enabling a single unified view of data from multiple sources and support the development of new innovative data driven cyber products, services and actionable insights.\n\nThe Role:\nWe are seeking a Senior Software Engineer who will:\n\nPerform data ingestion, aggregation, processing to drive and enable relevant insights from available data sets.\nPartner with various teams (i.e., Product Manager, Data Science, Platform Strategy, Technology) on data needs/requirements in order to deliver data solutions that generate business value.\nManipulate and analyze complex, high-volume, high-dimensionality data from varying sources using a variety of tools and data analysis techniques.\nIdentify innovative ideas and deliver proof of concepts, prototypes to deliver against the existing and future needs and propose new products, services and enhancements.\nIntegrate & Unify new data assets which increase the value proposition for our customers and enhance our existing solutions and services.\nAnalyse large volumes of transaction and product data to generate insights and actionable recommendations to drive business growth\nCollect and synthesize feedback from clients, development, product and sales teams for new solutions or product enhancements.\nApply knowledge of metrics, measurements, and benchmarking to complex and demanding solutions.\n\nAll about You\nMinimum 5-8 years of relevant experience.\nGood understanding of programming language preferably PySpark and Big Data technologies.\nExperience with Enterprise Business Intelligence Platform/Data platform.\nStrong SQL and higher-level programming languages with solid knowledge of data mining, machine learning algorithms and tools\nExperience with data integration tools - ETL/ELT tools (i.e. Apache NiFi, Azure Data Factory, Databricks)\nExposure to collecting and/or working with data including standardizing, summarizing, offering initial observations and highlighting inconsistencies.\nStrong understanding of the application of analytical methods and data visualization to support business decisions.\nAbility to understand complex operational systems and analytics/business intelligence tools for the delivery of information products and analytical offerings to a large, global user base.\nAble to work in a fast-paced, deadline-driven environment as part of a team and as an individual contributor\nAbility to easily move between business, analytical, and technical teams and articulate solution requirements for each group",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Product management', 'Data analysis', 'Information security', 'Analytical', 'Network security', 'Data mining', 'Business intelligence', 'Operations', 'Analytics', 'SQL']",2025-06-12 13:52:11
Data Architect - AWS,Happiest Minds Technologies,10 - 15 years,Not Disclosed,"['Noida', 'Pune', 'Bengaluru']","Roles and responsibilities\nWork closely with the Product Owners and stake holders to design the Technical Architecture for data platform to meet the requirements of the proposed solution.\nWork with the leadership to set the standards for software engineering practices within the machine learning engineering team and support across other disciplines\nPlay an active role in leading team meetings and workshops with clients.\nChoose and use the right analytical libraries, programming languages, and frameworks for each task.",,,,"['SQL', 'data architect', 'Python', 'Pyspark', 'Apache Airflow', 'GLUE', 'Kinesis', 'Amazon Redshift', 'Data Architecture Principles', 'Data Modeling', 'Data Warehousing', 'Athena', 'Lambda', 'AWS']",2025-06-12 13:52:13
Senior Software Quality Engineer,Mastercard,4 - 9 years,Not Disclosed,['Pune'],"Senior Software Quality Engineer\n?\n\nMastercard is a global technology company in the payments industry. We work to connect and power an inclusive, digital economy that benefits everyone, everywhere by making transactions safe, simple, smart and accessible. Using secure data and networks, partnerships and passion, our innovations and solutions help individuals, financial institutions, governments, and businesses realize their greatest potential.\n\n\n\nOverview:\n\nTransfer Solutions is responsible for driving Mastercard s expansion into new payment flows such as Disbursements & Remittances. The team is working on creating a market-leading money transfer proposition, Mastercard Move, to power the next generation of payments between people and businesses, whether money is moving domestically or across borders, by delivering the ability to pay and get paid with choice, transparency, and flexibility.\n\nThe Product & Engineering teams within Transfer Solutions are responsible for designing, developing, launching, and maintaining products and services designed to capture these flows from a wide range of customer segments. By addressing customer pain points for domestic and cross-border transfers, the goal is to scale Mastercard s Disbursements & Remittances business, trebling volume over the next 4 years.\n\nThe Role:\narticipate in requirements discussion, test planning, test data creation and execution of testing Plan in adherence with MasterCard standards, processes and best practices.\nWork with project teams to meet scheduled due dates, while identifying emerging issues and recommending solutions for problems and independently perform assigned tasks.\nDesign and develop test automation frameworks to validate system to system interfaces and complete software solutions (for Database/ETL, API and UI tests)\nInteract with business and development stakeholders to define test plans and schedules\nTranslate complex system requirements into test requirements and testing methods\nIdentify and implement complex automation efforts, including refactoring of automation code where needed\nDevelop test scripts and perform automated and manual exploratory testing to ensure software meets business and security requirements and established practices.\nDesign and develop test data management for defined test cases, recognize test environment preparation needs, and execute existing test plans and report results\nOwn responsibility for defect management and oversight and escalation of issues discovered during the testing phase\nDocument as per Software Development Best Practices and follow MasterCard Quality Assurance and Quality Control processes.\nDocument performance test strategies and test plans, and execute performance validation\nCollect quality metric data and communicate test status/risks to stakeholders\nAct as first-review for project-level reviews, walkthroughs and inspections\nProvide technical support and mentoring to junior team members\nPerform demos of new product functionality to stakeholders\nDevelop business and product knowledge over time.\nIdentify opportunities to improve effectiveness and time-to-market\nProvide training and guidance to team members on quality best practices and principles\nFacilitate knowledge sharing sessions to promote a culture of quality awareness\nBe a strong individual contributor to the implementation efforts of product solutions\n\nAll About You:\n\nBachelors degree in Information Technology, Computer Science or Management Information Systems or equivalent work experience\n8+ years of experience in the Software Engineering with a focus on Quality Engineering methodologies\nTechnical skills in Java, Selenium, Cucumber, Soap UI, Spring framework, REST, JSON, Eclipse, GIT, Jmeter/Blazemeter\nExcellent SQL skills to work on large and complex data sources and capability of comprehending and writing complex queries\nExperience testing APIs (REST and SOAP), web user interface, and/or reports\nExperience in implementing CI/CD build pipelines with tools like Git/Bit Bucket, Jenkins and Maven\nSuccessfully validated one or more application codebases via automation, for new feature functionality and regression testing\nExperience working in Agile teams and conversant with Agile/SAFe tenets and ceremonies. Strong analytical and problem-solving abilities, with quick adaptation to new technologies, methodologies, and systems\nExcellent English communication skills (both written and verbal) to effectively interact with multiple technical teams and other stakeholders\nHigh-energy, detail-oriented and proactive, with ability to function under pressure in an independent environment along with a high degree of initiative and self-motivation to drive results\nEager to experiment with new team processes and innovate on testing approach\nPrior experience with Data Analysis and Data Engineering is a plus\nStrong collaboration skills and ability to work effectively in a cross-functional, interdependent team environment",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Maven', 'Manager Quality Assurance', 'Eclipse', 'Information security', 'Agile', 'JSON', 'Selenium', 'Information technology', 'Technical support', 'SQL']",2025-06-12 13:52:16
HIH - Data Science Lead Analyst - Evernorth,Cigna Medical Group,5 - 8 years,Not Disclosed,['Hyderabad'],"Role Summary\nAs a member of the Data Science Center of Expertise (DSCOE), the DS Lead Analyst is responsible for leading and enabling Data Science within Cigna Group with demonstrable aptitude in Data Science (i) Technical Skills (ii) Leadership (iii) Scope & Impact (iv) Influence. Please see Qualifications section below for more details.\n\nThe role will support the development and maintenance of machine learning models, with a focus on ensuring that models meet Cigna requirements for governance and legal compliance. The role will require collaboration with other data scientists and involve work across many lines of business.\nKey Responsibilities:\nAnalyze model performance of new models with specific regards to requirements for legal compliance and governance standards around accuracy and bias;\nPerform periodic analyses of performance of existing models to ensure continued compliance with internal and external standards for accuracy and bias;\nConduct research (i.e. literature review) to understand when bias may be biologically or medically justifiable, and to what degree, for example: finding evidence from literature that heart disease is more prevalent among older populations\nUsing machine learning development tools to mitigate model bias when this is determined to be necessary\nCollaborating with data scientists, business stakeholders, and governance/compliance teams to ensure models meet compliance and governance standards\nQualifications:\nBachelors or Masters/PhD (preferred) in statistics or computer science or equivalent field with 5-8 years of relevant experience\nStrong proficiency in ML, statistics, python or R, SQL, version control (e.g., Git), health care data (e.g., claims, EHR)\nAbility to promote best coding practices, championing a culture of documentation/logging\nThorough understanding of ML lifecycle, including necessary tradeoffs and associated risks\nLeadership in Data Science\nCan own a project end-to-end e.g., scoping, business value estimation, ideation, dev, prod, timeline\nCollaborates and guides junior team members in completion of projects and career development\nWorks cross functionally with technical (e.g., Data Science, Data Engineering) and business (e.g., clinical, marketing, pricing, business analysts) to implement solutions with measurable value\nScope and Impact\nIndependently delivers clear and well-developed presentations for both technical and business audiences\nCreates data science specific project goals associated with project deliverables\nArticulates timeline changes, rationale, and goals to meet deadlines moving forward\nValues diversity, growth mindset, and improving health outcomes of our customers\n\nLevel of Influence\nCommunicate with stakeholders to identify opportunities and possible solutions based on business need\nDraft project charter, timeline, and features/stories\nInfluence matrix-partner leadership",Industry Type: Medical Services / Hospital,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Version control', 'GIT', 'Claims', 'data science', 'Legal compliance', 'Coding', 'Machine learning', 'SQL', 'Python']",2025-06-12 13:52:18
Data Quality Analyst,Yallas Technology Solutions Opc,5 - 10 years,Not Disclosed,[],"Title: Data Quality Analyst/Developer\nDuration: 6 months to 1 year contract\nLocation:  Remote\nNotice period - Immediate to 7 days\nUAN /EPFO Report Required\n\nWork Experience:\n5 + years of this experience - Experience doing Data Emendation\nDesign/Develop Rules, monitoring mechanisms, notification\nDesign/Develop UI, Workflows, security\nDesign/Develop analytics (overall DQ reporting, usage statistics, etc).\nDesign/Develop migration activities to migrate existing DQ assets between our existing DQ platform and new DQ platform.\nDesign integration with MDM & Catalog (as needed)\nMonitor system performance and suggest optimization strategies (as needed).\nWork with DT to maintain system - patches, backups, etc.\nWork with LYB's Data Stewards to support their governance activities.\nTesting\n\nThe DQ Analyst/Developer should have experience with IMDC (for the sake of our example) cloud DQ and observability, JSON (depending on tool) Deep SQL skills, Integration tools/methodologies - API as well as ETL, Data Analysis, Snowflake or Databricks knowledge (for lineage), Power BI (nice to have), SAP ECC knowledge (nice to have), experience with cloud platforms (Azure, AWS, Google).\nIf you are interested please share required details along with resume\nFull Name:\nCurrent or Previous organization:\nCurrent Location:\nTotal Experience:\nRelevant experience as Python Developer:\nhow many years of experience In Azure, AWS, Google\nHow many years of experience in UI, Workflows, security\nWorking as full time or contract:\nReason for job change:\nAny other offers inhand:\nCurrent CTC:\nexpected CTC:\nNotice Period:\nemail id:\ncontact Number :\nDomain name:\nare you ok to work Cotractual role?:\nshare your aadhar or pan card for the verification",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Temporary/Contractual","['Data quality analyst', 'cloud data quality', 'Azure', 'data quality developer', 'JSON', 'google', 'Informatica', 'AWS']",2025-06-12 13:52:20
Master Data Management Architect,K-logix Partnering Solutions,8 - 13 years,Not Disclosed,[],"Bachelor'sMaster's\nOverview:\n\nWe are seeking a highly skilled and experienced Celonis MDM Data Architect to lead the design, implementation, and optimization of our Master Data Management (MDM) solutions in alignment with Celonis Process Mining and Execution Management System (EMS) capabilities.\nThe ideal candidate will play a key role in bridging data architecture and business process insights, ensuring data quality, consistency, and governance across the enterprise.\n\nKey Responsibilities:\nDesign and implement MDM architecture and data models aligned with enterprise standards and best practices.\n• Lead the integration of Celonis with MDM platforms to drive intelligent process automation, data governance, and operational efficiencies.\n• Collaborate with business stakeholders and data stewards to define MDM policies, rules, and processes.\n• Support data profiling, data cleansing, and data harmonization efforts to improve master data quality.\n• Work closely with Celonis analysts, data engineers, and process owners to deliver actionable insights based on MDM-aligned process data.\n• Develop and maintain scalable, secure, and high-performance data pipelines and integration architectures.\n• Translate business requirements into technical solutions, ensuring alignment with both MDM and Celonis data models.\n• Create and maintain data architecture documentation, data dictionaries, and metadata repositories.\n• Monitor and optimize the performance of MDM systems and Celonis EMS integrations.\nQualifications:\nBachelors or Masters degree in Computer Science, Information Systems, Data Engineering, or a related field.\n• 7+ years of experience in data architecture, MDM, or enterprise data management.\n• 2+ years of hands-on experience with Celonis and process mining tools.\n• Proficient in MDM platforms (e.g., Informatica MDM, SAP MDG, Oracle MDM, etc.).\n• Strong knowledge of data modeling, data governance, and metadata management.\n• Proficiency in SQL, data integration tools (e.g., ETL/ELT platforms), and APIs.\n• Deep understanding of business process management and data-driven transformation initiatives.",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['Celonis', 'MDM', 'Master Data Management', 'ETL', 'Elt']",2025-06-12 13:52:22
Clinical Data Programming Lead,ICON plc,5 - 10 years,Not Disclosed,['Chennai'],"Clinical Data Programming Lead (SAS + EDC) - Bangalore/Chennai (Hybrid)\nThe Clinical Data Programming Lead role is part of the Investigator Payments Group (IPG) and will be involved in the programming, delivery and oversight of data integration solutions between the Electronic Data Capture (EDC) system, the Clinical Trial Management System (CTMS) and our payment entitlement calculation system (APECS).\nThe Clinical Data Programming Lead provides support to and acts as a back-up for the IPG Manager. To effectively assist the IPG Manager in leading the activities for those under his/her jurisdiction in a manner that ensures all timeframes and targets are met.",,,,"['Assurance', 'Manager Quality Assurance', 'EDC', 'System integration', 'clinical development', 'Clinical research', 'Healthcare', 'SAS Programming', 'Monitoring', 'clinical data']",2025-06-12 13:52:24
Data Scientist,Ltimindtree,7 - 12 years,Not Disclosed,['Hyderabad'],Data Scientist\n\nJob Description\n\nResponsibilities\n\nWork with team members across multiple disciplines to understand the data behind product features user behaviors the security landscape and our goals\nAnalyze data from several large sources then automate solutions using scheduled processes models and alerts\nWork with partners to design and improve metrics that guide our decisions for the product\nDetect patterns associated with fraudulent accounts and anomalous behavior\nSolve scientific problems and create new methods independently\nTranslate requirements and security questions into data insights\nSet up alerting mechanisms so our leadership is always aware of the security posture\n\nQualifications\n\nPostgraduate degree with specialization in machine learning artificial intelligence statistics or related fields or 2 years of equivalent work experience in applied machine learning and analytics\nExperience with SQL Snowflake and NoSQL databases\nProficiency in Python programming\nFamiliarity with statistics modeling and data visualization\n\nExperience\n\nExperience building statistical and machine learning models applying techniques such as regression classification clustering and anomaly detection Time series and Classical ML modeling\nFamiliarity with Snowflake SQL\nFamiliarity with cloud platforms such as AWS\nSome experience to software development or data engineering\nAnalyze business problems or research questions identify relevant data points and extract meaningful insights,Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Machine Learning', 'Snowflake Sql', 'AWS']",2025-06-12 13:52:27
Technical Specialist - Data Scientist,Fidelity International,8 - 9 years,Not Disclosed,['Gurugram'],"Application Deadline: 21 June 2025\nTitle Senior Analyst- Data Scientist\nDepartment Data Value\nLocation Gurgaon\nReports To Suman Kaur\nLevel 3\nWe re proud to have been helping our clients build better financial futures for over 50 years. How have we achieved this? By working together - and supporting each other - all over the world. So, join our Data Value team and feel like you re part of something bigger.\nAbout your team\nData Value team drives the renewed focus of extracting value from Fidelity s data for business and client insights and working as one voice with the business, technology, and data teams. The team s vision is to create measurable business impact by leveraging technology and utilising the skills to generate valuable insights and streamline engagements. The Data Science function within Data Value supports Fidelity International s Sales, Marketing, Propositions, Risk, Finance, Customer Service and HR teams across the globe. The key objectives of the function are:\nTo develop deep customer insights for our businesses helping them segment and target customers more effectively\nTo develop a fact-based understanding of sales trends and identify actionable sales growth opportunities for each of our sales channels\nTo understand customer preferences in terms of products, service attributes and marketing activity to help refine each of these\nTo help develop new services lines e.g. develop customer analytics for key IFAs, DC Clients, Individual clients etc.\nTo develop market and competitive intelligence in our key markets to help shape our business planning in those markets\nThe function works directly with business heads and other senior stakeholder s stakeholders to identify areas of analytics, define problem statements and develop key insights.\nAbout your role\nYou will be expected to take a leading role in developing the Data Science and Advanced Analytics solutions for our business. This will involve:\nEngaging with the key stakeholders to understand Fidelity s sales, marketing, client services and propositions context\nImplement advanced analytics solutions on On-Premises/Cloud platforms, develop proof-of-concepts and engage with internal and external ecosystem to progress the proof of concepts to production.\nEngaging and collaborating with different other internal teams like Data engineering, DevOps, technology team etc for development of new tools, capabilities, and solutions.\nMaximize Adoption of Cloud Based advanced analytics solutions: Build out sandbox analytics environments using Snowflake, AWS, Adobe, Salesforce.\nAbout you\nKey Responsibilities\nDeveloping and Delivering Data Science solutions for business (40%)\nPartner with internal (FIL teams) & external ecosystem to design and deliver advanced analytics enabled Data Science solutions\nCreate advanced analytics solution on quantitative and text data using Artificial Intelligence, Machine Learning and NLP techniques.\nCreate compelling visualisations that enable the smooth consumption of predictions and insights for customer benefit\n. Stakeholder Management (30%)\nWorks with channel heads/stakeholders and other sponsors understand the business problem and translate it into appropriate analytics solution.\nEngages with key stakeholders for smooth execution, delivery, and implementation of solutions\nAdoption of Cloud enabled Data Science solutions: (20%)\nMaximize Adoption of Cloud Based advanced analytics solution\nBuild out sandbox analytics environments using Snowflake, AWS, Adobe, Salesforce\nDeploy solutions in productions while adhering to best practices involving Model Explainability, MLOps, Feature Stores, Model Management, Responsible AI etc\nCollaboration and Ownership (10%)\nSharing of knowledge, best practices with the team including coaching or training in some of deep learning/machine learning methodologies. Provides mentoring, coaching, and consulting advice and guidance to staff, e.g. analytic methodologies, data recommendations\nTakes complete independent ownership of the projects and the initiatives in the team with the minimal support\nExperience and Qualifications Required\nQualifications:\nEngineer from IIT/Master s in field related to Data Science/Economics/Mathematics (Tie1 Institutions like ISI, Delhi School of Economics)/M.B.A from tier 1 institutions\nMust have Skills & Experience Required:\nOverall, 8+ years of experience in Data Science and Analytics\n5+ years of hands-on experience in - Statistical Modelling /Machine Learning Techniques/Natural Language Processing/Deep Learning\n5+ years of experience in Python/Machine Learning/Deep Learning\nExcellent problem-solving skills\nShould be able to run analytics applications such as Python, SAS and interpret statistical results\nImplementation of models with clear measurable outcomes\nGood to have Skills & Experience Required:\nAbility to engage in discussion with senior stakeholders on defining business problems, designing analyses projects, and articulating analytical insights to stakeholders.\nExperience on SPARK/Hadoop/Big Data Platforms is a plus\nExperience with unstructured data and big data\nExperience with secondary data and knowledge of primary market research is a plus.\nAbility to independently own and manage the projects with minimal support.\nExcellent analytical skills and a strong sense for structure and logic\nAbility to develop, test and validate hypotheses.\nFeel rewarded",Industry Type: Financial Services,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['SAS', 'Senior Analyst', 'Consulting', 'Machine learning', 'Business planning', 'Competitive intelligence', 'Customer service', 'Adobe', 'Stakeholder management', 'Salesforce']",2025-06-12 13:52:29
Full Stack Data Scientist,Vimo Getinsured,2 - 7 years,Not Disclosed,['Gurugram( Sector 61 Gurgaon )'],"About the Role\nAs a Data Science Engineer, you will need strong technical skills in data modeling, machine learning, data engineering, and software development. You will have the ability to conduct literature reviews and critically evaluate research papers to identify applicable techniques. Additionally, you should be able to design and implement efficient and scalable data processing pipelines, perform exploratory data analysis, and collaborate with other teams to integrate data science models into production systems. Passion for conversational AI and a desire to solve some of the most complex problems in the Natural Language Processing space are essential. You will work on highly scalable, stable, and automated deployments, aiming for high performance. Taking on the challenge of building and scaling a truly remarkable AI platform to impact the lives of millions of customers will be part of your responsibilities. Working in a challenging yet enjoyable environment, where learning new things is the norm, you should think of solutions beyond boundaries. You should also drive outcomes with full ownership, deeply believe in customer obsession, and thrive in a fast-paced environment of learning and innovation.\nYou will work in a challenging, consumer-facing problem space, where you can make an immediate impact. You will get to work with the latest technologies, learn to use new tools and get the opportunity to have your say in the final product. Youll work alongside a great team in an open, collaborative environment. We are part of Vimo, a well-funded, stable mid-size company with excellent salaries, medical/dental/vision coverage, and perks. Vimo is an Equal Opportunity Employer.",,,,"['python', 'Langchain', 'Neural Networks', 'LLM', 'Linux', 'Data Structures', 'Natural Language Processing', 'Jupyter Notebook', 'Machine Learning', 'Deep Learning', 'Numpy', 'Data Science', 'pandas', 'Nltk', 'Langgraph', 'Transformers', 'BERT', 'langsmith']",2025-06-12 13:52:32
Devops AWS DATA Engineeer|| Technical Analyst || 12Lakhs CTC,Robotics Technologies,7 - 9 years,11-12 Lacs P.A.,['Hyderabad( Banjara hills )'],"We are seeking a highly skilled Devops Engineer to join our dynamic development team. In this role, you will be responsible for designing, developing, and maintaining both frontend and backend components of our applications using Devops and associated technologies.\nYou will collaborate with cross-functional teams to deliver robust, scalable, and high-performing software solutions that meet our business needs. The ideal candidate will have a strong background in devops, experience with modern frontend frameworks, and a passion for full-stack development.\n\nRequirements:\nBachelor's degree in Computer Science Engineering, or a related field.\n7 to 9+ years of experience in full-stack development, with a strong focus on DevOps.\n\nDevOps with AWS Data Engineer - Roles & Responsibilities:\nUse AWS services like EC2, VPC, S3, IAM, RDS, and Route 53.\nAutomate infrastructure using Infrastructure as Code (IaC) tools like Terraform or AWS CloudFormation.\nBuild and maintain CI/CD pipelines using tools AWS CodePipeline, Jenkins,GitLab CI/CD.\nCross-Functional Collaboration\nAutomate build, test, and deployment processes for Java applications.\nUse Ansible, Chef, or AWS Systems Manager for managing configurations across environments.\nContainerize Java apps using Docker.\nDeploy and manage containers using Amazon ECS, EKS (Kubernetes), or Fargate.\nMonitoring & Logging using Amazon CloudWatch,Prometheus + Grafana,E\nStack (Elasticsearch, Logstash, Kibana),AWS X-Ray for distributed tracing manage access with IAM roles/policies.\nUse AWS Secrets Manager / Parameter Store for managing credentials.\nEnforce security best practices, encryption, and audits.\nAutomate backups for databases and services using AWS Backup, RDS Snapshots, and S3 lifecycle rules.\nImplement Disaster Recovery (DR) strategies.\nWork closely with development teams to integrate DevOps practices.\nDocument pipelines, architecture, and troubleshooting runbooks.\nMonitor and optimize AWS resource usage.\nUse AWS Cost Explorer, Budgets, and Savings Plans.\n\nMust-Have Skills:\nExperience working on Linux-based infrastructure.\nExcellent understanding of Ruby, Python, Perl, and Java.\nConfiguration and managing databases such as MySQL, Mongo.\nExcellent troubleshooting.\nSelecting and deploying appropriate CI/CD tools\nWorking knowledge of various tools, open-source technologies, and cloud services.\nAwareness of critical concepts in DevOps and Agile principles.\nManaging stakeholders and external interfaces.\nSetting up tools and required infrastructure.\nDefining and setting development, testing, release, update, and support processes for DevOps operation.\nHave the technical skills to review, verify, and validate the software code developed in the project.\nInterview Mode : F2F for who are residing in Hyderabad / Zoom for other states\nLocation : 43/A, MLA Colony,Road no 12, Banjara Hills, 500034\nTime : 2 - 4pm",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Iac', 'Devops', 'Jenkins', 'AWS', 'Kubernetes', 'RDS', 'Aws Cloudformation', 'Amazon Cloudwatch', 'Prometheus', 'Ci/Cd', 'Grafana', 'DR', 'Cloud Trail', 'Docker', 'IAM', 'Ansible / Chef', 'fargate', 'Gitlab', 'Monitoring', 'Python']",2025-06-12 13:52:34
"Director, Enterprise Data Architecture",Horizon Therapeutics,10 - 12 years,Not Disclosed,['Hyderabad'],"Career Category Engineering Job Description\nABOUT AMGEN\nAmgen harnesses the best of biology and technology to fight the world s toughest diseases, and make people s lives easier, fuller and longer. We discover, develop, manufacture and deliver innovative medicines to help millions of patients. Amgen helped establish the biotechnology industry more than 40 years ago and remains on the cutting-edge of innovation, using technology and human genetic data to push beyond what s known today.\nABOUT THE ROLE\nRole Description:\nThe Director for Data Architecture and Solutions will lead Amgen s enterprise data architecture and solutions strategy, overseeing the design, integration, and deployment of scalable, secure, and future-ready data systems. This leader will define the architectural vision and guide a high-performing team of architects and technical experts to implement data and analytics solutions that drive business value and innovation.\nThis role demands a strong blend of business acumen, deep technical expertise, and strategic thinking to align data capabilities with the companys mission and growth. The Director will also serve as a key liaison with executive leadership, influencing technology investment and enterprise data direction\n.\nRoles & Responsibilities:\nDevelop and own the enterprise data architecture and solutions roadmap, aligned with Amgen s business strategy and digital transformation goals.\nProvide executive leadership and oversight of data architecture initiatives across business domains (R&D, Commercial, Manufacturing, etc.).\nLead and grow a high-impact team of data and solution architects. Coach, mentor, and foster innovation and continuous improvement in the team.\nDesign and promote modern data architectures (data mesh, data fabric, lakehouse etc.) across hybrid cloud environments and enable for AI readiness.\nCollaborate with stakeholders to define solution blueprints, integrating business requirements with technical strategy to drive value.\nDrive enterprise-wide adoption of data modeling, metadata management, and data lineage standards.\nEnsure solutions meet enterprise-grade requirements for security, performance, scalability, compliance, and data governance.\nPartner closely with Data Engineering, Analytics, AI/ML, and IT Security teams to operationalize data solutions that enable advanced analytics and decision-making.\nChampion innovation and continuous evolution of Amgen s data and analytics landscape through new technologies and industry best practices.\nCommunicate architectural strategy and project outcomes to executive leadership and other non-technical stakeholders.\nFunctional Skills:\nMust-Have Skills:\n10+ years of experience in data architecture or solution architecture leadership roles, including experience at the enterprise level.\nProven experience leading architecture strategy and delivery in the life sciences or pharmaceutical industry.\nExpertise in cloud platforms (AWS, Azure, or GCP) and modern data technologies (data lakes, APIs, ETL/ELT frameworks).\nStrong understanding of data governance, compliance (e.g., HIPAA, GxP), and data privacy best practices.\nDemonstrated success managing cross-functional, global teams and large-scale data programs.\nExperience with enterprise architecture frameworks (TOGAF, Zachman, etc.).\nProven leadership skills with a track record of managing and mentoring high-performing data architecture teams.\nGood-to-Have Skills:\nMaster s or doctorate in Computer Science, Engineering, or related field.\nCertifications in cloud architecture (AWS, GCP, Azure).\nExperience integrating AI/ML solutions into enterprise Data Achitecture.\nFamiliarity with DevOps, CI/CD pipelines, and Infrastructure as Code (Terraform, CloudFormation).\nScaled Agile or similar methodology experience.\nLeadership and Communication Skills:\nStrategic thinker with the ability to influence at the executive level.\nStrong executive presence with excellent communication and storytelling skills.\nAbility to lead in a matrixed, global environment with multiple stakeholders.\nHighly collaborative, proactive, and business-oriented mindset.\nStrong organizational and prioritization skills to manage complex initiatives.\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nBasic Qualifications:\nDoctorate degree and 2 years of Information Systems experience, or\nMaster s degree and 6 years of Information Systems experience, or\nBachelor s degree and 8 years of Information Systems experience, or\nAssociates degree and 10 years of Information Systems experience, or\n4 years of managerial experience directly managing people and leadership experience leading teams, projects, or programs.\n.",Industry Type: Biotechnology,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Solution architecture', 'metadata', 'Data modeling', 'Enterprise architecture', 'TOGAF', 'HIPAA', 'Agile', 'Analytics', 'Data architecture']",2025-06-12 13:52:36
Devops AWS DATA Engineeer|| Technical Analyst || 12Lakhs CTC,Robotics Technologies,8 - 9 years,11-12 Lacs P.A.,['Hyderabad( Banjara hills )'],"We are seeking a highly skilled Devops Engineer to join our dynamic development team. In this role, you will be responsible for designing, developing, and maintaining both frontend and backend components of our applications using Devops and associated technologies.\nYou will collaborate with cross-functional teams to deliver robust, scalable, and high-performing software solutions that meet our business needs. The ideal candidate will have a strong background in devops, experience with modern frontend frameworks, and a passion for full-stack development.\n\nRequirements:\nBachelor's degree in Computer Science Engineering, or a related field.\n8 to 9+ years of experience in full-stack development, with a strong focus on DevOps.\n\nDevOps with AWS Data Engineer - Roles & Responsibilities:\nUse AWS services like EC2, VPC, S3, IAM, RDS, and Route 53.\nAutomate infrastructure using Infrastructure as Code (IaC) tools like Terraform or AWS CloudFormation.\nBuild and maintain CI/CD pipelines using tools AWS CodePipeline, Jenkins,GitLab CI/CD.\nCross-Functional Collaboration\nAutomate build, test, and deployment processes for Java applications.\nUse Ansible, Chef, or AWS Systems Manager for managing configurations across environments.\nContainerize Java apps using Docker.\nDeploy and manage containers using Amazon ECS, EKS (Kubernetes), or Fargate.\nMonitoring & Logging using Amazon CloudWatch,Prometheus + Grafana,E\nStack (Elasticsearch, Logstash, Kibana),AWS X-Ray for distributed tracing manage access with IAM roles/policies.\nUse AWS Secrets Manager / Parameter Store for managing credentials.\nEnforce security best practices, encryption, and audits.\nAutomate backups for databases and services using AWS Backup, RDS Snapshots, and S3 lifecycle rules.\nImplement Disaster Recovery (DR) strategies.\nWork closely with development teams to integrate DevOps practices.\nDocument pipelines, architecture, and troubleshooting runbooks.\nMonitor and optimize AWS resource usage.\nUse AWS Cost Explorer, Budgets, and Savings Plans.\n\nMust-Have Skills:\nExperience working on Linux-based infrastructure.\nExcellent understanding of Ruby, Python, Perl, and Java.\nConfiguration and managing databases such as MySQL, Mongo.\nExcellent troubleshooting.\nSelecting and deploying appropriate CI/CD tools\nWorking knowledge of various tools, open-source technologies, and cloud services.\nAwareness of critical concepts in DevOps and Agile principles.\nManaging stakeholders and external interfaces.\nSetting up tools and required infrastructure.\nDefining and setting development, testing, release, update, and support processes for DevOps operation.\nHave the technical skills to review, verify, and validate the software code developed in the project.\nInterview Mode : F2F for who are residing in Hyderabad / Zoom for other states\nLocation : 43/A, MLA Colony,Road no 12, Banjara Hills, 500034\nTime : 2 - 4pm",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Iac', 'Devops', 'Jenkins', 'AWS', 'Kubernetes', 'RDS', 'Aws Cloudformation', 'Amazon Cloudwatch', 'Prometheus', 'Ci/Cd', 'Grafana', 'DR', 'Cloud Trail', 'Docker', 'IAM', 'Ansible / Chef', 'fargate', 'Gitlab', 'Monitoring', 'Python']",2025-06-12 13:52:39
"Staff Engineer, Nodejs",Nagarro,7 - 10 years,Not Disclosed,['India'],"We're Nagarro.\n\nWe are a Digital Product Engineering company that is scaling in a big way! We build products, services, and experiences that inspire, excite, and delight. We work at scale across all devices and digital mediums, and our people exist everywhere in the world (18000+ experts across 38 countries, to be exact). Our work culture is dynamic and non-hierarchical. We're looking for great new colleagues. That's where you come in!\n\nREQUIREMENTS:\nTotal Experience 7+ years.\nExcellent knowledge developing scalable and highly available Restful APIs using NodeJS technologies.\nThorough understanding of React.js and its core principles and experience with popular React.js workflows (such as Flux or Redux or Context API or Data Structures).\nFamiliarity with common programming tools such as RESTful APIs, TypeScript, version control software, and remote deployment tools, CI/CD tools.\nUnderstanding of linter libraries (TSLINT, Prettier etc) and Unit testing using Jest, Enzyme, Jasmine or equivalent framework.\nStrong proficiency in JavaScript, including DOM manipulation and the JavaScript object model. Proficient with the latest versions of ECMAScript (JavaScript or TypeScript).\nUnderstanding of containerization, experienced in Dockers, Kubernetes.\nExposed to API gateway integrations like 3Scale.\nUnderstanding of Single-Sign-on or token-based authentication (Rest, JWT, OAuth).\nPossess expert knowledge of task/message queues include but not limited to: AWS, Microsoft Azure, Pushpin. and Kafka.\nPractical experience with GraphQL is good to have.\nWriting tested, idiomatic, and documented JavaScript, HTML and CSS.\nExperiencing in Developing responsive web-based UI.\nHave experience on Styled Components, Tailwind CSS, Material UI and other CSS-in-JS techniques.\nProblem-solving mindset with the ability to tackle complex data engineering challenges.\nStrong communication and teamwork skills, with the ability to mentor and collaborate effectively.\nRESPONSIBILITIES:\nWriting and reviewing great quality code\nUnderstanding the clients business use cases and technical requirements and be able to convert them into technical design which elegantly meets the requirements\nMapping decisions with requirements and be able to translate the same to developers.\nIdentifying different solutions and being able to narrow down the best option that meets the clients requirements.\nDefining guidelines and benchmarks for NFR considerations during project implementation\nWriting and reviewing design document explaining overall architecture, framework, and high-level design of the application for the developers.\nReviewing architecture and design on various aspects like extensibility, scalability, security, design patterns, user experience, NFRs, etc., and ensure that all relevant best practices are followed.\nDeveloping and designing the overall solution for defined functional and non-functional requirements; and defining technologies, patterns, and frameworks to materialize it\nUnderstanding and relating technology integration scenarios and applying these learnings in projects\nResolving issues that are raised during code/review, through exhaustive systematic analysis of the root cause, and being able to justify the decision taken.\nCarrying out POCs to make sure that suggested design/technologies meet the requirements",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure', 'Typescript', 'Node.Js', 'Docker', 'Microservices', 'Kubernetes']",2025-06-12 13:52:41
Azure Data Architect,Syren Technologies,10 - 18 years,Not Disclosed,[],"About Syren Cloud\n\nSyren Cloud Technologies is a cutting-edge company specializing in supply chain solutions and data engineering. Their intelligent insights, powered by technologies like AI and NLP, empower organizations with real-time visibility and proactive decision-making. From control towers to agile inventory management, Syren unlocks unparalleled success in supply chain management.\n\nRole Summary",,,,"['Pyspark', 'Azure', 'Architecture', 'Data Bricks']",2025-06-12 13:52:43
Data Architect,Calibo,12 - 16 years,Not Disclosed,[],"About the Role:\n\nWe are looking for a highly skilled Data Engineering Architect with strong Data Engineering pipeline implementation experience to serve as the lead Solution/Technical Architect and Subject Matter Expert for customer experience data solutions across multiple data sources. The ideal candidate will collaborate with the Enterprise Architect and the client IT team to establish and implement strategic initiatives.\n\nResponsibilities and Technical Skills:\n12+ years of relevant experience in designing and Architecting ETL, ELT, Reverse ETL, Data Management or Data Integration, Data Warehouse, Data Lake, and Data Migration.\nMust have expertise in building complex ETL pipelines and large Data Processing, Data Quality and Data security\nExperience in delivering quality work on time with multiple, competing priorities.\nExcellent troubleshooting and problem-solving skills must be able to consistently identify critical elements, variables and alternatives to develop solutions.\nExperience in identifying, analyzing and translating business requirements into conceptual, logical and physical data models in complex, multi-application environments.\nExperience with Agile and Scaled Agile Frameworks.\nExperience in identifying and documenting data integration issues, and challenges such as duplicate data, non-conformed data, and unclean data. Multiple platform development experience.\nStrong experience in performance tuning of ETL processes using Data Platforms\nMust have experience in handling Data formats like Delta Tables, Parquet files, Iceberg etc.\nExperience in Cloud technologies such as AWS/Azure or Google Cloud.\nApache Spark design and development experience using Scala, Java, Python or Data Frames with Resilient Distributed Datasets (RDDs).\nDevelopment experience in databases like Oracle, AWS Redshift, AWS RDS, Postgres Databricks and/or Snowflake.\nHands-on professional work experience with Python is highly desired.\nExperience in Hadoop ecosystem tools for real-time or batch data ingestion.\nStrong communication and teamwork skills to interface with development team members, business analysts, and project management. Excellent analytical skills.\nIdentification of data sources, internal and external, and defining a plan for data management as per business data strategy.\nCollaborating with cross-functional teams for the smooth functioning of the enterprise data system.\nManaging end-to-end data architecture, from selecting the platform, designing the technical architecture, and developing the application to finally testing and implementing the proposed solution.\nPlanning and execution of big data solutions using Databricks, Big Data, Hadoop, Big Query, Snowflake, MongoDB, DynamoDB, PostgreSQL and SQL Server\nHands-on experience in defining and implementing various Machine Learning models for different business needs.\nIntegrating technical functionality, ensuring data accessibility, accuracy, and security.\nProgramming / Scripting Languages like Python / Java / Go, Microservices\nMachine Learning / AI tools like Scikit-learn / TensorFlow / PyTorch",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['cloud', 'ETL', 'AWS', 'Data Handling', 'Spark']",2025-06-12 13:52:45
Data Analytics Mgr,Amgen Inc,4 - 6 years,Not Disclosed,['Hyderabad'],"What you will do\n\n\nIn this vital role you will report to the Organizational Planning Analytics & Insights Procurement & Sourcing Lead, you will support Amgens Tech & Workforce Strategy by applying business analytics and change leadership skills to drive insights that impact resource allocation and sourcing strategy.\n\nYour responsibilities include dashboard development, ad-hoc reporting, business partnering & engagement, and financial baselining. This role supports organizational change and enables the development of an integrated approach to global sourcing and financial planning.\n\nReporting to the Organizational Planning Analytics & Insights Procurement & Sourcing Lead, you will support Amgens Tech & Workforce Strategy by applying business analytics and change management skills to drive insights that impact resource allocation and sourcing strategy.\n\nYour responsibilities include dashboard development, ad-hoc reporting, business partnering & engagement, and financial baselining. This role supports change management and enables the development of an integrated approach to global sourcing and financial planning.\n\nRoles & Responsibilities:\nAddressing business challenges through process evaluation and insight generation.\nDevelop insights with a strong focus on Tableau and Power BI dashboard creation, as well as PowerPoint presentations.\nGuide data analysts and data engineers on standard methodologies for building data pipelines to support dashboards and other business objectives.\nConduct ad hoc analyses of FP&A and sourcing/procurement data.\nAddressing business challenges through process evaluation and insight generation.\nDevelop insights with a strong focus on Tableau and Power BI dashboard creation, as well as PowerPoint presentations.\nGuide data analysts and data engineers on standard methodologies for building data pipelines to support dashboards and other business objectives.\nConduct ad hoc analyses of FP&A and sourcing/procurement data.\nWhat we expect of you\n\nWe are all different, yet we all use our unique contributions to serve patients.\n\nBasic Qualifications:\nMasters degree and 4 to 6 years of applicable experience in business analysis (finance analysis, data analysis, sourcing analysis, or similar experience OR\nBachelors degree and 6 to 8 years of applicable experience in business analysis (finance analysis, data analysis, sourcing analysis, or similar experience OR\nDiploma and 10 to 12 years of applicable experience in business analysis (finance analysis, data analysis, sourcing analysis, or similar experience\nPreferred Qualifications:\nMasters degree in data science, business, statistics, data mining, applied mathematics, business analytics, engineering, computer science, or a related field\n4 years of relevant experience in data science, data analytics, consulting, and/or financial planning & analysis.\nA keen eye for design, with the ability to craft engaging PowerPoint decks and develop compelling Power BI and Tableau dashboards.\nProven expertise in statistical/mathematical modeling and working with structured/unstructured data.\nExperience with procurement, sourcing, and/or financial planning data.\nSkilled in automating data workflows using tools like Tableau, Python, R, Alteryx, and PowerApps.\nKnowledge of global finance systems, Procurement, and sourcing operations.\nExperience with data analysis, budgeting, forecasting, and strategic planning in the Bio-Pharmaceutical or biotech industry.\nGrowing in a start-up environment, building a data-driven transformation capability.\nUnderstanding of the Bio-Pharmaceutical and biotech industry trends and operations.\nProven ability to engage with cross-functional business leaders to align data strategies with corporate objectives, redefining complex data insights into actionable strategies.\nFlexible work models, including remote work arrangements, where possible\n\nAs we work to develop treatments that deal with others, we also work to care for your professional and personal growth and well-being. From our competitive benefits to our collaborative culture, well support your journey every step of the way.",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Analysis', 'data analytics', 'data science', 'mathematical modeling', 'financial planning', 'financial planning and analysis', 'statistics']",2025-06-12 13:52:47
MDM Data Scientist,Amgen Inc,3 - 8 years,Not Disclosed,['Hyderabad'],"ABOUT THE ROLE\nRole Description:\nWe are seeking an accomplished and visionary Data Scientist/ GenAIdeveloper to join Amgens Enterprise Data Management team. As part of MDM team, you will be responsible for designing, developing, and deploying Generative AI and ML models to power data-driven decisions across business domains. This role is ideal for an AI practitioner who thrives in a collaborative environment and brings a strategic mindset to applying advanced AI techniques to solve real-world problems.To succeed in this role, the candidate must have strong AI/ML, Data Science, GenAI experience along with MDM knowledge, hence the candidates having only MDM experience are not eligible for this role. Candidate must have AI/ML, data science and GenAI experience on technologies like (PySpark/PyTorch, TensorFlow, LLM, Autogen, Hugging FaceVectorDB,Embeddings, RAGsetc), along with knowledge of MDM (Master Data Management)\nRoles & Responsibilities:\nDevelop enterprise-level GenAI applications using LLM frameworks such as Langchain, Autogen, and Hugging Face.\nDesign and develop intelligent pipelines using PySpark, TensorFlow, and PyTorch within Databricks and AWS environments.\nImplement embedding models andmanage VectorStores for retrieval-augmented generation (RAG) solutions.\nIntegrate and leverage MDM platforms like Informatica and Reltio to supply high-quality structured data to ML systems.\nUtilize SQL and Python for data engineering, data wrangling, and pipeline automation.\nBuild scalable APIs and services to serve GenAI models in production.\nLead cross-functional collaboration with data scientists, engineers, and product teams to scope, design, and deploy AI-powered systems.\nEnsure model governance, version control, and auditability aligned with regulatory and compliance expectations.\nBasic Qualifications and Experience:\nMasters degree with 4 - 6 years of experience in Business, Engineering, IT or related field OR\nBachelors degree with 6 - 9 years of experience in Business, Engineering, IT or related field OR\nDiploma with 10 - 12 years of experience in Business, Engineering, IT or related field\nFunctional Skills:\nMust-Have Skills:\n6+ years of experience working in AI/ML or Data Science roles, including designing and implementing GenAI solutions.\nExtensive hands-on experience with LLM frameworks and tools such as Langchain, Autogen, Hugging Face, OpenAI APIs, and embedding models.\nStrong programming background with Python, PySpark, and experience in building scalable solutions using TensorFlow, PyTorch, and SK-Learn.\nProven track record of building and deploying AI/ML applications in cloud environments such as AWS.\nExpertise in developing APIs, automation pipelines, and serving GenAI models using frameworks like Django, FastAPI, and DataBricks.\nSolid experience integrating and managing MDM tools (Informatica/Reltio) and applying data governance best practices.\nGuide the team on development activities and lead the solution discussions\nMust have core technical capabilities in GenAI, Data Science space\nGood-to-Have Skills:\nPrior experience in Data Modeling, ETL development, and data profiling to support AI/ML workflows.\nWorking knowledge of Life Sciences or Pharma industry standards and regulatory considerations.\nProficiency in tools like JIRA and Confluence for Agile delivery and project collaboration.\nFamiliarity with MongoDB, VectorStores, and modern architecture principles for scalable GenAI applications.\nProfessional Certifications:\nAny ETL certification (e.g. Informatica)\nAny Data Analysis certification (SQL)\nAny cloud certification (AWS or AZURE)\nData Science and ML Certification\nSoft Skills:\nStrong analytical abilities to assess and improve master data processes and solutions.\nExcellent verbal and written communication skills, with the ability to convey complex data concepts clearly to technical and non-technical stakeholders.\nEffective problem-solving skills to address data-related issues and implement scalable solutions.\nAbility to work effectively with global, virtual teams\nWe will ensure that individuals with disabilities are provided with reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation.",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['MDM', 'GenAI', 'Langchain', 'PySpark', 'VectorStores', 'Hugging Face', 'LLM', 'Data Science', 'DataBricks', 'SK-Learn', 'AI/ML', 'Autogen', 'PyTorch', 'Django', 'OpenAI APIs', 'FastAPI', 'MongoDB', 'Data Modeling', 'PySpark/PyTorch', 'TensorFlow', 'Python']",2025-06-12 13:52:49
Principal - Data Architect,Affine Analytics,8 - 12 years,Not Disclosed,['Chennai'],"We are seeking a highly skilled Data Architect to design and implement robust, scalable, and secure data solutions on AWS Cloud. The ideal candidate should have expertise in AWS services, data modeling, ETL processes, and big data technologies, with hands-on experience in Glue, DMS, Python, PySpark, and MPP databases like Snowflake, Redshift, or Databricks.\nKey Responsibilities:\nArchitect and implement data solutions leveraging AWS services such as EC2, S3, IAM, Glue (Mandatory), and DMS for efficient data processing and storage.",,,,"['Python', 'S3', 'AWS Glue', 'DMS', 'SQL Server', 'Redshift', 'Glue', 'IAM', 'EC2', 'Snowflake', 'Databricks', 'Oracle', 'Lambda']",2025-06-12 13:52:51
MDM Data Scientist,Amgen Inc,4 - 9 years,Not Disclosed,['Hyderabad'],"ABOUT THE ROLE\nRole Description:\nWe are seeking an accomplished and visionary Data Scientist/ GenAIdeveloper to join Amgens Enterprise Data Management team. As part of MDM team, you will be responsible for designing, developing, and deploying Generative AI and ML models to power data-driven decisions across business domains. This role is ideal for an AI practitioner who thrives in a collaborative environment and brings a strategic mindset to applying advanced AI techniques to solve real-world problems.\nTo succeed in this role, the candidate must have strong AI/ML, Data Science, GenAI experience along with MDM knowledge, hence the candidates having only MDM experience are not eligible for this role. Candidate must have AI/ML, data science and GenAI experience on technologies like (PySpark/PyTorch, TensorFlow, LLM, Autogen, Hugging FaceVectorDB,Embeddings, RAGsetc), along with knowledge of MDM (Master Data Management)\nRoles & Responsibilities:\nDevelop enterprise-level GenAI applications using LLM frameworks such as Langchain, Autogen, and Hugging Face.\nDesign and develop intelligent pipelines using PySpark, TensorFlow, and PyTorch within Databricks and AWS environments.\nImplement embedding models andmanage VectorStores for retrieval-augmented generation (RAG) solutions.\nIntegrate and leverage MDM platforms like Informatica and Reltio to supply high-quality structured data to ML systems.\nUtilize SQL and Python for data engineering, data wrangling, and pipeline automation.\nBuild scalable APIs and services to serve GenAI models in production.\nLead cross-functional collaboration with data scientists, engineers, and product teams to scope, design, and deploy AI-powered systems.\nEnsure model governance, version control, and auditability aligned with regulatory and compliance expectations.\nBasic Qualifications and Experience:\nMasters degree with 4 - 6 years of experience in Business, Engineering, IT or related field OR\nBachelors degree with 6 - 9 years of experience in Business, Engineering, IT or related field OR\nDiploma with 10 - 12 years of experience in Business, Engineering, IT or related field\nFunctional Skills:\nMust-Have Skills:\n6+ years of experience working in AI/ML or Data Science roles, including designing and implementing GenAI solutions.\nExtensive hands-on experience with LLM frameworks and tools such as Langchain, Autogen, Hugging Face, OpenAI APIs, and embedding models.\nStrong programming background with Python, PySpark, and experience in building scalable solutions using TensorFlow, PyTorch, and SK-Learn.\nProven track record of building and deploying AI/ML applications in cloud environments such as AWS.\nExpertise in developing APIs, automation pipelines, and serving GenAI models using frameworks like Django, FastAPI, and DataBricks.\nSolid experience integrating and managing MDM tools (Informatica/Reltio) and applying data governance best practices.\nGuide the team on development activities and lead the solution discussions\nMust have core technical capabilities in GenAI, Data Science space\nGood-to-Have Skills:\nPrior experience in Data Modeling, ETL development, and data profiling to support AI/ML workflows.\nWorking knowledge of Life Sciences or Pharma industry standards and regulatory considerations.\nProficiency in tools like JIRA and Confluence for Agile delivery and project collaboration.\nFamiliarity with MongoDB, VectorStores, and modern architecture principles for scalable GenAI applications.\nProfessional Certifications:\nAny ETL certification (e.g. Informatica)\nAny Data Analysis certification (SQL)\nAny cloud certification (AWS or AZURE)\nData Science and ML Certification\nSoft Skills:\nStrong analytical abilities to assess and improve master data processes and solutions.\nExcellent verbal and written communication skills, with the ability to convey complex data concepts clearly to technical and non-technical stakeholders.\nEffective problem-solving skills to address data-related issues and implement scalable solutions.\nAbility to work effectively with global, virtual teams.",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'GenAI', 'Langchain', 'PySpark', 'Hugging Face', 'OpenAI API', 'Autogen', 'PyTorch', 'Django', 'MDM', 'FastAPI', 'Data Modeling', 'ETL', 'TensorFlow', 'Python']",2025-06-12 13:52:54
Job opening For Data Warehouse + ADF + ETL,bct,3 - 6 years,Not Disclosed,['Pune'],"Greetings of the Day !!!\n\nWe have job opening for Data Warehouse + ADF + ETL with one of our Client .If you are interested for this role , kindly share update resume along with below details in this email id : shaswati.m@bct-consulting.com\n\nJob Description:\nSenior Data Engineer\nAs a Senior Data Engineer, you will support the European World Area using the Windows & Azure suite of Analytics & Data platforms. The focus of the role is on the technical aspects and implementation of data gathering, integration and database design.\nWe look forward to seeing your application!\nIn This Role, Your Responsibilities Will Be:\nData Ingestion and Integration: Collaborate with Product Owners and analysts to understand data requirements & design, develop, and maintain data pipelines for ingesting, transforming, and integrating data from various sources into Azure Data Services.\nMigration of existing ETL packages: Migrate existing SSIS packages to Synapse pipelines\nData Modelling: Assist in designing and implementing data models, data warehouses, and databases in Azure Synapse Analytics, Azure Data Lake Storage, and other Azure services.\nData Transformation: Develop ETL (Extract, Transform, Load) processes using SQL Server Integration Services (SSIS), Azure Synapse Pipelines, or other relevant tools to prepare data for analysis and reporting.\nData Quality and Governance: Implement data quality checks and data governance practices to ensure the accuracy, consistency, and security of data assets.\nMonitoring and Optimization: Monitor and optimize data pipelines and workflows for performance, scalability, and cost efficiency.\nDocumentation: Maintain comprehensive documentation of processes, including data lineage, data dictionaries, and pipeline schedules.\nCollaboration: Work closely with cross-functional teams, including data analysts, data scientists, and business stakeholders, to understand their data needs and deliver solutions accordingly.\nAzure Services: Stay updated on Azure data services and best practices to recommend and implement improvements in our data architecture and processes\nFor This Role, You Will Need:\n3-5 years of experience in Data Warehousing with On-Premises or Cloud technologies\nStrong practical experience of Synapse pipelines / ADF.\nStrong practical experience of developing ETL packages using SSIS.\nStrong practical experience with T-SQL or any variant from other RDBMS.\nGraduate degree educated in computer science or a relevant subject.\nStrong analytical and problem-solving skills.\nStrong communication skills in dealing with internal customers from a range of functional areas.\nWillingness to work flexible working hours according to project requirements.\nTechnical documentation skills.\nFluent in English.\nPreferred Qualifications that Set You Apart:\nOracle PL/SQL.\nExperience in working on Azure Services like Azure Synapse Analytics, Azure Data Lake.\nWorking experience with Azure DevOps paired with knowledge of Agile and/or Scrum methods of delivery.\nLanguages: French, Italian, or Spanish would be an advantage.\nAgile certification.\nThanks,\nShaswati",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['ADF', 'ETL', 'SSIS', 'Data ware house']",2025-06-12 13:52:56
"Advisor, Software Development Engineering",Fiserv,10 - 15 years,Not Disclosed,['Pune'],"Dashboard Development & Management : Design and maintain advanced Splunk dashboards to deliver comprehensive insights into system performance and File Transmission component health.\nPerformance Optimization : Improve dashboard efficiency when handling large datasets using techniques such as optimized queries, summary indexing, and data models.\nAdvanced Regex Utilization : Apply sophisticated regular expressions to create accurate search queries and extract meaningful data.\nCustom Alert Configuration : Implement highly customized alerting mechanisms to detect anomalies, manage alert actions, throttle conditions, and integrate with lookup tables and dynamic time-based arguments.\nFile Transmission Monitoring : Track and report on each stage of file transmission, continuously refining monitoring strategies for enhanced reliability and visibility.\nCross-Functional Collaboration : Work closely with various teams to integrate Splunk monitoring with broader IT systems and workflows.\nConduct discovery of file transmission workflows, including file life cycle, endpoint configurations, log analysis, SLA definitions, and exception scenarios.\nDevelop and deploy advanced Splunk queries to ensure end-to-end visibility into file transmission processes.\nConfigure and optimize alerting mechanisms for timely detection and resolution of issues.\nDesign and implement IT Service Intelligence (ITSI) strategies to enhance monitoring capabilities and deliver actionable insights.\nEstablish and manage monitoring frameworks based on the file life cycle to ensure traceability and accountability.\nCollaborate with IT and operations teams to integrate Splunk with other tools and resolve data ingestion issues.\nAnalyze monitoring data to identify trends, detect anomalies, and recommend improvements.\nServe as a Splunk subject matter expert, providing guidance, best practices, and training to team members.\nWhat You Will Need to Have\nEducation : bachelors and/or masters degree in Information Technology, Computer Science, or a related field.\nExperience : Minimum of 10 years in IT, with a focus on Splunk, SFTP tools, data integration, or technical support roles.\nSplunk Expertise : Proficiency in advanced SPL techniques including subsearches, joins, and statistical functions.\nRegex Proficiency : Strong command of regular expressions for search and data extraction.\nDatabase Skills : Experience with relational databases and writing complex SQL queries with advanced joins.\nFile Transmission Tools : Hands-on experience with platforms like Sterling File Gateway, IBM Sterling, or other MFT solutions.\nAnalytical Thinking : Proven problem-solving skills and the ability to troubleshoot technical issues effectively.\nCommunication : Strong verbal and written communication skills for collaboration with internal and external stakeholders.\nAttention to Detail : High level of accuracy to ensure data integrity and reliability.\nWhat Would Be Great to Have\nScripting & Automation : Proficiency in Python or similar scripting languages to automate monitoring tasks.\nTool Experience : Familiarity with tools such as Dynatrace, Sterling File Gateway, and other MFT solutions.\nLinux Proficiency : Strong working knowledge of Linux and command-line operations.\nSecure File Transfer Protocols : Hands-on experience with SFTP and tools like SFG, NDM, and MFT using SSH encryption.\nTask Scheduling Tools : Experience with job scheduling platforms such as AutoSys, Control-M, or cron.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Automation', 'Linux', 'Analytical', 'Scheduling', 'data integrity', 'Information technology', 'Technical support', 'Monitoring', 'Data extraction']",2025-06-12 13:52:58
Azure Data Factory,Swift Staffing,8 - 12 years,6.5-14 Lacs P.A.,"['Mumbai', 'Hyderabad', 'Pune']","Job Description:\n5+ years in data engineering with at least 2 years on Azure Synapse.\nStrong SQL, Spark, and Data Lake integration experience.\nFamiliarity with Azure Data Factory, Power BI, and DevOps pipelines.\nExperience in AMS or managed services environments is a plus.\nDetailed JD\nDesign, develop, and maintain data pipelines using Azure Synapse Analytics.\nCollaborate with customer to ensure SLA adherence and incident resolution.\nOptimize Synapse SQL pools for performance and cost.\nImplement data security, access control, and compliance measures.\nParticipate in calibration and transition phases with client stakeholders",Industry Type: Recruitment / Staffing,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Azure Data Factory', 'Azure Synapse', 'Data Engineering', 'Azure Databricks', 'SQL Azure', 'Power Bi', 'Devops']",2025-06-12 13:53:00
Data Architect Telecom Domain databrick BSS OSS,fast growing Data Driven IT solutions an...,10 - 20 years,45-55 Lacs P.A.,"['Noida', 'Hyderabad', 'Gurugram']","Data Architect Telecom Domain\nTo design comprehensive data architecture and technical solutions specifically for telecommunications industry challenges, leveraging TMforum frameworks and modern data platforms. To work closely with customers, and technology partners to deliver data solutions that address complex telecommunications business requirements including customer experience management, network optimization, revenue assurance, and digital transformation initiatives.\nResponsibilities:\nDesign and articulate enterprise-scale telecom data architectures incorporating TMforum standards and frameworks, including SID (Shared Information/Data Model), TAM (Telecom Application Map), and eTOM (enhanced Telecom Operations Map)\nDevelop comprehensive data models aligned with TMforum guidelines for telecommunications domains such as Customer, Product, Service, Resource, and Partner management\nCreate data architectures that support telecom-specific use cases including customer journey analytics, network performance optimization, fraud detection, and revenue assurance\nDesign solutions leveraging Microsoft Azure and Databricks for telecom data processing and analytics\nConduct technical discovery sessions with telecom clients to understand their OSS/BSS architecture, network analytics needs, customer experience requirements, and digital transformation objectives\nDesign and deliver proof of concepts (POCs) and technical demonstrations showcasing modern data platforms solving real-world telecommunications challenges\nCreate comprehensive architectural diagrams and implementation roadmaps for telecom data ecosystems spanning cloud, on-premises, and hybrid environments\nEvaluate and recommend appropriate big data technologies, cloud platforms, and processing frameworks based on telecom-specific requirements and regulatory compliance needs.\nDesign data governance frameworks compliant with telecom industry standards and regulatory requirements (GDPR, data localization, etc.)\nStay current with the latest advancements in data technologies including cloud services, data processing frameworks, and AI/ML capabilities\nContribute to the development of best practices, reference architectures, and reusable solution components for accelerating proposal development\nQualifications:\nBachelor's or Master's degree in Computer Science, Telecommunications Engineering, Data Science, or a related technical field\n10+ years of experience in data architecture, data engineering, or solution architecture roles with at least 5 years in telecommunications industry\nDeep knowledge of TMforum frameworks including SID (Shared Information/Data Model), eTOM, TAM, and their practical implementation in telecom data architectures\nDemonstrated ability to estimate project efforts, resource requirements, and implementation timelines for complex telecom data initiatives\nHands-on experience building data models and platforms aligned with TMforum standards and telecommunications business processes\nStrong understanding of telecom OSS/BSS systems, network management, customer experience management, and revenue management domains\nHands-on experience with data platforms including Databricks, and Microsoft Azure in telecommunications contexts\nExperience with modern data processing frameworks such as Apache Kafka, Spark and Airflow for real-time telecom data streaming\nProficiency in Azure cloud platform and its respective data services with an understanding of telecom-specific deployment requirements\nKnowledge of system monitoring and observability tools for telecommunications data infrastructure\nExperience implementing automated testing frameworks for telecom data platforms and pipelines\nFamiliarity with telecom data integration patterns, ETL/ELT processes, and data governance practices specific to telecommunications\nExperience designing and implementing data lakes, data warehouses, and machine learning pipelines for telecom use cases\nProficiency in programming languages commonly used in data processing (Python, Scala, SQL) with telecom domain applications\nUnderstanding of telecommunications regulatory requirements and data privacy compliance (GDPR, local data protection laws)\nExcellent communication and presentation skills with ability to explain complex technical concepts to telecom stakeholders\nStrong problem-solving skills and ability to think creatively to address telecommunications industry challenges\nGood to have TMforum certifications or telecommunications industry certifications\nRelevant data platform certifications such as Databricks, Azure Data Engineer are a plus\nWillingness to travel as required\nif you will all or most of the criteria contact bdm@intellisearchonline.net M 9341626895",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Telecom Bss', 'Data Architect', 'Telecom OSS', 'ETOM', 'Data Bricks']",2025-06-12 13:53:03
Snowflake Developer with Azure Data Factory,Net Connect,6 - 10 years,6-11 Lacs P.A.,['Hyderabad'],Greetings from NCG!\n\nWe have a opening for Snowflake Developer role in Hyderabad office!\n\nBelow JD for your reference\n\nJob Description:,,,,"['Azure Data Factory', 'Snowflake', 'SQL']",2025-06-12 13:53:05
Calibration Engineer,Motherson Technology Services Limited,5 - 10 years,Not Disclosed,['Pune'],"Role & responsibilities\n• Define and update the Application project plan for the assigned Projects.\n• Manage with pro-active approach the project plan changes during the development phase.\n• Adopt calibration methodologies, procedures and tools shared by Marelli HQ.\n• Promote effective solutions together with team Application and Functions Design Teams\n• Guarantee the compliance of calibration process workflow with the standards defined by Team.\n• Promote the use of statistical analysis and big data management, in cooperation with team, to validate strategies performance and diagnosis robustness.\n• Customer technical reference for all the issues related to calibration.\n• Support on Customer site for calibration activities development, when requested.\n• Confirm with the Car Maker for the process of calibration via label review.\nCoordinate the activities on the test development vehicles assigned to each project.\n• Take part to calibration design review - risk analyses with Team or with the Customers.\n• Analysis and resolution of vehicle fleets and vehicle market concerns\n\nPreferred candidate profile\nSkill\nGasoline/CNG/Flex fuel working experience.\nMPFI/GDI engines.\nBase calibration on test bed and vehicle.\nTransient calibration.\nDrivability and test trips experience.\nStart ability calibration.\nIdle calibration.\nEmission calibration.\nOBD-1 and OBD-2B calibration.",Industry Type: IT Services & Consulting,"Department: Production, Manufacturing & Engineering","Employment Type: Full Time, Permanent","['gasoline', 'calibration', 'Obd', 'INCA', 'Emission', 'Engine Calibration']",2025-06-12 13:53:07
Data Architect (Data Bricks),Diacto Technologies Pvt Ltd,5 - 9 years,Not Disclosed,['Pune( Baner )'],"Job Overview:\nDiacto is seeking an experienced and highly skilled Data Architect to lead the design and development of scalable and efficient data solutions. The ideal candidate will have strong expertise in Azure Databricks, Snowflake (with DBT, GitHub, Airflow), and Google BigQuery. This is a full-time, on-site role based out of our Baner, Pune office.\n\nQualifications:\nB.E./B.Tech in Computer Science, IT, or related discipline\nMCS/MCA or equivalent preferred\n\nKey Responsibilities:\nDesign, build, and optimize robust data architecture frameworks for large-scale enterprise solutions\nArchitect and manage cloud-based data platforms using Azure Databricks, Snowflake, and BigQuery\nDefine and implement best practices for data modeling, integration, governance, and security\nCollaborate with engineering and analytics teams to ensure data solutions meet business needs\nLead development using tools such as DBT, Airflow, and GitHub for orchestration and version control\nTroubleshoot data issues and ensure system performance, reliability, and scalability\nGuide and mentor junior data engineers and developers\n\nExperience and Skills Required:\n5 to12 years of experience in data architecture, engineering, or analytics roles\nHands-on expertise in Databricks, especially Azure Databricks\nProficient in Snowflake, with working knowledge of DBT, Airflow, and GitHub\nExperience with Google BigQuery and cloud-native data processing workflows\nStrong knowledge of modern data architecture, data lakes, warehousing, and ETL pipelines\nExcellent problem-solving, communication, and analytical skills\n\nNice to Have:\nCertifications in Azure, Snowflake, or GCP\nExperience with containerization (Docker/Kubernetes)\nExposure to real-time data streaming and event-driven architecture\n\nWhy Join Diacto Technologies?\nCollaborate with experienced data professionals and work on high-impact projects\nExposure to a variety of industries and enterprise data ecosystems\nCompetitive compensation, learning opportunities, and an innovation-driven culture\nWork from our collaborative office space in Baner, Pune\nHow to Apply:\nOption 1 (Preferred)\n\nCopy and paste the following link on your browser and submit your application for the automated interview process: -\n\nhttps://app.candidhr.ai/app/candidate/gAAAAABoRrTQoMsfqaoNwTxsE_qwWYcpcRyYJk7NzSUmO3LKb6rM-8FcU58CUPYQKc65n66feHor-TGdCEfyouj0NmKdgYcNbA==/\n\nOption 2\n\n1. Please visit our website's career section at https://www.diacto.com/careers/\n2. Scroll down to the ""Who are we looking for?"" section\n3. Find the listing for "" Data Architect (Data Bricks)"" and\n4. Proceed with the virtual interview by clicking on ""Apply Now.""",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Snowflake', 'Azure Databricks', 'Airflow', 'Etl Pipelines', 'Github', 'google BigQuery', 'DBT', 'Data Security', 'Data Modeling', 'Elt', 'Data Governance']",2025-06-12 13:53:09
Data Scientist For DMAI,Prodapt Solutions,2 - 5 years,Not Disclosed,['Chennai'],"Overview\n\nThe Senior Data Science Engineer will leverage advanced data science techniques to solve complex business problems, guide decision-making processes, and mentor junior team members. This role requires a combination of technical expertise in data analysis, machine learning, and project management skills.\n\nResponsibilities\n\n Data Analysis and Modeling Analyze large-scale telecom datasets to extract actionable insights and build predictive models for network optimization and customer retention.\n Conduct statistical analyses  to validate models and ensure their effectiveness.\n Machine Learning Development Design and implement machine learning algorithms for fraud detection, churn prediction, and network failure analysis.\n Telecom-Specific Analytics Apply domain knowledge to improve customer experience by analyzing usage patterns, optimizing services, and predicting customer lifetime value.\n ETL Processes Develop robust pipelines for extracting, transforming, and loading telecom data from diverse sources.\n Collaboration Work closely with data scientists, software engineers, and telecom experts to deploy solutions that enhance operational efficiency.\n Data Governance :  Ensure data integrity, privacy, security and compliance with industry standards\n\n\nAdvanced degree in Data Science, Statistics, Computer Science, or a related field.\nExtensive experience in data science roles with a strong focus on machine learning and statistical modeling.\nProficiency in programming languages such as Python or R and strong SQL skills.\nFamiliarity with big data technologies (e.g., Hadoop, Spark) is advantageous.\nExpertise in cloud platforms such as AWS or Azure.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['project management', 'data analysis', 'machine learning', 'sql', 'statistical modeling', 'algorithms', 'python', 'big data technologies', 'microsoft azure', 'cloud platforms', 'r', 'data science', 'spark', 'data governance', 'hadoop', 'aws', 'etl', 'machine learning algorithms', 'statistics']",2025-06-12 13:53:12
Sr. Database Engineer For US shift (Eastern Time),TEOCO,5 - 10 years,Not Disclosed,"['Kolkata', 'Bengaluru']","Position: Sr. Database Engineer US shift (Eastern Time)\nLocation: Kolkata or Bangalore\nFull time permanent position\n\nUS shift (Eastern Time) : 4.30PM to 12.30AM IST (complete work from home)\n\nExperience required: 6-8+ years\n\n\nMajor skills required: SQL, C# or Python, any ETL tool\n\n\nProduct Development:\n\nWork with the business analysts to understand the high level business need and requirements;\nWork with operation team to understand issues and provide step-by-step solutions;\nImplement business logic using SQL;\nMonitor the system for any issues and quickly respond to emergencies;\nDeep detailed understanding of internal ETL tool;\nPrepare product for the release and drive the process;\nProficiency in query optimization (understanding query plans, improving execution time etc.);\nWrite scripts using internal code language (C# based) in order to optimize the process;\nUnderstand the overall process and data flows;\nPerform detailed analysis of the code and do research to help analysts understand current situation and make a decision.\n\nExperience and required skills:\n\nStrong understanding of relational databases;\nAdvanced SQL knowledge is required;\nWorking experience with MPP (MPP Massively Parallel Processing) Databases, understanding database design (data distribution, partitioning etc.);\nMedium Linux knowledge is required;\nC# or Python mid-level;\nExperience with analytic reporting tools such as SAP Business Object is preferred;\nAbility to work in a multi-cultured team environment;\nStrong oral and written communication skills.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SQL', 'C#', 'python', 'ETL']",2025-06-12 13:53:14
Data Scientist,Celebal Technologies,3 - 6 years,Not Disclosed,"['Mumbai', 'Navi Mumbai']","About Us: Celebal Technologies is a leading Solution Service company that provide Services the field of Data Science, Big Data, Enterprise Cloud & Automation. We are at the forefront of leveraging cuttingedge technologies to drive innovation and enhance our business processes. As part of our commitment to staying ahead in the industry, we are seeking a talented and experienced Data & AI Engineer with strong Azure cloud competencies to join our dynamic team.\n\nJob Summary: We are looking for a highly skilled Data Scientist with deep expertise in time series forecasting, particularly in demand forecasting and customer lifecycle analytics (CLV). The ideal candidate will be proficient in Python or PySpark, have hands-on experience with tools like Prophet and ARIMA, and be comfortable working in Databricks environments. Familiarity with classic ML models and optimization techniques is a plus.\n\nKey Responsibilities\n• Develop, deploy, and maintain time series forecasting models (Prophet, ARIMA, etc.) for demand forecasting and customer behavior modeling.\n• Design and implement Customer Lifetime Value (CLV) models to drive customer retention and engagement strategies.\n• Process and analyze large datasets using PySpark or Python (Pandas).\n• Partner with cross-functional teams to identify business needs and translate them into data science solutions.\n• Leverage classic ML techniques (classification, regression) and boosting algorithms (e.g., XGBoost, LightGBM) to support broader analytics use cases.\n• Use Databricks for collaborative development, data pipelines, and model orchestration.\n• Apply optimization techniques where relevant to improve forecast accuracy and business decision-making.\n• Present actionable insights and communicate model results effectively to technical and non-technical stakeholders.\n\nRequired Qualifications\n• Strong experience in Time Series Forecasting, with hands-on knowledge of Prophet, ARIMA, or equivalent Mandatory.\n• Proven track record in Demand Forecasting Highly Preferred.\n• Experience in modeling Customer Lifecycle Value (CLV) or similar customer analytics use cases Highly Preferred.\n• Proficiency in Python (Pandas) or PySpark Mandatory.\n• Experience with Databricks Mandatory.\n• Solid foundation in statistics, predictive modeling, and machine learning",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Machine Learning Operations', 'Demand Forecasting', 'Data Bricks', 'Pyspark', 'Large Language Model', 'Time Series', 'Spark', 'Machine Learning', 'Python']",2025-06-12 13:53:16
Senior Software Engineer-7916,WebMD,5 - 10 years,Not Disclosed,['Navi Mumbai'],"Position: Senior Software Engineer (Data Engineer)\nNo. of Positions: 1\nAbout WebMD:\nHeadquartered in El Segundo, Calif., Internet Brands is a fully integrated online media and software services\norganization focused on four high-value vertical categories: Health, Automotive, Legal, and Home/Travel. The\ncompanys award-winning consumer websites lead their categories and serve more than 250 million monthly\nvisitors, while a full range of web presence offerings has established deep, long-term relationships with SMB and\nenterprise clients. Internet Brands,powerful, proprietary operating platform provides the flexibility and\nscalability to fuel the companys continued growth. Internet Brands is a portfolio company of KKR and Temasek.\nWebMD Health Corp., an Internet Brands Company, is the leading provider of health information services, serving\npatients, physicians, health care professionals, employers, and health plans through our public and private online\nportals, mobile platforms, and health-focused publications. The WebMD Health Network includes WebMD\nHealth, Medscape, Jobson Healthcare Information, prIME Oncology, MediQuality, Frontline, QxMD, Vitals\nConsumer Services, MedicineNet, eMedicineHealth, RxList, OnHealth, Medscape Education, and other owned\nWebMD sites. WebMD, Medscape, CME Circle, Medpulse, eMedicine®, MedicineNet®, theheart.org®, and\nRxList® are among the trademarks of WebMD Health Corp. or its subsidiaries.\nFor Company details, visit our website: www.webmd.com / www.internetbrands.com\nAll qualified applicants will receive consideration for employment without regard to race, color, religion, sex,\nsexual orientation, gender identity, national origin, disability, or veteran status\nEducation: B.E. Computer Science/IT degree (or any other engineering discipline)\nExperience: 5+ years\nWork timings: 2:00 PM to 11:00 PM IST.\nDescription:\nWe are seeking an experienced and passionate Senior Software Developer to join our team. In this role,\nyou will work closely with cross-functional teams, developers, stakeholders, and business units to\ngather and analyze business requirements, design, build and implement ETL Solutions, and maintain\nthe infrastructure. The ideal candidate will have a strong background in business analysis, SQL, Unix,\nPython, ETL Tools to ensure the successful execution of projects.\nResponsibilities:\nLead requirements gathering sessions with key stakeholders to understand business needs and\nobjectives.\nCollaborate with and across Agile teams to design, develop, test, implement and support ETL\nprocesses for data transformation and preparation.\nManage data pipelines for analytics and operational use.\nEnsure data quality, data accuracy and integrity across multiple sources and systems.\nPerform unit tests and conduct reviews with other team members to make sure your code is\nrigorously designed, elegantly coded, and effectively tuned for performance.\nShould be able to come up with multiple approaches to any ETL\nproblem statement/solution/technical challenge and take well informed decision to pick the\nbest solution.\nAutomate ETL Processes using Cron and/or using Job Scheduler tools like AirFlow.\nAdhere to company standards and Serve as a key contributor to the design and development of\nexception handling, code/data standardization procedures, resolution steps and Quality Assurance\ncontrols.\nMaintain a version repository and ensure version control.\nCreate visual aids such as diagrams, charts, and screenshots to enhance documentation.\nWork with Infrastructure/systems team and developers to ensure all modules are up-to- date and\nare compatible with the code.",,,,"['ETL', 'SQL', 'UNIX', 'Python']",2025-06-12 13:53:18
Data Scientist,Mindpro Technologies,4 - 9 years,5-12 Lacs P.A.,"['Karur', 'Dharwad']","Greetings From Mind Pro Technologies Pvt ltd (www.mindprotech.com)\n\nJob Title : Data Scientist\nWork Location : Karur (Tamil Nadu) or Dharwad (Karnataka )\nNp : 15days or Less\n\n\nJOB DESCRIPTION:\n Must have At least 4+ Years of experience in Python with Data Science.\n Must have worked on at least one Live project.\nExperience in relevant field such as Statistics, Computer Science or Applied Math or Operational Research.\nMust have Masters in (Maths/Statistics or Applied Mathematics/Machine Learning etc.)\nHistory of successfully performing customer implementations\nStrong customer facing skills, and previous consulting experience.\nExperience of handling high frequency streaming data for real time analysis and reporting.\nFamiliarity with - Natural Language Processing, Statistical Analysis (distribution analysis, correlation, variance, deep learning.\nExperience in tools like AWS, IBM Watson is a plus.\nExperience with open source technologies is a must.\nExcellent communication\nAbility to lead & build strong teams\nAbility to work in an ambiguous environment\n\nDesired Skills and Experience\nLanguages/Tools: Python/R.\nApproaches: Machine Learning\nConcepts: Supervised ANN, Bayesian, Gaussian, Vector Quantization, Logistic Model, Statistical, Predictive Modeling, Minimum Message Length, SVM, Random Forest, Ensembles, ANOVA, Decision Trees, Hidden Markov Models\nUnsupervised ANN, ARL, Clustering Hierarchical, Cluster Analysis\nReinforcement\nGen AI, LLM, LSTM, RNN, CNN, KNN\nBig Data (Good to have): Hadoop /Kafka / Storm / Spark streaming\nOS: Linux, Windows 32/64 bits.\n\nNote:  should know supervised and unsupervised learning,   semi-supervised learning, neural networks concepts, and how ML algorithms works with training and testing data. Experience on particular data set to train, test and roll-out for production use\n\nTool sets : Python, R, MATLAB or  any AI frame work, Neural network, Gen AI, LLM\nContact Details:\n\nRecruitment Team\nMindpro Technologies Pvt Ltd (www.mindprotech.com)\n+91-04324-240904 / +91-9600672304",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Gen AI', 'Statistical Modeling', 'LLM', 'Predictive Modeling', 'Artificial Intelligence', 'Natural Language Processing', 'Neural Networks', 'Machine Learning', 'Deep Learning', 'Python']",2025-06-12 13:53:21
"Senior Manager, Software Engineering",Diligent Corporation,10 - 15 years,Not Disclosed,['Bengaluru'],"Position Overview\nYou will work closely with technology and business teams to understand requirements, design robust architectures, and influence technology choices to deliver innovative solutions. In addition to leading SaaS software development, you will drive initiatives in Data Engineering, Data Warehousing, and Artificial Intelligence (AI). You will collaborate with principal engineers and leadership, and have opportunities to cross-collaborate with inter-disciplinary teams to solve unique challenges in the GRC & ESG domain.\nKey Responsibilities\nShape the product and technical vision for the team, collaborating with product, business, and engineering leadership across the company.\nManage and mentor a team of engineers developing highly scalable, performant, maintainable, and well-tested SaaS features.\nLead the design and implementation of modern data warehouse solutions, ensuring data quality, scalability, and security.\nOversee the integration of advanced AI and machine learning models into SaaS products to deliver intelligent features and insights.\nHire, mentor, and lead a world-class group of engineers with expertise in SaaS, data engineering, and AI.\nFoster a culture of innovation, experimentation, and continuous improvement.\nEvaluate engineering requirements and design proposals, especially in the context of data-driven and AI-powered applications.\nAssess and develop the technical performance of individual contributors within the team.\nStay current with the latest frameworks and technologies in SaaS, Data Engineering, and AI, influencing technology choices for the application stack.\nFacilitate daily stand-ups, risk identification and mitigation, dependency resolution, and follow-ups for gap closure.\nPartner with Product Management and Business teams to drive the agenda, set priorities, create project plans, and deliver outstanding products.\nRequired Experience/Skills\n10 to 15 years of relevant experience in developing enterprise SaaS applications using MERN/.NET, MySQL, MS SQL, and caching technologies.\n2+ years of experience leading engineering teams building scalable platforms and architectures, including data engineering and AI initiatives.\nProven experience designing, building, and maintaining data warehouse solutions (such as Snowflake, Redshift, or BigQuery) and data pipelines (ETL/ELT).\nHands-on experience with AI/ML frameworks (such as TensorFlow, PyTorch, or Scikit-learn) and integrating AI models into production SaaS environments.\nStrong background in data modeling, data governance, and data quality best practices.\nExperience with cloud platforms (AWS/Azure), CI/CD, DevOps, scripting, and SQL/NoSQL databases.\nDemonstrated success in migrating monolithic applications to microservices and on-premises solutions to cloud environments.\nPassion for building a data-driven culture, growing talent, and making a significant impact through technology.\nStrong communication skills for engaging with end users, technical, and business teams to gather requirements and describe product features and technical designs.\nAbility to seek clarity in ambiguous situations and drive projects to completion.\nExperience in Agile development and knowledge of Scrum and Kanban methodologies.\nSelf-motivated learner and builder with a strong customer focus and a commitment to delivering high-quality solutions.",Industry Type: Design,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Product management', 'MS SQL', 'NoSQL', 'Data modeling', 'MySQL', 'Machine learning', 'Scrum', 'Data quality', 'SQL', 'Recruitment']",2025-06-12 13:53:23
"Senior Manager, Engineering",Reltio,10 - 15 years,Not Disclosed,['Bengaluru'],"Job Summary:\nAre you passionate about data management and unifying complex datasets? Do you have a track record of leading successful data unification projects? Reltio is seeking a dynamic and experienced Senior Manager to join our Data Unification Team in India. As a Senior Manager, you will play a pivotal role in driving the development and execution of data unification strategies and initiatives, ensuring high-quality and accurate data for our clients.\nJob Duties and Responsibilities:\nLeadership: Provide strong leadership and guidance to the Data Unification Team, driving a culture of excellence, innovation, and collaboration.\nData Unification Strategy: Develop and implement data unification strategies, frameworks, and best practices to deliver effective data management solutions.\nTeam Management: Lead, mentor, and inspire a team of data engineers and analysts, fostering their professional growth and ensuring the teams success in meeting project goals.\nData Governance: Define and enforce data governance policies, standards, and procedures to ensure data quality, integrity, and security across all data unification activities.\nProject Management: Oversee end-to-end project management, including scoping, planning, resource allocation, and execution of data unification projects, ensuring timely delivery within budget and scope.\nStakeholder Collaboration: Collaborate with cross-functional teams, including Product Management, Engineering, and Customer Success, to align data unification initiatives with overall business objectives and customer requirements.\nContinuous Improvement: Identify areas for process improvement, automation, and optimization, driving efficiency and scalability in data unification operations.\nIndustry Trends: Stay updated with industry trends, emerging technologies, and best practices in data management and unification, leveraging this knowledge to drive innovation and enhance our offerings.\nSkills You Must Have:\nMinimum of 9+ years of experience in data management, data unification, or related fields, with a focus on managing large-scale data projects.\nStrong leadership and managerial skills, with a proven track record of successfully leading and motivating high-performing teams.\nIn-depth knowledge of data unification methodologies, tools, and technologies, including Master Data Management (MDM) and data integration techniques.\nSolid understanding of data governance principles, data quality frameworks, and data security best practices.\nExcellent project management skills, with the ability to manage multiple projects simultaneously, prioritize tasks, and meet deadlines.\nStrong analytical and problem-solving abilities, with the capacity to analyze complex data sets, identify patterns, and propose innovative solutions.\nEffective communication and stakeholder management skills, with the ability to collaborate and influence cross-functional teams and senior leadership.\nBachelors or Masters degree in Computer Science, Information Systems, or a related field.",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Product management', 'Computer science', 'Automation', 'Team management', 'data security', 'Analytical', 'Process improvement', 'Data quality', 'Stakeholder management', 'Analytics']",2025-06-12 13:53:26
Data Stage Developer,Anblicks Solutions,6 - 11 years,Not Disclosed,['Chennai'],"Job Summary:\nWe are looking for a seasoned ETL Engineer with hands-on experience in Talend or IBM DataStage, preferably both, to lead data integration efforts in the mortgage domain. The ideal candidate will play a key role in designing, developing, and managing scalable ETL solutions that support critical mortgage data processing and analytics workloads.\n\nKey Responsibilities:\nEnd-to-end ETL solution development using Talend or DataStage.Design and implement robust data pipelines for mortgage origination, servicing, and compliance data.Collaborate with business stakeholders and data analysts to gather requirements and deliver optimized solutions.Perform code reviews, mentor junior team members, and ensure adherence to data quality and performance standards.Manage job orchestration, scheduling, and error handling mechanisms.Document ETL workflows, data dictionaries, and system processes.Ensure data privacy and compliance requirements are embedded in all solutions.\n\nRequired Skills:\nStrong experience in ETL tools Talend (preferred) or IBM DataStage.Solid understanding of mortgage lifecycle and related data domains.Proficiency in SQL and experience with relational databases (e.g., Oracle, SQL Server, Snowflake).Familiarity with job scheduling tools, version control, and CI/CD pipelines.Excellent problem-solving, leadership, and communication skills.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Datastage', 'Ibm Datastage', 'Etl Datastage']",2025-06-12 13:53:28
Data Protection Officer (DPO) / GRC Officer,Fynd (Shopsense Retail Technologies Ltd.),5 - 10 years,Not Disclosed,['Mumbai'],"Fynd is India s largest omnichannel platform and multi-platform tech company with expertise in retail tech and products in AI, ML, big data ops, gaming+crypto, image editing and learning space. Founded in 2012 by 3 IIT Bombay alumni: Farooq Adam, Harsh Shah and Sreeraman MG. We are headquartered in Mumbai and have 1000+ brands under management, more than 10k stores and servicing 23k + pin codes.\n\nWe are seeking a highly skilled Data Protection Officer (DPO) / GRC Officer responsible for ensuring compliance with global security and data protection regulations. The ideal candidate will oversee governance, risk, and compliance (GRC) programs, implement security frameworks, and safeguard sensitive data across the organization.\n\nWhat will you do at Fynd ?\n\n1. Governance, Risk, and Compliance (GRC):\nDevelop, implement, and maintain GRC frameworks to align with regulatory and industry standards.\nEstablish risk assessment methodologies and ensure mitigation strategies are in place.\nConduct IT General Controls (ITGC) assessments to ensure effective security controls and processes.\nOversee third-party risk assessments, ensuring vendors comply with security policies.\n2. Data Protection & Privacy Compliance:\nImplement and oversee compliance with DPDP (Digital Personal Data Protection Act, India) and GDPR regulations.\nAct as the point of contact for data protection authorities and internal privacy matters.\nConduct Data Protection Impact Assessments (DPIAs) and privacy risk assessments.\nDevelop and enforce privacy policies, data retention, and protection measures.\n3. Information Security Compliance & Certifications:\nLead and maintain compliance with ISO 27001, ensuring policies and controls meet certification requirements.\nManage SOC 2 compliance efforts, including security, availability, processing integrity, confidentiality, and privacy principles.\nOversee PCI-DSS compliance for handling cardholder data securely.\nEnsure alignment with NIST security frameworks for risk management and cybersecurity resilience.\n4. Business Continuity & Incident Management:\nDevelop and maintain a Business Continuity Management (BCM) program, including disaster recovery plans.\nLead security incident response and investigations to mitigate data breaches and cybersecurity threats.\nConduct regular tabletop exercises and audits to test resilience and readiness.\n\nSome Specific Requirements\nBachelor s/Master s degree in Information Security, Cybersecurity, Compliance, or a related field.\nProfessional certifications such as CIPP/E, CIPM, CISSP, CISM, CISA, ISO 27001 Lead Auditor, or CRISC are highly preferred.\n5+ years of experience in Data Protection, Compliance, GRC, or Cybersecurity roles.\nStrong knowledge of regulatory frameworks (SOC2, ISO27001, GDPR, DPDP, PCI-DSS, NIST, ITGC, Third-Party Risk Management).\nExperience in implementing GRC tools and automating compliance processes.\nExcellent stakeholder management skills with the ability to work cross-functionally.\nStrong analytical, problem-solving, and decision-making skills.\n\nWhat do we offer?\n\nGrowth\nGrowth knows no bounds, as we foster an environment that encourages creativity, embraces challenges, and cultivates a culture of continuous expansion. We are looking at new product lines, international markets and brilliant people to grow even further. We teach, groom and nurture our people to become leaders. You get to grow with a company that is growing exponentially.\n\nFlex University\nWe help you upskill by organising in-house courses on important subjects\nLearning Wallet: You can also do an external course to upskill and grow, we reimburse it for you.\n\nCulture\nCommunity and Team building activities\nHost weekly, quarterly and annual events/parties.\n\nWellness\nMediclaim policy for you + parents + spouse + kids\nExperienced therapist for better mental health, improve productivity & work-life balance\n\nWe work from the office 5 days a week to promote collaboration and teamwork. Join us to make an impact in an engaging, in-person environment!",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Cism', 'security compliance', 'Cisa', 'Information security', 'SOC', 'Risk assessment', 'Disaster recovery', 'Flex', 'Incident management', 'Risk management']",2025-06-12 13:53:30
Senior Software Engineer,Joules to Watts,4 - 6 years,2-6 Lacs P.A.,['Bengaluru( Whitefield )'],"Only for Immediate Joiners\nCore Responsibility:\nThe project team will be spread between Paris and Bangalore. So, the candidate with an experience of 3-6 years is expected to work and coordinate on daily basis with the remote teams. Ability to learn new technology / framework / methodology.\n    Hands-on individual responsible for producing excellent quality of code, adhering to expected coding standards and industry best practices. \n    Must have strong knowledge and working experience on Big DATA ecosystem.\n    Must have strong experience in SPARK/SCALA, NIFI, KAFKA, HIVE, PIG.\n   Strong knowledge and experience working on HQL (hive Query Language)\n•    Must have strong strong expertise in Debugging and Fixing Production Issues on BIG DATA eco System.\n    Knowledge on code version management using Git & Jenkins, Nexus.\n•    High levels of ownership and commitment on deliverables. Strong and Adaptive Communication Skills; Should be comfortable interacting with Paris counterparts  to probe a technical problem or clarify requirement specifications. \n\nKEY SKILLS:\nSound knowledge on SPARK/SCALA, NIFI, KAFKA - Must Have\nSound Knowledge on HQL\nKnowledge on Kibana, Elastic Search Log stash Good to know\nBasic Awareness of CD/CI concepts & Technologies\nBig Data Ecosystem Good to know",Industry Type: Banking,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Big Data', 'Hive', 'Apache Pig', 'Hadoop', 'SCALA', 'Kafka', 'Spark', 'Apache Nifi']",2025-06-12 13:53:32
Senior Lead business execution consultant,Wells Fargo,7 - 12 years,Not Disclosed,['Bengaluru'],"About this role:\nWells Fargo is seeking a Senior Lead business execution consultant\n\nIn this role, you will:\nAct as a Business Execution advisor to leadership to drive performance and initiatives, and develop and implement information delivery or presentations to key stakeholders and senior management",,,,"['Business execution', 'Business Implementation', 'Data Engineering', 'NLP', 'generative AI', 'Data Mining', 'machine learning', 'Strategic Planning', 'agentic AI']",2025-06-12 13:53:35
Sr. Associate Full Stack Software Engineer,Amgen Inc,1 - 3 years,Not Disclosed,['Hyderabad'],"What you will do\n\n\nIn this vital role you will be responsible for designing, developing, and maintaining software applications and solutions that meet business needs and ensuring the availability and performance of critical systems and applications. This role involves working closely with product managers, designers, data engineers, and other engineers to create high-quality, scalable software solutions and automating operations, monitoring system health, and responding to incidents to minimize downtime.\n\nYou will play a key role in a regulatory submission content automation initiative which will modernize and digitize the regulatory submission process, positioning Amgen as a leader in regulatory innovation. The initiative demonstrates innovative technologies, including Generative AI, Structured Content Management, and integrated data to automate the creation, and management of regulatory content.\n\n\n\nRoles & Responsibilities:\nPossesses strong rapid prototyping skills and can quickly translate concepts into working code\nContribute to both front-end and back-end development using cloud technology\nDevelop innovative solution using generative AI technologies\nEnsure code quality and consistency to standard methodologies\nCreate and maintain documentation on software architecture, design, deployment, disaster recovery, and operations\nIdentify and resolve technical challenges effectively\nStay updated with the latest trends and advancements\nWork closely with product team, business team, and other collaborators\nDesign, develop, and implement applications and modules, including custom reports, interfaces, and enhancements\nAnalyze and understand the functional and technical requirements of applications, solutions and systems and translate them into software architecture and design specifications\nDevelop and implement unit tests, integration tests, and other testing strategies to ensure the quality of the software\nIdentify and resolve software bugs and performance issues\nWork closely with multi-functional teams, including product management, design, and QA, to deliver high-quality software on time\nCustomize modules to meet specific business requirements\nWork on integrating with other systems and platforms to ensure seamless data flow and functionality\nProvide ongoing support and maintenance for applications, ensuring that they operate smoothly and efficiently\n\n\n\nWhat we expect of you\nWe are all different, yet we all use our unique contributions to serve patients.\nMasters degree and 1 to 3 years of experience in Computer Science, IT or related field OR\nBachelors degree and 3 to 5 years of experience in Computer Science, IT or related field OR\nDiploma and 7 to 9 years of experience in Computer Science, IT or related field\nPreferred Qualifications:\n\n\n\nFunctional\n\nSkills:\nMust-Have Skills:\nProficiency in Python/PySpark development, Fast API, PostgreSQL, Databricks, DevOps Tools, CI/CD, Data Ingestion.\nCandidates should be able to write clean, efficient, and maintainable code.\nKnowledge of HTML, CSS, and JavaScript, along with popular front-end frameworks like React or Angular, is required to build interactive and responsive web applications\nIn-depth knowledge of data engineering concepts, ETL processes, and data architecture principles. Solid understanding of cloud computing principles, particularly within the AWS ecosystem\nSolid understanding of software development methodologies, including Agile and Scrum\nExperience with version control systems like Git\nHands on experience with various cloud services, understand pros and cons of various cloud service in well architected cloud design principles\nStrong problem solving, analytical skills; Ability to learn quickly; Good communication and interpersonal skills\nExperienced with API integration, serverless, microservices architecture.\nExperience in SQL/NOSQL database, vector database for large language models\n\n\n\nGood-to-Have\n\nSkills:\nSolid understanding of cloud platforms (e.g., AWS, GCP, Azure) and containerization technologies (e.g., Docker, Kubernetes)\nExperience with monitoring and logging tools (e.g., Prometheus, Grafana, Splunk)\nExperience with data processing tools like Hadoop, Spark, or similar\n\n\n\nSoft\n\nSkills:\nExcellent analytical and solving skills\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation\nAbility to manage multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals\nStrong presentation and public speaking skills",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Python', 'PySpark development', 'Data Ingestion', 'PostgreSQL', 'Fast API', 'CI/CD', 'DevOps Tools', 'Databricks']",2025-06-12 13:53:37
Senior Software Engineer Python,Growexx,5 - 10 years,Not Disclosed,['Ahmedabad'],"built on top of the Revelation Open Insights analytics platform. This role is ideal for an engineer who thrives in complex, data-intensive environments and has a passion for modernizing lega operational stability. Key Responsibilities\nLead the maintenance, enhancement, and refactoring of the Python-based Legacy Registry system.\nCollaborate with data engineers, DevOps, and platform architects to ensure seamless integration with Revelation Open Insights.\nAnalyze and optimize legacy code for performance, scalability, and maintainability.\nDesign and implement automated testing strategies and CI/CD pipelines for legacy services.\nTroubleshoot and resolve production issues, ensuring high system availability and data integrity.\nDocument system architecture, workflows, and technical decisions to support long-term maintainability.\nParticipate in roadmap planning and contribute to modernization strategies.\nKey Skills\nDeep understanding of legacy system architecture, data modelling, and refactoring techniques.\nExperience working with SQL databases (e.g., PostgreSQL, MySQL) and data integration pipelines.\nFamiliarity with containerization (Docker), orchestration (Kubernetes), and CI/CD tools (e.g., GitHub Actions, Jenkins).\nStrong debugging, profiling, and performance tuning skills.\nExperience with enterprise data platforms or analytics systems.\nFamiliarity with the Revelation Open Insights platform or similar data intelligence tools.\nExposure to data governance, metadata management, or registry systems.\nEducation and Experience\nB.Tech or B.E. or MCA or BCA\n5+ years of professional experience in Python development, with a strong focus on backend systems\nAnalytical and Personal Skills Must have good logical reasoning and analytical skills\nAbility to break big goals to small incremental actions\nExcellent Communication and collaboration skills Demonstrate Ownership and Accountability of their work\nGreat attention to details\nDemonstrate ownership of tasks Positive and Cheerful outlook in life Work with the problem solver engineers team (Doc / PDF Only, Max file size 2 MB) By using this form you agree with the storage and handling of your data by this website. *\nYou cannot copy content of this page\nReconciliation Automation Data Sheet\nThis field is for validation purposes and should be left unchanged.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Performance tuning', 'System architecture', 'Automation', 'metadata', 'Postgresql', 'MySQL', 'Debugging', 'Analytics', 'SQL', 'Python']",2025-06-12 13:53:39
Specialist Data Scientist,Atlasrtx,3 - 7 years,Not Disclosed,['Pune'],"So, what s the role all about\n\nNICE provides state-of-the-art enterprise level AI and analytics for all forms of business communications between speech and digital. We are a world class research team developing new algorithms and approaches to help companies with solving critical issues such as identifying their best performing agents, preventing fraud, categorizing customer issues, and determining overall customer satisfaction. If you have interacted with a major contact center in the last decade, it is very likely we have processed your call.\n\nThe research group partners with all areas of NICE s business to scale out the delivery of new technology and AI models to customers around the world that are tailored to their company, industry, and language needs.\n\n\nHow will you make an impact\n\nConduct cutting-edge research and develop advanced NLP algorithms and models.\n\nBuild and fine-tune deep learning and machine learning models, with a focus on large language models.\n\nWork closely with internal stakeholders to define model requirements and ensure alignment with business objectives.\n\nDevelop AI predictive models and perform data and model accuracy analyses.\n\nProduce and present findings, technical concepts, and model recommendations to both technical and non-technical stakeholders.\n\nDevelop and maintain scripts/tools to automate both new model production and updates to existing model packages.\n\nStay abreast of the latest advancements in data science research and contribute to the development of our knowledge base.\n\nCollaborate with developers to design automation and tool improvements for model building.\n\nMaintain documentation of processes and projects across all supported languages and environments.\n\n\nHave you got what it takes\n\nMasters degree in the field of Computer Science, Technology, Engineering, Math, or equivalent practical experience\n\nMinimum of 8 years of data science work experience, including implementing machine learning and NLP models using real-life data.\n\nExperience with Retrieval-Augmented Generation (RAG) pipelines or LLMOps.\n\nAdvanced knowledge of statistics and machine learning algorithms.\n\nProficiency in Python programming and familiarity with R.\n\nExperience with deep learning models and libraries such as PyTorch, TensorFlow, and JAX.\n\nFamiliarity with relational databases and query languages (e. g. , MSSQL) and basic SQL knowledge.\n\nHands-on experience with transformer models (BERT, FlanT5, Llama, etc. ) and GenAI frameworks (HuggingFace, LangChain, Ollama, etc. ).\n\nExperience deploying NLP models in production environments, ensuring scalability and performance using AWS/GCP/Azure\n\nStrong verbal and written communication skills, including effective presentation abilities.\n\nAbility to work independently and as part of a team, demonstrating analytical thinking and problem-solving skills.\n\n\n\nYou will have an advantage if you also have:\n\nExpertise with Big Data technologies (e. g. , PySpark).\n\nBackground in knowledge graphs, graph databases, or GraphRAG architectures.\n\nUnderstanding of multimodal models (text, audio, vision).\n\nExperience in Customer Experience domains.\n\nExperience with package development and technical writing.\n\nFamiliarity with tools like Jira, Confluence, and source control packages and methodology.\n\nKnowledge and interest in foreign languages and linguistics.\n\nExperience working on international, globe-spanning teams and with AWS.\n\nPast participation in a formal research setting.\n\nExperience as part of a software organization.\n\n\n\nWhat s in it for you\n\n\n\nEnjoy NICE-FLEX!\n\n\n\nRequisition ID : 7481\nReporting into : Tech Manager\nRole Type : Individual Contributor\n\nAbout NICE",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Automation', 'Technical writing', 'GCP', 'Analytical', 'Machine learning', 'Flex', 'Analytics', 'SQL', 'Python']",2025-06-12 13:53:41
Lead ML Ops Engineer with GCP,TVS Next,8 - 10 years,Not Disclosed,['Bengaluru'],"What you’ll be doing:\nAssist in developing machine learning models based on project requirements\nWork with datasets by preprocessing, selecting appropriate data representations, and ensuring data quality.\nPerforming statistical analysis and fine-tuning using test results.\nSupport training and retraining of ML systems as needed.\nHelp build data pipelines for collecting and processing data efficiently.",,,,"['hive', 'kubernetes', 'data pipeline', 'sql', 'docker', 'tensorflow', 'java', 'product management', 'gcp', 'spark', 'pytorch', 'bigquery', 'hadoop', 'big data', 'programming', 'hbase', 'ml', 'cloud sql', 'python', 'airflow', 'cloud spanner', 'cloud pubsub', 'machine learning', 'data engineering', 'ops', 'mapreduce', 'kafka', 'cloud storage', 'hdfs', 'bigtable', 'aws']",2025-06-12 13:53:43
Quality Assurance - ETL QA Engineer - Lead,Kearney-Cervello,6 - 10 years,Not Disclosed,['Bengaluru'],"About the Role:\nAs a Lead ETL QA Engineer, you will drive the QA strategy and execution for a major data pipeline modernization initiative on Azure and Snowflake. This role requires a deep understanding of data quality frameworks, test planning, and stakeholder engagement. The candidate should possess leadership capabilities and be hands-on with SQL and automation.\n\nThe job responsibilities are as follows:",,,,"['Azure Data Factory', 'Quality Assurance', 'ETL Testing', 'SQL', 'Database Testing', 'Sql Database Testing']",2025-06-12 13:53:46
Senior JavaScript Software Engineer,Ciklum,6 - 10 years,Not Disclosed,"['Pune', 'Chennai']","Ciklum is looking for a Senior JavaScript Software Engineer to join our team full-time in India.\nWe are a custom product engineering company that supports both multinational organizations and scaling startups to solve their most complex business challenges. With a global team of over 4,000 highly skilled developers, consultants, analysts and product owners, we engineer technology that redefines industries and shapes the way people live.\n\nAbout the role:\nAs a Senior JavaScript Software Engineer, become a part of a cross-functional development team engineering experiences of tomorrow.\nClient for this project is a leading global provider of audit and assurance, consulting, financial advisory, risk advisory, tax, and related services. They are launching a digital transformation project to evaluate existing technology across the tax lifecycle and determine the best future state for that technology. This will include decomposing existing assets to determine functionality, assessment of those functionalities to determine the appropriate end state and building of new technologies to replace those functionalities.\n\nResponsibilities:\nParticipate in requirements analysis\nCollaborate with US and Vendors teams to produce software design and architecture\nWrite clean, scalable code using Angular with Typescript, HTML, CSS, and NET programming languages\nParticipate in pull request code review process\nTest and deploy applications and systems\nRevise, update, refactor and debug code\nDevelop, support and maintain applications and technology solutions\nEnsure that all development efforts meet or exceed client expectations. Applications should meet requirements of scope, functionality, and time and adhere to all defined and agreed upon standards\nBecome familiar with all development tools, testing tools, methodologies and processes\nBecome familiar with the project management methodology and processes\nEncourage collaborative efforts and camaraderie with on-shore and off-shore team members\nDemonstrate a strong working understanding of the best industry standards in software development and version controlling\nEnsure the quality and low bug rates of code released into production\nWork on agile projects, participate in daily SCRUM calls and provide task updates\nDuring design and key development phases, we might need to work a staggered shift as applicable to ensure appropriate overlap with US teams and project deliveries\n\nRequirements:\n  We know that sometimes, you cant tick every box. We would still love to hear from you if you think you will be a good fit\n6+ years of strong hands-on experience with JavaScript (ES6/ES2015+), HTML5, CSS3\n2+ years with hands-on experience with Typescript\n2+ years of hands-on experience with Angular 11+ component architecture, applying design patterns\nExperience with Angular 11+ and migrating to newer versions\nExperience with Angular State management or NgXs\nExperience with RxJS operators\nHands on experience with Kendo UI or Angular material or SpreadJS libraries\nExperience with Nx – Nrwl/Nx library for monorepos\nSkill for writing reusable components, Angular services, directives and pipes\nHands-on experience on C#, SQL Server, OOPS Concepts, Micro Services Architecture\nAt least two-year hands-on experience on .NET Core, ASP.NET Core Web API, SQL, NoSQL, Entity Framework 6 or above, Azure, Database performance tuning, Applying Design Patterns, Agile\n.Net back-end development with data engineering expertise. Experience with MS Fabric as a data platform/ Snowflake or similar tools would be a plus, but not a must need\nSkill for writing reusable libraries\nComfortable with Git & Git hooks using PowerShell, Terminal or a variation thereof\nFamiliarity with agile development methodologies\nExcellent Communication skills both oral & written\nExcellent troubleshooting and communication skills, ability to communicate clearly with US counterparts\n\nDesirable:\nExposure to micro-frontend architecture\nKnowledge on Yarn, Webpack, Mongo DB, NPM, Azure Devops Build/Release configuration\nSignalR, ASP.NET Core and WebSockets\nThis is an experienced level position, and we will train the qualified candidate in the required applications\nWillingness to work extra hours to meet deliverables\nExposure to Application Insights & Adobe Analytics\nUnderstanding of cloud infrastructure design and implementation\nExperience in CI/CD configuration\nGood knowledge of data analysis in enterprises\nExperience with Databricks, Snowflake\nExposure to Docker and its configurations, Experience with Kubernetes\n\nWhat's in it for you?\nCare: your mental and physical health is our priority. We ensure comprehensive company-paid medical insurance, as well as financial and legal consultation\nTailored education path: boost your skills and knowledge with our regular internal events (meetups, conferences, workshops), Udemy licence, language courses and company-paid certifications\nGrowth environment: share your experience and level up your expertise with a community of skilled professionals, locally and globally\nFlexibility: hybrid work mode at Chennai or Pune \nOpportunities: we value our specialists and always find the best options for them. Our Resourcing Team helps change a project if needed to help you grow, excel professionally and fulfil your potential\nGlobal impact: work on large-scale projects that redefine industries with international and fast-growing clients\nWelcoming environment: feel empowered with a friendly team, open-door policy, informal atmosphere within the company and regular team-building events\n\nAbout us:\nAt Ciklum, we are always exploring innovations, empowering each other to achieve more, and engineering solutions that matter. With us, you’ll work with cutting-edge technologies, contribute to impactful projects, and be part of a One Team culture that values collaboration and progress.\nIndia is a strategic innovation hub for Ciklum, with growing teams in Chennai and Pune leading advancements in EdgeTech, AR/VR, IoT, and beyond. Join us to collaborate on game-changing solutions and take your career to the next level.\nWant to learn more about us? Follow us on Instagram, Facebook, LinkedIn\n\nExplore, empower, engineer with Ciklum!\nExperiences of tomorrow. Engineered together\nInterested already?\nWe would love to get to know you! Submit your application. Can’t wait to see you at Ciklum.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Typescript', 'Angular', 'CSS', '.Net', 'HTML']",2025-06-12 13:53:48
Sr Software Engineer: Integration Engineer,HMH,5 - 7 years,Not Disclosed,['Pune'],"The Data Integration Engineer will play a key role in designing, building, and maintaining data integrations between core business systems such as Salesforce and SAP and our enterprise data warehouse on Snowflake. This position is ideal for an early-career professional (1 to 4 years of experience) eager to contribute to transformative data integration initiatives and learn in a collaborative, fast-paced environment.\n\nDuties & Responsibilities:\nCollaborate with cross-functional teams to understand business requirements and translate them into data integration solutions.\nDevelop and maintain ETL/ELT pipelines using modern tools like Informatica IDMC to connect source systems to Snowflake.\nEnsure data accuracy, consistency, and security in all integration workflows.\nMonitor, troubleshoot, and optimize data integration processes to meet performance and scalability goals.\nSupport ongoing integration projects, including Salesforce and SAP data pipelines, while adhering to best practices in data governance.\nDocument integration designs, workflows, and operational processes for effective knowledge sharing.\nAssist in implementing and improving data quality controls at the start of processes to ensure reliable outcomes.\nStay informed about the latest developments in integration technologies and contribute to team learning and improvement.",,,,"['GCP', 'Azure', 'IDMC', 'XML', 'CSV', 'JSON', 'SQL Server', 'AWS', 'data integration', 'data engineering']",2025-06-12 13:53:51
Data Model Architect,Prodapt Solutions,6 - 10 years,Not Disclosed,['Chennai'],"Overview\n\nProdapt is looking for a Data Model Architect. The candidate should be good with design and data architecture in Telecom domain.    \n\n\nResponsibilities\n\n Deliverables  \nDesign & Document – Data Model for CMDB, P-S-R Catalogue (Product, Service and Resource management layer)\nDesign & Document Build Interface Speciation for Data Integration.\n\n Activities  \n\n Data Architecture and Modeling  \nDesign and maintain conceptual, logical, and physical data models\nEnsure scalability and adaptability of data models for future organizational needs.\n\n Data Model   P-S-R catalogs   in the existing Catalogs,SOM,COM systems\n\n CMDB Design and Management  \nArchitect and optimize the CMDB to accurately reflect infrastructure components, telecom assets, and their relationships.\nDefine data governance standards and enforce data consistency across the CMDB.\n\n Design    data integrations   between across systems (e.g., OSS/BSS, network monitoring tools, billing systems).\n\n\nGood Communication skills.\n\nBachelors Degree.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data warehousing', 'data architecture', 'sql', 'data governance', 'data integration', 'python', 'oracle', 'performance tuning', 'power bi', 'microsoft azure', 'erwin', 'business intelligence', 'sql server', 'plsql', 'data quality', 'tableau', 'data modeling', 'dwbi', 'etl', 'ssis', 'aws', 'informatica', 'unix']",2025-06-12 13:53:53
Senior High Performance Computing Engineer,Amgen Inc,4 - 6 years,Not Disclosed,['Hyderabad'],"What you will do\nIn this vital role you will.\nRole Description:\nThe role is responsible for the design, integration, and management of high performance computing (HPC) systems that encompass both hardware and software components into the organizations network infrastructure. This individual will be responsible for all activities related to handling and supporting the Business and platforms including system administration, as well as incorporating new technologies under the challenge of a sophisticated and constantly evolving technology landscape. This role involves ensuring that all parts of a system work together seamlessly to meet the organizations requirements.\nRoles & Responsibilities:\nImplement, and manage cloud-based infrastructure that supports HPC environments that support data science (e.g. AI/ML workflows, Image Analysis).\nCollaborate with data scientists and ML engineers to deploy scalable machine learning models into production.\nEnsure the security, scalability, and reliability of HPC systems in the cloud.\nOptimize cloud resources for cost-effective and efficient use.\nKeep abreast of the latest in cloud services and industry standard processes.\nProvide technical leadership and guidance in cloud and HPC systems management.\nDevelop and maintain CI/CD pipelines for deploying resources to multi-cloud environments.\nMonitor and fix cluster operations/applications and cloud environments.\nDocument system design and operational procedures.\nBasic Qualifications:\nMasters degree with a 4 - 6 years of experience in Computer Science, IT or related field with hands-on HPC administration OR\nBachelors degree with 6 - 8 years of experience in Computer Science, IT or related field with hands-on HPC administration OR\nDiploma with 10-12 years of experience in Computer Science, IT or related field with hands-on HPC administration\nDemonstrable experience in cloud computing (preferably AWS) and cloud architecture.\nExperience with containerization technologies (Singularity, Docker) and cloud-based HPC solutions.\nExperience with infrastructure-as-code (IaC) tools such as Terraform, CloudFormation, Packer, Ansible and Git.\nExpert with scripting (Python or Bash) and Linux/Unix system administration (preferably Red Hat or Ubuntu).\nProficiency with job scheduling and resource management tools (SLURM, PBS, LSF, etc.).\nKnowledge of storage architectures and distributed file systems (Lustre, GPFS, Ceph).\nUnderstanding of networking architecture and security best practices.\nPreferred Qualifications:\nExperience supporting research in healthcare life sciences.\nExperience with Kubernetes (EKS) and service mesh architectures.\nKnowledge of AWS Lambda and event-driven architectures.\nExposure to multi-cloud environments (Azure, GCP).\nFamiliarity with machine learning frameworks (TensorFlow, PyTorch) and data pipelines.\nCertifications in cloud architecture (AWS Certified Solutions Architect, Google Cloud Professional Cloud Architect, etc.).\nExperience in an Agile development environment.\nPrior work with distributed computing and big data technologies (Hadoop, Spark).\nProfessional Certifications (please mention if the certification is preferred or mandatory for the role):\nRed Hat Certified Engineer (RHCE) or Linux Professional Institute Certification (LPIC)\nAWS Certified Solutions Architect Associate or Professional\nSoft Skills:\nStrong analytical and problem-solving skills.\nAbility to work effectively with global, virtual teams\nEffective communication and collaboration with cross-functional teams.\nAbility to work in a fast-paced, cloud-first environment.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['cloud computing', 'resource management', 'Ubuntu', 'Unix system administration', 'linux', 'unix production support', 'Python']",2025-06-12 13:53:56
Machine Learning Engineer,Avani Infosoft,1 - 2 years,2.4-6.6 Lacs P.A.,['Bengaluru( Kamakshipalya )'],"Responsibilities:\n* Develop machine learning models using TensorFlow, NumPy & OpenCV.\n* Implement computer vision solutions with CNNs & object detection techniques.\n\n\nProvident fund",Industry Type: BPM / BPO,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Handling', 'Object Detection', 'Opencv', 'Deployment', 'Model Development', 'Tensorflow', 'Cnn', 'Computer Vision', 'Machine Learning', 'Numpy', 'Deep Learning']",2025-06-12 13:53:58
Engineer,Reltio,2 - 7 years,Not Disclosed,['Bengaluru'],"Job Title: Engineer\nLocation: Bengaluru, India - (Hybrid)\nAt Reltio , we believe data should fuel business success. Reltio s AI-powered data unification and management capabilities encompassing entity resolution, multi-domain master data management (MDM), and data products transform siloed data from disparate sources into unified, trusted, and interoperable data. Reltio Data Cloud delivers interoperable data where and when its needed, empowering data and analytics leaders with unparalleled business responsiveness. Leading enterprise brands across multiple industries around the globe rely on our award-winning data unification and cloud-native MDM capabilities to improve efficiency, manage risk and drive growth.\nAt Reltio, our values guide everything we do. With an unyielding commitment to prioritizing our Customer First , we strive to ensure their success. We embrace our differences and are Better Together as One Reltio. We are always looking to Simplify and Share our knowledge when we collaborate to remove obstacles for each other. We hold ourselves accountable for our actions and outcomes and strive for excellence. We Own It . Every day, we innovate and evolve, so that today is Always Better Than Yesterday . If you share and embody these values, we invite you to join our team at Reltio and contribute to our mission of excellence.\nReltio has earned numerous awards and top rankings for our technology, our culture and our people. Reltio was founded on a distributed workforce and offers flexible work arrangements to help our people manage their personal and professional lives. If you re ready to work on unrivaled technology where your desire to be part of a collaborative team is met with a laser-focused mission to enable digital transformation with connected data, let s talk!\nJob Summary:\nCore Platform development is spread across multiple cross-functional teams, each building large and complex components of the MDM platform. Engineer plays senior role in a team is technically responsible for particular feature delivery which consists of:\nPresenting a solution to Architects\nTechnical drive feature delivery, make decisions, development leadership - distribute tasks across one or two regular engineers\nWork with QA to review testing approaches, provide all details needed for testing, and review test plans\nBe responsible for all internal feature documentation\nJob Duties and Responsibilities:\nWork closely with Tech leads and architects, Kanban master, QA and PM to deliver high-quality work\nComplex features development\nProvide good code coverage in unit and integration testing\nBe ready for every day technical decisions\nProfessional communication with other cross-functional team representatives\nBe ready for sustenance in areas under engineers responsibility\nSkills You Must Have:\n2+ years of experience in enterprise application design, development\n2+ Experience in backend applications development\nProven ability to deliver solutions, offer implementation support, and engage with customers.\n1+ years of experience in building scalable distributed data systems using Java and Cloud Technology.\nGood technical background with previous development experience for enterprise customers\nStrong Java knowledge (Java 8 and above), Java Runtime basics\nProvide Technical mentorship and leadership to junior team members.\nHands-on technically, comfortable reviewing and writing code.\nSolid foundation in computer science, with strong competencies in algorithms, data structures, software design, and building large, distributed systems\nExperience in performance optimisations, code profiling, and Java application runtime analysis\nSkills That Are Nice to Have:\nExperience in customer support in the SaaS area\nExperience in building scalable distributed data systems using Cloud Technology\nTechnical background with previous development experience for enterprise customers\nExperience with Big Data Technologies\nExperience with No-SQL databases\nExperience with cloud-managed services - AWS, GBT, Azure\nMS or equivalent experience\nWhy Join Reltio?*\nHealth & Wellness:\nComprehensive Group medical insurance including your parents with additional top-up options.\nAccidental Insurance\nLife insurance\nFree online unlimited doctor consultations\nAn Employee Assistance Program (EAP)\nWork-Life Balance:\n36 annual leaves, which includes Sick leaves - 18, Earned Leaves - 18\n26 weeks of maternity leave, 15 days of paternity leave\nVery unique to Reltio - 01 week of additional off as recharge week every year globally\nSupport for home office setup:\nHome office setup allowance.\nStay Connected, Work Flexibly: Mobile & Internet Reimbursement\nNo need to pack a lunch we ve got you covered with a free meal.And many more ..",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Software design', 'Backend', 'Managed services', 'Integration testing', 'Data structures', 'Customer support', 'Distribution system', 'Analytics', 'SQL']",2025-06-12 13:54:00
Sales Engineer - CLOUD Products ( Cochin Location ),Redington Group,0 - 2 years,2.75-6 Lacs P.A.,['Kochi'],Role & responsibilities\n\nTO Handle field sales ( CLOUD PRODUCTS )\n\n1. Primarily a hunter and hustler personality with 0-5 years of experience in SME & Enterprise Segment. Strong enterprise sales background in solutions / SaaS space ideally with knowledge of BI / analytics / Big data / AI/Azure /AWS.,,,,"['Cloud Technologies', 'Sales Engineering', 'Cloud Applications', 'Technical Sales']",2025-06-12 13:54:03
Associate Software Engineer – Biological Studies ( In Vivo ),Amgen Inc,0 - 3 years,Not Disclosed,['Hyderabad'],"In this vital role you will join a multi-functional team of scientists and software professionals that enables technology and data capabilities to evaluate drug candidates and assess their abilities to affect the biology of drug targets.\nThis team implements scientific software platforms that enable the capture, analysis, storage, and report of in vitro assays and in vivo / pre-clinical studies as well as those that manage compound inventories / biological sample banks. The ideal candidate possesses experience in the pharmaceutical or biotech industry, strong technical skills, and full stack software engineering experience (spanning SQL, back-end, front-end web technologies, automated testing).\nRoles & Responsibilities:\nDesign, develop, and implement applications and modules, including custom reports, interfaces, and enhancements\nAnalyze and understand the functional and technical requirements of applications, solutions and systems and translate them into software architecture and design specifications\nDevelop and execute unit tests, integration tests, and other testing strategies to ensure the quality of the software\nIdentify and resolve software bugs and performance issues\nWork closely with cross-functional teams, including product management, design, and QA, to deliver high-quality software on time\nMaintain documentation of software designs, code, and development processes\nCustomize modules to meet specific business requirements\nWork on integrating with other systems and platforms to ensure seamless data flow and functionality\nProvide ongoing support and maintenance for applications, ensuring that they operate smoothly and efficiently\nContribute to both front-end and back-end development using cloud technology\nDevelop innovative solution using generative AI technologies\nIdentify and resolve technical challenges effectively\nWork closely with product team, business team including scientists, and other stakeholders\nWhat we expect of you\nWe are all different, yet we all use our unique contributions to serve patients. The [vital attribute] professional we seek is a [type of person] with these qualifications.\nBasic Qualifications:\nBachelors degree and 0 to 3 years of experience in Computer Science, IT, Computational Chemistry, Computational Biology/Bioinformatics or related field OR\nDiploma and 4 to 7 years of experience in Computer Science, IT, Computational Chemistry, Computational Biology/Bioinformatics or related field\nPreferred Qualifications:\nExperience in implementing and supporting biopharma scientific software platforms\nFunctional Skills:\nMust-Have Skills:\nProficient in a General Purpose High Level Language (e.g. Python, Java, C#.NET)\nProficient in a Javascript UI Framework (e.g. React, ExtJs)\nProficient in a SQL (e.g. Oracle, PostGres, Databricks)\nExperience with event-based architecture (e.g. Mulesoft, AWS EventBridge, AWS Kinesis, Kafka)\nGood-to-Have Skills:\nStrong understanding of software development methodologies, mainly Agile and Scrum\nHands-on experience with Full Stack software development\nStrong understanding of cloud platforms (e.g AWS) and containerization technologies (e.g., Docker, Kubernetes)\nWorking experience with DevOps practices and CI/CD pipelines\nExperience with big data technologies (e.g., Spark, Databricks)\nExperience with API integration, serverless, microservices architecture (e.g. Mulesoft, AWS Kafka)\nExperience with monitoring and logging tools (e.g., Prometheus, Grafana, Splunk)\nExperience of infrastructure as code (IaC) tools (Terraform, CloudFormation)\nExperience with version control systems like Git\nExperience with automated testing tools and frameworks\nExperience with Benchling\nProfessional Certifications (please mention if the certification is preferred or mandatory for the role):\nAWS Certified Cloud Practitioner preferred\nSoft Skills:\nExcellent problem solving, analytical, and troubleshooting skills\nStrong communication and interpersonal skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation\nAbility to learn quickly & work independently\nTeam-oriented, with a focus on achieving team goals\nAbility to manage multiple priorities successfully\nStrong presentation and public speaking skills.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Python', 'C#', 'Java', 'JavaScript', 'Kafka', '.NET', 'React', 'ExtJs', 'SQL', 'Mulesoft', 'AWS EventBridge', 'AWS Kinesis']",2025-06-12 13:54:06
Database Engineer-Architect 7+ Years C2H,Greetings from BCforward INDIA TECHNOLOG...,7 - 12 years,15-30 Lacs P.A.,"['Bangalore Rural', 'Bengaluru']","Greetings from BCforward INDIA TECHNOLOGIES PRIVATE LIMITED.\n\nContract To Hire(C2H) Role\nLocation: Bangalore\nPayroll: BCforward\nWork Mode: Hybrid\n\nJD\n\nPreferred Skills: 5+ years relevant experience into Database Engineer-Architect(Postgres / Postgresql; Oracle; Linux)\n\nDescription:\nSkills: Postgres / Postgresql; Oracle; Linux\n\nDesirable attributes\nSkill at the Unix command line\nThe ability to write code or script, e.g. for test harnesses or other database-related tools and utilities\nExperience as a database administrator and/or a Unix system administrator\nKnowledge of virtualization and cloud infrastructures, and of implementations such as VMWare, OpenShift, Kubernetes and Docker\nKnowledge of AWS, Google Cloud, Azure or other public cloud offerings\nKnowledge of modern storage and compute technologies, including hyper-converged infrastructures\nBachelors degree preferred.\n\nPlease share your Updated Resume, PAN card soft copy, Passport size Photo & UAN History.\n\nInterested applicants can share updated resume to g.sreekanth@bcforward.com\n\nNote: Looking for Immediate to 15-Days joiners at most.\n\nAll the best",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Database Engineer', 'Postgresql', 'Oracle', 'database Architect', 'Linux', 'Unix command']",2025-06-12 13:54:08
Onix is Hiring Hadoop GCP Engineers!!!,Datametica,4 - 8 years,Not Disclosed,"['Pune', 'Bengaluru']","We are looking for skilled Hadoop and Google Cloud Platform (GCP) Engineers to join our dynamic team. If you have hands-on experience with Big Data technologies and cloud ecosystems, we want to hear from you!\nKey Skills:\nHadoop Ecosystem (HDFS, MapReduce, YARN, Hive, Spark)\nGoogle Cloud Platform (BigQuery, DataProc, Cloud Composer)\nData Ingestion & ETL pipelines\nStrong programming skills (Java, Python, Scala)\nExperience with real-time data processing (Kafka, Spark Streaming)\nWhy Join Us?\nWork on cutting-edge Big Data projects\nCollaborate with a passionate and innovative team\nOpportunities for growth and learning\nInterested candidates, please share your updated resume or connect with us directly!",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['gcp', 'Big Data', 'pyspark', 'Hive', 'Sqoop', 'mapreduce', 'Bigquery', 'Hadoop', 'Spark', 'YARN', 'pig', 'bigq']",2025-06-12 13:54:10
Security Engineer II,Tekion Corp,1 - 5 years,Not Disclosed,['Bengaluru'],"About Tekion:\nPositively disrupting an industry that has not seen any innovation in over 50 years, Tekion has challenged the paradigm with the first and fastest cloud-native automotive platform that includes the revolutionary Automotive Retail Cloud (ARC) for retailers, Automotive Enterprise Cloud (AEC) for manufacturers and other large automotive enterprises and Automotive Partner Cloud (APC) for technology and industry partners. Tekion connects the entire spectrum of the automotive retail ecosystem through one seamless platform. The transformative platform uses cutting-edge technology, big data, machine learning, and AI to seamlessly bring together OEMs, retailers/dealers and consumers. With its highly configurable integration and greater customer engagement capabilities, Tekion is enabling the best automotive retail experiences ever. Tekion employs close to 3,000 people across North America, Asia and Europe.",,,,"['Patch management', 'Computer science', 'Automation', 'Coding', 'SOC', 'Machine learning', 'Vulnerability', 'Information technology', 'Automotive', 'Python']",2025-06-12 13:54:13
Senior Copy Writer- WFH,Aegis Softtech,6 - 7 years,6.5-9 Lacs P.A.,"['Hyderabad', 'Chennai', 'Bengaluru']","Immediate Openings for Senior Copy Writer (Permanent WFH)\n\nCopywriter - Job Description\n\nAbout Aegis :\nAegis Softtech is a global technology services firm delivering customized software solutions in AI, ML, Cloud, Data Engineering, CRM Consulting, and more. We work with tech leaders, enterprises, and fast-scaling startups to help them solve business problems with scalable, future-ready software.\n\nWe are building a lean, quality-first content team.\nIf you're a copywriter who knows how to write for humans, doesn't hide behind jargon, and loves shaping complex tech stories into simple, compelling narratives, lets work together.\n\nWhat Youll Do:\n1.Craft compelling, conversion-driven content across formats: website pages, landing pages, email copy, social posts, and ad creatives\n2. Own copy development for key service areas like AI/ML, Data & Cloud, and CRM solutions (Microsoft Dynamics, Salesforce, etc.)\n3. Translate technical inputs into benefit-focused, client-first messaging that aligns with our authoritative yet approachable voice\n4.Collaborate with the Content Lead, designers, and developers to align messaging across touchpoints.\n5. Edit and refine content for clarity, brevity, tone, and SEO, without diluting meaning.\n6.Bring consistency to brand tone across different regions and verticals.\n\nWhat Were Looking For\n> 5–8 years of proven experience as a copywriter, ideally in the B2B tech or SaaS industry.\n> Comfort working across multiple formats and content lengths—from short CTAs to full-service pages.\n> A storytelling mindset with a keen understanding of buyer psychology and content structure.\n> Strong grasp of SEO principles and how to write for both humans and search engines.\n> Ability to simplify complex tech ideas without dumbing them down.\n> Self-driven, collaborative, and comfortable with remote async work.\n\nWhy Join Us\n> Flexible remote work with a global team of tech thinkers and builders\n> A chance to influence messaging at a strategic level, not just execute briefs\n> Open and transparent communication culture\n> Opportunity to work closely with a Lead who values content quality as much as delivery speed.\n\nTo Apply:\nSend your resume, a short note about why this role speaks to you, and 2–3 samples that show your ability to:\nTranslate tech to value.\nBuild momentum with words.\nWrite with clarity and character.\nEmail your application to hr@aegissofttech.com with the subject line: Copywriter Application – [Your Name].",Industry Type: IT Services & Consulting,"Department: Content, Editorial & Journalism","Employment Type: Full Time, Permanent","['SEO Writing', 'technical content', 'copy writing', 'Content Writing', 'Content Strategy']",2025-06-12 13:54:15
Sr Associate Software Engineer,Horizon Therapeutics,1 - 5 years,Not Disclosed,['Hyderabad'],"Career Category Information Systems Job Description\nRole Description:\nThe role is responsible for designing, developing, and maintaining software solutions for Research scientists . Additionally, it involves automating operations, monitoring system health, and responding to incidents to minimize downtime. Y ou will join a multi-functional team of scientists and software professionals that enables technology and data capabilities to evaluate drug candidates and assess their abilities to affect the biology of drug targets. This team implements scientific software platforms that enable the capture, analysis, storage, and report ing for our Large Molecule Discovery Research team (Design, Make, Test and Analyze processes) . The team also interfaces heavily with teams supporting our in vi tro assay management systems and our compound inventory platforms . The ideal candidate possesses experience in the pharmaceutical or biotech industry, strong technical skills, and full stack software engineering experience (spanning SQL, back-end, front-end web technologies, automated testing ).\nRoles Responsibilities:\nWork closely with product team, business team including scientists, and other stakeholders\nAnalyze and understand the functional and technical requirements of applications, solutions and systems and translate them into software architecture and design specifications\nDesign, develop, and implement applications and modules, including custom reports, interfaces, and enhancements\nDevelop and execute unit tests, integration tests, and other testing strategies to ensure the quality of the software\nConduct code reviews to ensure code quality and adherence to best practices\nCreate and maintain documentation on software architecture, design, deployment, disaster recovery, and operations\nProvide ongoing support and maintenance for applications, ensuring that they operate smoothly and efficiently\nStay updated with the latest technology and security trends and advancements\nBasic Qualifications and Experience:\nMaster s degree with 1 - 3 years of experience in Computer Science, IT , Computational Chemistry, Computational Biology/ Bioinformatics or related field OR\nBachelor s degree with 4 - 6 years of experience in Computer Science, IT , Computational Chemistry, Computational Biology/ Bioinformatics or related field OR\nDiploma with 7 - 9 years of experience in Computer Science, IT , Computational Chemistry, Computational Biology/ Bioinformatics or related field\nPreferred Qualifications and Experience:\n1 + years of experience in implementing and supporting biopharma scientific software platforms\nFunctional Skills:\nMust-Have Skills :\nProficient in Java or Python\nProficient in at least one JavaScript UI Framework ( e. g. ExtJS , React, or Angular)\nProficient in SQL ( e. g. Oracle, PostgreSQL , Databricks)\nGood-to-Have Skills:\nExperience with event-based architecture and serverless AWS services such as EventBridge , SQS, Lambda or ECS.\nExperience with Benchling\nHands - on experience with Full Stack software development\nStrong understanding of software development methodologies, mainly Agile and Scrum\nWorking experience with DevOps practices and CI/CD pipelines\nExperience of infrastructure as code ( IaC ) tools (Terraform, CloudFormation)\nExperience with monitoring and logging tools (e. g. , Prometheus, Grafana, Splunk)\nExperience with automated testing tools and frameworks\nExperience with big data technologies (e. g. , Spark, Databricks, Kafka)\nExperience with leveraging the use of AI-assistants ( e. g. GitHub Copilot) to accelerate software development and improve code quality\nProfessional Certifications (please mention if the certification is preferred or mandatory for the role):\nAWS Certified Cloud Practitioner preferred\nSoft Skills:\nExcellent problem solving, analytical , and troubleshooting skills\nStrong communication and interpersonal skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation\nAbility to learn quickly work independently\nTeam-oriented, with a focus on achieving team goals\nAbility to manage multiple priorities successfully\nStrong presentation and public speaking skills\nEQUAL OPPORTUNITY STATEMENT\nAmgen is an Equal Opportunity employer and will consider you without regard to your race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, or disability status.\nWe will ensure that individuals with disabilities are provided with reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request an accommodation .\n.",Industry Type: Biotechnology,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Computational chemistry', 'Front end', 'Pharma', 'Javascript', 'Oracle', 'Monitoring', 'SQL', 'Python', 'Computational biology']",2025-06-12 13:54:17
Staff Engineer,Reltio,10 - 15 years,Not Disclosed,['Bengaluru'],"Job Title: Staff Engineer\nLocation: Bengaluru, India - (Hybrid)\nAt Reltio , we believe data should fuel business success. Reltio s AI-powered data unification and management capabilities encompassing entity resolution, multi-domain master data management (MDM), and data products transform siloed data from disparate sources into unified, trusted, and interoperable data. Reltio Data Cloud delivers interoperable data where and when its needed, empowering data and analytics leaders with unparalleled business responsiveness. Leading enterprise brands across multiple industries around the globe rely on our award-winning data unification and cloud-native MDM capabilities to improve efficiency, manage risk and drive growth.\nAt Reltio, our values guide everything we do. With an unyielding commitment to prioritizing our Customer First , we strive to ensure their success. We embrace our differences and are Better Together as One Reltio. We are always looking to Simplify and Share our knowledge when we collaborate to remove obstacles for each other. We hold ourselves accountable for our actions and outcomes and strive for excellence. We Own It . Every day, we innovate and evolve, so that today is Always Better Than Yesterday . If you share and embody these values, we invite you to join our team at Reltio and contribute to our mission of excellence.\nReltio has earned numerous awards and top rankings for our technology, our culture and our people. Reltio was founded on a distributed workforce and offers flexible work arrangements to help our people manage their personal and professional lives. If you re ready to work on unrivaled technology where your desire to be part of a collaborative team is met with a laser-focused mission to enable digital transformation with connected data, let s talk!\nJob Summary:\nCore Platform development is spread across multiple cross-functional teams, each building a large and complex component of the MDM platform. As a Senior Engineer in the team, you will be responsible for successful delivery of features in our areas of ownership, covering the following key aspects:\nWork with PMs to build a solid product roadmap with clear, well-defined goals and KPIs\nDrive the design and development of features, provide thought and execution leadership\nWork across functions with QA, DevOps & Doc teams to ensure a high-quality delivery\nPresent design for new initiatives in the global forum of architects\nJob Duties and Responsibilities:\nDeliver features with high quality, and be customer-focused.\nChampion engineering excellence, raise the bar on code coverage, coding guidelines, and review processes.\nProvide technical leadership and mentor junior team members.\nDeal with ambiguity in complex problems, show bias for action.\nWork closely with architects to actively shape the architecture of your work area and the broader product.\nWork closely with PM, QA & Doc teams to ensure a thorough testing strategy, robust deployment and lucid documentation of features.\nEffective communication with cross-functional teams and global stakeholders.\nSkills You Must Have:\nAn engineering degree in the computer science field\nSolid foundation in computer science, with strong competencies in algorithms, data structures, software design, and building large, distributed systems\n8+ years of experience in the design and development of products for enterprise customers.\n4+ years of experience in building large-scale distributed data systems in Java and cloud technologies.\nProven track record of building functionalities from conception to delivery, engaging with customers and partners to drive successful adoption and customer satisfaction.\nExperience in leading projects with a crew of engineers, providing technical mentorship.\nExperience building services on one of the 3 major cloud service providers: AWS, Azure or GCP, hands-on experience in managed cloud databases\nSkills That Are Nice to Have:\nExperience working with NoSQL databases like Cassandra, and queue services like SQS\nExperience in cloud security principles\nExperience with Kubernetes\nExperience with big data technologies\nExperience in driving customer focus for SaaS products is a big plus\nWhy Join Reltio?*\nHealth & Wellness:\nComprehensive Group medical insurance, including your parents, with additional top-up options.\nAccidental Insurance\nLife insurance\nFree online unlimited doctor consultations\nAn Employee Assistance Program (EAP)\nWork-Life Balance:\n36 annual leaves, which include Sick leaves - 18, Earned Leaves - 18\n26 weeks of maternity leave, 15 days of paternity leave\nVery unique to Reltio - 01 week of additional off as recharge week every year globally\nSupport for home office setup:\nHome office setup allowance.\nStay Connected, Work Flexibly: Mobile & Internet Reimbursement\nNo need to pack a lunch we ve got you covered with a free meal. And many more ..",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Software design', 'NoSQL', 'Coding', 'cassandra', 'GCP', 'Data structures', 'Medical insurance', 'Distribution system', 'Analytics']",2025-06-12 13:54:20
Senior Applied AI Scientist,ZS,4 - 9 years,Not Disclosed,['Bengaluru'],"Write complex SQL queries for data extraction, perform exploratory data analysis (EDA) to uncover insights.\nStrong proficiency in Python and Py Spark for scalable data processing and analytics.\nCreate, transform, and optimize features to enhance model performance.\nTrain, evaluate, and maintain machine learning models in production.\nWrite efficient, maintainable, and version-controlled code that handles large datasets.\nRegularly update internal teams and clients on project progress, results, and insights.\nConduct hypothesis testing and experiment analysis to drive data-driven decisions using AB testing.",,,,"['Data analysis', 'data security', 'Financial planning', 'Management consulting', 'Machine learning', 'Data processing', 'Analytics', 'Data extraction', 'Python']",2025-06-12 13:54:22
Senior Manager - Internal Audit,Flipkart,7 - 10 years,Not Disclosed,['Bengaluru'],Experience in internal audits with Big4 or Large organizations risk and control functions Big data Analytics skills Excellent communications and presentation skills Analytical bent of mind Experience in an e-commerce or retail industry (desirable) Ability to manage multiple audit projects,Industry Type: Courier / Logistics,Department: Finance & Accounting,"Employment Type: Full Time, Permanent","['Internal audit', 'internal controls', 'COSO', 'Big data Analytics', 'SOX', 'SQL']",2025-06-12 13:54:24
"GenAI Field Architect, Customer Value Engineering",Ema Unlimited,8 - 13 years,Not Disclosed,['Bengaluru'],"Who we are\nEma is building the world s largest agentic AI platform to revolutionize enterprise productivity. Our proprietary technology enables companies to delegate repetitive tasks to Ema, the Universal AI employee, unlocking 10x gains in workforce efficiency. Founded by ex-Google, Coinbase, Flipkart, and Okta executives, we bring deep product and scaling expertise to this mission. Our team includes top engineers from Google, Microsoft Research, Facebook, Square, and Coinbase, and alumni of Stanford, MIT, UC Berkeley, CMU, and IITs.\nWe ve raised over $60M from top-tier investors including Accel, Naspers/Prosus, Section32, SCB10X, Sozo Ventures, Hitachi Ventures, and prominent angels like Sheryl Sandberg, Jerry Yang, and Dustin Moskovitz. Ema is headquartered in Silicon Valley with a second hub in Bangalore, India. This is a hybrid role requiring in-office work three days per week.\nRole Overview\nEma is on a mission to redefine the future of work by building a Universal AI Employee that empowers every worker to delegate repetitive, high-value tasks to intelligent, agentic AI systems. As the Architect on our Customer Value Engineering (CVE) team, you ll play a pivotal role in translating this vision into real-world impact. From discovery workshops to production deployments, you will lead the design of scalable, secure, and observable AI-powered workflows turning customer ambitions into enterprise-grade solutions that deliver measurable value.\nRoles and Responsibilities:\nWorkflow Discovery & Solution Design: Lead discovery workshops to understand human workflows, pain points, data flows, and integration needs. Translate business objectives into AI architecture blueprints covering integration, data, security, and success metrics. Author and maintain AI-Employee design documents to guide implementation from blueprint to SLOs.\nData Integration & Action Building: Use declarative connectors and APIs to ingest, normalize, and secure data from enterprise systems (CRM, ERP, ATS, etc.). Build and maintain reusable action blocks using REST/SOAP/RPA and integrate them into agent-based workflows.\nAgentic Reasoning & Prompt Engineering: Design and compose reasoning agents using modern frameworks (e.g., LangChain, Semantic Kernel). Tune prompts, set up memory management, and define mesh topologies to ensure robust agentic reasoning. Own prompt evaluation, experiment design (A/A, A/B), and rollback strategies.\nHuman-AI Interaction: Define and implement user interaction flows by integrating with Slack, Microsoft Teams, widgets, or customer-facing apps. Ensure seamless handoff between automated and human-in-the-loop experiences.\nMonitoring, Metrics & Observability: Define success metrics, KPIs, and SLOs; wire them into dashboards, alerts, and observability systems. Steward observability packs, including Terraform/Helm configurations and alerting strategies.\nSecurity, Identity & Permissions: Enforce zero-trust identity patterns including SSO (Okta/AD) and RBAC. Own auditing, access control, and compliance posture across deployments.\nCollaboration & Continuous Improvement: Lead technical, architectural, and security reviews with internal teams and customer stakeholders. Partner with CVEs and system integrators to unblock issues and ensure deployment success. Monitor production system health and performance, lead continuous-improvement sprints, and codify learnings into reusable playbooks and runbooks. Maintain registries of versioned prompts, connectors, and action templates to support scale and reuse. Channel field insights to Product and ML teams to refine roadmap and platform capabilities.\nQualifications\n8+ years in software/solutions architecture, including 3+ years with LLM or event-driven systems\nExpert in Python, REST/JSON APIs, and cloud infrastructure (GCP, AWS, or Azure)\nProven record deploying AI systems in enterprise environments\nFamiliarity with agent frameworks like LangChain, Semantic Kernel, or similar\nExperience integrating SaaS platforms like CRM, ATS, ERP\nStrong understanding of RBAC, SSO (Okta/AD), and security standards\nOutstanding communication and executive presence in high-stakes environments\nPreferred Qualifications\nExperience with SQL optimization, graph DBs, or real-time processing\nHands-on knowledge of Terraform, Helm, and infrastructure-as-code\nPrior work in FedRAMP, HIPAA, or similar compliance environments\nOpen-source contributions in AI infrastructure or observability domains\nSoft Skills:\nOwnership & Accountability : You drive end-to-end outcomes, not just deliverables\nCuriosity & Learning : You seek emerging tools and best practices in GenAI\nCollaboration : You thrive at the intersection of product, engineering, and customer success\nEmpathy & Clarity : You simplify the complex and build trust through communication",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['ERP', 'Product engineering', 'Workflow', 'JSON', 'microsoft', 'Open source', 'Monitoring', 'SQL', 'CRM', 'Python']",2025-06-12 13:54:27
Infrastructure Automation Engineer – ServiceNow,Amgen Inc,0 - 2 years,Not Disclosed,['Hyderabad'],"What you will do\nIn this vital role you will be responsible for developing innovative self-service solutions for our global workforce and further enhancing our self-service automation built on the ServiceNow platform.\nAs part of a scaled Agile product delivery team, the Developer works closely with product feature owners, project stakeholders, operational support teams, peer developers and testers to develop solutions to enhance self-service capabilities and solve business problems by identifying requirements, conducting feasibility analysis, proof of concepts and design sessions.\nThe Developer serves as a subject matter expert on the design, integration and operability of solutions to support innovation initiatives with business partners and shared services technology teams.\nThis role sits within the Digital, Technology and Innovation Infrastructure Automation product team and is tasked with delivering solutions that will integrate and facilitate the automation of various processes and enterprise systems. Please note, this is an onsite role based in Hyderabad.\nKey Responsibilities:\nDeliver outstanding self-service and automation experiences for our global workforce\nCreate ServiceNow catalog items, workflows, and cross-platform API integrations\nCreate and configure Business Rules, UI Policies, UI Actions, Client Scripts, REST APIs and ACLs including advanced scripting logic.\nCreate and configure Notifications, UI pages, UI Macros, Script Includes, Formatters, etc.\nCreate and maintain data integrations between ServiceNow and other systems\nDevelop system integrations and process automation\nParticipate in design review, client requirements sessions and development teams to deliver features and capabilities supporting automation initiatives\nCollaborate with product owners, stakeholders, testers and other developers to understand, estimate, prioritize and implement solutions\nDesign, code, debug, document, deploy and maintain solutions in a highly efficient and effective manner\nParticipate in problem analysis, code review, and system design\nRemain current on new technology and apply innovation to improve functionality\nCollaborate closely with stakeholders and team members to configure, improve and maintain current applications\nWork directly with users to resolve support issues within product team responsibilities\nMonitor health, performance and usage of developed solutions\nWhat we expect of you\nWe are all different, yet we all use our unique contributions to serve patients.\nBasic Qualifications:\nMasters degree and 0 to 2 years of experience in computer science, IT, or related field OR\nBachelors degree and 2 to 5 years of experience in computer science, IT, or related field OR\nDiploma and 4 to 7 years of experience in computer science, IT, or related field\nRequired Skills & Qualifications:\n3+ years of deep hands-on experience with ServiceNow administration and development in two or more products: ITSM, ITBM, ITOM, GRC, HRSD, or Security Operations\nServiceNow development using JavaScript, AngularJS, AJAX, HTML, CSS, and Bootstrap;\nStrong understanding of user-centered design and building scalable, high-performing web and mobile interfaces on the ServiceNow platform\nExperience creating and managing Scoped Applications\nWorkflow automation and integration development using REST, SOAP, or MID servers\nScripting skills in Python, Bash, or other programming languages\nWorking in an Agile (SAFe, Scrum, and Kanban) environment\nPreferred Qualifications:\nGood-to-Have Skills:\nExperience with other configuration management tools (e.g., Puppet, Chef).\nExperience with Linux administration, scripting (Python, Bash), and CI/CD tools (GitHub Actions, CodePipeline, etc.)\nExperience with Terraform & CloudFormation for AWS infrastructure automation\nKnowledge of AWS Lambda and event-driven architectures.\nExposure to multi-cloud environments (Azure, GCP)\nExperience operating within a validated systems environment (FDA, European Agency for the Evaluation of Medicinal Products, Ministry of Health, etc.)\nProfessional Certifications (preferred):\nService Now Certified System Administrator\nService Now Certified Application Developer\nService Now Certified Technical Architect\nSoft Skills:\nStrong analytical and problem-solving skills.\nAbility to work effectively with global, virtual teams\nEffective communication and collaboration with cross-functional teams.\nAbility to work in a fast-paced environment.\nShift Information: This position is required to be onsite and participate in 24/5 and weekend on call in rotation fashion and may require you to work a later shift. Candidates must be willing and able to work off hours, as required based on business requirements.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['ServiceNow', 'AngularJS', 'HTML', 'SAFe', 'ITBM', 'JavaScript', 'code review', 'Scrum', 'Python', 'CSS', 'Azure', 'UI pages', 'ITSM', 'system design', 'ITOM', 'Bash', 'SOAP', 'REST', 'UI Macros', 'problem analysis', 'GCP', 'GRC', 'HRSD', 'Script Includes', 'AJAX']",2025-06-12 13:54:29
Claims Business Process Analyst Senior,Optum,1 - 4 years,Not Disclosed,['Bengaluru'],"Optum is a global organization that delivers care, aided by technology to help millions of people live healthier lives. The work you do with our team will directly improve health outcomes by connecting people with the care, pharmacy benefits, data and resources they need to feel their best. Here, you will find a culture guided by inclusion, talented peers, comprehensive benefits and career development opportunities. Come make an impact on the communities we serve as you help us advance health optimization on a global scale. Join us to start Caring. Connecting. Growing together.\n\nPrimary Responsibilities:\nReporting Development and Data Integration\nAssist with data projects related to integration with our core claims adjudication engines, eligibility, and other database items as necessary\nSupport the data leads by producing ad hoc reports as needed based on requirements from the business\nReport on key milestones to our project leads\nEnsuring all reporting aligns with brand standards\nEnsuring PADU guidelines for tools, connections, and data security\nBuild a network with internal partners to assist with validating data quality\nAnalytical Skills Utilization\nApplying analytical skills and developing business knowledge to support operations\nIdentify automation opportunities through the trends and day to day tasks to help create efficiencies within the team\nPerform root cause analysis via the 5 why root causing to identify process gaps and initiate process improvement efforts\nAssist with user testing for reports, business insights dashboards, and assist with automation validation review\nComply with the terms and conditions of the employment contract, company policies and procedures, and any and all directives (such as, but not limited to, transfer and/or re-assignment to different work locations, change in teams and/or work shifts, policies in regards to flexibility of work benefits and/or work environment, alternative work arrangements, and other decisions that may arise due to the changing business environment). The Company may adopt, vary or rescind these policies and directives in its absolute discretion and without any limitation (implied or otherwise) on its ability to do so\n\nRequired Qualifications:\nDegree or equivalent data science, analysis, mathematics experience\nExperience supporting operational teams' performance with reports and analytics\nExperience using Word (creating templates/documents), PowerPoint (creation and presentation), Teams, and SharePoint (document access/storage, sharing, List development and management)\nBasic understanding of reporting using Business Insights tools including Tableau and PowerBI\nExpertise in Excel (data entry, sorting/filtering) and VBA\nProven solid communication skills including oral, written, and organizational skills\nProven ability to manage emotions effectively in high-pressure situations, maintaining composure, and fosters a positive work environment conducive to collaboration and productivity\nPreferred Qualifications:\nExperience leveraging and creating automation such as macros, PowerAutomate, Alteryx/ETL Applications\nExperience working with cloud-based servers, knowledge of database structure, stored procedures\nExperience performing root cause analysis and demonstrated problem solving skills\nKnowledge of R/Python, SQL, DAX or other coding languages\nKnowledge of multiple lines of business, benefit structures and claims processing systems",Industry Type: Retail,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['power bi', 'sql', 'alteryx', 'tableau', 'vba', 'python', 'macros', 'dbms', 'sharepoint', 'process improvement', 'business process analysis', 'root cause analysis', 'dashboards', 'stored procedures', 'data quality', 'r', 'data science', 'business insights', 'dax', 'etl', 'data integration']",2025-06-12 13:54:31
Senior Software Engineer,eG Innovations,5 - 8 years,Not Disclosed,['Chennai'],Responsibilities:\n\nDesign and manage application module for data Integration / ETL collection/processing/storage/retrieval) using latest technologies\nShould be completely aware of coding standards and should be able to design & develop high performance & scalable application\nAbility to prototype solutions quickly and analyse and compare multiple solutions and products based on requirements.,,,,"['Core Java', 'Spring Boot', 'JMS', 'Open AI', 'Kafka', 'J2Ee', 'ETL', 'Data Analytics', 'SOAP', 'Microservices', 'Data Integration']",2025-06-12 13:54:33
ETL Test Engineer - Automation Testing,Stanra Tech Solutions,6 - 7 years,Not Disclosed,['Thiruvananthapuram'],"ETL Test Automation Engineer Trivandrum (WFO)-Immediate Joiner ONLY\n\nRole : ETL Test Automation Engineer to design, implement, and maintain test automation frameworks for real-time streaming ETL pipelines in a cloud based environment.\n\nRequired Experience : 6-7 Year Mandatory in ETL Automation Testing\n\nWorking Hours : Normal working hours (Monday to Friday, )\n\nAdditional Information :\n\nStart Date : Immediate-Immediate Joiners Only Mandatory\n\nBackground Verification : Mandatory through a third-party verification process.\n\nRequired Experience : 6-7 Yr Mandatory in ETL Automation Testing\n\nPlease Note : We are looking for Immediate Joiner Only\n\nSkills :\nStrong knowledge of ETL Testing with a focus on streaming data pipelines (Apache Flink, Kafka).\nExperience with database testing and validation (PostgreSQL, NoSQL).\nProficiency in Java for writing test automation scripts.\nHands-on experience with automation scripting for ETL workflows, data validation, and transformation checks.\nExperience in performance testing and optimizing ETL jobs to handle large-scale data streams.\nProficient in writing and executing complex database queries for data validation and reconciliation.\nExperience with CI/CD tools (Jenkins, GitHub Actions, Azure DevOps) for integrating test automation.\nFamiliarity with cloud-based ETL testing in Azure.\nAbility to troubleshoot data flow and transformation issues in real-time.\nNice to Have:\nExperience with Karate DSL, NiFi and CLI-based testing tools.\nExperience in developing self-healing mechanisms for data integrity issues.\nHands-on experience with automated data drift detection.\nKnowledge of data observability tools.\nFamiliarity with containerization (Docker, Kubernetes) and Infrastructure as Code (Terraform, Ansible) for ETL deployment.\nExperience with testing message queues (Kafka, Pulsar, RabbitMQ, etc.).\nDomain knowledge of banking and financial institutions and/or large enterprise IT environment will be considered a strong asset .",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['ETL Testing', 'Datawarehouse Testing', 'Performance Testing', 'Integration Testing', 'Automation Testing', 'Test Strategy']",2025-06-12 13:54:35
Consultant - Software Engineer (with C#),Affine Analytics,5 - 8 years,Not Disclosed,['Bengaluru'],"We are looking for a highly skilled API & Pixel Tracking Integration Engineer to lead the development and deployment of server-side tracking and attribution solutions across multiple platforms. The ideal candidate brings deep expertise in CAPI integrations (Meta, Google, and other platforms), secure data handling using cryptographic techniques, and experience working within privacy-first environments like Azure Clean Rooms.\nThis role requires strong hands-on experience in C# development, Azure cloud services, OCI (Oracle Cloud Infrastructure), and marketing technology stacks including Adobe Tag Management and Pixel Management. You will work closely with engineering, analytics, and marketing teams to deliver scalable, compliant, and secure data tracking solutions that drive business insights and performance.",,,,"['C#', 'Azure Data Factory', 'Adobe Tag Management', 'ADF', 'ADLS', 'Azure Functions', 'cryptography', 'Key Vault', 'Cosmos DB']",2025-06-12 13:54:38
Senior QA Engineer - Digital & IT,Skillsoft Software Services,2 - 8 years,Not Disclosed,['Hyderabad'],"We are seeking a passionate technology enthusiast with a strong background in ensuring the delivery of high-quality software applications. If you thrive in a dynamic, fast-paced environment where accountability and creativity are valued, then this opportunity might be the right fit for you. We are looking for an experienced QA Engineer to join our dynamic team.\n\nAs a Senior QA Engineer, you will be an integral part of our agile scrum development process, leading automation efforts within the sprint and ensuring the robustness of our diverse Software-as-a-Service (SaaS) and custom applications. If ensuring software quality is your forte, we are eager to connect with you.",,,,"['automation framework', 'workday', 'business requirements', 'test cases', 'api automation', 'salesforce', 'scripting', 'dynamics', 'java', 'git', 'automation', 'selenium', 'api', 'communication skills', 'python', 'development', 'bdd', 'sap', 'software testing', 'javascript', 'tdd', 'web technologies', 'salesforce testing', 'scrum', 'gui', 'agile']",2025-06-12 13:54:40
Senior ODI Developer (OCI PaaS/IaaS Expertise),Oracle,5 - 10 years,Not Disclosed,"['Chennai', 'Bengaluru', 'Mumbai (All Areas)']","Role Overview:\nWe are seeking a highly skilled Senior ODI Developer with strong hands-on experience in SQL, PL/SQL, and Oracle Data Integrator (ODI) projects, particularly on OCI (Oracle Cloud Infrastructure) PaaS or IaaS platforms. The ideal candidate will design, implement, and optimize ETL processes, leveraging cloud-based solutions to meet evolving business needs. Prior experience in banking or insurance projects is a significant advantage.\nKey Responsibilities:\nDesign, develop, and deploy ETL processes using Oracle Data Integrator (ODI) on OCI PaaS/IaaS.\nConfigure and manage ODI instances on OCI, ensuring optimal performance and scalability.\nDevelop and optimize complex SQL and PL/SQL scripts for data extraction, transformation, and loading.\nImplement data integration solutions, connecting diverse data sources like cloud databases, on-premise systems, APIs, and flat files.\nMonitor and troubleshoot ODI jobs running on OCI to ensure seamless data flow and resolve any issues promptly.\nCollaborate with data architects and business analysts to understand integration requirements and deliver robust solutions.\nConduct performance tuning of ETL processes, SQL queries, and PL/SQL procedures.\nPrepare and maintain detailed technical documentation for developed solutions.\nAdhere to data security and compliance standards, particularly in cloud-based environments.\nProvide guidance and best practices for ODI and OCI-based data integration projects.\nSkills and Qualifications:\nMandatory Skills:\nStrong hands-on experience with Oracle Data Integrator (ODI) development and administration.\nProficiency in SQL and PL/SQL for complex data manipulation and query optimization.\nExperience deploying and managing ODI solutions on OCI PaaS/IaaS environments.\nDeep understanding of ETL processes, data warehousing concepts, and cloud data integration.\nPreferred Experience:\nHands-on experience in banking or insurance domain projects, with knowledge of domain-specific data structures.\nFamiliarity with OCI services like Autonomous Database, Object Storage, Compute, and Networking.\nExperience in integrating on-premise and cloud-based data sources.\nOther Skills:\nStrong problem-solving and debugging skills.\nExcellent communication and teamwork abilities.\nKnowledge of Agile methodologies and cloud-based DevOps practices.\nEducation and Experience:\nBachelors degree in computer science, Information Technology, or a related field.\n5 to 8 years of experience in ODI development, with at least 2 years of experience in OCI-based projects.\nDomain experience in banking or insurance is an added advantage.",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['Oracle Data Integrator', 'OCI', 'Data Integrator', 'Data Warehousing', 'ETL', 'SQL']",2025-06-12 13:54:42
"Quality Engineer, QA",XL India Business Services Pvt. Ltd,2 - 6 years,Not Disclosed,['Gurugram'],"Quality Engineer Bangalore/ Gurgaon, India AXA XL offers risk transfer and risk management solutions to clients globally\n\nWe offer worldwide capacity, flexible underwriting solutions, a wide variety of client-focused loss prevention services and a team-based account management approach\n\nAXA XL recognizes data and information as critical business assets, both in terms of managing risk and enabling new business opportunities\n\nThis data should not only be high quality, but also actionable - enabling AXA XL s executive leadership team to maximize benefits and facilitate sustained advantage\n\nOur Chief Data Office is focused on driving innovation through optimizing how we leverage data to drive strategy and create a new business model - disrupting the insurance market\n\nAs we develop an enterprise-wide data and digital strategy that moves us toward greater focus on the use of data and data-driven insights, we are seeking an Engineer for the Quality Engineering team\n\nThe Engineer sits next to our Business Partners and tests our AXIOM platform according to our stakeholders needs\n\nWhat you ll be DOING What will your essential responsibilities include? Possess excellent domain knowledge of Data warehousing technologies, SQL, Data Models to develop test strategies, approaches from Quality Engineering perspective\n\nIn close coordination with Project teams help lead all efforts from Quality Engineering perspective\n\nWork with data engineers or data scientists to collect and prepare the necessary test data sets\n\nEnsure the data adequately represents real-world scenarios and covers a diverse range of inputs\n\nExcellent domain knowledge of Data warehousing technologies, SQL, Data Models to build out test strategies and lead projects from Quality Engineering perspective\n\nWith an Automation-first mindset, work towards testing of user interfaces such as Business Intelligence solutions and validation of functionalities while constantly looking out for efficiency gains and process improvements\n\nTriage and Prioritization of stories and epics with all stakeholders to ensure optimal deliveries\n\nEngage with various stakeholders like Business Partners, Product Owners, Development and Infrastructure teams to ensure alignments with overall roadmap\n\nTrack current progress of testing activities, finding and tracking test metrics, estimating and communicating improvement actions based on the test metrics results and the experience\n\nAutomation for processes such as Data Loads, user interfaces such as Business Intelligence solutions and other validations of business KPIs\n\nAdopt and implement best practices towards Documentation of test plan, cases, results in JIRA\n\nTriage and Prioritization of defects with all stakeholders\n\nLeadership accountability for ensuring that every release to customers is fit for purpose, performant\n\nKnowledge on Scaled Agile, Scrum or Kanban methodology\n\nYou will report to Lead UAT\n\nWhat you will BRING We re looking for someone who has these abilities and skills: Required Skills and Abilities: A minimum of a bachelor s or masters degree (preferred) in a relevant discipline\n\nRelevant years of excellent testing background, including knowledge/experience in automation\n\nInsurance experience in data, underwriting, claims or operations, including influencing, collaborating, and leading efforts in complex, disparate, and interrelated teams\n\nExcellent Experience with SQL Server, Azure Databricks Notebook, PowerBI, ADLS, CosmosDB, SQL DW Analytics\n\nShould have a robust background in Software development with experience in ingesting, transforming, and storing data from large datasets using Pyspark in Azure Databricks with robust knowledge of distributed computing concepts\n\nHands-on experience in designing and developing ETL Pipelines in Pyspark in Azure Databricks with robust python scripting\n\nDesired Skills and Abilities: Having experience doing UAT/System Integration testing in the insurance industry\n\nExcellent technical testing experience such as API testing, UI automation is a plus\n\nKnowledge/Experience of Testing in cloud-based systems in different data staging layers",Industry Type: Insurance,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['System integration testing', 'Test planning', 'Account management', 'Business strategy', 'Business intelligence', 'Risk management', 'JIRA', 'Analytics', 'SQL', 'Python']",2025-06-12 13:54:45
"Engineer, ETL",XL India Business Services Pvt. Ltd,2 - 5 years,Not Disclosed,['Gurugram'],"Engineer, ETL Gurgaon/Bangalore, India AXA XL recognizes data and information as critical business assets, both in terms of managing risk and enabling new business opportunities\n\nThis data should not only be high quality, but also actionable - enabling AXA XL s executive leadership team to maximize benefits and facilitate sustained industrious advantage\n\nOur Chief Data Office also known as our Innovation, Data Intelligence & Analytics team (IDA) is focused on driving innovation through optimizing how we leverage data to drive strategy and create a new business model - disrupting the insurance market\n\nAs we develop an enterprise-wide data and digital strategy that moves us toward greater focus on the use of data and data-driven insights, we are seeking an Data Engineer\n\nThe role will support the team s efforts towards creating, enhancing, and stabilizing the Enterprise data lake through the development of the data pipelines\n\nThis role requires a person who is a team player and can work well with team members from other disciplines to deliver data in an efficient and strategic manner\n\nWhat you ll be DOING What will your essential responsibilities include? Act as a data engineering expert and partner to Global Technology and data consumers in controlling complexity and cost of the data platform, whilst enabling performance, governance, and maintainability of the estate\n\nUnderstand current and future data consumption patterns, architecture (granular level), partner with Architects to ensure optimal design of data layers\n\nApply best practices in Data architecture\n\nFor example, balance between materialization and virtualization, optimal level of de-normalization, caching and partitioning strategies, choice of storage and querying technology, performance tuning\n\nLeading and hands-on execution of research into new technologies\n\nFormulating frameworks for assessment of new technology vs business benefit, implications for data consumers\n\nAct as a best practice expert, blueprint creator of ways of working such as testing, logging, CI/CD, observability, release, enabling rapid growth in data inventory and utilization of Data Science Platform\n\nDesign prototypes and work in a fast-paced iterative solution delivery model\n\nDesign, Develop and maintain ETL pipelines using Pyspark in Azure Databricks using delta tables\n\nUse Harness for deployment pipeline\n\nMonitor Performance of ETL Jobs, resolve any issue that arose and improve the performance metrics as needed\n\nDiagnose system performance issue related to data processing and implement solution to address them\n\nCollaborate with other teams to ensure successful integration of data pipelines into larger system architecture requirement\n\nMaintain integrity and quality across all pipelines and environments\n\nUnderstand and follow secure coding practice to make sure code is not vulnerable\n\nYou will report to Technical Lead\n\nWhat you will BRING We re looking for someone who has these abilities and skills: Required Skills and Abilities: Effective Communication skills\n\nBachelor s degree in computer science, Mathematics, Statistics, Finance, related technical field, or equivalent work experience\n\nRelevant years of extensive work experience in various data engineering & modeling techniques (relational, data warehouse, semi-structured, etc), application development, advanced data querying skills\n\nRelevant years of programming experience using Databricks\n\nRelevant years of experience using Microsoft Azure suite of products (ADF, synapse and ADLS)\n\nSolid knowledge on network and firewall concepts\n\nSolid experience writing, optimizing and analyzing SQL\n\nRelevant years of experience with Python\n\nAbility to break complex data requirements and architect solutions into achievable targets\n\nRobust familiarity with Software Development Life Cycle (SDLC) processes and workflow, especially Agile\n\nExperience using Harness\n\nTechnical lead responsible for both individual and team deliveries\n\nDesired Skills and Abilities: Worked in big data migration projects\n\nWorked on performance tuning both at database and big data platforms\n\nAbility to interpret complex data requirements and architect solutions\n\nDistinctive problem-solving and analytical skills combined with robust business acumen\n\nExcellent basics on parquet files and delta files\n\nEffective Knowledge of Azure cloud computing platform\n\nFamiliarity with Reporting software - Power BI is a plus\n\nFamiliarity with DBT is a plus\n\nPassion for data and experience working within a data-driven organization\n\nYou care about what you do, and what we do",Industry Type: Insurance,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Performance tuning', 'System architecture', 'Coding', 'Agile', 'Workflow', 'Application development', 'SDLC', 'SQL', 'Python', 'Firewall']",2025-06-12 13:54:47
Artificial Intelligence Engineer,Infosys,5 - 10 years,5-10 Lacs P.A.,['Pune'],"Our client INFOSYS is looking for Artificial Intelligence Engineer position with 5+ years of experience in Pune location. CONTRACT TO HIRE AND WORK FROM OFFICE\n\nJob Description :\nMandatory Skills : Python + LLMs + AI + Azure Certified.\nole Definition:\nThis is a specialized role for an AI Software Engineers design, build, and deploy scalable AI models and systems. They work with machine learning frameworks, cloud platforms, and data engineering tools to create and optimize AI solutions.\nSkills:\nProficient:\nLanguages/Framework: Fast API, Azure UI Search API (React)\nCloud: Azure Cloud Basics (Azure DevOps)\nGitlab: Gitlab Pipeline\nAnsible and REX: Rex Deployment\nData Science: Prompt Engineering + Modern Testing\nData pipeline development\nUnderstanding of AI/ML algorithms and their applications\nMLOps frameworks\nKnowledge of cloud platforms (Azure ML especially)\nModel deployment process\no             Data pipeline monitoring\n              Expert: (in addition to proficient skills)\no             Languages/Framework: Azure Open AI\no             Data Science: Open AI GPT Family of models 4o/4/3, Embeddings + Vector Search\no             Databases and ETL: Azure Storage Account, Postgresql, Cosmos\no             Experience with ML frameworks (TensorFlow, PyTorch, Scikit-learn)\no             Knowledge of cloud platforms (AWS SageMaker, Google AI Platform)\no             Expertise in data preprocessing, feature engineering, and model evaluation\no             Understanding of software engineering principles (version control, CI/CD, containerization)\no             Familiarity with distributed computing and big data tools (Spark, Hadoop)\no             Ability to optimize models for performance and scalability\no             Experience with Azure AI Search",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Temporary/Contractual","['llm', 'Python', 'Artificial Intelligence']",2025-06-12 13:54:49
"Associate Engineer, Release Management",XL India Business Services Pvt. Ltd,1 - 5 years,Not Disclosed,['Gurugram'],"Associate Engineer, Release Management Gurgaon / Bangalore, India AXA XL offers risk transfer and risk management solutions to clients globally\n\nWe offer worldwide capacity, flexible underwriting solutions, a wide variety of client-focused loss prevention services and a team-based account management approach\n\nAXA XL recognizes data and information as critical business assets, both in terms of managing risk and enabling new business opportunities\n\nThis data should not only be high quality, but also actionable - enabling AXA XL s executive leadership team to maximize benefits and facilitate sustained unique features\n\nOur Chief Data Office also known as our Enterprise Business Data Solutions team (EDS) is focused on driving innovation through optimizing how we leverage data to drive strategy and create a new business model - disrupting the insurance market\n\nThis role is part of the Data Build Management, which is responsible for supporting various teams to plan, build and support through the product lifecycle to ensure, performance stability and reliability of data solutions by providing world-class support\n\nWhat you ll be DOING What will your essential responsibilities include? Provide top-class Build, Release Management, and DevSecOps functionalities and support for various data solutions owned and managed by the IDA organization\n\nAnalyze and mitigate risks (technical or otherwise) associated with data solution build and release delivery timelines\n\nProvide support for data pipelines and DataOps in cloud and big data environments\n\nEnforce governance and reporting, and ensure adherence to policies, standards, and best practices for Build Pipelines and Release Management\n\nPlan the release of project deliverables and manage the release lifecycle\n\nCommunicate project-related tasks such as plans, timelines, and requirements between different teams\n\nCoordinate the release schedule and resources required based on third-party applications, defect backlogs, planned releases, and infrastructure updates\n\nIdentify risks that could delay the release and manage them to ensure that the scheduled scope and quality of the release is not affected\n\nTrack progress and identify issues, if any, while continuously working to improve the release process\n\nEnsure that the release is planned according to requirements and budget\n\nSchedule release readiness reviews before deployment and milestone reviews after each release\n\nCreate implementation and deployment plans in accordance with the release schedule\n\nPlan and provide weekly updates on release activities\n\nLead Go-Live activities to ensure successful software deployment\n\nCollaborate with relevant development teams responsible for building the automation tools used to develop and deploy the software\n\nAttend CAB meetings to discuss release schedules with the team and identify any roadblocks\n\nMaintain documentation related to build and release procedures, various notification lists, and dependencies\n\nDemonstrate proactive communication with business users, development teams, technology, production support, delivery teams, and senior management\n\nEstablish a feedback loop by collaborating with internal and external teams\n\nPartner with the Product and Production Support teams as a Build/Release/Technical Subject Matter Expert (SME) for new development or migration of re-architected product functionalities to the new cloud platform\n\nOversee the development and maintenance of Build and Release Management processes and their documentation\n\nBuild and maintain various critical monitoring processes, alerts, and overall health reports (performance and functional) for production and pre-production environments to be used by the Production Support Teams\n\nEnsure timely and accurate completion of emergency release pipelines/processes in a manner that is auditable, testable, and maintainable\n\nEnsure that all builds are consistent with solution design, security recommendations, and business specifications\n\nAbility to work in a ""Follow the Sun"" support model, providing coverage across Digital Data Dev division responsibilities\n\nAchieve and maintain the highest levels of business customer confidence and net promoter score (NPS)\n\nYou will report to the Scientist, Digital Data Development\n\nWhat you will BRING We re looking for someone who has these abilities and skills: Required Skills and Abilities: A minimum of an Undergraduate University Degree in Computer Science or related fields with years of experience\n\nExperience in systems integration, and developer support tools Azure DevOps, Docker, Kubernetes, Harness, CICD, release management, and configuration management, Python\n\nUnderstanding of DataOps, ITIL and AGILE methodologies\n\nExperience in building reports & dashboards using Excel, and Power BI\n\nRelevant years of experience in supporting areas of Data Management\n\nOwn availability, scalability, and efficiency for day-to-day operations\n\nPossess a robust understanding of Azure fundamentals (Microsoft AZ-900)\n\nExperience using tools like Git, JIRA, etc Knowledge of artifact repositories such as JFrog and X-Ray\n\nDesired Skills and Abilities: Ability to troubleshoot issues and able to handle different types of user inquiries\n\nExercises judgment for decision-making on complex issues that impact the delivery\n\nMakes recommendations to management on new processes, tools and techniques, or the development of new products and services\n\nPossesses skills, awareness and consistent with project management, analytical and application skills\n\nConsistently determines and executes basic research and/or referrals resiliently\n\nDemonstrates level of experience/ability to influence, and understand business problems in technical terminology\n\nEffective analytical and problem-solving skills\n\nExtremely effective interpersonal skills and relationship building and able to liaise with staff at all levels in the organization\n\nExcellent writing skills, with the ability to create clear requirements, specifications and documentation for data systems",Industry Type: Insurance,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Production support', 'Data management', 'Project management', 'Configuration management', 'Agile', 'microsoft', 'Risk management', 'Release management', 'Monitoring', 'Python']",2025-06-12 13:54:52
Jr.AI Engineer,Tekone It Services,1 - 3 years,1.5-6.5 Lacs P.A.,['Hyderabad'],"Position Overview\nWe are hiring five AI Engineers with 12 years of experience to join our dynamic team in Hyderabad. The ideal candidates will have a solid foundation in Large Language Models (LLMs), LangChain, and Generative AI (GenAI) frameworks. This is a great opportunity to work on innovative AI solutions, contributing to projects that integrate LLMs, prompt engineering, RAG pipelines, and cloud-based deployments.\nKey Responsibilities\nContribute to the design and development of AI-powered applications utilizing LLMs (GPT-3.5, GPT-4, Gemini).\nAssist in building LangChain-based pipelines and workflows, including LangSmith and LangGraph.\nSupport the implementation of Retrieval-Augmented Generation (RAG) frameworks using vector databases such as ChromaDB.\nApply prompt engineering techniques to optimize model responses and improve contextual accuracy.\nDevelop RESTful APIs using Flask or FastAPI to enable model consumption in production environments.\nWrite and manage data workflows using SQL, PySpark, and Spark SQL.\nDeploy and monitor models on Azure Machine Learning or AWS Bedrock platforms.\nCollaborate with cross-functional teams, including data scientists, engineers, and business stakeholders.\nRequired Skills\nProficiency in Python, SQL, PySpark, and Spark SQL\nHands-on experience with LLMs: GPT-3.5, GPT-4, Gemini\nKnowledge of LangChain, LangSmith, LangGraph\nFamiliarity with Vector Databases (e.g., ChromaDB) and embeddings\nExperience with prompt engineering and RAG-based architectures\nExposure to cloud platforms such as Azure ML or AWS Bedrock\nStrong understanding of REST APIs and version control systems (Git/GitHub)\nPreferred Qualifications\nBachelor's degree in Computer Science, Artificial Intelligence, Data Science, or a related field\nInternship or academic project experience in NLP, LLMs, or GenAI technologies\nFamiliarity with MLOps tools and practices (e.g., CI/CD, Airflow)\nStrong problem-solving abilities, attention to detail, and a collaborative mindset",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'Prompt Engineering', 'Artificial Intelligence', 'llm']",2025-06-12 13:54:54
"Engineer, Release Management",XL India Business Services Pvt. Ltd,2 - 6 years,Not Disclosed,['Gurugram'],"Engineer, Release Management Gurgaon / Bangalore, India AXA XL offers risk transfer and risk management solutions to clients globally\n\nWe offer worldwide capacity, flexible underwriting solutions, a wide variety of client-focused loss prevention services and a team-based account management approach\n\nAXA XL recognizes data and information as critical business assets, both in terms of managing risk and enabling new business opportunities\n\nThis data should not only be high quality, but also actionable - enabling AXA XL s executive leadership team to maximize benefits and facilitate sustained unique features\n\nOur Chief Data Office also known as our Enterprise Business Data Solutions team (EDS) is focused on driving innovation through optimizing how we leverage data to drive strategy and create a new business model - disrupting the insurance market\n\nThis role is part of the Data Build Management, which is responsible for supporting various teams to plan, build and support through the product lifecycle to ensure, performance stability and reliability of data solutions by providing world-class support\n\nWhat you ll be DOING What will your essential responsibilities include? Provide top-class Build, Release Management, and DevSecOps functionalities and support for various data solutions owned and managed by the IDA organization\n\nAnalyze and mitigate risks (technical or otherwise) associated with data solution build and release delivery timelines\n\nProvide support for data pipelines and DataOps in cloud and big data environments\n\nEnforce governance and reporting, and ensure adherence to policies, standards, and best practices for Build Pipelines and Release Management\n\nPlan the release of project deliverables and manage the release lifecycle\n\nCommunicate project-related tasks such as plans, timelines, and requirements\n\nbetween different teams\n\nCoordinate the release schedule and resources required based on third-party applications, defect backlogs, planned releases, and infrastructure updates\n\nIdentify risks that could delay the release and manage them to ensure that the scheduled scope and quality of the release is not affected\n\nTrack progress and identify issues, if any, while continuously working to improve the release process\n\nEnsure that the release is planned according to requirements and budget\n\nSchedule release readiness reviews before deployment and milestone reviews after each release\n\nCreate implementation and deployment plans in accordance with the release schedule\n\nPlan and provide weekly updates on release activities\n\nLead Go-Live activities to ensure successful software deployment\n\nCollaborate with relevant development teams responsible for building the automation tools used to develop and deploy the software\n\nAttend CAB meetings to discuss release schedules with the team and identify any roadblocks\n\nMaintain documentation related to build and release procedures, various notification lists, and dependencies\n\nDemonstrate proactive communication with business users, development teams, technology, production support, delivery teams, and senior management\n\nEstablish a feedback loop by collaborating with internal and external teams\n\nPartner with the Product and Production Support teams as a Build/Release/Technical Subject Matter Expert (SME) for new development or migration of re-architected product functionalities to the new cloud platform\n\nOversee the development and maintenance of Build and Release Management processes and their documentation\n\nBuild and maintain various critical monitoring processes, alerts, and overall health reports (performance and functional) for production and pre-production environments to be used by the Production Support Teams\n\nEnsure timely and accurate completion of emergency release pipelines/processes in a manner that is auditable, testable, and maintainable\n\nEnsure that all builds are consistent with solution design, security recommendations, and business specifications\n\nAbility to work in a ""Follow the Sun"" support model, providing coverage across Digital Data Dev division responsibilities\n\nAchieve and maintain the highest levels of business customer confidence and net promoter score (NPS)\n\nYou will report to the Scientist, Digital Data Development\n\nWhat you will BRING We re looking for someone who has these abilities and skills: Required Skills and Abilities: A minimum of an Undergraduate University Degree in Computer Science or related fields with years of experience\n\nExperience in systems integration, and developer support tools Azure DevOps, Docker, Kubernetes, Harness, CICD, release management, and configuration management, Python\n\nUnderstanding of DataOps, ITIL and AGILE methodologies\n\nExperience in building reports & dashboards using Excel, and Power BI\n\nRelevant years of experience in supporting areas of Data Management\n\nOwn availability, scalability, and efficiency for day-to-day operations\n\nPossess a robust understanding of Azure fundamentals (Microsoft AZ-900)\n\nExperience using tools like Git, JIRA, etc Knowledge of artifact repositories such as JFrog and X-Ray\n\nDesired Skills and Abilities: Ability to troubleshoot issues and able to handle different types of user inquiries\n\nExercises judgment for decision-making on complex issues that impact the delivery\n\nMakes recommendations to management on new processes, tools and techniques, or the development of new products and services\n\nPossesses skills, awareness and consistent with project management, analytical and application skills\n\nConsistently determines and executes basic research and/or referrals resiliently\n\nDemonstrates level of experience/ability to influence, and understand business problems in technical terminology\n\nEffective analytical and problem-solving skills\n\nExtremely effective interpersonal skills and relationship building and able to liaise with staff at all levels in the organization\n\nExcellent writing skills, with the ability to create clear requirements, specifications and documentation for data systems",Industry Type: Insurance,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Production support', 'Data management', 'Project management', 'Configuration management', 'Agile', 'microsoft', 'Risk management', 'Release management', 'Monitoring', 'Python']",2025-06-12 13:54:56
Machine Learning Engineer,Draft N Craft Legal Outsourcing,2 - 3 years,10-15 Lacs P.A.,['New Delhi'],"Role Overview:\n\nAs an ML Engineer you will embark on a wonderful journey of developing and implementing various Machine Learning models that will ultimately act as an enabler in the companys growth.\nSince Draft Craft is a legal services-oriented company in which we serve clients from across the border, your work will be majorly aimed towards creating ML workflows that will serve the legal industry and help improve efficiency of the in-house teams. Draft n Craft offers you the perfect opportunity to grow and hone your ML/Data Engineering skills while contributing towards building a worthwhile product.\n\nKey Responsibilities:\nDeveloping data ingestion & data preprocessing pipelines for transforming data presented for legal requirements to extract key and actionable insights. \nData cleaning for supplying accurate, consistent & relevant content to ML models.\nExploring and experimenting with different ML models and architectures that can be used with data from the legal industry in a safe and compliant manner.\nDeveloping and deploying ML models that can function in production environments for the use-cases required by the company.\nAnalyzing key metrics for model performance and devising methods to improve efficiencies of the models.\nDocument the steps involved in data preprocessing, model development, and optimizations undertaken.\nExplore techniques for feature extraction, transformation, and selection to improve model performance.\nUnderstanding software development related terminologies to collaborate with the existing team of software engineers within the company.\nDevise methods of integration of the ML models within the company’s already developed software solutions and employee workflows.\n\nRequired Qualifications:\nDegree holder from Computer Science Engineering, Data Science or related fields.\nMinimum experience of 2 years working as an ML engineer or Data Scientist in a professional capacity.\nStrong programming skills including python and familiarity with related libraries Tensorflow, PyTorch, Pandas etc.\nDatabase Querying in terms of SQL/NoSQL.\nWorking with data extracting and ETL pipelines for pre-processing of data/documents.\nRelevant experience working in NLP, Neural Networks, and Gen AI technologies.\nFamiliarity with working or applying transfer-learning on LLM models like Llama, etc.\nSome experience in software development is preferred.\nKnowledge of deploying ML models and data pipelines to cloud services like AWS/Azure.",Industry Type: Legal,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Generative Ai', 'Retrieval Augmented Generation', 'Python', 'NoSQL', 'Large Language Model', 'Data Extraction', 'Machine Learning', 'SQL']",2025-06-12 13:54:58
Machine Learning Engineer,Tek Ninjas,8 - 13 years,Not Disclosed,['Pune'],"Skills:\n     Proficient:\nLanguages/Framework: Fast API, Azure UI Search API (React)\nCloud: Azure Cloud Basics (Azure DevOps)\nGitlab: Gitlab Pipeline\nAnsible and REX: Rex Deployment\nData Science: Prompt Engineering + Modern Testing\nData pipeline development\nUnderstanding of AI/ML algorithms and their applications\nMLOps frameworks\nKnowledge of cloud platforms (Azure ML especially)\nModel deployment process\nData pipeline monitoring\nLanguages/Framework: Azure Open AI\nData Science: Open AI GPT Family of models 4o/4/3, Embeddings + Vector Search\nDatabases and ETL: Azure Storage Account, Postgresql, Cosmos\nExperience with ML frameworks (TensorFlow, PyTorch, Scikit-learn)\nKnowledge of cloud platforms (AWS SageMaker, Google AI Platform)\nExpertise in data preprocessing, feature engineering, and model evaluation\nUnderstanding of software engineering principles (version control, CI/CD, containerization)\nFamiliarity with distributed computing and big data tools (Spark, Hadoop)\nAbility to optimize models for performance and scalability\nExperience with Azure AI Search\nDesired skills*\nAzure DevOps; MLOps frameworks; Postgresql; Cosmos",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['MLOPS', 'Aiml', 'Machine Learning', 'Azure Devops']",2025-06-12 13:55:01
Ai Ml Engineer,Tek Ninjas,8 - 13 years,Not Disclosed,['Pune'],"Skills:\nProficient:\nLanguages/Framework: Fast API, Azure UI Search API (React)\nCloud: Azure Cloud Basics (Azure DevOps)\nGitlab: Gitlab Pipeline\nAnsible and REX: Rex Deployment\nData Science: Prompt Engineering + Modern Testing\nData pipeline development\nUnderstanding of AI/ML algorithms and their applications\nMLOps frameworks\nKnowledge of cloud platforms (Azure ML especially)\nModel deployment process\nData pipeline monitoring\nLanguages/Framework: Azure Open AI\nData Science: Open AI GPT Family of models 4o/4/3, Embeddings + Vector Search\nDatabases and ETL: Azure Storage Account, PostgreSQL, Cosmos\nExperience with ML frameworks (TensorFlow, PyTorch, Scikit-learn)\nKnowledge of cloud platforms (AWS SageMaker, Google AI Platform)\nExpertise in data preprocessing, feature engineering, and model evaluation\nUnderstanding of software engineering principles (version control, CI/CD, containerization)\nFamiliarity with distributed computing and big data tools (Spark, Hadoop)\nAbility to optimize models for performance and scalability\nExperience with Azure AI Search\nDesired skills*\nAzure DevOps; MLOps frameworks; Postgresql; Cosmos",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['M lops', 'Machine Learning', 'Azure Devops', 'Azure Machine Learning']",2025-06-12 13:55:03
Software Engineer-8910,WebMD,3 - 8 years,Not Disclosed,['Navi Mumbai'],"Position: Software Engineer\npositions: 1\nNo.of\nAbout Company:\nHeadquartered in El Segundo, Calif., Internet Brands is a fully integrated online media and\nsoftware services organization focused on four high-value vertical categories: Health,\nAutomotive, Legal, and Home/Travel. The company's award-winning consumer websites\nlead their categories and serve more than 250 million monthly visitors, while a full range of\nweb presence offerings has established deep, long-term relationships with SMB and\nenterprise clients. Internet Brands' powerful, proprietary operating platform provides the\nflexibility and scalability to fuel the company's continued growth. Internet Brands is a portfolio\ncompany of KKR and Temasek.\nWebMD Health Corp., an Internet Brands Company, is the leading provider of health\ninformation services, serving patients, physicians, health care professionals, employers, and\nhealth plans through our public and private online portals, mobile platforms, and health-\nfocused publications. The WebMD Health Network includes WebMD Health, Medscape,\nJobson Healthcare Information, prIME Oncology, MediQuality, Frontline, QxMD, Vitals\nConsumer Services, MedicineNet, eMedicineHealth, RxList, OnHealth, Medscape\nEducation, and other owned WebMD sites. WebMD, Medscape, CME Circle,\nMedpulse®, eMedicine®, MedicineNet®, theheart.org®, and RxList® are among the\ntrademarks of WebMD Health Corp. or its subsidiaries.\nAll qualified applicants will receive consideration for employment without regard to race,\ncolor, religion, sex, sexual orientation, gender identity, national origin, disability, or veteran\nstatus.\nFor Company details, visit our website: www.webmd.com and www.internetbrands.com\nAll qualified applicants will receive consideration for employment without regard to\nrace, color, religion, sex, sexual orientation, gender identity, national origin, disability,\nor veteran status.\nFor Company details, visit our website: www.webmd.com\nhttps://www.webmd.com/corporate/physicians-interactive",,,,"['ETL', 'SQL', 'Data Warehousing']",2025-06-12 13:55:05
BI Engineer,Amgen Inc,2 - 5 years,Not Disclosed,['Hyderabad'],"Role Description:\nLets do this. We are seeking an experienced Senior BI Engineer to lead the design, development, and optimization of scalable business intelligence (BI) solutions that empower data-driven decision-making across the organization. The ideal candidate is highly skilled in data modeling, dashboard development, ETL design, and cloud-based BI platforms, with a passion for turning complex data into clear, actionable insights. As a senior member of the BI team, you will work closely with data engineers, analysts, business stakeholders, and product teams to deliver robust, user-friendly analytics solutions that support strategic and operational goals.\nRoles & Responsibilities:\nDesign, develop, and maintain enterprise-grade BI dashboards and reports using tools like Power BI, Tableau, or Looker.\nBuild and optimize semantic models, tabular data structures, and reusable datasets for self-service BI users.\nPartner with business stakeholders to translate requirements into technical solutions, delivering accurate, relevant, and timely insights.\nWork closely with data engineering teams to integrate BI solutions into data lake, warehouse, or lakehouse architectures (e.g., Snowflake, Redshift, Databricks, BigQuery).\nImplement best practices for BI development, including version control, performance optimization, and data governance.\nEnsure BI solutions are secure, scalable, and aligned with enterprise data governance standards.\nMentor junior BI developers and analysts, setting standards for dashboard usability, data visualization, and design consistency.\nCollaborate with cross-functional teams to promote self-service BI adoption and data literacy throughout the organization.\nMonitor BI performance, usage, and adoption, providing continuous improvements and training to enhance impact.\nMust-Have Skills:\n5 - 8 years of experience in BI development and data visualization, with deep expertise in tools such as Power BI, Tableau, or Looker.\nStrong knowledge of SQL, data modeling techniques, and BI architecture best practices.\nExperience working with data warehouses and cloud data platforms\nProficiency in building dashboards, KPIs, and executive-level reporting that align with business priorities.\nSolid understanding of ETL/ELT processes, data pipelines, and integration with BI tools.\nStrong collaboration skills with the ability to work effectively across engineering, product, finance, and business teams.\nExcellent communication skills, with a proven ability to translate technical concepts into business value.\nGood-to-Have Skills:\nExperience in cloud platforms (AWS, Azure, or GCP) and modern data stack environments.\nFamiliarity with data governance, data cataloging, metadata management, and access control.\nExposure to Agile methodologies, CI/CD for BI, and DevOps practices.\nBI or data certifications (e.g., Microsoft Certified: Power BI Data Analyst, Tableau Certified Professional).\nEducation and Professional Certifications\nMasters degree and 3 to 4 + years of Computer Science, IT or related field experience\nOR\nBachelors degree and 5 to 8 + years of Computer Science, IT or related field experience\nPowerBI / Tableau certifications preferred\nScaled Agile SAFe certification preferred\nSoft Skills:\nExcellent analytical and troubleshooting skills.\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nTeam-oriented, with a focus on achieving team goals.\nAbility to learn quickly, be organized and detail oriented.\nStrong presentation and public speaking skills.",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['BI development', 'Azure', 'Power BI', 'BI tools', 'Agile methodologies', 'data modeling techniques', 'Tableau', 'SQL', 'BI architecture', 'GCP', 'CI/CD', 'cloud data platforms', 'data visualization', 'data warehouses', 'AWS']",2025-06-12 13:55:07
Engineering Manager,Amgen Inc,10 - 14 years,Not Disclosed,['Hyderabad'],"Role Description:\nWe are seeking a seasoned Engineering Manager (Data Engineering) to lead the end-to-end management of enterprise data assets and operational data workflows. This role is critical in ensuring the availability, quality, consistency, and timeliness of data across platforms and functions, supporting analytics, reporting, compliance, and digital transformation initiatives. You will be responsible for the day-to-day data operations, manage a team of data professionals, and drive process excellence in data intake, transformation, validation, and delivery. You will work closely with cross-functional teams including data engineering, analytics, IT, governance, and business stakeholders to align operational data capabilities with enterprise needs.\nRoles & Responsibilities:\nLead and manage the enterprise data operations team, responsible for data ingestion, processing, validation, quality control, and publishing to various downstream systems.\nDefine and implement standard operating procedures for data lifecycle management, ensuring accuracy, completeness, and integrity of critical data assets.\nOversee and continuously improve daily operational workflows, including scheduling, monitoring, and troubleshooting data jobs across cloud and on-premise environments.\nEstablish and track key data operations metrics (SLAs, throughput, latency, data quality, incident resolution) and drive continuous improvements.\nPartner with data engineering and platform teams to optimize pipelines, support new data integrations, and ensure scalability and resilience of operational data flows.\nCollaborate with data governance, compliance, and security teams to maintain regulatory compliance, data privacy, and access controls.\nServe as the primary escalation point for data incidents and outages, ensuring rapid response and root cause analysis.\nBuild strong relationships with business and analytics teams to understand data consumption patterns, prioritize operational needs, and align with business objectives.\nDrive adoption of best practices for documentation, metadata, lineage, and change management across data operations processes.\nMentor and develop a high-performing team of data operations analysts and leads.\nFunctional Skills:\nMust-Have Skills:\nExperience managing a team of data engineers in biotech/pharma domain companies.\nExperience in designing and maintaining data pipelines and analytics solutions that extract, transform, and load data from multiple source systems.\nDemonstrated hands-on experience with cloud platforms (AWS) and the ability to architect cost-effective and scalable data solutions.\nExperience managing data workflows in cloud environments such as AWS, Azure, or GCP.\nStrong problem-solving skills with the ability to analyze complex data flow issues and implement sustainable solutions.\nWorking knowledge of SQL, Python, or scripting languages for process monitoring and automation.\nExperience collaborating with data engineering, analytics, IT operations, and business teams in a matrixed organization.\nFamiliarity with data governance, metadata management, access control, and regulatory requirements (e.g., GDPR, HIPAA, SOX).\nExcellent leadership, communication, and stakeholder engagement skills.\nWell versed with full stack development & DataOps automation, logging frameworks, and pipeline orchestration tools.\nStrong analytical and problem-solving skills to address complex data challenges.\nEffective communication and interpersonal skills to collaborate with cross-functional teams.\nGood-to-Have Skills:\nData Engineering Management experience in Biotech/Life Sciences/Pharma\nExperience using graph databases such as Stardog or Marklogic or Neo4J or Allegrograph, etc.\nEducation and Professional Certifications\n9 to 12 years of experience in Computer Science, IT or related field\nAWS Certified Data Engineer preferred\nDatabricks Certificate preferred\nScaled Agile SAFe certification preferred\nSoft Skills:\nExcellent analytical and troubleshooting skills\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation\nAbility to manage multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals\nStrong presentation and public speaking skills",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data operations', 'fullstack development', 'GCP', 'stakeholder engagement', 'troubleshooting', 'cloud platforms', 'AWS']",2025-06-12 13:55:09
Machine Learning Engineer (Full Stack),Hubnex,5 - 10 years,Not Disclosed,['Gurugram'],"Machine Learning Engineer (Full Stack)\nLocation: Gurugram, India (On-site/Hybrid)\nType: Full-Time | 5+ Years Experience | AI & Product Engineering\nHubnex Labs is seeking a highly skilled Machine Learning Engineer with strong capabilities in Full Stack Development to lead the development and deployment of production-grade AI systems. This role requires expertise in building end-to-end ML pipelines from data preprocessing to deployment while also contributing to the full stack of software platforms that power our solutions.\nKey Responsibilities Machine Learning & Data Science\nUnderstand business goals and translate them into ML-based solutions\nDevelop, analyze, and compare machine learning algorithms for various problem statements\nBuild robust validation strategies and design appropriate preprocessing and feature engineering pipelines\nPerform data exploration, visualization , and quality verification , including data cleaning and augmentation\nTrain models, tune hyperparameters , and interpret model performance\nAnalyze errors and design strategies to improve model robustness\nDiscover and utilize public datasets for model training and benchmarking\nDeploy models into production environments with real-world performance and latency considerations\nSoftware & System Development\nDesign and develop end-to-end production systems , including backend APIs and frontend interfaces\nMaintain full stack web applications , ensuring seamless ML model integration\nEnsure efficient use of hardware resources for training and inference\nCollaborate cross-functionally with engineering, product, and design teams\nTechnical Skills Required\n5+ years of hands-on experience in machine learning and full stack development\nProficiency in Python and ML libraries like scikit-learn , pandas , NumPy , etc.\nDeep learning experience using TensorFlow , Keras , or equivalent frameworks\nProficiency with OpenCV and image/video processing techniques\nExperience with data visualization tools and big data handling\nStrong understanding of data pipelines , feature engineering , and augmentation techniques\nProficiency in Full Stack Web Development (e.g., React, Node.js, Express, MongoDB or similar)\nExperience deploying models using REST APIs, Flask/FastAPI, Docker, etc.\nFamiliarity with Linux environments and GPU-accelerated compute systems\nUnderstanding of hardware requirements and optimization for real-time ML performance\nWhy Join Hubnex Labs?\nWork on impactful AI products deployed in real-world use cases\nBe a part of a fast-growing tech consulting and product innovation company\nCollaborate with a diverse team of engineers, data scientists, and innovators\nFlexible and collaborative culture based in Gurugram , with hybrid work options\nIdeal Candidate\nPassionate about building smart systems that go live in production\nCan operate independently and take full ownership of ML products\nBlends deep technical skill with product thinking and business awareness",Industry Type: Internet,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Product engineering', 'Backend', 'Linux', 'Product innovation', 'Consulting', 'Machine learning', 'Web development', 'MongoDB', 'Business awareness', 'Python']",2025-06-12 13:55:12
Etl Engineer,Global Energy,4 - 9 years,Not Disclosed,[],"Role & responsibilities\n\nWe are looking for 5 years of experience in ETL\n\nWork mode :Remote\n\nMandatory skills : CData, ETl\n\nNeed a dedicated ETL engineer to manage all Extract, Transform, Load (ETL) processes.\nStrong expertise in ETL workflows is required.\nMust have hands-on experience with C-Data, as it is the primary tool used by the client.\nPreferably someone with advanced or in-depth experience in C-Data, not just basic knowledge.\nSome exposure or understanding of Salesforce is expected, since it is the client's main system.\nFamiliarity with complex healthcare data is preferred, as the data can be intricate and challenging to work with.\nData handling with C-Data drivers & knowledge should be there",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Cdata sync', 'ETL', 'Salesforce Integration', 'Etl Process', 'Informatica']",2025-06-12 13:55:13
Lead Power BI Developer,Conduent,8 - 13 years,Not Disclosed,['Bengaluru'],"Job Overview: \nWe are looking for a BI & Visualization Developer who will be part of our Analytics Practice and will be expected to actively work in a multi-disciplinary fast paced environment. This role requires a broad range of skills and the ability to step into different roles depending on the size and scope of the project; its primary responsibility is to support the design, development and maintainance of business intelligence and analytics solutions.\n\n\n Responsibilities: \nDevelop reports, dashboards, and advanced visualizations. Works closely with the product managers, business analysts, clients etc. to understand the needs / requirements and develop visualizations needed.\nProvide support to new of existing applications while recommending best practices and leading projects to implement new functionality.\nLearn and develop new visualization techniques as required to keep up with the contemporary visualization design and presentation.\nReviews the solution requirements and architecture to ensure selection of appropriate technology, efficient use of resources and integration of multiple systems and technology.\nCollaborate in design reviews and code reviews to ensure standards are met. Recommend new standards for visualizations.\nBuild and reuse template/components/web services across multiple dashboards\nSupport presentations to Customers and Partners\nAdvising on new technology trends and possible adoption to maintain competitive advantage\nMentoring Associates\n\n\n Experience Needed: \n8+ years of related experience is required.\nA Bachelor degree or Masters degree in Computer Science or related technical discipline is required\nHighly skilled in data visualization tools like PowerBI, Tableau, Qlikview etc.\nVery Good Understanding of PowerBI Tabular Model/Azure Analysis Services using large datasets.\nStrong SQL coding experience with performance optimization experience for data queries.\nUnderstands different data models like normalized, de-normalied, stars, and snowflake models.\nWorked in big data environments, cloud data stores, different RDBMS and OLAP solutions.\nExperience in design, development, and deployment of BI systems.\nCandidates with ETL experience preferred.\nIs familiar with the principles and practices involved in development and maintenance of software solutions and architectures and in service delivery.\nHas strong technical background and remains evergreen with technology and industry developments.\nAdditional\nDemonstrated ability to have successfully completed multiple, complex technical projects\nPrior experience with application delivery using an Onshore/Offshore model\nExperience with business processes across multiple Master data domains in a services based company\nDemonstrates a rational and organized approach to the tasks undertaken and an awareness of the need to achieve quality.\nDemonstrates high standards of professional behavior in dealings with clients, colleagues and staff.\nStrong written communication skills. Is effective and persuasive in both written and oral communication.\nExperience with gathering end user requirements and writing technical documentation\nTime management and multitasking skills to effectively meet deadlines under time-to-market pressure\nMay require occasional travel",Industry Type: BPM / BPO,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sql coding', 'azure analysis services', 'sql', 'tableau', 'tabular model', 'snowflake', 'rdbms', 'web services', 'bi', 'power bi', 'business analysis', 'business intelligence', 'dashboards', 'qlikview', 'report development', 'azure analysis', 'olap', 'code review', 'data visualization', 'etl', 'big data']",2025-06-12 13:55:16
Starburst Engineer,Luxoft,1 - 13 years,Not Disclosed,['Pune'],"Design, develop, and maintain scalable data solutions using Starburst.\nCollaborate with cross-functional teams to integrate Starburst with existing data sources and tools.\nOptimize query performance and ensure data security and compliance.\nImplement monitoring and alerting systems for data platform health.\nStay updated with the latest developments in data engineering and analytics.\nSkills\nMust have\nBachelors degree or Masters in a related technical field; or equivalent related professional experience.\nPrior experience as a Software Engineer applying new engineering principles to improve existing systems including leading complex, well defined projects.\nStrong knowledge of Big-Data Languages including:\nSQL\nHive\nSpark/Pyspark\nPresto\nPython\nStrong knowledge of Big-Data Platforms, such as:o The Apache Hadoop ecosystemo AWS EMRo Qubole or Trino/Starburst\nGood knowledge and experience in cloud platforms such as AWS, GCP, or Azure.\nContinuous learner with the ability to apply previous experience and knowledge to quickly master new technologies.\nDemonstrates the ability to select among technology available to implement and solve for need.\nAble to understand and design moderately complex systems.\nUnderstanding of testing and monitoring tools.\nAbility to test, debug, fix issues within established SLAs.\nExperience with data visualization tools (e.g., Tableau, Power BI).\nUnderstanding of data governance and compliance standards.\nNice to have\nData Architecture & Engineering: Design and implement efficient and scalable data warehousing solutions using Azure Databricks and Microsoft Fabric.\nBusiness Intelligence & Data Visualization: Create insightful Power BI dashboards to help drive business decisions.\nOther\nLanguages\nEnglish: C1 Advanced\nSeniority\nSenior\nRefer a Friend\nPositive work environments and stellar reputations attract and retain top talent. Find out why Luxoft stands apart from the rest.\nRecommend a friend\nRelated jobs View all vacancies\nSenior Azure AI & ML Engineer\nData Science\nUnited States of America\nAlpharetta\nSenior Azure AI & ML Engineer\nData Science\nUnited States of America\nRemote United States\nData Engineer with Neo4j\nData Science\nIndia\nChennai\nPune, India\nReq. VR-114886\nData Science\nBCM Industry\n05/06/2025\nReq. VR-114886\nApply for Starburst Engineer in Pune\n*",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data security', 'data governance', 'Engineering Design', 'Apache', 'Business intelligence', 'microsoft', 'Analytics', 'SQL', 'Python', 'Data architecture']",2025-06-12 13:55:18
Lead Pyspark Developer,Synechron,5 - 10 years,Not Disclosed,"['Chennai', 'Bengaluru']","job requisition idJR1027452\n\n\n\nOverall Responsibilities:\nData Pipeline Development:Design, develop, and maintain highly scalable and optimized ETL pipelines using PySpark on the Cloudera Data Platform, ensuring data integrity and accuracy.\nData Ingestion:Implement and manage data ingestion processes from a variety of sources (e.g., relational databases, APIs, file systems) to the data lake or data warehouse on CDP.\nData Transformation and Processing:Use PySpark to process, cleanse, and transform large datasets into meaningful formats that support analytical needs and business requirements.\nPerformance Optimization:Conduct performance tuning of PySpark code and Cloudera components, optimizing resource utilization and reducing runtime of ETL processes.\nData Quality and Validation:Implement data quality checks, monitoring, and validation routines to ensure data accuracy and reliability throughout the pipeline.\nAutomation and Orchestration:Automate data workflows using tools like Apache Oozie, Airflow, or similar orchestration tools within the Cloudera ecosystem.\nMonitoring and Maintenance:Monitor pipeline performance, troubleshoot issues, and perform routine maintenance on the Cloudera Data Platform and associated data processes.\nCollaboration:Work closely with other data engineers, analysts, product managers, and other stakeholders to understand data requirements and support various data-driven initiatives.\nDocumentation:Maintain thorough documentation of data engineering processes, code, and pipeline configurations.\n\n\n\nSoftware :\nAdvanced proficiency in PySpark, including working with RDDs, DataFrames, and optimization techniques.\nStrong experience with Cloudera Data Platform (CDP) components, including Cloudera Manager, Hive, Impala, HDFS, and HBase.\nKnowledge of data warehousing concepts, ETL best practices, and experience with SQL-based tools (e.g., Hive, Impala).\nFamiliarity with Hadoop, Kafka, and other distributed computing tools.\nExperience with Apache Oozie, Airflow, or similar orchestration frameworks.\nStrong scripting skills in Linux.\n\n\n\nCategory-wise Technical\n\nSkills:\nPySpark:Advanced proficiency in PySpark, including working with RDDs, DataFrames, and optimization techniques.\nCloudera Data Platform:Strong experience with Cloudera Data Platform (CDP) components, including Cloudera Manager, Hive, Impala, HDFS, and HBase.\nData Warehousing:Knowledge of data warehousing concepts, ETL best practices, and experience with SQL-based tools (e.g., Hive, Impala).\nBig Data Technologies:Familiarity with Hadoop, Kafka, and other distributed computing tools.\nOrchestration and Scheduling:Experience with Apache Oozie, Airflow, or similar orchestration frameworks.\nScripting and Automation:Strong scripting skills in Linux.\n\n\n\nExperience:\n5-12 years of experience as a Data Engineer, with a strong focus on PySpark and the Cloudera Data Platform.\nProven track record of implementing data engineering best practices.\nExperience in data ingestion, transformation, and optimization on the Cloudera Data Platform.\n\n\n\nDay-to-Day Activities:\nDesign, develop, and maintain ETL pipelines using PySpark on CDP.\nImplement and manage data ingestion processes from various sources.\nProcess, cleanse, and transform large datasets using PySpark.\nConduct performance tuning and optimization of ETL processes.\nImplement data quality checks and validation routines.\nAutomate data workflows using orchestration tools.\nMonitor pipeline performance and troubleshoot issues.\nCollaborate with team members to understand data requirements.\nMaintain documentation of data engineering processes and configurations.\n\n\n\nQualifications:\nBachelors or Masters degree in Computer Science, Data Engineering, Information Systems, or a related field.\nRelevant certifications in PySpark and Cloudera technologies are a plus.\n\n\n\nSoft\n\nSkills:\nStrong analytical and problem-solving skills.\nExcellent verbal and written communication abilities.\nAbility to work independently and collaboratively in a team environment.\nAttention to detail and commitment to data quality.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['cloudera', 'hive', 'pyspark', 'linux', 'hadoop', 'scala', 'amazon redshift', 'data warehousing', 'emr', 'sql', 'docker', 'apache', 'java', 'spark', 'gcp', 'etl', 'big data', 'hbase', 'data lake', 'python', 'oozie', 'airflow', 'microsoft azure', 'impala', 'data engineering', 'nosql', 'amazon ec2', 'mapreduce', 'kafka', 'sqoop', 'aws']",2025-06-12 13:55:21
Specialist Software Engineer - Large Molecule Discovery,Amgen Inc,4 - 6 years,Not Disclosed,['Hyderabad'],"What you will do\nIn this vital role you will.\nRole Description:\nThe role is responsible for designing, developing, and maintaining software solutions for Research scientists. Additionally, it involves automating operations, monitoring system health, and responding to incidents to minimize downtime.\nYou will join a multi-functional team of scientists and software professionals that enables technology and data capabilities to evaluate drug candidates and assess their abilities to affect the biology of drug targets. This team implements scientific software platforms that enable the capture, analysis, storage, and reporting for our Large Molecule Discovery Research team (Design, Make, Test and Analyze processes).\nThe team also interfaces heavily with teams supporting our in vitro assay management systems and our compound inventory platforms. The ideal candidate possesses experience in the pharmaceutical or biotech industry, strong technical skills, and full stack software engineering experience (spanning SQL, back-end, front-end web technologies, automated testing).\nRoles & Responsibilities:\nTake ownership of complex software projects from conception to deployment\nWork closely with product team, business team including scientists, and other collaborators\nAnalyze and understand the functional and technical requirements of applications, solutions and systems and translate them into software architecture and design specifications\nDesign, develop, and implement applications and modules, including custom reports, interfaces, and enhancements\nDevelop and execute unit tests, integration tests, and other testing strategies to ensure the quality of the software\nConduct code reviews to ensure code quality and alignment to standard methodologies\nCreate and maintain documentation on software architecture, design, deployment, disaster recovery, and operations\nProvide ongoing support and maintenance for applications, ensuring that they operate smoothly and efficiently\nStay updated with the latest technology and security trends and advancements\nWhat we expect of you\nWe are all different, yet we all use our unique contributions to serve patients. The professional we seek is a [type of person] with these qualifications.\nBasic Qualifications:\nDoctorate Degree OR Masters degree with 4 - 6 years of experience in Computer Science, IT, Computational Chemistry, Computational Biology/Bioinformatics or related field OR\nBachelors degree with 6 - 8 years of experience in Computer Science, IT, Computational Chemistry, Computational Biology/ Bioinformatics or related field OR\nDiploma with 10 - 12 years of experience in Computer Science, IT, Computational Chemistry, Computational Biology/ Bioinformatics or related field\nPreferred Qualifications and Experience:\n3+ years of experience in implementing and supporting biopharma scientific software platforms\nSome experience with ML or generative AI technologies\nProficient in Java or Python\nProficient in at least one JavaScript UI Framework (e.g. ExtJS, React, or Angular)\nProficient in SQL (e.g. Oracle, PostgreSQL, Databricks)\nExperience with event-based architecture and serverless AWS services such as EventBridge, SQS, Lambda or ECS.\nPreferred Qualifications:\nExperience with Benchling\nHands-on experience with Full Stack software development\nStrong understanding of software development methodologies, mainly Agile and Scrum\nWorking experience with DevOps practices and CI/CD pipelines\nExperience of infrastructure as code (IaC) tools (Terraform, CloudFormation)\nExperience with monitoring and logging tools (e.g., Prometheus, Grafana, Splunk)\nExperience with automated testing tools and frameworks\nExperience with big data technologies (e.g., Spark, Databricks, Kafka)\nExperience with leveraging the use of AI-assistants (e.g. GitHub Copilot) to accelerate software development and improve code quality\nProfessional Certifications (please mention if the certification is preferred or mandatory for the role):\nAWS Certified Cloud Practitioner preferred\nSoft Skills:\nExcellent problem solving, analytical, and troubleshooting skills\nStrong communication and interpersonal skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation\nAbility to learn quickly & work independently\nTeam-oriented, with a focus on achieving team goals\nAbility to manage multiple priorities successfully\nStrong presentation and public speaking skills.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Large Molecule Discovery', 'Java', 'DevOps', 'PostgreSQL', 'JavaScript', 'Databricks', 'Oracle', 'React', 'AWS', 'ExtJS', 'Angular', 'Python']",2025-06-12 13:55:23
Informatica IDMC Engineer,Qentelli,5 - 10 years,15-25 Lacs P.A.,['Hyderabad'],"Job Summary:\nWe are seeking a highly skilled Informatica IDMC & Data Governance Specialist to join our dynamic team. This role requires hands-on expertise in various IDMC modules, including Snowflake, Cloud Data Quality (CDQ), Cloud Application Integration (CAI), Cloud Data Governance & Catalog (CDGC), and Cloud Data Marketplace (CDMP). The ideal candidate will have a strong background in data governance and data quality, with a proven ability to create and configure data profiles and quality rules. If you have a passion for cloud technologies and data management, this is an exciting opportunity to contribute to the ongoing success of our data initiatives.\n\nKey Responsibilities:\nIDMC Modules Management: Manage and support the following IDMC modules:\nSnowflake (including RBAC)\nCloud Data Quality (CDQ)\nCloud Application Integration (CAI)\nCloud Data Governance & Catalog (CDGC)\nCloud Data Marketplace (CDMP)\nData Profiling & Quality: Create and configure Data Profiles with appropriate quality rules to ensure data meets required standards.\nData Governance & Compliance: Collaborate with data governance teams to ensure compliance with data governance policies and standards.\nPlatform Optimization & Troubleshooting: Continuously monitor the performance of IDMC platforms, resolving issues, optimizing configurations, and supporting ongoing integrations and data workflows.\nCollaboration & Support: Work cross-functionally with infrastructure, application, and data teams to ensure efficient use of the IDMC platform and ensure business data needs are met.\n\nMandatory Skills:\nExtensive work experience is required for below areas to configure everything from scratch for any complex requirement\nCDI (Cloud Data Integration)\nCDQ (Cloud Data Quality)\nCAI (Cloud Application Integration)\nEDC (On-Prem Informatica Enterprise Data Catalog)\n\nNice-to-Have Skills:\nKnowledge of Data Integration (IPC) and Git.\nPrior experience in Data Governance initiatives, including the use of governance frameworks and best practices.\n\nQualifications:\nStrong analytical and problem-solving abilities.\nExcellent communication skills, both written and verbal, to explain technical concepts to nontechnical stakeholders.\nAbility to prioritize tasks, manage multiple projects, and work in a fast-paced environment.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Cloud Data Governance & Catalog', 'Cloud Application Integration', 'Cloud Data Quality', 'Cloud Data Marketplace', 'Snowflake']",2025-06-12 13:55:26
EPM Principal/Sr Principal Consultant FCCS/ARCS/TRCS/PBCS/EPBCS/PCMCS,Oracle,10 - 20 years,Not Disclosed,"['Pune', 'Bengaluru', 'Mumbai (All Areas)']","Hiring for Oracle Experts at different level from 5-20 years.\n\nJob locations - Bangalore, Mumbai, Pune, Hyderabad, Chennai, Kolkata, Noida, Gurgaon, Gandhinagar\n\nEPM Products - ARCS, TRCS, FCCS, EPBCS, PBCS, EPRCS, PCMCS\n\nPlanning - PBCS/ EPBCS\nExperience in Implementation of Hyperion with strong Application Development process experience on Hyperion EPM Product Suite.\nExperience in Requirement Gathering & Solution Design\nSound knowledge on Hyperion Planning/PBCS/EPBCS\nSound functional knowledge (Understand of planning modelling like P&L, BS, Workforce, Capex planning etc.. and inter dependencies)\nSound Knowledge on Business Rules/Forms / Task Lists / Reports.\nHands on Experience on Planning Modules is must.\nGood communication Skills\n\nFCCS\n\nFunction as applications design architect/Lead for Oracle FCCS\nApplication Design point of contact for FCCS Analyst Teams\nProvide Solutions to existing Architecture Design on the current system\nCollaborate effectively with other groups\nAdditional Requirements:\nEPM Experience 5+ Years\nExperience in Implementation of EPM cloud with strong Application Development process, experience on FCCS/HFM and good knowledge on consolidation process.\nExperience in Requirement Gathering & Solution Design\nDesired functional knowledge (Understand of Income statement, Balance Sheet, different methods of consolidation and their calculations and disclosure in financial statements)\nSound functional knowledge Finance/accounting/ General Ledger/Sub Ledgers\nSound Knowledge on Financial Reports and SmartView Reports\n\nARCS\nExperience implementing ARCS from design, configuration, data integration, and testing\nSound knowledge on ARM/ARCS including Reconciliation Compliance & Transaction Matching\nFunctional knowledge of Finance/accounting and account reconciliation is a must\nKnowledge and experience working with a consolidation tool and general ledger is a plus\nProvide Solutions to existing Architecture Design on current system\nCollaborate effectively with other groups\nAdditional Requirements:\nEPM Experience 5+ Years\nExperience in Implementation of Hyperion with strong Application Development process experience on Hyperion EPM Product Suite.\nExperience in Requirement Gathering & Solution Design\nSound functional knowledge Finance/accounting/ General Ledger/Sub Ledgers\nSound Knowledge on standard and custom reports\n\nTRCS\nFunction as applications design architect/Lead for Tax Reporting Cloud application development\nApplication Design point of contact for Tax Reporting Teams\nProvide Solutions to existing Architecture Design on current system\nCollaborate effectively with other groups\n\nEPM Experience 5+ Years\nExperience in Implementation of Hyperion with strong Application Development process experience on Hyperion EPM Product Suite.\nExperience in Requirement Gathering & Solution Design\nSound knowledge on Tax reporting and compliance processes, tax accounting and direct tax functions, CBCR and Deferred Tax Calculations\nSound knowledge on Hyperion Consolidation\nDesired functional knowledge (Understand of Income statement, Balance Sheet, different methods of consolidation and their calculations and disclosure in financial statements)\nSound Knowledge on Business Rules/Forms / Task Lists / Reports.\nGood communication Skills\n\nPCMCS\nFunction as applications design architect/Lead for Profitability and Cost Management\nApplication Design point of contact Profitability and Cost Management\nProvide Solutions to existing Architecture Design on current system\nAble to understand functional requirement of client and build the solution accordingly.\nCollaborate effectively with other groups\nAdditional Requirements:\nEPM Experience 7+ Years\nShould have completed at least 3 implementations on PCMCS.\nIn depth understanding of Oracle Hyperion Essbase (ASO and BSO)\nIn depth knowledge in Integration (Data Management)\nAbility to design and develop complex Reports using Web Reporting Studio.\nExperience in Implementation of Hyperion with strong Application Development process experience on Hyperion EPM Product Suite.\nExperience in Requirement Gathering & Solution Design\nAble to leverage the modern best practices for design-development and automation process wherever required.\nSound functional knowledge\nKnowledge on Microsoft office tools including Excel, Word and Power point, leverage Smartview to build report and ad hoc analysis.\nPCMCS Certification is a value add",Industry Type: Software Product,Department: Consulting,"Employment Type: Full Time, Permanent","['EPM', 'Cloud EPM', 'solution design', 'consulting', 'implementation', 'EPBCS', 'EPRCS', 'PBCS', 'TRCS', 'PCMCS', 'FCCS', 'ARCS']",2025-06-12 13:55:28
AI/ML,Larsen & Toubro (L&T),2 - 4 years,Not Disclosed,"['Chennai', 'Bengaluru']","Experience Required\n\n2 to 4 years of experience in AI/ML model development, deployment, and optimization. Hands-on experience in building machine learning pipelines and working with large datasets\n\nDomain Experience (Functional)\nExperience in domains such as natural language processing (NLP), computer vision, predictive analytics, or recommendation systems. Exposure to industry-specific AI applications (e.g., healthcare, finance, retail, manufacturing) is a plus.\n\nQualification\nBachelors or Masters degree in Computer Science, Artificial Intelligence, Data Science, Mathematics, or a related field\n\nRoles & Responsibilities\nDesign, develop, and deploy machine learning and deep learning models.\nCollaborate with data engineers and domain experts to collect, clean, and preprocess data.\nConduct experiments, evaluate model performance, and iterate for improvement.\nIntegrate AI models into production systems and monitor their performance.\nStay updated with the latest research and advancements in AI/ML.\nDocument model development processes and contribute to knowledge sharing.\n\nTechnical Skills\n\nProficient in Python and core ML libraries: TensorFlow, PyTorch, Scikit-learn.\nStrong with Pandas, NumPy for data handling.\nSolid grasp of ML algorithms, statistics, and model evaluation.\nFamiliar with cloud platforms (AWS/Azure/GCP).\nExperience with Git and basic CI/CD for model deployment",Industry Type: Engineering & Construction,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Npl', 'Aiml', 'Tensorflow', 'Ci/Cd', 'Machine Learning', 'Deep Learning', 'Scikit-Learn', 'Numpy', 'Pytorch', 'GCP', 'Pandas', 'Microsoft Azure', 'AWS', 'Python']",2025-06-12 13:55:30
Senior Analytics Consultant,Wells Fargo,4 - 9 years,Not Disclosed,['Hyderabad'],"About this role:\nWells Fargo is seeking a Senior Analytics Consultant with a proven track record of success preferably in the banking industry.\n\nIn this role, you will:\nConsult, review and research moderately complex business, operational, and technical challenges that require an in-depth evaluation of variable data factors",,,,"['data manipulation', 'Data Engineering', 'data analysis', 'data management', 'SQL']",2025-06-12 13:55:32
Software Engineer,Growexx,2 - 7 years,Not Disclosed,['Ahmedabad'],"join our growing engineering team. You will play a key role in designing, developing, and maintaining scalable software solutions that power our analytics platform. This is an exciting opportunity to work on impactful projects in a collaborative, fast-paced environment. Key Responsibilities\nDesign, develop, test, and deploy high-quality software solutions\nCollaborate with product managers, designers, and data scientists to deliver new features and enhancements\nWrite clean, maintainable, and efficient code following best practices\nParticipate in code reviews and contribute to the continuous improvement of engineering processes\nTroubleshoot and resolve technical issues across the stack\nStay current with emerging technologies and propose innovative solutions\nKey Skills\nProficiency in one or more programming languages (e.g., Python, JavaScript, TypeScript, Go, Java)\nExperience with modern web frameworks (e.g., React, Angular, Vue)\nFamiliarity with RESTful APIs, microservices, and cloud platforms (e.g., AWS, Azure, GCP)\nStrong problem-solving skills and attention to detail\nExperience with data visualization libraries (e.g., D3.js, Plotly)\nKnowledge of data pipelines, ETL processes, or big data technologie\nFamiliarity with containerization (Docker, Kubernetes)\nExposure to machine learning or AI-driven applications\nEducation and Experience\nBachelor s degree in Computer Science, Engineering, or related field\n2+ years of professional software development experience Analytical and Personal skills Must have good logical reasoning and analytical skills\nAbility to break big goals to small incremental actions\nExcellent Communication skills in English both written and verbal\nDemonstrate Ownership and Accountability of their work\nGreat attention to details\nDemonstrate ownership of tasks Positive and Cheerful outlook in life Work with the problem solver engineers team (Doc / PDF Only, Max file size 2 MB) By using this form you agree with the storage and handling of your data by this website. *\nYou cannot copy content of this page\nReconciliation Automation Data Sheet\nThis field is for validation purposes and should be left unchanged.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Automation', 'PDF', 'Analytical', 'Reconciliation', 'Machine learning', 'Javascript', 'Continuous improvement', 'Analytics', 'Python']",2025-06-12 13:55:35
Asset & Wealth Management - AM FI Macro Strats - Associate,Goldman Sachs,2 - 7 years,Not Disclosed,['Bengaluru'],"Who We Are\nAt Goldman Sachs, we connect people, capital and ideas to help solve problems for our clients. We are a leading global financial services firm providing investment banking, securities and investment management services to a substantial and diversified client base that includes corporations, financial institutions, governments and individuals.\nAt Goldman Sachs, our Engineers don t just make things - we make things possible. We change the world by connecting people and capital with ideas and solve the most challenging and pressing engineering problems for our clients. Our engineering teams build scalable software and systems, architect low latency infrastructure solutions, proactively guard against cyber threats, and leverage machine learning alongside financial engineering to continuously turn data into action.\nEngineering, which is comprised of our Technology Division and global strategist groups, is at the critical center of our business. Our dynamic environment requires innovative strategic thinking. Want to push the limit of digital possibilities? Start here.\nGoldman Sachs Asset & Wealth Management\nAs one of the worlds leading asset managers, our mission is to help our clients achieve their investment goals. To best serve our clients diverse and evolving needs, we have built our business to be global, broad and deep across asset classes, geographies and solutions.\nGoldman Sachs Asset & Wealth Management is one of the worlds leading asset management institutions. AWM delivers innovative investment solutions managing close to Two Trillion US Dollars on a global, multi-product platform. In addition to traditional products (e.g. Equities, Fixed Income) our product offering also includes Hedge Funds, Private Equity, Fund of Funds, Quantitative Strategies, Fundamental Equity and a Multi-Asset Pension Solutions Business. Software is engineered in a fast-paced, dynamic environment, adapting to market and customer needs to deliver robust solutions in an ever-changing business environment. AM Data Engineering builds on top of cutting edge in-house and cloud platforms complimented with a strong focus on leveraging open source solutions.\nBusiness Overview\nThe External Investing Group ( XIG ) provides investors with investment and advisory solutions across leading private equity funds, hedge fund managers, real estate managers, public equity strategies, and fixed income strategies. XIG manages globally diversified programs, targeted sector-specific strategies, customized portfolios, and a range of advisory services. Our investors access opportunities through new fund commitments, fund-of-fund investments, strategic partnerships, secondary-market investments, co-investments, and seed-capital investments. With over 350 professionals across 11 offices around the world, XIG provides manager diligence, portfolio construction, risk management, and liquidity solutions to investors, drawing on Goldman Sachs market insights and risk management expertise. We extend these global capabilities to the world s leading sovereign wealth funds, pension plans, governments, financial institutions, endowments, foundations, and family offices, for which we invest or advise on over $300 billion of alternative investments, public equity strategies, and fixed income strategies.\nWhat We Do\nWithin Asset Management, Strategists (also known as Strats ) play important roles in research, valuation, portfolio construction, and risk management analytics. A Strategist will apply quantitative and analytical methods to come up with solutions that are accurate, robust, and scalable. Strats are innovators and problem-solvers, building novel and creative solutions for manager selection, portfolio construction, and risk management. You will develop advanced computational models, architectures, and applications to meet the challenges of a rapidly growing and evolving business.\nStrats collaborate across the business to develop solutions. These daily interactions with other team members across geographies demand an ability to communicate clearly about complex financial, business, and mathematical concepts. We look for creative collaborators who evolve, adapt to change, and thrive in a fast-paced global environment.\nBasic Qualifications\nOutstanding background in a quantitative discipline, with excellent analytical, quantitative, and problem-solving skills, and demonstrated abilities in research and data visualization\nProgramming expertise in a scripting language (e.g. Python, R, Matlab)\nStrong general and technical communication skills, with an ability to effectively articulate complex financial and mathematical concepts\nCreativity and problem-solving skills\nAbility to work independently and in a team environment\n2+ years of applicable experience\nGoldman Sachs Engineering Culture",Industry Type: Banking,"Department: BFSI, Investments & Trading","Employment Type: Full Time, Permanent","['Wealth management', 'Analytical', 'Fixed income', 'Investment banking', 'Asset management', 'Investment management', 'Risk management', 'Private equity', 'Analytics', 'Financial services']",2025-06-12 13:55:37
Sr. Technology Auditor,AMERICAN EXPRESS,2 - 4 years,13-18 Lacs P.A.,"['Gurugram', 'Delhi / NCR']","Role & responsibilities\n•       Translate business risks, controls and supporting data into analytic requirements and partners with colleagues to build effective analytics and insights\n•       Responsible for multiple simultaneous audit projects of all sizes and complexity across multiple business areas within and outside of local region, in unfamiliar areas, and for different audit leaders\n•       Link analytics and insights to ongoing strategic initiatives\n•       Apply proven/ advanced data algorithms, advanced analytic and modeling techniques to draw insights essential to driving improvement initiatives",,,,"['Natural Language Processing', 'Tableau', 'Machine Learning', 'SQL', 'Python']",2025-06-12 13:55:40
Full Stack AI Engineer (Lead),Inclusive Business Solutions,3 - 8 years,20-35 Lacs P.A.,[],"AI specialists for Full Stack, Computer Vision, and Speech Processing roles. Responsibilities include real-time data integration, emotion recognition, and speech analysis. Must have expertise in AI frameworks, ML models, and optimization techniques.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Speech Recognition', 'Natural Language Processing', 'Computer Vision', 'Machine Learning', 'Python', 'Tensorflow', 'Large Language Model', 'Artificial Intelligence', 'AWS', 'Deep Learning']",2025-06-12 13:55:42
Customer Support Engineer/ OPS Engineer,Pratiti Technologies,1 - 4 years,Not Disclosed,['Pune'],"[{""Posting_Title"":""Customer Support Engineer/ OPS Engineer"" , ""Is_Locked"":false , ""City"":""Pune City"",""Industry"":""Software Product"",""Job_Description"":""\nCustomer Support Engineer/ OPS Engineer\n\nRole description for Customer Support Engineer/ OPS Engineer\nWe are looking for a dynamic executive to join our customer support operations team. The\nexecutive will be responsible for technical customer support and manage relationships with\ncustomers. This position is ideal for young solar photovoltaic enthusiast in renewable energy\nsector, who is looking for demonstrating technical understanding in challenges in solar PV\npower plant O&M and asset management and providing best customer support service.\n\nThe executive would be closely working directly with customer support manager and\noperations head on daily basis. The executive will be hands on working on Apollo\\u2122, our\npatented performance intelligence and health analytics solution for solar PV power plants.\nResponsibilities\n1. Configuration of new plants on to Apollo\n2. Checking all engineering files of the power plant received from the customer\n3. Setting up plant topology and configuration in Apollo\n4. Setting up data pipeline connection between Apollo and customer site\n5. Activating all product features in apollo relayed to central monitoring, analytics and\nCMMS\n6. Validating plant onboarding checks and preparing handover files to the customer\n7. Creating UAT report for the handover plants\nMust-Haves\n\\u2219 Customer Support and Technical Support skills\n\\u2219 Troubleshooting and Field Service abilities\n\\u2219 Data Integration Protocol\n\\u2219 Experience in renewable energy or related industry is a plus\n\\u2219 Knowledge of solar and wind power plant operations\n\\u2219 Bachelor\\u2019s degree in Engineering or renewable field\n\\u2219 Pune based candidates only\n\nQualification\nBachelor in Computer Science/Engineering\n\na\n\n\n\n\n\nCustomer Support Engineer/ OPS Engineer\n\n\n\n\n"",""Job_Type"":""Full time"" , ""Job_Opening_Name"":""Customer Support Engineer/ OPS Engineer"" , ""State"":""Maharashtra"" , ""Country"":""India"" , ""Zip_Code"":""411001"" , ""id"":""481259000051634803"" , ""Publish"":true , ""Date_Opened"":""2025-04-17"" , ""Keep_on_Career_Site"":false}]",Industry Type: IT Services & Consulting,"Department: Customer Success, Service & Operations","Employment Type: Full Time, Permanent","['Computer science', 'Plant operations', 'Renewable energy', 'Senior Executive', 'Customer support', 'Asset management', 'Troubleshooting', 'Technical support', 'Monitoring', 'Analytics']",2025-06-12 13:55:45
"Business Research Analyst - II, RBS ACCX Program",Amazon,3 - 8 years,Not Disclosed,['Bengaluru'],"Amazon.com strives to be Earths most customer-centric company where people can find and discover virtually anything they want to buy online. By giving customers more of what they want low prices, vast selection, and convenience Amazon.com continues to grow and evolve as a world-class e-commerce platform. Amazons evolution from Web site to e-commerce partner to development platform is driven by the spirit of innovation that is part of the companys DNA. The worlds brightest technology minds come to Amazon.com to research and develop technology that improves the lives of shoppers and sellers around the world.\n\nOverview of the role\nThe Business Research Analyst will be responsible for Data and Machine learning part of continuous improvement projects across the Discoverability space. This will require collaboration with local and global teams. The Research Analyst should be a self-starter who is passionate about discovering and solving complicated problems, learning complex systems, working with numbers, and organizing and communicating data and reports. The Research Analyst will perform Big data analysis to identify patterns, train model to generate product to product relationship and product to brand & model relationship. The Research Analyst is also expected to continuously improve the ML/LLM solutions in terms of precision & recall, efficiency and scalability. The Research Analyst should be able to write clear and detailed functional specifications based on business requirements.\n\n\nScoping, driving and delivering complex projects across multiple teams.\nPerforms root cause analysis by understanding the data need, get data / pull the data and analyze it to form the hypothesis and validate it using data.\nBuild programs to create a culture of continuous improvement within the business unit, and foster a customer-centric focus on the quality, productivity, and scalability of our services.\nFind the scalable solution for business problem by executing pilots and build Deterministic and ML/LLM models.\nManages meetings, business and technical discussions regarding their part of the projects.\nMakes recommendations and decisions that impact development schedules and the success for a product or project.\nDrives team(s)/partners to meet program and/or product goals.\nCoordinates design effort between internal team and External team to develop optimal solutions.\nPerforms supporting research, conduct analysis of the bigger part of the projects and effectively interpret reports to identify opportunities, optimize processes, and implement changes.\nAbility to convince and interact with stakeholders at all level either to gather data and information or to execute and implement according to the plan.\nAbility to deal with ambiguity and problem solver\nCommunicate ideas effectively and with influence (both verbally and in writing), within and outside the team.\n\nKey Performance Areas:\nSolve large and complex business problems by aligning multiple teams together.\nData analytics and Data Sciences\nMachine learning\nProject/Program Management\nAutomation initiative conceptualization and implementation\nBig Data analytics\nProduct development Scoping and Testing\nDefect Elimination\nAgile Continuous Improvement\n\nAbout the team\nThe RBS group in Chennai/Bangalore is an integral part of Amazon online product lifecycle and buying operations. The team is designed to ensure Amazon remains competitive in the online retail space with the best price, wide selection and good product information. The team s primary role is to create and enhance retail selection on the worldwide Amazon online catalog. The tasks handled by this group have a direct impact on customer buying decisions and online user experience. 3+ years of analyzing and interpreting data with Redshift, Oracle, NoSQL etc. experience\nExperience with data visualization using Tableau, Quicksight, or similar tools\nExperience with data modeling, warehousing and building ETL pipelines\nExperience writing complex SQL queries\nExperience in Statistical Analysis packages such as R, SAS and Matlab\nExperience using SQL to pull data from a database or data warehouse and scripting experience (Python) to process data for modeling Experience with AWS solutions such as EC2, DynamoDB, S3, and Redshift\nExperience in data mining, ETL, etc. and using databases in a business environment with large-scale, complex datasets",,,,"['Automation', 'Data analysis', 'SAS', 'Data modeling', 'Machine learning', 'Agile', 'Oracle', 'Data mining', 'MATLAB', 'Python']",2025-06-12 13:55:48
"Sr Business Analyst (Process Modeling, and Stakeholder Collaboration)",Synechron,7 - 12 years,Not Disclosed,"['Pune', 'Hinjewadi']","Job Summary\nSynechron is seeking a highly experienced and detail-oriented Senior Business Analyst to join our dynamic team. In this role, you will serve as a key contributor to our business analysis function, translating complex business needs into effective solutions that support organizational goals. Your expertise will enable our teams to deliver value-driven projects efficiently and effectively, ensuring alignment with strategic objectives and stakeholder expectations.\nSoftware Requirements\nRequired Skills:\nBusiness analysis tools (e.g., Microsoft Visio, (version 2016 or later))\nData analysis and visualization software (e.g., Microsoft Excel - advanced proficiency, tools like Tableau or Power BI)\nRequirement management tools (e.g., Jira, Confluence - recent versions)\nWorkflow and process modeling software (e.g., BPMN tools)\nPreferred Skills:\nBasic understanding of enterprise-level ERP/CRM systems (e.g., SAP, Salesforce)\nKnowledge of project management tools (e.g., Microsoft Project, MS Teams)\nOverall Responsibilities\nGather, analyze, and document business requirements by engaging with stakeholders, ensuring clarity, completeness, and alignment with organizational objectives.\nDevelop detailed functional specifications, use cases, process flows, and user stories to guide development teams and project execution.\nFacilitate communication between business units and technical teams to ensure a mutual understanding of project scope and deliverables.\nSupport project planning, monitoring progress, and ensuring deliverables meet quality standards and deadlines.\nContribute to process improvement initiatives by analyzing current workflows and recommending efficiencies.\nAssist in testing and validating solutions to verify they meet business needs and specifications.\nProvide ongoing support during implementation, including stakeholder training and documentation.\nStrategic Objectives:\nDeliver comprehensive requirements that enable timely and successful project deliveries.\nEnhance stakeholder engagement and satisfaction through clear communication and tailored solutions.\nPromote continuous improvement by identifying opportunities to optimize business processes.\nPerformance Outcomes & Expectations:\nAccurate and comprehensive requirement documentation.\nSuccessful facilitation of collaborative sessions and stakeholder buy-in.\nOn-time delivery of specifications and supporting documentation.\nPositive feedback from stakeholders regarding clarity and usability of deliverables.\nTechnical Skills (By Category)\nProgramming Languages:\nRequired: Basic understanding of scripting or programming concepts (e.g., SQL, Python) is preferred but not mandatory.\nPreferred: None specifically required.\nDatabases/Data Management:\nRequired: Experience with relational databases (e.g., SQL Server, Oracle) and data querying techniques.\nPreferred: Experience with big data tools or NoSQL databases.\nCloud Technologies:\nRequired: Familiarity with cloud platforms (e.g., AWS, Azure) focusing on cloud-based data storage and services.\nPreferred: Certification or practical experience in cloud services.\nFrameworks and Libraries:\nRequired: Understanding of business process frameworks (e.g., BPMN, UML modeling).\nPreferred: Knowledge of agile frameworks like Scrum or Kanban.\nDevelopment Tools & Methodologies:\nRequired: Experience with Agile, Scrum, or Waterfall project methodologies.\nPreferred: Exposure to DevOps practices.\nSecurity Protocols:\nOptional: Basic understanding of data security, compliance, and privacy protocols relevant to business analysis.\nExperience Requirements\nMinimum of 7+ years in business analysis roles within financial services or related industries.\nProven track record of managing complex projects from requirements gathering through implementation.\nExtensive experience in stakeholder engagement, documentation, and process modeling.\nExperience working in diverse regulatory environments and compliance standards is advantageous.\nCandidates with alternative pathways demonstrating equivalent skillssuch as extensive cross-functional project leadershipare encouraged to apply.\nDay-to-Day Activities\nConduct interviews and workshops with stakeholders to elicit detailed business requirements.\nAnalyze existing business processes and document workflows to identify improvement opportunities.\nPrepare functional specifications, use cases, user stories, and process diagrams for project teams.\nCollaborate closely with developers, testers, and project managers in an Agile or traditional setting.\nParticipate in sprint planning, review sessions, and status meetings.\nSupport user acceptance testing (UAT) and assist with issue resolution.\nMaintain clear and organized documentation of requirements, decisions, and project artifacts.\nProvide ongoing communication and updates to stakeholders on project progress.\nDecision-Making Authority & Responsibilities:\nValidate solution approaches against requirements.\nRecommend process improvements and inform implementation strategies.\nEscalate issues related to scope or requirements misalignment to project leadership.\nQualifications\nBachelors degree in Business Administration, Information Systems, Computer Science, or related field.\nRelevant certifications (preferred but not mandatory): CBAP, CCBA, PMI-PBA, or equivalents.\nParticipation in ongoing professional development, such as courses in business analysis, project management, or domain-specific training.\nDemonstrated commitment to continuous learning and adapting industry best practices.\nProfessional Competencies\nStrong analytical and critical thinking skills, with an ability to interpret complex data and business scenarios.\nEffective collaboration and stakeholder management skills across varying levels of the organization.\nExcellent written and verbal communication abilities, ensuring clarity and mutual understanding.\nResilience and adaptability in fast-paced environments, with a proactive approach to problem-solving.\nInnovative mindset, open to leveraging new tools and methods to enhance processes.\nSkilled in prioritizing tasks, managing time efficiently, and meeting deadlines.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Python', 'Azure', 'Kanban', 'NoSQL', 'Scrum', 'SQL Server', 'Oracle', 'AWS']",2025-06-12 13:55:50
Azure Devops Engineer,Celebal Technologies,4 - 9 years,10-20 Lacs P.A.,['Jaipur( Malviya Nagar )'],"Job Role Azure DevOps Engineer\n\nJob Location – Jaipur\nExperience Required-4+ Years\n\nAbout Us -Celebal Technologies is a premier software services company in the field of Data Science, Big Data and Enterprise Cloud. Celebal Technologies helps you to discover the competitive advantage by employing intelligent data solutions using cutting-edge technology solutions that can bring massive value to your organization. The core offerings are around ""Data to Intelligence"", wherein we leverage data to extract intelligence and patterns thereby facilitating smarter and quicker decision making for clients. With Celebal Technologies, who understands the core value of modern analytics over the enterprise, we help the business in improving business intelligence and more data-driven in architecting solutions.",,,,"['Azure Kubernetes', 'Azure Cloud', 'Azure Devops', 'Azure Pipelines', 'Ci Cd Pipeline', 'Container Orchestration', 'Aks', 'Arm Templates', 'Azure Monitoring', 'Terraform', 'Containerization', 'Docker', 'Kubernetes']",2025-06-12 13:55:52
Sr. Developer,Cognizant,7 - 10 years,Not Disclosed,['Chennai'],"Data Engineer Skills and Qualifications\nSQL - Mandatory\nStrong knowledge of AWS services (e.g., S3, Glue, Redshift, Lambda). - Mandatory\nExperience working with DBT – Nice to have\nProficiency in PySpark or Python for big data processing. - Mandatory\nExperience with orchestration tools like Apache Airflow and AWS CodePipeline. - Mandatory\nFamiliarity with CI/CD tools and DevOps practices.",,,,"['continuous integration', 'kubernetes', 'orchestration', 'aws iam', 'modeling', 'amazon redshift', 'data warehousing', 'pyspark', 'ci/cd', 'aws codedeploy', 'tools', 'sql', 'docker', 'apache', 'java', 'data modeling', 'devops', 'linux', 'etl', 'big data', 'cd', 'python', 'airflow', 'data processing', 'javascript', 'lambda expressions', 'aws', 'etl process']",2025-06-12 13:55:55
Sr. Computer Scientist-I,Adobe,8 - 13 years,Not Disclosed,['Noida'],"Our engineering team develops the Adobe Experience Platform, offering innovative data management and analytics.\nDeveloping a reliable, resilient system at large scale is crucial. We use Big Data and open-source tech for Adobes services.\nOur support for large enterprise products spans across geographies, requiring us to manage disparate data sources and ingestion mechanisms. The data must be easily accessible at very low latency to support various scenarios and use cases. We seek candidates with deep expertise in building low latency services at high scales who can lead us in accomplishing our vision.\n  What you will need to succeed\n8+ years in design and development of data-driven large distributed systems\n3+ years as an architect building large-scale data-intensive distributed systems and services\nRelevant experience building application layers on top of Apache Spark\nStrong experience with Hive SQL and Presto DB\nExperience leading architecture designs to approval while collaborating with multiple collaborators, dependencies, and internal/external customer requirements\nIn-depth work experience with open-source technologies like Apache Kafka, Apache Spark, Kubernetes, etc\nExperience with big data technologies on public clouds such as Azure, AWS, or Google Cloud Platform\nExperience with in-memory distributed caches like Redis, Memcached, etc\nStrong coding (design patterns) and design proficiencies setting examples for others; contributions to open source are highly desirable\nProficiency in data structures and algorithms\nCost consciousness around computation and memory requirements\nStrong verbal and written communication skills\nBTech/MTech/MS in Computer Science\nWhat you'll do\nLead the technical design and implementation strategy for major systems and components of the Adobe Experience Platform\nEvaluate and drive the architecture and technology choices for major systems/components\nDesign, build, and deploy products with outstanding quality\nInnovate the current system to improve robustness, ease, and convenience\nArticulate design and code choices to cross-functional teams\nMentor and guide a high-performing team\nReview and provide feedback on features, technology, architecture, design, time & budget estimates, and test strategies\nEngage in creative problem-solving\nDevelop and evolve engineering standard methodologies to improve the team s efficiency\nPartner with other teams across Adobe to achieve common goals",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data management', 'Coding', 'Data structures', 'Open source', 'Adobe', 'big data', 'Distribution system', 'Analytics', 'SQL']",2025-06-12 13:55:57
Senior Mis Executive,Thyrocare,2 - 6 years,2.5-5 Lacs P.A.,['Mumbai (All Areas)'],"Roles and Responsibilities\nAnalyse, extract, and review relevant data from various client and proprietary systems to create spreadsheet reports for use of management, efficiency, quality, and productivity analysis, employee stack-ranking, cost analysis, billing/invoicing\nAnalyse data and publish daily, weekly and monthly reports as per the pre-defined timelines\nBuild and manage the tools to collect raw data that is not available from current software and systems.\nCreate multi-level reports from the same data to serve multiple stakeholders with minimum manual re-work\nSuch other activities as may be assigned by your manager\nParticipate in cross-functional meetings to resolve recurring customer issues.\nAnalyze current business processes and make recommendations for improvements\nMaintain thorough understanding of data and information resources\nMaintain a status on all projects and proactively communicate with management\n\nDesired Candidate Profile\nCandidates with 3+ Years of experience in creating and maintaining MIS reports for Banking, Financial Services, BPO/KPO/LPO, or Back-office Operations.\nAdvanced Excel Knowledge including but not limited to: Creating Impressive Dashboards, working with large excel data running into lakhs of rows and several hundred columns, use of excel tools and formulas: pivot, xlookup, vlookup, index, sumifs, countifs, maxifs, sumproduct, offset, string and date related formulas, multiple layer nested if loops, rank, use of array formulas such as unique, filter, sort, sortby.\nAnd Knowledge of Big Data analysis tools such as SQL, Python etc\nKnowledge of Power Suite: Power Query, Power Automate and Power BI would be an added advantage\nProficiency with macros and VBA coding would be an added advantage\nHigh attention to detail\nMust have an analytical bent of mind\nShould be able to build tools with high scalability and agility\nQualifications/ Requirements:\nUG- Any Graduate\nHigh producer with attention to quality\nStrong PC skills, with demonstrated proficiency with Microsoft Office\nWilling to work in shifts as per business requirements\nWillingness to learn and invest time and effort for career development.",Industry Type: Pharmaceutical & Life Sciences,"Department: Customer Success, Service & Operations","Employment Type: Full Time, Permanent","['Purchase', 'MIS', 'Advanced Excel', 'MIS Preparation', 'MIS Operations', 'Supply Chain Management', 'MIS Reporting', 'Management Information System', 'Inventory', 'Excel Report Preparation']",2025-06-12 13:55:59
Pyspark Developer,Synechron,5 - 10 years,Not Disclosed,['Bengaluru'],"Responsibilities:\nAbility to design and build Python-based code generation framework and runtime engine by reading Business Rules repository in order to.\n\nRequirements:\nMinimum 5 years of experience in build & deployment of Bigdata applications using SparkSQL, SparkStreaming in Python;",,,,"['Pyspark', 'Airflow', 'Hive', 'python', 'Sqoop', 'Hadoop', 'Big data', 'MongoDB', 'SQL', 'HBase']",2025-06-12 13:56:02
"Assoc Dir , Project Manager - Reltio MDM",Iqvia Biotech,10 - 15 years,Not Disclosed,"['Kochi', 'Bengaluru']","12 years of experience: Significant experience in project management, with a focus on MDM and ETL projects in life science. PMP certified preferred.\nProject Planning and Execution: Define project scope, objectives, timelines, and resources. Develop project plans, schedules, and budgets, and manage their execution. Should have a background in data integration projects and hands-on experience in managing risks and interdependencies with upstream and downstream applications.\nMDM and ETL Expertise : Should have lead MDM and ETL projects, ensuring data quality, consistency, and accuracy.\nStakeholder Management : Communicate effectively with stakeholders, manage expectations, and ensure their satisfaction.\nProject Management Methodologies : Familiarity with project management methodologies (e.g., Agile, Waterfall). Experience with JIRA or other tools.\nStrong Leadership and Communication Skills : Ability to lead and motivate teams, communicate effectively with stakeholders, and manage expectations. Communicates progress and escalates key decisions, issues, risks, and opportunities as required to achieve project objectives and deliverables.\nLocation : Kochi primary, Bangalore- Secondary\n. We create intelligent connections to accelerate the development and commercialization of innovative medical treatments to help improve patient outcomes and population health worldwide . Learn more at https://jobs.iqvia.com",Industry Type: Medical Devices & Equipment,Department: Project & Program Management,"Employment Type: Full Time, Permanent","['PMP', 'Project management', 'Agile', 'Healthcare', 'Clinical research', 'Project planning', 'Data quality', 'Life sciences', 'JIRA', 'Downstream']",2025-06-12 13:56:04
Senior MSBI Developer (SQL & SSRS/SSIS Expert),Synechron,8 - 10 years,Not Disclosed,"['Pune', 'Hinjewadi']","job requisition idJR1027513\n\nJob Summary\nSynechron is seeking an experienced and detail-oriented Senior MSBI Developerexpertise in MSBI (Microsoft Business Intelligence) to join our data and analytics team. In this role, you will contribute to designing, developing, and maintaining robust reporting and data integration solutions that support our business objectives. Your expertise will help deliver actionable insights, improve decision-making processes, and enhance overall data management efficiency within the organization.\n\nSoftware\n\nRequired\n\nSkills:\nMSBI Suite (including SSIS, SSRS, SSAS)\nSQL Server (including SQL Server Management Studio and Query Performance Tuning)\nVersionsRecent versions of SQL Server (2016 or later preferred)\nProven experience in creating complex reports, data transformation, and integration workflows\nPreferred\n\nSkills:\nPower BI or other visualization tools\nExperience with cloud-based data solutions (e.g., Azure SQL, Synapse Analytics)\nOverall Responsibilities\nDevelop, implement, and maintain MSBI solutions such as SSIS packages, SSRS reports, and data models to meet business requirements\nCollaborate with business stakeholders and data teams to gather reporting needs and translate them into scalable solutions\nOptimize and troubleshoot existing reports and data pipelines to improve performance and reliability\nEnsure data accuracy, security, and compliance within reporting processes\nDocument solution architectures, workflows, and processes for ongoing support and knowledge sharing\nParticipate in team initiatives to enhance data governance and best practices\nContribute to strategic planning for data platform evolution and modernization\nTechnical Skills (By Category)\n\nProgramming Languages:\nRequiredSQL (Advanced proficiency in query writing, stored procedures, and performance tuning)\nPreferredT-SQL scripting for data transformations and automation\nDatabases / Data Management:\nRequiredDeep knowledge of relational database concepts with extensive experience in SQL Server databases\nPreferredFamiliarity with data warehouse concepts, OLAP cubes, and data mart design\nCloud Technologies:\nDesiredBasic understanding of cloud-based data platforms like Azure Data Factory, Azure Synapse\nFrameworks and Libraries:\nNot directly applicable, focus on MSBI tools\nDevelopment Tools and Methodologies:\nExperience working within Agile development environments\nData pipeline development and testing best practices\nSecurity Protocols:\nImplement data security measures, role-based access controls, and ensure compliance with data privacy policies\nExperience\n8 to 10 years of professional experience in software development with substantial hands-on MSBI expertise\nDemonstrated experience in designing and deploying enterprise-level BI solutions\nDomain experience in finance, healthcare, retail, or similar industries is preferred\nAlternative candidacyExtensive prior experience with BI tools and proven success in similar roles may be considered in lieu of exact industry background\nDay-to-Day Activities\nDesign and develop SSIS data integration workflows to automate data loading processes\nCreate and optimize SSRS reports and dashboards for various organizational units\nEngage in troubleshooting and resolving technical issues in existing BI solutions\nCollaborate with data architects, developers, and business analysts to align data solutions with business needs\nConduct code reviews, testing, and validation of reports and data pipelines\nParticipate in scrum meetings, planning sessions, and stakeholder discussions\nEnsure documentation of solutions, processes, and workflows for ease of maintenance and scalability\nQualifications\nBachelors degree or equivalent in Computer Science, Information Technology, or related field\nRelevant certifications in Microsoft BI or SQL Server (e.g., Microsoft Certified Data Engineer Associate) preferred\nOngoing engagement in professional development related to BI, data management, and analytics tools\nProfessional Competencies\nAnalytical mindset with strong problem-solving abilities in data solution development\nCapable of working collaboratively across diverse teams and communicating technical concepts effectively\nStakeholder management skills to interpret and prioritize reporting needs\nAdaptability to evolving technologies and continuous learning mindset\nFocus on delivering high-quality, sustainable data solutions with attention to detail\nEffective time management, prioritizing tasks to meet project deadlines",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['performance tuning', 'stored procedures', 'sql scripting', 'query writing', 'msbi', 'sql server database', 'software development', 'ssas', 'power bi', 'microsoft azure', 'data mart design', 'olap cubes', 'sql server', 'sql azure', 'ssrs', 'data warehousing concepts', 'data transformation', 'ssis']",2025-06-12 13:56:06
Dot Net Architect,Conduent,10 - 15 years,Not Disclosed,['Bengaluru'],"Responsibilities \nDesign and develop highly scalable web based applications based on business needs\nDesign and customize software for client use with the aim of optimizing operational efficiency\nA deep understanding of, and ability to use and explain all aspects of application integration in .NET and data integration with SQL Server and associated technologies and standards\nDesign and provide architecture solutions based on the business needs\nStrong background in building and operating SAAS platforms using the Microsoft technology stack with modern services based architectures\nAbility to recommend and configure Azure subscriptions and establish connectivity\nWork with IT teams to setup new application architecture requirements\nCoordinate releases with Quality Assurance Team and implement SDLC work flows and better source code integration\nImplement build process and continuous build integration with Unit Testing framework\nDevelop and maintain a thorough understanding of business needs from both technical and business perspectives\nAssist and mentor junior team members to enforce development guidelines\nTake technical ownership of products and provide support with quick turnaround\nEffectively prioritize and execute tasks in a high-pressure environment\n\n\n Qualifications / Experience \nBachelor\\u2019s/Master\\u2019s degree in Computer Science / Computer Engineering\nMinimum of 10+ years\\u2019 experience in building enterprise scale windows and web application using Microsoft .NET technologies\n5+ years of experience in C#, ASP.NET MVC and Microsoft Web API\n1+ years of experience in Angular 2 or higher\nExperience in solution architecture in .Net technologies\nExperience in any of the following are also desirableBootstrap, Knockout, entity framework nhibernate, Subversion, Linq, Asynchronous Module Definition (such as requirejs)\nIn depth knowledge on design patterns and unit testing frameworks\nExperience with Agile application development\nSQL server performance tuning (SQL Server 2014/2016) and troubleshooting\nAbility to work with a sense of urgency and attention to detail\nExcellent oral and written communication skills",Industry Type: BPM / BPO,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sql server', 'application integration', 'design patterns', '.net', 'data integration', 'c#', 'unit testing framework', 'web application', 'unit testing', 'scale', 'knockoutjs', 'net mvc', 'angular', 'asp.net core mvc', 'linq', 'saas applications', 'web api', 'agile', 'sdlc', 'asp', 'microsoft net']",2025-06-12 13:56:08
Deputy General Manager- Electrical,Tata Consultant Engineering Limited,16 - 25 years,Not Disclosed,['Bengaluru'],"About Us\nTata Consulting Engineers Limited (TCE) is the largest Indian private sector engineering and project consultancy and an emerging global leader in integrated engineering solutions. With more than 60 years of engineering excellence, TCE has a presence in over 64 countries and over 12000 completed projects, the company operates in 3 core Industry segments -Infrastructure (Water, Environment, Urban Development, Buildings, Manufacturing Facilities, Ports and Harbours, Transportation), Power (Thermal, Hydro, Nuclear, Renewable, Transmission and Distribution) and Resources - Hydrocarbons and Chemicals (Oil, Gas and Refineries, Chemicals, Petrochemicals, Fertilizers, Speciality Chemicals, Pulp and Paper, Cement, Food, Pharmaceuticals and Beverages, Tyre, Glass) as well as Mining and Metallurgy (Mining, Geology, Beneficiation, Steel, Non-ferrous). TCE serves domestic as well as international markets and is known for several first-of-its-kind projects offering Engineering Studies, Design Engineering Services, Project Management Consultancy Services, OPEX and IIOT across all three verticals. A part of Tata Group - India’s most respected group, TCE is a 100 percent subsidiary of Tata Sons Limited Design your Future with us At TCE, you will experience a supportive environment that empowers you to excel, whether you are based in our offices or at a client site. We embrace diversity, equity, and inclusion, fostering a workplace where every individual can thrive by contributing their unique skills and perspectives to deliver exceptional results for our clients. Our comprehensive compensation and benefits packages are designed to meet the diverse needs of our employees and their families, complemented by a robust global well-being program. As a leading global infrastructure firm, we are committed to your growth and success, offering access to cutting-edge technology and impactful projects that offer flexibility and significant professional opportunities. Join us and become part of a global company that values your potential and supports your career development.\nPurpose & Scope of Position\nEngineering Manager (EM) is responsible for engineering and design matters on the project and assists the project manager. An EM will fill 3 main roles during the execution of a project. These are Manager, Technical Leader and Communicator. The EM on a project is responsible for scope definition and manages the planning, resourcing and scheduling all the engineering related activities. In addition, the EM provides technical leadership, integrates the disciplines and resolves issues. The EM is responsible for delivery of all the technical deliverables to achieve the project objectives within the specified time frame and budget.\nExperience\n• Typically 15 years & above of experience in a multi-disciplinary environment on major projects \nDesign and procurement activities related to electrical systems / equipment.\nSizing calculations for all major electrical equipment or systems.\nPreparation of RFQs for major electrical packages\nVendor print review for all electrical equipment or systems along with cable schedules & interconnection wiring diagrams\nExposure on Layouts such as General Arrangement drawings, Cable & conduit drawings, Earthing & Lightning Protection drawings, Lighting drawings.\nExposure on Control and protection, review of schematics for EHV, MV & LV systems.\nExposure to Load flow, Short circuit, Largest Motor starting studies using ETAP.\nEstimation of BOQs for cabling, earthing, lightning & lighting systems.\nExposure to relay setting and co-ordination\nInspection services for major electrical equipment (Optional)\nFamiliarity in ETAP, DIALux, AutoGrid Pro, etc.\nFamiliarity in 3D Engineering tools Aveva E3D, Hexagon Smart 3D, Navisworks\nQualification\n• Postgraduate or graduate in an engineering discipline • Registration as a professional engineer with the governing authority (preferable)\nKey Responsibilities\n1.Ensure the scope of work is developed and effective change management system is in place 2. Provide input into the monthly progress report with respect to design progress and issues of concern and recommended changes if required to achieve overall objectives 3. Contribute to the development of the project execution plan together with the project leadership team and allocate roles and responsibilities 4. Make Contact with client management at key milestones / tollgates throughout project to ensure that engineering deliverables are meeting client requirements. 5. Direct and review the engineering activities to ensure that the work quality is satisfactory and appropriate technical personnel development programs are conducted 6. Involve specialist expertise from within TCE or externally as necessary. work with Technology Organization and DHs for mandatory reviews 7. Raise safety awareness and ensure the design for safety principles are applied to the projects 8. Ensure Engineering risk assessments are carried out and all identified issues are addressed 9. Arrange and facilitate design reviews and participate as required in engineering and management reviews 10. Co-ordinate quality audit verifications to ensure compliance with all relevant engineering standards and internal procedures for all design activities. Instigate corrective actions as required. 11. Cooperate with the TCE engineering practice of other BU’s as directed on matters involving sharing of available expertise. 12. Responsible to ensure timely availability of inter-disciplinary data, integration of the same and resolution of any issues and challenges 13. Obtain regular feedback from customer and take appropriate action 14. Document value additions and best practices and ensure communication of the same to other project teams and leadership 15. Recommend rewards and recognition for exemplary performance from project resources\nCompetencies\nManages Conflict\nSelf-Development\nDrives Results\nEnsures Accountability\nOptimizes Work processes\nPlans and Aligns\nDecision Quality\nSituational Adaptability\nTech Savvy\nInterpersonal Savvy",Industry Type: IT Services & Consulting,"Department: Production, Manufacturing & Engineering","Employment Type: Full Time, Permanent","['dialux', 'lightning protection', 'design engineering', 'change management', 'smart', 'wiring', 'cable schedule', 'refinery', '3d', 'general', 'oil', 'drawing', 'gas', 'cabling', 'cables', 'equipment', 'engineering', 'lighting', 'schematic', 'estimation', 'electricals', 'etap', 'site', 'e3d', 'earthing', 'electrical equipment', 'petrochemical']",2025-06-12 13:56:11
CAD Engineering Manager (Engineer-to-Order),Pratiti Technologies,4 - 7 years,Not Disclosed,['Pune'],"[{""Posting_Title"":""CAD Engineering Manager (Engineer-to-Order)"" , ""Is_Locked"":false , ""City"":""Pune City"",""Industry"":""Software Product"",""Job_Description"":""\nJob Summary:\nWe are seeking a highly motivated and experienced CAD Engineering Manager to lead our team in developing and implementing KBEsolutions for our ETO products. The ideal candidate will possess a strongunderstanding of electrical and mechanical design principles, experience withplatforms like Rulestream and AutoCAD, and a proven track record of drivingprocess improvements through KBE. This role will be responsible for managing ateam of engineers, developing KBE strategies, and ensuring the successful implementationof KBE tools to streamline our design and manufacturing processes.\nResponsibilities:\nKBE Strategy & Development:\nDevelop and implement a comprehensive KBE strategy to automate and optimize design and engineering processes for ETO products.\nIdentify and evaluate opportunities for KBE implementation across various product lines and design workflows.\nDefine and maintain KBE standards, guidelines, and best practices.\nTeam Leadership & Management:\nLead and mentor a team of KBE engineers, providing technical guidance and support.\nManage project timelines, resources, and budgets to ensure successful KBE implementation.\nFoster a collaborative and innovative team environment.\nRulestream & AutoCAD Expertise:\nLead the development and maintenance of KBE applications using Rulestream.\nIntegrate Rulestream with AutoCAD and other design tools to automate design tasks.\nTroubleshoot and resolve technical issues related to Rulestream and AutoCAD.\nEnsure proper data flow between CAD and KBE systems.\nETO Process Optimization:\nAnalyze existing ETO design and manufacturing processes to identify areas for improvement through KBE.\nDevelop and implement KBE solutions to reduce design cycle time, improve design accuracy, and enhance product quality.\nWork closely with cross-functional teams (e.g., sales, manufacturing, quality) to ensure seamless KBE integration.\nDocumentation & Training:\nDevelop and maintain comprehensive documentation for KBE applications and processes.\nProvide training and support to engineers and other stakeholders on KBE tools and methodologies.\nMaintain well documented libraries of rules and configurations.\nContinuous Improvement:\nStay up-to-date on the latest KBE technologies and industry trends.\nDrive continuous improvement initiatives to enhance KBE capabilities and efficiency.\nQualifications:\nBachelors or Masters degree in Mechanical Engineering, Electrical Engineering, or a related field.\nMinimum 1 of [Number] years of experience in engineering, with a focus on ETO industries.\nProven experience in developing and implementing KBE solutions using Rulestream.\nStrong proficiency in AutoCAD for electrical and mechanical design.\nExcellent understanding of electrical and mechanical design principles and practices.\nExperience with database management and data integration.\nStrong project management and leadership skills.\nExcellent communication and interpersonal skills.\nExperience with other CAD/CAM/CAE software is a plus.\nUnderstanding of configuration management.\nExperience in creating and maintaining complex rule sets.\nPreferred Qualifications:\nExperience with other KBE platforms.\nKnowledge of manufacturing processes and materials.\nExperience with PLM/PDM systems.\n\n\n"",""Job_Type"":""Full time"",""Job_Opening_Name"":""CAD Engineering Manager (Engineer-to-Order)"" , ""State"":""Maharashtra"" , ""Country"":""India"" , ""Zip_Code"":""411045"" , ""id"":""481259000051041047"" , ""Publish"":true , ""Date_Opened"":""2025-04-17"" , ""Keep_on_Career_Site"":false}]",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['PLM', 'Product quality', 'Process optimization', 'AutoCAD', 'Project management', 'Configuration management', 'CAD', 'Mechanical design', 'Continuous improvement', 'KBE']",2025-06-12 13:56:14
MDM Technical Lead,Iqvia Biotech,5 - 7 years,Not Disclosed,"['Kochi', 'Bengaluru']","Overview\nAs an MDM Technical Delivery Manager, you will be responsible for leading and overseeing the end-to-end delivery of Master Data Management (MDM) solutions. You will collaborate with cross-functional teams to drive technical implementation, ensure data governance, and align with business objectives. Your expertise in MDM platforms, integration strategies, and project execution will be key to delivering high-quality solutions\nKey Responsibilities\nOversee a team of experienced professionals, fostering collaboration and high performance.\nGuide and mentor team members, supporting their job performance and career growth.\nLead the technical delivery of MDM implementations, ensuring successful project execution.\nDefine MDM architecture, strategy, and integration frameworks with enterprise systems.\nCollaborate with business stakeholders to understand data requirements and align solutions.\nOversee data governance, quality, and compliance with regulatory standards.\nManage MDM development teams, ensuring adherence to best practices and standards.\nOptimize data models, workflows, and processes for efficient MDM operations.\nDrive continuous improvements in MDM technologies, methodologies, and performance.\nCommunicate project updates, risks, and resolutions to leadership and stakeholders.\nRequired Qualifications\n\nBachelor s degree in Computer Engineering, Computer Science, or a related field.\n5-7+ years of experience in software development and data Management.\n5+ years of expertise in MDM implementation, with hands-on experience in Reltio, DataBricks, Azure, Oracle, and Snowflake.\nStrong background in integration design and development.\nStrong expertise in data integration design, ETL processes, and API development.\nAt least 2+ years in an MDM Technical Lead and Delivery role.\nProven track record in leading MDM projects and cross-functional teams.\nSolid understanding of diverse data sets, sources, and country-specific data models.\nExperience in life sciences MDM implementations.\nExperience in life sciences, healthcare, or pharmaceutical industries is a plus.\nExcellent communication, leadership, and problem-solving skills.\n. We create intelligent connections to accelerate the development and commercialization of innovative medical treatments to help improve patient outcomes and population health worldwide . Learn more at https://jobs.iqvia.com",Industry Type: Medical Devices & Equipment,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Data management', 'Pharma', 'data governance', 'Healthcare', 'Clinical research', 'Technical Lead', 'Life sciences', 'Oracle', 'Project execution']",2025-06-12 13:56:16
Python Fullstack Developer,CGI,2 - 5 years,Not Disclosed,['Bengaluru'],"Role & responsibilities\nExcellent Knowledge & Understanding of Python, Pandas and Oracle, SQL.\nGood Knowledge & Understanding of data modelling.\nFlair for learning new tools & technology.\n2-5 years of experience in Banking IT, with a good understanding of the Corporate and Institutional\nBanking activity. Knowledge of Capital Markets an asset.\nGood knowledge and understanding of Windows Batch or PowerShell scripting.\nGood knowledge in systems, application frameworks, database optimization, and experience being\nresponsible for the success of software development projects.\nProven record interpreting and fulfilling requirements by developing high performing, scalable and\nmaintainable solutions with multiple technologies.\nHands-on experience with SDLC methodologies and best practices including Waterfall Process, Agile\nmethodologies, deployment automation, code reviews, and test-driven development.\nStrong coordination and organizational skills\nExcellent communication skills and multi-tasking capabilities.\nBeing aware of new technologies and frameworks.\nExperience and knowledge of Python language features like basic data types, functions with keywords\nargs, default args, variable length args.\nExperience in using Python database API with any relational database like Oracle/SQL Server using pure\nPython or native database driver.\nExperience in using built in collection types [for example: sequence types, dict, sets].\nExperience in using text manipulation facilities like regex, string methods from the standard library.\nExperience in using file and directory manipulation modules like paths, globs from the standard library.\nKnowledge of widely used libraries in data science like NumPy, Pandas is a must.\nVisualization is a good to have but not on the mandatory path.\nKnowledge and Experience in using collections from the collections module will be good to have though\nnot mandatory.\nFamiliarity with date manipulation using modules like datetime.\n•\nKnowledge of Generators will be good to have though not mandatory\n\nPreferred candidate profile",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Django', 'Pandas', 'Oracle', 'Python', 'SQL']",2025-06-12 13:56:18
Etl Tester- Bangalore(Pan India Infosys),Infosys,3 - 8 years,Not Disclosed,"['Hyderabad', 'Chennai', 'Bengaluru']","Job description\nHiring for ETL Testing with experience range 3 to 10 years\nMandatory Skills: ETL/DWH/Big data testing\nEducation: BE/B.Tech/MCA/M.Tech/MSc./MS\nResponsibilities\nA day in the life of an Infoscion\nAs part of the Infosys consulting team, your primary role would be to actively aid the consulting team in different phases of the project including problem definition, effort estimation, diagnosis, solution generation and design and deployment\nYou will explore the alternatives to the recommended solutions based on research that includes literature surveys, information available in public domains, vendor evaluation information, etc. and build POCs\nYou will create requirement specifications from the business needs, define the to-be-processes and detailed functional designs based on requirements.\nYou will support configuring solution requirements on the products; understand if any issues, diagnose the root-cause of such issues, seek clarifications, and then identify and shortlist solution alternatives\nYou will also contribute to unit-level and organizational initiatives with an objective of providing high quality value adding solutions to customers. If you think you fit right in to help our clients navigate their next in their digital transformation journey, this is the place for you!",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['ETL Testing', 'Big Data Testing', 'DWH Testing']",2025-06-12 13:56:20
Quantitative Analytics Manager,Wells Fargo,4 - 8 years,Not Disclosed,['Bengaluru'],"In this role, you will:\nManage a team responsible for the creation and implementation of low to moderate complex financial areas\nMitigate operational risk and compute capital requirements\nDetermine scope and prioritization of work in consultation with experienced management\nParticipate in the development of strategy, policies, procedures, and organizational controls with model users, developers, validators, and technology",,,,"['Quantitative Analytics', 'strategy Planning', 'marketing', 'Git', 'GitHub', 'talent development', 'credit risk analysis']",2025-06-12 13:56:22
Lead Analytics Consultant,Wells Fargo,5 - 10 years,Not Disclosed,['Hyderabad'],"locationsHyderabad, India\nposted onPosted Yesterday\njob requisition idR-446112\nAbout this role:\nWells Fargo is seeking a Lead Analytics Consultant People Analytics. As a consultant, you will work as analytics professional in HR People Analytics and Business Insights delivery team and will be responsible for effective delivery of projects as per the business priority. The incumbent is expected to be an expert into executive summary, people strategy, HR consulting, HR advisory, advanced analytics & data science and value addition to the projects.",,,,"['Data Analytics', 'Data science tools', 'product lifecycle', 'SAS programming', 'ETL development', 'Alteryx', 'Data Management', 'Tableau Prep', 'SQL']",2025-06-12 13:56:24
Etl Tester - Pan India,Infosys,3 - 8 years,Not Disclosed,"['Kolkata', 'Chennai', 'Bengaluru']","Job description,\n\nHiring for ETL testing with experience range 3-10 years\n\nMandatory Skills: ETL Testing\n\nLocation - Bangalore/Hyderabad/Pune/kolkata/Chennai/Bhubaneswar\n\nEducation: BE/B.Tech/MCA/M.Tech/MSc./MS\n\nResponsibilities\nA day in the life of an Infoscion\nAs part of the Infosys consulting team, your primary role would be to actively aid the consulting team in different phases of the project including problem definition, effort estimation, diagnosis, solution generation and design and deployment\nYou will explore the alternatives to the recommended solutions based on research that includes literature surveys, information available in public domains, vendor evaluation information, etc. and build POCs\nYou will create requirement specifications from the business needs, define the to-be-processes and detailed functional designs based on requirements.\nYou will support configuring solution requirements on the products; understand if any issues, diagnose the root-cause of such issues, seek clarifications, and then identify and shortlist solution alternatives\nYou will also contribute to unit-level and organizational initiatives with an objective of providing high quality value adding solutions to customers. If you think you fit right in to help our clients navigate their next in their digital transformation journey, this is the place for you!",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['ETL Testing', 'ETL', 'Etl Testers']",2025-06-12 13:56:26
Power Bi Engineer,Scalable Systems,4 - 6 years,10-15 Lacs P.A.,['Kochi'],Power Bi Engineer\nLocation : Kochi\nFulltime,Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Power Bi', 'Data Visualization', 'Power System', 'Dashboard Development']",2025-06-12 13:56:28
Technical Illustrator(Automotive),Cyient,4 - 9 years,Not Disclosed,"['Pune', 'Bengaluru']","Create and revise Parts Lists in the Parts Catalogs for automotive, agriculture and construction equipment.\nCreating exploded view artworks associated with parts catalogs as per given standards.\nProduce complete, clear and accurate parts catalogs content as per customer standards and guidelines.\nProcessing of information and data (engineering documents, changes, etc.) from\nEngineering, Manufacturing, Parts Marketing, Customer Service, Parts Warehouse and Suppliers.",,,,"['Iso Draw', 'Technical Illustration']",2025-06-12 13:56:30
Python/Pyspark developer,Zensar,4 - 5 years,Not Disclosed,"['Pune', 'Bengaluru']","Job Description:\nWe are seeking a highly skilled and motivated Python/PySpark Developer to join our growing team. In this role, you will be responsible for designing, developing, and maintaining high-performance data processing pipelines using Python and the PySpark framework. You will work closely with data engineers, data scientists, and other stakeholders to deliver impactful data-driven solutions.\nResponsibilities:\n- Design, develop, and implement scalable and efficient data pipelines using PySpark.\n- Write clean, well-documented, and maintainable Python code.\n- Optimize data processing performance and resource utilization.\n- Implement ETL (Extract, Transform, Load) processes to migrate and transform data across various systems.\n- Collaborate with data scientists and analysts to understand data requirements and translate them into technical solutions.\n- Troubleshoot and debug data processing issues.\n- Stay up-to-date with the latest advancements in big data technologies and best practices.\nQualifications:\n- Bachelor's degree in Computer Science, Engineering, or a related field.\n- 3+ years of experience in Python development.\n- 2+ years of experience with PySpark and Spark ecosystem.\n- Strong understanding of data structures, algorithms, and object-oriented programming.\n- Experience with SQL and relational databases.\n- Familiarity with cloud platforms such as AWS, Azure, or GCP (preferred).\n- Excellent problem-solving and analytical skills.\n- Strong communication and teamwork skills.\nBonus Points:\n- Experience with data visualization tools (e.g., Tableau, Power BI).\n- Knowledge of machine learning and data science concepts.\n- Experience with containerization technologies (e.g., Docker, Kubernetes).\n- Contributions to open-source projects.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'Cloud Technologies', 'SQL', 'Python']",2025-06-12 13:56:32
Sr SQL Developer,HTC Global Services,8 - 13 years,Not Disclosed,['Hyderabad'],"We are looking for a skilled SSIS/T-SQL Developer with 8 years of good expertise. The incumbent will be responsible for developing, deploying and maintaining SSIS packages, as well as writing and optimizing T-SQL queries, stored procedures and functions. The role involves working closely with stakeholders to understand data requirements and deliver high-quality data solutions.\nRequirements:\nStrong understanding of relational database concepts and data modelling.\nExperience with SQL query optimization and performance tuning.",,,,"['T-SQL', 'Performance tuning', 'SQL queries', 'Data migration', 'query optimization', 'Data modeling', 'Debugging', 'Stored procedures', 'SSIS', 'Data warehousing']",2025-06-12 13:56:35
Learning Consultant India Learning Consultant,Zensar,5 - 10 years,Not Disclosed,"['Kolkata', 'Mumbai', 'New Delhi', 'Hyderabad', 'Pune', 'Chennai', 'Bengaluru']","Consultants are responsible for advising, guiding, scoping, developing strategic learning plans that align to channel partner business teams and their outcomes. Although this is a learning consulting role, having a strong ability to get buy-ins from senior business stakeholders and being able to showcase L&D is crucial to success. They establish and embed processes for learner journeys into partner organizations. Motivated, analytical, and strategic thinkers who also have excellent stakeholder management skills and the ability to develop and execute plans will succeed in this consultative role.\n  High-level skills and experience\nStrong consultative mindset and consulting skill set with a strong point-of-view on solutions.\nStrategic, big picture thinking and connecting the dots.\nAbility to understand the why along with the what and how of a partner need (ie, we'll-rounded learning plans that drive business outcomes rather than just chasing headcount/numbers)\nStrong commercial awareness/business acumen of the client s and partner s business.\nAbility to align learning with client and partner ROI and business drivers.\nThought leadership with new ideas, strategies, framework, processes.\nExecutive presence with senior stakeholders\nData analytics (ability to dig into trends and forecast) and data story telling.\nTechnical prowess to pick up data/AI/cloud/technical knowledge.\nExcellent oral/written communication and presentation skills for business reviews with senior stakeholders.\nHighly collaborative across various business functions/senior stakeholder management.\nProactive and independent, ability to excel in a fast-paced environment while working from vision through execution.\nExpected tasks and responsibilities.\nExecute frameworks to assess capability and capacity skill gaps.\nConduct partner needs analysis (discovery and scoping at business, technical and learning levels)\nEnabling consulting conversations with business leaders in partner teams to understand their business drivers.\nPropose strategic learning plans to all internal and external stakeholders.\nPrescribe relevant learning based on scope and workload (such as certifications, solutions, products, etc)\nBuild custom partner learning plans, where required, based on the results of scoping/needs analysis and business priority.\nMonitor learner progress and detect any issues.\nCreate and present Quarterly/Monthly Business Reviews with senior L&D/Business/Technology stakeholders\nMonitor and analyze learning stats for data-driven decisions.\nExplore business problems and create different solution models.\nMake recommendations for improvement and present to partners and internal stakeholders.\nDevelop and implement new procedures and/or training to support proposed changes.\nQualifications\n5+ years relevant experience being an L&D consultant or in any business facing consultant role driving outcomes.\nbachelors degree in Engineering and Computer Science, or a related field (required)\nMasters candidates (preferred)\nCertifications in Data Engineering, AI/ML, Cloud platforms (preferred).\nChannel partner/pre-sales/data & AI background encouraged.\nAbout the role\nChannel consultants are part of the Channel and Partner Success Consulting team and are passionate about helping our client strengthen their channel partner readiness program through technical and pre-sales capability and capacity assessment. They work closely with our client s channel partners to build partner teams and skills that deliver to business objectives and outcomes. The client is a global technology company.",Industry Type: IT Services & Consulting,Department: Strategic & Top Management,"Employment Type: Full Time, Permanent","['Training', 'Analytical', 'Consulting', 'Cloud', 'Manager Technology', 'Presales', 'Data analytics', 'Stakeholder management', 'Monitoring']",2025-06-12 13:56:37
AVP - Finance Analyst,Wells Fargo,2 - 7 years,Not Disclosed,['Bengaluru'],"About this role:\nWells Fargo is seeking a Finance Analyst\n\nIn this role, you will:\nParticipate in functions related to financial research and reporting\nForecast analysis of key metrics, as well as other financial consulting related to business performance, operating and strategic reviews",,,,"['financial research', 'Data analysis', 'Project management', 'documentation', 'Gap analysis', 'financial consulting', 'SQL']",2025-06-12 13:56:39
Lead Business Analyst,Indegene,3 - 8 years,Not Disclosed,['Bengaluru'],"Reporting and Optimization in Adobe Analytics, Google Analytics\nCreate documents like Business Req. Doc, Tech Spec Doc etc\nDo the measurement planning for Digital Analytics and Implementation projects\nDesign a solution and digital strategy\nExperience in Data integration and BigQuery integration\nCreate data visualization dashboards specially on Workspace, Data Studio, MS Excel and Adobe Report builder\nDevelop the strategy of enterprise level solutions as we'll as architecting extensible and maintainable solutions utilizing the Adobe and Google analytics platforms\nUnderstand and use multitude of tag managers and writing JavaScript code to realize client driven business requirements\nExcellent understanding of digital analytics specially Clickstream Data\nAgile method understanding\n\nManagement Skills:\nExcellent written and oral communication skills\nExcellent listener\nStaying abreast of new technologies and issues in the software-as-a-service industry, including current technologies, platforms, standards and methodologies\n\nYour impact:\nAbout you: (Desired profile)\nMust have: (Requirements)\nAnalytics Platforms - Google Analytics, Adobe Analytics/Omniture SiteCatalyst\nBig Query\nNice to have: (Additional desired qualities)\nTag Managers - Adobe Launch/DTM, Tealium IQ, Google Tag Manager, Piwik Pro, Signal/Bright Tag\nOptimization Platform - Adobe Target, Google Optimize, Optimizely1+ years in a client facing role for solutioning and / or evangelizing technology approaches.\nProgramming Languages - JavaScript, jQuery\nMarkup Languages - HTML, CSS",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['jQuery', 'Google Analytics', 'Service industry', 'Javascript', 'Agile', 'Healthcare', 'HTML', 'Omniture', 'data visualization', 'Adobe']",2025-06-12 13:56:41
"Walk-in For Testing Positions at Chennai, Pune, Bangalore Location",Hexaware Technologies,5 - 10 years,Not Disclosed,"['Pune', 'Chennai', 'Bengaluru']",Exciting Career Opportunity at Hexaware Technologies!\n\nJoin our dynamic team and advance your career with us! We are looking for committed professionals to fill the following positions:\n\n- API Automation Testing (Rest Assured) (6 to 10 Years)\n- Python Automation (6 to 10 Years)\n- ETL Testing (Bigdata/Cloud) (6 to 12 Years),,,,"['Api Automation', 'Cloud Testing', 'Python Testing', 'Automation Testing', 'ETL Testing', 'Big Data Testing', 'Rest Assured', 'Selenium Testing', 'Selenium With Java']",2025-06-12 13:56:44
Oracle CDC Specialist Oracle CDC Specialist,Zensar,5 - 10 years,Not Disclosed,"['Kolkata', 'Mumbai', 'New Delhi', 'Hyderabad', 'Pune', 'Chennai', 'Bengaluru']","IT Specialist - Oracle CDC, Kafka Connectors & Docker\nJob Overview\nWe are seeking a skilled IT Specialist with expertise in Oracle Change Data Capture (CDC), Kafka topics, event streams, and running Kafka connectors on Docker containers. The ideal candidate will design, implement, and maintain robust data integration solutions to support our real-time data processing needs.\nKey Responsibilities\nConfigure and manage Oracle CDC to capture and process real-time data changes.\nDesign and maintain Kafka topics and event streams for efficient data flow.\nDeploy and operate Kafka connectors within Docker containers for seamless integration.\nMonitor and optimize performance of data pipelines and streaming processes.\nCollaborate with cross-functional teams to ensure data integrity and system scalability.\nTroubleshoot and resolve issues related to data streaming and containerized environments.\nRequired Skills and Qualifications\nBachelor s degree in Computer Science, IT, or related field (or equivalent experience).\n5+ years of experience with Oracle CDC for real-time data capture.\nStrong knowledge of Oracle databases, specifically CDC capabilities such as LogMiner & XStreams API\nStrong knowledge of Apache Kafka, including topic management and event streaming.\nProficiency in deploying and managing Kafka connectors in Docker containers.\nAble to deploy java monitoring through JMX for Kafka connectors.\nFamiliarity with container orchestration tools (e.g., Kubernetes) is a plus.\nExcellent problem-solving skills and ability to work in a fast-paced environment.\nPreferred Qualifications\nExperience with cloud platforms (e.g., AWS).\nKnowledge of additional streaming technologies or data integration tools.\nStrong scripting skills (e.g., Python, Bash) for automation.\nIT Specialist - Oracle CDC, Kafka Connectors & Docker\nJob Overview\nWe are seeking a skilled IT Specialist with expertise in Oracle Change Data Capture (CDC), Kafka topics, event streams, and running Kafka connectors on Docker containers. The ideal candidate will design, implement, and maintain robust data integration solutions to support our real-time data processing needs.\nKey Responsibilities\nConfigure and manage Oracle CDC to capture and process real-time data changes.\nDesign and maintain Kafka topics and event streams for efficient data flow.\nDeploy and operate Kafka connectors within Docker containers for seamless integration.\nMonitor and optimize performance of data pipelines and streaming processes.\nCollaborate with cross-functional teams to ensure data integrity and system scalability.\nTroubleshoot and resolve issues related to data streaming and containerized environments.\nRequired Skills and Qualifications\nBachelor s degree in Computer Science, IT, or related field (or equivalent experience).\n5+ years of experience with Oracle CDC for real-time data capture.\nStrong knowledge of Oracle databases, specifically CDC capabilities such as LogMiner & XStreams API\nStrong knowledge of Apache Kafka, including topic management and event streaming.\nProficiency in deploying and managing Kafka connectors in Docker containers.\nAble to deploy java monitoring through JMX for Kafka connectors.\nFamiliarity with container orchestration tools (e.g., Kubernetes) is a plus.\nExcellent problem-solving skills and ability to work in a fast-paced environment.\nPreferred Qualifications\nExperience with cloud platforms (e.g., AWS).\nKnowledge of additional streaming technologies or data integration tools.\nStrong scripting skills (e.g., Python, Bash) for automation.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Automation', 'orchestration', 'JMX', 'Data processing', 'data integrity', 'Oracle', 'Apache', 'Python', 'Scripting']",2025-06-12 13:56:46
Manager - BIM,Axtria,10 - 15 years,Not Disclosed,['Bengaluru'],"Position Summary \n\nLooking for a Salesforce Data Cloud Engineer to design, implement, and manage data integrations and solutions using Salesforce Data Cloud (formerly Salesforce CDP). This role is essential for building a unified, 360-degree view of the customer by integrating and harmonizing data across platforms.\n\n Job Responsibilities \n\nConsolidate the Customer data to create a Unified Customer profile\nDesign and implement data ingestion pipelines into Salesforce Data Cloud from internal and third-party systems .\nWork with stakeholders to define Customer 360 data model requirements, identity resolution rules, and calculated insights.\nConfigure and manage the Data Cloud environment, including data streams, data bundles, and harmonization.\nImplement identity resolution, micro segmentation, and activation strategies.\nCollaborate with Salesforce Marketing Cloud, to enable real-time personalization and journey orchestration.\nEnsure data governance, and platform security.\nMonitor data quality, ingestion jobs, and overall platform performance.\n\n\n Education \n\nBE/B.Tech\nMaster of Computer Application\n\n Work Experience \nOverall experience of minimum 10 years in Data Management and Data Engineering role, with a minimum experience of 3 years as Salesforce Data Cloud Data Engineer\nHands-on experience with Salesforce Data Cloud (CDP), including data ingestion, harmonization, and segmentation.\nProficient in working with large datasets, data modeling, and ETL/ELT processes.\nUnderstanding of Salesforce core clouds (Sales, Service, Marketing) and how they integrate with Data Cloud.\nExperience with Salesforce tools such as Marketing Cloud.\nStrong knowledge of SQL, JSON, Apache Iceberg and data transformation logic.\nFamiliarity with identity resolution and customer 360 data unification concepts.\nSalesforce certifications (e.g., Salesforce Data Cloud Accredited Professional, Salesforce Administrator, Platform App Builder).\nExperience with CDP platforms other than Salesforce (e.g., Segment, Adobe Experience Platform (Good to have)).\nExperience with cloud data storage and processing tools (Azure, Snowflake, etc.).\n\n\n Behavioural Competencies \n\nTeamwork & Leadership\nMotivation to Learn and Grow\nOwnership\nCultural Fit\nTalent Management\n\n Technical Competencies \n\nProblem Solving\nAzure Data Factory\nAzure DevOps\nAzure SQL",Industry Type: Analytics / KPO / Research,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sql', 'apache', 'data modeling', 'data transformation', 'etl', 'snowflake', 'navisworks', 'data management', 'bim', 'revit architecture', 'microsoft azure', 'revit mep', 'azure data factory', 'autocad', 'azure devops', 'salesforce', 'talent management', 'sql azure', 'revit', 'json', 'salesforce core']",2025-06-12 13:56:49
Test Engineer - C# Selenium,Encora,5 - 8 years,Not Disclosed,['Chennai'],Greetings from Encora Innovation Labs Pvt Ltd!\n\nWe have received your profile for Test Engineer role. We would appreciate if you take out a few minutes of your time and share the below information to proceed further interview process.\n\nImportant Note : Please do not reply if you have a 60/90-days notice period. Kindly excuse us for candidates with a longer notice period .,,,,"['C#', 'Automation Testing', 'Appium', 'API', 'Agile', 'Selenium', 'Manual Testing']",2025-06-12 13:56:51
Sr. ETL QA /ETL QA Lead,Visionet Systems,6 - 11 years,Not Disclosed,['Bengaluru'],"Position Summary:\nWe are seeking a highly skilled ETL QA Engineer with at least 6 years of experience in ETL/data pipeline testing on the AWS cloud stack , specifically with Redshift, AWS Glue, S3 , and related data integration tools. The ideal candidate should be proficient in SQL , capable of reviewing and validating stored procedures , and should have the ability to automate ETL test cases using Python or suitable automation frameworks . Strong communication skills are essential, and web application testing exposure is a plus.\nTechnical Skills Required:",,,,"['Data validation', 'orchestration', 'Web testing', 'Test scenarios', 'Selenium', 'Stored procedures', 'Test cases', 'Reporting tools', 'SQL', 'Python']",2025-06-12 13:56:54
YASH Technologies is Hiring - Sr. Consultant - SAP Build Apps,Yash Technologies,1 - 3 years,Not Disclosed,"['Hyderabad', 'Pune']","Develop applications using SAP Build Apps , SAP Build Process Automation, and SAP Build Work Zone.\nIntegrate SAP Build solutions with core SAP systems (SAP S/4HANA, SAP ECC, SAP BTP)\nConfigure and extend SAP Fiori applications using SAP Build tools.\nImplement process automation workflows using SAP Build Process Automation.\nWork with APIs (OData, REST) to enable data integration with SAP and non-SAP systems.\nEnsure application performance, quality, and responsiveness.\nTroubleshoot and resolve technical issues related to SAP Build apps and integrations.",,,,"['SAP ECC', 'SAP Build Apps', 'SAP systems', 'SAP BTP', 'SAP S/4HANA']",2025-06-12 13:56:56
Senior Software Developer - Java & Angular,S&P Global Market Intelligence,7 - 12 years,Not Disclosed,"['Mumbai', 'Maharastra']","Grade Level (for internal use) : - 10\nThe Team\nYou will be an expert contributor and part of the Rating Organizations Data Services Product Engineering Team\nThis team, who has a broad and expert knowledge on Ratings organizations critical data domains, technology stacks and architectural patterns, fosters knowledge sharing and collaboration that results in a unified strategy\nAll Data Services team members provide leadership, innovation, timely delivery, and the ability to articulate business value\nBe a part of a unique opportunity to build and evolve S&P Ratings next gen analytics platform\nResponsibilities:\nDesign and implement innovative software solutions to enhance S&P Ratings' cloud-based data platforms.\nMentor a team of engineers fostering a culture of trust, continuous growth, and collaborative problem-solving.\nCollaborate with business partners to understand requirements, ensuring technical solutions align with business goals.\nManage and improve existing software solutions, ensuring high performance and scalability.\nParticipate actively in all Agile scrum ceremonies, contributing to the continuous improvement of team processes.\nProduce comprehensive technical design documents and conduct technical walkthroughs.\nExperience & Qualifications:\nBachelors degree in computer science, Information Systems, Engineering, equivalent or more is required\nProficient with software development lifecycle (SDLC) methodologies like Agile, Test-driven development\n7+ years of development experience in enterprise products, modern web development technologies Java/J2EE, UI frameworks like Angular, React, SQL, Oracle, NoSQL Databases like MongoDB\nExperience designing transactional/data warehouse/data lake and data integrations with Big data eco system leveraging AWS cloud technologies\nExp. with Delta Lake systems like Databricks using AWS cloud technologies and PySpark is a plus\nThorough understanding of distributed computing\nPassionate, smart, and articulate developer\nQuality first mindset with a strong background and experience with developing products for a global audience at scale\nExcellent analytical thinking, interpersonal, oral and written communication skills with strong ability to influence both IT and business partners\nSuperior knowledge of system architecture, object-oriented design, and design patterns.\nGood work ethic, self-starter, and results-oriented\nExcellent communication skills are essential, with strong verbal and writing proficiencies\nAdditional Preferred Qualifications:\nExperience working AWS\nExperience with SAFe Agile Framework\nBachelor's/PG degree in Computer Science, Information Systems or equivalent.\nHands-on experience contributing to application architecture & designs, proven software/enterprise integration design principles\nAbility to prioritize and manage work to critical project timelines in a fast-paced environment\nExcellent Analytical and communication skills are essential, with strong verbal and writing proficiencies\nAbility to train and mentor\nBenefits:\nHealth & Wellness: Health care coverage designed for the mind and body.\nContinuous Learning: Access a wealth of resources to grow your career and learn valuable new skills.\nInvest in Your Future: Secure your financial future through competitive pay, retirement planning, a continuing education program with a company-matched student loan contribution, and financial wellness programs.\nFamily Friendly Perks: Its not just about you. S&P Global has perks for your partners and little ones, too, with some best-in class benefits for families.\nBeyond the Basics: From retail discounts to referral incentive awardssmall perks can make a big difference.",Industry Type: Banking,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Java', 'PySpark', 'Delta Lake systems', 'React', 'Angular', 'UI frameworks', 'SQL', 'NoSQL Databases', 'MongoDB', 'Databricks', 'Oracle', 'AWS', 'AWS cloud']",2025-06-12 13:56:58
EPM Principal/Sr Principal Consultant FCCS/ARCS/TRCS/PBCS/EPBCS/PCMCS,Oracle,10 - 20 years,Not Disclosed,"['Noida', 'Gurugram', 'Chennai']","Hiring for Oracle Experts at different level from 5-20 years.\n\nJob locations - Bangalore, Mumbai, Pune, Hyderabad, Chennai, Kolkata, Noida, Gurgaon, Gandhinagar\n\nEPM Products - ARCS, TRCS, FCCS, EPBCS, PBCS, EPRCS, PCMCS\n\nPlanning - PBCS/ EPBCS\nExperience in Implementation of Hyperion with strong Application Development process experience on Hyperion EPM Product Suite.\nExperience in Requirement Gathering & Solution Design\nSound knowledge on Hyperion Planning/PBCS/EPBCS\nSound functional knowledge (Understand of planning modelling like P&L, BS, Workforce, Capex planning etc.. and inter dependencies)\nSound Knowledge on Business Rules/Forms / Task Lists / Reports.\nHands on Experience on Planning Modules is must.\nGood communication Skills\n\nFCCS\n\nFunction as applications design architect/Lead for Oracle FCCS\nApplication Design point of contact for FCCS Analyst Teams\nProvide Solutions to existing Architecture Design on the current system\nCollaborate effectively with other groups\nEPM Experience 5+ Years\nExperience in Implementation of EPM cloud with strong Application Development process, experience on FCCS/HFM and good knowledge on consolidation process.\nExperience in Requirement Gathering & Solution Design\nDesired functional knowledge (Understand of Income statement, Balance Sheet, different methods of consolidation and their calculations and disclosure in financial statements)\nSound functional knowledge Finance/accounting/ General Ledger/Sub Ledgers\nSound Knowledge on Financial Reports and SmartView Reports\n\nARCS\nExperience implementing ARCS from design, configuration, data integration, and testing\nSound knowledge on ARM/ARCS including Reconciliation Compliance & Transaction Matching\nFunctional knowledge of Finance/accounting and account reconciliation is a must\nKnowledge and experience working with a consolidation tool and general ledger is a plus\nProvide Solutions to existing Architecture Design on current system\nCollaborate effectively with other groups\nEPM Experience 5+ Years\nExperience in Implementation of Hyperion with strong Application Development process experience on Hyperion EPM Product Suite.\nExperience in Requirement Gathering & Solution Design\nSound functional knowledge Finance/accounting/ General Ledger/Sub Ledgers\nSound Knowledge on standard and custom reports\n\nTRCS\nFunction as applications design architect/Lead for Tax Reporting Cloud application development\nApplication Design point of contact for Tax Reporting Teams\nProvide Solutions to existing Architecture Design on current system\nCollaborate effectively with other groups\nEPM Experience 5+ Years\nExperience in Implementation of Hyperion with strong Application Development process experience on Hyperion EPM Product Suite.\nExperience in Requirement Gathering & Solution Design\nSound knowledge on Tax reporting and compliance processes, tax accounting and direct tax functions, CBCR and Deferred Tax Calculations\nSound knowledge on Hyperion Consolidation\nDesired functional knowledge (Understand of Income statement, Balance Sheet, different methods of consolidation and their calculations and disclosure in financial statements)\nSound Knowledge on Business Rules/Forms / Task Lists / Reports.\nGood communication Skills\n\nPCMCS\nFunction as applications design architect/Lead for Profitability and Cost Management\nApplication Design point of contact Profitability and Cost Management\nProvide Solutions to existing Architecture Design on current system\nAble to understand functional requirement of client and build the solution accordingly.\nCollaborate effectively with other groups\nEPM Experience 5+ Years\nShould have completed at least 3 implementations on PCMCS.\nIn depth understanding of Oracle Hyperion Essbase (ASO and BSO)\nIn depth knowledge in Integration (Data Management)\nAbility to design and develop complex Reports using Web Reporting Studio.\nExperience in Implementation of Hyperion with strong Application Development process experience on Hyperion EPM Product Suite.\nExperience in Requirement Gathering & Solution Design\nAble to leverage the modern best practices for design-development and automation process wherever required.\nSound functional knowledge\nKnowledge on Microsoft office tools including Excel, Word and Power point, leverage Smartview to build report and ad hoc analysis.\nPCMCS Certification is a value add",Industry Type: Software Product,Department: Consulting,"Employment Type: Full Time, Permanent","['EPM', 'Cloud EPM', 'solution design', 'consulting', 'implementation', 'EPBCS', 'EPRCS', 'PBCS', 'TRCS', 'PCMCS', 'FCCS', 'ARCS']",2025-06-12 13:57:01
Technical Architect,PwC India,10 - 15 years,Not Disclosed,"['Mumbai', 'Navi Mumbai', 'Gurugram']","Role Description\nWe are looking for a suitable candidate for the opening of Data/Technical Architect role for Data Management, preferably for one who has worked in Insurance or Banking and Financial Services domain and holds relevant experience of 10+ years. The candidate should be willing to take up the role of Senior Manager/Associate Director in an organization based on overall experience.\nLocation : Mumbai and Gurugram\nRelevant experience : 10+ years",,,,"['Data Architecture', 'Technical Architecture', 'Java', 'Bigquery', 'SCALA', 'Data Lake', 'Data Warehousing', 'Data Modeling', 'Data Bricks', 'Python']",2025-06-12 13:57:03
Artificial Intelligence Architect,Emerson,10 - 20 years,Not Disclosed,['Pune'],"Role & responsibilities\nDesign robust and scalable AI/ML architectures that support the development and deployment of machine learning models and AI solutions.\nDevelop and guide the implementation of end-to-end AI/ML solutions, including model development, data processing, and system integration.\nEvaluate and recommend the latest AI/ML technologies, frameworks, and tools to enhance system capabilities and performance.\nCollaborate with software engineers and other development teams to integrate AI/ML solutions into existing systems and applications. Ensure seamless operation and performance.\nWork with cross-functional teams, including developers, data scientists, machine learning engineers, and business stakeholders, to understand requirements and design solutions that align with business objectives.\n\nPreferred candidate profile\nBachelors degree in computer science, Data Science, Statistics, or a related field or a master's degree or higher is preferred.\nMore than 3 years of experience in designing and implementing AI/ML architectures, with a proven track record of successful projects.\nExtensive experience with machine learning frameworks (e.g., Go, TensorFlow, PyTorch), programming languages C#, .Net, NodeJS and data processing tools.\nStrong understanding of system architecture principles, including distributed systems, microservices, and cloud computing.\nExperience with Microsoft Azure cloud services and their AI/ML offerings\nExperience with event-handling systems such as Kafka\nExperience with big data technologies and data engineering practices.\nExcellent verbal and written communication skills, with the ability to convey complex technical concepts to non-technical stakeholders.",Industry Type: Industrial Automation,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Aiml', 'Ml', 'Python', 'Tensorflow', 'Pytorch', 'Architecture', 'Artificial Intelligence', '.Net', 'Machine Learning', 'Scikit-Learn']",2025-06-12 13:57:05
"Sr. Associate, Application Developer",XL India Business Services Pvt. Ltd,2 - 5 years,Not Disclosed,"['Hyderabad', 'Ahmedabad', 'Bengaluru']","Senior Associate Application Developer Bangalore, Karnataka, India We are seeking a skilled Senior Associate Application Developer with excellent expertise in C#, Salesforce, and DevOps practices\n\nThe ideal candidate will play a key role in developing, implementing, and maintaining software applications that align with our business goals\n\nYou will work collaboratively with cross-functional teams to deliver high-quality solutions that enhance our operational efficiency\n\nWhat you ll be DOING What will your essential responsibilities include? Design, develop, and maintain applications using C# and Salesforce technologies\n\nCollaborate with business analysts and stakeholders to gather requirements and translate them into technical specifications\n\nImplement DevOps practices to automate the deployment and monitoring of applications, ensuring a seamless integration process\n\nTroubleshoot and resolve application issues, providing support to end-users as needed\n\nCollaborate with the product owner and other squad members to understand user requirements and translate them into functional and technical specifications Carry out code reviews and provide feedback to other developers to ensure that the code meets functional and non-functional requirements Participate in agile ceremonies such as sprint planning, daily stand-ups, sprint reviews, etc to ensure smooth delivery Developing and maintaining custom data integration solutions with various sources and formats, including structured and unstructured data, to ensure data quality and consistency\n\nManaging and monitoring data pipelines and ensuring their availability, reliability, and scalability\n\nTroubleshooting issues related to data ingestion and transformation\n\nYou will report to the Release Train Engineer\n\nWhat you will BRING We re looking for someone who has these abilities and skills: Required Skills and Abilities: Developing and maintaining custom data integration solutions with various sources and formats, including structured and unstructured data, to ensure data quality and consistency\n\nEnsures that the solution and codebase is maintainable, scalable and adheres to best practices of software development\n\nDelivering high-quality, scalable, and reliable data ingestion pipelines\n\nSupports other members of the squad in resolving technical questions related to best practice, feasibility etc Desired Skills and Abilities: Insurance background\n\nProficiency in programming language such as C#\n\nExperience with data integration tools\n\nExcellent understanding of database management systems, data warehousing, and data modeling\n\nExperience with cloud platforms such as Azure\n\nFamiliarity with big data technologies such as Hadoop, Spark, and Kafka\n\nKnowledge of data privacy and security regulations such as GDPR\n\nExcellent problem-solving and troubleshooting skills\n\nEffective communication and collaboration skills\n\nWho WE are AXA XL, the P&C and specialty risk division of AXA, is known for solving complex risks\n\nFor mid-sized companies, multinationals and even some inspirational individuals we don t just provide re/insurance, we reinvent it\n\nHow? By combining a comprehensive and efficient capital platform, data-driven insights, leading technology, and the best talent in an agile and inclusive workspace, empowered to deliver top client service across all our lines of business property, casualty, professional, financial lines and specialty\n\nWith an innovative and flexible approach to risk solutions, we partner with those who move the world forward\n\nLearn more at axaxl\n\ncom What we OFFER Inclusion AXA XL is committed to equal employment opportunity and will consider applicants regardless of gender, sexual orientation, age, ethnicity and origins, marital status, religion, disability, or any other protected characteristic\n\nAt AXA XL, we know that an inclusive culture and a diverse workforce enable business growth and are critical to our success\n\nThat s why we have made a strategic commitment to attract, develop, advance and retain the most diverse workforce possible, and create an inclusive culture where everyone can bring their full selves to work and can reach their highest potential\n\nIt s about helping one another and our business to move forward and succeed\n\nFive Business Resource Groups focused on gender, LGBTQ+, ethnicity and origins, disability and inclusion with 20 Chapters around the globe Robust support for Flexible Working Arrangements Enhanced family friendly leave benefits Named to the Diversity Best Practices Index Signatory to the UK Women in Finance Charter Learn more at axaxl\n\ncom / about-us / inclusion-and-diversity\n\nAXA XL is an Equal Opportunity Employer\n\nTotal Rewards AXA XL s Reward program is designed to take care of what matters most to you, covering the full picture of your health, wellbeing, lifestyle and financial security\n\nIt provides competitive compensation and personalized, inclusive benefits that evolve as you do\n\nWe re committed to rewarding your contribution for the long term, so you can be your best self today and look forward to the future with confidence\n\nSustainability At AXA XL, Sustainability is integral to our business strategy\n\nIn an ever-changing world, AXA XL protects what matters most for our clients and communities\n\nWe know that sustainability is at the root of a more resilient future\n\nOur 2023-26 Sustainability strategy, called Roots of resilience , focuses on protecting natural ecosystems, addressing climate change, and embedding sustainable practices across our operations\n\nOur Pillars: Valuing nature: How we impact nature affects how nature impacts us\n\nResilient ecosystems - the foundation of a sustainable planet and society - are essential to our future\n\nWe re committed to protecting and restoring nature - from mangrove forests to the bees in our backyard - by increasing biodiversity awareness and inspiring clients and colleagues to put nature at the heart of their plans\n\nAddressing climate change: The effects of a changing climate are far reaching and significant\n\nUnpredictable weather, increasing temperatures, and rising sea levels cause both social inequalities and environmental disruption\n\nWere building a net zero strategy, developing insurance products and services, and mobilizing to advance thought leadership and investment in societal-led solutions\n\nIntegrating ESG: All companies have a role to play in building a more resilient future\n\nIncorporating ESG considerations into our internal processes and practices builds resilience from the roots of our business\n\nWe re training our colleagues, engaging our external partners, and evolving our sustainability governance and reporting\n\nAXA Hearts in Action: We have established volunteering and charitable giving programs to help colleagues support causes that matter most to them, known as AXA XL s Hearts in Action programs\n\nThese include our Matching Gifts program, Volunteering Leave, and our annual volunteering day - the Global Day of Giving\n\nFor more information, please see axaxl\n\ncom/sustainability",Industry Type: Insurance,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data modeling', 'Database management', 'Agile', 'Data quality', 'data privacy', 'Business strategy', 'Application software', 'Operations', 'Monitoring', 'Salesforce']",2025-06-12 13:57:07
Solution Architect,Ericsson,9 - 10 years,Not Disclosed,['Gurugram'],"About this opportunity:\n\nAt Ericsson, we are offering a fantastic opportunity for a passionate and motivated Solution Architect to join our dynamic and diverse team. In this role, you will contribute to the design, construction, and management of Ericsson-based solutions. Familiarity with big data technologies, agile methodology and practices constitutes an integral part of the role.\nWhat you will do:\nManaging the overall operations of multiple solutions deployed within the customer environment.\nCustomer engagement is essential to secure agreements on the proposed solutions.\nPrepare technical presentations, proposals, and conduct walkthroughs with customers.\nLead the technical risk analysis and assist the Program Manager/Program Director in the overall risk analysis process.\nManage internal and external stakeholders to identify and bridge gaps.\nIdentify New Business Opportunities.\nLeading the delivery team by assigning tasks and reviewing progress.\nLead User Acceptance Testing (UAT) for the Customer.\nManaging the L1, L2, L3, and CNS (Support) teams, as well as the customers Operations and Maintenance (O&M) team.\nIdentify scope creep and change requests during the delivery phase.\nSupport Pre-Sales Activities\nPrepare Effort Estimation\nLead Customer Presentations and Demonstrations\nInterface with third-party providers (3PP) and original equipment manufacturers (OEMs) to evaluate and integrate their solutions into Ericssons offerings.\nAct as a Solution Lifecycle Manager for the proposed or implemented solution.\nProactively develop competence in new solution areas within the domain and technologies.\nMentor solution integrators, developers, and system architects, providing a transparent and open environment for growth and development.",,,,"['Maven', 'Performance management', 'XML', 'Configuration management', 'MySQL', 'SNMP', 'Presales', 'JSON', 'JIRA', 'Python']",2025-06-12 13:57:09
Associate - Digital Product Management,AMERICAN EXPRESS,5 - 10 years,Not Disclosed,['Gurugram'],"In this role, the person will report to the Product Manager Travel & Lifestyle Services, this role is an exciting opportunity for a PO/Analyst, the person will be working on data related products and to maintain quality of data for TLS in the Big Data Platform Cornerstone.\n  Minimum Qualifications\n5+ years experience in travel domain or minimum background in financial domain\nAt least 5 years of experience in technology product management or data-related products.\nAt least 5 years of experience in Software Architecture and Software Development.\n3 years experience with SQL\nExperience with agile methodologies, ie, rally, agile.\nAn ability to solve complex problems and a highly analytical approach.\nDemonstrate the ability to learn and be curious to understand and master the travel domain. You are excited and passionate for the travel domain.\nSelf-starter with the ability to think creatively and strategically\nStrong communication and stakeholder management skills\nExcellent communication skills with the ability to engage, influence, and inspire partners to drive collaboration and alignment.\nDemonstrate the ability to maintain a positive attitude and sense of humor in the face of chaos and challenges\nHas a successful record of leading and coordinating business, delivery, and technology teams to define, prioritize, and deliver on a product roadmap\nStrong product management skills that will take full ownership from analysis through implementation.\nHigh degree of organization, individual initiative, and personal accountability.\nPlatform Knowledge\nExperience working w/ Hadoop and Big Data Platform Cornerstone, Google Cloud Platform (GCP)\nProficient in Microsoft Suit, Power BI, Tableau, and SQL\nEducation\nBachelors in related fields (Computer Science, Information Technology, Engineer, Electronics)\nPreferred Qualifications\nMasters in related in fields (Computer Science, Information Technology, Engineer, Electronics)\nWe back you with benefits that support your holistic we'll-being so you can be and deliver your best. This means caring for you and your loved ones physical, financial, and mental health, as we'll as providing the flexibility you need to thrive personally and professionally:\nCompetitive base salaries\nBonus incentives\nSupport for financial-we'll-being and retirement\nComprehensive medical, dental, vision, life insurance, and disability benefits (depending on location)\nFlexible working model with hybrid, onsite or virtual arrangements depending on role and business need\nGenerous paid parental leave policies (depending on your location)\nFree access to global on-site we'llness centers staffed with nurses and doctors (depending on location)\nFree and confidential counseling support through our Healthy Minds program\nCareer development and training opportunities",Industry Type: Financial Services,Department: Product Management,"Employment Type: Full Time, Permanent","['Product management', 'Career development', 'Finance', 'Analytical', 'Agile', 'microsoft', 'Stakeholder management', 'Information technology', 'SQL']",2025-06-12 13:57:12
Manager - Software Development,Amway,8 - 12 years,Not Disclosed,['Hyderabad'],"Primary Responsibilities\nCloud Expertise: Familiarity or hands-on experience with AWS and Google Cloud Platform (GCP) technologies to support data transformation, data structures, metadata management, dependency tracking, and workload orchestration.\nCollaboration & Independence: Self-motivated and capable of supporting the data needs of multiple teams, systems, and products within Amways data ecosystem.\nBig Data & Distributed Systems: Strong understanding of distributed systems for large-scale data processing and analytics, with a proven track record of manipulating, processing, and deriving insights from large, complex, and disconnected datasets.",,,,"['Data Transformation', 'GCP', 'Cloud', 'AWS']",2025-06-12 13:57:14
Consultant,Amdocs,4 - 9 years,Not Disclosed,['Pune'],"Amdocs helps those who build the future to make it amazing. With our market-leading portfolio of software products and services, we unlock our customers innovative potential, empowering them to provide next-generation communication and media experiences for both the individual end user and enterprise customers. Our employees around the globe are here to accelerate service providers migration to the cloud, enable them to differentiate in the 5G era, and digitalize and automate their operations. Listed on the NASDAQ Global Select Market, Amdocs had revenue of $5.00 billion in fiscal 2024. For more information, visit www.amdocs.com\n\n\nIn one sentence\n\nWe are seeking a Data Engineer with advanced expertise in Databricks SQL, PySpark, Spark SQL, and workflow orchestration using Airflow. The successful candidate will lead critical projects, including migrating SQL Server Stored Procedures to Databricks Notebooks, designing incremental data pipelines, and orchestrating workflows in Azure Databricks\n\n\nWhat will your job look like\n\nMigrate SQL Server Stored Procedures to Databricks Notebooks, leveraging PySpark and Spark SQL for complex transformations.\nDesign, build, and maintain incremental data load pipelines to handle dynamic updates from various sources, ensuring scalability and efficiency.\nDevelop robust data ingestion pipelines to load data into the Databricks Bronze layer from relational databases, APIs, and file systems.\nImplement incremental data transformation workflows to update silver and gold layer datasets in near real-time, adhering to Delta Lake best practices.\nIntegrate Airflow with Databricks to orchestrate end-to-end workflows, including dependency management, error handling, and scheduling.\nUnderstand business and technical requirements, translating them into scalable Databricks solutions.\nOptimize Spark jobs and queries for performance, scalability, and cost-efficiency in a distributed environment.\nImplement robust data quality checks, monitoring solutions, and governance frameworks within Databricks.\nCollaborate with team members on Databricks best practices, reusable solutions, and incremental loading strategies\n\n\nAll you need is...\n\nBachelor s degree in computer science, Information Systems, or a related discipline.\n4+ years of hands-on experience with Databricks, including expertise in Databricks SQL, PySpark, and Spark SQL.\nProven experience in incremental data loading techniques into Databricks, leveraging Delta Lake's features (e.g., time travel, MERGE INTO).\nStrong understanding of data warehousing concepts, including data partitioning, and indexing for efficient querying.\nProficiency in T-SQL and experience in migrating SQL Server Stored Procedures to Databricks.\nSolid knowledge of Azure Cloud Services, particularly Azure Databricks and Azure Data Lake Storage.\nExpertise in Airflow integration for workflow orchestration, including designing and managing DAGs.\nFamiliarity with version control systems (e.g., Git) and CI/CD pipelines for data engineering workflows.\nExcellent analytical and problem-solving skills with a focus on detail-oriented development.\n  Preferred Qualifications  \nAdvanced knowledge of Delta Lake optimizations, such as compaction, Z-ordering, and vacuuming.\nExperience with real-time streaming data pipelines using tools like Kafka or Azure Event Hubs.\nFamiliarity with advanced Airflow features, such as SLA monitoring and external task dependencies.\nCertifications such as Databricks Certified Associate Developer for Apache Spark or equivalent.\nExperience in Agile development methodologie\n\n\nWhy you will love this job:\nYou will be able to use your specific insights to lead business change on a large scale and drive transformation within our organization.\nYou will be a key member of a global, dynamic and highly collaborative team with various possibilities for personal and professional development.\nYou will have the opportunity to work in multinational environment for the global market leader in its field!\nWe offer a wide range of stellar benefits including health, dental, vision, and life insurance as well as paid time off, sick time, and parental leave!",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['azure databricks', 'airflow', 'pyspark', 'sql', 'spark', 'azure cloud services', 'continuous integration', 'azure data lake', 'workflow orchestration', 'ci/cd', 'warehouse', 't-sql', 'sql server', 'stored procedures', 'data bricks', 'git', 'kafka', 'data warehousing concepts', 'agile']",2025-06-12 13:57:17
Advisor Application Support,Fiserv,10 - 15 years,Not Disclosed,['Pune'],"Design and Development Automation Script : Design and maintain advanced Python scripts to deliver comprehensive insights into File Transmission component and its various Life Cycle.\nPerformance Optimization : Improve efficiency when handling large datasets using techniques such as optimized large data manipulation, and RDBMS data models.\nAdvanced Regex Utilization : Apply sophisticated regular expressions to create accurate field extraction and mapping to the large dataset.\nFile Transmission Monitoring Automation : Track and report on each stage of file transmission, continuously refining monitoring strategies for enhanced reliability and visibility.\nCross-Functional Collaboration : Work closely with various teams to integrate Python script with broader IT systems and workflows.\nDevelop and maintain automation scripts using Python for testing, data validation, and system operations.\nDesign and implement automation frameworks.\nAutomate File Transmission applications using Python and Selenium.\nMaintain automated workflows and troubleshooting issues in context of File Transmissions.\nWrite reusable, scalable, and maintainable code with proper documentation.\nWhat You Will Need to Have\nEducation : bachelors and/or masters degree in Information Technology, Computer Science, or a related field.\nExperience : Minimum of 10 years in IT, with a focus on Python, SFTP tools, data integration, or technical support roles.\nProficiency in Python programming.\nExperience with Selenium for automation.\nFamiliarity with test automation frameworks like PyTest or Robot Framework.\nUnderstanding of REST APIs and tools like Postman or Python requests.\nBasic knowledge of Linux/Unix environments and shell scripting.\nDatabase Skills : Experience with relational databases and writing complex SQL queries with advanced joins.\nFile Transmission Tools : Hands-on experience with platforms like Sterling File Gateway, IBM Sterling, or other MFT solutions.\nAnalytical Thinking : Proven problem-solving skills and the ability to troubleshoot technical issues effectively.\nCommunication : Strong verbal and written communication skills for collaboration with internal and external stakeholders.\nWhat Would Be Great to Have (Optional)\nTool Experience : Familiarity with tools such as Splunk, Dynatrace, Sterling File Gateway, File Transfer tool.\nLinux : Working knowledge of Linux and command-line operations.\nSecure File Transfer Protocols : Hands-on experience with SFTP and tools like SFG, NDM, and MFT using SSH encryption.\nTask Scheduling Tools : Experience with job scheduling platforms such as AutoSys, Control-M, or cron",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Unix', 'Application support', 'Linux', 'RDBMS', 'Shell scripting', 'Selenium', 'Troubleshooting', 'Information technology', 'Technical support', 'Python']",2025-06-12 13:57:19
Power BI Developer,Kellogg Brown & Root (KBR),5 - 10 years,Not Disclosed,['Chennai'],"Title:\nPower BI Developer\nCollaborate with all levels of finance organization on reporting requirements for both internal and external customers.\nWork independently and in partnership with business owners to provide innovative interactive reporting solutions to address a wide range of business needs using Power BI, Power Query, VBA, Cognos and other reporting tools.\nTransform financial data into visualization charts using Power BI and other reporting tools.\nLeverage multiple databases to merge and compile information to calculate relevant financial and business performance metrics.\nMaximize automation of routine tasks and processes using advanced toolsets (Artificial Intelligence or AI , Optical Character Recognition or OCR , Robotic Process Automation or RPA or Bots ).\nAutomate translation and migration of data between different systems (Costpoint, Cobra, EPM, EDW, OnBase).\nEnsure data quality by identifying and correcting errors, inconsistencies, and missing data to improve accuracy.\nCreate documentation and work instructions for applications and processes, ensure compliance with KBR IT standards and controls.\nBasic Qualifications:\nBachelor s Degree or equivalent in Finance, Accounting, Business Information Technology, Business Analytics, Information Systems or a related field.\nProficiency in Power BI, Data Modeling, SQL, VBA, Power Query.\nExpert understanding of Power BI functionality (reporting, publishing, security, mobile app).\nFoundational understanding of financial reporting metrics (Revenue, Cost of Goods Sold, Indirect Rate Application, EBIT, Cashflow, DSO, DPO)\nWorking knowledge of project management core concepts (contract types, cost sets, schedule, budgets).\nExperience with data analysis techniques, data integration, data modeling and data visualization.\nFamiliarity with basic software testing and implementation concepts and methods.\nPreferred Qualifications:\nWorking knowledge of Costpoint, Cobra, Hyperion (EPM, FCCS), OnBase, EDW, MSD.\nCapability with alternate programming and reporting tools (DAX, Python or R, Appian, Cognos).\nProject management Professional (PMP) or EVMS certification.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['PMP', 'Data analysis', 'Publishing', 'Data modeling', 'Cognos', 'Hyperion', 'Data quality', 'Information technology', 'SQL', 'Python']",2025-06-12 13:57:21
EPBCS Principal Consultant//EPBCS Senior Consultant,Leading MNC,4 - 9 years,25-40 Lacs P.A.,[],"Leading MNC in Bangalore & Kochi\nHiring for EPBCS Principal Consultant/EPBCS Senior Consultant\nImmediate Joiners///Lesser notice period\nExperience 4-10 Years\n. The ideal candidate will have a strong background in Oracle Cloud EPM, particularly PBCS/EPBCS, and experience across multiple full-cycle implementation projects. You should be proficient in configuring out-of-the- box modules, writing Groovy scripts, and integrating data using tools like FDMEE, Data Management, or ODI. A solid understanding of financial and functional processes, coupled with excellent communication and documentation skills, is essential. Experience in public sector implementations and automation using EPM Automate is highly desirable.\n\nKey Responsibilities\nLead 1- 4 full-cycle Oracle Cloud EPM (PBCS/EPBCS) implementations.\nGather business requirements and create functional/technical specifications.\nConfigure and customize EPBCS modules (Financials, Workforce, Capex, Projects, Strategic Modeling).\nDevelop Groovy scripts, business rules, and automation using EPM Automate.\nManage data integrations using FDMEE, Data Management, or ODI.\nConduct system testing, troubleshoot issues, and support users post-go-live.\nBuild reports using Excel, SmartView, and Essbase tools.\nWork independently and document solutions clearly.\nCollaborate with clients and cross-functional teams to ensure project success.\nExperience in public sector implementations is a plus.\n\nRequired Skills & Qualifications\nRelevant experience ranging from 4 to 10 years\nExperience should comprise of exposure to at least 2-4 full project life cycles (Development projects)\nSolid experience with Oracle Cloud EPM (preferably PBCS/EPBCS)\nExtensive experience in analyzing requirements, writing functional specifications, conducting tests, troubleshooting issues and interfacing with business users\nExperience in Groovy scripting.\nExperience in out of the box modules Financials, Workforce, Capex, Projects and Strategic Modelling\nImplementing EPBCS at Public Sectors will be an added advantage.\nSpecific knowledge of integration tools such as; FDMEE, Data Management or ODI (Oracle Data Integrator)\nExperience with automation scripts such as EPM Automate\nExtensive knowledge of Excel, Essbase Spreadsheet Add-in, and SmartView\nExcellent written and communication skills\nKnowledge of financial process and functional processes\nAbility to work independently with minimum guidance\nStrong technical documentation skills\nExperience in a project team environment using structured methodologies for gathering business requirements, business processes, data conversions and system interfaces\n\nInterested Candidates can mail their cv at simmi@hiresquad.in or call at 8467054123",Industry Type: IT Services & Consulting,Department: Other,"Employment Type: Full Time, Permanent","['EPM', 'Epbcs', 'Smartview', 'Fdme', 'Oracle Epm', 'Essbase', 'Fccs']",2025-06-12 13:57:24
Senior DevSecOps (Bangalore) - 8+ Years - Hybrid,Databuzz Ltd,8 - 10 years,5-15 Lacs P.A.,['Bengaluru'],"Databuzz is Hiring for Senior DevSecOps Engineer Dynamics 365 (Bangalore) - 8+ Years - Hybrid\n\nPlease mail your profile to jagadish.raju@databuzzltd.com with the below details, If you are Interested.\n\nAbout DatabuzzLTD: Databuzz is One stop shop for data analytics specialized in Data Science, Big Data, Data Engineering, AI & ML, Cloud Infrastructure and Devops. We are an MNC based in both UK and INDIA. We are a ISO 27001 & GDPR complaint company.\n\nCTC -\nECTC -\nNotice Period/LWD - (Candidate serving notice period will be preferred)\nDOB-\n\nPosition: Senior DevSecOps Engineer Dynamics 365 (Bangalore) - 8+ Years - Hybrid\n\nMandatory Skills:\n\nShould have experience in Azure DevOps\nShould have experience Dynamics - 365\nStrong experience with Python and PowerShell for scripting and automation tasks\nExperienced in working with Kubernetes, Terraform,\nGood to have Service Now, YAML\n\nRegards,\nJagadish Raju - Talent Acquisition Specialist\njagadish.raju@databuzzltd.com",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Dynamics 365', 'Powershell', 'Azure Devops', 'Python', 'Terraform', 'Docker', 'Servicenow', 'Yaml', 'Kubernetes']",2025-06-12 13:57:26
"Startup Account Manager, North",Amazon,10 - 15 years,Not Disclosed,['Gurugram'],"Sales, Marketing and Global Services (SMGS)\n\nAWS Sales, Marketing, and Global Services (SMGS) is responsible for driving revenue, adoption, and growth from the largest and fastest growing smalland mid-market accounts to enterprise-level customers including public sector.\n\nAmazon Web Services (AWS) offers a set of cloud services that enable all companies, from startups to enterprises, to run virtually everything in the cloud, including mobile applications, big data analytics, AI/ML platforms, and microservices/serverless infrastructures. AWS India Pvt. Ltd. , the reseller for cloud services in India, is looking for a Senior Startup Account Manager to help drive the growth of high-potential startups in India.\nYou need to possess passion about Startups, be a self-starter with a strong entrepreneurial spirit who is prepared to work in a fast-paced, often ambiguous environment, execute against ambitious goals, and consistently embrace the Amazon Culture. Your responsibilities will include driving growth and user adoption, migrations and ensuring startups select AWS as their preferred cloud provider in India. You will work closely with counterparts in business development, marketing, solution architecture and partner teams to lead execution of BD plays.\n\nThe candidate should have technical background that enables him/her to drive engagement at the CXO level as well as with software developers and IT architects. The candidate should be an exceptional analytical thinker who thrives in fast-paced dynamic environments and has excellent communication and presentation skills. The candidate should be visioning and executing via collaboration with an extended team to address all startup s needs.\n\n\nEnsure customer success with early and growth stage startups in India\nDrive growth and market share in a defined territory\nAccelerate customer adoption through well-developed BD engagements\nDevelop and execute against a comprehensive account/territory plan.\nCreate & articulate compelling value propositions around AWS services.\nAccelerate customer adoption by engaging Founders, CXO, Board of Directors and VC influencers\nWork with AWS partners to manage joint selling opportunities\nAssist customers in identifying use cases for priority adoption of AWS as well as best practices implementations\nDevelop long-term strategic relationships with key accounts.\n\nA day in the life\nMeet startup CXOs and help them Build on AWS\nLeverage AWS startup programs to support early stage startups to bring idea to market\nTrack investments, technology trends; build coverage plans and oversee execution\nCollaborate with cross functional teams such as Sales, VC BD, Solutions Architect, Partners, Marketing\nEnsure high standards and maintain sales pipeline hygiene\n\nAbout the team\nThe AWS Startups team partners with startups around the world to build, launch, grow, and help scale their business. We don t just support startups with cloud infrastructure, but also partner with our startup customers throughout their journey by providing resources to tackle challenges from early stage fundraising to building technical teams and developing startup culture.\n\nAbout AWS\n\nDiverse Experiences\nAWS values diverse experiences. Even if you do not meet all of the preferred qualifications and skills listed in the job description, we encourage candidates to apply. If your career is just starting, hasn t followed a traditional path, or includes alternative experiences, don t let it stop you from applying.\n\nWhy AWS?\nAmazon Web Services (AWS) is the world s most comprehensive and broadly adopted cloud platform. We pioneered cloud computing and never stopped innovating that s why customers from the most successful startups to Global 500 companies trust our robust suite of products and services to power their businesses.\n\nInclusive Team Culture\nHere at AWS, it s in our nature to learn and be curious. Our employee-led affinity groups foster a culture of inclusion that empower us to be proud of our differences. Ongoing events and learning experiences, including our Conversations on Race and Ethnicity (CORE) and AmazeCon (gender diversity) conferences, inspire us to never stop embracing our uniqueness.\n\nMentorship & Career Growth\nWe re continuously raising our performance bar as we strive to become Earth s Best Employer. That s why you ll find endless knowledge-sharing, mentorship and other career-advancing resources here to help you develop into a better-rounded professional.\n\nWork/Life Balance\nWe value work-life harmony. Achieving success at work should never come at the expense of sacrifices at home, which is why we strive for flexibility as part of our working culture. When we feel supported in the workplace and at home, there s nothing we can t achieve in the cloud. 10+ years of technology experience with a focus on field BD (quota-carrying) Experience in working with Startups in identifying, developing, negotiating, and closing large-scale technology deals.\nExperience in positioning and selling technology to new customers and in new market segments. Experience in proactively growing customer relationships within an account while expanding their understanding of the customer s business.\nExcellent verbal and written communications skills Functioned in an environment where they managed an account list in technology which included large growth in net new opportunities.\nProven track record of consistent territory growth and quota attainment. BA/BS/B.Tech degree required. Masters or MBA is a plus.\nUnderstanding of AWS and/or technology as a service (Iaas,SaaS,PaaS) is preferred.",,,,"['Solution architecture', 'Territory growth', 'Cloud computing', 'big data analytics', 'Cloud Services', 'Analytical', 'PAAS', 'cxo', 'Mobile applications', 'Solution Architect']",2025-06-12 13:57:29
Sr ETL/SSIS developer,Sagility India,6 - 11 years,Not Disclosed,['Bengaluru'],"Job Summary\nWe are seeking a highly skilled and self-driven SSIS with strong communication and client-facing skills to join our healthcare analytics team. This role requires a combination of deep technical expertise in SSIS and data integration along with the ability to consult and collaborate directly with clients to understand and address their data needs.\nThe ideal candidate will be experienced in building and maintaining scalable data pipelines, working with diverse healthcare data sources, and ensuring data quality and availability for downstream analytics. You will play a key role in delivering clean, trusted, and timely data for insights and reporting.\nKey Responsibilities\nDesign, develop, and maintain robust and scalable SSIS to support healthcare analytics and reporting platforms.\nEngage directly with clients to gather requirements, provide consultation, and translate business needs into technical solutions.\nIntegrate and normalize data from diverse healthcare data sources, including claims, EMR, lab, pharmacy, and eligibility systems.\nEnsure data accuracy, completeness, and consistency throughout ingestion and transformation processes.\nOptimize and tune data workflows for performance and scalability in a cloud or on-premise data platform.\nTroubleshoot and resolve data issues in a timely and proactive manner to support high data availability.\nCollaborate with analysts, data scientists, and business stakeholders to ensure data pipelines meet analytical needs.\nCreate and maintain comprehensive technical documentation for data pipelines, data dictionaries, and workflows.\nStay informed on healthcare compliance requirements (e.g., HIPAA), and ensure data handling practices follow regulatory standards.\nRequired Skills and Qualifications\n6+ years of experience in SSIS development and data engineering\nProven ability to interact directly with clients and translate business problems into data solutions\nStrong experience with SQL, SSIS, or PySpark for data processing\nDeep understanding of data warehousing concepts and dimensional modeling\nExperience working with healthcare datasets (e.g., claims, eligibility, clinical data)\nFamiliarity with cloud platforms (Azure, AWS, or GCP) and data lakes\nStrong troubleshooting, problem-solving, and performance tuning skills\nExcellent verbal and written communication skills\nBachelor's or Masters degree in Computer Science, Engineering, Information Systems, or a related field\nPreferred Qualifications\nProficiency in building data pipelines using tools such as Azure Data Factory, Informatica, Databricks, or equivalent\nExperience with FHIR, HL7, or other healthcare data standards\nFamiliarity with HIPAA and healthcare compliance requirements\nKnowledge of reporting tools like Power BI or Tableau\nExposure to CI/CD and data pipeline automation\nWhy Join Us?\nWork on high-impact healthcare projects with meaningful outcomes\nEngage directly with clients and make a tangible difference in their data strategy\nCollaborative team culture and continuous learning opportunities\nFlexible work arrangements and competitive compensation\n\nLocation - Bangalore\nShit Timing - 2 Pm to 11 PM\nWork - Hybrid\n\nRegards,\nnaveen.vediyappan@sagility.com",Industry Type: BPM / BPO,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SSIS', 'SQL', 'ETL']",2025-06-12 13:57:31
SR. Databricks Developer,Labcorp,7 - 12 years,Not Disclosed,['Bengaluru'],"Labcorp is hiring a Senior Data engineer.  This person will be an integrated member of Labcorp Data and Analytics team and work within the IT team.   Play a crucial role in designing, developing and maintaining data solutions using Databricks, Fabric, Spark, PySpark and Python.  Responsible to review business requests and translate them into technical solution and technical specification.  In addition, work with team members to mentor fellow developers to grow their knowledge and expertise.  Work in a fast paced and high-volume processing environment, where quality and attention to detail are vital.\n\nRESPONSIBILITIES:\nDesign and implement end-to-end data engineering solutions by leveraging the full suite of Databricks, Fabric tools, including data ingestion, transformation, and modeling.\nDesign, develop and maintain end-to-end data pipelines by using spark, ensuring scalability, reliability, and cost optimized solutions.\nConduct performance tuning and troubleshooting to identify and resolve any issues.\nImplement data governance and security best practices, including role-based access control, encryption, and auditing.\nWork in fast-paced environment and perform effectively in an agile development environment.\n\nREQUIREMENTS:\n8+ years of experience in designing and implementing data solutions with at least 4+ years of experience in data engineering.\nExtensive experience with Databricks, Fabric, including a deep understanding of its architecture, data modeling, and real-time analytics.\nMinimum 6+ years of experience in Spark, PySpark and Python.\nMust have strong experience in SQL, Spark SQL, data modeling & RDBMS concepts.\nStrong knowledge of Data Fabric services, particularly Data engineering, Data warehouse, Data factory, and Real- time intelligence.\nStrong problem-solving skills, with ability to perform multi-tasking.\nFamiliarity with security best practices in cloud environments, Active Directory, encryption, and data privacy compliance.\nCommunicate effectively in both oral and written.\nExperience in AGILE development, SCRUM and Application Lifecycle Management (ALM).\nPreference given to current or former Labcorp employees.\n\nEDUCATION:\nBachelors in engineering, MCA.",Industry Type: Medical Services / Hospital (Diagnostics),Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Pyspark', 'Data Bricks', 'Python', 'Parquet', 'UDP', 'Shell Scripting', 'Microsoft SQL Server', 'DW BI project', 'Kafka', 'Mapreduce', 'EMR', 'Redshift', 'Hive', 'MySQL', 'Spark', 'Aws Databricks', 'Oracle', 'Redshift spectrum', 'Fabric', 'Lambda', 'Athena']",2025-06-12 13:57:34
"Spark, Java, Kafka- Hyderabad",Cognizant,12 - 15 years,Not Disclosed,['Hyderabad'],"Skill: Java, Spark, Kafka\nExperience: 10 to 16 years\nLocation: Hyderabad\n As Data Engineer, you will :\n       Support in designing and rolling out the data architecture and infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources\n       Identify data source, design  and implement data schema/models and integrate data that meet the requirements of the business stakeholders",,,,"['hive', 'cloudera', 'modeling', 'scala', 'data warehousing', 'apache pig', 'data pipeline', 'data architecture', 'scalability', 'sql', 'java', 'data modeling', 'spark', 'mysql', 'hadoop', 'etl', 'big data', 'hbase', 'python', 'oozie', 'data processing', 'airflow', 'elt', 'data engineering', 'nosql', 'mapreduce', 'kafka', 'feasibility analysis', 'hdfs', 'sqoop', 'aws']",2025-06-12 13:57:36
Senior Etl Informatica Developer,VBeyond,6 - 8 years,19-25 Lacs P.A.,"['Noida', 'Chennai', 'Bengaluru']","We are seeking a highly skilled and experienced Senior ETL & Reporting QA Analyst to join our dynamic team. The ideal candidate will bring strong expertise in ETL and Report Testing, with a solid command of SQL, and hands-on experience in Informatica, as well as BI Reporting tools. A strong understanding of the Insurance domain is crucial to this role. This position will be instrumental in ensuring the accuracy, reliability, and performance of our data pipelines and reporting solutions.\n\nKey Responsibilities:\nDesign, develop, and execute detailed test plans and test cases for ETL processes, data migration, and data warehousing solutions.\nPerform data validation and data reconciliation using complex SQL queries across various source and target systems.\nValidate Informatica ETL workflows and mappings to ensure accurate data transformation and loading.\nConduct end-to-end report testing and dashboard validations using Cognos (preferred), or comparable BI tools such as Tableau or Power BI.\nCollaborate with cross-functional teams including Business Analysts, Developers, and Data Engineers to understand business requirements and transform them into comprehensive test strategies.\nIdentify, log, and track defects to closure using test management tools and actively participate in defect triage meetings.\nMaintain and enhance test automation scripts and frameworks where applicable.\nEnsure data integrity, consistency, and compliance across reporting environments, particularly in the insurance domain context.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Informatica', 'ETL', 'Power Bi', 'Insurance Domain', 'Tableau', 'SQL']",2025-06-12 13:57:38
"Salesforce Developer (Apex, Lightning Web Components & Salesforce)",Synechron,5 - 10 years,Not Disclosed,['Pune'],"job requisition idJR1023217\n\nJob Summary\nSynechron is seeking a skilled and dedicated\n\nSalesforce Developer to join our dynamic team. In this role, you will be responsible for designing, developing, and implementing robust Salesforce solutions that enhance business processes and improve user experience. You will work closely with business analysts, product managers, and technical teams to deliver high-quality, scalable applications and configurations within the Salesforce platform. Your contribution will directly support our organization's digital transformation initiatives and ensure our Salesforce ecosystem aligns with evolving industry standards and client needs.\n\nSoftware\n\nRequired\n\nSkills:\nExtensive hands-on experience with Salesforce development including Apex, Lightning Web Components (LWC), Visualforce, and Salesforce configuration tools.\nProficiency with Salesforce SFDX, Visual Studio Code, and Git-based version control systems (e.g., Git, Bitbucket).\nKnowledge of Salesforce's declarative tools such as Flows, Process Builder, Approval Processes, and Email Templates.\nStrong understanding of Salesforce security model (Profiles, Permission Sets, Sharing Settings, Sharing Rules).\nExperience with Salesforce Mobile and Experience Cloud setup and configurations.\nGood understanding of Salesforce Governor Limits and best practices for optimization.\nExperience with SOQL, SOSL, DML operations, and data modeling.\nKnowledge of Salesforce integration techniques, including REST/SOAP APIs, and middleware tools like MuleSoft or AutoRabbit (preferred).\nFamiliar with development tools such as Salesforce DX, VS Code, and CI/CD pipelines using Jenkins or similar tool\nPreferred\n\nSkills:\nSalesforce Admin and Developer certifications.\nExperience with Einstein Analytics, Mulesoft, or AutoRabbit.\nKnowledge of industry-specific compliance or security standards, especially within Financial Services.\nOverall Responsibilities\nDevelop, customize, and maintain Salesforce applications, including Apex classes, triggers, Lightning Web Components, and Visualforce pages.\nDesign scalable and reusable Lightning components and configurations that meet business requirements.\nCollaborate with functional stakeholders to translate business needs into technical solutions.\nEnsure the integrity, security, and performance tuning of Salesforce applications.\nMaintain continuous alignment with Salesforce best practices, including governance, security policies, and performance optimization.\nCreate technical documentation, including design specifications, test cases, and deployment instructions.\nConduct code reviews, unit testing, and participate in release management.\nSupport data integrations with external systems using APIs or middleware.\nContribute to SCRUM/Agile development cycles, providing timely updates and collaborating effectively with team members.\nTechnical Skills (By Category)\nProgramming Languages:\nEssentialApex, JavaScript\nPreferredJava, R, or additional languages relevant to integration and automation\nDatabases & Data Management:\nStrong knowledge of Salesforce objects, data modeling, SOQL, SOSL, and DML operations\nCloud Technologies:\nSalesforce Cloud (Experience Cloud, Mobile)\nOptionalMuleSoft, AutoRabbit for integrations\nFrameworks and Libraries:\nLightning Web Components (LWC), Aura Components\nDevelopment Tools & Methodologies:\nSalesforce DX, Visual Studio Code, Git/Bitbucket, Agile/Scrum methodologies\nSecurity Protocols:\nSalesforce security model, sharing settings, and best practices for data protection\nExperience\nMinimum of 5+ years of extensive Salesforce development experience including large-scale customizations and integrations.\nProven track record delivering end-to-end Salesforce solutions in complex enterprise environments.\nExperience working within Agile teams with continuous integration/deployment.\nIndustry experience in Financial Services is preferred, especially in Investment Banking, Asset Management, or Banking.\nDemonstrated ability to work collaboratively with cross-functional teams and stakeholders.\nDay-to-Day Activities\nDevelop and test Apex classes, triggers, Lightning Web Components, and other custom Salesforce components.\nCollaborate with business analysts and product owners to refine requirements.\nConduct code reviews, unit testing, and assist in user acceptance testing.\nManage Salesforce releases, deployments, and configurations.\nInvestigate and debug system issues, and optimize platform performance.\nParticipate in sprint planning, stand-ups, and retrospectives.\nStay current on Salesforce platform updates, new features, and best practices.\nSupport ongoing enhancements and user support initiatives.\nQualifications\nBachelor's degree in Computer Science, Information Technology, or a related field.\nSalesforce certifications such as Salesforce Certified Platform Developer I & II and Salesforce Certified Service Cloud Consultant are strongly preferred.\nStrong understanding of Salesforce architecture, frameworks, and best practices.\nProfessional Competencies\nExcellent problem-solving and analytical skills.\nEffective communication skills to interface with technical and non-technical stakeholders.\nAbility to work independently and as part of a team in a fast-paced environment.\nStrong organizational skills with an ability to prioritize tasks effectively.\nSelf-motivated learner committed to staying current with Salesforce updates and industry trends.\nAdaptability and eagerness to contribute to innovative solutions.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sosl', 'soql', 'salesforce', 'sales force development', 'salesforce integration', 'visualforce', 'salesforce dx', 'continuous integration', 'rest', 'dml', 'salesforce lightning', 'ci/cd', 'salesforce security model', 'javascript', 'apex', 'visual studio code', 'git', 'data modeling', 'lwc', 'agile', 'soap']",2025-06-12 13:57:41
Cyber Security Manager // 6-10 years // Mumbai,2coms,7 - 12 years,Not Disclosed,['Mumbai'],"SUMMARY\nOur client is IT MNC part of one of the major insurance groups based out of Germany and Europe. The Group is represented in around 30 countries worldwide, with Over 40,000 people worldwide, focusing mainly on Europe and Asia. Our client offers a comprehensive range of insurances, pensions, investments and services by focusing on all cutting edge technologies majorly on Could, Digital, Robotics Automation, IoT, Voice Recognition, Big Data science, advanced mobile solutions and much more to accommodate the customers future needs around the globe thru supporting millions of internal and external customers with state of-the-art IT solutions to everyday problems & dedicated to bringing digital innovations to every aspect of the landscape of insurance.\n \nJob Location: Hiranandani Gardens, Powai, Mumbai\n \nMode: Work from Office\n\n\nRequirements\nRoles & Responsibilities:\nDefine project scope,   objectives, and deliverables in collaboration with stakeholders.\nDevelop comprehensive project plans, including timelines, budgets, and resource allocation.\nManage and coordinate project teams, including security engineers, analysts, and other technical resources.\nTrack project progress, identify and manage risks and issues, and implement effective mitigation strategies.\nEnsure adherence to project management methodologies and best practices.\nStay up-to-date with the latest cyber security trends   and technologies.\nSkill & Competencies:\nStrong track record of delivering IT projects in a large, complex environment.  (7 years), especially experience in the implementation of financial and regulatory requirements in the CFO context in Group-wide systems and their integration\nProven 5+ years experience as a PM \nBachelor's degree in Computer Science, Information Technology, or a related field.\nProven experience  (typically 5+ years) managing IT projects, with a significant focus on cyber security initiatives.",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['risk management', 'it security', 'cyber security', 'information technology', 'iso', 'owasp', 'soc', 'ceh', 'information security', 'vulnerability management', 'siem', 'vulnerability assessment', 'cissp', 'nessus', 'cyber', 'security', 'it projects', 'security engineering', 'application security', 'vapt', 'penetration testing']",2025-06-12 13:57:44
"Manager, WFM, Account Health Support (AHS)",Amazon,3 - 8 years,Not Disclosed,['Hyderabad'],"Amazon.com strives to be Earths most customer-centric company where people can find and discover virtually anything they want to buy online. By giving customers more of what they want low prices, vast selection, and convenience Amazon.com continues to grow and evolve as a world-class e-commerce platform. The Account Health Support (AHS) Specialist within the AHS team acts as the primary interface between Amazon and our Selling partners. We obsess over providing world class support to Sellers selling on the Amazon platform. We strive to predict Sellers needs before they recognize they may have a problem, create innovative self-help tools, and provide solutions to help our partners better serve their customers.\n\n\nCalculate demand volume forecast at interval level, day level, week level and at month level, along with knowledge of statistical indicators to check their accuracy.\nCapacity planning at weekly/ monthly level, so that the required headcount for hiring could be shared with senior leadership.\nCreate schedules on excel and on a scheduling tool (preferably Aspect) based week level, day level and interval level volume pattern.\nDiving deep into data/processes to identify problems and solutions and presenting them to leadership.\nKeeping regular communication with site operations, senior leadership, technology teams and other stakeholders to manage critical parameters, employee experience, contingency etc.\nPublishing reports of critical WFM and other important parameters to drive efficiency in them and to keep all relevant stakeholders regularly informed.\nCreating employees rotational plan and conducting shift bid process to help shift rollover for frontline staff.\nOptimizing break, meeting and other non-productive activities, managing interval level service level.\nManaging real time analysts and schedulers.\n\nA day in the life\nThe ideal candidate is passionate about leveraging data and tools to deliver actionable insights that drive improvements in planning accuracy, and has a strong delivery record and experienced in driving execution in a cross-functional environment, backed by analysis and data. They thrive in a fast-paced environment, relishes working with large transactional volumes and big data and enjoys the challenge of highly complex, and sometimes ambiguous, business context. You will work cross-functionally to ensure that decisions are made and actioned, which will ensure our operations have the volume to run as efficiently as possible.\n\nAbout the team\nThe Account Health Support Workforce Management Team has a mission of fulfilling the Service Level agreements continuously in partnership with Operations, throughout all verticals/marketplaces along with optimum utilization of the available resources and meeting the goal thresholds for all the capacity level attributes (Shrinkage, TPH etc.). To attain to the program objectives, AHS Workforce team sets appropriate goals for Operations (Shrinkage), drives effective queue management, time to time checks to ensure capacity on each Vertical is sufficient to handle projected volume and take necessary actions to meet the requirement if otherwise, scheduling heads appropriately to match the incoming patterns, queueing tasks manually to fill for the deficit in projected volume and to support any new launches, effective management of non-production time to reduce idle hours and sharing reports on different performance metrics to drive the results.\n3+ years of team management experience\n3+ years of program or project management experience\n3+ years of working cross functionally with tech and non-tech teams experience\n3+ years of delivering cross functional projects experience\nExperience defining program requirements and using data and metrics to determine improvements Experience implementing repeatable processes and driving automation or standardization\nExperience in data mining, data management, reporting, and SQL queries",Industry Type: Internet,Department: Project & Program Management,"Employment Type: Full Time, Permanent","['Automation', 'Service level', 'Team management', 'Publishing', 'Workforce management', 'Data management', 'Project management', 'Scheduling', 'Data mining', 'Capacity planning']",2025-06-12 13:57:46
Application Developer - Dot Net IFRS 17 // Mumbai // 4-8 Yrs,2coms,4 - 9 years,Not Disclosed,['Mumbai'],"SUMMARY\nAbout the client:\n\nOur client is an IT Technology & Services Management MNC , supporting millions of internal and external customers with state of-the-art IT solutions to everyday problems & dedicated to bringing digital innovations to every aspect of the landscape of insurance. Our Client is a part of one of the major insurance groups based out of Germany and Europe. The Group is represented in around 26 countries worldwide, with Over 37,000 people worldwide, focusing mainly on Europe and Asia & offers a comprehensive range of insurances, pensions, investments and services by focusing on all cutting edge technologies majorly on Could, Digital, Robotics Automation, IoT, Voice Recognition, Big Data science, advanced mobile solutions and much more to accommodate the customers future needs around the globe.\n\n\n\n\n\nRequirements\nCompentences for .NET\nNET Framework (incl. .net core), SQL Server, Entity Framework\nPrioritize business impact and urgency\nAbility to learn new technology and methodology quickly.\nKnowledge User Task API, GUI (Graphical User Interfaces) Design Compentences for NTS (New Tech Stack)\nNTS (New Tech Stack), Container runtime environment with Docker containers and Kubernetes, Cloud with AWS (Amazon Web Services), CI/CD - Continuous Integration / Continuous Deployment with Jenkins, Knowledge Source Management Github and Nexus\nOpenshift, Kerberos Authentication, Competences for Cluster Workflow\nDesign + implementation of process model, Design + implementation of input interfaces in REST format\nDevelopment of flow services below process model\n\nEducational Qualifications:\nBachelor’s or Master’s  degree in Computer Science /Engineering/Information Technology\nCandidate with non-computer science degree must have minimum 1 year of relevant experience\nMBA in IT / Insurance/Finance   can also apply for Requirements Engineer and Test Engineer role.\n\n\n\nBenefits",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['continuous integration', 'kubernetes', 'nexus', 'web services', 'openshift', 'ci/cd', 'kerberos', 'ccm', 'business objects administration', 'docker', 'profibus', 'jenkins', 'cd', 'rest', 'e-discovery', 'github', 'entity framework', 'intools', 'sql server', 'application development', 'net application', '.net core', '.net', 'abb dcs', 'aws']",2025-06-12 13:57:48
IT Project Associate,ZS,1 - 3 years,Not Disclosed,['Pune'],"Executes the end-to-end management of application development projects: including resource management, change management, vendor coordination, communications, training requirements, and budget (if applicable).\nEstimate the resources and participants needed to achieve project goals.\nReviews and recommends changes, reductions or additions to the overall project.\nActs as the liaison between IT, vendors, and end-users.\nMaintains the efficiency of the project coordination process such as planning, scheduling, and budget and risk assessment.",,,,"['Change management', 'Team management', 'Project management', 'Risk assessment', 'Consulting', 'Financial planning', 'Management consulting', 'Scheduling', 'Application development', 'Resource management']",2025-06-12 13:57:51
AI Associate Consultant - Platform Delivery,ZS,3 - 5 years,Not Disclosed,['Gurugram'],"Customer Success Associate Consultant design, develop and execute high-impact analytics solutions for large, complex, structured, and unstructured data sets (including big data) to drive impact on client business (topline). This person will lead the engagement for AI based SaaS product deployment to clients across industries. Leverage their strong Data Science, analytics and engineering skills to build Advanced analytics processes, build scalable and operational process pipelines and find data-driven insights that help our clients solve their most important business problems and bring optimizations. Associate Consultants also engage with Project Leadership team and clients to help them understand the insights, summaries, implications and make plans to act on them.",,,,"['Hospitality', 'Team management', 'Analytical', 'Management consulting', 'Financial planning', 'Healthcare', 'Predictive modeling', 'Stakeholder management', 'Analytics', 'SQL']",2025-06-12 13:57:53
Business Analyst,CGI,6 - 11 years,Not Disclosed,['Hyderabad'],"Business Data Analyst - HealthCare\nPosition Description\nJob Summary\nWe are seeking an experienced and results-driven Business Data Analyst with 5+ years of hands-on experience in data analytics, visualization, and business insight generation. This role is ideal for someone who thrives at the intersection of business and datatranslating complex data sets into compelling insights, dashboards, and strategies that support decision-making across the organization.\nYou will collaborate closely with stakeholders across departments to identify business needs, design and build analytical solutions, and tell compelling data stories using advanced visualization tools.\nKey Responsibilities\nData Analytics & Insights Analyze large and complex data sets to identify trends, anomalies, and opportunities that help drive business strategy and operational efficiency.\n• Dashboard Development & Data Visualization Design, develop, and maintain interactive dashboards and visual reports using tools like Power BI, Tableau, or Looker to enable data-driven decisions.\n• Business Stakeholder Engagement Collaborate with cross-functional teams to understand business goals, define metrics, and convert ambiguous requirements into concrete analytical deliverables.\n• KPI Definition & Performance Monitoring Define, track, and report key performance indicators (KPIs), ensuring alignment with business objectives and consistent measurement across teams.\n• Data Modeling & Reporting Automation Work with data engineering and BI teams to create scalable, reusable data models and automate recurring reports and analysis processes.\n• Storytelling with Data Communicate findings through clear narratives supported by data visualizations and actionable recommendations to both technical and non-technical audiences.\n• Data Quality & Governance Ensure accuracy, consistency, and integrity of data through validation, testing, and documentation practices.\nRequired Qualifications\nBachelor’s or Master’s degree in Business, Economics, Statistics, Computer Science, Information Systems, or a related field.\n• 5+ years of professional experience in a data analyst or business analyst role with a focus on data visualization and analytics.\n• Proficiency in data visualization tools: Power BI, Tableau, Looker (at least one).\n• Strong experience in SQL and working with relational databases to extract, manipulate, and analyze data.\n• Deep understanding of business processes, KPIs, and analytical methods.\n• Excellent problem-solving skills with attention to detail and accuracy.\n• Strong communication and stakeholder management skills with the ability to explain technical concepts in a clear and business-friendly manner.\n• Experience working in Agile or fast-paced environments.\nPreferred Qualifications\nExperience working with cloud data platforms (e.g., Snowflake, BigQuery, Redshift).\n• Exposure to Python or R for data manipulation and statistical analysis.\n• Knowledge of data warehousing, dimensional modeling, or ELT/ETL processes.\n• Domain experience in Healthcare is a plus.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Healthcare Domain', 'Bigquery', 'Redshift Aws', 'Snowflake', 'Data Analytics', 'Data Visualization', 'Python']",2025-06-12 13:57:55
Power platform Developer,ITC Infotech,3 - 8 years,9-19 Lacs P.A.,['Bengaluru'],"Key Responsibilities:\nDesign, develop, test, and deploy custom business applications using Microsoft Power Platform (Power Apps, Power Automate, Power BI, Power Virtual Agents).\nCollaborate with business users and IT teams to gather and analyze requirements.\nCreate automated workflows and process automation using Power Automate to improve operational efficiency.\nDevelop interactive dashboards and reports using Power BI to provide actionable insights.",,,,"['Canvas', 'Power Platform', 'Powerapps', 'Microsoft Power Automate', 'Power Automate']",2025-06-12 13:57:57
Senior Power BI Developer,Luxoft,6 - 11 years,Not Disclosed,['Bengaluru'],"Power BI Dashboard Development (UI Dashboards)\nDesign, develop, and maintain visually compelling, interactive Power BI dashboards aligned with business needs.\nCollaborate with business stakeholders to gather requirements, develop mockups, and refine dashboard UX.\nImplement advanced Power BI features like bookmarks, drill-throughs, dynamic tooltips, and DAX calculations.\nConduct regular UX/UI audits and performance tuning on reports.\nData Modeling in SQL Server & Dataverse\nBuild and manage scalable, efficient data models in Power BI, Dataverse, and SQL Server.\nApply best practices in dimensional modeling (star/snowflake schema) to support analytical use cases.\nEnsure data consistency, accuracy, and alignment across multiple sources and business areas.\nPerform optimization of models and queries for performance and load times.\nPower BI Dataflows & ETL Pipelines\nDevelop and maintain reusable Power BI Dataflows for centralized data transformations.\nCreate ETL processes using Power Query, integrating data from diverse sources including SQL Server, Excel, APIs, and Dataverse.\nAutomate data refresh schedules and monitor dependencies across datasets and reports.\nEnsure efficient data pipeline architecture for reuse, scalability, and maintenance.\nSkills\nMust have\nExperience: 6+ years in Business Intelligence or Data Analytics with a strong focus on Power BI and SQL Server.\nTechnical Skills:\nExpert-level Power BI development, including DAX, custom visuals, and report optimization.\nStrong knowledge of SQL (T-SQL) and relational database design.\nExperience with Dataverse and Power Platform integration.\nProficiency in Power Query, Dataflows, and ETL development.\nModeling: Proven experience in dimensional modeling, star/snowflake schema, and performance tuning.\nData Integration: Skilled in connecting and transforming data from various sources, including APIs, Excel, and cloud data services.\nCollaboration: Ability to work with stakeholders to define KPIs, business logic, and dashboard UX.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Performance tuning', 'data services', 'Data modeling', 'Database design', 'Analytical', 'Schema', 'power bi', 'SSIS', 'Business intelligence', 'microsoft']",2025-06-12 13:58:00
Director - BIM,Axtria,5 - 10 years,Not Disclosed,['Noida'],"Position Summary \nThis position is part of the technical leadership in data warehousing and Business Intelligence areas. Someone who can work on multiple project streams and clients for better business decision making especially in the area of Lifesciences/ Pharmaceutical domain.\n\n Job Responsibilities \n\no Technology Leadership – Lead guide the team independently or with little support to design, implement deliver complex cloud data management and BI project assignments. o Technical portfolio – Expertise in a range of BI and data hosting technologies like the AWS stack (Redshift, EC2), Snowflake, Spark, Full Stack, Qlik, Tableau, Microstrategy o Project Management – Get accurate briefs from the Client and translate into tasks for team members with priorities and timeline plans. Must maintain high standards of quality and thoroughness. Should be able to monitor accuracy and quality of others' work. Ability to think in advance about potential risks and mitigation plans. o Logical Thinking – Able to think analytically, use a systematic and logical approach to analyze data, problems, and situations. Must be able to guide team members in analysis. o Handle Client Relationship, P&L – Manage client relationship and client expectations independently. Should be able to deliver results back to the Client independently. Should have excellent communication skills.\n\n Education \n\nBE/B.Tech\nMaster of Computer Application\n\n Work Experience \n\nMinimum of 5 years of relevant experience in Pharma domain. TechnicalShould have 15 years of hands on experience in the following tools\nMust have working knowledge of toolsAtleast 2 of the following – Qlikview, QlikSense, Tableau, Microstrategy, Spotfire Aware of techniques such asUI design, Report modeling, performance tuning and regression testing Basic expertise with MS excel Advanced expertise with SQL FunctionalShould have experience in following concepts and technologies\nSpecifics\nPharma data sources like IMS, Veeva, Symphony, Cegedim etc.\nBusiness processes like alignment, market definition, segmentation, sales crediting, activity metrics calculation 0-2 years of relevant experience in a large/midsize IT services/Consulting/Analytics Company1-3 years of relevant experience in a large/midsize IT services/Consulting/Analytics Company3-5 years of relevant experience in a large/midsize IT services/Consulting/Analytics Company3-5 years of relevant experience in a large/midsize IT services/Consulting/Analytics Company\n\n Behavioural Competencies \n\nProject Management\nCommunication\nAttention to P&L Impact\nTeamwork & Leadership\nMotivation to Learn and Grow\nLifescience Knowledge\nOwnership\nCultural Fit\nScale of resources managed\nScale of revenues managed / delivered\nProblem solving\nTalent Management\nCapability Building / Thought Leadership\n\n Technical Competencies \n\nAWS KnowHow\nFormal Industry Certification AWS Certified Cloud Practitioner\nSnowflake\nData Engineering\nData Governance\nData Modelling\nData Operations (Service Management)\nData Warehousing & Data Lake\nDatabricks\nDataiku\nFormal Industry Certification Informatica_Cloud Data Warehouse & Data Lake Modernization\nMaster Data Management\nPatient Data Analytics Know How\nPharma Commercial Data - US\nPharma Commercial Data - EU",Industry Type: Analytics / KPO / Research,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['spotfire', 'sql', 'qlikview', 'microstrategy', 'tableau', 'snowflake', 'pharmaceutical', 'project management', 'performance tuning', 'regression testing', 'hosting', 'amazon redshift', 'bi', 'bim', 'sales', 'veeva', 'advanced ms excel', 'talent management', 'amazon ec2', 'spark', 'full stack', 'aws']",2025-06-12 13:58:02
Business Technology Solutions Associate Consultant,ZS,4 - 6 years,Not Disclosed,"['Pune', 'Gurugram']","Undertake primary ownership in driving self and team effort across all phases of a project lifecycle;\nTranslate business requirements into technical terms and drive team effort to design, build and manage technology solutions that solve business problems;\nApply appropriate development methodologies (eg: agile, waterfall) and best practices (eg: mid-development client reviews, embedded QA procedures, unit testing) to ensure successful and timely completion;\nPartner with Project lead/ Program lead in delivering projects and assist in project management responsibility like - project plans, people management, staffing and risk mitigation;",,,,"['Compliance', 'Data management', 'Staffing', 'MIS', 'Project management', 'Consulting', 'Financial planning', 'Data processing', 'Scheduling', 'SQL']",2025-06-12 13:58:05
Associate - Business Information Management,Axtria,5 - 10 years,Not Disclosed,['Gurugram'],"Position Summary \n\nThis is the Requisition for Employee Referrals Campaign and JD is Generic.\n\nWe are looking for Associates with 5+ years of experience in delivering solutions around Data Engineering, Big data analytics and data lakes, MDM, BI, and data visualization. Experienced to Integrate and standardize structured and unstructured data to enable faster insights using cloud technology. Enabling data-driven insights across the enterprise.\n\n Job Responsibilities \n\n\nHe/she should be able to design implement and deliver complex Data Warehousing/Data Lake, Cloud Data Management, and Data Integration project assignments.\n\nTechnical Design and Development – Expertise in any of the following skills.\n\nAny ETL tools (Informatica, Talend, Matillion, Data Stage), andhosting technologies like the AWS stack (Redshift, EC2) is mandatory.\n\nAny BI toolsamong Tablau, Qlik & Power BI and MSTR.\n\nInformatica MDM, Customer Data Management.\n\nExpert knowledge of SQL with the capability to performance tune complex SQL queries in tradition and distributed RDDMS systems is must.\n\nExperience across Python, PySpark and Unix/Linux Shell Scripting.\n\nProject Managementis\n\nmust to have. Should be able create simple to complex project plans in Microsoft Project Plan and think in advance about potential risks and mitigation plans as per project plan.\n\nTask Management – Should be able to onboard team on the project plan and delegate tasks to accomplish milestones as per plan. Should be comfortable in discussing and prioritizing work items with team members in an onshore-offshore model.\n\nHandle Client Relationship – Manage client communication and client expectations independently or with support of reporting manager. Should be able to deliver results back to the Client as per plan. Should have excellent communication skills.\n\n\n Education \n\nBachelor of Technology\nMaster's Equivalent - Engineering\n\n Work Experience \n\nOverall, 5- 7years of relevant experience inData Warehousing, Data management projects with some experience in the Pharma domain.\n\nWe are hiring for following roles across Data management tech stacks -\n\nETL toolsamong Informatica, IICS/Snowflake,Python& Matillion and other Cloud ETL.\n\nBI toolsamong Power BI and Tableau.\n\nMDM - Informatica/ Raltio, Customer Data Management.\n\nAzure cloud Developer using Data Factory and Databricks\n\nData Modeler-Modelling of data - understanding source data, creating data models for landing, integration.\n\nPython/PySpark -Spark/ PySpark Design, Development, and Deployment",Industry Type: Analytics / KPO / Research,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['aws stack', 'sql', 'etl tool', 'data visualization', 'sql queries', 'data management', 'amazon redshift', 'bi', 'data warehousing', 'pyspark', 'spark', 'etl', 'data lake', 'snowflake', 'python', 'big data analytics', 'datastage', 'talend', 'power bi', 'data engineering', 'tableau', 'mdm', 'aws', 'informatica', 'unix']",2025-06-12 13:58:07
OAC ODI Architect (Senior Oracle Analytics Consultant),Mastek,10 - 15 years,15-30 Lacs P.A.,"['Ahmedabad', 'Chennai']","We are looking for OAC ODI Architect to be based in Ahemdabad or Chennai\nMinimum Architect exp 4 Yrs\nOracle Analytics Consultant (OAC, ODI, FDI) Tech Architect\nLocation: [Specify Location or Remote] Chennai Ahmedabad\nExpected DOJ June\nEmployment Type: Full-time\nExperience Level: 10 - 15+ Years\nJob Summary:\nWe are seeking an experienced and results-driven Senior Oracle Analytics Consultant with over 10 years of hands-on experience in Oracle Analytics Cloud (OAC), Oracle Data Integrator (ODI), and Fusion Data Intelligence (FDI). The ideal candidate will have a deep understanding of enterprise data architecture, data integration best practices, and cloud-based analytics solutions. This role involves working closely with cross-functional teams to design, implement, and support advanced analytics and data integration solutions that drive business value.\nKey Responsibilities:\nLead the design, development, and deployment of analytics solutions using Oracle Analytics Cloud (OAC).\nArchitect and implement data integration pipelines using Oracle Data Integrator (ODI) for on-prem and cloud data sources.\nCollaborate with business and IT stakeholders to design and deploy Fusion Data Intelligence (FDI) based dashboards and KPIs.\nOptimize performance of OAC dashboards and reports, including data modeling and visualization best practices.\nDevelop and manage data models, RPDs, and semantic layers within OAC.\nBuild and maintain ETL mappings, packages, and workflows in ODI.\nIntegrate Oracle Fusion Applications with OAC and FDI for near-real-time reporting.\nDrive data governance and quality initiatives across analytics platforms.\nTroubleshoot technical issues and provide solutions in a timely manner.\nMentor junior developers and provide technical leadership on complex projects.\nQualifications:\nBachelors or Masters degree in Computer Science, Information Systems, or related field.\n10+ years of relevant experience with strong focus on:\nOracle Analytics Cloud (OAC) - Must\nOracle Data Integrator (ODI) - Must\nFusion Data Intelligence (FDI) Good to Have\nExpertise in Oracle Fusion ERP/HCM data models and subject areas.\nExperience integrating multiple data sources, including on-premise and cloud systems.\nStrong understanding of SQL, PL/SQL, and performance tuning.\nFamiliarity with data lake architecture, data warehousing, and ELT/ETL design patterns.\nProven experience working in Agile/DevOps environments.\nExcellent communication, analytical thinking, and problem-solving skills.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Oac', 'ODI', 'Odi Architecture', 'FDI']",2025-06-12 13:58:09
Snowflake Architect,Allegis Global Solutions (AGS),9 - 14 years,Not Disclosed,[],"Snowflake Architect\nJob Location: Hyderabad / Bangalore / Chennai / Kolkata / Noida/ Gurgaon / Pune / Indore / Mumbai\n\nJob Details\n\nTechnical Expertise:\nStrong proficiency in Snowflake architecture, including data sharing, partitioning, clustering, and materialized views.\nAdvanced experience with DBT for data transformations and workflow management.\nExpertise in Azure services, including Azure Data Factory, Azure Data Lake, Azure Synapse, and Azure Functions.\nData Engineering:\nProficiency in SQL, Python, or other relevant programming languages.\nStrong understanding of data modeling concepts, including star schema and normalization.\nHands-on experience with ETL/ELT pipelines and data integration tools.\nSoft Skills:\nExcellent problem-solving and analytical skills.\nStrong communication and stakeholder management abilities.\nAbility to work in agile teams and handle multiple priorities.\nPreferred Qualifications:\nCertifications in Snowflake, DBT, or Azure Data Engineering.\nFamiliar with data visualization tools like Power BI or Tableau.\nKnowledge of CI/CD pipelines and DevOps practices for data workflows.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure', 'Snowflake', 'Data Build Tool', 'SQL']",2025-06-12 13:58:12
Manager - BIM,Axtria,8 - 13 years,Not Disclosed,['Pune'],"We are looking for a highly skilled and experienced Data Engineering Manager to lead our data engineering team. The ideal candidate will possess a strong technical background, strong project management abilities, and excellent client handling/stakeholder management skills. This role requires a strategic thinker who can drive the design, development and implementation of data solutions that meet our clients needs while ensuring the highest standards of quality and efficiency.\nJob Responsibilities\nTechnology Leadership Lead guide the team independently or with little support to design, implement deliver complex cloud-based data engineering / data warehousing project assignments\nSolution Architecture & Review Expertise in conceptualizing solution architecture and low-level design in a range of data engineering (Matillion, Informatica, Talend, Python, dbt, Airflow, Apache Spark, Databricks, Redshift) and cloud hosting (AWS, Azure) technologies\nManaging projects in fast paced agile ecosystem and ensuring quality deliverables within stringent timelines\nResponsible for Risk Management, maintaining the Risk documentation and mitigations plan.\nDrive continuous improvement in a Lean/Agile environment, implementing DevOps delivery approaches encompassing CI/CD, build automation and deployments.\nCommunication & Logical Thinking Demonstrates strong analytical skills, employing a systematic and logical approach to data analysis, problem-solving, and situational assessment. Capable of effectively presenting and defending team viewpoints, while securing buy-in from both technical and client stakeholders.\nHandle Client Relationship Manage client relationship and client expectations independently. Should be able to deliver results back to the Client independently. Should have excellent communication skills.\nEducation\nBE/B.Tech\nMaster of Computer Application\nWork Experience\nShould have expertise and 8+ years of working experience in at least two ETL tools among Matillion, dbt, pyspark, Informatica, and Talend\nShould have expertise and working experience in at least two databases among Databricks, Redshift, Snowflake, SQL Server, Oracle\nShould have strong Data Warehousing, Data Integration and Data Modeling fundamentals like Star Schema, Snowflake Schema, Dimension Tables and Fact Tables.\nStrong experience on SQL building blocks. Creating complex SQL queries and Procedures.\nExperience in AWS or Azure cloud and its service offerings\nAware of techniques such as: Data Modelling, Performance tuning and regression testing\nWillingness to learn and take ownership of tasks.\nExcellent written/verbal communication and problem-solving skills and\nUnderstanding and working experience on Pharma commercial data sets like IQVIA, Veeva, Symphony, Liquid Hub, Cegedim etc. would be an advantage\nHands-on in scrum methodology (Sprint planning, execution and retrospection)\nBehavioural Competencies\nTeamwork & Leadership\nMotivation to Learn and Grow\nOwnership\nCultural Fit\nTalent Management\nTechnical Competencies\nProblem Solving\nLifescience Knowledge\nCommunication\nAgile\nPySpark\nData Modelling\nDesigning technical architecture\nAWS Data Pipeline",Industry Type: Analytics / KPO / Research,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['BIM', 'Azure', 'Snowflake', 'Databricks', 'SQL Server', 'Oracle', 'AWS', 'Redshift']",2025-06-12 13:58:14
Manager - BIM,Axtria,5 - 10 years,Not Disclosed,['Gurugram'],"Position Summary \n\nTo be a technology expert architecting solutions and mentoring people in BI / Reporting processes with prior expertise in the Pharma domain.\n\n Job Responsibilities \n\no Technology Leadership – Lead guide the team independently or with little support to design, implement deliver complex reporting and BI project assignments. o Technical portfolio – Expertise in a range of BI and hosting technologies like the AWS stack (Redshift, EC2), Qlikview, QlikSense, Tableau, Microstrategy, Spotfire o Project Management – Get accurate briefs from the Client and translate into tasks for team members with priorities and timeline plans. Must maintain high standards of quality and thoroughness. Should be able to monitor accuracy and quality of others' work. Ability to think in advance about potential risks and mitigation plans. o Logical Thinking – Able to think analytically, use a systematic and logical approach to analyze data, problems, and situations. Must be able to guide team members in analysis. o Handle Client Relationship – Manage client relationship and client expectations independently. Should be able to deliver results back to the Client independently. Should have excellent communication skills.\n\n Education \n\nBE/B.Tech\nMaster of Computer Application\n\n Work Experience \n\n- Minimum of 5 years of relevant experience in Pharma domain.\n- Technical:\nShould have 10+ years of hands on experience in the following tools:\nMust have working knowledge of toolsAtleast 2 of the following – Qlikview, QlikSense, Tableau, Microstrategy, Spotfire/ (Informatica, SSIS, Talend & metallion)/ Big Data technologies - Hadoop ecosystem.\nAware of techniques such asUI design, Report modeling, performance tuning and regression testing\nBasic expertise with MS excel\nAdvanced expertise with SQL\n- Functional:\nShould have experience in following concepts and technologies:\nSpecifics:\nPharma data sources like IMS, Veeva, Symphony, Cegedim etc.\nBusiness processes like alignment, market definition, segmentation, sales crediting, activity metrics calculation\nCalculation of all sales, activity and managed care KPIs\n\n Behavioural Competencies \n\nTeamwork & Leadership\nMotivation to Learn and Grow\nOwnership\nCultural Fit\nTalent Management\n\n Technical Competencies \n\nProblem Solving\nLifescience Knowledge\nCommunication\nProject Management\nAttention to P&L Impact\nCapability Building / Thought Leadership\nScale of revenues managed / delivered",Industry Type: Analytics / KPO / Research,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['bi', 'sql', 'spotfire', 'qlikview', 'tableau', 'hive', 'pharmaceutical', 'regression testing', 'scala', 'amazon redshift', 'bim', 'big data technologies', 'spark', 'hadoop', 'big data', 'project management', 'performance tuning', 'oozie', 'talend', 'microstrategy', 'amazon ec2', 'sqoop', 'ssis', 'aws', 'informatica']",2025-06-12 13:58:17
Sap Cloud Platform Integration Consultant(lookingfor immediate joiner),Xforia Technologies,3 - 8 years,Not Disclosed,"['Pune', 'Bengaluru']","Role & responsibilities:\nProfessional Experience:\n\n10+ years of extensive experience in SAP integration technologies, with a strong focus on SAP Integration Suite.\nProven experience in architecting and implementing large-scale integration solutions.\nDeep understanding of various integration methodologies, tools, and platforms (e.g., SAP PI/PO, Ariba Managed Gateway for Spend and Network, SAP Cloud Integration for Data Services, Dell Boomi).\nProven track record in leading and mentoring technical teams.\n\nTechnical Skills:\n\nExpertise in SAP Integration Suite, including designing and implementing complex integration scenarios.\nStrong programming skills in Java, Groovy, or other relevant scripting languages.\nProficient in SOAP, REST, OData, and other web service protocols.\nExtensive experience with SAP APIs, BAPIs, IDocs, RFCs, and ALE.\nKnowledge of security and compliance standards, especially as they relate to data integration.\n\nSoft Skills:\nExceptional problem-solving and analytical skills.\nExcellent leadership and project management abilities.sibilities",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['client handling', 'end to end implementation', 'CPI']",2025-06-12 13:58:19
Global Talent Acquisition Specialist II Recruiter,Innodata India,5 - 7 years,Not Disclosed,['Noida'],"Role Overview:\nWe are looking for a dynamic and experienced Recruiter / Talent Acquisition Specialist with strong hands-on experience in global hiring across APAC, EMEA (Europe, Middle East, Africa), and the UK. This role demands someone who can handle language-based hiring as well as large-scale hiring drives in AI/data service functions like content moderation, annotation, and related roles.\nKey Responsibilities:\nManage end-to-end recruitment for global roles across APAC, EMEA, and UK regions.\nUtilize LinkedIn Recruiter and other global sourcing tools effectively.\nExperience in language-based hiring: Mandarin, French, Spanish, and other multilingual roles.\nHandle bulk hiring and specialized hiring for roles like Content Moderators, Annotators, Language Experts etc.\nPartner with business stakeholders to understand hiring needs and deliver within tight timelines.\nBuild a strong pipeline of candidates and maintain proactive candidate engagement.\nEnsure a seamless candidate experience and adherence to hiring SLAs.\nRequired Skills & Experience:\n57 years of proven experience in global recruitment.\nExcellent sourcing skills using LinkedIn Recruiter, job boards, and headhunting.\nPrior experience in language/content/annotation-based hiring is a must.\nExperience hiring for AI/Data Engineering/Content Services domains is a strong plus.\nStrong communication and stakeholder management skills.\nAbility to thrive in a fast-paced, target-driven environment.\nImmediate joiners preferred.",Industry Type: IT Services & Consulting,Department: Human Resources,"Employment Type: Full Time, Permanent","['International Hiring', 'EMEA', 'Global Talent Acquisition', 'Apac Recruitment']",2025-06-12 13:58:22
Manager - BIM,Axtria,4 - 8 years,Not Disclosed,['Noida'],"Position Summary \n\nTo be a technology expert architecting solutions and mentoring people in BI / Reporting processes with prior expertise in the Pharma domain.\n\n Job Responsibilities \nIndependently, he/she should be able to drive and deliver complex reporting and BI project assignments in PowerBI on AWS/Azure Cloud. Should be able to design and deliver across Power BI services, Power Query, DAX, and data modelling concepts. Should be able to write complex SQLs focusing on Data Aggregation and analytic calculations used in the reporting KPIs.\nBe able to analyse the data and understand the requirements directly from customer or from project teams across pharma commercial data sets\nShould be able to drive the team on the day-to-day tasks in alignment with the project plan and collaborate with team to accomplish milestones as per plan. Should be comfortable in discussing and prioritizing work items in an onshore-offshore model.\nAble to think analytically, use a systematic and logical approach to analyse data, problems, and situations.\nManage client communication and client expectations independently. Should be able to deliver results back to the Client as per plan. Should have excellent communication skills .\n\n\n Education \n\nBE/B.Tech\nMaster of Computer Application\n\n Work Experience \nShould have 4-8 years of working on experience in developing Power BI reports. Must have proficiency in Power BI services, Power Query, DAX, and data modelling concepts.\nShould have experience in design techniques such as UI designing and creating mock-ups/intuitive visualizations seamless user experience.\nShould have expertise in writing complex SQLs focusing on Data Aggregation and analytic calculations used for deriving the reporting KPIs.\nStrong understanding of data integration, ETL processes, data warehousing , preferably on AWS Redshift and/or Snowflake.\nExcellent problem-solving skills with the ability to troubleshoot and resolve technical issues.\nStrong communication and interpersonal skills, with the ability to collaborate effectively with cross-functional teams.\nGood to have experience in the Pharma Commercial data sets and related KPIs for Sale Performance, Managed Market, Customer 360, Patient Journey etc.\nGood to have experience and additional know-how on other reporting tools.\n\n\n Behavioural Competencies \n\nTeamwork & Leadership\nMotivation to Learn and Grow\nOwnership\nCultural Fit\nTalent Management\n\n Technical Competencies \n\nProblem Solving\nLifescience Knowledge\nCommunication\nCapability Building / Thought Leadership\nPower BI\nSQL\nBusiness Intelligence(BI)\nSnowflake",Industry Type: Analytics / KPO / Research,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data warehousing', 'dax', 'power bi', 'power query', 'etl process', 'amazon redshift', 'bim', 'sql', 'sqls', 'java', 'data modeling', 'ssrs', 'asp.net', 'etl', 'c#', 'snowflake', 'power bi reports', 'ssas', 'user interface designing', 'vb', 'azure cloud', 'sql server', 'data crunching', 'aws', 'ssis', 'data integration']",2025-06-12 13:58:24
Senior ServiceNow Developer,Luxoft,6 - 11 years,Not Disclosed,['Bengaluru'],"Internal Data Structures & Configuration\nDesign, build, and maintain data models, tables, and relationships within the ServiceNow platform.\nExtend and customize out-of-the-box modules (e.g., CMDB, Incident, Change, Request, etc.) to meet business requirements.\nEnsure data integrity, normalization, and performance optimization across the ServiceNow environment.\nCollaborate with stakeholders to translate business requirements into scalable ServiceNow configurations or custom applications.\nReporting & Dashboards\nDevelop real-time dashboards and reports using ServiceNow Reporting Tools and Performance Analytics.\nDeliver insights into key ITSM metrics such as SLAs, incident trends, and operational KPIs.\nAutomate the generation and distribution of recurring reports to stakeholders.\nWork with business and technical teams to define and implement reporting frameworks tailored to their needs.\nAutomated Feeds & API Integration\nDevelop and manage robust data integrations using ServiceNow REST/SOAP APIs.\nBuild and maintain data pipelines to and from external systems (e.g., CMDB, HRIS, ERP, Flexera, etc.).\nImplement secure, scalable automation for data exchange with appropriate error handling, logging, and monitoring.\nTroubleshoot and resolve integration-related issues to ensure smooth system interoperability.\nSkills\nMust have\nMinimum 6+ years of hands-on experience with ServiceNow, including ITSM, CMDB, and integrations.\nTechnical Expertise:\nAdvanced knowledge of ServiceNow architecture, configuration, and scripting (JavaScript, Glide).\nStrong experience with REST/SOAP APIs for ServiceNow integrations.\nSolid understanding of relational databases, data normalization, and model optimization.\nFamiliarity with common enterprise systems such as ERP, HRIS, Flexera, and CMDB tools.\nReporting Skills:\nProficiency in ServiceNow Performance Analytics, standard reporting, and dashboard design.\nExperience defining KPIs and building automated reporting solutions.\nSoft Skills:\nStrong communication and collaboration skills.\nProven ability to translate business requirements into scalable ServiceNow solutions.\nAnalytical and detail-oriented mindset with a problem-solving approach.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['ERP', 'Automation', 'SAP', 'HRIS', 'Analytical', 'Javascript', 'Data structures', 'Risk management', 'Analytics', 'Monitoring']",2025-06-12 13:58:26
Hadoop with Scala Developer,Allime Tech Solutions,8 - 12 years,Not Disclosed,['Bengaluru'],"Allime Tech Solutions is looking for Hadoop with Scala Developer to join our dynamic team and embark on a rewarding career journey.\nA Developer is responsible for designing, developing, and maintaining software applications and systems\nThey collaborate with a team of software developers, designers, and stakeholders to create software solutions that meet the needs of the business\nKey responsibilities:Design, code, test, and debug software applications and systemsCollaborate with cross-functional teams to identify and resolve software issuesWrite clean, efficient, and well-documented codeStay current with emerging technologies and industry trendsParticipate in code reviews to ensure code quality and adherence to coding standardsParticipate in the full software development life cycle, from requirement gathering to deploymentProvide technical support and troubleshooting for production issues\nRequirements:Strong programming skills in one or more programming languages, such as Python, Java, C++, or JavaScriptExperience with software development tools, such as version control systems (e\ng\nGit), integrated development environments (IDEs), and debugging toolsFamiliarity with software design patterns and best practicesGood communication and collaboration skills",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['hive', 'c++', 'python', 'software testing', 'scala', 'software design', 'version control', 'hadoop development', 'apache pig', 'sql', 'coding', 'git', 'java', 'spark', 'design patterns', 'software solutions', 'debugging', 'troubleshooting', 'code review', 'hadoop', 'sqoop', 'programming', 'communication skills']",2025-06-12 13:58:29
o9 SSIS integration Consultant,Thoucentric,3 - 8 years,Not Disclosed,"['Hyderabad', 'Chennai', 'Bengaluru']","At Thoucentric, we work on various problem statements.\nThe most popular ones are -\nBuilding capabilities that address a market need, basis our ongoing research efforts\nSolving a specific use case for a current or potential client based on challenges on-ground\nDeveloping new systems that help be a better employer and a better partner to clients",,,,"['O9', 'SSIS', 'SQL']",2025-06-12 13:58:31
.Net Fullstack Developer,S&P Global Market Intelligence,3 - 8 years,Not Disclosed,['Hyderabad'],"The Team:\nOur team is responsible for the design, architecture, and development of our client facing applications using MI Platform and Office Add-Ins that are regularly updated as new technologies emerge. You will have the opportunity every day to work with people from a wide variety of backgrounds and will be able to develop a close team dynamic with coworkers from around the globe.\nThe Impact:\nAs a member of the Team, you will be responsible for analysis, design, architecture, development, and support several applications.The ideal candidate should have expertise with cutting edge technologies and a desire to drive change through all alignment across the enterprise. The role requires the candidate to be a hands-on problem solver and developer helping to extend and manage the applications. The work you do will be used every single day, its the essential code youll write that provides the data and analytics required for crucial, daily decisions in the capital and commodities markets.\nWhats in it for you:\nBuild a career with a global company\nExposure to work on Latest cutting-edge Technologies and Grow and improve your skills by working on enterprise level products.\nOpportunity to grow personally and professionally.\nWork on code that fuels the global financial markets.\nResponsibilities:\nDemonstrate a strong sense of ownership and responsibility with release goals. This includes understanding requirements, technical specifications, design, architecture, implementation, unit testing, builds/deployments, and code management.\nEnsure compliance through the adoption of enterprise standards and promotion of best practice guiding principles aligned with organization standards.\nHands-on position requiring strong analytical, architecture, development and debugging skills that includes both development and operations.\nAttaint in-depth Functional knowledge of the domain that we are working on.\nUnderstand Incident Management, Problem Management and able to do root cause analysis.\nEnsure data governance principles adopted, data quality checks and data lineage implemented in each hop of the Data.\nProvide technical guidance and mentoring to the team and help them adopt change as new processes are introduced\nChampion best practices and serve as a subject matter authority\nDevelop solutions to develop/support key business needs\nEngineer components and common services based on standard development models, languages and tools\nProduce system design documents and lead technical walkthroughs\nRegularly evaluate cloud applications, hardware, and software.\nProduce high quality code\nCollaborate effectively with technical and non-technical partners\nAs a team-member should continuously improve the architecture\nWhat We Are Looking For:\nBasic Qualifications\nBachelor'smasters degree in computer science, Information Systems or equivalent.\n3-10 years of experience in application development using Microsoft Technologies.\nKnowledge of object-oriented design, .NET framework and design patterns.\nCommand of essential technologies: HTML, Single Page Application (SPA) frameworks, JavaScript Frameworks (jQuery, KnockoutJS, TypeScript, Durandal, React JS, Require), C#, .NET Framework, CSS, LESS, SQL Server, Web API, Web services\nGood to have: ASP.Net\nExperience with developing solutions involving relational database technologies on SQL Server platform, stored procedure programming experience using Transact SQL.\nExperience deploying data engineering solutions in public clouds like AWS, GCP, or Azure, leveraging cloud power to its fullest.\nProficient with software development lifecycle (SDLC) methodologies like Agile, Test- driven development.\nExperience working with any relational Databases preferably SQL Server.\nExperience in continuous delivery through CI/CD pipelines, containers, and orchestration technologies.\nExperience working with cross functional teams, with strong interpersonal and written communication skills.\nCandidate must have the desire and ability to quickly understand and work within new technologies.\nGood communication and collaboration skills and Ability to work as team player, train and mentor when needed.",Industry Type: Banking,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['.Net', 'C#', 'CSS', 'Web API', 'KnockoutJS', 'HTML', 'SQL Server', '.NET Framework', 'React JS', 'Single Page Application', 'TypeScript', 'software development lifecycle', 'jQuery', 'Web services', 'ASP.Net', 'Durandal', 'LESS']",2025-06-12 13:58:33
"Senior Backend Developer (Java, Spring Boot)",Apex One,6 - 11 years,Not Disclosed,['Bengaluru'],"Senior Backend Developer (Java, Spring Boot, object databases, Elasticsearch)\nRequirements:\n6+ years of experience\nProficiency in Java, Spring Boot, object databases, ElasticSearch/Solr,\nPractice in using AWS cloud, Docker & Kubernetes, REST APIs\nExperience in building scalable and high-performance systems\nStrong communication skills in English (B2+)\nNice-to-have: Knowledge of Python, ETL experience, and big data solutions\nResponsibilities:\nMaintenance of a large modern search platform\nHandling production issues and incidents\nOptimizing and maintaining existing code for performance and availability\nEnsuring high performance and availability of the system\nEngage in the Release Process\nTeam Information:\nWork within a SAFe, scrum / kanban methodology, and agile approach\nCollaborative and friendly atmosphere\nUtilization of microservice architecture and extensive CI/CD automation\nTools used: git, IntelliJ, Jira, Confluence, i3 by Tieto as search backend",Industry Type: Management Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Backend Development', 'Java', 'Kanban', 'Elasticsearch', 'object databases', 'CI/CD', 'scrum', 'Spring Boot']",2025-06-12 13:58:36
"Senior Specialist, BA/DA",XL India Business Services Pvt. Ltd,2 - 7 years,Not Disclosed,['Gurugram'],"Senior Specialist, BA/DA Gurgaon/Bangalore, India AXA XL recognizes data and information as critical business assets, both in terms of managing risk and enabling new business opportunities\n\nThis data should not only be high quality, but also actionable - enabling AXA XL s executive leadership team to maximize benefits and facilitate sustained dynamic advantage\n\nOur Innovation, Data & Analytics function is focused on driving innovation through optimizing how we leverage data to drive strategy and differentiate ourselves from the competition\n\nAs we develop an enterprise-wide data and digital strategy that moves us toward greater focus on the use of data and data-driven insights, we are seeking a Senior Specialist for our Data Sourcing & Solutions team\n\nThe role sits across the Innovation, Data & Analytics Department to ensure customer requirements are properly captured and transformed into actionable data specifications\n\nSuccess in the role will require focus on proactive management of the sourcing and management of data from source through usage\n\nWhat you ll be DOING What will your essential responsibilities include? Identify, evaluate, and acquire various data sources that align with the customer needs\n\nThis may involve collaborating with business stakeholders, third party vendors and source system teams\n\nDesign and implement data integration strategies to combine diverse datasets from internal and external sources\n\nThis would include accountable for documenting data requirements to the ETL processes, APIs, and data pipelines\n\nDevelop data solutions to address specific business challenges\n\nThis might involve creating custom data models that provide actionable insights which integrate with the existing data assets\n\nOversee the organization and management of data within databases, ensuring data security, integrity, and accessibility\n\nAble to work in Agile framework by defining and prioritizing the product backlog, collaborating with agile teams to deliver the business goals and customer needs\n\nWork closely with cross-functional teams, including Data Engineers, Data Science, Data Management, Data Governance, Data Quality, BI Solutions and Stakeholders\n\nImplement measures to maintain data accuracy, consistency, and completeness\n\nPerform data validation and cleansing as needed\n\nAdhere to data governance standards, ensuring compliance with regulations and internal policies related to data usage and privacy\n\nProficiency in various data technologies such as SQL, Azure cloud technologies, Databricks to analyze and produce data insights\n\nStay updated with emerging technologies in data management\n\nDeveloping expertise in the Insurance domain to better understand the context of the data\n\nIdentify data related issues, troubleshoot problems, and recommend solutions to enhance data sourcing and integration processes\n\nProvide guidance and mentorship to junior analysts and team members, fostering a culture of continuous learning and improvement\n\nTranslate complex technical concepts into understandable insights for non-technical stakeholders to drive data-informed decision-making\n\nExplore innovative approaches to data acquisition, integration, and solution development that can lead to improved efficiency and effectiveness\n\nIt is pivotal in ensuring that an organization s data ecosystem is robust, well-integrated, and capable of providing accurate and actionable insights to become a data driven organization\n\nInstill a customer-first attitude, prioritizing service for our business stakeholders above all else\n\nYou will report to Lead Specialist, Data Sourcing & Solutions\n\nWhat you will BRING We re looking for someone who has these abilities and skills: Required Skills and Abilities: Extensive experience in a data role (business analyst, data analyst, analytics) preferably in the Insurance industry and within a data division\n\nExcellent presentation, communication (oral & written), and relationship building skills, across all levels of management and customer interaction\n\nExcellent SQL knowledge, exposure to Azure cloud technologies (including Databricks) and technical ability to query AXA XL data sources to understand our data\n\nDeep insurance experience in data, underwriting, claims and/or operations, including influencing, collaborating, and leading efforts in complex, disparate and inter-related teams with competing priorities\n\nPassion for data and experience working within a data driven organization\n\nIntegrate internal data with external industry data to deliver holistic solutions\n\nWork with unstructured data to unlock information needed by the business to create unique products for the insurance industry\n\nPossesses excellent exploratory analysis skills and high intellectual curiosity\n\nDisplays exceptional organizational skills and is detail oriented\n\nEffective conceptual thinker who connects dots, critical thinking, and analytical skills\n\nAbility to take ownership, work under pressure, and meet deadlines\n\nAbility to work with team members across the globe and across departments\n\nDesired Skills and Abilities: Builds trust and rapport within and across groups\n\nApplies in-depth knowledge of business and specialized areas to solve business problems and understand integration challenges and long-term impact creatively and strategically\n\nAbility to manage data needs of an individual project(s) while being able to understand the broader enterprise data perspective\n\nExpected to recommend innovation and improvement to policies, procedures, deploying resources and performing core activities",Industry Type: Insurance,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data validation', 'Claims', 'Data management', 'data security', 'Underwriting', 'Agile', 'data governance', 'Data quality', 'Business strategy', 'Customer interaction']",2025-06-12 13:58:38
"Digital Analytics Manager, BI Solution Design & Transformation",XL India Business Services Pvt. Ltd,6 - 11 years,Not Disclosed,['Gurugram'],"Senior Specialist - BI answer Design & Transformation Bengaluru (Bangalore) India or Gurgaon India AXA XL recognizes digital, data and information assets are critical for the business, both in terms of managing risk and enabling new business opportunities\n\nData and Insights should not only be high quality, but also actionable - enabling AXA XL s executive leadership team to maximize benefits and achieve sustained dynamic advantage\n\nOur Innovation, Data & Analytics (IDA) function is focused on driving innovation by optimizing how we leverage digital, data and AI to drive strategy and differentiate ourselves from the competition\n\nAs we develop an enterprise-wide data and digital strategy that moves us toward greater focus on the use of data and strengthen our capabilities, we are seeking a Senior Specialist to join our BI & Reporting function, as part of our BI Solution Design & Transformation team\n\nIn this role, you will be a key technical BI & Reporting lead for all our global Power BI (inc\n\nPower Apps, Power Automate etc) solutions, and will lead efforts to enhance, transform and streamline the BI platform landscape\n\nYou will be an ambassador of new and existing BI solutions across all regions and must be comfortable proactively engaging with business users\n\nThis will enable the organization to gain necessary, timely insights to drive business decisions, build our dynamic advantage and help differentiate in the market through easy-to-use, scalable, cost effective, secure, and high-performance BI platforms\n\nIn this role, you will report to the Division Lead, BI Solution Design & Transformation, based in London\n\nYou will work within a global team in your day-to-day work with teams and business users around the world\n\nWhat you ll be doing What will your essential responsibilities include? Be a technical lead and go to expert in the team on the development of new BI Reporting solutions globally, and make sure new products are fit for customer purpose, built to best practice standards, and efficiently utilize company resources and data assets\n\nBe able to work directly with customers in the business, to understand requirements, design new solutions and troubleshoot issues, as well as prioritize work based upon a deep customer understanding\n\nBe able to advise the wider team and bring industry best practice techniques to make sure our BI platforms are well-governed, maintained, remain secure and optimized for our customers\n\nThis includes AI in BI and related design, testing, end to end roll out and ongoing continuous improvement\n\nEnable business to self-serve and meet BI demands through provision and ongoing management of appropriate tools, coupled with leading relevant training for the business\n\nProactively partner with key areas within Global Technology, Transformation & Change Delivery, Group and other teams for Projects and for Business-as-Usual deliverables\n\nInstill a customer-first culture, prioritizing service for our business stakeholders above all else\n\nWhat you will BRING We re looking for someone who has these abilities and skills: Required Skills and Abilities: Very high technical proficiency in use, development and solution design in Microsoft Power BI (inc\n\nPower Apps, Power Automate etc), with experience of usage in large, global, and complicated organization\n\nBe an expert on data modelling best practices, data engineering and data management (inc\n\nSQL)\n\nAble to articulate these to both technical and non-technical users\n\nAbility to communicate effectively directly with users, peers, senior management and teams across the globe, manage stakeholders effectively and navigate a matrixed virtual global organization\n\nBrings in a collaborative spirit, can-do attitude and a Customer First mindset to everything they put their mind to\n\nPassion for digital, data and AI and experience working within a digital and data driven organization\n\nA minimum of a bachelor s or masters degree in a relevant discipline\n\nApplies in-depth knowledge of business and specialized areas to solve problems and understand integration challenges and long-term impact creatively and strategically\n\nIs a self-starter who can operate independently and lead others in the team in strategic thinking and solution design\n\nDesired Skills and Abilities: Use and development skills in AI in BI; Power BI Copilot, connections with LLMs/models\n\nDatabricks and Databricks Genie ideal\n\nOther programing language proficiency, e g R, Python PowerShell etc Ability to operate and thrive in an agile team working environment (inc\n\nuse of Jira and other workflow tools)\n\nInsurance experience with both financial and non-financial metric understanding\n\nWho WE are AXA XL, the P&C and specialty risk division of AXA, is known for solving complex risks\n\nFor mid-sized companies, multinationals and even some inspirational individuals we don t just provide re/insurance, we reinvent it\n\nHow? By combining a comprehensive and efficient capital platform, data-driven insights, leading technology, and the best talent in an agile and inclusive workspace, empowered to deliver top client service across all our lines of business property, casualty, professional, financial lines and specialty\n\nWith an innovative and flexible approach to risk solutions, we partner with those who move the world forward\n\nLearn more at axaxl\n\ncom What we OFFER Inclusion AXA XL is committed to equal employment opportunity and will consider applicants regardless of gender, sexual orientation, age, ethnicity and origins, marital status, religion, disability, or any other protected characteristic\n\nAt AXA XL, we know that an inclusive culture and a diverse workforce enable business growth and are critical to our success\n\nThat s why we have made a strategic commitment to attract, develop, advance and retain the most diverse workforce possible, and create an inclusive culture where everyone can bring their full selves to work and can reach their highest potential\n\nIt s about helping one another and our business to move forward and succeed\n\nFive Business Resource Groups focused on gender, LGBTQ+, ethnicity and origins, disability and inclusion with 20 Chapters around the globe Robust support for Flexible Working Arrangements Enhanced family friendly leave benefits Named to the Diversity Best Practices Index Signatory to the UK Women in Finance Charter Learn more at axaxl\n\ncom / about-us / inclusion-and-diversity\n\nAXA XL is an Equal Opportunity Employer\n\nTotal Rewards AXA XL s Reward program is designed to take care of what matters most to you, covering the full picture of your health, wellbeing, lifestyle and financial security\n\nIt provides dynamic compensation and personalized, inclusive benefits that evolve as you do\n\nWe re committed to rewarding your contribution for the long term, so you can be your best self today and look forward to the future with confidence\n\nSustainability At AXA XL, Sustainability is integral to our business strategy\n\nIn an ever-changing world, AXA XL protects what matters most for our clients and communities\n\nWe know that sustainability is at the root of a more resilient future\n\nOur 2023-26 Sustainability strategy, called Roots of resilience, focuses on protecting natural ecosystems, addressing climate change, and embedding sustainable practices across our operations\n\nOur Pillars: Valuing nature: How we impact nature affects how nature impacts us\n\nResilient ecosystems - the foundation of a sustainable planet and society - are essential to our future\n\nWe re committed to protecting and restoring nature - from mangrove forests to the bees in our backyard - by increasing biodiversity awareness and inspiring clients and colleagues to put nature at the heart of their plans\n\nAddressing climate change: The effects of a changing climate are far reaching and significant\n\nUnpredictable weather, increasing temperatures, and rising sea levels cause both social inequalities and environmental disruption\n\nWere building a net zero strategy, developing insurance products and services, and mobilizing to advance thought leadership and investment in societal-led solutions\n\nIntegrating ESG: All companies have a role to play in building a more resilient future\n\nIncorporating ESG considerations into our internal processes and practices builds resilience from the roots of our business\n\nWe re training our colleagues, engaging our external partners, and evolving our sustainability governance and reporting\n\nAXA Hearts in Action: We have established volunteering and charitable giving programs to help colleagues support causes that matter most to them, known as AXA XL s Hearts in Action programs\n\nThese include our Matching Gifts program, Volunteering Leave, and our annual volunteering day - the Global Day of Giving\n\nFor more information, please see Sustainability at AXA XL\n\nThe U\n\nS\n\npay range for this position is USD 106,500 - 186,500\n\nActual pay will be determined based upon the individual s skills, experience, and location\n\nWe strive for market alignment and internal equity with our colleagues pay\n\nAt AXA XL, we know how important physical, mental, and financial health are to our employees, which is why we are proud to offer benefits such as a dynamic retirement savings plan, health and wellness programs, and many other benefits\n\nWe also believe in fostering our colleagues development and offer a wide range of learning opportunities for colleagues to hone their professional skills and to position themselves for the next step of their careers\n\nFor more details about AXA XL s benefits offerings, please visit US Benefits at a Glance 2025",Industry Type: Insurance,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data management', 'Powershell', 'Agile', 'Workflow', 'Business strategy', 'microsoft', 'JIRA', 'Continuous improvement', 'SQL', 'Python']",2025-06-12 13:58:40
Senior Power BI Developer,Luxoft,6 - 11 years,Not Disclosed,['Gurugram'],"Power BI Dashboard Development (UI Dashboards)\nDesign, develop, and maintain visually compelling, interactive Power BI dashboards aligned with business needs.\nCollaborate with business stakeholders to gather requirements, develop mockups, and refine dashboard UX.\nImplement advanced Power BI features like bookmarks, drill-throughs, dynamic tooltips, and DAX calculations.\nConduct regular UX/UI audits and performance tuning on reports.\nData Modeling in SQL Server & Dataverse\nBuild and manage scalable, efficient data models in Power BI, Dataverse, and SQL Server.\nApply best practices in dimensional modeling (star/snowflake schema) to support analytical use cases.\nEnsure data consistency, accuracy, and alignment across multiple sources and business areas.\nPerform optimization of models and queries for performance and load times.\nPower BI Dataflows & ETL Pipelines\nDevelop and maintain reusable Power BI Dataflows for centralized data transformations.\nCreate ETL processes using Power Query, integrating data from diverse sources including SQL Server, Excel, APIs, and Dataverse.\nAutomate data refresh schedules and monitor dependencies across datasets and reports.\nEnsure efficient data pipeline architecture for reuse, scalability, and maintenance.\nSkills\nMust have\nExperience: 6+ years in Business Intelligence or Data Analytics with a strong focus on Power BI and SQL Server.\nTechnical Skills:\nExpert-level Power BI development, including DAX, custom visuals, and report optimization.\nStrong knowledge of SQL (T-SQL) and relational database design.\nExperience with Dataverse and Power Platform integration.\nProficiency in Power Query, Dataflows, and ETL development.\nModeling: Proven experience in dimensional modeling, star/snowflake schema, and performance tuning.\nData Integration: Skilled in connecting and transforming data from various sources, including APIs, Excel, and cloud data services.\nCollaboration: Ability to work with stakeholders to define KPIs, business logic, and dashboard UX.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Performance tuning', 'data services', 'Excel', 'Data modeling', 'Database design', 'Analytical', 'Schema', 'power bi', 'Business intelligence', 'microsoft']",2025-06-12 13:58:42
Senior Research Analyst,Demandbase,8 - 10 years,Not Disclosed,['Hyderabad'],"Introduction to Demandbase:\nDemandbase is the Smarter GTM company for B2B brands. We help marketing and sales teams overcome the disruptive data and technology fragmentation that inhibits insight and forces them to spam their prospects. We do this by injecting Account Intelligence into every step of the buyer journey, wherever our clients interact with customers, and by helping them orchestrate every action across systems and channels - through advertising, account-based experience, and sales motions. The result? You spot opportunities earlier, engage with them more intelligently, and close deals faster.\nAs a company, we re as committed to growing careers as we are to building world-class technology. We invest heavily in people, our culture, and the community around us. We have offices in the San Francisco Bay Area, New York, Seattle, and teams in the UK and India . We have also been continuously recognized as one of the best places to work in the San Francisco Bay Area.\nWere committed to attracting, developing, retaining, and promoting a diverse workforce. By ensuring that every Demandbase employee is able to bring a diversity of talents to work, were increasingly capable of living out our mission to transform how B2B goes to market. We encourage people from historically underrepresented backgrounds and all walks of life to apply. Come grow with us at Demandbase!\nAbout the Role:\nWe are seeking a highly motivated and detail-oriented Senior Research Analyst to join our dynamic team. This role is crucial for driving informed business decisions through data gathering, analysis, and insightful reporting. The ideal candidate will possess a strong understanding of business research methodologies, data analysis techniques, and a passion for data accuracy and problem-solving.\nKey Responsibilities:\nLead Comprehensive Data Research and Analysis: Source, collect, research, and analyze data from a variety of business information sources and specialized databases to generate actionable insights.\nDrive Data-Driven Decision-Making: Conduct in-depth strategic analysis to identify trends, anomalies, and root causes, translating complex findings into clear, impactful recommendations for product and business growth.\nEnsure Data Quality and Integrity: Apply strong problem-solving skills to resolve data queries, perform rigorous quality checks, and proactively identify and address data coverage gaps.\nProvide Training and Knowledge Transfer: Mentor and train new team members on industry best practices and advanced data analysis techniques.\nLeverage Domain and Product Expertise: Work closely with data engineers, product teams, and business stakeholders to define and deliver technical roadmaps, ensuring sound solutions and maximizing customer value.\nRequired Skills & Experience:\nBachelor s or Master s degree in Business or Commerce\n8-10 years of relevant work experience\nExpertise in sourcing and extracting data from diverse business information sources\nAdvanced proficiency in Microsoft Excel (e.g., pivot tables, VLOOKUP, complex formulas, data validation, charting) for data manipulation, analysis, and reporting\nSkill in translating complex data into visually compelling narratives for various audiences\nAbility to design and create clear, insightful, and actionable dashboards and reports\nExcellent communication and interpersonal skills\nSelf-organized and self-driven, with strong personal integrity\nStrong understanding and application of data quality principles and best practices\nAbility to perform root cause analysis on large datasets and identify underlying business drivers\nProven ability to train and mentor new team members, sharing best practices and advanced techniques and strong knowledge transfer skills.\nA strong passion for data, continuous learning, and staying updated with industry best practices and emerging analytical techniques.\nStrong organizational and time management skills\nAbility to work independently, manage multiple priorities, and meet deadlines in a fast-paced environment.\nOur Commitment to Diversity, Equity, and Inclusion at Demandbase\nAt Demandbase, we believe in creating a workplace culture that values and celebrates diversity in all its forms. We recognize that everyone brings unique experiences, perspectives, and identities to the table, and we are committed to building a community where everyone feels valued, respected, and supported. Discrimination of any kind is not tolerated, and we strive to ensure that every individual has an equal opportunity to succeed and grow, regardless of their gender identity, sexual orientation, disability, race, ethnicity, background, marital status, genetic information, education level, veteran status, national origin, or any other protected status. We do not automatically disqualify applicants with criminal records and will consider each applicant on a case-by-case basis.\nWe recognize that not all candidates will have every skill or qualification listed in this job description. If you feel you have the level of experience to be successful in the role, we encourage you to apply!\nWe acknowledge that true diversity and inclusion require ongoing effort, and we are committed to doing the work required to make our workplace a safe and equitable space for all. Join us in building a community where we can learn from each other, celebrate our differences, and work together.\nPersonal information that you submit will be used by Demandbase for recruiting and other business purposes. Our Privacy Policy explains how we collect and use personal information.\nPersonal information that you submit will be used by Demandbase for recruiting and other business purposes. Our Privacy Policy explains how we collect and use personal information.",Industry Type: Advertising & Marketing,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['SAN', 'Data analysis', 'Data validation', 'Excel', 'Data research', 'Business research', 'VLOOKUP', 'Analytical', 'Data quality', 'Senior Research Analyst']",2025-06-12 13:58:45
AI Delivery Lead,Tata Technologies,10 - 15 years,Not Disclosed,['Pune'],"10+ years of experience in machine learning, AI, or data science, with at least 3 years in a leadership role.Proven track record of delivering ML/AI projects at scale in an enterprise environment.Deep functional expertise in AI/ML, coupled with a solid understan ding of data science solution developmentExperience in managing teams and stakeholder expectations.Strong communication skills and demonstrated experience in stakeholder managementTechnical SkillsStrong expertise in ML frameworks and tools (e.g., TensorFlow, P yTorch, Scikit-learn).Good experience in Gen AI (various LLMs and its framework)Proficiency in programming languages like Python, R, or Java.Familiarity with cloud platforms (AWS, Azure, GCP) for ML workflows.Solid understanding of MLOps principles, including CI/CD pipelines, model deployment, and monitoring.Knowledge of big data technologies (e.g., Spark, Hadoop) and data engineering bes t practices.Preferred Certifications AWS/Azure/GCP Certified ML",Industry Type: Building Material (Cement),Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['CI/CD pipelines', 'Java', 'Azure', 'R', 'GCP', 'Hadoop', 'Spark', 'AWS', 'Python']",2025-06-12 13:58:47
Senior Power BI Developer,Luxoft,6 - 11 years,Not Disclosed,['Chennai'],"Power BI Dashboard Development (UI Dashboards)\nDesign, develop, and maintain visually compelling, interactive Power BI dashboards aligned with business needs.\nCollaborate with business stakeholders to gather requirements, develop mockups, and refine dashboard UX.\nImplement advanced Power BI features like bookmarks, drill-throughs, dynamic tooltips, and DAX calculations.\nConduct regular UX/UI audits and performance tuning on reports.\nData Modeling in SQL Server & Dataverse\nBuild and manage scalable, efficient data models in Power BI, Dataverse, and SQL Server.\nApply best practices in dimensional modeling (star/snowflake schema) to support analytical use cases.\nEnsure data consistency, accuracy, and alignment across multiple sources and business areas.\nPerform optimization of models and queries for performance and load times.\nPower BI Dataflows & ETL Pipelines\nDevelop and maintain reusable Power BI Dataflows for centralized data transformations.\nCreate ETL processes using Power Query, integrating data from diverse sources including SQL Server, Excel, APIs, and Dataverse.\nAutomate data refresh schedules and monitor dependencies across datasets and reports.\nEnsure efficient data pipeline architecture for reuse, scalability, and maintenance.\nSkills\nMust have\nExperience: 6+ years in Business Intelligence or Data Analytics with a strong focus on Power BI and SQL Server.\nTechnical Skills:\nExpert-level Power BI development, including DAX, custom visuals, and report optimization.\nStrong knowledge of SQL (T-SQL) and relational database design.\nExperience with Dataverse and Power Platform integration.\nProficiency in Power Query, Dataflows, and ETL development.\nModeling: Proven experience in dimensional modeling, star/snowflake schema, and performance tuning.\nData Integration: Skilled in connecting and transforming data from various sources, including APIs, Excel, and cloud data services.\nCollaboration: Ability to work with stakeholders to define KPIs, business logic, and dashboard UX.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Performance tuning', 'data services', 'Data modeling', 'Database design', 'Analytical', 'Schema', 'power bi', 'SSIS', 'Business intelligence', 'microsoft']",2025-06-12 13:58:49
Product manager (Solar),Renew,4 - 7 years,Not Disclosed,"['Kolkata', 'Mumbai', 'New Delhi', 'Hyderabad', 'Pune', 'Chennai', 'Bengaluru']","Responsibilities\nUnderstand Business problems and identify constraints\nDesign digital and advance analytics solutions to Business problems\nImplement the solution with an understanding of end-to-end architecture\nIdentify opportunities for implementation of new use cases\nKeep updates of any policy changes in power markets\nEnsure ReD targets are met and delivered on time\nEnsure documentation of Use Cases\nOur Ideal Candidate\nEducation - Engineering (Electrical/Electronics/IT) + MBA\nExperience Range - 4 to 7 years\nExperience in Renewable energy/ Storage/Hydro/RTC power/Power Trading\nGood program management, Project planning & coordination skills\nGood Experience of working in cross functional teams\nGood IT skills - understanding different solutions and matching solutions to problems\nAnalytical approach to solving problems with focus on solution delivery\nCapable of extrapolating current situation to future scenarios\nFunctional/ Domain expertise\nKnowledge of Power markets is a must\nShould have experience in evaluating or implementing any of the new technologies (BESS/ Hybrids/ EV, Charging Infra/ Pumped Storage/ Hydrogen/Market procurement of RE - GTAM)\nParticipated in some Digital transformation/enablement exercise in organisation\nBasic understanding of work of Data Scientists & Data engineering roles is a plus\nExperience in agile working methodology would be a plus\nExperience with tools like PowerBI, Tableau, JIRA would be a plus\nCommunication skills\nAbility to communicate with cross functional roles is a must\nExcellent written and verbal communication skills\nGood presentation skills\nTeamwork\nAbility to work as a self-motivated team player\nHas worked in large teams with an agile setup in the past\nHandle multiple projects across intra and inter-department teams",Industry Type: Power,Department: Product Management,"Employment Type: Full Time, Permanent","['Procurement', 'RTC', 'Renewable energy', 'Analytical', 'Agile', 'Project planning', 'JIRA', 'digital transformation', 'Analytics', 'Hydro']",2025-06-12 13:58:52
Technical lead - Salesforce,XL India Business Services Pvt. Ltd,4 - 9 years,Not Disclosed,"['Hyderabad', 'Ahmedabad', 'Bengaluru']","Technical Architect-AVP Bangalore, Karnataka, India We are looking for an experienced Technical Architect to lead the design and implementation of complex software solutions\n\nThe ideal candidate will possess a deep understanding of software architecture principles, technology stacks, and development methodologies\n\nYou will collaborate with cross-functional teams to ensure that our technology solutions are scalable, secure, and aligned with business objectives\n\nWhat you ll be DOING What will your essential responsibilities include? Responsible for the design and technical delivery of the Salesforce Underwriter Journey\n\nCollaborate with business analysts and stakeholders to gather requirements and translate them into technical specifications\n\nProducing quality, secure, scalable, high-performing, and resilient designs for new or improved services\n\nLead the systems analysts, developers, and testers in sympathetic change to the applications\n\nResponsible for partnering with engineers, DevOps engineers, Administrators, and other roles responsible for implementing solutions on Azure to ensure sound infrastructure solution options are leveraged Accountable for leading technical project delivery within the applications landscape\n\nResponsible for handling multiple tech initiatives in Agile delivery models\n\nActively lead the development teams to help assist PI planning and prioritization\n\nFor internal assets, support Product Owners to develop and maintain the Product Roadmap\n\nDefine and maintain development standards such as system and data design, coding, etc Maintain a capacity plan with historical performance metrics, a future forecast, and a capacity model to ensure services and infrastructure deliver performance and growth targets in a cost effective and proactive manner\n\nManage architecture exceptions for the application, including identifying, documenting, taking through exception approval process, and remediation where and when possible\n\nMonitor application services to ensure performance consistently meets non-functional requirements (response time, security, etc)\n\nLeads the DevOps team and developers in targeted use of DevOps for their application platform assets\n\nYou will report to the Release Train Engineer\n\nWhat you will BRING We re looking for someone who has these abilities and skills: Required Skills and Abilities: Developing and maintaining custom data integration solutions with various sources and formats, including structured and unstructured data, to ensure data quality and consistency\n\nEnsures that the solution and codebase is maintainable, scalable and adheres to best practices of software development\n\nDelivering high-quality, scalable, and reliable data ingestion pipelines\n\nSupports other members of the squad in resolving technical questions related to best practice, feasibility etc Desired Skills and Abilities: Insurance background\n\nProficiency in programming language such as C#\n\nExperience with data integration tools\n\nExcellent analytical skills to evaluate complex problems and devise efficient solutions\n\nExperience with cloud platforms such as Azure, including continuous integration and continuous deployment (CI/CD)\n\nFamiliarity with big data technologies such as Hadoop, Spark, and Kafka\n\nKnowledge of data privacy and security regulations such as GDPR\n\nEffective communication and collaboration skills\n\nWho WE are AXA XL, the P&C and specialty risk division of AXA, is known for solving complex risks\n\nFor mid-sized companies, multinationals and even some inspirational individuals we don t just provide re/insurance, we reinvent it\n\nHow? By combining a comprehensive and efficient capital platform, data-driven insights, leading technology, and the best talent in an agile and inclusive workspace, empowered to deliver top client service across all our lines of business property, casualty, professional, financial lines and specialty\n\nWith an innovative and flexible approach to risk solutions, we partner with those who move the world forward\n\nLearn more at axaxl\n\ncom What we OFFER Inclusion AXA XL is committed to equal employment opportunity and will consider applicants regardless of gender, sexual orientation, age, ethnicity and origins, marital status, religion, disability, or any other protected characteristic\n\nAt AXA XL, we know that an inclusive culture and a diverse workforce enable business growth and are critical to our success\n\nThat s why we have made a strategic commitment to attract, develop, advance and retain the most diverse workforce possible, and create an inclusive culture where everyone can bring their full selves to work and can reach their highest potential\n\nIt s about helping one another and our business to move forward and succeed\n\nFive Business Resource Groups focused on gender, LGBTQ+, ethnicity and origins, disability and inclusion with 20 Chapters around the globe Robust support for Flexible Working Arrangements Enhanced family friendly leave benefits Named to the Diversity Best Practices Index Signatory to the UK Women in Finance Charter Learn more at axaxl\n\ncom / about-us / inclusion-and-diversity\n\nAXA XL is an Equal Opportunity Employer\n\nTotal Rewards AXA XL s Reward program is designed to take care of what matters most to you, covering the full picture of your health, wellbeing, lifestyle and financial security\n\nIt provides competitive compensation and personalized, inclusive benefits that evolve as you do\n\nWe re committed to rewarding your contribution for the long term, so you can be your best self today and look forward to the future with confidence\n\nSustainability At AXA XL, Sustainability is integral to our business strategy\n\nIn an ever-changing world, AXA XL protects what matters most for our clients and communities\n\nWe know that sustainability is at the root of a more resilient future\n\nOur 2023-26 Sustainability strategy, called Roots of resilience , focuses on protecting natural ecosystems, addressing climate change, and embedding sustainable practices across our operations\n\nOur Pillars: Valuing nature: How we impact nature affects how nature impacts us\n\nResilient ecosystems - the foundation of a sustainable planet and society - are essential to our future\n\nWe re committed to protecting and restoring nature - from mangrove forests to the bees in our backyard - by increasing biodiversity awareness and inspiring clients and colleagues to put nature at the heart of their plans\n\nAddressing climate change: The effects of a changing climate are far reaching and significant\n\nUnpredictable weather, increasing temperatures, and rising sea levels cause both social inequalities and environmental disruption\n\nWere building a net zero strategy, developing insurance products and services, and mobilizing to advance thought leadership and investment in societal-led solutions\n\nIntegrating ESG: All companies have a role to play in building a more resilient future\n\nIncorporating ESG considerations into our internal processes and practices builds resilience from the roots of our business\n\nWe re training our colleagues, engaging our external partners, and evolving our sustainability governance and reporting\n\nAXA Hearts in Action: We have established volunteering and charitable giving programs to help colleagues support causes that matter most to them, known as AXA XL s Hearts in Action programs\n\nThese include our Matching Gifts program, Volunteering Leave, and our annual volunteering day - the Global Day of Giving\n\nFor more information, please see axaxl\n\ncom/sustainability",Industry Type: Insurance,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['remediation', 'continuous integration', 'software architecture', 'Coding', 'Agile', 'Data quality', 'data privacy', 'Business strategy', 'Project delivery', 'Salesforce']",2025-06-12 13:58:54
Senior Manager Information Systems,Amgen Inc,8 - 13 years,Not Disclosed,['Hyderabad'],"What you will do\nIn this vital role you will develop an insight driven sensing capability with a focus on revolutionizing decision making. In this role you will lead the technical delivery for this capability as part of a team data engineers and software engineers. The team will rely on your leadership to own and refine the vision, feature prioritization, partner alignment, and experience leading solution delivery while building this ground-breaking new capability for Amgen. You will drive the software engineering side of the product release and will deliver for the outcomes.\nRoles & Responsibilities:\nLead delivery of overall product and product features from concept to end of life management of the product team comprising of technical engineers, product owners and data scientists to ensure that business, quality, and functional goals are met with each product release\nDrives excellence and quality for the respective product releases, collaborating with Partner teams.\nImpacts quality, efficiency and effectiveness of own team. Has significant input into priorities.\nIncorporate and prioritize feature requests into product roadmap; Able to translate roadmap into execution\nDesign and implement usability, quality, and delivery of a product or feature\nPlan releases and upgrades with no impacts to business\nHands on expertise in driving quality and best in class Agile engineering practices\nEncourage and motivate the product team to deliver innovative and exciting solutions with an appropriate sense of urgency\nManages progress of work and addresses production issues during sprints\nCommunication with partners to make sure goals are clear and the vision is aligned with business objectives\nDirect management and staff development of team members\nWhat we expect of you\nWe are all different, yet we all use our unique contributions to serve patients.\nBasic Qualifications:\nMasters degree and 8 to 10 years of Information Systems experience OR\nBachelors degree and 10 to 14 years ofInformation Systems experience OR\nDiploma and 14 to 18 years of Information Systems experience\nThorough understanding of modern web application development and delivery, Gen AI applications development, Data integration and enterprise data fabric concepts, methodologies, and technologies e.g. AWS technologies, Databricks\nDemonstrated experience in building strong teams with consistent practices.\nDemonstrated experience in navigating matrix organization and leading change.\nPrior experience writing business case documents and securing funding for product team delivery; Financial/Spend management for small to medium product teams is a plus.\nIn-depth knowledge of Agile process and principles.\nDefine success metrics for developer productivity metrics; on a monthly/quarterly basis analyze how the product team is performing against established KPIs.\nFunctional Skills:\nLeadership:\nInfluences through Collaboration: Builds direct and behind-the-scenes support for ideas by working collaboratively with others.\nStrategic Thinking: Anticipates downstream consequences and tailors influencing strategies to achieve positive outcomes.\nTransparent Decision-Making: Clearly articulates the rationale behind decisions and their potential implications, continuously reflecting on successes and failures to enhance performance and decision-making.\nAdaptive Leadership: Recognizes the need for change and actively participates in technical strategy planning.\nPreferred Qualifications:\nStrong influencing skills, influence stakeholders and be able to balance priorities.\nPrior experience in vendor management.\nPrior hands-on experience leading full stack development using infrastructure cloud services (AWS preferred) and cloud-native tools and design patterns (Containers, Serverless, Docker, etc.)\nExperience with developing solutions on AWS technologies such as S3, EMR, Spark,\nAthena, Redshift and others\nFamiliarity with cloud security (AWS /Azure/ GCP)\nConceptual understanding of DevOps tools (Ansible/ Chef / Puppet / Docker /Jenkins)\nProfessional Certifications\nAWS Certified Solutions Architect (preferred)\nCertified DevOps Engineer (preferred)\nCertified Agile Leader or similar (preferred)\nSoft Skills:\nStrong desire for continuous learning to pick new tools/technologies.\nHigh attention to detail is essential with critical thinking ability.\nShould be an active contributor on technological communities/forums\nProactively engages with cross-functional teams to resolve issues and design solutions using critical thinking and analysis skills and best practices.\nInfluences and energizes others toward the common vision and goal. Maintains excitement for a process and drives to new directions of meeting the goal even when odds and setbacks render one path impassable\nEstablished habit of proactive thinking and behavior and the desire and ability to self-start/learn and apply new technologies\nExcellent organizational and time-management skills\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nTeam-oriented, with a focus on achieving team goals\nStrong presentation and public speaking skills.\nShift Information:\nThis position requires you to work a later shift and may be assigned a second or third shift schedule. Candidates must be willing and able to work during evening or night shifts, as required based on business requirements.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Information Systems', 'Azure', 'DevOps', 'Gen AI application development', 'GCP', 'Data integration', 'AWS', 'web application development']",2025-06-12 13:58:56
Test Lead,Infosys,5 - 10 years,10-20 Lacs P.A.,[],"Role & responsibilities\n\nThe Test Lead oversees the testing strategy and execution for the Microsoft Fabric migration and Power BI reporting solutions. This offshore role ensures quality, reliability, and client satisfaction through rigorous validation.\nThe successful candidate will have a strong testing background and coordination skills.\nResponsibilities\nDevelop and execute the testing strategy for Microsoft Fabric and Power BI deliverables.\nValidate data migration, pipeline functionality, and report accuracy against requirements.\nCoordinate with the Offshore Project Manager to align testing with development milestones.\nCollaborate with onsite technical leads to validate results and resolve defects. • Oversee offshore testers, ensuring comprehensive coverage and quality standards.\nProactively identify risks and articulate solutions to minimize delivery issues.\nSkills\nBachelors degree in IT, computer science, or a related field.\n5+ years of experience in test leadership for data platforms and BI solutions.\nKnowledge of Microsoft Fabric, Power BI, and data migration testing.\nProficiency with testing tools (e.g., Azure DevOps, Selenium) and SQL.\nStrong communication and stakeholder management skills.\nDetail-oriented with a focus on quality and continuous improvement\n1. JD for Data Modeler\nThe Data Modeler designs and implements data models for Microsoft Fabric and Power BI, supporting the migration from Oracle/Informatica. This offshore role ensures optimized data structures for performance and reporting needs. The successful candidate will bring expertise in data modeling and a collaborative approach.\nResponsibilities\nDevelop conceptual, logical, and physical data models for Microsoft Fabric and Power BI solutions.\nImplement data models for relational, dimensional, and data lake environments on target platforms.\nCollaborate with the Offshore Data Engineer and Onsite Data Modernization Architect to ensure model alignment.\nDefine and govern data modeling standards, tools, and best practices.\nOptimize data structures for query performance and scalability.\nProvide updates on modeling progress and dependencies to the Offshore Project Manager.\nSkills\nBachelor’s or master’s degree in computer science, data science, or a related field.\n5+ years of data modeling experience with relational and NoSQL platforms.\nProficiency with modeling tools (e.g., Erwin, ER/Studio) and SQL.\nExperience with Microsoft Fabric, data lakes, and BI data structures.\nStrong analytical and communication skills for team collaboration.\nAttention to detail with a focus on performance and consistency.\nmanagement, communication, and presentation",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Test lead', 'Migration', 'power bi', 'microsoft fabric']",2025-06-12 13:58:59
Senior Project Manager,PM,5 - 7 years,10-15 Lacs P.A.,['Hyderabad'],"Role description\n\nWere looking for a driven, organized team member to support the Digital & Analytics team with support of Talent transformation projects from both a systems and process perspective. The role will primarily provide PMO support. The individual will need to demonstrate strong project management skills, be very collaborative and detail oriented to coordinate meetings, track and update project plans, risks, and decision logs. This individual would also need to create project materials, support design sessions, user acceptance testing, and escalate project or system issues as needed.\n\nWork youll do As the Talent PMO Support you will:\n\nSupport the Digital & Analytics Manager with support of Talent transformation projects.\nTrack and drive closure of action items and open decisions.\nSchedule follow up calls, take notes, and distribute action items from discussions.\nCoordinate with Talent process owners and subject matter advisors to manage change requests and risks, actions, and decisions.\nCoordinate across Talent, Technology, and Consulting teams to track and escalate issues as appropriate.\nUpdate the Talent project plan items, resource tracker and the risks, actions and decisions log as needed.\nLeverage shared project team site and OneNote notebook to ensure structure and access to communications, materials, and documents for all project team members.\nSupport testing, cut-over, training, and service rehearsal testing processes as needed.\nCollaborate with the Consulting, Technology, and Talent team members to ensure project deliverables move forward.\nQualifications:\n\nBachelors degree and 5-7 years of relevant work experience\nBackground and experience with project management support to implement Talent processes, from ideation through deployment phases.\nStrong written/verbal executive communication and presentation skills; strong listening, facilitation and influencing skills with audiences of all management and leadership levels.\nWorks well in a dynamic, complex, client, and team focused environment with minimal oversight and an agile mindset\nExcited by prospect of working in a developing, ambiguous, and challenging situation.\nProficient Microsoft Office skills (e.g., PowerPoint, Excel, OneNote, Word, Teams)",Industry Type: IT Services & Consulting,Department: Project & Program Management,"Employment Type: Full Time, Temporary/Contractual","['Digital Project Management', 'PMO', 'Project Management', 'data engineer', 'Scrum', 'Microsoft']",2025-06-12 13:59:01
Senior Business Analyst,Skillsoft Software Services,2 - 7 years,Not Disclosed,['Hyderabad'],"India-based candidates only. We’re primarily a NYC-based team, but have a growing international team. \n \nROLE OVERVIEW:  \nAs a Senior Data Analyst, you will be pivotal in driving strategic decision-making for the Codecademy consumer and enterprise business lines. Reporting to the senior manager, Strategy and business Operations, this role will work cross-functionally to tackle the business’ highest priorities. You will utilize your technical expertise in data analytics, financial modeling, and executive communication to create actionable business strategies that drive growth.",,,,"['snowflake', 'python', 'data analytics', 'data analysis', 'modeling', 'analytical', 'verbal communication', 'business analysis', 'sql', 'analytics', 'marketing analytics', 'data integration tools', 'looker', 'writing', 'financial modelling', 'data visualization', 'business operations', 'reporting', 'communication skills']",2025-06-12 13:59:04
ETL QA Lead,Wissen Technology,8 - 13 years,Not Disclosed,"['Hyderabad', 'Pune', 'Bengaluru']","Job Description for QA Engineer:\n7+ years of experience in ETL Testing, Snowflake, DWH Concepts.\nStrong SQL knowledge & debugging skills are a must.\nExperience on Azure and Snowflake Testing is plus\nExperience with Qlik Replicate and Compose tools (Change Data Capture) tools is considered a plus\nStrong Data warehousing Concepts, ETL tools like Talend Cloud Data Integration, Pentaho/Kettle tool\nExperience in JIRA, Xray defect management toolis good to have.\nExposure to the financial domain knowledge is considered a plus\nTesting the data-readiness (data quality) address code or data issues\nDemonstrated ability to rationalize problems and use judgment and innovation to define clear and concise solutions\nDemonstrate strong collaborative experience across regions (APAC, EMEA and NA) to effectively and efficiently identify root cause of code/data issues and come up with a permanent solution\nPrior experience with State Street and Charles River Development (CRD) considered a plus\nExperience in tools such as PowerPoint, Excel, SQL\nExposure to Third party data providers such as Bloomberg, Reuters, MSCI and other Rating agencies is a plus",Industry Type: IT Services & Consulting,"Department: BFSI, Investments & Trading","Employment Type: Full Time, Permanent","['Data Warehouse Testing', 'ETL Testing', 'SQL', 'Snowflake']",2025-06-12 13:59:06
Senior ServiceNow Developer,Luxoft,6 - 11 years,Not Disclosed,['Gurugram'],"Internal Data Structures & Configuration\nDesign, build, and maintain data models, tables, and relationships within the ServiceNow platform.\nExtend and customize out-of-the-box modules (e.g., CMDB, Incident, Change, Request, etc.) to meet business requirements.\nEnsure data integrity, normalization, and performance optimization across the ServiceNow environment.\nCollaborate with stakeholders to translate business requirements into scalable ServiceNow configurations or custom applications.\nReporting & Dashboards\nDevelop real-time dashboards and reports using ServiceNow Reporting Tools and Performance Analytics.\nDeliver insights into key ITSM metrics such as SLAs, incident trends, and operational KPIs.\nAutomate the generation and distribution of recurring reports to stakeholders.\nWork with business and technical teams to define and implement reporting frameworks tailored to their needs.\nAutomated Feeds & API Integration\nDevelop and manage robust data integrations using ServiceNow REST/SOAP APIs.\nBuild and maintain data pipelines to and from external systems (e.g., CMDB, HRIS, ERP, Flexera, etc.).\nImplement secure, scalable automation for data exchange with appropriate error handling, logging, and monitoring.\nTroubleshoot and resolve integration-related issues to ensure smooth system interoperability.\nSkills\nMust have\nMinimum 6+ years of hands-on experience with ServiceNow, including ITSM, CMDB, and integrations.\nTechnical Expertise:\nAdvanced knowledge of ServiceNow architecture, configuration, and scripting (JavaScript, Glide).\nStrong experience with REST/SOAP APIs for ServiceNow integrations.\nSolid understanding of relational databases, data normalization, and model optimization.\nFamiliarity with common enterprise systems such as ERP, HRIS, Flexera, and CMDB tools.\nReporting Skills:\nProficiency in ServiceNow Performance Analytics, standard reporting, and dashboard design.\nExperience defining KPIs and building automated reporting solutions.\nSoft Skills:\nStrong communication and collaboration skills.\nProven ability to translate business requirements into scalable ServiceNow solutions.\nAnalytical and detail-oriented mindset with a problem-solving approach.\n",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['ERP', 'Automation', 'SAP', 'HRIS', 'Analytical', 'Javascript', 'Data structures', 'Risk management', 'Analytics', 'Monitoring']",2025-06-12 13:59:08
Senior ServiceNow Developer,Luxoft,6 - 11 years,Not Disclosed,['Chennai'],"Internal Data Structures & Configuration\nDesign, build, and maintain data models, tables, and relationships within the ServiceNow platform.\nExtend and customize out-of-the-box modules (e.g., CMDB, Incident, Change, Request, etc.) to meet business requirements.\nEnsure data integrity, normalization, and performance optimization across the ServiceNow environment.\nCollaborate with stakeholders to translate business requirements into scalable ServiceNow configurations or custom applications.\nReporting & Dashboards\nDevelop real-time dashboards and reports using ServiceNow Reporting Tools and Performance Analytics.\nDeliver insights into key ITSM metrics such as SLAs, incident trends, and operational KPIs.\nAutomate the generation and distribution of recurring reports to stakeholders.\nWork with business and technical teams to define and implement reporting frameworks tailored to their needs.\nAutomated Feeds & API Integration\nDevelop and manage robust data integrations using ServiceNow REST/SOAP APIs.\nBuild and maintain data pipelines to and from external systems (e.g., CMDB, HRIS, ERP, Flexera, etc.).\nImplement secure, scalable automation for data exchange with appropriate error handling, logging, and monitoring.\nTroubleshoot and resolve integration-related issues to ensure smooth system interoperability.\nSkills\nMust have\nMinimum 6+ years of hands-on experience with ServiceNow, including ITSM, CMDB, and integrations.\nTechnical Expertise:\nAdvanced knowledge of ServiceNow architecture, configuration, and scripting (JavaScript, Glide).\nStrong experience with REST/SOAP APIs for ServiceNow integrations.\nSolid understanding of relational databases, data normalization, and model optimization.\nFamiliarity with common enterprise systems such as ERP, HRIS, Flexera, and CMDB tools.\nReporting Skills:\nProficiency in ServiceNow Performance Analytics, standard reporting, and dashboard design.\nExperience defining KPIs and building automated reporting solutions.\nSoft Skills:\nStrong communication and collaboration skills.\nProven ability to translate business requirements into scalable ServiceNow solutions.\nAnalytical and detail-oriented mindset with a problem-solving approach.\nNice to have\nN/A.\n",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['ERP', 'Automation', 'SAP', 'HRIS', 'Analytical', 'Javascript', 'Data structures', 'Risk management', 'Analytics', 'Monitoring']",2025-06-12 13:59:11
AWS Cloud Tech Lead,ITC Infotech,5 - 9 years,Not Disclosed,['Pune'],"You will participate in the design, development, and deployment of scalable and robust applications using AWS Cloudfront, S3 buckets, Node.js, TypeScript, React.js, Next.js, Stencil.js, and Aurora Postgres SQL.\nImplement microservices that integrate with Databricks as a Data Lake and consume data products.\nKey Responsibilities\nCollaborate closely with other technical leads to align on standards, interfaces, and dependencies",,,,"['System architecture', 'Backend', 'Front end', 'Coding', 'Postgresql', 'Agile', 'Scrum', 'JIRA', 'AWS', 'microservices']",2025-06-12 13:59:13
Backend Developer - Python,Derisk360,6 - 11 years,Not Disclosed,"['Chennai', 'Gurugram', 'Bengaluru']","Join our engineering team as a Senior Backend Engineer and lead the development of cloud-native, scalable microservices RESTful APIs using modern Python frameworks . Youll work with CI/CD tools to build robust backend systems powering next-gen platforms. If you have hands-on experience with , and are skilled in distributed systems relational/NoSQL databases , we want to hear from you. Key Responsibilities: Microservices Development Design, build, and optimize microservices architecture using patterns like Service Discovery Circuit Breaker API Gateway Saga orchestration REST API Engineering Develop high-performance frameworks like Django REST Framework Cloud-Native Backend Systems Build and deploy containerized applications . Familiarity with Kubernetes (K8s) for orchestration is a plus. CI/CD Automation Create and maintain DevOps pipelines GitLab CI/CD GitHub Actions for automated testing and deployment. Source Code Management Collaborate through Git-based version control , ensuring code quality via pull requests peer reviews on platforms like Event-Driven Architecture Implement and manage data streaming messaging pipelines Apache Kafka Amazon Kinesis , or equivalent. Database Engineering Work with , and optionally solutions such as Cloud Infrastructure Architect and manage AWS backend services Big Data Integration (Desirable) for distributed data processing and scalable ETL workflows in data engineering Polyglot Collaboration Integrate with backend services or data processors developed in , or other enterprise technologies. Required Skills & Qualifications: Bachelors or Masters in Computer Science Software Engineering , or a related technical field. 6+ years in backend development Proven expertise in API development cloud-native applications Proficiency in database schema design , and query optimization. Strong grasp of DevOps best practices Git workflows code quality standards Experience with streaming platforms message queues event-driven design Nice to Have: Exposure to big data tools (e.g., Familiarity with Agile/Scrum methodologies cross-functional teams Competitive salary and performance-based bonuses Opportunity to build next-gen backend platforms for global-scale applications. Work with a team that values",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Architect', 'Backend', 'Automation', 'MySQL', 'Cloud', 'Management', 'big data', 'Apache', 'SQL', 'Python']",2025-06-12 13:59:16
IICS/IDMC Developer,Qualitest,0 - 5 years,Not Disclosed,['Bengaluru'],"Design, develop, and maintain data integration workflows using Informatica IICS (Cloud Data Integration and Application Integration).\nDevelop and optimize ETL solutions using Informatica PowerCenter.\nWork on Snowflake to support data warehousing solutions, including data ingestion, transformation, and performance tuning.\nWrite efficient and optimized SQL and PL/SQL queries for data extraction, transformation, and validation.\nDevelop and support Unix Shell Scripts for automation and job scheduling.\nCollaborate with business and technical stakeholders to understand requirements and deliver scalable solutions.\nParticipate in design reviews, code reviews, and performance tuning exercises.\nContribute to cloud migration and modernization initiatives, particularly on Azure.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Unix', 'Performance tuning', 'Automation', 'Assurance', 'Managed services', 'Functional testing', 'PLSQL', 'Healthcare', 'Scheduling', 'Informatica']",2025-06-12 13:59:18
Sr. Databricks Developer,Newscape Consulting,7 - 9 years,Not Disclosed,['Pune( Baner )'],"We are looking for a highly skilled Senior Databricks Developer to join our data engineering team. You will be responsible for building scalable and efficient data pipelines using Databricks, Apache Spark, Delta Lake, and cloud-native services (Azure/AWS/GCP). You will work closely with data architects, data scientists, and business stakeholders to deliver high-performance, production-grade solutions.\nKey Responsibilities :\n- Design, build, and maintain scalable and efficient data pipelines on Databricks using PySpark, Spark SQL, and optionally Scala.\n- Work with Databricks components including Workspace, Jobs, DLT (Delta Live Tables), Repos, and Unity Catalog.\n- Implement and optimize Delta Lake solutions aligned with Lakehouse and Medallion architecture best practices.\n- Collaborate with data architects, engineers, and business teams to understand requirements and deliver production-grade solutions.\n- Integrate CI/CD pipelines using tools such as Azure DevOps, GitHub Actions, or similar for Databricks deployments.\n- Ensure data quality, consistency, governance, and security by using tools like Unity Catalog or Azure Purview.\n- Use orchestration tools such as Apache Airflow, Azure Data Factory, or Databricks Workflows to schedule and monitor pipelines.\n- Apply strong SQL skills and data warehousing concepts in data modeling and transformation logic.\n- Communicate effectively with technical and non-technical stakeholders to translate business requirements into technical solutions.\nRequired Skills and Qualifications :\n- Hands-on experience in data engineering, with specifically in Databricks.\n- Deep expertise in Databricks Workspace, Jobs, DLT, Repos, and Unity Catalog.\n- Strong programming skills in PySpark, Spark SQL; Scala experience is a plus.\n- Proficient in working with one or more cloud platforms : Azure, AWS, or GCP.\n- Experience with Delta Lake, Lakehouse architecture, and medallion architecture patterns.\n- Proficient in building CI/CD pipelines for Databricks using DevOps tools.\n- Familiarity with orchestration and ETL/ELT tools such as Airflow, ADF, or Databricks Workflows.\n- Strong understanding of data governance, metadata management, and lineage tracking.\n- Excellent analytical, communication, and stakeholder management skills.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Data Factory', 'Pyspark', 'Azure Databricks', 'ETL', 'Delta Lake', 'Azure Data Lake', 'Apache', 'Data Bricks']",2025-06-12 13:59:20
MS CRM Lead,Ducen,10 - 15 years,Not Disclosed,"['Kolkata', 'Mumbai', 'New Delhi', 'Hyderabad', 'Pune', 'Chennai', 'Bengaluru']","Job title: Microsoft Dynamics CRM Lead Developer\nProgram description:\nDynamics CRM Lead Developer to become part of our DEV team creating and supporting application in several different environments. Must have a strong understanding of data analytics relating to MS SQL Server. The ideal candidate will have a deep understanding of MS Dynamics, SQL databases, and business processes to support the agency goals and departments use of Microsoft Dynamics for customer service and business case management.\nThe ideal candidate will bring several qualities to the team, including:\nThe ability to translate user requirements into application, workflow, and database configurations, within the context of a configurable CRM application.\nStrong skills in Microsoft Excel, data management and data integration skills with an ability to organize, analyze and correlate data information.\nSolid interpersonal skills with the ability to communicate with technical and non-technical stakeholders.\nThe ability to organize, analyze and correlate data and information.\nThe ability to work and collaborate with others in a fast-paced environment with organization, attention to detail, responsiveness, and accountability.\nQualifications:\n10+ years of IT experience including 6+ years in Microsoft Dynamics CRM development and Lead role\nExperience and in-depth hands-on knowledge of the Microsoft Dynamics 365 CRM platform, the entity model, security model, and Web services\nStrong functional knowledge of Microsoft Dynamics CRM Customer Service, Sales, Marketing, and modules\nSkills on CRM processes in Insurance industry preferably P&C (Property and Casualty)\nExperience with ASP.NET, MVC 4.0, C#, Java script, jQuery, Bootstrap, ADO.NET\nExperience with DevOps, Agile-Scrum\nStrong proficiency in Microsoft Dynamics 365 platform and its various modules\nExperience/proficiency with Power Platform (Power Apps, Power Automate and Power BI)\nUnderstanding of web services, REST APIs, and integration patterns (pub/sub patterns preferred)\nFamiliarity with Azure cloud services and DevOps practices\nEducation Required\nBachelor s degree in computer science, or related field\nNice to Have\nProperty and Casualty Insurance industry experience\nBuilding and deploying Dynamics 365 packages in development, testing, and production environments.\nCandidate Privacy Policy\nOrion Systems Integrators, LLC and its subsidiaries and its affiliates (collectively, Orion, we or us ) are committed to protecting your privacy. This (orioninc.com) ( Notice ) explains:\nWhat information we collect during our application and recruitment process and why we collect it;\nHow we handle that information; and\nHow to access and update that information.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'MS SQL', 'jQuery', 'Data management', 'Workflow', 'Life sciences', 'Customer service', 'Automotive', 'Financial services', 'CRM']",2025-06-12 13:59:22
Etl Developer,Tata Communications,2 - 4 years,Not Disclosed,['Chennai'],"This is an operational role responsible for providing data analysis & management support. The incumbent may seek appropriate level of guidance and advice to ensure delivery of quality outcomes.\nResponsibilities\nGathering and preparing relevant data to use in analytics applications.\nAcquiring data from primary or secondary data sources and support in maintaining databases\nIdentify, analyze, and interpret trends or patterns in data sets.\nFilter and clean data by reviewing computer reports, printouts, and performance indicators to locate and correct code problems.\nDevelop and Support ETL Jobs, Schedule batch jobs via CRON, Database modeling for RDBMS\nGather data requirements, follow Scrum methodology, ownership from development to deployment",,,,"['Etl Development', 'Informatica', 'Informatica Powercenter', 'ETL']",2025-06-12 13:59:25
F2F Weekend Drive - Bangalore- 14th June - DS Gen AI,Ltimindtree,6 - 11 years,Not Disclosed,['Bengaluru'],"Job description\nWe are having a F2F weekend drive for the requirement of a Data Scientist + Gen AI at our LTIM Bangalore Whitefield office.\nDate - 14th June 2025\nExperience - 6+ Years\nMandatory Skills - Data Science, Gen AI, Python, RAG and Azure/AWS, AI/ML, NLP\nLocation - LTIMindtree Bangalore Whitefield Office\nSecondary - (Any) Machine Learning, Deep Learning, ChatGPT, Langchain, Prompt, vector stores, RAG, llama, Computer vision, Deep learning, Machine learning, OCR, Transformer, regression, forecasting, classification, hyper parameter tunning, MLOps, Inference, Model training, Model Deployment\n\nGeneric JD-\nMore than 6 years of experience in Data Engineering, Data Science and AI / ML domain\nExcellent understanding of machine learning techniques and algorithms, such as GPTs, CNN, RNN, k-NN, Naive Bayes, SVM, Decision Forests, etc.\nExperience using business intelligence tools (e.g. Tableau, PowerBI) and data frameworks (e.g. Hadoop)\nExperience in Cloud native skills.\nKnowledge of SQL and Python; familiarity with Scala, Java or C++ is an asset\nAnalytical mind and business acumen and Strong math skills (e.g. statistics, algebra)\nExperience with common data science toolkits, such as TensorFlow, KERAs, PyTorch, PANDAs, Microsoft CNTK, NumPy etc. Deep expertise in at least one of these is highly desirable.\nExperience with NLP, NLG and Large Language Models like BERT, LLaMa, LaMDA, GPT, BLOOM, PaLM, DALL-E, etc.\nGreat communication and presentation skills. Should have experience in working in a fast-paced team culture.\nExperience with AIML and Big Data technologies like AWS SageMaker, Azure Cognitive Services, Google Colab, Jupyter Notebook, Hadoop, PySpark, HIVE, AWS EMR etc.\nExperience with NoSQL databases, such as MongoDB, Cassandra, HBase, Vector databases\nGood understanding of applied statistics skills, such as distributions, statistical testing, regression, etc.\nShould be a data-oriented person with analytical mind and business acumen.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Generative AI', 'Machine Learning', 'Deep Learning', 'Python', 'Azure']",2025-06-12 13:59:27
Power Bi & SQL Developer,Sonata Software,3 - 6 years,Not Disclosed,['Chennai'],"Role & responsibilities\n\nWe are seeking a talented and detail-oriented Power BI Developer with strong skills in SQL and experience working with Azure Databricks. The ideal candidate will be responsible for transforming raw data into meaningful insights using business intelligence tools and data engineering practices. This role involves building dashboards, writing optimized queries, and working with large-scale data platforms to support business decision-making.\nKey Responsibilities:",,,,"['Power BI', 'SQL', 'Data Bricks']",2025-06-12 13:59:30
Azure and GenAi,Sightspectrum,5 - 10 years,20-30 Lacs P.A.,"['Hyderabad', 'Ahmedabad', 'Bengaluru']",Role & responsibilities\n\nGenAI\nAzure\nAI ML,Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['GenAi', 'Azure', 'AIML']",2025-06-12 13:59:32
Principal Architect-Platform & Applications @ Bangalore_Urgent,"A leader in this space, we deliver world...",13 - 20 years,Not Disclosed,['Bengaluru'],"Principal Architect - Platform & Application Architect\n\nExperience\n15+ years in software/data platform architecture\n5+ years in architectural leadership roles\nArchitecture & Data Platform Expertise\n\nEducation\nBachelors/Master’s in CS, Engineering, or related field\n\n\nTitle: Principal Architect\n\nLocation: Onsite Bangalore\n\nExperience: 15+ years in software & data platform architecture and technology strategy\n\nRole Overview\n\nWe are seeking a Platform & Application Architect to lead the design and implementation of a next-generation, multi-domain data platform and its ecosystem of applications. In this strategic and hands-on role, you will define the overall architecture, select and evolve the technology stack, and establish best practices for governance, scalability, and performance. Your responsibilities will span across the full data lifecycle—ingestion, processing, storage, and analytics—while ensuring the platform is adaptable to diverse and evolving customer needs. This role requires close collaboration with product and business teams to translate strategy into actionable, high-impact platform & products.\n\nKey Responsibilities\n\n1. Architecture & Strategy\nDesign the end-to-end architecture for a On-prem / hybrid data platform (data lake/lakehouse, data warehouse, streaming, and analytics components).\nDefine and document data blueprints, data domain models, and architectural standards.\nLead build vs. buy evaluations for platform components and recommend best-fit tools and technologies.\n2. Data Ingestion & Processing\nArchitect batch and real-time ingestion pipelines using tools like Kafka, Apache NiFi, Flink, or Airbyte.\nOversee scalable ETL/ELT processes and orchestrators (Airflow, dbt, Dagster).\nSupport diverse data sources: IoT, operational databases, APIs, flat files, unstructured data.\n3. Storage & Modeling\nDefine strategies for data storage and partitioning (data lakes, warehouses, Delta Lake, Iceberg, or Hudi).\nDevelop efficient data strategies for both OLAP and OLTP workloads.\nGuide schema evolution, data versioning, and performance tuning.\n4. Governance, Security, and Compliance\nEstablish data governance, cataloging, and lineage tracking frameworks.\nImplement access controls, encryption, and audit trails to ensure compliance with DPDPA, GDPR, HIPAA, etc.\nPromote standardization and best practices across business units.\n5. Platform Engineering & DevOps\nCollaborate with infrastructure and DevOps teams to define CI/CD, monitoring, and DataOps pipelines.\nEnsure observability, reliability, and cost efficiency of the platform.\nDefine SLAs, capacity planning, and disaster recovery plans.\n6. Collaboration & Mentorship\nWork closely with data engineers, scientists, analysts, and product owners to align platform capabilities with business goals.\nMentor teams on architecture principles, technology choices, and operational excellence.\nSkills & Qualifications\n\nBachelor’s or Master’s degree in Computer Science, Engineering, or a related field.\n12+ years of experience in software engineering, including 5+ years in architectural leadership roles.\nProven expertise in designing and scaling distributed systems, microservices, APIs, and event-driven architectures using Java, Python, or Node.js.\nStrong hands-on experience with building scalable data platforms on premise/Hybrid/cloud environments.\nDeep knowledge of modern data lake and warehouse technologies (e.g., Snowflake, BigQuery, Redshift) and table formats like Delta Lake or Iceberg.\nFamiliarity with data mesh, data fabric, and lakehouse paradigms.\nStrong understanding of system reliability, observability, DevSecOps practices, and platform engineering principles.\nDemonstrated success in leading large-scale architectural initiatives across enterprise-grade or consumer-facing platforms.\nExcellent communication, documentation, and presentation skills, with the ability to simplify complex concepts and influence at executive levels.\nCertifications such as TOGAF or AWS Solutions Architect (Professional) and experience in regulated domains (e.g., finance, healthcare, aviation) are desirable.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Architecture', 'Principal Architect', 'on-prem data platforms data lakes', 'Data Platform', 'Designed hybrid', 'Airflow', 'Airbyte', 'architectural leadership', 'Kafka', 'lakehouses', 'Flink', 'or Node.js', 'microservices', 'Hudi', 'streaming', 'DWH', 'dbt', 'Dagster', 'Delta Lake', 'Iceberg', 'distributed systems', 'NiFi', 'Python']",2025-06-12 13:59:34
"Technical Architect, AVP",XL India Business Services Pvt. Ltd,13 - 20 years,Not Disclosed,"['Hyderabad', 'Ahmedabad', 'Bengaluru']","Technical Architect-AVP Bangalore, Karnataka, India We are looking for an experienced Technical Architect to lead the design and implementation of complex software solutions\n\nThe ideal candidate will possess a deep understanding of software architecture principles, technology stacks, and development methodologies\n\nYou will collaborate with cross-functional teams to ensure that our technology solutions are scalable, secure, and aligned with business objectives\n\nWhat you ll be DOING What will your essential responsibilities include? Responsible for the design and technical delivery of the Salesforce Underwriter Journey\n\nCollaborate with business analysts and stakeholders to gather requirements and translate them into technical specifications\n\nProducing quality, secure, scalable, high-performing, and resilient designs for new or improved services\n\nLead the systems analysts, developers, and testers in sympathetic change to the applications\n\nResponsible for partnering with engineers, DevOps engineers, Administrators, and other roles responsible for implementing solutions on Azure to ensure sound infrastructure solution options are leveraged Accountable for leading technical project delivery within the applications landscape\n\nResponsible for handling multiple tech initiatives in Agile delivery models\n\nActively lead the development teams to help assist PI planning and prioritization\n\nFor internal assets, support Product Owners to develop and maintain the Product Roadmap\n\nDefine and maintain development standards such as system and data design, coding, etc Maintain a capacity plan with historical performance metrics, a future forecast, and a capacity model to ensure services and infrastructure deliver performance and growth targets in a cost effective and proactive manner\n\nManage architecture exceptions for the application, including identifying, documenting, taking through exception approval process, and remediation where and when possible\n\nMonitor application services to ensure performance consistently meets non-functional requirements (response time, security, etc)\n\nLeads the DevOps team and developers in targeted use of DevOps for their application platform assets\n\nYou will report to the Release Train Engineer\n\nWhat you will BRING We re looking for someone who has these abilities and skills: Required Skills and Abilities: Developing and maintaining custom data integration solutions with various sources and formats, including structured and unstructured data, to ensure data quality and consistency\n\nEnsures that the solution and codebase is maintainable, scalable and adheres to best practices of software development\n\nDelivering high-quality, scalable, and reliable data ingestion pipelines\n\nSupports other members of the squad in resolving technical questions related to best practice, feasibility etc Desired Skills and Abilities: Insurance background\n\nProficiency in programming language such as C#\n\nExperience with data integration tools\n\nExcellent analytical skills to evaluate complex problems and devise efficient solutions\n\nExperience with cloud platforms such as Azure, including continuous integration and continuous deployment (CI/CD)\n\nFamiliarity with big data technologies such as Hadoop, Spark, and Kafka\n\nKnowledge of data privacy and security regulations such as GDPR\n\nEffective communication and collaboration skills\n\nWho WE are AXA XL, the P&C and specialty risk division of AXA, is known for solving complex risks\n\nFor mid-sized companies, multinationals and even some inspirational individuals we don t just provide re/insurance, we reinvent it\n\nHow? By combining a comprehensive and efficient capital platform, data-driven insights, leading technology, and the best talent in an agile and inclusive workspace, empowered to deliver top client service across all our lines of business property, casualty, professional, financial lines and specialty\n\nWith an innovative and flexible approach to risk solutions, we partner with those who move the world forward\n\nLearn more at axaxl\n\ncom What we OFFER Inclusion AXA XL is committed to equal employment opportunity and will consider applicants regardless of gender, sexual orientation, age, ethnicity and origins, marital status, religion, disability, or any other protected characteristic\n\nAt AXA XL, we know that an inclusive culture and a diverse workforce enable business growth and are critical to our success\n\nThat s why we have made a strategic commitment to attract, develop, advance and retain the most diverse workforce possible, and create an inclusive culture where everyone can bring their full selves to work and can reach their highest potential\n\nIt s about helping one another and our business to move forward and succeed\n\nFive Business Resource Groups focused on gender, LGBTQ+, ethnicity and origins, disability and inclusion with 20 Chapters around the globe Robust support for Flexible Working Arrangements Enhanced family friendly leave benefits Named to the Diversity Best Practices Index Signatory to the UK Women in Finance Charter Learn more at axaxl\n\ncom / about-us / inclusion-and-diversity\n\nAXA XL is an Equal Opportunity Employer\n\nTotal Rewards AXA XL s Reward program is designed to take care of what matters most to you, covering the full picture of your health, wellbeing, lifestyle and financial security\n\nIt provides competitive compensation and personalized, inclusive benefits that evolve as you do\n\nWe re committed to rewarding your contribution for the long term, so you can be your best self today and look forward to the future with confidence\n\nSustainability At AXA XL, Sustainability is integral to our business strategy\n\nIn an ever-changing world, AXA XL protects what matters most for our clients and communities\n\nWe know that sustainability is at the root of a more resilient future\n\nOur 2023-26 Sustainability strategy, called Roots of resilience , focuses on protecting natural ecosystems, addressing climate change, and embedding sustainable practices across our operations\n\nOur Pillars: Valuing nature: How we impact nature affects how nature impacts us\n\nResilient ecosystems - the foundation of a sustainable planet and society - are essential to our future\n\nWe re committed to protecting and restoring nature - from mangrove forests to the bees in our backyard - by increasing biodiversity awareness and inspiring clients and colleagues to put nature at the heart of their plans\n\nAddressing climate change: The effects of a changing climate are far reaching and significant\n\nUnpredictable weather, increasing temperatures, and rising sea levels cause both social inequalities and environmental disruption\n\nWere building a net zero strategy, developing insurance products and services, and mobilizing to advance thought leadership and investment in societal-led solutions\n\nIntegrating ESG: All companies have a role to play in building a more resilient future\n\nIncorporating ESG considerations into our internal processes and practices builds resilience from the roots of our business\n\nWe re training our colleagues, engaging our external partners, and evolving our sustainability governance and reporting\n\nAXA Hearts in Action: We have established volunteering and charitable giving programs to help colleagues support causes that matter most to them, known as AXA XL s Hearts in Action programs\n\nThese include our Matching Gifts program, Volunteering Leave, and our annual volunteering day - the Global Day of Giving\n\nFor more information, please see axaxl\n\ncom/sustainability",Industry Type: Insurance,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['remediation', 'continuous integration', 'software architecture', 'Coding', 'Agile', 'Data quality', 'data privacy', 'Business strategy', 'Project delivery', 'Salesforce']",2025-06-12 14:00:10
Loyalty Program - Delhi,Orient Electric,8 - 13 years,Not Disclosed,"['New Delhi', 'Delhi / NCR']","Role & responsibilities\n\nOrient Electric runs several loyalty programs across key influencers to improve recommendation of Orient products across the value chain. This role is a cross functional lead role across the several business units and various departments including sales, marketing, category and technology teams to ensure smooth functioning of the loyalty programs of the company.\nIt also requires first principle disruptive thinking to create unique value propositions for these influencers and an agile mindset for execution with speed with multiple stakeholders.\nIdeal Experience\n\nWe are looking for someone with experience in product marketing, sales excellence, customer value propositions/loyalty programs. Should have an entrepreneurial mindset with ability to create solutions from scratch with an ability to analyse data continuously to improve outcomes. Prior experience in categories where influencers play a strong role in decision making (Home improvement, FMEG, Paint etc) and B2B experience will be valuable. Understanding of digital media landscapes, marketing automation systems and data integrations is needed with excellent project management skills.\n\nSet up and scale various loyalty programs for Electricians, Retailers, Shop Boys, Architects and Interior Designers to grow market share across categories that OEL operates in including\nDefining the overall structure of the program across BUs – Rewards Framework and value proposition for each influencer segment\nBuild and Maintain technology interface and processes to run the program smoothly\nContinue to deeply understand the motivations of influencers for brand recommendation and build learnings into the program. Leverage audience insights and segmentation to improve outcomes\nDefine, monitor and publish key success metrics for onboarding, earning and redemption of rewards and quantify correlation of the same to business growth\nBuild dashboards with key success metrics and set goals across business units for the program’s success\nLiaison with key members of sales, marketing, service to identify areas of opportunity and growth\nDrive user app penetration and engagement metrics and deliver on goals of acquisition, usage and redemption. Reduce end user churn through proactive actions and ensure active engagement goals are met\nExecute multichannel campaigns and programs to improve engagement and retention on the platform. Execute multiple channels of marketing including email, push notifications, product messaging ads etc to improve engagement goals\nCollaborate with business and product teams to facilitate new product launches and other adoption initiatives\nResponsible for financial hygiene and compliance and audits for the program",Industry Type: Consumer Electronics & Appliances,Department: Other,"Employment Type: Full Time, Permanent","['Loyalty', 'Loyalty Program Management', 'Loyalty Programs']",2025-06-12 14:00:12
Position For Business Intelligence Developer with ( DBT & Snowflake ),Synergy Technologies,5 - 9 years,Not Disclosed,[],"Hi ,  \nSynergy Technologies is a leader in technology services and consulting. We enable clients across the world to create and execute strategies .We help our clients find the right problems to solve, and to solve these effectively. We bring our expertise and innovation to every project we undertake\n\nPosition: Business Intelligence Developer\nDuration :  Contract to Full Time",,,,"['BI tools', 'DBT', 'Snowflake', 'DAX calculations', 'Business Intelligence Developer', 'AWS']",2025-06-12 14:00:14
Principle Architect - Backend,Circles.Life,5 - 8 years,Not Disclosed,"['Kolkata', 'Mumbai', 'New Delhi', 'Hyderabad', 'Pune', 'Chennai', 'Bengaluru']","Circles.Life is looking for Principle Architect - Backend to join our dynamic team and embark on a rewarding career journey\nLead and manage a team of architects, providing technical guidance and mentoring\nDevelop and oversee the execution of the architectural design for new projects or systems\nCollaborate with stakeholders to determine project requirements and ensure the design meets those requirements\nProvide technical expertise on system architecture, design patterns, and development methodologies\nIdentify and address technical issues throughout the development process\nEnsure that the final product meets quality standards, including performance, security, and scalability\nProvide leadership in the selection of technology platforms, development tools, and frameworks\nManage and prioritize project timelines and budgets, ensuring that projects are delivered on time and within budget\nCommunicate with stakeholders and team members to ensure that all project goals are met\nExcellent problem-solving skills and ability to identify and address technical issues\nStrong communication and interpersonal skills",Industry Type: Advertising & Marketing,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['architectural design', 'project management', 'enterprise architecture', 'microsoft azure', 'mentoring', 'machine learning', 'scalability', 'microservices', 'docker', 'system architecture', 'design patterns', 'design', 'aws', 'cloud computing', 'big data', 'communication skills', 'architecture']",2025-06-12 14:00:16
Architect,Trianz,12 - 17 years,Not Disclosed,['Bengaluru'],"Role Snowflake Architect\nJob Summary -\nAs a Data Architect, you are core to the D&AI (Data & AI) Practices success. Data is foundational to everything we do, and you are accountable for defining and delivering best-in-class Snowflake data management solutions across all major cloud platforms. This is a senior role with high visibility and reporting to the D&AI Practice Tower Lead.\nJob Responsibilities\nArchitectural Design: Architect secure, scalable, highly performant data engineering and management solutions, including data warehouses, data lake, ELT / ETL and real-time data engineering / pipeline solutions. Support Principal Data Architect in defining and maintaining Practice reference data engineering and data management architectures.\nSnowflake Implementation: Design and manage scalable end-to-end data solutions leveraging native Snowflake workloads including : Data Engineering; Data Lake; Data Warehouse; Applications; Unistore; AI/ML; Governed Collaboration, Marketplace, Streamlit.\nHyperscaler Design: Competently leverage data-related cloud platform (AWS or Azure) capabilities to architect and develop end-to-end data engineering and data management solutions.\nClient Engagement: Regular collaboration and partnership with clients to understand their challenges and needs then translate requirements into data solutions that drive customer value. Support proposal development.\nData Modeling: Create and maintain conceptual, logical, and physical data models that support both transactional and analytical needs. Ensure data models are optimized for performance and scalability.\nCreativity: Be an out-of-the-box thinker and passionate about applying your skills to new and existing solutions alike while always demonstrating a customer-first mentality.\nMandatory Skills\n12+ years hands-on data solution architecture and implementation experience on modern cloud platforms (AWS preferred) including microservice and event-driven architectures.\nSnowflake SnowPro Advanced Architect certification. An architectural certification on either AWS, Azure or GCP.\nHands-on experience with Snowflake capabilities including Snowpipe, Snowpark, Cortex, Polaris Catalog, native applications, Notebooks, Horizon, Marketplace, Streamlit.\nPractical experience with end-to-end data engineering and data management supporting functions including data modeling (conceptual, logical & physical), BI & analytics, data governance, data quality, data security / privacy / compliance, IAM, performance optimization. Advanced SQL and data profiling.\nPython, Scala or Java. Strong communication skills with the ability to convey technical concepts to non-technical users.\nStrong, self-management skills demonstrating ability to multitask and self-manage goals and activities.\nAdditional / Nice-to-have Qualifications-\nSnowflake SnowPro Advanced Data Engineer certification Snowflake SnowPro Advanced Data Scientist certification Snowflake SnowPro Advanced Administrator certification Snowflake SnowPro Advanced Data Analyst certification\nRequired Education Master or Bachelor (CS, IT, Applied Mathematics or demonstrated experience)",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Solution architecture', 'Architect', 'Data management', 'Data modeling', 'data security', 'Analytical', 'Data quality', 'Analytics', 'SQL', 'Python']",2025-06-12 14:00:19
Software Developer - Python,Squarepoint Technologies India,6 - 10 years,Not Disclosed,['Bengaluru'],"Team: Development - Alpha Data\n\nPosition Overview:\nWe are seeking an experienced Python developer to join our Alpha Data team, responsible for delivering a vast quantity of data served to users worldwide. You will be a cornerstone of a growing Data team, becoming a technical subject matter expert and developing strong working relationships with quant researchers, traders, and fellow colleagues across our Technology organisation.\nAlpha Data teams are able to deploy valuable data to the rest of the Squarepoint business at speed. Ingestion pipelines and data transformation jobs are resilient and highly maintainable, while the data models are carefully designed in close collaboration with our researchers for efficient query construction and alpha generation.\nWe achieve an economy of scale through building new frameworks, libraries, and services used to increase the team's quality of life, throughput, and code quality. Teamwork and collaboration are encouraged, excellence is rewarded and diversity of thought and creative solutions are valued. Our emphasis is on a culture of learning, development, and growth.\nTake part ownership of our ever-growing estate of data pipelines,\nPropose and contribute to new abstractions and improvements - make a real positive impact across our team globally,\nDesign, implement, test, optimize and troubleshoot our data pipelines, frameworks, and services,\nCollaborate with researchers to onboard new datasets,\nRegularly take the lead on production support operations - during normal working hours only.\n\nRequired Qualifications:\n5+ years of experience coding to a high standard in Python, React, Javascript\nBachelor's degree in a STEM subject,\nExperience with and knowledge of SQL, and one or more common RDBMS systems (we mostly use Postgres),\nPractical knowledge of commonly used protocols and tools used to transfer data (e.g. FTP, SFTP, HTTP APIs, AWS S3),\nExcellent communication skills.\n\nNice to haves\nExperience with big data frameworks, databases, distributed systems, or Cloud development.\nExperience with any of these: C++, kdb+/q, Rust.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Python', 'Python Framework', 'Full Stack', 'Backend Development', 'SQL', 'Backend', 'Software Development', 'Full Software Development Life Cycle', 'RDBMS', 'Fullstack Development', 'Javascript', 'Python Data', 'Python Development', 'React.Js']",2025-06-12 14:00:21
Oracle GRC(PAN INDIA) -8+ Yrs -Hybrid- Immediate joiners,Databuzzltd,8 - 12 years,Not Disclosed,"['Hyderabad', 'Bengaluru', 'Mumbai (All Areas)']","Databuzz is Hiring for Oracle GRC(PAN INDIA) -8+ Yrs -Hybrid- Immediate joiners\n\nPlease mail your profile to alekya.chebrolu@databuzzltd.com with the below details, If\nyou are Interested.\n\nAbout DatabuzzLTD:\n\nDatabuzz is One stop shop for data analytics specialized in Data Science, Big Data, Data Engineering, AI & ML, Cloud Infrastructure and Devops. We are an MNC based in both UK and INDIA. We are a ISO 27001 & GDPR complaint company.\n\nCTC -\nECTC -\nNotice Period/LWD - (Candidate serving notice period will be preferred)\n\nPosition: Oracle GRC(PAN INDIA) -8+ Yrs -Hybrid\nExp -8+ yrs\n\nMandatory Skills:\nShould have Oracle GRC, E Business Suite Governance, Risks and Compliance\nDevelop and Implement GRC Programs and Policies Create and enforce governance risk management and compliance programs to ensure the organization adheres to regulatory requirements and internal policies\nShould have Knowledge of software engineering methodologies reporting tools modeling and testing\nShould have Lean Six Sigma and Business Process Modelling Understanding of Lean Six Sigma and Business Process Modelling and Notation\n\n\nRegards,\nAlekya\nTalent Acquisition Specialist\nalekya.chebrolu@databuzzltd.com",Industry Type: IT Services & Consulting,Department: Other,"Employment Type: Full Time, Permanent","['Oracle Grc', 'Risk Management', 'E Business Suite Governance']",2025-06-12 14:00:24
Azure Databricks (Power Bi and AAS expert),Apex One,4 - 8 years,Not Disclosed,['Bengaluru'],"Power Bi and AAS expert (Strong SC or Specialist Senior)\nShould have hands-on experience of Data Modelling in Azure SQL Data Warehouse and Azure Analysis Service\nShould be able to write and test Dex queries.\nShould be able generate Paginated Reports in Power BI\nShould have minimum 3 Years working experience in delivering projects in Power Bi\nMust Have:-\n3 to 8 years of experience working on design, develop, and deploy ETL processes on Databricks to support data integration and transformation.\nOptimize and tune Databricks jobs for performance and scalability.\nExperience with Scala and/or Python programming languages.\nProficiency in SQL for querying and managing data.\nExpertise in ETL (Extract, Transform, Load) processes.\nKnowledge of data modeling and data warehousing concepts.\nImplement best practices for data pipelines, including monitoring, logging, and error handling.\nExcellent problem-solving skills and attention to detail.\nExcellent written and verbal communication skills\nStrong analytical and problem-solving abilities.\nExperience in version control systems (e.g., Git) to manage and track changes to the codebase.\nDocument technical designs, processes, and procedures related to Databricks development.\nStay current with Databricks platform updates and recommend improvements to existing process.\nGood to Have:-\nAgile delivery experience.\nExperience with cloud services, particularly Azure (Azure Databricks), AWS (AWS Glue, EMR), or Google Cloud Platform (GCP).\nKnowledge of Agile and Scrum Software Development Methodologies.\nUnderstanding of data lake architectures.\nFamiliarity with tools like Apache NiFi, Talend, or Informatica.\nSkills in designing and implementing data models.",Industry Type: Management Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Databricks', 'Data Modelling', 'data lake architectures', 'Power Bi', 'Azure SQL Data Warehouse', 'Scrum Software Development Methodologies', 'Google Cloud Platform', 'technical designs', 'data warehousing', 'Apache NiFi', 'data modeling', 'ETL', 'Talend']",2025-06-12 14:00:26
.NET developer(Azure)-7+yrs-Pan India-Hybrid,Databuzz ltd,7 - 12 years,Not Disclosed,"['Pune', 'Bengaluru', 'Mumbai (All Areas)']","Databuzz is Hiring for .NET developer(Azure)-7+yrs-Pan India-Hybrid\n\nPlease mail your profile to haritha.jaddu@databuzzltd.com with the below details, If\nyou are Interested.\n\nAbout DatabuzzLTD:\nDatabuzz is One stop shop for data analytics specialized in Data Science, Big Data, Data Engineering, AI & ML, Cloud Infrastructure and Devops. We are an MNC based in both UK and INDIA. We are a ISO 27001 & GDPR complaint company.\n\nCTC -\nECTC -\nNotice Period/LWD - (Candidate serving notice period will be preferred)\n\nPosition: .NET developer(Azure)\nLocation: Pan India\nExp -7+ yrs\n\nMandatory skills :\nCandidate should have strong .NET development experience\nShould have Cloud services for application deployment and integration, including components such as databases, message queuing, secret management storage & retrieval (any cloud provider is acceptable; AWS experience is a plus).\nShould have Message Queuing services like Kafka, RabbitMQ etc\nShould have TDD, Unit & Integration testing experience\nExperience on SQL and NOSQL datastores\nSOLID principles and hands on experience applying them\nCI/CD processes (No experience required but at least understanding on delivering code from development to PROD is required)\n\n\nRegards,\nHaritha\nTalent Acquisition specialist\nharitha.jaddu@databuzzltd.com",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sql/nosql', 'TDD', '.Net', 'Messaging Queue', 'azure', 'Unit Integration Testing', 'Microservices']",2025-06-12 14:00:29
Oracle Fusion SCM with Procurement(PAN INDIA) -8+ Yrs -Hybrid,Databuzzltd,8 - 12 years,Not Disclosed,"['Hyderabad', 'Bengaluru', 'Mumbai (All Areas)']","Databuzz is Hiring for Oracle Fusion SCM with Procurement(PAN INDIA) -8+ Yrs -Hybrid- Immediate joiners\n\nPlease mail your profile to alekya.chebrolu@databuzzltd.com with the below details, If\nyou are Interested.\n\nAbout DatabuzzLTD:\n\nDatabuzz is One stop shop for data analytics specialized in Data Science, Big Data, Data Engineering, AI & ML, Cloud Infrastructure and Devops. We are an MNC based in both UK and INDIA. We are a ISO 27001 & GDPR complaint company.\n\nCTC -\nECTC -\nNotice Period/LWD - (Candidate serving notice period will be preferred)\n\nPosition: Oracle Fusion SCM with Procurement(PAN INDIA) -8+ Yrs -Hybrid(PAN INDIA)\nExp -8+ yrs\n\nMandatory Skills:\nShould have Fusion SCM, Procurement\nShould have experience with 7 years on Fusion Finance implementation and support\nImplementation and Configuration Lead the implementation of Oracle Fusion Financials modules including General Ledger Accounts Payable Account Receivable Cash Management and Fixed Assets\nTesting and Quality Assurance Develop and execute test plans to ensure the successful implementation of Oracle Fusion Financials\nHands-on experience in configuring and customizing Oracle Fusion applications\n\n\nRegards,\nAlekya\nTalent Acquisition Specialist\nalekya.chebrolu@databuzzltd.com",Industry Type: IT Services & Consulting,Department: Other,"Employment Type: Full Time, Permanent","['Oracle Fusion SCM', 'Oracle Fusion Financials']",2025-06-12 14:00:31
MDM Developer,NetApp,5 - 8 years,Not Disclosed,['Bengaluru'],"Job Summary\nParticipates in reviewing, analyzing, and modifying client/server applications and systems.\nJob Requirements\nDevelop and maintain integrations between CDM and other systems, such as CRM, order management, and other boundary systems.\nCustomize and extend MDM functionality using Oracle tools, such as Oracle Integration Cloud, Oracle Application Composer, Oracle Visual Builder.",,,,"['data management', 'web services', 'unit testing', 'groovy scripting', 'data migration', 'tools', 'oracle fusion', 'master data management', 'sql', 'plsql', 'cloud', 'operations', 'java', 'spark', 'oracle erp', 'visual', 'etl', 'pubsub', 'rest', 'python', 'oracle', 'software testing', 'application', 'mdm', 'integration', 'oracle sql', 'data integration']",2025-06-12 14:00:34
Java or Spark developer,IPS GROUP,6 - 10 years,Not Disclosed,"['Hyderabad', 'Bengaluru', 'Mumbai (All Areas)']","1 Role- Java Spark Developer\n2 Technical Skill Set- Spark / Java Big Data\n3 Experience - 6 to 10 yrs\n4 Location- Bengaluru, Mumbai, Hyderabad\n\n*Must-Have*\nSpark programming\nJava / J2EE. Oracle Database, Microservices, Springboot\nAWS\n*Good-to-Have*\n1 Experience in writing Spark programming for Bigdata Hadoop.\n2 Good and hands-on experienced in Java, , Microservices, Springboot ,AWS and Spark programming.\n3 Ability to understand and do shell scripting in Unix.\n4 Having Java/J2EE experience is a plus along with working in Agile environment.\n5",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Java', 'Spark Programming', 'Spark', 'Pyspark', 'Oracle Database', 'Unix Shell Scripting', 'J2Ee', 'Spring Boot', 'pyspark developer', 'Microservices', 'java developer', 'Big Data Hadoop', 'AWS']",2025-06-12 14:00:36
SAP PP Consultant - S/4 HANA Module,Zettamine Labs,6 - 11 years,Not Disclosed,"['Mumbai', 'Pune', 'Bengaluru']","Location : Pune, Mumbai, Bangalore\n\nNotice Period : Immediate\n\nJob Overview :\n\nWe are looking for an experienced SAP PP (Production Planning) Consultant to join our team. The ideal candidate will have strong expertise in SAP PP module, including configuration, implementation, and integration with other SAP modules.\n\nKey Responsibilities :\n\n- Implement and configure SAP PP to optimize production planning and control.\n\n- Collaborate with stakeholders to gather business requirements and translate them into system solutions.\n\n- Perform end-to-end process mapping for demand planning, capacity planning, and shop floor control.\n\n- Ensure seamless integration with SAP MM, SD, and QM modules.\n\n- Troubleshoot and resolve technical issues in SAP PP environments.\n\n- Provide end-user training and documentation support.\n\n- Work closely with SAP ERP teams to enable master data integration and production planning\nrequirements.\n\n- Support SAP S/4HANA migration and implementation projects.\n\n- Optimize Bill of Materials (BOM), Routing, and Work Centers for efficient production\nprocesses.\n\n- Lead data migration from legacy systems to SAP PP.\n\nRequired Skills and Experience :\n\n- 6-12 years of experience in SAP PP implementation.\n\n- Strong knowledge of production planning, demand management, and shop floor control.\n\n- Hands-on experience in SAP PP configuration, development (ABAP), and integration.\n\n- Expertise in SAP S/4HANA integration for production planning.\n\n- Experience in capacity planning, MRP, and scheduling.\n\n- Strong understanding of warehouse business processes related to production planning.",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['S/4 HANA Module', 'SAP QM', 'S/4 HANA', 'SAP SD', 'SAP PP', 'SAP ABAP', 'SAP MM', 'SAP Integration', 'SAP Implementation', 'SAP WM']",2025-06-12 14:00:39
SAP IS Retail Consultant,Bytespoke Com,7 - 10 years,Not Disclosed,"['Kolkata', 'Mumbai', 'New Delhi', 'Hyderabad', 'Pune', 'Chennai', 'Bengaluru']","Contract\nJob Summary :\nWe are seeking an experienced SAP Consultant - DTC (Direct-to-Consumer) with strong expertise in SAP Customer Activity Repository (CAR) and Retail (S/4HANA Fashion) solutions. The consultant will play a critical role in integrating core merchandising (SAP S/4 Fashion) with multi-channel POS systems, covering areas such as POS data capture via CAR, retail price and markdown management, and sales audit processes.\nThis role involves close collaboration with global retail business stakeholders and offers exposure to the latest SAP cloud technologies , providing a unique opportunity to enhance your experience in a mature SAP ecosystem.\nExperience & Skills Required :\n7-10 years of SAP consulting experience with SAP CAR , Retail , AFS , or S/4HANA Fashion\nProven experience in POS data integration , retail pricing , and sales audit processes\nFull lifecycle implementation experience in ERP transformation programs (minimum 1-2 end-to-end cycles)\nIn-depth knowledge of POS data processing (sales, inventory, receipts, tenders, financial transactions) within SAP ERP\nFamiliarity with S/4HANA Fiori apps , Launchpad, Personas, and retail-specific transactions\nAbility to work independently and collaboratively across global teams\nExcellent problem-solving, communication, and stakeholder management skills\nNice to Have :\nExperience in Agile project environments\nKnowledge of integration components across functional SAP modules",Industry Type: Management Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['Retail', 'SAP ERP', 'SAP', 'Sales audit', 'Sales', 'SAP IS-Retail', 'Agile', 'Data processing', 'Merchandising', 'Stakeholder management']",2025-06-12 14:00:42
Artificial Intelligence Architect,Ltimindtree,12 - 16 years,Not Disclosed,"['Hyderabad', 'Bengaluru', 'Mumbai (All Areas)']",We are looking for an experienced AI ML Developers experience in data science specializing in machine learning python statistical modelling and big data technologies pyspark sql.\n\nThe ideal candidate will have a strong background in developing and deploying machine learning models optimizing ML pipelines and handling largescale structured and unstructured data to drive business impact.\n\nDeep understanding of supervised and unsupervised learning including regression classification Multiclass classification clustering and NLP Proficiency in statistical analysis AB testing and causal inference techniques Experience with model deployment and MLOps in cloud environments AWS GCP \n\nKey Responsibilities\n\nDevelop and deploy machine learning models and predictive analytics solutions for business impact\nWork with largescale structured and unstructured data to extract insights and build scalable models\nDesign implement and optimize ML pipelines for realtime and batch processing\nCollaborate with engineering product and business stakeholders to translate business problems into data science solutions\nApply statistical modeling AB testing and causal inference techniques to evaluate business performance\nApply machine learning and statistical techniques for audience segmentation helping to identify patterns and optimise business strategies\nDrive research and innovation by staying updated with cuttingedge MLAI advancements and incorporating them into our solutions\nOptimize data science models for performance scalability and interpretability in production environments\nMentor junior data scientists and contribute to best practices in data science and engineering,Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Architect', 'MLOps', 'Machine Learning', 'Ai Solutions', 'Aiml', 'Ml']",2025-06-12 14:00:44
Data Engineer,Accenture,15 - 20 years,Not Disclosed,['Bengaluru'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Neo4j, Stardog\n\n\n\n\nGood to have skills :JavaMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions that facilitate data generation, collection, and processing. Your typical day will involve creating data pipelines, ensuring data quality, and implementing ETL processes to migrate and deploy data across various systems. You will collaborate with cross-functional teams to understand their data needs and provide effective solutions, ensuring that the data infrastructure is robust and scalable to meet the demands of the organization.\nRoles & Responsibilities:- Expected to be an SME.- Collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Mentor junior team members to enhance their skills and knowledge in data engineering.- Continuously evaluate and improve data processes to enhance efficiency and effectiveness.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Neo4j.- Good To Have\n\n\n\n\nSkills:\nExperience with Java.- Strong understanding of data modeling and graph database concepts.- Experience with data integration tools and ETL processes.- Familiarity with data quality frameworks and best practices.- Proficient in programming languages such as Python or Scala for data manipulation.\nAdditional Information:- The candidate should have minimum 5 years of experience in Neo4j.- This position is based at our Bengaluru office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['scala', 'java', 'data modeling', 'python', 'neo4j', 'hive', 'pyspark', 'data warehousing', 'sql', 'spark', 'hadoop', 'data visualization', 'etl', 'big data', 'data manipulation', 'airflow', 'machine learning', 'data engineering', 'data quality', 'tableau', 'mapreduce', 'kafka', 'sqoop', 'aws', 'etl process']",2025-06-13 05:09:18
Data Engineer,HARMAN,5 - 10 years,Not Disclosed,['Bengaluru'],"-Strong analytical thinking and problem-solving skills, with the ability to translate complex data into actionable insights\n-Excellent communication skills, with the ability to effectively convey complex findings to both technical and non-technical stakeholders.\nCandidate to work form SRIB Bangalore with 3 days working from office is mandatory\n  What You Will Do",,,,"['Digital media', 'CTV', 'Analytical', 'Machine learning', 'Agile', 'Data processing', 'Automotive', 'Python']",2025-06-13 05:09:19
Big Data Developer/Data Engineer,Grid Dynamics,5 - 10 years,Not Disclosed,['Bengaluru'],"Role & responsibilities\nExperience: 5 - 8 years\nEmployment Type: Full-Time\n\nJob Summary:\nWe are looking for a highly skilled Scala and Spark Developer to join our data engineering team. The ideal candidate will have strong experience in building scalable data processing solutions using Apache Spark and writing robust, high-performance applications in Scala. You will work closely with data scientists, data analysts, and product teams to design, develop, and optimize large-scale data pipelines and ETL workflows.\n\nKey Responsibilities:\nDevelop and maintain scalable data processing pipelines using Apache Spark and Scala.\nWork on batch and real-time data processing using Spark (RDD/DataFrame/Dataset).\nWrite efficient and maintainable code following best practices and coding standards.\nCollaborate with cross-functional teams to understand data requirements and implement solutions.\nOptimize performance of Spark jobs and troubleshoot data-related issues.\nIntegrate data from multiple sources and ensure data quality and consistency.\nParticipate in design reviews, code reviews, and provide technical leadership when needed.\nContribute to data modeling, schema design, and architecture discussions.\nRequired Skills:\nStrong programming skills in Scala.\nExpertise in Apache Spark (Core, SQL, Streaming).\nHands-on experience with distributed computing and large-scale data processing.\nExperience with data formats like Parquet, Avro, ORC, and JSON.\nGood understanding of functional programming concepts.\nFamiliarity with data ingestion tools (Kafka, Flume, Sqoop, etc.).\nExperience working with Hadoop ecosystem (HDFS, Hive, YARN, etc.) is a plus.\nStrong SQL skills and experience working with relational and NoSQL databases.\nExperience with version control tools like Git.\nPreferred Qualifications:\nBachelor's or Masters degree in Computer Science, Engineering, or related field.\nExperience with cloud platforms like AWS, Azure, or GCP (especially EMR, Databricks, etc.).\nKnowledge of containerization (Docker, Kubernetes) is a plus.\nFamiliarity with CI/CD tools and DevOps practices.ndidate profile",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Scala', 'Pyspark', 'Spark']",2025-06-13 05:09:21
Hadoop Data Engineer,Info Origin,0 - 2 years,Not Disclosed,['Gurugram'],"Job Overview:\nWe are looking for experienced Data Engineers proficient in Hadoop, Hive, Python, SQL, and Pyspark/Spark to join our dynamic team. Candidates will be responsible for designing, developing, and maintaining scalable big data solutions.\nKey Responsibilities:\nDevelop and optimize data pipelines for large-scale data processing.\nWork with structured and unstructured datasets to derive actionable insights.\nCollaborate with cross-functional teams to enhance data-driven decision-making.\nEnsure the performance, scalability, and reliability of data architectures.\nImplement best practices for data security and governance.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['hive', 'Scalability', 'data security', 'spark', 'Hadoop', 'Data processing', 'big data', 'SQL', 'Python']",2025-06-13 05:09:23
Data Engineer,Capgemini,6 - 9 years,Not Disclosed,['Gurugram'],"\nesign, implement, and maintain data pipelines for data ingestion, processing, and transformation in Azure.\nWork together with data scientists and analysts to understand the needs for data and create effective data workflows.\nCreate and maintain data storage solutions including Azure SQL Database, Azure Data Lake, and Azure Blob Storage.\nUtilizing Azure Data Factory or comparable technologies, create and maintain ETL (Extract, Transform, Load) operations.\nImplementing data validation and cleansing procedures will ensure the quality, integrity, and dependability of the data.\nImprove the scalability, efficiency, and cost-effectiveness of data pipelines.\nMonitoring and resolving data pipeline problems will guarantee consistency and availability of the data.\nWorks in the area of Software Engineering, which encompasses the development, maintenance and optimization of software solutions/applications.1. Applies scientific methods to analyse and solve software engineering problems.2. He/she is responsible for the development and application of software engineering practice and knowledge, in research, design, development and maintenance.3. His/her work requires the exercise of original thought and judgement and the ability to supervise the technical and administrative work of other software engineers.4. The software engineer builds skills and expertise of his/her software engineering discipline to reach standard software engineer skills expectations for the applicable role, as defined in Professional Communities.5. The software engineer collaborates and acts as team player with other software engineers and stakeholders.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['azure data lake', 'azure data factory', 'sql', 'azure blob storage', 'sql azure', 'hive', 'azure databricks', 'python', 'data validation', 'pyspark', 'data warehousing', 'power bi', 'data engineering', 'spark', 'data ingestion', 'software engineering', 'hadoop', 'etl', 'big data', 'aws', 'sql database']",2025-06-13 05:09:24
"Data Engineer, Devices",Amazon,5 - 10 years,Not Disclosed,['Noida'],"Are you a highly skilled data engineer and project leaderDo you think big, enjoy complexity and building solutions that scaleAre you curious to know what you could achieve in a company that pushes the boundaries of modern technologyIf you answered yes and you have a background in FinTech you ll love this role and Amazon s data obsessed culture.\n\nAmazon Devices and Services Fintech is the global team that designs and builds the financial planning and analysis tools for wide variety of Amazon s new and established organizations. From Kindle to Ring and even new and exciting companies like Kuiper (our new interstellar satellite play) this team enjoys a wide variety of complex and interesting problem spaces. They are almost like FinTech consultants embedded in Amazon.\n\nThis team are looking for a Data Engineer to build and enhance the businesses finance systems with TM1 at its core. You will manage all aspects from requirements gathering, technical design, development, deployment, and integration to solve budgeting, planning, performance management and reporting challenges\n\n\nDesign and implement next generation financial solutions assisted by almost unlimited access to AWS resources including EC2, RDS, Redshift, Stepfunctions, EMR, Lambda and 3rd party software TM1.\nBuild and deliver high quality data pipelines capable of scaling from running for a single month of data during month end close to 150 and more months when doing restatements.\nContinually improve ongoing reporting and analysis processes and infrastructure, automating or simplifying self-service capabilities for customers.\nDive deep to resolve problems at their root, looking for failure patterns and suggesting and implementing fixes or enhancements.\nPrepare runbooks, methods of procedures, tutorials, training videos on best practices for global delivery.\nSolve unique challenges presented by the massive data volume and diverse data sets working for one of the largest companies in the wo 5+ years data engineering experience.\nExtensive experience writing SQL queries and stored procedures.\nExperience with big data tools and distributed computing.\nFinance experience, exhibiting knowledge of financial reporting, budgeting and forecasting functions and processes.\nBachelors degree. Experience with non-relational databases / data stores (object storage, document or key-value stores, graph databases, column-family databases)\nExperience with AWS technologies like Redshift, S3, AWS Glue, EMR, Kinesis, FireHose, Lambda, and IAM roles and permissions.\nExperience with programming languages such as python, java shell scripts.\nExperience with IBM Planning Analytics/TM1 both scripting processes and writing rules.\nExperience with design delivery of formal training curriculum and programs.\nProject management, scoping, reporting, and scheduling experience.",,,,"['Performance management', 'Financial reporting', 'Project management', 'Financial planning', 'Scheduling', 'Stored procedures', 'Budgeting', 'Forecasting', 'Analytics', 'Python']",2025-06-13 05:09:26
Data Engineer,AMERICAN EXPRESS,3 - 8 years,Not Disclosed,['Chennai'],"You Lead the Way. We've Got Your Back.\n\nWith the right backing, people and businesses have the power to progress in incredible ways. When you join Team Amex, you become part of a global and diverse community of colleagues with an unwavering commitment to back our customers, communities and each other. Here, youll learn and grow as we help you create a career journey thats unique and meaningful to you with benefits, programs, and flexibility that support you personally and professionally.\nAt American Express, you’ll be recognized for your contributions, leadership, and impact—every colleague has the opportunity to share in the company’s success. Together, we’ll win as a team, striving to uphold our company values and powerful backing promise to provide the world’s best customer experience every day. And we’ll do it with the utmost integrity, and in an environment where everyone is seen, heard and feels like they belong. As part of our diverse tech team, you can architect, code and ship software that makes us an essential part of our customers’ digital lives. Here, you can work alongside talented engineers in an open, supportive, inclusive environment where your voice is valued, and you make your own decisions on what tech to use to solve challenging problems. Amex offers a range of opportunities to work with the latest technologies and encourages you to back the broader engineering community through open source. And because we understand the importance of keeping your skills fresh and relevant, we give you dedicated time to invest in your professional development. Find your place in technology on #TeamAmex.",,,,"['Data Engineering', 'GCP', 'Airflow', 'Pyspark', 'Bigquery', 'Hadoop', 'Big Data', 'SQL', 'Python', 'Backend Development']",2025-06-13 05:09:28
Data Engineer,Bajaj Financial Securities,2 - 5 years,Not Disclosed,['Pune'],"We're Hiring: Data Engineer | 25 Years Experience | AWS + Real-time Focus\nJoin our fast-moving team as a Data Engineer where you'll build scalable, real-time data pipelines, own cloud infrastructure, and collaborate across teams to drive data-first decisions.\nIf you're strong in Python, experienced with streaming platforms like Kafka/Kinesis, and have shipped cloud-native data pipelines (preferably AWS) — we want to hear from you.\nMust-Haves:\n2–5 years of experience in Data Engineering\nPython (Pandas, PySpark, async), SQL, ETL/ELT\nStreaming experience (Kafka/Kinesis)\nAWS cloud stack (Glue, Lambda, S3, Athena)\nExperience in APIs, data warehousing, and data modelling\nBonus if you know: Docker, Kubernetes, Airflow/dbt, or have a background in MLOps.Role & responsibilities\n\n\nPreferred candidate profile",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Aws Lambda', 'Docker', 'Cloud Platform', 'Data Warehousing', 'Python', 'Pyspark', 'Api Gateway', 'Kinesis', 'Kafka', 'ETL', 'SQL', 'Kubernetes']",2025-06-13 05:09:30
"Senior Staff Engineer, Big Data Engineer",Nagarro,9 - 13 years,Not Disclosed,['India'],"We're Nagarro.\nWe are a Digital Product Engineering company that is scaling in a big way! We build products, services, and experiences that inspire, excite, and delight. We work at scale across all devices and digital mediums, and our people exist everywhere in the world (18000+ experts across 38 countries, to be exact). Our work culture is dynamic and non-hierarchical. We're looking for great new colleagues. That's where you come in!\n\nREQUIREMENTS:\nTotal experience 9+ years.\nExcellent knowledge and experience in Big data engineer.\nStrong working experience with architecture and development in Apache Spark, Spark, Python, Azure Databricks, Data Pipelines, Azure Devops, Kafka, SQL Server/NoSQL.\nStrong expertise in Python, Django Rest Framework, Databricks and PostgreSQL.\nHands on experience in building data pipelines and building data frameworks for unit testing, data lineage tracking, and automation.\nFamiliarity with streaming technologies (e.g., Kafka, Kinesis, Flink).\nExperience with building and maintaining a cloud system.\nFamiliarity with data modeling, data warehousing, and building distributed systems.\nExpertise in Spanner for high-availability, scalable database solutions.\nKnowledge of data governance and security practices in cloud-based environments.\nProblem-solving mindset with the ability to tackle complex data engineering challenges.\nStrong communication and teamwork skills, with the ability to mentor and collaborate effectively.\n\nRESPONSIBILITIES:\nWriting and reviewing great quality code\nUnderstanding the clients business use cases and technical requirements and be able to convert them in to technical design which elegantly meets the requirements\nMapping decisions with requirements and be able to translate the same to developers\nIdentifying different solutions and being able to narrow down the best option that meets the clients requirements\nDefining guidelines and benchmarks for NFR considerations during project implementation\nWriting and reviewing design document explaining overall architecture, framework, and high-level design of the application for the developers\nReviewing architecture and design on various aspects like extensibility, scalability, security, design patterns, user experience, NFRs, etc., and ensure that all relevant best practices are followed\nDeveloping and designing the overall solution for defined functional and non-functional requirements; and defining technologies, patterns, and frameworks to materialize it\nUnderstanding and relating technology integration scenarios and applying these learnings in projects\nResolving issues that are raised during code/review, through exhaustive systematic analysis of the root cause, and being able to justify the decision taken\nCarrying out POCs to make sure that suggested design/technologies meet the requirements",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Big Data', 'Spark', 'Data Bricks', 'Python', 'Pyspark', 'Django Framework', 'Azure Databricks', 'SQL Server']",2025-06-13 05:09:31
Senior Big Data Engineer,Qualcomm,2 - 7 years,Not Disclosed,['Hyderabad'],"Job Area: Engineering Group, Engineering Group > Software Engineering\n\nGeneral Summary:\n\nAs a leading technology innovator, Qualcomm pushes the boundaries of what's possible to enable next-generation experiences and drives digital transformation to help create a smarter, connected future for all. As a Qualcomm Software Engineer, you will design, develop, create, modify, and validate embedded and cloud edge software, applications, and/or specialized utility programs that launch cutting-edge, world class products that meet and exceed customer needs. Qualcomm Software Engineers collaborate with systems, hardware, architecture, test engineers, and other teams to design system-level software solutions and obtain information on performance requirements and interfaces.\n\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 2+ years of Software Engineering or related work experience.\nOR\nMaster's degree in Engineering, Information Systems, Computer Science, or related field and 1+ year of Software Engineering or related work experience.\nOR\nPhD in Engineering, Information Systems, Computer Science, or related field.\n\n2+ years of academic or work experience with Programming Language such as C, C++, Java, Python, etc.\n\nGeneral Summary\n\nPreferred Qualifications\n\n3+ years of experience as a Data Engineer or in a similar role\n\nExperience with\n\ndata modeling, data warehousing, and building ETL pipelines\n\nSolid working experience with\n\nPython, AWS analytical technologies and related resources (Glue, Athena, QuickSight, SageMaker, etc.,)\n\nExperience with\n\nBig Data tools, platforms and architecture with solid working experience with SQL\n\nExperience working in a very large data warehousing environment,\n\nDistributed System.\n\nSolid understanding on various data exchange formats and complexities\n\nIndustry experience in software development, data engineering, business intelligence, data science, or related field with a track record of manipulating, processing, and extracting value from large datasets\n\nStrong data visualization skills\n\nBasic understanding of Machine Learning; Prior experience in ML Engineering a plus\n\nAbility to manage on-premises data and make it inter-operate with AWS based pipelines\n\nAbility to interface with Wireless Systems/SW engineers and understand the Wireless ML domain; Prior experience in Wireless (5G) domain a plus\n\n\nEducation\n\nBachelor's degree in computer science, engineering, mathematics, or a related technical discipline\n\nPreferred QualificationsMasters in CS/ECE with a Data Science / ML Specialization\n\n\nMinimum Qualifications:\n\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 3+ years of Software Engineering or related work experience.\n\nOR\n\nMaster's degree in Engineering, Information Systems, Computer Science, or related field\n\nOR\n\nPhD in Engineering, Information Systems, Computer Science, or related field.\n\n3+ years of experience with Programming Language such as C, C++, Java, Python, etc.\n\n\nDevelops, creates, and modifies general computer applications software or specialized utility programs. Analyzes user needs and develops software solutions. Designs software or customizes software for client use with the aim of optimizing operational efficiency. May analyze and design databases within an application area, working individually or coordinating database development as part of a team. Modifies existing software to correct errors, allow it to adapt to new hardware, or to improve its performance. Analyzes user needs and software requirements to determine feasibility of design within time and cost constraints. Confers with systems analysts, engineers, programmers and others to design system and to obtain information on project limitations and capabilities, performance requirements and interfaces. Stores, retrieves, and manipulates data for analysis of system capabilities and requirements. Designs, develops, and modifies software systems, using scientific analysis and mathematical models to predict and measure outcome and consequences of design.\n\nPrincipal Duties and Responsibilities:\n\nCompletes assigned coding tasks to specifications on time without significant errors or bugs.\n\nAdapts to changes and setbacks in order to manage pressure and meet deadlines.\n\nCollaborates with others inside project team to accomplish project objectives.\n\nCommunicates with project lead to provide status and information about impending obstacles.\n\nQuickly resolves complex software issues and bugs.\n\nGathers, integrates, and interprets information specific to a module or sub-block of code from a variety of sources in order to troubleshoot issues and find solutions.\n\nSeeks others' opinions and shares own opinions with others about ways in which a problem can be addressed differently.\n\nParticipates in technical conversations with tech leads/managers.\n\nAnticipates and communicates issues with project team to maintain open communication.\n\nMakes decisions based on incomplete or changing specifications and obtains adequate resources needed to complete assigned tasks.\n\nPrioritizes project deadlines and deliverables with minimal supervision.\n\nResolves straightforward technical issues and escalates more complex technical issues to an appropriate party (e.g., project lead, colleagues).\n\nWrites readable code for large features or significant bug fixes to support collaboration with other engineers.\n\nDetermines which work tasks are most important for self and junior engineers, stays focused, and deals with setbacks in a timely manner.\n\nUnit tests own code to verify the stability and functionality of a feature.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'sql', 'software engineering', 'data visualization', 'aws', 'quicksight', 'c', 'software development', 'glue', 'aws sagemaker', 'data warehousing', 'machine learning', 'business intelligence', 'data engineering', 'java', 'data science', 'data modeling', 'athena', 'wireless', 'big data', 'etl', 'ml']",2025-06-13 05:09:33
Data Engineer,Accenture,15 - 20 years,Not Disclosed,['Pune'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Neo4j, Stardog\n\n\n\n\nGood to have skills :JavaMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions that facilitate data generation, collection, and processing. Your typical day will involve creating data pipelines, ensuring data quality, and implementing ETL processes to migrate and deploy data across various systems. You will collaborate with cross-functional teams to understand their data needs and provide effective solutions, ensuring that the data infrastructure is robust and scalable to meet the demands of the organization.\nRoles & Responsibilities:- Expected to be an SME.- Collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Mentor junior team members to enhance their skills and knowledge in data engineering.- Continuously evaluate and improve data processes to enhance efficiency and effectiveness.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Neo4j.- Good To Have\n\n\n\n\nSkills:\nExperience with Java.- Strong understanding of data modeling and graph database concepts.- Experience with data integration tools and ETL processes.- Familiarity with data quality frameworks and best practices.- Proficient in programming languages such as Python or Scala for data manipulation.\nAdditional Information:- The candidate should have minimum 5 years of experience in Neo4j.- This position is based at our Bengaluru office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['scala', 'java', 'data modeling', 'python', 'neo4j', 'hive', 'pyspark', 'data warehousing', 'sql', 'spark', 'hadoop', 'data visualization', 'etl', 'big data', 'data manipulation', 'airflow', 'machine learning', 'data engineering', 'data quality', 'tableau', 'mapreduce', 'kafka', 'sqoop', 'aws', 'etl process']",2025-06-13 05:09:35
Data Engineer,Accenture,5 - 10 years,Not Disclosed,['Bengaluru'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL processes to migrate and deploy data across systems. Your day will involve working on data solutions and ensuring data integrity and quality.\nRoles & Responsibilities:- Expected to be an SME, collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Develop and maintain data pipelines for efficient data processing.- Implement ETL processes to migrate and deploy data across systems.- Ensure data quality and integrity throughout the data solutions.- Collaborate with cross-functional teams to optimize data processes.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Strong understanding of data engineering principles.- Experience with cloud-based data solutions like AWS or Azure.- Knowledge of SQL and NoSQL databases.- Hands-on experience with data modeling and schema design.\nAdditional Information:- The candidate should have a minimum of 5 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Bengaluru office.- A 15 years full-time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analytics', 'data engineering', 'sql', 'etl', 'aws', 'hive', 'python', 'data processing', 'microsoft azure', 'pyspark', 'data warehousing', 'data integrity', 'knowledge of sql', 'nosql', 'database design', 'data quality', 'data modeling', 'spark', 'hadoop', 'big data', 'etl process', 'nosql databases']",2025-06-13 05:09:37
Data Engineer,Accenture,3 - 8 years,Not Disclosed,['Bengaluru'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Informatica MDM\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions that facilitate data generation, collection, and processing. Your typical day will involve creating data pipelines, ensuring data quality, and implementing ETL processes to effectively migrate and deploy data across various systems, contributing to the overall efficiency and reliability of data management within the organization.\nRoles & Responsibilities:- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Collaborate with cross-functional teams to gather requirements and translate them into technical specifications.- Monitor and optimize data pipelines for performance and reliability.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Informatica MDM.- Good To Have\n\n\n\n\nSkills:\nExperience with data warehousing concepts and practices.- Strong understanding of data modeling techniques and best practices.- Familiarity with SQL and database management systems.- Experience in implementing data governance and data quality frameworks.\nAdditional Information:- The candidate should have minimum 3 years of experience in Informatica MDM.- This position is based at our Bengaluru office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data warehousing', 'sql', 'data modeling', 'etl', 'informatica mdm', 'hive', 'python', 'data management', 'data engineering', 'data quality', 'tableau', 'spark', 'mdm', 'data governance', 'data warehousing concepts', 'technical specifications', 'hadoop', 'big data', 'informatica', 'etl process']",2025-06-13 05:09:39
Data Engineer,Accenture,12 - 15 years,Not Disclosed,['Bengaluru'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Data Engineering\n\n\n\n\nGood to have skills :Java Enterprise EditionMinimum\n\n\n\n12 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions that facilitate data generation, collection, and processing. Your typical day will involve creating data pipelines, ensuring data quality, and implementing ETL processes to migrate and deploy data across various systems. You will collaborate with cross-functional teams to understand their data needs and provide effective solutions, ensuring that the data infrastructure is robust and scalable to meet the demands of the organization.\nRoles & Responsibilities:- Expected to be an SME.- Collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Expected to provide solutions to problems that apply across multiple teams.- Mentor junior team members to enhance their skills and knowledge in data engineering.- Continuously evaluate and improve data processes to enhance efficiency and effectiveness.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Data Engineering.- Strong understanding of data modeling and database design principles.- Experience with ETL tools and frameworks.- Familiarity with cloud platforms such as AWS, Azure, or Google Cloud.- Knowledge of data warehousing concepts and technologies.\nAdditional Information:- The candidate should have minimum 12 years of experience in Data Engineering.- This position is based at our Bengaluru office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data engineering', 'database design', 'data modeling', 'design principles', 'aws', 'hive', 'python', 'scala', 'microsoft azure', 'data warehousing', 'java collections', 'sql', 'data quality', 'java', 'gcp', 'etl tool', 'spark', 'data warehousing concepts', 'hadoop', 'etl', 'big data', 'etl process']",2025-06-13 05:09:41
Data Engineer,Accenture,2 - 7 years,Not Disclosed,['Bengaluru'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Google Cloud Data Services\n\n\n\n\nGood to have skills :GCP Dataflow, Data EngineeringMinimum\n\n\n\n2 year(s) of experience is required\n\n\n\n\nEducational Qualification :standard 15 years\n\n\nSummary:As a Data Engineer, you will be responsible for designing, developing, and maintaining data solutions for data generation, collection, and processing. Your role involves creating data pipelines, ensuring data quality, and implementing ETL processes to migrate and deploy data across systems. You will play a crucial part in the data management process.\nRoles & Responsibilities:- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work-related problems.- Develop and maintain data solutions for data generation, collection, and processing.- Create data pipelines to streamline data flow.- Ensure data quality and integrity throughout the data lifecycle.- Implement ETL processes for data migration and deployment.- Collaborate with cross-functional teams to optimize data processes.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Google Cloud Data Services.- Good To Have\n\n\n\n\nSkills:\nExperience with Data Engineering and GCP Dataflow.- Strong understanding of cloud-based data services.- Experience in designing and implementing data pipelines.- Knowledge of ETL processes and data migration techniques.\nAdditional Information:- The candidate should have a minimum of 2 years of experience in Google Cloud Data Services.- This position is based at our Bengaluru office.- A standard 15 years of education is required.\n\nQualification\n\nstandard 15 years",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['gcp', 'etl', 'data services', 'google', 'data engineering', 'hive', 'data management', 'data warehousing', 'data migration', 'business intelligence', 'sql', 'plsql', 'data modeling', 'spark', 'hadoop', 'big data', 'python', 'sql server', 'data quality', 'tableau', 'aws', 'ssis', 'data flow', 'informatica', 'etl process']",2025-06-13 05:09:43
Data Engineer,Accenture,15 - 20 years,Not Disclosed,['Bengaluru'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Data Modeling Techniques and Methodologies\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n12 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL processes to migrate and deploy data across systems. Your day will involve working on data architecture and collaborating with cross-functional teams to optimize data processes.\nRoles & Responsibilities:- Expected to be an SME.- Collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Expected to provide solutions to problems that apply across multiple teams.- Lead data modeling initiatives to design and implement data structures.- Optimize data storage and retrieval processes.- Develop and maintain data pipelines for efficient data flow.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Data Modeling Techniques and Methodologies.- Strong understanding of database management systems.- Experience with data warehousing and ETL processes.- Knowledge of data governance and compliance.- Hands-on experience with data visualization tools.\nAdditional Information:- The candidate should have a minimum of 12 years of experience in Data Modeling Techniques and Methodologies.- This position is based at our Bengaluru office.PFB Education details- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['database management system', 'data warehousing', 'data modeling', 'data visualization', 'etl', 'hive', 'python', 'data architecture', 'data engineering', 'sql', 'database management', 'data quality', 'tableau', 'spark', 'data governance', 'data structures', 'hadoop', 'big data', 'data flow', 'etl process']",2025-06-13 05:09:45
Data Engineer,Accenture,15 - 20 years,Not Disclosed,['Bengaluru'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Data Architecture Principles\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n12 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions for data generation, collection, and processing. You will create data pipelines, ensure data quality, and implement ETL processes to migrate and deploy data across systems. Your day will involve working on various data-related tasks and collaborating with teams to optimize data processes.\nRoles & Responsibilities:- Expected to be an SME.- Collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Expected to provide solutions to problems that apply across multiple teams.- Develop innovative data solutions to meet business requirements.- Optimize data pipelines for efficiency and scalability.- Implement data governance policies to ensure data quality and security.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Data Architecture Principles.- Strong understanding of data modeling and database design.- Experience with ETL tools and processes.- Knowledge of cloud platforms and big data technologies.- Good To Have\n\n\n\n\nSkills:\nData management and governance expertise.\nAdditional Information:- The candidate should have a minimum of 12 years of experience in Data Architecture Principles.- This position is based at our Bengaluru office.Education information - - A 15 years full-time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data management', 'data architecture', 'database design', 'data architecture principles', 'data modeling', 'hive', 'python', 'big data technologies', 'cloud platforms', 'data engineering', 'sql', 'data quality', 'etl tool', 'spark', 'data governance', 'hadoop', 'etl', 'big data', 'etl process']",2025-06-13 05:09:46
Data Engineer,Accenture,15 - 20 years,Not Disclosed,['Bhubaneswar'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :PySpark, Microsoft Azure Databricks, Microsoft Azure Analytics Services, Microsoft Azure Data Services\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions that facilitate data generation, collection, and processing. Your typical day will involve creating data pipelines, ensuring data quality, and implementing ETL processes to migrate and deploy data across various systems. You will collaborate with cross-functional teams to understand data requirements and deliver effective solutions that meet business needs.\nRoles & Responsibilities:- Expected to be an SME.- Collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Mentor junior team members to enhance their skills and knowledge.- Continuously evaluate and improve data processes to enhance efficiency.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in PySpark, Microsoft Azure Databricks, Microsoft Azure Data Services, Microsoft Azure Analytics Services.- Strong experience in designing and implementing data pipelines.- Proficient in data modeling and database design.- Familiarity with data warehousing concepts and technologies.- Experience with data quality and data governance practices.\nAdditional Information:- The candidate should have minimum 5 years of experience in PySpark.- This position is based at our Bengaluru office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['azure databricks', 'data services', 'pyspark', 'microsoft azure', 'data modeling', 'hive', 'python', 'analytics services', 'azure analytics', 'data warehousing', 'data engineering', 'sql', 'database design', 'data quality', 'spark', 'data governance', 'data warehousing concepts', 'etl', 'etl process']",2025-06-13 05:09:48
Data Engineer,Accenture,3 - 8 years,Not Disclosed,['Hyderabad'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Informatica Data Quality\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions that facilitate data generation, collection, and processing. Your typical day will involve creating data pipelines, ensuring data quality, and implementing ETL processes to effectively migrate and deploy data across various systems. You will collaborate with team members to enhance data workflows and contribute to the overall efficiency of data management practices.\nRoles & Responsibilities:- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Assist in the design and implementation of data architecture to support data initiatives.- Monitor and optimize data pipelines for performance and reliability.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Informatica Data Quality.- Strong understanding of data integration techniques and ETL processes.- Experience with data profiling and data cleansing methodologies.- Familiarity with database management systems and SQL.- Knowledge of data governance and data quality best practices.\nAdditional Information:- The candidate should have minimum 3 years of experience in Informatica Data Quality.- This position is based at our Hyderabad office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['informatica data quality', 'sql', 'etl', 'data integration', 'etl process', 'hive', 'python', 'data management', 'data architecture', 'data engineering', 'data cleansing', 'database management', 'profiling', 'spark', 'data governance', 'hadoop', 'big data', 'informatica', 'data profiling']",2025-06-13 05:09:50
Data Engineer,Accenture,3 - 8 years,Not Disclosed,['Navi Mumbai'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :Business AgilityMinimum\n\n\n\n12 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions that facilitate data generation, collection, and processing. Your typical day will involve creating data pipelines, ensuring data quality, and implementing ETL processes to migrate and deploy data across various systems. You will collaborate with cross-functional teams to understand data requirements and deliver effective solutions that meet business needs. Additionally, you will monitor and optimize data workflows to enhance performance and reliability, ensuring that data is accessible and actionable for stakeholders.\nRoles & Responsibilities:- Need Databricks resource with Azure cloud experience- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Collaborate with data architects and analysts to design scalable data solutions.- Implement best practices for data governance and security throughout the data lifecycle.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Good To Have\n\n\n\n\nSkills:\nExperience with Business Agility.- Strong understanding of data modeling and database design principles.- Experience with data integration tools and ETL processes.- Familiarity with cloud platforms and services related to data storage and processing.\nAdditional Information:- The candidate should have minimum 3 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Pune office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data analytics', 'database design', 'data modeling', 'design principles', 'etl', 'hive', 'python', 'data warehousing', 'power bi', 'machine learning', 'data engineering', 'sql server', 'sql', 'data bricks', 'data quality', 'tableau', 'spark', 'data governance', 'hadoop', 'big data', 'aws', 'ssis', 'etl process']",2025-06-13 05:09:52
Data Engineer,Accenture,3 - 8 years,Not Disclosed,['Chennai'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :Business AgilityMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions that facilitate data generation, collection, and processing. Your typical day will involve creating data pipelines, ensuring data quality, and implementing ETL processes to migrate and deploy data across various systems. You will collaborate with cross-functional teams to understand data requirements and deliver effective solutions that meet business needs. Additionally, you will monitor and optimize data workflows to enhance performance and reliability, ensuring that data is accessible and actionable for stakeholders.\nRoles & Responsibilities:- Need Databricks resource with Azure cloud experience- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Collaborate with data architects and analysts to design scalable data solutions.- Implement best practices for data governance and security throughout the data lifecycle.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Good To Have\n\n\n\n\nSkills:\nExperience with Business Agility.- Strong understanding of data modeling and database design principles.- Experience with data integration tools and ETL processes.- Familiarity with cloud platforms and services related to data storage and processing.\nAdditional Information:- The candidate should have minimum 3 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Pune office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data analytics', 'database design', 'data modeling', 'design principles', 'etl', 'hive', 'python', 'data warehousing', 'power bi', 'machine learning', 'data engineering', 'sql server', 'sql', 'data bricks', 'data quality', 'tableau', 'spark', 'data governance', 'hadoop', 'big data', 'aws', 'ssis', 'etl process']",2025-06-13 05:09:54
Data Engineer,Accenture,3 - 8 years,Not Disclosed,['Pune'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions that facilitate data generation, collection, and processing. Your typical day will involve creating data pipelines, ensuring data quality, and implementing ETL processes to effectively migrate and deploy data across various systems. You will collaborate with cross-functional teams to understand data requirements and contribute to the overall data strategy of the organization, ensuring that data solutions are efficient, scalable, and aligned with business objectives.\nRoles & Responsibilities:- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Collaborate with stakeholders to gather and analyze data requirements.- Design and implement robust data pipelines to support data processing and analytics.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Strong understanding of data modeling and database design principles.- Experience with ETL tools and data integration techniques.- Familiarity with cloud platforms and services related to data storage and processing.- Knowledge of programming languages such as Python or Scala for data manipulation.\nAdditional Information:- The candidate should have minimum 3 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Pune office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'data analytics', 'database design', 'data modeling', 'design principles', 'hive', 'scala', 'data manipulation', 'data processing', 'pyspark', 'data warehousing', 'data engineering', 'sql', 'data quality', 'tableau', 'etl tool', 'spark', 'hadoop', 'etl', 'big data', 'data integration', 'etl process']",2025-06-13 05:09:56
Data Engineer,Accenture,3 - 8 years,Not Disclosed,['Indore'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :Business AgilityMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions that facilitate data generation, collection, and processing. Your typical day will involve creating data pipelines, ensuring data quality, and implementing ETL processes to migrate and deploy data across various systems. You will collaborate with cross-functional teams to understand data requirements and deliver effective solutions that meet business needs. Additionally, you will monitor and optimize data workflows to enhance performance and reliability, ensuring that data is accessible and actionable for stakeholders.\nRoles & Responsibilities:- Need Databricks resource with Azure cloud experience- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Collaborate with data architects and analysts to design scalable data solutions.- Implement best practices for data governance and security throughout the data lifecycle.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Good To Have\n\n\n\n\nSkills:\nExperience with Business Agility.- Strong understanding of data modeling and database design principles.- Experience with data integration tools and ETL processes.- Familiarity with cloud platforms and services related to data storage and processing.\nAdditional Information:- The candidate should have minimum 3 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Pune office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analytics', 'database design', 'data modeling', 'design principles', 'etl', 'hive', 'python', 'data warehousing', 'power bi', 'machine learning', 'data engineering', 'sql server', 'sql', 'data bricks', 'data quality', 'tableau', 'spark', 'data governance', 'hadoop', 'big data', 'aws', 'ssis', 'etl process']",2025-06-13 05:09:58
Data Engineer,Accenture,3 - 8 years,Not Disclosed,['Bhubaneswar'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :Business AgilityMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions that facilitate data generation, collection, and processing. Your typical day will involve creating data pipelines, ensuring data quality, and implementing ETL processes to migrate and deploy data across various systems. You will collaborate with cross-functional teams to understand data requirements and deliver effective solutions that meet business needs. Additionally, you will monitor and optimize data workflows to enhance performance and reliability, ensuring that data is accessible and actionable for stakeholders.\nRoles & Responsibilities:- Need Databricks resource with Azure cloud experience- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Collaborate with data architects and analysts to design scalable data solutions.- Implement best practices for data governance and security throughout the data lifecycle.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Good To Have\n\n\n\n\nSkills:\nExperience with Business Agility.- Strong understanding of data modeling and database design principles.- Experience with data integration tools and ETL processes.- Familiarity with cloud platforms and services related to data storage and processing.\nAdditional Information:- The candidate should have minimum 3 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Pune office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analytics', 'database design', 'data modeling', 'design principles', 'etl', 'hive', 'python', 'data warehousing', 'power bi', 'machine learning', 'data engineering', 'sql server', 'sql', 'data bricks', 'data quality', 'tableau', 'spark', 'data governance', 'hadoop', 'big data', 'aws', 'ssis', 'etl process']",2025-06-13 05:10:00
Data Engineer,Accenture,3 - 8 years,Not Disclosed,['Bhubaneswar'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :Business AgilityMinimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions that facilitate data generation, collection, and processing. Your typical day will involve creating data pipelines, ensuring data quality, and implementing ETL processes to migrate and deploy data across various systems. You will collaborate with cross-functional teams to understand data requirements and deliver effective solutions that meet business needs. Additionally, you will monitor and optimize data workflows to enhance performance and reliability, ensuring that data is accessible and actionable for stakeholders.\nRoles & Responsibilities:- Need Databricks resource with Azure cloud experience- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Collaborate with data architects and analysts to design scalable data solutions.- Implement best practices for data governance and security throughout the data lifecycle.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Good To Have\n\n\n\n\nSkills:\nExperience with Business Agility.- Strong understanding of data modeling and database design principles.- Experience with data integration tools and ETL processes.- Familiarity with cloud platforms and services related to data storage and processing.\nAdditional Information:- The candidate should have minimum 3 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Pune office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data analytics', 'database design', 'data modeling', 'design principles', 'etl', 'hive', 'python', 'data warehousing', 'power bi', 'machine learning', 'data engineering', 'sql server', 'sql', 'data bricks', 'data quality', 'tableau', 'spark', 'data governance', 'hadoop', 'big data', 'aws', 'ssis', 'etl process']",2025-06-13 05:10:02
Data Engineer,Accenture,3 - 8 years,Not Disclosed,['Pune'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :Business AgilityMinimum\n\n\n\n12 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions that facilitate data generation, collection, and processing. Your typical day will involve creating data pipelines, ensuring data quality, and implementing ETL processes to migrate and deploy data across various systems. You will collaborate with cross-functional teams to understand data requirements and deliver effective solutions that meet business needs. Additionally, you will monitor and optimize data workflows to enhance performance and reliability, ensuring that data is accessible and actionable for stakeholders.\nRoles & Responsibilities:- Need Databricks resource with Azure cloud experience- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Collaborate with data architects and analysts to design scalable data solutions.- Implement best practices for data governance and security throughout the data lifecycle.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Good To Have\n\n\n\n\nSkills:\nExperience with Business Agility.- Strong understanding of data modeling and database design principles.- Experience with data integration tools and ETL processes.- Familiarity with cloud platforms and services related to data storage and processing.\nAdditional Information:- The candidate should have minimum 3 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Pune office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data analytics', 'database design', 'data modeling', 'design principles', 'etl', 'hive', 'python', 'data warehousing', 'power bi', 'machine learning', 'data engineering', 'sql server', 'sql', 'data bricks', 'data quality', 'tableau', 'spark', 'data governance', 'hadoop', 'big data', 'aws', 'ssis', 'etl process']",2025-06-13 05:10:03
Data Engineer,Accenture,3 - 8 years,Not Disclosed,['Chennai'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :Business AgilityMinimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions that facilitate data generation, collection, and processing. Your typical day will involve creating data pipelines, ensuring data quality, and implementing ETL processes to migrate and deploy data across various systems. You will collaborate with cross-functional teams to understand data requirements and deliver effective solutions that meet business needs. Additionally, you will monitor and optimize data workflows to enhance performance and reliability, ensuring that data is accessible and actionable for stakeholders.\nRoles & Responsibilities:- Need Databricks resource with Azure cloud experience- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Collaborate with data architects and analysts to design scalable data solutions.- Implement best practices for data governance and security throughout the data lifecycle.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Good To Have\n\n\n\n\nSkills:\nExperience with Business Agility.- Strong understanding of data modeling and database design principles.- Experience with data integration tools and ETL processes.- Familiarity with cloud platforms and services related to data storage and processing.\nAdditional Information:- The candidate should have minimum 3 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Pune office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analytics', 'database design', 'data modeling', 'design principles', 'etl', 'hive', 'python', 'data warehousing', 'power bi', 'machine learning', 'data engineering', 'sql server', 'sql', 'data bricks', 'data quality', 'tableau', 'spark', 'data governance', 'hadoop', 'big data', 'aws', 'ssis', 'etl process']",2025-06-13 05:10:05
Data Engineer,Accenture,3 - 8 years,Not Disclosed,['Hyderabad'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :Business AgilityMinimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions that facilitate data generation, collection, and processing. Your typical day will involve creating data pipelines, ensuring data quality, and implementing ETL processes to migrate and deploy data across various systems. You will collaborate with cross-functional teams to understand data requirements and deliver effective solutions that meet business needs. Additionally, you will monitor and optimize data workflows to enhance performance and reliability, ensuring that data is accessible and actionable for stakeholders.\nRoles & Responsibilities:- Need Databricks resource with Azure cloud experience- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Collaborate with data architects and analysts to design scalable data solutions.- Implement best practices for data governance and security throughout the data lifecycle.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Good To Have\n\n\n\n\nSkills:\nExperience with Business Agility.- Strong understanding of data modeling and database design principles.- Experience with data integration tools and ETL processes.- Familiarity with cloud platforms and services related to data storage and processing.\nAdditional Information:- The candidate should have minimum 3 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Pune office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data analytics', 'database design', 'data modeling', 'design principles', 'etl', 'hive', 'python', 'data warehousing', 'power bi', 'machine learning', 'data engineering', 'sql server', 'sql', 'data bricks', 'data quality', 'tableau', 'spark', 'data governance', 'hadoop', 'big data', 'aws', 'ssis', 'etl process']",2025-06-13 05:10:07
Data Engineer,Accenture,3 - 8 years,Not Disclosed,['Navi Mumbai'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :Business AgilityMinimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions that facilitate data generation, collection, and processing. Your typical day will involve creating data pipelines, ensuring data quality, and implementing ETL processes to migrate and deploy data across various systems. You will collaborate with cross-functional teams to understand data requirements and deliver effective solutions that meet business needs. Additionally, you will monitor and optimize data workflows to enhance performance and reliability, ensuring that data is accessible and actionable for stakeholders.\nRoles & Responsibilities:- Need Databricks resource with Azure cloud experience- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Collaborate with data architects and analysts to design scalable data solutions.- Implement best practices for data governance and security throughout the data lifecycle.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Good To Have\n\n\n\n\nSkills:\nExperience with Business Agility.- Strong understanding of data modeling and database design principles.- Experience with data integration tools and ETL processes.- Familiarity with cloud platforms and services related to data storage and processing.\nAdditional Information:- The candidate should have minimum 3 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Pune office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analytics', 'database design', 'data modeling', 'design principles', 'etl', 'hive', 'python', 'data warehousing', 'power bi', 'machine learning', 'data engineering', 'sql server', 'sql', 'data bricks', 'data quality', 'tableau', 'spark', 'data governance', 'hadoop', 'big data', 'aws', 'ssis', 'etl process']",2025-06-13 05:10:09
Data Engineer,Capgemini,6 - 9 years,Not Disclosed,['Hyderabad'],"\nDesign, implement, and maintain data pipelines for data ingestion, processing, and transformation in Azure.\nWork together with data scientists and analysts to understand the needs for data and create effective data workflows.\nCreate and maintain data storage solutions including Azure SQL Database, Azure Data Lake, and Azure Blob Storage.\nUtilizing Azure Data Factory or comparable technologies, create and maintain ETL (Extract, Transform, Load) operations.\nImplementing data validation and cleansing procedures will ensure the quality, integrity, and dependability of the data.\nImprove the scalability, efficiency, and cost-effectiveness of data pipelines.\nMonitoring and resolving data pipeline problems will guarantee consistency and availability of the data.\nWorks in the area of Software Engineering, which encompasses the development, maintenance and optimization of software solutions/applications.1. Applies scientific methods to analyse and solve software engineering problems.2. He/she is responsible for the development and application of software engineering practice and knowledge, in research, design, development and maintenance.3. His/her work requires the exercise of original thought and judgement and the ability to supervise the technical and administrative work of other software engineers.4. The software engineer builds skills and expertise of his/her software engineering discipline to reach standard software engineer skills expectations for the applicable role, as defined in Professional Communities.5. The software engineer collaborates and acts as team player with other software engineers and stakeholders.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['azure data lake', 'azure data factory', 'sql', 'azure blob storage', 'sql azure', 'hive', 'azure databricks', 'python', 'data validation', 'pyspark', 'data warehousing', 'power bi', 'data engineering', 'spark', 'data ingestion', 'software engineering', 'hadoop', 'etl', 'big data', 'aws', 'sql database']",2025-06-13 05:10:11
Data Engineer,Accenture,15 - 20 years,Not Disclosed,['Hyderabad'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Microsoft Azure Data Services\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n2 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions that facilitate data generation, collection, and processing. Your typical day will involve creating data pipelines, ensuring data quality, and implementing ETL processes to effectively migrate and deploy data across various systems, contributing to the overall efficiency and reliability of data management within the organization.\nRoles & Responsibilities:- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Collaborate with cross-functional teams to gather requirements and deliver data solutions that meet business needs.- Monitor and optimize data pipelines for performance and reliability.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Microsoft Azure Data Services.- Good To Have\n\n\n\n\nSkills:\nExperience with Azure Data Factory, Azure SQL Database, and Azure Synapse Analytics.- Strong understanding of data modeling and database design principles.- Experience with data integration and ETL tools.- Familiarity with data governance and data quality best practices.\nAdditional Information:- The candidate should have minimum 2 years of experience in Microsoft Azure Data Services.- This position is based at our Hyderabad office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data services', 'microsoft azure', 'database design', 'data modeling', 'design principles', 'python', 'data management', 'azure synapse', 'azure data factory', 'data engineering', 'sql', 'data quality', 'sql azure', 'etl tool', 'data governance', 'etl', 'data integration', 'etl process', 'sql database']",2025-06-13 05:10:13
Data Engineer,Accenture,3 - 8 years,Not Disclosed,['Indore'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Google BigQuery\n\n\n\n\nGood to have skills :Microsoft SQL Server, Google Cloud Data ServicesMinimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will be responsible for designing, developing, and maintaining data solutions for data generation, collection, and processing. You will create data pipelines, ensure data quality, and implement ETL processes to migrate and deploy data across systems.\nRoles & Responsibilities:- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Develop and maintain data pipelines.- Ensure data quality throughout the data lifecycle.- Implement ETL processes for data migration and deployment.- Collaborate with cross-functional teams to understand data requirements.- Optimize data storage and retrieval processes.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Google BigQuery.- Strong understanding of data engineering principles.- Experience with cloud-based data services.- Knowledge of SQL and database management systems.- Hands-on experience with data modeling and schema design.\nAdditional Information:- The candidate should have a minimum of 3 years of experience in Google BigQuery.- This position is based at our Mumbai office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data engineering', 'sql', 'data modeling', 'bigquery', 'etl', 'schema', 'hive', 'data services', 'python', 'amazon redshift', 'data warehousing', 'google', 'data migration', 'knowledge of sql', 'sql server', 'database design', 'data quality', 'tableau', 'spark', 'etl tool', 'hadoop', 'big data', 'aws', 'etl process']",2025-06-13 05:10:15
Data Engineer,Accenture,2 - 3 years,Not Disclosed,['Kochi'],"Job Title - Data Engineer Sr.Analyst ACS SONG\n\n\n\nManagement Level:Level 10 Sr. Analyst\n\n\n\nLocation:Kochi, Coimbatore, Trivandrum\n\n\n\nMust have skills:Python/Scala, Pyspark/Pytorch\n\n\n\n\nGood to have skills:Redshift\n\n\n\n\n\n\n\nJob\n\n\nSummary\n\nYoull capture user requirements and translate them into business and digitally enabled solutions across a range of industries. Your responsibilities will include:\n\n\n\nRoles and Responsibilities\n\nDesigning, developing, optimizing, and maintaining data pipelines that adhere to ETL principles and business goals\n\nSolving complex data problems to deliver insights that helps our business to achieve their goals.\n\nSource data (structured unstructured) from various touchpoints, format and organize them into an analyzable format.\n\nCreating data products for analytics team members to improve productivity\n\nCalling of AI services like vision, translation etc. to generate an outcome that can be used in further steps along the pipeline.\n\nFostering a culture of sharing, re-use, design and operational efficiency of data and analytical solutions\n\nPreparing data to create a unified database and build tracking solutions ensuring data quality\n\nCreate Production grade analytical assets deployed using the guiding principles of CI/CD.\n\n\n\n\nProfessional and Technical Skills\n\nExpert in Python, Scala, Pyspark, Pytorch, Javascript (any 2 at least)\n\nExtensive experience in data analysis (Big data- Apache Spark environments), data libraries (e.g. Pandas, SciPy, Tensorflow, Keras etc.), and SQL. 2-3 years of hands-on experience working on these technologies.\n\nExperience in one of the many BI tools such as Tableau, Power BI, Looker.\n\nGood working knowledge of key concepts in data analytics, such as dimensional modeling, ETL, reporting/dashboarding, data governance, dealing with structured and unstructured data, and corresponding infrastructure needs.\n\nWorked extensively in Microsoft Azure (ADF, Function Apps, ADLS, Azure SQL), AWS (Lambda,Glue,S3), Databricks analytical platforms/tools, Snowflake Cloud Datawarehouse.\n\n\n\n\nAdditional Information\n\nExperience working in cloud Data warehouses like Redshift or Synapse\n\nCertification in any one of the following or equivalent\n\nAWS- AWS certified data Analytics- Speciality\n\nAzure- Microsoft certified Azure Data Scientist Associate\n\nSnowflake- Snowpro core- Data Engineer\n\nDatabricks Data Engineering\n\n\nAbout Our Company | Accenture (do not remove the hyperlink)\n\n\n\nQualification\n\n\n\nExperience:3.5 -5 years of experience is required\n\n\n\n\nEducational Qualification:Graduation (Accurate educational details should capture)",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['scala', 'pyspark', 'pytorch', 'python', 'microsoft azure', 'glue', 'amazon redshift', 'sql', 'tensorflow', 'sql azure', 'spark', 'keras', 'big data', 'etl', 'snowflake', 'scipy', 'data analysis', 'azure data lake', 'power bi', 'data engineering', 'javascript', 'data bricks', 'pandas', 'tableau', 'lambda expressions', 'aws']",2025-06-13 05:10:17
Data Engineer,Accenture,5 - 10 years,Not Disclosed,['Chennai'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL processes to migrate and deploy data across systems. Your day will involve working on data architecture and engineering tasks to support business operations and decision-making.\nRoles & Responsibilities:- Expected to be an SME, collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Develop and maintain data pipelines for efficient data processing.- Implement ETL processes to ensure seamless data migration and deployment.- Collaborate with cross-functional teams to design and optimize data solutions.- Conduct data quality assessments and implement improvements for data integrity.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Strong understanding of data architecture principles.- Experience in designing and implementing data solutions.- Proficient in SQL and other data querying languages.- Knowledge of cloud platforms such as AWS or Azure.\nAdditional Information:- The candidate should have a minimum of 5 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Chennai office.- A 15 years full-time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analytics', 'sql', 'data architecture principles', 'etl', 'aws', 'hive', 'python', 'data processing', 'airflow', 'microsoft azure', 'pyspark', 'data warehousing', 'data integrity', 'data migration', 'data engineering', 'data quality', 'spark', 'hadoop', 'business operations', 'big data', 'etl process']",2025-06-13 05:10:19
Data Engineer,Accenture,3 - 8 years,Not Disclosed,['Coimbatore'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Talend ETL\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL processes to migrate and deploy data across systems. Be involved in the end-to-end data management process.\nRoles & Responsibilities:- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work-related problems.- Develop and maintain data pipelines for efficient data processing.- Ensure data quality and integrity throughout the data lifecycle.- Implement ETL processes to extract, transform, and load data.- Collaborate with cross-functional teams to optimize data solutions.- Conduct data analysis to identify trends and insights.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Talend ETL.- Strong understanding of data integration and ETL processes.- Experience with data modeling and database design.- Knowledge of SQL and database querying languages.- Hands-on experience with data warehousing concepts.\nAdditional Information:- The candidate should have a minimum of 3 years of experience in Talend ETL.- This position is based at our Hyderabad office.- A 15 years full-time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['sql', 'talend etl', 'etl', 'data integration', 'etl process', 'hive', 'python', 'data analysis', 'data management', 'talend', 'data processing', 'data warehousing', 'knowledge of sql', 'data engineering', 'database design', 'data quality', 'data modeling', 'spark', 'data warehousing concepts', 'hadoop']",2025-06-13 05:10:21
Data Engineer,AMERICAN EXPRESS,2 - 4 years,13-17 Lacs P.A.,"['Gurugram', 'Delhi / NCR']","Role & responsibilities\nUnderstanding business use cases and be able to convert to technical design\nPart of a cross-disciplinary team, working closely with other data engineers, software engineers, data scientists, data managers and business partners.\nYou will be designing scalable, testable and maintainable data pipelines\nIdentify areas for data governance improvements and help to resolve data quality problems through the appropriate choice of error detection and correction, process control and improvement, or process design changes",,,,"['Spark', 'SQL', 'Python', 'Hadoop', 'Big Data']",2025-06-13 05:10:22
Data Engineer,Grid Dynamics,4 - 9 years,Not Disclosed,['Bengaluru'],"Required Qualifications:\n4+ years of professional experience in data engineering and data analysis roles.\nStrong proficiency in SQL and experience with database management systems such as MySQL, PostgreSQL, Oracle, and MongoDB.\nHands-on experience with big data tools like Hadoop and Apache Spark.\nProficient in Python programming.\nExperience with data visualization tools such as Tableau, Power BI, and Jupyter Notebooks.\nProven ability to design, build, and maintain scalable ETL pipelines using tools like Apache Airflow, DBT, Composer (GCP), Control-M, Cron, and Luigi.\nFamiliarity with data engineering tools including Hive, Kafka, Informatica, Talend, SSIS, and Dataflow.\nExperience working with cloud data warehouses and services (Snowflake, Redshift, BigQuery, AWS Glue, GCP Dataflow, Azure Data Factory).\nUnderstanding of data modeling concepts and data lake/data warehouse architectures.\nExperience supporting CI/CD practices with Git, Docker, Terraform, and DevOps workflows.\nKnowledge of both relational and NoSQL databases, including PostgreSQL, BigQuery, MongoDB, and DynamoDB.\nExposure to Agile and DevOps methodologies.\nExperience with at least one cloud platform:\nGoogle Cloud Platform (BigQuery, Dataflow, Composer, Cloud Storage, Pub/Sub)\nAmazon Web Services (S3, Glue, Redshift, Lambda, Athena)\nMicrosoft Azure (Data Factory, Synapse Analytics, Blob Storage)\nEssential functions\nKey Responsibilities:\nDesign, develop, and maintain robust, scalable ETL pipelines using Apache Airflow, DBT, Composer (GCP), Control-M, Cron, Luigi, and similar tools.\nBuild and optimize data architectures including data lakes and data warehouses.\nIntegrate data from multiple sources ensuring data quality and consistency.\nCollaborate with data scientists, analysts, and stakeholders to translate business requirements into technical solutions.\nAnalyze complex datasets to identify trends, generate actionable insights, and support decision-making.\nDevelop and maintain dashboards and reports using Tableau, Power BI, and Jupyter Notebooks for visualization and pipeline validation.\nManage and optimize relational and NoSQL databases such as MySQL, PostgreSQL, Oracle, MongoDB, and DynamoDB.\nWork with big data tools and frameworks including Hadoop, Spark, Hive, Kafka, Informatica, Talend, SSIS, and Dataflow.\nUtilize cloud data services and warehouses like AWS Glue, GCP Dataflow, Azure Data Factory, Snowflake, Redshift, and BigQuery.\nSupport CI/CD pipelines and DevOps workflows using Git, Docker, Terraform, and related tools.\nEnsure data governance, security, and compliance standards are met.\nParticipate in Agile and DevOps processes to enhance data engineering workflows.\nQualifications\nData Engineer with experience in MySQL or SQL or PL/SQL and any cloud experience like GCP or AWS or Azure\nWould be a plus\nPreferred Skills:\nStrong problem-solving and communication skills.\nAbility to work independently and collaboratively in a team environment.\nExperience with service development, REST APIs, and automation testing is a plus.\nFamiliarity with version control systems and workflow automation.\nWe offer\nOpportunity to work on bleeding-edge projects\nWork with a highly motivated and dedicated team\nCompetitive salary\nFlexible schedule\nBenefits package - medical insurance, sports\nCorporate social events\nProfessional development opportunities\nWell-equipped office",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data analysis', 'Automation', 'Data modeling', 'MySQL', 'Workflow', 'Informatica', 'Oracle', 'Apache', 'SSIS', 'Analytics']",2025-06-13 05:10:24
Data Engineer,Infiniti Research,3 - 7 years,22.5-25 Lacs P.A.,['Bengaluru'],"Role & responsibilities\n3-6 years of experience in Data Engineering Pipeline Ownership and Quality Assurance, with hands-on expertise in building, testing, and maintaining data pipelines.\nProficiency with Azure Data Factory (ADF), Azure Databricks (ADB), and PySpark for data pipeline orchestration and processing large-scale datasets.\nStrong experience in writing SQL queries and performing data validation, data profiling, and schema checks.\nExperience with big data validation, including schema enforcement, data integrity checks, and automated anomaly detection.\nAbility to design, develop, and implement automated test cases to monitor and improve data pipeline efficiency.\nDeep understanding of Medallion Architecture (Raw, Bronze, Silver, Gold) for structured data flow management.\nHands-on experience with Apache Airflow for scheduling, monitoring, and managing workflows.\nStrong knowledge of Python for developing data quality scripts, test automation, and ETL validations.\nFamiliarity with CI/CD pipelines for deploying and automating data engineering workflows.\nSolid data governance and data security practices within the Azure ecosystem.\n\nAdditional Requirements:\nOwnership of data pipelines ensuring end-to-end execution, monitoring, and troubleshooting failures proactively.\nStrong stakeholder management skills, including follow-ups with business teams across multiple regions to gather requirements, address issues, and optimize processes.\nTime flexibility to align with global teams for efficient communication and collaboration.\nExcellent problem-solving skills with the ability to simulate and test edge cases in data processing environments.\nStrong communication skills to document and articulate pipeline issues, troubleshooting steps, and solutions effectively.\nExperience with Unity Catalog or willingness to learn.\n\nPreferred candidate profile\nImmediate Joiner's",Industry Type: Analytics / KPO / Research,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['ADF', 'pyspark', 'Unity Catalog', 'ADB', 'SQL', 'Medallion Architecture']",2025-06-13 05:10:26
Data Engineer,Grid Dynamics,4 - 9 years,Not Disclosed,['Bengaluru'],"Qualifications we are looking for\nMaster/Bachelor degree in Computer Science, Electrical Engineering, Information Systems or other technical discipline; advanced degree preferred.\nMinimum of 7+ years of software development experience (with a concentration in data centric initiatives), with demonstrated expertise in leveraging standard development best practice methodologies.\nMinimum 4+ years of experience in Hadoop using Core Java Programming, Spark, Scala, Hive and Go lang\nExpertise in Object Oriented Programming Language Java\nExperience using CI/CD Process, version control and bug tracking tools.\nExperience in handling very large data volume in Real Time and batch mode.\nExperience with automation of job execution and validation\nStrong knowledge of Database concepts\nStrong team player.\nStrong communication skills with proven ability to present complex ideas and document in a clear and concise way.\nQuick learner; self-starter, detailed and in-depth.",Industry Type: IT Services & Consulting,Department: Other,"Employment Type: Full Time, Permanent","['Data Engineering', 'SCALA', 'Bigdata Technologies', 'Spark']",2025-06-13 05:10:27
Data Engineer,Dun & Bradstreet,5 - 9 years,Not Disclosed,['Hyderabad'],"Key Responsibilities:\n1. Design, build, and deploy new data pipelines within our Big Data Eco-Systems using Streamsets/Talend/Informatica BDM etc. Document new/existing pipelines, Datasets.\n2. Design ETL/ELT data pipelines using StreamSets, Informatica or any other ETL processing engine. Familiarity with Data Pipelines, Data Lakes and modern Data Warehousing practices (virtual data warehouse, push down analytics etc.)\n3. Expert level programming skills on Python\n4. Expert level programming skills on Spark\n5. Cloud Based Infrastructure: GCP\n6. Experience with one of the ETL Informatica, StreamSets in creation of complex parallel loads, Cluster Batch Execution and dependency creation using Jobs/Topologies/Workflows etc.,\n7. Experience in SQL and conversion of SQL stored procedures into Informatica/StreamSets, Strong exposure working with web service origins/targets/processors/executors, XML/JSON Sources and Restful APIs.\n8. Strong exposure working with relation databases DB2, Oracle & SQL Server including complex SQL constructs and DDL generation.\n9. Exposure to Apache Airflow for scheduling jobs\n10. Strong knowledge of Big data Architecture (HDFS), Cluster installation, configuration, monitoring, cluster security, cluster resources management, maintenance, and performance tuning\n11. Create POCs to enable new workloads and technical capabilities on the Platform.\n12. Work with the platform and infrastructure engineers to implement these capabilities in production.\n13. Manage workloads and enable workload optimization including managing resource allocation and scheduling across multiple tenants to fulfill SLAs.\n14. Participate in planning activities, Data Science and perform activities to increase platform skills\n\nKey Requirements:\n1. Minimum 6 years of experience in ETL/ELT Technologies, preferably StreamSets/Informatica/Talend etc.,\n2. Minimum of 6 years hands-on experience with Big Data technologies e.g. Hadoop, Spark, Hive.\n3. Minimum 3+ years of experience on Spark\n4. Minimum 3 years of experience in Cloud environments, preferably GCP\n5. Minimum of 2 years working in a Big Data service delivery (or equivalent) roles focusing on the following disciplines:\n6. Any experience with NoSQL and Graph databases\n7. Informatica or StreamSets Data integration (ETL/ELT)\n8. Exposure to role and attribute based access controls\n9. Hands on experience with managing solutions deployed in the Cloud, preferably on GCP\n10. Experience working in a Global company, working in a DevOps model is a plus",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Airflow', 'GCP', 'Data engineer', 'Spark', 'ETL']",2025-06-13 05:10:29
Data Engineer || Paisabazaar || Gurgaon,Paisabazaar,3 - 5 years,Not Disclosed,['Gurugram'],"Qualifications for Data Engineer :\n3+ Years of experience in building and optimizing big data solutions required to fulfill business and technology requirements.\n4+ years of technical expertise in areas of design and implementation using big data technology Hadoop, Hive, Spark, Python/Java.\nStrong analytic skills to understand and create solutions for business use cases.\nEnsure best practices to implement data governance principles, data quality checks on each data layer.",,,,"['Pyspark', 'Data Engineering', 'Hadoop', 'Hive', 'Java', 'Scala Programming', 'Big Data', 'SQL', 'Azure Cloud', 'GCP', 'Spark', 'AWS', 'Python']",2025-06-13 05:10:31
DataBricks - Data Engineering Professional,Wipro,3 - 5 years,Not Disclosed,['Pune'],"Role Purpose\nThe purpose of this role is to design, test and maintain software programs for operating systems or applications which needs to be deployed at a client end and ensure its meet 100% quality assurance parameters\n\nDo\n1. Instrumental in understanding the requirements and design of the product/ software\n\n\n\n\n\n\n\n\n\n\nMandatory Skills: DataBricks - Data Engineering. Experience: 3-5 Years.",,,,"['Data Engineering', 'data bricks', 'software development life cycle', 'continuous integration', 'software development', 'mis', 'software management', 'root cause analysis']",2025-06-13 05:10:33
Data Engineer,ZS,1 - 6 years,Not Disclosed,['Pune'],"Create and maintain optimal data pipeline architecture.\nIdentify, design, and implement internal process improvements, automating manual processes, optimizing data delivery, re-designing infrastructure for scalability.\nDesign, develop and deploy high volume ETL pipelines to manage complex and near-real time data collection.\nDevelop and optimize SQL queries and stored procedures to meet business requirements.\nDesign, implement, and maintain REST APIs for data interaction between systems.\nEnsure performance, security, and availability of databases.",,,,"['Root cause analysis', 'SQL database', 'Management consulting', 'Financial planning', 'Data collection', 'Manager Technology', 'Engineering Manager', 'Information technology', 'Analytics']",2025-06-13 05:10:35
Data Engineer,Kinara Capital,0 - 1 years,3-5 Lacs P.A.,['Bengaluru'],"1\nAbout Company\nKinara Capital is a FinTech NBFC dedicated to driving Financial Inclusion\nin the MSME sector. Our mission is to transform lives, livelihoods, and\nlocal economies by providing fast and flexible loans without property\ncollateral to small business entrepreneurs. Led by a women-majority\nmanagement team, Kinara Capital values diversity and inclusion and\nfosters a collaborative working environment.\nKinara Capital is the only company from India recognized globally by the\nWorld Bank/IFC with a gold award in 2019 as Bank of the Year-Asia’ for\nour innovative work in SME financing. Kinara Capital is an RBI-registered\nSystemically Important NBFC.\nHeadquartered in Bangalore, we have 110 branches across Karnataka,\nGujarat, Maharashtra, Andhra Pradesh, Telangana, Tamil Nadu, and UT\nPuducherry with more than 1000 employees. https://kinaracapital.com/\nTitle:\nData Engineer\nTeam:\nData Warehouse Team\nPurpose of Job:\nThis is a hands-on coding and designing position and we are looking for\nan exceptionally talented data engineer who has exposure in\nimplementing AWS services to build data pipelines, api integration and\ndesigning data warehouse.\nJob Responsibilities:\nExcellent coding skills in Python, PySpark, SQL.\nHave extensive experience in Spark ecosystem and has\nworked on both real time and batch processing\nHave experience in AWS Glue, EMR, DMS, Lambda, S3,\nDynamoDB, Step functions, Airflow, RDS, Aurora etc.\nExperience with modern Database systems such as\nRedshift, Presto, Hive etc.\nWorked on building data lakes in the past on S3 or\nApache Hudi\nSolid understanding of Data Warehousing Concepts\nGood to have experience on tools such as Kafka or Kinesis\nGood to have AWS Developer Associate or Solutions\nArchitect Associate Certification\nQualifications:\nAt least a bachelor’s degree in Science, Engineering, Applied\nMathematics.\nOther Requirements:\nLearning Attitude and good communication skills\nReport to:\nLead Data Engineer\nPlace of work:\nHead office, Bangalore.\nJob Type:\nFull Time\nNo. of Posts:\n2",Industry Type: FinTech / Payments,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Amazon Redshift', 'Data Warehousing', 'Spark', 'Python', 'SQL', 'Snowflake', 'ETL']",2025-06-13 05:10:37
DE&A - Core - Big Data Engineering - ETL Orchestration DE&A - Core,Zensar,5 - 9 years,Not Disclosed,"['Kolkata', 'Mumbai', 'New Delhi', 'Hyderabad', 'Pune', 'Chennai', 'Bengaluru']","Design of complex ETL interfaces with agnostic tool set for various source/target types (SAP BODS preferred)\nPerformance Tuning & Troubleshooting skills across all technologies used\nStrong in DB and SQLs with decent data modeling skills\nAbility to lead developers and QA and adhere to committed timelines\nAgile experience is preferred\nAbility to work with upstream, downstream, and reporting system stakeholders and convert business requirements into technical requirements (JIRA stories)\n10 years of experience in:\n- ETL tools\n- Repository maintenance\n- Migration of jobs/workflows/dataflows\n- Job monitoring, break fix, user maintenance\n- CDC (Change Data Capture)\n- Oracle Golden Gate\n- Working with different sources/targets (Oracle, SAP, Files as source; SQL Server, file uploads as targets)\nRole Description :\n- Responsible for planning, developing, implementing, and managing data warehouse project deliverables\n- Design documents and high-level data mapping for ETL specifications\n- Create transformations in ETL jobs to achieve data cleansing and standardizations during initial data load\n- Provide support for integration testing, defect fixing, and deployment\nProficient knowledge in:\n- Transformations like Query Push down, SQL Table Comparison, Pivot, Look up, etc.\n- Databases like Oracle, SQL Queries, SQL Server\n- Dimensional modeling, data mining, and data warehouse concepts\nExcellent analytical skills\nKnowledge to transfer data from Oracle, SQL Server tables, and Web Services either incrementally (Delta Load) or full load to the data warehouse on a periodic basis\nTroubleshooting existing ETL jobs and improving performance of existing jobs\nCreating and loading data into aggregate tables using transformations\nAbility to perform tasks individually and independently\nDesign of complex ETL interfaces with agnostic tool set for various source/target types (SAP BODS preferred)\nPerformance Tuning & Troubleshooting skills across all technologies used\nStrong in DB and SQLs with decent data modeling skills\nAbility to lead developers and QA and adhere to committed timelines\nAgile experience is preferred\nAbility to work with upstream, downstream, and reporting system stakeholders and convert business requirements into technical requirements (JIRA stories)\n10 years of experience in:\n- ETL tools\n- Repository maintenance\n- Migration of jobs/workflows/dataflows\n- Job monitoring, break fix, user maintenance\n- CDC (Change Data Capture)\n- Oracle Golden Gate\n- Working with different sources/targets (Oracle, SAP, Files as source; SQL Server, file uploads as targets)\nRole Description :\n- Responsible for planning, developing, implementing, and managing data warehouse project deliverables\n- Design documents and high-level data mapping for ETL specifications\n- Create transformations in ETL jobs to achieve data cleansing and standardizations during initial data load\n- Provide support for integration testing, defect fixing, and deployment\nProficient knowledge in:\n- Transformations like Query Push down, SQL Table Comparison, Pivot, Look up, etc.\n- Databases like Oracle, SQL Queries, SQL Server\n- Dimensional modeling, data mining, and data warehouse concepts\nExcellent analytical skills\nKnowledge to transfer data from Oracle, SQL Server tables, and Web Services either incrementally (Delta Load) or full load to the data warehouse on a periodic basis\nTroubleshooting existing ETL jobs and improving performance of existing jobs\nCreating and loading data into aggregate tables using transformations\nAbility to perform tasks individually and independently",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Performance tuning', 'SAP', 'Data modeling', 'Integration testing', 'Agile', 'data mapping', 'Troubleshooting', 'Data mining', 'JIRA', 'Monitoring']",2025-06-13 05:10:38
Senior Cloud Data Engineer,PwC India,10 - 15 years,12-22 Lacs P.A.,['Bengaluru'],"Role & responsibilities\n\nExperienced Senior Data Engineer utilizing Big Data & Google Cloud technologies to develop large scale, on-cloud data processing pipelines and data warehouses with Overall 12 to 15 years of experience\nHave 3 to 4 years of experience of leading Data Engineer teams developing enterprise grade data processing pipelines on multi Clouds like GCP and AWS",,,,"['Pyspark', 'Python', 'SQL', 'Datafactory', 'GCP', 'Microsoft Azure', 'ETL', 'AWS', 'Data Bricks']",2025-06-13 05:10:40
"Senior Data Engineer ( T-SQL & SSIS,Data Warehousing & ETL Specialist)",Synechron,5 - 10 years,Not Disclosed,"['Hyderabad', 'Pune', 'Bengaluru']","Job Summary\nSynechron is seeking a highly skilled Senior Data Engineer specializing in T-SQL and SSIS to lead and advance our data integration and warehousing initiatives. In this role, you will design, develop, and optimize complex ETL processes and database solutions to support enterprise data needs. Your expertise will enable efficient data flow, ensure data integrity, and facilitate actionable insights, contributing to our organizations commitment to data-driven decision-making and operational excellence.\nSoftware Requirements\nOverall Responsibilities\nTechnical Skills (By Category)\nExperience Requirements\nDay-to-Day Activities\nQualifications\nProfessional Competencies",,,,"['Data Engineering', 'T-SQL', 'Azure Data Factory', 'query optimization', 'performance tuning', 'database security', 'AWS Glue', 'Data Warehousing', 'SSIS', 'ETL']",2025-06-13 05:10:42
Sr. Data Engineer (Analyst- BI/Visualization),Visa,5 - 10 years,Not Disclosed,['Bengaluru'],"We are looking for a seasoned individual contributor who is comfortable engaging with business owners, data enthusiasts, and technology teams. Successful candidates have strong experience in developing complex data solutions for business intelligence applications. This role will be hands-on and focused on building data pipelines, business intelligence solutions at scale and with a focus on sustained operational excellence.\nHelp improve Visa s decision-making by making data more accessible and relevant to key partners\nDevelop deep partnerships with engineering, finance, and product teams to deliver on major cross-functional solution development\nWork with a team of talented data and analytics engineers with the ability to not only keep up with, but also pioneer, in this space.\nCreate extract, transform, load (ETL) processes for easy ingestion and use\nDevelop visualizations to make your sophisticated analyses accessible to a broad audience\nFind opportunities to create, automate and scale repeatable financial and statistical analysis\nProvide technical leadership in a team that generates business insights based on big data, identify impactful recommendations, and communicate the findings to clients\nBrainstorm innovative ways to use our unique data to answer business problems\nCollaborate with and influence leadership so that your teams work directly impacts company strategy and direction.\nCommunicate effectively to all levels of the organization, including executives.\n\n\nBasic Qualifications\n5 or more years of work experience with a Bachelors Degree or an Advanced Degree (e.g. Masters, MBA, JD, MD, or PhD)\nPreferred Qualifications\n5 or more years of work experience with a Bachelor s Degree or more than 2 years of work experience with an Advanced Degree (e.g. Masters, MBA, JD, MD)\n4+ years experience in data-based decision-making or quantitative analysis\nMaster s degree in Statistics, Operations Research, Applied Mathematics, Economics, Data Science, Business Analytics, Computer Science, or a related technical field\nExposure to Financial Services/ Payments data analytics and ETL development.\nExperience and expertise with data visualization using Tableau, Power BI or similar tools\nExperience in implementing ETL pipelines in Spark, Python, HIVE or SAS that process transaction and account level data and standardization of data pipelines.\nAdvanced experience in writing and optimizing efficient SQL queries with Python, Spark, Hive, Scala handling Large Data Sets in Big-Data Environments.\nExperience with Unix/Shell and exposure to Scheduling tools like Oozie and Airflow.\nStrong written, verbal, and interpersonal skills needed to effectively communicate technical insights and recommendations with business customers and leadership team.\nGood business acumen to orient data analysis to business needs of clients.\nAbility to translate data and technical concepts into requirements documents, business cases and user stories.\nAbility to learn new tools and paradigms as data science continues to evolve at Visa and elsewhere.\nDemonstrated intellectual and analytical rigor, team oriented, energetic, collaborative, diplomatic, and flexible style.\nPrevious experience with product valuations, financial engineering, customer lifetime values or net present value methodologies",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Unix', 'Computer science', 'Data analysis', 'SAS', 'Business analytics', 'Analytical', 'Scheduling', 'Business intelligence', 'Financial services', 'Python']",2025-06-13 05:10:44
IN Senior Associate Azure Data Engineer Data & Analaytics,PwC Service Delivery Center,2 - 5 years,Not Disclosed,['Bengaluru'],"Not Applicable\nSpecialism\nData, Analytics & AI\nManagement Level\nSenior Associate\n& Summary\nAt PwC, our people in data and analytics engineering focus on leveraging advanced technologies and techniques to design and develop robust data solutions for clients. They play a crucial role in transforming raw data into actionable insights, enabling informed decisionmaking and driving business growth.\n\nIn data engineering at PwC, you will focus on designing and building data infrastructure and systems to enable efficient data processing and analysis. You will be responsible for developing and implementing data pipelines, data integration, and data transformation solutions.\n& Summary A career within Data and Analytics services will provide you with the opportunity to help organisations uncover enterprise insights and drive business results using smarter data analytics. We focus on a collection of organisational technology capabilities, including business intelligence, data management, and data assurance that help our clients drive innovation, growth, and change within their organisations in order to keep up with the changing nature of customers and technology. We make impactful decisions by mixing mind and machine to leverage data, understand and navigate risk, and help our clients gain a competitive edge.\nResponsibilities\nDesign, develop, and optimize data pipelines and ETL processes using PySpark or Scala to extract, transform, and load large volumes of structured and unstructured data from diverse sources.\nImplement data ingestion, processing, and storage solutions on Azure cloud platform, leveraging services such as Azure Databricks, Azure Data Lake Storage, and Azure Synapse Analytics.\nDevelop and maintain data models, schemas, and metadata to support efficient data access, query performance, and analytics requirements.\nMonitor pipeline performance, troubleshoot issues, and optimize data processing workflows for scalability, reliability, and costeffectiveness.\nImplement data security and compliance measures to protect sensitive information and ensure regulatory compliance.\nRequirement\nProven experience as a Data Engineer, with expertise in building and optimizing data pipelines using PySpark, Scala, and Apache Spark.\nHandson experience with cloud platforms, particularly Azure, and proficiency in Azure services such as Azure Databricks, Azure Data Lake Storage, Azure Synapse Analytics, and Azure SQL Database.\nStrong programming skills in Python and Scala, with experience in software development, version control, and CI/CD practices.\nFamiliarity with data warehousing concepts, dimensional modeling, and relational databases (e.g., SQL Server, PostgreSQL, MySQL).\nExperience with big data technologies and frameworks (e.g., Hadoop, Hive, HBase) is a plus.\nMandatory skill sets\nSpark, Pyspark, Azure\nPreferred skill sets\nSpark, Pyspark, Azure\nYears of experience required\n4 8\nEducation qualification\nB.Tech / M.Tech / MBA / MCA\nEducation\nDegrees/Field of Study required Bachelor of Technology, Master of Business Administration\nDegrees/Field of Study preferred\nRequired Skills\nMicrosoft Azure\nAccepting Feedback, Accepting Feedback, Active Listening, Agile Scalability, Amazon Web Services (AWS), Analytical Thinking, Apache Airflow, Apache Hadoop, Azure Data Factory, Communication, Creativity, Data Anonymization, Data Architecture, Database Administration, Database Management System (DBMS), Database Optimization, Database Security Best Practices, Databricks Unified Data Analytics Platform, Data Engineering, Data Engineering Platforms, Data Infrastructure, Data Integration, Data Lake, Data Modeling, Data Pipeline {+ 27 more}\nTravel Requirements\nGovernment Clearance Required?",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data management', 'Data modeling', 'Postgresql', 'MySQL', 'Database administration', 'Agile', 'Apache', 'Business intelligence', 'SQL', 'Python']",2025-06-13 05:10:46
Senior Data Engineer,Robert Bosch Engineering and Business Solutions Private Limited,4 - 8 years,Not Disclosed,['Bengaluru'],"As a Data engineer in our team, you work with large scale manufacturing data coming from our globally distributed plants. You will focus on building efficient, scalable data-driven applications.\nThe data sets produced by these applications - whether data streams or data at rest - need to be highly available, reliable, consistent and quality-assured so that they can serve as input to wide range of other use cases and downstream applications.\nWe run these applications on Azure databricks, you will be building applications, you will also contribute to scaling the platform including topics such as automation and observability.\nFinally, you are expected to interact with customers and other technical teams e. g. for requirements clarification definition of data models.\nPrimary responsibilities:\nBe a key contributor to the Bosch hybrid cloud data platform (on-prem cloud)\nDesigning building data pipelines on a global scale, ranging from small to huge datasets\nDesign applications and data models based on deep business understanding and customer requirements\nDirectly work with architects and technical leadership to design implement applications and / or architectural components\nArchitectural proposal and estimation for the application, technical leadership to the team\nCoordination/Collaboration with central teams for tasks and standards\nDevelop data integration workflow in Azure\nDeveloping streaming application using scala.\nIntegrating the end-to-end Azure Databricks pipeline to take data from source systems to target system ensuring the quality and consistency of data.\nDefining data quality and validation checks.\nConfiguring data processing and transformation.\nWriting unit test cases for data pipelines.\nDefining and implementing data quality and validation check.\nTuning pipeline configurations for optimal performance.\nParticipate in Peer review and PR review for the code written by team members",Industry Type: Automobile,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Automation', 'Architecture', 'Technical leadership', 'Data processing', 'Workflow', 'Data quality', 'Test cases', 'Unit testing', 'Downstream']",2025-06-13 05:10:47
Senior Data Engineering Analyst,Optum,4 - 7 years,Not Disclosed,['Bengaluru'],"Job Description\n\nExperience 4 to 7 years.\nExperience in any ETL tools [e.g. DataStage] with implementation experience in large Data Warehouse\nProficiency in programming languages such as Python etc.\nExperience with data warehousing solutions (e.g., Snowflake, Redshift) and big data technologies (e.g., Hadoop, Spark).\nStrong knowledge of SQL and database management systems.\nFamiliarity with cloud platforms (e.g., AWS, Azure, GCP) and data pipeline orchestration tools (e.g. Airflow).\nProven ability to lead and develop high-performing teams, with excellent communication and interpersonal skills.\nStrong analytical and problem-solving abilities, with a focus on delivering actionable insights.\nResponsibilities\nDesign, develop, and maintain advanced data pipelines and ETL processes using niche technologies.\nCollaborate with cross-functional teams to understand complex data requirements and deliver tailored solutions.\nEnsure data quality and integrity by implementing robust data validation and monitoring processes.\nOptimize data systems for performance, scalability, and reliability.\nDevelop comprehensive documentation for data engineering processes and systems.",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['ETL', 'SQL', 'Python', 'Azure', 'Datastage', 'Snowflake', 'Ab Initio', 'Informatica', 'Teradata', 'AWS']",2025-06-13 05:10:49
IN Manager Sr Cloud Data Engineer,PwC Service Delivery Center,3 - 4 years,Not Disclosed,['Bengaluru'],"Not Applicable\nSpecialism\nSAP\n& Summary\nAt PwC, our people in data and analytics engineering focus on leveraging advanced technologies and techniques to design and develop robust data solutions for clients. They play a crucial role in transforming raw data into actionable insights, enabling informed decisionmaking and driving business growth.\n\nIn data engineering at PwC, you will focus on designing and building data infrastructure and systems to enable efficient data processing and analysis. You will be responsible for developing and implementing data pipelines, data integration, and data transformation solutions.\nWhy PWC\nAt PwC , you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purposeled and valuesdriven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us .\nAt PwC , we believe in providing equal employment opportunities, without any discrimination on the grounds of gender, ethnic background, age, disability, marital status, sexual orientation, pregnancy, gender identity or expression, religion or other beliefs, perceived differences and status protected by law. We strive to create an environment where each one of our people can bring their true selves and contribute to their personal growth and the firm s growth. To enable this, we have zero tolerance for any discrimination and harassment based on the above considerations.\n& Summary We are looking for a seasoned Sr Cloud Data Engineer\nResponsibilities\nHave 3 to 4 years of experience of leading Data Engineer teams developing enterprise grade data processing pipelines on multi Clouds like GCP and AWS Has led at least one project of medium to high complexity of migrating ETL pipelines and Data warehouses to cloud. 3 to 5 years of latest experience should be with premium consulting companies Indepth handson expertise with Google and AWS Cloud Platform services especially BigQuery, Dataform, Dataplex , Redshift. Exceptional communication skills to converse equally well with Data Engineers, Technology and Business leadership. Ability to leverage knowledge on GCP to other cloud environments.\nMandatory skill sets\n(AWS, Azure, GCP) services such as GCP BigQuery, Dataform AWS Redshift, Python\nPreferred skill sets\nDevops\nYears of experience required\n10 15 Years\nEducation qualification\nBE/B.Tech/MBA/MCA/M.Tech\nEducation\nDegrees/Field of Study required Bachelor of Engineering, Master of Business Administration, Bachelor of Technology\nDegrees/Field of Study preferred\nRequired Skills\nAWS Devops, Microsoft Azure, Python (Programming Language)\nDevOps\nNo",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Business administration', 'SAP', 'Consulting', 'Cloud', 'Manager Technology', 'Data processing', 'microsoft azure', 'Corporate advisory', 'AWS', 'Analytics']",2025-06-13 05:10:51
IN Manager Sr Cloud Data Engineer,PwC Service Delivery Center,3 - 4 years,Not Disclosed,['Bengaluru'],"Not Applicable\nSpecialism\nSAP\n& Summary\nAt PwC, our people in data and analytics engineering focus on leveraging advanced technologies and techniques to design and develop robust data solutions for clients. They play a crucial role in transforming raw data into actionable insights, enabling informed decisionmaking and driving business growth.\n\nIn data engineering at PwC, you will focus on designing and building data infrastructure and systems to enable efficient data processing and analysis. You will be responsible for developing and implementing data pipelines, data integration, and data transformation solutions.\nAt PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purposeled and valuesdriven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us .\n& Summary We are looking for a seasoned Sr Cloud Data Engineer\nResponsibilities\nHave 3 to 4 years of experience of leading Data Engineer teams developing enterprise grade data processing pipelines on multi Clouds like GCP and AWS Has led at least one project of medium to high complexity of migrating ETL pipelines and Data warehouses to cloud. 3 to 5 years of latest experience should be with premium consulting companies Indepth handson expertise with Google and AWS Cloud Platform services especially BigQuery, Dataform, Dataplex , Redshift. Exceptional communication skills to converse equally well with Data Engineers, Technology and Business leadership. Ability to leverage knowledge on GCP to other cloud environments.\nMandatory skill sets\n(AWS, Azure, GCP) services such as GCP BigQuery, Dataform AWS Redshift, Python\nPreferred skill sets\nDevops\nYears of experience required\n10 15 Years\nEducation qualification\nBE/B.Tech/MBA/MCA/M.Tech\nEducation\nDegrees/Field of Study required Master of Business Administration, Bachelor of Engineering, Bachelor of Technology\nDegrees/Field of Study preferred\nRequired Skills\nAWS Devops, Microsoft Azure, Python (Programming Language)\nDevOps\nNo",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Business administration', 'SAP', 'Consulting', 'Cloud', 'Manager Technology', 'Data processing', 'microsoft azure', 'Corporate advisory', 'AWS', 'Analytics']",2025-06-13 05:10:53
IN Senior Associate Cloud Data Engineer,PwC Service Delivery Center,5 - 8 years,Not Disclosed,['Bengaluru'],"Not Applicable\nSpecialism\nSAP\nManagement Level\nSenior Associate\n& Summary\nAt PwC, our people in data and analytics engineering focus on leveraging advanced technologies and techniques to design and develop robust data solutions for clients. They play a crucial role in transforming raw data into actionable insights, enabling informed decisionmaking and driving business growth.\n\nIn data engineering at PwC, you will focus on designing and building data infrastructure and systems to enable efficient data processing and analysis. You will be responsible for developing and implementing data pipelines, data integration, and data transformation solutions.\nWhy PWC\nAt PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purposeled and valuesdriven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us .\nResponsibilities\nStrong handson experience with multi cloud (AWS, Azure, GCP) services such as GCP BigQuery, Dataform AWS Redshift, Proficient in PySpark and SQL for building scalable data processing pipelines Knowledge of utilizing serverless technologies like AWS Lambda, and Google Cloud Functions Experience in orchestration frameworks like Apache Airflow, Kubernetes, and Jenkins to manage and orchestrate data pipelines Experience in developing and optimizing ETL/ELT pipelines and working on cloud data warehouse migration projects. Exposure to clientfacing roles, with strong problemsolving and communication skills. Prior experience in consulting or working in a consulting environment is preferred.\nMandatory skill sets\n(AWS, Azure, GCP) services such as GCP BigQuery, Dataform AWS Redshift, Python\nPreferred skill sets\nDevops\nYears of experience required\n5 8 Years\nEducation qualification\nBE/B.Tech/MBA/MCA/M.Tech\nEducation\nDegrees/Field of Study required Master of Business Administration, Bachelor of Technology, Bachelor of Engineering\nDegrees/Field of Study preferred\nRequired Skills\nAWS Devops, Microsoft Azure, Python (Programming Language)\nDevOps\nTravel Requirements\nGovernment Clearance Required?",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Business administration', 'SAP', 'Consulting', 'Manager Technology', 'Data processing', 'microsoft azure', 'Corporate advisory', 'AWS', 'Analytics', 'SQL']",2025-06-13 05:10:55
IN Senior Associate Cloud Data Engineer,PwC Service Delivery Center,5 - 8 years,Not Disclosed,['Bengaluru'],"Not Applicable\nSpecialism\nSAP\nManagement Level\nSenior Associate\n& Summary\nAt PwC, our people in data and analytics engineering focus on leveraging advanced technologies and techniques to design and develop robust data solutions for clients. They play a crucial role in transforming raw data into actionable insights, enabling informed decisionmaking and driving business growth.\n\nIn data engineering at PwC, you will focus on designing and building data infrastructure and systems to enable efficient data processing and analysis. You will be responsible for developing and implementing data pipelines, data integration, and data transformation solutions.\nWhy PWC\nAt PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purposeled and valuesdriven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us .\nResponsibilities\nStrong handson experience with multi cloud (AWS, Azure, GCP) services such as GCP BigQuery, Dataform AWS Redshift, Proficient in PySpark and SQL for building scalable data processing pipelines Knowledge of utilizing serverless technologies like AWS Lambda, and Google Cloud Functions Experience in orchestration frameworks like Apache Airflow, Kubernetes, and Jenkins to manage and orchestrate data pipelines Experience in developing and optimizing ETL/ELT pipelines and working on cloud data warehouse migration projects. Exposure to clientfacing roles, with strong problemsolving and communication skills. Prior experience in consulting or working in a consulting environment is preferred.\nMandatory skill sets\n(AWS, Azure, GCP) services such as GCP BigQuery, Dataform AWS Redshift, Python\nPreferred skill sets\nDevops\nYears of experience required\n5 8 Years\nEducation qualification\nBE/B.Tech/MBA/MCA/M.Tech\nEducation\nDegrees/Field of Study required Bachelor of Engineering, Bachelor of Technology, Master of Business Administration\nDegrees/Field of Study preferred\nRequired Skills\nAWS Devops\nAccepting Feedback, Accepting Feedback, Active Listening, Agile Scalability, Amazon Web Services (AWS), Analytical Thinking, Apache Airflow, Apache Hadoop, Azure Data Factory, Communication, Creativity, Data Anonymization, Data Architecture, Database Administration, Database Management System (DBMS), Database Optimization, Database Security Best Practices, Databricks Unified Data Analytics Platform, Data Engineering, Data Engineering Platforms, Data Infrastructure, Data Integration, Data Lake, Data Modeling, Data Pipeline {+ 27 more}\nTravel Requirements\nGovernment Clearance Required?",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SAP', 'Data modeling', 'Analytical', 'Consulting', 'Database administration', 'Data processing', 'Corporate advisory', 'DBMS', 'Database management system', 'SQL']",2025-06-13 05:10:57
IN Senior Associate Cloud Data Engineer,PwC Service Delivery Center,5 - 8 years,Not Disclosed,['Bengaluru'],"Not Applicable\nSpecialism\nSAP\nManagement Level\nSenior Associate\n& Summary\nAt PwC, our people in data and analytics engineering focus on leveraging advanced technologies and techniques to design and develop robust data solutions for clients. They play a crucial role in transforming raw data into actionable insights, enabling informed decisionmaking and driving business growth.\n\nIn data engineering at PwC, you will focus on designing and building data infrastructure and systems to enable efficient data processing and analysis. You will be responsible for developing and implementing data pipelines, data integration, and data transformation solutions.\nWhy PWC\nAt PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purposeled and valuesdriven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us .\n& Summary We are looking for a seasoned Cloud Data Engineer\nResponsibilities\nStrong handson experience with multi cloud (AWS, Azure, GCP) services such as GCP BigQuery, Dataform AWS Redshift, Proficient in PySpark and SQL for building scalable data processing pipelines Knowledge of utilizing serverless technologies like AWS Lambda, and Google Cloud Functions Experience in orchestration frameworks like Apache Airflow, Kubernetes, and Jenkins to manage and orchestrate data pipelines Experience in developing and optimizing ETL/ELT pipelines and working on cloud data warehouse migration projects. Exposure to clientfacing roles, with strong problemsolving and communication skills. Prior experience in consulting or working in a consulting environment is preferred.\nMandatory skill sets\n(AWS, Azure, GCP) services such as GCP BigQuery, Dataform AWS Redshift, Python\nPreferred skill sets\nDevops\nYears of experience required\n5 8 Years\nEducation qualification\nBE/B.Tech/MBA/MCA/M.Tech\nEducation\nDegrees/Field of Study required Master of Business Administration, Master of Engineering, Bachelor of Engineering\nDegrees/Field of Study preferred\nRequired Skills\nCloud Data Catalog\nAccepting Feedback, Accepting Feedback, Active Listening, Agile Scalability, Amazon Web Services (AWS), Analytical Thinking, Apache Airflow, Apache Hadoop, Azure Data Factory, Communication, Creativity, Data Anonymization, Data Architecture, Database Administration, Database Management System (DBMS), Database Optimization, Database Security Best Practices, Databricks Unified Data Analytics Platform, Data Engineering, Data Engineering Platforms, Data Infrastructure, Data Integration, Data Lake, Data Modeling, Data Pipeline {+ 27 more}\nTravel Requirements\nGovernment Clearance Required?",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SAP', 'GCP', 'Data modeling', 'Analytical', 'Consulting', 'Agile', 'DBMS', 'Apache', 'SQL', 'Python']",2025-06-13 05:10:58
IN Senior Associate Cloud Data Engineer,PwC Service Delivery Center,5 - 8 years,Not Disclosed,['Bengaluru'],"Not Applicable\nSpecialism\nSAP\nManagement Level\nSenior Associate\n& Summary\nAt PwC, our people in data and analytics engineering focus on leveraging advanced technologies and techniques to design and develop robust data solutions for clients. They play a crucial role in transforming raw data into actionable insights, enabling informed decisionmaking and driving business growth.\n\nIn data engineering at PwC, you will focus on designing and building data infrastructure and systems to enable efficient data processing and analysis. You will be responsible for developing and implementing data pipelines, data integration, and data transformation solutions.\nWhy PWC\nAt PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purposeled and valuesdriven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us .\nResponsibilities\nStrong handson experience with multi cloud (AWS, Azure, GCP) services such as GCP BigQuery, Dataform AWS Redshift, Proficient in PySpark and SQL for building scalable data processing pipelines Knowledge of utilizing serverless technologies like AWS Lambda, and Google Cloud Functions Experience in orchestration frameworks like Apache Airflow, Kubernetes, and Jenkins to manage and orchestrate data pipelines Experience in developing and optimizing ETL/ELT pipelines and working on cloud data warehouse migration projects. Exposure to clientfacing roles, with strong problemsolving and communication skills. Prior experience in consulting or working in a consulting environment is preferred.\nMandatory skill sets\n(AWS, Azure, GCP) services such as GCP BigQuery, Dataform AWS Redshift, Python\nPreferred skill sets\nDevops\nYears of experience required\n5 8 Years\nEducation qualification\nBE/B.Tech/MBA/MCA/M.Tech\nEducation\nDegrees/Field of Study required Bachelor of Engineering, Master of Business Administration, Bachelor of Technology\nDegrees/Field of Study preferred\nRequired Skills\nAWS Devops\nAccepting Feedback, Accepting Feedback, Active Listening, Agile Scalability, Amazon Web Services (AWS), Analytical Thinking, Apache Airflow, Apache Hadoop, Azure Data Factory, Communication, Creativity, Data Anonymization, Data Architecture, Database Administration, Database Management System (DBMS), Database Optimization, Database Security Best Practices, Databricks Unified Data Analytics Platform, Data Engineering, Data Engineering Platforms, Data Infrastructure, Data Integration, Data Lake, Data Modeling, Data Pipeline {+ 27 more}\nTravel Requirements\nGovernment Clearance Required?",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SAP', 'Data modeling', 'Analytical', 'Consulting', 'Database administration', 'Data processing', 'Corporate advisory', 'DBMS', 'Database management system', 'SQL']",2025-06-13 05:11:00
IN Senior Associate Cloud Data Engineer-- Data and Analytics,PwC Service Delivery Center,4 - 7 years,Not Disclosed,['Bengaluru'],"Not Applicable\nSpecialism\nData, Analytics & AI\nManagement Level\nSenior Associate\n& Summary\nAt PwC, our people in data and analytics engineering focus on leveraging advanced technologies and techniques to design and develop robust data solutions for clients. They play a crucial role in transforming raw data into actionable insights, enabling informed decisionmaking and driving business growth.\n\nIn data engineering at PwC, you will focus on designing and building data infrastructure and systems to enable efficient data processing and analysis. You will be responsible for developing and implementing data pipelines, data integration, and data transformation solutions.\nWhy PWC\n& Summary\ns\nJob Title Cloud Data Engineer (AWS/Azure/Databricks/GCP)\nExperience 47 years in Data Engineering\nWe are seeking skilled and dynamic Cloud Data Engineers specializing in AWS, Azure, Databricks, and GCP. The ideal candidate will have a strong background in data engineering, with a focus on data ingestion, transformation, and warehousing. They should also possess excellent knowledge of PySpark or Spark, and a proven ability to optimize performance in Spark job executions.\nKey Responsibilities\nDesign, build, and maintain scalable data pipelines for a variety of cloud platforms including AWS, Azure, Databricks, and GCP.\nImplement data ingestion and transformation processes to facilitate efficient data warehousing.\nUtilize cloud services to enhance data processing capabilities\nAWS Glue, Athena, Lambda, Redshift, Step Functions, DynamoDB, SNS.\nAzure Data Factory, Synapse Analytics, Functions, Cosmos DB, Event Grid, Logic Apps, Service Bus.\nGCP Dataflow, BigQuery, DataProc, Cloud Functions, Bigtable, Pub/Sub, Data Fusion.\nOptimize Spark job performance to ensure high efficiency and reliability.\nStay proactive in learning and implementing new technologies to improve data processing frameworks.\nCollaborate with crossfunctional teams to deliver robust data solutions.\nWork on Spark Streaming for realtime data processing as necessary.\nQualifications\n47 years of experience in data engineering with a strong focus on cloud environments.\nProficiency in PySpark or Spark is mandatory.\nProven experience with data ingestion, transformation, and data warehousing.\nIndepth knowledge and handson experience with cloud services(AWS/Azure/GCP)\nDemonstrated ability in performance optimization of Spark jobs.\nStrong problemsolving skills and the ability to work independently as well as in a team.\nCloud Certification (AWS, Azure, or GCP) is a plus.\nFamiliarity with Spark Streaming is a bonus.\nMandatory skill sets\nPython, Pyspark, SQL with (AWS or Azure or GCP)\nPreferred skill sets\nPython, Pyspark, SQL with (AWS or Azure or GCP)\nYears of experience required\n710\nEducation qualification\nBE/BTECH, ME/MTECH, MBA, MCA\nEducation\nDegrees/Field of Study required Bachelor of Engineering, Master of Business Administration, Bachelor of Technology, Master of Engineering\nDegrees/Field of Study preferred\nRequired Skills\nPySpark, Python (Programming Language), Structured Query Language (SQL)\nAccepting Feedback, Accepting Feedback, Active Listening, Agile Scalability, Amazon Web Services (AWS), Analytical Thinking, Apache Airflow, Apache Hadoop, Azure Data Factory, Communication, Creativity, Data Anonymization, Data Architecture, Database Administration, Database Management System (DBMS), Database Optimization, Database Security Best Practices, Databricks Unified Data Analytics Platform, Data Engineering, Data Engineering Platforms, Data Infrastructure, Data Integration, Data Lake, Data Modeling, Data Pipeline {+ 28 more}\nTravel Requirements\nGovernment Clearance Required?",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['GCP', 'Data modeling', 'Analytical', 'Agile', 'Data processing', 'DBMS', 'Apache', 'AWS', 'SQL', 'Python']",2025-06-13 05:11:02
Data Engineer,Orangemint Technologies,3 - 5 years,20-25 Lacs P.A.,['Bengaluru'],"Company name: PulseData labs Pvt Ltd (captive Unit for URUS, USA)\nAbout URUS We are the URUS family (US), a global leader in products and services for Agritech.\nSENIOR DATA ENGINEER This role is responsible for the design, development, and maintenance of data integration and reporting solutions. The ideal candidate will possess expertise in Databricks and strong skills in SQL Server, SSIS and SSRS, and experience with other modern data engineering tools such as Azure Data Factory. This position requires a proactive and results-oriented individual with a passion for data and a strong understanding of data warehousing principles.\nResponsibilities Data Integration\nDesign, develop, and maintain robust and efficient ETL pipelines and processes on Databricks.\nTroubleshoot and resolve Databricks pipeline errors and performance issues.\nMaintain legacy SSIS packages for ETL processes.\nTroubleshoot and resolve SSIS package errors and performance issues.\nOptimize data flow performance and minimize data latency.\nImplement data quality checks and validations within ETL processes.\nDatabricks Development\nDevelop and maintain Databricks pipelines and datasets using Python, Spark and SQL.\nMigrate legacy SSIS packages to Databricks pipelines.\nOptimize Databricks jobs for performance and cost-effectiveness.\nIntegrate Databricks with other data sources and systems.\nParticipate in the design and implementation of data lake architectures.\nData Warehousing\nParticipate in the design and implementation of data warehousing solutions.\nSupport data quality initiatives and implement data cleansing procedures.\nReporting and Analytics\nCollaborate with business users to understand data requirements for department driven reporting needs.\nMaintain existing library of complex SSRS reports, dashboards, and visualizations.\nTroubleshoot and resolve SSRS report issues, including performance bottlenecks and data inconsistencies.\nCollaboration and Communication\nComfortable in entrepreneurial, self-starting, and fast-paced environment, working both independently and with our highly skilled teams.\nCollaborate effectively with business users, data analysts, and other IT teams.\nCommunicate technical information clearly and concisely, both verbally and in writing.\nDocument all development work and procedures thoroughly.\nContinuous Growth\nKeep abreast of the latest advancements in data integration, reporting, and data engineering technologies.\nContinuously improve skills and knowledge through training and self-learning.\nThis job description reflects managements assignment of essential functions; it does not prescribe or restrict the tasks that may be assigned.\nRequirements\nBachelor's degree in computer science, Information Systems, or a related field.\n2+ years of experience in data integration and reporting.\nExtensive experience with Databricks, including Python, Spark, and Delta Lake.\nStrong proficiency in SQL Server, including T-SQL, stored procedures, and functions.\nExperience with SSIS (SQL Server Integration Services) development and maintenance.\nExperience with SSRS (SQL Server Reporting Services) report design and development.\nExperience with data warehousing concepts and best practices.\nExperience with Microsoft Azure cloud platform and Microsoft Fabric desirable.\nStrong analytical and problem-solving skills.\nExcellent communication and interpersonal skills.\nAbility to work independently and as part of a team.\nExperience with Agile methodologies.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Etl Development', 'Azure Databricks', 'Databricks Engineer', 'Spark', 'SQL Server', 'Data Warehousing', 'Pythonspark']",2025-06-13 05:11:04
Data Engineer,Axis Finance (AFL),7 - 11 years,Not Disclosed,"['Mumbai', 'Mumbai (All Areas)']","Key Responsibilities:\nShould have experience in below\nDesign, develop, and implement a Data Lake House architecture on AWS, ensuring scalability, flexibility, and performance.\nBuild ETL/ELT pipelines for ingesting, transforming, and processing structured and unstructured data.\nCollaborate with cross-functional teams to gather data requirements and deliver data solutions aligned with business needs.\nDevelop and manage data models, schemas, and data lakes for analytics, reporting, and BI purposes.\nImplement data governance practices, ensuring data quality, security, and compliance.\nPerform data integration between on-premise and cloud systems using AWS services.\nMonitor and troubleshoot data pipelines and infrastructure for reliability and scalability.\nSkills and Qualifications:\n7 + years of experience in data engineering, with a focus on cloud data platforms.\nStrong experience with AWS services: S3, Glue, Redshift, Athena, Lambda, IAM, RDS, and EC2.\nHands-on experience in building data lakes, data warehouses, and lake house architectures.\nShould have experience in ETL/ELT pipelines using tools like AWS Glue, Apache Spark, or similar.\nExpertise in SQL and Python or Java for data processing and transformations.\nFamiliarity with data modeling and schema design in cloud environments.\nUnderstanding of data security and governance practices, including IAM policies and data encryption.\nExperience with big data technologies (e.g., Hadoop, Spark) and data streaming services (e.g., Kinesis, Kafka).\nHave lending domain knowledge will be added advantage\nPreferred Skills:\nExperience with Databricks or similar platforms for data engineering.\nFamiliarity with DevOps practices for deploying data solutions on AWS (CI/CD pipelines).\nKnowledge of API integration and cloud data migration strategies.",Industry Type: NBFC,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['pipeline tools', 'lending domain', 'AWS', 'data models', 'spark', 'devops', 'databricks', 'date engineering platforms', 'hadoop', 'data lake house', 'API integration']",2025-06-13 05:11:06
Data Engineer,Supersourcing,4 - 8 years,Not Disclosed,"['Noida', 'Pune', 'Bengaluru']","Job Description-\nWe are hiring a Data Engineer with strong expertise in JAVA, Apache Spark and AWS Cloud. You will design and develop high-performance, scalable applications and data pipelines for cloud-based environments.\n\nKey Skills:\n3+ years in Java (Java 8+), Spring Boot, and REST APIs\n3+ years in Apache Spark (Core, SQL, Streaming)\nStrong hands-on with AWS services: S3, EC2, Lambda, Glue, EMR\nExperience with microservices, CI/CD, and Git\nGood understanding of distributed systems and performance tuning\n\nNice to Have:\nExperience with Kafka, Airflow, Docker, or Kubernetes\nAWS Certification (Developer/Architect)",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Java', 'Spark', 'Pyspark', 'SQL']",2025-06-13 05:11:07
Data Engineer,Aqilea Softech,5 - 9 years,13-20 Lacs P.A.,"['Bangalore Rural', 'Bengaluru']","Job Title: Data Engineer\nCompany : Aqilea India(Client : H&M India)\nEmployment Type: Full Time\nLocation: Bangalore(Hybrid)\nExperience: 4.5 to 9 years\nClient : H&M India\n\nAt H&M, we welcome you to be yourself and feel like you truly belong. Help us reimagine the future of an entire industry by making everyone look, feel, and do good. We take pride in our history of making fashion accessible to everyone and led by our values we strive to build a more welcoming, inclusive, and sustainable industry. We are privileged to have more than 120,000 colleagues, in over 75 countries across the world. Thats 120 000 individuals with unique experiences, skills, and passions. At H&M, we believe everyone can make an impact, we believe in giving people responsibility and a strong sense of ownership. Our business is your business, and when you grow, we grow.\nWebsite : https://career.hm.com/\n\nWe are seeking a skilled and forward-thinking Data Engineer to join our Emerging Tech team. This role is designed for someone passionate about working with cutting-edge technologies such as AI, machine learning, IoT, and big data to turn complex data sets into actionable insights.\nAs the Data Engineer in Emerging Tech, you will be responsible for designing, implementing, and optimizing data architectures and processes that support the integration of next-generation technologies. Your role will involve working with large-scale datasets, building predictive models, and utilizing emerging tools to enable data-driven decision-making across the business. You ll collaborate with technical and business teams to uncover insights, streamline data pipelines, and ensure the best use of advanced analytics technologies.\n\nKey Responsibilities:\nDesign and build scalable data architectures and pipelines that support machine learning, analytics, and IoT initiatives.\nDevelop and optimize data models and algorithms to process and analyse large-scale, complex data sets.\nImplement data governance, security, and compliance measures to ensure high-quality\nCollaborate with cross-functional teams (engineering, product, and business) to translate business requirements into data-driven solutions.\nEvaluate, integrate, and optimize new data technologies to enhance analytics capabilities and drive business outcomes.\nApply statistical methods, machine learning models, and data visualization techniques to deliver actionable insights.\nEstablish best practices for data management, including data quality, consistency, and scalability.\nConduct analysis to identify trends, patterns, and correlations within data to support strategic business initiatives.\nStay updated on the latest trends and innovations in data technologies and emerging data management practices.\n\nSkills Required :\nBachelors or masters degree in data science, Computer Science, Engineering, Statistics, or a related field.\n4.5-9 years of experience in data engineering, data science, or a similar analytical role, with a focus on emerging technologies.\nProficiency with big data frameworks (e.g., Hadoop, Spark, Kafka) and experience with modern cloud platforms (AWS, Azure, or GCP).\nSolid skills in Python, SQL, and optionally R, along with experience using machine learning libraries such as Scikit-learn, TensorFlow, or PyTorch.\nExperience with data visualization tools (e.g., Tableau or Power BI or D3.js) to communicate insights effectively.\nFamiliarity with IoT and edge computing data architectures is a plus.\nUnderstanding of data governance, compliance, and privacy standards.\nAbility to work with both structured and unstructured data.\nExcellent problem-solving, communication, and collaboration skills, with the ability to work in a fast-paced, cross-functional team environment.\nA passion for emerging technologies and a continuous desire to learn and innovate.\nInterested Candidates can share your Resumes to mail id karthik.prakadish@aqilea.com",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Powerbi', 'Hadoop', 'Kafka', 'Tableau', 'Azure', 'GCP', 'Data Engineer', 'Spark', 'AWS', 'Python', 'SQL']",2025-06-13 05:11:09
Data Engineer,LTIMindtree,5 - 8 years,Not Disclosed,"['Noida', 'Chennai', 'Bengaluru']","1.Big Data Engineer:\n\nCompany: LTIMINDTREE\nMandotory skills - Python,Pyspark & AWS\nLocation : Noida & Pan India\nExperience : 5-8Years\nSalary: 19LPA\n\nShare your cv at Muktai.S@alphacom.in",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'AWS', 'Python']",2025-06-13 05:11:11
nior Data Engineer,M360 Research,5 - 8 years,Not Disclosed,['Bengaluru'],"M3 Global Research, an M3 company, is seeking a Senior Data Engineer to join our data engineering team. This role will focus on building and maintaining robust data pipelines, working closely with stakeholders to ensure data solutions align with business objectives, and utilizing tools like Power BI for data visualization and reporting. The ideal candidate has strong analytical skills, a passion for data-driven decision-making, and excellent communication abilities to work effectively with stakeholders across the organization.\nEssential Duties and Responsibilities:\nDesign, develop, and maintain high-quality, secure data pipelines and processes to manage and transform data efficiently.\nLead the architecture and implementation of data models, schemas, and integrations that support business intelligence and reporting needs.\nCollaborate with cross-functional teams to understand data requirements and deliver optimal data solutions that align with business goals.\nMaintain and enhance data infrastructure, including data warehouses, lakes, and integration tools.\nProvide guidance on best practices for data management, security, and compliance.\nSupport Power BI and other visualization tools, ensuring consistent and reliable access to data insights.\nOversee the delivery of data initiatives, ensuring they meet project milestones, KPIs, and deadlines.\nEssential Job Functions:\nMaintain regular and punctual attendance.\nWork cooperatively and communicate effectively with team members and stakeholders.\nComply with all company policies and procedures.\nSupervisory Responsibility:\nYes\nOutcomes:\nDeliver high-quality, reliable data solutions.\nProvide stakeholders with clear and actionable insights through Power BI reports.\nEnsure data pipelines and ETL processes are optimized and running efficiently.\nFoster strong relationships with stakeholders to ensure their data needs are met.\nCompetencies:\nAttention to detail.\nAnalytical thinking and problem-solving skills.\nStrong communication and interpersonal skills to engage effectively with stakeholders.\nAbility to work in a fast-paced, agile environment.\nKnowledge and Skills:\nProficiency with data engineering tools and technologies (eg, SQL, Python, ETL tools).\nStrong experience with Power BI for data visualization and reporting.\nFamiliarity with cloud-based data platforms (eg, AWS, Azure, Google Cloud).\nExperience with data modeling, data warehousing, and designing scalable data architectures.\nStrong knowledge of database systems (eg, SQL Server, Oracle, PostgreSQL).\nExperience working in an Agile development environment.\nExcellent communication skills to work effectively with both technical and non-technical stakeholders.\nAbility to multi-task and manage multiple projects simultaneously.\nProblem-solving mindset with a desire to continuously improve data processes.\nMinimum Experience:\n5+ years of experience in data engineering or related fields.\n2+ years of experience with Power BI or similar data visualization tools.\nEducation and Training Required:\nbachelors degree in computer science, Data Science, or a related field, or equivalent experience",Industry Type: Analytics / KPO / Research,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data management', 'ISO', 'Analytical', 'Healthcare', 'Market research', 'Oracle', 'Business intelligence', 'Japanese', 'Recruitment', 'SQL']",2025-06-13 05:11:13
Data Engineer- MS Fabric,InfoCepts,5 - 10 years,Not Disclosed,"['Kolkata', 'Mumbai', 'New Delhi', 'Hyderabad', 'Pune', 'Chennai', 'Bengaluru']","Position: Data Engineer - MS Fabric\nPurpose of the Position: As an MS Fabric Data engineer you will be responsible for designing, implementing, and managing scalable data pipelines. Strong experience in implementation and management of lake House using MS Fabric Azure Tech stack (ADLS Gen2, ADF, Azure SQL) .\nProficiency in data integration techniques, ETL processes and data pipeline architectures. Well versed in Data Quality rules, principles and implementation.\nLocation: Bangalore/ Pune/ Nagpur/ Chennai\nType of Employment: FTE",,,,"['Performance tuning', 'Data modeling', 'Coding', 'XML', 'Scheduling', 'Data quality', 'JSON', 'Business intelligence', 'Data architecture', 'Python']",2025-06-13 05:11:14
Data Engineer,Clifyx Technology,5 - 10 years,Not Disclosed,['Bengaluru'],"2\nData Engineer\nAzure Synapse/ADF , Workiva\nTo manage and maintain the associated Connector, Chains, Tables and Queries, making updates, as needed, as new metrics or requirements are identified\nDevelop functional and technical requirements for any changes impacting wData (Workiva Data)\nConfigure and unit test any changes impacting wData (connector, chains, tables, queries\nPromote wData changes",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['hive', 'python', 'oracle', 'azure data lake', 'informatica powercenter', 'azure synapse', 'microsoft azure', 'data warehousing', 'power bi', 'azure data factory', 'data engineering', 'sql server', 'sql', 'plsql', 'sql azure', 'spark', 'oracle adf', 'hadoop', 'etl', 'ssis', 'big data', 'informatica', 'unix']",2025-06-13 05:11:16
Databricks Data Engineer,Mumba Technologies,6 - 10 years,22.5-25 Lacs P.A.,['Bengaluru'],"Mandatory Skills & Experience:\n6 to 8 years of experience in data engineering, with strong experience in Oracle\nDWH/ODS environments.\nMinimum 3+ years hands-on experience in Databricks (including PySpark, SQL,\nDelta Lake, Workflows).\nStrong understanding of Lakehouse architecture, cloud data platforms, and big\ndata processing.\n\nProven experience in migrating data warehouse and ETL workloads from Oracle to\ncloud platforms.\nExperience with PL/SQL, query tuning, and reverse engineering legacy systems.\nExposure to Pentaho and/or TIBCO Data Virtualization/Integration tools.\nExperience with CI/CD pipelines, version control (e.g., Git), and automated\ntesting.\nFamiliarity with data governance, security policies, and compliance in cloud\nenvironments.\nStrong communication and documentation skills.\n\nPreferred Skills (Advantage):\nExperience in cloud migration projects (AWS/Azure).\nKnowledge of Delta Lake, Unity Catalog, and Databricks workflows.\nExposure to Kafka for real-time data streaming.\nExperience with ETL tools like Pentaho or Tibco will be an added advantage.\nAWS/Azure/Databricks certifications\nTools & Technologies:\nDatabricks, Oracle, Hadoop (HDFS, Hive, Sqoop), AWS (S3, EMR, Glue, Lamda, RDS)\nPySpark, SQL, Python, Kafka\nCI/CD (Jenkins, GitHub Actions), Orchestration (Airflow, Control-M)\nJIRA, Confluence, Git (GitHub/Bitbucket)\nCloud Certifications (Preferred):\nDatabricks Certified Data Engineer\nAWS Certified Solutions Architect/Developer",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'Delta Lake', 'Databricks', 'Oracle DWH', 'Data warehouse', 'SQL']",2025-06-13 05:11:18
Data Engineer - Databricks,KPI Partners,3 - 6 years,Not Disclosed,"['Hyderabad', 'Pune', 'Bengaluru']","About KPI Partners.\nKPI Partners is a leading provider of data analytics solutions, dedicated to helping organizations transform data into actionable insights. Our innovative approach combines advanced technology with expert consulting, allowing businesses to leverage their data for improved performance and decision-making.\n\nJob Description.\nWe are seeking a skilled and motivated Data Engineer with experience in Databricks to join our dynamic team. The ideal candidate will be responsible for designing, building, and maintaining scalable data pipelines and data processing solutions that support our analytics initiatives. You will collaborate closely with data scientists, analysts, and other engineers to ensure the consistent flow of high-quality data across our platforms.",,,,"['python', 'data analytics', 'analytical', 'scala', 'pyspark', 'microsoft azure', 'data warehousing', 'data pipeline', 'data architecture', 'data engineering', 'sql', 'data bricks', 'cloud', 'analytics', 'data quality', 'data modeling', 'gcp', 'teamwork', 'integration', 'aws', 'etl', 'programming', 'communication skills', 'etl scripts']",2025-06-13 05:11:20
Data Engineer - Databricks,Inorg,2 - 5 years,Not Disclosed,"['Kolkata', 'Mumbai', 'New Delhi', 'Hyderabad', 'Pune', 'Chennai', 'Bengaluru']",InOrg Global is looking for Data Engineer - Databricks to join our dynamic team and embark on a rewarding career journey.\n\nLiaising with coworkers and clients to elucidate the requirements for each task.\nConceptualizing and generating infrastructure that allows big data to be accessed and analyzed.\nReformulating existing frameworks to optimize their functioning.\nTesting such structures to ensure that they are fit for use.\nPreparing raw data for manipulation by data scientists.\nDetecting and correcting errors in your work.\nEnsuring that your work remains backed up and readily accessible to relevant coworkers.\nRemaining up - to - date with industry standards and technological advancements that will improve the quality of your outputs.,Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent",['Data Engineer - Databricks'],2025-06-13 05:11:21
Consultant - Data Engineer (with Fabric),Affine Analytics,5 - 8 years,Not Disclosed,['Bengaluru'],"We are looking for a highly skilled API & Pixel Tracking Integration Engineer to lead the development and deployment of server-side tracking and attribution solutions across multiple platforms. The ideal candidate brings deep expertise in CAPI integrations (Meta, Google, and other platforms), secure data handling using cryptographic techniques, and experience working within privacy-first environments like Azure Clean Rooms.\nThis role requires strong hands-on experience in C# development, Azure cloud services, OCI (Oracle Cloud Infrastructure), and marketing technology stacks including Adobe Tag Management and Pixel Management. You will work closely with engineering, analytics, and marketing teams to deliver scalable, compliant, and secure data tracking solutions that drive business insights and performance.",,,,"['Adobe Tag Management', 'Data Engineering', 'Azure Data Factory', 'Azure security', 'ADF', 'ADLS', 'Azure Functions', 'Meta CAPI', 'Google Enhanced Conversions', 'Key Vault', 'Cosmos DB']",2025-06-13 05:11:23
Consultant - Data Engineer,Affine Analytics,5 - 8 years,Not Disclosed,['Bengaluru'],"We are looking for a highly skilled API & Pixel Tracking Integration Engineer to lead the development and deployment of server-side tracking and attribution solutions across multiple platforms. The ideal candidate brings deep expertise in CAPI integrations (Meta, Google, and other platforms), secure data handling using cryptographic techniques, and experience working within privacy-first environments like Azure Clean Rooms.\nThis role requires strong hands-on experience in Azure cloud services, OCI (Oracle Cloud Infrastructure), and marketing technology stacks including Adobe Tag Management and Pixel Management. You will work closely with engineering, analytics, and marketing teams to deliver scalable, compliant, and secure data tracking solutions that drive business insights and performance.",,,,"['Python', 'Azure Cloud technologies', 'Azure Data Factory', 'Adobe Tag Management', 'Azure security', 'ADF', 'ADLS', 'Azure Functions', 'Key Vault']",2025-06-13 05:11:25
Data Engineer,Talent Aspire,2 - 7 years,Not Disclosed,"['Chandigarh', 'Bengaluru']","As the Data Engineer, you will play a pivotal role in shaping our data infrastructure and\nexecuting against our strategy. You will ideate alongside engineering, data and our clients to\ndeploy data products with an innovative and meaningful impact to clients. You will design, build, and maintain scalable data pipelines and workflows on AWS. Additionally, your expertise in AI and machine learning will enhance our ability to deliver smarter, more predictive solutions.\n\nKey Responsibilities\nCollaborate with other engineers, customers to brainstorm and develop impactful data\nproducts tailored to our clients.\nLeverage AI and machine learning techniques to integrate intelligent features into our\nofferings.\nDevelop, and optimize end-to-end data pipelines on AWS\nFollow best practices in software architecture and development.\nImplement effective cost management and performance optimization strategies.\nDevelop and maintain systems using Python, SQL, PySpark, and Django for front-end\ndevelopment.\nWork directly with clients and end-users and address their data needs\nUtilize databases and tools including and not limited to, Postgres, Redshift, Airflow, and\nMongoDB to support our data ecosystem.\nLeverage AI frameworks and libraries to integrate advanced analytics into our solutions.\nQualifications\n\nExperience:\nMinimum of 3 years of experience in data engineering, software development, or\nrelated roles.\nProven track record in designing and deploying AWS cloud infrastructure\nsolutions\nAt least 2 years in data analysis and mining techniques to aid in descriptive and\ndiagnostic insights\nExtensive hands-on experience with Postgres, Redshift, Airflow, MongoDB, and\nreal-time data workflows.\n\nTechnical Skills:\nExpertise in Python, SQL, and PySpark\nStrong background in software architecture and scalable development practices.\nTableau, Metabase or similar viz tools experience\nWorking knowledge of AI frameworks and libraries is a plus.\nLeadership & Communication:\nDemonstrates ownership and accountability for delivery with a strong\ncommitment to quality.\nExcellent communication skills with a history of effective client and end-user\nengagement.\nStartup & Fintech Mindset:\nAdaptability and agility to thrive in a fast-paced, early-stage startup environment.\nPassion for fintech innovation and a strong desire to make a meaningful impact\non the future of finance.",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Engineering', 'performance optimization strategies', 'PySpark', 'Django', 'cost management', 'AWS', 'AI frameworks', 'Python', 'SQL']",2025-06-13 05:11:26
Data Engineer-Pyspark,A leading technology services and consul...,5 - 10 years,Not Disclosed,"['Pune', 'Chennai', 'Bengaluru']","Our client is Global IT Service & Consulting Organization\nExp-5+ yrs\n\nSkil Apache Spark\n\nLocation- Bangalore, Hyderabad, Pune, Chennai, Coimbatore, Gr. Noida\n\n\nExcellent Knowledge on Spark; The professional must have a thorough understanding Spark framework, Performance Tuning etc\nExcellent Knowledge and hands-on experience of at least 4+ years in Scala or PySpark\nExcellent Knowledge of the Hadoop eco System- Knowledge of Hive mandatory\nStrong Unix and Shell Scripting Skills\nExcellent Inter-personal skills and for experienced candidates Excellent leadership skills\nMandatory for anyone to have Good knowledge of any of the CSPs like Azure,AWS or GCP; Certifications on Azure will be additional Pl",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Pyspark', 'Data Engineering', 'Cloud', 'Microsoft Azure', 'Python', 'GCP', 'data engineer', 'Hadoop', 'AWS']",2025-06-13 05:11:28
Data Engineer,Clifyx Technology,5 - 8 years,Not Disclosed,['Bengaluru'],"BE/B Tech/MCA/MSc Comp science. -Only\nDetailed job description - Skill Set:\n7+ years of experience with data analytics, data modeling, and database design.\n5+ years of experience with Vertica.\n2+ years of coding and scripting (Python/Java/Scala) and design experience.\n2+ years of experience with Airflow.\nExperience with ELT methodologies and tools.\nExperience with GitHub.\nExpertise in tuning and troubleshooting SQL.\nStrong data integrity, analytical and multitasking skills.\nExcellent communication, problem solving, organizational and analytical skills.\nMandatory Skills\nSQL, Python and Vertica",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Analytical skills', 'vertica', 'Coding', 'Data modeling', 'Database design', 'Billing', 'data integrity', 'Troubleshooting', 'SQL', 'Python']",2025-06-13 05:11:29
Microsoft Fabrics Data Engineer,Swits Digital,5 - 10 years,Not Disclosed,['Bengaluru'],"Job TItle: Microsoft Fabric Data Engineer\nLocation: Bangalore\nJob Type: Conract (24 Months)\nJob Description:\nWe are seeking a highly skilled and experienced Microsoft Fabric Data Engineer/Architect to design, develop, and maintain robust, scalable, and secure data solutions within the Microsoft Fabric ecosystem. This role will leverage the full suite of Microsoft Azure data services, including Azure Data Bricks, Azure Data Factory, and Azure Data Lake, to build end-to-end data pipelines, data warehouses, and data lakehouses that enable advanced analytics and business intelligence.\nRequired Skills & Qualifications:\nBachelors degree in Computer Science, Engineering, or a related field.\n5+ years of experience in data architecture and engineering, with a strong focus on Microsoft Azure data platforms.\nProven hands-on expertise with Microsoft Fabric and its components, including:\nOneLake\nData Factory (Pipelines, Dataflows Gen2)\nSynapse Analytics (Data Warehousing, SQL analytics endpoint)\nLakehouses and Warehouses\nNotebooks (PySpark)\nExtensive experience with Azure Data Bricks, including Spark development (PySpark, Scala, SQL).\nStrong proficiency in Azure Data Factory for building and orchestrating ETL/ELT pipelines.\nDeep understanding and experience with Azure Data Lake Storage Gen2.\nProficiency in SQL (T-SQL, Spark SQL), Python, and/or other relevant scripting languages.\nSolid understanding of data warehousing concepts, dimensional modeling, and data lakehouse architectures.\nExperience with data governance principles and tools (e.g., Microsoft Purview).\nFamiliarity with CI/CD practices, version control (Git), and DevOps for data pipelines.\nExcellent problem-solving, analytical, and communication skills.\nAbility to work independently and collaboratively in a fast-paced, agile environment.\nPreferred Qualifications:\nMicrosoft certifications in Azure Data Engineering (e.g., DP-203, DP-600: Microsoft Fabric Analytics Engineer Associate).\nExperience with Power BI for data visualization and reporting.\nFamiliarity with real-time analytics and streaming data processing.\nExposure to machine learning workflows and integrating ML models with data solutions",Industry Type: Recruitment / Staffing,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['GIT', 'Analytical', 'microsoft azure', 'data visualization', 'microsoft', 'Business intelligence', 'Data warehousing', 'Analytics', 'Data architecture', 'Python']",2025-06-13 05:11:31
Data Engineer with Neo4j,Luxoft,3 - 5 years,Not Disclosed,['Gurugram'],"Graph Data Modeling & Implementation.\nDesign and implement complex graph data models using Cypher and Neo4j best practices.\nLeverage APOC procedures, custom plugins, and advanced graph algorithms to solve domain-specific problems.\nOversee integration of Neo4j with other enterprise systems, microservices, and data platforms.\nDevelop and maintain APIs and services in Java, Python, or JavaScript to interact with the graph database.\nMentor junior developers and review code to maintain high-quality standards.\nEstablish guidelines for performance tuning, scalability, security, and disaster recovery in Neo4j environments.\nWork with data scientists, analysts, and business stakeholders to translate complex requirements into graph-based solutions.\nSkills\nMust have\n12+ years in software/data engineering, with at least 3-5 years hands-on experience with Neo4j.\nLead the technical strategy, architecture, and delivery of Neo4j-based solutions.\nDesign, model, and implement complex graph data structures using Cypher and Neo4j best practices.\nGuide the integration of Neo4j with other data platforms and microservices.\nCollaborate with cross-functional teams to understand business needs and translate them into graph-based models.\nMentor junior developers and ensure code quality through reviews and best practices.\nDefine and enforce performance tuning, security standards, and disaster recovery strategies for Neo4j.\nStay up-to-date with emerging technologies in the graph database and data engineering space.\nStrong proficiency in Cypher query language, graph modeling, and data visualization tools (e.g., Bloom, Neo4j Browser).\nSolid background in Java, Python, or JavaScript and experience integrating Neo4j with these languages.\nExperience with APOC procedures, Neo4j plugins, and query optimization.\nFamiliarity with cloud platforms (AWS) and containerization tools (Docker, Kubernetes).\nProven experience leading engineering teams or projects.\nExcellent problem-solving and communication skills.\nNice to have\nN/A\nOther\nLanguages\nEnglish: C1 Advanced\nSeniority\nSenior\nRefer a Friend\nPositive work environments and stellar reputations attract and retain top talent. Find out why Luxoft stands apart from the rest.\nRecommend a friend\nRelated jobs View all vacancies\nSenior Flexera Data Analyst\nData Science\nIndia\nBengaluru\nSenior Flexera Data Analyst\nData Science\nIndia\nChennai\nData Scientist\nData Science\nIndia\nBengaluru\nGurugram, India\nReq. VR-114556\nData Science\nBCM Industry\n23/05/2025\nReq. VR-114556\nApply for Data Engineer with Neo4j in Gurugram\n*",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Performance tuning', 'query optimization', 'neo4j', 'data science', 'Data modeling', 'Disaster recovery', 'Javascript', 'Data structures', 'data visualization', 'Python']",2025-06-13 05:11:33
Data Engineer with Neo4j,Luxoft,3 - 5 years,Not Disclosed,['Bengaluru'],"Graph Data Modeling & Implementation.\nDesign and implement complex graph data models using Cypher and Neo4j best practices.\nLeverage APOC procedures, custom plugins, and advanced graph algorithms to solve domain-specific problems.\nOversee integration of Neo4j with other enterprise systems, microservices, and data platforms.\nDevelop and maintain APIs and services in Java, Python, or JavaScript to interact with the graph database.\nMentor junior developers and review code to maintain high-quality standards.\nEstablish guidelines for performance tuning, scalability, security, and disaster recovery in Neo4j environments.\nWork with data scientists, analysts, and business stakeholders to translate complex requirements into graph-based solutions.\nSkills\nMust have\n12+ years in software/data engineering, with at least 3-5 years hands-on experience with Neo4j.\nLead the technical strategy, architecture, and delivery of Neo4j-based solutions.\nDesign, model, and implement complex graph data structures using Cypher and Neo4j best practices.\nGuide the integration of Neo4j with other data platforms and microservices.\nCollaborate with cross-functional teams to understand business needs and translate them into graph-based models.\nMentor junior developers and ensure code quality through reviews and best practices.\nDefine and enforce performance tuning, security standards, and disaster recovery strategies for Neo4j.\nStay up-to-date with emerging technologies in the graph database and data engineering space.\nStrong proficiency in Cypher query language, graph modeling, and data visualization tools (e.g., Bloom, Neo4j Browser).\nSolid background in Java, Python, or JavaScript and experience integrating Neo4j with these languages.\nExperience with APOC procedures, Neo4j plugins, and query optimization.\nFamiliarity with cloud platforms (AWS) and containerization tools (Docker, Kubernetes).\nProven experience leading engineering teams or projects.\nExcellent problem-solving and communication skills.\nNice to have\nN/A\nOther\nLanguages\nEnglish: C1 Advanced\nSeniority\nSenior\nRefer a Friend\nPositive work environments and stellar reputations attract and retain top talent. Find out why Luxoft stands apart from the rest.\nRecommend a friend\nRelated jobs View all vacancies\nSenior Flexera Data Analyst\nData Science\nIndia\nGurugram\nSenior Flexera Data Analyst\nData Science\nIndia\nChennai\nBusiness Analyst\nData Science\nPoland\nRemote Poland\nBengaluru, India\nReq. VR-114556\nData Science\nBCM Industry\n23/05/2025\nReq. VR-114556\nApply for Data Engineer with Neo4j in Bengaluru\n*",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Performance tuning', 'query optimization', 'neo4j', 'data science', 'Data modeling', 'Disaster recovery', 'Javascript', 'Data structures', 'data visualization', 'Python']",2025-06-13 05:11:35
Data Engineer with Neo4j,Luxoft,3 - 5 years,Not Disclosed,['Chennai'],"Graph Data Modeling & Implementation.\nDesign and implement complex graph data models using Cypher and Neo4j best practices.\nLeverage APOC procedures, custom plugins, and advanced graph algorithms to solve domain-specific problems.\nOversee integration of Neo4j with other enterprise systems, microservices, and data platforms.\nDevelop and maintain APIs and services in Java, Python, or JavaScript to interact with the graph database.\nMentor junior developers and review code to maintain high-quality standards.\nEstablish guidelines for performance tuning, scalability, security, and disaster recovery in Neo4j environments.\nWork with data scientists, analysts, and business stakeholders to translate complex requirements into graph-based solutions.\nSkills\nMust have\n12+ years in software/data engineering, with at least 3-5 years hands-on experience with Neo4j.\nLead the technical strategy, architecture, and delivery of Neo4j-based solutions.\nDesign, model, and implement complex graph data structures using Cypher and Neo4j best practices.\nGuide the integration of Neo4j with other data platforms and microservices.\nCollaborate with cross-functional teams to understand business needs and translate them into graph-based models.\nMentor junior developers and ensure code quality through reviews and best practices.\nDefine and enforce performance tuning, security standards, and disaster recovery strategies for Neo4j.\nStay up-to-date with emerging technologies in the graph database and data engineering space.\nStrong proficiency in Cypher query language, graph modeling, and data visualization tools (e.g., Bloom, Neo4j Browser).\nSolid background in Java, Python, or JavaScript and experience integrating Neo4j with these languages.\nExperience with APOC procedures, Neo4j plugins, and query optimization.\nFamiliarity with cloud platforms (AWS) and containerization tools (Docker, Kubernetes).\nProven experience leading engineering teams or projects.\nExcellent problem-solving and communication skills.\nNice to have\nN/A\nOther\nLanguages\nEnglish: C1 Advanced\nSeniority\nSenior\nRefer a Friend\nPositive work environments and stellar reputations attract and retain top talent. Find out why Luxoft stands apart from the rest.\nRecommend a friend\nRelated jobs View all vacancies\nSenior Flexera Data Analyst\nData Science\nIndia\nGurugram\nSenior Flexera Data Analyst\nData Science\nIndia\nBengaluru\nData Scientist\nData Science\nIndia\nBengaluru\nChennai, India\nReq. VR-114556\nData Science\nBCM Industry\n23/05/2025\nReq. VR-114556\nApply for Data Engineer with Neo4j in Chennai\n*",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Performance tuning', 'query optimization', 'neo4j', 'data science', 'Data modeling', 'Disaster recovery', 'Javascript', 'Data structures', 'data visualization', 'Python']",2025-06-13 05:11:36
Data Engineer,Luxoft,5 - 8 years,Not Disclosed,['Pune'],"Help Group Enterprise Architecture team to develop our suite of EA tools and workbenches\nWork in the development team to support the development of portfolio health insights\nBuild data applications from cloud infrastructure to visualization layer\nProduce clear and commented code\nProduce clear and comprehensive documentation\nPlay an active role with technology support teams and ensure deliverables are completed or escalated on time\nProvide support on any related presentations, communications, and trainings\nBe a team player, working across the organization with skills to indirectly manage and influence\nBe a self-starter willing to inform and educate others\nSkills\nMust have\nB.Sc./M.Sc. degree in computing or similar\n5-8+ years experience as a Data Engineer, ideally in a large corporate environment\nIn-depth knowledge of SQL and data modelling/data processing\nStrong experience working with Microsoft Azure\nExperience with visualisation tools like PowerBI (or Tableau, QlikView or similar)\nExperience working with Git, JIRA, GitLab\nStrong flair for data analytics\nStrong flair for IT architecture and IT architecture metrics\nExcellent stakeholder interaction and communication skills\nUnderstanding of performance implications when making design decisions to deliver performant and maintainable software.\nExcellent end-to-end SDLC process understanding.\nProven track record of delivering complex data apps on tight timelines\nFluent in English both written and spoken.\nPassionate about development with focus on data and cloud\nAnalytical and logical, with strong problem solving skills\nA team player, comfortable with taking the lead on complex tasks\nAn excellent communicator who is adept in, handling ambiguity and communicating with both technical and non-technical audiences\nComfortable with working in cross-functional global teams to effect change\nPassionate about learning and developing your hard and soft professional skills\nNice to have\nExperience working in the financial industry\nExperience in complex metrics design and reporting\nExperience in using artificial intelligence for data analytics\nOther\nLanguages\nEnglish: C1 Advanced\nSeniority\nSenior\nRefer a Friend\nPositive work environments and stellar reputations attract and retain top talent. Find out why Luxoft stands apart from the rest.\nRecommend a friend\nRelated jobs View all vacancies\nSenior Power BI Developer\nBI Engineering\nIndia\nBengaluru\nSenior Power BI Developer\nBI Engineering\nIndia\nChennai\nSenior Power BI Developer\nBI Engineering\nIndia\nGurugram\nPune, India\nReq. VR-114797\nBI Engineering\nBCM Industry\n02/06/2025\nReq. VR-114797\nApply for Data Engineer in Pune\n*",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['GIT', 'Enterprise architecture', 'Analytical', 'Artificial Intelligence', 'Data processing', 'Data analytics', 'QlikView', 'JIRA', 'SDLC', 'SQL']",2025-06-13 05:11:38
Data Engineer,Amakshar Technology,4 - 7 years,Not Disclosed,"['Mumbai', 'Pune', 'Chennai', 'Bengaluru']","Job Category: IT\nJob Type: Full Time\nJob Location: Bangalore Chennai Mumbai Pune\nLocation- Mumbai, Pune, Bangalore, Chennai\nExperience- 5+\nData Engineer: Expertise in Python Language is MUST. SQL (should be able to write complex SQL Queries) is MUST Data Lake Development experience. Orchestration (Apache Airflow is preferred). Spark and Hive: Optimization of Spark/PySpark and Hive apps is MUST Trino/(AWS Athena) (Good to have) Snowflake (good to have). Data Quality (good to have). File Storage (S3 is good to have)\nKind Note: Please apply or share your resume only if it matches the above criteria",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['hive', 'SQL queries', 'orchestration', 'spark', 'Data quality', 'Apache', 'AWS', 'Python']",2025-06-13 05:11:40
Data Engineer,Lenskart,1 - 4 years,Not Disclosed,['Bengaluru'],"Key Responsibilities\nBuild and maintain scalable ETL/ELT data pipelines using Python and cloud-native tools.\nDesign and optimize data models and queries on Google BigQuery for analytical workloads.\nDevelop, schedule, and monitor workflows using orchestration tools like Apache Airflow or Cloud Composer.\nIngest and integrate data from multiple structured and semi-structured sources, including MySQL, MongoDB, APIs, and cloud storage.",,,,"['GCP', 'Bigquery', 'MySQL', 'MongoDB', 'Python']",2025-06-13 05:11:41
Data Engineer (5-10 Years) | @ Banking | Bangalore & Mumbai,Net Connect,5 - 10 years,Not Disclosed,"['Bengaluru', 'Mumbai (All Areas)']","Job Summary\nWe are seeking a skilled Data Engineer to design, develop, and optimize scalable data pipelines and infrastructure. The ideal candidate will have expertise in relational databases, data modeling, cloud migration, and automation, working closely with cross-functional teams to drive data-driven decision-making.\n\nKey Responsibilities",,,,"['Data Modeling', 'ETL', 'Scripting', 'SQL', 'Azure Data Factory', 'Informatica']",2025-06-13 05:11:43
Data Scientist (Data & AI Engineer),Visa,2 - 7 years,Not Disclosed,['Bengaluru'],"The Client Services BI & Analytics team strives to create an open, trusting data culture where the cost of curiosity - the number of steps, amount of time, and complexity of effort needed to use operational data to derive insights - is as low as possible. We govern Client Services operational data and metrics, create easily usable dashboards and data sources, and analyze data to share insights.\nWe are a part of the Client Services Global Business Operations function and work with all levels of stakeholders, from executive leaders sharing insights with the C-Suite to customer-facing colleagues who rely on our assets to incorporate data into their daily responsibilities.\nThis specialist role makes data available from new sources, builds robust data models, creates and optimizes data enrichment pipelines, and provides engineering support to specific projects. You will partner with our Data Visualizers and Solution Designers to ensure that data needed by the business is available and accurate and to develop certified data sets. This technical lead and architect role is a force multiplier to our Visualizers, Analysts, and other data users across Client Services.\nResponsibilities\nDesign, develop, and maintain scalable data pipelines and systems.\nMonitor and troubleshoot data pipeline issues to ensure seamless data flow.\nEstablish data processes and automation based on business and technology requirements, leveraging Visa s supported data platforms and tools\nDeliver small to large data engineering and Machine learning projects either individually or as part of a project team\nSetup ML Ops pipelines to Productionalize ML models and setting up Gen AI pipelines\nCollaborate with cross-functional teams to understand data requirements and ensure data quality, with a focus on implementing data validation and data quality checks at various stages of the pipeline\nProvide expertise in data warehousing, ETL, and data modeling to support data-driven decision making, with a strong understanding of best practices in data pipeline design and performance optimization\nExtract and manipulate large datasets using standard tools such as Hadoop (Hive), Spark, Python (pandas, NumPy), Presto, and SQL\nDevelop data solutions using Agile principles\nProvide ongoing production support\nCommunicate complex concepts in a clear and effective manner\nStay up to date with the latest data engineering trends and technologies to ensure the companys data infrastructure is always state-of-the-art, with an understanding of best practices in cloud-based data engineering\nThis is a remote position. A remote position does not require job duties be performed within proximity of a Visa office location. Remote positions may be required to be present at a Visa office with scheduled notice.\n\n\nBasic Qualifications\n-2 or more years of work experience with a Bachelor s Degree or an Advanced Degree (e.g. Masters, MBA, JD, MD, or PhD)\n\nPreferred Qualifications\n-3 or more years of work experience with a Bachelor s Degree or more than 2 years of work experience with an Advanced Degree (e.g. Masters, MBA, JD, MD)\n-3+ years of work experience with a bachelor s degree in the STEM field.\n-Strong experience with SQL, Python, Hadoop, Spark, Hive, Airflow and MPP data bases\n-5+ years of analytics experience with a focus on data Engineering and AI\n-Experience with both traditional data warehousing tools and techniques (such as SSIS, ODI, and on-prem SQL Server, Oracle) as well as modern technologies (such as Hadoop, Denodo, Spark, Airflow, and Python), and a solid understanding of best practices in data engineering\n-Advanced knowledge of SQL (e.g., understands subqueries, self-joining tables, stored procedures, can read an execution plan, SQL tuning, etc.)\n-Solid understanding of best practices in data warehousing, ETL, data modeling, and data architecture.\n-Experience with NoSQL databases (e.g., MongoDB, Cassandra)\n-Experience with cloud-based data warehousing and data pipeline management (AWS, GCP, Azure)\n-Experience in Python, Spark, and exposure to scheduling tools like Tuber/Airflow is preferred.\n-Able to create data dictionaries, setup and monitor data validation alerts, and execute periodic jobs to maintain data pipelines for completed projects\n-Experience with visualization software (e.g., Tableau, QlikView, PowerBI) is a plus.\n-A team player and collaborator, able to work well with a diverse group of individuals in a matrixed environment",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Automation', 'Production support', 'Data modeling', 'Agile', 'Stored procedures', 'QlikView', 'Oracle', 'SSIS', 'Analytics', 'Python']",2025-06-13 05:11:44
Big Data Engineer - Hadoop,Info Origin Technologies Pvt Ltd,3 - 7 years,Not Disclosed,"['Hyderabad', 'Gurugram']","Role: Hadoop Data Engineer\nLocation: Gurgaon / Hyderabad\nWork Mode: Hybrid\nEmployment Type: Full-Time\nInterview Mode: First Video then In Person\nJob Description\nJob Overview:\nWe are looking for experienced Data Engineers proficient in Hadoop, Hive, Python, SQL, and Pyspark/Spark to join our dynamic team. Candidates will be responsible for designing, developing, and maintaining scalable big data solutions.\nKey Responsibilities:\nDevelop and optimize data pipelines for large-scale data processing.\nWork with structured and unstructured datasets to derive actionable insights.\nCollaborate with cross-functional teams to enhance data-driven decision-making.\nEnsure the performance, scalability, and reliability of data architectures.\nImplement best practices for data security and governance.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Hive', 'Hadoop', 'Pyspark', 'Big Data', 'Python', 'SQL']",2025-06-13 05:11:46
Senior Data Engineer- (Big data and Data Pipelines),Findem,5 - 9 years,Not Disclosed,"['New Delhi', 'Bengaluru']","What is Findem:\n\nFindem is the only talent data platform that combines 3D data with AI. It automates and consolidates top-of-funnel activities across your entire talent ecosystem, bringing together sourcing, CRM, and analytics into one place. Only 3D data connects people and company data over time - making an individual s entire career instantly accessible in a single click, removing the guesswork, and unlocking insights about the market and your competition no one else can. Powered by 3D data, Findem s automated workflows across the talent lifecycle are the ultimate competitive advantage. Enabling talent teams to deliver continuous pipelines of top, diverse candidates while creating better talent experiences, Findem transforms the way companies plan, hire, and manage talent. Learn more at www.findem.ai\n\nExperience - 5 - 9 years\n\nWe are looking for an experienced Big Data Engineer, who will be responsible for building, deploying and managing various data pipelines, data lake and Big data processing solutions using Big data and ETL technologies.\n\nLocation- Delhi, India\nHybrid- 3 days onsite\nResponsibilities\nBuild data pipelines, Big data processing solutions and data lake infrastructure using various Big data and ETL technologies\nAssemble and process large, complex data sets that meet functional non-functional business requirements ETL from a wide variety of sources like MongoDB, S3, Server-to-Server, Kafka etc., and processing using SQL and big data technologies\nBuild analytical tools to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics Build interactive and ad-hoc query self-serve tools for analytics use cases\nBuild data models and data schema for performance, scalability and functional requirement perspective Build processes supporting data transformation, metadata, dependency and workflow management\nResearch, experiment and prototype new tools/technologies and make them successful\nSkill Requirements\nMust have-Strong in Python/Scala\nMust have experience in Big data technologies like Spark, Hadoop, Athena / Presto, Redshift, Kafka etc\nExperience in various file formats like parquet, JSON, Avro, orc etc\nExperience in workflow management tools like airflow Experience with batch processing, streaming and message queues\nAny of visualization tools like Redash, Tableau, Kibana etc\nExperience in working with structured and unstructured data sets\nStrong problem solving skills\nGood to have\nExposure to NoSQL like MongoDB\nExposure to Cloud platforms like AWS, GCP, etc\nExposure to Microservices architecture\nExposure to Machine learning techniques\nThe role is full-time and comes with full benefits. We are globally headquartered in the San Francisco Bay Area with our India headquarters in Bengaluru.\n\nEqual Opportunity",Industry Type: Management Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SAN', 'metadata', 'Prototype', 'Machine learning', 'Schema', 'JSON', 'Analytics', 'SQL', 'CRM', 'Python']",2025-06-13 05:11:48
Data Engineer (C2H),First Mile Consulting,4 - 8 years,Not Disclosed,"['Hyderabad', 'Pune', 'Chennai', 'Bengaluru']","Very strong in python, pyspark and SQL. Good experience in any cloud . They use AWS but any cloud experience is ok. They will train on other things but if candidates have experience with ETL (like AWS Airflow), datalakes like Snowflake",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Temporary/Contractual","['Pyspark', 'Cloud', 'Data engineer', 'Python', 'Sql', 'Airflow']",2025-06-13 05:11:50
Data Engineer,Prohr Strategies,9 - 11 years,Not Disclosed,['Bengaluru'],"Hands-on Data Engineer with strong Databricks expertise in Git/DevOps integration, Unity Catalog governance, and performance tuning of data transformation workloads. Skilled in optimizing pipelines and ensuring secure, efficient data operations.",Industry Type: Emerging Technologies (AI/ML),Department: Data Science & Analytics,"Employment Type: Full Time, Temporary/Contractual","['Data Transformation', 'GIT', 'Azure Databricks', 'Databricks', 'Devops', 'Data Engineering', 'Governance', 'Catalog', 'Code Versioning Tools']",2025-06-13 05:11:51
Data Engineer - Ranchi,Teqfocus Consulting,0 - 2 years,Not Disclosed,['Ranchi'],"Job Title: Data Engineer\nExperience: 1+ Years (Freshers with relevant training & certification may apply)\nLocation: Ranchi (Work from Office)\n\nJob Summary:\nWe are looking for a Data Engineer with at least 1 year of hands-on experience in data engineering practices. The ideal candidate will work closely with our data and analytics teams to build robust and scalable data pipelines. Experience with Snowflake is a plus.\n\nKey Responsibilities:\nDesign, build, and maintain data pipelines using modern data engineering tools.\nTransform and clean data from multiple sources for reporting and analytics.\nOptimize data pipelines for performance and scalability.\nCollaborate with cross-functional teams including BI, analytics, and application developers.\nMonitor, troubleshoot, and maintain data workflows.\n\nRequired Skills:\nStrong understanding of data warehousing concepts.\nProficiency in SQL and Python.\nKnowledge of ETL tools and processes.\nFamiliarity with cloud platforms such as AWS, Snowflake, Databricks, Azure or GCP.\nExposure to data visualization tools is a plus.\n\nGood to have any of below certification:\nSnowflake SnowPro Core Certification\nSnowflake Advanced: Data Engineer Certification\nGoogle Cloud Professional Data Engineer\nMicrosoft Certified: Azure Data Engineer Associate\nAWS Certified Data Analytics Specialty\n\nQualifications:\nBachelor's degree in Computer Science, Information Technology, or related field.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Snowflake', 'AWS', 'Data Bricks', 'GCP', 'Microsoft Azure', 'Python']",2025-06-13 05:11:53
Data Engineer - Python Programming,Leading Client,5 - 7 years,Not Disclosed,['Indore'],"We are looking for a skilled Data Engineer with strong hands-on experience in Clickhouse, Kubernetes, SQL, Python, and FastAPI, along with a good understanding of PostgreSQL.\nThe ideal candidate will be responsible for building and maintaining efficient data pipelines, optimizing query performance, and developing APIs to support scalable data services.\n\n- Design, build, and maintain scalable and efficient data pipelines and ETL processes.\n\n- Develop and optimize Clickhouse databases for high-performance analytics.\n\n- Create RESTful APIs using FastAPI to expose data services.\n\n- Work with Kubernetes for container orchestration and deployment of data services.\n\n- Write complex SQL queries to extract, transform, and analyze data from PostgreSQL and Clickhouse.\n\n- Collaborate with data scientists, analysts, and backend teams to support data needs and ensure data quality.\n\n- Monitor, troubleshoot, and improve performance of data infrastructure.\n\n- Strong experience in Clickhouse - data modeling, query optimization, performance tuning.\n\n- Expertise in SQL - including complex joins, window functions, and optimization.\n\n- Proficient in Python, especially for data processing (Pandas, NumPy) and scripting.\n\n- Experience with FastAPI for creating lightweight APIs and microservices.\n\n- Hands-on experience with PostgreSQL - schema design, indexing, and performance.\n\n- Solid knowledge of Kubernetes managing containers, deployments, and scaling.\n\n- Understanding of software engineering best practices (CI/CD, version control, testing).\n\n- Experience with cloud platforms like AWS, GCP, or Azure.\n\n- Knowledge of data warehousing and distributed data systems.\n\n- Familiarity with Docker, Helm, and monitoring tools like Prometheus/Grafana.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'PostgreSQL', 'Data Modeling', 'Data Warehousing', 'Data Analytics', 'ETL', 'Python', 'SQL']",2025-06-13 05:11:55
Data Engineer - Python Programming,Leading Client,5 - 7 years,Not Disclosed,['Pune'],"We are looking for a skilled Data Engineer with strong hands-on experience in Clickhouse, Kubernetes, SQL, Python, and FastAPI, along with a good understanding of PostgreSQL.\nThe ideal candidate will be responsible for building and maintaining efficient data pipelines, optimizing query performance, and developing APIs to support scalable data services.\n\n- Design, build, and maintain scalable and efficient data pipelines and ETL processes.\n\n- Develop and optimize Clickhouse databases for high-performance analytics.\n\n- Create RESTful APIs using FastAPI to expose data services.\n\n- Work with Kubernetes for container orchestration and deployment of data services.\n\n- Write complex SQL queries to extract, transform, and analyze data from PostgreSQL and Clickhouse.\n\n- Collaborate with data scientists, analysts, and backend teams to support data needs and ensure data quality.\n\n- Monitor, troubleshoot, and improve performance of data infrastructure.\n\n- Strong experience in Clickhouse - data modeling, query optimization, performance tuning.\n\n- Expertise in SQL - including complex joins, window functions, and optimization.\n\n- Proficient in Python, especially for data processing (Pandas, NumPy) and scripting.\n\n- Experience with FastAPI for creating lightweight APIs and microservices.\n\n- Hands-on experience with PostgreSQL - schema design, indexing, and performance.\n\n- Solid knowledge of Kubernetes managing containers, deployments, and scaling.\n\n- Understanding of software engineering best practices (CI/CD, version control, testing).\n\n- Experience with cloud platforms like AWS, GCP, or Azure.\n\n- Knowledge of data warehousing and distributed data systems.\n\n- Familiarity with Docker, Helm, and monitoring tools like Prometheus/Grafana.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'PostgreSQL', 'Data Modeling', 'Data Warehousing', 'Data Analytics', 'ETL', 'Python', 'SQL']",2025-06-13 05:11:57
Data Engineer - Python Programming,Leading Client,5 - 7 years,Not Disclosed,"['Mumbai', 'Any Location']","We are looking for a skilled Data Engineer with strong hands-on experience in Clickhouse, Kubernetes, SQL, Python, and FastAPI, along with a good understanding of PostgreSQL.\nThe ideal candidate will be responsible for building and maintaining efficient data pipelines, optimizing query performance, and developing APIs to support scalable data services.\n\n- Design, build, and maintain scalable and efficient data pipelines and ETL processes.\n\n- Develop and optimize Clickhouse databases for high-performance analytics.\n\n- Create RESTful APIs using FastAPI to expose data services.\n\n- Work with Kubernetes for container orchestration and deployment of data services.\n\n- Write complex SQL queries to extract, transform, and analyze data from PostgreSQL and Clickhouse.\n\n- Collaborate with data scientists, analysts, and backend teams to support data needs and ensure data quality.\n\n- Monitor, troubleshoot, and improve performance of data infrastructure.\n\n- Strong experience in Clickhouse - data modeling, query optimization, performance tuning.\n\n- Expertise in SQL - including complex joins, window functions, and optimization.\n\n- Proficient in Python, especially for data processing (Pandas, NumPy) and scripting.\n\n- Experience with FastAPI for creating lightweight APIs and microservices.\n\n- Hands-on experience with PostgreSQL - schema design, indexing, and performance.\n\n- Solid knowledge of Kubernetes managing containers, deployments, and scaling.\n\n- Understanding of software engineering best practices (CI/CD, version control, testing).\n\n- Experience with cloud platforms like AWS, GCP, or Azure.\n\n- Knowledge of data warehousing and distributed data systems.\n\n- Familiarity with Docker, Helm, and monitoring tools like Prometheus/Grafana.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'PostgreSQL', 'Data Modeling', 'Data Warehousing', 'Data Analytics', 'ETL', 'Python', 'SQL']",2025-06-13 05:11:59
Data Engineer - Python Programming,Leading Client,5 - 7 years,Not Disclosed,['Nagpur'],"We are looking for a skilled Data Engineer with strong hands-on experience in Clickhouse, Kubernetes, SQL, Python, and FastAPI, along with a good understanding of PostgreSQL.\nThe ideal candidate will be responsible for building and maintaining efficient data pipelines, optimizing query performance, and developing APIs to support scalable data services.\n\n- Design, build, and maintain scalable and efficient data pipelines and ETL processes.\n\n- Develop and optimize Clickhouse databases for high-performance analytics.\n\n- Create RESTful APIs using FastAPI to expose data services.\n\n- Work with Kubernetes for container orchestration and deployment of data services.\n\n- Write complex SQL queries to extract, transform, and analyze data from PostgreSQL and Clickhouse.\n\n- Collaborate with data scientists, analysts, and backend teams to support data needs and ensure data quality.\n\n- Monitor, troubleshoot, and improve performance of data infrastructure.\n\n- Strong experience in Clickhouse - data modeling, query optimization, performance tuning.\n\n- Expertise in SQL - including complex joins, window functions, and optimization.\n\n- Proficient in Python, especially for data processing (Pandas, NumPy) and scripting.\n\n- Experience with FastAPI for creating lightweight APIs and microservices.\n\n- Hands-on experience with PostgreSQL - schema design, indexing, and performance.\n\n- Solid knowledge of Kubernetes managing containers, deployments, and scaling.\n\n- Understanding of software engineering best practices (CI/CD, version control, testing).\n\n- Experience with cloud platforms like AWS, GCP, or Azure.\n\n- Knowledge of data warehousing and distributed data systems.\n\n- Familiarity with Docker, Helm, and monitoring tools like Prometheus/Grafana.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Python Programming', 'Data Engineering', 'PostgreSQL', 'Data Modeling', 'Data Warehousing', 'Data Analytics', 'ETL', 'Python', 'SQL']",2025-06-13 05:12:00
Data Engineer - Python Programming,Leading Client,5 - 7 years,Not Disclosed,['Ahmedabad'],"We are looking for a skilled Data Engineer with strong hands-on experience in Clickhouse, Kubernetes, SQL, Python, and FastAPI, along with a good understanding of PostgreSQL.\nThe ideal candidate will be responsible for building and maintaining efficient data pipelines, optimizing query performance, and developing APIs to support scalable data services.\n\n- Design, build, and maintain scalable and efficient data pipelines and ETL processes.\n\n- Develop and optimize Clickhouse databases for high-performance analytics.\n\n- Create RESTful APIs using FastAPI to expose data services.\n\n- Work with Kubernetes for container orchestration and deployment of data services.\n\n- Write complex SQL queries to extract, transform, and analyze data from PostgreSQL and Clickhouse.\n\n- Collaborate with data scientists, analysts, and backend teams to support data needs and ensure data quality.\n\n- Monitor, troubleshoot, and improve performance of data infrastructure.\n\n- Strong experience in Clickhouse - data modeling, query optimization, performance tuning.\n\n- Expertise in SQL - including complex joins, window functions, and optimization.\n\n- Proficient in Python, especially for data processing (Pandas, NumPy) and scripting.\n\n- Experience with FastAPI for creating lightweight APIs and microservices.\n\n- Hands-on experience with PostgreSQL - schema design, indexing, and performance.\n\n- Solid knowledge of Kubernetes managing containers, deployments, and scaling.\n\n- Understanding of software engineering best practices (CI/CD, version control, testing).\n\n- Experience with cloud platforms like AWS, GCP, or Azure.\n\n- Knowledge of data warehousing and distributed data systems.\n\n- Familiarity with Docker, Helm, and monitoring tools like Prometheus/Grafana.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Python Programming', 'Data Engineering', 'PostgreSQL', 'Data Modeling', 'Data Warehousing', 'Data Analytics', 'ETL', 'Python', 'SQL']",2025-06-13 05:12:02
Data Engineer - Python Programming,Leading Client,5 - 7 years,Not Disclosed,['Delhi / NCR'],"We are looking for a skilled Data Engineer with strong hands-on experience in Clickhouse, Kubernetes, SQL, Python, and FastAPI, along with a good understanding of PostgreSQL.\nThe ideal candidate will be responsible for building and maintaining efficient data pipelines, optimizing query performance, and developing APIs to support scalable data services.\n\n- Design, build, and maintain scalable and efficient data pipelines and ETL processes.\n\n- Develop and optimize Clickhouse databases for high-performance analytics.\n\n- Create RESTful APIs using FastAPI to expose data services.\n\n- Work with Kubernetes for container orchestration and deployment of data services.\n\n- Write complex SQL queries to extract, transform, and analyze data from PostgreSQL and Clickhouse.\n\n- Collaborate with data scientists, analysts, and backend teams to support data needs and ensure data quality.\n\n- Monitor, troubleshoot, and improve performance of data infrastructure.\n\n- Strong experience in Clickhouse - data modeling, query optimization, performance tuning.\n\n- Expertise in SQL - including complex joins, window functions, and optimization.\n\n- Proficient in Python, especially for data processing (Pandas, NumPy) and scripting.\n\n- Experience with FastAPI for creating lightweight APIs and microservices.\n\n- Hands-on experience with PostgreSQL - schema design, indexing, and performance.\n\n- Solid knowledge of Kubernetes managing containers, deployments, and scaling.\n\n- Understanding of software engineering best practices (CI/CD, version control, testing).\n\n- Experience with cloud platforms like AWS, GCP, or Azure.\n\n- Knowledge of data warehousing and distributed data systems.\n\n- Familiarity with Docker, Helm, and monitoring tools like Prometheus/Grafana.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Python Programming', 'Data Engineering', 'PostgreSQL', 'Data Modeling', 'Data Warehousing', 'Data Analytics', 'ETL', 'Python', 'SQL']",2025-06-13 05:12:04
Data Engineer _Technology Lead,Broadridge,6 - 10 years,Not Disclosed,['Bengaluru'],"Key Responsibilities:\nAnalyzes and solve problems using technical experience, judgment and precedents\nProvides informal guidance to new team members\nExplains complex information to others in straightforward situations\n1. Data Engineering and Modelling:\nDesign & Develop Scalable Data Pipelines: Leverage AWS technologies to design, develop, and manage end-to-end data pipelines with services like .",,,,"['Star Schema', 'Snowflake', 'AWS', 'Apache Airflow']",2025-06-13 05:12:05
Lead Data Engineer,Acuity Knowledge Partners,8 - 13 years,Not Disclosed,"['Pune', 'Gurugram', 'Bengaluru']","Preferred candidate profile\n\n9+ years of overall experience in software development with a focus on data projects using Python, PySpark, and associated frameworks.\nProven experience as a Data Engineer with experience in Azure cloud.\nExperience implementing solutions using Azure cloud services, Azure Data Factory, Azure Lake Gen 2, Azure Databases, Azure Data Fabric, API Gateway management, Azure Functions.",,,,"['Pyspark', 'Azure Cloud', 'Python', 'SQL']",2025-06-13 05:12:07
Data Engineer Sr. Analyst,Accenture,5 - 7 years,Not Disclosed,['Kochi'],"Job Title - + +\n\n\n\nManagement Level:\n\n\n\nLocation:Kochi, Coimbatore, Trivandrum\n\n\n\nMust have skills:Databricks including Spark-based ETL, Delta Lake\n\n\n\n\nGood to have skills:Pyspark\n\n\n\nJob\n\n\nSummary\n\nWe are seeking a highly skilled and experienced Senior Data Engineer to join our growing Data and Analytics team. The ideal candidate will have deep expertise in Databricks and cloud data warehousing, with a proven track record of designing and building scalable data pipelines, optimizing data architectures, and enabling robust analytics capabilities. This role involves working collaboratively with cross-functional teams to ensure the organization leverages data as a strategic asset. Your responsibilities will include:\n\n\n\nRoles and Responsibilities\nDesign, build, and maintain scalable data pipelines and ETL processes using Databricks and other modern tools.\nArchitect, implement, and manage cloud-based data warehousing solutions on Databricks (Lakehouse Architecture)\nDevelop and maintain optimized data lake architectures to support advanced analytics and machine learning use cases.\nCollaborate with stakeholders to gather requirements, design solutions, and ensure high-quality data delivery.\nOptimize data pipelines for performance and cost efficiency.\nImplement and enforce best practices for data governance, access control, security, and compliance in the cloud.\nMonitor and troubleshoot data pipelines to ensure reliability and accuracy.\nLead and mentor junior engineers, fostering a culture of continuous learning and innovation.\nExcellent communication skills\nAbility to work independently and along with client based out of western Europe.\n\n\n\nProfessional and Technical Skills\n3.5-5 years of experience in Data Engineering roles with a focus on cloud platforms.\nProficiency in Databricks, including Spark-based ETL, Delta Lake, and SQL.\nStrong experience with one or more cloud platforms (AWS preferred).\nHandson Experience with Delta lake, Unity Catalog, and Lakehouse architecture concepts.\nStrong programming skills in Python and SQL; experience with Pyspark a plus.\nSolid understanding of data modeling concepts and practices (e.g., star schema, dimensional modeling).\nKnowledge of CI/CD practices and version control systems (e.g., Git).\nFamiliarity with data governance and security practices, including GDPR and CCPA compliance.\n\n\n\n\nAdditional Information\nExperience with Airflow or similar workflow orchestration tools.\nExposure to machine learning workflows and MLOps.\nCertification in Databricks, AWS\nFamiliarity with data visualization tools such as Power BI\n\n(do not remove the hyperlink)Qualification\n\n\n\nExperience:3.5 -5 years of experience is required\n\n\n\n\nEducational Qualification:Graduation (Accurate educational details should capture)",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data warehousing', 'sql', 'data modeling', 'python', 'data bricks', 'hive', 'kubernetes', 'catalog', 'pyspark', 'data architecture', 'docker', 'ansible', 'git', 'java', 'spark', 'devops', 'hadoop', 'etl', 'big data', 'data lake', 'airflow', 'power bi', 'cloud platforms', 'machine learning', 'data engineering', 'aws']",2025-06-13 05:12:09
"Senior Manager, Senior Data Engineer",Merck Sharp & Dohme (MSD),6 - 11 years,Not Disclosed,['Hyderabad'],"Senior Manager, Data Engineer\nThe Opportunity\nBased in Hyderabad, join a global healthcare biopharma company and be part of a 130- year legacy of success backed by ethical integrity, forward momentum, and an inspiring mission to achieve new milestones in global healthcare.\nBe part of an organisation driven by digital technology and data-backed approaches that support a diversified portfolio of prescription medicines, vaccines, and animal health products.\nDrive innovation and execution excellence. Be a part of a team with passion for using data, analytics, and insights to drive decision-making, and which creates custom software, allowing us to tackle some of the worlds greatest health threats.\nOur Technology Centers focus on creating a space where teams can come together to deliver business solutions that save and improve lives. An integral part of our company s IT operating model, Tech Centers are globally distributed locations where each IT division has employees to enable our digital transformation journey and drive business outcomes. These locations, in addition to the other sites, are essential to supporting our business and strategy.\nA focused group of leaders in each Tech Center helps to ensure we can manage and improve each location, from investing in growth, success, and well-being of our people, to making sure colleagues from each IT division feel a sense of belonging to managing critical emergencies. And together, we must leverage the strength of our team to collaborate globally to optimize connections and share best practices across the Tech Centers.\nRole Overview\nResponsibilities\nDesigns, builds, and maintains data pipeline architecture - ingest, process, and publish data for consumption.\nBatch processes collected data, formats data in an optimized way to bring it analyze-ready\nEnsures best practices sharing and across the organization\nEnables delivery of data-analytics projects\nDevelops deep knowledge of the companys supported technology; understands the whole complexity/dependencies between multiple teams, platforms (people, technologies)\nCommunicates intensively with other platform/competencies to comprehend new trends and methodologies being implemented/considered within the company ecosystem\nUnderstands the customer and stakeholders business needs/priorities and helps building solutions that support our business goals\nEstablishes and manages the close relationship with customers/stakeholders\nHas overview of the date engineering market development to be able to come up/explore new ways of delivering pipelines to increase their value/contribution\nBuilds community of practice leveraging experience from delivering complex analytics projects\nIs accountable for ensuring that the team delivers solutions with high quality standards, timeliness, compliance and excellent user experience\nContributes to innovative experiments, specifically to idea generation, idea incubation and/or experimentation, identifying tangible and measurable criteria\nQualifications:\nBachelor s degree in Computer Science, Data Science, Information Technology, Engineering or a related field.\n3+ plus years of experience as a Data Engineer or in a similar role, with a strong portfolio of data projects.\n3+ plus years experience SQL skills, with the ability to write and optimize queries for large datasets.\n1+ plus years experience and proficiency in Python for data manipulation, automation, and pipeline development.\nExperience with Databricks including creating notebooks and utilizing Spark for big data processing.\nStrong experience with data warehousing solution (such as Snowflake), including schema design and performance optimization.\nExperience with data governance and quality management tools, particularly Collibra DQ.\nStrong analytical and problem-solving skills, with an attention to detail.\nSAP Basis experience working on SAP S/4HANA deployments on Cloud platforms (example: AWS, GCP or Azure).\nOur technology teams operate as business partners, proposing ideas and innovative solutions that enable new organizational capabilities. We collaborate internationally to deliver services and solutions that help everyone be more productive and enable innovation.\nWho we are:\nWhat we look for:\n#HYDIT\nCurrent Employees apply HERE\nCurrent Contingent Workers apply HERE\nSearch Firm Representatives Please Read Carefully\nEmployee Status:\nRegular\nRelocation:\nVISA Sponsorship:\nTravel Requirements:\nFlexible Work Arrangements:\nHybrid\nShift:\nValid Driving License:\nHazardous Material(s):\n\nRequired Skills:\nBusiness, Business, Business Intelligence (BI), Business Management, Contractor Management, Cost Reduction, Database Administration, Database Optimization, Data Engineering, Data Flows, Data Infrastructure, Data Management, Data Modeling, Data Optimization, Data Quality, Data Visualization, Design Applications, ETL Tools, Information Management, Management Process, Operating Cost Reduction, Senior Program Management, Social Collaboration, Software Development, Software Development Life Cycle (SDLC) {+ 1 more}\n\nPreferred Skills:\nJob Posting End Date:\n08/13/2025\n*A job posting is effective until 11:59:59PM on the day BEFORE the listed job posting end date. Please ensure you apply to a job posting no later than the day BEFORE the job posting end date.",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Automation', 'Data management', 'Data modeling', 'Analytical', 'Healthcare', 'Data processing', 'Business intelligence', 'Information technology', 'Analytics', 'SQL']",2025-06-13 05:12:10
Senior Data Engineer,Randstad Global,5 - 8 years,Not Disclosed,['Hyderabad'],"Key Responsibilities\n\nDesign, develop, and maintain data pipelines and ETL processes focused on data transformation using SQL, Python, and DBT.\nDevelop complex SQL queries for data extraction, transformation, and loading (ETL), with focus on aggregation, cleansing, and modeling.\nLeverage Python to automate transformation tasks and implement custom logic in data workflows.\nBuild and manage DBT models for reusable, maintainable, and scalable data pipelines.\nAssemble reusable and scalable datasets aligned with business needs.\nEnsure data accuracy, consistency, and completeness through thorough validation, testing, and documentation.\nDevelop and maintain data storage solutions and implement transformation logic for analysis and reporting.\nCollaborate with cross-functional stakeholders to understand data requirements and provide actionable insights.\nTroubleshoot and resolve data-related technical issues and identify opportunities to improve data quality and reliability.\nDocument technical specifications and data transformation processes.\nAdhere to information security policies, ensure data compliance with PII, GDPR, and other relevant regulations.\nRequired Skills & Experience\n\nHands-on experience with Google Cloud Platform (GCP) and Google BigQuery.\nStrong expertise in SQL, database design, and query optimization.\nProven experience in designing and implementing ETL pipelines and data transformation flows.\nTechnical proficiency in data modeling, data mining, and segmentation techniques.\nExcellent numerical, analytical, and problem-solving skills.\nStrong attention to detail and a critical mindset for evaluating information.\nEffective stakeholder management and ability to handle multiple priorities.\nExcellent verbal and written communication skills.\nAbility to self-manage workload and work collaboratively within a team.\nDesirable Skills\n\nProficiency in SQL and Python programming.\nHands-on experience with DBT and version control tools like GitLab.\nFamiliarity with complex financial data models.\nExperience with data visualization tools such as Tableau or Google Data Studio.\nExposure to Salesforce and Salesforce Einstein Analytics.\nUnderstanding of Agile/Scrum methodologies.\nBachelor's degree in Computer Science, Information Technology, or a related field.\nData engineering certification and basic knowledge of AI/ML fundamentals are a plus.",Industry Type: Recruitment / Staffing,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Python', 'Airflow', 'Google Cloud Platforms', 'ETL', 'SQL']",2025-06-13 05:12:12
Data Engineer Sr. Analyst,Accenture,2 - 3 years,Not Disclosed,['Kochi'],"Job Title - + +\n\n\n\nManagement Level:\n\n\n\nLocation:Kochi, Coimbatore, Trivandrum\n\n\n\nMust have skills:Python/Scala, Pyspark/Pytorch\n\n\n\n\nGood to have skills:Redshift\n\n\n\nJob\n\n\nSummary\n\nYoull capture user requirements and translate them into business and digitally enabled solutions across a range of industries. Your responsibilities will include:\n\n\n\nRoles and Responsibilities\nDesigning, developing, optimizing, and maintaining data pipelines that adhere to ETL principles and business goals\nSolving complex data problems to deliver insights that helps our business to achieve their goals.\nSource data (structured unstructured) from various touchpoints, format and organize them into an analyzable format.\nCreating data products for analytics team members to improve productivity\nCalling of AI services like vision, translation etc. to generate an outcome that can be used in further steps along the pipeline.\nFostering a culture of sharing, re-use, design and operational efficiency of data and analytical solutions\nPreparing data to create a unified database and build tracking solutions ensuring data quality\nCreate Production grade analytical assets deployed using the guiding principles of CI/CD.\n\n\nProfessional and Technical Skills\nExpert in Python, Scala, Pyspark, Pytorch, Javascript (any 2 at least)\nExtensive experience in data analysis (Big data- Apache Spark environments), data libraries (e.g. Pandas, SciPy, Tensorflow, Keras etc.), and SQL. 2-3 years of hands-on experience working on these technologies.\nExperience in one of the many BI tools such as Tableau, Power BI, Looker.\nGood working knowledge of key concepts in data analytics, such as dimensional modeling, ETL, reporting/dashboarding, data governance, dealing with structured and unstructured data, and corresponding infrastructure needs.\nWorked extensively in Microsoft Azure (ADF, Function Apps, ADLS, Azure SQL), AWS (Lambda,Glue,S3), Databricks analytical platforms/tools, Snowflake Cloud Datawarehouse.\n\n\n\n\nAdditional Information\nExperience working in cloud Data warehouses like Redshift or Synapse\nCertification in any one of the following or equivalent\nAWS- AWS certified data Analytics- Speciality\nAzure- Microsoft certified Azure Data Scientist Associate\nSnowflake- Snowpro core- Data Engineer\nDatabricks Data Engineering\n\nQualification\n\n\n\nExperience:3.5 -5 years of experience is required\n\n\n\n\nEducational Qualification:Graduation (Accurate educational details should capture)",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['scala', 'pyspark', 'pytorch', 'python', 'microsoft azure', 'glue', 'amazon redshift', 'sql', 'tensorflow', 'sql azure', 'spark', 'keras', 'big data', 'etl', 'scipy', 'snowflake', 'data analysis', 'azure data lake', 'power bi', 'data engineering', 'javascript', 'pandas', 'data bricks', 'tableau', 'lambda expressions', 'aws']",2025-06-13 05:12:14
Data Engineer - Senior Analyst,Accenture,2 - 3 years,Not Disclosed,['Kochi'],"Job Title - + +\n\n\n\nManagement Level:\n\n\n\nLocation:Kochi, Coimbatore, Trivandrum\n\n\n\nMust have skills:Python/Scala, Pyspark/Pytorch\n\n\n\n\nGood to have skills:Redshift\n\n\n\nExperience:3.5 -5 years of experience is required\n\n\n\n\nEducational Qualification:Graduation (Accurate educational details should capture)\n\n\n\nJob\n\n\nSummary\n\nYoull capture user requirements and translate them into business and digitally enabled solutions across a range of industries. Your responsibilities will include:\n\n\n\nRoles and Responsibilities\nDesigning, developing, optimizing, and maintaining data pipelines that adhere to ETL principles and business goals\nSolving complex data problems to deliver insights that helps our business to achieve their goals.\nSource data (structured unstructured) from various touchpoints, format and organize them into an analyzable format.\nCreating data products for analytics team members to improve productivity\nCalling of AI services like vision, translation etc. to generate an outcome that can be used in further steps along the pipeline.\nFostering a culture of sharing, re-use, design and operational efficiency of data and analytical solutions\nPreparing data to create a unified database and build tracking solutions ensuring data quality\nCreate Production grade analytical assets deployed using the guiding principles of CI/CD.\n\n\nProfessional and Technical Skills\nExpert in Python, Scala, Pyspark, Pytorch, Javascript (any 2 at least)\nExtensive experience in data analysis (Big data- Apache Spark environments), data libraries (e.g. Pandas, SciPy, Tensorflow, Keras etc.), and SQL. 2-3 years of hands-on experience working on these technologies.\nExperience in one of the many BI tools such as Tableau, Power BI, Looker.\nGood working knowledge of key concepts in data analytics, such as dimensional modeling, ETL, reporting/dashboarding, data governance, dealing with structured and unstructured data, and corresponding infrastructure needs.\nWorked extensively in Microsoft Azure (ADF, Function Apps, ADLS, Azure SQL), AWS (Lambda,Glue,S3), Databricks analytical platforms/tools, Snowflake Cloud Datawarehouse.\n\n\n\n\nAdditional Information\nExperience working in cloud Data warehouses like Redshift or Synapse\nCertification in any one of the following or equivalent\nAWS- AWS certified data Analytics- Speciality\nAzure- Microsoft certified Azure Data Scientist Associate\nSnowflake- Snowpro core- Data Engineer\nDatabricks Data Engineering\n\nQualification\n\n\n\nExperience:3.5 -5 years of experience is required\n\n\n\n\nEducational Qualification:Graduation (Accurate educational details should capture)",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['scala', 'pyspark', 'pytorch', 'python', 'microsoft azure', 'glue', 'amazon redshift', 'sql', 'tensorflow', 'sql azure', 'spark', 'keras', 'big data', 'etl', 'scipy', 'snowflake', 'data analysis', 'azure data lake', 'power bi', 'data engineering', 'javascript', 'pandas', 'data bricks', 'tableau', 'lambda expressions', 'aws']",2025-06-13 05:12:16
Senior PySpark Data Engineer,Synechron,7 - 12 years,Not Disclosed,"['Pune', 'Hinjewadi']","Job Summary\nSynechron is seeking an experienced and technically proficient Senior PySpark Data Engineer to join our data engineering team. In this role, you will be responsible for developing, optimizing, and maintaining large-scale data processing solutions using PySpark. Your expertise will support our organizations efforts to leverage big data for actionable insights, enabling data-driven decision-making and strategic initiatives.\nSoftware Requirements\nRequired Skills:\nProficiency in PySpark\nFamiliarity with Hadoop ecosystem components (e.g., HDFS, Hive, Spark SQL)\nExperience with Linux/Unix operating systems\nData processing tools like Apache Kafka or similar streaming platforms\nPreferred Skills:\nExperience with cloud-based big data platforms (e.g., AWS EMR, Azure HDInsight)\nKnowledge of Python (beyond PySpark), Java or Scala relevant to big data applications\nFamiliarity with data orchestration tools (e.g., Apache Airflow, Luigi)\nOverall Responsibilities\nDesign, develop, and optimize scalable data processing pipelines using PySpark.\nCollaborate with data engineers, data scientists, and business analysts to understand data requirements and deliver solutions.\nImplement data transformations, aggregations, and extraction processes to support analytics and reporting.\nManage large datasets in distributed storage systems, ensuring data integrity, security, and performance.\nTroubleshoot and resolve performance issues within big data workflows.\nDocument data processes, architectures, and best practices to promote consistency and knowledge sharing.\nSupport data migration and integration efforts across varied platforms.\nStrategic Objectives:\nEnable efficient and reliable data processing to meet organizational analytics and reporting needs.\nMaintain high standards of data security, compliance, and operational durability.\nDrive continuous improvement in data workflows and infrastructure.\nPerformance Outcomes & Expectations:\nEfficient processing of large-scale data workloads with minimum downtime.\nClear, maintainable, and well-documented code.\nActive participation in team reviews, knowledge transfer, and innovation initiatives.\nTechnical Skills (By Category)\nProgramming Languages:\nRequired: PySpark (essential); Python (needed for scripting and automation)\nPreferred: Java, Scala\nDatabases/Data Management:\nRequired: Experience with distributed data storage (HDFS, S3, or similar) and data warehousing solutions (Hive, Snowflake)\nPreferred: Experience with NoSQL databases (Cassandra, HBase)\nCloud Technologies:\nRequired: Familiarity with deploying and managing big data solutions on cloud platforms such as AWS (EMR), Azure, or GCP\nPreferred: Cloud certifications\nFrameworks and Libraries:\nRequired: Spark SQL, Spark MLlib (basic familiarity)\nPreferred: Integration with streaming platforms (e.g., Kafka), data validation tools\nDevelopment Tools and Methodologies:\nRequired: Version control systems (e.g., Git), Agile/Scrum methodologies\nPreferred: CI/CD pipelines, containerization (Docker, Kubernetes)\nSecurity Protocols:\nOptional: Basic understanding of data security practices and compliance standards relevant to big data management\nExperience Requirements\nMinimum of 7+ years of experience in big data environments with hands-on PySpark development.\nProven ability to design and implement large-scale data pipelines.\nExperience working with cloud and on-premises big data architectures.\nPreference for candidates with domain-specific experience in finance, banking, or related sectors.\nCandidates with substantial related experience and strong technical skills in big data, even from different domains, are encouraged to apply.\nDay-to-Day Activities\nDevelop, test, and deploy PySpark data processing jobs to meet project specifications.\nCollaborate in multi-disciplinary teams during sprint planning, stand-ups, and code reviews.\nOptimize existing data pipelines for performance and scalability.\nMonitor data workflows, troubleshoot issues, and implement fixes.\nEngage with stakeholders to gather new data requirements, ensuring solutions are aligned with business needs.\nContribute to documentation, standards, and best practices for data engineering processes.\nSupport the onboarding of new data sources, including integration and validation.\nDecision-Making Authority & Responsibilities:\nIdentify performance bottlenecks and propose effective solutions.\nDecide on appropriate data processing approaches based on project requirements.\nEscalate issues that impact project timelines or data integrity.\nQualifications\nBachelors degree in Computer Science, Information Technology, or related field. Equivalent experience considered.\nRelevant certifications are preferred: Cloudera, Databricks, AWS Certified Data Analytics, or similar.\nCommitment to ongoing professional development in data engineering and big data technologies.\nDemonstrated ability to adapt to evolving data tools and frameworks.\nProfessional Competencies\nStrong analytical and problem-solving skills, with the ability to model complex data workflows.\nExcellent communication skills to articulate technical solutions to non-technical stakeholders.\nEffective teamwork and collaboration in a multidisciplinary environment.\nAdaptability to new technologies and emerging trends in big data.\nAbility to prioritize tasks effectively and manage time in fast-paced projects.\nInnovation mindset, actively seeking ways to improve data infrastructure and processes.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['PySpark', 'S3', 'Unix operating systems', 'Spark SQL', 'Luigi', 'HDFS', 'AWS EMR', 'Apache Airflow', 'Hive', 'Linux', 'Azure HDInsight', 'Apache Kafka', 'AWS']",2025-06-13 05:12:18
Senior Data Engineer - Azure,Blend360 India,6 - 11 years,Not Disclosed,['Hyderabad'],"Job Description\nAs a Senior Data Engineer, your role is to spearhead the data engineering teams and elevate the team to the next level! You will be responsible for laying out the architecture of the new project as well as selecting the tech stack associated with it. You will plan out the development cycles deploying AGILE if possible and create the foundations for good data stewardship with our new data products!\nYou will also set up a solid code framework that needs to be built to purpose yet have enough flexibility to adapt to new business use cases tough but rewarding challenge!\n\nResponsibilities\nCollaborate with several stakeholders to deeply understand the needs of data practitioners to deliver at scale\nLead Data Engineers to define, build and maintain Data Platform\nWork on building Data Lake in Azure Fabric processing data from multiple sources\nMigrating existing data store from Azure Synapse to Azure Fabric\nImplement data governance and access control\nDrive development effort End-to-End for on-time delivery of high-quality solutions that conform to requirements, conform to the architectural vision, and comply with all applicable standards.\nPresent technical solutions, capabilities, considerations, and features in business terms. Effectively communicate status, issues, and risks in a precise and timely manner.\nFurther develop critical initiatives, such as Data Discovery, Data Lineage and Data Quality\nLeading team and Mentor junior resources\nHelp your team members grow in their role and achieve their career aspirations\nBuild data systems, pipelines, analytical tools and programs\nConduct complex data analysis and report on results\n\n\nQualifications\n5+ Years of Experience as a data engineer or similar role in Azure Synapses, ADF or\nrelevant exp in Azure Fabric\nDegree in Computer Science, Data Science, Mathematics, IT, or similar fie",Industry Type: Advertising & Marketing,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Data analysis', 'Data modeling', 'Analytical', 'Agile', 'data governance', 'Data quality', 'Data mining', 'SQL', 'Python']",2025-06-13 05:12:20
Senior Data Engineer - Azure,Blend360 India,6 - 11 years,Not Disclosed,['Hyderabad'],"Job Description\nAs a Senior Data Engineer, your role is to spearhead the data engineering teams and elevate the team to the next level! You will be responsible for laying out the architecture of the new project as well as selecting the tech stack associated with it. You will plan out the development cycles deploying AGILE if possible and create the foundations for good data stewardship with our new data products!\nYou will also set up a solid code framework that needs to be built to purpose yet have enough flexibility to adapt to new business use cases tough but rewarding challenge!\n\nResponsibilities\nCollaborate with several stakeholders to deeply understand the needs of data practitioners to deliver at scale\nLead Data Engineers to define, build and maintain Data Platform\nWork on building Data Lake in Azure Fabric processing data from multiple sources\nMigrating existing data store from Azure Synapse to Azure Fabric\nImplement data governance and access control\nDrive development effort End-to-End for on-time delivery of high-quality solutions that conform to requirements, conform to the architectural vision, and comply with all applicable standards.\nPresent technical solutions, capabilities, considerations, and features in business terms. Effectively communicate status, issues, and risks in a precise and timely manner.\nFurther develop critical initiatives, such as Data Discovery, Data Lineage and Data Quality\nLeading team and Mentor junior resources\nHelp your team members grow in their role and achieve their career aspirations\nBuild data systems, pipelines, analytical tools and programs\nConduct complex data analysis and report on results",Industry Type: Advertising & Marketing,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Access control', 'Data analysis', 'Team leading', 'Architecture', 'Analytical', 'Agile', 'data governance', 'Data processing', 'Mentor', 'Data quality']",2025-06-13 05:12:22
Hadoop Data Engineer/ Senior Software Engineer,Hsbc,2 - 11 years,Not Disclosed,['Pune'],"Some careers shine brighter than others.\nIf you re looking for a career that will help you stand out, join HSBC and fulfil your potential. Whether you want a career that could take you to the top, or simply take you in an exciting new direction, HSBC offers opportunities, support and rewards that will take you further.\nHSBC is one of the largest banking and financial services organisations in the world, with operations in 64 countries and territories. We aim to be where the growth is, enabling businesses to thrive and economies to prosper, and, ultimately, helping people to fulfil their hopes and realise their ambitions.\nWe are currently seeking an experienced professional to join our team in the role of Software Engineer\nIn this role you will be\nExpertise in Scala-Spark/Python-Spark development and should be able to Work with Agile application dev team to implement data strategies.\nDesign and implement scalable data architectures to support the banks data needs.\nDevelop and maintain ETL (Extract, Transform, Load) processes.\nEnsure the data infrastructure is reliable, scalable, and secure.\nOversee the integration of diverse data sources into a cohesive data platform.\nEnsure data quality, data governance, and compliance with regulatory requirements.\nMonitor and optimize data pipeline performance.\nTroubleshoot and resolve data-related issues promptly.\nImplement monitoring and alerting systems for data processes.\nTroubleshoot and resolve technical issues optimizing system performance ensuring reliability.\nCreate and maintain technical documentation for new and existing system ensuring that information is accessible to the team.\nImplementing and monitoring solutions that identify both system bottlenecks and production issues.\n\n\n\n\n\n\n\n\n\n\nRequirements\n\n\n\nTo be successful in this role, you should meet the following requirements:\nExperience in data engineering or related field and hands-on experience of building and maintenance of ETL Data pipelines\nGood experience in Designing and Developing Spark Applications using Scala or Python.\nGood experience with database technologies (SQL, NoSQL), data warehousing solutions, and big data technologies (Hadoop, Spark)\nProficiency in programming languages such as Python, Java, or Scala.\nOptimization and Performance Tuning of Spark Applications\nGIT Experience on creating, merging and managing Repos.\nPerform unit testing and performance testing.\nGood understanding of ETL processes and data pipeline orchestration tools like Airflow, Control-M.\nStrong problem-solving skills and ability to work under pressure.\nExcellent communication and interpersonal skills.",Industry Type: Consumer Electronics & Appliances,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Performance tuning', 'Performance testing', 'Agile', 'Control-M', 'Data quality', 'Unit testing', 'Financial services', 'SQL', 'Python', 'Technical documentation']",2025-06-13 05:12:23
Senior Data Engineer,Qualcomm,5 - 10 years,Not Disclosed,['Hyderabad'],"Job Area: Information Technology Group, Information Technology Group > IT Data Engineer\n\nGeneral Summary:\n\nDeveloper will play an integral role in the PTEIT Machine Learning Data Engineering team. Design, develop and support data pipelines in a hybrid cloud environment to enable advanced analytics. Design, develop and support CI/CD of data pipelines and services. - 5+ years of experience with Python or equivalent programming using OOPS, Data Structures and Algorithms - Develop new services in AWS using server-less and container-based services. - 3+ years of hands-on experience with AWS Suite of services (EC2, IAM, S3, CDK, Glue, Athena, Lambda, RedShift, Snowflake, RDS) - 3+ years of expertise in scheduling data flows using Apache Airflow - 3+ years of strong data modelling (Functional, Logical and Physical) and data architecture experience in Data Lake and/or Data Warehouse - 3+ years of experience with SQL databases - 3+ years of experience with CI/CD and DevOps using Jenkins - 3+ years of experience with Event driven architecture specially on Change Data Capture - 3+ years of Experience in Apache Spark, SQL, Redshift (or) Big Query (or) Snowflake, Databricks - Deep understanding building the efficient data pipelines with data observability, data quality, schema drift, alerting and monitoring. - Good understanding of the Data Catalogs, Data Governance, Compliance, Security, Data sharing - Experience in building the reusable services across the data processing systems. - Should have the ability to work and contribute beyond defined responsibilities - Excellent communication and inter-personal skills with deep problem-solving skills.\n\nMinimum Qualifications:\n3+ years of IT-related work experience with a Bachelor's degree in Computer Engineering, Computer Science, Information Systems or a related field.\nOR\n5+ years of IT-related work experience without a Bachelors degree.\n\n2+ years of any combination of academic or work experience with programming (e.g., Java, Python).\n1+ year of any combination of academic or work experience with SQL or NoSQL Databases.\n1+ year of any combination of academic or work experience with Data Structures and algorithms.\n5 years of Industry experience and minimum 3 years experience in Data Engineering development with highly reputed organizations- Proficiency in Python and AWS- Excellent problem-solving skills- Deep understanding of data structures and algorithms- Proven experience in building cloud native software preferably with AWS suit of services- Proven experience in design and develop data models using RDBMS (Oracle, MySQL, etc.)\n\nDesirable - Exposure or experience in other cloud platforms (Azure and GCP) - Experience working on internals of large-scale distributed systems and databases such as Hadoop, Spark - Working experience on Data Lakehouse platforms (One House, Databricks Lakehouse) - Working experience on Data Lakehouse File Formats (Delta Lake, Iceberg, Hudi)\n\nBachelor's or Master's degree in Computer Science, Software Engineering, or a related field.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['algorithms', 'python', 'data quality', 'data structures', 'aws', 'schema', 'continuous integration', 'glue', 'amazon redshift', 'event driven architecture', 'ci/cd', 'data engineering', 'sql', 'alerts', 'java', 'data modeling', 'spark', 'devops', 'data flow', 'nosql databases', 'sql database']",2025-06-13 05:12:26
Senior Data Engineer - Azure,Blend360 India,3 - 6 years,Not Disclosed,['Hyderabad'],"Job Description\nAs a Data Engineer, your role is to spearhead the data engineering teams and elevate the team to the next level! You will be responsible for laying out the architecture of the new project as well as selecting the tech stack associated with it. You will plan out the development cycles deploying AGILE if possible and create the foundations for good data stewardship with our new data products!\nYou will also set up a solid code framework that needs to be built to purpose yet have enough flexibility to adapt to new business use cases tough but rewarding challenge!\n\nResponsibilities\nCollaborate with several stakeholders to deeply understand the needs of data practitioners to deliver at scale\nLead Data Engineers to define, build and maintain Data Platform\nWork on building Data Lake in Azure Fabric processing data from multiple sources\nMigrating existing data store from Azure Synapse to Azure Fabric\nImplement data governance and access control\nDrive development effort End-to-End for on-time delivery of high-quality solutions that conform to requirements, conform to the architectural vision, and comply with all applicable standards.\nPresent technical solutions, capabilities, considerations, and features in business terms. Effectively communicate status, issues, and risks in a precise and timely manner.\nFurther develop critical initiatives, such as Data Discovery, Data Lineage and Data Quality\nLeading team and Mentor junior resources\nHelp your team members grow in their role and achieve their career aspirations\nBuild data systems, pipelines, analytical tools and programs\nConduct complex data analysis and report on results\n\n\nQualifications\n3+ Years of Experience as a data engineer or similar role in Azure Synapses, ADF or\nrelevant exp in Azure Fabric\nDegree in Computer Science, Data Science, Mathematics, IT, or similar fiel",Industry Type: Advertising & Marketing,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Data analysis', 'Data modeling', 'Analytical', 'Agile', 'data governance', 'Data quality', 'Data mining', 'SQL', 'Python']",2025-06-13 05:12:27
Senior Data Engineer - AWS,Blend360 India,5 - 10 years,Not Disclosed,['Hyderabad'],"Job Description\nWe are looking for an experienced Senior Data Engineer with a strong foundation in Python, SQL, and Spark , and hands-on expertise in AWS, Databricks . In this role, you will build and maintain scalable data pipelines and architecture to support analytics, data science, and business intelligence initiatives. You ll work closely with cross-functional teams to drive data reliability, quality, and performance.\nResponsibilities:\nDesign, develop, and optimize scalable data pipelines using Databricks in AWS such as Glue, S3, Lambda, EMR, Databricks notebooks, workflows and jobs.\nBuilding data lake in WS Databricks.\nBuild and maintain robust ETL/ELT workflows using Python and SQL to handle structured and semi-structured data.\nDevelop distributed data processing solutions using Apache Spark or PySpark .\nPartner with data scientists and analysts to provide high-quality, accessible, and well-structured data.\nEnsure data quality, governance, security, and compliance across pipelines and data stores.\nMonitor, troubleshoot, and improve the performance of data systems and pipelines.\nParticipate in code reviews and help establish engineering best practices.\nMentor junior data engineers and support their technical development.\n\n\nQualifications\nRequirements\nBachelors or masters degree in computer science, Engineering, or a related field.\n5+ years of hands-on experience in data",Industry Type: Advertising & Marketing,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Automation', 'Version control', 'GIT', 'Workflow', 'Data quality', 'Business intelligence', 'Analytics', 'SQL', 'Python']",2025-06-13 05:12:29
Senior Data Engineer,Fractal Analytics,8 - 10 years,Not Disclosed,['Mumbai'],"Job Description:\nAs a Backend (Java) Engineer, you would be part of the team consisting of Scrum Master, Cloud Engineers, AI/ML Engineers, and UI/UX Engineers to build end-to-end Data to Decision Systems.\nMandatory:\n8+ years of demonstrable experience designing, building, and working as a Java Developer for enterprise web applications\nIdeally, this would include the following:\no Expert-level proficiency with Java\no Expert-level proficiency with SpringBoot\nFamiliarity with common databases (RDBMS such as MySQL & NoSQL such as MongoDB) and data warehousing concepts (OLAP, OLTP)\nUnderstanding of REST concepts and building/interacting with REST APIs\nDeep understanding of core backend concepts:\no Develop and design RESTful services and APIs\no Develop functional databases, applications, and servers to support websites on the back end\no Performance optimization and multithreading concepts\no Experience with deploying and maintaining high traffic infrastructure (performance testing is a plus)\nIn addition, the ideal candidate would have great problem-solving skills, and familiarity with code versioning tools such as GitHub",,,,"['Backend', 'Multithreading', 'RDBMS', 'MySQL', 'Performance testing', 'OLAP', 'Scrum', 'MongoDB', 'Apache', 'OLTP']",2025-06-13 05:12:31
Senior - AWS Data Engineering,KPMG India,4 - 8 years,Not Disclosed,['Gurugram'],"KPMG India is looking for Senior - AWS Data Engineering to join our dynamic team and embark on a rewarding career journey Designs and builds scalable data pipelines using AWS servicesOptimizes data ingestion, storage, and processingCollaborates with data scientists and analystsEnsures performance, security, and compliance",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Networking', 'Focus', 'Manager Technology', 'professional services', 'AWS', 'international clients']",2025-06-13 05:12:32
Sr Analyst I Data Engineering,DXC Technology,9 - 12 years,Not Disclosed,['Hyderabad'],"Job Description:\nEssential Job Functions:\nParticipate in data engineering tasks, including data processing and integration activities.\nAssist in the development and maintenance of data pipelines.\nCollaborate with team members to collect, process, and store data.\nContribute to data quality assurance efforts and adherence to data standards.\nUse data engineering tools and techniques to analyze and generate insights from data.\nCollaborate with data engineers and other analysts on data-related projects.\nSeek out opportunities to enhance data engineering skills and domain knowledge.\nStay informed about data engineering trends and best practices.\n\nBasic Qualifications:\nBachelors degree in a relevant field or equivalent combination of education and experience\nTypically, 5+ years of relevant work experience in industry, with a minimum of 2 years in a similar role\nProven experience in data engineering\nProficiencies in data engineering tools and technologies\nA continuous learner that stays abreast with industry knowledge and technology\n\nOther Qualifications:\nAdvanced degree in a relevant field a plus\nRelevant certifications, such as Oracle Certified Professional, MySQL Database Administrator a plus\nRecruitment fraud is a scheme in which fictitious job opportunities are offered to job seekers typically through online services, such as false websites, or through unsolicited emails claiming to be from the company. These emails may request recipients to provide personal information or to make payments as part of their illegitimate recruiting process. DXC does not make offers of employment via social media networks and DXC never asks for any money or payments from applicants at any point in the recruitment process, nor ask a job seeker to purchase IT or other equipment on our behalf. More information on employment scams is available here .",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Usage', 'Manager Quality Assurance', 'Senior Analyst', 'Social media', 'Manager Technology', 'Data processing', 'Data quality', 'Oracle', 'mysql database administrator', 'Recruitment']",2025-06-13 05:12:34
Data Engineer,Luxoft,5 - 10 years,Not Disclosed,['Pune'],"Project description\nYou'll be working in the GM Business Analytics team located in Pune. The successful candidate will be a member of the global Distribution team, which has team members in London and Pune.\n\nWe work as part of a global team providing analytical solutions for IB distribution/sales people. Solutions deployed should be extensible globally with minimal localization.\n\nResponsibilities\n\nAre you passionate about data and analyticsAre you keen to be part of the journey to modernize a data warehouse/ analytics suite of application(s). Do you take pride in the quality of software delivered for each development iteration\n\nWe're looking for someone like that to join us and\n\nbe a part of a high-performing team on a high-profile project.\n\nsolve challenging problems in an elegant way\n\nmaster state-of-the-art technologies\n\nbuild a highly responsive and fast updating application in an Agile & Lean environment\n\napply best development practices and effectively utilize technologies\n\nwork across the full delivery cycle to ensure high-quality delivery\n\nwrite high-quality code and adhere to coding standards\n\nwork collaboratively with diverse team(s) of technologists\n\nYou are:\n\nCurious and collaborative, comfortable working independently, as well as in a team\n\nFocused on delivery to the business\n\nStrong in analytical skills. For example, the candidate must understand the key dependencies among existing systems in terms of the flow of data among them. It is essential that the candidate learns to understand the 'big picture' of how IB industry/business functions.\n\nAble to quickly absorb new terminology and business requirements\n\nAlready strong in analytical tools, technologies, platforms, etc. The candidate must also demonstrate a strong desire for learning and self-improvement.\n\nOpen to learning home-grown technologies, support current state infrastructure and help drive future state migrations. imaginative and creative with newer technologies\n\nAble to accurately and pragmatically estimate the development effort required for specific objectives\n\nYou will have the opportunity to work under minimal supervision to understand local and global system requirements, design and implement the required functionality/bug fixes/enhancements. You will be responsible for components that are developed across the whole team and deployed globally.\n\nYou will also have the opportunity to provide third-line support to the application's global user community, which will include assisting dedicated support staff and liaising with the members of other development teams directly, some of which will be local and some remote.\n\nSkills\nMust have\n\nA bachelor's or master's degree, preferably in Information Technology or a related field (computer science, mathematics, etc.), focusing on data engineering.\n\n5+ years of relevant experience as a data engineer in Big Data is required.\n\nStrong Knowledge of programming languages (Python / Scala) and Big Data technologies (Spark, Databricks or equivalent) is required.\n\nStrong experience in executing complex data analysis and running complex SQL/Spark queries.\n\nStrong experience in building complex data transformations in SQL/Spark.\n\nStrong knowledge of Database technologies is required.\n\nStrong knowledge of Azure Cloud is advantageous.\n\nGood understanding and experience with Agile methodologies and delivery.\n\nStrong communication skills with the ability to build partnerships with stakeholders.\n\nStrong analytical, data management and problem-solving skills.\n\nNice to have\n\nExperience working on the QlikView tool\n\nUnderstanding of QlikView scripting and data model\n\nOther\n\nLanguages\n\nEnglishC1 Advanced\n\nSeniority\n\nSenior",Industry Type: Legal,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analysis', 'data management', 'big data technologies', 'sql', 'spark', 'python', 'scala', 'mathematics', 'business analytics', 'data engineering', 'azure cloud', 'qlikview', 'data bricks', 'computer science', 'database creation', 'data transformation', 'agile', 'big data', 'agile methodology']",2025-06-13 05:12:36
Data Engineer,Mobile Programming,5 - 10 years,Not Disclosed,['Mumbai'],"Candidate Skill:Technical Skills - Data Engineering | ETL | SQL | Python | AWS | Azure | Google Cloud | Hadoop | Spark | Kafka | Data Warehousing | Data Modeling | NoSQL | Data Quality\nWe are looking for an experienced Data Engineer to join our team in Mumbai. As a Data Engineer, you will be responsible for designing, building, and maintaining efficient data pipelines that transform raw data into actionable insights. You will work closely with data scientists and analysts to ensure that data is accessible, reliable, and optimized for analysis. Your role will also involve handling large datasets, ensuring data quality, and implementing data processing frameworks.\nKey Responsibilities:Design and build scalable data pipelines for processing, transforming, and integrating large datasets.Develop and maintain ETL processes to extract, transform, and load data from multiple sources into the data warehouse.Collaborate with data scientists and analysts to ensure that the data is optimized for analysis and modeling.Ensure the quality, integrity, and security of data throughout its lifecycle.\nWork with cloud-based technologies for data storage and processing (AWS, Azure, GCP).Implement data processing frameworks for efficient handling of structured and unstructured data.Troubleshoot and resolve issues related to data pipelines and workflows.Automate data integration processes and ensure data consistency and accuracy across systems.\nRequired Skills:\n5+ years of experience in Data Engineering with hands-on experience in data pipeline development.Strong expertise in ETL processes, data integration, and data warehousing.Proficiency in SQL, Python, and other programming languages for data manipulation.Experience with cloud technologies such as AWS, Azure, or Google Cloud.Knowledge of big data technologies like Hadoop, Spark, or Kafka is a plus.Strong understanding of data modeling, data quality, and data governance.\nFamiliarity with NoSQL databases (e.g., MongoDB, Cassandra) and relational databases.Strong analytical and problem-solving skills with the ability to work with large, complex datasets.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Azure', 'Hadoop', 'Kafka', 'SQL', 'Data Quality', 'NoSQL', 'GCP', 'Spark', 'Data Warehousing', 'Data Modeling', 'ETL', 'AWS', 'Python']",2025-06-13 05:12:38
Data Engineer For a US based IT Company based in Hyderabad,GLOBAL INSTITUTE FOR STAFFING & TRAINING...,5 - 10 years,16-20 Lacs P.A.,['Hyderabad( Kokapeta Village )'],"We are Hiring Data Engineer for a US based IT Company Based in Hyderabad. Candidates with minimum 5 Years of experience in Data Engineering can apply.\n\nThis job is for 1 year contract only\n\nJob Title: Data Engineer\nLocation: Hyderabad\nCTC: Upto 20 LPA\nExperience: 5+ Years\n\nJob Overview:\nWe are looking for a seasoned Senior Data Engineer with deep hands-on experience in Talend and IBM DataStage to join our growing enterprise data team. This role will focus on designing and optimizing complex data integration solutions that support enterprise-wide analytics, reporting, and compliance initiatives.\nIn this senior-level position, you will collaborate with data architects, analysts, and key stakeholders to facilitate large-scale data movement, enhance data quality, and uphold governance and security protocols.\n\nKey Responsibilities:\nDevelop, maintain, and enhance scalable ETL pipelines using Talend and IBM DataStage\nPartner with data architects and analysts to deliver efficient and reliable data integration solutions\nReview and optimize existing ETL workflows for performance, scalability, and reliability\nConsolidate data from multiple sourcesboth structured and unstructuredinto data lakes and enterprise platforms\nImplement rigorous data validation and quality assurance procedures to ensure data accuracy and integrity\nAdhere to best practices for ETL development, including source control and automated deployment\nMaintain clear and comprehensive documentation of data processes, mappings, and transformation rules\nSupport enterprise initiatives around data migration, modernization, and cloud transformation\nMentor junior engineers and participate in code reviews and team learning sessions\nRequired Qualifications:\nMinimum 5 years of experience in data engineering or ETL development\nProficient with Talend (Open Studio and/or Talend Cloud) and IBM DataStage\nStrong skills in SQL, data profiling, and performance tuning\nExperience handling large datasets and complex data workflows\nSolid understanding of data warehousing, data modeling, and data lake architecture\nFamiliarity with version control systems (e.g., Git) and CI/CD pipelines\nStrong analytical and troubleshooting skills\nEffective verbal and written communication, with strong documentation habits\n\nPreferred Qualifications:\nPrior experience in banking or financial services\nExposure to cloud platforms such as AWS, Azure, or Google Cloud\nKnowledge of data governance tools (e.g., Collibra, Alation)\nAwareness of data privacy regulations (e.g., GDPR, CCPA)\nExperience working in Agile/Scrum environments\n\nFor further assistance contact/whatsapp: 9354909518 or write to priya@gist.org.in",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Temporary/Contractual","['Data Engineering', 'Kafka', 'Snowflake', 'Java', 'Spark', 'mongo', 'Ci/Cd', 'Data Pipeline', 'Agile', 'Scrum', 'Data Modeling', 'Talend', 'Oracle', 'AWS', 'Data Governance', 'Gdpr', 'azure', 'Python', 'Shell Scripting', 'Postgresql', 'Ibm Datastage', 'Workflow', 'Data Framework', 'GCPA', 'SQL', 'GIT', 'Amazon Redshift', 'GCP', 'Data Lake', 'Data Warehousing', 'ETL']",2025-06-13 05:12:40
Data Engineer,Atyeti,2 - 4 years,Not Disclosed,['Pune'],"Role & responsibilities\n\nDevelop and Maintain Data Pipelines: Design, develop, and manage scalable ETL pipelines to process large datasets using PySpark, Databricks, and other big data technologies.\nData Integration and Transformation: Work with various structured and unstructured data sources to build efficient data workflows and integrate them into a central data warehouse.\nCollaborate with Data Scientists & Analysts: Work closely with the data science and business intelligence teams to ensure the right data is available for advanced analytics, machine learning, and reporting.",,,,"['Azure Synapse', 'Pyspark', 'ETL', 'Python']",2025-06-13 05:12:42
Data Engineer,Amgen Inc,1 - 6 years,Not Disclosed,['Hyderabad'],"ABOUT THE ROLE\nRole Description:\nAs part of the cybersecurity organization, the Data Engineer is responsible for designing, building, and maintaining data infrastructure to support data-driven decision-making. This role involves working with large datasets, developing reports, executing data governance initiatives, and ensuring data is accessible, reliable, and efficiently managed. The ideal candidate has strong technical skills, experience with big data technologies, and a deep understanding of data architecture, ETL processes, and cybersecurity data frameworks.",,,,"['Data engineering', 'data security', 'Agile', 'cloud data platforms', 'Databricks', 'data governance frameworks', 'ETL', 'AWS', 'SQL', 'Python']",2025-06-13 05:12:43
Data Engineer,Amgen Inc,1 - 3 years,Not Disclosed,['Hyderabad'],"What you will do\nIn this vital role you will be responsible to design, develop, and optimize data pipelines, data integration frameworks, and metadata-driven architectures that enable seamless data access and analytics. This role prefers deep expertise in big data processing, distributed computing, data modeling, and governance frameworks to support self-service analytics, AI-driven insights, and enterprise-wide data management.\nDesign, develop, and maintain complex ETL/ELT data pipelines in Databricks using PySpark, Scala, and SQL to process large-scale datasets",,,,"['Data engineering', 'Maven', 'data validation', 'PySpark', 'Scala', 'APIs', 'SQL Server', 'SQL', 'Jenkins', 'Git', 'MySQL', 'troubleshooting', 'MongoDB', 'ETL']",2025-06-13 05:12:45
Data Engineer,Amgen Inc,1 - 6 years,Not Disclosed,['Hyderabad'],"Role Description:\nAs part of the cybersecurity organization, In this vital role you will be responsible for designing, building, and maintaining data infrastructure to support data-driven decision-making. This role involves working with large datasets, developing reports, executing data governance initiatives, and ensuring data is accessible, reliable, and efficiently managed. The role sits at the intersection of data infrastructure and business insight delivery, requiring the Data Engineer to design and build robust data pipelines while also translating data into meaningful visualizations for stakeholders across the organization. The ideal candidate has strong technical skills, experience with big data technologies, and a deep understanding of data architecture, ETL processes, and cybersecurity data frameworks.",,,,"['data engineering', 'data analysis', 'data modeling', 'analysis tools', 'data warehousing', 'troubleshooting', 'data architecture', 'data integration', 'etl process']",2025-06-13 05:12:47
Data Engineer,Luxoft,5 - 10 years,Not Disclosed,['Pune'],"Are you passionate about data and analytics? Are you keen to be part of the journey to modernize a data warehouse/ analytics suite of application(s). Do you take pride in the quality of software delivered for each development iteration?\nWere looking for someone like that to join us and\nbe a part of a high-performing team on a high-profile project.\nsolve challenging problems in an elegant way\nmaster state-of-the-art technologies\nbuild a highly responsive and fast updating application in an Agile & Lean environment\napply best development practices and effectively utilize technologies\nwork across the full delivery cycle to ensure high-quality delivery\nwrite high-quality code and adhere to coding standards\nwork collaboratively with diverse team(s) of technologists\nYou are:\nCurious and collaborative, comfortable working independently, as well as in a team\nFocused on delivery to the business\nStrong in analytical skills. For example, the candidate must understand the key dependencies among existing systems in terms of the flow of data among them. It is essential that the candidate learns to understand the big picture of how IB industry/business functions.\nAble to quickly absorb new terminology and business requirements\nAlready strong in analytical tools, technologies, platforms, etc. The candidate must also demonstrate a strong desire for learning and self-improvement.\nOpen to learning home-grown technologies, support current state infrastructure and help drive future state migrations. imaginative and creative with newer technologies\nAble to accurately and pragmatically estimate the development effort required for specific objectives\nYou will have the opportunity to work under minimal supervision to understand local and global system requirements, design and implement the required functionality/bug fixes/enhancements. You will be responsible for components that are developed across the whole team and deployed globally.\nYou will also have the opportunity to provide third-line support to the applications global user community, which will include assisting dedicated support staff and liaising with the members of other development teams directly, some of which will be local and some remote.\nSkills\nMust have\nA bachelors or masters degree, preferably in Information Technology or a related field (computer science, mathematics, etc.), focusing on data engineering.\n5+ years of relevant experience as a data engineer in Big Data is required.\nStrong Knowledge of programming languages (Python / Scala) and Big Data technologies (Spark, Databricks or equivalent) is required.\nStrong experience in executing complex data analysis and running complex SQL/Spark queries.\nStrong experience in building complex data transformations in SQL/Spark.\nStrong knowledge of Database technologies is required.\nStrong knowledge of Azure Cloud is advantageous.\nGood understanding and experience with Agile methodologies and delivery.\nStrong communication skills with the ability to build partnerships with stakeholders.\nStrong analytical, data management and problem-solving skills.\nNice to have\nExperience working on the QlikView tool\nUnderstanding of QlikView scripting and data model\nOther\nLanguages\nEnglish: C1 Advanced\nSeniority\nSenior\nRefer a Friend\nPositive work environments and stellar reputations attract and retain top talent. Find out why Luxoft stands apart from the rest.\nRecommend a friend\nRelated jobs View all vacancies\nBig Data Engineer (Scala/Java/Python)\nBigData Development\nUnited States of America\nStamford, US\nBig Data Engineer (Scala/Java/Python)\nBigData Development\nUnited States of America\nWeehawken\nData Engineer - PostgreSQL\nBigData Development\nPoland\nRemote Poland\nPune, India\nReq. VR-114879\nBigData Development\nBCM Industry\n05/06/2025\nReq. VR-114879\nApply for Data Engineer in Pune\n*",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Data analysis', 'Data management', 'Coding', 'Postgresql', 'Agile', 'Information technology', 'Analytics', 'SQL', 'Python']",2025-06-13 05:12:48
Data Engineer,Databeat,3 - 7 years,Not Disclosed,['Hyderabad( Rai Durg )'],"Experience Required: 3+ years\n\nTechnical knowledge: AWS, Python, SQL, S3, EC2, Glue, Athena, Lambda, DynamoDB, RedShift, Step Functions, Cloud Formation, CI/CD Pipelines, Github, EMR, RDS,AWS Lake Formation, GitLab, Jenkins and AWS CodePipeline.\n\n\n\nRole Summary: As a Senior Data Engineer,with over 3 years of expertise in Python, PySpark, SQL to design, develop and optimize complex data pipelines, support data modeling, and contribute to the architecture that supports big data processing and analytics to cutting-edge cloud solutions that drive business growth. You will lead the design and implementation of scalable, high-performance data solutions on AWS and mentor junior team members.This role demands a deep understanding of AWS services, big data tools, and complex architectures to support large-scale data processing and advanced analytics.\nKey Responsibilities:\nDesign and develop robust, scalable data pipelines using AWS services, Python, PySpark, and SQL that integrate seamlessly with the broader data and product ecosystem.\nLead the migration of legacy data warehouses and data marts to AWS cloud-based data lake and data warehouse solutions.\nOptimize data processing and storage for performance and cost.\nImplement data security and compliance best practices, in collaboration with the IT security team.\nBuild flexible and scalable systems to handle the growing demands of real-time analytics and big data processing.\nWork closely with data scientists and analysts to support their data needs and assist in building complex queries and data analysis pipelines.\nCollaborate with cross-functional teams to understand their data needs and translate them into technical requirements.\nContinuously evaluate new technologies and AWS services to enhance data capabilities and performance.\nCreate and maintain comprehensive documentation of data pipelines, architectures, and workflows.\nParticipate in code reviews and ensure that all solutions are aligned to pre-defined architectural specifications.\nPresent findings to executive leadership and recommend data-driven strategies for business growth.\nCommunicate effectively with different levels of management to gather use cases/requirements and provide designs that cater to those stakeholders.\nHandle clients in multiple industries at the same time, balancing their unique needs.\nProvide mentoring and guidance to junior data engineers and team members.\n\n\n\nRequirements:\n3+ years of experience in a data engineering role with a strong focus on AWS, Python, PySpark, Hive, and SQL.\nProven experience in designing and delivering large-scale data warehousing and data processing solutions.\nLead the design and implementation of complex, scalable data pipelines using AWS services such as S3, EC2, EMR, RDS, Redshift, Glue, Lambda, Athena, and AWS Lake Formation.\nBachelor's or Masters degree in Computer Science, Engineering, or a related technical field.\nDeep knowledge of big data technologies and ETL tools, such as Apache Spark, PySpark, Hadoop, Kafka, and Spark Streaming.\nImplement data architecture patterns, including event-driven pipelines, Lambda architectures, and data lakes.\nIncorporate modern tools like Databricks, Airflow, and Terraform for orchestration and infrastructure as code.\nImplement CI/CD using GitLab, Jenkins, and AWS CodePipeline.\nEnsure data security, governance, and compliance by leveraging tools such as IAM, KMS, and AWS CloudTrail.\nMentor junior engineers, fostering a culture of continuous learning and improvement.\nExcellent problem-solving and analytical skills, with a strategic mindset.\nStrong communication and leadership skills, with the ability to influence stakeholders at all levels.\nAbility to work independently as well as part of a team in a fast-paced environment.\nAdvanced data visualization skills and the ability to present complex data in a clear and concise manner.\nExcellent communication skills, both written and verbal, to collaborate effectively across teams and levels.\n\nPreferred Skills:\nExperience with Databricks, Snowflake, and machine learning pipelines.\nExposure to real-time data streaming technologies and architectures.\nFamiliarity with containerization and serverless computing (Docker, Kubernetes, AWS Lambda).",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'Aws Glue', 'SQL', 'Data Pipeline', 'Python', 'Amazon Ec2', 'Data Engineering', 'Data Bricks', 'Aws Lambda', 'Amazon Redshift', 'Azure Cloud', 'Data Lake', 'Data Modeling', 'Athena']",2025-06-13 05:12:50
Data Engineer,Xenonstack,2 - 5 years,Not Disclosed,['Mohali( Phase 8B Mohali )'],"At XenonStack, We committed to become the Most Value Driven Cloud Native, Platform Engineering and Decision Driven Analytics Company. Our Consulting Services and Solutions towards the Neural Company and its Key Drivers.\nXenonStacks DataOps team is looking for a Data Engineer who will be responsible for employing techniques to create and sustain structures that allow for the analysis of data while remaining familiar with dominant programming and deployment strategies in the field.\nYou should demonstrate flexibility, creativity, and the capacity to receive and utilize constructive criticism. The ideal candidate should be highly skilled in all aspects of Python, Java/Scala, SQL and analytical skills.\nJob Responsibilities:\nDevelop, construct, test and maintain Data Platform Architectures\nAlign Data Architecture with business requirements\nLiaising with co-workers and clients to elucidate the requirements for each task.\nScalable and High Performant Data Platform Infrastructure that allows big data to be accessed and analysed quickly by BI & AI Teams.\nReformulating existing frameworks to optimize their functioning.\nTransforming Raw Data into InSights for manipulation by Data Scientists.\nEnsuring that your work remains backed up and readily accessible to relevant co-workers.\nRemaining up-to-date with industry standards and technological advancements that will improve the quality of your outputs.\nRequirements:\nTechnical Requirements\nExperience of Python, Java/Scala\nGreat Statistical / SQL based Analytical Skills\nExperience of Data Analytics Architectural Design Patterns for Batch, Event Driven and Real-Time Analytics Use Cases\nUnderstanding of Data warehousing, ETL tools, machine learning, Data EPIs\nExcellent in Algorithms and Data Systems\nUnderstanding of Distributed System for Data Processing and Analytics\nFamiliarity with Popular Data Analytics Framework like Hadoop , Spark , Delta Lake , Time Series / Analytical Stores Stores.\nProfessional Attributes:\nExcellent communication skills & Attention to detail.\nAnalytical mind and problem-solving Aptitude with Strong Organizational skills & Visual Thinking.\nBenefits:\nDiscover the benefits of joining our team:\nDynamic and purposeful work culture in a people-oriented organization contributing to multi-million-dollar projects with guaranteed job security.\nOpen, authentic, and transparent communication fostering a warm work environment.\nRegular constructive feedback and exposure to diverse technologies.\nRecognition and rewards for exceptional performance achievements.\nAccess to certification courses & Skill Sessions to develop continually and refine your skills.\nAdditional allowances for team members assigned to specific projects.\nSpecial skill allowances to acknowledge and compensate for unique expertise.\nComprehensive medical insurance policy for your health and well-being.\nTo Learn more about the company -\nWebsite - http://www.xenonstack.com/",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Hadoop', 'Spark', 'ETL', 'Python', 'SQL', 'Java', 'Data Processing', 'Machine Learning']",2025-06-13 05:12:52
Data Engineer,Luxoft,5 - 8 years,Not Disclosed,['Pune'],"Project description\nAre you passionate about leveraging the latest technologies for strategic changeDo you enjoy problem solving in clever waysAre you organized enough to drive change across complex data systemsIf so, you could be the right person for this role.\nAs an experienced data engineer, you will join a global data analytics team in our Group Chief Technology Officer / Enterprise Architecture organization supporting our strategic initiatives which ranges from portfolio health to integration.\n\nResponsibilities\n\nHelp Group Enterprise Architecture team to develop our suite of EA tools and workbenches\n\nWork in the development team to support the development of portfolio health insights\n\nBuild data applications from cloud infrastructure to visualization layer\n\nProduce clear and commented code\n\nProduce clear and comprehensive documentation\n\nPlay an active role with technology support teams and ensure deliverables are completed or escalated on time\n\nProvide support on any related presentations, communications, and trainings\n\nBe a team player, working across the organization with skills to indirectly manage and influence\n\nBe a self-starter willing to inform and educate others\n\nSkills\nMust have\n\nB.Sc./M.Sc. degree in computing or similar\n\n5-8+ years' experience as a Data Engineer, ideally in a large corporate environment\n\nIn-depth knowledge of SQL and data modelling/data processing\n\nStrong experience working with Microsoft Azure\n\nExperience with visualisation tools like PowerBI (or Tableau, QlikView or similar)\n\nExperience working with Git, JIRA, GitLab\n\nStrong flair for data analytics\n\nStrong flair for IT architecture and IT architecture metrics\n\nExcellent stakeholder interaction and communication skills\n\nUnderstanding of performance implications when making design decisions to deliver performant and maintainable software.\n\nExcellent end-to-end SDLC process understanding.\n\nProven track record of delivering complex data apps on tight timelines\n\nFluent in English both written and spoken.\n\nPassionate about development with focus on data and cloud\n\nAnalytical and logical, with strong problem solving skills\n\nA team player, comfortable with taking the lead on complex tasks\n\nAn excellent communicator who is adept in, handling ambiguity and communicating with both technical and non-technical audiences\n\nComfortable with working in cross-functional global teams to effect change\n\nPassionate about learning and developing your hard and soft professional skills\n\nNice to have\n\nExperience working in the financial industry\n\nExperience in complex metrics design and reporting\n\nExperience in using artificial intelligence for data analytics\n\nOther\n\nLanguages\n\nEnglishC1 Advanced\n\nSeniority\n\nSenior",Industry Type: Legal,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analytics', 'data processing', 'microsoft azure', 'sql', 'data modeling', 'screening', 'it architecture', 'hiring', 'power bi', 'hrsd', 'knowledge of sql', 'data engineering', 'artificial intelligence', 'sourcing', 'qlikview', 'talent acquisition', 'tableau', 'git', 'recruitment', 'gitlab', 'sdlc', 'jira']",2025-06-13 05:12:54
Data Engineer,Shyftlabs,5 - 10 years,Not Disclosed,['Noida'],"Position Overview\nWe are looking for an experienced Lead Data Engineer to join our dynamic team. If you are passionate about building scalable software solutions, and work collaboratively with cross-functional teams to define requirements and deliver solutions we would love to hear from you.\nJob Responsibilities:\nDevelop and maintain data pipelines and ETL/ELT processes using Python\nDesign and implement scalable, high-performance applications\nWork collaboratively with cross-functional teams to define requirements and deliver solutions\nDevelop and manage near real-time data streaming solutions using Pub, Sub or Beam.\nContribute to code reviews, architecture discussions, and continuous improvement initiatives\nMonitor and troubleshoot production systems to ensure reliability and performance\nBasic Qualifications:\n5+ years of professional software development experience with Python\nStrong understanding of software engineering best practices (testing, version control, CI/CD)\nExperience building and optimizing ETL/ELT processes and data pipelines\nProficiency with SQL and database concepts\nExperience with data processing frameworks (e.g., Pandas)\nUnderstanding of software design patterns and architectural principles\nAbility to write clean, well-documented, and maintainable code\nExperience with unit testing and test automation\nExperience working with any cloud provider (GCP is preferred)\nExperience with CI/CD pipelines and Infrastructure as code\nExperience with Containerization technologies like Docker or Kubernetes\nBachelors degree in Computer Science, Engineering, or related field (or equivalent experience)\nProven track record of delivering complex software projects\nExcellent problem-solving and analytical thinking skills\nStrong communication skills and ability to work in a collaborative environment\nPreferred Qualifications:\nExperience with GCP services, particularly Cloud Run and Dataflow\nExperience with stream processing technologies (Pub/Sub)\nFamiliarity with big data technologies (Airflow)\nExperience with data visualization tools and libraries\nKnowledge of CI/CD pipelines with Gitlab and infrastructure as code with Terraform\nFamiliarity with platforms like Snowflake, Bigquery or Databricks,.\nGCP Data engineer certification",Industry Type: Management Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Software design', 'Version control', 'Analytical', 'Data processing', 'Unit testing', 'data visualization', 'Continuous improvement', 'SQL', 'Python']",2025-06-13 05:12:55
Microsoft Fabric Data engineer,Bahwan CyberTek,12 - 14 years,20-30 Lacs P.A.,"['Indore', 'Hyderabad']","Microsoft Fabric Data engineer\n\nCTC Range 12 14 Years\nLocation – Hyderabad/Indore\nNotice Period - Immediate\n* Primary Skill\n\nMicrosoft Fabric\nSecondary Skill 1\n\nAzure Data Factory (ADF)\n12+ years of experience in Microsoft Azure Data Engineering for analytical projects.\nProven expertise in designing, developing, and deploying high-volume, end-to-end ETL pipelines for complex models, including batch, and real-time data integration frameworks using Azure, Microsoft Fabric and Databricks.\nExtensive hands-on experience with Azure Data Factory, Databricks (with Unity Catalog), Azure Functions, Synapse Analytics, Data Lake, Delta Lake, and Azure SQL Database for managing and processing large-scale data integrations.\nExperience in Databricks cluster optimization and workflow management to ensure cost-effective and high-performance processing.\nSound knowledge of data modelling, data governance, data quality management, and data modernization processes.\nDevelop architecture blueprints and technical design documentation for Azure-based data solutions.\nProvide technical leadership and guidance on cloud architecture best practices, ensuring scalable and secure solutions.\nKeep abreast of emerging Azure technologies and recommend enhancements to existing systems.\nLead proof of concepts (PoCs) and adopt agile delivery methodologies for solution development and delivery.\nwww.yash.com\n\n'Information transmitted by this e-mail is proprietary to YASH Technologies and/ or its Customers and is intended for use only by the individual or entity to which it is addressed, and may contain information that is privileged, confidential or exempt from disclosure under applicable law. If you are not the intended recipient or it appears that this mail has been forwarded to you without proper authority, you are notified that any use or dissemination of this information in any manner is strictly prohibited. In such cases, please notify us immediately at info@yash.com and delete this mail from your records.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Databricks', 'Azure Data Factory', 'Azure Synapse', 'Microsoft Fabric', 'Microsoft Azure Data Engineering', 'Pyspark', 'Data Bricks', 'SQL', 'Azure Data Lake', 'Microsoft Azure', 'Spark', 'ETL', 'Python']",2025-06-13 05:12:57
Data Engineer,Konrad Group,3 - 7 years,15-30 Lacs P.A.,['Gurugram( Sector 42 Gurgaon )'],"Who We Are\n\nKonrad is a next generation digital consultancy. We are dedicated to solving complex business problems for our global clients with creative and forward-thinking solutions. Our employees enjoy a culture built on innovation and a commitment to creating best-in-class digital products in use by hundreds of millions of consumers around the world. We hire exceptionally smart, analytical, and hard working people who are lifelong learners.\nAbout The Role\nAs a Data Engineer youll be tasked with designing, building, and maintaining scalable data platforms and pipelines. Your deep knowledge of data platforms such as Azure Fabric, Databricks, and Snowflake will be essential as you collaborate closely with data analysts, scientists, and other engineers to ensure reliable, secure, and efficient data solutions.\n\nWhat Youll Do\n\nDesign, build, and manage robust data pipelines and data architectures.\nImplement solutions leveraging platforms such as Azure Fabric, Databricks, and Snowflake.\nOptimize data workflows, ensuring reliability, scalability, and performance.\nCollaborate with internal stakeholders to understand data needs and deliver tailored solutions.\nEnsure data security and compliance with industry standards and best practices.\nPerform data modelling, data extraction, transformation, and loading (ETL/ELT).\nIdentify and recommend innovative solutions to enhance data quality and analytics capabilities.\n\nQualifications\n\nBachelors degree or higher in Computer Science, Data Engineering, Information Technology, or a related field.\nAt least 3 years of professional experience as a Data Engineer or similar role.\nProficiency in data platforms such as Azure Fabric, Databricks, and Snowflake.\nHands-on experience with data pipeline tools, cloud services, and storage solutions.\nStrong programming skills in SQL, Python, or related languages.\nExperience with big data technologies and concepts (Spark, Hadoop, Kafka).\nExcellent analytical, troubleshooting, and problem-solving skills.\nAbility to effectively communicate technical concepts clearly to non-technical stakeholders.\nAdvanced English\n\nNice to have\n\nCertifications related to Azure Data Engineering, Databricks, or Snowflake.\nFamiliarity with DevOps practices and CI/CD pipelines.\n\nPerks and Benefits\n\nComprehensive Health & Wellness Benefits Package \nSocials, Outings & Retreats\nCulture of Learning & Development\nFlexible Working Hours\nWork from Home Flexibility\nService Recognition Programs\n\nKonrad is committed to maintaining a diverse work environment and is proud to be an equal opportunity employer. All qualified applicants, regardless of race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status will receive consideration for employment. If you have any accessibility requirements or concerns regarding the hiring process or employment with us, please notify us so we can provide suitable accommodation.\nWhile we sincerely appreciate all applications, only those candidates selected for an interview will be contacted.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Hadoop', 'Azure Data Factory', 'Azure Databricks', 'Spark', 'Fabric', 'Python']",2025-06-13 05:12:59
Data Engineer,Infoobjects Inc.,3 - 6 years,Not Disclosed,['Jaipur'],"Role & responsibilities:\nDesign, develop, and maintain robust ETL/ELT pipelines to ingest and process data from multiple sources.\nBuild and maintain scalable and reliable data warehouses, data lakes, and data marts.\nCollaborate with data scientists, analysts, and business stakeholders to understand data needs and deliver solutions.\nEnsure data quality, integrity, and security across all data systems.\nOptimize data pipeline performance and troubleshoot issues in a timely manner.\nImplement data governance and best practices in data management.\nAutomate data validation, monitoring, and reporting processes.\n\n\n\nPreferred candidate profile:\nBachelor's or Masters degree in Computer Science, Engineering, Information Systems, or related field.\nProven experience (X+ years) as a Data Engineer or similar role.\nStrong programming skills in Python, Java, or Scala.\nProficiency with SQL and working knowledge of relational databases (e.g., PostgreSQL, MySQL).\nHands-on experience with big data technologies (e.g., Spark, Hadoop).\nFamiliarity with cloud platforms such as AWS, GCP, or Azure (e.g., S3, Redshift, BigQuery, Data Factory).\nExperience with orchestration tools like Airflow or Prefect.\nKnowledge of data modeling, warehousing, and architecture design principles.\nStrong problem-solving skills and attention to detail.\n\nPerks and benefits\nFree Meals\nPF and Gratuity\nMedical and Term Insurance",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SCALA', 'Kafka', 'AWS', 'Python', 'Pyspark', 'Java', 'Postgresql', 'Hadoop', 'Spark', 'ETL', 'SQL']",2025-06-13 05:13:00
Data Engineer,XL India Business Services Pvt. Ltd,1 - 7 years,Not Disclosed,['Gurugram'],"Senior Engineer, Data Modeling Gurgaon/Bangalore, India AXA XL recognizes data and information as critical business assets, both in terms of managing risk and enabling new business opportunities\n\nThis data should not only be high quality, but also actionable - enabling AXA XL s executive leadership team to maximize benefits and facilitate sustained industrious advantage\n\nOur Chief Data Office also known as our Innovation, Data Intelligence & Analytics team (IDA) is focused on driving innovation through optimizing how we leverage data to drive strategy and create a new business model - disrupting the insurance market\n\nAs we develop an enterprise-wide data and digital strategy that moves us toward greater focus on the use of data and data-driven insights, we are seeking a Data Engineer\n\nThe role will support the team s efforts towards creating, enhancing, and stabilizing the Enterprise data lake through the development of the data pipelines\n\nThis role requires a person who is a team player and can work well with team members from other disciplines to deliver data in an efficient and strategic manner\n\nWhat you ll be doing What will your essential responsibilities include? Act as a data engineering expert and partner to Global Technology and data consumers in controlling complexity and cost of the data platform, whilst enabling performance, governance, and maintainability of the estate\n\nUnderstand current and future data consumption patterns, architecture (granular level), partner with Architects to make sure optimal design of data layers\n\nApply best practices in Data architecture\n\nFor example, balance between materialization and virtualization, optimal level of de-normalization, caching and partitioning strategies, choice of storage and querying technology, performance tuning\n\nLeading and hands-on execution of research into new technologies\n\nFormulating frameworks for assessment of new technology vs business benefit, implications for data consumers\n\nAct as a best practice expert, blueprint creator of ways of working such as testing, logging, CI/CD, observability, release, enabling rapid growth in data inventory and utilization of Data Science Platform\n\nDesign prototypes and work in a fast-paced iterative solution delivery model\n\nDesign, Develop and maintain ETL pipelines using Py spark in Azure Databricks using delta tables\n\nUse Harness for deployment pipeline\n\nMonitor Performance of ETL Jobs, resolve any issue that arose and improve the performance metrics as needed\n\nDiagnose system performance issue related to data processing and implement solution to address them\n\nCollaborate with other teams to make sure successful integration of data pipelines into larger system architecture requirement\n\nMaintain integrity and quality across all pipelines and environments\n\nUnderstand and follow secure coding practice to make sure code is not vulnerable\n\nYou will report to the Application Manager\n\nWhat you will BRING We re looking for someone who has these abilities and skills: Required Skills and Abilities: Effective Communication skills\n\nBachelor s degree in computer science, Mathematics, Statistics, Finance, related technical field, or equivalent work experience\n\nRelevant years of extensive work experience in various data engineering & modeling techniques (relational, data warehouse, semi-structured, etc), application development, advanced data querying skills\n\nRelevant years of programming experience using Databricks\n\nRelevant years of experience using Microsoft Azure suite of products (ADF, synapse and ADLS)\n\nSolid knowledge on network and firewall concepts\n\nSolid experience writing, optimizing and analyzing SQL\n\nRelevant years of experience with Python\n\nAbility to break complex data requirements and architect solutions into achievable targets\n\nRobust familiarity with Software Development Life Cycle (SDLC) processes and workflow, especially Agile\n\nExperience using Harness\n\nTechnical lead responsible for both individual and team deliveries\n\nDesired Skills and Abilities: Worked in big data migration projects\n\nWorked on performance tuning both at database and big data platforms\n\nAbility to interpret complex data requirements and architect solutions\n\nDistinctive problem-solving and analytical skills combined with robust business acumen\n\nExcellent basics on parquet files and delta files\n\nEffective Knowledge of Azure cloud computing platform\n\nFamiliarity with Reporting software - Power BI is a plus\n\nFamiliarity with DBT is a plus\n\nPassion for data and experience working within a data-driven organization\n\nYou care about what you do, and what we do\n\nWho WE are AXA XL, the P&C and specialty risk division of AXA, is known for solving complex risks\n\nFor mid-sized companies, multinationals and even some inspirational individuals we don t just provide re/insurance, we reinvent it\n\nHow? By combining a comprehensive and efficient capital platform, data-driven insights, leading technology, and the best talent in an agile and inclusive workspace, empowered to deliver top client service across all our lines of business property, casualty, professional, financial lines and specialty\n\nWith an innovative and flexible approach to risk solutions, we partner with those who move the world forward\n\nLearn more at axaxl\n\ncom What we OFFER Inclusion AXA XL is committed to equal employment opportunity and will consider applicants regardless of gender, sexual orientation, age, ethnicity and origins, marital status, religion, disability, or any other protected characteristic\n\nAt AXA XL, we know that an inclusive culture and a diverse workforce enable business growth and are critical to our success\n\nThat s why we have made a strategic commitment to attract, develop, advance and retain the most diverse workforce possible, and create an inclusive culture where everyone can bring their full selves to work and can reach their highest potential\n\nIt s about helping one another and our business to move forward and succeed\n\nFive Business Resource Groups focused on gender, LGBTQ+, ethnicity and origins, disability and inclusion with 20 Chapters around the globe Robust support for Flexible Working Arrangements Enhanced family friendly leave benefits Named to the Diversity Best Practices Index Signatory to the UK Women in Finance Charter Learn more at axaxl\n\ncom / about-us / inclusion-and-diversity\n\nAXA XL is an Equal Opportunity Employer\n\nTotal Rewards AXA XL s Reward program is designed to take care of what matters most to you, covering the full picture of your health, wellbeing, lifestyle and financial security\n\nIt provides dynamic compensation and personalized, inclusive benefits that evolve as you do\n\nWe re committed to rewarding your contribution for the long term, so you can be your best self today and look forward to the future with confidence\n\nSustainability At AXA XL, Sustainability is integral to our business strategy\n\nIn an ever-changing world, AXA XL protects what matters most for our clients and communities\n\nWe know that sustainability is at the root of a more resilient future\n\nOur 2023-26 Sustainability strategy, called Roots of resilience , focuses on protecting natural ecosystems, addressing climate change, and embedding sustainable practices across our operations\n\nOur Pillars: Valuing nature: How we impact nature affects how nature impacts us\n\nResilient ecosystems - the foundation of a sustainable planet and society - are essential to our future\n\nWe re committed to protecting and restoring nature - from mangrove forests to the bees in our backyard - by increasing biodiversity awareness and inspiring clients and colleagues to put nature at the heart of their plans\n\nAddressing climate change: The effects of a changing climate are far reaching and significant\n\nUnpredictable weather, increasing temperatures, and rising sea levels cause both social inequalities and environmental disruption\n\nWere building a net zero strategy, developing insurance products and services, and mobilizing to advance thought leadership and investment in societal-led solutions\n\nIntegrating ESG: All companies have a role to play in building a more resilient future\n\nIncorporating ESG considerations into our internal processes and practices builds resilience from the roots of our business\n\nWe re training our colleagues, engaging our external partners, and evolving our sustainability governance and reporting\n\nAXA Hearts in Action: We have established volunteering and charitable giving programs to help colleagues support causes that matter most to them, known as AXA XL s Hearts in Action programs\n\nThese include our Matching Gifts program, Volunteering Leave, and our annual volunteering day - the Global Day of Giving\n\nFor more information, please see axaxl\n\ncom/sustainability",Industry Type: Insurance,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Performance tuning', 'Data modeling', 'Coding', 'Agile', 'Workflow', 'Application development', 'SDLC', 'SQL', 'Python', 'Firewall']",2025-06-13 05:13:02
Data Engineer 4,Comcast,5 - 11 years,Not Disclosed,['Chennai'],".\nResponsible for designing, building and overseeing the deployment and operation of technology architecture, solutions and software to capture, manage, store and utilize structured and unstructured data from internal and external sources. Establishes and builds processes and structures based on business and technical requirements to channel data from multiple inputs, route appropriately and store using any combination of distributed (cloud) structures, local databases, and other applicable storage forms as required. Develops technical tools and programming that leverage artificial intelligence, machine learning and big-data techniques to cleanse, organize and transform data and to maintain, defend and update data structures and integrity on an automated basis. Creates and establishes design standards and assurance processes for software, systems and applications development to ensure compatibility and operability of data connections, flows and storage requirements. Reviews internal and external business and product requirements for data operations and activity and suggests changes and upgrades to systems and storage to accommodate ongoing needs. Work with data modelers/analysts to understand the business problems they are trying to solve then create or augment data assets to feed their analysis. Integrates knowledge of business and functional priorities. Acts as a key contributor in a complex and crucial environment. May lead teams or projects and shares expertise.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBachelors Degree\nWhile possessing the stated degree is preferred, Comcast also may consider applicants who hold some combination of coursework and experience, or who have extensive related professional experience.\n7-10 Years\nComcast is proud to be an equal opportunity workplace. We will consider all qualified applicants for employment without regard to race, color, religion, age, sex, sexual orientation, gender identity, national origin, disability, veteran status, genetic information, or any other basis protected by applicable law.",,,,"['Engineering services', 'Assurance', 'Process optimization', 'MySQL', 'Machine learning', 'Data structures', 'Data quality', 'Troubleshooting', 'Downstream', 'Python']",2025-06-13 05:13:04
Data Engineer,IT Services Company,2 - 3 years,6-7 Lacs P.A.,['Pune'],"Data Engineer\nJob Description :\nJash Data Sciences: Letting Data Speak!\nDo you love solving real-world data problems with the latest and best techniques? And having fun while solving them in a team! Then come and join our high-energy team of passionate data people. Jash Data Sciences is the right place for you.\nWe are a cutting-edge Data Sciences and Data Engineering startup based in Pune, India. We believe in continuous learning and evolving together. And we let the data speak!\nWhat will you be doing?\nYou will be discovering trends in the data sets and developing algorithms to transform\nraw data for further analytics\nCreate Data Pipelines to bring in data from various sources, with different formats,\ntransform it, and finally load it to the target database.\nImplement ETL/ ELT processes in the cloud using tools like AirFlow, Glue, Stitch, Cloud\nData Fusion, and DataFlow.\nDesign and implement Data Lake, Data Warehouse, and Data Marts in AWS, GCP, or\nAzure using Redshift, BigQuery, PostgreSQL, etc.\nCreating efficient SQL queries and understanding query execution plans for tuning\nqueries on engines like PostgreSQL.\nPerformance tuning of OLAP/ OLTP databases by creating indices, tables, and views.\nWrite Python scripts for the orchestration of data pipelines\nHave thoughtful discussions with customers to understand their data engineering\nrequirements. Break complex requirements into smaller tasks for execution.\nWhat do we need from you?\nStrong Python coding skills with basic knowledge of algorithms/data structures and\ntheir application.\nStrong understanding of Data Engineering concepts including ETL, ELT, Data Lake, Data\nWarehousing, and Data Pipelines.\nExperience designing and implementing Data Lakes, Data Warehouses, and Data Marts\nthat support terabytes of scale data.\nA track record of implementing Data Pipelines on public cloud environments\n(AWS/GCP/Azure) is highly desirable\nA clear understanding of Database concepts like indexing, query performance\noptimization, views, and various types of schemas.\nHands-on SQL programming experience with knowledge of windowing functions,\nsubqueries, and various types of joins.\nExperience working with Big Data technologies like PySpark/ Hadoop\nA good team player with the ability to communicate with clarity\nShow us your git repo/ blog!\nQualification\n1-2 years of experience working on Data Engineering projects for Data Engineer I\n2-5 years of experience working on Data Engineering projects for Data Engineer II\n1-5 years of Hands-on Python programming experience\nBachelors/Masters' degree in Computer Science is good to have\nCourses or Certifications in the area of Data Engineering will be given a higher preference.\nCandidates who have demonstrated a drive for learning and keeping up to date with technology by continuing to do various courses/self-learning will be given high preference.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Airflow', 'Elt', 'Data Mart', 'Data Pipeline', 'ETL', 'Pyspark', 'Hadoop', 'Data Bricks', 'SQL', 'Data Fusion', 'Glue', 'GCP', 'Data Flow', 'Data Warehousing', 'Azzure', 'AWS']",2025-06-13 05:13:06
Data Engineer- MS Fabric,InfoCepts,5 - 9 years,Not Disclosed,['India'],"Position: Data Engineer – MS Fabric\n  Purpose of the Position: As an MS Fabric Data engineer you will be responsible for designing, implementing, and managing scalable data pipelines. Strong experience in implementation and management of lake House using MS Fabric Azure Tech stack (ADLS Gen2, ADF, Azure SQL) .\nProficiency in data integration techniques, ETL processes and data pipeline architectures. Well versed in Data Quality rules, principles and implementation.\n",,,,"['components', 'data', 'scala', 'delta', 'pyspark', 'data warehousing', 'rules', 'azure data factory', 'sql', 'parquet', 'analytics', 'sql azure', 'spark', 'oracle adf', 'data pipeline architecture', 'etl', 'python', 'azure synapse', 'microsoft azure', 'power bi', 'data bricks', 'data quality', 'system', 't', 'fabric', 'data integration', 'etl process']",2025-06-13 05:13:08
Deputy Manager - Data Engineer - Analytics,IBM,2 - 7 years,Not Disclosed,['Bengaluru'],"Develop, test and support future-ready data solutions for customers across industry verticals\nDevelop, test, and support end-to-end batch and near real-time data flows/pipelines\nDemonstrate understanding in data architectures, modern data platforms, big data, analytics, cloud platforms, data governance and information management and associated technologies\nCommunicates risks and ensures understanding of these risks.\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nMinimum of 2+ years of related experience required\nExperience in modeling and business system designs\nGood hands-on experience on DataStage, Cloud based ETL Services\nHave great expertise in writing TSQL code\nWell versed with data warehouse schemas and OLAP techniques\n\n\nPreferred technical and professional experience\nAbility to manage and make decisions about competing priorities and resources.\nAbility to delegate where appropriate\nMust be a strong team player/leader\nAbility to lead Data transformation project with multiple junior data engineers\nStrong oral written and interpersonal skills for interacting and throughout all levels of the organization.\nAbility to clearly communicate complex business problems and technical solutions.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['information management', 'cloud platforms', 'data architecture', 'data governance', 'big data', 'schema', 'python', 'data analytics', 'datastage', 'microsoft azure', 'warehouse', 't-sql', 'data engineering', 'ansible', 'docker', 'sql', 'java', 'devops', 'linux', 'olap', 'jenkins', 'shell scripting', 'etl', 'aws']",2025-06-13 05:13:10
Data Engineer-Data Platforms-Google,IBM,5 - 7 years,Not Disclosed,['Bengaluru'],"A career in IBM Consulting is rooted by long-term relationships and close collaboration with clients across the globe.You'll work with visionaries across multiple industries to improve the hybrid cloud and Al journey for the most innovative and valuable companies in the world. Your ability to accelerate impact and make meaningful change for your clients is enabled by our strategic partner ecosystem and our robust technology platforms across the IBM portfolio; including Software and Red Hat.\n\nIn your role, you will be responsible for:\nSkilled Multiple GCP services - GCS, BigQuery, Cloud SQL, Dataflow, Pub/Sub, Cloud Run, Workflow, Composer, Error reporting, Log explorer etc.\nMust have Python and SQL work experience & Proactive, collaborative and ability to respond to critical situation\nAbility to analyse data for functional business requirements & front face customer\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\n5 to 7 years of relevant experience working as technical analyst with Big Query on GCP platform.\nSkilled in multiple GCP services - GCS, Cloud SQL, Dataflow, Pub/Sub, Cloud Run, Workflow, Composer, Error reporting, Log explorer\nAmbitious individual who can work under their own direction towards agreed targets/goals and with creative approach to work\nYou love collaborative environments that use agile methodologies to encourage creative design thinking and find innovative ways to develop with cutting edge technologies.\nEnd to End functional knowledge of the data pipeline/transformation implementation that the candidate has done, should understand the purpose/KPIs for which data transformation was done\n\n\nPreferred technical and professional experience\nExperience with AEM Core Technologies OSGI Services, Apache Sling ,Granite Framework., Java Content Repository API, Java 8+, Localization\nFamiliarity with building tools, Jenkin and Maven , Knowledge of version control tools, especially Git, Knowledge of Patterns and Good Practices to design and develop quality and clean code, Knowledge of HTML, CSS, and JavaScript , jQuery\nFamiliarity with task management, bug tracking, and collaboration tools like JIRA and Confluence",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sql', 'java', 'html', 'python', 'javascript', 'hive', 'css', 'confluence', 'aem', 'data warehousing', 'apache sling', 'jquery', 'gen', 'git', 'gcp', 'spark', 'jenkins', 'bigquery', 'data transformation', 'hadoop', 'big data', 'etl', 'jira', 'cloud sql', 'maven', 'airflow', 'osgi', 'granite', 'agile', 'sqoop', 'aws']",2025-06-13 05:13:12
Data Engineer-Data Platforms-Google,IBM,5 - 7 years,Not Disclosed,['Bengaluru'],"Skilled Multiple GCP services - GCS, BigQuery, Cloud SQL, Dataflow, Pub/Sub, Cloud Run, Workflow, Composer, Error reporting, Log explorer etc.\nMust have Python and SQL work experience & Proactive, collaborative and ability to respond to critical situation\nAbility to analyse data for functional business requirements & front face customer\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\n5 to 7 years of relevant experience working as technical analyst with Big Query on GCP platform.\nSkilled in multiple GCP services - GCS, Cloud SQL, Dataflow, Pub/Sub, Cloud Run, Workflow, Composer, Error reporting, Log explorer\nYou love collaborative environments that use agile methodologies to encourage creative design thinking and find innovative ways to develop with cutting edge technologies\nAmbitious individual who can work under their own direction towards agreed targets/goals and with creative approach to work\n\n\nPreferred technical and professional experience\nCreate up to 3 bullets maxitive individual with an ability to manage change and proven time management\nProven interpersonal skills while contributing to team effort by accomplishing related results as needed\nUp-to-date technical knowledge by attending educational workshops, reviewing publications (encouraging then to focus on required skills)",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sql', 'gcp', 'bigquery', 'cloud sql', 'python', 'hive', 'gen', 'java', 'postgresql', 'spark', 'linux', 'mysql', 'hadoop', 'big data', 'pubsub', 'airflow', 'application engine', 'machine learning', 'sql server', 'dataproc', 'cloud storage', 'bigtable', 'agile', 'sqoop', 'aws', 'data flow']",2025-06-13 05:13:14
Data Engineer-Data Platforms,IBM,2 - 5 years,Not Disclosed,['Bengaluru'],"As a Data Engineer at IBM, you'll play a vital role in the development, design of application, provide regular support/guidance to project teams on complex coding, issue resolution and execution.\n\nYour primary responsibilities include:\nLead the design and construction of new solutions using the latest technologies, always looking to add business value and meet user requirements.\nStrive for continuous improvements by testing the build solution and working under an agile framework.\nDiscover and implement the latest technologies trends to maximize and build creative solutions\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nExperience with Apache Spark (PySpark)In-depth knowledge of Spark’s architecture, core APIs, and PySpark for distributed data processing.\nBig Data TechnologiesFamiliarity with Hadoop, HDFS, Kafka, and other big data tools. Data Engineering\n\nSkills:\nStrong understanding of ETL pipelines, data modeling, and data warehousing concepts.\nStrong proficiency in PythonExpertise in Python programming with a focus on data processing and manipulation. Data Processing FrameworksKnowledge of data processing libraries such as Pandas, NumPy.\nSQL ProficiencyExperience writing optimized SQL queries for large-scale data analysis and transformation.\nCloud PlatformsExperience working with cloud platforms like AWS, Azure, or GCP, including using cloud storage systems\n\n\nPreferred technical and professional experience\nDefine, drive, and implement an architecture strategy and standards for end-to-end monitoring.\nPartner with the rest of the technology teams including application development, enterprise architecture, testing services, network engineering,\nGood to have detection and prevention tools for Company products and Platform and customer-facing",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'data processing', 'pyspark', 'data modeling', 'spark', 'data analysis', 'microsoft azure', 'data warehousing', 'cloud platforms', 'numpy', 'data engineering', 'sql', 'pandas', 'spring boot', 'etl pipelines', 'java', 'gcp', 'kafka', 'data warehousing concepts', 'hadoop', 'big data', 'aws', 'etl']",2025-06-13 05:13:15
Sales Excellence - COE - Data Engineering Specialist,Accenture,5 - 8 years,Not Disclosed,['Bengaluru'],"Job Title -\n\n\n\nSales Excellence - COE - Data Engineering Specialist\n\n\n\nManagement Level:\n\n\n\n9-Team Lead/Consultant\n\n\n\nLocation:\n\n\n\nMumbai, MDC2C\n\n\n\nMust-have skills:Sales\n\n\n\n\nGood to have skills:Data Science, SQL, Automation, Machine Learning\n\n\n\nJob\n\n\nSummary:\n\nApply deep statistical tools and techniques to find relationships between variables\n\n\n\n\nRoles & Responsibilities:\n\n- Apply deep statistical tools and techniques to find relationships between variables.\n\n- Develop intellectual property for analytical methodologies and optimization techniques.\n\n- Identify data requirements and develop analytic solutions to solve business issues.\n\nJob Title - Analytics & Modelling Specialist\n\nManagement Level :9-Specialist\n\nLocation:Bangalore/ Gurgaon/Hyderabad/Mumbai\n\nMust have skills:Python, Data Analysis, Data Visualization, SQL\nGood to have skills:Machine Learning\n\nJob\n\n\nSummary:\n\nThe Center of Excellence (COE) makes sure that the sales and pricing methods and offerings of Sales Excellence are effective.\n\n- The COE supports salespeople through its business partners and Analytics and Sales Operations teams.\n\nThe Data Engineer helps manage data sources and environments, utilizing large data sets and maintaining their integrity to create models and apps that deliver insights to the organization.\nRoles & Responsibilities:\n\nBuild and manage data models that bring together data from different sources.\n\nHelp consolidate and cleanse data for use by the modeling and development teams.\n\nStructure data for use in analytics applications.\n\nLead a team of Data Engineers effectively.\nProfessional & Technical\n\n\n\n\nSkills:\nA bachelors degree or equivalent\n\nTotal experience Range:5-8 years in the relevant field\n\nA minimum of 3 years of GCP experience with exposure to machine learning/data science\n\nExperience in configuration the machine learning workflow in GCP.\n\nA minimum of 5 years Advanced SQL knowledge and experience working with relational databases\n\nA minimum of 3 years Familiarity and hands on experience in different SQL objects like stored procedures, functions, views etc.,\n\nA minimum of 3 years Building of data flow components and processing systems to extract, transform, load and integrate data from various sources.\n\nA minimum of 3 years Hands on experience in advanced excel topics such as cube functions, VBA Automation, Power Pivot etc.\n\nA minimum of 3 years Hands on experience in Python\nAdditional Information:\n\nUnderstanding of sales processes and systems.\n\nMasters degree in a technical field.\n\nExperience with quality assurance processes.\n\nExperience in project management.\n\nYou May Also Need:\n\nAbility to work flexible hours according to business needs.\n\nMust have good internet connectivity and a distraction-free environment for working at home, in accordance with local guidelines.\n\n\n\n\nProfessional & Technical\n\n\n\n\nSkills:\n\n\n- Relevant experience in the required domain.\n\n- Strong analytical, problem-solving, and communication skills.\n\n- Ability to work in a fast-paced, dynamic environment.\n\n\n\n\nAdditional Information:\n\n- Opportunity to work on innovative projects.\n\n- Career growth and leadership exposure.\n\n\n\n\n\nAbout Our Company | AccentureQualification\n\n\n\nExperience:8 to 10 Years\n\n\n\n\nEducational Qualification:\n\n\n\nB.Com",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'data analysis', 'sales', 'sql', 'data visualization', 'hive', 'advance sql', 'ssas', 'dbms', 'machine learning', 'data engineering', 'power pivot', 'sql server', 'vba automation', 'data science', 'gcp', 'spark', 'advanced excel', 'hadoop', 'ssis', 'etl', 'big data', 'data flow', 'sql joins']",2025-06-13 05:13:17
"Data Engineer II, SCOT - AIM",Amazon,3 - 8 years,Not Disclosed,['Bengaluru'],"SCOTs Automated Inventory Management (AIM) team seeks talented individuals passionate about solving complex problems and driving impactful business decisions for our executives. The AIM team owns critical Tier 1 metrics for Amazon Retail stores, providing key insights to improve store health monitoring. We focus on enhancing selection, product availability, inventory efficiency, and inventory readiness to fulfill customer orders (FastTrack) while enabling accelerated delivery Speed Fulfillment Worldwide. This improves both Customer Experience (CX) and Long-Term Free Cash Flow (LTFCF) outcomes. Our approach involves creating standardized, scalable, and automated systems and tools to identify and reduce supply chain defects in our systems and inputs, while driving operational leverage and scaling.\n\nAs a Data Engineer, you will analyze large-scale business data, solve real-world problems, and develop metrics and business cases to delight our customers worldwide. You will work closely with Scientists, Engineers, and Product Managers to build scalable, high-impact products, architect data pipelines, and transform data into actionable insights to manage business at scale. We are looking for people who are motivated by thinking big, moving fast, and exploring business insights. If you love to implement solutions to hard problems while working hard, having fun, and making history, this may be the opportunity for you.\n\nAbout the team\nSupply Chain Optimization Technologies (SCOT) is the name of a complex group of systems designed to make the best decisions when it comes to forecasting, buying, placing, and shipping inventory. Functionally these teams work together to drive in-stock, drive placement, drive inventory removal and manage the customer experience.\n\n3+ years of data engineering experience\n4+ years of SQL experience\nExperience with data modeling, warehousing and building ETL pipelines Experience with AWS technologies like Redshift, S3, AWS Glue, EMR, Kinesis, FireHose, Lambda, and IAM roles and permissions\nExperience with non-relational databases / data stores (object storage, document or key-value stores, graph databases, column-family databases)\nExperience with big data technologies such as: Hadoop, Hive, Spark, EMR",,,,"['Supply chain', 'data engineer ii', 'Data modeling', 'Inventory management', 'Cash flow', 'Customer experience', 'Forecasting', 'Operations', 'Monitoring', 'SQL']",2025-06-13 05:13:19
Data Engineer II,Amazon,3 - 8 years,Not Disclosed,['Bengaluru'],"Amazon s Consumer Payments organization is seeking a highly quantitative, experienced Data Engineer to drive growth through analytics, automation of data pipelines, and enhancement of self-serve experiences. . You will succeed in this role if you are an organized self-starter who can learn new technologies quickly and excel in a fast-paced environment. In this position, you will be a key contributor and sparring partner, developing analytics and insights that global executive management teams and business leaders will use to define global strategies and deep dive businesses.\nYou will be part the team that is focused on acquiring new merchants from around the world to payments around the world. The position is based in India but will interact with global leaders and teams in Europe, Japan, US, and other regions. You should be highly analytical, resourceful, customer focused, team oriented, and have an ability to work independently under time constraints to meet deadlines. You will be comfortable thinking big and diving deep. A proven track record in taking on end-to-end ownership and successfully delivering results in a fast-paced, dynamic business environment is strongly preferred.\nResponsibilities include but not limited to:\nDesign, develop, implement, test, and operate large-scale, high-volume, high-performance data structures for analytics and Reporting.\nImplement data structures using best practices in data modeling, ETL/ELT processes, and SQL, AWS Redshift, and OLAP technologies, Model data and metadata for ad hoc and pre-built reporting.\nWork with product tech teams and build robust and scalable data integration (ETL) pipelines using SQL, Python and Spark.\nContinually improve ongoing reporting and analysis processes, automating or simplifying self-service support for customers.\nInterface with business customers, gathering requirements and delivering complete reporting solutions.\nCollaborate with Analysts, Business Intelligence Engineers and Product Managers to implement algorithms that exploit rich data sets for statistical analysis, and machine learning.\nParticipate in strategic tactical planning discussions, including annual budget processes.\nCommunicate effectively with product / business / tech-teams / other Data teams.\n3+ years of data engineering experience\nExperience with data modeling, warehousing and building ETL pipelines Experience with AWS technologies like Redshift, S3, AWS Glue, EMR, Kinesis, FireHose, Lambda, and IAM roles and permissions\nExperience with non-relational databases / data stores (object storage, document or key-value stores, graph databases, column-family databases)",,,,"['Automation', 'metadata', 'Data modeling', 'Machine learning', 'Data structures', 'OLAP', 'Business intelligence', 'Analytics', 'SQL', 'Python']",2025-06-13 05:13:21
Data Engineer II,Amazon,3 - 8 years,Not Disclosed,['Bengaluru'],"Amazon strives to be the worlds most customer-centric company, where customers can research and purchase anything they might want online\nWe set big goals and are looking for people who can help us reach and exceed them\nThe CPT Data Engineering & Analytics (DEA) team builds and maintains critical data infrastructure that enhances seller experience and protects the privacy of Amazon business partners throughout their lifecycle\nWe are looking for a strong Data Engineer to join our team\n\nThe Data Engineer I will work with well-defined requirements to develop and maintain data pipelines that help internal teams gather required insights for business decisions timely and accurately\nYou will collaborate with a team of Data Scientists, Business Analysts and other Engineers to build solutions that reduce investigation defects and assess the health of our Operations business while ensuring data quality and regulatory compliance\n\nThe ideal candidate must be passionate about building reliable data infrastructure, detail-oriented, and driven to help protect Amazons customers and business partners\nThey will be an individual contributor who works effectively with guidance from senior team members to successfully implement data solutions\nThe candidate must be proficient in SQL and at least one scripting language (e\ng\nPython, Perl, Scala), with strong understanding of data management fundamentals and distributed systems concepts\n\n\nBuild and optimize physical data models and data pipelines for simple datasets\nWrite secure, stable, testable, maintainable code with minimal defects\nTroubleshoot existing datasets and maintain data quality\nParticipate in team design, scoping, and prioritization discussions\nDocument solutions to ensure ease of use and maintainability\nHandle data in accordance with Amazon policies and security requirements Masters degree in computer science, engineering, analytics, mathematics, statistics, IT or equivalent\n3+ years of data engineering experience\nExperience with SQL\nExperience with data modeling, warehousing and building ETL pipelines\nKnowledge of distributed systems concepts from data storage and compute perspective\nAbility to work effectively in a team environment Experience with AWS technologies like Redshift, S3, AWS Glue, EMR, Kinesis, FireHose, Lambda, and IAM roles and permissions\nFamiliarity with big data technologies (Hadoop, Spark, etc\n)\nKnowledge of data security and privacy best practices\nStrong problem-solving and analytical skills\nExcellent written and verbal communication skills",Industry Type: Internet,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Data management', 'Data modeling', 'data security', 'Perl', 'Data quality', 'Distribution system', 'Analytics', 'SQL', 'Python']",2025-06-13 05:13:23
Data Engineer,Hinduja Tech,6 - 10 years,Not Disclosed,['Pune'],"Education: Bachelors or masters degree in computer science, Information Technology, Engineering, or a related field.Experience: 6-10 years\n8+ years of experience in data engineering or a related field.\nStrong hands-on experience with Azure Databricks, Spark, Python/Scala, CICD, Scripting for data processing.\nExperience working in multiple file formats like Parquet, Delta, and Iceberg.\nKnowledge of Kafka or similar streaming technologies for real-time data ingestion.",,,,"['Data Engineer', 'Azure Databricks', 'ETL', 'Pyspark', 'AWS', 'Python', 'SQL']",2025-06-13 05:13:25
Data Engineer,Diverse Lynx,3 - 6 years,Not Disclosed,['Noida'],"Responsibilities\nAs part of the Client delivery team, your primary role would be to interface with the client for quality assurance, issue resolution and ensuring high customer satisfaction.\nYou will understand requirements, create and review designs, validate the architecture and ensure high levels of service offerings to clients in the technology domain.\nYou will participate in project estimation, provide inputs for solution delivery, conduct technical risk planning, perform code reviews and unit test plan reviews.\nYou will lead and guide your teams towards developing optimized high quality code deliverables, continual knowledge management and adherence to the organizational guidelines and processes.\nYou would be a key contributor to building efficient programs/ systems and if you think you fit right in to help our clients navigate their next in their digital transformation journey, this is the place for you!If you think you fit right in to help our clients navigate their next in their digital transformation journey, this is the place for you!",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Manager Quality Assurance', 'Project estimation', 'Architecture', 'Customer satisfaction', 'Test planning', 'Issue resolution', 'Unit testing', 'Management', 'Solution delivery', 'digital transformation']",2025-06-13 05:13:26
Data Engineer,Mobio Solutions,3 - 5 years,Not Disclosed,['Ahmedabad'],"Job Overview:\nWe are looking for a skilled and experienced Data Engineer to join our team. The ideal candidate will have a strong background in Azure Data Factory, Databricks, Pyspark, Python , Azure SQL and other Azure cloud services, and will be responsible for building and managing scalable data pipelines, data lakes, and data warehouses . Experience with Azure Synapse Analytics, Microsoft Fabric or PowerBI will be considered a strong advantage.\nKey Responsibilities:\nDesign, develop, and manage robust and scalable ETL/ELT pipelines using Azure Data Factory and Databricks\nWork with PySpark and Python to transform and process large datasets\nBuild and maintain data lakes and data warehouses on Azure Cloud\nCollaborate with data architects, analysts, and stakeholders to gather and translate requirements into technical solutions\nEnsure data quality, consistency, and integrity across systems\nOptimize performance and cost of data pipelines and cloud infrastructure\nImplement best practices for security, governance, and monitoring of data pipelines\nMaintain and document data workflows and architecture\nRequired Skills & Qualifications:\n3-5 years of experience in Data Engineering\nStrong hands-on experience with:\nAzure Data Factory (ADF)\nAzure Databricks\nAzure SQL\nPySpark and Python\nAzure Storage (Blob, Data Lake Gen2)\nHands-on experience with data warehouse/Lakehouse/data lake architecture\nFamiliarity with Delta Lake, MLflow, and Unity Catalog is a plus\nGood understanding of SQL and performance tuning\nKnowledge of CI/CD in Azure for data pipelines\nExcellent problem-solving skills and ability to work independently\nPreferred Skills:\nExperience with Azure Synapse Analytics\nFamiliarity with Microsoft Fabric\nWorking knowledge of Power BI for data visualization and dashboarding\nExposure to DevOps and infrastructure as code (IaC) in Azure\nUnderstanding of data governance and security best practices\nDatabricks certification (e.g., Databricks Certified Data Engineer Associate/Professional)",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Performance tuning', 'Cloud', 'data governance', 'Data quality', 'data visualization', 'microsoft', 'Analytics', 'Monitoring', 'SQL', 'Python']",2025-06-13 05:13:28
Data Engineer - Asset Lending,Investec Global Services,2 - 5 years,Not Disclosed,['Mumbai'],"About the role :\nPrimary function of the role is to deliver high quality data engineering solutions to business and end users across Asset Lending (Asset Finance, Working Capital and Asset Based Lending businesses either directly via self-service data products, or by working closely with the Analytics team, providing modelled data warehouses on which they can add reporting and analytics.\nReporting to the Head of ccc Technology, this role will fill a crucial role in bridging the gap between business needs, the requirements from the data analytics team and translating these into engineering delivery.\nKey Responsibilities :\nWork closely with end-users and Data Analysts to understand the business and their data requirements\nCarry out ad hoc data analysis and data wrangling using Synapse Analytics and Databricks\nBuilding dynamic meta-data driven data ingestion patterns using Azure Data Factory and Databricks\nBuild and maintain the Enterprise Data Warehouse (using Data Vault 2.0 methodology)\nBuild and maintain business focused data products and data marts\nBuild and maintain Azure Analysis Services databases and cubes\nShare support and operational duties within the wider engineering and data teams\nWork with Architecture and Engineering teams to deliver on these projects. and ensure that supporting code and infrastructure follows best practices outlined by these teams.\nHelp define test criteria to establish clear conditions for success and ensure alignment with business objectives.\nManage their user stories and acceptance criteria through to production into day-to-day support\nAssist in the testing and validation of new requirements and processes to ensure they meet business need\nWhat are we looking for?\nExcellent data analysis and exploration using T-SQL\nStrong SQL programming (stored procedures, functions)\nExtensive experience with SQL Server and SSIS\nKnowledge and experience of data warehouse modelling methodologies (Kimball, dimensional modelling, Data Vault 2.0)\nExperience in Azure one or more of the following: Data Factory, Databricks, Synapse Analytics, ADLS Gen2\nExperience in building robust and performant ETL processes\nBuild and maintain Analysis Services databases and cubes (both multidimensional and tabular)\nExperience in using source control & ADO\nUnderstanding and experience of deployment pipelines\n",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data analysis', 'Agile', 'Stored procedures', 'SSIS', 'Operations', 'Data warehousing', 'Analytics', 'Analysis services', 'SQL', 'google maps']",2025-06-13 05:13:30
Data Engineer,Smartavya Analytica,3 - 5 years,Not Disclosed,['Pune'],"Job Title: Data Engineer\nLocation: Pune, India (On-site)\nExperience: 3 5 years\nEmployment Type: Full-time\n\nJob Summary\nWe are looking for a hands-on Data Engineer who can design and build modern Lakehouse solutions on Microsoft Azure. You will own data ingestion from source-system APIs through Azure Data Factory into OneLake, curate bronze silver gold layers on Delta Lake, and deliver dimensional models that power analytics at scale.",,,,"['Azure Data Factory', 'Azure Synapse', 'Adls Gen2', 'Data Lake', 'Fabric']",2025-06-13 05:13:31
Data Engineer - SSIS - 5+ Years - Gurugram (Hybrid),Crescendo Global,5 - 10 years,Not Disclosed,['Gurugram'],"Data Engineer - SSIS - 5+ Years - Gurugram (Hybrid)\n\nAre you a skilled Data Engineer with expertise in SSIS and 5+ years of experience? Do you have a passion for analytics and want to work in a hybrid setup in Gurugram? Our client is seeking a talented individual to join their team and contribute to their data engineering projects.\n\nLocation : Gurugram (Hybrid)\n\nYour Future EmployerOur client is a leading organization in the analytics domain, known for fostering an inclusive and diverse work environment. They are committed to providing their employees with opportunities for growth and development.\n\nResponsibilities\nDesign, develop, and maintain data pipelines using SSIS for efficient data processing\nCollaborate with cross-functional teams to understand data requirements and provide effective data solutions\nOptimize data pipelines for performance and scalability\nEnsure data quality and integrity throughout the data engineering process\n\nRequirements\n5+ years of experience in data engineering with a strong focus on SSIS\nProficiency in data warehousing concepts and ETL processes\nHands-on experience with SQL databases and data modeling4.Strong analytical and problem-solving skills\nBachelor's degree in Computer Science, Engineering, or related field\n\nWhat's in it for you : In this role, you will have the opportunity to work on challenging projects and enhance your expertise in data engineering. The organization offers a competitive compensation package and a supportive work environment where your contributions are valued.\n\nReach us : If you feel this opportunity is well aligned with your career progression plans, please feel free to reach me with your updated profile at rohit.kumar@crescendogroup.in\n\nDisclaimer : Crescendo Global specializes in Senior to C-level niche recruitment. We are passionate about empowering job seekers and employers with an engaging memorable job search and leadership hiring experience. Crescendo Global does not discriminate on the basis of race, religion, color, origin, gender, sexual orientation, age, marital status, veteran status or disability status.\n\nNote : We receive a lot of applications on a daily basis so it becomes a bit difficult for us to get back to each candidate. Please assume that your profile has not been shortlisted in case you don't hear back from us in 1 week. Your patience is highly appreciated.\n\nScammers can misuse Crescendo Globals name for fake job offers. We never ask for money, purchases, or system upgrades. Verify all opportunities at www.crescendo-global.com and report fraud immediately. Stay alert!\n\nProfile keywords : Data Engineer, SSIS, Data Warehousing, ETL, SQL, Analytics",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Engineering', 'SSIS', 'SQL', 'Analytics']",2025-06-13 05:13:33
Data Engineer,Fortune India 500 Chemicals Firm,12 - 18 years,Not Disclosed,['Mumbai (All Areas)'],"Skills:\nData Management: Expertise in data warehousing, SQL/NoSQL, cloud platforms (AWS, Azure, GCP)\nETL Tools: Proficient in Informatica, Talend, Azure Data Factory\nModelling: Strong in dimensional modelling, star/snowflake schema\nGovernance & Compliance: Knowledge of GDPR, HIPAA, data governance frameworks\nLanguages: T-SQL, PL/SQL\nSoft Skills: Effective communicator, strong analytical and problem-solving skills\nKey Responsibilities:\nArchitecture: Designed scalable, high-performance data warehouse architectures and data models\nETL & Integration: Led ETL design/development for structured/unstructured data across platforms\nGovernance: Defined data quality standards and collaborated on data governance policy implementation\nCollaboration: Interfaced with BI, data science, and business teams to align data strategies\nPerformance & Security: Optimized queries/ETL jobs and ensured data security and compliance\nDocumentation: Maintained standards and documentation for architecture, ETL, and workflows",Industry Type: Chemicals,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Warehousing', 'GCP', 'Snowflake', 'Microsoft Azure', 'Dimensional Modeling', 'Data Modeling', 'ETL', 'AWS']",2025-06-13 05:13:35
Data Engineer,Centrilogic,15 - 20 years,Not Disclosed,['Hyderabad'],"Data Engineer\n\nPurpose:\n\nOver 15 years, we have become a premier global provider of multi-cloud management, cloud-native application development solutions, and strategic end-to-end digital transformation services.\nHeadquartered in Canada and with regional headquarters in the U.S. and the United Kingdom, Centrilogic delivers smart, streamlined solutions to clients worldwide.\n\nWe are looking for a passionate and experienced Data Engineer to work with our other 70 Software, Data and DevOps engineers to guide and assist our clients data modernization journey.\n\nOur team works with companies with ambitious missions - clients who are creating new, innovative products, often in uncharted markets. We work as embedded members and leaders of our clients development and data teams. We bring experienced senior engineers, leading-edge technologies and mindsets, and creative thinking. We show our clients how to move to the modern frameworks of data infrastructures and processing, and we help them reach their full potential with the power of data.\n\nIn this role, youll be the day-to-day primary point of contact with our clients to modernize their data infrastructures, architecture, and pipelines.\n\nPrincipal Responsibilities:\n\nConsulting clients on cloud-first strategies for core bet-the-company data initiatives\nProviding thought leadership on both process and technical matters\nBecoming a real champion and trusted advisor to our clients on all facets of Data Engineering\nDesigning, developing, deploying, and supporting the modernization and transformation of our client s end-to-end data strategy, including infrastructure, collection, transmission, processing, and analytics\nMentoring and educating clients teams to keep them up to speed with the latest approaches, tools and skills, and setting them up for continued success post-delivery\n\nRequired Experience and Skills:\n\nMust have either Microsoft Certified Azure Data Engineer Associate or Fabric Data Engineer Associate certification.\nMust have experience working in a consulting or contracting capacity on large data management and modernization programs.\nExperience with SQL Servers, data engineering, on platforms such as Azure Data Factory, Databricks, Data Lake, and Synapse.\nStrong knowledge and demonstrated experience with Delta Lake and Lakehouse Architecture.\nStrong knowledge of securing Azure environment, such as RBAC, Key Vault, and Azure Security Center.\nStrong knowledge of Kafka and Spark and extensive experience using them in a production environment.\nStrong and demonstrable experience as DBA in large-scale MS SQL environments deployed in Azure.\nStrong problem-solving skills, with the ability to get to the route of an issue quickly.\nStrong knowledge of Scala or Python.\nStrong knowledge of Linux administration and networking.\nScripting skills and Infrastructure as Code (IaC) experience using PowerShell, Bash, and ARM templates.\nUnderstanding of security and corporate governance issues related with cloud-first data architecture, as well as accepted industry solutions.\nExperience in enabling continuous delivery for development teams using scripted cloud provisioning and automated tooling.\nExperience working with Agile development methodology that is fit for purpose.\nSound business judgment and demonstrated leadership",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['MS SQL', 'Networking', 'Data management', 'Powershell', 'Consulting', 'Application development', 'microsoft', 'Analytics', 'Python', 'Data architecture']",2025-06-13 05:13:36
Data Engineer,zocket,4 - 5 years,Not Disclosed,['Chennai'],"About Zocket: AI Assistant for Marketers\nZocket empowers businesses to scale social media advertising seamlessly with advanced AI . From search to conversions, Zocket automates the entire workflow effortlessly.\n\nHeadquartered in Chennai and San Francisco , with 500+ customers across 25+ countries.\n\nOur GenAI Product Suite:\nCreative Studio: Make ad creatives 10x faster, 20x better\n\nAudience Assistant: Precision targeting, platform-wide\n\nSnoop AI: Competitive & industry insights in seconds\n\nInsights AI: 360 performance dashboard across channels\n\nGrowth Infra: Whitelisted infra for unbounded scale\n\nTrusted and Recognized:\nAWS GenAI Accelerator 2024 (Top 80 global startups)\n\nGoogle for Startups (GFSA Class VIII Ai-first)\n\nNASSCOM Gen AI Foundry\n\n4.9 Trustpilot | #1 Product of the Day & Week on Product Hunt\n\n\nKey Responsibilities\n\nDesign, build, and maintain robust ETL pipelines to process large-scale structured and unstructured data.\n\nWork with GraphQL databases and Vector DBs (e.g., Pinecone, Chroma) to power intelligent, AI-driven features.\n\nBuild and optimize scalable ML pipelines and contribute to model lifecycle management .\n\nImplement real-time and batch data streaming solutions using Kafka and Spark .\n\nWork with Airflow to schedule and orchestrate data workflows.\n\nDevelop efficient data models in PostgreSQL and ensure high-performance data access.\n\nIntegrate and optimize Elasticsearch for fast search and analytics.\n\nEnsure high data quality and reliability through monitoring and automation.\n\nCollaborate with Product, Data Science, and Engineering teams to deliver scalable data solutions.\n\nOwn the data infrastructure on AWS - from data lake to warehouse and analytics stack.\n\nDrive internal data tooling improvements and support analytics for business decision-making.\n\n\nKey Requirements\n\n4-5 years of experience as a Data Engineer or in a similar backend/data-intensive role.\n\nStrong programming experience with Python and familiarity with ML model development workflows .\n\nHands-on experience with GraphQL and Vector DBs (Pinecone, Chroma).\n\nProficiency in PostgreSQL and working knowledge of NoSQL and search systems like Elasticsearch .\n\nSolid understanding of data engineering on AWS (EC2, S3, RDS, Redshift, etc.).\n\nExperience with Apache Kafka , Apache Spark , and Airflow is essential.\n\nFamiliarity with modern ML Ops and data science integration is a plus.\n\nStrong analytical thinking and problem-solving skills.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Automation', 'Backend', 'data science', 'Analytical', 'Social media', 'Workflow', 'Data quality', 'AWS', 'Analytics', 'Monitoring']",2025-06-13 05:13:38
Data Engineer,Trantor,5 - 10 years,Not Disclosed,[],"We are looking for a skilled and motivated Data Engineer with deep expertise in GCP,\nBigQuery, Apache Airflow to join our data platform team. The ideal candidate should have hands-on experience building scalable data pipelines, automating workflows, migrating large-scale datasets, and optimizing distributed systems. The candidate should have experience with building Web APIs using Python. This role will play a key part in designing and maintaining robust data engineering solutions across cloud and on-prem environments.\nKey Responsibilities\nBigQuery & Cloud Data Pipelines:\nDesign and implement scalable ETL pipelines for ingesting large-scale datasets.\nBuild solutions for efficient querying of tables in BigQuery.\nAutomated scheduled data ingestion using Google Cloud services and scheduled\nApache Airflow DAGs",,,,"['Airflow', 'Etl Pipelines', 'GCP', 'Bigquery', 'Python', 'SFTP', 'ETL', 'SQL']",2025-06-13 05:13:40
Data Engineer,Society Managers,3 - 5 years,Not Disclosed,['Mumbai (All Areas)'],"We are seeking a skilled and driven SDE-II (Data Engineering) to join our dynamic team. In this role, you will design, develop, and maintain scalable data pipelines, working with large, complex datasets. Youll collaborate closely with cross-functional teams to gather data requirements and contribute to the architecture of our data systems, leveraging your expertise in tools like Databricks, Spark, and SQL.\n\nRoles and responsibilities\nData Pipeline Development: Design,build, and maintain scalable data pipelines using Databricks, Python,and Spark.\nData Processing & Transformation: Handle large, complex datasets to ensure efficient data processing and transformations.\nCollaboration: Work with cross-functional teams to gather, understand, and implement data requirements.\nSQL & ETL: Write and optimize SQL queries for data extraction, transformation, and loading (ETL) processes.\nData Quality & Security: Ensure data accuracy, integrity, and security across all stages of the data lifecycle.\nSystem Design & Architecture: Contribute to the design and architecture of scalable data systems and solutions.\nRequired Skills and Qualification\nExperience: 3+ years of experience in data engineering or a related field.\nDatabricks & Spark: Strong expertise in Databricks and distributed data processing with Spark.\nProgramming: Proficiency in Python for data engineering tasks.\nSQL Optimization: Solid experience in writing and optimizing complex SQL queries.\nData Systems Knowledge: Hands-on experience with large-scale data systems and tools.\nDomain Knowledge: Familiarity with Capital Market/Private Equity is a plus (relaxation may apply).\nData Visualization: Experience with Tableau for creating insightful data visualizations and reports.\nPreferred skills\nCloud Platforms: Familiarity with cloud services like AWS, Azure, or GCP.\nData Warehousing & ETL:Experience with data warehousing concepts and ETL processes.\nAnalytical Skills: Strong problem-solving and analytical capabilities Analytics Tools: Hands-on experience with tools like Amplitude, PostHog, Google Analytics, or Mixpanel.\nAdditional Tools: Knowledge of Python for web scraping and frameworks like Django (good to have).",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'Microsoft Azure', 'Data Bricks', 'Spark', 'ETL', 'Python', 'SQL']",2025-06-13 05:13:41
"Data Engineer Openings at Advantum Health, Hyderabad",Advantum Health,3 - 5 years,Not Disclosed,['Hyderabad'],"Data Engineer openings at Advantum Health Pvt Ltd, Hyderabad.\nOverview:\nWe are looking for a Data Engineer to build and optimize robust data pipelines that support AI and RCM analytics. This role involves integrating structured and unstructured data from diverse healthcare systems into scalable, AI-ready datasets.\nKey Responsibilities:\nDesign, implement, and optimize data pipelines for ingesting and transforming healthcare and RCM data.\nBuild data marts and warehouses to support analytics and machine learning.\nEnsure data quality, lineage, and governance across AI use cases.\nIntegrate data from EMRs, billing platforms, claims databases, and third-party APIs.\nSupport data infrastructure in a HIPAA-compliant cloud environment.\nQualifications:\nBachelors in Computer Science, Data Engineering, or related field.\n3+ years of experience with ETL/ELT pipelines using tools like Apache Airflow, dbt, or Azure Data Factory.\nStrong SQL and Python skills.\nExperience with healthcare data standards (HL7, FHIR, X12) preferred.\nFamiliarity with data lake house architectures and AI integration best practices\nPh: 9177078628\nEmail id: jobs@advantumhealth.com\nAddress: Advantum Health Private Limited, Cyber gateway, Block C, 4th floor Hitech City, Hyderabad.\nDo follow us on LinkedIn, Facebook, Instagram, YouTube and Threads\nAdvantum Health LinkedIn Page:\nhttps://lnkd.in/gVcQAXK3\n\nAdvantum Health Facebook Page:\nhttps://lnkd.in/g7ARQ378\n\nAdvantum Health Instagram Page:\nhttps://lnkd.in/gtQnB_Gc\n\nAdvantum Health India YouTube link:\nhttps://lnkd.in/g_AxPaPp\n\nAdvantum Health Threads link:\nhttps://lnkd.in/gyq73iQ6",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Data Factory', 'SQL', 'Python', 'Airflow', 'ETL', 'Elt']",2025-06-13 05:13:43
Data Engineer,Cloud Angles Digital Transformation,3 - 5 years,Not Disclosed,['Noida'],"Essential Functions/Responsibilities/Duties\n•       Work closely with Senior Business Intelligence engineer and BI architect to understand the schema objects and build BI reports and Dashboards\n•       Participation in sprint refinement, planning, and kick-off to understand the Agile process and Sprint priorities\n•       Develop necessary transformations and aggregate tables required for the reporting\\Dashboard needs\n•       Understand the Schema layer in MicroStrategy and business requirements\n•       Develop complex reports and Dashboards in MicroStrategy\n•       Investigate and troubleshoot issues with Dashboard and reports\n•       Proactively researching new technologies and proposing improvements to processes and tech stack\n•       Create test cases and scenarios to validate the dashboards and maintain data accuracy\nEducation and Experience\n•       3 years of experience in Business Intelligence and Data warehousing\n•       3+ years of experience in MicroStrategy Reports and Dashboard development\n•       2 years of experience in SQL\n•       Bachelors or masters degree in IT or Computer Science or ECE.\n•       Nice to have – Any MicroStrategy certifications\nRequired Knowledge, Skills, and Abilities\n•       Good in writing complex SQL, including aggregate functions, subqueries and complex date calculations and able to teach these concepts to others.\n•       Detail oriented and able to examine data and code for quality and accuracy.\n•       Self-Starter – taking initiative when inefficiencies or opportunities are seen.\n•       Good understanding of modern relational and non-relational models and differences between them\n•       Good understanding of Datawarehouse concepts, snowflake & star schema architecture and SCD concepts\n•       Good understanding of MicroStrategy Schema objects\n•       Develop Public objects such as metrics, filters, prompts, derived objects, custom groups and consolidations in MicroStrategy\n•       Develop complex reports and dashboards using OLAP and MTDI cubes\n•       Create complex dashboards with data blending\n•       Understand VLDB settings and report optimization\n•       Understand security filters and connection mappings in MSTR\nWork Environment\nAt Personify Health, we value and celebrate diversity and are committed to creating an inclusive environment for all employees. We believe in creating teams made up of individuals with various backgrounds, experiences, and perspectives. Diversity inspires innovation and collaboration and challenges us to produce better solutions. But more than this, diversity is our strength and a catalyst in our ability to change lives for the good. \nPhysical Requirements\n•       Constantly operates a computer and other office productivity machinery, such as copy machine, computer printer, calculator, etc.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Microstrategy', 'SQL', 'Dashboards']",2025-06-13 05:13:44
Data Engineer,Reputed Client,5 - 10 years,18-25 Lacs P.A.,[],"Data Engineer\n(Python, PySpark, SQL and Spark SQL)\n\nExperience - 5-10 Years\nMandate Skills: Python, PySpark, SQL and SparkSQL\nWorking Hours: 11:00 am to 8 pm\n\n(Candidate has to be flexible. 4-hour overlap with US business hours)\n\nSalary : 1.50 LPM to 2 LPM + Tax (Rate is not fixed, negotiable depending upon the candidate feedback)\n\nRemote / Hybrid (3 Days in a week WFO) (Pune, Bangalore, Noida, Mumbai, Hyderabad)\n\nNOTE: Need candidates within these cities, they have to collect assets from the office / need to be available for meetings - if they are working remotely)\n\nIt's a 6 months (C2H role).",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Temporary/Contractual","['Python', 'PySpark', 'Spark', 'SQL']",2025-06-13 05:13:46
Data Engineer,Conversehr Business Solutions,4 - 7 years,15-30 Lacs P.A.,['Hyderabad'],"What are the ongoing responsibilities of Data Engineer responsible for?\nWe are building a growing Data and AI team. You will play a critical role in the efforts to centralize structured and unstructured data for the firm. We seek a candidate with skills in data modeling, data management and data governance, and can contribute first-hand towards firms data strategy. The ideal candidate is a self-starter with a strong technical foundation, a collaborative mindset, and the ability to navigate complex data challenges #ASSOCIATE\nWhat ideal qualifications, skills & experience would help someone to be successful?\nBachelors degree in computer science or computer applications; or equivalent experience in lieu of degree with 3 years of industry experience.\nStrong expertise in data modeling and data management concepts. Experience in implementing master data management is preferred.\nSound knowledge on Snowflake and data warehousing techniques.\nExperience in building, optimizing, and maintaining data pipelines and data management frameworks to support business needs.\nProficiency in at least one programming language, preferably python.\nCollaborate with cross-functional teams to translate business needs into scalable data and AI-driven solutions.\nTake ownership of projects from ideation to production, operating in a startup-like culture within an enterprise environment. Excellent communication, collaboration, and ownership mindset.\nFoundational Knowledge of API development and integration.\nKnowledge of Tableau, Alteryx is good-to-have.\nWork Shift Timings - 2:00 PM - 11:00 PM IST",Industry Type: Financial Services (Asset Management),Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Snowflake', 'Master Data Management', 'Python', 'Etl Pipelines', 'Alteryx', 'ai', 'Data Modeling', 'Tableau', 'ETL']",2025-06-13 05:13:48
Cloud Data Engineer,PwC India,3 - 8 years,Not Disclosed,"['Gurugram', 'Bengaluru']","Job Description:\n\nWe are seeking skilled and dynamic Cloud Data Engineers specializing in AWS, Azure, Databricks. The ideal candidate will have a strong background in data engineering, with a focus on data ingestion, transformation, and warehousing. They should also possess excellent knowledge of PySpark or Spark, and a proven ability to optimize performance in Spark job executions.\n\nKey Responsibilities:",,,,"['AWS OR Azure', 'Azure Data Engineer OR AWS Data Engineer', 'Azure', 'AWS']",2025-06-13 05:13:49
AWS Data Engineer,Infosys,5 - 9 years,Not Disclosed,"['Hyderabad', 'Pune', 'Bengaluru']","AWS Data Engineer\n\nTo Apply, use the below link:\nhttps://career.infosys.com/jobdesc?jobReferenceCode=INFSYS-EXTERNAL-210775&rc=0\n\nJOB Profile:\nSignificant 5 to 9 years of experience in designing and implementing scalable data engineering solutions on AWS.\nStrong proficiency in Python programming language.\nExpertise in serverless architecture and AWS services such as Lambda, Glue, Redshift, Kinesis, SNS, SQS, and CloudFormation.\nExperience with Infrastructure as Code (IaC) using AWS CDK for defining and provisioning AWS resources.\nProven leadership skills with the ability to mentor and guide junior team members.\nExcellent understanding of data modeling concepts and experience with tools like ERStudio.\nStrong communication and collaboration skills, with the ability to work effectively in a cross-functional team environment.\nExperience with Apache Airflow for orchestrating data pipelines is a plus.\nKnowledge of Data Lakehouse, dbt, or Apache Hudi data format is a plus.\n\n\nRoles and Responsibilities\nDesign, develop, test, deploy, and maintain large-scale data pipelines using AWS services such as S3, Glue, Lambda, Redshift.\nCollaborate with cross-functional teams to gather requirements and design solutions that meet business needs.\nDesired Candidate Profile\n5-9 years of experience in an IT industry setting with expertise in Python programming language (Pyspark).\nStrong understanding of AWS ecosystem including S3, Glue, Lambda, Redshift.\nBachelor's degree in Any Specialization (B.Tech/B.E.).",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Aws Glue', 'AWS Data Engineer', 'Pyspark', 'Aws Lambda', 'Redshift Aws', 'Python']",2025-06-13 05:13:51
Data Engineer II,Flipkart,1 - 3 years,Not Disclosed,['Bengaluru'],"Skills Required :\nKafka, Spark Streaming. Proficiency in one of the programming languages preferably Java, Scala or Python.\nEducation/Qualification :\nBachelor's Degree in Computer Science, Engineering, Technology or related field\nDesirable Skills :\nKafka, Spark Streaming. Proficiency in one of the programming languages preferably Java, Scala or Python.",Industry Type: Courier / Logistics,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'java', 'Scala', 'Kafka', 'Spark Streaming', 'Python']",2025-06-13 05:13:53
S&C Global Network - AI - CG&S - Data Engineer Consultant,Accenture,3 - 7 years,Not Disclosed,['Bengaluru'],"Job Title:Industry & Function AI Data Engineer + S&C GN\n\n\n\nManagement Level:09 - Consultant\n\n\n\nLocation:Primary - Bengaluru, Secondary - Gurugram\n\n\n\nMust-Have Skills:Data Engineering expertise, Cloud platforms:AWS, Azure, GCP, Proficiency in Python, SQL, PySpark and ETL frameworks\n\n\n\nGood-to-Have Skills:LLM Architecture, Containerization tools:Docker, Kubernetes, Real-time data processing tools:Kafka, Flink, Certifications like AWS Certified Data Analytics Specialty, Google Professional Data Engineer,Snowflake,DBT,etc.\n\n\n\nJob\n\n\nSummary:\n\nAs a Data Engineer, you will play a critical role in designing, implementing, and optimizing data infrastructure to power analytics, machine learning, and enterprise decision-making. Your work will ensure high-quality, reliable data is accessible for actionable insights. This involves leveraging technical expertise, collaborating with stakeholders, and staying updated with the latest tools and technologies to deliver scalable and efficient data solutions.\n\n\n\n\nRoles & Responsibilities:\nBuild and Maintain Data Infrastructure:Design, implement, and optimize scalable data pipelines and systems for seamless ingestion, transformation, and storage of data.\nCollaborate with Stakeholders:Work closely with business teams, data analysts, and data scientists to understand data requirements and deliver actionable solutions.\nLeverage Tools and Technologies:Utilize Python, SQL, PySpark, and ETL frameworks to manage large datasets efficiently.\nCloud Integration:Develop secure, scalable, and cost-efficient solutions using cloud platforms such as Azure, AWS, and GCP.\nEnsure Data Quality:Focus on data reliability, consistency, and quality using automation and monitoring techniques.\nDocument and Share Best Practices:Create detailed documentation, share best practices, and mentor team members to promote a strong data culture.\nContinuous Learning:Stay updated with the latest tools and technologies in data engineering through professional development opportunities.\n\n\n\n\n\nProfessional & Technical\n\n\n\n\nSkills:\n\nStrong proficiency in programming languages such as Python, SQL, and PySpark\nExperience with cloud platforms (AWS, Azure, GCP) and their data services\nFamiliarity with ETL frameworks and data pipeline design\nStrong knowledge of traditional statistical methods, basic machine learning techniques.\nKnowledge of containerization tools (Docker, Kubernetes)\nKnowing LLM, RAG & Agentic AI architecture\nCertification in Data Science or related fields (e.g., AWS Certified Data Analytics Specialty, Google Professional Data Engineer)\n\n\n\n\n\nAdditional Information:\n\nThe ideal candidate has a robust educational background in data engineering or a related field and a proven track record of building scalable, high-quality data solutions in the Consumer Goods sector.\n\nThis position offers opportunities to design and implement cutting-edge data systems that drive business transformation, collaborate with global teams to solve complex data challenges and deliver measurable business outcomes and enhance your expertise by working on innovative projects utilizing the latest technologies in cloud, data engineering, and AI.\n\n\n\nAbout Our Company | Accenture\n\nQualification\n\n\n\nExperience:Minimum 3-7 years in data engineering or related fields, with a focus on the Consumer Goods Industry\n\n\n\n\nEducational Qualification:Bachelors or Masters degree in Computer Science, Information Systems, Engineering, or a related field",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'pyspark', 'data engineering', 'sql', 'machine learning algorithms', 'kubernetes', 'snowflake', 'data analytics', 'microsoft azure', 'cloud platforms', 'machine learning', 'apache flink', 'artificial intelligence', 'docker', 'pipeline', 'data science', 'gcp', 'kafka', 'aws', 'etl', 'etl scripts']",2025-06-13 05:13:55
Data Engineer--Operations,Robert Bosch Engineering and Business Solutions Private Limited,2 - 6 years,Not Disclosed,['Bengaluru'],"As a Data engineer in Operations, you will work on the operational management, monitoring, and support of scalable data pipelines running in Azure Databricks, Hadoop and Radium. You will ensure the reliability, performance, and availability of data workflows and maintain production environments. You will collaborate closely with data engineers, architects, and platform teams to implement best practices in data pipeline operations and incident management to ensure data availability and data completeness.\nPrimary responsibilities:\nOperational support and incident management for Azure Databricks, Hadoop, Radium data pipelines.\nCollaborating with data engineering and platform teams to define and enforce operational standards, SLAs, and best practices.\nDesigning and implementing monitoring, alerting, and logging solutions for Azure Databricks pipelines.\nCoordinating with central teams to ensure compliance with organizational operational standards and security policies.\nDeveloping and maintaining runbooks, SOPs, and troubleshooting guides for pipeline issues.\nManaging the end-to-end lifecycle of data pipeline incidents, including root cause analysis and remediation.\nOverseeing pipeline deployments, rollbacks, and change management using CI/CD tools such as Azure DevOps.\nEnsuring data quality and validation checks are effectively monitored in production.\nWorking closely with platform and infrastructure teams to address pipeline and environment-related issues.\nProviding technical feedback and mentoring junior operations engineers.\nConducting peer reviews of operational scripts and automation code.\nAutomating manual operational tasks using Scala and Python scripts.\nManaging escalations and coordinating critical production issue resolution.\nParticipating in post-mortem reviews and continuous improvement initiatives for data pipeline operations.",Industry Type: Automobile,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Automation', 'Change management', 'SCALA', 'Incident management', 'Data quality', 'Troubleshooting', 'Operations', 'Monitoring', 'Python']",2025-06-13 05:13:57
Cloud Data Engineer,Wipro,8 - 12 years,Not Disclosed,"['Hyderabad', 'Pune', 'Bengaluru']",Role: Cloud Data Engineer\nLocation: Wipro PAN India\nHybrid 3 days in Wipro office\n\nJD:\nStrong - SQL\nStrong - Python,,,,"['SQL', 'Python', 'AZURE', 'GCP', 'AWS']",2025-06-13 05:13:58
Azure Data Engineer,Wipro,5 - 8 years,Not Disclosed,['Bengaluru'],"Role Purpose\nThe purpose of the role is to support process delivery by ensuring daily performance of the Production Specialists, resolve technical escalations and develop technical capability within the Production Specialists.\n\nDo\nOversee and support process by reviewing daily transactions on performance parameters\n\n\n\nMandatory Skills: Azure Data Factory. Experience: 5-8 Years.",,,,"['Azure', 'azure databricks', 'azure data lake', 'ssas', 'ssrs', 'microsoft azure', 'azure data factory', 'ssis', 'msbi', 'sql server', 'sql']",2025-06-13 05:14:00
Data Engineer III,Expedia Group,5 - 10 years,Not Disclosed,['Bengaluru'],"Why Join Us?\nTo shape the future of travel, people must come first. Guided by our Values and Leadership Agreements, we foster an open culture where everyone belongs, differences are celebrated and know that when one of us wins, we all win.\nWe provide a full benefits package, including exciting travel perks, generous time-off, parental leave, a flexible work model (with some pretty cool offices), and career development resources, all to fuel our employees passion for travel and ensure a rewarding career journey. We re building a more open world. Join us.\nData Engineer III\nIntroduction to the Team\nExpedia Technology teams partner with our Product teams to create innovative products, services, and tools to deliver high-quality experiences for travelers, partners, and our employees. A singular technology platform powered by data and machine learning provides secure, differentiated, and personalized experiences that drive loyalty and traveler satisfaction.\nExpedia Group is seeking a skilled and motivated Data Engineer III to join our Finance Business Intelligence team supporting the Product & Technology Finance organization. In this role, you will help drive data infrastructure and analytics solutions that support strategic financial planning, reporting, and operational decision-making across the Global Finance community. You ll work closely with Finance and Technology partners to ensure data accuracy, accessibility, and usability in support of Expedia s business objectives.\nAs a Data Engineer III, you have strong experience working with a variety of datasets, data environments, tools, and analytical techniques. You enjoy a fun, collaborative and stimulating team environment. Successful candidates should be able to own projects end-to-end, including identifying problems and solutions, building and maintain data pipelines and dashboards, distilling key insights and communicate to stakeholders.\nIn this role, you will:\nDevelop new and improve existing end to end Business Intelligence products (data pipelines, Tableau dashboards, and Machine Learning predictive forecasting models).\nDrive internal efficiencies through streamline code/documentation/Tableau development to maintain high data integrity.\nTroubleshoot and resolve production issues with the team products (automation opportunities, optimizations, back-end data issues, data reconciliations).\nProactively reach out to subject matter experts /stakeholders and collaborate to solve problems.\nRespond to ad hoc data requests and conduct analysis to provide valuable insights to stakeholders.\nCollaborate and coordinate with team members/stakeholders to translate complex data into meaningful insights, that improve the analytical capabilities of the business.\nApply knowledge of database design to support migration of data pipelines from on prem to cloud environment (including data extraction, ingestion, processing of large data sets)\nSupport dashboard development on cloud environment to enable self-service reporting.\nCommunicate clearly on current work status and design considerations\nThink broadly and comprehend the how, why, and what behind data architecture designs\nExperience & Qualifications:\nBachelor s in Computer Science, Mathematics, Statistics, Information Systems, or related field\n5+ years experience in a Data Analyst, Data Engineer or Business Analyst role\nProven expertise in SQL, with practical experience utilizing query engines including SQL Server, Starburst, Trino, Querybook and data science tools such as Python/R, SparkSQL.\nProficient visualization skills (Tableau, Looker, or similar) and excel modeling/report automation.\nExceptional understanding of relational and dimensional datasets, data warehouse and data mining and applies database design principles to solve data requirements\nExperience building robust data extract, load and transform (ELT) processes, that source data from multiple databases.\nDemonstrated record of defining and executing key analysis and solving problems with minimal supervision.\nDynamic individual contributor who consistently enhances operational playbooks to address business problems.\n3+ year working in a hybrid environment that uses both on-premise and cloud technologies is preferred.\nExperience working in an environment that manipulates large datasets on the cloud platform preferred.\nBackground in analytics, finance or a comparable reporting and analytics role preferred.\nAccommodation requests\nIf you need assistance with any part of the application or recruiting process due to a disability, or other physical or mental health conditions, please reach out to our Recruiting Accommodations Team through the Accommodation Request .\nWe are proud to be named as a Best Place to Work on Glassdoor in 2024 and be recognized for award-winning culture by organizations like Forbes, TIME, Disability:IN, and others.\nExpedia Groups family of brands includes: Brand Expedia , Hotels.com , Expedia Partner Solutions, Vrbo , trivago , Orbitz , Travelocity , Hotwire , Wotif , ebookers , CheapTickets , Expedia Group Media Solutions, Expedia Local Expert , CarRentals.com , and Expedia Cruises . 2024 Expedia, Inc. All rights reserved. Trademarks and logos are the property of their respective owners. . Never provide sensitive, personal information to someone unless you re confident who the recipient is. Expedia Group does not extend job offers via email or any other messaging tools to individuals with whom we have not made prior contact. Our email domain is @expediagroup.com. The official website to find and apply for job openings at Expedia Group is careers.expediagroup.com/jobs .\nExpedia is committed to creating an inclusive work environment with a diverse workforce. All qualified applicants will receive consideration for employment without regard to race, religion, gender, sexual orientation, national origin, disability or age.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Automation', 'Database design', 'Machine learning', 'Business intelligence', 'Data mining', 'Analytics', 'SQL', 'Python', 'Data architecture']",2025-06-13 05:14:02
Azure Data Engineer,Infosys,5 - 9 years,Not Disclosed,"['Hyderabad', 'Pune', 'Bengaluru']","Job description\nWe are looking for Azure Data Engineer's resources having minimum 5 to 9 years of Experience.\n\nRole & responsibilities\nBlend of technical expertise with 5 to 9 year of experience, analytical problem-solving, and collaboration with cross-functional teams. Design and implement Azure data engineering solutions (Ingestion & Curation)\nCreate and maintain Azure data solutions including Azure SQL Database, Azure Data Lake, and Azure Blob Storage.\nDesign, implement, and maintain data pipelines for data ingestion, processing, and transformation in Azure.\nUtilizing Azure Data Factory or comparable technologies, create and maintain ETL (Extract, Transform, Load) operations\nUse Azure Data Factory and Databricks to assemble large, complex data sets\nImplementing data validation and cleansing procedures will ensure the quality, integrity, and dependability of the data.\nEnsure data quality / security and compliance.\nOptimize Azure SQL databases for efficient query performance.\nCollaborate with data engineers, and other stakeholders to understand requirements and translate them into scalable and reliable data platform architectures.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Databricks', 'Azure Data Factory', 'Azure Synapse', 'Pyspark', 'Azure Data Lake']",2025-06-13 05:14:04
AWS Data Engineer,Infosys,5 - 9 years,Not Disclosed,"['Hyderabad', 'Pune', 'Bengaluru']","Significant 5 to 9 years of experience in designing and implementing scalable data engineering solutions on AWS.\nStrong proficiency in Python programming language.\nExpertise in serverless architecture and AWS services such as Lambda, Glue, Redshift, Kinesis, SNS, SQS, and CloudFormation.\nExperience with Infrastructure as Code (IaC) using AWS CDK for defining and provisioning AWS resources.\nProven leadership skills with the ability to mentor and guide junior team members.\nExcellent understanding of data modeling concepts and experience with tools like ERStudio.\nStrong communication and collaboration skills, with the ability to work effectively in a cross-functional team environment.\nExperience with Apache Airflow for orchestrating data pipelines is a plus.\nKnowledge of Data Lakehouse, dbt, or Apache Hudi data format is a plus.\nRoles and Responsibilities\nDesign, develop, test, deploy, and maintain large-scale data pipelines using AWS services such as S3, Glue, Lambda, Redshift.\nCollaborate with cross-functional teams to gather requirements and design solutions that meet business needs.\nDesired Candidate Profile\n5-9 years of experience in an IT industry setting with expertise in Python programming language (Pyspark).\nStrong understanding of AWS ecosystem including S3, Glue, Lambda, Redshift.\nBachelor's degree in Any Specialization (B.Tech/B.E.).",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Aws Glue', 'Pyspark', 'Aws Lambda', 'Amazon Redshift', 'Athena', 'Python']",2025-06-13 05:14:06
Azure Data Engineer,Hexaware Technologies,6 - 10 years,Not Disclosed,"['Chennai', 'Bengaluru']","Hi\n\nWork Location : Chennai AND Bangalore\nWork location : Imm - 30 days\n\nPrimary: Azure Databricks,ADF, Pyspark SQL",,,,"['Pyspark', 'Azure Databricks', 'SQL', 'Azure Data Factory']",2025-06-13 05:14:07
Cloud Data Engineer,PwC India,5 - 8 years,10-20 Lacs P.A.,['Bengaluru'],"Role & responsibilities\nStrong hands-on experience with multi cloud (AWS, Azure, GCP)  services such as GCP BigQuery, Dataform AWS Redshift, \nProficient in PySpark and SQL for building scalable data processing pipelines\nKnowledge of utilizing serverless technologies like AWS Lambda, and Google Cloud Functions \nExperience in orchestration frameworks like Apache Airflow, Kubernetes, and Jenkins to manage and orchestrate data pipelines",,,,"['Pyspark', 'Python', 'SQL', 'Datafactory', 'GCP', 'Microsoft Azure', 'AWS', 'Data Bricks']",2025-06-13 05:14:09
Lead Big Data Engineer - Python & Spark,Hubnex,7 - 12 years,Not Disclosed,['Bengaluru'],"Job Title: Lead Big Data Engineer - Python & Spark\nLocation: Gurgaon, India\nExperience: 7+ Years\nEmployment Type: Full-Time | Onsite\nDepartment: Data Engineering\nAbout Hubnex Labs:\nHubnex Labs is a forward-looking IT consulting and software services company, building next-generation data platforms, AI systems, and enterprise-grade applications. We re looking for a Lead Big Data Engineer to drive the development and deployment of high-performance data processing solutions.\nRole Overview:\nAs a Lead Big Data Engineer , you will be responsible for architecting and implementing scalable big data solutions using Spark, Python, Hive, and related technologies. You will mentor a team of developers and work closely with cross-functional stakeholders to ensure on-time, error-free software delivery.\nKey Responsibilities:\nLead and mentor a team of Python and big data developers to deliver robust data-driven applications\nDesign, develop, and maintain scalable data processing pipelines using Spark (Scala/PySpark), Hive, and Hadoop ecosystems\nWrite efficient, reusable, and well-documented code in Python, SQL, and shell scripting\nOptimize Spark applications for performance and scalability; tune existing Hadoop-based systems\nCollaborate with QA, DevOps, and business teams to ensure high-quality software delivery\nPerform code reviews , enforce coding standards, and contribute to overall architectural decisions\nActively participate in daily Scrum meetings , sprint planning, retrospectives, and release cycles\nTroubleshoot complex issues across the full data pipeline\nRequired Qualifications:\n7+ years of hands-on experience in software development, with a strong background in big data frameworks\nDeep expertise in Hadoop ecosystem including HDFS, Hive, HQL, Spark (Scala/PySpark), Sqoop\n4+ years of programming experience using Python , SQL , and Unix shell scripting\nProven experience in leading development teams and delivering enterprise-level solutions\nStrong grasp of Spark architecture , performance tuning, and data frame APIs\nExcellent understanding of database concepts , data structures , and distributed systems\nBachelors degree in Computer Science , Engineering , or a related field\nExceptional communication, problem-solving, and leadership skills\nPreferred Skills:\nExperience with CI/CD pipelines for data projects\nFamiliarity with cloud-based data platforms (AWS EMR, GCP DataProc, or Azure HDInsight)\nWorking knowledge of Kafka , Airflow , or other data orchestration tools\nWhy Join Hubnex Labs?\nWork on mission-critical big data solutions that impact businesses globally\nBe part of an innovative, agile, and supportive team\nLeadership opportunities and exposure to latest technologies in AI and cloud computing\nCompetitive salary, performance incentives, and professional growth support",Industry Type: Internet,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Performance tuning', 'Cloud computing', 'Coding', 'Agile', 'Data structures', 'Scrum', 'Unix shell scripting', 'SQL', 'Python']",2025-06-13 05:14:11
Data Engineer-Data Integration,IBM,2 - 5 years,Not Disclosed,['Pune'],"As Data Engineer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\n\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise seach applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modeling results\n\n\n Your primary responsibilities include: \nDevelop & maintain data pipelines for batch & stream processing using informatica power centre or cloud ETL/ELT tools.\nLiaise with business team and technical leads, gather requirements, identify data sources, identify data quality issues, design target data structures, develop pipelines and data processing routines, perform unit testing and support UAT.\nWork with data scientist and business analytics team to assist in data ingestion and data-related technical issues.\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nExpertise in Data warehousing/ information Management/ Data Integration/Business Intelligence using ETL tool Informatica PowerCenter\nKnowledge of Cloud, Power BI, Data migration on cloud skills.\nExperience in Unix shell scripting and python\nExperience with relational SQL, Big Data etc\n\n\nPreferred technical and professional experience\nKnowledge of MS-Azure Cloud\nExperience in Informatica PowerCenter\nExperience in Unix shell scripting and python",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['information management', 'data warehousing', 'business intelligence', 'etl', 'data integration', 'python', 'informatica powercenter', 'power bi', 'relational sql', 'data migration', 'azure cloud', 'sql', 'elastic search', 'unix shell scripting', 'splunk', 'agile', 'big data', 'informatica']",2025-06-13 05:14:13
Data Engineer-Data Integration,IBM,2 - 5 years,Not Disclosed,['Pune'],"As Data Engineer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nExpertise in Data warehousing/ information Management/ Data Integration/Business Intelligence using ETL tool Informatica PowerCenter\nKnowledge of Cloud, Power BI, Data migration on cloud skills.\nExperience in Unix shell scripting and python\nExperience with relational SQL, Big Data etc\n\n\nPreferred technical and professional experience\nKnowledge of MS-Azure Cloud\nExperience in Informatica PowerCenter\nExperience in Unix shell scripting and python",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['information management', 'data warehousing', 'business intelligence', 'etl', 'data integration', 'python', 'data management', 'informatica powercenter', 'power bi', 'relational sql', 'data migration', 'azure cloud', 'sql', 'unix shell scripting', 'java', 'etl tool', 'big data', 'informatica', 'unix']",2025-06-13 05:14:14
Senior Data Engineer,Grid Dynamics,8 - 13 years,15-25 Lacs P.A.,['Bengaluru'],"We are looking for an enthusiastic and technology-proficient Big Data Engineer, who is eager to participate in the design and implementation of a top-notch Big Data solution to be deployed at massive scale.\nOur customer is one of the world's largest technology companies based in Silicon Valley with operations all over the world. On this project we are working on the bleeding-edge of Big Data technology to develop high performance data analytics platform, which handles petabytes datasets.\nEssential functions\nParticipate in design and development of Big Data analytical applications.\nDesign, support and continuously enhance the project code base, continuous integration pipeline, etc.\nWrite complex ETL processes and frameworks for analytics and data management.\nImplement large-scale near real-time streaming data processing pipelines.\nWork inside the team of industry experts on the cutting edge Big Data technologies to develop solutions for deployment at massive scale.\nQualifications\nStrong coding experience with Scala, Spark,Hive, Hadoop.\nIn-depth knowledge of Hadoop and Spark, experience with data mining and stream processing technologies (Kafka, Spark Streaming, Akka Streams).\nUnderstanding of the best practices in data quality and quality engineering.\nExperience with version control systems, Git in particular.\nDesire and ability for quick learning of new tools and technologies.\nWould be a plus\nKnowledge of Unix-based operating systems (bash/ssh/ps/grep etc.).\nExperience with Github-based development processes.\nExperience with JVM build systems (SBT, Maven, Gradle).\nWe offer\nOpportunity to work on bleeding-edge projects\nWork with a highly motivated and dedicated team\nCompetitive salary\nFlexible schedule\nBenefits package - medical insurance, sports\nCorporate social events\nProfessional development opportunities\nWell-equipped office\nAbout us\nGrid Dynamics (NASDAQ: GDYN) is a leading provider of technology consulting, platform and product engineering, AI, and advanced analytics services. Fusing technical vision with business acumen, we solve the most pressing technical challenges and enable positive business outcomes for enterprise companies undergoing business transformation. A key differentiator for Grid Dynamics is our 8 years of experience and leadership in enterprise AI, supported by profound expertise and ongoing investment in data, analytics, cloud & DevOps, application modernization and customer experience. Founded in 2006, Grid Dynamics is headquartered in Silicon Valley with offices across the Americas, Europe, and India.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Hive', 'SCALA', 'Hadoop', 'Big Data', 'Spark']",2025-06-13 05:14:16
Data Engineer,Meritus Management Service,4 - 9 years,9-18 Lacs P.A.,"['Pune', 'Gurugram']","The first Data Engineer specializes in traditional ETL with SAS DI and Big Data (Hadoop, Hive). The second is more versatile, skilled in modern data engineering with Python, MongoDB, and real-time processing.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Etl Pipelines', 'Big Data', 'Informatica', 'SAS DI', 'SQL', 'Hive', 'Hadoop', 'Talend', 'ETL Tool', 'Python']",2025-06-13 05:14:18
Data Engineer - ETL/ Python,Meritus Management Service,5 - 7 years,10-14 Lacs P.A.,['Indore'],"Focus on Python, you'll play a crucial role in designing, developing, and maintaining data pipelines and ETL processes. Python to manage large datasets, automate data workflows, and ensure data accuracy and efficiency across our organization.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pandas', 'MySQL', 'Sqlalchemy', 'Numpy', 'Python', 'Azure Synapse', 'Postgresql', 'Etl Process', 'SQL']",2025-06-13 05:14:20
Big Data Engineer,Apexon,11 - 16 years,Not Disclosed,['Bengaluru'],"We enable #HumanFirstDigital\n\nJob Summary:\nWe are looking for a highly experienced and strategic Data Engineer to drive the design, development, and optimization of our enterprise data platform. This role requires deep technical expertise in AWS, StreamSets, and Snowflake, along with solid experience in Kubernetes, Apache Airflow, and unit testing. The ideal candidate will lead a team of data engineers and play a key role in delivering scalable, secure, and high-performance data solutions for both historical and incremental data loads.\nKey Responsibilities:\nLead the architecture, design, and implementation of end-to-end data pipelines using StreamSets and Snowflake.\nOversee the development of scalable ETL/ELT processes for historical data migration and incremental data ingestion.\nGuide the team in leveraging AWS services (S3, Lambda, Glue, IAM, etc.) to build cloud-native data solutions.\nProvide technical leadership in deploying and managing containerized applications using Kubernetes.\nDefine and implement workflow orchestration strategies using Apache Airflow.\nEstablish best practices for unit testing, code quality, and data validation.\nCollaborate with data architects, analysts, and business stakeholders to align data solutions with business goals.\nMentor junior engineers and foster a culture of continuous improvement and innovation.\nMonitor and optimize data workflows for performance, scalability, and cost-efficiency.\nRequired Skills & Qualifications:\nHigh proficiency in AWS, including hands-on experience with core services (S3, Lambda, Glue, IAM, CloudWatch).\nExpert-level experience with StreamSets, including Data Collector, Transformer, and Control Hub.\nStrong Snowflake expertise, including data modeling, SnowSQL, and performance tuning.\nMedium-level experience with Kubernetes, including container orchestration and deployment.\nWorking knowledge of Apache Airflow for workflow scheduling and monitoring.\nExperience with unit testing frameworks and practices in data engineering.\nProven experience in building and managing ETL pipelines for both batch and real-time data.\nStrong command of SQL and scripting languages such as Python or Shell.\nExperience with CI/CD pipelines and version control tools (e.g., Git, Jenkins).\nPreferred Qualifications:\nAWS certification (e.g., AWS Certified Data Analytics, Solutions Architect).\nExperience with data governance, security, and compliance frameworks.\nFamiliarity with Agile methodologies and tools like Jira and Confluence.\nPrior experience in a leadership or mentoring role within a data engineering team.\nOur Commitment to Diversity & Inclusion:\nOur Perks and Benefits:\nOur benefits and rewards program has been thoughtfully designed to recognize your skills and contributions, elevate your learning/upskilling experience and provide care and support for you and your loved ones. As an Apexon Associate, you get continuous skill-based development, opportunities for career advancement, and access to comprehensive health and well-being benefits and assistance.\nWe also offer:\no Group Health Insurance covering family of 4\no Term Insurance and Accident Insurance\no Paid Holidays & Earned Leaves\no Paid Parental LeaveoLearning & Career Development\no Employee Wellness\nJob Location : Bengaluru, India",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Performance tuning', 'Data modeling', 'Agile', 'Wellness', 'Workflow', 'Healthcare', 'Unit testing', 'Apache', 'SQL', 'Python']",2025-06-13 05:14:22
Big Data Engineer,Rarr Technologies,6 - 8 years,Not Disclosed,['Bengaluru'],Job description\n\nProven experience working with data pipelines ETL BI regardless of the technology\nProven experience working with AWS including at least 3 of RedShift S3 EMR Cloud Formation DynamoDB RDS lambda\nBig Data technologies and distributed systems one of Spark Presto or Hive\nPython language scripting and object oriented\nFluency in SQL for data warehousing RedShift in particular is a plus\nGood understanding on data warehousing and Data modelling concepts\nFamiliar with GIT Linux CICD pipelines is a plus\nStrong systems process orientation with demonstrated analytical thinking organization skills and problem solving skills\nAbility to self manage prioritize and execute tasks in a demanding environment\nStrong consultancy orientation and experience with the ability to form collaborative\nproductive working relationships across diverse teams and cultures is a must\nWillingness and ability to train and teach others\nAbility to facilitate meetings and follow up with resulting action items,Industry Type: IT Services & Consulting,Department: Other,"Employment Type: Full Time, Permanent","['python scripting', 'Big Data Technologies', 'ETL', 'AWS']",2025-06-13 05:14:24
Big Data Engineer,Client of Hiresquad Resources,5 - 8 years,22.5-30 Lacs P.A.,"['Noida', 'Hyderabad', 'Bengaluru']","Role: Data Engineer\nExp: 5 to 8 Years\nLocation: Bangalore, Noida, and Hyderabad (Hybrid, weekly 2 Days office must)\nNP: Immediate to 15 Days (Try to find only immediate joiners)\n\n\nNote:\nCandidate must have experience in Python, Kafka Streams, Pyspark, and Azure Databricks.\nNot looking for candidates who have only Exp in Pyspark and not in Python.\n\n\nJob Title: SSE Kafka, Python, and Azure Databricks (Healthcare Data Project)\nExperience:  5 to 8 years\n\nRole Overview:\nWe are looking for a highly skilled with expertise in Kafka, Python, and Azure Databricks (preferred) to drive our healthcare data engineering projects. The ideal candidate will have deep experience in real-time data streaming, cloud-based data platforms, and large-scale data processing. This role requires strong technical leadership, problem-solving abilities, and the ability to collaborate with cross-functional teams.\n\nKey Responsibilities:\nLead the design, development, and implementation of real-time data pipelines using Kafka, Python, and Azure Databricks.\nArchitect scalable data streaming and processing solutions to support healthcare data workflows.\nDevelop, optimize, and maintain ETL/ELT pipelines for structured and unstructured healthcare data.\nEnsure data integrity, security, and compliance with healthcare regulations (HIPAA, HITRUST, etc.).\nCollaborate with data engineers, analysts, and business stakeholders to understand requirements and translate them into technical solutions.\nTroubleshoot and optimize Kafka streaming applications, Python scripts, and Databricks workflows.\nMentor junior engineers, conduct code reviews, and ensure best practices in data engineering.\nStay updated with the latest cloud technologies, big data frameworks, and industry trends.\n\n\nRequired Skills & Qualifications:\n4+ years of experience in data engineering, with strong proficiency in Kafka and Python.\nExpertise in Kafka Streams, Kafka Connect, and Schema Registry for real-time data processing.\nExperience with Azure Databricks (or willingness to learn and adopt it quickly).\nHands-on experience with cloud platforms (Azure preferred, AWS or GCP is a plus).\nProficiency in SQL, NoSQL databases, and data modeling for big data processing.\nKnowledge of containerization (Docker, Kubernetes) and CI/CD pipelines for data applications.\nExperience working with healthcare data (EHR, claims, HL7, FHIR, etc.) is a plus.\nStrong analytical skills, problem-solving mindset, and ability to lead complex data projects.\nExcellent communication and stakeholder management skills.\n\n\n\nEmail: Sam@hiresquad.in",Industry Type: Medical Services / Hospital,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Kafka', 'Azure Databricks', 'Python', 'Etl Pipelines', 'Pyspark', 'spark architecture', 'Data Engineering', 'opps concepts', 'Data Streaming', 'Medallion Architecture', 'python scripts', 'schema registry', 'SQL Database', 'Nosql Databases', 'spark tuning', 'Kafka Streams', 'kafka connect']",2025-06-13 05:14:26
"Senior Data Engineer (Snowflake, DBT)",Allegis Global Solutions (AGS),5 - 10 years,Not Disclosed,[],"Senior Data Engineer (Snowflake, DBT, Azure)\nJob Location: Hyderabad / Bangalore / Chennai / Kolkata / Noida/ Gurgaon / Pune / Indore / Mumbai\n\nJob Details\nTechnical Expertise:\nStrong proficiency in Snowflake architecture, including data sharing, partitioning, clustering, and materialized views.\nAdvanced experience with DBT for data transformations and workflow management.\nExpertise in Azure services, including Azure Data Factory, Azure Data Lake, Azure Synapse, and Azure Functions.\nData Engineering:\nProficiency in SQL, Python, or other relevant programming languages.\nStrong understanding of data modeling concepts, including star schema and normalization.\nHands-on experience with ETL/ELT pipelines and data integration tools.\n\nPreferred Qualifications:\nCertifications in Snowflake, DBT, or Azure Data Engineering.\nFamiliar with data visualization tools like Power BI or Tableau.\nKnowledge of CI/CD pipelines and DevOps practices for data workflows.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Snowflake', 'Data Build Tool', 'Azure']",2025-06-13 05:14:28
Architect (Data Engineering),Amgen Inc,9 - 12 years,Not Disclosed,['Hyderabad'],"ABOUT THE ROLE\n\nRole Description:\n\nWe are seeking a Data Solutions Architect with deep expertise in Biotech/Pharma to design, implement, and optimize scalable and high-performance data solutions that support enterprise analytics, AI-driven insights, and digital transformation initiatives. This role will focus on data strategy, architecture, governance, security, and operational efficiency, ensuring seamless data integration across modern cloud platforms. The ideal candidate will work closely with engineering teams, business stakeholders, and leadership to establish a future-ready data ecosystem, balancing performance, cost-efficiency, security, and usability. This position requires expertise in modern cloud-based data architectures, data engineering best practices, and Scaled Agile methodologies.",,,,"['Data Engineering', 'continuous integration', 'technical leadership', 'metadata management', 'presentation skills', 'ci/cd', 'distributed computing', 'sql', 'data bricks', 'git', 'data modeling', 'spark', 'devops', 'data governance', 'jenkins', 'troubleshooting', 'access control', 'etl']",2025-06-13 05:14:30
Azure Data Engineer,HTC Global Services,4 - 8 years,Not Disclosed,['Bengaluru( Murugeshpalya )'],"Job Summary:\nWe are looking for a highly skilled Azure Data Engineer with experience in building and managing scalable data pipelines using Azure Data Factory, Synapse, and Databricks. The ideal candidate should be proficient in big data tools and Azure services, with strong programming knowledge and a solid understanding of data architecture and cloud platforms.\n\nKey Responsibilities:",,,,"['Power Bi', 'Azure Databricks', 'Azure Data Factory', 'Synapse', 'Python', 'Java', 'Scala', 'Kafka', 'big data tools', 'SQL', 'EventHub', 'Azure cloud services', 'Spark']",2025-06-13 05:14:32
Tech Lead-Data Engineering,Ameriprise Financial,7 - 10 years,Not Disclosed,['Hyderabad'],"Key Responsibilities\nDesign and develop high-volume, data engineering solutions for mission-critical systems with quality.\nMaking enhancements to various applications that meets business and auditing requirements.\nResearch and evaluate alternative solutions and make recommendations on improving the product to meet business and information risk requirements.\nEvaluate service level issues and suggested enhancements to diagnose and address underlying system problems and inefficiencies.\nParticipate in full development lifecycle activities for the product (coding, testing, release activities).\nSupport Release activities on weekends as required.\nSupport any application issues reported during weekends.\nCoordinating day-To-day activities for multiple projects with onshore and offshore team members. Ensuring the availability of platform in lower environments\n\nRequired Qualifications\n7+ years of overall IT experience, which includes hands on experience in Big Data technologies.\nMandatory - Hands on experience in Python and PySpark.\nBuild pySpark applications using Spark Dataframes in Python using Jupyter notebook and PyCharm(IDE).\nWorked on optimizing spark jobs that processes huge volumes of data.\nHands on experience in version control tools like Git.\nWorked on Amazon s Analytics services like Amazon EMR, Amazon Athena, AWS Glue.\nWorked on Amazon s Compute services like Amazon Lambda, Amazon EC2 and Amazon s Storage service like S3 and few other services like SNS.\nExperience/knowledge of bash/shell scripting will be a plus.\nHas built ETL processes to take data, copy it, structurally transform it etc. involving a wide variety of formats like CSV, TSV, XML and JSON.\nExperience in working with fixed width, delimited , multi record file formats etc.\nGood to have knowledge of datawarehousing concepts - dimensions, facts, schemas- snowflake, star etc.\nHave worked with columnar storage formats- Parquet,Avro,ORC etc. Well versed with compression techniques - Snappy, Gzip.\nGood to have knowledge of AWS databases (atleast one) Aurora, RDS, Redshift, ElastiCache, DynamoDB.\nHands on experience in tools like Jenkins to build, test and deploy the applications\nAwareness of Devops concepts and be able to work in an automated release pipeline environment.\nExcellent debugging skills.\n\nPreferred Qualifications\nExperience working with US Clients and Business partners.\nKnowledge on Front end frameworks.\nExposure to BFSI domain is a good to have.\nHands on experience on any API Gateway and management platform.\nAWMPO AWMPS Presidents Office\nTechnology",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Front end', 'Version control', 'Coding', 'XML', 'Shell scripting', 'Debugging', 'JSON', 'Asset management', 'Analytics', 'Python']",2025-06-13 05:14:34
Lead Data Engineer - Azure,Blend360 India,7 - 12 years,Not Disclosed,['Hyderabad'],"Job Description\nAs a Sr Data Engineer, your role is to spearhead the data engineering teams and elevate the team to the next level! You will be responsible for laying out the architecture of the new project as well as selecting the tech stack associated with it. You will plan out the development cycles deploying AGILE if possible and create the foundations for good data stewardship with our new data products!\nYou will also set up a solid code framework that needs to be built to purpose yet have enough flexibility to adapt to new business use cases tough but rewarding challenge!\n\nResponsibilities\nCollaborate with several stakeholders to deeply understand the needs of data practitioners to deliver at scale\nLead Data Engineers to define, build and maintain Data Platform\nWork on building Data Lake in Azure Fabric processing data from multiple sources\nMigrating existing data store from Azure Synapse to Azure Fabric\nImplement data governance and access control\nDrive development effort End-to-End for on-time delivery of high-quality solutions that conform to requirements, conform to the architectural vision, and comply with all applicable standards.\nPresent technical solutions, capabilities, considerations, and features in business terms. Effectively communicate status, issues, and risks in a precise and timely manner.\nFurther develop critical initiatives, such as Data Discovery, Data Lineage and Data Quality\nLeading team and Mentor junior resources\nHelp your team members grow in their role and achieve their career aspirations\nBuild data systems, pipelines, analytical tools and programs\nConduct complex data analysis and report on results\n\n\nQualifications\n7+ Years of Experience as a data engineer or similar role in Azure Synapses, ADF or\nrelevant exp in Azure Fabric\nDegree in Computer Science, Data Science, Mathematics, IT, or similar fiel",Industry Type: Advertising & Marketing,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Data analysis', 'Data modeling', 'Analytical', 'Agile', 'data governance', 'Data quality', 'Data mining', 'SQL', 'Python']",2025-06-13 05:14:36
Lead Data Engineer - Azure,Blend360 India,7 - 12 years,Not Disclosed,['Hyderabad'],"Job Description\nAs a Lead Data Engineer, your role is to spearhead the data engineering teams and elevate the team to the next level! You will be responsible for laying out the architecture of the new project as well as selecting the tech stack associated with it. You will plan out the development cycles deploying AGILE if possible and create the foundations for good data stewardship with our new data products!\nYou will also set up a solid code framework that needs to be built to purpose yet have enough flexibility to adapt to new business use cases tough but rewarding challenge!\n\nResponsibilities\nCollaborate with several stakeholders to deeply understand the needs of data practitioners to deliver at scale\nLead Data Engineers to define, build and maintain Data Platform\nWork on building Data Lake in Azure Fabric processing data from multiple sources\nMigrating existing data store from Azure Synapse to Azure Fabric\nImplement data governance and access control\nDrive development effort End-to-End for on-time delivery of high-quality solutions that conform to requirements, conform to the architectural vision, and comply with all applicable standards.\nPresent technical solutions, capabilities, considerations, and features in business terms. Effectively communicate status, issues, and risks in a precise and timely manner.\nFurther develop critical initiatives, such as Data Discovery, Data Lineage and Data Quality\nLeading team and Mentor junior resources\nHelp your team members grow in their role and achieve their career aspirations\nBuild data systems, pipelines, analytical tools and programs\nConduct complex data analysis and report on results\n\n\nQualifications\n7+ Years of Experience as a data engineer or similar role in Azure Synapses, ADF or\nrelevant exp in Azure Fabric\nDegree in Computer Science, Data Science, Mathematics, IT, or similar fiel",Industry Type: Advertising & Marketing,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Data analysis', 'Data modeling', 'Analytical', 'Agile', 'data governance', 'Data quality', 'Data mining', 'SQL', 'Python']",2025-06-13 05:14:38
Sr Data Engineer - Lead,Clifyx Technology,7 - 11 years,Not Disclosed,['Bengaluru'],"Developing Scala Spark pipelines that are resilient, modular and tested.\nHelp automate and scale governance through technology enablement\nEnable users finding the ""right data for the ""right use case\nParticipate in identifying and proposing solutions to data quality issues, and data management solutions\nSupport technical implementation of solutions through data pipeline development\nMaintain technical processes and procedures for data management\nVery good understanding of MS Azure Data Lake and associated setups\nETL knowledge to build semantic layers for reporting\nCreation / modification of pipelines based on source and target systems\nUser and access Management and Training end users",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Training', 'Usage', 'Data management', 'Access management', 'spark', 'Billing', 'SCALA', 'Manager Technology', 'Data quality', 'Testing']",2025-06-13 05:14:40
Senior Developer / Lead Data Engineer - Incorta,KPI Partners,4 - 9 years,Not Disclosed,"['Hyderabad', 'Pune', 'Bengaluru']","About Us:\nKPI Partners is a leading provider of data analytics and performance management solutions, dedicated to helping organizations harness the power of their data to drive business success. Our team of experts is at the forefront of the data revolution, delivering innovative solutions to our clients. We are currently seeking a talented and experienced Senior Developer / Lead Data Engineer with expertise in Incorta to join our dynamic team.\n\n",,,,"['python', 'oracle', 'data analytics', 'analytical', 'scala', 'pyspark', 'microsoft azure', 'cloud platforms', 'data pipeline', 'relational databases', 'data engineering', 'sql server', 'sql', 'analytics', 'java', 'data modeling', 'collaboration', 'data integration tools', 'mysql', 'etl', 'aws', 'programming', 'communication skills']",2025-06-13 05:14:42
Data Engineer-Data Platforms,IBM,5 - 10 years,Not Disclosed,['Navi Mumbai'],"As a Big Data Engineer, you will develop, maintain, evaluate, and test big data solutions. You will be involved in data engineering activities like creating pipelines/workflows for Source to Target and implementing solutions that tackle the clients needs.\n\nYour primary responsibilities include:\nDesign, build, optimize and support new and existing data models and ETL processes based on our clients business requirements.\nBuild, deploy and manage data infrastructure that can adequately handle the needs of a rapidly growing data driven organization.\nCoordinate data access and security to enable data scientists and analysts to easily access to data whenever they need too\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nMust have 5+ years exp in Big Data -Hadoop Spark -Scala ,Python\nHbase, Hive Good to have Aws -S3,\nathena ,Dynomo DB, Lambda, Jenkins GIT\nDeveloped Python and pyspark programs for data analysis.\nGood working experience with python to develop Custom Framework for generating of rules (just like rules engine).\nDeveloped Python code to gather the data from HBase and designs the solution to implement using Pyspark. Apache Spark DataFrames/RDD's were used to apply business transformations and utilized Hive Context objects to perform read/write operations\n\n\nPreferred technical and professional experience\nUnderstanding of Devops.\nExperience in building scalable end-to-end data ingestion and processing solutions\nExperience with object-oriented and/or functional programming languages, such as Python, Java and Scala",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['scala', 'hadoop spark', 'spark', 'big data', 'python', 'hive', 'cloudera', 'pyspark', 'sql', 'java', 'git', 'postgresql', 'devops', 'jenkins', 'data ingestion', 'mysql', 'hadoop', 'etl', 'hbase', 'data analysis', 'dynamo db', 'oozie', 'microsoft azure', 'impala', 'data engineering', 'lambda expressions', 'kafka', 'sqoop', 'aws']",2025-06-13 05:14:44
Data Engineer Specialist,Accenture,3 - 4 years,Not Disclosed,['Kochi'],"Job Title - + +\n\n\n\nManagement Level :\n\n\n\nLocation:Kochi, Coimbatore, Trivandrum\n\n\n\nMust have skills:Python, Pyspark\n\n\n\n\nGood to have skills:Redshift\n\n\n\nJob\n\n\nSummary: We are seeking a highly skilled and experienced Senior Data Engineer to join our growing Data and Analytics team. The ideal candidate will have deep expertise in Databricks and cloud data warehousing, with a proven track record of designing and building scalable data pipelines, optimizing data architectures, and enabling robust analytics capabilities. This role involves working collaboratively with cross-functional teams to ensure the organization leverages data as a strategic asset. Your responsibilities will include:\n\n\n\n\nRoles & Responsibilities\nDesign, build, and maintain scalable data pipelines and ETL processes using Databricks and other modern tools.\nArchitect, implement, and manage cloud-based data warehousing solutions on Databricks (Lakehouse Architecture)\nDevelop and maintain optimized data lake architectures to support advanced analytics and machine learning use cases.\nCollaborate with stakeholders to gather requirements, design solutions, and ensure high-quality data delivery.\nOptimize data pipelines for performance and cost efficiency.\nImplement and enforce best practices for data governance, access control, security, and compliance in the cloud.\nMonitor and troubleshoot data pipelines to ensure reliability and accuracy.\nLead and mentor junior engineers, fostering a culture of continuous learning and innovation.\nExcellent communication skills\nAbility to work independently and along with client based out of western Europe\n\n\n\n\nProfessional & Technical\n\n\n\n\nSkills:\nDesigning, developing, optimizing, and maintaining data pipelines that adhere to ETL principles and business goals\nSolving complex data problems to deliver insights that helps our business to achieve their goals.\nSource data (structured unstructured) from various touchpoints, format and organize them into an analyzable format.\nCreating data products for analytics team members to improve productivity\nCalling of AI services like vision, translation etc. to generate an outcome that can be used in further steps along the pipeline.\nFostering a culture of sharing, re-use, design and operational efficiency of data and analytical solutions\nPreparing data to create a unified database and build tracking solutions ensuring data quality\nCreate Production grade analytical assets deployed using the guiding principles of CI/CD.\n\n\n\nProfessional and Technical Skills\nExpert in Python, Scala, Pyspark, Pytorch, Javascript (any 2 at least)\nExtensive experience in data analysis (Big data- Apache Spark environments), data libraries (e.g. Pandas, SciPy, Tensorflow, Keras etc.), and SQL. 3-4 years of hands-on experience working on these technologies.\nExperience in one of the many BI tools such as Tableau, Power BI, Looker.\nGood working knowledge of key concepts in data analytics, such as dimensional modeling, ETL, reporting/dashboarding, data governance, dealing with structured and unstructured data, and corresponding infrastructure needs.\nWorked extensively in Microsoft Azure (ADF, Function Apps, ADLS, Azure SQL), AWS (Lambda,Glue,S3), Databricks analytical platforms/tools, Snowflake Cloud Datawarehouse.\n\n\n\n\nAdditional Information\nExperience working in cloud Data warehouses like Redshift or Synapse\nCertification in any one of the following or equivalent\nAWS- AWS certified data Analytics- Speciality\nAzure- Microsoft certified Azure Data Scientist Associate\nSnowflake- Snowpro core- Data Engineer\nDatabricks Data Engineering\n\nQualification\n\n\n\nExperience:5-8 years of experience is required\n\n\n\n\nEducational Qualification:Graduation (Accurate educational details should capture)",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['scala', 'pyspark', 'pytorch', 'python', 'data bricks', 'glue', 'amazon redshift', 'data warehousing', 'sql', 'tensorflow', 'sql azure', 'spark', 'keras', 'big data', 'etl', 'snowflake', 'scipy', 'data analysis', 'azure data lake', 'microsoft azure', 'power bi', 'javascript', 'pandas', 'tableau', 'lambda expressions', 'aws']",2025-06-13 05:14:46
Data Engineer-Data Platforms,IBM,2 - 5 years,Not Disclosed,['Mumbai'],"As a Data Engineer at IBM, you'll play a vital role in the development, design of application, provide regular support/guidance to project teams on complex coding, issue resolution and execution.\nYour primary responsibilities include\nLead the design and construction of new solutions using the latest technologies, always looking to add business value and meet user requirements.\nStrive for continuous improvements by testing the build solution and working under an agile framework.\nDiscover and implement the latest technologies trends to maximize and build creative solutions\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nExperience with Apache Spark (PySpark)In-depth knowledge of Spark’s architecture, core APIs, and PySpark for distributed data processing.\nBig Data TechnologiesFamiliarity with Hadoop, HDFS, Kafka, and other big data tools. Data Engineering\n\nSkills:\nStrong understanding of ETL pipelines, data modeling, and data warehousing concepts.\nStrong proficiency in PythonExpertise in Python programming with a focus on data processing and manipulation. Data Processing FrameworksKnowledge of data processing libraries such as Pandas, NumPy.\nSQL ProficiencyExperience writing optimized SQL queries for large-scale data analysis and transformation.\nCloud PlatformsExperience working with cloud platforms like AWS, Azure, or GCP, including using cloud storage systems\n\n\nPreferred technical and professional experience\nDefine, drive, and implement an architecture strategy and standards for end-to-end monitoring.\nPartner with the rest of the technology teams including application development, enterprise architecture, testing services, network engineering,\nGood to have detection and prevention tools for Company products and Platform and customer-facing",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'data processing', 'pyspark', 'data modeling', 'spark', 'data analysis', 'microsoft azure', 'data warehousing', 'cloud platforms', 'numpy', 'data engineering', 'sql', 'pandas', 'spring boot', 'etl pipelines', 'java', 'gcp', 'kafka', 'data warehousing concepts', 'hadoop', 'big data', 'aws', 'etl']",2025-06-13 05:14:48
Data Engineer-Data Platforms-Google,IBM,2 - 5 years,Not Disclosed,['Hyderabad'],"As an Associate Software Developer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\n\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\n\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise seach applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modeling results\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nDevelop/Convert the database (Hadoop to GCP) of the specific objects (tables, views, procedures, functions, triggers, etc.) from one database to another database platform Implementation of a specific Data Replication mechanism (CDC, file data transfer, bulk data transfer, etc.).\nExpose data as API Participation in modernization roadmap journey Analyze discovery and analysis outcomes Lead discovery and analysis workshops/playbacks Identification of the applications dependencies, source, and target database incompatibilities.\nAnalyze the non-functional requirements (security, HA, RTO/RPO, storage, compute, network, performance bench, etc.).\nPrepare the effort estimates, WBS, staffing plan, RACI, RAID etc. .\nLeads the team to adopt right tools for various migration and modernization method\n\n\nPreferred technical and professional experience\nYou thrive on teamwork and have excellent verbal and written communication skills.\nAbility to communicate with internal and external clients to understand and define business needs, providing analytical solutions\nAbility to communicate results to technical and non-technical audiences",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['elastic search', 'gcp', 'splunk', 'hadoop', 'big data', 'hive', 'python', 'data management', 'presentation skills', 'microsoft azure', 'machine learning', 'javascript', 'sql', 'docker', 'java', 'git', 'spark', 'linux', 'jenkins', 'html', 'mysql', 'aws']",2025-06-13 05:14:50
Data Engineer-Business Intelligence,IBM,3 - 8 years,Not Disclosed,['Pune'],"Provide expertise in analysis, requirements gathering, design, coordination, customization, testing and support of reports, in client’s environment\nDevelop and maintain a strong working relationship with business and technical members of the team\nRelentless focus on quality and continuous improvement\nPerform root cause analysis of reports issues\nDevelopment / evolutionary maintenance of the environment, performance, capability and availability.\nAssisting in defining technical requirements and developing solutions\nEffective content and source-code management, troubleshooting and debugging.\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nCognos Developer & Admin Required. EducationThe resource should be full time MCA/M. Tech/B. Tech/B.E. and should preferably have relevant certifications\nExperienceThe resource should have a minimum of 3 years of experience of working in the in BI DW projects in areas pertaining to reporting and visualization using cognos.\nThe resources shall have worked in at least two projects where they were involved in developing reporting/ visualization\nHe shall have good understanding of UNIX.\nShould be well conversant in English and should have excellent writing, MIS, communication, time management and multi-tasking skill\n\n\nPreferred technical and professional experience\nExperience with various cloud and integration platforms (e.g. AWS, Google, Azure)\nAgile mindset – ability to process changes of priorities and requests, ownership, critical thinking\nExperience with an ETL/Data Integration tool (eg. IBM InfoSphere DataStage, Azure Data Factory, Informatica PowerCenter)",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['informatica powercenter', 'microsoft azure', 'mis', 'bi dw', 'etl', 'datastage', 'bi', 'azure data factory', 'cognos', 'root cause analysis', 'business intelligence', 'sql', 'dw', 'ibm datastage', 'troubleshooting', 'debugging', 'agile', 'data visualization', 'aws', 'data integration', 'informatica', 'unix']",2025-06-13 05:14:52
Data Engineer-Data Platforms,IBM,2 - 5 years,Not Disclosed,['Mumbai'],"Experience with Scala object-oriented/object function Strong SQL background.\nExperience in Spark SQL, Hive, Data Engineer.\nSQL Experience with data pipelines & Data Lake Strong background in distributed comp.\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nSQL Experience with data pipelines & Data Lake Strong background in distributed comp\nExperience with Scala object-oriented/object function Strong SQL background\n\n\nPreferred technical and professional experience\nCore Scala Development Experience",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['hive', 'scala', 'sql', 'spark', 'data lake', 'amazon redshift', 'pyspark', 'data warehousing', 'emr', 'java', 'data modeling', 'mysql', 'hadoop', 'big data', 'etl', 'python', 'microsoft azure', 'machine learning', 'data engineering', 'sql server', 'nosql', 'amazon ec2', 'kafka', 'sqoop', 'aws']",2025-06-13 05:14:54
Data Engineer-Data Platforms,IBM,2 - 5 years,Not Disclosed,['Pune'],"As a Data Engineer at IBM, you'll play a vital role in the development, design of application, provide regular support/guidance to project teams on complex coding, issue resolution and execution.\nYour primary responsibilities include:\nLead the design and construction of new solutions using the latest technologies, always looking to add business value and meet user requirements.\nStrive for continuous improvements by testing the build solution and working under an agile framework.\nDiscover and implement the latest technologies trends to maximize and build creative solutions\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nExperience with Apache Spark (PySpark)In-depth knowledge of Spark’s architecture, core APIs, and PySpark for distributed data processing.\nBig Data TechnologiesFamiliarity with Hadoop, HDFS, Kafka, and other big data tools. Data Engineering\n\nSkills:\nStrong understanding of ETL pipelines, data modeling, and data warehousing concepts.\nStrong proficiency in PythonExpertise in Python programming with a focus on data processing and manipulation. Data Processing FrameworksKnowledge of data processing libraries such as Pandas, NumPy.\nSQL ProficiencyExperience writing optimized SQL queries for large-scale data analysis and transformation.\nCloud PlatformsExperience working with cloud platforms like AWS, Azure, or GCP, including using cloud storage systems\n\n\nPreferred technical and professional experience\nDefine, drive, and implement an architecture strategy and standards for end-to-end monitoring.\nPartner with the rest of the technology teams including application development, enterprise architecture, testing services, network engineering,\nGood to have detection and prevention tools for Company products and Platform and customer-facing",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'data processing', 'pyspark', 'data modeling', 'spark', 'data analysis', 'microsoft azure', 'data warehousing', 'cloud platforms', 'numpy', 'data engineering', 'sql', 'pandas', 'spring boot', 'etl pipelines', 'java', 'gcp', 'kafka', 'data warehousing concepts', 'hadoop', 'big data', 'aws', 'etl']",2025-06-13 05:14:56
Data Engineer-Data Platforms,IBM,2 - 5 years,Not Disclosed,['Navi Mumbai'],"As a Data Engineer at IBM, you'll play a vital role in the development, design of application, provide regular support/guidance to project teams on complex coding, issue resolution and execution.\n\nYour primary responsibilities include:\nLead the design and construction of new solutions using the latest technologies, always looking to add business value and meet user requirements.\nStrive for continuous improvements by testing the build solution and working under an agile framework.\nDiscover and implement the latest technologies trends to maximize and build creative solutions\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nExperience with Apache Spark (PySpark)In-depth knowledge of Spark’s architecture, core APIs, and PySpark for distributed data processing.\nBig Data TechnologiesFamiliarity with Hadoop, HDFS, Kafka, and other big data tools. Data Engineering\n\nSkills:\nStrong understanding of ETL pipelines, data modeling, and data warehousing concepts.\nStrong proficiency in PythonExpertise in Python programming with a focus on data processing and manipulation. Data Processing FrameworksKnowledge of data processing libraries such as Pandas, NumPy.\nSQL ProficiencyExperience writing optimized SQL queries for large-scale data analysis and transformation.\nCloud PlatformsExperience working with cloud platforms like AWS, Azure, or GCP, including using cloud storage systems\n\n\nPreferred technical and professional experience\nDefine, drive, and implement an architecture strategy and standards for end-to-end monitoring.\nPartner with the rest of the technology teams including application development, enterprise architecture, testing services, network engineering,\nGood to have detection and prevention tools for Company products and Platform and customer-facing",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'data processing', 'pyspark', 'data modeling', 'spark', 'data analysis', 'microsoft azure', 'data warehousing', 'cloud platforms', 'numpy', 'data engineering', 'sql', 'pandas', 'spring boot', 'etl pipelines', 'java', 'gcp', 'kafka', 'data warehousing concepts', 'hadoop', 'big data', 'aws', 'etl']",2025-06-13 05:14:58
Data Engineer-Data Platforms-AWS,IBM,4 - 9 years,Not Disclosed,['Kochi'],"As Data Engineer, you will develop, maintain, evaluate and test big data solutions. You will be involved in the development of data solutions using Spark Framework with Python or Scala on Hadoop and Azure Cloud Data Platform\n\nResponsibilities:\nExperienced in building data pipelines to Ingest, process, and transform data from files, streams and databases. Process the data with Spark, Python, PySpark and Hive, Hbase or other NoSQL databases on Azure Cloud Data Platform or HDFS\nExperienced in develop efficient software code for multiple use cases leveraging Spark Framework / using Python or Scala and Big Data technologies for various use cases built on the platform\nExperience in developing streaming pipelines\nExperience to work with Hadoop / Azure eco system components to implement scalable solutions to meet the ever-increasing data volumes, using big data/cloud technologies Apache Spark, Kafka, any Cloud computing etc\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nMinimum 4+ years of experience in Big Data technologies with extensive data engineering experience in Spark / Python or Scala;\nMinimum 3 years of experience on Cloud Data Platforms on Azure;\nExperience in DataBricks / Azure HDInsight / Azure Data Factory, Synapse, SQL Server DB\nGood to excellent SQL skills\nExposure to streaming solutions and message brokers like Kafka technologies\n\n\nPreferred technical and professional experience\nCertification in Azure and Data Bricks or Cloudera Spark Certified developers",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'big data technologies', 'data engineering', 'sql', 'spark', 'hive', 'cloudera', 'scala', 'pyspark', 'microsoft azure', 'azure data factory', 'azure hdinsight', 'azure cloud', 'sql server', 'nosql', 'data bricks', 'apache', 'kafka', 'hadoop', 'big data', 'aws', 'cloud computing', 'hbase', 'nosql databases']",2025-06-13 05:15:00
"Data Engineer, Alexa AI Developer Tech",Amazon,3 - 8 years,Not Disclosed,['Pune'],"Alexa+ is our next-generation assistant powered by generative AI. Alexa+ is more conversational, smarter, personalized, and gets things done.\n\nOur goal is make Alexa+ an instantly familiar personal assistant that is always ready to help or entertain on any device. At the core of this vision is Alexa AI Developer Tech, a close-knit team that s dedicated to providing software developers with the tools, primitives, and services they need to easily create engaging customer experiences that expand the wealth of information, products and services available on Alexa+.\n\nYou will join a growing organization working on top technology using Generative AI and have an enormous opportunity to make an impact on the design, architecture, and implementation of products used every day, by people you know.\n\nWe re working hard, having fun, and making history; come join us!\n\n\nWork with a team of product and program managers, engineering leaders, and business leaders to build data architectures and platforms to support business\nDesign, develop, and operate high-scalable, high-performance, low-cost, and accurate data pipelines in distributed data processing platforms\nRecognize and adopt best practices in data processing, reporting, and analysis: data integrity, test design, analysis, validation, and documentation\nKeep up to date with big data technologies, evaluate and make decisions around the use of new or existing software products to design the data architecture\nDesign, build and own all the components of a high-volume data warehouse end to end.\nProvide end-to-end data engineering support for project lifecycle execution (design, execution and risk assessment)\nContinually improve ongoing reporting and analysis processes, automating or simplifying self-service support for customers\nInterface with other technology teams to extract, transform, and load (ETL) data from a wide variety of data sources\nOwn the functional and nonfunctional scaling of software systems in your ownership area. *Implement big data solutions for distributed computing.\n\nAbout the team\nAlexa AI Developer Tech is an organization within Alexa on a mission to empower developers to create delightful and engaging experiences by making Alexa more natural, accurate, conversational, and personalized. 3+ years of data engineering experience\n4+ years of SQL experience\nExperience with data modeling, warehousing and building ETL pipelines Experience with AWS technologies like Redshift, S3, AWS Glue, EMR, Kinesis, FireHose, Lambda, and IAM roles and permissions\nExperience with non-relational databases / data stores (object storage, document or key-value stores, graph databases, column-family databases)",,,,"['Data modeling', 'Business design', 'Risk assessment', 'Test design', 'Architectural design', 'Data processing', 'data integrity', 'Design analysis', 'SQL', 'Data architecture']",2025-06-13 05:15:02
"Data Engineer II, TFAW/Sherlock",Amazon,3 - 8 years,Not Disclosed,['Hyderabad'],"As a Data Engineer you will be working on building and maintaining complex data pipelines, assemble large and complex datasets to generate business insights and to enable data driven decision making and support the rapidly growing and dynamic business demand for data.\nYou will have an opportunity to collaborate and work with various teams of Business analysts, Managers, Software Dev Engineers, and Data Engineers to determine how best to design, implement and support solutions. You will be challenged and provided with tremendous growth opportunity in a customer facing, fast paced, agile environment.\n\n\nDesign, implement and support an analytical data platform solutions for data driven decisions and insights\nDesign data schema and operate internal data warehouses SQL/NOSQL database systems\nWork on different data model designs, architecture, implementation, discussions and optimizations\nInterface with other teams to extract, transform, and load data from a wide variety of data sources using AWS big data technologies like EMR, RedShift, Elastic Search etc.\nWork on different AWS technologies such as S3, RedShift, Lambda, Glue, etc.. and Explore and learn the latest AWS technologies to provide new capabilities and increase efficiency\nWork on data lake platform and different components in the data lake such as Hadoop, Amazon S3 etc.\nWork on SQL technologies on Hadoop such as Spark, Hive, Impala etc..\nHelp continually improve ongoing analysis processes, optimizing or simplifying self-service support for customers\nMust possess strong verbal and written communication skills, be self-driven, and deliver high quality results in a fast-paced environment.\nRecognize and adopt best practices in reporting and analysis: data integrity, test design, analysis, validation, and documentation.\nEnjoy working closely with your peers in a group of talented engineers and gain knowledge.\nBe enthusiastic about building deep domain knowledge on various Amazon s business domains.\nOwn the development and maintenance of ongoing metrics, reports, analyses, dashboards, etc. to drive key business decisions. 3+ years of data engineering experience\n4+ years of SQL experience\nExperience with data modeling, warehousing and building ETL pipelines Experience with AWS technologies like Redshift, S3, AWS Glue, EMR, Kinesis, FireHose, Lambda, and IAM roles and permissions\nExperience with non-relational databases / data stores (object storage, document or key-value stores, graph databases, column-family databases)",,,,"['NoSQL', 'data engineer ii', 'Business Analyst', 'Analytical', 'Schema', 'Test design', 'Agile', 'data integrity', 'Design analysis', 'SQL']",2025-06-13 05:15:03
Azure Cloud Data Engineering Consultant,Optum,7 - 10 years,17-27.5 Lacs P.A.,['Gurugram'],"Primary Responsibilities:\nDesign and develop applications and services running on Azure, with a strong emphasis on Azure Databricks, ensuring optimal performance, scalability, and security.\nBuild and maintain data pipelines using Azure Databricks and other Azure data integration tools.\nWrite, read, and debug Spark, Scala, and Python code to process and analyze large datasets.\nWrite extensive query in SQL and Snowflake\nImplement security and access control measures and regularly audit Azure platform and infrastructure to ensure compliance.\nCreate, understand, and validate design and estimated effort for given module/task, and be able to justify it.\nPossess solid troubleshooting skills and perform troubleshooting of issues in different technologies and environments.\nImplement and adhere to best engineering practices like design, unit testing, functional testing automation, continuous integration, and delivery.\nMaintain code quality by writing clean, maintainable, and testable code.\nMonitor performance and optimize resources to ensure cost-effectiveness and high availability.\nDefine and document best practices and strategies regarding application deployment and infrastructure maintenance.\nProvide technical support and consultation for infrastructure questions.\nHelp develop, manage, and monitor continuous integration and delivery systems.\nTake accountability and ownership of features and teamwork.\nComply with the terms and conditions of the employment contract, company policies and procedures, and any directives.\nRequired Qualifications:\nB.Tech/MCA (Minimum 16 years of formal education)\nOverall 7+ years of experience.\nMinimum of 3 years of experience in Azure (ADF), Databricks and DevOps.\n5 years of experience in writing advanced level SQL.\n2-3 years of experience in writing, reading, and debugging Spark, Scala, and Python code.\n3 or more years of experience in architecting, designing, developing, and implementing cloud solutions on Azure.\nProficiency in programming languages and scripting tools.\nUnderstanding of cloud data storage and database technologies such as SQL and NoSQL.\nProven ability to collaborate with multidisciplinary teams of business analysts, developers, data scientists, and subject-matter experts.\nFamiliarity with DevOps practices and tools, such as continuous integration and continuous deployment (CI/CD) and Teraform.\nProven proactive approach to spotting problems, areas for improvement, and performance bottlenecks.\nProven excellent communication, writing, and presentation skills.\nExperience in interacting with international customers to gather requirements and convert them into solutions using relevant skills.\nPreferred Qualifications:\nKnowledge of AI/ML or LLM (GenAI).\nKnowledge of US Healthcare domain and experience with healthcare data.\nExperience and skills with Snowflake.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Data Factory', 'Azure Databricks', 'ETL', 'SQL', 'Python', 'Airflow', 'Pyspark', 'Snowflake', 'SCALA', 'Spark', 'Data Bricks']",2025-06-13 05:15:05
SNOWFLAKE DATA ENGINEER,Capgemini,6 - 11 years,Not Disclosed,['Chennai'],"Your Role \n\nWorks in the area of Software Engineering, which encompasses the development, maintenance and optimization of software solutions/applications.\n1. Applies scientific methods to analyse and solve software engineering problems.\n2. He/she is responsible for the development and application of software engineering practice and knowledge, in research, design, development and maintenance.\n3. His/her work requires the exercise of original thought and judgement and the ability to supervise the technical and administrative work of other software engineers.\n4. The software engineer builds skills and expertise of his/her software engineering discipline to reach standard software engineer skills expectations for the applicable role, as defined in Professional Communities.\n5. The software engineer collaborates and acts as team player with other software engineers and stakeholders.\nWorks in the area of Software Engineering, which encompasses the development, maintenance and optimization of software solutions/applications.1. Applies scientific methods to analyse and solve software engineering problems.2. He/she is responsible for the development and application of software engineering practice and knowledge, in research, design, development and maintenance.3. His/her work requires the exercise of original thought and judgement and the ability to supervise the technical and administrative work of other software engineers.4. The software engineer builds skills and expertise of his/her software engineering discipline to reach standard software engineer skills expectations for the applicable role, as defined in Professional Communities.5. The software engineer collaborates and acts as team player with other software engineers and stakeholders.\n\n Your Profile \n4+ years of experience in data architecture, data warehousing, and cloud data solutions.\nMinimum 3+ years of hands-on experience with End to end Snowflake implementation.\nExperience in developing data architecture and roadmap strategies with knowledge to establish data governance and quality frameworks within Snowflake\nExpertise or strong knowledge in Snowflake best practices, performance tuning, and query optimisation.\nExperience with cloud platforms like AWS or Azure and familiarity with Snowflakes integration with these environments.\nStrong knowledge in at least one cloud(AWS or Azure) is mandatory\nSolid understanding of SQL, Python, and scripting for data processing and analytics.\nExperience in leading teams and managing complex data migration projects.\nStrong communication skills, with the ability to explain technical concepts to non-technical stakeholders.\n\nKnowledge on new Snowflake features,AI capabilities and industry trends to drive innovation and continuous improvement.\n\n  \n\n Skills (competencies) \n\nVerbal Communication",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['snowflake', 'python', 'microsoft azure', 'sql', 'aws', 'head hunting', 'screening', 'performance tuning', 'data processing', 'data warehousing', 'hrsd', 'data architecture', 'data migration', 'sourcing', 'talent acquisition', 'it recruitment', 'technical recruitment', 'recruitment', 'data governance']",2025-06-13 05:15:07
Data Engineer-Business Intelligence,IBM,5 - 10 years,Not Disclosed,['Hyderabad'],"Provide expertise in analysis, requirements gathering, design, coordination, customization, testing and support of reports, in client’s environment\nDevelop and maintain a strong working relationship with business and technical members of the team\nRelentless focus on quality and continuous improvement\nPerform root cause analysis of reports issues\nDevelopment / evolutionary maintenance of the environment, performance, capability and availability.\nAssisting in defining technical requirements and developing solutions\nEffective content and source-code management, troubleshooting and debugging\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\n5+ years of experience with BI tools, with expertise and/or certification in at least one major BI platform – Tableau preferred.\nAdvanced knowledge of SQL, including the ability to write complex stored procedures, views, and functions.\nProven capability in data storytelling and visualization, delivering actionable insights through compelling presentations.\nExcellent communication skills, with the ability to convey complex analytical findings to non-technical stakeholders in a clear, concise, and meaningful way.\n5.Identifying and analyzing industry trends, geographic variations, competitor strategies, and emerging customer behavior\n\n\nPreferred technical and professional experience\nTroubleshooting capabilities to debug Data controls Capable of converting business requirements into workable model.\nGood communication skills, willingness to learn new technologies, Team Player, Self-Motivated, Positive Attitude.\nMust have thorough understanding of SQL & advance SQL (Joining & Relationships)",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['advance sql', 'sql', 'bi tools', 'debugging', 'troubleshooting', 'python', 'data analysis', 'data analytics', 'bi', 'data warehousing', 'power bi', 'business analysis', 'machine learning', 'business intelligence', 'sql server', 'qlikview', 'tableau', 'r', 'data visualization', 'etl', 'ssis']",2025-06-13 05:15:10
Cloud Data Engineer - GCP,Synechron,2 - 3 years,Not Disclosed,"['Hyderabad', 'Gachibowli']","Job Summary\nSynechron is seeking a highly motivated and skilled Senior Cloud Data Engineer GCP to join our cloud solutions team. In this role, you will collaborate closely with clients and internal stakeholders to design, implement, and manage scalable, secure, and high-performance cloud-based data solutions on Google Cloud Platform (GCP). You will leverage your technical expertise to ensure the integrity, security, and efficiency of cloud data architectures, enabling the organization to derive maximum value from cloud data assets. This role contributes directly to our mission of delivering innovative digital transformation solutions and supports the organizations strategic objectives of scalable and sustainable cloud infrastructure.\nSoftware Requirements\nOverall Responsibilities\nTechnical Skills (By Category)\nExperience Requirements\nDay-to-Day Activities\nQualifications\nProfessional Competencies",,,,"['GCP', 'Jenkins', 'Java', 'NoSQL', 'Bash scripts', 'Data Studio', 'Data Management', 'CI/CD', 'Apache Beam', 'MongoDB', 'Cloud Build']",2025-06-13 05:15:12
Data Engineer III,Expedia Group,6 - 11 years,Not Disclosed,['Gurugram'],"Why Join Us?\nTo shape the future of travel, people must come first. Guided by our Values and Leadership Agreements, we foster an open culture where everyone belongs, differences are celebrated and know that when one of us wins, we all win.\nWe provide a full benefits package, including exciting travel perks, generous time-off, parental leave, a flexible work model (with some pretty cool offices), and career development resources, all to fuel our employees passion for travel and ensure a rewarding career journey. We re building a more open world. Join us.\nData Engineer III\nExpedia Group s CTO Enablement team is looking for a highly motivated Data Engineer III to lead the design, delivery, and stewardship of business-critical data infrastructure that powers our Capitalization program and Business Operations functions . This role is at the intersection of finance, strategy, and engineering , where data precision and operational rigor directly support the company s financial integrity and execution effectiveness.\nYou will collaborate with stakeholders across Finance, BizOps, and Technology to build scalable data solutions that ensure capitalization accuracy, enable deep operational analytics, and streamline financial and business reporting at scale.\nWhat you will do:\nDesign, build, and maintain high-scale data pipelines and transformation logic to support CapEx/OpEx classification, capitalization tracking, and operational data modeling.\nDeliver clean, well-documented, governed datasets that drive finance reporting, strategic planning, and key operational dashboards.\nPartner with cross-functional teams (Finance, Engineering, Strategy) to translate business and compliance requirements into technical solutions.\nLead the development of data models and ETL processes to support performance monitoring, workforce utilization, project financials, and business KPIs.\nEstablish and enforce data quality, lineage, and access control standards to ensure trust in business-critical data.\nProactively identify and resolve data reliability issues related to financial close processes, budget tracking, and capitalization rules.\nServe as a technical advisor to BizOps and Finance stakeholders, recommending improvements in tooling, architecture, and process automation.\nMentor other engineers and contribute to the growth of a high-performance data team culture.\nWho you are:\n6+ years of experience in data engineering , analytics engineering , or data infrastructure roles with a focus on operational and financial data.\nExpertise in SQL and Python , and experience with data pipeline orchestration tools such as Airflow , dbt , or equivalent.\nStrong understanding of cloud-based data platforms (e.g., Snowflake, BigQuery, Redshift, or Databricks).\nDeep familiarity with capitalization standards , CapEx/OpEx distinction, and operational reporting in a tech-driven environment.\nDemonstrated ability to build scalable, reliable ETL/ELT workflows that serve diverse analytical and reporting needs.\nExperience working cross-functionally in complex organizations with multiple stakeholder groups.\nPassion for operational excellence, data governance, and driving actionable business insights from data.\nPreferred qualifications:\n- Experience supporting BizOps , FP&A , or Product Finance teams with data tooling and reporting.\n- Familiarity with BI platforms like Looker , Power BI , or Tableau .\n- Exposure to agile delivery frameworks and enterprise-level operational rhythms.\nAccommodation requests\nIf you need assistance with any part of the application or recruiting process due to a disability, or other physical or mental health conditions, please reach out to our Recruiting Accommodations Team through the Accommodation Request .\nWe are proud to be named as a Best Place to Work on Glassdoor in 2024 and be recognized for award-winning culture by organizations like Forbes, TIME, Disability:IN, and others.\nExpedia Groups family of brands includes: Brand Expedia , Hotels.com , Expedia Partner Solutions, Vrbo , trivago , Orbitz , Travelocity , Hotwire , Wotif , ebookers , CheapTickets , Expedia Group Media Solutions, Expedia Local Expert , CarRentals.com , and Expedia Cruises . 2024 Expedia, Inc. All rights reserved. Trademarks and logos are the property of their respective owners. . Never provide sensitive, personal information to someone unless you re confident who the recipient is. Expedia Group does not extend job offers via email or any other messaging tools to individuals with whom we have not made prior contact. Our email domain is @expediagroup.com. The official website to find and apply for job openings at Expedia Group is careers.expediagroup.com/jobs .\nExpedia is committed to creating an inclusive work environment with a diverse workforce. All qualified applicants will receive consideration for employment without regard to race, religion, gender, sexual orientation, national origin, disability or age.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Operational excellence', 'Data modeling', 'Talent acquisition', 'Analytical', 'Strategic planning', 'Data quality', 'Operations', 'Analytics', 'SQL', 'Business operations']",2025-06-13 05:15:13
Collibra Data Governance Engineer,Allegis Group,6 - 11 years,Not Disclosed,[],"Collibra Data Governance Engineer\nJob Location : Hyderabad / Bangalore / Chennai / Noida/ Gurgaon / Pune / Indore / Mumbai/ Kolkata\nRequired Skills\n5+ years of experience in data governance and/or metadata management.\nHands-on experience with Collibra Data Governance Center (Collibra DGC), including workflow configuration, cataloging, and operating model customization.\nStrong knowledge of metadata management, data lineage, and data quality principles.\nHands-on experience with Snowflake\nFamiliarity with data integration tools and AWS cloud platform\nExperience with SQL and working knowledge of relational databases.\nUnderstanding of data privacy regulations (e.g., GDPR, CCPA) and compliance frameworks.\nPreferred Skills\nCertifications such as Collibra Certified Solution Architect.\nExperience integrating Collibra with tools like Snowflake, Tableau or other BI/analytics platforms.\nExposure to DataOps, MDM (Master Data Management), and data governance frameworks like DAMA-DMBOK.\nStrong communication and stakeholder management skills.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Collibra', 'Metadata', 'Data Governance']",2025-06-13 05:15:15
Azure Senior Data Engineers,IT Services & Consulting,5 - 9 years,14-24 Lacs P.A.,['Bengaluru'],"Job Title: Senior Data Engineer Azure\nLocation: Bengaluru\nExperience: 6+ years (3+ years on Azure data services preferred)\nDepartment: Data Engineering / IT\nJob Summary:\nWe are seeking a highly skilled Senior Data Engineer with expertise in Microsoft Azure to design, develop, and optimize data pipelines, data lakes, and warehouse solutions. The ideal candidate will play a key role in building scalable and secure data platforms to support business intelligence, analytics, and machine learning use cases.\nKey Responsibilities:\nDesign, build, and maintain scalable data pipelines using Azure Data Factory, Databricks, Synapse Analytics, and related tools.\nDevelop and optimize ETL/ELT processes for structured and unstructured data.\nImplement data lake and data warehouse solutions following best practices and security standards.\nCollaborate with data scientists, analysts, and business stakeholders to understand data requirements.\nEnsure data quality, lineage, and governance using tools like Purview, Azure Monitor, and Data Catalog.\nMonitor and troubleshoot performance issues across data flows and batch processing pipelines.\nSupport real-time data integration and streaming solutions using Azure Event Hubs, Stream Analytics, or Kafka.\nMaintain and enhance CI/CD pipelines for data solutions using Azure DevOps or GitHub Actions.\nLead and mentor junior engineers in best practices for Azure data engineering.\nRequired Skills & Qualifications:\nBachelor’s or Master’s degree in Computer Science, Engineering, or related field.\n6+ years of experience in data engineering roles, with 3+ years working with Azure data services.\nProficiency in SQL, Python or Scala.\nExperience with tools like Azure Data Factory, Azure Synapse Analytics, Azure Databricks, Azure SQL Database, and Azure Blob Storage.\nStrong understanding of data modeling, data warehousing, and data lake architecture.\nFamiliarity with DevOps practices, infrastructure-as-code (IaC) tools (ARM, Bicep, Terraform), and CI/CD pipelines.\nKnowledge of data governance and data security best practices on cloud platforms.\nExcellent communication and documentation skills.\nPreferred Qualifications:\nAzure certification (e.g., Azure Data Engineer Associate, Azure Solutions Architect, etc.)\nExperience with big data frameworks (e.g., Spark, Hadoop).\nKnowledge of machine learning pipelines or MLOps in Azure.\nExperience with Power BI or integration with other visualization tools.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Azure Senior Data Engineers', 'Azure Data Factory', 'azure']",2025-06-13 05:15:17
Senior Data Engineer -Bangalore,Happiest Minds Technologies,6 - 10 years,Not Disclosed,['Bengaluru'],"Job Overview:\nThe primary purpose of this role is to translate business requirements and functional specifications into logical program designs and to deliver dashboards, schema, data pipelines, and software solutions. This includes developing, configuring, or modifying data components within various complex business and/or enterprise application solutions in various computing environments. You will partner closely with multiple Business partners, Product Owners, Data Strategy, Data Platform, Data Science and Machine Learning (MLOps) teams to drive innovative data products for end users. Additionally, you will help shape overall solution & data products, develop scalable solutions through best-in-class engineering practices.",,,,"['NoSQL', 'big data systems', 'Data Pipeline', 'MongoDB', 'SQL', 'Hive', 'GIT', 'Hadoop', 'Kafka', 'Agile', 'MQL', 'Ci/Cd']",2025-06-13 05:15:19
Senior Data Engineer,PulseData labs Pvt Ltd,7 - 10 years,Not Disclosed,['Bengaluru'],"Company name: PulseData labs Pvt Ltd (captive Unit for URUS, USA)\n\nAbout URUS\nWe are the URUS family (US), a global leader in products and services for Agritech.\n\nSENIOR DATA ENGINEER\nThis role is responsible for the design, development, and maintenance of data integration and reporting solutions. The ideal candidate will possess expertise in Databricks and strong skills in SQL Server, SSIS and SSRS, and experience with other modern data engineering tools such as Azure Data Factory. This position requires a proactive and results-oriented individual with a passion for data and a strong understanding of data warehousing principles.\n\nResponsibilities\nData Integration\nDesign, develop, and maintain robust and efficient ETL pipelines and processes on Databricks.\nTroubleshoot and resolve Databricks pipeline errors and performance issues.\nMaintain legacy SSIS packages for ETL processes.\nTroubleshoot and resolve SSIS package errors and performance issues.\nOptimize data flow performance and minimize data latency.\nImplement data quality checks and validations within ETL processes.\nDatabricks Development\nDevelop and maintain Databricks pipelines and datasets using Python, Spark and SQL.\nMigrate legacy SSIS packages to Databricks pipelines.\nOptimize Databricks jobs for performance and cost-effectiveness.\nIntegrate Databricks with other data sources and systems.\nParticipate in the design and implementation of data lake architectures.\nData Warehousing\nParticipate in the design and implementation of data warehousing solutions.\nSupport data quality initiatives and implement data cleansing procedures.\nReporting and Analytics\nCollaborate with business users to understand data requirements for department driven reporting needs.\nMaintain existing library of complex SSRS reports, dashboards, and visualizations.\nTroubleshoot and resolve SSRS report issues, including performance bottlenecks and data inconsistencies.\nCollaboration and Communication\nComfortable in entrepreneurial, self-starting, and fast-paced environment, working both independently and with our highly skilled teams.\nCollaborate effectively with business users, data analysts, and other IT teams.\nCommunicate technical information clearly and concisely, both verbally and in writing.\nDocument all development work and procedures thoroughly.\nContinuous Growth\nKeep abreast of the latest advancements in data integration, reporting, and data engineering technologies.\nContinuously improve skills and knowledge through training and self-learning.\nThis job description reflects managements assignment of essential functions; it does not prescribe or restrict the tasks that may be assigned.\n\nRequirements\nBachelor's degree in computer science, Information Systems, or a related field.\n7+ years of experience in data integration and reporting.\nExtensive experience with Databricks, including Python, Spark, and Delta Lake.\nStrong proficiency in SQL Server, including T-SQL, stored procedures, and functions.\nExperience with SSIS (SQL Server Integration Services) development and maintenance.\nExperience with SSRS (SQL Server Reporting Services) report design and development.\nExperience with data warehousing concepts and best practices.\nExperience with Microsoft Azure cloud platform and Microsoft Fabric desirable.\nStrong analytical and problem-solving skills.\nExcellent communication and interpersonal skills.\nAbility to work independently and as part of a team.\nExperience with Agile methodologies.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Etl Development', 'Azure Databricks', 'Spark', 'SQL Server', 'Databricks Engineer', 'Data Warehousing', 'Pythonspark']",2025-06-13 05:15:21
"Senior Data Engineer - Airflow, PLSQL",Relanto Global,5 - 10 years,Not Disclosed,['Bengaluru'],"PositionSenior Data Engineer - Airflow, PLSQL \n\n Experience5+ Years \n\n LocationBangalore/Hyderabad/Pune \n\n\n\nSeeking a Senior Data Engineer with strong expertise in Apache Airflow and Oracle PL/SQL, along with working experience in Snowflake and Agile methodologies. The ideal candidate will also take up Scrum Master responsibilities and lead a data engineering scrum team to deliver robust, scalable data solutions.\n\n\n Key Responsibilities: \nDesign, develop, and maintain scalable data pipelines using Apache Airflow.\nWrite and optimize complex PL/SQL queries, procedures, and packages on Oracle databases.\nCollaborate with cross-functional teams to design efficient data models and integration workflows.\nWork with Snowflake for data warehousing and analytics use cases.\nOwn the delivery of sprint goals, backlog grooming, and facilitation of agile ceremonies as the Scrum Master.\nMonitor pipeline health and troubleshoot production data issues proactively.\nEnsure code quality, documentation, and best practices across the team.\nMentor junior data engineers and promote a culture of continuous improvement.\n\n\n Required Skills and Qualifications: \n5+ years of experience as a Data Engineer in enterprise environments.\nStrong expertise in  Apache Airflow  for orchestrating workflows.\nExpert in  Oracle PL/SQL  - stored procedures, performance tuning, debugging.\nHands-on experience with  Snowflake  - data modeling, SQL, optimization.\nWorking knowledge of version control (Git) and CI/CD practices.\nPrior experience or certification as a  Scrum Master  is highly desirable.\nStrong analytical and problem-solving skills with attention to detail.\nExcellent communication and leadership skills.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sql', 'plsql', 'stored procedures', 'oracle pl', 'performance tuning', 'hive', 'continuous integration', 'ci/cd', 'data warehousing', 'pyspark', 'git', 'apache', 'data modeling', 'spark', 'debugging', 'hadoop', 'big data', 'snowflake', 'python', 'oracle', 'sql queries', 'airflow', 'data engineering', 'agile', 'sqoop']",2025-06-13 05:15:23
Sr Data Engineer,Lowes Services India Private limited,5 - 10 years,Not Disclosed,['Bengaluru'],"We are seeking a seasoned Senior Data Engineer to join our Marketing Data Platform team. This role is pivotal in designing, building, and optimizing scalable data pipelines and infrastructure that support our marketing analytics and customer engagement strategies. The ideal candidate will have extensive experience with big data technologies, cloud platforms, and a strong understanding of marketing data dynamics.\n\nData Pipeline Development & Optimization\nDesign, develop, and maintain robust ETL/ELT pipelines using Apache PySpark on GCP services like Dataproc and Cloud Composer.\nEnsure data pipelines are scalable, efficient, and reliable to handle large volumes of marketing data.\nData Warehousing & Modeling\nImplement and manage data warehousing solutions using BigQuery, ensuring optimal performance and cost-efficiency.\nDevelop and maintain data models that support marketing analytics and reporting needs.\nCollaboration & Stakeholder Engagement\nWork closely with marketing analysts, data scientists, and cross-functional teams to understand data requirements and deliver solutions that drive business insights.\nTranslate complex business requirements into technical specifications and data architecture.\nData Quality & Governance\nImplement data quality checks and monitoring to ensure the accuracy and integrity of marketing data.\nAdhere to data governance policies and ensure compliance with data privacy regulations.\nContinuous Improvement & Innovation\nStay abreast of emerging technologies and industry trends in data engineering and marketing analytics.\nPropose and implement improvements to existing data processes and infrastructure\n  Years of Experience\n5 Years in Data Engineer space\n  Education Qualification & Certifications\nB.Tech or MCA\n  Experience\nProven experience with Apache PySpark, GCP (including Dataproc, BigQuery, Cloud Composer), and data pipeline orchestration.\nTechnical Skills\nProficiency in SQL and Python.\nExperience with data modeling, ETL/ELT processes, and data warehousing concepts.",Industry Type: Retail,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['orchestration', 'Data modeling', 'data governance', 'Data quality', 'Apache', 'Continuous improvement', 'Monitoring', 'SQL', 'Python', 'Data architecture']",2025-06-13 05:15:24
Senior Data Engineer,Talentien Global Solutions,4 - 8 years,12-18 Lacs P.A.,"['Hyderabad', 'Chennai', 'Coimbatore']","We are seeking a skilled and motivated Data Engineer to join our dynamic team. The ideal candidate will have experience in designing, developing, and maintaining scalable data pipelines and architectures using Hadoop, PySpark, ETL processes, and Cloud technologies.\n\nResponsibilities:\nDesign, develop, and maintain data pipelines for processing large-scale datasets.\nBuild efficient ETL workflows to transform and integrate data from multiple sources.\nDevelop and optimize Hadoop and PySpark applications for data processing.\nEnsure data quality, governance, and security standards are met across systems.\nImplement and manage Cloud-based data solutions (AWS, Azure, or GCP).\nCollaborate with data scientists and analysts to support business intelligence initiatives.\nTroubleshoot performance issues and optimize query executions in big data environments.\nStay updated with industry trends and advancements in big data and cloud technologies.\nRequired Skills:\nStrong programming skills in Python, Scala, or Java.\nHands-on experience with Hadoop ecosystem (HDFS, Hive, Spark, etc.).\nExpertise in PySpark for distributed data processing.\nProficiency in ETL tools and workflows (SSIS, Apache Nifi, or custom pipelines).\nExperience with Cloud platforms (AWS, Azure, GCP) and their data-related services.\nKnowledge of SQL and NoSQL databases.\nFamiliarity with data warehousing concepts and data modeling techniques.\nStrong analytical and problem-solving skills.\n\nInterested can reach us at +91 7305206696/ saranyadevib@talentien.com",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'Data Engineering', 'Hadoop', 'Spark', 'ETL', 'Airflow', 'Etl Pipelines', 'Big Data', 'EMR', 'Gcp Cloud', 'Data Bricks', 'Azure Cloud', 'Data Pipeline', 'SCALA', 'Snowflake', 'Data Lake', 'Data Warehousing', 'Data Modeling', 'AWS', 'Python']",2025-06-13 05:15:26
Senior Data Engineer,SPAN.IO,5 - 10 years,Not Disclosed,['Bengaluru( Indira Nagar )'],"Senior Data Engineer\n\nOur Mission\n\nSPAN is enabling electrification for all\nWe are a mission-driven company designing, building, and deploying products that electrify the built environment, reduce carbon emissions, and slow the effects of climate change.\nDecarbonization is the process to reduce or remove greenhouse gas emissions, especially carbon dioxide, from entering our atmosphere.\nElectrification is the process of replacing fossil fuel appliances that run on gas or oil with all-electric upgrades for a cleaner way to power our lives.\n\nAt SPAN, we believe in:\nEnabling homes and vehicles powered by clean energy\nMaking electrification upgrades possible\nBuilding more resilient homes with reliable backup\nDesigning a flexible and distributed electrical grid\n\nThe Role\nAs a Data Engineer you would be working to design, build, test and create infrastructure necessary for real time analytics and batch analytics pipelines. You will work with multiple teams within the org to provide analysis, insights on the data. You will also be involved in writing ETL processes that support data ingestion. You will also guide and enforce best practices for data management, governance and security. You will build infrastructure to monitor these data pipelines / ETL jobs / tasks and create tooling/infrastructure for providing visibility into these.\n\nResponsibilities\nWe are looking for a Data Engineer with passion for building data pipelines, working with product, data science and business intelligence teams and delivering great solutions. As a part of the team you:-\nAcquire deep business understanding on how SPAN data flows from IoT device to cloud through the system and build scalable and optimized data solutions that impact many stakeholders.\nBe an advocate for data quality and excellence of our platform.\nBuild tools that help streamline the management and operation of our data ecosystem.\nEnsure best practices and standards in our data ecosystem are shared across teams.\nWork with teams within the company to build close relationships with our partners to understand the value our platform can bring and how we can make it better.\nImprove data discovery by creating data exploration processes and promoting adoption of data sources across the company.\nHave a desire to write tools and applications to automate work rather than do everything by hand.\nAssist internal teams in building out data logging, alerting and monitoring for their applications\nAre passionate about CI/CD process.\nDesign, develop and establish KPIs to monitor analysis and provide strategic insights to drive growth and performance.\n\nAbout You\n\nRequired Qualifications\nBachelor's Degree in a quantitative discipline: computer science, statistics, operations research, informatics, engineering, applied mathematics, economics, etc.\n5+ years of relevant work experience in data engineering, business intelligence, research or related fields.\nExpert level production-grade, programming experience in at least one of these languages (Python, Kotlin, or other JVM based languages)\nExperience in writing clean, concise and well structured code in one of the above languages.\nExperience working with Infrastructure-as-code tools: Pulumi, Terraform, etc.\nExperience working with CI/CD systems: Circle-CI, Github Actions, Argo-CD, etc.\nExperience managing data engineering infrastructure through Docker and Kubernetes\nExperience working with latency data processing solutions like Flink, Prefect, AWS Kinesis, Kafka, Spark Stream processing etc.\nExperience with SQL/Relational databases, OLAP databases like Snowflake.\nExperience working in AWS: S3, Glue, Athena, MSK, EMR, ECR etc.\n\nBonus Qualifications\nExperience with the Energy industry\nExperience with building IoT and/or hardware products\nUnderstanding of electrical systems and residential loads\nExperience with data visualization using Tableau.\nExperience in Data loading tools like FiveTran as well as data debugging tools such as DataDog\n\nLife at SPAN\nOur Bengaluru team plays a pivotal role in SPANs continued growth and expansion. Together, were driving engineering, product development, and operational excellence to shape the future of home energy solutions.\nAs part of our team in India, youll have the opportunity to collaborate closely with our teams in the US and across the globe. This international collaboration fosters innovation, learning, and growth, while helping us achieve our bold mission of electrifying homes and advancing clean energy solutions worldwide.\nOur in-office culture offers the chance for dynamic interactions and hands-on teamwork, making SPAN a truly collaborative environment where every team members contribution matters.\nOur climate-focused culture is driven by a team of forward-thinkers, engineers, and problem-solvers who push boundaries every day.\nDo mission-driven work: Every role at SPAN directly advances clean energy adoption.\nBring powerful ideas to life: We encourage diverse ideas and perspectives to drive stronger products.\nNurture an innovation-first mindset: We encourage big thinking and bold action.\nDeliver exceptional customer value: We value hard work, and the ability to deliver exceptional customer value.\n\nBenefits at SPAN India\nGenerous paid leave\nComprehensive Insurance & Health Benefits\nCentrally located office in Bengaluru with easy access to public transit, dining, and city amenities\n\nInterested in joining our team? Apply today and well be in touch with the next steps!",Industry Type: Electronics Manufacturing,"Department: Production, Manufacturing & Engineering","Employment Type: Full Time, Permanent","['Terraform', 'Snowflake', 'AWS', 'Python', 'SQL', 'Java', 'Apache Flink', 'Kotlin']",2025-06-13 05:15:28
Senior Data Engineer,Epsilon,5 - 9 years,Not Disclosed,['Bengaluru'],"This position in the Engineering team under the Digital Experience organization. We drive the first mile of the customer experience through personalization of offers and content. We are currently on the lookout for a smart, highly driven engineer.\nYou will be part of a team that is focused on building & managing solutions, pipelines using marketing technology stacks. You will also be expected to Identify and implement improvements including for optimizing data delivery and automate processes/pipelines.\nThe incumbent is also expected to partner with various stakeholders, bring scientific rigor to design and develop high quality solutions.\nCandidate must have excellent verbal and written communication skills and be comfortable working in an entrepreneurial, startup environment within a larger company.\nClick here to view how Epsilon transforms marketing with 1 View, 1 Vision and 1 Voice.\n\nBrief Description of Role:\nExperience with both structured and unstructured data\nExperience working on AdTech or MarTech technologies.\nExperience in relational and non-relational databases and SQL (NoSQL is a plus).\nUnderstanding of Data Modeling, Data Catalog concepts and tools\nAbility to deal with data imperfections such as missing values, outliers, inconsistent formatting, etc.\nCollaborate with other members of the team to ensure high quality deliverables\nLearning and implementing the latest design patterns in data engineering\n\nData Management\nExperience with both structured and unstructured data\nExperience building Data and CI/CD pipelines\nExperience working on AdTech or MarTech technologies is added advantage\nExperience in relational and non-relational databases and SQL (NoSQL is a plus).\nHands on experience building ETL workflows/pipelines on large volumes of data\nGood understanding of Data Modeling, Data Warehouse, Data Catalog concepts and tools\nAble to identify, join, explore, and examine data from multiple disparate sources and formats\nAbility to reduce large quantities of unstructured or formless data and get it into a form in which it can be analyzed\nAbility to deal with data imperfections such as missing values, outliers, inconsistent formatting, etc.\nDevelopment\nAbility to write code in programming languages such as Python and shell script on Linux\nFamiliarity with development methodology such as Agile/Scrum\nLove to learn new technologies, keep abreast of the latest technologies within the cloud architecture, and drive your organization to adapt to emerging best practices\nGood knowledge of working in UNIX/LINUX systems\nQualifications\nBachelors degree in computer science with 5+ years of similar experience\nTech Stack: Python, SQL, Scripting language (preferably JavaScript)\nExperience or knowledge on Adobe Experience Platform (RT-CDP/AEP)\nExperience working in Cloud Platforms (GCP or AWS)\nFamiliarity with automated unit/integration test frameworks\nGood written and spoken communication skills, team player.\nStrong analytic thought process and ability to interpret findings",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'Data Engineering', 'Data Bricks', 'Python', 'SQL', 'Azure Aws', 'AWS']",2025-06-13 05:15:30
Senior Data Engineer,Suzva Software Technologies,6 - 8 years,Not Disclosed,"['Mumbai', 'Delhi / NCR', 'Bengaluru']","JobOpening Senior Data Engineer (Remote, Contract 6 Months)\nRemote | Contract Duration: 6 Months | Experience: 6-8 Years\n\nWe are hiring a Senior Data Engineer for a 6-month remote contract position. The ideal candidate is highly skilled in building scalable data pipelines and working within the Azure cloud ecosystem, especially Databricks, ADF, and PySpark. You'll work closely with cross-functional teams to deliver enterprise-level data engineering solutions.\n\n#KeyResponsibilities\nBuild scalable ETL pipelines and implement robust data solutions in Azure.\n\nManage and orchestrate workflows using ADF, Databricks, ADLS Gen2, and Key Vaults.\n\nDesign and maintain secure and efficient data lake architecture.\n\nWork with stakeholders to gather data requirements and translate them into technical specs.\n\nImplement CI/CD pipelines for seamless data deployment using Azure DevOps.\n\nMonitor data quality, performance bottlenecks, and scalability issues.\n\nWrite clean, organized, reusable PySpark code in an Agile environment.\n\nDocument pipelines, architectures, and best practices for reuse.\n\n#MustHaveSkills\nExperience: 6+ years in Data Engineering\n\nTech Stack: SQL, Python, PySpark, Spark, Azure Databricks, ADF, ADLS Gen2, Azure DevOps, Key Vaults\n\nCore Expertise: Data Warehousing, ETL, Data Pipelines, Data Modelling, Data Governance\n\nAgile, SDLC, Containerization (Docker), Clean coding practices\n\n#GoodToHaveSkills\nEvent Hubs, Logic Apps\n\nPower BI\n\nStrong logic building and competitive programming background\n\nMode: Remote\n\nDuration: 6 Months\nLocations : Mumbai, Delhi / NCR, Bengaluru , Kolkata, Chennai, Hyderabad, Ahmedabad, Pune, Remote",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Databricks', 'Data Modelling', 'Data Pipelines', 'ADF', 'PySpark', 'Data Warehousing', 'ETL', 'Data Governance']",2025-06-13 05:15:32
"Senior Data Engineer Databricks, ADF, PySpark",Suzva Software Technologies,6 - 11 years,Not Disclosed,"['Mumbai', 'Delhi / NCR', 'Bengaluru']","Senior Data Engineer (Remote, Contract 6 Months) Databricks, ADF, and PySpark.\nWe are hiring a Senior Data Engineer for a 6-month remote contract position. The ideal candidate is highly skilled in building scalable data pipelines and working within the Azure cloud ecosystem, especially Databricks, ADF, and PySpark. You'll work closely with cross-functional teams to deliver enterprise-level data engineering solutions.\n\nKeyResponsibilities\nBuild scalable ETL pipelines and implement robust data solutions in Azure.\n\nManage and orchestrate workflows using ADF, Databricks, ADLS Gen2, and Key Vaults.\n\nDesign and maintain secure and efficient data lake architecture.\n\nWork with stakeholders to gather data requirements and translate them into technical specs.\n\nImplement CI/CD pipelines for seamless data deployment using Azure DevOps.\n\nMonitor data quality, performance bottlenecks, and scalability issues.\n\nWrite clean, organized, reusable PySpark code in an Agile environment.\n\nDocument pipelines, architectures, and best practices for reuse.\n\nMustHaveSkills\nExperience: 6+ years in Data Engineering\n\nTech Stack: SQL, Python, PySpark, Spark, Azure Databricks, ADF, ADLS Gen2, Azure DevOps, Key Vaults\n\nCore Expertise: Data Warehousing, ETL, Data Pipelines, Data Modelling, Data Governance\n\nAgile, SDLC, Containerization (Docker), Clean coding practices\n\nGoodToHaveSkills\nEvent Hubs, Logic Apps\n\nPower BI\n\nStrong logic building and competitive programming background\n\nLocation : - Remote,Mumbai, Delhi / NCR, Bengaluru , Kolkata, Chennai, Hyderabad, Ahmedabad, Pune",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Databricks', 'ADF', 'PySpark', 'ADLS Gen2', 'Azure Databricks', 'Key Vaults', 'Spark', 'Azure DevOps', 'SQL', 'Python']",2025-06-13 05:15:34
Senior Data Engineer,Eurofins Digital Testing,7 - 10 years,Not Disclosed,['Bengaluru'],"Job Description\nSenior Data Engineer - Quality Engineering\nExperience Range: 7-10 Years\nLocation: Bangalore (Hybrid Mode)\nResillion, a leading quality engineering company with offices around the world, is seeking a talented Data Engineer to join our growing India team. In this role, you will play a critical part in building and maintaining the data infrastructure that supports our AI-powered testing tools and analytics solutions. You will have technical responsibility for the entire data lifecycle, from data acquisition and ingestion to storage, processing, and analysis.\nResponsibilities:\nCollaborate with GenAI engineers, Software Engineers, Test automation engineers and other stakeholders within Resillion to understand data needs and translate them into technical solutions, including designing data pipelines for training & deploying models and data pre-processing for AI, including generative AI applications.\nDesign, implement, and advise on data migration testing strategies and data quality assurance strategies for Resillion customers, ensuring a smooth transition of data to new customer systems.\nDesign, develop, and implement scalable data pipelines using cloud-based data engineering tools and technologies, with a focus on both Microsoft Azure solutions (e.g., Azure Data Factory, Azure Databricks) and Google Cloud Platform (GCP) solutions (e.g., Google Cloud Dataflow, Google Cloud Dataproc).\nWrite efficient and maintainable code to extract, transform, and load data from various sources, leveraging your expertise in Azure Data Lake Storage and other relevant Azure services, as well as Google Cloud Storage and other relevant GCP services.\nBuild and manage data warehouses and data lakes for quality engineering data, utilizing your knowledge of Azure Synapse Analytics or similar technologies, and Google BigQuery or similar technologies.\nDevelop and implement data quality checks and monitoring procedures to ensure data integrity using Azure Data Catalog or other appropriate tools, as well as Google Cloud Data Catalog or other appropriate tools.\nAutomate data engineering tasks and workflows using Azure automation tools and GCP automation tools (e.g., Cloud Functions, Cloud Composer).\nSet up quality intelligence dashboards for quality assurance data using Microsoft Power BI to provide stakeholders with clear and actionable insights.\nStay up-to-date with the latest data engineering tools and technologies, including advancements in AI, Machine Learning (MLOps), and generative AI for data processing in both the Azure and GCP environments.\nAdvise on and implement test data generation strategies and solutions for various testing needs.\n\n\nQualifications\nMinimum 7+ years of experience in data engineering or a related field\nProven experience in designing, developing, and deploying data pipelines",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Engineering services', 'Manager Quality Assurance', 'GCP', 'Quality engineering', 'Machine learning', 'Quality Engineer', 'Monitoring', 'Analytics', 'SQL', 'Python']",2025-06-13 05:15:35
Senior Data Engineer,Wavicle Data Solutions,6 - 11 years,15-25 Lacs P.A.,"['Chennai', 'Coimbatore', 'Bengaluru']","Hi Professionals,\n\nWe are looking for Senior Data Engineer for Permanent Role\n\nWork Location: Hybrid Chennai, Coimbatore or Bangalore\n\nExperience: 6 to 12 Years\n\nNotice Period: 0 TO 15 Days or Immediate Joiner.\n\nSkills:\n1. Python\n2. Pyspark\n3. SQL\n4. AWS\n5. GCP\n6. MLOps\n\nInterested can send your resume to gowtham.veerasamy@wavicledata.com.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'GCP', 'AWS', 'Ml']",2025-06-13 05:15:37
Senior Software Engineer - Data Engineering,Zendesk,2 - 6 years,Not Disclosed,['Bengaluru'],"Design, build, and maintain data quality systems and pipelines.\nWork with tools such as Snowflake, Docker/Kubernetes, and Kafka to enable scalable, observable data movement.\nCollaborate cross-functionally to close skill gaps in DQ and data platform tooling.\nContribute to building internal tooling that supports schema validation, data experimentation, and automated checks.\nCollaborate cross-functionally with data producers, analytics engineers, platform teams, and business stakeholders.\nOwn the reliability, scalability, and performance of ingestion systems deployed on AWS\nArchitect and build core components of our real-time ingestion platform using Kafka, Snowpipe Streaming.\nChampion software engineering excellence including testing, observability, CI/CD, and automation\nDrive the development of platform tools that ensure data quality, observability, and lineage through Protobuf-based schema management..\nParticipate in the implementation of ingestion best practices and reusable frameworks across data and software engineering teams.\nCore Skills:\nSolid programming experience (preferably in Java )\nExperience with distributed data systems ( Kafka, Snowflake )\nFamiliarity with Data Quality tooling and concepts\nGood working knowledge of SQL (especially for diagnostics and DQ workflows)\nExperience with containerization (Docker, Kubernetes )\nStrong debugging, observability, and pipeline reliability practices\n\nWhat You Bring:\nA systems mindset with strong software engineering fundamentals.\nPassion for building resilient, high-throughput, real-time platforms.\nAbility to influence technical direction across teams and drive alignment.\nStrong communication and mentoring skills.\nA bias toward automation, continuous improvement, and platform thinking.\n\nNice to Haves:\nExperience with GenAI tools or supporting ML/AI data workflows\nFamiliarity with cloud-native data platforms (e.g., AWS, GCP)\nExposure to dbt or ELT frameworks",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Automation', 'GCP', 'Quality systems', 'Debugging', 'Data quality', 'Customer service', 'Continuous improvement', 'Monitoring', 'Analytics', 'SQL']",2025-06-13 05:15:39
Senior GCP Data Engineer,Swits Digital,6 - 9 years,Not Disclosed,['Bengaluru'],"Job Title: Senior GCP Data Engineer\nLocation: Chennai, Bangalore, Hyderabad\nExperience: 6-9 Years\nJob Summary:\nWe are seeking a GCP Data & Cloud Engineer with strong expertise in Google Cloud Platform services, including BigQuery, Cloud Run, Cloud Storage , and Pub/Sub . The ideal candidate will have deep experience in SQL coding , data pipeline development, and deploying cloud-native solutions.\nKey Responsibilities:\nDesign, implement, and optimize scalable data pipelines and services using GCP\nBuild and manage cloud-native applications deployed via Cloud Run\nDevelop complex and performance-optimized SQL queries for analytics and data transformation\nManage and automate data storage, retrieval, and archival using Cloud Storage\nImplement event-driven architectures using Google Pub/Sub\nWork with large datasets in BigQuery , including ETL/ELT design and query optimization\nEnsure security, monitoring, and compliance of cloud-based systems\nCollaborate with data analysts, engineers, and product teams to deliver end-to-end cloud solutions\nRequired Skills & Experience:\n3+ years of experience working with Google Cloud Platform (GCP)\nStrong proficiency in SQL coding , query tuning, and handling complex data transformations\nHands-on experience with:\nBigQuery\nCloud Run\nCloud Storage\nPub/Sub\nUnderstanding of data pipeline and ETL/ELT workflows in cloud environments\nFamiliarity with containerized services and CI/CD pipelines\nExperience in scripting languages (e.g., Python, Shell) is a plus\nStrong analytical and problem-solving skills",Industry Type: Recruitment / Staffing,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['SUB', 'query optimization', 'GCP', 'Analytical', 'Cloud', 'query', 'cloud storage', 'Security monitoring', 'SQL coding', 'Python']",2025-06-13 05:15:40
"Lead Engineer, Data Engineering (J2EE/Angular/React/React Full Stack)",S&P Global Market Intelligence,10 - 15 years,Not Disclosed,"['Mumbai', 'Maharastra']","About the Role:\nGrade Level (for internal use): 11\nThe Team\nYou will be an expert contributor and part of the Rating Organizations Data Services Product Engineering Team. This team, who has a broad and expert knowledge on Ratings organizations critical data domains, technology stacks and architectural patterns, fosters knowledge sharing and collaboration that results in a unified strategy. All Data Services team members provide leadership, innovation, timely delivery, and the ability to articulate business value. Be a part of a unique opportunity to build and evolve S&P Ratings next gen analytics platform.\nResponsibilities:\nArchitect, design, and implement innovative software solutions to enhance S&P Ratings' cloud-based analytics platform.\nMentor a team of engineers (as required), fostering a culture of trust, continuous growth, and collaborative problem-solving.\nCollaborate with business partners to understand requirements, ensuring technical solutions align with business goals.\nManage and improve existing software solutions, ensuring high performance and scalability.\nParticipate actively in all Agile scrum ceremonies, contributing to the continuous improvement of team processes.\nProduce comprehensive technical design documents and conduct technical walkthroughs.\nExperience & Qualifications:\nBachelors degree in computer science, Information Systems, Engineering, equivalent or more is required\nProficient with software development lifecycle (SDLC) methodologies like Agile, Test-driven development\n10+ years of experience with 4+ years designing/developing enterprise products, modern tech stacks and data platforms\n4+ years of hands-on experience contributing to application architecture & designs, proven software/enterprise integration design patterns and full-stack knowledge including modern distributed front end and back-end technology stacks\n5+ years full stack development experience in modern web development technologies, Java/J2EE, UI frameworks like Angular, React, SQL, Oracle, NoSQL Databases like MongoDB\nExperience designing transactional/data warehouse/data lake and data integrations with Big data eco system leveraging AWS cloud technologies\nThorough understanding of distributed computing\nPassionate, smart, and articulate developer\nQuality first mindset with a strong background and experience with developing products for a global audience at scale\nExcellent analytical thinking, interpersonal, oral and written communication skills with strong ability to influence both IT and business partners\nSuperior knowledge of system architecture, object-oriented design, and design patterns.\nGood work ethic, self-starter, and results-oriented\nExcellent communication skills are essential, with strong verbal and writing proficiencies\nExp. with Delta Lake systems like Databricks using AWS cloud technologies and PySpark is a plus\nAdditional Preferred Qualifications:\nExperience working AWS\nExperience with SAFe Agile Framework\nBachelor's/PG degree in Computer Science, Information Systems or equivalent.\nHands-on experience contributing to application architecture & designs, proven software/enterprise integration design principles\nAbility to prioritize and manage work to critical project timelines in a fast-paced environment\nExcellent Analytical and communication skills are essential, with strong verbal and writing proficiencies\nAbility to train and mentor",Industry Type: Banking,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'AWS cloud technologies', 'PySpark', 'J2EE', 'React Full Stack', 'Databricks', 'React', 'Angular']",2025-06-13 05:15:43
Data Engineer IV - Big Data / Spark,Sadup Soft,5 - 7 years,Not Disclosed,['Chennai'],"Must have skills :\n\n- Minimum of 5-7 years of experience in software development, with a focus on Java and infrastructure tools.\n\n- Min 6+ years of experience as a Data Engineer.\n\n- Good Experience in handling Big Data Spark, Hive SQL, BigQuery, SQL.\n\n- Candidate worked on cloud platforms and GCP would be an added advantage.\n\n- Good understanding of Hadoop based ecosystem including hard sequel, HDFS would be very essential.\n\n- Very good professional knowledge of PySpark or using Scala\n\nResponsibilities :\n\n- Collaborate with cross-functional teams such as Data Scientists, Product Partners and Partner Team Developers to identify opportunities for Big Data, Query ( Spark, Hive SQL, BigQuery, SQL ) tuning opportunities that can be solved using machine learning and generative AI.\n\n- Write clean, high-performance, high-quality, maintainable code.\n\n- Design and develop Big Data Engineering Solutions Applications for above ensuring scalability, efficiency, and maintainability of such solutions.\n\nRequirements :\n\n- A Bachelor or Master's degree in Computer Science or a related field.\n\n- Proven experience working as a Big Data & MLOps Engineer, with a focus on Spark, Scala Spark or PySpark, Spark SQL, BigQuery, Python, Google Cloud,.\n\n- Deep understanding and experience in tuning Dataproc, BigQuery, Spark Applications.\n\n- Solid knowledge of software engineering best practices, including version control systems (e.g Git), code reviews, and testing methodologies.\n\n- Strong communication skills to effectively collaborate and present findings to both technical and non-technical stakeholders.\n\n- Proven ability to adapt and learn new technologies and frameworks quickly.\n\n- A proactive mindset with a passion for continuous learning and research.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Big Data', 'Data Engineering', 'BigQuery', 'GCP', 'Spark', 'Machine Learning', 'Python', 'SQL']",2025-06-13 05:15:44
Lead AWS Glue Data Engineer,Allegis Group,8 - 13 years,Not Disclosed,[],"Lead AWS Glue Data Engineer\nJob Location : Hyderabad / Bangalore / Chennai / Noida/ Gurgaon / Pune / Indore / Mumbai/ Kolkata\n\nWe are seeking a skilled Lead AWS Data Engineer with 8+ years of strong programming and SQL skills to join our team. The ideal candidate will have hands-on experience with AWS Data Analytics services and a basic understanding of general AWS services. Additionally, prior experience with Oracle and Postgres databases and secondary skills in Python and Azure DevOps will be an advantage.\n\nKey Responsibilities:\nDesign, develop, and optimize data pipelines using AWS Data Analytics services such as RDS, DMS, Glue, Lambda, Redshift, and Athena.\nImplement data migration and transformation processes using AWS DMS and Glue.\nWork with SQL (Oracle & Postgres) to query, manipulate, and analyse large datasets.\nDevelop and maintain ETL/ELT workflows for data ingestion and transformation.\nUtilize AWS services like S3, IAM, CloudWatch, and VPC to ensure secure and efficient data operations.\nWrite clean and efficient Python scripts for automation and data processing.\nCollaborate with DevOps teams using Azure DevOps for CI/CD pipelines and infrastructure management.\nMonitor and troubleshoot data workflows to ensure high availability and performance.\n\nPreferred Qualifications:\nAWS certifications in Data Analytics, Solutions Architect, or DevOps.\nExperience with data warehousing concepts and data lake implementations.\nHands-on experience with Infrastructure as Code (IaC) tools like Terraform or CloudFormation.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['RDS', 'Glue', 'DMS', 'Lambda', 'Redshift', 'Athena']",2025-06-13 05:15:46
Senior Data Engineer,The Main Stage Productions,4 - 6 years,Not Disclosed,['Bengaluru'],"Design and implement cloud-native data architectures on AWS, including data lakes, data warehouses, and streaming pipelines using services like S3, Glue, Redshift, Athena, EMR, Lake Formation, and Kinesis.\nDevelop and orchestrate ETL/ELT pipelines\n\nRequired Candidate profile\nParticipate in pre-sales and consulting activities such as:\nEngaging with clients to gather requirements and propose AWS-based data engineering solutions.\nSupporting RFPs/RFIs, technical proposals",Industry Type: Advertising & Marketing,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['AWS Glue', 'GitHub Actions', 'PySpark', 'Scala', 'CodePipeline', 'Step Functions', 'data engineering']",2025-06-13 05:15:48
Data Engineering - Senior Developer with Salesforce,Job Delights,5 - 10 years,25-27.5 Lacs P.A.,"['Hyderabad', 'Chennai', 'Bengaluru']","Data Engineering with SQL, Python, ETL & SalesForce Marketing Cloud (Must)",Industry Type: Software Product,Department: Other,"Employment Type: Full Time, Permanent","['Salesforce Marketing Cloud', 'SQL', 'Marketing Cloud', 'Salesforce', 'Python']",2025-06-13 05:15:50
Data Engineer - SAS Migration,Crisil,2 - 4 years,Not Disclosed,['Mumbai'],"The SAS to Databricks Migration Developer will be responsible for migrating existing SAS code, data processes, and workflows to the Databricks platform\n\nThis role requires expertise in both SAS and Databricks, with a focus on converting SAS logic into scalable PySpark and Python code\n\nThe developer will design, implement, and optimize data pipelines, ensuring seamless integration and functionality within the Databricks environment\n\nCollaboration with various teams is essential to understand data requirements and deliver solutions that meet business needs",Industry Type: Financial Services,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['hive', 'scala', 'pyspark', 'data warehousing', 'data migration', 'azure data factory', 'sql', 'sql azure', 'java', 'spark', 'mysql', 'hadoop', 'big data', 'etl', 'python', 'sas', 'microsoft azure', 'power bi', 'machine learning', 'sql server', 'data bricks', 'migration', 'sqoop', 'aws', 'ssis']",2025-06-13 05:15:52
IT Manager - Data Engineering & Analytics,ZS,12 - 15 years,Not Disclosed,['Pune'],"IT MANAGER, DATA ENGINEERING AND ANALYTICS will lead a team of data engineers and analysts responsible for designing, developing, and maintaining robust data systems and integrations. This role is critical for ensuring the smooth collection, transformation, integration and visualization of data, making it easily accessible for analytics and decision-making across the organization. The Manager will collaborate closely with analysts, developers, business leaders and other stakeholders to ensure that the data infrastructure meets business needs and is scalable, reliable, and efficient.\n",,,,"['Data modeling', 'Project management', 'Analytical', 'Financial planning', 'Management consulting', 'Data quality', 'Troubleshooting', 'Stakeholder management', 'Analytics', 'SQL']",2025-06-13 05:15:54
IN Senior Associate GenAI S/W Engineer- Data and Analytics,PwC Service Delivery Center,1 - 7 years,Not Disclosed,['Bengaluru'],"Not Applicable\nSpecialism\nData, Analytics & AI\nManagement Level\nSenior Associate\n& Summary\nAt PwC, our people in data and analytics engineering focus on leveraging advanced technologies and techniques to design and develop robust data solutions for clients. They play a crucial role in transforming raw data into actionable insights, enabling informed decisionmaking and driving business growth.\n\nIn data engineering at PwC, you will focus on designing and building data infrastructure and systems to enable efficient data processing and analysis. You will be responsible for developing and implementing data pipelines, data integration, and data transformation solutions.\nWhy PWC\n& Summary\nJob Overview\nWe are seeking a highly skilled and versatile polyglot Full Stack Developer with expertise in modern frontend and backend technologies, cloudbased solutions, AI/ML and Gen AI. The ideal candidate will have a strong foundation in fullstack development, cloud platforms (preferably Azure), and handson experience in Gen AI, AI and machine learning technologies.\nKey Responsibilities\nDevelop and maintain web applications using Angular / React.js , .NET , and Python .\nDesign, deploy, and optimize Azure native PaaS and SaaS services, including but not limited to Function Apps , Service Bus , Storage Accounts , SQL Databases , Key vaults, ADF, Data Bricks and REST APIs with Open API specifications.\nImplement security best practices for data in transit and rest. Authentication best practices SSO, OAuth 2.0 and Auth0.\nUtilize Python for developing data processing and advanced AI/ML models using libraries like pandas , NumPy , scikitlearn and Langchain , Llamaindex , Azure OpenAI SDK\nLeverage Agentic frameworks like Crew AI, Autogen etc.\nWell versed with RAG and Agentic Architecture.\nStrong in Design patterns Architectural, Data, Object oriented\nLeverage azure serverless components to build highly scalable and efficient solutions.\nCreate, integrate, and manage workflows using Power Platform , including Power Automate , Power Pages , and SharePoint .\nApply expertise in machine learning , deep learning , and Generative AI to solve complex problems.\nPrimary Skills\nProficiency in React.js , .NET , and Python .\nStrong knowledge of Azure Cloud Services , including serverless architectures and data security.\nExperience with Python Data Analytics libraries\npandas\nNumPy\nscikitlearn\nMatplotlib\nSeaborn\nExperience with Python Generative AI Frameworks\nLangchain\nLlamaIndex\nCrew AI\nAutoGen\nFamiliarity with REST API design , Swagger documentation , and authentication best practices .\nSecondary Skills\nExperience with Power Platform tools such as Power Automate, Power Pages, and SharePoint integration.\nKnowledge of Power BI for data visualization (preferred).\nPreferred Knowledge Areas Nice to have\nIndepth understanding of Machine Learning , deep learning, supervised, unsupervised algorithms.\nMandatory skill sets\nAI, ML\nPreferred skill sets\nAI, ML\nYears of experience required\n3 7 years\nEducation qualification\nBE/BTECH, ME/MTECH, MBA, MCA\nEducation\nDegrees/Field of Study required Bachelor of Technology, Master of Business Administration, Bachelor of Engineering, Master of Engineering\nDegrees/Field of Study preferred\nRequired Skills\nGame AI\nAccepting Feedback, Accepting Feedback, Active Listening, Agile Scalability, Amazon Web Services (AWS), Analytical Thinking, Apache Airflow, Apache Hadoop, Azure Data Factory, Communication, Creativity, Data Anonymization, Data Architecture, Database Administration, Database Management System (DBMS), Database Optimization, Database Security Best Practices, Databricks Unified Data Analytics Platform, Data Engineering, Data Engineering Platforms, Data Infrastructure, Data Integration, Data Lake, Data Modeling, Data Pipeline {+ 28 more}\nTravel Requirements\nGovernment Clearance Required?",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Front end', 'Architecture', 'Data modeling', 'data security', 'Machine learning', 'data visualization', 'Apache', 'SQL', 'Python', 'Data architecture']",2025-06-13 05:15:55
Linux Kernel Engineer Senior For Data Center SoC,Qualcomm,2 - 7 years,Not Disclosed,['Bengaluru'],"Job Area: Engineering Group, Engineering Group > Software Engineering\n\nGeneral Summary:\n\nAs a Senior Software Engineer, you will play a pivotal role in designing, developing, optimizing, and commercializing software solutions for Qualcomms next-generation data center platforms. You will collaborate closely with cross-functional teams to advance critical technologies such as virtualization, memory management, scheduling, and the Linux Kernel.\n\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 2+ years of Software Engineering or related work experience.\nOR\nMaster's degree in Engineering, Information Systems, Computer Science, or related field and 1+ year of Software Engineering or related work experience.\nOR\nPhD in Engineering, Information Systems, Computer Science, or related field.\n\n2+ years of academic or work experience with Programming Language such as C, C++, Java, Python, etc.\nCollaborate within the team and across teams to design, develop, and release our software, tooling, and practices to meet community standards and internal and external requirements.\nBring up platform solutions across the Qualcomm chipset portfolio.\nTriage software build, tooling, packaging, functional, or stability failures.\nGuide and support development teams inside and outside the Linux organization, focusing on Linux userspace software functionality, integration, and maintenance.\nWork with development and product teams as necessary for issue resolution.\n\n\nPreferred Qualifications:\nMaster's Degree in Engineering, Information Systems, Computer Science, or a related field.\nStrong background in Computer Science and software fundamentals.\nWorking knowledge of C, C++, and proficiency in scripting languages (Bash, Python, etc.).\nExperience using git/gerrit.\nStrong understanding of the Linux kernel, configuration techniques like ACPI and device tree, system services, and various components that make up a Linux distribution.\nExperience with Linux distributions such as Debian, Ubuntu, RedHat, Yocto, etc.\nFamiliarity with package managers and their workings is crucial.\nFamiliarity with CI/CD tools.\nProven ability and interest in debugging complex compute and data center systems.\nStrong ability to solve problems in a non-linear fashion.\nQuick learner; able to grasp concepts with only basic training and the initiative to ask questions and investigate new areas and concepts as needed.\nPrior experience with Qualcomm software platforms is a plus.\nMature interpersonal skills with an ability to collaboratively work within the team and with many varied teams to resolve problems spanning many disciplines.\nProven ability to work in a dynamic, multi-tasked environment.\nExcellent written and verbal communication skills are required.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['computer science', 'linux', 'software engineering', 'scripting languages', 'linux kernel', 'continuous integration', 'c++', 'redhat linux', 'ci/cd', 'gerrit', 'git', 'java', 'yocto', 'embedded systems', 'debian', 'html', 'mysql', 'python', 'c', 'ubuntu', 'javascript', 'data center', 'embedded c', 'bash', 'aws']",2025-06-13 05:15:57
"Data Engineering : Sr Software Engineer, Tech Lead & Sr Tech Lead",Reflion Tech,7 - 12 years,22.5-37.5 Lacs P.A.,"['Mumbai( Ghansoli )', 'Navi Mumbai', 'Mumbai (All Areas)']","Hiring: Data Engineering Senior Software Engineer / Tech Lead / Senior Tech Lead\n\n- Hybrid (3 Days from office) | Shift: 2 PM 11 PM IST\n- Experience: 5 to 12+ years (based on role & grade)\n\nOpen Grades/Roles:\nSenior Software Engineer: 58 Years\nTech Lead: 7–10 Years\nSenior Tech Lead: 10–12+ Years\n\nJob Description – Data Engineering Team\n\nCore Responsibilities (Common to All Levels):\n\nDesign, build and optimize ETL/ELT pipelines using tools like Pentaho, Talend, or similar\nWork on traditional databases (PostgreSQL, MSSQL, Oracle) and MPP/modern systems (Vertica, Redshift, BigQuery, MongoDB)\nCollaborate cross-functionally with BI, Finance, Sales, and Marketing teams to define data needs\nParticipate in data modeling (ER/DW/Star schema), data quality checks, and data integration\nImplement solutions involving messaging systems (Kafka), REST APIs, and scheduler tools (Airflow, Autosys, Control-M)\nEnsure code versioning and documentation standards are followed (Git/Bitbucket)\n\nAdditional Responsibilities by Grade\n\nSenior Software Engineer (5–8 Yrs):\nFocus on hands-on development of ETL pipelines, data models, and data inventory\nAssist in architecture discussions and POCs\nGood to have: Tableau/Cognos, Python/Perl scripting, GCP exposure\n\nTech Lead (7–10 Yrs):\nLead mid-sized data projects and small teams\nDecide on ETL strategy (Push Down/Push Up) and performance tuning\nStrong working knowledge of orchestration tools, resource management, and agile delivery\n\nSenior Tech Lead (10–12+ Yrs):\nDrive data architecture, infrastructure decisions, and internal framework enhancements\nOversee large-scale data ingestion, profiling, and reconciliation across systems\nMentoring junior leads and owning stakeholder delivery end-to-end\nAdvantageous: Experience with AdTech/Marketing data, Hadoop ecosystem (Hive, Spark, Sqoop)\n\n- Must-Have Skills (All Levels):\n\nETL Tools: Pentaho / Talend / SSIS / Informatica\nDatabases: PostgreSQL, Oracle, MSSQL, Vertica / Redshift / BigQuery\nOrchestration: Airflow / Autosys / Control-M / JAMS\nModeling: Dimensional Modeling, ER Diagrams\nScripting: Python or Perl (Preferred)\nAgile Environment, Git-based Version Control\nStrong Communication and Documentation",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'SQL', 'ETL', 'Orchestration', 'Postgresql', 'Peri', 'Informatica', 'ETL Tool', 'SSIS', 'Elt', 'Modeling', 'MongoDB', 'Data Architecture', 'Talend', 'Pentaho', 'Python']",2025-06-13 05:15:59
Senior / Lead Data Engineer,Atlas,1 - 2 years,Not Disclosed,['Pune'],"1. Designing and developing data pipelines: Lead data engineers are responsible for designing and developing data pipelines that move data from various sources to storage and processing systems.\n2. Building and maintaining data infrastructure: Lead data engineers are responsible for building and maintaining data infrastructure, such as data warehouses, data lakes, and data marts.\n3. Ensuring data quality and integrity: Lead data engineers are responsible for ensuring data quality and integrity, by setting up data validation processes and implementing data quality checks.\n4. Managing data storage and retrieval: Lead data engineers are responsible for managing data storage and retrieval, by designing and implementing data storage systems, such as NoSQL databases or Hadoop clusters.\n5. Developing and maintaining data models: Lead data engineers are responsible for developing and maintaining data models, such as data dictionaries and entity-relationship diagrams, to ensure consistency in data architecture.\n6. Managing data security and privacy: Lead data engineers are responsible for managing data security and privacy, by implementing security measures, such as access controls and encryption, to protect sensitive data.\n7. Leading and managing a team: Lead data engineers may be responsible for leading and managing a team of data engineers, providing guidance and support for their work.",Industry Type: Industrial Equipment / Machinery,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent",['Senior Lead'],2025-06-13 05:16:00
Data Streaming Engineer,Data Streaming Engineer,5 - 10 years,Not Disclosed,"['Pune', 'Chennai', 'Bengaluru']","Hello Candidates,\n\nWe are Hiring !!\n\nJob Position - Data Streaming Engineer\nExperience - 5+ years\nLocation - Mumbai, Pune , Chennai , Bangalore\nWork mode - Hybrid ( 3 days WFO)\n\nJOB DESCRIPTION\n\nRequest for Data Streaming Engineer Data Streaming @ offshore :\n• Flink , Python Language.\n• Data Lake Systems. (OLAP Systems).\n• SQL (should be able to write complex SQL Queries)\n• Orchestration (Apache Airflow is preferred).\n• Hadoop (Spark and Hive: Optimization of Spark and Hive apps).\n• Snowflake (good to have).\n• Data Quality (good to have).\n• File Storage (S3 is good to have)\n\nNOTE - Candidates can share their resume on - shrutia.talentsketchers@gmail.com",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Flink', 'Apache Airflow', 'Data Quality', 'Hadoop', 'Snowflake', 'Data Lake', 'orchastration', 'Python', 'SQL']",2025-06-13 05:16:02
Data Lineage Engineers,Altimetrik,5 - 10 years,15-30 Lacs P.A.,"['Pune', 'Chennai', 'Bengaluru']","Role & responsibilities\nSkill need :  Data Lineage with Ab-initio - Metadata hub\n\nSkills & Experience:\nExpertise in mHub or similar tools, data pipelines, and cloud platforms.\nProficiency in Python, Oracle, SQL, Java, and ETL tools.\n5-10 years of experience in data engineering and governance.",,,,"['Ab-initio', 'Metadata hub', 'Python', 'mhub', 'ETL', 'Oracle', 'SQL']",2025-06-13 05:16:04
Consultant - Lead Data Engineer,Affine Analytics,5 - 8 years,Not Disclosed,['Bengaluru'],"Strong experience with Python, SQL, pySpark, AWS Glue. Good to have - Shell Scripting, Kafka\nGood knowledge of DevOps pipeline usage (Jenkins, Bitbucket, EKS, Lightspeed)\nExperience of AWS tools (AWS S3, EC2, Athena, Redshift, Glue, EMR, Lambda, RDS, Kinesis, DynamoDB, QuickSight etc.).\nOrchestration using Airflow\nGood to have - Streaming technologies and processing engines, Kinesis, Kafka, Pub/Sub and Spark Streaming\nGood debugging skills",,,,"['Python', 'RDS', 'Shell Scripting', 'Kafka', 'AWS Glue', 'DynamoDB', 'Lightspeed', 'EMR', 'EKS', 'pySpark', 'Redshift', 'SQL', 'Jenkins', 'QuickSight', 'Glue', 'EC2', 'Kinesis', 'AWS S3', 'Bitbucket', 'Athena', 'Lambda']",2025-06-13 05:16:06
Senior Data Engineer,ANS Group,2 - 6 years,Not Disclosed,['Ahmedabad'],"ANS Group is looking for Senior Data Engineer\nThe job responsibilities of a Senior Data Engineer may include:1\n\nDesigning and implementing scalable and reliable data pipelines, data models, and data infrastructure for processing large and complex datasets\n\n2\n\nDeveloping and maintaining databases, data warehouses, and data lakes that store and manage the organization's data\n\n3\n\nDeveloping and implementing data integration and ETL (Extract, Transform, Load) processes to ensure that data flows smoothly and accurately between different systems and data sources\n\n4\n\nEnsuring data quality, consistency, and accuracy through data profiling, cleansing, and validation\n\n5\n\nBuilding and maintaining data processing and analytics systems that support business intelligence, machine learning, and other data-driven applications\n\n6\n\nOptimizing the performance and scalability of data systems and infrastructure to ensure that they can handle the organization's growing data needs\n\nTo be a successful Senior Data Engineer, one must have in-depth knowledge of database architecture, data modeling, data integration, and ETL processes\n\nThey should also be proficient in programming languages such as Python, Java, or SQL and have experience working with big data technologies like Hadoop, Spark, and NoSQL databases\n\nStrong communication and leadership skills",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'data warehousing', 'data pipeline', 'machine learning', 'data engineering', 'sql', 'nosql', 'database design', 'data quality', 'java', 'data modeling', 'spark', 'leadership', 'hadoop', 'etl', 'data integration', 'programming', 'data lake', 'etl process', 'communication skills', 'data profiling']",2025-06-13 05:16:08
Senior Data Engineer,Amgen Inc,9 - 12 years,Not Disclosed,['Hyderabad'],"Role Description:\nWe are looking for highly motivated expert Senior Data Engineer who can own the design & development of complex data pipelines, solutions and frameworks. The ideal candidate will be responsible to design, develop, and optimize data pipelines, data integration frameworks, and metadata-driven architectures that enable seamless data access and analytics. This role prefers deep expertise in big data processing, distributed computing, data modeling, and governance frameworks to support self-service analytics, AI-driven insights, and enterprise-wide data management.",,,,"['Data engineering', 'performance tuning', 'data security', 'data processing', 'Hadoop', 'Apache Spark', 'SQL', 'CI/CD', 'troubleshooting', 'big data', 'aws', 'ETL', 'Python']",2025-06-13 05:16:10
Senior Data Engineer,Amgen Inc,3 - 8 years,Not Disclosed,['Hyderabad'],"Role Description:\nWe are looking for highly motivated expert Senior Data Engineer who can own the design & development of complex data pipelines, solutions and frameworks. The ideal candidate will be responsible to design, develop, and optimize data pipelines, data integration frameworks, and metadata-driven architectures that enable seamless data access and analytics. This role prefers deep expertise in big data processing, distributed computing, data modeling, and governance frameworks to support self-service analytics, AI-driven insights, and enterprise-wide data management.",,,,"['Data Engineering', 'Git', 'PySpark', 'CI/CD', 'Databricks', 'ETL', 'NOSQL', 'AWS', 'data integration', 'SQL', 'Apache Spark', 'Python']",2025-06-13 05:16:11
Senior Data Engineer,Amgen Inc,3 - 7 years,Not Disclosed,['Hyderabad'],"Role Description:\nWe are looking for highly motivated expert Senior Data Engineer who can own the design & development of complex data pipelines, solutions and frameworks. The ideal candidate will be responsible to design, develop, and optimize data pipelines, data integration frameworks, and metadata-driven architectures that enable seamless data access and analytics. This role prefers deep expertise in big data processing, distributed computing, data modeling, and governance frameworks to support self-service analytics, AI-driven insights, and enterprise-wide data management.",,,,"['Data Engineering', 'Maven', 'SparkSQL Apache Spark', 'PySpark', 'Subversion', 'OLAP', 'Scaled Agile methodologies', 'SQL', 'Scaled Agile Framework', 'Jenkins', 'NOSQL database', 'Git', 'Databricks', 'Data Fabric', 'Data Mesh', 'AWS', 'Python']",2025-06-13 05:16:13
Senior Data Engineer (Data Architect),Adastra Corp,8 - 13 years,Not Disclosed,[],"Join our innovative team and architect the future of data solutions on Azure, Synapse, and Databricks!\nSenior Data Engineer (Data Architect)\nAdditional Details:\nNotice Period: 30 days (maximum)\nLocation: Remote\nAbout the Role\nDesign and implement scalable data pipelines, data warehouses, and data lakes that drive business growth. Collaborate with stakeholders to deliver data-driven insights and shape the data landscape.\nRequirements\n8+ years of experience in data engineering and data architecture\nStrong expertise in Azure services (Synapse Analytics, Databricks, Storage, Active Directory)\nProven experience in designing and implementing data pipelines, data warehouses, and data lakes\nStrong understanding of data governance, data quality, and data security\nExperience with infrastructure design and implementation, including DevOps practices and tools",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Azure', 'DESIGN', 'Architecture', 'Synapse Analytics']",2025-06-13 05:16:14
Senior Data Engineer,Ensemble Health Partners,5 - 9 years,Not Disclosed,[],"Company Overview:\nAs Ensemble Health Partners Company, we're at the forefront of innovation, leveraging cutting-edge technology to drive meaningful impact in the Revenue Cycle Management landscape. Our future-forward technology combines tightly integrated data ingestion, workflow automation and business intelligence solutions on a modern cloud architecture. We have the second-largest share in the RCM space in the US Market with 10000+ professionals working in the organization. With 10 Technology Patents in our name, we believe the best results come from a combination of skilled and experienced team, proven and repeatable processes, and modern and flexible technologies. As a leading player in the industry, we offer an environment that fosters growth, creativity, and collaboration, where your expertise will be valued, and your contributions will make a difference.\n\nRole & responsibilities :\nExperience : 5-9 Years\nLocation : remote/wfh\n\nPosition Summary :\nDesign and maintain scalable data pipelines, manage ETL processes and data warehouses, ensure data quality and governance, collaborate with cross-functional teams, support machine learning deployment, lead projects, mentor juniors, work with big data and cloud technologies, and bring expertise in Spark, Databricks, Streaming/Reactive/Event-driven systems, Agentic programming, and LLM application development.\n\nRequired Skills :\nSpark, Databricks, Streaming/Reactive /Event driven, Agentic programming & LLM Application Experience\n5+ years of coding experience with Microsoft SQL.\n3+ years working with big data technologies including but not limited to Databricks, Apache Spark, Python, Microsoft Azure (Data Factory, Dataflows, Azure Functions, Azure Service Bus) with a willingness and ability to learn new ones\nExcellent understanding of engineering fundamentals: testing automation, code reviews, telemetry, iterative delivery and DevOps\nExperience with polyglot storage architectures including relational, columnar, key-value, graph or equivalent\nExperience with Delta tables as well as Parquet files stored in ADLS\nExperience delivering applications using componentized and distributed architectures using event driven patterns\nDemonstrated ability to communicate effectively to both technical and non-technical, globally distributed audiences\nSolid foundations in formal architecture, design patterns and best practices\nExperience working with healthcare datasets\n\nWhy Join US?\nWe adapt emerging technologies to practical uses to deliver concrete solutions that bring maximum impact to providers bottom line. We currently have 10 Technology Patents in our name.\nWe offer you a great organization to work for, where you will get to do best work of your career and grow with the team that is shaping the future of Revenue Cycle Management.\nWe have our strong focus on Learning and development. We have the best Industry standard professional development policies to support the learning goals of our associates.\nWe have flexible/ remote working/ working from home options\nBenefits\nHealth Benefits and Insurance Coverage for family and parents. Accidental Insurance for the associate.\nCompliant with all Labor Laws- Maternity benefits, Paternity Leaves.\nCompany Swags- Welcome Packages, Work Anniversary Kits\nExclusive Referral Policy\nProfessional Development Program and Reimbursements.\nRemote work flexibility to work from home.\nPlease share your resume on yash.arora@ensemblehp.com with current ctc, expected ctc, notice period.",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Data Factory', 'Pyspark', 'Azure Functions', 'Azure Databricks']",2025-06-13 05:16:16
"Walk-In Senior Data Engineer - DataStage, Azure & Power BI",Net Connect,6 - 10 years,5-11 Lacs P.A.,['Hyderabad( Madhapur )'],"Greetings from NCG!\n\nWe have a opening for Snowflake Developer role in Hyderabad office!\nBelow JD for your reference\n\nJob Description:\n\nWe are hiring an experienced Senior Data Engineer with strong expertise in IBM DataStage, , and . The ideal candidate will be responsible for end-to-end data integration, transformation, and reporting solutions that drive business decisions.",,,,"['Azure Data Factory', 'Datastage', 'Etl Datastage']",2025-06-13 05:16:18
Senior Data Engineer,Amgen Inc,3 - 7 years,Not Disclosed,['Hyderabad'],"What you will do\nRole Description:\nWe are seeking a Senior Data Engineer with expertise in Graph Data technologies to join our data engineering team and contribute to the development of scalable, high-performance data pipelines and advanced data models that power next-generation applications and analytics. This role combines core data engineering skills with specialized knowledge in graph data structures, graph databases, and relationship-centric data modeling, enabling the organization to leverage connected data for deep insights, pattern detection, and advanced analytics use cases. The ideal candidate will have a strong background in data architecture, big data processing, and Graph technologies and will work closely with data scientists, analysts, architects, and business stakeholders to design and deliver graph-based data engineering solutions.",,,,"['Data Engineering', 'SPARQL', 'Maven', 'PySpark', 'GSQL', 'Subversion', 'AWS services', 'Stardog', 'Cypher', 'SAFe', 'Jenkins', 'DevOps', 'Git', 'Neo4j', 'Delta Lake', 'Graph Databases', 'Spark', 'Marklogic', 'Gremlin']",2025-06-13 05:16:19
"Senior data engineer - Python, Pyspark, AWS - 5+ years Gurgaon",One of the largest insurance providers.,5 - 10 years,Not Disclosed,['Gurugram'],"Senior data engineer - Python, Pyspark, AWS - 5+ years Gurgaon\n\nSummary: An excellent opportunity for someone having a minimum of five years of experience with expertise in building data pipelines. A person must have experience in Python, Pyspark and AWS.\n\nLocation- Gurgaon (Hybrid)\n\nYour Future Employer- One of the largest insurance providers.\n\nResponsibilities-\nTo design, develop, and maintain large-scale data pipelines that can handle large datasets from multiple sources.\nReal-time data replication and batch processing of data using distributed computing platforms like Spark, Kafka, etc.\nTo optimize the performance of data processing jobs and ensure system scalability and reliability.\nTo collaborate with DevOps teams to manage infrastructure, including cloud environments like AWS.\nTo collaborate with data scientists, analysts, and business stakeholders to develop tools and platforms that enable advanced analytics and reporting.\n\nRequirements-\nHands-on experience with AWS services such as S3, DMS, Lambda, EMR, Glue, Redshift, RDS (Postgres) Athena, Kinesics, etc.\nExpertise in data modeling and knowledge of modern file and table formats.\nProficiency in programming languages such as Python, PySpark, and SQL/PLSQL for implementing data pipelines and ETL processes.\nExperience data architecting or deploying Cloud/Virtualization solutions (Like Data Lake, EDW, Mart ) in the enterprise.\nCloud/hybrid cloud (preferably AWS) solution for data strategy for Data lake, BI and Analytics.\nWhat is in for you-\nA stimulating working environment with equal employment opportunities.\nGrowing of skills while working with industry leaders and top brands.\nA meritocratic culture with great career progression.\n\nReach us- If you feel that you are the right fit for the role please share your updated CV at randhawa.harmeen@crescendogroup.in\n\nDisclaimer- Crescendo Global specializes in Senior to C-level niche recruitment. We are passionate about empowering job seekers and employers with an engaging memorable job search and leadership hiring experience. Crescendo Global does not discriminate on the basis of race, religion, color, origin, gender, sexual orientation, age, marital status, veteran status, or disability status.",Industry Type: Insurance,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Pyspark', 'Data Pipeline', 'AWS', 'Data Ingestion', 'Data Engineering', 'Data Processing']",2025-06-13 05:16:21
Senior Data Engineer with Dot Net,Swits Digital,4 - 7 years,Not Disclosed,['Gurugram'],"Job Title: Senior Data Engineer with .NET\n\nExperience: 6+ Years\n\nLocation: Gurgaon, India\n\n\n\nRequired Skills:\n\n\n\n\n5+ years of experience in distributed computing (Spark) and software development\n\n\n\n3+ years of hands-on experience in Spark-Scala\n\n\n\n5+ years of experience in Data Engineering\n\n\n\n5+ years of experience in Python\n\n\n\n5+ years of experience in .NET Core\n\n\n\nExperience with Dapper, SQL, XUnit, NUnit, and RabbitMQ\n\n\n\nProficiency in working with databases (preferably Postgres)\n\n\n\nSolid understanding of Object-Oriented Programming principles\n\n\n\nExperience in Agile development environments (Scrum/Kanban)\n\n\n\nExperience with version control tools (preferably Git)\n\n\n\nCI/CD pipeline experience\n\n\n\nStrong exposure to automated testing including Integration/Delta, Load, and Performance testing\n\n\n\n\nGood to Have Skills:\n\n\n\n\nExperience with Docker and containerized deployments\n\n\n\nExperience with Kubernetes\n\n\n\nFamiliarity with Airflow\n\n\n\nExperience working in cloud environments (GCP and Azure)\n\n\n\nExposure to TeamCity CI and Octopus Deploy",Industry Type: Recruitment / Staffing,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Automation testing', 'GIT', 'Version control', 'spark', 'Agile development', 'Performance testing', 'Deployment', 'Object oriented programming', 'SQL', 'Python']",2025-06-13 05:16:23
Senior Data Engineer - Azure,Blend360 India,6 - 11 years,Not Disclosed,['Hyderabad'],"As a Senior Data Engineer, your role is to spearhead the data engineering teams and elevate the team to the next level! You will be responsible for laying out the architecture of the new project as well as selecting the tech stack associated with it. You will plan out the development cycles deploying AGILE if possible and create the foundations for good data stewardship with our new data products!\nYou will also set up a solid code framework that needs to be built to purpose yet have enough flexibility to adapt to new business use cases tough but rewarding challenge!\n\nResponsibilities\nCollaborate with several stakeholders to deeply understand the needs of data practitioners to deliver at scale\nLead Data Engineers to define, build and maintain Data Platform\nWork on building Data Lake in Azure Fabric processing data from multiple sources\nMigrating existing data store from Azure Synapse to Azure Fabric\nImplement data governance and access control\nDrive development effort End-to-End for on-time delivery of high-quality solutions that conform to requirements, conform to the architectural vision, and comply with all applicable standards.\nPresent technical solutions, capabilities, considerations, and features in business terms. Effectively communicate status, issues, and risks in a precise and timely manner.\nFurther develop critical initiatives, such as Data Discovery, Data Lineage and Data Quality\nLeading team and Mentor junior resources\nHelp your team members grow in their role and achieve their career aspirations\nBuild data systems, pipelines, analytical tools and programs\nConduct complex data analysis and report on results",Industry Type: Industrial Automation,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Access control', 'Data analysis', 'Team leading', 'Architecture', 'Analytical', 'Agile', 'data governance', 'Data processing', 'Mentor', 'Data quality']",2025-06-13 05:16:24
Senior Data Engineer - Azure,Blend360 India,5 - 11 years,Not Disclosed,['Hyderabad'],"As a Senior Data Engineer, your role is to spearhead the data engineering teams and elevate the team to the next level! You will be responsible for laying out the architecture of the new project as well as selecting the tech stack associated with it. You will plan out the development cycles deploying AGILE if possible and create the foundations for good data stewardship with our new data products!\nYou will also set up a solid code framework that needs to be built to purpose yet have enough flexibility to adapt to new business use cases tough but rewarding challenge!\n\nResponsibilities\nCollaborate with several stakeholders to deeply understand the needs of data practitioners to deliver at scale\nLead Data Engineers to define, build and maintain Data Platform\nWork on building Data Lake in Azure Fabric processing data from multiple sources\nMigrating existing data store from Azure Synapse to Azure Fabric\nImplement data governance and access control\nDrive development effort End-to-End for on-time delivery of high-quality solutions that conform to requirements, conform to the architectural vision, and comply with all applicable standards.\nPresent technical solutions, capabilities, considerations, and features in business terms. Effectively communicate status, issues, and risks in a precise and timely manner.\nFurther develop critical initiatives, such as Data Discovery, Data Lineage and Data Quality\nLeading team and Mentor junior resources\nHelp your team members grow in their role and achieve their career aspirations\nBuild data systems, pipelines, analytical tools and programs\nConduct complex data analysis and report on results\n\n\n5+ Years of Experience as a data engineer or similar role in Azure Synapses, ADF or\nrelevant exp in Azure Fabric\nDegree in Computer Science, Data Science, Mathematics, IT, or similar field\nMust have experience e",Industry Type: Industrial Automation,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Data analysis', 'Data modeling', 'Analytical', 'Agile', 'data governance', 'Data quality', 'Data mining', 'SQL', 'Python']",2025-06-13 05:16:26
Data Engineer / Sr. Data Entry Specialist,Techwaukee,2 - 4 years,Not Disclosed,[],Role Purpose\nThe purpose of this role is to execute the process and drive the performance of the team on the key metrices of the process.\nJob Details\nCountry/Region: India\nEmployment Type: Remote\nWork Type: Contract,,,,"['Product management', 'ERP', 'Data modeling', 'XML', 'MySQL', 'JSON', 'Informatica', 'Information technology', 'SQL', 'Python']",2025-06-13 05:16:28
Senior Data engineer,Wissen Technology,10 - 17 years,Not Disclosed,['Pune'],"Wissen Technology is Hiring fo r Senior Data engineer\n\nAbout Wissen Technology:\nWissen Technology is a globally recognized organization known for building solid technology teams, working with major financial institutions, and delivering high-quality solutions in IT services. With a strong presence in the financial industry, we provide cutting-edge solutions to address complex business challenges\n\nRole Overview : We are looking for a highly skilled and experienced Senior Data Engineer to join our growing data engineering team in Bangalore . In this role, you will be responsible for designing, developing, and maintaining scalable and efficient data pipelines and data architectures. You will collaborate closely with cross-functional teams to ensure data accessibility, reliability, and performance to support business intelligence, analytics, and data science initiatives\n\nExperience : 9-13 Years\nLocation: Bangalore\nKey Responsibilities\nDesign , build, and maintain scalable and reliable data pipelines for ingesting, transforming, and loading structured and unstructured data from diverse sources.\nDevelop optimized SQL queries and modular scripts for data manipulation and transformation.\nCreate and maintain data models and schemas to support advanced analytics, reporting, and operational processes.\nAutomate routine data engineering tasks and data quality checks using scripting (Python, Shell, etc.) and orchestration tools.\nCollaborate with engineering, product, and data science teams to understand data requirements and deliver high-quality solutions.\nAct as a liaison between engineering and business teams to translate business needs into technical solutions.\nImplement monitoring, alerting, and troubleshooting mechanisms for data pipelines and dashboards to ensure data integrity and availability.\nDefine and implement best practices for data validation, data governance, and compliance.\nManage test data and QA environments, supporting test data management processes.\nWork in an Agile environment and contribute to continuous improvement initiatives across the data engineering landscape.\n\n\nRequired Skills and Qualification\nBachelors or masters degree in computer science , Information Systems, or a related field.\n9+ years of professional experience in data engineering or a related field.\nStrong expertise in SQL development and performance tuning for both RDBMS and cloud-based databases.\nHands-on experience with cloud platforms, particularly AWS (e.g., S3, Glue, Lambda, Redshift, EMR).\nExperience with modern data warehouse technologies like Snowflake and Amazon Redshift .\nStrong background in data modeling , ETL/ELT architecture, and data warehousing concepts.\nProficiency in programming languages such as Python , Scala , or Java .\nExperience working with",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Telecom', 'Performance tuning', 'Manager Quality Assurance', 'Data management', 'Consulting', 'Manager Technology', 'Healthcare', 'Business intelligence', 'Monitoring', 'Python']",2025-06-13 05:16:30
Sr. Data Engineer - Snowflake,Freelancer Monika,5 - 10 years,15-25 Lacs P.A.,['Pune'],"Role & responsibilities\nDesigned and implemented end-to-end data pipeline using DBT, Snowflake  \nCreated and structure DBT models like staging, transformation, marts, YAML configurations for models and tests, dbt  seeds.  \n\nHands-on experience on DBT Jinja templating, macro development, dbt jobs and snapshot management for Slowly  changing dimensions.  \n\nDevelop python script for data cleaning, transformation and automation of repetitive task.  \nExperienced in loading structured and semi-structured data from AWS S3 to Snowflake by designing file formats,  \nconfiguring storage integration, and automating data loads using Snow pipe.  \nDesigned scalable incremental models for handling large datasets, reducing resource usage\n\nPreferred candidate profile\nCandidate must have 5+ Yrs experience.\nEarly joiner, who can join within a month",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Snowflake', 'DBT', 'Data Engineer', 'Snowflake Sql', 'Snow Flake Schema', 'Data Build Tool', 'Snowflake Db', 'ETL', 'SQL']",2025-06-13 05:16:31
Senior Azure Data Engineer,Cloud Angles Digital Transformation,8 - 12 years,Not Disclosed,['Hyderabad'],"Job Summary:\nWe are seeking a highly skilled Data Engineer with expertise in leveraging Data Lake architecture and the Azure cloud platform to develop, deploy, and optimise data-driven solutions. . You will play a pivotal role in transforming raw data into actionable insights, supporting strategic decision-making across the organisation.\nResponsibilities\nDesign and implement scalable data science solutions using Azure Data Lake, Azure Data Bricks, Azure Data Factory and related Azure services.\nDevelop, train, and deploy machine learning models to address business challenges.\nCollaborate with data engineering teams to optimise data pipelines and ensure seamless data integration within Azure cloud infrastructure.\nConduct exploratory data analysis (EDA) to identify trends, patterns, and insights.\nBuild predictive and prescriptive models to support decision-making processes.\nExpertise in developing end-to-end Machine learning lifecycle utilizing crisp-DM which includes of data collection, cleansing, visualization, preprocessing, model development, model validation and model retraining\nProficient in building and implementing RAG systems that enhance the accuracy and relevance of model outputs by integrating retrieval mechanisms with generative models.\nEnsure data security, compliance, and governance within the Azure cloud ecosystem.\nMonitor and optimise model performance and scalability in production environments.\nPrepare clear and concise documentation for developed models and workflows.\nSkills Required:\nGood experience in using Pyspark, Python, MLops (Optional), ML flow (Optional), Azure Data Lake Storage. Unity Catalog\nWorked and utilized data from various RDBMS like MYSQL, SQL Server, Postgres and NoSQL databases like MongoDB, Cassandra, Redis and graph DB like Neo4j, Grakn.\nProven experience as a Data Engineer with a strong focus on Azure cloud platform and Data Lake architecture.\nProficiency in Python, Pyspark,\nHands-on experience with Azure services such as Azure Data Lake, Azure Synapse Analytics, Azure Machine Learning, Azure Databricks, and Azure Functions.\nStrong knowledge of SQL and experience in querying large datasets from Data Lakes.\nFamiliarity with data engineering tools and frameworks for data ingestion and transformation in Azure.\nExperience with version control systems (e.g., Git) and CI/CD pipelines for machine learning projects.\nExcellent problem-solving skills and the ability to work collaboratively in a team environment.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Data Factory', 'Azure Data Engineering', 'Azure Databricks', 'Pyspark', 'Azure Data Lake', 'Python']",2025-06-13 05:16:33
Sr Data Engineering Manager,Amgen Inc,12 - 15 years,Not Disclosed,['Hyderabad'],"ABOUT THE ROLE\nRole Description:\nWe are seeking a Senior Data Engineering Manager with a strong background in Regulatory or Integrated Product Teams within the Biotech or Pharmaceutical domain. This role will lead the end-to-end data strategy and execution for regulatory product submissions, lifecycle management, and compliance reporting, ensuring timely and accurate delivery of regulatory data assets across global markets.You will be embedded in a cross-functional Regulatory Integrated Product Team (IPT) and serve as the data and technology lead, driving integration between scientific, regulatory, and engineering functions to support submission-ready data and regulatory intelligence solutions.",,,,"['Data Engineering', 'engineering strategy', 'DevOps', 'Project Management', 'DataOps', 'Agile', 'data strategy']",2025-06-13 05:16:35
"Sr. Data Engineer, R&D Data Catalyst Team",Amgen Inc,7 - 9 years,Not Disclosed,['Hyderabad'],"The R&D Data Catalyst Team is responsible for buildingData Searching, Cohort Building, and Knowledge Management tools that provide the Amgen scientific community with visibility to Amgens wealth of human datasets, projects and study histories, and knowledge over various scientific findings. These solutions are pivotal tools in Amgens goal to accelerate the speed of discovery, and speed to market of advanced precision medications.\nThe Data Engineer will be responsible for the end-to-end development of an enterprise analytics and data mastering solution leveraging Databricks and Power BI. This role requiresexpertise in both data architecture and analytics, with the ability to create scalable, reliable, and high-performing",,,,"['Data Engineering', 'data analysis', 'ETL processes', 'DAX', 'Business Objects', 'data warehouse design', 'ETL', 'PowerBI Models', 'AWS', 'Power Query']",2025-06-13 05:16:37
Senior Data Engineer,Keeptruckin,6 - 11 years,Not Disclosed,[],"Who we are:\nMotive empowers the people who run physical operations with tools to make their work safer, more productive, and more profitable. For the first time ever, safety, operations and finance teams can manage their drivers, vehicles, equipment, and fleet related spend in a single system. Combined with industry leading AI, the Motive platform gives you complete visibility and control, and significantly reduces manual workloads by automating and simplifying tasks.\nMotive serves more than 120,000 customers - from Fortune 500 enterprises to small businesses - across a wide range of industries, including transportation and logistics, construction, energy, field service, manufacturing, agriculture, food and beverage, retail, and the public sector.\nVisit gomotive.com to learn more.\nAbout the Role:\nAs a Senior Data Engineer you will be part of the core data team building out world class data models and pipelines that will feed into our products. You will be working on the intersection of all data streams in the company from our IOT data consuming 100s of thousands of data points per minute to our user data. You will partner closely with both the Product, Engineering, as well as the Strategic Analytics teams. We are seeking strong team players who thrive on innovation and continuous improvement. We pride ourselves on our culture, and ability to work effectively across a highly diversified team.\nWhat Youll Do:\nBuild data pipelines based on product data which will go into our product for Enterprise customers\nArchitect and design data models in collaboration with data and product teams\nCommunicate effectively across multiple teams and projects.\nActively work on deploying Data Ops into Motive, driving the most robust data models using the latest tools in the market\nYou will be working with airflow, aws, great expectations and table creation frameworks similar to dbt.\nWhat Were Looking For:\nBachelors degree or higher in a quantitative field, e.g. Computer Science, Math, Economics, or Statistics\n6+ years experience in Data Engineering, including experience building modeled tables\nExpertise with data engineering stack, dBt, Snowflake, airflow, data observability tools, AWS.\nExpertise in SQL and Python\nWillingness to learn new technologies\nSolid communication, collaboration, and people skills\nCreating a diverse and inclusive workplace is one of Motives core values. We are an equal opportunity employer and welcome people of different backgrounds, experiences, abilities and perspectives.\nPlease review our Candidate Privacy Notice here .\nUK Candidate Privacy Notice here .\nThe applicant must be authorized to receive and access those commodities and technologies controlled under U.S. Export Administration Regulations. It is Motives policy to require that employees be authorized to receive access to Motive products and technology.\n#LI-Remote",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Product engineering', 'Manager Technology', 'Continuous improvement', 'AWS', 'Analytics', 'Team building', 'SQL', 'Python', 'Logistics']",2025-06-13 05:16:38
Senior Data Engineer (Identity),Kargo,6 - 11 years,Not Disclosed,[],"Success takes all kinds. Diversity describes our workforce. Inclusion defines our culture. We do not discriminate on the basis of race, color, religion, sex, sexual orientation, gender identity, marital status, age, national origin, protected veteran status, disability or other legally protected status. Individuals with disabilities are provided reasonable accommodation to participate in the job application process, perform essential job functions, and receive other benefits and privileges of employment.\nTitle: Senior Data Engineer\nJob Type: Permanent\n\nJob Location: Remote\nThe Opportunity\nAt Kargo, we are rapidly evolving our data infrastructure and capabilities to address challenges of data scale, new methodologies for onboarding and targeting, and rigorous privacy standards. Were looking for an experienced Senior Data Engineer to join our team, focusing on hands-on implementation, creative problem-solving, and exploring new technical approaches. Youll work collaboratively with our technical leads and peers, actively enhancing and scaling the data processes that drive powerful targeting systems.\nThe Daily To-Do\nIndependently implement, optimize, and maintain robust ETL/ELT pipelines using Python, Airflow, Spark, Iceberg, Snowflake, Aerospike, Docker, Kubernetes (EKS), AWS, and real-time streaming technologies like Kafka and Flink.\nEngage proactively in collaborative design and brainstorming sessions, contributing technical insights and innovative ideas for solving complex data engineering challenges.\nSupport the definition and implementation of robust testing strategies, and guide the team in adopting disciplined CI/CD practices using ArgoCD to enable efficient and reliable deployments.\nMonitor and optimize data systems and infrastructure to ensure operational reliability, performance efficiency, and cost-effectiveness.\nActively contribute to onboarding new datasets, enhancing targeting capabilities, and exploring modern privacy-compliant methodologies.\nMaintain thorough documentation of technical implementations, operational procedures, and best practices for effective knowledge sharing and onboarding.\nQualifications:\nStrong expertise in implementing, maintaining, and optimizing large-scale data systems with minimal oversight.\nDeep proficiency in Python, Spark, and Iceberg, with a clear understanding of data structuring for efficiency and performance.\nExperience with Airflow for building robust data workflows is strongly preferred.\nExtensive DevOps experience, particularly with AWS (including EKS), Docker, Kubernetes, CI/CD automation using ArgoCD, and monitoring via Prometheus.\nFamiliarity with Snowflake, including writing and optimizing SQL queries and understanding Snowflakes performance and cost dynamics.\nComfort with Agile methodologies, including regular use of Jira and Confluence for task management and documentation.\nProven ability to independently drive implementation and problem-solving, turning ambiguity into clearly defined actions.\nExcellent communication skills to effectively engage in discussions with technical teams and stakeholders.\nFamiliarity with identity, privacy, and targeting methodologies in AdTech is required.\nFollow Our Lead\nBig Picture: kargo.com\nThe Latest: Instagram ( @kargomobile ) and LinkedIn ( Kargo )",Industry Type: Advertising & Marketing,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SQL queries', 'Automation', 'CTV', 'spark', 'Agile', 'JIRA', 'Operations', 'Cost', 'AWS', 'Python']",2025-06-13 05:16:40
"Senior Data Engineer II, Business Intelligence & Reporting",XL India Business Services Pvt. Ltd,3 - 7 years,Not Disclosed,['Gurugram'],"Senior Data Engineer II, Business Intelligence & Reporting Gurgaon, Haryana, India AXA XL recognizes digital, data, and information assets are critical for the business, both in terms of managing risk and enabling new business opportunities\n\nThis data should not only be high quality, but also actionable - enabling AXA XL s executive leadership team to maximize benefits and facilitate sustained dynamic advantage\n\nOur Data, Intelligence & Analytics function is focused on driving innovation by optimizing how we leverage digital, data, and AI to drive strategy and differentiate ourselves from the competition\n\nAs we develop an enterprise-wide data and digital strategy that moves us toward a greater focus on the use of data and strengthening our digital, AI capabilities, we are seeking a Deputy Manager, BI and Reporting\n\nIn this role, you will support/manage BI & reporting\n\nWhat you ll be DOING Your essential responsibilities include: BI & Reporting Management: Oversee and support Business Intelligence (BI) and Reporting products, ensuring their effectiveness and alignment with organizational goals\n\nStakeholder Engagement: Manage Business as Usual (BAU) activities for BI and Reporting, fostering effective communication and relationships with stakeholders to understand their needs and expectations\n\nModel Integration: Energize and synergize various Business Intelligence models and reporting systems to enhance data insights and reporting capabilities\n\nStrategic Initiative Support: Collaborate with the Data Intelligence and Analytics (DIA) team on various strategic initiatives, enabling the development of BI and Reporting functions and related capabilities\n\nTalent Development: Foster the growth of BI and Reporting talent across AXA XL by promoting an inclusive and diverse environment that encourages the utilization and value creation of our strategic digital, data, and analytics assets\n\nCustomer-Centric Culture: Instill a customer-first mindset within the team, prioritizing exceptional service for business stakeholders and ensuring their needs are met\n\nTeam Development: Contribute to the enhancement of the Business Intelligence teams tools, skills, and culture, driving positive impacts on team performance and outcomes\n\nYou will report to Senior Manager, Business Intelligence & Reporting\n\nWhat you will BRING At AXA XL, we view individuals holistically through their People, Business, and Technical Skills\n\nWe re interested in what you bring, how you think, and your potential for growth\n\nWe value diverse backgrounds and perspectives, recognizing that each person contributes uniquely to our teams success\n\nWe value relevant education and experience in a related field\n\nAdditionally, we encourage candidates with diverse educational backgrounds or equivalent experience to apply\n\nHere are some of the key skills important for the role: People Skills Customer Centricity: Brings a collaborative spirit, a can-do attitude, and a Customer First mindset, ensuring that stakeholder needs are prioritized\n\nCross-Functional Collaboration: Ability to communicate effectively with teams, peers, and stakeholders across the globe, fostering collaboration and understanding\n\nAble to help and guide team members on technical issues, fostering their development and promoting self-directed problem-solving\n\nGrowth Mindset: Passion for digital, data, and AI, along with a commitment to personal and team development in a digital and data-driven organization\n\nResilience: Ability to lead a project or team, demonstrating adaptability and leadership under various circumstances\n\nAnalytical & Strategic Mindset: Ability to analyze data effectively and develop strategic insights that drive decision-making and improve business outcomes\n\nPerformance Excellence: Commitment to delivering high-quality results and continuously improving processes and performance metrics within the team\n\nBUSINESS Skills Business & Insurance Acumen: Ability to showcase relevant industry knowledge supporting multiple specialty areas of Data and Analytics\n\nStakeholder Management: Ability to manage stakeholders effectively, understanding their needs and ensuring clear communication and support\n\nSimplifies Complexity: Ability to distill complex data concepts and analyses into clear, actionable insights for stakeholders\n\nEnsuring that technical information is accessible, enabling informed decision-making and fostering collaboration between technical and non-technical teams\n\nTECHNICAL Skills Data Visualization: Experience with end-user BI tools like Power BI, enabling effective presentation and visualization of data insights\n\nReporting Tools: Proficincy in SQL, Advanced Excel, MS Access, and VBA, allowing for effective data manipulation and reporting\n\nData Analytics: Ability to help and guide team members on technical issues, fostering skill development within the team to self-directedly manage data analytics tasks",Industry Type: Insurance,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Stakeholder Engagement', 'MS Access', 'Analytical', 'Agile', 'digital strategy', 'Business strategy', 'data visualization', 'Stakeholder management', 'Reporting tools', 'SQL']",2025-06-13 05:16:42
Senior Data Engineer - AWS,Blend360 India,6 - 10 years,Not Disclosed,['Hyderabad'],"We are looking for an experienced Senior Data Engineer with a strong foundation in Python, SQL, and Spark , and hands-on expertise in AWS, Databricks . In this role, you will build and maintain scalable data pipelines and architecture to support analytics, data science, and business intelligence initiatives. You ll work closely with cross-functional teams to drive data reliability, quality, and performance.\nResponsibilities:\nDesign, develop, and optimize scalable data pipelines using Databricks in AWS such as Glue, S3, Lambda, EMR, Databricks notebooks, workflows and jobs.\nBuilding data lake in WS Databricks.\nBuild and maintain robust ETL/ELT workflows using Python and SQL to handle structured and semi-structured data.\nDevelop distributed data processing solutions using Apache Spark or PySpark .\nPartner with data scientists and analysts to provide high-quality, accessible, and well-structured data.\nEnsure data quality, governance, security, and compliance across pipelines and data stores.\nMonitor, troubleshoot, and improve the performance of data systems and pipelines.\nParticipate in code reviews and help establish engineering best practices.\nMentor junior data engineers and support their technical development.\n\n\nRequirements\nBachelors or masters degree in computer science, Engineering, or a related field.\n5+ years of hands-on experience in data engineering , with at lea",Industry Type: Industrial Automation,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Automation', 'Version control', 'GIT', 'Workflow', 'Data quality', 'Business intelligence', 'Analytics', 'SQL', 'Python']",2025-06-13 05:16:44
Senior Data Engineer - Azure,Blend360 India,6 - 11 years,Not Disclosed,['Hyderabad'],"As a Data Engineer, your role is to spearhead the data engineering teams and elevate the team to the next level! You will be responsible for laying out the architecture of the new project as well as selecting the tech stack associated with it. You will plan out the development cycles deploying AGILE if possible and create the foundations for good data stewardship with our new data products!\nYou will also set up a solid code framework that needs to be built to purpose yet have enough flexibility to adapt to new business use cases tough but rewarding challenge!\n\nResponsibilities\nCollaborate with several stakeholders to deeply understand the needs of data practitioners to deliver at scale\nLead Data Engineers to define, build and maintain Data Platform\nWork on building Data Lake in Azure Fabric processing data from multiple sources\nMigrating existing data store from Azure Synapse to Azure Fabric\nImplement data governance and access control\nDrive development effort End-to-End for on-time delivery of high-quality solutions that conform to requirements, conform to the architectural vision, and comply with all applicable standards.\nPresent technical solutions, capabilities, considerations, and features in business terms. Effectively communicate status, issues, and risks in a precise and timely manner.\nFurther develop critical initiatives, such as Data Discovery, Data Lineage and Data Quality\nLeading team and Mentor junior resources\nHelp your team members grow in their role and achieve their career aspirations\nBuild data systems, pipelines, analytical tools and programs\nConduct complex data analysis and report on results\n\n\n3+ Years of Experience as a data engineer or similar role in Azure Synapses, ADF or\nrelevant exp in Azure Fabric\nDegree in Computer Science, Data Science, Mathematics, IT, or similar field\nMust have experience e",Industry Type: Industrial Automation,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Data analysis', 'Data modeling', 'Analytical', 'Agile', 'data governance', 'Data quality', 'Data mining', 'SQL', 'Python']",2025-06-13 05:16:45
Senior Data Engineer,Neal Analytics,10 - 15 years,Not Disclosed,['Mumbai'],"Its fun to work in a company where people truly BELIEVE in what they are doing!\nWere committed to bringing passion and customer focus to the business.\nJob Description:\nAs a Backend (Java) Engineer, you would be part of the team consisting of Scrum Master, Cloud Engineers, AI/ML Engineers, and UI/UX Engineers to build end-to-end Data to Decision Systems.\nMandatory:\n8+ years of demonstrable experience designing, building, and working as a Java Developer for enterprise web applications\nIdeally, this would include the following:\no Expert-level proficiency with Java\no Expert-level proficiency with SpringBoot\nFamiliarity with common databases (RDBMS such as MySQL & NoSQL such as MongoDB) and data warehousing concepts (OLAP, OLTP)\nUnderstanding of REST concepts and building/interacting with REST APIs\nDeep understanding of core backend concepts:\no Develop and design RESTful services and APIs\no Develop functional databases, applications, and servers to support websites on the back end\no Performance optimization and multithreading concepts\no Experience with deploying and maintaining high traffic infrastructure (performance testing is a plus)\nIn addition, the ideal candidate would have great problem-solving skills, and familiarity with code versioning tools such as GitHub\nGood to have:\nFamiliarity with Microsoft Azure Cloud Services (particularly Azure Web App, Storage and VM), or familiarity with AWS (EC2 containers) or GCP Services.\nExperience with Microservices, Messaging Brokers (e.g., RabbitMQ)\nExperience with fine-tuning reverse proxy engines such as Nginx, Apache HTTPD\nIf you like wild growth and working with happy, enthusiastic over-achievers, youll enjoy your career with us!\nNot the right fit? Let us know youre interested in a future opportunity by clicking Introduce Yourself in the top-right corner of the page or create an account to set up email alerts as new job postings become available that meet your interest!",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Backend', 'Multithreading', 'RDBMS', 'MySQL', 'Performance testing', 'OLAP', 'Scrum', 'MongoDB', 'Apache', 'OLTP']",2025-06-13 05:16:47
Senior Data Engineer : 7+ Years,Jayam Solutions Pvt Ltd - CMMI Level III Company,5 - 9 years,Not Disclosed,['Hyderabad( Madhapur )'],"Job Description:\nPosition: Sr.Data Engineer\nExperience: Minimum 7 years\nLocation: Hyderabad\nJob Summary:\n\nWhat Youll Do\n\nDesign and build efficient, reusable, and reliable data architecture leveraging technologies like Apache Flink, Spark, Beam and Redis to support large-scale, real-time, and batch data processing.\nParticipate in architecture and system design discussions, ensuring alignment with business objectives and technology strategy, and advocating for best practices in distributed data systems.\nIndependently perform hands-on development and coding of data applications and pipelines using Java, Scala, and Python, including unit testing and code reviews.\nMonitor key product and data pipeline metrics, identify root causes of anomalies, and provide actionable insights to senior management on data and business health.\nMaintain and optimize existing datalake infrastructure, lead migrations to lakehouse architectures, and automate deployment of data pipelines and machine learning feature engineering requests.\nAcquire and integrate data from primary and secondary sources, maintaining robust databases and data systems to support operational and exploratory analytics.\nEngage with internal stakeholders (business teams, product owners, data scientists) to define priorities, refine processes, and act as a point of contact for resolving stakeholder issues.\nDrive continuous improvement by establishing and promoting technical standards, enhancing productivity, monitoring, tooling, and adopting industry best practices.\n\nWhat Youll Bring\n\nBachelors degree or higher in Computer Science, Engineering, or a quantitative discipline, or equivalent professional experience demonstrating exceptional ability.\n7+ years of work experience in data engineering and platform engineering, with a proven track record in designing and building scalable data architectures.\nExtensive hands-on experience with modern data stacks, including datalake, lakehouse, streaming data (Flink, Spark), and AWS or equivalent cloud platforms.\nCloud - AWS\nApache Flink/Spark , Redis\nDatabase platform- Databricks.\nProficiency in programming languages such as Java, Scala, and Python(Good to have) for data engineering and pipeline development.\nExpertise in distributed data processing and caching technologies, including Apache Flink, Spark, and Redis.\nExperience with workflow orchestration, automation, and DevOps tools (Kubernetes,git,Terraform, CI/CD).\nAbility to perform under pressure, managing competing demands and tight deadlines while maintaining high-quality deliverables.\nStrong passion and curiosity for data, with a commitment to data-driven decision making and continuous learning.\nExceptional attention to detail and professionalism in report and dashboard creation.\nExcellent team player, able to collaborate across diverse functional groups and communicate complex technical concepts clearly.\nOutstanding verbal and written communication skills to effectively manage and articulate the health and integrity of data and systems to stakeholders.\n\nPlease feel free to contact us: 9440806850\nEmail ID : careers@jayamsolutions.com",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Apache Flink', 'Redis', 'Spark', 'Python', 'SCALA', 'Ci/Cd', 'Devops', 'AWS']",2025-06-13 05:16:49
Senior Data Engineer,Conversehr Business Solutions,7 - 12 years,30-45 Lacs P.A.,['Hyderabad'],"What is the Data team responsible for?\nAs a Senior Data Engineer, youll be a key member of the Data & AI team. This team is responsible for designing and delivering data engineering, analytics, and generative AI solutions that drive meaningful business impact. Were looking for a pragmatic, results-driven problem solver who thrives in a fast-paced environment and is passionate about building solutions on a scale #MID_SENIOR_LEVEL\nThe ideal candidate has a strong technical foundation, a collaborative mindset, and the ability to navigate complex challenges. You should be comfortable working in a fast-moving, startup-like environment within an established enterprise, and should bring strong skill sets to adapt to new solutions fast. You will play a pivotal role in optimizing data infrastructure, enabling data-driven decision-making and integrating AI across the organization.\nWhat is the Lead Software Engineer (Senior Data Engineer) responsible for?\nServe as a hands-on technical lead, driving project execution and delivery in our growing data team based in the Hyderabad office.\nCollaborate closely with the U.S.-based team and cross-functional stakeholders to understand business needs and deliver scalable solutions.\nLead the initiative to build firmwide data models and master data management solutions for structured data (in Snowflake) and manage unstructured data using vector embeddings.\nBuild, maintain, and optimize robust data pipelines and frameworks to support business intelligence and operational workflows.\nDevelop dashboards and data visualizations that support strategic business decisions.\nStay current with emerging trends in data engineering and help implement best practices within the team.\nMentor and support junior engineers, fostering a culture of learning and technical excellence.\nWhat ideal qualifications, skills & experience would help someone to be successful?\nBachelors or master’s degree in computer science, data science, engineering, or a related field.\n7+ years of experience in data engineering including 3+ years in a technical leadership role.\nStrong SQL skills and hands-on experience with modern data pipeline technologies (e.g., Spark, Flink).\nDeep expertise in the Snowflake ecosystem, including data modeling, data warehousing, and master data management.\nProficiency in at least one programming language - Python preferred.\nExperience with Tableau and Alteryx is a plus.\nSelf-starter with a passion for learning new tools and technologies.\nStrong communication skills and a collaborative, ownership-driven mindset.\nWork Shift Timings - 2:00 PM - 11:00 PM IST",Industry Type: Investment Banking / Venture Capital / Private Equity,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Engineering', 'Snowflake', 'Python', 'flink', 'Data Pipeline', 'Spark', 'SQL']",2025-06-13 05:16:51
Senior Data Engineer,Jeavio,5 - 10 years,Not Disclosed,[],"We are seeking an experienced Senior Data Engineer to join our team. The ideal candidate will have a strong background in data engineering and AWS infrastructure, with hands-on experience in building and maintaining data pipelines and the necessary infrastructure components. The role will involve using a mix of data engineering tools and AWS services to design, build, and optimize data architecture.\n\nKey Responsibilities:\nDesign, develop, and maintain data pipelines using Airflow and AWS services.\nImplement and manage data warehousing solutions with Databricks and PostgreSQL.\nAutomate tasks using GIT / Jenkins.\nDevelop and optimize ETL processes, leveraging AWS services like S3, Lambda, AppFlow, and DMS.\nCreate and maintain visual dashboards and reports using Looker.\nCollaborate with cross-functional teams to ensure smooth integration of infrastructure components.\nEnsure the scalability, reliability, and performance of data platforms.\nWork with Jenkins for infrastructure automation.\n\nTechnical and functional areas of expertise:\nWorking as a senior individual contributor on a data intensive project\nStrong experience in building high performance, resilient & secure data processing pipelines preferably using Python based stack.\nExtensive experience in building data intensive applications with a deep understanding of querying and modeling with relational databases preferably on time-series data.\nIntermediate proficiency in AWS services (S3, Airflow)\nProficiency in Python and PySpark\nProficiency with ThoughtSpot or Databricks.\nIntermediate proficiency in database scripting (SQL)\nBasic experience with Jenkins for task automation\n\nNice to Have :\nIntermediate proficiency in data analytics tools (Power BI / Tableau / Looker / ThoughSpot)\nExperience working with AWS Lambda, Glue, AppFlow, and other AWS transfer services.\nExposure to PySpark and data automation tools like Jenkins or CircleCI.\nFamiliarity with Terraform for infrastructure-as-code.\nExperience in data quality testing to ensure the accuracy and reliability of data pipelines.\nProven experience working directly with U.S. client stakeholders.\nAbility to work independently and take the lead on tasks.\n\nEducation and experience:\nBachelors or masters in computer science or related fields.\n5+ years of experience\n\nStack/Skills needed:\nDatabricks\nPostgreSQL\nPython & Pyspark\nAWS Stack\nPower BI / Tableau / Looker / ThoughSpot\nFamiliarity with GIT and/or CI/CD tools",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Pyspark', 'Data Engineering', 'AWS', 'Data Bricks', 'Python', 'Etl Pipelines', 'Airflow', 'Database Scripting', 'Postgresql', 'Looker', 'SQL']",2025-06-13 05:16:52
Data Engineer sr Associate,Product base company,8 - 13 years,Not Disclosed,['Hyderabad( Gachibowli )'],"Bachelors degree in computer science, engineering, or a related field. Master’s degree preferred.\nData: 5+ years of experience with data analytics and data warehousing. Sound knowledge of data warehousing concepts.\nSQL: 5+ years of hands-on experience on SQL and query optimization for data pipelines.\nELT/ETL: 5+ years of experience in Informatica/ 3+ years of experience in IICS/IDMC\nMigration Experience: Experience Informatica on prem to IICS/IDMC migration\nCloud: 5+ years’ experience working in AWS cloud environment\nPython: 5+ years of hands-on experience of development with Python\nWorkflow: 4+ years of experience in orchestration and scheduling tools (e.g. Apache Airflow)\nAdvanced Data Processing: Experience using data processing technologies such as Apache Spark or Kafka\nTroubleshooting: Experience with troubleshooting and root cause analysis to determine and remediate potential issues\nCommunication: Excellent communication, problem-solving and organizational and analytical skills\nAble to work independently and to provide leadership to small teams of developers.\nReporting: Experience with data reporting (e.g. MicroStrategy, Tableau, Looker) and data cataloging tools (e.g. Alation)\nExperience in Design and Implementation of ETL solutions with effective design and optimized performance, ETL Development with industry standard recommendations for jobs recovery, fail over, logging, alerting mechanisms. Role & responsibilities\n\n\nPreferred candidate profile",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Airflow', 'Unix', 'IICS', 'Informatica', 'Python', 'Informatica Cloud', 'Data Engineer', 'AWS', 'SQL']",2025-06-13 05:16:54
Senior AWS Data Engineer,Exavalu,7 - 12 years,Not Disclosed,[],"7-12 years of experience using AWS Data Landscape and data Ingestion pipeline.\nAble to understand and explain the data ingestion from different sources like file, database, applications etc\nBuild and enhance the Python, PySpark Based Framework for ingestion.Data engineering experience in AWS Data Services like Glue, EMR, Airflow, CloudWatch, Lambda, Step functions, Event triggers.\nAble to work as a senior engineer with sole interaction point with different business functional teams.\nRequirements\n7 to 12 years of experience in ETL and Data Engineering roles.\nAWS Glue, PySpark, and Amazon Redshift.\nStrong command of SQL and procedural programming in cloud or enterprise databases.\nDeep understanding of data warehousing concepts and data modeling.\nProven ability to deliver efficient, we'll-documented, and scalable data pipelines on AWS.\nFamiliarity with Airflow, AWS Lambda, and other orchestration tools is a plus.\nAWS Certification (eg, AWS Data Analytics Specialty) is an advantage.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data services', 'Data modeling', 'amazon redshift', 'Database', 'Programming', 'Data analytics', 'AWS', 'Data warehousing', 'SQL', 'Python']",2025-06-13 05:16:55
Senior Databricks Data Engineer,"Creative Capsule, LLC",5 - 10 years,Not Disclosed,['Panaji'],"This position will primarily be responsible for designing, developing, and maintaining robust ETL/ELT pipelines for data ingestion, transformation, and storage. The role also involves designing and developing scalable data solutions.\nYou will work with a team responsible for ensuring the availability, reliability, and performance of data systems and requires a good understanding of infrastructure management, cost optimization, and performance tuning in Databricks environments. The candidate will design, develop, and maintain scalable data pipelines of Databricks on cloud platforms (Databricks, Azure, AWS, or GCP).\nThe person will also work with a collaborative team responsible for driving client performance by combining data-driven insights and strategic thinking to solve business challenges. The candidate should have strong organizational, critical thinking, and communication skills to interact effectively with stakeholders.\nResponsibilities:\nDesign, develop, and manage end-to-end data pipelines on Databricks using Spark, Delta Lake, and related technologies\nImplement and optimize ETL/ELT workflows for ingesting, transforming, and storing large volumes of structured and semi-structured data\nManage and monitor Databricks infrastructure, including cluster configuration, auto-scaling, job execution, and resource utilization\nIdentify and implement cost-saving strategies across Databricks workspaces through efficient pipeline design, job scheduling, and infrastructure tuning\nOrchestrate workflows using tools like Apache Airflow, Azure Data Factory, or similar orchestration frameworks\nCollaborate with data architects and platform engineers to ensure efficient data architecture and performance tuning\nMonitor data quality, integrity, and lineage across all pipelines and proactively troubleshoot data issues\nDevelop, maintain, and optimize data models and storage layers that support BI/reporting and analytics use cases\nPartner with business stakeholders to translate business needs into technical specifications and scalable data solutions\nMaintain comprehensive documentation of pipeline architecture, configurations, and operational best practices\nTechnical Qualifications:\nExperience in infrastructure management on Databricks: cluster setup, scaling, security roles, workspace configuration\nExperience in cost optimization techniques in Databricks (e.g., job cluster vs. all-purpose cluster usage, cost/performance tradeoffs)\nExperience with orchestration tools such as Apache Airflow, Azure Data Factory, or AWS Glue Workflows\nExperience with relational (e.g., PostgreSQL, SQL Server) and NoSQL (e.g., MongoDB) databases\nStrong proficiency in Apache Spark and Delta Lake within the Databricks ecosystem\nProficient in Python and SQL for data processing and pipeline development\nFamiliarity with data warehousing, data lakes, and distributed data processing frameworks\nUnderstanding of BI tools like Power BI, Tableau, or Looker is a plus\nPersonal Skills:\nStrong analytical skills: ability to read business requirements, analyze problems, and propose solutions\nAbility to identify alternatives and find optimal solutions\nAbility to follow through and ensure logical implementation\nQuick learner with the ability to adapt to new concepts and software\nAbility to work effectively in a team environment\nStrong time management skills, capable of handling multiple tasks and competing deadlines\nEffective written and verbal communication skills\nEducation and Work Experience:\nBackground in Computer Science, Information Technology, Data Science, or a related field preferred\nMinimum 5 years of experience in Data Engineering with at least 2 years of hands-on experience with Databricks (Azure, AWS, or GCP)\nCertification in Databricks, Azure Data Engineering, or any related data technology is an added advantage",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Performance tuning', 'Infrastructure management', 'Postgresql', 'Scheduling', 'Data quality', 'Information technology', 'Analytics', 'SQL', 'Python']",2025-06-13 05:16:57
Data Engineer - GCP,Happiest Minds Technologies,5 - 10 years,8-18 Lacs P.A.,"['Pune', 'Bengaluru']","Key Responsibilities:\n• Data Pipeline Development: Designing, building, and maintaining robust data pipelines to move data from various sources (e.g., databases, external APIs, logs) to centralized data systems, such as data lakes or warehouses.\n• Data Integration: Integrating data from multiple sources and ensuring it's processed in a consistent, usable format. This involves transforming, cleaning, and validating data to meet the needs of products, analysts and data scientists.\n• Database Management: Creating, managing, and optimizing databases for storing large amounts of structured and unstructured data. Ensuring high availability, scalability, and security of data storage solutions.\nIdentifying and resolving issues related to the speed and efficiency of data systems. This could include optimizing queries, storage systems, and improving overall system architecture.\n• : Automating routine tasks, such as data extraction, transformation, and loading (ETL), to ensure smooth data flows with minimal manual intervention.\n• : Working closely with Work closely with product managers, UX/UI designers, and other stakeholders to understand data requirements and ensure data is in the right format for analysis and modeling.\n• : Ensuring data integrity and compliance with data governance policies, including data quality standards, privacy regulations (e.g., GDPR), and security protocols.\n: Continuously monitoring data pipelines and databases for any disruptions or errors and troubleshooting any issues that arise to ensure continuous data flow.\n• : Staying up to date with emerging data tools, technologies, and best practices in order to improve data systems and infrastructure.\n• : Documenting data systems, pipeline processes, and data architectures, providing clear instructions for the team to follow, and ensuring that the architecture is understandable for stakeholders.",,,,"['Data Engineering', 'gcp', 'Python', 'SQL', 'Pyspark', 'Bigquery', 'Google Cloud Platforms']",2025-06-13 05:16:59
Azure Data Engineer,JRD Systems,7 - 12 years,Not Disclosed,"['Hyderabad', 'Bengaluru']","Cloud Data Engineer\n\nThe Cloud Data Engineer will be responsible for developing the data lake platform and all applications on Azure cloud. Proficiency in data engineering, data modeling, SQL, and Python programming is essential. The Data Engineer will provide design and development solutions for applications in the cloud.\nEssential Job Functions:\nUnderstand requirements and collaborate with the team to design and deliver projects.\nDesign and implement data lake house projects within Azure.\nDevelop application lifecycle utilizing Microsoft Azure technologies.\nParticipate in design, planning, and necessary documentation.\nEngage in Agile ceremonies including daily standups, scrum, retrospectives, demos, and code reviews.\nHands-on experience with Python/SQL development and Azure data pipelines.\nCollaborate with the team to develop and deliver cross-functional products.\nKey Skills:\na. Data Engineering and SQL\nb. Python\nc. PySpark\nd. Azure Data Lake and ADF\ne. Databricks\nf. CI/CD\ng. Strong communication\nOther Responsibilities:\nDocument and maintain project artifacts.\nMaintain comprehensive knowledge of industry standards, methodologies, processes, and best practices.\nComplete training as required for Privacy, Code of Conduct, etc.\nPromptly report any known or suspected loss, theft, or unauthorized disclosure or use of PI to the General Counsel/Chief Compliance Officer or Chief Information Officer.\nAdhere to the company's compliance program.\nSafeguard the company's intellectual property, information, and assets.\nOther duties as assigned.\nMinimum Qualifications and Job Requirements:\nBachelor's degree in Computer Science.\n7 years of hands-on experience in designing and developing distributed data pipelines.\n5 years of hands-on experience in Azure data service technologies.\n5 years of hands-on experience in Python, SQL, Object-oriented programming, ETL, and unit testing.\nExperience with data integration with APIs, Web services, Queues.\nExperience with Azure DevOps and CI/CD as well as agile tools and processes including JIRA, Confluence.\n*Required: Azure data engineering associate and databricks data engineering certification",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'Delta Table', 'Azure Databricks', 'SQL', 'Python', 'SCALA', 'Big Data', 'Kafka', 'Azure Data Lake', 'Spark', 'ETL', 'Data Bricks']",2025-06-13 05:17:00
Azure Data Engineer,Big4,7 - 12 years,18-30 Lacs P.A.,['Bengaluru'],"Urgently Hiring for Senior Azure Data Engineer\n\nJob Location- Bangalore\nMinimum exp - Total 7+yrs with min 4 years relevant exp\n\nKeywords Databricks, Pyspark, SCALA, SQL, Live / Streaming data, batch processing data\n\nShare CV siddhi.pandey@adecco.com\nOR Call 6366783349\n\nRoles and Responsibilities:\nThe Data Engineer will work on data engineering projects for various business units, focusing on delivery of complex data management solutions by leveraging industry best practices. They work with the project team to build the most efficient data pipelines and data management solutions that make data easily available for consuming applications and analytical solutions. A Data engineer is expected to possess strong technical skills\n\nKey Characteristics\nTechnology champion who constantly pursues skill enhancement and has inherent curiosity to understand work from multiple dimensions\nInterest and passion in Big Data technologies and appreciates the value that can be brought in with an effective data management solution\nHas worked on real data challenges and handled high volume, velocity, and variety of data.\nExcellent analytical & problem-solving skills, willingness to take ownership and resolve technical challenges.\nContributes to community building initiatives like CoE, CoP.\nMandatory skills:\nAzure - Master\nELT - Skill\nData Modeling - Skill\nData Integration & Ingestion - Skill\nData Manipulation and Processing - Skill\nGITHUB, Action, Azure DevOps - Skill\nData factory, Databricks, SQL DB, Synapse, Stream Analytics, Glue, Airflow, Kinesis, Redshift, SonarQube, PyTest - Skill\nOptional skills:\nExperience in project management, running a scrum team.\nExperience working with BPC, Planning.\nExposure to working with external technical ecosystem.\nMKDocs documentation\n\nShare CV siddhi.pandey@adecco.com\nOR Call 6366783349",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['databricks', 'Azure Synapse', 'Pyspark', 'Stream Analytics', 'SCALA', 'SQL Azure', 'Data Bricks', 'SQL']",2025-06-13 05:17:02
Data Engineer I,Swiss Re,1 - 3 years,Not Disclosed,['Bengaluru'],"About the Role:\nAs a Data Engineer, you will be responsible for implementing data pipelines and analytics\nsolutions to support key decision-making processes in our Life Health Reinsurance business. You will become part of a project that is leveraging cutting edge technology that applies Big Data and Machine Learning to solve new and emerging problems for Swiss Re. You will be expected to gain a full understanding of the reinsurance data and business logic required to deliver analytics solutions.\nKey responsibilities include:\nWork closely with Product Owners and Engineering Leads to understand requirements and evaluate the implementation effort.\nDevelop and maintain scalable data transformation pipelines\nImplement analytics models and visualizations to provide actionable data insights\nCollaborate within a global development team to design and deliver solutions.\nAbout the Team:\nLife Health Data Analytics Engineering is a key tech partner for our Life Health Reinsurance division, supporting in the transformation of the data landscape and the creation of innovative analytical products and capabilities. A large globally distributed team working in an agile development landscape, we deliver solutions to make better use of our reinsurance data and enhance our ability to make data-driven decisions across the business value chain.\nAbout You:\nAre you eager to disrupt the industry with us and make an impactDo you wish to have your talent recognized and rewardedThen join our growing team and become part of the next wave of data\ninnovation. Key qualifications include:\nBachelors degree level or equivalent in Computer Science, Data Science or similar discipline\nAt least 1-3 years of experience working with large scale software systems\nProficient in Python/PySpark\nProficient in SQL (Spark SQL preferred)\nPalantir Foundry experience is a strong plus.\nExperience working with large data sets on enterprise data platforms and distributed computing (Spark/Hive/Hadoop preferred)\nExperience with JavaScript/HTML/CSS a plus\nExperience working in a Cloud environment such as AWS or Azure is a plus\nStrong analytical and problem-solving skills\nEnthusiasm to work in a global and multicultural environment of internal and external professionals\nStrong interpersonal and communication skills, demonstrating a clear and articulate standard of written and verbal communication in complex environments\nAbout Swiss Re\n.\n\n\n\nIf you are an experienced professional returning to the workforce after a career break, we encourage you to apply for open positions that match your skills and experience.\nKeywords:\nReference Code: 134085",Industry Type: Insurance,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Reinsurance', 'data science', 'spark', 'Analytical', 'Machine learning', 'Javascript', 'HTML', 'SQL', 'Python']",2025-06-13 05:17:03
"Data Engineer, Product Analytics",Meta,2 - 7 years,Not Disclosed,['Bengaluru'],"Apply to this job\nAs a Data Engineer at Meta, you will shape the future of people-facing and business-facing products we build across our entire family of applications (Facebook, Instagram, Messenger, WhatsApp, Reality Labs, Threads). Your technical skills and analytical mindset will be utilized designing and building some of the worlds most extensive data sets, helping to craft experiences for billions of people and hundreds of millions of businesses worldwide.In this role, you will collaborate with software engineering, data science, and product management teams to design/build scalable data solutions across Meta to optimize growth, strategy, and user experience for our 3 billion plus users, as well as our internal employee community.You will be at the forefront of identifying and solving some of the most interesting data challenges at a scale few companies can match. By joining Meta, you will become part of a world-class data engineering community dedicated to skill development and career growth in data engineering and beyond.Data Engineering: You will guide teams by building optimal data artifacts (including datasets and visualizations) to address key questions. You will refine our systems, design logging solutions, and create scalable data models. Ensuring data security and quality, and with a focus on efficiency, you will suggest architecture and development approaches and data management standards to address complex analytical problems.Product leadership: You will use data to shape product development, identify new opportunities, and tackle upcoming challenges. Youll ensure our products add value for users and businesses, by prioritizing projects, and driving innovative solutions to respond to challenges or opportunities.Communication and influence: You wont simply present data, but tell data-driven stories. You will convince and influence your partners using clear insights and recommendations. You will build credibility through structure and clarity, and be a trusted strategic partner.\nData Engineer, Product Analytics Responsibilities\nCollaborate with engineers, product managers, and data scientists to understand data needs, representing key data insights in a meaningful way\nDesign, build, and launch collections of sophisticated data models and visualizations that support multiple use cases across different products or domains\nDefine and manage Service Level Agreements for all data sets in allocated areas of ownership\nSolve challenging data integration problems, utilizing optimal Extract, Transform, Load (ETL) patterns, frameworks, query techniques, sourcing from structured and unstructured data sources\nImprove logging\nAssist in owning existing processes running in production, optimizing complex code through advanced algorithmic concepts\nOptimize pipelines, dashboards, frameworks, and systems to facilitate easier development of data artifacts\nInfluence product and cross-functional teams to identify data opportunities to drive impact\nMinimum Qualifications\nBachelors degree in Computer Science, Computer Engineering, relevant technical field, or equivalent\n2+ years of experience where the primary responsibility involves working with data. This could include roles such as data analyst, data scientist, data engineer, or similar positions\n2+ years of experience with SQL, ETL, data modeling, and at least one programming language (e.g., Python, C++, C#, Scala or others.)\nPreferred Qualifications\nMasters or Ph.D degree in a STEM field\nAbout Meta\n.\n\n\nEqual Employment Opportunity\n.\n\nMeta is committed to providing reasonable accommodations for qualified individuals with disabilities and disabled veterans in our job application procedures. If you need assistance or an accommodation due to a disability, fill out the Accommodations request form .",Industry Type: Internet,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Product management', 'Computer science', 'C++', 'Data management', 'Data modeling', 'data security', 'Analytical', 'Analytics', 'SQL', 'Python']",2025-06-13 05:17:05
Data Engineer II - Marketplace (Experimentation Track),Booking Holdings,5 - 10 years,Not Disclosed,['Bengaluru'],"We are looking for a Data Engineer to join our team and help us to improve the platform that supports one of the best experimentation tools in the world.\nYou will work side by side with other data engineers and site reliability engineers to improve the reliability, scalability, maintenance and operations of all the data products that are part of the experimentation tool at Booking.com.\nYour day to day work includes but is not limited to: maintenance and operations of data pipelines and products that handles data at big scale; the development of capabilities for monitoring, alerting, testing and troubleshooting of the data ecosystem of the experiment platform; and the delivery of data products that produce metrics for experimentation at scale. You will collaborate with colleagues in Amsterdam to achieve results the right way. This will include engineering managers, product managers, engineers and data scientists.",,,,"['Data Engineering', 'Airflow', 'Java', 'CDC', 'NoSQL', 'Snowflake', 'DBT', 'Kafka', 'Python']",2025-06-13 05:17:07
Data Engineering Manager,NOVARTIS,6 - 8 years,Not Disclosed,['Hyderabad'],"Summary\nWe are seeking a highly skilled and motivated GCP Data Engineering Manager to join our dynamic team. As a Data Engineering manager specializing in Google Cloud Platform (GCP), you will play a crucial role in designing, implementing, and maintaining scalable data pipelines and\nsystems. You will leverage your expertise in Google Big Query, SQL, Python, and analytical skills to drive data-driven decision-making processes and support various business functions.\nAbout the Role\nKey Responsibilities:\nData Pipeline Development: Design, develop, and maintain robust data pipelines using GCP services like Dataflow, Dataproc, ensuring high performance and scalability.\nGoogle Big Query Expertise: Utilize your hands-on experience with Google Big Query to manage and optimize data storage, retrieval, and processing.\nSQL Proficiency: Write and optimize complex SQL queries to transform and analyze large datasets, ensuring data accuracy and integrity.\nPython Programming: Develop and maintain Python scripts for data processing, automation, and integration with other systems and tools.\nData Integration: Collaborate with data analysts, and other stakeholders to integrate data from various sources, ensuring seamless data flow and consistency.\nData Quality and Governance: Implement data quality checks, validation processes, and governance frameworks to maintain high data standards.\nPerformance Tuning: Monitor and optimize the performance of data pipelines, queries, and storage solutions to ensure efficient data processing.\nDocumentation: Create comprehensive documentation for data pipelines, processes, and best practices to facilitate knowledge sharing and team collaboration.\nMinimum Qualifications:\nProven experience (minimum 6 - 8 yrs) in Data Engineer, with significant hands-on experience in Google Cloud Platform (GCP) and Google Big Query.\nProficiency in SQL for data transformation, analysis and performance optimization.\nStrong programming skills in Python, with experience in developing data processing scripts and automation.\nProven analytical skills with the ability to interpret complex data and provide actionable insights.\nExcellent problem-solving abilities and attention to detail.\nStrong communication and collaboration skills, with the ability to work effectively in a team enviro\nDesired Skills :\nExperience with Google Analytics data and understanding of digital marketing data.\nFamiliarity with other GCP services such as Cloud Storage, Dataflow, Pub/Sub, and Dataproc.\nKnowledge of data visualization tools such as Looker, Tableau, or Data Studio.\nExperience with machine learning frameworks and libraries.\nWhy Novartis: Helping people with disease and their families takes more than innovative science. It takes a community of smart, passionate people like you. Collaborating, supporting and inspiring each other. Combining to achieve breakthroughs that change patients lives. Ready to create a brighter future together? https://www. novartis. com / about / strategy / people-and-culture\nJoin our Novartis Network: Not the right Novartis role for you? Sign up to our talent community to stay connected and learn about suitable career opportunities as soon as they come up: https://talentnetwork. novartis. com/network\nBenefits and Rewards: Read our handbook to learn about all the ways we ll help you thrive personally and professionally:",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Performance tuning', 'Automation', 'Google Analytics', 'Machine learning', 'Data processing', 'Data quality', 'data visualization', 'Digital marketing', 'SQL', 'Python']",2025-06-13 05:17:09
Immediate Joiner- Data Engineer,Healthedge,1 - 4 years,Not Disclosed,['Bengaluru'],"Data Engineer\nYou will be working with agile cross functional software development teams developing cutting age software to solve a significant problem in the Provider Data Management space. This hire will have experience building large scale complex data systems involving multiple cross functional data sets and teams. The ideal candidate will be excited about working on new product development, is comfortable pushing the envelope and challenging the status quo, sets high standards for him/herself and the team, and works well with ambiguity.\nWhat you will do:\nBuild data pipelines to assemble large, complex sets of data that meet non-functional and functional business requirements.\nWork closely with data architect, SMEs and other technology partners to develop & execute data architecture and product roadmap.\nBuild analytical tools to utilize the data pipeline, providing actionable insight into key business performance including operational efficiency and business metrics.\nWork with stakeholders including the leadership, product, customer teams to support their data infrastructure needs while assisting with data-related technical issues.\nAct as a subject matter expert to other team members for technical guidance, solution design and best practices within the customer organization.\nKeep current on big data and data visualization technology trends, evaluate, work on proof-of-concept and make recommendations on cloud technologies.\nWhat you bring:\n2+ years of data engineering experience working in partnership with large data sets (preferably terabyte scale)\nExperience in building data pipelines using any of the ETL tools such as Glue, ADF, Notebooks, Stored Procedures, SQL/Python constructs or similar.\nDeep experience working with industry standard RDBMS such Postgres, SQL Server, Oracle, MySQL etc. and any of the analytical cloud databases such as Big Query, Redshift, Snowflake or similar\nAdvanced SQL expertise and solid programming experience with Python and/or Spark\nExperience working with orchestration tools such as Airflow and building complex dependency workflows.\nExperience, developing and implementing Data Warehouse or Data Lake Architectures, OLAP technologies, data modeling with star/snowflake-schemas to enable analytics & reporting.\nGreat problem-solving capabilities, troubleshooting data issues and experience in stabilizing big data systems.\nExcellent communication and presentation skills as youll be regularly interacting with stakeholders and engineering leadership.\nBachelors or master's in quantitative disciplines such as Computer Science, Computer Engineering, Analytics, Mathematics, Statistics, Information Systems, or other scientific fields.\nBonus points:\nHands-on deep experience with cloud data migration, and experience working with analytic platforms like Fabric, Databricks on the cloud.\nCertification in one of the cloud platforms (AWS/GCP/Azure)\nExperience or demonstrated understanding with real-time data streaming tools like Kafka, Kinesis or any similar tools.",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['ETL', 'SQL', 'Pyspark', 'Cloud', 'Python']",2025-06-13 05:17:11
Data Engineer - AWS,Happiest Minds Technologies,6 - 10 years,Not Disclosed,"['Pune', 'Bengaluru']","Role & responsibilities\nEssential Skills: Experience: 6 to 10 yrs\n- Technical Expertise: Proficiency in AWS services such as Amazon S3, Redshift, EMR, Glue, Lambda, and Kinesis. Strong skills in SQL and experience with scripting languages like Python or Java.\n- Data Engineering Experience: Hands on experience in building and maintaining data pipelines, data modeling, and working with big data technologies.\n- Problem-Solving Skills: Ability to analyze complex data issues and develop effective solutions to optimize data processing and storage.",,,,"['Data Engineering', 'Pyspark', 'Aws Lambda', 'Amazon Redshift', 'Aws Glue', 'Athena', 'AWS', 'Python', 'SQL']",2025-06-13 05:17:12
Tech. PM - Data Engineering-Data Analytics@ Gurgaon/Blore_Urgent,A global leader in delivering innovative...,5 - 10 years,Not Disclosed,"['Gurugram', 'Bengaluru']","Job Title - Technical Project Manager\n\nLocation - Gurgaon/ Bangalore\n\nNature of Job - Permanent\n\nDepartment - data analytics\n\nWhat you will be doing\n\n\nDemonstrated client servicing and business analytics skills with at least 5 - 9 years of experience as data engineer, BI developer, data analyst, technical project manager, program manager etc.\nTechnical project management- drive BRD, project scope, resource allocation, team\ncoordination, stakeholder communication, UAT, Prod fix, change requests, project governance\nSound knowledge of banking industry (payments, retail operations, fraud etc.)\nStrong ETL experience or experienced Teradata developer\nManaging team of business analysts, BI developers, ETL developers to ensure that projects are completed on time\nResponsible for providing thought leadership and technical advice on business issues\nDesign methodological frameworks and solutions.\n\n\nWhat were looking for\n\n\nBachelors/masters degree in computer science/data science/AI/statistics, Certification in Gen AI. Masters degree Preferred.\nManage multiple projects, at a time, from inception to delivery\nSuperior problem-solving, analytical, and quantitative skills\nEntrepreneurial mindset, coupled with a “can do” attitude\nDemonstrated ability to collaborate with cross-functional, cross-border teams and coach / mentor colleagues.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Technical Project Manager', 'Data Engineering', 'multiple projects', 'Technical project management', 'Data Analytics', 'project scope', 'ETL Pipeline', 'team coordination', 'resource allocation', 'Prod fix', 'drive BRD', 'program manager', 'Big data']",2025-06-13 05:17:14
Expert Cloud Data Engineer - (AWS/Python/Pyspark/Kafka/SQL),BMW Techworks India,9 - 13 years,15-27.5 Lacs P.A.,"['Chennai', 'Bengaluru']","Job Description\nRole Expert Cloud Data Engineer\nLocation Pune/Chennai\nExperience: 9 to 12 years and above.\nWhat awaits you/ Job Profile\nYou will lead strategic data engineering initiatives, ensuring that cloud data solutions align with business objectives. You will oversee large-scale projects, mentor teams, and drive innovation in cloud-based data engineering.\nKey Responsibilities:\nArchitect and implement enterprise-level data solutions that are scalable and secure.\nLead cross-functional teams to execute large-scale data projects.\nDevelop data governance strategies and lifecycle management best practices.\nExplore and integrate cutting-edge cloud and AI technologies to enhance data capabilities.\nMentor junior and senior engineers, fostering a culture of technical excellence.\nConduct high-level data analysis and make strategic recommendations.\nDrive cost optimization and performance tuning across cloud-based data platforms.\nWhat should you bring along\n8+ years of experience in data engineering, AWS Cloud Architecture, and DevOps.\nStrong understanding of data security, compliance, and governance in cloud environments.\nAbility to evaluate and implement emerging technologies in data engineering.\nExcellent knowledge of Terraform and GitHub Actions.\nExperienced in being in the lead of a feature team.\nMust have technical skill\nAdvanced Python, PySpark, SQL, Java/Scala (preferred)\nAWS (advanced expertise in Glue, Redshift, Athena, Kinesis, EMR), Cloud Architect Certification\nKafka, Flink, Spark Streaming\nGood to have technical skills\nTerraform, AWS CloudFormation\nGitHub Actions, Jenkins, Kubernetes\nIntegration of ML models into cloud data pipelines\nData Governance & Security: Role-based access control (RBAC), encryption, compliance frameworks",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'Terraform', 'Kafka', 'AWS', 'Python']",2025-06-13 05:17:16
Azure Data Engineer ( Azure Databricks),Apex One,4 - 8 years,Not Disclosed,"['Hyderabad', 'Bengaluru']","Job Summary\nWe are seeking a skilled Azure Data Engineer with 4 years of overall experience, including at least 2 years of hands-on experience with Azure Databricks (Must). The ideal candidate will have strong expertise in building and maintaining scalable data pipelines and working across cloud-based data platforms.\nKey Responsibilities\nDesign, develop, and optimize large-scale data pipelines using Azure Data Factory, Azure Databricks, and Azure Synapse.\nImplement data lake solutions and work with structured and unstructured datasets in Azure Data Lake Storage (ADLS).\nCollaborate with data scientists, analysts, and engineering teams to design and deliver end-to-end data solutions.\nDevelop ETL/ELT processes and integrate data from multiple sources.\nMonitor, debug, and optimize workflows for performance and cost-efficiency.\nEnsure data governance, quality, and security best practices are maintained.\nMust-Have Skills\n4+ years of total experience in data engineering.\n2+ years of experience with Azure Databricks (PySpark, Notebooks, Delta Lake).\nStrong experience with Azure Data Factory, Azure SQL, and ADLS.\nProficient in writing SQL queries and Python/Scala scripting.\nUnderstanding of CI/CD pipelines and version control systems (e.g., Git).\nSolid grasp of data modeling and warehousing concepts.",Industry Type: Management Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure', 'Azure Data Factory', 'SQL queries', 'PySpark', 'Delta Lake', 'Azure Databricks', 'Notebooks', 'Azure SQL']",2025-06-13 05:17:17
Jr Data Engineer,BMW Techworks India,4 - 6 years,Not Disclosed,"['Chennai', 'Bengaluru']","What awaits you/ Job Profile\nAnalyze, Organize data and Build data systems with multiple data sources\nWorking closely with business and market team for the Data analysis.\nWhat should you bring along\nBuild data systems and pipelines\nStandardize the data assets which maximize data reusability\nEvaluate business needs and objectives\nImplementing data pipeline to ingest and cleanse raw data from various source systems including SaaS, Cloud based and On-prep databases\nCombine raw information from different sources\nExplore ways to enhance data quality and reliability\nImplementing functional requirements including incremental loads, data snapshots and identity resolution\nOptimizing data pipelines to improve performance and cost, while ensuring a high quality of data within the data lake:\nMonitoring services and jobs for cost and performance, ensuring continual operations of data pipelines, and fixing of defects.\nConstantly looking for opportunities to optimize data pipelines to improve performance.\nMust have technical skill\nMust have coding skills in Spark/Pyspark, Python and SQL\nMust have Knowledge of AWS tools, Glue, Athena ,step functions and S3\nGood to have technical skills\nGood knowledge of CI/CD (Github / Github Actions, Terraform)\nKnowledge on Agile methodologies.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data analysis', 'github', 'Coding', 'Agile', 'Data quality', 'Cost', 'AWS', 'Monitoring', 'SQL', 'Python']",2025-06-13 05:17:19
Mid-Level Data Engineer,BMW Techworks India,6 - 8 years,Not Disclosed,"['Chennai', 'Bengaluru']","What awaits you/ Job Profile\nProvide estimates for requirements, analyses and develop as per the requirement.\nDeveloping and maintaining data pipelines and ETL (Extract, Transform, Load) processes to extract data efficiently and reliably from various sources, transform it into a usable format, and load it into the appropriate data repositories.\nCreating and maintaining logical and physical data models that align with the organizations data architecture and business needs. This includes defining data schemas, tables, relationships, and indexing strategies for optimal data retrieval and analysis.\nCollaborating with cross-functional teams and stakeholders to ensure data security, privacy, and compliance with regulations.\nCollaborate with downstream application to understand their needs and build the data storage and optimize as per their need.\nWorking closely with other stakeholders and Business to understand data requirements and translate them into technical solutions.\nFamiliar with Agile methodologies and have prior experience working with Agile teams using Scrum/Kanban\nLead Technical discussions with customers to find the best possible solutions.\nProactively identify and implement opportunities to automate tasks and develop reusable frameworks.\nOptimizing data pipelines to improve performance and cost, while ensuring a high quality of data within the data lake.\nMonitoring services and jobs for cost and performance, ensuring continual operations of data pipelines, and fixing of defects.\nConstantly looking for opportunities to optimize data pipelines to improve performance\nWhat should you bring along\nMust Have:\nHand on Expertise of 6- 8 years in AWS services like S3, Lambda, Glue, Athena, RDS, Step functions, SNS, SQS, API Gateway, Security, Access and Role permissions, Logging and monitoring Services.\nGood hand on knowledge on Python, Spark, Hive and Unix, AWS CLI\nPrior experience in working with streaming solution like Kafka\nPrior experience in implementing different file storage types like Delta-lake / Ice-berg.\nExcellent knowledge in Data modeling and Designing ETL pipeline.\nMust have strong knowledge in using different databases such as MySQL, Oracle and Writing complex queries.\nStrong experience working in a continuous integration and Deployment process.\nNice to Have:\nHand on experience in the Terraform, GIT, GIT Actions. CICD pipeline and Amazon Q.\nMust have technical skill\nPyspark, AWS ,SQL, Kafka, Glue, IAM. S3, Lambda, Step Function, Athena\nGood to have Technical skills\nTerraform, GIT, GIT Actions. CICD pipeline , AI",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Unix', 'Data modeling', 'data security', 'MySQL', 'Agile', 'Scrum', 'Oracle', 'Monitoring', 'SQL', 'Python']",2025-06-13 05:17:21
Aws Data Engineer,Hiring for Leading MNC Company,4 - 6 years,Not Disclosed,"['Hyderabad', 'Pune', 'Bengaluru']","Warm Greetings from SP Staffing!!\n\nRole:AWS Data Engineer\nExperience Required :4 to 6 yrs\nWork Location :Bangalore/Pune/Hyderabad/Chennai\n\nRequired Skills,\n\nPyspark\nAWS Glue\n\nInterested candidates can send resumes to nandhini.spstaffing@gmail.com",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'Aws Glue', 'python', 'EMR', 'Aws Emr', 'AWS Data Engineer', 'Aws Lambda', 'Lakehouse', 'spark', 'Data Engineer', 'ETL', 'AWS', 'Athena', 'gateway']",2025-06-13 05:17:23
Gcp Data Engineer,Saama Technologies,3 - 8 years,Not Disclosed,"['Pune', 'Chennai', 'Coimbatore']","We are looking for immediate joiners only.\nPosition: GCP Data Engineer\nWe are seeking a skilled and experienced GCP Data Engineer to join our dynamic team. The ideal candidate will have a strong background in Google Cloud Platform (GCP), BigQuery, Dataform, and data warehouse concepts. Experience with Airflow/Cloud Composer and cloud computing knowledge will be a significant advantage.\nResponsibilities:\n- Designing, developing, and maintaining data pipelines and workflows on the Google Cloud Platform.",,,,"['Pyspark', 'GCP', 'Python', 'SQL', 'Google Cloud Platforms']",2025-06-13 05:17:24
GCP Data Engineer,TVS Next,3 - 5 years,Not Disclosed,['Bengaluru'],"What you’ll be doing:\nAssist in developing machine learning models based on project requirements\nWork with datasets by preprocessing, selecting appropriate data representations, and ensuring data quality.\nPerforming statistical analysis and fine-tuning using test results.\nSupport training and retraining of ML systems as needed.\nHelp build data pipelines for collecting and processing data efficiently.",,,,"['kubernetes', 'pyspark', 'data pipeline', 'sql', 'docker', 'cloud', 'tensorflow', 'java', 'spark', 'gcp', 'pytorch', 'bigquery', 'programming', 'ml', 'cloud sql', 'cd', 'python', 'airflow', 'cloud spanner', 'cloud pubsub', 'application engine', 'machine learning', 'apache flink', 'data engineering', 'dataproc', 'kafka', 'cloud storage', 'terraform', 'bigtable']",2025-06-13 05:17:26
GCP Data Engineer,Swits Digital,4 - 6 years,Not Disclosed,['Bengaluru'],"Job Title: GCP Data Engineer\nLocation: Chennai, Bangalore, Hyderabad\nExperience: 4-6 Years\nJob Summary:\nWe are seeking a GCP Data & Cloud Engineer with strong expertise in Google Cloud Platform services, including BigQuery, Cloud Run, Cloud Storage , and Pub/Sub . The ideal candidate will have deep experience in SQL coding , data pipeline development, and deploying cloud-native solutions.\nKey Responsibilities:\nDesign, implement, and optimize scalable data pipelines and services using GCP\nBuild and manage cloud-native applications deployed via Cloud Run\nDevelop complex and performance-optimized SQL queries for analytics and data transformation\nManage and automate data storage, retrieval, and archival using Cloud Storage\nImplement event-driven architectures using Google Pub/Sub\nWork with large datasets in BigQuery , including ETL/ELT design and query optimization\nEnsure security, monitoring, and compliance of cloud-based systems\nCollaborate with data analysts, engineers, and product teams to deliver end-to-end cloud solutions\nRequired Skills & Experience:\n4 years of experience working with Google Cloud Platform (GCP)\nStrong proficiency in SQL coding , query tuning, and handling complex data transformations\nHands-on experience with:\nBigQuery\nCloud Run\nCloud Storage\nPub/Sub\nUnderstanding of data pipeline and ETL/ELT workflows in cloud environments\nFamiliarity with containerized services and CI/CD pipelines\nExperience in scripting languages (e.g., Python, Shell) is a plus\nStrong analytical and problem-solving skills",Industry Type: Recruitment / Staffing,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['SUB', 'query optimization', 'GCP', 'Analytical', 'Cloud', 'query', 'cloud storage', 'Analytics', 'SQL coding', 'Python']",2025-06-13 05:17:28
Python Data Engineer with AI/ML,Expleo,5 - 6 years,Not Disclosed,['Pune'],"Overview\nWe are looking for a Python Data Engineer with expertise in real-time data monitoring, extraction, transformation, and visualization. The ideal candidate will have experience working with Oracle SQL databases, multithreading, and AI/ML techniques and should be proficient in deploying Python applications on IIS servers . The role involves developing a system to monitor live files and folders, extract data, transform it using various techniques, and display insights on a Plotly Dash-based dashboard .\nResponsibilities",,,,"['Computer science', 'IIS', 'Data analysis', 'Backend', 'Multithreading', 'Debugging', 'Data processing', 'Troubleshooting', 'Python', 'Data extraction']",2025-06-13 05:17:29
Data Engineering Specialist,Overture Rede,10 - 15 years,Not Disclosed,"['Mumbai', 'Hyderabad', 'Gurugram', 'Bengaluru']","Job Title: Sales Excellence COE Data Engineering Specialist\nLocations: Mumbai / Bangalore / Gurgaon / Hyderabad\nExperience: 1012 Years Level: Team Lead / Specialist (Level 9)\n\nJob Role\nLead data engineering efforts to support sales insights through scalable pipelines, statistical modeling, and ML workflows across cloud platforms.\n\nRequired Skills\nProficiency in Python\nAdvanced SQL (Views, Functions, Procedures)\nExperience with Google Cloud Platform (GCP) ML workflow setup\nStrong in Data Modeling and ETL Development\nExcel skills including VBA, Power Pivot, Cube Functions\nSolid understanding of Sales Processes\n\n\n",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Statistical modeling', 'Sales', 'Excel', 'VBA', 'Data modeling', 'GCP', 'Cloud', 'Workflow', 'SQL', 'Python']",2025-06-13 05:17:31
Data Engineer KL-BL,Puresoftware,5 - 12 years,Not Disclosed,['Bengaluru'],"Core Competences Required and Desired Attributes:\nBachelors degree in computer science, Information Technology, or a related field.\nProficiency in Azure Data Factory, Azure Databricks and Unity Catalog, Azure SQL Database, and other Azure data services.\nStrong programming skills in SQL, Python and PySpark languages.\nExperience in the Asset Management domain would be preferable.\nStrong proficiency in data analysis and data modelling, with the ability to extract insights from complex data sets.\nHands-on experience in Power BI, including creating custom visuals, DAX expressions, and data modelling.\nFamiliarity with Azure Analysis Services, data modelling techniques, and optimization.\nExperience with data quality and data governance frameworks with an ability to debug, fine tune and optimise large scale data processing jobs.\nStrong analytical and problem-solving skills, with a keen eye for detail.\nExcellent communication and interpersonal skills, with the ability to work collaboratively in a team environment.\nProactive and self-motivated, with the ability to manage multiple tasks and deliver high-quality results within deadlines.",Industry Type: Management Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data analysis', 'Interpersonal skills', 'Data modeling', 'Analytical', 'data governance', 'Data quality', 'Asset management', 'Information technology', 'SQL', 'Python']",2025-06-13 05:17:33
Data Engineer (AWS),Neoware Technology Solutions,4 - 10 years,Not Disclosed,"['Chennai', 'Bengaluru']","Data Engineer (AWS) - Neoware Technology Solutions Private Limited Data Engineer (AWS)\nRequirements\n4 - 10 years of hands-on experience in designing, developing and implementing data engineering solutions.\nStrong SQL development skills, including performance tuning and query optimization.\nGood understanding of data concepts.\nProficiency in Python and a solid understanding of programming concepts.\nHands-on experience with PySpark or Spark Scala for building data pipelines.\nUnderstanding of streaming data pipelines for near real-time analytics.\nExperience with Azure services including Data Factory, Functions, Databricks, Synapse Analytics, Event Hub, Stream Analytics and Data Lake Storage.\nFamiliarity with at least one NoSQL database.\nKnowledge of modern data architecture patterns and industry trends in data engineering.\nUnderstanding of data governance concepts for data platforms and analytical solutions.\nExperience with Git for managing version control for source code.\nExperience with DevOps processes, including experience implementing CI/CD pipelines for data engineering solutions.\nStrong analytical and problem-solving skills.\nExcellent communication and teamwork skills.\nResponsibilities\nAzure Certifications related to Data Engineering are highly preferred.\nExperience with Amazon AppFlow, EKS, API Gateway, NoSQL database services.\nStrong understanding and experience with BI/visualization tools like Power BI.\nChennai, Bangalore\nFull time\n4+ years\nOther positions Principal Architect (Data and Cloud) Development",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Performance tuning', 'Version control', 'GIT', 'NoSQL', 'query optimization', 'Analytical', 'data governance', 'Analytics', 'Python', 'Data architecture']",2025-06-13 05:17:35
Data Engineer (Azure),Neoware Technology Solutions,4 - 10 years,Not Disclosed,"['Chennai', 'Bengaluru']","Data Engineer (Azure) - Neoware Technology Solutions Private Limited Data Engineer (Azure)\nRequirements\n4 - 10 years of hands-on experience in designing, developing and implementing data engineering solutions.\nStrong SQL development skills, including performance tuning and query optimization.\nGood understanding of data concepts.\nProficiency in Python and a solid understanding of programming concepts.\nHands-on experience with PySpark or Spark Scala for building data pipelines.\nEnsure data consistency and address ambiguities or inconsistencies across datasets.\nUnderstanding of streaming data pipelines for near real-time analytics.\nExperience with Azure services including Data Factory, Functions, Databricks, Synapse Analytics, Event Hub, Stream Analytics and Data Lake Storage.\nFamiliarity with at least one NoSQL database.\nKnowledge of modern data architecture patterns and industry trends in data engineering.\nUnderstanding of data governance concepts for data platforms and analytical solutions.\nExperience with Git for managing version control for source code.\nExperience with DevOps processes, including experience implementing CI/CD pipelines for data engineering solutions.\nStrong analytical and problem-solving skills.\nExcellent communication and teamwork skills.\nResponsibilities\nAzure Certifications related to Data Engineering are highly preferred.\nExperience with Azure Kubernetes Service (AKS), Container Apps and API Management.\nStrong understanding and experience with BI/visualization tools like Power BI.\nChennai, Bangalore\nFull time\n4+ years\nOther positions\nChennai / Bangalore / Mumbai\n3+ years\nPrincipal Architect (Data and Cloud) Development",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Performance tuning', 'Version control', 'GIT', 'query optimization', 'NoSQL', 'Analytical', 'SCALA', 'Analytics', 'Python', 'Data architecture']",2025-06-13 05:17:37
Director - Data Engineering,Blend360 India,10 - 15 years,Not Disclosed,['Bengaluru'],"We are seeking a strategic Director of Data & AI Engineering to lead the growth and evolution of our data engineering function. This role will play a pivotal part in designing scalable platforms, enabling advanced AI and ML applications, and building a world-class engineering team. The ideal candidate is both a seasoned technical leader and a visionary strategist who thrives at the intersection of innovation, execution, and impact. You will collaborate with internal and client teams to develop systems and infrastructure that power intelligent products and data-driven decision-making. You will also build and mentor a high-performing team of Data and AI Engineers, foster a modern data culture, and champion best practices for scalable architecture, AI integration, and engineering excellence.\nResponsibilities:\nSolution Design & Engineering Leadership\nArchitect and build scalable, high-performance data and AI pipelines using tools such as Spark, PySpark, SQL, Python, DBT, Airflow, and cloud-native platforms (AWS, GCP, or Azure).\nLead the design of hybrid/on-prem data platforms, incorporating security, governance, and performance optimization.\nStrategic Client & Stakeholder Engagement\nServe as a trusted technical advisor to internal and external stakeholders.\nTranslate complex business needs into practical engineering solutions and oversee the end-to-end delivery lifecycle.\nAI-Driven Productivity & Innovation\nIntroduce and scale AI-driven tools and practices to accelerate development and enhance data quality, resilience, and maintainability.\nChampion the adoption of generative AI and foundation models to enable intelligent automation and insight generation.\nGrowth & Team Leadership\nBuild, lead, and inspire a diverse team of Data Engineers, ML Engineers, and AI Specialists.\nSet a clear vision and goals, provide mentorship, and cultivate a strong engineering culture and standards.\nPlatform and Data Strategy\nLead initiatives to modernize data infrastructure, improve data discoverability, and support real-time analytics and experimentation.\nCollaborate cross-functionally to shape product data strategies and influence the overall AI roadmap.\nPresales & Business Development Support\nPartner with sales and solution teams to craft compelling proposals, technical solutions, and client presentations.\nRepresent the engineering function in client discussions, workshops, and RFP responses to articulate value and differentiation.\nSupport opportunity scoping, estimation, and roadmap planning for prospective engagements.\n\n\n10+ years of experience in data engineering, AI/ML engineering, or product/platform engineering.\nProven track record of leading high-performing teams and managing senior engineers and managers.\nBachelor s or masters degre",Industry Type: Industrial Automation,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Automation', 'GCP', 'Business Development Manager', 'Social media', 'Data quality', 'RFP', 'Analytics', 'SQL', 'Python']",2025-06-13 05:17:38
Snowflake Data Engineer,Tredence,3 - 8 years,Not Disclosed,['Bengaluru'],"Role & responsibilities\n\nDesign, build, and maintain scalable data pipelines using DBT and Airflow.\nDevelop and optimize SQL queries and data models in Snowflake.\nImplement ETL/ELT workflows, ensuring data quality, performance, and reliability.\nWork with Python for data processing, automation, and integration tasks.\nHandle JSON data structures for data ingestion, transformation, and APIs.\nLeverage AWS services (e.g., S3, Lambda, Glue, Redshift) for cloud-based data solutions. Collaborate with data analysts, engineers, and business teams to deliver high-quality data products.",,,,"['Snowflake', 'DBT', 'SQL']",2025-06-13 05:17:40
"Data Engineer( Python, AWS, Databricks, EKS, Airflow)",Banking,5 - 9 years,Not Disclosed,['Bengaluru'],"Exprence 5-8 Years\nLocation - Bangalore\nMode C2H\n\nHands on data engineering experience.\nHands on experience with Python programming\nHands-on Experience with AWS & EKS\nWorking knowledge of Unix, Databases, SQL\nWorking Knowledge on Databricks\nWorking Knowledge on Airflow and DBT",Industry Type: Investment Banking / Venture Capital / Private Equity,Department: Engineering - Software & QA,"Employment Type: Full Time, Temporary/Contractual","['Airflow', 'Data Engineering', 'AWS', 'Python', 'SQL', 'Databricks', 'Eks']",2025-06-13 05:17:42
Urgent hiring For Cloud Data Engineer,Wowjobs,7 - 10 years,30-45 Lacs P.A.,"['Hyderabad', 'Chennai', 'Bengaluru']","Role & responsibilities\n*  Design, Build, and Maintain ETL Pipelines: Develop robust, scalable, and efficient ETL workflows to ingest, transform, and load data into distributed data products within the Data Mesh architecture.\n*   Data Transformation with dbt: Use dbt to build modular, reusable transformation workflows that align with the principles of Data Products.\n*   Cloud Expertise: Leverage Google Cloud Platform (GCP) services such as BigQuery, Cloud Storage, Pub/Sub, and Dataflow to implement highly scalable data solutions.\n*   Data Quality & Governance: Enforce strict data quality standards by implementing validation checks, anomaly detection mechanisms, and monitoring frameworks.\n*   Performance Optimization: Continuously optimize ETL pipelines for speed, scalability, and cost efficiency.\n*   Collaboration & Ownership: Work closely with data product owners, BI developers, and stakeholders to understand requirements and deliver on expectations. Take full ownership of your deliverables.\n*   Documentation & Standards: Maintain detailed documentation of ETL workflows, enforce coding standards, and adhere to best practices in data engineering.\n*   Troubleshooting & Issue Resolution: Proactively identify bottlenecks or issues in pipelines and resolve them quickly with minimal disruption.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Python coding', 'Cloud data engineer', 'ETL Workflow']",2025-06-13 05:17:44
Gcp Data Engineer,Royal Cyber,9 - 14 years,Not Disclosed,[],"Minimum 7+ years in data engineering with 5+ years of hands-on experience on GCP.\nProven track record with tools and services like BigQuery, Cloud Composer (Apache Airflow), Cloud Functions, Pub/Sub, Cloud Storage, Dataflow, and IAM/VPC.\nDemonstrated expertise in Apache Spark (batch and streaming), PySpark, and building scalable API integrations.\nAdvanced Airflow skills including custom operators, dynamic DAGs, and workflow performance tuning.\nCertifications\nGoogle Cloud Professional Data Engineer certification preferred.\nKey Skills\nMandatory Technical Skills\nAdvanced Python (PySpark, Pandas, pytest) for automation and data pipelines.\nStrong SQL with experience in window functions, CTEs, partitioning, and optimization.\nProficiency in GCP services including BigQuery, Dataflow, Cloud Composer, Cloud Functions, and Cloud Storage.\nHands-on with Apache Airflow, including dynamic DAGs, retries, and SLA enforcement.\nExpertise in API data ingestion, Postman collections, and REST/GraphQL integration workflows.\nFamiliarity with CI/CD workflows using Git, Jenkins, or Bitbucket.\nExperience with infrastructure security and governance using IAM and VPC.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Part Time, Temporary/Contractual","['GCP', 'Bigquery', 'Google Cloud Platforms', 'Cloud Storage', 'Data Flow']",2025-06-13 05:17:45
Aws Data Engineer,Hiring for Leading MNC Company!!,8 - 13 years,Not Disclosed,['Bengaluru'],"Warm Greetings from SP Staffing!!\n\nRole:AWS Data Engineer\nExperience Required :8 to 15 yrs\nWork Location :Bangalore\n\nRequired Skills,\n\nTechnical knowledge of data engineering solutions and practices. Implementation of data pipelines using tools like EMR, AWS Glue, AWS Lambda, AWS Step Functions, API Gateway, Athena\nProficient in Python and Spark, with a focus on ETL data processing and data engineering practices.\n\nInterested candidates can send resumes to nandhini.spstaffing@gmail.com",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineer', 'AWS', 'Pyspark', 'python', 'EMR', 'Aws Glue', 'Aws Emr', 'AWS Data Engineer', 'Aws Lambda', 'Lakehouse', 'spark', 'ETL', 'Athena', 'gateway']",2025-06-13 05:17:47
Data Engineer-Having Stratup-Mid-Size company Exp.@ Bangalore_Urgent,"As a leader in this space, we deliver wo...",8 - 13 years,Not Disclosed,['Bengaluru'],"Data Engineer\n\nLocation: Bangalore - Onsite\nExperience: 8 - 15 years\nType: Full-time\n\nRole Overview\n\nWe are seeking an experienced Data Engineer to build and maintain scalable, high-performance data pipelines and infrastructure for our next-generation data platform. The platform ingests and processes real-time and historical data from diverse industrial sources such as airport systems, sensors, cameras, and APIs. You will work closely with AI/ML engineers, data scientists, and DevOps to enable reliable analytics, forecasting, and anomaly detection use cases.\nKey Responsibilities\nDesign and implement real-time (Kafka, Spark/Flink) and batch (Airflow, Spark) pipelines for high-throughput data ingestion, processing, and transformation.\nDevelop data models and manage data lakes and warehouses (Delta Lake, Iceberg, etc) to support both analytical and ML workloads.\nIntegrate data from diverse sources: IoT sensors, databases (SQL/NoSQL), REST APIs, and flat files.\nEnsure pipeline scalability, observability, and data quality through monitoring, alerting, validation, and lineage tracking.\nCollaborate with AI/ML teams to provision clean and ML-ready datasets for training and inference.\nDeploy, optimize, and manage pipelines and data infrastructure across on-premise and hybrid environments.\nParticipate in architectural decisions to ensure resilient, cost-effective, and secure data flows.\nContribute to infrastructure-as-code and automation for data deployment using Terraform, Ansible, or similar tools.\n\n\nQualifications & Required Skills\n\nBachelors or Master’s in Computer Science, Engineering, or related field.\n6+ years in data engineering roles, with at least 2 years handling real-time or streaming pipelines.\nStrong programming skills in Python/Java and SQL.\nExperience with Apache Kafka, Apache Spark, or Apache Flink for real-time and batch processing.\nHands-on with Airflow, dbt, or other orchestration tools.\nFamiliarity with data modeling (OLAP/OLTP), schema evolution, and format handling (Parquet, Avro, ORC).\nExperience with hybrid/on-prem and cloud platforms (AWS/GCP/Azure) deployments.\nProficient in working with data lakes/warehouses like Snowflake, BigQuery, Redshift, or Delta Lake.\nKnowledge of DevOps practices, Docker/Kubernetes, Terraform or Ansible.\nExposure to data observability, data cataloging, and quality tools (e.g., Great Expectations, OpenMetadata).\nGood-to-Have\nExperience with time-series databases (e.g., InfluxDB, TimescaleDB) and sensor data.\nPrior experience in domains such as aviation, manufacturing, or logistics is a plus.\n\nRole & responsibilities\n\n\nPreferred candidate profile",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['aviation', 'Data Modeling', 'Python', 'OLAP', 'Cloud', 'ORC', 'logistics', 'Avro', 'Terraform', 'Snowflake', 'manufacturing', 'AWS', 'Parquet', 'Java', 'Azure', 'BigQuery', 'Data', 'Redshift', 'SQL', 'TimescaleDB', 'GCP', 'InfluxDB', 'dbt', 'Ansible', 'OLTP', 'Kubernetes']",2025-06-13 05:17:49
Azure Data Engineer/Lead/Architect (5 - 20 Years) (Pan India Location),Allegis Group,5 - 10 years,Not Disclosed,[],"Azure Data Engineer/Lead/Architect (5 - 20 Years) (Pan India Location)\nJob Location : Hyderabad / Bangalore / Chennai / Kolkata / Noida/ Gurgaon / Pune / Indore / Mumbai\n\n\n5 -20 years of relevant hands on development experience. And 4+ years as Azure Data Engineering role\nProficient in Azure technologies like ADB, ADF, SQL(capability of writing complex SQL queries), ADB, PySpark, Python, Synapse, Delta Tables, Unity Catalog\nHands on in Python, PySpark or Spark SQL\nHands on in Azure Analytics and DevOps\nTaking part in Proof of Concepts (POCs) and pilot solutions preparation\nAbility to conduct data profiling, cataloguing, and mapping for technical design and construction of technical data flows\nExperience in business processing mapping of data and analytics solutions",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Analytics', 'Azure Data Engineering', 'Azure Databricks', 'Devops', 'Python', 'Azure Data Factory', 'Pyspark', 'Azure', 'Adb']",2025-06-13 05:17:51
Senior Data Engineer,Binary Infoways,5 - 9 years,12-19.2 Lacs P.A.,['Hyderabad'],"Responsibilities:\n* Design, develop & maintain data pipelines using Airflow, Python & SQL.\n* Optimize performance through Spark & Splunk analytics.\n* Collaborate with cross-functional teams on big data initiatives.\n* AWS",Industry Type: BPM / BPO,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Airflow', 'Big Data Technologies', 'ETL', 'AWS', 'Python', 'Glue', 'Snowflake', 'Spark', 'Splunk', 'SQL']",2025-06-13 05:17:53
Senior Data Engineer,Dfcs Technologies,8 - 13 years,Not Disclosed,['Chennai'],"Architect & Build Scalable Systems: Design and implement a petabyte-scale lakehouse\nArchitectures to unify data lakes and warehouses. Real-Time Data Engineering: Develop and optimize streaming pipelines using Kafka, Pulsar, and Flink.\n\nRequired Candidate profile\nData engineering experience with large-scale systems• Expert proficiency in Java for data-intensive applications. Handson experience with lakehouse architectures, stream processing, & event streaming",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Java', 'Terraform', 'Delta Lake', 'Data Engineer', 'Streaming', 'Streaming Framework', 'Pyspark', 'Apache Iceberg', 'lakehouse', 'Apache Flink', 'Kafka', 'Spark Streaming', 'Stream Processing', 'Spark Core', 'Apache Storm', 'Apache Pulsar', 'Kafta', 'Data Pipeline', 'Streaming Kafka', 'Clickhouse', 'Spark', 'AWS', 'Kafka Streams', 'Streams']",2025-06-13 05:17:55
Sr Data Engineer - Fully Remote & Immediate Opportunity,Zealogics.com,10 - 15 years,Not Disclosed,[],"10 yrs of exp working in cloud-native data (Azure Preferred),Databricks, SQL,PySpark, migrating from Hive Metastore to Unity Catalog, Unity Catalog, implementing Row-Level Security (RLS), metadata-driven ETL design patterns,Databricks certifications",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'Azure', 'Metadata', 'Data Bricks', 'Unity Catalog', 'ETL design', 'SQL']",2025-06-13 05:17:57
Senior Data Engineer,Conviction HR,8 - 10 years,Not Disclosed,"['Kolkata', 'Hyderabad', 'Pune( Malad )']","Must have -Azure Data Factory (Mandatory). Azure Databricks, Pyspark and Python and advance SQL Azure eco-system. 1) Advanced SQL Skills. 2)Data Analysis. 3) Data Models. 4) Python (Desired). 5) Automation - Experience required : 8 to 10 years.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Data Factory', 'Data Engineering', 'Python', 'Azure Databricks', 'Data Modeling', 'Data Bricks', 'SQL']",2025-06-13 05:17:58
Azure Data Engineer,Jurist Associates,5 - 10 years,7-12 Lacs P.A.,"['Kochi', 'Hyderabad', 'Bengaluru']","Design, build, and maintain scalable and efficient data pipelines using Azure services such as Azure Data Factory (ADF), Azure Databricks, and Azure Synapse Analytics. Develop and optimize ETL/ELT workflows for ingestion, cleansing, transformation,\n\nRequired Candidate profile\nStrong understanding of data warehouse architecture, data lakes, and big data frameworks. Candidates who have atleast 5 years of experience should only apply.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Data Engineering', 'Azure Data Factory', 'GenAI Tools', 'Azure Data Warehouse', 'Azure data lake Gen2', 'Azure Synapse Analytics', 'Azure Databricks', 'Azure Data Lake', 'Nosql Databases', 'Data Modeling', 'Semantic Analytics', 'ML/DL Models']",2025-06-13 05:18:00
Data Engineer - Financial Analytics Specialist,RWS Group,5 - 8 years,Not Disclosed,['Bengaluru'],"Job Purpose -\nWe are seeking a Data Engineer Financial Analytics Specialist to join our Data and Analytics team at RWS. This role combines advanced technical skills in SQL and ETL with analytical thinking and financial business acumen. You will be instrumental in designing and implementing data transformations and pipelines to support complex business requirements, particularly in financial data systems. This is a hands-on technical role with a significant focus on problem-solving, mathematical reasoning, and creating solutions for intricate business processes.\nThis position is ideal for someone with a quantitative background in computer science, mathematics, or a related field, who thrives on developing innovative solutions, optimizing data workflows, and ensuring data accuracy. We are looking for a candidate who is self-motivated, detail-oriented, and possesses exceptional technical skills, alongside a strong sense of ownership and accountability. The primary focus of this role will be financial data projects within a collaborative and dynamic team environment.\n\nAbout Group Technology-\nGroup Technology enables the organization to achieve its strategic direction whilst driving shareholder value. The division establishes common standards and IT governance across the business. It further develops and manages core applications enabling smooth operational running of the organization across all functions. We drive and deliver future roadmaps aligned to the overall strategic direction of the business. Group Technology support services to over 7500 end users across the globe, manage the information security operation and safeguard all our assets. Our core Group Technology functions include Technical Architecture, Network & Voice, IT Security, Service Delivery, Solutions Delivery and Asset Management. Group Technology has a global presence across all regions with over 400 staff.\nOur Data Engineering team is responsible for developing, building, maintaining, and managing data pipelines. This requires working with large datasets, databases, and the software used to analyse them including cloud systems like AWS or Azure.\n\nJob Overview\nKey Responsibilities\nWe are seeking a Data Engineer Financial Analytics Specialist to join our Data and Analytics team at RWS. This role combines advanced technical skills in SQL and ETL with analytical thinking and financial business acumen. You will be instrumental in designing and implementing data transformations and pipelines to support complex business requirements, particularly in financial data systems. This is a hands-on technical role with a significant focus on problem-solving, mathematical reasoning, and creating solutions for intricate business processes.\nThis position is ideal for someone with a quantitative background in computer science, mathematics, or a related field, who thrives on developing innovative solutions, optimizing data workflows, and ensuring data accuracy. We are looking for a candidate who is self-motivated, detail-oriented, and possesses exceptional technical skills, alongside a strong sense of ownership and accountability. The primary focus of this role will be financial data projects within a collaborative and dynamic team environment.\nKey Responsibilities\nWrite advanced T-SQL queries, stored procedures, and functions to handle complex data transformations, ensuring optimal performance and scalability.\nPerform query performance optimization, including indexing strategies and tuning for large datasets.\nDesign, develop, and maintain ETL/ELT pipelines using tools like SSIS, ADF, or equivalent, ensuring seamless data integration and transformation.\nBuild and automate data workflows to support accurate and efficient data operations.\nCollaborate with stakeholders to understand financial business processes and requirements.\nDevelop tailored data solutions to address challenges such as cost allocation, weighted distributions, and revenue recognition using mathematical and logical reasoning.\nConduct root cause analysis for data issues, resolving them swiftly to ensure data accuracy and reliability.\nApply critical thinking and problem-solving to transform business logic into actionable data workflows.\nPartner with cross-functional teams to ensure data solutions align with business objectives.\nDocument data workflows, pipelines, and processes to support ongoing maintenance and scalability.\nRequired Skills and Experiences\n\nMinimum 5+ years of experience require in the similar role or same skillset\nAdvanced proficiency in T-SQL and SQL Server, including tools like SSMS, SQL Profiler, and SQL Agent.\nProven experience developing and optimizing complex queries, stored procedures, and ETL pipelines.\nProficiency with SSIS for data integration and transformation.\nStrong critical thinking and mathematical reasoning skills to design efficient solutions for business problems.\nDemonstrated ability to handle complex financial data challenges, including proportional distributions and other mathematical transformations.\nSolid understanding of financial concepts and processes, enabling seamless translation of business needs into data solutions.\nAbility to work effectively with business stakeholders, translating requirements into technical solutions.\nExcellent interpersonal and communication skills, fostering collaboration across teams.\nFamiliarity with Python, C#, or similar scripting languages for backend integration.\nExperience with cloud ETL tools such as Azure Data Factory (ADF), Fivetran, or Airbyte.\nKnowledge of data governance and compliance principles.\n\nShift Timings-\nThere will be UK shift timings - 1:30 PM 9:30 PM (IST)\n\nRWS Values -\nGet the 3Ps right Partner, Pioneer, Progress and well Deliver together as One RWS.\nFor further information, please visit: RWS\nRWS embraces DEI and promotes equal opportunity, we are an Equal Opportunity Employer and prohibit discrimination and harassment of any kind. RWS is committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment. All employment decisions at RWS are based on business needs, job requirements and individual qualifications, without regard to race, religion, nationality, ethnicity, sex, age, disability, or sexual orientation. RWS will not tolerate discrimination based on any of these characteristics\nRecruitment Agencies: RWS Holdings PLC does not accept agency resumes. Please do not forward any unsolicited resumes to any RWS employees. Any unsolicited resume received will be treated as the property of RWS and Terms & Conditions associated with the use of such resume will be considered null and void.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['T-SQL', 'Analytical Skills', 'Finance', 'SSIS', 'SQL', 'Critical Thinking', 'Azure', 'Analytical Ability', 'SSRS', 'ETL']",2025-06-13 05:18:02
Associate Data Engineer,Amgen Inc,0 - 3 years,Not Disclosed,['Hyderabad'],"What you will do\nIn this vital role We are seeking a Associate Data Engineer to design, build, and maintain scalable data solutions that drive business insights. You will work with large datasets, cloud platforms (AWS preferred), and big data technologies to develop ETL pipelines, ensure data quality, and support data governance initiatives.\nDevelop and maintain data pipelines, ETL/ELT processes, and data integration solutions.\nDesign and implement data models, data dictionaries, and documentation for accuracy and consistency.",,,,"['Data Engineering', 'data analysis', 'data modeling', 'data warehousing', 'data visualization', 'Databricks', 'ETL', 'AWS', 'SQL', 'Apache Spark', 'Python']",2025-06-13 05:18:04
Associate Data Engineer,Amgen Inc,0 - 2 years,Not Disclosed,['Hyderabad'],"Role Description:\nWe are looking for an Associate Data Engineer with deep expertise in writing data pipelines to build scalable, high-performance data solutions. The ideal candidate will be responsible for developing, optimizing and maintaining complex data pipelines, integration frameworks, and metadata-driven architectures that enable seamless access and analytics. This role prefers deep understanding of the big data processing, distributed computing, data modeling, and governance frameworks to support self-service analytics, AI-driven insights, and enterprise-wide data management.",,,,"['Data Engineering', 'Maven', 'test automation', 'data engineering lifecycle', 'Scaled Agile methodologies', 'JIRA', 'SQL', 'Apache Spark', 'Jenkins', 'Agile DevOps tools', 'ETL platform', 'Confluence', 'Scaled Agile', 'Delta Lake', 'Agile', 'Databricks', 'AWS', 'Python']",2025-06-13 05:18:06
Software Engineer (Java + Big Data),Impetus Technologies,5 - 7 years,Not Disclosed,['Chennai'],"Job: Software Developer (Java + Big Data)\nLocation: Indore\nYears of experience: 5-7 years\n\nRequisition Description\n1. Problem solving and analytical skills\n2. Good verbal and written communication skills\n\nRoles and Responsibilities\n\n1. Design and develop high performance, scale-able applications with Java + Bigdata  as minimum required skill .\nJava, Microservices , Spring boot, API ,Bigdata-Hive, Spark, Pyspark\n2. Build and maintain efficient data pipelines to process large volumes of structured and unstructured data.\n3. Develop micro-services, API and distributed systems\n4. Worked on Spark, HDFS, CEPh, Solr/Elastic search, Kafka, Deltalake\n5. Mentor and Guide junior members",Industry Type: IT Services & Consulting,Department: Project & Program Management,"Employment Type: Full Time, Permanent","['Java', 'Hive', 'Big Data', 'Spring Boot', 'Microservices', 'Hdfs', 'Spark Streaming']",2025-06-13 05:18:08
"Data Engineer - Snowflake, Azure Data Factory (ADF)",Suzva Software Technologies,0 - 1 years,Not Disclosed,['Mumbai'],"We are seeking an experienced Data Engineer to join our team for a 6-month contract assignment. The ideal candidate will work on data warehouse development, ETL pipelines, and analytics enablement using Snowflake, Azure Data Factory (ADF), dbt, and other tools.\n\nThis role requires strong hands-on experience with data integration platforms, documentation, and pipeline optimizationespecially in cloud environments such as Azure and AWS.\n\n#KeyResponsibilities\nBuild and maintain ETL pipelines using Fivetran, dbt, and Azure Data Factory\n\nMonitor and support production ETL jobs\n\nDevelop and maintain data lineage documentation for all systems\n\nDesign data mapping and documentation to aid QA/UAT testing\n\nEvaluate and recommend modern data integration tools\n\nOptimize shared data workflows and batch schedules\n\nCollaborate with Data Quality Analysts to ensure accuracy and integrity of data flows\n\nParticipate in performance tuning and improvement recommendations\n\nSupport BI/MDM initiatives including Data Vault and Data Lakes\n\n#RequiredSkills\n7+ years of experience in data engineering roles\n\nStrong command of SQL, with 5+ years of hands-on development\n\nDeep experience with Snowflake, Azure Data Factory, dbt\n\nStrong background with ETL tools (Informatica, Talend, ADF, dbt, etc.)\n\nBachelor's in CS, Engineering, Math, or related field\n\nExperience in healthcare domain (working with PHI/PII data)\n\nFamiliarity with scripting/programming (Python, Perl, Java, Linux-based environments)\n\nExcellent communication and documentation skills\n\nExperience with BI tools like Power BI, Cognos, etc.\n\nOrganized, self-starter with strong time-management and critical thinking abilities\n\n#NiceToHave\nExperience with Data Lakes and Data Vaults\n\nQA & UAT alignment with clear development documentation\nMulti-cloud experience (especially Azure, AWS)",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Java', 'Azure', 'Power BI', 'UAT', 'Perl', 'QA', 'Azure Data Factory', 'Linux', 'Cognos', 'Snowflake', 'ETL', 'AWS', 'Python']",2025-06-13 05:18:09
Lead Security Engineer (Data Protection),Flipkart,7 - 11 years,Not Disclosed,['Bengaluru'],"Skills Required :\n7+ years of experience in the field of information security. Strong technical expertise in DLP and data classification methodologies\nDesirable Skills :\nWork done in AI, automations",Industry Type: Courier / Logistics,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['AI', 'automations', 'Data Protection', 'information security', 'DLP']",2025-06-13 05:18:11
Data Platform Engineer,Accenture,7 - 12 years,Not Disclosed,['Bengaluru'],"Project Role :Data Platform Engineer\n\n\n\n\n\nProject Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification : Engineering graduate preferably Computer Science graduate 15 years of full time education\n\n\nSummary:As a Data Platform Engineer, you will be responsible for assisting with the blueprint and design of the data platform components using Databricks Unified Data Analytics Platform. Your typical day will involve collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\nRoles & Responsibilities:- Assist with the blueprint and design of the data platform components using Databricks Unified Data Analytics Platform.- Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.- Develop and maintain data pipelines and ETL processes using Databricks Unified Data Analytics Platform.- Design and implement data security and access controls for the data platform.- Troubleshoot and resolve issues related to the data platform and data pipelines.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nExperience with Databricks Unified Data Analytics Platform.- Must To Have\n\n\n\n\nSkills:\nStrong understanding of data modeling and database design principles.- Good To Have\n\n\n\n\nSkills:\nExperience with cloud-based data platforms such as AWS or Azure.- Good To Have\n\n\n\n\nSkills:\nExperience with data security and access controls.- Good To Have\n\n\n\n\nSkills:\nExperience with data visualization tools such as Tableau or Power BI.\nAdditional Information:- The candidate should have a minimum of 7.5 years of experience in Databricks Unified Data Analytics Platform.- The ideal candidate will possess a strong educational background in computer science or a related field, along with a proven track record of delivering impactful data-driven solutions.- This position is based at our Bangalore, Hyderabad, Chennai and Pune Offices.- Mandatory office (RTO) for 2- 3 days and have to work on 2 shifts (Shift A- 10:00am to 8:00pm IST and Shift B - 12:30pm to 10:30 pm IST)\n\nQualification\n\nEngineering graduate preferably Computer Science graduate 15 years of full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['database design', 'data modeling', 'data analytics', 'microsoft azure', 'design principles', 'hive', 'sql', 'java', 'spark', 'design patterns', 'oops', 'mysql', 'hadoop', 'etl', 'big data', 'c#', 'rest', 'python', 'data security', 'power bi', 'javascript', 'sql server', 'data bricks', 'tableau', 'kafka', 'sqoop', 'aws']",2025-06-13 05:18:13
Data Platform Engineer,Accenture,7 - 12 years,Not Disclosed,['Bengaluru'],"Project Role :Data Platform Engineer\n\n\n\n\n\nProject Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Platform Engineer, you will assist with the data platform blueprint and design, collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. You will play a crucial role in shaping the data platform components.\nRoles & Responsibilities:- Expected to be an SME, collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Lead the implementation of data platform solutions.- Conduct performance tuning and optimization of data platform components.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Strong understanding of cloud-based data platforms.- Experience in designing and implementing data pipelines.- Knowledge of data governance and security best practices.\nAdditional Information:- The candidate should have a minimum of 7.5 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Bengaluru office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['hive', 'data analytics', 'data modeling', 'spark', 'data governance', 'python', 'amazon redshift', 'data warehousing', 'microsoft azure', 'emr', 'machine learning', 'sql', 'nosql', 'amazon ec2', 'java', 'kafka', 'mysql', 'hadoop', 'sqoop', 'big data', 'aws', 'etl']",2025-06-13 05:18:15
Data Platform Engineer,Accenture,12 - 15 years,Not Disclosed,['Bengaluru'],"Project Role :Data Platform Engineer\n\n\n\n\n\nProject Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\n\n\n\nMust have skills :Collibra Data Governance\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n12 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Platform Engineer, you will assist with the data platform blueprint and design, encompassing the relevant data platform components. Your typical day will involve collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models, while also engaging in discussions to refine and enhance the overall data architecture. You will be involved in various stages of the data platform lifecycle, ensuring that all components work harmoniously to support the organization's data needs and objectives.\nRoles & Responsibilities:- Expected to be an SME.- Collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Expected to provide solutions to problems that apply across multiple teams.- Facilitate knowledge sharing sessions to enhance team capabilities and foster a culture of continuous improvement.- Monitor and evaluate team performance, providing constructive feedback to ensure alignment with project goals.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Collibra Data Governance.- Strong understanding of data governance frameworks and best practices.- Experience with data integration tools and techniques.- Familiarity with data modeling concepts and methodologies.- Ability to analyze and interpret complex data sets to inform decision-making.\nAdditional Information:- The candidate should have minimum 12 years of experience in Collibra Data Governance.- This position is based at our Bengaluru office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'data architecture', 'sql', 'data modeling', 'data governance', 'data analysis', 'oracle', 'data management', 'data warehousing', 'business analysis', 'machine learning', 'business intelligence', 'javascript', 'sql server', 'data quality', 'tableau', 'java', 'html', 'mysql', 'etl', 'informatica']",2025-06-13 05:18:17
Data Platform Engineer,Accenture,3 - 8 years,Not Disclosed,['Bengaluru'],"Project Role :Data Platform Engineer\n\n\n\n\n\nProject Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Platform Engineer, you will assist with the data platform blueprint and design, collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. You will play a crucial role in shaping the data platform components.\nRoles & Responsibilities:- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Collaborate with Integration Architects and Data Architects to design and implement data platform components.- Ensure seamless integration between various systems and data models.- Develop and maintain data platform blueprints.- Optimize data platform performance and scalability.- Provide technical guidance and support to team members.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Strong understanding of data platform architecture and design principles.- Experience with cloud-based data platforms like AWS or Azure.- Hands-on experience with data integration tools and technologies.- Knowledge of data governance and security best practices.\nAdditional Information:- The candidate should have a minimum of 3 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Bengaluru office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analytics', 'microsoft azure', 'platform architecture', 'design principles', 'aws', 'kubernetes', 'c++', 'oracle', 'enterprise architecture', 'microservices', 'docker', 'infrastructure architecture', 'java', 'data modeling', 'gcp', 'design patterns', 'data governance', 'agile', 'hadoop']",2025-06-13 05:18:18
Hadoop & UNIX Shell Scripting Engineer (Data Management),Synechron,5 - 10 years,Not Disclosed,['Bengaluru'],"Job Summary\nSynechron is seeking a dedicated and technically skilled Hadoop Shell Scripting Engineer to manage and optimize our Hadoop ecosystem. The role involves developing automation utilities, troubleshooting complex issues, and collaborating with vendors for platform enhancements. Your expertise will directly support enterprise data processing, performance tuning, and cloud migration initiatives, ensuring reliable and efficient data infrastructure that aligns with organizational goals.\nSoftware Requirements\nRequired Skills:\nStrong proficiency in UNIX Shell scripting with hands-on experience in developing automation utilities\nIn-depth understanding of Hadoop architecture and ecosystem components (HDFS, Hive, Spark)\nExperience with SQL querying and database systems\nFamiliarity with Git and enterprise version control practices\nWorking knowledge of DevOps and CI/CD tools and processes\nPreferred Skills:\nExperience with Python scripting for automation and utility development\nKnowledge of Java programming language\nFamiliarity with cloud platforms (AWS, Azure, GCP) related to Hadoop ecosystem support\nExposure to Hadoop vendor support and collaboration processes (e.g., Cloudera)\nOverall Responsibilities\nDevelop, maintain, and enhance scripts and utilities to automate Hadoop cluster management and data processing tasks\nServe as the Level 3 point of contact for issues related to Hadoop and Spark platforms\nPerform performance tuning and capacity planning to support enterprise data workloads\nConduct proof-of-concept tests for emerging technologies and evaluate their suitability for cloud migration projects\nCollaborate with vendor support teams and internal stakeholders for issue resolution, feature requests, and platform improvements\nReview and validate all changes going into production to ensure stability and performance\nContinuously analyze process inefficiencies and develop new automation utilities to enhance productivity\nAssist in capacity management and performance monitoring activities\nTechnical Skills (By Category)\nProgramming Languages:\nEssential: UNIX Shell scripting\nPreferred: Python, Java\nDatabases & Data Management:\nEssential: Knowledge of SQL querying and database systems\nPreferred: Experience with Hive, HDFS\nCloud Technologies:\nPreferred: Basic familiarity with cloud platforms for Hadoop ecosystem support and migration\nFrameworks & Libraries:\nNot specifically applicable; focus on scripting and platform tools\nDevelopment Tools & Methodologies:\nEssential: Git, version control, DevOps practices, CI/CD pipelines\nPreferred: Automation frameworks, monitoring tools\nSecurity Protocols:\nNot explicitly specified but familiarity with secure scripting and data access controls is advantageous\nExperience Requirements\nMinimum of 5+ years of hands-on experience working with Hadoop clusters and scripting in UNIX shell\nProven experience in managing enterprise Hadoop/Spark environments\nExperience in performance tuning, capacity planning, and utility development\nExposure to cloud migrations or proof-of-concept evaluations is a plus\nBackground in data engineering or platform support roles preferred\nDay-to-Day Activities\nDevelop and enhance UNIX shell scripts for Hadoop automation and utility management\nTroubleshoot and resolve complex platform issues as the Level 3 point of contact\nWork with application teams to optimize queries and data workflows\nEngage with vendor support teams for platform issues and feature requests\nPerform system performance reviews, capacity assessments, and tuning activities\nLead initiatives for process automation, efficiency improvement, and new technology evaluations\nDocument procedures, scripts, and platform configurations\nParticipate in team meetings, provide technical feedback, and collaborate across teams on platform health\nQualifications\nEducational Requirements:\nBachelor's degree in Computer Science, Information Technology, or related field\nEquivalent professional experience in data engineering, platform support, or Hadoop administration\nCertifications (Preferred):\nCertificates in Hadoop ecosystem, Linux scripting, or cloud platform certifications\nTraining & Professional Development:\nOngoing learning related to big data platforms, automation, and cloud migration\nProfessional Competencies\nStrong analytical and troubleshooting skills\nExcellent written and verbal communication skills\nProven ability to work independently with minimal supervision\nCollaborative team player with a positive attitude\nAbility to prioritize tasks effectively and resolve issues swiftly\nAdaptability to evolving technologies and environments\nFocus on quality, security, and process improvement",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Hadoop', 'Java', 'Automation', 'DevOps', 'Data Management', 'CI/CD', 'Performance Tuning', 'UNIX Shell Scripting', 'Python', 'SQL']",2025-06-13 05:18:20
Data Platform Engineer,Accenture,5 - 10 years,Not Disclosed,['Pune'],"Project Role :Data Platform Engineer\n\n\n\n\n\nProject Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Platform Engineer, you will assist with the data platform blueprint and design, collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. You will play a crucial role in the development and maintenance of the data platform components, contributing to the overall success of the project.\nRoles & Responsibilities:- Expected to be an SME, collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Assist with the data platform blueprint and design.- Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.- Develop and maintain data platform components.- Contribute to the overall success of the project.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Strong understanding of statistical analysis and machine learning algorithms.- Experience with data visualization tools such as Tableau or Power BI.- Hands-on implementing various machine learning algorithms such as linear regression, logistic regression, decision trees, and clustering algorithms.- Solid grasp of data munging techniques, including data cleaning, transformation, and normalization to ensure data quality and integrity.\nAdditional Information:- The candidate should have a minimum of 5 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Bengaluru office.- 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analytics', 'tableau', 'machine learning algorithms', 'statistics', 'data munging', 'python', 'natural language processing', 'power bi', 'machine learning', 'sql', 'data bricks', 'data quality', 'r', 'data modeling', 'data science', 'predictive modeling', 'text mining', 'logistic regression']",2025-06-13 05:18:22
Data Platform Engineer,Accenture,7 - 12 years,Not Disclosed,['Pune'],"Project Role :Data Platform Engineer\n\n\n\n\n\nProject Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Platform Engineer, you will assist with the data platform blueprint and design, collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. You will play a crucial role in the development and maintenance of the data platform components, contributing to the overall success of the project.\nRoles & Responsibilities:- Expected to be an SME, collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Assist with the data platform blueprint and design.- Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.- Develop and maintain data platform components.- Contribute to the overall success of the project.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Strong understanding of statistical analysis and machine learning algorithms.- Experience with data visualization tools such as Tableau or Power BI.- Hands-on implementing various machine learning algorithms such as linear regression, logistic regression, decision trees, and clustering algorithms.- Solid grasp of data munging techniques, including data cleaning, transformation, and normalization to ensure data quality and integrity.\nAdditional Information:- The candidate should have a minimum of 7.5 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Bengaluru office.- 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analytics', 'tableau', 'machine learning algorithms', 'statistics', 'data munging', 'python', 'natural language processing', 'power bi', 'machine learning', 'sql', 'data bricks', 'data quality', 'r', 'data modeling', 'data science', 'predictive modeling', 'text mining', 'logistic regression']",2025-06-13 05:18:24
Data Platform Engineer,Accenture,5 - 10 years,Not Disclosed,['Pune'],"Project Role :Data Platform Engineer\n\n\n\n\n\nProject Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :A Engineering graduate preferably Computer Science graduate 15 years of full time education\n\n\nSummary:Overall 7+ years of experience In Industry including 4 Years of experience As Developer using Big Data Technologies like Databricks/Spark and Hadoop Ecosystems - Hands on experience on Unified Data Analytics with Databricks, Databricks Workspace User Interface, Managing Databricks Notebooks, Delta Lake with Python, Delta Lake with Spark SQL - Good understanding of Spark Architecture with Databricks, Structured Streaming.\nSetting Up cloud platform with Databricks, Databricks Workspace- Working knowledge on distributed processing, data warehouse concepts, NoSQL, huge amount of data processing, RDBMS, Testing, Data management principles, Data mining and Data modellingAs a Data Platform Engineer, you will be responsible for assisting with the blueprint and design of the data platform components using Databricks Unified Data Analytics Platform. Your typical day will involve collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\nRoles & Responsibilities:- Assist with the blueprint and design of the data platform components using Databricks Unified Data Analytics Platform.- Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.- Develop and maintain data pipelines using Databricks Unified Data Analytics Platform.- Troubleshoot and resolve issues related to data pipelines and data platform components.- Ensure data quality and integrity by implementing data validation and testing procedures.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nExperience with Databricks Unified Data Analytics Platform.- Must To Have\n\n\n\n\nSkills:\nStrong understanding of data modeling and database design principles.- Good To Have\n\n\n\n\nSkills:\nExperience with Apache Spark and Hadoop.- Good To Have\n\n\n\n\nSkills:\nExperience with cloud-based data platforms such as AWS or Azure.- Proficiency in programming languages such as Python or Java.- Experience with data integration and ETL tools such as Apache NiFi or Talend.\nAdditional Information:- The candidate should have a minimum of 5 years of experience in Databricks Unified Data Analytics Platform.- The ideal candidate will possess a strong educational background in computer science, software engineering, or a related field, along with a proven track record of delivering impactful data-driven solutions.- This position is based at our Chennai, Bengaluru, Hyderabad and Pune office.\n\nQualification\n\nA Engineering graduate preferably Computer Science graduate 15 years of full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data analytics', 'database design', 'data modeling', 'software engineering', 'design principles', 'python', 'rdbms', 'data management', 'talend', 'microsoft azure', 'nosql', 'spark programming', 'data bricks', 'data quality', 'java', 'apache nifi', 'spark', 'hadoop', 'big data', 'aws', 'etl', 'data integration']",2025-06-13 05:18:26
Data Platform Engineer,Accenture,7 - 12 years,Not Disclosed,['Bengaluru'],"Project Role :Data Platform Engineer\n\n\n\n\n\nProject Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Platform Engineer, you will assist with the data platform blueprint and design, collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. You will play a crucial role in the development and maintenance of the data platform components, contributing to the overall success of the project.\nRoles & Responsibilities:- Expected to be an SME, collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Assist with the data platform blueprint and design.- Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.- Develop and maintain data platform components.- Contribute to the overall success of the project.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Strong understanding of statistical analysis and machine learning algorithms.- Experience with data visualization tools such as Tableau or Power BI.- Hands-on implementing various machine learning algorithms such as linear regression, logistic regression, decision trees, and clustering algorithms.- Solid grasp of data munging techniques, including data cleaning, transformation, and normalization to ensure data quality and integrity.\nAdditional Information:- The candidate should have a minimum of 7.5 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Bengaluru office.- 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analytics', 'tableau', 'machine learning algorithms', 'statistics', 'data munging', 'python', 'natural language processing', 'power bi', 'machine learning', 'sql', 'data bricks', 'data quality', 'r', 'data modeling', 'data science', 'predictive modeling', 'text mining', 'logistic regression']",2025-06-13 05:18:28
Specialist Data/AI Engineering,ATT Communication Services,4 - 9 years,Not Disclosed,['Bengaluru'],"Key Roles and Responsibilities\nDevelop, enhance, and support assigned applications, ensuring seamless functionality and high performance.\nProvide subject matter expertise, acting as a go-to resource for application-specific knowledge.\nCollaborate and communicate effectively with team members, stakeholders, and end-users.\nTroubleshoot issues, monitor performance, and optimize processes for continuous improvement.\nStay current with industry trends and share best practices with the team.\nAdhere to company policies, procedures, and security requirements while maintaining compliance standards.\nFlexible with shifts and occasional weekend support.\nKey Competencies\nFull life-cycle experience on enterprise software development projects.\nExperience in relational databases/ data marts/data warehouses and complex SQL programming.\nExtensive experience in ETL, shell or python scripting, data modelling, analysis, and preparation\nExperience in Unix/Linux system, files systems, shell scripting.\nGood to have knowledge on any cloud platforms like Azure, Databricks etc.\nGood to have experience in BI Reporting tools Power BI\nGood problem-solving and analytical skills used to resolve technical problems.\nMust possess a good understanding of Compliance and Security Standards (preferably US standards).\nAbility to work independently but must be a team player. Should be able to drive business decisions and take ownership of their work.\nExperience in presentation design, development, delivery, and good communication skills to present analytical results and recommendations for action-oriented data driven decisions and associated operational and financial impacts.\nRequired/Desired Skills\nApplication: Databricks\nLanguages: Python, Shell Scripting, PLSQL, SQL\nCloud Technologies: Azure.\nTools: Jupyter Notebooks, SQL Developer, Postman, IDE.\nDatabase: Oracle, Snowflake and SQL Server.\nDevops: Jenkins, Kubernetes and Docker.\nBachelor s degree in computer science, Information Systems or related field.\n4+ years of experience in working in Engineering or Development roles with Compliance Standards\n4+ years of experience building high transaction defensive web applications and Python applications\n2+ years of experience in cloud technologies: AWS, Azure, OpenStack, Ansible, Chef or Terraform\n2+ years of experience in creating application for container services Docker, Kubernetes,\n2+ years of experience in build and CICD technologies: GitHub, Maven, Jenkins, Nexus or Sonar\nProficiency in Unix/Linux command line\n1+ years of experience in building applications using Large Language Models.\n1+ years of experience in building intelligent automation using Power Automate\n1+ years of experience in using Agentic framework\n1+ years of experience in building accurate prompts for AI tools.\nExpert knowledge and experience working with asynchronous message processing, stream processing and event driven computing.\nExperience working within Agile/Scrum/Kanban development team\nFamiliarity with HTML5, JavaScript frameworks, and CSS3\nCertified in Java, Spring or Azure technologies\nExcellent written and verbal communication skills with demonstrated ability to present complex technical information in a clear manner to peers, developers, and senior leaders\nEducation & Qualifications\nUniversity Degree in Computer Science and/or Analytics\nMinimum Experience required: 6-9 years in relational database design & development, ETL development, GenAI\nAdditional Details\nShift timing (if any): 12.30 to 9.30 IST(Bangalore)\nWork mode: Hybrid (3 days mandatory in office)\nLocation: Bangalore\n#DataPlatform\n#DataScience\nJob ID R-61633 Date posted 06/03/2025",Industry Type: Telecom / ISP,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Unix', 'Computer science', 'Automation', 'Linux', 'Database design', 'Shell scripting', 'Javascript', 'Design development', 'SQL', 'Python']",2025-06-13 05:18:30
Graph Engineer- Data Science,HARMAN,4 - 9 years,Not Disclosed,['Bengaluru'],"Job Description\nIntroduction: Digital Transformation Solutions (DTS)\n.\nExtensive experience in defining, developing, and implementing security software, ideally with a strong embedded firmware development background\nAbout the Role\nThis position offers an opportunity to work in a globally distributed team where you will get a unique opportunity of personal development in a multi-cultural environment. You will also get a challenging environment to develop expertise in the technologies useful in the industry.",,,,"['Computer science', 'Product quality', 'UML', 'XML', 'Relationship', 'Javascript', 'HTML', 'Oracle', 'Automotive', 'Python']",2025-06-13 05:18:32
Site Reliability Engineer - Cloud & Data Center Networking,IBM,5 - 10 years,Not Disclosed,['Bengaluru'],"- Manage, optimise and resolve issues observed in the VPC environment through detailed debugging\n\n- Manage extremely good timelines for the events and actions taken in our incident records for future reference for improvements\n\n- Manage and optimize underlay network infrastructure including routing, switching, and physical connectivity in data centers\n\n- Collaborate with cloud architects to troubleshoot and resolve networking issues across the Cloud Infrastructure\n\n- Monitor network performance and proactively resolve issues using tools like Splunk, AppNeta or equivalent\n\n- Document procedures and call out anomalies when observed during run-book executions\n\n- Also improve run-books as and when issues are discovered\n\n- Participate in on-call rotations and incident response as needed\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\n\n- Bachelor’s degree in Computer Science, Information Technology, or related field (or equivalent experience).\n\n- 5+ years of experience in enterprise networking, with a strong focus on both cloud and data center networking environments\n\n- Proficiency in overlay technologiesVXLAN, NVGRE, VPC\n\n- Strong understanding of underlay technologiesBGP, OSPF, MPLS, Ethernet, Spine-Leaf architectures\n\n- Relevant certifications (e.g., CCNP, CCIE, AWS Advanced Networking) are a plus\n\n\nPreferred technical and professional experience\n\n- Exposure to container networking (Kubernetes)\n\n- Familiarity with network automation tools (e.g., Ansible, Terraform, Python scripting)",Industry Type: IT Services & Consulting,Department: Engineering - Hardware & Networks,"Employment Type: Full Time, Permanent","['networking', 'ospf', 'ethernet', 'data center', 'mpls', 'container', 'kubernetes', 'python', 'network infrastructure', 'reliability', 'site reliability engineering', 'ansible', 'docker', 'automation tools', 'java', 'devops', 'linux', 'jenkins', 'splunk', 'cloud infrastructure', 'debugging', 'terraform', 'aws']",2025-06-13 05:18:33
Data Platform Engineer,Accenture,3 - 8 years,Not Disclosed,['Bengaluru'],"Project Role :Data Platform Engineer\n\n\n\n\n\nProject Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Platform Engineer, you will assist with the data platform blueprint and design, collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. You will play a crucial role in the development and maintenance of the data platform components, contributing to the overall success of the project.\nRoles & Responsibilities:- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Assist with the data platform blueprint and design.- Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.- Develop and maintain data platform components.- Contribute to the overall success of the project.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Strong understanding of statistical analysis and machine learning algorithms.- Experience with data visualization tools such as Tableau or Power BI.- Hands-on implementing various machine learning algorithms such as linear regression, logistic regression, decision trees, and clustering algorithms.- Solid grasp of data munging techniques, including data cleaning, transformation, and normalization to ensure data quality and integrity.\nAdditional Information:- The candidate should have a minimum of 3 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Hyderabad office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analytics', 'tableau', 'machine learning algorithms', 'statistics', 'data munging', 'python', 'data analysis', 'natural language processing', 'power bi', 'machine learning', 'sql', 'data quality', 'r', 'data modeling', 'data science', 'predictive modeling', 'text mining', 'logistic regression']",2025-06-13 05:18:35
Data Platform Engineer,Accenture,3 - 8 years,Not Disclosed,['Chennai'],"Project Role :Data Platform Engineer\n\n\n\n\n\nProject Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Platform Engineer, you will assist with the data platform blueprint and design, encompassing the relevant data platform components. You will collaborate with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models. Your typical day will involve working on the data platform blueprint and design, collaborating with architects, and ensuring seamless integration between systems and data models.\nRoles & Responsibilities:- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Assist with the data platform blueprint and design.- Collaborate with Integration Architects and Data Architects.- Ensure cohesive integration between systems and data models.- Implement data platform components.- Troubleshoot and resolve data platform issues.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Strong understanding of statistical analysis and machine learning algorithms.- Experience with data visualization tools such as Tableau or Power BI.- Hands-on implementing various machine learning algorithms such as linear regression, logistic regression, decision trees, and clustering algorithms.- Solid grasp of data munging techniques, including data cleaning, transformation, and normalization to ensure data quality and integrity.\nAdditional Information:- The candidate should have a minimum of 3 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Bengaluru office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analytics', 'tableau', 'machine learning algorithms', 'statistics', 'data munging', 'python', 'natural language processing', 'power bi', 'data architecture', 'machine learning', 'sql', 'data quality', 'r', 'data modeling', 'data science', 'predictive modeling', 'text mining']",2025-06-13 05:18:37
AM Client Workflows & Ref Data - Associate - Software Engineering,Goldman Sachs,2 - 5 years,Not Disclosed,['Bengaluru'],"Work with a global team of highly motivated platform engineers and software developers building integrated architectures for secure, scalable infrastructure services serving a diverse set of use cases.\nPartner with colleagues from across technology and risk to ensure an outstanding platform is delivered.\nHelp to provide frictionless integration with the firm s runtime, deployment and SDLC technologies.\nCollaborate on feature design and problem solving.\nHelp to ensure reliability, define, measure, and meet service level objectives.\nQuality coding & integration, testing, release, and demise of software products supporting AWM functions.\nEngage in quality assurance and production troubleshooting.\nHelp to communicate and promote best practices for software engineering across the Asset Management tech stack.\nBasic Qualifications\nA strong grounding in software engineering concepts and implementation of architecture design patterns.\nA good understanding of multiple aspects of software development in microservices architecture, full stack development experience, Identity / access management and technology risk.\nSound SDLC and practices and tooling experience - version control, CI/CD and configuration management tools.\nAbility to communicate technical concepts effectively, both written and orally, as we'll as interpersonal skills required to collaborate effectively with colleagues across diverse technology teams.\nExperience meeting demands for high availability and scalable system requirements.\nAbility to reason about performance, security, and process interactions in complex distributed systems.\nAbility to understand and effectively debug both new and existing software.\nExperience with metrics and monitoring tooling, including the ability to use metrics to rationally derive system health and availability information.\nExperience in auditing and supporting software based on sound SRE principles.\nPreferred Qualifications\n3+ Years of Experience using and/or supporting Java based frameworks & SQL / NOSQL data stores.\nExperience with deploying software to containerized environments - Kubernetes/Docker.\nScripting skills using Python, Shell or bash.\nExperience with Terraform or similar infrastructure-as-code platforms.\nExperience building services using public cloud providers such as AWS, Azure or GCP.",Industry Type: Banking,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Manager Quality Assurance', 'Coding', 'Configuration management', 'Investment banking', 'Asset management', 'Troubleshooting', 'SDLC', 'Monitoring', 'SQL', 'Python']",2025-06-13 05:18:39
"Associate Director, Data Science/Software Engineering",ATT Communication Services,10 - 15 years,Not Disclosed,['Bengaluru'],"Associate Director, Data Science/Software Engineering:\nAT&T is one of the leading service providers in the telecommunication sector and propelling it into the data and AI driven era is powered by CDO (Chief Data Office) . CDO is empowering AT&T, through execution, self-service, and as a data and AI center of excellence, to unlock transformative insights and actions that drive value for the company and its customers.\nEmployees in CDO imagine, innovate, and unlock data & AI driven insights and actions that create value for our customers and the enterprise. Part of the work, we govern data collection and use, mitigate for potential bias in machine learning models, and encourage an enterprise culture of responsible AI.\nAT&T s Chief Data Office (CDO) is harnessing data and making AT&T s data assets and ground-breaking AI functionality accessible to employees across the firm. In addition, our talented employees are a significant component that contributes to AT&T s place as the U.S. company with the sixth most AI-related patents. CDO also maintains academic and tech partnerships to cultivate the next generation of experts in statistics and machine learning, statistical computing, data visualization, text mining, time series modelling, data stream and database management, data quality and anomaly detection, data privacy, and more.\nWe are looking for an accomplished and visionary professional for the role of Associate Director, Data Science/Software Engineering to join our team and lead the development of cutting-edge software solutions. This is a hands-on leadership position that requires the fine balance of supervising and leading people while providing significant technical contributions to the projects you will be responsible for. As a key technical leader, you will leverage your expertise in full-stack development, DevOps best practices, Data analysis, AI/ML and Generative AI to lead your team in creating scalable, reliable, and efficient systems.\nThis role demands a strategic thinker and hands-on contributor who can work across multiple teams, drive innovation, and ensure technical excellence. You will be instrumental in shaping the technical roadmap, mentoring teams, and delivering transformative solutions that align with business objectives.\nKey Responsibilities:\nTechnical Leadership:\nDefine and drive the technical vision and architecture for scalable, resilient, and secure full-stack applications utilizing data powered insights.\nLead end-to-end software development projects from concept to deployment and maintenance.\nCollaborate with cross-functional teams to translate business requirements into technical solutions.\nServe as a mentor and technical advisor to engineering teams, fostering a culture of innovation and excellence.\nFull-Stack Development:\nDesign and implement scalable and high-performance web applications using modern front-end and back-end frameworks (e.g., React, Angular, Node.js, Python, Java).\nDevelop modular and reusable APIs (RESTful or GraphQL) with an emphasis on maintainability and performance.\nEnsure seamless integration of front-end and back-end systems while maintaining best practices for UI/UX design.\nOptimize database structures and queries for both relational (e.g., MySQL, PostgreSQL) and non-relational (e.g., MongoDB, DynamoDB) databases.\nDevOps and Automation:\nArchitect and implement CI/CD pipelines to streamline build, test, and deployment processes.\nEnsure seamless deployment and scalability of applications through containerization tools (e.g., Docker) and orchestration platforms (e.g., Kubernetes).\nLeverage infrastructure-as-code solutions (e.g., Terraform, Ansible) to automate infrastructure provisioning and management.\nMonitor application performance, troubleshoot issues, and ensure high availability through tools like Prometheus, Grafana, or New Relic.\nShell Scripting and Automation:\nDevelop and maintain shell scripts to automate routine tasks, system monitoring, and application deployments.\nDebug and troubleshoot production issues using scripting techniques to ensure minimal downtime.\nEnhance system efficiency by automating log analysis, error detection, and reporting.\nStrategic Contribution:\nCollaborate with stakeholders to align technical priorities with business goals.\nEvaluate emerging technologies and tools to recommend and implement solutions that advance the organization s technical capabilities.\nEstablish and enforce software engineering best practices, ensuring robust security, scalability, and maintainability.\nQualifications:\nEducation:\nBachelor s or Master s degree in Computer Science, Software Engineering, or a related field. A Ph.D. is a plus.\nExperience:\n13+ years of experience in software engineering, including hands-on experience with full-stack development and DevOps practices.\nProven track record of delivering large-scale, high-impact software solutions in a leadership capacity.\nTechnical Expertise:\nAdvanced proficiency in front-end frameworks (React, Angular, or Vue.js) and back-end technologies (Node.js, Python, Java, Go, etc.).\nStrong experience with DevOps tools (Jenkins, GitLab CI/CD, Docker, Kubernetes).\nDeep understanding of cloud platforms (AWS, Azure, GCP), including architecture and deployment strategies.\nSolid grasp of database technologies (SQL and NoSQL) and optimization techniques.\nProficiency in writing, debugging, and maintaining shell scripts for automation and system monitoring.\nStrong knowledge of microservices architecture, API gateways, and distributed systems.\nSoft Skills:\nExceptional problem-solving and critical-thinking abilities.\nStrong leadership and mentoring skills, with the ability to inspire and guide teams.\nExcellent communication skills, both written and verbal, to collaborate effectively with technical and non-technical stakeholders.\nStrategic mindset, capable of balancing technical depth with business impact.\nPreferred Qualifications:\nExperience with serverless computing frameworks (e.g., AWS Lambda).\nCertifications in cloud platforms (e.g., AWS Certified Solutions Architect, Azure DevOps Engineer Expert).\nKnowledge of security best practices in software development and DevOps.\n#DataEngineering\nLocation:\nIND:KA:Bengaluru / Innovator Building, Itpb, Whitefield Rd - Adm: Intl Tech Park, Innovator Bldg\nJob ID R-66889 Date posted 05/14/2025",Industry Type: Telecom / ISP,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Automation', 'Data analysis', 'Front end', 'Postgresql', 'MySQL', 'Shell scripting', 'Telecommunication', 'SQL', 'Python']",2025-06-13 05:18:41
Lead Data Engineer,Moreyeahs,7 - 9 years,Not Disclosed,['Indore'],"We are seeking a highly skilled and experienced Lead Data Engineer (7+ years) to join our dynamic team. As a Lead Data Engineer, you will play a crucial role in designing, developing, and maintaining our data infrastructure. You will be responsible for ensuring the efficient and reliable collection, storage, and transformation of large-scale data to support business intelligence, analytics, and data-driven decision-making.\n\nKey Responsibilities :\n\nData Architecture & Design :\n- Lead the design and implementation of robust data architectures that support data warehousing (DWH), data integration, and analytics platforms.\n- Develop and maintain ETL (Extract, Transform, Load) pipelines to ensure the efficient processing of large datasets.\n\nETL Development :\n- Design, develop, and optimize ETL processes using tools like Informatica Power Center, Intelligent Data Management Cloud (IDMC), or custom Python scripts.\n- Implement data transformation and cleansing processes to ensure data quality and consistency across the enterprise.\n\nData Warehouse Development :\n- Build and maintain scalable data warehouse solutions using Snowflake, Databricks, Redshift, or similar technologies.\n\n- Ensure efficient storage, retrieval, and processing of structured and semi-structured data.\n\nBig Data & Cloud Technologies :\n- Utilize AWS Glue and PySpark for large-scale data processing and transformation.\n- Implement and manage data pipelines using Apache Airflow for orchestration and scheduling.\n- Leverage cloud platforms (AWS, Azure, GCP) for data storage, processing, and analytics.\n\nData Management & Governance :\n- Establish and enforce data governance and security best practices.\n- Ensure data integrity, accuracy, and availability across all data platforms.\n- Implement monitoring and alerting systems to ensure data pipeline reliability.\n\nCollaboration & Leadership :\n\n- Work closely with data Stewards, analysts, and business stakeholders to understand data requirements and deliver solutions that meet business needs.\n- Mentor and guide junior data engineers, fostering a culture of continuous learning and development within the team.\n- Lead data-related projects from inception to delivery, ensuring alignment with business objectives and timelines.\n\nDatabase Management :\n- Design and manage relational databases (RDBMS) to support transactional and analytical workloads.\n- Optimize SQL queries for performance and scalability across various database platforms.\n\nRequired Skills & Qualifications :\nEducation: Bachelors or Masters degree in Computer Science, Information Systems, Engineering, or a related field.\n\nExperience :\n- Minimum of 7+ years of experience in data engineering, ETL, and data warehouse development.\n- Proven experience with ETL tools like Informatica Power Center or IDMC.\n- Strong proficiency in Python and PySpark for data processing.\n- Experience with cloud-based data platforms such as AWS Glue, Snowflake, Databricks, or Redshift.\n- Hands-on experience with SQL and RDBMS platforms (e.g., Oracle, MySQL, PostgreSQL).\n- Familiarity with data orchestration tools like Apache Airflow.\nTechnical Skills :\n- Advanced knowledge of data warehousing concepts and best practices.\n- Strong understanding of data modeling, schema design, and data governance.\n- Proficiency in designing and implementing scalable ETL pipelines.\n- Experience with cloud infrastructure (AWS, Azure, GCP) for data storage and processing.\n\nSoft Skills :\n- Excellent communication and collaboration skills.\n- Ability to lead and mentor a team of engineers.\n- Strong problem-solving and analytical thinking abilities.\n- Ability to manage multiple projects and prioritize tasks effectively.\n\nPreferred Qualifications :\n- Experience with machine learning workflows and data science tools.\n- Certification in AWS, Snowflake, Databricks, or relevant data engineering technologies.\n- Experience with Agile methodologies and DevOps practices.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data modeling', 'RDBMS', 'MySQL', 'Agile', 'Informatica', 'Oracle', 'Apache', 'Business intelligence', 'Analytics', 'Python']",2025-06-13 05:18:43
Lead Data Engineer,Shyftlabs,5 - 10 years,Not Disclosed,['Noida'],"Position Overview\nWe are looking for an experienced Lead Data Engineer to join our dynamic team. If you are passionate about building scalable software solutions, and work collaboratively with cross-functional teams to define requirements and deliver solutions we would love to hear from you.\nJob Responsibilities:\nDevelop and maintain data pipelines and ETL/ELT processes using Python\nDesign and implement scalable, high-performance applications\nWork collaboratively with cross-functional teams to define requirements and deliver solutions\nDevelop and manage near real-time data streaming solutions using Pub, Sub or Beam.\nContribute to code reviews, architecture discussions, and continuous improvement initiatives\nMonitor and troubleshoot production systems to ensure reliability and performance\nBasic Qualifications:\n5+ years of professional software development experience with Python\nStrong understanding of software engineering best practices (testing, version control, CI/CD)\nExperience building and optimizing ETL/ELT processes and data pipelines\nProficiency with SQL and database concepts\nExperience with data processing frameworks (e.g., Pandas)\nUnderstanding of software design patterns and architectural principles\nAbility to write clean, well-documented, and maintainable code\nExperience with unit testing and test automation\nExperience working with any cloud provider (GCP is preferred)\nExperience with CI/CD pipelines and Infrastructure as code\nExperience with Containerization technologies like Docker or Kubernetes\nBachelors degree in Computer Science, Engineering, or related field (or equivalent experience)\nProven track record of delivering complex software projects\nExcellent problem-solving and analytical thinking skills\nStrong communication skills and ability to work in a collaborative environment\nPreferred Qualifications:\nExperience with GCP services, particularly Cloud Run and Dataflow\nExperience with stream processing technologies (Pub/Sub)\nFamiliarity with big data technologies (Airflow)\nExperience with data visualization tools and libraries\nKnowledge of CI/CD pipelines with Gitlab and infrastructure as code with Terraform\nFamiliarity with platforms like Snowflake, Bigquery or Databricks,.\nGCP Data engineer certification",Industry Type: Management Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Software design', 'Version control', 'Analytical', 'Data processing', 'Unit testing', 'data visualization', 'Continuous improvement', 'SQL', 'Python']",2025-06-13 05:18:45
Lead Data Engineer,anblicks,6 - 8 years,Not Disclosed,['Ahmedabad'],"Job Summary:\nWe are seeking a Senior Data Engineer with hands-on experience building scalable data pipelines using Microsoft Fabric. The role focuses on delivering ingestion, transformation, and enrichment workflows across medallion architecture.\nKey Responsibilities:\nDevelop and maintain data pipelines using Microsoft Fabric Data Factory and OneLake.\nDesign and build ingestion and transformation pipelines for structured and unstructured data.\nImplement frameworks for metadata tagging, version control, and batch tracking.\nEnsure security, quality, and compliance of data pipelines.\nContribute to CI/CD integration, observability, and documentation.\nCollaborate with data architects and analysts to meet business requirements.\nQualifications:\n6+ years of experience in data engineering; 2+ years working on Microsoft Fabric or Azure Data services.\nHands-on with tools like Azure Data Factory, Fabric, Databricks, or Synapse.\nStrong SQL and data processing skills (e.g., PySpark, Python).\nExperience with data cataloging, lineage, and governance frameworks.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['metadata', 'data services', 'Version control', 'Compliance', 'Architecture', 'Data processing', 'microsoft', 'SQL', 'Python']",2025-06-13 05:18:47
Lead Data Engineer (Immediate joiner),Decision Point,4 - 9 years,15-30 Lacs P.A.,"['Gurugram', 'Chennai']","Role & responsibilities\n• Assume ownership of Data Engineering projects from inception to completion.\nImplement fully operational Unified Data Platform solutions in production environments using technologies like Databricks, Snowflake, Azure Synapse etc.\nShowcase proficiency in Data Modelling and Data Architecture\nUtilize modern data transformation tools such as DBT (Data Build Tool) to streamline and automate data pipelines (nice to have).",,,,"['Pyspark', 'Azure Databricks', 'SQL', 'Azure Synapse', 'Python', 'Etl Pipelines', 'Airflow', 'Bigquery', 'Advance Sql', 'Azure Cloud', 'GCP', 'Data Modeling', 'Data Architecture', 'AWS']",2025-06-13 05:18:48
Lead Data Engineer - Azure,Blend360 India,7 - 12 years,Not Disclosed,['Hyderabad'],"As a Sr Data Engineer, your role is to spearhead the data engineering teams and elevate the team to the next level! You will be responsible for laying out the architecture of the new project as well as selecting the tech stack associated with it. You will plan out the development cycles deploying AGILE if possible and create the foundations for good data stewardship with our new data products!\nYou will also set up a solid code framework that needs to be built to purpose yet have enough flexibility to adapt to new business use cases tough but rewarding challenge!\n\nResponsibilities\nCollaborate with several stakeholders to deeply understand the needs of data practitioners to deliver at scale\nLead Data Engineers to define, build and maintain Data Platform\nWork on building Data Lake in Azure Fabric processing data from multiple sources\nMigrating existing data store from Azure Synapse to Azure Fabric\nImplement data governance and access control\nDrive development effort End-to-End for on-time delivery of high-quality solutions that conform to requirements, conform to the architectural vision, and comply with all applicable standards.\nPresent technical solutions, capabilities, considerations, and features in business terms. Effectively communicate status, issues, and risks in a precise and timely manner.\nFurther develop critical initiatives, such as Data Discovery, Data Lineage and Data Quality\nLeading team and Mentor junior resources\nHelp your team members grow in their role and achieve their career aspirations\nBuild data systems, pipelines, analytical tools and programs\nConduct complex data analysis and report on results\n\n\n7+ Years of Experience as a data engineer or similar role in Azure Synapses, ADF or\nrelevant exp in Azure Fabric\nDegree in Computer Science, Data Science, Mathematics, IT, or similar field\nMust have experience e",Industry Type: Industrial Automation,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Data analysis', 'Data modeling', 'Analytical', 'Agile', 'data governance', 'Data quality', 'Data mining', 'SQL', 'Python']",2025-06-13 05:18:50
Lead Data Engineer,Prolegion,8 - 12 years,Not Disclosed,['Hyderabad'],"Job Summary:\nWe are seeking a highly skilled Lead Data Engineer/Associate Architect to lead the design, implementation, and optimization of scalable data architectures. The ideal candidate will have a deep understanding of data modeling, ETL processes, cloud data solutions, and big data technologies. You will work closely with cross-functional teams to build robust, high-performance data pipelines and infrastructure to enable data-driven decision-making.\n\nExperience: 8 - 12+ years\nWork Location: Hyderabad (Hybrid)\nMandatory skills: Python, SQL, Snowflake\n\nResponsibilities:\nDesign and Develop scalable and resilient data architectures that support business needs, analytics, and AI/ML workloads.\nData Pipeline Development: Design and implement robust ETL/ELT processes to ensure efficient data ingestion, transformation, and storage.\nBig Data & Cloud Solutions: Architect data solutions using cloud platforms like AWS, Azure, or GCP, leveraging services such as Snowflake, Redshift, BigQuery, and Databricks.\nDatabase Optimization: Ensure performance tuning, indexing strategies, and query optimization for relational and NoSQL databases.\nData Governance & Security: Implement best practices for data quality, metadata management, compliance (GDPR, CCPA), and security.\nCollaboration & Leadership: Work closely with data engineers, analysts, and business stakeholders to translate business requirements into scalable solutions.\nTechnology Evaluation: Stay updated with emerging trends, assess new tools and frameworks, and drive innovation in data engineering.\n\nRequired Skills:\nEducation: Bachelors or Masters degree in Computer Science, Data Engineering, or a related field.\nExperience: 8 - 12+ years of experience in data engineering\nCloud Platforms: Strong expertise in AWS data services.\nBig Data Technologies: Experience with Hadoop, Spark, Kafka, and related frameworks.\nDatabases: Hands-on experience with SQL, NoSQL, and columnar databases such as PostgreSQL, MongoDB, Cassandra, and Snowflake.\nProgramming: Proficiency in Python, Scala, or Java for data processing and automation.\nETL Tools: Experience with tools like Apache Airflow, Talend, DBT, or Informatica.\nMachine Learning & AI Integration (Preferred): Understanding of how to architect data solutions for AI/ML applications\n\n,",Industry Type: Defence & Aerospace,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Performance tuning', 'Automation', 'Data modeling', 'Postgresql', 'Informatica', 'Apache', 'Analytics', 'SQL', 'Python']",2025-06-13 05:18:52
Data Engineering Lead,Yotta Techports,10 - 15 years,30-35 Lacs P.A.,['Hyderabad'],"Responsibilities:\nLead and manage an offshore team of data engineers, providing strategic guidance, mentorship, and support to ensure the successful delivery of projects and the development of team members.\nCollaborate closely with onshore stakeholders to understand project requirements, allocate resources efficiently, and ensure alignment with client expectations and project timelines.\nDrive the technical design, implementation, and optimization of data pipelines, ETL processes, and data warehouses, ensuring scalability, performance, and reliability.\nDefine and enforce engineering best practices, coding standards, and data quality standards to maintain high-quality deliverables and mitigate project risks.\nStay abreast of emerging technologies and industry trends in data engineering, and provide recommendations for tooling, process improvements, and skill development.\nAssume a data architect role as needed, leading the design and implementation of data architecture solutions, data modeling, and optimization strategies.\nDemonstrate proficiency in AWS services such as:\nExpertise in cloud data services, including AWS services like Amazon Redshift, Amazon EMR, and AWS Glue, to design and implement scalable data solutions.\nExperience with cloud infrastructure services such as AWS EC2, AWS S3, to optimize data processing and storage.\nKnowledge of cloud security best practices, IAM roles, and encryption mechanisms to ensure data privacy and compliance.\nProficiency in managing or implementing cloud data warehouse solutions, including data modeling, schema design, performance tuning, and optimization techniques.\nDemonstrate proficiency in modern data platforms such as Snowflake and Databricks, including:\nDeep understanding of Snowflake's architecture, capabilities, and best practices for designing and implementing data warehouse solutions.\nHands-on experience with Databricks for data engineering, data processing, and machine learning tasks, leveraging Spark clusters for scalable data processing.\nAbility to optimize Snowflake and Databricks configurations for performance, scalability, and cost-effectiveness.\nManage the offshore team's performance, including resource allocation, performance evaluations, and professional development, to maximize team productivity and morale.\n\nQualifications:\nBachelor's degree in Computer Science, Engineering, or a related field; advanced degree preferred.\n10+ years of experience in data engineering, with a proven track record of leadership and technical expertise in managing complex data projects.\nProficiency in programming languages such as Python, Java, or Scala, as well as expertise in SQL and relational databases (e.g., PostgreSQL, MySQL).\nStrong understanding of distributed computing, cloud technologies (e.g., AWS), and big data frameworks (e.g., Hadoop, Spark).\nExperience with data architecture design, data modeling, and optimization techniques.\nExcellent communication, collaboration, and leadership skills, with the ability to effectively manage remote teams and engage with onshore stakeholders.\nProven ability to adapt to evolving project requirements and effectively prioritize tasks in a fast-paced environment.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Team Handling', 'Snowflake', 'Data Services', 'Cloud Infrastructure', 'Data Bricks']",2025-06-13 05:18:53
Data Engineering Lead Microsoft Power BI,Client of Techs To Suit,8 - 12 years,25-32.5 Lacs P.A.,"['Indore', 'Hyderabad', 'Ahmedabad']","Poistion - Data Engineering Lead\nExp - 8 to 12 Years\nJob Location: Hyderabad, Ahmedabad, Indore, India.\n\nMust be Able to join in 30 days\nJob Summary:\nAs a Data Engineering Lead, your role will involve designing, developing, and implementing\ninteractive dashboards and reports using data engineering tools. You will work closely with\nstakeholders to gather requirements and translate them into effective data visualizations that\nprovide valuable insights. Additionally, you will be responsible for extracting, transforming, and\nloading data from multiple sources into Power BI, ensuring its accuracy and integrity. Your\nexpertise in Power BI and data analytics will contribute to informed decision-making and\nsupport the organization in driving data-centric strategies and initiatives.\nWe are looking for you!\nAs an ideal candidate for the Data Engineering Lead position, you embody the qualities of a\nteam player with a relentless get-it-done attitude. Your intellectual curiosity and customer\nfocus drive you to continuously seek new ways to add value to your job accomplishments. You\nthrive under pressure, maintaining a positive attitude and understanding that your career is a\njourney. You are willing to make the right choices to support your growth. In addition to your\nexcellent communication skills, both written and verbal, you have a proven ability to create\nvisually compelling designs using tools like Power BI and Tableau that effectively\ncommunicate our core values.\nYou build high-performing, scalable, enterprise-grade applications and teams. Your creativity\nand proactive nature enable you to think differently, find innovative solutions, deliver high-\nquality outputs, and ensure customers remain referenceable. With over eight years of\nexperience in data engineering, you possess a strong sense of self-motivation and take\nownership of your responsibilities. You prefer to work independently with little to no\nsupervision.\nYou are process-oriented, adopt a methodical approach, and demonstrate a quality-first\nmindset. You have led mid to large-size teams and accounts, consistently using constructive\nfeedback mechanisms to improve productivity, accountability, and performance within the\nteam. Your track record showcases your results-driven approach, as you have consistently\ndelivered successful projects with customer case studies published on public platforms.\nOverall, you possess a unique combination of skills, qualities, and experiences that make you\nan ideal fit to lead our data engineering team(s). You value inclusivity and want to join a culture\nthat empowers you to show up as your authentic self.\nYou know that success hinges on commitment, our differences make us stronger, and the\nfinish line is always sweeter when the whole team crosses together. In your role, you shouldbe driving the team using data, data, and more data. You will manage multiple teams, oversee\nagile stories and their statuses, handle escalations and mitigations, plan ahead, identify hiring\nneeds, collaborate with recruitment teams for hiring, enable sales with pre-sales teams, and\nwork closely with development managers/leads for solutioning and delivery statuses, as well\nas architects for technology research and solutions.\nWhat You Will Do:\nAnalyze Business Requirements.\nAnalyze the Data Model and do GAP analysis with Business Requirements and Power\nBI. Design and Model Power BI schema.\nTransformation of Data in Power BI/SQL/ETL Tool.\nCreate DAX Formula, Reports, and Dashboards. Able to write DAX formulas.\nExperience writing SQL Queries and stored procedures.\nDesign effective Power BI solutions based on business requirements.\nManage a team of Power BI developers and guide their work.\nIntegrate data from various sources into Power BI for analysis.\nOptimize performance of reports and dashboards for smooth usage.\nCollaborate with stakeholders to align Power BI projects with goals.\nKnowledge of Data Warehousing(must), Data Engineering is a plus",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Power Bi Dashboards', 'Microsoft Power Bi', 'SQL Queries', 'Azure Databricks', 'Dax', 'Azure Data Factory']",2025-06-13 05:18:55
Lead Data Engineer - Azure,Blend360 India,7 - 12 years,Not Disclosed,['Hyderabad'],"As a Lead Data Engineer, your role is to spearhead the data engineering teams and elevate the team to the next level! You will be responsible for laying out the architecture of the new project as well as selecting the tech stack associated with it. You will plan out the development cycles deploying AGILE if possible and create the foundations for good data stewardship with our new data products!\nYou will also set up a solid code framework that needs to be built to purpose yet have enough flexibility to adapt to new business use cases tough but rewarding challenge!\n\nResponsibilities\nCollaborate with several stakeholders to deeply understand the needs of data practitioners to deliver at scale\nLead Data Engineers to define, build and maintain Data Platform\nWork on building Data Lake in Azure Fabric processing data from multiple sources\nMigrating existing data store from Azure Synapse to Azure Fabric\nImplement data governance and access control\nDrive development effort End-to-End for on-time delivery of high-quality solutions that conform to requirements, conform to the architectural vision, and comply with all applicable standards.\nPresent technical solutions, capabilities, considerations, and features in business terms. Effectively communicate status, issues, and risks in a precise and timely manner.\nFurther develop critical initiatives, such as Data Discovery, Data Lineage and Data Quality\nLeading team and Mentor junior resources\nHelp your team members grow in their role and achieve their career aspirations\nBuild data systems, pipelines, analytical tools and programs\nConduct complex data analysis and report on results\n\n\n7+ Years of Experience as a data engineer or similar role in Azure Synapses, ADF or\nrelevant exp in Azure Fabric\nDegree in Computer Science, Data Science, Mathematics, IT, or similar field\nMust have experience e",Industry Type: Industrial Automation,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Data analysis', 'Data modeling', 'Analytical', 'Agile', 'data governance', 'Data quality', 'Data mining', 'SQL', 'Python']",2025-06-13 05:18:57
Senior Engineer - Data Science,Sasken Technologies,2 - 5 years,Not Disclosed,['Bengaluru'],"Job Summary\nPerson at this position has gained significant work experience to be able to apply their knowledge effectively and deliver results. Person at this position is also able to demonstrate the ability to analyse and interpret complex problems and improve change or adapt existing methods to solve the problem.\nPerson at this position regularly interacts with interfacing groups / customer on technical issue clarification and resolves the issues. Also participates actively in important project/ work related activities and contributes towards identifying important issues and risks. Reaches out for guidance and advice to ensure high quality of deliverables.\nPerson at this position consistently seek opportunities to enhance their existing skills, acquire more complex skills and work towards enhancing their proficiency level in their field of specialisation.\nWorks under limited supervision of Team Lead/ Project Manager.\n\n\nRoles & Responsibilities\nResponsible for design, coding, testing, bug fixing, documentation and technical support in the assigned area. Responsible for on time delivery while adhering to quality and productivity goals. Responsible for adhering to guidelines and checklists for all deliverable reviews, sending status report to team lead and following relevant organizational processes. Responsible for customer collaboration and interactions and support to customer queries. Expected to enhance technical capabilities by attending trainings, self-study and periodic technical assessments. Expected to participate in technical initiatives related to project and organization and deliver training as per plan and quality.\n\n\nEducation and Experience Required\nEngineering graduate, MCA, etc Experience: 2-5 years\n\nCompetencies Description\nData Science TCB is applicable to one who\n1) Analyses data to arrive at patterns/Insights/models\n2) Come up with models based on the data to provide recommendations, predictive analytics etc\n3) Provides implementation of the models in R, Matlab etc\n4) Can understand and apply machine learning/AI techniques\nPlatforms-\nUnix\nTechnology Standard-\nNA\nTools-\nR, Matlab, Spark Machine Learning, Python-ML, SPSS, SAS\nLanguages-\nR, Perl, Python, Scala\nSpecialization-\nCOGNITIVE ANALYTICS INCLUDING COMPUTER VISION, AI and ML, STATISTICS.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'R', 'SAS', 'Scala', 'Perl', 'SPSS', 'Spark', 'machine learning', 'Python']",2025-06-13 05:18:58
Gcp Data Engineer,Estuate Software,6 - 11 years,Not Disclosed,['Hyderabad'],"Design, build,\nJob Title: Data Engineer / Integration Engineer\nJob Summary:\nWe are seeking a highly skilled Data Engineer / Integration Engineer to join our team. The ideal candidate will have expertise in Python, workflow orchestration, cloud platforms (GCP/Google BigQuery), big data frameworks (Apache Spark or similar), API integration, and Oracle EBS. The role involves designing, developing, and maintaining scalable data pipelines, integrating various systems, and ensuring data quality and consistency across platforms. Knowledge of Ascend.io is a plus.\nKey Responsibilities:\nDesign, build, and maintain scalable data pipelines and workflows.\nDevelop and optimize ETL/ELT processes using Python and workflow automation tools.\nImplement and manage data integration between various systems, including APIs and Oracle EBS.\nWork with Google Cloud Platform (GCP) or Google BigQuery (GBQ) for data storage, processing, and analytics.\nUtilize Apache Spark or similar big data frameworks for efficient data processing.\nDevelop robust API integrations for seamless data exchange between applications.\nEnsure data accuracy, consistency, and security across all systems.\nMonitor and troubleshoot data pipelines, identifying and resolving performance issues.\nCollaborate with data analysts, engineers, and business teams to align data solutions with business goals.\nDocument data workflows, processes, and best practices for future reference.\nRequired Skills & Qualifications:\nStrong proficiency in Python for data engineering and workflow automation.\nExperience with workflow orchestration tools (e.g., Apache Airflow, Prefect, or similar).\nHands-on experience with Google Cloud Platform (GCP) or Google BigQuery (GBQ).\nExpertise in big data processing frameworks, such as Apache Spark.\nExperience with API integrations (REST, SOAP, GraphQL) and handling structured/unstructured data.\nStrong problem-solving skills and ability to optimize data pipelines for performance.\nExperience working in an agile environment with CI/CD processes.\nStrong communication and collaboration skills.\nPreferred Skills & Nice-to-Have:\nExperience with Ascend.io platform for data pipeline automation.\nKnowledge of SQL and NoSQL databases.\nFamiliarity with Docker and Kubernetes for containerized workloads.\nExposure to machine learning workflows is a plus.\nWhy Join Us?\nOpportunity to work on cutting-edge data engineering projects.\nCollaborative and dynamic work environment.\nCompetitive compensation and benefits.\nProfessional growth opportunities with exposure to the latest technologies.\n\nHow to Apply:\nInterested candidates can apply by sending their resume to 8892751405 / deekshith.naidu@estuate.com or through Naukri",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Airflow', 'GCP', 'Bigquery', 'SQL']",2025-06-13 05:19:00
Manager Data Engineer - Research Data and Analytics,Amgen Inc,4 - 6 years,Not Disclosed,['Hyderabad'],"What you will do\nIn this vital role you will create and develop data lake solutions for scientific data that drive business decisions for Research. You will build scalable and high-performance data engineering solutions for large scientific datasets and collaborate with Research collaborators. You will also provide technical leadership to junior team members. The ideal candidate possesses experience in the pharmaceutical or biotech industry, demonstrates deep technical skills, is proficient with big data technologies, and has a deep understanding of data architecture and ETL processes.\nRoles & Responsibilities:",,,,"['Data Engineering', 'Spotfire', 'PySpark', 'PostgreSQL', 'Plotly', 'SparkSQL', 'SQL server', 'SQL', 'process mapping', 'Dash', 'MySQL', 'ETL', 'Oracle', 'data governance frameworks', 'Python']",2025-06-13 05:19:02
Azure Data Engineer,Arges Global,2 - 5 years,8-18 Lacs P.A.,['Pune( Baner )'],"Scope of Work:\nCollaborate with the lead Business / Data Analyst to gather and analyse business requirements for data processing and reporting solutions.\nMaintain and run existing Python code, ensuring smooth execution and troubleshooting any issues that arise.\nDevelop new features and enhancements for data processing, ingestion, transformation, and report building.\nImplement best coding practices to improve code quality, maintainability, and efficiency.\nWork within Microsoft Fabric to manage data integration, warehousing, and analytics, ensuring optimal performance and reliability.\nSupport and maintain CI/CD workflows using Git-based deployments or other automated deployment tools, preferably in Fabric.\nDevelop complex business rules and logic in Python to meet functional specifications and reporting needs.\nParticipate in an agile development environment, providing feedback, iterating on improvements, and supporting continuous integration and delivery processes.\nRequirements:\nThis person will be an individual contributor responsible for programming, maintenance support, and troubleshooting tasks related to data movement, processing, ingestion, transformation, and report building.\nAdvanced-level Python developer.\nModerate-level experience in working in Microsoft Fabric environment (at least one and preferably two or more client projects in Fabric).\nWell-versed with understanding of modelling, databases, data warehousing, data integration, and technical elements of business intelligence technologies.\nAbility to understand business requirements and translate them into functional specifications for reporting applications.\nExperience in GIT-based deployments or other CI/CD workflow options, preferably in Fabric.\nStrong verbal and written communication skills.\nAbility to perform in an agile environment where continual development is prioritized.\nWorking experience in the financial industry domain and familiarity with financial accounting terms and statements like general ledger, balance sheet, and profit & loss statements would be a plus.\nAbility to create Power BI dashboards, KPI scorecards, and visual reports would be a plus.\nDegree in Computer Science or Information Systems, along with a good understanding of financial terms or working experience in banking/financial institutions, is preferred.",Industry Type: Financial Services (Asset Management),Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Power Bi', 'Microsoft Azure', 'Python', 'Azure Data Factory', 'Microsoft Fabric', 'Azure Databricks', 'Azure Data Lake']",2025-06-13 05:19:03
Data Engineering Manager,Amgen Inc,8 - 12 years,Not Disclosed,['Hyderabad'],"ABOUT THE ROLE\n\n\nRole Description:\n\nWe are seeking a seasoned Engineering Manager (Data Engineering) to lead the end-to-end management of enterprise data assets and operational data workflows. This role is critical in ensuring the availability, quality, consistency, and timeliness of data across platforms and functions, supporting analytics, reporting, compliance, and digital transformation initiatives. You will be responsible for the day-to-day data operations, manage a team of data professionals, and drive process excellence in data intake, transformation, validation, and delivery. You will work closely with cross-functional teams including data engineering, analytics, IT, governance, and business stakeholders to align operational data capabilities with enterprise needs.",,,,"['Data Engineering', 'fullstack development', 'logging framework', 'stakeholder engagement', 'troubleshooting', 'cloud platforms']",2025-06-13 05:19:06
AWS Data Engineer,DATATERRAIN LTD,10 - 15 years,Not Disclosed,['Hyderabad'],"Role AWS Data Engineer Experience 10+ Years Hyderabad Salary As per Industry Job Summary :\nWe are looking for a highly skilled Data Engineer to join our team. The ideal candidate should have strong experience working in an AWS environment, proficiency in ETL tools, and expertise in reporting tools. The candidate must be able to work onsite at the Hyderabad office on a daily basis.\nKey Responsibilities:\n1. Design, develop, and maintain data pipelines using AWS technologies.\n2. Work with ETL tools to process and transform large datasets efficiently.\n3. Develop and optimize reports and dashboards using reporting tools.\n4. Ensure data integrity, security, and governance across all data pipelines.\n5. Collaborate with cross-functional teams to understand data requirements.\n6. Troubleshoot and resolve data-related issues proactively.\nRequired Skills & Qualifications:\n1. Strong experience in AWS services\n2. Proficiency in any ETL tool\n3. Proficiency in any reporting tool\n4. Strong understanding of data modeling, warehousing, and big data concepts .\n5. Ability to work onsite daily at the Hyderabad clients office.\n6. Bachelors degree in Computer Science, Software Engineering, or a related field\nNote: Only shortlisted candidates will be contacted for further steps in the selection process.",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Selection process', 'Warehouse', 'Data modeling', 'data integrity', 'Troubleshooting', 'big data', 'ETL tool', 'AWS', 'Reporting tools']",2025-06-13 05:19:07
AWS Data Engineer,Exavalu,2 - 5 years,Not Disclosed,[],"Exavalu is looking for AWS Data Engineer to join our dynamic team and embark on a rewarding career journey\nBe responsible for the planning, implementation, and growth of the AWS cloud infrastructure\nBuild, release, and manage the configuration of all production systems\nManage a continuous integration and deployment methodology for server-based technologies\nWork alongside architecture and engineering teams to design and implement any scalable software services\nEnsure necessary system security by using best in class cloud security solutionsImplement continuous integration/continuous delivery (CI/CD) pipelines when necessary\nRecommend process and architecture improvements\nTroubleshoot the system and solve problems across all platform and application domains\nOversee pre-production acceptance testing to ensure the high quality of a companys services and products",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent",['AWS Data Engineer'],2025-06-13 05:19:09
Data Engineering Automation Tester,Swits Digital,3 - 6 years,Not Disclosed,['Gurugram'],"Job Title: Data Engineering Automation Tester\n\nExperience: 6+ Years\n\nLocation: Gurgaon, India\n\n\nMandatory Skills:\n\n\n\n\nStrong experience in distributed computing (Spark) and software development.\n\n\n\nProficiency in working with databases (preferably Postgres).\n\n\n\nSolid understanding of Object-Oriented Programming and development principles.\n\n\n\nExperience in Agile development methodologies (Scrum/Kanban).\n\n\n\nHands-on experience with version control tools (preferably Git).\n\n\n\nExposure to CI/CD pipelines.\n\n\n\nStrong background in automated testing including Integration/Delta, Load, and Performance testing.\n\n\n\nExtensive experience in database testing (preferably Postgres).\n\n\n\n\nGood to Have Skills:\n\n\n\n\nExposure to Docker and containerized environments.\n\n\n\nExperience with Spark-Scala.\n\n\n\nKnowledge of Data Engineering principles.\n\n\n\nExperience in Python and .NET Core.\n\n\n\nFamiliarity with Kubernetes.\n\n\n\nExperience with Airflow.\n\n\n\nWorking knowledge of cloud platforms (GCP and Azure).\n\n\n\nExperience with TeamCity CI and Octopus Deploy.",Industry Type: Recruitment / Staffing,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Database testing', 'Automation', 'Automation testing', 'GIT', 'Version control', 'spark', 'Agile development', 'Performance testing', 'Object oriented programming', 'Python']",2025-06-13 05:19:11
GCP Data Engineer,Swits Digital,2 - 7 years,Not Disclosed,['Chennai'],"Job Title: GCP Data Engineer\nLocation: Chennai, India\nJob type: FTE\nMandatory Skills: Google Cloud Platform - Biq Query, Data Flow, Dataproc, Data Fusion, TERRAFORM, Tekton,Cloud SQL, AIRFLOW, POSTGRES, Airflow PySpark, Python, API\nJob Description:\n2+Years in GCP Services, Biq Query, Data Flow, Dataproc, DataPlex,DataFusion, Terraform, Tekton, Cloud SQL, Redis Memory, Airflow, Cloud Storage\n2+ Years inData Transfer Utilities\n2+ Years in Git / any other version control tool\n2+ Years in Confluent Kafka\n1+ Years of Experience in API Development\n2+ Years in Agile Framework\n4+ years of strong experience in python, Pyspark development.\n4+ years of shell scripting to develop the adhoc jobsfor data importing/exporting",Industry Type: Recruitment / Staffing,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Version control', 'GIT', 'GCP', 'Shell scripting', 'query', 'Agile', 'cloud storage', 'SQL', 'Python']",2025-06-13 05:19:12
GCP Data Engineer,Product based Fortune Global 500 MNC in ...,4 - 8 years,6-16 Lacs P.A.,"['Hyderabad', 'Chennai']","Role & responsibilities\nBachelors degree or four or more years of work experience.\nFour or more years of work experience.\nExperience with Data Warehouse concepts and Data Management life cycle.\nExperience in any DBMS\nExperience in Shell scripting, Spark, Scala.\nExperience in GCP/Big Query, composer, Airflow.\nExperience in real time streaming\nExperience in DevOps",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['GCP', 'Bigquery', 'Cloud Storage', 'Pubsub', 'Data Flow', 'Dataproc', 'Spark', 'Composing', 'Dataprep', 'Python']",2025-06-13 05:19:14
Aws Data Engineer,Astrosoft Technologies,2 - 4 years,6.5-10 Lacs P.A.,['Hyderabad( Gachibowli )'],"Company: AstroSoft Technologies (https://www.astrosofttech.com/)\nAstrosoft is an award-winning company that specializes in the areas of Data, Analytics, Cloud, AI/ML, Innovation, Digital. We have a customer first mindset and take extreme ownership in delivering solutions and projects for our customers and have consistently been recognized by our clients as the premium partner to work with. We bring to bear top tier talent, a robust and structured project execution framework, our significant experience over the years and have an impeccable record in delivering solutions and projects for our clients.\nFounded in 2004, Headquarters in FL,USA, Corporate Office - India, Hyderabad\nBenefits from Astrosoft Technologies\nH1B Sponsorship (Depends on Project & Performance)\nLunch & Dinner (Every day)\nHealth Insurance Coverage- Group\nIndustry Standards Leave Policy\nSkill Enhancement Certification\nHybrid Mode\nRole & responsibilities\nJob Title: AWS Engineer\nRequired Skills:\nMinimum of 2+ years direct experience with AWS Data Engineer\nStrong Experience in AWS Services like Redshift & ETL Glue, Spark, Python, Lambda,Kafka,S3, EMR Etc experience is a must.\nMonitoring tools - Cloudwatch\nExperience in Development & Support Projects as well.\nStrong verbal and written communication skills\nStrong experience and understanding of streaming architecture and development practices using kafka, spark, flink etc,\nStrong AWS development experience using S3, SNS, SQS, MWAA (Airflow) Glue, DMS and EMR.\nStrong knowledge of one or more programing languages Python/Java/Scala (ideally Python)\nExperience using Terraform to build IAC components in AWS.\nStrong experience with ETL Tools in AWS; ODI experience is as plus.\nStrong experience with Database Platforms: Oracle, AWS Redshift\nStrong experience in SQL tuning, tuning ETL solutions, physical optimization of databases.\nVery familiar with SRE concepts which includes evaluating and implementing monitoring and observability tools like Splunk, Data Dog, CloudWatch and other job, log or dashboard concepts for customer support and application health checks.\nAbility to collaborate with our business partners to understand and implement their requirements.\nExcellent interpersonal skills and be able to build consensus across teams.\nStrong critical thinking and ability to think out-of-the box.\nSelf-motivated and able to perform under pressure.\nAWS certified (preferred)\nQualifications:\nEducation: Bachelor's degree in Computer Science, Information Technology, or a related field (or equivalent work experience).\nExperience:\nProven experience as an AWS Support Engineer or in a similar role.\nHands-on experience with a wide range of AWS services (e.g., EC2, S3, RDS, Lambda, CloudFormation, IAM).\nSoft Skills:\nExcellent communication and interpersonal skills.\nStrong problem-solving and analytical skills.\nAbility to work independently and as part of a team.\nCustomer-focused mindset with a commitment to delivering high-quality support.\nWhat We Offer:\nCompetitive salary and benefits package.\nOpportunities for professional growth and development.\nA collaborative and supportive work environment.\nAccess to the latest AWS technologies and training resources.\nIf you are passionate about cloud technology and enjoy helping customers solve complex technical challenges, we would love to hear from you!\nAcknowledge the mail with your updated cv,\nJulekha.Gousiya@astrosofttech.com\n\n\n\nDetails As we discussed, Please revert with your acknowledgment.\nTotal Experience-\nAws\nRedshift\nGlue-\nCurrent Location-\nCurrent Company-\nC-CTC-\nEx-CTC –\nOffer –\nNP –\nReady to Relocate Hyderabad (Y/N) – Yes\n(Hybrid) –(Y/N) - Yes",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['AWS', 'Glue', 'redshift', 'Spark']",2025-06-13 05:19:16
DBA DATA Engineer || 12 Lakhs CTC,Robotics Technologies,6 - 10 years,12 Lacs P.A.,['Hyderabad( Banjara hills )'],"Dear Candidate,\nWe are seeking a skilled and experienced DBA Data Engineer to join our growing data team. The ideal candidate will play a key role in designing, implementing, and maintaining our databases and data pipeline architecture. You will collaborate with software engineers, data analysts, and DevOps teams to ensure efficient data flow, data integrity, and optimal database performance across all systems. This role requires a strong foundation in database administration, SQL performance tuning, data modeling, and experience with both on-prem and cloud-based environments.\n\nRequirements:\nBachelors degree in Computer Science, Information Systems, or a related field.\n6+ years of experience in database administration and data engineering.\nProven expertise in RDBMS (Oracle, MySQL, and PostgreSQL) and NoSQL systems (MongoDB, Cassandra).\nExperience managing databases in cloud environments (AWS, Azure, or GCP).\nProficiency in ETL processes and tools (e.g., Apache NiFi, Talend, Informatica, AWS Glue).\nStrong experience with scripting languages such as Python, Bash, or PowerShell.\n\nDBA Data Engineer Roles & Responsibilities:\nDesign and maintain scalable and high-performance database architectures.\nMonitor and optimize database performance using tools like CloudWatch, Oracle Enterprise Manager, pgAdmin, Mongo Compass, or Dynatrace.\nDevelop and manage ETL/ELT pipelines to support business intelligence and analytics.\nEnsure data integrity and security through best practices in backup, recovery, and encryption.\nAutomate regular database maintenance tasks using scripting and scheduled jobs.\nImplement high availability, failover, and disaster recovery strategies.\nConduct performance tuning of queries, stored procedures, indexes, and table structures.\nCollaborate with DevOps to automate database deployments using CI/CD and IaC tools (e.g., Terraform, AWS CloudFormation).\nDesign and implement data models, including star/snowflake schemas for data warehousing.\nDocument data flows, data dictionaries, and database configurations.\nManage user access and security policies using IAM roles or database-native permissions.\nAnalyze existing data systems and propose modernization or migration plans (on-prem to cloud, SQL to NoSQL, etc.).\nUse AWS RDS, Amazon Redshift, Azure SQL Database, or Google BigQuery as needed.\nStay up-to-date with emerging database technologies and make recommendations.\n\nMust-Have Skills:\nDeep knowledge of SQL and database performance tuning.\nHands-on experience with database migrations and replication strategies.\nFamiliarity with data governance, data quality, and compliance frameworks (GDPR, HIPAA, etc.).\nStrong problem-solving and troubleshooting skills.\nExperience with data streaming platforms such as Apache Kafka, AWS Kinesis, or Apache Flink is a plus.\nExperience with data lake and data warehouse architectures.\nExcellent communication and documentation skills.\n\nSoft Skills:\nProblem-Solving: Ability to analyze complex problems and develop effective solutions.\nCommunication Skills: Strong verbal and written communication skills to effectively collaborate with cross-functional teams.\nAnalytical Thinking: Ability to think critically and analytically to solve technical challenges.\nTime Management: Capable of managing multiple tasks and deadlines in a fast-paced environment.\nAdaptability: Ability to quickly learn and adapt to new technologies and methodologies.\n\nInterview Mode : F2F for who are residing in Hyderabad / Zoom for other states\nLocation : 43/A, MLA Colony,Road no 12, Banjara Hills, 500034\nTime : 2 - 4pm",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Database Administration', 'Database Migration', 'Dba Skills', 'High Availability', 'Db Administration', 'Hadr', 'Database Security', 'Disaster Recovery', 'Dr Testing', 'DR', 'Luw', 'Db Upgrade', 'Brtools', 'UDDI', 'Os Migration', 'Dbase', 'DBMS', 'IBM DB2', 'Db Migration', 'Disaster Recovery Planning', 'Db Dba', 'Backup And Recovery']",2025-06-13 05:19:17
Assoc. Data Engineer - R&D Precision Medicine Team,Amgen Inc,1 - 3 years,Not Disclosed,['Hyderabad'],"What you will do\n\n\nThe R&D Precision Medicine team is responsible for Data Standardization, Data Searching, Cohort Building, and Knowledge Management tools that provide the Amgen scientific community with access to Amgens wealth of human datasets, projects and study histories, and knowledge over various scientific findings. These data include clinical data, omics, and images. These solutions are pivotal tools in Amgens goal to accelerate the speed of discovery, and speed to market of advanced precision medications.\n\nThe Data Engineer will be responsible for full stack development of enterprise analytics and data mastering solutions leveraging Databricks and Power BI. This role requires expertise in both data architecture and analytics, with the ability to create scalable, reliable, and high-performing enterprise solutions that support research cohort-building and advanced AI pipelines. The ideal candidate will have experience creating and surfacing large unified repositories of human data, based on integrations from multiple repositories and solutions, and be exceptionally skilled with data analysis and profiling.\n\nYou will collaborate closely with partners, product team members, and related IT teams, to design and implement data models, integrate data from various sources, and ensure best practices for data governance and security. The ideal candidate will have a solid background in data warehousing, ETL, Databricks, Power BI, and enterprise data mastering.\n\n\n\n\n\n\nWe are all different, yet we all use our unique contributions to serve patients. The professional we seek is someone with these qualifications.",,,,"['Data Engineering', 'data lakes', 'data pipelines', 'ETL processes', 'AWS', 'data warehouses', 'BI solutions']",2025-06-13 05:19:19
Data Engineer - R&D Data Catalyst Team,Amgen Inc,1 - 3 years,Not Disclosed,['Hyderabad'],"What you will do\nIn this vital role, you will be responsible for the end-to-end development of an enterprise analytics and data mastering solution using Databricks and Power BI. This role requires expertise in both data architecture and analytics, with the ability to create scalable, reliable, and impactful enterprise solutions that research cohort-building and advanced research pipeline. The ideal candidate will have experience creating and surfacing large unified repositories of human data, based on integrations from multiple repositories and solutions, and be extraordinarily skilled with data analysis and profiling.",,,,"['Data Engineering', 'data management', 'Power BI', 'data governance', 'data warehousing', 'Databricks', 'ETL', 'AWS']",2025-06-13 05:19:21
MDM Associate Data Engineer,Amgen Inc,1 - 4 years,Not Disclosed,['Hyderabad'],"We are seeking an MDM Associate Data Engineerwith 25 years of experience to support and enhance our enterprise MDM (Master Data Management) platforms using Informatica/Reltio. This role is critical in delivering high-quality master data solutions across the organization, utilizing modern tools like Databricks and AWS to drive insights and ensure data reliability. The ideal candidate will have strong SQL, data profiling, and experience working with cross-functional teams in a pharma environment.To succeed in this role, the candidate must have strong data engineering experience along with MDM knowledge, hence the candidates having only MDM experience are not eligible for this role. Candidate must have data engineering experience on technologies like (SQL, Python,",,,,"['MDM', 'PySpark', 'AWS architecture', 'Jira', 'Reltio', 'SQL', 'Informatica MDM', 'data modeling', 'Confluence', 'IDQ', 'Databricks', 'data stewardship processes', 'Python']",2025-06-13 05:19:23
Data Engineering Specialist,Sanofi,5 - 10 years,Not Disclosed,['Hyderabad'],"We are seeking an experienced Data Engineering Specialist interested in challenging the status quo to ensure the seamless creation and operation of the data pipelines that are needed by Sanofi s advanced analytic, AI and ML initiatives for the betterment of our global patients and customers.\nSanofi has recently embarked into a vast and ambitious digital transformation program. A cornerstone of this roadmap is the acceleration of its data transformation and of the adoption of artificial intelligence (AI) and machine learning (ML) solutions, to accelerate R&D, manufacturing and commercial performance and bring better drugs and vaccines to patients faster, to improve health and save lives\nMain Responsibilities:\nEstablish technical designs to meet Sanofi requirements aligned with the architectural and Data standards\nOwnership of the entire back end of the application, including the design, implementation, test, and troubleshooting of the core application logic, databases, data ingestion and transformation, data processing and orchestration of pipelines, APIs, CI/CD integration and other processes\nFine-tune and optimize queries using Snowflake platform and database techniques\nOptimize ETL/data pipelines to balance performance, functionality, and other operational requirements.\nAssess and resolve data pipeline issues to ensure performance and timeliness of execution\nAssist with technical solution discovery to ensure technical feasibility.\nAssist in setting up and managing CI/CD pipelines and development of automated tests\nDeveloping and managing microservices using python\nConduct peer reviews for quality, consistency, and rigor for production level solution\nDesign application architecture for efficient concurrent user handling, ensuring optimal performance during high usage periods\nOwn all areas of the product lifecycle: design, development, test, deployment, operation, and support\nQualifications:\n5+ years of relevant experience developing backend, integration, data pipelining, and infrastructure\nExpertise in database optimization and performance improvement\nExpertise in Python, PySpark, and Snowpark\nExperience data warehousing and object-relational database (Snowflake and PostgreSQL) and writing efficient SQL queries\nExperience in cloud-based data platforms (Snowflake, AWS)\nProficiency in developing robust, reliable APIs using Python and FastAPI Framework\nExpert in ELT and ETL & experience working with large data sets and performance and query optimization. IICS is a plus\nUnderstanding of data structures and algorithms\nUnderstanding of DBT is a plus\nExperience in modern testing framework (SonarQube, K6 is a plus)\nStrong collaboration skills, willingness to work with others to ensure seamless integration of the server-side and client-side\nKnowledge of DevOps best practices and associated tools is a plus, especially in the setup, configuration, maintenance, and troubleshooting of associated tools:\nContainers and containerization technologies (Kubernetes, Argo, Red Hat OpenShift)\nInfrastructure as code (Terraform)\nMonitoring and Logging (CloudWatch, Grafana)\nCI/CD Pipelines (JFrog Artifactory)\nScripting and automation (Python, GitHub, Github actions)\nExperience with JIRA & Confluence\nWorkflow orchestration (Airflow)\nMessage brokers (RabbitMQ)\nEducation: bachelors degree in computer science, engineering, or similar quantitative field of study\nWhy choose us\nBring the miracles of science to life alongside a supportive, future-focused team.\nDiscover endless opportunities to grow your talent and drive your career, whether it s through a promotion or lateral move, at home or internationally.\nEnjoy a thoughtful, we'll-crafted rewards package that recognizes your contribution and amplifies your impact.\nTake good care of yourself and your family, with a wide range of health and we'llbeing benefits including high-quality healthcare, prevention and we'llness programs and at least 14 weeks gender-neutral parental leave.\nOpportunity to work in an international environment, collaborating with diverse business teams and vendors, working in a dynamic team, and fully empowe'red to propose and implement innovative ideas.",Industry Type: Medical Services / Hospital,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Automation', 'github', 'Postgresql', 'Data structures', 'Healthcare', 'Troubleshooting', 'JIRA', 'Monitoring', 'Python']",2025-06-13 05:19:24
Engineer II - Data Engg & AI,anblicks,5 - 10 years,Not Disclosed,['Ahmedabad'],"We are seeking a highly skilled and experienced Senior AI Engineer to lead the design, development, and deployment of advanced AI systems. You will work on cutting-edge machine learning models, natural language processing, computer vision, and AI infrastructure to solve real-world problems and drive innovation across our products and services.\nKey Responsibilities:\nDesign, develop, and deploy scalable AI/ML models for production environments.\nLead end-to-end AI project lifecycles from data collection and preprocessing to model training, evaluation, and deployment.\nCollaborate with cross-functional teams including data scientists, software engineers, and product managers.\nOptimize model performance and ensure robustness, fairness, and explainability.\nStay current with the latest research and advancements in AI and machine learning.\nMentor junior engineers and contribute to building a strong AI engineering culture.\nRequired Qualifications:\nBachelor s or Master s degree in Computer Science, Artificial Intelligence, Machine Learning, or a related field (PhD preferred).\n5+ years of experience in AI/ML engineering with a strong portfolio of deployed models.\nProficiency in Python and ML libraries such as TensorFlow, PyTorch, Scikit-learn, etc.\nExperience with cloud platforms (AWS, Azure) and MLOps tools.\nStrong understanding of data structures, algorithms, and software engineering principles.\nExcellent problem-solving and communication skills.\nPreferred Qualifications:\nExperience with LLMs, RAG, or agentic AI (Crew AI) systems.\nFamiliarity with vector databases, prompt engineering, and AI safety practices.\nContributions to open-source AI projects or published research papers.\nExperience with real-time inference systems and edge AI.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Computer vision', 'Artificial Intelligence', 'Machine learning', 'Data collection', 'Data structures', 'Natural language processing', 'Research', 'Open source', 'Python']",2025-06-13 05:19:26
"Data Engineer, AVP",NatWest Markets,16 - 18 years,Not Disclosed,['Gurugram'],"Join us as a Data Engineer\nWe re looking for someone to build effortless, digital first customer experiences to help simplify our organisation and keep our data safe and secure\nDay-to-day, you ll develop innovative, data-driven solutions through data pipelines, modelling and ETL design while inspiring to be commercially successful through insights\nIf you re ready for a new challenge, and want to bring a competitive edge to your career profile by delivering streaming data ingestions, this could be the role for you\nWere offering this role at assistant vice president level\nWhat you ll do\nYour daily responsibilities will include you developing a comprehensive knowledge of our data structures and metrics, advocating for change when needed for product development. You ll also provide transformation solutions and carry out complex data extractions.\nWe ll expect you to develop a clear understanding of data platform cost levels to build cost-effective and strategic solutions. You ll also source new data by using the most appropriate tooling before integrating it into the overall solution to deliver it to our customers.\nYou ll also be responsible for:\nDriving customer value by understanding complex business problems and requirements to correctly apply the most appropriate and reusable tools to build data solutions\nParticipating in the data engineering community to deliver opportunities to support our strategic direction\nCarrying out complex data engineering tasks to build a scalable data architecture and the transformation of data to make it usable to analysts and data scientists\nBuilding advanced automation of data engineering pipelines through the removal of manual stages\nLeading on the planning and design of complex products and providing guidance to colleagues and the wider team when required\nThe skills you ll need\nTo be successful in this role, you ll have an understanding of data usage and dependencies with wider teams and the end customer. You ll also have experience of extracting value and features from large scale data.\nWe ll expect you to have experience of ETL technical design, data quality testing, cleansing and monitoring, data sourcing, exploration and analysis, and data warehousing and data modelling capabilities.\nYou ll also need:\nExperience of using programming languages alongside knowledge of data and software engineering fundamentals\nGood knowledge of modern code development practices\nGreat communication skills with the ability to proactively engage with a range of stakeholders\nHours\n45\nJob Posting Closing Date:\n16/06/2025",Industry Type: Banking,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Automation', 'Usage', 'Technical design', 'Programming', 'Data structures', 'Data quality', 'Assistant Vice President', 'Data warehousing', 'Monitoring', 'Data architecture']",2025-06-13 05:19:28
Azure Data Engineer (Azure Databricks must),Fortune India 500 IT Services Firm,5 - 8 years,Not Disclosed,['Hyderabad'],"We are looking for an experienced Azure Data Engineer with strong expertise in Azure Databricks to join our data engineering team.\n\nMandatory skill- Azure Databricks\nExperience- 5 to 8 years\nLocation- Hyderabad\nKey Responsibilities:\nDesign and build data pipelines and ETL/ELT workflows using Azure Databricks and Azure Data Factory\nIngest, clean, transform, and process large datasets from diverse sources (structured and unstructured)\nImplement Delta Lake solutions and optimize Spark jobs for performance and reliability\nIntegrate Azure Databricks with other Azure services including Data Lake Storage, Synapse Analytics, and Event Hubs\n\n\n\nInterested candidates share your CV at himani.girnar@alikethoughts.com with below details\n\nCandidate's name-\nEmail and Alternate Email ID-\nContact and Alternate Contact no-\nTotal exp-\nRelevant experience-\nCurrent Org-\nNotice period-\nCCTC-\nECTC-\nCurrent Location-\nPreferred Location-\nPancard No-",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Azure Databricks', 'Azure Data Factory', 'Pyspark', 'Azure Data Lake', 'SQL']",2025-06-13 05:19:29
DBA DATA Engineer || 12 Lakhs CTC,Robotics Technologies,8 - 9 years,12 Lacs P.A.,['Hyderabad( Banjara hills )'],"Dear Candidate,\nWe are seeking a skilled and experienced DBA Data Engineer to join our growing data team. The ideal candidate will play a key role in designing, implementing, and maintaining our databases and data pipeline architecture. You will collaborate with software engineers, data analysts, and DevOps teams to ensure efficient data flow, data integrity, and optimal database performance across all systems. This role requires a strong foundation in database administration, SQL performance tuning, data modeling, and experience with both on-prem and cloud-based environments.\n\nRequirements:\nBachelors degree in Computer Science, Information Systems, or a related field.\n8+ years of experience in database administration and data engineering.\nProven expertise in RDBMS (Oracle, MySQL, and PostgreSQL) and NoSQL systems (MongoDB, Cassandra).\nExperience managing databases in cloud environments (AWS, Azure, or GCP).\nProficiency in ETL processes and tools (e.g., Apache NiFi, Talend, Informatica, AWS Glue).\nStrong experience with scripting languages such as Python, Bash, or PowerShell.\n\nDBA Data Engineer Roles & Responsibilities:\nDesign and maintain scalable and high-performance database architectures.\nMonitor and optimize database performance using tools like CloudWatch, Oracle Enterprise Manager, pgAdmin, Mongo Compass, or Dynatrace.\nDevelop and manage ETL/ELT pipelines to support business intelligence and analytics.\nEnsure data integrity and security through best practices in backup, recovery, and encryption.\nAutomate regular database maintenance tasks using scripting and scheduled jobs.\nImplement high availability, failover, and disaster recovery strategies.\nConduct performance tuning of queries, stored procedures, indexes, and table structures.\nCollaborate with DevOps to automate database deployments using CI/CD and IaC tools (e.g., Terraform, AWS CloudFormation).\nDesign and implement data models, including star/snowflake schemas for data warehousing.\nDocument data flows, data dictionaries, and database configurations.\nManage user access and security policies using IAM roles or database-native permissions.\nAnalyze existing data systems and propose modernization or migration plans (on-prem to cloud, SQL to NoSQL, etc.).\nUse AWS RDS, Amazon Redshift, Azure SQL Database, or Google BigQuery as needed.\nStay up-to-date with emerging database technologies and make recommendations.\n\nMust-Have Skills:\nDeep knowledge of SQL and database performance tuning.\nHands-on experience with database migrations and replication strategies.\nFamiliarity with data governance, data quality, and compliance frameworks (GDPR, HIPAA, etc.).\nStrong problem-solving and troubleshooting skills.\nExperience with data streaming platforms such as Apache Kafka, AWS Kinesis, or Apache Flink is a plus.\nExperience with data lake and data warehouse architectures.\nExcellent communication and documentation skills.\n\nSoft Skills:\nProblem-Solving: Ability to analyze complex problems and develop effective solutions.\nCommunication Skills: Strong verbal and written communication skills to effectively collaborate with cross-functional teams.\nAnalytical Thinking: Ability to think critically and analytically to solve technical challenges.\nTime Management: Capable of managing multiple tasks and deadlines in a fast-paced environment.\nAdaptability: Ability to quickly learn and adapt to new technologies and methodologies.\n\nInterview Mode : F2F for who are residing in Hyderabad / Zoom for other states\nLocation : 43/A, MLA Colony,Road no 12, Banjara Hills, 500034\nTime : 2 - 4pm",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Database Administration', 'Database Migration', 'Dba Skills', 'High Availability', 'Db Administration', 'Hadr', 'Database Security', 'Disaster Recovery', 'Dr Testing', 'DR', 'Luw', 'Db Upgrade', 'Brtools', 'UDDI', 'Os Migration', 'Dbase', 'DBMS', 'IBM DB2', 'Db Migration', 'Disaster Recovery Planning', 'Db Dba', 'Backup And Recovery']",2025-06-13 05:19:31
Snowflake Data Engineer,Prudent Globaltech Solutions,7 - 12 years,16-27.5 Lacs P.A.,['Hyderabad( Madhapur )'],"Job Description Data Engineer\nWe are seeking a highly skilled Data Engineer with extensive experience in Snowflake, Data Build Tool (dbt), Snaplogic, SQL Server, PostgreSQL, Azure Data Factory, and other ETL tools. The ideal candidate will have a strong ability to optimize SQL queries and a good working knowledge of Python. A positive attitude and excellent teamwork skills are essential.\n\nRole & responsibilities\nData Pipeline Development: Design, develop, and maintain scalable data pipelines using Snowflake, DBT, Snaplogic, and ETL tools.",,,,"['Snowflake', 'Azure Data Factory', 'ADF', 'Data Engineer', 'ETL', 'SQL']",2025-06-13 05:19:33
Snowflake Data Engineer,Epam Systems,5 - 10 years,Not Disclosed,['Chennai'],"Key Skills:\nSnowflake (Snow SQL, Snow PLSQL and Snowpark)\nStrong Python\nAirflow/DBT\nAny DevOps tools\nAWS/Azure Cloud Skills\n\nRequirements:\nLooking for engineer for information warehouse\nWarehouse is based on AWS/Azure, DBT, Snowflake.\nStrong programming experience with Python.\nExperience with workflow management tools like Argo/Oozie/Airflow.\nExperience in Snowflake modelling - roles, schema, databases\nExperience in data Modeling (Data Vault).\nExperience in design and development of data transformation pipelines using the DBT framework.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Snowflake', 'Python', 'Azure Cloud', 'AWS', 'SQL']",2025-06-13 05:19:35
Azure Data Engineer,CODERZON Technologies Pvt Ltd,3 - 8 years,6-18 Lacs P.A.,['Kochi'],"Looking for a Data Engineer with 3+ yrs exp in Azure Data Factory, Synapse, Data Lake, Databricks, SQL, Python, Spark, CI/CD. Preferred: DP-203 cert, real-time data tools (Kafka, Stream Analytics), data governance (Purview), Power BI.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Data Factory', 'Azure Synapse', 'Pyspark', 'Azure Databricks', 'Azure Data Lake', 'SQL Azure', 'Python']",2025-06-13 05:19:36
Hadoop Data Engineer,Envision Technology Solutions,3 - 8 years,5-15 Lacs P.A.,"['New Delhi', 'Hyderabad', 'Gurugram']","Primary Skill – Hadoop, Hive, Python, SQL, Pyspark/Spark.\nLocation –Hyderabad / Gurgaon;",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Hadoop', 'Hive', 'Spark', 'Python', 'SQL']",2025-06-13 05:19:38
Lead Infrastructure Engineer - Data Platforms,General Mills,6 - 10 years,Not Disclosed,['Mumbai'],"Position Title\nLead Infrastructure Engineer - Data Platforms\nFunction/Group\nDigital and Technology\nLocation\nMumbai\nShift Timing\nGeneral\nRole Reports to\nDT Manager Cloud Data Platforms\nRemote/Hybrid/in-Office\nHybrid\nABOUT GENERAL MILLS\nWe make food the world loves: 100 brands. In 100 countries. Across six continents. With iconic brands like Cheerios, Pillsbury, Betty Crocker, Nature Valley, and H agen-Dazs, we ve been serving up food the world loves for 155 years (and counting). Each of our brands has a unique story to tell.\nHow we make our food is as important as the food we make. Our values are baked into our legacy and continue to accelerate\nus into the future as an innovative force for good. General Mills was founded in 1866 when Cadwallader Washburn boldly bought the largest flour mill west of the Mississippi. That pioneering spirit lives on today through our leadership team who upholds a vision of relentless innovation while being a force for good. For more details check out http://www.generalmills.com\nGeneral Mills India Center (GIC) is our global capability center in Mumbai that works as an extension of our global organization delivering business value, service excellence and growth, while standing for good for our planet and people.\nWith our team of 1800+ professionals, we deliver superior value across the areas of Supply chain (SC) , Digital Technology (DT) Innovation, Technology Quality (ITQ), Consumer and Market Intelligence (CMI), Sales Strategy Intelligence (SSI) , Global Shared Services (GSS) , Finance Shared Services (FSS) and Human Resources Shared Services (HRSS).For more details check out https://www.generalmills.co.in\nWe advocate for advancing equity and inclusion to create more equitable workplaces and a better tomorrow.\nJOB OVERVIEW\nFunction Overview\nThe Digital and Technology team at General Mills stands as the largest and foremost unit, dedicated to exploring the latest trends and innovations in technology while leading the adoption of cutting-edge technologies across the organization. Collaborating closely with global business teams, the focus is on understanding business models and identifying opportunities to leverage technology for increased efficiency and disruption. The teams expertise spans a wide range of areas, including AI/ML, Data Science, IoT, NLP, Cloud, Infrastructure, RPA and Automation, Digital Transformation, Cyber Security, Blockchain, SAP S4 HANA and Enterprise Architecture. The MillsWorks initiative embodies an agile@scale delivery model, where business and technology teams operate cohesively in pods with a unified mission to deliver value for the company. Employees working on significant technology projects are recognized as Digital Transformation change agents.\nThe team places a strong emphasis on service partnerships and employee engagement with a commitment to advancing equity and supporting communities. In fostering an inclusive culture, the team values individuals passionate about learning and growing with technology, exemplified by the Work with Heartphilosophy, emphasizing results over facetime. Those intrigued by the prospect of contributing to the digital transformation journey of a Fortune 500 company are encouraged to explore more details about the function through the provided Link\nPurpose of the role\nThe Digital and Technology team of General Mills India Centre is looking for a MS SQL Server / Cloud database administrator (DBA) with Operations-oriented/DevOps skill set to support Cloud/On Prem databases. This opportunity is in a fast-paced environment as we migrate and refactor enterprise-scale databases from our on-premises datacenters to a cloud environment. The ideal candidate will have experience operating in a fast-paced, complex, and multi-platform environment and will contribute to the strategic direction of our database infrastructure.\nKEY ACCOUNTABILITIES\nProvide technical leadership for migrating existing Microsoft SQL Server and NoSQL Databases to Public Cloud, developing and owning migration processes, documentation, and tooling. This includes defining strategies and roadmaps for database migrations, ensuring alignment with overall IT strategy and leveraging automation wherever possible to streamline the process.\nExpertly administer and manage mission-critical, complex, and high-volume Database Platforms in a 24/7 environment, implementing and maintaining automated monitoring and alerting systems to proactively identify and address potential issues.\nProactively identify and address potential database performance bottlenecks, proposing and implementing automated solutions to optimize database efficiency and scalability. This includes developing and deploying automated scripts for performance tuning and optimization.\nLead the design and implementation of highly available and scalable database solutions in the cloud (GCP preferred), ensuring compliance with security and governance standards and utilizing Infrastructure as Code (IaC) for automated provisioning and management of database infrastructure.\nAdminister and troubleshoot SQL Server, PostgreSQL, MySQL, and NoSQL DBs (MongoDB), implementing best practices and resolving complex performance issues through automated scripting and tooling.\nDevelop and maintain comprehensive documentation for database systems, processes, and procedures.\nChampion the adoption of DevOps practices, including CI/CD, infrastructure as code (Terraform), and automation (Ansible, Python, PowerShell) to streamline database management and deployment. Actively participate in the development and improvement of automated deployment pipelines.\nCollaborate with application stakeholders to understand their database requirements and provide technical guidance on database design and optimization, emphasizing automation opportunities to improve development workflows.\nContribute to the development and implementation of database security policies and procedures, ensuring compliance with industry best practices and leveraging automation for security auditing and vulnerability management.\nActively participate in Agile development processes, contributing to sprint planning, daily stand-ups, and retrospectives, focusing on automation opportunities to improve team efficiency and reduce manual effort.\nMentor and guide junior team members, fostering a culture of knowledge sharing and continuous learning, particularly around automation and DevOps practices.\nStay abreast of emerging technologies and trends in database administration and cloud computing, recommending and implementing innovative solutions to improve database performance and reliability, with a focus on automation and efficiency gains.\nMINIMUM QUALIFICATIONS\n9+ years of hands-on experience with leading design, refactoring, or migration of databases in cloud infrastructures and services for at least one of Microsoft Azure, Amazon Web Services, and Google GCP (preferred). Experience with automating database migration processes is highly desirable.\nExperience maintaining and administering with CI/CD tools such as Ansible, GitHub, and Artifactory in a cloud environment and developing/writing scripts using advanced DevOps languages such as Python, PowerShell. Demonstrated ability to design and implement automated workflows.\nExperience working with infrastructure as code such as Terraform, or equivalent. Proven ability to automate infrastructure provisioning and management.\nExperience with concepts, processes tools required for cloud adoption including cloud security, governance, and integration. Experience with automating security and compliance tasks is a plus.\nExperience with SQL Server AlwaysOn and Windows clustering. Experience with automating the management of high-availability clusters is preferred.\nExperience with agile techniques and methods.\nFamiliarity with user expectations for cloud Databases (to be able to design user-centric engineering services e.g., provisioning self-service workflow). Experience with automating user provisioning and access management is a plus.\nWorking knowledge of DevOps, Agile development processes, exploration, and POCs.\nAbility to work collaboratively across functional team boundaries.\nPREFERRED QUALIFICATIONS\nGood understanding hands-on experience of Linux OS and Windows Server (2012+).\nExperience working with high availability, disaster recovery, backup strategies, and server tuning strategies including parameters, resources, contention, etc.\nExcellent interpersonal and communication skills.\nAbility to work in a fast-paced team environment.\nFlexibility, reliability, initiative, responsibility, and a can-domentality.",Industry Type: Food Processing,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Automation', 'MS SQL', 'SAP', 'Linux', 'MySQL', 'Infrastructure', 'Workflow', 'Windows', 'SQL', 'Python']",2025-06-13 05:19:40
Cloud Data Platform Engineer - C,Capgemini,3 - 7 years,Not Disclosed,['Mumbai'],"\nA Data Platform Engineer specialises in the design, build, and maintenance of cloud-based data infrastructure and platforms for data-intensive applications and services. They develop Infrastructure as Code and manage the foundational systems and tools for efficient data storage, processing, and management. This role involves architecting robust and scalable cloud data infrastructure, including selecting and implementing suitable storage solutions, data processing frameworks, and data orchestration tools. Additionally, a Data Platform Engineer ensures the continuous evolution of the data platform to meet changing data needs and leverage technological advancements, while maintaining high levels of data security, availability, and performance. They are also tasked with creating and managing processes and tools that enhance operational efficiency, including optimising data flow and ensuring seamless data integration, all of which are essential for enabling developers to build, deploy, and operate data-centric applications efficiently.\n\n - Grade Specific \nAn expert on the principles and practices associated with data platform engineering, particularly within cloud environments, and demonstrates proficiency in specific technical areas related to cloud-based data infrastructure, automation, and scalability.Key responsibilities encompass:Team Leadership and ManagementSupervising a team of platform engineers, with a focus on team dynamics and the efficient delivery of cloud platform solutions.Technical Guidance and Decision-MakingProviding technical leadership and making pivotal decisions concerning platform architecture, tools, and processes. Balancing hands-on involvement with strategic oversight.Mentorship and Skill DevelopmentGuiding team members through mentorship, enhancing their technical proficiencies, and nurturing a culture of continual learning and innovation in platform engineering practices.In-Depth Technical ProficiencyPossessing a comprehensive understanding of platform engineering principles and practices, and demonstrating expertise in crucial technical areas such as cloud services, automation, and system architecture.Community ContributionMaking significant contributions to the development of the platform engineering community, staying informed about emerging trends, and applying this knowledge to drive enhancements in capability.\n\n Skills (competencies)",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['cloud services', 'cloud platform', 'cloud data flow', 'system architecture', 'data flow', 'hive', 'snowflake', 'python', 'airflow', 'microsoft azure', 'data warehousing', 'data engineering', 'sql', 'docker', 'spark', 'gcp', 'kafka', 'hadoop', 'sqoop', 'bigquery', 'big data', 'aws', 'etl', 'data integration']",2025-06-13 05:19:42
Data Platform Engineer,Accenture,7 - 12 years,Not Disclosed,['Hyderabad'],"Project Role :Data Platform Engineer\n\n\n\n\n\nProject Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification : Engineering graduate preferably Computer Science graduate 15 years of full time education\n\n\nSummary:As a Data Platform Engineer, you will assist with the data platform blueprint and design, collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. You will play a crucial role in shaping the data platform components.\nRoles & Responsibilities:- Expected to be an SME- Collaborate and manage the team to perform- Responsible for team decisions- Engage with multiple teams and contribute on key decisions- Provide solutions to problems for their immediate team and across multiple teams- Lead data platform blueprint and design- Implement data platform components effectively- Ensure seamless integration between systems and data models\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform- Strong understanding of data platform architecture- Experience in data integration and data modeling- Knowledge of cloud platforms like AWS or Azure- Hands-on experience with SQL and NoSQL databases\nAdditional Information:- The candidate should have a minimum of 7.5 years of experience in Databricks Unified Data Analytics Platform- This position is based at our Hyderabad office- An Engineering graduate preferably Computer Science graduate with 15 years of full-time education is required\n\nQualification\n\nEngineering graduate preferably Computer Science graduate 15 years of full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['sql', 'data modeling', 'data analytics', 'platform architecture', 'aws', 'kubernetes', 'continuous integration', 'openshift', 'docker', 'microservices', 'iot', 'java', 'git', 'gcp', 'devops', 'linux', 'jenkins', 'mysql', 'hadoop', 'big data', 'microsoft azure', 'cloud platforms', 'nosql', 'agile', 'data integration']",2025-06-13 05:19:44
Data Platform Engineer,Accenture,3 - 8 years,Not Disclosed,['Pune'],"Project Role :Data Platform Engineer\n\n\n\n\n\nProject Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Platform Engineer, you will assist with the data platform blueprint and design, collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. You will play a crucial role in shaping the data platform components.\nRoles & Responsibilities:- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Collaborate with Integration Architects and Data Architects to design and implement data platform components.- Ensure seamless integration between various systems and data models.- Develop and maintain data platform blueprints.- Provide technical expertise in data platform design and implementation.- Troubleshoot and resolve data platform related issues.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Strong understanding of data platform architecture and design principles.- Experience in implementing and optimizing data pipelines.- Knowledge of cloud-based data solutions.- Hands-on experience with data platform security and compliance measures.\nAdditional Information:- The candidate should have a minimum of 3 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Pune office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analytics', 'java', 'data modeling', 'platform architecture', 'design principles', 'hive', 'python', 'oracle', 'enterprise architecture', 'microsoft azure', 'data warehousing', 'sql', 'docker', 'infrastructure architecture', 'spark', 'gcp', 'design patterns', 'hadoop', 'agile', 'aws', 'big data']",2025-06-13 05:19:45
Data Platform Engineer,Accenture,7 - 12 years,Not Disclosed,['Pune'],"Project Role :Data Platform Engineer\n\n\n\n\n\nProject Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification : Engineering graduate preferably Computer Science graduate 15 years of full time education\n\n\nSummary:As a Data Platform Engineer, you will be responsible for assisting with the blueprint and design of the data platform components using Databricks Unified Data Analytics Platform. Your typical day will involve collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\nRoles & Responsibilities:- Assist with the blueprint and design of the data platform components using Databricks Unified Data Analytics Platform.- Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.- Develop and maintain data pipelines and ETL processes using Databricks Unified Data Analytics Platform.- Design and implement data security and access controls for the data platform.- Troubleshoot and resolve issues related to the data platform and data pipelines.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nExperience with Databricks Unified Data Analytics Platform.- Must To Have\n\n\n\n\nSkills:\nStrong understanding of data modeling and database design principles.- Good To Have\n\n\n\n\nSkills:\nExperience with cloud-based data platforms such as AWS or Azure.- Good To Have\n\n\n\n\nSkills:\nExperience with data security and access controls.- Good To Have\n\n\n\n\nSkills:\nExperience with data visualization tools such as Tableau or Power BI.\nAdditional Information:- The candidate should have a minimum of 7.5 years of experience in Databricks Unified Data Analytics Platform.- The ideal candidate will possess a strong educational background in computer science or a related field, along with a proven track record of delivering impactful data-driven solutions.- This position is based at our Bangalore, Hyderabad, Chennai and Pune Offices.- Mandatory office (RTO) for 2- 3 days and have to work on 2 shifts (Shift A- 10:00am to 8:00pm IST and Shift B - 12:30pm to 10:30 pm IST)\n\nQualification\n\nEngineering graduate preferably Computer Science graduate 15 years of full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['database design', 'data modeling', 'data analytics', 'microsoft azure', 'design principles', 'hive', 'sql', 'java', 'spark', 'design patterns', 'oops', 'mysql', 'hadoop', 'etl', 'big data', 'c#', 'rest', 'python', 'data security', 'power bi', 'javascript', 'sql server', 'data bricks', 'tableau', 'kafka', 'sqoop', 'aws']",2025-06-13 05:19:48
"AI/ML Engineer (Specializing in NLP/ML, Large Data Processing,",Synechron,8 - 10 years,Not Disclosed,"['Pune', 'Hinjewadi']","job requisition idJR1027361\n\nJob Summary\nSynechron seeks a highly skilled AI/ML Engineer specializing in Natural Language Processing (NLP), Large Language Models (LLMs), Foundation Models (FMs), and Generative AI (GenAI). The successful candidate will design, develop, and deploy advanced AI solutions, contributing to innovative projects that transform monolithic systems into scalable microservices integrated with leading cloud platforms such as Azure, Amazon Bedrock, and Google Gemini. This role plays a critical part in advancing Synechrons capabilities in cutting-edge AI technologies, enabling impactful business insights and product innovations.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCandidates with extensive research or academic experience in AI/ML, especially in NLP and large-scale data processing, are eligible if they have practical deployment experience.",,,,"['python', 'data management', 'data processing', 'pipeline', 'big data', 'continuous integration', 'kubernetes', 'deploying models', 'natural language processing', 'ci/cd', 'fms', 'artificial intelligence', 'docker', 'sql', 'microservices', 'tensorflow', 'java', 'pytorch', 'jenkins', 'keras', 'aws']",2025-06-13 05:19:50
Data Platform Engineer,Accenture,7 - 12 years,Not Disclosed,['Hyderabad'],"Project Role :Data Platform Engineer\n\n\n\n\n\nProject Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Platform Engineer, you will assist with the data platform blueprint and design, collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. You will play a crucial role in the development and maintenance of the data platform components, contributing to the overall success of the organization.\nRoles & Responsibilities:- Expected to be an SME, collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Assist with the data platform blueprint and design.- Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.- Develop and maintain data platform components.- Contribute to the overall success of the organization.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Strong understanding of statistical analysis and machine learning algorithms.- Experience with data visualization tools such as Tableau or Power BI.- Hands-on implementing various machine learning algorithms such as linear regression, logistic regression, decision trees, and clustering algorithms.- Solid grasp of data munging techniques, including data cleaning, transformation, and normalization to ensure data quality and integrity.\nAdditional Information:- The candidate should have a minimum of 7.5 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Hyderabad office.- 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analytics', 'tableau', 'machine learning algorithms', 'statistics', 'data munging', 'python', 'natural language processing', 'power bi', 'machine learning', 'sql', 'data bricks', 'data quality', 'r', 'data modeling', 'data science', 'predictive modeling', 'text mining', 'logistic regression']",2025-06-13 05:19:51
Quality Engineer-Data,IBM,2 - 5 years,Not Disclosed,['Kochi'],The ability to be a team player\nThe ability and skill to train other people in procedural and technical topics\nStrong communication and collaboration skills\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nAble to write complex SQL queries ;\nHaving experience in Azure Databricks\n\n\nPreferred technical and professional experience\nExcellent communication and stakeholder management skills,Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['azure databricks', 'sql queries', 'sql', 'data quality', 'stakeholder management', 'hive', 'python', 'azure data lake', 'scala', 'pyspark', 'microsoft azure', 'data warehousing', 'power bi', 'azure data factory', 'sql server', 'tableau', 'sql azure', 'spark', 'hadoop', 'big data', 'etl', 'ssis']",2025-06-13 05:19:53
Data Platform Engineer,Accenture,7 - 12 years,Not Disclosed,['Chennai'],"Project Role :Data Platform Engineer\n\n\n\n\n\nProject Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification : Engineering graduate preferably Computer Science graduate 15 years of full time education\n\n\nSummary:As a Data Platform Engineer, you will be responsible for assisting with the blueprint and design of the data platform components using Databricks Unified Data Analytics Platform. Your typical day will involve collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\nRoles & Responsibilities:- Assist with the blueprint and design of the data platform components using Databricks Unified Data Analytics Platform.- Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.- Develop and maintain data pipelines using Databricks Unified Data Analytics Platform.- Design and implement data security and access controls using Databricks Unified Data Analytics Platform.- Troubleshoot and resolve issues related to data platform components using Databricks Unified Data Analytics Platform.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nExperience with Databricks Unified Data Analytics Platform.- Good To Have\n\n\n\n\nSkills:\nExperience with other big data technologies such as Hadoop, Spark, and Kafka.- Strong understanding of data modeling and database design principles.- Experience with data security and access controls.- Experience with data pipeline development and maintenance.- Experience with troubleshooting and resolving issues related to data platform components.\nAdditional Information:- The candidate should have a minimum of 7.5 years of experience in Databricks Unified Data Analytics Platform.- The ideal candidate will possess a strong educational background in computer science or a related field, along with a proven track record of delivering impactful data-driven solutions.- This position is based at our Bangalore, Hyderabad, Chennai and Pune Offices.- Mandatory office (RTO) for 2- 3 days and have to work on 2 shifts (Shift A- 10:00am to 8:00pm IST and Shift B - 12:30pm to 10:30 pm IST)\n\nQualification\n\nEngineering graduate preferably Computer Science graduate 15 years of full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['database design', 'data modeling', 'spark', 'data analytics', 'design principles', 'hive', 'sql', 'java', 'design patterns', 'oops', 'mysql', 'hadoop', 'big data', 'etl', 'c#', 'rest', 'python', 'microsoft azure', 'javascript', 'sql server', 'data bricks', 'amazon ec2', 'kafka', 'troubleshooting', 'sqoop', 'aws']",2025-06-13 05:19:55
Data Platform Engineer,Accenture,7 - 12 years,Not Disclosed,['Chennai'],"Project Role :Data Platform Engineer\n\n\n\n\n\nProject Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification : Engineering graduate preferably Computer Science graduate 15 years of full time education\n\n\nSummary:As a Data Platform Engineer, you will be responsible for assisting with the blueprint and design of the data platform components using Databricks Unified Data Analytics Platform. Your typical day will involve collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\nRoles & Responsibilities:- Assist with the blueprint and design of the data platform components using Databricks Unified Data Analytics Platform.- Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.- Develop and maintain data pipelines using Databricks Unified Data Analytics Platform.- Design and implement data security and access controls using Databricks Unified Data Analytics Platform.- Troubleshoot and resolve issues related to data platform components using Databricks Unified Data Analytics Platform.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nExperience with Databricks Unified Data Analytics Platform.- Must To Have\n\n\n\n\nSkills:\nStrong understanding of data modeling and database design principles.- Good To Have\n\n\n\n\nSkills:\nExperience with cloud-based data platforms such as AWS or Azure.- Good To Have\n\n\n\n\nSkills:\nExperience with data security and access controls.- Good To Have\n\n\n\n\nSkills:\nExperience with data pipeline development and maintenance.\nAdditional Information:- The candidate should have a minimum of 7.5 years of experience in Databricks Unified Data Analytics Platform.- The ideal candidate will possess a strong educational background in computer science or a related field, along with a proven track record of delivering impactful data-driven solutions.- This position is based at our Chennai office.\n\nQualification\n\nEngineering graduate preferably Computer Science graduate 15 years of full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['database design', 'data modeling', 'data analytics', 'microsoft azure', 'design principles', 'hive', 'data warehousing', 'sql', 'java', 'spark', 'design patterns', 'mysql', 'hadoop', 'big data', 'etl', 'rest', 'python', 'data security', 'javascript', 'sql server', 'data bricks', 'pipeline', 'kafka', 'sqoop', 'aws']",2025-06-13 05:19:57
Data Platform Engineer,Accenture,7 - 12 years,Not Disclosed,['Chennai'],"Project Role :Data Platform Engineer\n\n\n\n\n\nProject Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification : Engineering graduate preferably Computer Science graduate 15 years of full time education\n\n\nSummary:As a Data Platform Engineer, you will be responsible for assisting with the blueprint and design of the data platform components using Databricks Unified Data Analytics Platform. Your typical day will involve collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\nRoles & Responsibilities:- Assist with the blueprint and design of the data platform components using Databricks Unified Data Analytics Platform.- Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.- Develop and maintain data pipelines using Databricks Unified Data Analytics Platform.- Design and implement data security and access controls using Databricks Unified Data Analytics Platform.- Troubleshoot and resolve issues related to data platform components using Databricks Unified Data Analytics Platform.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nExperience with Databricks Unified Data Analytics Platform.- Must To Have\n\n\n\n\nSkills:\nStrong understanding of data platform components and architecture.- Good To Have\n\n\n\n\nSkills:\nExperience with cloud-based data platforms such as AWS or Azure.- Good To Have\n\n\n\n\nSkills:\nExperience with data security and access controls.- Good To Have\n\n\n\n\nSkills:\nExperience with data pipeline development and maintenance.\nAdditional Information:- The candidate should have a minimum of 7.5 years of experience in Databricks Unified Data Analytics Platform.- The ideal candidate will possess a strong educational background in computer science or a related field, along with a proven track record of delivering impactful data-driven solutions.-This position is based at our Bangalore, Hyderabad, Chennai and Pune Offices.- Mandatory office (RTO) for 2- 3 days and have to work on 2 shifts (Shift A- 10:00am to 8:00pm IST and Shift B - 12:30pm to 10:30 pm IST)\n\nQualification\n\nEngineering graduate preferably Computer Science graduate 15 years of full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data analytics', 'data security', 'microsoft azure', 'data bricks', 'aws', 'hive', 'amazon redshift', 'pyspark', 'data warehousing', 'emr', 'sql', 'java', 'data modeling', 'spark', 'mysql', 'hadoop', 'big data', 'etl', 'python', 'machine learning', 'sql server', 'nosql', 'pipeline', 'amazon ec2', 'kafka', 'sqoop']",2025-06-13 05:19:59
Data Platform Engineer,Accenture,2 - 7 years,Not Disclosed,['Pune'],"Project Role :Data Platform Engineer\n\n\n\n\n\nProject Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Platform Engineer, you will assist with the data platform blueprint and design, collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. You will play a crucial role in the development and maintenance of the data platform components, contributing to the overall success of the project.\nRoles & Responsibilities:- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Assist with the data platform blueprint and design.- Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.- Develop and maintain data platform components.- Contribute to the overall success of the project.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Strong understanding of statistical analysis and machine learning algorithms.- Experience with data visualization tools such as Tableau or Power BI.- Hands-on implementing various machine learning algorithms such as linear regression, logistic regression, decision trees, and clustering algorithms.- Solid grasp of data munging techniques, including data cleaning, transformation, and normalization to ensure data quality and integrity.\nAdditional Information:- The candidate should have a minimum of 2 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Pune office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analytics', 'tableau', 'machine learning algorithms', 'statistics', 'data munging', 'python', 'data analysis', 'natural language processing', 'power bi', 'machine learning', 'sql', 'data quality', 'r', 'data modeling', 'data science', 'predictive modeling', 'text mining', 'logistic regression']",2025-06-13 05:20:01
Data Platform Engineer,Accenture,3 - 8 years,Not Disclosed,['Pune'],"Project Role :Data Platform Engineer\n\n\n\n\n\nProject Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Platform Engineer, you will assist with the data platform blueprint and design, collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. You will play a crucial role in the development and maintenance of the data platform components, contributing to the overall success of the project.\nRoles & Responsibilities:- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Assist with the data platform blueprint and design.- Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.- Develop and maintain data platform components.- Contribute to the overall success of the project.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Strong understanding of statistical analysis and machine learning algorithms.- Experience with data visualization tools such as Tableau or Power BI.- Hands-on implementing various machine learning algorithms such as linear regression, logistic regression, decision trees, and clustering algorithms.- Solid grasp of data munging techniques, including data cleaning, transformation, and normalization to ensure data quality and integrity.\nAdditional Information:- The candidate should have a minimum of 3 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Pune office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analytics', 'tableau', 'machine learning algorithms', 'statistics', 'data munging', 'python', 'data analysis', 'natural language processing', 'power bi', 'machine learning', 'sql', 'data quality', 'r', 'data modeling', 'data science', 'predictive modeling', 'text mining', 'logistic regression']",2025-06-13 05:20:03
"Level 2 Market Data Support Engineer (Windows, Linux, SQL, IM)",Synechron,3 - 7 years,Not Disclosed,['Pune'],"Job Summary\nSynechron is seeking a technically skilled and proactive Level 2 Market Data Support Engineer specializing in Market Data operations to join our dedicated back-office support team. This role is pivotal in maintaining the stability, security, and efficiency of our market data systems and related infrastructure. The ideal candidate will deliver advanced technical support, troubleshoot complex issues, and contribute to process improvements, ensuring seamless access and data accuracy in a highly regulated financial environment.\nThis position provides an opportunity to work with cross-functional teams, implement best practices, and support critical business functions. It requires a combination of technical expertise, analytical acumen, and effective communication skills to sustain the integrity of our market data platform and ensure high service standards.\nSoftware Requirements\nRequired Skills:\nProficiency with Windows Server management and troubleshooting (user accounts, system configuration)\nExperience managing and troubleshooting Unix/Linux environments\nStrong SQL skills for data retrieval, analysis, and troubleshooting (writing complex queries)\nHands-on experience with ITSM tools such as ServiceNow and JIRA for incident tracking\nFamiliarity with version control tools like Git for release and deployment management\nKnowledge of Batch Processing Systems to analyze historic system trends\nPreferred Skills:\nExposure to cloud platforms (AWS, Azure) is a plus\nKnowledge of monitoring tools and infrastructure automation techniques\nExperience with scripting languages such as PowerShell or Bash\nOverall Responsibilities\nProvide Level 2 support for market data systems, addressing incidents, service requests, and operational issues\nTroubleshoot and resolve technical issues related to Windows and Unix/Linux server environments, databases, and network/configuration discrepancies\nConduct root cause analysis to identify recurrence patterns and develop strategies for systemic resolution\nAssist with system upgrades, patches, and deployment procedures, ensuring minimal impact on business operations\nMonitor system health, review logs, and generate reports on system performance, data accuracy, and incident trends\nDocument procedures, incident resolutions, and system configurations to facilitate ongoing knowledge sharing\nCollaborate closely with cross-functional teams, including application support, infrastructure, and security teams to resolve technical problems effectively\nSupport incident progress tracking and facilitate resolution through ITSM tools\nWork in rotational shifts from 11:00 am IST to Midnight IST, ensuring 24/7 operational support\nStrategic objectives:\nEnsure high availability, data integrity, and security of market data systems\nEnhance incident response processes and reduce recurring issues\nDrive continuous process improvements and operational efficiencies\nPerformance outcomes:\nTimely resolution of incidents with minimal business impact\nAccurate documentation and effective communication with stakeholders\nSuccessful implementation of upgrades and systemic enhancements\nTechnical Skills (By Category)\nOperating Systems (Essential):\nWindows Server (administration, troubleshooting)\nUnix/Linux distributions (server management, scripting, troubleshooting)\nDatabases & Data Management (Essential):\nSQL query development and troubleshooting\nData integrity checks and analysis\nIncident & Change Management (Essential):\nUse of ServiceNow and JIRA platforms for incident management and tracking\nInfrastructure & Network (Essential):\nBasic understanding of networking and system configurations\nAbility to troubleshoot connectivity issues related to server and network\nScripting & Automation (Preferred):\nPowerShell, Bash scripting for routine automation and data analysis\nAdditional Skills (Preferred):\nMonitoring tools and dashboards (e.g., Nagios, LogicMonitor)\nCloud environments experience (AWS, Azure)\nExperience Requirements\n3 to 7 years of experience in market data support, IT operations, or application support roles within capital markets or financial sectors\nProven experience troubleshooting Windows and Unix/Linux server environments\nFamiliarity with database query formulation, analysis, and data reconciliation\nStrong incident management experience using ITSM tools (ServiceNow, JIRA)\nExperience supporting mission-critical financial systems is preferred\nAlternative pathways:\nCandidates with extensive technical support in related financial support roles demonstrating problem-solving and troubleshooting skills may be considered\nDay-to-Day Activities\nMonitor market data system logs and dashboards for anomalies\nTroubleshoot and resolve hardware, OS, database, and network issues\nAnalyze incident tickets, perform root cause analysis, and escalate as needed\nAssist in deploying patches, upgrades, and system changes with minimal disruption\nGenerate performance and incident trend reports\nMaintain detailed documentation, runbooks, and knowledge base articles\nCoordinate with infrastructure, application support, and security teams for issue resolution and process improvements\nSupport system audits, security compliance, and performance testing\nEngage in shift handovers, communicate incident status, and support team collaboration\nQualifications\nBachelor's degree in Computer Science, Information Technology, or related field; equivalent work experience accepted\nRelevant certifications such as ITIL Foundation, Certified Support Engineer, or industry-specific certifications are a plus\nWillingness to work rotational shifts from 11:00 am IST to Midnight IST\nDemonstrated technical expertise in Windows, Unix/Linux, databases, and incident management\nStrong analytical and troubleshooting skills with attention to detail\nEffective communicator capable of liaising with technical and non-technical stakeholders\nProfessional Competencies\nCritical thinking and advanced problem-solving capabilities\nExcellent verbal and written communication skills\nStakeholder-focused mindset with an emphasis on service quality\nStrong organizational skills for managing multiple incidents and tasks\nFlexibility and adaptability to changing priorities and shift schedules\nInitiative for continuous learning and process improvement\nCollaborative approach to team working and cross-departmental cooperation",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Market Data Support', 'Batch Processing Systems', 'ServiceNow', 'Git', 'Linux', 'Windows Server management', 'Unix/Linux', 'ITSM tools', 'Windows', 'Incident Management', 'SQL']",2025-06-13 05:20:05
Software Engineer Data Privacy,Bajaj Finserv Health,4 - 6 years,Not Disclosed,['Pune( Viman Nagar )'],"Job Description:\nKnowledgeable and proactive Data Privacy engineer to manage privacy and data protection initiatives in compliance with Indian data privacy regulations, including the Digital Personal Data Protection Act, 2023 (DPDP Act).\nThis role will work closely with legal, IT, security, HR, and business teams to implement privacy by design and foster a strong culture of privacy data protection\n\nKey Objectives/Responsibilities of this Role:\nImplement and oversee the data privacy program implementation in accordance with the Privacy Laws and requirements (DPDP Act, 2023 and other applicable Indian laws)\nDocument and maintain records of processing activities\nDraft, review, and update privacy notices, policies, consent mechanisms, and contractual clauses for data processing.\nConduct and review Data Protection Impact Assessments (DPIAs) and help identify and mitigate privacy risks.\nTrain and educate employees on Data privacy requirements and best practices.\nPerform regular compliance audits and assessments across internal departments and third-party vendors.\nDevelop and deliver training and awareness programs to ensure employees understand their responsibilities under the DPDP Act and internal privacy policies.\nHandle and ensure timely response to data principal rights requests (access, correction, erasure, grievance redressal) as per Privacy Law requirements\nCollaborate with legal and IT/security teams to ensure privacy by design and default in new processes, technologies, and products.\nStay updated with latest developments in data protection laws and regulations.\nSupport incident response and breach notification processes as per regulatory timelines and requirements.\nAssist the Data Privacy Officer in preparing reports for internal leadership or regulators, as required.\nServe as a subject matter expert on Indian data privacy regulations and provide guidance to cross-functional teams.\n\nMandatory Skillset & Experience:\nMinimum 4+ years of relevant experience in information security, compliance, or legal roles with focus on data privacy\nIn-depth knowledge of the Data Protection and Privacy Regulations, and awareness of related Indian IT regulations.\nExperienced in using Data protection management tools and softwares\nUnderstanding of global data privacy frameworks (GDPR, etc.) is a plus.\nStrong analytical and problem-solving skills with a practical approach to risk and compliance.\nAbility to communicate complex privacy topics in a clear and business-friendly manner.\nExperience working with privacy compliance tools and risk management systems.\n\nBehavioral Skills:\nSelf-driven and proactive.\nPlan and execute projects in a timely fashion meeting the project timelines\nExperienced in coordinating activities across different teams and stakeholders\nContinuous process improvement / quality assurance experience is a plus\nProactively identify potential data privacy issues and problems in business processes\n\nPreferred Qualification:\nBachelors or Masters degree in law, Technology, Compliance, Risk Management, or a related field.\n\nPreferred Certifications:\nDCPP (Data Protection Certified Professional - India)\nCIPP/A, CIPM (IAPP certifications)\nISO/IEC 27701, ISO 27001 (Good to have)",Industry Type: Medical Services / Hospital,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Data Protection Manager', 'Privacy Regulation', 'Data Privacy Law', 'Data Privacy', 'Gdpr']",2025-06-13 05:20:06
Lead Engineer - Data Science,Sasken Technologies,5 - 8 years,Not Disclosed,['Bengaluru'],"Job Summary\nPerson at this position takes ownership of a module and associated quality and delivery. Person at this position provides instructions, guidance and advice to team members to ensure quality and on time delivery.\nPerson at this position is expected to be able to instruct and review the quality of work done by technical staff.\nPerson at this position should be able to identify key issues and challenges by themselves, prioritize the tasks and deliver results with minimal direction and supervision.\nPerson at this position has the ability to investigate the root cause of the problem and come up alternatives/ solutions based on sound technical foundation gained through in-depth knowledge of technology, standards, tools and processes.\nPerson has the ability to organize and draw connections among ideas and distinguish between those which are implementable.\nPerson demonstrates a degree of flexibility in resolving problems/ issues that atleast to in-depth command of all techniques, processes, tools and standards within the relevant field of specialisation.\n\n\nRoles & Responsibilities\nResponsible for requirement analysis and feasibility study including system level work estimation while considering risk identification and mitigation.\nResponsible for design, coding, testing, bug fixing, documentation and technical support in the assigned area. Responsible for on time delivery while adhering to quality and productivity goals.\nResponsible for traceability of the requirements from design to delivery Code optimization and coverage.\nResponsible for conducting reviews, identifying risks and ownership of quality of deliverables.\nResponsible for identifying training needs of the team.\nExpected to enhance technical capabilities by attending trainings, self-study and periodic technical assessments.\nExpected to participate in technical initiatives related to project and organization and deliver training as per plan and quality.\nExpected to be a technical mentor for junior members.\nPerson may be given additional responsibility of managing people based on discretion of Project Manager.\n\nEducation and Experience Required\nEngineering graduate, MCA, etc Experience: 5-8 years\n\n\nCompetencies Description\nData Science TCB is applicable to one who\n1) Analyses data to arrive at patterns/Insights/models\n2) Come up with models based on the data to provide recommendations, predictive analytics etc\n3) Provides implementation of the models in R, Matlab etc\n4) Can understand and apply machine learning/AI techniques\nPlatforms-\nUnix\nTools-\nR, Matlab, Spark Machine Learning, Python-ML, SPSS, SAS\nLanguages-\nR, Perl, Python, Scala\nSpecialization-\nCOGNITIVE ANALYTICS INCLUDING COMPUTER VISION, AI and ML, STATISTICS",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Unix', 'R', 'SAS', 'Scala', 'Perl', 'SPSS', 'Machine Learning', 'Python']",2025-06-13 05:20:08
"Sr. Staff Engineer, Data Frameworks",NetSkope Software,10 - 15 years,Not Disclosed,"['Kolkata', 'Mumbai', 'New Delhi', 'Hyderabad', 'Pune', 'Chennai', 'Bengaluru']","As a Sr. Staff Engineer on the Data Engineering Team you'll be working on some of the hardest problems in the field of Data, Cloud and Security with a mission to achieve the highest standards of customer success. You will be building blocks of technology that will define Netskope s future. You will leverage open source Technologies around OLAP, OLTP, Streaming, Big Data and ML models. You will help design, and build an end-to-end system to manage the data and infrastructure used to improve security insights for our global customer base.\nYou will be part of a growing team of renowned industry experts in the exciting space of Data and Cloud Analytics\nYour contributions will have a major impact on our global customer-base and across the industry through our market-leading products\nYou will solve complex, interesting challenges, and improve the depth and breadth of your technical and business skills.\nWhat you will be doing\nConceiving and building services used by Netskope products to validate, transform, load and perform analytics of large amounts of data using distributed systems with cloud scale and reliability.\nHelping other teams architect their applications using services from the Data team wile using best practices and sound designs.\nEvaluating many open source technologies to find the best fit for our needs, and contributing to some of them.\nWorking with the Application Development and Product Management teams to scale their underlying services\nProviding easy-to-use analytics of usage patterns, anticipating capacity issues and helping with long term planning\nLearning about and designing large-scale, reliable enterprise services.\nWorking with great people in a fun, collaborative environment.\nCreating scalable data mining and data analytics frameworks using cutting edge tools and techniques\nRequired skills and experience\n10+ years of industry experience building highly scalable distributed Data systems\nProgramming experience in Python, Java or Golang\nExcellent data structure and algorithm skills\nProven good development practices like automated testing, measuring code coverage.\nProven experience developing complex Data Platforms and Solutions using Technologies like Kafka, Kubernetes, MySql, Hadoop, Big Query and other open source databases\nExperience designing and implementing large, fault-tolerant and distributed systems around columnar data stores.\nExcellent written and verbal communication skills\nBonus points for contributions to the open source community\nEducation\nBSCS or equivalent required, MSCS or equivalent strongly preferred",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Product management', 'data security', 'MySQL', 'OLAP', 'Application development', 'Open source', 'Data mining', 'OLTP', 'Distribution system', 'Python']",2025-06-13 05:20:10
Data Science & AI Engineer,Blue Altair,5 - 8 years,Not Disclosed,['Pune'],"Greetings from Blue Altair!\nJob Overview:\nWe are seeking an experienced and highly skilled Data Science and AI Engineer to join our dynamic team. The ideal candidate will have 5+ years of experience working on cutting-edge data science and AI technologies across various cloud platforms with a strong focus to work on LLMs and SLMs. The role demands a professional capable of performing in a client-facing environment, as well as mentoring and guiding junior team members.\n\nTitle: Consultant/Sr. Consultant - Data Science Engineer\nExperience: 5-8 years\nLocation: Pune/Bangalore (Hybrid)\n\nRoles and responsibilities:\nDevelop, implement, and optimize machine learning models and AI algorithms to solve complex business problems.\nDesign, build, and fine-tune AI models, particularly focusing on LLMs and SLMs, using state-of-the-art techniques and architectures.\nApply advanced techniques in prompt engineering, model fine-tuning, and optimization to tailor models for specific business needs.\nDeploy and manage machine learning models and pipelines on cloud platforms (AWS, GCP, Azure, etc.).\nWork closely with clients to understand their data and AI needs and provide tailored solutions.\nCollaborate with cross-functional teams to integrate AI solutions into broader software architectures.\nMentor junior team members and provide guidance in implementing best practices in data science and AI development.\nStay up-to-date with the latest trends and advancements in data science, AI, and cloud technologies.\nPrepare technical documentation and present insights to both technical and non-technical stakeholders.\n\nRequirement:\n5+ years of experience in data science, machine learning, and AI technologies.\nProven experience working with cloud platforms such as Google Cloud, Microsoft Azure, or AWS.\nExpertise in programming languages such as Python, R, Julia, and AI frameworks like TensorFlow, PyTorch, Scikit-learn, Hugging face Transformers.\nKnowledge of data visualization tools (e.g., Matplotlib, Seaborn, Tableau)\nSolid understanding of data engineering concepts including ETL, data pipelines, and databases (SQL, NoSQL).\nExperience with MLOps practices and deployment of models in production environments.\nFamiliarity with NLP (Natural Language Processing) tasks and working with large-scale datasets.\nHands-on experience with generative AI models like GPT, Gemini, Claude, Mistral etc.\nClient-facing experience with strong communication skills to manage and engage stakeholders.\nStrong problem-solving skills and analytical mindset.\nAbility to work independently and as part of a team and mentor and provide technical leadership to junior team members.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['LLMs', 'Artificial Intelligence', 'MLOps', 'RAG', 'Natural Language Processing', 'Neural Networks', 'LLM', 'Machine Learning', 'AI Models', 'Data Science', 'PyTorch', 'SLM', 'AI Automation']",2025-06-13 05:20:12
Senior Engineering Manager - Data Operations,Amgen Inc,12 - 15 years,Not Disclosed,['Hyderabad'],"ABOUT THE ROLE\nRole Description:\nWe are seeking a seasoned Senior Engineering Manager(Data Engineering) to lead the end-to-end management of enterprise data assets and operational data workflows. This role is critical in ensuring the availability, quality, consistency, and timeliness of data across platforms and functions, supporting analytics, reporting, compliance, and digital transformation initiatives.As a senior leader in the data organization, you will oversee the day-to-day data operations, manage a team of data professionals, and drive process excellence in data intake, transformation, validation, and delivery. You will work closely with cross-functional teams including data engineering, analytics, IT, governance, and business stakeholders to align operational data capabilities with enterprise needs.",,,,"['Data Operations', 'Azure', 'Data Engineering', 'Neo4J', 'GCP', 'Engineering Management', 'troubleshooting', 'Stardog', 'Marklogic', 'AWS']",2025-06-13 05:20:13
"Sr Validation Engineer, R&D Data Catalyst Team",Amgen Inc,4 - 6 years,Not Disclosed,['Hyderabad'],"Role Description:\nThe Validation and Testing Leadis responsible forleading the testing activities for software applications and solutions that meet business needs and ensuring the availability of critical systems and applications. This roleis for a lead tester with experience testing data management solutions and data analytics products, and with experience with designing and executing testing for GxP validated systems. The role involves working closely with product managers, designers, and other engineers to test high-quality, scalable software solutions.\nRoles & Responsibilities:",,,,"['Validation engineering', 'project management', 'data analytics', 'data validation', 'troubleshooting', 'agile', 'change management', 'sql', 'data profiling', 'jira']",2025-06-13 05:20:15
Senior Associate Data Security Engineer,Amgen Inc,1 - 3 years,Not Disclosed,['Hyderabad'],"What you will do\nIn this vital role the Senior Associate Data Security Engineer role will cover Data Loss Prevention (DLP) and Data Security Posture Management (DSPM) technologies. This role will report to the Manager, Data Security. This position will provide essential services that enable us to better pursue our mission.\nSr. Associate Data Security Engineers operate, manage, and improve Amgens DLP and DSPM solutions. In our Data Security team, they will operate data protection security technologies in a rapidly changing global security sector. They will work with other engineers and business units to help craft, build, coordinate, configure, and implement critical preventive and detective security controls related to the protection of Amgen data.",,,,"['Data Security', 'PowerShell', 'configuration management', 'Cloud Access Security', 'Linux', 'Incident management', 'ITIL', 'Python', 'SQL']",2025-06-13 05:20:17
Senior Staff Engineer - Data Science,The TJX Companies Inc,10 - 15 years,Not Disclosed,['Hyderabad'],"TJX Companies\nAt TJX Companies, every day brings new opportunities for growth, exploration, and achievement. You ll be part of our vibrant team that embraces diversity, fosters collaboration, and prioritizes your development. Whether you re working in our four global Home Offices, Distribution Centers or Retail Stores TJ Maxx, Marshalls, Homegoods, Homesense, Sierra, Winners, and TK Maxx, you ll find abundant opportunities to learn, thrive, and make an impact. Come join our TJX family a Fortune 100 company and the world s leading off-price retailer.\nJob Description:\nSr.Staff Engineer- Data Science\nWhat you ll discover\nInclusive culture and career growth opportunities\nA truly Global IT Organization that collaborates across North America, Europe, Asia and Australia, click here to learn more\nChallenging, collaborative, and team-based environment\nWhat you ll do\nThe Global Supply Chain - Logistics Team is responsible for managing various supply chain logistics related solutions within TJX IT. The organization delivers capabilities that enrich the customer experience and provide business value. We seek a motivated, talented Staff Engineer with good understanding of cloud base, database and BI concepts to help architect enterprise reporting solutions across global buying, planning and allocations.\nWhat you ll need\nThe Global Supply Chain - Logistics Team thrives on strong relationships with our business partners and working diligently to address their needs which supports TJX growth and operational stability. On this tightly knit and fast-paced solution delivery team you will be constantly challenged to stretch and think outside the box.\nYou will be working with product teams, architecture and business partners to strategically plan and deliver the product features by connecting the technical and business worlds. You will need to break down complex problems into steps that drive product development while keeping product quality and security as the priority. You will be responsible for most architecture, design and technical decisions within the assigned scope.\nKey Responsibilities:\nDesign, develop, test and deploy AI solutions using Azure AI services to meet business requirements.\nTrain, fine-tune, and evaluate AI models, including large language models (LLMs), ensuring they meet performance criteria and integrate seamlessly into new or existing solutions.\nDevelop and integrate APIs to enable smooth interaction between AI models and other applications, facilitating efficient model serving.\nCollaborate effectively with cross-functional teams, including data scientists, software engineers, and business stakeholders, to deliver comprehensive AI solutions.\nOptimize AI and ML model performance through techniques such as hyperparameter tuning and model compression to enhance efficiency and effectiveness.\nMonitor and maintain AI systems, providing technical support and troubleshooting to ensure continuous operation and reliability.\nCreate comprehensive documentation for AI solutions, including design documents, user guides, and operational procedures, to support development and maintenance.\nStay updated with the latest advancements in AI, machine learning, and cloud technologies, demonstrating a commitment to continuous learning and improvement.\nDesign, code, deploy, and support software components, working collaboratively with AI architects and engineers to build impactful systems and services.\nLead medium to large initiatives, prioritizing and assigning tasks, providing guidance, and resolving issues to ensure successful project delivery.\nMinimum Qualifications\nBachelors degree in computer science, engineering, or related field\n10+ years of experience in data/software engineering, design, implementation and architecture.\nAt least 5+ years of hands-on experience in developing AI/ML solutions, with a focus on deploying them in a cloud environment.\nDeep understanding of AI and ML algorithms with focus on Operations Research / Optimization knowledge (preferably Metaheuristics / Genetic Algorithms).\nStrong programming skills in Python with advanced OOPS concepts.\nGood understanding of structured, semi structured, and unstructured data, Data modelling, Data analysis, ETL and ELT.\nProficiency with Databricks & PySpark.\nExperience with MLOps practices including CI/CD for machine learning models.\nKnowledge of security best practices for deploying AI solutions, including data encryption and access control.\nKnowledge of ethical considerations in AI, including bias detection and mitigation strategies.\nThis role operates in an Agile/Scrum environment and requires a solid understanding of the full software lifecycle, including functional requirement gathering, design and development, testing of software applications, and documenting requirements and technical specifications.\nFully Owns Epic with limited or no guidance. Gives guidance and unblocks others; finds opportunities to mentor and grow teammates.\nDemonstrated leadership in the fields of Data/ Software Engineering or data science.\nStrong communication and influence skills. Solid team leadership with mentorship skills\nAbility to understand the work environment and competing priorities in conjunction with developing/meeting project goals\nShows a positive, open-minded and can-do attitude\nExperience in the following technologies:\nAdvanced Python programming (OOPS)\nOperations Research / Optimization knowledge (preferably Metaheuristics / Genetic Algorithms)\nDatabricks with Pyspark\nAzure / Cloud knowledge\nGithub / version control\nFunctional knowledge on Supply Chain / Logistics is preferred.\nIn addition to our open door policy and supportive work environment, we also strive to provide a competitive salary and benefits package. TJX considers all applicants for employment without regard to race, color, religion, gender, sexual orientation, national origin, age, disability, gender identity and expression, marital or military status, or based on any individuals status in any group or class protected by applicable federal, state, or local law. TJX also provides reasonable accommodations to qualified individuals with disabilities in accordance with the Americans with Disabilities Act and applicable state and local law.\nAddress:\nSalarpuria Sattva Knowledge City, Inorbit Road\nLocation:\nAPAC Home Office Hyderabad IN",Industry Type: Retail,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Supply chain', 'Computer science', 'Data analysis', 'Version control', 'Machine learning', 'Troubleshooting', 'Project delivery', 'Technical support', 'Python', 'Logistics']",2025-06-13 05:20:18
Staff Software Engineer - Cloud Data Pipeline,Calix,12 - 15 years,Not Disclosed,['Bengaluru'],"This position is based in Bangalore, India.\n\nCalix is leading a service provider transformation to deliver a differentiated subscriber experience around the Smart Home and Business, while monetizing their network using Role based Cloud Services, Telemetry, Analytics, Automation, and the deployment of Software Driven Adaptive networks.\nAs part of a high performing global team, the right candidate will play a significant role as Calix Cloud Data Engineer involved in architecture design, implementation, technical leadership in data ingestion, extraction, and transformation domain.\nResponsibilities and Duties:\nTechnical leadership in all phases of software design and development in meeting requirements of service stability, reliability, scalability, and security.\nHiring, training, provide technical direction and lead discussions and coordinate deliverables across multiple engineering teams globally.\nWork closely with Cloud product owners to understand, analyze product requirements, provide feedback, coordinate resources and deliver a complete solution.\nDrive evaluation and selection of best fit, efficient, cost-effective solution stack for the Calix Cloud data platform.\nDrive development of scalable data pipeline infrastructure and services for enabling operationally efficient analytics solutions for Calix Cloud suite of products.\nCreate and extend data lake solution to enable data science workbenches and implement quality systems to ensure data quality, consistency, security, compliance, and lineage.\nDrive continuous optimization of the data pipelines with automation, and tools.\nHave a Test first mindset and use modern DevSecOps practices for Agile development.\nCollaborate with senior leadership to translate platform opportunities into an actionable roadmap, track progress, and deliver new platform capabilities on-time and on-budget.\nTriage and resolve customer escalations and technical issues.\nQualifications:\n12 + years of highly technical, hands-on software engineering experience with at least 7 years of Cloud based solution development\n3+ experience leading and mentoring engineering team with strong technical direction and delivering high quality software on schedule, including delivery for large, cross-functional projects and working with geographically distributed teams\nStrong, creative problem-solving skills and ability to abstract and share details to create meaningful articulation.\nPassionate about delivering high quality software solutions and enabling automation in all phases.\nGood understanding of big data engineering challenges and proven experience with data platform engineering (batch and streaming, ingestion, storage, processing, management, governance, integration, consumption patterns)\nExperience in designing and performance tuning batch-based, low latency real-time streaming and event-based data solutions (Kafka, Spark, Flink or similar frameworks).\nPractical experience of architecting with GCP Cloud platform and services and especially the Data ecosystem: BigQuery, Datastream, DataProc, Composer etc.\nDeep understanding of Data Cataloging, Data Governance, Data Privacy principles and frameworks to integrate into the Data engineering flows.\nAdvanced knowledge of Data Lake technologies, data storage formats (Parquet, ORC, Avro) and query engines and associated concepts for consumption layers.\nExperience implementing solutions that adhere to best practices and guidelines for different privacy and compliance practices around data (GDPR, CCPA).\nHands on expert level on one or more of the following programming languages - Python, Java\nOrganized and goal-focused, ability to deliver in a fast-paced environment.\nBS degree in Computer Science, Engineering, Mathematics, or relevant industry standard experience to match",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Performance tuning', 'Automation', 'Software design', 'Quality systems', 'data governance', 'Data quality', 'data privacy', 'Analytics', 'Python']",2025-06-13 05:20:20
"Software Engineer II, Backend - Data Platform",Abnormal Ai,3 - 8 years,Not Disclosed,['Bengaluru'],"We are seeking a highly skilled Software Engineer II to help shape the future of AI-powered application development. If youre passionate about cutting-edge technology, scalable systems, and solving real-world challenges, this is your opportunity.\n\nWhat Youll Do as a Software Engineer II\nAs part of our engineering team, you will:\n\nLeverage AI-powered Development Use Cursor, Copilot, and other AI tools to enhance productivity, optimize workflows, and automate repetitive tasks.\nDevelop Data-Driven Applications Build consumer-grade interfaces and APIs that power our advanced behavioral AI insights.\nCombat Modern Cyber Threats Design and deploy secure, scalable systems that detect and prevent sophisticated cyberattacks.\nCollaborate with Fortune 500 Enterprises Work with customers and security teams to rapidly iterate and deliver impactful solutions.\nBuild at Scale Design backend services and cloud architectures that support billions of security events across enterprises worldwide.\n\nTechnical Requirements\nWere looking for engineers who have:\n\n3+ years of professional experience working on data-intensive applications and distributed systems. This is not a Data Engineering role.\nBackend development experience with Python or Go.\nDepth in at least one key area of the data platform tech stack - batch processing, streaming systems, data orchestration, data infrastructure.\nFamiliarity with AI development tools such as Cursor, GitHub Copilot, or Claude.\nExperience / passion in building scalable, enterprise-grade applications.\nExperience with any big data technologies such as Spark or Databricks or Trino or Kafka/Hadoop etc.\nKnowledge of cloud platforms (AWS, GCP, or Azure) and containerization (Docker, Kubernetes).\nStrong fundamentals in computer science, data structures, and performance optimization.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Access control', 'Backend', 'Compliance', 'Software Engineer II', 'Manager Technology', 'Healthcare', 'Sales Development Representative', 'Genetics', 'Security operations', 'Recruitment']",2025-06-13 05:20:22
Remote Data Visualization Engineer 36Lakhs CTC|| Kandi Srinivasa Reddy,Integra Technologies,8 - 11 years,35-37.5 Lacs P.A.,"['Kolkata', 'Ahmedabad', 'Bengaluru']","Dear Candidate,\nWe are seeking a Data Visualization Engineer to turn complex data into clear, engaging visual insights that empower business decisions. This role involves working closely with analysts and stakeholders to build interactive dashboards and reports.\n\nKey Responsibilities:\nDesign and develop visualizations using tools like Power BI, Tableau, or Looker.\nTranslate data into compelling stories and business insights.\nOptimize dashboard performance and usability for end users.\nCollaborate with data engineering and analytics teams.\nImplement visualization standards and data governance practices.\nRequired Skills & Qualifications:\nProficiency in data visualization tools (Power BI, Tableau, D3.js).\nStrong knowledge of SQL and data modeling.\nUnderstanding of UX principles in data presentation.\nExperience working with large datasets and cloud-based data platforms.\nKnowledge of scripting languages (Python, R) is a plus.\nSoft Skills:\nStrong troubleshooting and problem-solving skills.\nAbility to work independently and in a team.\nExcellent communication and documentation skills.\n\nNote: If interested, please share your updated resume and preferred time for a discussion. If shortlisted, our HR team will contact you.\n\nKandi Srinivasa Reddy\nDelivery Manager\nIntegra Technologies",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Tableau', 'Quicksight', 'Data Visualization', 'Dashboard Development', 'Business Intelligence', 'Power Bi', 'Bi Tools', 'Dashboarding', 'Business Objects', 'Reporting Tools', 'SQL', 'Microstrategy', 'Dashboards', 'QlikView']",2025-06-13 05:20:24
Oracle BRM Data Migration Engineer,Techstar Group,5 - 10 years,Not Disclosed,"['Noida', 'Hyderabad', 'Bengaluru']","Work Location : Hyderabad, Bangalore, Noida, Pune\nQualifications and Skills :\n- Proven expertise in Oracle BRM (Mandatory skill) with a strong understanding of its architecture and modules to effectively manage data migration processes.\n\n- Hands-on experience in data migration activities, particularly with Oracle BRM, ensuring high efficiency and accuracy throughout migration projects.\n\n- Knowledge in SQL for querying and managing databases, crucial for data migration and integration tasks.\n\n- Strong knowledge of ETL tools and processes for efficient data extraction, transformation, and loading from various sources.\n\n- Ability to perform detailed data mapping, ensuring logical transformation and compatibility between source and target system data structures.\n\n- Experience in data cleansing techniques to ensure data integrity and consistency throughout the migration process.\n\n- Understanding of data quality principles and practices, essential to maintain high standards of data accuracy and dependability.\n\n- Proficiency in scripting for automation of data migration tasks, enhancing efficiency and reducing potential for errors.\n\n- Excellent analytical and problem-solving skills to identify and address data-related challenges and opportunities.\n\n- Handling the execution of the data migration and validations.\n\n- Handle the develop Migration strategy documents and techniques. Execute data integrity testing post migration.\n\n- Understanding BRM : Having a working knowledge of BRM data migration components, the BRM 12 schema, and the data model\n\n- Data migration strategy : Developing a migration strategy and implementation plan\n\n- Data loading : Being able to load data and integrate it with systems\n\n- Post-migration analysis : Performing post-migration analysis on events, invoices, open items, bills, and dunning\n\n- Data reconciliation : Developing scripts to reconcile migrated data\n\n- Working Knowledge of all the BRM Data migration components.\n\n- Must have hands-on in BRM to verify the sanity of the Data migration.\n\n- Advantage - Programming skills on Java technologies. Exp. in C/C++, Oracle 12c/19c, PL/SQL, PCM Java, BRM Webservice, Scripting language (perl/python)\n\nRoles and Responsibilities :\n\n- Analyze client data and formulating effective data migration plans tailored to Oracle BRM specifications.\n\n- Collaborate with cross-functional teams to gather and interpret data migration requirements accurately.\n\n- Develop and implement efficient data migration scripts and processes, ensuring minimal disruption to business operations.\n\n- Conduct thorough testing and validation of data migration outputs to guarantee data accuracy and conformity.\n\n- Monitor and troubleshoot migration activities to ensure seamless execution and rectify any issues promptly.\n\n- Document data migration processes, maps, and transformations for knowledge sharing and continuous improvement.\n\n- Liaise with stakeholders to present progress updates and discuss ongoing improvements to data migration practices.\n\n- Contribute to the development of data migration best practices and reusable frameworks within the organization.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Oracle BRM', 'Java', 'Data Quality', 'Oracle Apps', 'C++', 'PL/SQL', 'Data Migration', 'Data reconciliation', 'ETL', 'SQL']",2025-06-13 05:20:25
Staff Software Engineer - Cloud Infrastructure-Network Data Ingestion,Calix,3 - 7 years,Not Disclosed,['Bengaluru'],"Calix is leading a service provider transformation to deliver a differentiated subscriber experience around the Smart Home and Business, while monetizing their network using Role based Cloud Services, Telemetry, Analytics, Automation, and the deployment of Software Driven Adaptive networks.\n\nAs part of a high performing global engineering team, the right candidate will play a critical role in expanding the Calix Cloud Infrastructure capabilities and be part of a team that leads the effort defining and architecting a world class in-home eco-system. Calix Cloud empowers service providers with solutions for Insights and real time data to support delivery of the best customer experiences, improve operational efficiency and drive market penetration.\nResponsibilities\nDesign, develop and maintain backend infrastructure, workflows, and services for collection, processing, analysis, correlation, and monitoring in Calix Cloud\nDevelop solutions to support onboarding, partner integrations, managing, collecting, and analyzing data from large scale deployment of home networks and access network systems and make them available as insights for various BSP user roles.\nWork closely with Cloud product owners to understand, analyze product requirements, provide feedback, and deliver a complete solution.\nTechnical leadership of software design in meeting requirements of service stability, reliability, scalability, and security\nParticipate and drive technical discussions within engineering group in all phases of the SDLC: review requirements, produce design documents, participate in peer reviews, produce test plans, support QA team, provide internal training and support TAC team.\nSupport test strategy and automation in both end-to-end solution and functional testing.\nCustomer facing engineering role in debugging and resolving field issues.\nQualifications:\n10+ years of highly technical, hands-on software engineering experience delivering quality software releases.\nIndependent and Self driven and works in a Team.\nStrong, creative problem-solving skills and ability to abstract and share details to create meaningful articulation.\nAbility to drive technical discussions across x-functional teams.\nStrong Implementation background in distributed design, data consumption patterns, and pipelines and experience in designing real-time streaming and event-based data solutions\nProficient in design and implementation of microservices-based, API/Endpoint architectures\nStrong background in designing and developing event-based / pub-sub workflows & data ingestion solutions. Proficiency and hands on experience with Kafka at scale (or similar) desired.\nGood Experience with load balancers, WebSocket, MQTT and similar technologies at different layers for efficient data abstraction and transfer for large scale data connections / large flow of data\nGood understanding of implementation and deployment of Cloud based solutions (preferably GCP)\nStrong background in transactional databases and good understanding and experience with no-SQL datastores and working in defining optimal data models.\nGood understanding of Networking concepts.\nExpert in Go. Proficiency in other languages like Java, Python/JavaScript a plus.\nOrganized and goal-focused, ability to deliver in a fast-paced environment.\nEducation:\nBS degree in Computer Science, engineering, or mathematics or equivalent experience.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Automation', 'Software design', 'Functional testing', 'Debugging', 'Javascript', 'SDLC', 'Analytics', 'Monitoring', 'SQL', 'Python']",2025-06-13 05:20:27
Avaloq Software Engineer (Data),Luxoft,3 - 5 years,Not Disclosed,[],"Your expertise in Avaloq technologies, database management, and software development combined with a solid understanding of the Avaloq Enterprise-Wide Object Model will be critical in shaping and enhancing our data capabilities. This is a fantastic opportunity to bring your passion for innovation into a fast-paced, dynamic environment where your impact will be tangible.\nSkills\nMust have\nYou will have between 3-5 years experience working as an Avaloq Developer.\nMapping physical data from Avaloq and other platforms to the Enterprise Data Model, ensuring adherence to reference data standards and clear conceptual definitions.\nPerforming data quality assessments, developing strategies to enhance data integrity, and creating Avaloq system functionalities to mitigate the risk of poor data at source.\nSupporting option analysis, data profiling, and interfacing with external rule/result repositories.\nReviewing and optimising Avaloq APDM outcomes, defining treatment strategies, and improving system performance.\nIntegrating the Avaloq APDM with external data minimisation orchestration tools.\nDesigning and delivering data quality improvement solutions, including mass-data manipulation scripts and associated testing and reconciliation processes.\nAnalysing business requirements and developing tailored software solutions.\nSupporting technical analysis and enhancements for Avaloq change requests and incidents.\nCollaborating with stakeholders to build alignment around technology change initiatives.\nBuilding and maintaining synthetic data delivery routines for test environments.\nManaging market data ingestion into the Avaloq Core Platform (ACP) via third-party tools.\nNice to have\nACCP certification\nOther\nLanguages\nEnglish: C2 Proficient\nSeniority\nSenior\nRefer a Friend\nPositive work environments and stellar reputations attract and retain top talent. Find out why Luxoft stands apart from the rest.\nRecommend a friend\nRelated jobs View all vacancies\nData Modelling Expert (with Avaloq experience)\nAvaloq\nIndia\nBengaluru\nAvaloq Technical Lead\nAvaloq\nAustralia\nSydney\nSenior Avaloq Engineer\nAvaloq\nAustralia\nSydney\nRemote India, India\nReq. VR-114137\nAvaloq\nBCM Industry\n21/05/2025\nReq. VR-114137\nApply for Avaloq Software Engineer (Data) in Remote India\n*",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Technical analysis', 'Quality improvement', 'orchestration', 'Data modeling', 'Reconciliation', 'Technical Lead', 'Data quality', 'market data', 'data integrity', 'Software solutions']",2025-06-13 05:20:29
ML Engineer/Data Scientist,Altimetrik,6 - 8 years,15-30 Lacs P.A.,"['Hyderabad', 'Chennai', 'Bengaluru']","Role & responsibilities\nData Scientist /ML engineers : ML Engineer with Python, SQL, Machine Learning, Azure skills(Good to have)",Industry Type: IT Services & Consulting,,,"['Machine Learning', 'Python', 'SQL', 'Data Science', 'Ml', 'azure']",2025-06-13 05:20:31
Big Data Lead,IQVIA,8 - 13 years,25-40 Lacs P.A.,['Bengaluru'],"Job Title / Primary Skill: Big Data Developer (Lead/Associate Manager)\nManagement Level: G150\nYears of Experience: 8 to 13 years\nJob Location: Bangalore (Hybrid)\nMust Have Skills: Big data, Spark, Scala, SQL, Hadoop Ecosystem.\nEducational Qualification: BE/BTech/ MTech/ MCA, Bachelor or masters degree in Computer Science,\n\nJob Overview\nOverall Experience 8+ years in IT, Software Engineering or relevant discipline.\nDesigns, develops, implements, and updates software systems in accordance with the needs of the organization.\nEvaluates, schedules, and resources development projects; investigates user needs; and documents, tests, and maintains computer programs.\nJob Description:\nWe look for developers to have good knowledge of Scala programming skills and Knowledge of SQL\nTechnical Skills:\nScala, Python -> Scala is often used for Hadoop-based projects, while Python and Scala are choices for Apache Spark-based projects.\nSQL -> Knowledge of SQL (Structured Query Language) is important for querying and manipulating data\nShell Script -> Shell scripts are used for batch processing of data, it can be used for scheduling the jobs and shell scripts are often used for deploying applications\nSpark Scala -> Spark Scala allows you to write Spark applications using the Spark API in Scala\nSpark SQL -> It allows to work with structured data using SQL-like queries and Data Frame APIs.\nWe can execute SQL queries against Data Frames, enabling easy data exploration, transformation, and analysis.\n\nThe typical tasks and responsibilities of a Big Data Developer include:\n1. Data Ingestion: Collecting and importing data from various sources, such as databases, logs, APIs into the Big Data infrastructure.\n2. Data Processing: Designing data pipelines to clean, transform, and prepare raw data for analysis. This often involves using technologies like Apache Hadoop, Apache Spark.\n3. Data Storage: Selecting appropriate data storage technologies like Hadoop Distributed File System (HDFS), HIVE, IMPALA, or cloud-based storage solutions (Snowflake, Databricks).",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SCALA', 'Big Data', 'Spark', 'Hive', 'Apache Pig', 'Hadoop', 'Hadoop Development', 'Mapreduce', 'Hdfs', 'Impala', 'YARN']",2025-06-13 05:20:33
Big Data Lead,Hexaware Technologies,8 - 13 years,18-25 Lacs P.A.,"['Chennai', 'Bengaluru', 'Mumbai (All Areas)']","As an Azure Data Engineer, we are looking for candidates who possess expertise in the following:\nDatabricks\nData Factory\nSQL\nPyspark/Spark\n\nRoles and Responsibilities:",,,,"['Databricks', 'Sql', 'Python']",2025-06-13 05:20:34
Big Data Developer - N,Infosys,5 - 10 years,Not Disclosed,"['Hyderabad', 'Pune', 'Bengaluru']","Job description\n\nHiring for Bigdata Developer with experience range 5 to 15 years.\n\nMandatory Skills: Bigdata, Scala, Spark, Hive, Kafka\n\nEducation: BE/B.Tech/MCA/M.Tech/MSc./MSts",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Hive', 'SCALA', 'Big Data', 'Kafka', 'Spark', 'Bigdata Technologies']",2025-06-13 05:20:36
Cloud SRE - Big Data Platform,ANZ,6 - 11 years,Not Disclosed,['Bengaluru'],"As a Cloud Big Data SRE in our Cloud Infrastructure Team, you ll play a key role in hprimarily lead the engineering and operational activities on an enterprise big data platform (EBD) on GCP. The platform is vast with Petabytes of data mostly retail and commercial bank customers which is used for banks critical customer remediation program and is significant in regulatory compliance. Additionally, the role involves providing platform engineering leadership and mentorship within the EDAE team, significantly influencing the direction and success of ANZs data capabilities on a broad scale.\nBanking is changing and we re changing with it, giving our people great opportunities to try new things, learn and grow. Whatever your role at ANZ, you ll be building your future, while helping to build ours.\nRole Type:Permanent\nRole Location:Bengaluru\n\nWhat will your day look like?\nLead SRE principles adoption in squad through monitoring of reliability of platform and applications, automate operations.\nEnsure services meet defined uptime, performance and latency targets.\nSolving ambiguous and complex data platform engineering problems.\nWork collaboratively within and across teams, Tech Areas and Domains.\nUtilise tools and practices to maintain, verify and deploy solutions in the most efficient ways, we place a high emphasis on Software Fundamentals.\nImplements a culture within the Tribe and the Chapter, encouraging best practices around reviews, quality, and documentation.\nProvide ongoing support for platforms as required e.g. problem and incident management troubleshoot.\nWhat will you bring?\n\nTo grow and be successful in this role, you will ideally bring the following:\nExperienced in managing enterprise-scale big data lake platforms on cloud, with a strong focus on platform engineering, service hosting, and dependency management across distributed systems.\nSpecialize in SRE practices including reliability engineering, observability implementation, proactive monitoring, and incident response operations to ensure platform stability and operational excellence.\nProven experience in devops, automation, manage and version codebases , configurations and infrastructure automation using Terraform.\nProficiency in Python, with experience in a secondary language like Golang or Java.\n6+ years of experience as a Cloud SRE/Engineer (preferably GCP) with applications utilizing services such as:\nBig Data Ecosystem : Hadoop cluster and technologies like Spark, Hive, and HBase deployed on cloud platforms\nWorkflow Orchestration : Airflow (or equivalent managed services) for job scheduling and pipeline orchestration\nContainerization & Orchestration : Kubernetes for scalable deployment and orchestration of containerized workloads\nObservability: Centralized logging and metrics-based monitoring using cloud-native or open-source solutions\nCloud Object Storage: Scalable storage solutions such as GCS, S3, or Azure Blob Storage\nDistributed SQL Stores: Managed relational databases like Cloud SQL, Amazon RDS, or Azure SQL\nIdentity & Access Management: Role-based access control and policy enforcement via IAM across cloud environments\nYou re not expected to have 100% of these skills. At ANZ a growth mindset is at the heart of our culture, so if you have most of these things in your toolbox, we d love to hear from you.\nSo why join us?\nANZ is a place where big things happen as we work together to provide banking and financial services across more than 30 markets. With more than 7,500 people, our Bengaluru team is the banks largest technology, data and operations centre outside Australia. In operation for over 33 years, the centre is critical in delivering the banks strategy and making an impact for our millions of customers around the world. Our Bengaluru team not only drives the transformation initiatives of the bank, it also drives a culture that makes ANZ a great place to be. Were proud that people feel they can be themselves at ANZ and 90 percent of our people feel they belong.\nWe want to continue building a diverse workplace and welcome applications from everyone. Please talk to us about any adjustments you may require to our recruitment process or the role itself. If you are a candidate with a disability or access requirements, let us know how we can provide you with additional support.\nTo find out more about working at ANZ visit https://www.anz.com/careers/ . You can apply for this role by visiting ANZ Careers and searching for reference number 98658\n.\nJob Posting End Date\n18/06/2025 , 11.59pm, (Melbourne Australia)",Industry Type: Banking,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Automation', 'Access management', 'Workflow', 'Incident management', 'Open source', 'Distribution system', 'Monitoring', 'Financial services', 'SQL', 'Python']",2025-06-13 05:20:38
Big Data Developer/ Senior Big Data Developer,Grid Dynamics,5 - 10 years,Not Disclosed,"['Hyderabad', 'Chennai', 'Bengaluru']","About us\nGrid Dynamics (NASDAQ: GDYN) is a leading provider of technology consulting, platform and product engineering, AI, and advanced analytics services. Fusing technical vision with business acumen, we solve the most pressing technical challenges and enable positive business outcomes for enterprise companies undergoing business transformation. A key differentiator for Grid Dynamics is our 8 years of experience and leadership in enterprise AI, supported by profound expertise and ongoing investment in data, analytics, cloud & DevOps, application modernization and customer experience. Founded in 2006, Grid Dynamics is headquartered in Silicon Valley with offices across the Americas, Europe, and India.\n\nRole & responsibilities\nWe are looking for an enthusiastic and technology-proficient Big Data Engineer, who is eager to participate in the design and implementation of a top-notch Big Data solution to be deployed at massive scale.\nOur customer is one of the world's largest technology companies based in Silicon Valley with operations all over the world. On this project we are working on the bleeding-edge of Big Data technology to develop high performance data analytics platform, which handles petabytes datasets.\nEssential functions\nParticipate in design and development of Big Data analytical applications.\nDesign, support and continuously enhance the project code base, continuous integration pipeline, etc.\nWrite complex ETL processes and frameworks for analytics and data management.\nImplement large-scale near real-time streaming data processing pipelines.\nWork inside the team of industry experts on the cutting edge Big Data technologies to develop solutions for deployment at massive scale.\nQualifications\nStrong coding experience with Scala, Spark,Hive, Hadoop.\nIn-depth knowledge of Hadoop and Spark, experience with data mining and stream processing technologies (Kafka, Spark Streaming, Akka Streams).\nUnderstanding of the best practices in data quality and quality engineering.\nExperience with version control systems, Git in particular.\nDesire and ability for quick learning of new tools and technologies.\nWould be a plus\nKnowledge of Unix-based operating systems (bash/ssh/ps/grep etc.).\nExperience with Github-based development processes.\nExperience with JVM build systems (SBT, Maven, Gradle).\nWe offer\nOpportunity to work on bleeding-edge projects\nWork with a highly motivated and dedicated team\nCompetitive salary\nFlexible schedule\nBenefits package - medical insurance, sports\nCorporate social events\nProfessional development opportunities\nWell-equipped office",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Big Data', 'SCALA', 'Hadoop', 'Spark']",2025-06-13 05:20:40
Big Data Developer-STG(P),Infosys,5 - 10 years,Not Disclosed,"['Hyderabad', 'Chennai', 'Bengaluru']","Role & responsibilities\n\nA day in the life of an Infoscion\nAs part of the Infosys delivery team, your primary role would be to ensure effective Design, Development, Validation and Support activities, to assure that our clients are satisfied with the high levels of service in the technology domain.\nYou will gather the requirements and specifications to understand the client requirements in a detailed manner and translate the same into system requirements.\nYou will play a key role in the overall estimation of work requirements to provide the right information on project estimations to Technology Leads and Project Managers.\nYou would be a key contributor to building efficient programs/ systems and if you think you fit right in to help our clients navigate their next in their digital transformation journey, this is the place for you!\nIf you think you fit right in to help our clients navigate their next in their digital transformation journey, this is the place for you!\n\nPrimary skills:Technology->Functional Programming->Scala\n\nAdditional information(Optional)\nKnowledge of design principles and fundamentals of architecture\nUnderstanding of performance engineering\nKnowledge of quality processes and estimation techniques\nBasic understanding of project domain\nAbility to translate functional / nonfunctional requirements to systems requirements\nAbility to design and code complex programs\nAbility to write test cases and scenarios based on the specifications\nGood understanding of SDLC and agile methodologies\nAwareness of latest technologies and trends\nLogical thinking and problem solving skills along with an ability to collaborate\n\n\nPreferred candidate profile",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SCALA', 'Big Data', 'Spark', 'Hive', 'Kafka']",2025-06-13 05:20:42
Azure Data Factory (ADF) Engineer,Arieotech Solutions,3 - 5 years,Not Disclosed,['Pune'],"We are seeking an experienced Azure Data Factory Engineer to design, develop, and manage data pipelines using Azure Data Factory. The ideal candidate will possess hands-on expertise in ADF components and activities, and have practical knowledge of incremental data loading, file management, API integration, and cloud storage solutions. This role involves automating data workflows, optimizing performance, and ensuring the seamless flow of data within our cloud environment.\nKey Responsibilities:\nDesign and Develop Data Pipelines: Build and maintain scalable data pipelines using Azure Data Factory, ensuring efficient and reliable data movement and transformation.\nIncremental Data Loads: Implement and manage incremental data loading processes to ensure that only updated or new data is processed, optimizing data pipeline performance and reducing resource consumption.\nFile Management: Handle data ingestion and management from various file sources, including CSV, JSON, and Parquet formats, ensuring data accuracy and consistency.\nAPI Integration: Develop and configure data pipelines to interact with RESTful APIs for data extraction and integration, handling authentication and data retrieval processes effectively.\nCloud Storage Management: Work with Azure Blob Storage and Azure Data Lake Storage to manage and utilize cloud storage solutions, ensuring data is securely stored and easily accessible.\nADF Automation: Leverage Azure Data Factory s automation capabilities to schedule and monitor data workflows, ensuring timely execution and error-free operations.\nPerformance Optimization: Continuously monitor and optimize data pipeline performance, troubleshoot issues, and implement best practices to enhance efficiency.\nCollaboration: Work closely with data engineers, analysts, and other stakeholders to gather requirements, provide technical guidance, and ensure successful data integration solutions.\nQualifications:\nEducational Background: Bachelor s degree in Computer Science, Information Technology, or a related field (B. E, B.Tech, MCA, MCS). Advanced degrees or certifications are a plus.\nExperience: Minimum 3-5 years of hands-on experience with Azure Data Factory, including designing and implementing complex data pipelines.\nTechnical Skills:\nStrong knowledge of ADF components and activities, including datasets, pipelines, data flows, and triggers.\nProficiency in incremental data loading techniques and optimization strategies.\nExperience working with various file formats and handling large-scale data files.\nProven ability to integrate and interact with APIs for data retrieval and processing.\nHands-on experience with Azure Blob Storage and Azure Data Lake Storage.\nFamiliarity with ADF automation features and scheduling.\nSoft Skills:\nStrong problem-solving and analytical skills.\nExcellent communication and collaboration abilities.\nAbility to work independently and manage multiple tasks effectively.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Analytical skills', 'Automation', 'Storage management', 'cloud storage', 'JSON', 'Scheduling', 'Management', 'Information technology', 'Data extraction']",2025-06-13 05:20:43
"Associate Engineer, Digital Data Development",XL India Business Services Pvt. Ltd,1 - 4 years,Not Disclosed,['Gurugram'],"Engineer, Digital Data Development Gurgaon/ Bangalore, India AXA XL offers risk transfer and risk management solutions to clients globally\n\nWe offer worldwide capacity, flexible underwriting solutions, a wide variety of client-focused loss prevention services, and a team-based account management approach\n\nAXA XL recognizes data and information as critical business assets, both in terms of managing risk and enabling new business opportunities\n\nThis data should not only be high quality, but also actionable - enabling AXA XL s executive leadership team to maximize benefits and facilitate sustained dynamic advantage\n\nOur Innovation, Data, and Analytics (IDA) organization is focused on driving innovation by optimizing how we leverage data to drive strategy and create a new business model - disrupting the insurance market\n\nThis role is part of the Digital Data Dev Division within the Digital Transformation vertical of IDA\n\nIt will be responsible for different aspects of Data Product development lifecycle activities, including but not limited to Data Production Support, business stakeholders engagement for usage & problem resolutions, Product migrations, and platform/data product rollouts, performance stability & reliability\n\nWhat you ll be DOING What will your essential responsibilities include? Hands-on experience with CI/CD tools: Harness, Azure DevOps\n\nImplement and manage DevSecOps tools and CI/CD pipelines with security controls\n\nAutomate security scanning and compliance checks (SAST, DAST, container scanning, etc)\n\nCollaborate with development, operations, and security teams to embed security best practices\n\nConduct threat modeling, vulnerability assessments, and risk\n\nBuild, Release Management & DevSecOps support for various data solutions owned and managed by IDA organization\n\nExperience with cloud platforms like Azure is preferred\n\nProficiency in scripting languages: Python, Bash, PowerShell\n\nFamiliarity with containerization and orchestration: Docker, Kubernetes, OpenShift\n\nExperience using Tools like Git, JIRA, Confluence etc Knowledge of Artifactory like JFrog / X-Ray\n\nExperience of working with Agile methodologies\n\nGood knowledge of OOP concepts & Microservice-based architecture\n\nAnalyze and mitigate risks (technical or otherwise) about Data Solution build & release delivery timelines\n\nProvide top-class DevSecOps functionalities and support\n\nPartner with the Product & Production Support team(s) as a Data/DevSecOps/Technical SME for migration of re-architected Product/Product functionalities to the new Cloud Platform\n\nDemonstrate proactive communication with Business users, Development, Technology, Production Support, and Delivery Teams, and Senior Management\n\nProvide day-to-day management of the DevSecOps services and ensure smooth operation of the Release pipelines to various Environments\n\nWork in the Follow the Sun support model providing cross-team support coverage across Digital Data Dev division responsibilities\n\nBuild/Setup/Maintain various critical monitoring processes, alerts, and overall health reports (performance and functional) of production, and pre-production environments to be used by the Production Support Teams\n\nWork with Product Teams to build deployment pipelines for various Data Science Products used within IDA/Pricing & Analytics Teams\n\nOversee the development and maintenance of Build & Release Management processes and their documentation\n\nEnsure that all policies, standards, and best practices are followed and kept up to date\n\nTimely and accurate completion of emergency Release pipelines/processes in a manner that is auditable, testable, and maintainable\n\nEnsure any builds are consistent with Solution design, Security recommendations and business specifications\n\nAchieve & maintain the highest business customer confidence and net promoter score (NPS)\n\nGood grasp of Azure fundamentals (Microsoft AZ-900)\n\nRobust understanding of Designing and Implementing DevOps/DevSecOps Solutions (Microsoft AZ-400)\n\nKnowledge of Python or R Programming Language is a plus\n\nYou will report to Senior Delivery Lead\n\nWhat you will BRING We re looking for someone who has these abilities and skills: Required Skills and Abilities: Excellent understanding of DevOps principles with integrated security practices\n\nA minimum of an Undergraduate University Degree in Computer Science or related fields\n\nExtensive experience in data-focused roles (analytics, specialist, or engineer) and one or more areas of Build, Release & Data Management\n\nDistinctive problem-solving and analytical skills combined with robust business acumen\n\nExperience/knowledge of Microservices, Dot Net, R Programming Language, Python, Azure, and Kibana\n\nExperience with SQL, HIVE, ADLS, and Document Databases like Cosmos, SQL Databases & SQL DW Analytics\n\nExperience/Understanding of systems integration, and developer support tools Azure DevOps/DevSecOps, CI/CD pipelines, Release Management, Configuration Management, and Automation\n\nData Engineering background or working experience with ETL and big data platforms (HDInsight / ADLS / Data Bricks) a plus\n\nDesired Skills and Abilities: Demonstrates a level of experience/ability to influence and understand business problems in technical terminology and able to liaise with staff at all levels in the organization\n\nExcellent writing skills, with the ability to create clear requirements, specifications, and documentation for data systems\n\nExperience with multiple software delivery models (Waterfall, Agile, etc) is a plus\n\nPrevious experience leading small teams with a mix of onsite/offshore developers",Industry Type: Insurance,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Automation', 'Production support', 'Configuration management', 'Agile', 'microsoft', 'Risk management', 'Release management', 'Analytics', 'SQL', 'Python']",2025-06-13 05:20:45
"Staff Engr - Data Science(Advanced OOPS,Python,PySpark,Databricks)",The TJX Companies Inc,5 - 10 years,Not Disclosed,['Hyderabad'],"TJX Companies\nAt TJX Companies, every day brings new opportunities for growth, exploration, and achievement. You ll be part of our vibrant team that embraces diversity, fosters collaboration, and prioritizes your development. Whether you re working in our four global Home Offices, Distribution Centers or Retail Stores TJ Maxx, Marshalls, Homegoods, Homesense, Sierra, Winners, and TK Maxx, you ll find abundant opportunities to learn, thrive, and make an impact. Come join our TJX family a Fortune 100 company and the world s leading off-price retailer.\nJob Description:\nAbout TJX:\nAt TJX, is a Fortune 100 company that operates off-price retailers of apparel and home fashions. TJX India - Hyderabad is the IT home office in the global technology organization of off-price apparel and home fashion retailer TJX, established to deliver innovative solutions that help transform operations globally. At TJX, we strive to build a workplace where our Associates contributions are welcomed and are embedded in our purpose to provide excellent value to our customers every day. At TJX India, we take a long-term view of your career. We have a high-performance culture that rewards Associates with career growth opportunities, preferred assignments, and upward career advancement. We take well-being very seriously and are committed to offering a great work-life balance for all our Associates.\nWhat you ll discover\nInclusive culture and career growth opportunities\nA truly Global IT Organization that collaborates across North America, Europe, Asia and Australia, click here to learn more\nChallenging, collaborative, and team-based environment\nWhat you ll do\nThe Global Supply Chain - Logistics Team is responsible for managing various supply chain logistics related solutions within TJX IT. The organization delivers capabilities that enrich the customer experience and provide business value. We seek a motivated, talented Staff E ngineer with good understanding of cloud base, database and BI concepts to help architect enterprise reporting solutions across global buying, planning and allocations .\nWhat you ll need\nThe Global Supply Chain - Logistics Team thrives on strong relationships with our business partners and working diligently to address their needs which supports TJX growth and operational stability. On this tightly knit and fast-paced solution delivery team you will be constantly challenged to stretch and think outside the box .\nYou will be working with product teams, architecture and business partners to strategically plan and deliver the product features by connecting the technical and business worlds. You will need to break down complex problems into steps that drive product development while keeping product quality and security as the priority. You will be responsible for most architecture, design and technical decisions within the assigned scope.\nKey Responsibilities:\nDesign, develop, test and deploy AI solutions using Azure AI services to meet business requirements , working collaboratively with architects and other engineers.\nTrain, fine-tune, and evaluate AI models, including large language models (LLMs), ensuring they meet performance criteria and integrate seamlessly into new or existing solutions.\nDevelop and integrate APIs to enable smooth interaction between AI models and other applications, facilitating efficient model serving.\nCollaborate effectively with cross-functional teams, including data scientists, software engineers, and business stakeholders, to deliver comprehensive AI solutions.\nOptimize AI and ML model performance through techniques such as hyperparameter tuning and model compression to enhance efficiency and effectiveness.\nMonitor and maintain AI systems, providing technical support and troubleshooting to ensure continuous operation and reliability.\nCreate comprehensive documentation for AI solutions, including design documents, user guides, and operational procedures, to support development and maintenance.\nStay updated with the latest advancements in AI, machine learning, and cloud technologies, demonstrating a commitment to continuous learning and improvement.\nDesign, code, deploy, and support software components, working collaboratively with AI architects and engineers to build impactful systems and services.\nLead medium complex initiatives, prioritizing and assigning tasks, providing guidance, and resolving issues to ensure successful project delivery.\nMinimum Qualifications\nBachelors degree in computer science, engineering, or related field\n8 + years of experience in data /software engineering, design, implementation and architecture.\nAt least 5+ years of hands-on experience in developing AI/ML solutions, with a focus on deploying them in a cloud environment .\nDeep understanding of AI and ML algorithms with focus on Operations Research / Optimization knowledge ( preferably M etaheuristics / Genetic Algorithms) .\nStrong programming skills in Python with advanced OOPS concepts.\nGood understanding of structured, semi structured, and unstructured data, Data modelling, Data analysis, ETL and ELT .\nProficiency with Databricks & PySpark .\nExperience with MLOps practices including CI/CD for machine learning models.\nKnowledge of security best practices for deploying AI solutions, including data encryption and access control.\nKnowledge of ethical considerations in AI, including bias detection and mitigation strategies.\nThis role operates in an Agile/Scrum environment and requires a solid understanding of the full software lifecycle, including functional requirement gathering, design and development, testing of software applications, and documenting requirements and technical specifications.\nFully Owns Epics with decreasing guidance. Takes initiative through identifying gaps and opportunities.\nStrong communication and influence skills. Solid team leadership with mentorship skills\nAbility to understand the work environment and competing priorities in conjunction with developing/meeting project goals .\nShows a positive, open-minded, and can-do attitude .\nExperience in the following technologies:\nAdvanced Python programming ( OOPS)\nOperations Research / Optimization knowledge ( preferably M etaheuristics / Genetic Algorithms)\nDatabricks with Pyspark\nAzure / Cloud knowledge\nGithub / version control\nFunctional knowledge on Supply Chain / Logistics is preferred.\nIn addition to our open door policy and supportive work environment, we also strive to provide a competitive salary and benefits package. TJX considers all applicants for employment without regard to race, color, religion, gender, sexual orientation, national origin, age, disability, gender identity and expression, marital or military status, or based on any individuals status in any group or class protected by applicable federal, state, or local law. TJX also provides reasonable accommodations to qualified individuals with disabilities in accordance with the Americans with Disabilities Act and applicable state and local law.\nAddress:\nSalarpuria Sattva Knowledge City, Inorbit Road\nLocation:\nAPAC Home Office Hyderabad IN",Industry Type: Retail,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Supply chain', 'Data analysis', 'Operations research', 'Architecture', 'OOPS', 'Cloud', 'Machine learning', 'Technical support', 'Python', 'Logistics']",2025-06-13 05:20:47
AI Python Data Science Engineer,Probeseven,4 - 5 years,Not Disclosed,['Coimbatore'],"AI Python Data Science Engineer\nHot Openings\nAs a data science and analytics engineer, you will be involved in developing computer visions and data algorithms. Artificial intelligence development and deep machine learning implementations will be part of your development and deployment to the cloud.\n\nExperience for senior positions: 4 - 5+ years\nExperience for junior positions: 2 - 3 years\n\nRequired Tech Skills\nExperience in Python (must have) and/or R language.\nExperience with computer vision algorithms and data science.\nExperience in deep machine learning models.\nWell-versed in data visualization techniques.\nTroubleshoot and resolve code issues.\nCollaborate with data engineers to design and integrate the data sources.\nExperience in handling multiple priorities with Agile development.\nExperience with Git and working in a collaborative and distributive team environment.\nRequired Soft Skills\nExcellent listening, verbal, and written communication skills.\nStrong interpersonal & customer relationship skills.\nStrong analytical, problem solving, and decision-making skills.\nDocumentation skills.\nApply now\nHot Openings\nPHP + Node.js Developers\nFull Time\nTech Development\nExperience 4 - 6+ years",Industry Type: Film / Music / Entertainment,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer vision', 'GIT', 'data science', 'Analytical', 'Artificial Intelligence', 'Machine learning', 'PHP', 'Customer relationship', 'Analytics', 'Python']",2025-06-13 05:20:48
Data Visualization Engineer,Maimsd Technology,4 - 7 years,Not Disclosed,['Pune'],"Experience : 4 - 7 Yrs\n\nEmployment Type : Full Time, Permanent\n\nWorking mode : Regular\n\nNotice Period : Immediate - 15 Day\n\nAbout the Role :\n\nWe are seeking a skilled Data Visualization Engineer to join our team and transform raw data into actionable insights. You will play a crucial role in designing, developing, and maintaining interactive dashboards and reports using Power BI, Power Apps, Power Query, and Power Automate.\n\nResponsibilities :\n\n- Data Visualization : Create compelling and informative dashboards and reports using Power BI, effectively communicating complex data to stakeholders.\n\n- Data Integration : Import data from various sources (e.g., databases, spreadsheets, APIs) using Power Query and ensure data quality and consistency.\n\n- Data Modeling : Develop robust data models in Power BI to support complex analysis and reporting requirements.\n\n- Power Apps Development : Create custom applications using Power Apps to enable data-driven decision-making and automate workflows.\n\n- Power Automate Integration : Automate repetitive tasks and workflows using Power Automate to improve efficiency and reduce errors.\n\n- Cloud Data Analytics : Leverage cloud-based data analytics platforms to process and analyze large datasets.\n\n- Collaboration : Work closely with data analysts, data scientists, and business users to understand their requirements and deliver effective visualizations.\n\nQualifications :\n\nExperience : 4-7 years of experience in data visualization and business intelligence.\n\nTechnical Skills :\n\n- Proficiency in Power BI, Power Apps, Power Query, and Power Automate.\n\n- Strong understanding of data modeling and ETL processes.\n\n- Experience working with cloud-based data analytics platforms.\n\n- Familiarity with SQL and other data query languages.\n\nSoft Skills :\n\n- Excellent communication and interpersonal skills.\n\n- Strong problem-solving and analytical abilities.\n\n- Attention to detail and ability to deliver high-quality work",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Visualization', 'Reporting Analytics', 'Power BI', 'Dashboard Design', 'Power Automate', 'Data Modeling', 'ETL', 'Power Query M', 'Data Integration', 'SQL']",2025-06-13 05:20:50
Big Data Developer,Techstar Group,7 - 10 years,Not Disclosed,['Hyderabad'],"Responsibilities of the Candidate :\n\n- Be responsible for the design and development of big data solutions. Partner with domain experts, product managers, analysts, and data scientists to develop Big Data pipelines in Hadoop\n\n- Be responsible for moving all legacy workloads to a cloud platform\n\n- Work with data scientists to build Client pipelines using heterogeneous sources and provide engineering services for data PySpark science applications\n\n- Ensure automation through CI/CD across platforms both in cloud and on-premises\n\n- Define needs around maintainability, testability, performance, security, quality, and usability for the data platform\n\n- Drive implementation, consistent patterns, reusable components, and coding standards for data engineering processes\n\n- Convert SAS-based pipelines into languages like PySpark, and Scala to execute on Hadoop and non-Hadoop ecosystems\n\n- Tune Big data applications on Hadoop and non-Hadoop platforms for optimal performance\n\n- Apply an in-depth understanding of how data analytics collectively integrate within the sub-function as well as coordinate and contribute to the objectives of the entire function.\n\n- Produce a detailed analysis of issues where the best course of action is not evident from the information available, but actions must be recommended/taken.\n\n- Assess risk when business decisions are made, demonstrating particular consideration for the firm's reputation and safeguarding Citigroup, its clients, and assets, by driving compliance with applicable laws, rules, and regulations, adhering to Policy, applying sound ethical judgment regarding personal behavior, conduct, and business practices, and escalating, managing and reporting control issues with transparency\n\nRequirements :\n\n- 6+ years of total IT experience\n\n- 3+ years of experience with Hadoop (Cloudera)/big data technologies\n\n- Knowledge of the Hadoop ecosystem and Big Data technologies Hands-on experience with the Hadoop eco-system (HDFS, MapReduce, Hive, Pig, Impala, Spark, Kafka, Kudu, Solr)\n\n- Experience in designing and developing Data Pipelines for Data Ingestion or Transformation using Java Scala or Python.\n\n- Experience with Spark programming (Pyspark, Scala, or Java)\n\n- Hands-on experience with Python/Pyspark/Scala and basic libraries for machine learning is required.\n\n- Proficient in programming in Java or Python with prior Apache Beam/Spark experience a plus.\n\n- Hand on experience in CI/CD, Scheduling and Scripting\n\n- Ensure automation through CI/CD across platforms both in cloud and on-premises\n\n- System level understanding - Data structures, algorithms, distributed storage & compute\n\n- Can-do attitude on solving complex business problems, good interpersonal and teamwork skills",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Big Data', 'Hive', 'Data Engineering', 'Data Pipeline', 'PySpark', 'Hadoop', 'Kafka', 'HDFS', 'Spark', 'Python']",2025-06-13 05:20:52
Data Analytics Engineer,Automotive Industry,5 - 6 years,Not Disclosed,['Chennai'],"Position: Data Analytics Engineer\nExp: 5 -6 years\nNP: Immediate - 30 days\nQualification: B.tech\nLocation: Chennai Hybrid\nPrimary: Google Cloud Platform ,Python skills and Big Data pipeline\nSecondary: Big Query SQ, coding, testing, implementing, debugging workflows and apps\nKindly share your updated resume to aishwarya_s@onwardgroup.com\nKindly fill the below details\nTotal Exp:\nRelevant Exp:\nNotice Period: CTC:\nECTC:\nIf servicing NP, Last working Day, offered location & CTC:\nAvailable for Video modes interview on Weekdays (Y/N) :\nPAN Number:\nName as Per PAN Card:\nDate of Birth:\nAlternative Contact No:\nReason for Job Change:",Industry Type: Automobile,Department: Engineering - Hardware & Networks,"Employment Type: Full Time, Permanent","['GCP', 'Bigquery', 'Google Cloud Platform', 'Query SQ', 'Python']",2025-06-13 05:20:54
Software Data Operations Engineer (BS+2),MAQ Software,2 - 5 years,Not Disclosed,['Noida'],"MAQ LLC d.b.a MAQ Software hasmultiple openings at Redmond, WA for:\nSoftware Data Operations Engineer (BS+2)\n\nResponsible for gathering & analyzing business requirements from customers. Implement,test and integrate software applications for use by customers. Develop &review cost effective data architecture to ensure appropriateness with currentindustry advances in data management, cloud & user experience. Automateuser test scenarios, debug & fix errors in cloud-based data infrastructure,reporting applications to meet customer needs. Must be able to traveltemporarily to client sites and or relocate throughout the United States.\n\nRequirements:Bachelors Degree or foreign equivalent in Computer Science, ComputerApplications, Computer Information Systems, Information Technology or relatedfield with two years of work experience in job offered, software engineer, systemsanalyst or related job.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Software Data Operations', 'software applications', 'data management', 'Data Operations', 'data architecture', 'data infrastructure']",2025-06-13 05:20:55
JavaScript Engineer For Training AI Data,G2i Inc,3 - 8 years,Not Disclosed,[],"Evaluating the quality of AI-generated code, including human-readable summaries of your rationale.\nBuilding and evaluating React components, hooks, and modern JavaScript solutions.\nSolving coding problems and writing functional and efficient JavaScript/React code.\nWriting robust test cases to confirm code works efficiently and effectively.\nCreating instructions to help others and reviewing code before it goes into the model.\nEngaging in a variety of projects, from evaluating code snippets to developing full mobile applications using chatbots.\nPay Rates\nCompensation rates average at $30/hr and can go up to $50+/hr. Expectations are 15+ hours per week; however, there is no upper limit. You can work as much as you want and will be paid weekly per hour of work done on the platform.\nContract Length\nThis is a long-term contract with no end date. We expect to have work for the next 2 years. You can end the contract at any time, but we hope you will commit to 12 months of work.\nFlexible Schedules\nDevelopers can set their own hours. Ideal candidates will be interested in spending 40 hours a week. You will be assigned to projects, so strong performers will adapt to the urgency of projects and stay engaged, but we are incredibly flexible on working hours. You can take a 3-hour lunch with no problem. Instead of tracking your hours, you are paid according to time spent on the platform, calculated in the coding exercises.\nInterview Process\nApply using this Ashby form.\nIf you seem like a good fit, well send an async RLHF code review that will take 35 minutes and must be finished within 72 hours of us sending it.\nYou ll receive credentials to the RLHF platform. We re doing regular calls to answer any further questions about onboarding, as well as providing a support team at your disposal.\nYou ll perform a simulated production-level task (RLHF task) on the platform. This will be the final stage, which will ultimately determine your leveling and which project you ll be assigned. Successful completion of this process provides you with an opportunity to work on projects as they become available.\nTech Stack Priorities\nThe current priority for this team is frontend engineers who are well versed in JavaScript, React, and modern web development frameworks and libraries.\nRequired Qualifications\n3+ years of experience in a software engineering/software development role.\nStrong proficiency with JavaScript/React and frontend development.\nComplete fluency in the English language.\nAbility to articulate complex technical concepts clearly and engagingly.\nExcellent attention to detail and ability to maintain consistency in writing. Solid understanding of grammar, punctuation, and style guidelines.\nNice To Haves:\nBachelors or Masters degree in Computer Science.\nExperience with modern JavaScript frameworks and libraries (Next.js, Vue, Angular).\nFamiliarity with frontend testing frameworks (Jest, React Testing Library, Cypress).\nKnowledge of state management solutions (Redux, Context API, MobX).\nExperience with TypeScript and modern frontend tooling.\nRecognized accomplishments or contributions to the coding community or in projects.\nProven analytical skills with an ability to approach problems creatively.\nAdept communication skills, especially when understanding and discussing project requirements.\nA commitment to continuous learning and staying updated with the latest coding advancements and best practices.\nEnthusiasm for teaching AI models and experience with technical writing!",Industry Type: Software Product,Department: Other,"Employment Type: Full Time, Permanent","['Computer science', 'Analytical skills', 'Front end', 'Technical writing', 'Coding', 'Web development', 'Javascript', 'Test cases', 'Mobile applications', 'Software engineering']",2025-06-13 05:20:57
Data Warehouse Engineer (SSIS),Tek Experts,9 - 10 years,Not Disclosed,['New Delhi'],"We re searching for a Data Warehouse Engineer with a proven track record of developing Business Intelligence solutions to drive key outcomes. The role is responsible for different layers of the data hierarchy - including database design, data collection and storage techniques, to a deep understanding of data transformation tools and methodologies, the provisioning and managing of analytical databases, and building infrastructures that bring machine learning capabilities into production. The role will operate within the Business Analytics unit under the Business Analytics Global Manager.\nAt TeKnowledge , your work makes an impact from day one. We partner with organizations to deliver AI-First Expert Technology Services that\ndrive meaningful impact in AI, Customer Experience, and Cybersecurity. We turn complexity into clarity and potential into progress in a place where people lead and tech empowers.\nYou ll be part of a diverse and inclusive team where trust, teamwork, and shared success fuel everything we do. We push boundaries, using advanced technologies to solve complex challenges for clients around the world.\nHere, your work drives real change, and your ideas help shape the future of technology. We invest in you with top-tier training, mentorship, and career development ensuring you stay ahead in an ever-evolving world.\nWhy You ll Enjoy It Here:\nBe Part of Something Big A growing company where your contributions matter.\nMake an Immediate Impact Support groundbreaking technologies with real-world results.\nWork on Cutting-Edge Tech AI, cybersecurity, and next-gen digital solutions.\nThrive in an Inclusive Team A culture built on trust, collaboration, and respect.\nWe Care Integrity, empathy, and purpose guide every decision.\nWe re looking for innovators, problem-solvers, and experts ready to drive change and grow with us.\nWe Are TeKnowledge. Where People Lead and Tech Empowers.\n\nResponsibilities\n\nEnd to end development of data models, including gathering the data sources, developing ETL processes, and developing front end data model solutions for our business to help our users make smarter decisions.\nFull responsibility on our DWH including infrastructure, data modeling, audit logging, etc.\nBuilding automated validation processes to ensure data integrity.\nOptimizing processes run-time and solving problems in a scalable manner.\nTaking full ownership of designing, building and deployment of data products.\nCollaborating with Backend, Data Science, Data Analysis, and Product teams.\n\nQualifications\n\nBachelor s degree in Information Systems, Industrial Engineering, or equivalent experience is required.\nProfessional fluency in English is essential, both written and spoken.\nThree or more years of experience in BI solutions development (DWH, ETL) as well as in SQL and ETL tools like SSIS is required.\nExperience with building automated validation processes to ensure data integrity is considered an advantage.\nExperience in data modeling, working ETL tools and methodology, as well as with BI reporting tools is required.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data analysis', 'Backend', 'Front end', 'Database design', 'Business analytics', 'Machine learning', 'HTML', 'SSIS', 'Business intelligence', 'SQL']",2025-06-13 05:20:58
Specialist Data Security Engineer,Amgen Inc,4 - 6 years,Not Disclosed,['Hyderabad'],"What you will do\nIn this vital role the Specialist Data Security Engineer covering Data Loss Prevention (DLP) and Cloud Access Security Broker (CASB) technologies. This role will report to the Manager, Data Security. This position will provide essential services that enable us to better pursue our mission.\nSpecialist Data Security Engineers operate, manage, and improve Amgens DLP and Cloud Access Security Broker (CASB) solutions. In our Data Security team, they will identify emerging risks related to changes in cloud technologies, advise management, and develop technical remediations to address those risks. Specialists lead the development of processes and procedures for multiple solutions which enable business units to remediate identify cloud data exposures. They run multiple projects simultaneously to implement and improve the cloud data security protection and use advanced analytics to demonstrate success.",,,,"['Data Security Engineering', 'SkyHigh', 'Data Protection', 'Cloud Access Security Platforms', 'Netskope', 'security frameworks', 'Agile methodology', 'IT operations', 'ITIL', 'Elastica']",2025-06-13 05:21:01
"Quant Data Specialist, Aladdin Financial Engineering - Associate",Primetrace Technologies,3 - 6 years,Not Disclosed,['Gurugram'],"About this role\nAbout Aladdin Financial Engineering (AFE):\nJoin a diverse and collaborative team of over 3 00 modelers and technologists in Aladdin Financial Engineering (AFE) within BlackRock Solutions, the business responsible for the research and development of Aladdin s financial models. This group is also accountable for analytics production, enhancing the infrastructure platform and delivering analytics content to portfolio and risk management professionals (both within BlackRock and across the Aladdin client community). The models developed and supported by AFE span a wide array of financial products covering equities, fixed income, commodities, derivatives, and private markets. AFE provides investment insights that range from an analysis of cash flows on a single bond, to the overall financial risk associated with an entire portfolio, balance sheet, or enterprise.\nRole Description:\nWe are looking for a person to join the Advanced Data Analytics team with AFE Single Security . Advanced Data Analytics is a team of Quantitative Data and Product Specialists, focused on delivering Single Security Data Content, Governance and Product Solutions and Research Platform. The team leverages data, cloud, and emerging technologies in building an innovative data platform, with the focus on business and research use cases in the S ingle S ecurity space. The team uses various statistical/mathematical methodologies to derive insights and generate content to help develop predictive models, clustering, and classification solutions and enable Governance . The team works on Mortgage, Structured & Credit Products.\nWe are looking for a person to help build and expand Data & Analytics Content in the Credit space . The person will be responsible for building, enhancing, and maintaining the Credit Content Suite . The person will work on the below -\nCredit Derived Data Content\nModel & Data Governance\nCredit Model & Analytics\nExperience\nExperience on Scala\nKnowledge of ETL, data curation and analytical jobs using distributed computing framework with Spark\nKnowledge and Experience of working with large enterprise databases like Snowflake, Cassandra & Cloud manged services like Dataproc , Databricks\nKnowledge of financial instruments like Corporate Bonds, Derivatives etc.\nKnowledge of regression methodologies\nAptitude for design and building tools for D ata Governance\nPython knowledge is a plus\nQualifications\nBachelors / masters in computer science with a major in Math, Econ, or related field\n3 - 6 years of relevant experience\nOur benefits\n\n.\nOur hybrid work model\n.\nAbout BlackRock\n.\nThis mission would not be possible without our smartest investment - the one we make in our employees. It s why we re dedicated to creating an environment where our colleagues feel welcomed, valued and supported with networks, benefits and development opportunities to help them thrive.\nFor additional information on BlackRock, please visit @blackrock | Twitter: @blackrock | LinkedIn: www.linkedin.com / company / blackrock\nBlackRock is proud to be an Equal Opportunity Employer. We evaluate qualified applicants without regard to age, disability, family status, gender identity, race, religion, sex, sexual orientation and other protected attributes at law.",Industry Type: Financial Services,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Analytical', 'Fixed income', 'Financial risk', 'Finance', 'Healthcare', 'Data analytics', 'Risk management', 'Analytics', 'Balance Sheet', 'Financial engineering']",2025-06-13 05:21:02
Data Center Critical Facilities Engineer (Electrical),Equinix,3 - 4 years,Not Disclosed,['Mumbai'],"Who are we?\nEquinix is the world s digital infrastructure company , operating over 260 data centers across the globe. Digital leaders harness Equinixs trusted platform to bring together and interconnect foundational infrastructure at software speed. Equinix enables organizations to access all the right places, partners and possibilities to scale with agility, speed the launch of digital services, deliver world-class experiences and multiply their value, while supporting their sustainability goals.\nJoining our operations team means that you will be at the forefront of all we do, maintaining critical facilities infrastructure as part of a close-knit team delivering best-in-class service to our data center customers. We embrace diversity in thought and contribution and are committed to providing an equitable work environment that is foundational to our core values as a company and is vital to our success.",,,,"['Capacity management', 'Access control', 'Protection system', 'Compliance', 'Infrastructure', 'Corrective maintenance', 'Site administration', 'Vendor', 'Fire protection', 'Electricals']",2025-06-13 05:21:04
"Delivery Head - Infrastructure Engineering, Data Center",Bajaj Allianz General Insurance Company Limited,15 - 20 years,Not Disclosed,['Pune'],"The role requires strong leadership, strategic thinking, and the ability to drive innovation and efficiency within the technology department. It demands extensive experience in leading complex data center infrastructures, focusing on servers, SAN storage, high availability, disaster recovery, and hybrid environments, including data center operations and physical servers (blade and rack). Responsibilities include designing and testing backup strategies, maintaining documentation, ensuring compliance with regulations, and conducting product and vendor evaluations. Collaboration with various IT teams, the security team, and business stakeholders is essential\n",,,,"['process setting', 'document management', 'vmware', 'center', 'microsoft azure', 'itil service management', 'storage', 'shuttering', 'analysis', 'problem management', 'change management', 'cloud', 'data center', 'operations', 'service delivery', 'incident management', 'leadership', 'it infrastructure management', 'itil']",2025-06-13 05:21:06
Engineer- Data Support,Powerica,3 - 5 years,Not Disclosed,['Mumbai'],Job profile\na. Co-ordinate with project team\nb. Upload BVQ in oracle system\nc. BOQ revision and making entries.\nd. Data Accuracy\ne. Process management.\n\nDiploma in Engineering\nExp 3-5 Years’ experience.,Industry Type: Power,Department: Project & Program Management,"Employment Type: Full Time, Permanent","['Boq Preparation', 'Oracle', 'BVQ']",2025-06-13 05:21:08
Data Loss Prevention Engineer,Credent Infotech Solutions Llp,2 - 3 years,Not Disclosed,['Mumbai( kandivali )'],"Daily Monitoring and Investigation\n\nMonitor DLP alerts across email, endpoint, web, and cloud.\nPerform triage to determine false positives, true positives, and actual incidents.\nDocument findings and escalate critical violations per SOPs.\nIncident Response Support\n\nSupport incident response by providing evidence, logs, and context around DLP policy violations.\nCoordinate with IT, HR, and Legal teams for user engagement, awareness, and disciplinary action if necessary.\nParticipate in Root Cause Analysis (RCA) for recurring or high-severity incidents.\nPolicy Tuning and Optimization\n\nAnalyse alert trends and false positive patterns to suggest and implement policy refinements.\nWork with business and security teams to validate policy changes and test updated rulesets before production deployment.\nMaintain documentation of policy changes, rationales, and approvals.\nLifecycle Management\n\nSupport onboarding business units, or geographies into DLP coverage.\nMaintain and update DLP dashboards and reporting structures.\nStakeholder Communication\n\nProvide regular reports to CISO on DLP violations\nInterface with Data Owners, Business Units, and Compliance teams for policy alignment and exception management.",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Endpoint Protection', 'Incident Response', 'Symantec Dlp', 'SIEM', 'Data Classification', 'Risk Assessment', 'Compliance', 'Powershell', 'Information Security', 'Dlp', 'Casb Netskope', 'Data Protection Manager', 'Microsoft Purview']",2025-06-13 05:21:09
S&C Global Network - AI - Life Sciences -Data Science Sr. Manager,Accenture,11 - 15 years,Not Disclosed,['Bengaluru'],"JR:\n\n\n\nR00229254\n\n\n\nExperience:\n\n\n\n11-15 Years\n\n\n\n\nEducational Qualification:\n\n\n\nBachelors or Masters degree in Statistics, Data Science, Applied Mathematics, Business Analytics, Computer Science, Information Systems, or other Quantitative field.\n\n\n\n---------------------------------------------------------------------\n\n\n\nJob Title -\n\n\n\nS&C Global Network - AI - Healthcare Analytics - Senior Manager\n\n\n\nManagement Level:\n\n\n\n6-Senior Manager\n\n\n\nLocation:\n\n\n\nBangalore/Gurgaon\n\n\n\nMust-have skills:R,Phython,SQL,Spark,Tableau ,Power BI\n\n\n\n\nGood to have skills:Ability to leverage design thinking, business process optimization, and stakeholder management skills.\n\n\n\nJob\n\n\nSummary:\n\nThis role involves driving strategic initiatives, managing business transformations, and leveraging industry expertise to create value-driven solutions.\n\n\n\n\nRoles & Responsibilities:\n\nProvide strategic advisory services, conduct market research, and develop data-driven recommendations to enhance business performance.\n\nAs part of our Data & AI practice, you will join a worldwide network of smart and driven colleagues experienced in leading AI/ML/Statistical tools, methods and applications. From data to analytics and insights to actions, our forward-thinking consultants provide analytically-informed, issue-based insights at scale to help our clients improve outcomes and achieve high performance.\n\n\n\nWHATS IN IT FOR YOU\nAn opportunity to work on high-visibility projects with top Pharma clients around the globe.\nPotential to Co-create with leaders in strategy, industry experts, enterprise function practitioners, and business intelligence professionals to shape and recommend innovative solutions that leverage emerging technologies.\nAbility to embed responsible business into everythingfrom how you service your clients to how you operate as a responsible professional.\nPersonalized training modules to develop your strategy & consulting acumen to grow your skills, industry knowledge, and capabilities.\nOpportunity to thrive in a culture that is committed to accelerating equality for all. Engage in boundaryless collaboration across the entire organization.\n\n\n\n\nWhat you would do in this role\nLead proposals, and business development efforts and coordinate with other colleagues to cross-sell/ up-sell Life Sciences offerings to existing as well as potential clients.\nLead client discussions, developing new industry Point of View (PoV), re-usable assets (tools)\nCollaborate closely with cross-functional teams including Data engineering, technology, and business stakeholders to identify opportunities for leveraging data to drive business solutions.\nLead and manage teams to deliver transformative and innovative client projects.\nGuide teams on analytical and AI methods and approaches\nManage client relationships to foster trust, deliver value, and build the Accenture brand\nDrive consulting practice innovation and thought leadership in your area of specialization\nSupport strategies and operating models focused on some business units and assess likely competitive responses. Also, assess implementation readiness and points of greatest impact.\nExecute a transformational change plan aligned with the clients business strategy and context for change. Engage stakeholders in the change journey and build commitment to change.\n\n\n\n\n\nProfessional & Technical\n\n\n\n\nSkills:\n\n\n- Relevant experience in the required domain.\n\n- Strong analytical, problem-solving, and communication skills.\n\n- Ability to work in a fast-paced, dynamic environment.\nProven experience in cross-sell/ up-sell\nLeverage ones hands-on experience of working across one or more of these areas such as real-world evidence data, R&D clinical data, and digital marketing data.\nExperience with handling Datasets like Komodo, RAVE, IQVIA, Truven, Optum, SHS, Specialty Pharmacy, PSP, etc.\nExperience in building and deployment of Statistical Models/Machine Learning including Segmentation & predictive modeling, hypothesis testing, multivariate statistical analysis, time series techniques, and optimization.\nExcellent analytical and problem-solving skills, with a data-driven mindset.\nAbility to solve complex business problems and deliver client delight.\nStrong writing skills to build points of view on current industry trends.\nGood Client handling skills; able to demonstrate thought leadership & problem-solving skills.\n\n\n\n\n\nAdditional Information:\n\n- Opportunity to work on innovative projects.\n\n- Career growth and leadership exposure.\n\n\n\n\n\nAbout Our Company | AccentureQualification\n\n\n\nExperience:\n\n\n\n11-15 Years\n\n\n\n\nEducational Qualification:\n\n\n\nBachelors or Masters degree in Statistics, Data Science, Applied Mathematics, Business Analytics, Computer Science, Information Systems, or other Quantitative field.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'power bi', 'sql', 'tableau', 'r', 'hypothesis testing', 'time series', 'business analytics', 'machine learning', 'data engineering', 'business intelligence', 'artificial intelligence', 'data science', 'computer science', 'spark', 'predictive modeling', 'segmentation', 'statistics', 'ml']",2025-06-13 05:21:11
Senior Data Scientist,Ericsson,3 - 8 years,Not Disclosed,['Bengaluru'],"About this Opportunity\nThe complexity of running and optimizing the next generation of wireless networks, such as 5G with distributed edge compute, will require Machine Learning (ML) and Artificial Intelligence (AI) technologies. Ericsson is setting up an AI Accelerator Hub in India to fast-track our strategy execution, using Machine Intelligence (MI) to drive thought leadership, automate, and transform Ericsson s offerings and operations. We collaborate with academia and industry to develop state-of-the-art solutions that simplify and automate processes, creating new value through data insights.\n\nAs a Senior Data Scientist, you will apply your knowledge of data science and ML tools backed with strong programming skills to solve real-world problems.\nResponsibilities:\n1. Lead AI/ML features/capabilities in product/business areas\n2. Define business metrics of success for AI/ML projects and translate them into model metrics\n3. Lead end-to-end development and deployment of Generative AI solutions for enterprise use cases\n4. Design and implement architectures for vector search, embedding models, and RAG systems\n5. Fine-tune and evaluate large language models (LLMs) for domain-specific tasks\n6. Collaborate with stakeholders to translate vague problems into concrete Generative AI use cases\n7. Develop and deploy generative AI solutions using AWS services such as SageMaker, Bedrock, and other AWS AI tools. Provide technical expertise and guidance on implementing GenAI models and best practices within the AWS ecosystem.\n8. Develop secure, scalable, and production-grade AI pipelines\n9. Ensure ethical and responsible AI practices\n10. Mentor junior team members in GenAI frameworks and best practices\n11. Stay current with research and industry trends in Generative AI and apply cutting-edge techniques\n12. Contribute to internal AI governance, tooling frameworks, and reusable components\n13. Work with large datasets including petabytes of 4G/5G networks and IoT data\n14. Propose/select/test predictive models and other ML systems\n15. Define visualization and dashboarding requirements with business stakeholders\n16. Build proof-of-concepts for business opportunities using AI/ML\n17. Lead functional and technical analysis to define AI/ML-driven business opportunities\n18. Work with multiple data sources and apply the right feature engineering to AI models\n19. Lead studies and creative usage of new/existing data sources",,,,"['Wireless', 'Computer science', 'Data analysis', 'cassandra', 'Neural networks', 'Artificial Intelligence', 'Machine learning', 'Telecommunication', 'data visualization', 'Python']",2025-06-13 05:21:13
"SENIOR, DATA SCIENTIST",Walmart,3 - 8 years,Not Disclosed,['Bengaluru'],"Position Summary...\nWhat youll do...\nAbout Team\nThe Catalog Data Science Team at Walmart Global Tech is focused on using the latest research in generative AI (GenAI), artificial intelligence (AI), machine learning (ML), statistics, deep learning, computer vision and optimization to implement solutions that ensure Walmart s product catalog is accurate, complete, and optimized for customer experience. Our team tackles complex data science and ML engineering challenges related to product classification, attribute extraction, trust & safety, and catalog optimization, empowering next-generation retail use cases.\nThe Data Science and ML Engineering community at Walmart Global Tech is active in most of the Hack events, utilizing the petabytes of data at our disposal, to build some of the coolest ideas. All the work we do at Walmart Global Tech will eventually benefit our operations & our associates, helping Customers Save Money to Live Better.\nWhat youll do:\nAs a Senior Data Scientist - ML Engineer, you ll have the opportunity to:\nDrive research initiatives and proof-of-concepts that push the state of the art in generative AI and large-scale machine learning.\nDesign and implement high-throughput, low-latency AI/ML pipelines and microservices that operate at global scale.\nOversee data ingestion, model training, evaluation, deployment and monitoring-ensuring performance, quality and reliability.\nCustomize and optimize LLMs for specific business use cases, balancing accuracy, latency and cost.\nPrototype novel generative AI solutions, integrate advancements into production, and collaborate with research partners.\nChampion best practices in data quality, lineage, governance and cost optimization across ML pipelines.\nMentor a team of ML engineers, establish coding standards, conduct design reviews, and foster a culture of continuous improvement.\nPresent your team s work at top-tier AI/ML conferences, publish scientific papers, and cultivate partnerships with universities and research labs.\nWhat youll bring\nPhD in Computer Science, Statistics, Applied Mathematics or related field with 3+ years experience in ML engineering-or Master s with 6+ years or Bachelor s with 8+ years.\nProven track record of leading and scaling AI/ML products in production environments.\nDeep expertise in generative AI, large-scale model deployment, and fine-tuning of transformer-based architectures.\nStrong programming skills in Python, or equivalent, and experience with big data frameworks (Spark, Hadoop) and ML platforms (TensorFlow, PyTorch).\nDemonstrated history of scientific publications or patents in AI/ML.\nExcellent communication skills, a growth mindset, and the ability to drive cross-functional collaboration.\nAbout Walmart Global Tech\n.\n.\nFlexible, hybrid work\n.\nBenefits\n.\nBelonging\n.\n.\nEqual Opportunity Employer\nWalmart, Inc., is an Equal Opportunities Employer - By Choice. We believe we are best equipped to help our associates, customers and the communities we serve live better when we really know them. That means understanding, respecting and valuing unique styles, experiences, identities, ideas and opinions - while being inclusive of all people.\nMinimum Qualifications...\nMinimum Qualifications:Option 1- Bachelors degree in Statistics, Economics, Analytics, Mathematics, Computer Science, Information Technology, or related field and 3 years experience in an analytics related field. Option 2- Masters degree in Statistics, Economics, Analytics, Mathematics, Computer Science, Information Technology, or related field and 1 years experience in an analytics related field. Option 3 - 5 years experience in an analytics or related field.\nPreferred Qualifications...\nPrimary Location...\n\n\n",Industry Type: Retail,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Prototype', 'Networking', 'Coding', 'Machine learning', 'Continuous improvement', 'Information technology', 'Monitoring', 'Analytics', 'Python']",2025-06-13 05:21:15
Data Governance Engineers,Meritus Management Service,4 - 9 years,14-17 Lacs P.A.,"['Pune', 'Gurugram']","Define, implement, & enforce data governance policies & standards to ensure data quality, consistency, & compliance across the organization\nCollaborate with data stewards, business users, & IT teams to maintain metadata, lineage, & data catalog tools",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Stewardship', 'Metadata', 'Data Governance', 'Metadata Management', 'Data Lineage', 'Data Modeling', 'SQL']",2025-06-13 05:21:17
Data Modeler,Synechron,0 - 2 years,Not Disclosed,['Bengaluru'],"Synechron is seeking a knowledgeable and proactive Data Modeler to guide the design and development of data structures that support our clients' business objectives. In this role, you will collaborate with cross-functional teams to translate business requirements into scalable and efficient data models, ensuring data accuracy, consistency, and integrity. You will contribute to creating sustainable and compliant data architectures that leverage emerging technologies such as cloud, IoT, mobile, and blockchain. Your work will be instrumental in enabling data-driven decision-making and operational excellence across projects.Software Required\n\nSkills:\nStrong understanding of data modeling concepts, methodologies, and tools Experience with data modeling for diverse technology platforms including cloud, mobile, IoT, and blockchain Familiarity with database management systems (e.g., relational, NoSQL) Knowledge of SDLC and Agile development practices Proficiency in modeling tools such as ERwin, PowerDesigner, or similar Preferred Skills:\nExperience with data integration tools and ETL processes Knowledge of data governance and compliance standards Familiarity with cloud platforms (AWS, Azure, GCP) and how they impact data architectureOverall Responsibilities Collaborate with business analysts, data engineers, and stakeholders to understand data requirements and translate them into robust data models Design logical and physical data models optimized for performance, scalability, and maintainability Develop and maintain documentation for data structures, including data dictionaries and metadata Conduct reviews of data models and code to ensure adherence to quality standards and best practices Assist in designing data security and privacy measures in alignment with organizational policies Stay informed about emerging data modeling trends and incorporate best practices into project delivery Support data migration, integration, and transformation activities as needed Provide technical guidance and mentorship related to data modeling standardsTechnical Skills (By Category) Data Modeling & Data Management: EssentialLogical/physical data modeling, ER diagrams, data dictionaries PreferredDimensional modeling, data warehousing, master data management Programming Languages: PreferredSQL (expertise in writing complex queries) OptionalPython, R for data analysis and scripting Databases & Data Storage Technologies: EssentialRelational databases (e.g., Oracle, SQL Server, MySQL) PreferredNoSQL (e.g., MongoDB, Cassandra), cloud-native data stores Cloud Technologies: PreferredBasic understanding of cloud data solutions (AWS, Azure, GCP) Frameworks & Libraries: Not typically required, but familiarity with data integration frameworks is advantageous Development Tools & Methodologies: EssentialData modeling tools (ERwin, PowerDesigner), version control (Git), Agile/Scrum workflows Security & Compliance: Knowledge of data security best practices, regulatory standards like GDPR, HIPAAExperience Minimum of 8+ years of direct experience in data modeling, data architecture, or related roles Proven experience designing data models for complex systems across multiple platforms (cloud, mobile, IoT, blockchain) Experience working in Agile environments using tools like JIRA, Confluence, Git Preference for candidates with experience supporting data governance and data quality initiativesNoteEquivalent demonstrated experience in relevant projects or certifications can qualify candidates.Day-to-Day Activities Participate in daily stand-ups and project planning sessions Collaborate with cross-functional teams to understand and analyze business requirements Create, review, and refine data models and associated documentation Develop data schemas, dictionaries, and standards to ensure consistency Support data migration, integration, and performance tuning activities Conduct peer reviews and provide feedback on data models and solutions Keep current with the latest industry developments in data architecture and modeling Troubleshoot and resolve data-related technical issuesQualifications Bachelors or Masters degree in Computer Science, Data Science, Information Technology, or related fields Demonstrated experience with data modeling tools and techniques in diverse technological environments Certifications related to data modeling, data management, or cloud platforms (preferred)Professional Competencies Strong analytical and critical thinking skills to develop optimal data solutions Effective communication skills for translating technical concepts to non-technical stakeholders Ability to work independently and in collaborative team environments Skilled problem solver able to handle complex data challenges Adaptability to rapidly evolving technologies and project requirements Excellent time management and prioritization skills to deliver quality outputs consistently",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data modeling', 'modeling tools', 'relational databases', 'scrum', 'agile', 'confluence', 'hipaa', 'data warehousing', 'data architecture', 'erwin', 'sql', 'git', 'gcp', 'mysql', 'etl', 'mongodb', 'jira', 'python', 'oracle', 'microsoft azure', 'sql server', 'nosql', 'gdpr', 'cassandra', 'aws', 'data integration', 'sdlc']",2025-06-13 05:21:19
Senior Data Analyst-Azure Data Factory,Lumen Technologies,8 - 12 years,Not Disclosed,['Bengaluru'],"Were looking for a Senior Data Analyst with a strong foundation in Azure-based data engineering and Machine Learning to design, develop, and optimize robust data pipelines, applications, and analytics infrastructure. This role demands deep technical expertise, cross-functional collaboration, and the ability to align data solutions with dynamic business needs.\nKey Responsibilities:\nData Pipeline Development:\nDesign and implement efficient data pipelines using Azure Databricks with PySpark to transform and process large datasets.\nOptimize data workflows for scalability, reliability, and performance.\nApplication Integration:\nCollaborate with cross-functional teams to develop APIs using the .NET Framework for Azure Web Application integration.\nEnsure smooth data exchange between applications and downstream systems.\nData Warehousing and Analytics:\nBuild and manage data warehousing solutions using Synapse Analytics and Azure Data Factory (ADF).\nDevelop and maintain reusable and scalable data models to support business intelligence needs.\nAutomation and Orchestration:\nUtilize Azure Logic Apps, Function Apps, and Azure DevOps to automate workflows and streamline deployments.\nImplement CI/CD pipelines for efficient code deployment and testing.\nInfrastructure Management:\nOversee Azure infrastructure management and maintenance, ensuring a secure and optimized environment.\nProvide support for performance tuning and capacity planning.\nBusiness Alignment:\nGain a deep understanding of AMO data sources and their business implications.\nWork closely with stakeholders to provide customized solutions aligning with business needs.\nBAU Support:\nMonitor and support data engineering workflows and application functionality in BAU mode.\nTroubleshoot and resolve production issues promptly to ensure business continuity.\nTechnical Expertise:\nProficiency in Microsoft SQL for complex data queries and database management.\nAdvanced knowledge of Azure Databricks and PySpark for data engineering and ETL processes.\nExperience with Azure Data Factory (ADF) for orchestrating data workflows.\nExpertise in Azure Synapse Analytics for data integration and analytics.\nProficiency in .NET Framework for API development and integration.\nCloud and DevOps Skills:\nStrong experience in Azure Infrastructure Management and optimization.\nHands-on knowledge of Azure Logic Apps, Function Apps, and Azure DevOps for CI/CD automation.\n""We are an equal opportunity employer committed to fair and ethical hiring practices. We do not charge any fees or accept any form of payment from candidates at any stage of the recruitment process. If anyone claims to offer employment opportunities in our company in exchange for money or any other benefit, please treat it as fraudulent and report it immediately.""\n#LI-BS1",Industry Type: Telecom / ISP,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Performance tuning', 'Automation', 'orchestration', 'Infrastructure management', 'Machine learning', 'Business intelligence', 'Business continuity', 'Analytics', 'Downstream', 'Capacity planning']",2025-06-13 05:21:21
Big Data Engineer_ Info Edge _Noida,Info Edge,1 - 4 years,14-19 Lacs P.A.,['Noida'],"About Info Edge India Ltd.\nInfo Edge: Info Edge (India) Limited (NSE: NAUKRI) is among the leading internet companies in India. Info Edge, Indias premier online classifieds company is fundamentally in the matching business. With a network of 62 offices located in 43 cities throughout India, Info Edge has 5000 plus employees engaged in innovation, product development, integration with mobile and social media, technology and technology updation, research and development, quality assurance, sales, marketing and payment collection.\nThe umbrella brand has an online recruitment classifieds, www.naukri.com– India’s No. 1 Jobsite with over 75% traffic share, a matrimony classifieds, www.jeevansathi.com, a real estate classifieds, www.99acres.com– India’s largest property marketplace and an education classifieds, www.shiksha.com. Find out more about the Company at",,,,"['Data Modeling', 'Python', 'SCALA']",2025-06-13 05:21:22
"Senior Data Scientist (AI/ML, Data Analysis, Cloud (AWS), and Model",Synechron,8 - 13 years,Not Disclosed,['Pune'],"job requisition idJR1027352\n\nJob Summary\nSynechron is seeking an analytical and innovative Senior Data Scientist to support and advance our data-driven initiatives. The ideal candidate will have a solid understanding of data science principles, hands-on experience with AI/ML tools and techniques, and the ability to interpret complex data sets to deliver actionable insights. This role contributes to the organizations strategic decision-making and technology innovation by applying advanced analytics and machine learning models in a collaborative environment.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCandidates with extensive research or academic experience in AI/ML can be considered, provided they demonstrate practical application of skills.",,,,"['java', 'data science', 'python', 'deploying models', 'aws', 'continuous integration', 'kubernetes', 'scikit-learn', 'ci/cd', 'artificial intelligence', 'sql', 'docker', 'tensorflow', 'spark', 'pytorch', 'keras', 'hadoop', 'big data', 'mongodb', 'microsoft azure', 'nosql', 'pandas', 'amazon ec2', 'r', 'cassandra', 'agile']",2025-06-13 05:21:25
Data Techology Senior Associate,MSCI Services,4 - 7 years,Not Disclosed,['Pune'],"Overview\nThe Data Technology team at MSCI is responsible for meeting the data requirements across various business areas, including Index, Analytics, and Sustainability. Our team collates data from multiple sources such as vendors (e.g., Bloomberg, Reuters), website acquisitions, and web scraping (e.g., financial news sites, company websites, exchange websites, filings). This data can be in structured or semi-structured formats. We normalize the data, perform quality checks, assign internal identifiers, and release it to downstream applications.\nResponsibilities\nAs data engineers, we build scalable systems to process data in various formats and volumes, ranging from megabytes to terabytes. Our systems perform quality checks, match data across various sources, and release it in multiple formats. We leverage the latest technologies, sources, and tools to process the data. Some of the exciting technologies we work with include Snowflake, Databricks, and Apache Spark.\nQualifications\nCore Java, Spring Boot, Apache Spark, Spring Batch, Python. Exposure to sql databases like Oracle, Mysql, Microsoft Sql is a must. Any experience/knowledge/certification on Cloud technology preferrably Microsoft Azure or Google cloud platform is good to have. Exposures to non sql databases like Neo4j or Document database is again good to have.\n What we offer you\nTransparent compensation schemes and comprehensive employee benefits, tailored to your location, ensuring your financial security, health, and overall wellbeing.\nFlexible working arrangements, advanced technology, and collaborative workspaces.\nA culture of high performance and innovation where we experiment with new ideas and take responsibility for achieving results.\nA global network of talented colleagues, who inspire, support, and share their expertise to innovate and deliver for our clients.\nGlobal Orientation program to kickstart your journey, followed by access to our Learning@MSCI platform, LinkedIn Learning Pro and tailored learning opportunities for ongoing skills development.\nMulti-directional career paths that offer professional growth and development through new challenges, internal mobility and expanded roles.\nWe actively nurture an environment that builds a sense of inclusion belonging and connection, including eight Employee Resource Groups. All Abilities, Asian Support Network, Black Leadership Network, Climate Action Network, Hola! MSCI, Pride & Allies, Women in Tech, and Women’s Leadership Forum.\nAt MSCI we are passionate about what we do, and we are inspired by our purpose – to power better investment decisions. You’ll be part of an industry-leading network of creative, curious, and entrepreneurial pioneers. This is a space where you can challenge yourself, set new standards and perform beyond expectations for yourself, our clients, and our industry.\nMSCI is a leading provider of critical decision support tools and services for the global investment community. With over 50 years of expertise in research, data, and technology, we power better investment decisions by enabling clients to understand and analyze key drivers of risk and return and confidently build more effective portfolios. We create industry-leading research-enhanced solutions that clients use to gain insight into and improve transparency across the investment process.\nMSCI Inc. is an equal opportunity employer. It is the policy of the firm to ensure equal employment opportunity without discrimination or harassment on the basis of race, color, religion, creed, age, sex, gender, gender identity, sexual orientation, national origin, citizenship, disability, marital and civil partnership/union status, pregnancy (including unlawful discrimination on the basis of a legally protected parental leave), veteran status, or any other characteristic protected by law. MSCI is also committed to working with and providing reasonable accommodations to individuals with disabilities. If you are an individual with a disability and would like to request a reasonable accommodation for any part of the application process, please email Disability.Assistance@msci.com and indicate the specifics of the assistance needed. Please note, this e-mail is intended only for individuals who are requesting a reasonable workplace accommodation; it is not intended for other inquiries.\n To all recruitment agencies\nMSCI does not accept unsolicited CVs/Resumes. Please do not forward CVs/Resumes to any MSCI employee, location, or website. MSCI is not responsible for any fees related to unsolicited CVs/Resumes.\n Note on recruitment scams\nWe are aware of recruitment scams where fraudsters impersonating MSCI personnel may try and elicit personal information from job seekers. Read our full note on careers.msci.com",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['hive', 'access', 'scala', 'pyspark', 'data warehousing', 'hibernate', 'research', 'sql', 'analytics', 'spring', 'java', 'spring batch', 'spark', 'gcp', 'mysql', 'html', 'hadoop', 'big data', 'etl', 'snowflake', 'python', 'oracle', 'data analysis', 'microsoft azure', 'power bi', 'sql server', 'javascript', 'data bricks', 'spring boot', 'tableau', 'neo4j', 'aws', 'sql database']",2025-06-13 05:21:26
"Senior Python Developer (Machine Learning,Data Analysis,Visualization)",Synechron,3 - 5 years,Not Disclosed,"['Pune', 'Hinjewadi']","Software Requirements\nRequired Skills:\nProficiency in Python (version 3.6+) with experience in data analysis, manipulation, and scripting\nKnowledge of SQL for data extraction, transformation, and database querying\nExperience with data visualization tools such as PowerBI, Tableau, or QlikView\nFamiliarity with AI and Machine Learning frameworks such as TensorFlow, Keras, PyTorch, or equivalent",,,,"['Python', 'PostgreSQL', 'MySQL', 'Data Analysis', 'Data Visualization', 'Oracle', 'ETL', 'Machine Learning']",2025-06-13 05:21:28
Data Techology Senior Associate,MSCI Services,4 - 8 years,Not Disclosed,['Pune'],"As data engineers, we build scalable systems to process data in various formats and volumes, ranging from megabytes to terabytes. Our systems perform quality checks, match data across various sources, and release it in multiple formats. We leverage the latest technologies, sources, and tools to process the data. Some of the exciting technologies we work with include Snowflake, Databricks, and Apache Spark.\nYour skills and experience that will help you excel\nCore Java, Spring Boot, Apache Spark, Spring Batch, Python. Exposure to sql databases like Oracle, Mysql, Microsoft Sql is a must. Any experience / knowledge / certification on Cloud technology preferrably Microsoft Azure or Google cloud platform is good to have. Exposures to non sql databases like Neo4j or Document database is again good to have.\n  What we offer you\nTransparent compensation schemes and comprehensive employee benefits, tailored to your location, ensuring your financial security, health, and overall we'llbeing.\nFlexible working arrangements, advanced technology, and collaborative workspaces.\nA culture of high performance and innovation where we experiment with new ideas and take responsibility for achieving results.\nA global network of talented colleagues, who inspire, support, and share their expertise to innovate and deliver for our clients.\nGlobal Orientation program to kickstart your journey, followe'd by access to our Learning@MSCI platform, LinkedIn Learning Pro and tailored learning opportunities for ongoing skills development.\nMulti-directional career paths that offer professional growth and development through new challenges, internal mobility and expanded roles.\nWe actively nurture an environment that builds a sense of inclusion belonging and connection, including eight Employee Resource Groups. All Abilities, Asian Support Network, Black Leadership Network, Climate Action Network, Hola! MSCI, Pride & Allies, Women in Tech, and Women s Leadership Forum.",Industry Type: Financial Services,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['CVS', 'Core Java', 'Bloomberg', 'spring batch', 'MySQL', 'Oracle', 'Analytics', 'Downstream', 'Python', 'Recruitment']",2025-06-13 05:21:30
Senior Data Scientist,Capgemini,5 - 9 years,Not Disclosed,['Gurugram'],"At Capgemini Invent, we believe difference drives change. As inventive transformation consultants, we blend our strategic, creative and scientific capabilities,collaborating closely with clients to deliver cutting-edge solutions. Join us to drive transformation tailored to our client's challenges of today and tomorrow.Informed and validated by science and data. Superpowered by creativity and design. All underpinned by technology created with purpose.\n\n \n\nYour role \n\nAs a Senior Data Scientist, you are expected to develop and implement Artificial Intelligence based solutions across various disciplines for the Intelligent Industry vertical of Capgemini Invent. You are expected to work as an individual contributor or along with a team to help design and develop ML/NLP models as per the requirement. You will work closely with the Product Owner, Systems Architect and other key stakeholders right from conceptualization till the implementation of the project. You should take ownership while understanding the client requirement, the data to be used, security & privacy needs and the infrastructure to be used for the development and implementation.\n\nThe candidate will be responsible for executing data science projects independently to deliver business outcomes and is expected to demonstrate domain expertise, develop, and execute program plans and proactively solicit feedback from stakeholders to identify improvement actions. This role requires a strong technical background, excellent problem-solving skills, and the ability to work collaboratively with stakeholders from different functional and business teams.\nThe role also requires the candidate to collaborate on ML asset creation and eager to learn and impart trainings to fellow data science professionals. We expect thought leadership from the candidate, especially on proposing to build a ML/NLP asset based on expected industry requirements. Experience in building Industry specific (e.g. Manufacturing, R&D, Supply Chain, Life Sciences etc), production ready AI Models using microservices and web-services is a plus.\n\nProgramming Languages Python NumPy, SciPy, Pandas, MatPlotLib, Seaborne\nDatabases RDBMS (MySQL, Oracle etc.), NoSQL Stores (HBase, Cassandra etc.)\nML/DL Frameworks SciKitLearn, TensorFlow (Keras), PyTorch,\nBig data ML Frameworks - Spark (Spark-ML, Graph-X), H2O\nCloud Azure/AWS/GCP\n\n \n\nYour Profile \n\nPredictive and Prescriptive modelling using Statistical and Machine Learning algorithms including but not limited to Time Series, Regression, Trees, Ensembles, Neural-Nets (Deep & Shallow CNN, LSTM, Transformers etc.). Experience with open-source OCR engines like Tesseract, Speech recognition, Computer Vision, face recognition, emotion detection etc. is a plus.\nUnsupervised learning Market Basket Analysis, Collaborative Filtering, Dimensionality Reduction, good understanding of common matrix decomposition approaches like SVD. Various Clustering approaches Hierarchical, Centroid-based, Density-based, Distribution-based, Graph-based clustering like Spectral.\nNLP Information Extraction, Similarity Matching, Sentiment Analysis, Text Clustering, Semantic Analysis, Document Summarization, Context Mapping/Understanding, Intent Classification, Word Embeddings, Vector Space Models, experience with libraries like NLTK, Spacy, Stanford Core-NLP is a plus. Usage of Transformers for NLP and experience with LLMs like (ChatGPT, Llama) and usage of RAGs (vector stores like LangChain & LangGraps), building Agentic AI applications.\nModel Deployment ML pipeline formation, data security and scrutiny check and ML-Ops for productionizing a built model on-premises and on cloud.\n\nRequired Qualifications\nMasters degree in a quantitative field such as Mathematics, Statistics, Machine Learning, Computer Science or Engineering or a bachelors degree with relevant experience.\nGood experience in programming with languages such as Python/Java/Scala, SQL and experience with data visualization tools like Tableau or Power BI.\n\nPreferred Experience\nExperienced in Agile way of working, manage team effort and track through JIRA\nExperience in Proposal, RFP, RFQ and pitch creations and delivery to the big forum.\nExperience in POC, MVP, PoV and assets creations with innovative use cases\nExperience working in a consulting environment is highly desirable.\nPresupposition\n\nHigh Impact client communication\nThe job may also entail sitting as well as working at a computer for extended periods of time. Candidates should be able to effectively communicate by telephone, email, and face to face.\n\n \n\nWhat you will love about working here \nWe recognize the significance of flexible work arrangements to provide support. Be it remote work, or flexible work hours, you will get an environment to maintain healthy work life balance.\nAt the heart of our mission is your career growth. Our array of career growth programs and diverse professions are crafted to support you in exploring a world of opportunities.\nEquip yourself with valuable certifications in the latest technologies such as Generative AI.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['numpy', 'sql', 'java', 'python', 'pandas', 'scala', 'poc', 'nltk', 'dl', 'artificial intelligence', 'tensorflow', 'spacy', 'spark', 'gcp', 'pytorch', 'keras', 'mysql', 'hbase', 'ml', 'jira', 'scipy', 'rdbms', 'oracle', 'mvp', 'microsoft azure', 'power bi', 'nosql', 'tableau', 'cassandra', 'matplotlib', 'agile', 'aws']",2025-06-13 05:21:32
Data Scientist Sr. Analyst,Accenture,5 - 10 years,Not Disclosed,['Kochi'],"Job Title - + +\n\n\n\nManagement Level:\n\n\n\nLocation:Kochi, Coimbatore, Trivandrum\n\n\n\nMust have skills:Big Data, Python or R\n\n\n\n\nGood to have skills:Scala, SQL\n\n\n\nJob\n\n\nSummary\n\nA Data Scientist is expected to be hands-on to deliver end to end vis a vis projects undertaken in the Analytics space. They must have a proven ability to drive business results with their data-based insights. They must be comfortable working with a wide range of stakeholders and functional teams. The right candidate will have a passion for discovering solutions hidden in large data sets and working with stakeholders to improve business outcomes.\n\n\n\nRoles and Responsibilities\nIdentify valuable data sources and collection processes\nSupervise preprocessing of structured and unstructured data\nAnalyze large amounts of information to discover trends and patterns for insurance industry.\nBuild predictive models and machine-learning algorithms\nCombine models through ensemble modeling\nPresent information using data visualization techniques\nCollaborate with engineering and product development teams\nHands-on knowledge of implementing various AI algorithms and best-fit scenarios\nHas worked on Generative AI based implementations\n\n\n\nProfessional and Technical Skills\n3.5-5 years experience in Analytics systems/program delivery; at least 2 Big Data or Advanced Analytics project implementation experience\nExperience using statistical computer languages (R, Python, SQL, Pyspark, etc.) to manipulate data and draw insights from large data sets; familiarity with Scala, Java or C++\nKnowledge of a variety of machine learning techniques (clustering, decision tree learning, artificial neural networks, etc.) and their real-world advantages/drawbacks\nKnowledge of advanced statistical techniques and concepts (regression, properties of distributions, statistical tests and proper usage, etc.) and experience with applications\nHands on experience in Azure/AWS analytics platform (3+ years)\nExperience using variations of Databricks or similar analytical applications in AWS/Azure\nExperience using business intelligence tools (e.g. Tableau) and data frameworks (e.g. Hadoop)\nStrong mathematical skills (e.g. statistics, algebra)\nExcellent communication and presentation skills\nDeploying data pipelines in production based on Continuous Delivery practices.\n\n\n\n\nAdditional Information\nMulti Industry domain experience\nExpert in Python, Scala, SQL\nKnowledge of Tableau/Power BI or similar self-service visualization tools\nInterpersonal and Team skills should be top notch\nNice to have leadership experience in the past\nQualification\n\n\n\nExperience:3.5 -5 years of experience is required\n\n\n\n\nEducational Qualification:Graduation (Accurate educational details should capture)",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'scala', 'sql', 'r', 'big data', 'advanced analytics', 'mathematics', 'data manipulation', 'presentation skills', 'microsoft azure', 'pyspark', 'power bi', 'machine learning', 'javascript', 'aws kinesis', 'tableau', 'decision tree', 'java', 'hadoop', 'data visualization', 'aws', 'statistics']",2025-06-13 05:21:34
Senior Analyst - Data Governance & Management,AMERICAN EXPRESS,3 - 7 years,Not Disclosed,['Gurugram'],"Here, your voice and ideas matter, your work makes an impact, and together, you will help us define the future of American Express.\nAt American Express, you ll be recognized for your contributions, leadership, and impact every colleague has the opportunity to share in the company s success. Together, we ll win as a team, striving to uphold our company values and powerful backing promise to provide the world s best customer experience every day. And we ll do it with the utmost integrity, and in an environment where everyone is seen, heard and feels like they belong.\nJoin Team Amex and lets lead the way together.",,,,"['Career development', 'metadata', 'Manager Quality Assurance', 'Data management', 'Finance', 'Shell scripting', 'Wellness', 'Data quality', 'Risk management', 'SQL']",2025-06-13 05:21:36
IN_Manager_Azure Data Engineer_Data & Analytics_Advisory_Bangalore,PwC Service Delivery Center,4 - 8 years,Not Disclosed,['Bengaluru'],"Not Applicable\nSpecialism\nSAP\n& Summary\nAt PwC, our people in data and analytics engineering focus on leveraging advanced technologies and techniques to design and develop robust data solutions for clients. They play a crucial role in transforming raw data into actionable insights, enabling informed decisionmaking and driving business growth.\n\nIn data engineering at PwC, you will focus on designing and building data infrastructure and systems to enable efficient data processing and analysis. You will be responsible for developing and implementing data pipelines, data integration, and data transformation solutions.\nAt PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purposeled and valuesdriven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us .\n& Summary A career within Data and Analytics services will provide you with the opportunity to help organisations uncover enterprise insights and drive business results using smarter data analytics. We focus on a collection of organisational technology capabilities, including business intelligence, data management, and data assurance that help our clients drive innovation, growth, and change within their organisations in order to keep up with the changing nature of customers and technology. We make impactful decisions by mixing mind and machine to leverage data, understand and navigate risk, and help our clients gain a competitive edge.\nResponsibilities\nDesign, develop, and optimize data pipelines and ETL processes using PySpark or Scala to extract, transform, and load large volumes of structured and unstructured data from diverse sources. Implement data ingestion, processing, and storage solutions on Azure cloud platform, leveraging services such as Azure Databricks, Azure Data Lake Storage, and Azure Synapse Analytics. Develop and maintain data models, schemas, and metadata to support efficient data access, query performance, and analytics requirements. Monitor pipeline performance, troubleshoot issues, and optimize data processing workflows for scalability, reliability, and costeffectiveness. Implement data security and compliance measures to protect sensitive information and ensure regulatory compliance. Requirement Proven experience as a Data Engineer, with expertise in building and optimizing data pipelines using PySpark, Scala, and Apache Spark. Handson experience with cloud platforms, particularly Azure, and proficiency in Azure services such as Azure Databricks, Azure Data Lake Storage, Azure Synapse Analytics, and Azure SQL Database. Strong programming skills in Python and Scala, with experience in software development, version control, and CI/CD practices. Familiarity with data warehousing concepts, dimensional modeling, and relational databases (e.g., SQL Server, PostgreSQL, MySQL).\nExperience with big data technologies and frameworks (e.g., Hadoop, Hive, HBase) is a plus.\nMandatory skill sets\nSpark, Pyspark, Azure\nPreferred skill sets\nSpark, Pyspark, Azure\nYears of experience required\n4 8\nEducation qualification\nB.Tech / M.Tech / MBA / MCA\nEducation\nDegrees/Field of Study required Bachelor of Engineering, Master of Business Administration, Master of Engineering\nDegrees/Field of Study preferred\nRequired Skills\nPython (Programming Language)\nAccepting Feedback, Accepting Feedback, Active Listening, Agile Scalability, Amazon Web Services (AWS), Analytical Thinking, Apache Airflow, Apache Hadoop, Azure Data Factory, Coaching and Feedback, Communication, Creativity, Data Anonymization, Data Architecture, Database Administration, Database Management System (DBMS), Database Optimization, Database Security Best Practices, Databricks Unified Data Analytics Platform, Data Engineering, Data Engineering Platforms, Data Infrastructure, Data Integration, Data Lake, Data Modeling {+ 32 more}\nNo",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['SAP', 'Data management', 'Data modeling', 'Postgresql', 'MySQL', 'Database administration', 'Agile', 'Apache', 'Business intelligence', 'Python']",2025-06-13 05:21:37
Data Scientist-Artificial Intelligence,IBM,5 - 7 years,Not Disclosed,['Bengaluru'],"Work with broader team to build, analyze and improve the AI solutions.\nYou will also work with our software developers in consuming different enterprise applications\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nResource should have 5-7 years of experience. Sound knowledge of Python and should know how to use the ML related services.\nProficient in Python with focus on Data Analytics Packages.\nStrategy Analyse large, complex data sets and provide actionable insights to inform business decisions.\nStrategy Design and implementing data models that help in identifying patterns and trends. Collaboration Work with data engineers to optimize and maintain data pipelines.\nPerform quantitative analyses that translate data into actionable insights and provide analytical, data-driven decision-making. Identify and recommend process improvements to enhance the efficiency of the data platform. Develop and maintain data models, algorithms, and statistical models\n\n\nPreferred technical and professional experience\nExperience with conversation analytics. Experience with cloud technologies\nExperience with data exploration tools such as Tableu",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['algorithms', 'python', 'data analytics', 'tableau', 'ml', 'hive', 'data analysis', 'natural language processing', 'pyspark', 'data warehousing', 'machine learning', 'artificial intelligence', 'sql', 'pandas', 'deep learning', 'java', 'data science', 'spark', 'kafka', 'hadoop', 'big data', 'aws', 'etl']",2025-06-13 05:21:39
S&C GN - Data&AI - CMT Eng - Consultant,Accenture,2 - 7 years,Not Disclosed,['Bengaluru'],"Job Title - S&C Global Network - AI - CMT DE- Consultant\n\n\n\nManagement Level:9- Consultant\n\n\n\nLocation:Open\n\n\n\nMust-have skills:Data Engineering\n\n\n\n\nGood to have skills:Ability to leverage design thinking, business process optimization, and stakeholder management skills.\n\n\n\nJob\n\n\nSummary:\n\nWe are looking for a passionate and results-driven\n\n\n\nData Engineerto join our growing data team. You will be responsible for designing, building, and maintaining scalable data pipelines and infrastructure that support data-driven decision-making across the organization.\n\n\n\n\nRoles & Responsibilities:\n\nDesign, build, and maintain robust, scalable, and efficient data pipelines (ETL/ELT).\nWork with structured and unstructured data across a wide variety of data sources.\nCollaborate with data analysts, data scientists, and business stakeholders to understand data requirements.\nOptimize data systems and architecture for performance, scalability, and reliability.\nMonitor data quality and support initiatives to ensure clean, accurate, and consistent data.\nDevelop and maintain data models and metadata.\nImplement and maintain best practices in data governance, security, and compliance.\n\n\n\n\nProfessional & Technical\n\n\n\n\nSkills:\n\n2+ years in data engineering or related fields\nProficiency in SQL and experience with relational databases (e.g., PostgreSQL, MySQL).\nStrong programming skills in Python, Scala, or Java.\nExperience with big data technologies such as Spark, Hadoop, or Hive.\nFamiliarity with cloud platforms like AWS, Azure, or GCP, especially services like S3, Redshift, BigQuery, or Azure Data Lake.\nExperience with orchestration tools like Airflow, Luigi, or similar.\nSolid understanding of data warehousing concepts and data modeling techniques.\nGood problem-solving skills and attention to detail.\nExperience with modern data stack tools like dbt, Snowflake, or Databricks.\nKnowledge of CI/CD pipelines and version control (e.g., Git).\nExposure to containerization (Docker, Kubernetes) and infrastructure as code (Terraform, CloudFormation).\n\n\n\n\nAdditional Information: - The ideal candidate will possess a strong educational background in quantitative discipline and experience in working with Hi-Tech clients\n\n- This position is based at our Bengaluru (preferred) and other AI Accenture locations.\n\n\n\n\n\nAbout Our Company | Accenture\n\nQualification\n\n\n\nExperience:4+ years\n\n\n\n\nEducational Qualification:Btech/ BE",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'scala', 'data engineering', 'sql', 'java', 'hive', 'continuous integration', 'kubernetes', 'snowflake', 'amazon redshift', 'airflow', 'microsoft azure', 'ci/cd', 'aws cloudformation', 'docker', 'data bricks', 'data modeling', 'spark', 'gcp', 'data warehousing concepts', 'terraform', 'hadoop', 'aws']",2025-06-13 05:21:41
Data Science,Global Banking Organization,5 - 10 years,Not Disclosed,['Bengaluru'],"Key Skills: Machine Learning, Data Science, Azure, Python, Hadoop.\nRoles and Responsibilities:\nStrong understanding of Math, Statistics, and the theoretical foundations of Statistical & Machine Learning, including Parametric and Non-parametric models.\nApply advanced data mining techniques to curate, process, and transform raw data into reliable datasets.\nUse various statistical techniques and ML methods to perform predictive modeling/classification for problems related to clients, distribution, sales, client profiles, and segmentation, and provide actionable insights for business decision-making.\nDemonstrate expertise in the full Machine Learning lifecycle--feature engineering, training, validation, scaling, deployment, scoring, monitoring, and feedback loops.\nProficiency in Python visualization libraries such as matplotlib and seaborn.\nExperience with cloud computing infrastructure like Azure, including Machine Learning Studio, Azure Data Factory, Synapse, Python, and PySpark.\nAbility to develop, test, and deploy models on cloud/web platforms.\nExcellent knowledge of Deep Learning Architectures, including Convolutional Neural Networks and Transformer/LLM Foundation Models.\nStrong expertise in supervised and adversarial learning techniques.\nRobust working knowledge of deep learning frameworks such as TensorFlow, Keras, and PyTorch.\nExcellent Python coding skills.\nExperience with version control tools (Git, GitHub/GitLab) and data version control.\nExperience in end-to-end model deployment and productionization.\nDemonstrated proficiency in deploying, scaling, and optimizing ML models in production environments with low latency, high availability, and cost efficiency.\nSkilled in model interpretability and CI/CD for ML using tools like MLflow and Kubeflow, with the ability to implement automated monitoring, logging, and retraining strategies.\nExperience Requirement:\n5-12 years of experience in designing and deploying deep learning and machine learning solutions.\nProven track record of delivering AI/ML solutions in real-world business applications at scale.\nHands-on experience working in cross-functional teams including data engineers, product managers, and business stakeholders.\nExperience mentoring junior data scientists and providing technical leadership within a data science team.\nExperience working with big data tools and environments such as Hadoop, Spark, or Databricks is a plus.\nPrior experience in managing model lifecycle in enterprise production environments including drift detection and retraining pipelines.\nEducation: B.Tech.",Industry Type: Financial Services,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Azure', 'Hadoop.', 'Machine Learning', 'Python']",2025-06-13 05:21:43
"Data Analyst, Staff",Qualcomm,4 - 7 years,Not Disclosed,['Bengaluru'],"Job Area: Miscellaneous Group, Miscellaneous Group > Data Analyst\n \n\nQualcomm Overview: \nQualcomm is a company of inventors that unlocked 5G ushering in an age of rapid acceleration in connectivity and new possibilities that will transform industries, create jobs, and enrich lives. But this is just the beginning. It takes inventive minds with diverse skills, backgrounds, and cultures to transform 5Gs potential into world-changing technologies and products. This is the Invention Age - and this is where you come in.\n\nGeneral Summary:\n\nAbout the Team\n\nQualcomm's People Analytics team plays a crucial role in transforming data into strategic workforce insights that drive HR and business decisions. As part of this lean but high-impact team, you will have the opportunity to analyze workforce trends, ensure data accuracy, and collaborate with key stakeholders to enhance our data ecosystem. This role is ideal for a generalist who thrives in a fast-paced, evolving environment""”someone who can independently conduct data analyses, communicate insights effectively, and work cross-functionally to enhance our People Analytics infrastructure.\n\nWhy Join Us\n\n\nEnd-to-End ImpactWork on the full analytics cycle""”from data extraction to insight generation""”driving meaningful HR and business decisions.\n\n\nCollaboration at ScalePartner with HR leaders, IT, and other analysts to ensure seamless data integration and analytics excellence.\n\n\nData-Driven CultureBe a key player in refining our data lake, ensuring data integrity, and influencing data governance efforts.\n\n\nProfessional GrowthGain exposure to multiple areas of people analytics, including analytics, storytelling, and stakeholder engagement.\n\n\nKey Responsibilities\n\n\nPeople Analytics & Insights\nAnalyze HR and workforce data to identify trends, generate insights, and provide recommendations to business and HR leaders.\nDevelop thoughtful insights to support ongoing HR and business decision-making.\nPresent findings in a clear and compelling way to stakeholders at various levels, including senior leadership.\n\n\nData Quality & Governance\nEnsure accuracy, consistency, and completeness of data when pulling from the data lake and other sources.\nIdentify and troubleshoot data inconsistencies, collaborating with IT and other teams to resolve issues.\nDocument and maintain data definitions, sources, and reporting standards to drive consistency across analytics initiatives.\n\n\nCollaboration & Stakeholder Management\nWork closely with other analysts on the team to align methodologies, share best practices, and enhance analytical capabilities.\nAct as a bridge between People Analytics, HR, and IT teams to define and communicate data requirements.\nPartner with IT and data engineering teams to improve data infrastructure and expand available datasets.\n\n\nQualifications\n\nRequired4-7 years experience in a People Analytics focused role\n\n\nAnalytical & Technical Skills\nStrong ability to analyze, interpret, and visualize HR and workforce data to drive insights.\nExperience working with large datasets and ensuring data integrity.\nProficiency in Excel and at least one data visualization tool (e.g., Tableau, Power BI).\n\n\nCommunication & Stakeholder Management\nAbility to communicate data insights effectively to both technical and non-technical audiences.\nStrong documentation skills to define and communicate data requirements clearly.\nExperience collaborating with cross-functional teams, including HR, IT, and business stakeholders.\n\n\nPreferred:\n\n\nTechnical Proficiency\nExperience with SQL, Python, or R for data manipulation and analysis.\nFamiliarity with HR systems (e.g., Workday) and cloud-based data platforms.\n\n\nPeople Analytics Expertise\nPrior experience in HR analytics, workforce planning, or related fields.\nUnderstanding of key HR metrics and workforce trends (e.g., turnover, engagement, diversity analytics).\n\n\nAdditional Information\nThis is an office-based position (4 days a week onsite) with possible locations that may include India and Mexico",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['sql', 'people analytics', 'documentation', 'tableau', 'data integration tools', 'hiring', 'data warehousing', 'data architecture', 'sourcing', 'jquery', 'staffing', 'plsql', 'oracle 10g', 'java', 'etl tool', 'html', 'etl', 'mongodb', 'python', 'oracle', 'power bi', 'hrsd', 'r', 'node.js', 'hr analytics', 'angularjs']",2025-06-13 05:21:44
IN_Manager_Azure Data Engineer_Data Analytics_Advisory,PwC Service Delivery Center,5 - 10 years,Not Disclosed,['Bengaluru'],"Not Applicable\nSpecialism\nData, Analytics & AI\n& Summary\nAt PwC, our people in data and analytics engineering focus on leveraging advanced technologies and techniques to design and develop robust data solutions for clients. They play a crucial role in transforming raw data into actionable insights, enabling informed decisionmaking and driving business growth.\n\nIn data engineering at PwC, you will focus on designing and building data infrastructure and systems to enable efficient data processing and analysis. You will be responsible for developing and implementing data pipelines, data integration, and data transformation solutions.\n& Summary A career within Data and Analytics services will provide you with the opportunity to help organisations uncover enterprise insights and drive business results using smarter data analytics. We focus on a collection of organisational technology capabilities, including business intelligence, data management, and data assurance that help our clients drive innovation, growth, and change within their organisations in order to keep up with the changing nature of customers and technology. We make impactful decisions by mixing mind and machine to leverage data, understand and navigate risk, and help our clients gain a competitive edge.\nResponsibilities\nMust have\nCandidates with minimum 5 years of relevant experience for 1012 years of total experience (Architect / Managerial level).\nDeep expertise with technologies such as Data factory, Data Bricks (Advanced), SQLDB (writing complex Stored Procedures), Synapse, Python scripting (mandatory), Pyspark scripting, Azure Analysis Services.\nMust be certified with DP 203 (Azure Data Engineer Associate), Databricks Certified Data Engineer Professional (Architect / Managerial level)\nStrong troubleshooting and debugging skills. Proven experience in working source control technologies (such as GITHUB, Azure DevOps), build and release pipelines.\nExperience in writing complex PySpark queries to perform data analysis.\nMandatory skill sets\nAzure Databricks, Pyspark, Datafactory\nPreferred skill sets\nAzure Databricks, Pyspark, Datafactory, Python, Azure Devops\nYears of experience required\n712yrs\nEducation qualification\nB.Tech / M.Tech / MBA / MCA\nEducation\nDegrees/Field of Study required Bachelor of Technology, Master of Business Administration\nDegrees/Field of Study preferred\nRequired Skills\nMicrosoft Azure\nAccepting Feedback, Accepting Feedback, Active Listening, Agile Scalability, Amazon Web Services (AWS), Analytical Thinking, Apache Airflow, Apache Hadoop, Azure Data Factory, Coaching and Feedback, Communication, Creativity, Data Anonymization, Data Architecture, Database Administration, Database Management System (DBMS), Database Optimization, Database Security Best Practices, Databricks Unified Data Analytics Platform, Data Engineering, Data Engineering Platforms, Data Infrastructure, Data Integration, Data Lake, Data Modeling {+ 32 more}\nTravel Requirements\nGovernment Clearance Required?",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data analysis', 'Data modeling', 'Debugging', 'Database administration', 'Agile', 'Stored procedures', 'Apache', 'Business intelligence', 'Troubleshooting', 'Python']",2025-06-13 05:21:46
AWS Data Engineer_ Capgemini_ Pan India Location,Capgemini,5 - 10 years,Not Disclosed,"['Chennai', 'Bengaluru', 'Mumbai (All Areas)']","Role & responsibilities\nAWS S3, Glue, API Gateway, Crawler, Athena, , Lambda, Dynamic DB, Redshift is an advantage\nExperience/knowledge with streaming technologies is must preferably Kafka\nShould have knowledge/experience with SQL\nGood analytical skills\nFamiliar working on Linux platforms\nHave good understanding on pros and cons and the cost impact of the AWS services being leveraged\nGood Communication skills.\n\nPrimary Skills\nAWS S3, Glue, Athena, Python or Pyspark\nShould have knowledge/experience with SQL\n\nSecondary Skills\nExcellent verbal and written communication and interpersonal skills\nAbility to work independently and within a team environment",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Lamda', 'AWS', 'Athena', 'SQL', 'Python', 'S', 'Amazon Redshift', 'Aws Glue']",2025-06-13 05:21:48
Data Bricks,PwC India,7 - 12 years,Not Disclosed,['Bengaluru'],"Job Summary:\n\nWe are seeking a talented Data Engineer with strong expertise in Databricks, specifically in Unity Catalog, PySpark, and SQL, to join our data team. Youll play a key role in building secure, scalable data pipelines and implementing robust data governance strategies using Unity Catalog.\n\nKey Responsibilities:",,,,"['DataBricks', 'Data Bricks', 'Pyspark', 'Delta Lake', 'Databricks Engineer', 'Unity Catalog', 'SQL']",2025-06-13 05:21:50
S&C GN - Data&AI - Life Sciences - Consultant,Accenture,4 - 9 years,Not Disclosed,['Bengaluru'],"Management Level:Ind&Func AI Decision Science Consultant\n\n\n\n\nJob Location:Bangalore / Gurgaon\n\n\n\nMust-have\n\n\n\n\nSkills:\nExcellent understanding of Pharma data sets commercial, clinical, Leverage ones hands on experience of working across one or more of these areas such as real-world evidence data, Statistical Models/Machine Learning including Segmentation & predictive modeling, hypothesis testing, multivariate statistical analysis, time series techniques, and optimization.\n\n\n\nGood-to-have\n\n\n\n\nSkills:\nProgramming languages such as R, Python, SQL, Spark, AWS, Azure, or Google Cloud for deploying and scaling language models, Data Visualization tools like Tableau, Power BI.\n\n\n\nExperience:Proven experience (4+ years) in working on Life Sciences/Pharma/Healthcare projects and delivering successful outcomes.\n\n\n\n\nEducational Qualification:Bachelors or Masters degree in Statistics, Data Science, Applied Mathematics, Business Analytics, Computer Science, Information Systems, or other Quantitative field.\n\n\n\nJob\n\n\nSummary\n\nThis role involves driving strategic initiatives, managing business transformations, and leveraging industry expertise to create value-driven solutions. Provide strategic advisory services, conduct market research, and develop data-driven recommendations to enhance business performance.\n\n\n\nKey Responsibilities\nAn opportunity to work on high-visibility projects with top Pharma clients around the globe.\nPersonalized training modules to develop your strategy & consulting acumen to grow your skills, industry knowledge, and capabilities.\nProvide Subject matter expertise in various sub-segments of the LS industry.\nSupport delivery of small to medium-sized teams to deliver consulting projects for global clients.\nResponsibilities may include strategy, implementation, process design, and change management for specific modules.\nWork with the team or as an Individual contributor on the project assigned which includes a variety of skills to be utilized from Data Engineering to Data Science\nDevelop assets and methodologies, point-of-view, research, or white papers for use by the team and the larger community.\nAcquire new skills that have utility across industry groups.\nSupport strategies and operating models focused on some business units and assess likely competitive responses. Also, assess implementation readiness and points of greatest impact.\n\n\n\n\n\nAdditional Information\nProficient in Excel, MS Word, PowerPoint, etc.\nAbility to solve complex business problems and deliver client delight.\nStrong writing skills to build points of view on current industry trends.\nGood communication, interpersonal, and presentation skills\n\n\nAbout Our Company | Accenture (do not remove the hyperlink)\n\nQualification\n\n\n\nExperience:Proven experience (4+ years) in working on Life Sciences/Pharma/Healthcare projects and delivering successful outcomes.\n\n\n\n\nEducational Qualification:Bachelors or Masters degree in Statistics, Data Science, Applied Mathematics, Business Analytics, Computer Science, Information Systems, or other Quantitative field.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['pharmaceutical', 'python', 'sql', 'life sciences', 'aws', 'microsoft azure', 'power bi', 'time series', 'machine learning', 'data engineering', 'artificial intelligence', 'tableau', 'r', 'data science', 'gcp', 'spark', 'predictive modeling', 'statistical modeling', 'data visualization', 'statistics']",2025-06-13 05:21:51
S&C Global Network - AI - CG&S - Consultant Data Science,Accenture,4 - 8 years,Not Disclosed,['Bengaluru'],"Job Title: Industry & Function AI Decision Science Consultant S&C Global Network\n\n\n\nManagement Level: 9 Consultant\n\n\n\nLocation: Primary - Bengaluru, Secondary - Gurugram\n\n\n\nMust-Have\n\n\n\n\nSkills:\nData Science, AI, ML, Experience with cloud platforms such as AWS, Azure, or Google Cloud, Hands-on experience in programming languages like Python, R, PySpark, and SQL\n\n\n\nGood-to-Have\n\n\n\n\nSkills:\nDeep Learning Techniques (e.g. RNN, CNN), Visualization tools like Power BI and Tableau, Exposure to tools like ChatGPT, Llama 2, Hugging Face, etc.\n\n\n\nJob\n\n\nSummary:\n\nAs an Industry & Function AI Decision Science Consultant, you will leverage your expertise in data science and Consumer Goods domain knowledge to design and deliver AI-driven solutions. Your role will include strategic analysis, project delivery, solution development, and technical execution to empower businesses with actionable insights and enable automated and augmented decision-making.\n\n\n\n\nRoles & Responsibilities:\nConduct strategic analysis of the AI, analytics, and data maturity landscape for clients in the Consumer Goods domain\nLead data science engagements, manage delivery teams, and build innovative AI capabilities\nDevelop and implement advanced analytics solutions tailored to client requirements\nUtilize languages like Python, PySpark, R, and SQL for data wrangling and machine learning model development\nLeverage cloud technologies (Azure, AWS, GCP) to integrate and implement AI solutions\nTranslate complex data into compelling narratives for effective data storytelling\nMentor junior team members and contribute to thought leadership\n\n\n\n\n\nProfessional & Technical\n\n\n\n\nSkills:\n\nProficiency in Python, R, PySpark, and SQL\nStrong knowledge of traditional statistical methods, machine learning techniques, and deep learning\nHands-on experience in Consumer Goods & Services domain\nCloud integration skills with platforms like AWS, Azure, or Google Cloud\nExperience with optimization techniques (exact and evolutionary)\nCertifications like AWS Certified Data Analytics Specialty or Google Professional Data Engineer\nFamiliarity with visualization tools like Tableau and Power BI\nExposure to large language models (e.g., ChatGPT, Llama 2)\nFamiliarity with version control systems like Git.\n\n\n\n\n\nAdditional Information:\nThe ideal candidate will have a strong educational background in data science, computer science, or a related field, along with a proven track record of delivering impactful AI-driven solutions in the Consumer Goods industry.\n\n\n\n\nAbout Our Company | Accenture\n\nQualification\n\n\n\nExperience: Minimum 4-8 years of hands-on experience in data science with a focus on the Consumer Goods industry\n\n\n\n\nEducational Qualification:Bachelors or Masters degree in Statistics, Economics, Mathematics, Computer Science, or equivalent degree with Data Science specialization (from a premier institute)",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'artificial intelligence', 'deep learning', 'data science', 'ml', 'advanced analytics', 'cnn', 'data analytics', 'pyspark', 'microsoft azure', 'power bi', 'machine learning', 'sql', 'r', 'tableau', 'git', 'rnn', 'gcp', 'machine learning algorithms', 'aws', 'consumer goods', 'statistics']",2025-06-13 05:21:53
S&C GN - Data&AI - Life Sciences - Analyst,Accenture,2 - 7 years,Not Disclosed,['Bengaluru'],"Management Level:Ind & Func AI Decision Science Analyst\n\n\n\n\nJob Location:Bangalore / Gurgaon\n\n\n\nMust-have\n\n\n\n\nSkills:\nLife Sciences/Pharma/Healthcare projects and delivering successful outcomes, commercial, clinical, Statistical Models/Machine Learning including Segmentation & predictive modeling, hypothesis testing, multivariate statistical analysis, time series techniques, and optimization.\n\n\n\nGood-to-have\n\n\n\n\nSkills:\nProficiency in Programming languages such as R, Python, SQL, Spark, AWS, Azure, or Google Cloud for deploying and scaling language models, Data Visualization tools like Tableau, Power BI\n\n\n\nJob\n\n\nSummary\n\nWe are seeking an experienced and visionary - Accenture S&C Global Network - Data & AI practice help our clients grow their business in entirely new ways. Analytics enables our clients to achieve high performance through insights from data - insights that inform better decisions and strengthen customer relationships. From strategy to execution, Accenture works with organizations to develop analytic capabilities - from accessing and reporting on data to predictive modelling - to outperform the competition.\n\n\n\nKey Responsibilities\nSupport delivery of small to medium-sized teams to deliver consulting projects for global clients.\nAn opportunity to work on high-visibility projects with top Pharma clients around the globe.\nPersonalized training modules to develop your strategy & consulting acumen to grow your skills, industry knowledge, and capabilities.\nResponsibilities may include strategy, implementation, process design, and change management for specific modules.\nWork with the team or as an Individual contributor on the project assigned which includes a variety of skills to be utilized from Data Engineering to Data Science\nDevelop assets and methodologies, point-of-view, research, or white papers for use by the team and the larger community.\nWork on variety of projects in Data Modeling, Data Engineering, Data Visualization, Data Science etc.,\nAcquire new skills that have utility across industry groups.\n\n\n\n\n\nAdditional Information\nAbility to solve complex business problems and deliver client delight.\nStrong writing skills to build points of view on current industry trends.\nGood communication, interpersonal, and presentation skills\n\n\nAbout Our Company | Accenture (do not remove the hyperlink)\n\n\nQualification\n\n\n\nExperience:Proven experience (2+ years) in working on Life Sciences/Pharma/Healthcare projects and delivering successful outcomes.\n\n\n\n\nEducational Qualification:Bachelors or Masters degree in Statistics, Data Science, Applied Mathematics, Business Analytics, Computer Science, Information Systems, or other Quantitative field.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['pharmaceutical', 'python', 'sql', 'life sciences', 'aws', 'presentation skills', 'microsoft azure', 'power bi', 'time series', 'machine learning', 'artificial intelligence', 'tableau', 'r', 'data science', 'gcp', 'spark', 'predictive modeling', 'statistical modeling', 'data visualization', 'statistics']",2025-06-13 05:21:55
S&C Global Network - AI - CG&S - Consultant Data Science,Accenture,4 - 8 years,Not Disclosed,['Bengaluru'],"Job Title: Industry & Function AI Decision Science Consultant S&C Global Network\n\n\n\nManagement Level: 9 Consultant\n\n\n\nLocation: Primary - Bengaluru, Secondary - Gurugram\n\n\n\nMust-Have\n\n\n\n\nSkills:\nData Science, AI, ML, Experience with cloud platforms such as AWS, Azure, or Google Cloud, Hands-on experience in programming languages like Python, R, PySpark, and SQL\n\n\n\nGood-to-Have\n\n\n\n\nSkills:\nDeep Learning Techniques (e.g. RNN, CNN), Visualization tools like Power BI and Tableau, Exposure to tools like ChatGPT, Llama 2, Hugging Face, etc.\n\n\n\nJob\n\n\nSummary:\n\nAs an Industry & Function AI Decision Science Consultant, you will leverage your expertise in data science and Consumer Goods domain knowledge to design and deliver AI-driven solutions. Your role will include strategic analysis, project delivery, solution development, and technical execution to empower businesses with actionable insights and enable automated and augmented decision-making.\n\n\n\n\nRoles & Responsibilities:\nConduct strategic analysis of the AI, analytics, and data maturity landscape for clients in the Consumer Goods domain\nLead data science engagements, manage delivery teams, and build innovative AI capabilities\nDevelop and implement advanced analytics solutions tailored to client requirements\nUtilize languages like Python, PySpark, R, and SQL for data wrangling and machine learning model development\nLeverage cloud technologies (Azure, AWS, GCP) to integrate and implement AI solutions\nTranslate complex data into compelling narratives for effective data storytelling\nMentor junior team members and contribute to thought leadership\n\n\n\n\n\nProfessional & Technical\n\n\n\n\nSkills:\n\nProficiency in Python, R, PySpark, and SQL\nStrong knowledge of traditional statistical methods, machine learning techniques, and deep learning\nHands-on experience in Consumer Goods & Services domain\nCloud integration skills with platforms like AWS, Azure, or Google Cloud\nExperience with optimization techniques (exact and evolutionary)\nCertifications like AWS Certified Data Analytics Specialty or Google Professional Data Engineer\nFamiliarity with visualization tools like Tableau and Power BI\nExposure to large language models (e.g., ChatGPT, Llama 2)\nFamiliarity with version control systems like Git.\n\n\n\n\n\nAdditional Information:\nThe ideal candidate will have a strong educational background in data science, computer science, or a related field, along with a proven track record of delivering impactful AI-driven solutions in the Consumer Goods industry.\n\n\n\n\nAbout Our Company | Accenture\n\nQualification\n\n\n\nExperience: Minimum 4-8 years of hands-on experience in data science with a focus on the Consumer Goods industry\n\n\n\n\nEducational Qualification:Bachelors or Masters degree in Statistics, Economics, Mathematics, Computer Science, or equivalent degree with Data Science specialization (from a premier institute)",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'artificial intelligence', 'deep learning', 'data science', 'ml', 'advanced analytics', 'cnn', 'data analytics', 'pyspark', 'microsoft azure', 'power bi', 'machine learning', 'sql', 'r', 'tableau', 'git', 'rnn', 'gcp', 'machine learning algorithms', 'aws', 'consumer goods', 'statistics']",2025-06-13 05:21:57
S&C Global Network - AI - CG&S - Consultant Data Science,Accenture,4 - 8 years,Not Disclosed,['Bengaluru'],"Job Title: Industry & Function AI Decision Science Consultant S&C Global Network\n\n\n\nManagement Level: 9 Consultant\n\n\n\nLocation: Primary - Bengaluru, Secondary - Gurugram\n\n\n\nMust-Have\n\n\n\n\nSkills:\nData Science, AI, ML, Experience with cloud platforms such as AWS, Azure, or Google Cloud, Hands-on experience in programming languages like Python, R, PySpark, and SQL\n\n\n\nGood-to-Have\n\n\n\n\nSkills:\nDeep Learning Techniques (e.g. RNN, CNN), Visualization tools like Power BI and Tableau, Exposure to tools like ChatGPT, Llama 2, Hugging Face, etc.\n\n\n\nJob\n\n\nSummary:\n\nAs an Industry & Function AI Decision Science Consultant, you will leverage your expertise in data science and Consumer Goods domain knowledge to design and deliver AI-driven solutions. Your role will include strategic analysis, project delivery, solution development, and technical execution to empower businesses with actionable insights and enable automated and augmented decision-making.\n\n\n\n\nRoles & Responsibilities:\nConduct strategic analysis of the AI, analytics, and data maturity landscape for clients in the Consumer Goods domain\nLead data science engagements, manage delivery teams, and build innovative AI capabilities\nDevelop and implement advanced analytics solutions tailored to client requirements\nUtilize languages like Python, PySpark, R, and SQL for data wrangling and machine learning model development\nLeverage cloud technologies (Azure, AWS, GCP) to integrate and implement AI solutions\nTranslate complex data into compelling narratives for effective data storytelling\nMentor junior team members and contribute to thought leadership\n\n\n\n\n\nProfessional & Technical\n\n\n\n\nSkills:\n\nProficiency in Python, R, PySpark, and SQL\nStrong knowledge of traditional statistical methods, machine learning techniques, and deep learning\nHands-on experience in Consumer Goods & Services domain\nCloud integration skills with platforms like AWS, Azure, or Google Cloud\nExperience with optimization techniques (exact and evolutionary)\nCertifications like AWS Certified Data Analytics Specialty or Google Professional Data Engineer\nFamiliarity with visualization tools like Tableau and Power BI\nExposure to large language models (e.g., ChatGPT, Llama 2)\nFamiliarity with version control systems like Git.\n\n\n\n\n\nAdditional Information:\nThe ideal candidate will have a strong educational background in data science, computer science, or a related field, along with a proven track record of delivering impactful AI-driven solutions in the Consumer Goods industry.\n\n\n\n\nAbout Our Company | Accenture\n\nQualification\n\n\n\nExperience: Minimum 4-8 years of hands-on experience in data science with a focus on the Consumer Goods industry\n\n\n\n\nEducational Qualification:Bachelors or Masters degree in Statistics, Economics, Mathematics, Computer Science, or equivalent degree with Data Science specialization (from a premier institute)",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'artificial intelligence', 'deep learning', 'data science', 'ml', 'advanced analytics', 'cnn', 'data analytics', 'pyspark', 'microsoft azure', 'power bi', 'machine learning', 'sql', 'r', 'tableau', 'git', 'rnn', 'gcp', 'machine learning algorithms', 'aws', 'consumer goods', 'statistics']",2025-06-13 05:21:59
Data Scientist-Artificial Intelligence,IBM,3 - 7 years,Not Disclosed,['Bengaluru'],"As an Associate Data Scientist at IBM, you will work to solve business problems using leading edge and open-source tools such as Python, R, and TensorFlow, combined with IBM tools and our AI application suites. You will prepare, analyze, and understand data to deliver insight, predict emerging trends, and provide recommendations to stakeholders.\n\nIn your role, you may be responsible for\nImplementing and validating predictive and prescriptive models and creating and maintaining statistical models with a focus on big data & incorporating machine learning. techniques in your projects\nWriting programs to cleanse and integrate data in an efficient and reusable manner\nWorking in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors\nCommunicating with internal and external clients to understand and define business needs and appropriate modelling techniques to provide analytical solutions.\nEvaluating modelling results and communicating the results to technical and non-technical audiences\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nProof of Concept (POC) DevelopmentDevelop POCs to validate and showcase the feasibility and effectiveness of the proposed AI solutions.\nCollaborate with development teams to implement and iterate on POCs, ensuring alignment with customer requirements and expectations.\nHelp in showcasing the ability of Gen AI code assistant to refactor/rewrite and document code from one language to another, particularly COBOL to JAVA through rapid prototypes/ PoC\nDocument solution architectures, design decisions, implementation details, and lessons learned.\nCreate technical documentation, white papers, and best practice guides\n\n\nPreferred technical and professional experience\nStrong programming skills, with proficiency in Python and experience with AI frameworks such as TensorFlow, PyTorch, Keras or Hugging Face.\nUnderstanding in the usage of libraries such as SciKit Learn, Pandas, Matplotlib, etc. Familiarity with cloud platforms\nExperience and working knowledge in COBOL & JAVA would be preferred",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'scikit-learn', 'tensorflow', 'pytorch', 'keras', 'natural language processing', 'neural networks', 'predictive', 'huggingface', 'machine learning', 'prototype', 'artificial intelligence', 'sql', 'pandas', 'deep learning', 'r', 'java', 'cobol', 'data science', 'matplotlib', 'big data', 'statistics']",2025-06-13 05:22:01
S&C GN - Data&AI - Retail - Consultant,Accenture,4 - 9 years,Not Disclosed,['Bengaluru'],"Job Title - Retail Specialized Data Scientist Level 9 SnC GN Data & AI\n\n\n\nManagement Level:09 - Consultant\n\n\n\nLocation:Bangalore / Gurgaon / Mumbai / Chennai / Pune / Hyderabad / Kolkata\n\n\n\nMust have skills:\nA solid understanding of retail industry dynamics, including key performance indicators (KPIs) such as sales trends, customer segmentation, inventory turnover, and promotions.\nStrong ability to communicate complex data insights to non-technical stakeholders, including senior management, marketing, and operational teams.\nMeticulous in ensuring data quality, accuracy, and consistency when handling large, complex datasets.\nGather and clean data from various retail sources, such as sales transactions, customer interactions, inventory management, website traffic, and marketing campaigns.\nStrong proficiency in Python for data manipulation, statistical analysis, and machine learning (libraries like Pandas, NumPy, Scikit-learn).\nExpertise in supervised and unsupervised learning algorithms\nUse advanced analytics to optimize pricing strategies based on market demand, competitor pricing, and customer price sensitivity.\n\n\n\n\nGood to have skills:\nFamiliarity with big data processing platforms like Apache Spark, Hadoop, or cloud-based platforms such as AWS or Google Cloud for large-scale data processing.\nExperience with ETL (Extract, Transform, Load) processes and tools like Apache Airflow to automate data workflows.\nFamiliarity with designing scalable and efficient data pipelines and architecture.\nExperience with tools like Tableau, Power BI, Matplotlib, and Seaborn to create meaningful visualizations that present data insights clearly.\n\n\nJob\n\n\nSummary: The Retail Specialized Data Scientist will play a pivotal role in utilizing advanced analytics, machine learning, and statistical modeling techniques to help our retail business make data-driven decisions. This individual will work closely with teams across marketing, product management, supply chain, and customer insights to drive business strategies and innovations. The ideal candidate should have experience in retail analytics and the ability to translate data into actionable insights.\n\n\n\n\nRoles & Responsibilities:\nLeverage Retail Knowledge:Utilize your deep understanding of the retail industry (merchandising, customer behavior, product lifecycle) to design AI solutions that address critical retail business needs.\nGather and clean data from various retail sources, such as sales transactions, customer interactions, inventory management, website traffic, and marketing campaigns.\nApply machine learning algorithms, such as classification, clustering, regression, and deep learning, to enhance predictive models.\nUse AI-driven techniques for personalization, demand forecasting, and fraud detection.\nUse advanced statistical methods help optimize existing use cases and build new products to serve new challenges and use cases.\nStay updated on the latest trends in data science and retail technology.\nCollaborate with executives, product managers, and marketing teams to translate insights into business actions.\n\n\n\n\nProfessional & Technical Skills:\nStrong analytical and statistical skills.\nExpertise in machine learning and AI.\nExperience with retail-specific datasets and KPIs.\nProficiency in data visualization and reporting tools.\nAbility to work with large datasets and complex data structures.\nStrong communication skills to interact with both technical and non-technical stakeholders.\nA solid understanding of the retail business and consumer behavior.\nProgramming Languages:Python, R, SQL, Scala\nData Analysis Tools:Pandas, NumPy, Scikit-learn, TensorFlow, Keras\nVisualization Tools:Tableau, Power BI, Matplotlib, Seaborn\nBig Data Technologies:Hadoop, Spark, AWS, Google Cloud\nDatabases:SQL, NoSQL (MongoDB, Cassandra)\n\n\n\n\nAdditional Information: -\n\nQualification\n\n\n\nExperience:Minimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification:Bachelors or Master's degree in Data Science, Statistics, Computer Science, Mathematics, or a related field.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'machine learning', 'artificial intelligence', 'data visualization', 'statistics', 'algorithms', 'data manipulation', 'scikit-learn', 'scala', 'numpy', 'unsupervised learning', 'sql', 'pandas', 'tensorflow', 'spark', 'consumer behavior', 'keras', 'hadoop', 'aws', 'reporting tools', 'retail business']",2025-06-13 05:22:04
Data Modeler,Accenture,15 - 20 years,Not Disclosed,['Bengaluru'],"Project Role :Data Modeler\n\n\n\n\n\nProject Role Description :Work with key business representatives, data owners, end users, application designers and data architects to model current and new data.\n\n\n\nMust have skills :Microsoft Power Business Intelligence (BI)\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Modeler, you will engage with key business representatives, data owners, end users, application designers, and data architects to model both current and new data. Your typical day will involve collaborating with various stakeholders to understand their data needs, analyzing existing data structures, and designing effective data models that support business objectives. You will also be responsible for ensuring that the data models are aligned with best practices and meet the requirements of the organization, facilitating seamless data integration and accessibility across different platforms.\nRoles & Responsibilities:- Expected to be an SME.- Collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Facilitate training sessions for junior team members to enhance their understanding of data modeling.- Continuously evaluate and improve data modeling processes to ensure efficiency and effectiveness.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Microsoft Power Business Intelligence (BI).- Strong understanding of data modeling concepts and best practices.- Experience with data integration techniques and tools.- Familiarity with database management systems and SQL.- Ability to communicate complex data concepts to non-technical stakeholders.\nAdditional Information:- The candidate should have minimum 5 years of experience in Microsoft Power Business Intelligence (BI).- This position is based at our Bengaluru office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['business intelligence', 'sql', 'database management', 'data modeling', 'data integration', '3d modeling', 'tekla structures', '3ds max', 'python', '3d modeler', 'oracle', 'texturing', 'bi', 'data warehousing', 'sme', 'photoshop', 'autocad', 'sql server', 'maya', 'plsql', 'modeler', 'etl', 'tekla', 'informatica']",2025-06-13 05:22:06
IN_Director_Senior Data Architect_Data & Analytics_Advisory_ Bangalore,PwC Service Delivery Center,12 - 18 years,Not Disclosed,['Bengaluru'],"Not Applicable\nSpecialism\nData, Analytics & AI\nManagement Level\nDirector\n& Summary\n\n\n.\n& Summary We are seeking an experienced Senior Data Architect to lead the design and development of our data architecture, leveraging cloudbased technologies, big data processing frameworks, and DevOps practices. The ideal candidate will have a strong background in data warehousing, data pipelines, performance optimization, and collaboration with DevOps teams.\nResponsibilities\n1. Design and implement endtoend data pipelines using cloudbased services (AWS/ GCP/Azure) and conventional data processing frameworks.\n2. Lead the development of data architecture, ensuring scalability, security, and performance.\n3. Collaborate with crossfunctional teams, including DevOps, to design and implement data lakes, data warehouses, and data ingestion/extraction processes. 4. Develop and optimize data processing workflows using PySpark, Kafka, and other big data processing frameworks.\n5. Ensure data quality, integrity, and security across all data pipelines and architectures.\n6. Provide technical leadership and guidance to junior team members.\n7. Design and implement data load strategies, data partitioning, and data storage solutions.\n8. Collaborate with stakeholders to understand business requirements and develop data solutions to meet those needs.\n9. Work closely with DevOps team to ensure seamless integration of data pipelines with overall system architecture.\n10. Participate in design and implementation of CI/CD pipelines for data workflows.\nDevOps Requirements\n1. Knowledge of DevOps practices and tools, such as Jenkins, GitLab CI/CD, or Apache Airflow.\n2. Experience with containerization using Docker.\n3. Understanding of infrastructure as code (IaC) concepts using tools like Terraform or AWS CloudFormation.\n4. Familiarity with monitoring and logging tools, such as Prometheus, Grafana, or ELK Stack.\nRequirements\n1. 1214 years of experience for Senior Data Architect in data architecture, data warehousing, and big data processing.\n2. Strong expertise in cloudbased technologies (AWS/ GCP/ Azure) and data processing frameworks (PySpark, Kafka, Flink , Beam etc.).\n3. Experience with data ingestion, data extraction, data warehousing, and data lakes.\n4. Strong understanding of performance optimization, data partitioning, and data storage solutions.\n5. Excellent leadership and communication skills.\n6. Experience with NoSQL databases is a plus.\nMandatory skill sets\n1. Experience with agile development methodologies.\n2. Certification in cloudbased technologies (AWS / GCP/ Azure) or data processing frameworks.\n3. Experience with data governance, data quality, and data security.\nPreferred skill sets\nKnowledge of AgenticAI and GenAI is added advantage\nYears of experience required\n12 to 18 years\nEducation qualification\nGraduate Engineer or Management Graduate\nEducation\nDegrees/Field of Study required Bachelor of Engineering\nDegrees/Field of Study preferred\nRequired Skills\nAWS Devops\nAccepting Feedback, Accepting Feedback, Active Listening, Analytical Reasoning, Analytical Thinking, Application Software, Business Data Analytics, Business Management, Business Technology, Business Transformation, Coaching and Feedback, Communication, Creativity, Documentation Development, Embracing Change, Emotional Regulation, Empathy, Implementation Research, Implementation Support, Implementing Technology, Inclusion, Influence, Innovation, Intellectual Curiosity, Learning Agility {+ 28 more}\nTravel Requirements\nGovernment Clearance Required?",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['System architecture', 'Business transformation', 'GCP', 'Analytical', 'Consulting', 'Data processing', 'Data quality', 'Operations', 'Monitoring', 'Data architecture']",2025-06-13 05:22:07
"STAFF, DATA SCIENTIST",Walmart,5 - 10 years,Not Disclosed,['Bengaluru'],"Position Summary...\nWhat youll do...\nAbout Team\nThe Catalog Data Science Team at Walmart Global Tech is focused on using the latest research in generative AI (GenAI), artificial intelligence (AI), machine learning (ML), statistics, deep learning, computer vision and optimization to implement solutions that ensure Walmart s product catalog is accurate, complete, and optimized for customer experience. Our team tackles complex data science and ML engineering challenges related to product classification, attribute extraction, trust & safety, and catalog optimization, empowering next-generation retail use cases.\nThe Data Science and ML Engineering community at Walmart Global Tech is active in most of the Hack events, utilizing the petabytes of data at our disposal, to build some of the coolest ideas. All the work we do at Walmart Global Tech will eventually benefit our operations & our associates, helping Customers Save Money to Live Better.\nWhat youll do:\nWe are looking for a Staff Machine Learning Engineer who can help build large scale AI/ML/Optimization products. Expected qualities include ability to build, deploy, maintain and troubleshoot large scale systems.\nAs a Staff ML Engineer, you ll have the opportunity to\nDrive research initiatives and proof-of-concepts that push the state of the art in generative AI and large-scale machine learning.\nDesign and implement high-throughput, low-latency AI/ML pipelines and microservices that operate at global scale.\nOversee data ingestion, model training, evaluation, deployment and monitoring-ensuring performance, quality and reliability.\nCustomize and optimize LLMs for specific business use cases, balancing accuracy, latency and cost.\nPrototype novel generative AI solutions, integrate advancements into production, and collaborate with research partners.\nChampion best practices in data quality, lineage, governance and cost optimization across ML pipelines.\nMentor a team of ML engineers, establish coding standards, conduct design reviews, and foster a culture of continuous improvement.\nPresent your team s work at top-tier AI/ML conferences, publish scientific papers, and cultivate partnerships with universities and research labs.\nWhat youll bring:\nPhD in Computer Science, Statistics, Applied Mathematics or related field with 5+ years experience in ML engineering-or Master s with 8+ years or Bachelor s with 10+ years.\nProven track record of leading and scaling AI/ML products in production environments.\nDeep expertise in generative AI, large-scale model deployment, and fine-tuning of transformer-based architectures.\nStrong programming skills in Python, or equivalent, and experience with big data frameworks (Spark, Hadoop) and ML platforms (TensorFlow, PyTorch).\nDemonstrated history of scientific publications or patents in AI/ML.\nExcellent communication skills, a growth mindset, and the ability to drive cross-functional collaboration.\nAbout Walmart Global Tech\n.\n.\nFlexible, hybrid work\n.\nBenefits\n.\nBelonging\n.\n.\nEqual Opportunity Employer\nWalmart, Inc., is an Equal Opportunities Employer - By Choice. We believe we are best equipped to help our associates, customers and the communities we serve live better when we really know them. That means understanding, respecting and valuing unique styles, experiences, identities, ideas and\nMinimum Qualifications...\nMinimum Qualifications:Option 1: Bachelors degree in Statistics, Economics, Analytics, Mathematics, Computer Science, Information Technology or related field and 4 years experience in an analytics related field. Option 2: Masters degree in Statistics, Economics, Analytics, Mathematics, Computer Science, Information Technology or related field and 2 years experience in an analytics related field. Option 3: 6 years experience in an analytics or related field.\nPreferred Qualifications...\nPrimary Location...",Industry Type: Retail,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Prototype', 'Networking', 'Coding', 'Machine learning', 'Continuous improvement', 'Information technology', 'Monitoring', 'Analytics', 'Python']",2025-06-13 05:22:09
AVP Data Management Analyst,Wells Fargo,2 - 7 years,Not Disclosed,['Bengaluru'],"About this role:\nWells Fargo is seeking a Data Management Analyst\n\nIn this role, you will:\nParticipate in less complex analysis to identify and remediate data quality or integrity issues and to identify and remediate process or control gaps\nAdhere to data governance standards and procedures",,,,"['Data Management', 'Agile Methodology', 'Funds Transfer Pricing', 'Financial Data Mapping', 'Big Data Query Techniques', 'Lineage Tracing', 'Data warehousing', 'Data Governance', 'Jira', 'Market Risks', 'SQL']",2025-06-13 05:22:11
Advanced Data Science Associate,ZS,0 - 2 years,Not Disclosed,['Bengaluru'],"Develop advanced and efficient statistically effective algorithms that solve problems of high dimensionality .\nUtilize technical skills such as hypothesis testing, machine learning and retrieval processes to apply statistical and data mining techniques to identify trends, create figures, and analyze other relevant information.\nCollaborate with clients and other stakeholders at ZS to integrate and effectively communicate analysis findings.\nContribute to the assessment of emerging datasets and technologies that impact our analytical",,,,"['Text mining', 'Analytical', 'Management consulting', 'Financial planning', 'Machine learning', 'Hypothesis Testing', 'Predictive modeling', 'Data mining', 'big data']",2025-06-13 05:22:13
Data Analyst - Gurugram,Infosys,5 - 10 years,Not Disclosed,"['Chennai', 'Bengaluru', 'PAN INDIA']","Responsibilities:\nUnderstand architecture requirements and ensure effective design, development, validation, and support activities.\nAnalyze user requirements, envisioning system features and functionality.\nIdentify bottlenecks and bugs, and recommend system solutions by comparing advantages and disadvantages of custom development.\nContribute to team meetings, troubleshooting development and production problems across multiple environments and operating platforms.\nEnsure effective design, development, validation, and support activities for Big Data solutions.\nTechnical and Professional Requirements:\nSkills:\nProficiency in Scala, Spark, Hive, and Kafka.\nIn-depth knowledge of design issues and best practices.\nSolid understanding of object-oriented programming.\nFamiliarity with various design, architectural patterns, and software development processes.\nExperience with both external and embedded databases.\nCreating database schemas that represent and support business processes.\nImplementing automated testing platforms and unit tests.\nPreferred Skills:\nTechnology -> Big Data -> Scala, Spark, Hive, Kafka\nAdditional Responsibilities:\nCompetencies:\nGood verbal and written communication skills.\nAbility to communicate with remote teams effectively.\nHigh flexibility to travel.\nEducational Requirements:Master of Computer Applications, Master of Technology, Master of Engineering, MSc, Bachelor of Technology, Bachelor of Computer Applications, Bachelor of Computer Science, Bachelor of Engineering",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Spark', 'Hive', 'Hadoop', 'Big Data', 'Kafka']",2025-06-13 05:22:15
Big data Developer,Diverse Lynx,3 - 5 years,Not Disclosed,['Bengaluru'],"Total Yrs. of Experience 8+ Yrs Relevant Yrs. of experience 4+ Yrs Detailed JD (Roles and Responsibilities)\nGather operational Client on business processes and policies from multiple sources\nPrepare periodical and ad-hoc reports using operational data\nDevelop semantic core to align data with business processes\nSupport operations teams work streams for data processing, analysis and reporting\nAnalyse data and Create dashboards for the senior management\nDesign and implement optimal processes\nRegression testing of the releases\nMandatory skills\nBig Data : Spark, Hive, DataBricks\nLanguage : SQL, JAVA /Python\nBI Analytics: Power BI (DAX), Tableau , Dataiku\nOperating System : Unix\nExperience with Data Migration, Data Engineering, Data Analysis\nDesired/ Secondary skills\nBig Data : SCALA, HADOOP\nTools: DB Visualizer, JIRA, GIT, Bitbucket, Control-M\nStrong problem-solving skills and the ability to work independently and in a team environment.\nExcellent communication skills and the ability to work effectively with cross-functional teams.\nDomain eCommerce / Retail Max Vendor Rate in Per Day (Currency in relevance to work location) 11000 INR/Day Delivery Anchor for tracking the sourcing statistics, technical evaluation, interviews and feedback etc. Prem_dason@infosys.com Work Location given in ECMS ID Bangalore (Hyd, Pune, Trivandrum locations also ok)",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Unix', 'Data analysis', 'Data migration', 'Control-M', 'Data processing', 'Regression testing', 'Operations', 'Analytics', 'SQL', 'Python']",2025-06-13 05:22:17
"ETL Developer ( SSIS, Informatica ,Talend, Big Data)Group Manager",Vuram,5 - 10 years,Not Disclosed,['Bengaluru'],"Design, develop, and maintain relational and non-relational database systems.\nDefine analytical architecture including datalakes, lakehouse, data mesh and medallion patterns\nAbility to understand & analyze business requirements and translate them into analytical or relational database designAble to design for non-SQL datastores\nOptimize SQL queries, stored procedures, and database performance.Create and maintain ETL processes for data integration from various sources.\nWork closely with application teams to design database schemas and support integration.Monitor, troubleshoot, and resolve database issues related to performance, storage, and replication.Implement data security, backup, recovery, and disaster recovery procedures.\nEnsure data integrity and enforce best practices in database development.\nParticipate in code reviews and mentor junior developers.Collaborate with business and analytics teams for reporting and data warehousing needs.Must Have: Strong expertise in SQL and PL/SQL\nHands-on experience with at least one RDBMS: SQL Server, Oracle, PostgreSQL, or MySQL\nExperience with NoSQL databases: MongoDB, Cassandra, or Redis (at least one)\nETL Development Tools: SSIS, Informatica, Talend, or equivalent\nExperience in performance tuning and optimization\nDatabase design and modeling tools: Erwin, dbForge, or similar\nCloud platforms: Experience with AWS RDS, Azure SQL, or Google Cloud SQL\nBackup and Recovery Management, High Availability, and Disaster Recovery Planning\nUnderstanding of indexing, partitioning, replication, and sharding\nKnowledge of CI/CD pipelines and DevOps practices for database deployments\nExperience with Big Data technologies (Hadoop, Spark)Experience working in Agile/Scrum environments\n\n\nQualifications\nBachelor s or Master s degree in Computer Science, Information Technology, or a related field.8+ years of relevant experience in database design and development",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Performance tuning', 'RDBMS', 'Database design', 'MySQL', 'PLSQL', 'Informatica', 'Stored procedures', 'Oracle', 'SSIS', 'SQL']",2025-06-13 05:22:18
Analyst - Data Analytics,AMERICAN EXPRESS,0 - 4 years,Not Disclosed,['Gurugram'],The American Express Enterprise Digital Experimentation & Analytics (EDEA) leads the Enterprise Product Analytics and Experimentation charter for Brand & Performance Marketing and Digital Acquisition & Membership experiences as we'll as Enterprise Platforms. The focus of this collaborative team is to drive growth by enabling efficiencies in paid performance channels & evolve our digital experiences with actionable insights & analytics. The team specializes in using data around digital product usage to drive improvements in the acquisition customer experience to deliver higher satisfaction and business value.\n,,,,"['Mining', 'Career development', 'Finance', 'Analytical', 'Data processing', 'Analytics', 'SQL']",2025-06-13 05:22:20
Senior Data Scientist | Snowflakes | Tableau | AI/ML,Cisco,0 - 2 years,Not Disclosed,['Bengaluru'],"Job posting may be removed earlier if the position is filled or if a sufficient number of applications are received.\n\nMeet the Team\n\nWe are a dynamic and innovative team of Data Engineers, Data Architects, and Data Scientists based in Bangalore, India. Our mission is to harness the power of data to provide actionable insights that empower executives to make informed, data-driven decisions. By analyzing and interpreting complex datasets, we enable the organization to understand the health of the business and identify opportunities for growth and improvement.\n\nYour Impact\n\nWe are seeking a highly experienced and skilled Senior Data Scientist to join our dynamic team. The ideal candidate will possess deep expertise in machine learning models, artificial intelligence (AI), generative AI, and data visualization. Proficiency in Tableau and other visualization tools is essential. This role requires hands-on experience with databases such as Snowflake and Teradata, as well as advanced knowledge in various data science and AI techniques. The successful candidate will play a pivotal role in driving data-driven decision-making and innovation within our organization.\n\nKey Responsibilities\nDesign, develop, and implement advanced machine learning models to solve complex business problems.\nApply AI techniques and generative AI models to enhance data analysis and predictive capabilities.\nUtilize Tableau and other visualization tools to create insightful and actionable dashboards for stakeholders.\nManage and optimize large datasets using Snowflake and Teradata databases.\nCollaborate with cross-functional teams to understand business needs and translate them into analytical solutions.\nStay updated with the latest advancements in data science, machine learning, and AI technologies.\nMentor and guide junior data scientists, fostering a culture of continuous learning and development.\nCommunicate complex analytical concepts and results to non-technical stakeholders effectively.\nKey Technologies &\n\nSkills:\nMachine Learning ModelsSupervised learning, unsupervised learning, reinforcement learning, deep learning, neural networks, decision trees, random forests, support vector machines (SVM), clustering algorithms, etc.\nAI TechniquesNatural language processing (NLP), computer vision, generative adversarial networks (GANs), transfer learning, etc.\nVisualization ToolsTableau, Power BI, Matplotlib, Seaborn, Plotly, etc.\nDatabasesSnowflake, Teradata, SQL, NoSQL databases.\nProgramming LanguagesPython (essential), R, SQL.\nPython LibrariesTensorFlow, PyTorch, scikit-learn, pandas, NumPy, Keras, SciPy, etc.\nData ProcessingETL processes, data warehousing, data lakes.\nCloud PlatformsAWS, Azure, Google Cloud Platform.\nMinimum Qualifications\nBachelor's or Master's degree in Computer Science, Statistics, Mathematics, Data Science, or a related field.\nMinimum of [X] years of experience as a Data Scientist or in a similar role.\nProven track record in developing and deploying machine learning models and AI solutions.\nStrong expertise in data visualization tools, particularly Tableau.\nExtensive experience with Snowflake and Teradata databases.\nExcellent problem-solving skills and the ability to work independently and collaboratively.\nExceptional communication skills with the ability to convey complex information clearly.\nPreferred Qualifications (Provide up to five (5) bullet points these can include soft skills)\nExcellent communication and collaboration skills to work effectively in cross-functional teams.\nAbility to translate business requirements into technical solutions.\nStrong problem-solving skills and the ability to work with complex datasets.\nExperience in statistical analysis and machine learning techniques.\nUnderstanding of business domains such as sales, financials, marketing, and telemetry.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['machine learning', 'artificial intelligence', 'sql', 'tableau', 'data visualization', 'snowflake', 'scipy', 'python', 'scikit-learn', 'data warehousing', 'numpy', 'pandas', 'tensorflow', 'data integration tools', 'matplotlib', 'pytorch', 'keras', 'machine learning algorithms', 'etl', 'nosql databases']",2025-06-13 05:22:22
DE&A - AIML - Data Science - Data Science (Other) DE&A - AIML,Zensar,15 - 18 years,Not Disclosed,"['Kolkata', 'Mumbai', 'New Delhi', 'Hyderabad', 'Pune', 'Chennai', 'Bengaluru']","Experience: 15+ years overall | Minimum 10 full-cycle AI/ML project implementations , including GenAI experience\nRole Summary:\nWe are seeking a AI Architect to lead strategic AI transformation initiatives. This role demands deep hands-on experience in AI, Machine Learning (ML), and Generative AI (GenAI) , along with the ability to engage directly with C-level stakeholders , align technical delivery with business objectives, and drive enterprise-wide adoption of advanced AI solutions.\nThe ideal candidate is a techno-strategic leader who can take AI/ML/GenAI projects from ideation to production building architectures, leading cross-functional teams, and ensuring regulatory and operational alignment in BFSI environments.\nKey\nConsulting & Business Alignment\nPartner with senior business and IT leadership , including CIOs, CDOs, and COOs , to identify high-impact use cases across retail banking, insurance, credit, and capital markets.\nTranslate complex BFSI challenges into technically feasible and scalable AI/ML/GenAI solutions.\nCreate strategic roadmaps, capability assessments, and PoV/PoC execution plans that align with business KPIs and regulatory needs.\nSolution Architecture & Delivery Leadership\nDesign and lead delivery of AI/ML/GenAI pipelines covering data ingestion, model training, validation, deployment, and monitoring.\nBuild and scale GenAI-based solutions like LLM-driven chatbots, intelligent document processing, RAG pipelines, summarization tools , and virtual assistants.\nArchitect cloud-native AI platforms using AWS (SageMaker, Bedrock) , Azure (ML, OpenAI) , or GCP (Vertex AI, BigQuery, LangChain) .\nDefine and implement MLOps and LLMOps frameworks for versioning, retraining, CI/CD, and production observability.\nEnsure adherence to Responsible AI principles , including explainability, bias mitigation, auditability, and regulatory compliance\nEngineering & Integration\nWork closely with data engineering teams to acquire, transform, and pipeline data from core banking systems, CRMs, claims systems, and real-time feeds.\nDesign architecture for data lakes, feature stores, and vector databases supporting AI and GenAI use cases.\nEnable seamless integration of AI capabilities into enterprise workflows, customer platforms, and decision engines via APIs and microservices.\nRequired Skills & Experience:\n15+ years of experience in AI/ML, data engineering, and cloud architecture.\nMinimum of 10 end-to-end AI/ML project implementations from use case discovery through to productionization.\nProven expertise in: (Any One)\nAI/ML frameworks : scikit-learn, XGBoost, TensorFlow, PyTorch\nGenAI/LLM platforms : OpenAI, Cohere, Mistral, LangChain, Hugging Face, vector DBs (Pinecone, FAISS, Chroma)\nCloud platforms : AWS, Azure, GCP - including AI/ML & GenAI native services\nMLOps/LLMOps tools : MLflow, Kubeflow, SageMaker Pipelines, Vertex AI Pipelines\nStrong experience with data security, governance, model risk management , and AI compliance frameworks relevant to BFSI.\nAbility to lead large cross-functional teams and engage both technical teams and senior stakeholders.\nExperience: 15+ years overall | Minimum 10 full-cycle AI/ML project implementations , including GenAI experience\nRole Summary:\nWe are seeking a AI Architect to lead strategic AI transformation initiatives. This role demands deep hands-on experience in AI, Machine Learning (ML), and Generative AI (GenAI) , along with the ability to engage directly with C-level stakeholders , align technical delivery with business objectives, and drive enterprise-wide adoption of advanced AI solutions.\nThe ideal candidate is a techno-strategic leader who can take AI/ML/GenAI projects from ideation to production building architectures, leading cross-functional teams, and ensuring regulatory and operational alignment in BFSI environments.\nKey\nConsulting & Business Alignment\nPartner with senior business and IT leadership , including CIOs, CDOs, and COOs , to identify high-impact use cases across retail banking, insurance, credit, and capital markets.\nTranslate complex BFSI challenges into technically feasible and scalable AI/ML/GenAI solutions.\nCreate strategic roadmaps, capability assessments, and PoV/PoC execution plans that align with business KPIs and regulatory needs.\nSolution Architecture & Delivery Leadership\nDesign and lead delivery of AI/ML/GenAI pipelines covering data ingestion, model training, validation, deployment, and monitoring.\nBuild and scale GenAI-based solutions like LLM-driven chatbots, intelligent document processing, RAG pipelines, summarization tools , and virtual assistants.\nArchitect cloud-native AI platforms using AWS (SageMaker, Bedrock) , Azure (ML, OpenAI) , or GCP (Vertex AI, BigQuery, LangChain) .\nDefine and implement MLOps and LLMOps frameworks for versioning, retraining, CI/CD, and production observability.\nEnsure adherence to Responsible AI principles , including explainability, bias mitigation, auditability, and regulatory compliance\nEngineering & Integration\nWork closely with data engineering teams to acquire, transform, and pipeline data from core banking systems, CRMs, claims systems, and real-time feeds.\nDesign architecture for data lakes, feature stores, and vector databases supporting AI and GenAI use cases.\nEnable seamless integration of AI capabilities into enterprise workflows, customer platforms, and decision engines via APIs and microservices.\nRequired Skills & Experience:\n15+ years of experience in AI/ML, data engineering, and cloud architecture.\nMinimum of 10 end-to-end AI/ML project implementations from use case discovery through to productionization.\nProven expertise in: (Any One)\nAI/ML frameworks : scikit-learn, XGBoost, TensorFlow, PyTorch\nGenAI/LLM platforms : OpenAI, Cohere, Mistral, LangChain, Hugging Face, vector DBs (Pinecone, FAISS, Chroma)\nCloud platforms : AWS, Azure, GCP - including AI/ML & GenAI native services\nMLOps/LLMOps tools : MLflow, Kubeflow, SageMaker Pipelines, Vertex AI Pipelines\nStrong experience with data security, governance, model risk management , and AI compliance frameworks relevant to BFSI.\nAbility to lead large cross-functional teams and engage both technical teams and senior stakeholders.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Solution architecture', 'Architect', 'Bfsi', 'data security', 'Consulting', 'Machine learning', 'Risk management', 'Operations', 'Monitoring', 'Core banking']",2025-06-13 05:22:24
Data Scientist,Amgen Inc,1 - 6 years,Not Disclosed,['Hyderabad'],"ABOUT THE ROLE\nRole Description:\nThe Data Scientist is responsible for developing and implementing AI-driven solutions to enhance cybersecurity measures within the organization. This role involves leveraging data science techniques to analyze security data, detect threats, and automate security processes. The Data Scientist will work closely with cybersecurity teams to identify data-driven automation opportunities, strengthening the organizations security posture.\nRoles & Responsibilities:",,,,"['data science', 'R', 'PyTorch', 'SAS', 'predictive analytics', 'Scikit-learn', 'SPSS', 'machine learning', 'data engineering', 'Python', 'TensorFlow']",2025-06-13 05:22:26
Data & Gen AI Specialist,Altimetrik,1 - 4 years,Not Disclosed,['Bengaluru'],"Job Title: Data & GenAI AWS Specialist\nExperience: 1-4 Years\nLocation: Bangalore\nMandatory Qualification: B.E./ B.Tech/ M.Tech/ MS from IIT or IISc ONLY\nJob Overview:\nWe are seeking a seasoned Data & GenAI Specialist with deep expertise in AWS Managed Services (PaaS) to join our innovative team. The ideal candidate will have extensive experience in designing sophisticated, scalable architectures for data pipelines and Generative AI (GenAI) solutions leveraging cloud services.",,,,"['Generative Ai', 'Cloud', 'Data Science', 'Open Source', 'Data Pipeline', 'GCP', 'Azure Cloud', 'Snowflake', 'Machine Learning', 'AWS']",2025-06-13 05:22:28
Data Scientist - Python / Machine Learning,Blueberry Unicorn Services,6 - 11 years,Not Disclosed,"['Hyderabad', 'Bengaluru']","Working Hours : 2PM to 11PM IST\n\nMid-Level ML Engineers / Data Scientist Role : (4-5 years of experience )\n\n- Experience processing, filtering, and presenting large quantities (100K to Millions of rows) of data using Pandas and PySpark\n\n- Experience with statistical analysis, data modeling, machine learning, optimizations, regression modeling and forecasting, time series analysis, data mining, and demand modeling.\n\n- Experience applying various machine learning techniques and understanding the key parameters that affect their performance.\n\n- Experience with Predictive analytics (e.g., forecasting, time-series, neural networks) and Prescriptive analytics (e.g., stochastic optimization, bandits, reinforcement learning).\n\n- Experience with Python and Python packages like NumPy, Pandas and deep learning frameworks like TensorFlow, Pytorch and Keras\n\n- Experience in Big Data ecosystem with frameworks like Spark, PySpark , Unstructured DBs like Elasticsearch and MongoDB\n\n- Proficiency with TABLEAU or other web-based interfaces to create graphic-rich customizable plots, charts data maps etc.\n\n- Able to write SQL scripts for analysis and reporting (Redshift, SQL, MySQL).\n\n- Previous experience in ML, data scientist or optimization engineer role with a large technology company.\n\n- Experience in an operational environment developing, fast-prototyping, piloting, and launching analytic products.\n\n- Ability to develop experimental and analytic plans for data modeling processes, use of strong baselines, ability to accurately determine cause and effect relations.\n\n- Experience in creating data driven visualizations to describe an end-to-end system.\n\n- Excellent written and verbal communication skills. The role requires effective communication with colleagues from computer science, operations research, and business backgrounds.\n\n- Bachelors or Masters in Artificial Intelligence, Computer Science, Statistics, Applied Math, or a related field.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Machine Learning', 'Data Science', 'Data Scientist', 'Artificial Intelligence', 'Data Management', 'Big Data', 'Data Modeling', 'Spark', 'Numpy', 'Python', 'Predictive Analytics']",2025-06-13 05:22:29
Data Science Manager,ZS,10 - 15 years,Not Disclosed,"['Pune', 'Bengaluru']","A key enabler of our services is leveraging data in delivering client solutions. The data available about customers is getting richer and the problems that our customers are trying to answer continue to evolve. In our endeavor to stay ahead in providing solutions to these evolving complex problems, ZS has set up an Advanced Data Science which has three major focus areas:\nResearch the evolving datasets and advanced analytical techniques to develop new offerings/solutions\nDeliver client impact by collaboratively implementing these solutions",,,,"['Team management', 'data science', 'Pharma', 'Analytical', 'Management consulting', 'Financial planning', 'Healthcare', 'Project planning', 'Predictive modeling', 'Financial services']",2025-06-13 05:22:31
Enterprise Data Operations Manager,Pepsico,12 - 17 years,Not Disclosed,['Hyderabad'],"Overview\n\nDeputy Director - Data Engineering\n\nPepsiCo operates in an environment undergoing immense and rapid change. Big-data and digital technologies are driving business transformation that is unlocking new capabilities and business innovations in areas like eCommerce, mobile experiences and IoT. The key to winning in these areas is being able to leverage enterprise data foundations built on PepsiCos global business scale to enable business insights, advanced analytics, and new product development. PepsiCos Data Management and Operations team is tasked with the responsibility of developing quality data collection processes, maintaining the integrity of our data foundations, and enabling business leaders and data scientists across the company to have rapid access to the data they need for decision-making and innovation.",,,,"['Data Engineering', 'Pyspark', 'Azure', 'Power BI', 'Github', 'Azure Databricks', 'Tableau', 'ADO', 'Scala programming', 'SQL', 'Azure Data Factory', 'Azure Machine learning', 'Data Lakehouse', 'Azure Data Engineering', 'CI/CD', 'Data Warehousing', 'Data Analytics', 'AWS', 'Python']",2025-06-13 05:22:33
ETL Developer - Data & Analytics,Canpack India,3 - 5 years,Not Disclosed,['Pune'],"Giorgi Global Holdings, Inc. ( GGH ) is a privately held, diversified consumer products/packaging company with approximately 11,000 employees and operations in 20 countries. GGH consists of four US based companies ( The Giorgi Companies ) and one global packaging company ( CANPACK ).\nGGH has embarked on a transformation journey to become a digital, technology enabled, customer-centric, data and insights-driven organization. This transformation is evolving our business, strategy, core operations and IT solutions.\nAs an ETL Developer, you will be an integral part of our Data and Analytics team, working closely with the ETL Architect and other developers to design, develop, and maintain efficient data integration and transformation solutions. We are looking for a highly skilled ETL Developer with a deep understanding of ETL processes and data warehousing. The ideal candidate is passionate about optimizing data extraction, transformation, and loading workflows, ensuring high performance, accuracy, and scalability to support business intelligence initiatives.\nWhat you will do:\n1. Design, develop, test and maintain ETL processes and data pipelines to support data integration and transformation needs.\n2. Continuously improve ETL performance and reliability through best practices and optimization techniques.\n3. Develop and implement data validation and quality checks to ensure the integrity and consistency of data.\n4. Collaborate with ETL Architect, Data Engineers, and Business Intelligence teams to understand business requirements and translate them into technical solutions.\n5. Monitor, troubleshoot, and resolve ETL job failures, performance bottlenecks, and data discrepancies.\n6. Proactively identify and resolve ETL-related issues, minimizing impact on business operations.\n7. Contribute to documentation, training, and knowledge sharing to enhance team capabilities.\n8. Communicate progress and challenges clearly to both technical and non-technical teams\nEssential Requirements:\nBachelor s or master s degree in information technology, Computer Science, or a related field.\n3-5 years of relevant experience.\nPower-BI, Tabular Editor/Dax Studio, ALM/Github/Azure Devops skills\nExposure to SAP Systems/Modules like SD, MD, etc. to understand functional data.,\nExposure to MS Fbric, MS Azure Synapse Analytics\nCompetencies needed:\n- Hands-on experience with ETL development and data integration for large-scale systems\n- Experience with platforms such as Synapse Analytics, Azure Data Factory, Fabric, Redshift or Databricks\n- A solid understanding of data warehousing and ETL processes\n- Advanced SQL and PL/SQL skills such as query optimization, complex joins, window functions\n- Expertise in Python (pySpark) programming with a focus on data manipulation and analysis\n- Experience with Azure DevOps and CI/CD process\n- Excellent problem-solving and analytical skills\n- Experience in creating post-implementation documentation\n- Strong team collaboration skills\n- Attention to detail and a commitment to quality\nStrong interpersonal skills including analytical thinking, creativity, organizational abilities, high commitment, initiative in task execution, and a fast-learning capability for understanding IT concepts\n\nIf you are a current CANPACK employee, please apply through your Workday account .\nCANPACK Group is an Equal Opportunity Employer and all qualified applicants will receive consideration for employment without regard to race, colour, religion, age, sex, sexual orientation, gender identity, national origin, disability, or any other characteristic protected by law or not related to job requirements, unless such distinction is required by law.",Industry Type: Packaging & Containers,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'SAP', 'Analytical', 'Packaging', 'PLSQL', 'Business intelligence', 'Information technology', 'Analytics', 'Python', 'Data extraction']",2025-06-13 05:22:34
Data Product Owner,Capgemini,9 - 14 years,Not Disclosed,['Hyderabad'],"\n\nThe Product Owner III will be responsible for defining and prioritizing features and user stories, outlining acceptance criteria, and collaborating with cross-functional teams to ensure successful delivery of product increments. This role requires strong communication skills to effectively engage with stakeholders, gather requirements, and facilitate product demos.\n\nThe ideal candidate should have a deep understanding of agile methodologies, experience in the insurance sector, and possess the ability to translate complex needs into actionable tasks for the development team.\n\n Key Responsibilities: \nDefine and communicate the  vision, roadmap, and backlog  for data products.\nManages team backlog items and prioritizes based on business value.\nPartners with the business owner to understand needs, manage scope and add/eliminate user stories while contributing heavy influence to build an effective strategy.\nTranslate business requirements into scalable data product features.\nCollaborate with data engineers, analysts, and business stakeholders to prioritize and deliver impactful solutions.\nChampion  data governance , privacy, and compliance best practices.\nAct as the voice of the customer to ensure usability and adoption of data products.\nLead Agile ceremonies (e.g., backlog grooming, sprint planning, demos) and maintain a clear product backlog.\nMonitor data product performance and continuously identify areas for improvement.\nSupport the integration of AI/ML solutions and advanced analytics into product offerings.\n\n\n  \n\n Required Skills & Experience: \nProven experience as a Product Owner, ideally in data or analytics domains.\nStrong understanding of  data engineering ,  data architecture , and  cloud platforms  (AWS, Azure, GCP).\nFamiliarity with  SQL , data modeling, and modern data stack tools (e.g., Snowflake, dbt, Airflow).\nExcellent  stakeholder management  and communication skills across technical and non-technical teams.\nStrong  business acumen  and ability to align data products with strategic goals.\nExperience with  Agile/Scrum methodologies  and working in cross-functional teams.\nAbility to  translate data insights into compelling stories and recommendations .\n\n\n",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data architecture', 'business acumen', 'data engineering', 'stakeholder management', 'agile methodology', 'snowflake', 'advanced analytics', 'microsoft azure', 'cloud platforms', 'user stories', 'demo', 'sql', 'data modeling', 'gcp', 'sprint planning', 'scrum', 'product performance', 'agile', 'aws', 'ml']",2025-06-13 05:22:36
S&C Global Network - AI - Life Sciences -Data Science Consultant,Accenture,4 - 9 years,Not Disclosed,['Gurugram'],"Job Title -\n\n\n\nS&C Global Network - AI - Healthcare Analytics - Consultant\n\n\n\nManagement Level:\n\n\n\n9-Team Lead/Consultant\n\n\n\nLocation:\n\n\n\nBangalore/Gurgaon\n\n\n\nMust-have skills:R,Phython,SQL,Spark,Tableau ,Power BI\n\n\n\n\nGood to have skills:Ability to leverage design thinking, business process optimization, and stakeholder management skills.\n\n\n\nJob\n\n\nSummary:\n\nThis role involves driving strategic initiatives, managing business transformations, and leveraging industry expertise to create value-driven solutions.\n\n\n\n\nRoles & Responsibilities:\n\nProvide strategic advisory services, conduct market research, and develop data-driven recommendations to enhance business performance.\n\n\n\nWHATS IN IT FOR YOU\nAn opportunity to work on high-visibility projects with top Pharma clients around the globe.\nPotential to Co-create with leaders in strategy, industry experts, enterprise function practitioners, and business intelligence professionals to shape and recommend innovative solutions that leverage emerging technologies.\nAbility to embed responsible business into everythingfrom how you service your clients to how you operate as a responsible professional.\nPersonalized training modules to develop your strategy & consulting acumen to grow your skills, industry knowledge, and capabilities.\nOpportunity to thrive in a culture that is committed to accelerating equality for all. Engage in boundaryless collaboration across the entire organization.\n\n\n\n\nWhat you would do in this role\nSupport delivery of small to medium-sized teams to deliver consulting projects for global clients.\nResponsibilities may include strategy, implementation, process design, and change management for specific modules.\nWork with the team or as an Individual contributor on the project assigned which includes a variety of skills to be utilized from Data Engineering to Data Science\nProvide Subject matter expertise in various sub-segments of the LS industry.\nDevelop assets and methodologies, point-of-view, research, or white papers for use by the team and the larger community.\nAcquire new skills that have utility across industry groups.\nSupport strategies and operating models focused on some business units and assess likely competitive responses. Also, assess implementation readiness and points of greatest impact.\nCo-lead proposals, and business development efforts and coordinate with other colleagues to create consensus-driven deliverables.\nExecute a transformational change plan aligned with the clients business strategy and context for change. Engage stakeholders in the change journey and build commitment to change.\nMake presentations wherever required to a known audience or client on functional aspects of his or her domain.\n\n\n\nWho are we looking for\nBachelors or Masters degree in Statistics, Data Science, Applied Mathematics, Business Analytics, Computer Science, Information Systems, or other Quantitative field.\nProven experience (4+ years) in working on Life Sciences/Pharma/Healthcare projects and delivering successful outcomes.\nExcellent understanding of Pharma data sets commercial, clinical, RWE (Real World Evidence) & EMR (Electronic medical records)\nLeverage ones hands on experience of working across one or more of these areas such as real-world evidence data, R&D clinical data, digital marketing data.\nHands-on experience with handling Datasets like Komodo, RAVE, IQVIA, Truven, Optum etc.\nHands-on experience in building and deployment of Statistical Models/Machine Learning including Segmentation & predictive modeling, hypothesis testing, multivariate statistical analysis, time series techniques, and optimization.\nProficiency in Programming languages such as R, Python, SQL, Spark, etc.\nAbility to work with large data sets and present findings/insights to key stakeholders; Data management using databases like SQL.\nExperience with any of the cloud platforms like AWS, Azure, or Google Cloud for deploying and scaling language models.\nExperience with any of the Data Visualization tools like Tableau, Power BI, Qlikview, Spotfire is good to have.\nExcellent analytical and problem-solving skills, with a data-driven mindset.\nProficient in Excel, MS Word, PowerPoint, etc.\nAbility to solve complex business problems and deliver client delight.\nStrong writing skills to build points of view on current industry trends.\nGood communication, interpersonal, and presentation skills\n\n\n\n\n\nProfessional & Technical\n\n\n\n\nSkills:\n\n\n- Relevant experience in the required domain.\n\n- Strong analytical, problem-solving, and communication skills.\n\n- Ability to work in a fast-paced, dynamic environment.\n\n\n\n\nAdditional Information:\n\n- Opportunity to work on innovative projects.\n\n- Career growth and leadership exposure.\n\n\n\n\n\nAbout Our Company | Accenture\nQualification\n\n\n\nExperience:\n\n\n\n4-8 Years\n\n\n\n\nEducational Qualification:\n\n\n\nBachelors or Masters degree in Statistics, Data Science, Applied Mathematics, Business Analytics, Computer Science, Information Systems, or other Quantitative field.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'sql', 'tableau', 'r', 'spark', 'spotfire', 'power bi', 'microsoft azure', 'time series', 'emr', 'machine learning', 'data engineering', 'artificial intelligence', 'qlikview', 'data science', 'gcp', 'predictive modeling', 'segmentation', 'life sciences', 'data visualization', 'aws', 'statistics']",2025-06-13 05:22:38
S&C GN - Data&AI - Life Sciences - Consultant,Accenture,4 - 9 years,Not Disclosed,['Gurugram'],"Management Level:Ind&Func AI Decision Science Consultant\n\n\n\n\nJob Location:Bangalore / Gurgaon\n\n\n\nMust-have\n\n\n\n\nSkills:\nExcellent understanding of Pharma data sets commercial, clinical, Leverage ones hands on experience of working across one or more of these areas such as real-world evidence data, Statistical Models/Machine Learning including Segmentation & predictive modeling, hypothesis testing, multivariate statistical analysis, time series techniques, and optimization.\n\n\n\nGood-to-have\n\n\n\n\nSkills:\nProgramming languages such as R, Python, SQL, Spark, AWS, Azure, or Google Cloud for deploying and scaling language models, Data Visualization tools like Tableau, Power BI.\n\n\n\n\nJob\n\n\nSummary\n\nThis role involves driving strategic initiatives, managing business transformations, and leveraging industry expertise to create value-driven solutions. Provide strategic advisory services, conduct market research, and develop data-driven recommendations to enhance business performance.\n\n\n\nKey Responsibilities\nAn opportunity to work on high-visibility projects with top Pharma clients around the globe.\nPersonalized training modules to develop your strategy & consulting acumen to grow your skills, industry knowledge, and capabilities.\nProvide Subject matter expertise in various sub-segments of the LS industry.\nSupport delivery of small to medium-sized teams to deliver consulting projects for global clients.\nResponsibilities may include strategy, implementation, process design, and change management for specific modules.\nWork with the team or as an Individual contributor on the project assigned which includes a variety of skills to be utilized from Data Engineering to Data Science\nDevelop assets and methodologies, point-of-view, research, or white papers for use by the team and the larger community.\nAcquire new skills that have utility across industry groups.\nSupport strategies and operating models focused on some business units and assess likely competitive responses. Also, assess implementation readiness and points of greatest impact.\n\n\n\n\n\nAdditional Information\nProficient in Excel, MS Word, PowerPoint, etc.\nAbility to solve complex business problems and deliver client delight.\nStrong writing skills to build points of view on current industry trends.\nGood communication, interpersonal, and presentation skills\n\n\nAbout Our Company | Accenture (do not remove the hyperlink)\n\nQualification\n\n\n\nExperience:Proven experience (4+ years) in working on Life Sciences/Pharma/Healthcare projects and delivering successful outcomes.\n\n\n\n\nEducational Qualification:Bachelors or Masters degree in Statistics, Data Science, Applied Mathematics, Business Analytics, Computer Science, Information Systems, or other Quantitative field.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['pharmaceutical', 'python', 'sql', 'life sciences', 'aws', 'microsoft azure', 'power bi', 'time series', 'machine learning', 'data engineering', 'artificial intelligence', 'tableau', 'r', 'data science', 'gcp', 'spark', 'predictive modeling', 'statistical modeling', 'data visualization', 'statistics']",2025-06-13 05:22:40
S&C GN - Data&AI - Life Sciences - Analyst,Accenture,2 - 7 years,Not Disclosed,['Gurugram'],"Management Level:Ind & Func AI Decision Science Analyst\n\n\n\n\nJob Location:Bangalore / Gurgaon\n\n\n\nMust-have\n\n\n\n\nSkills:\nLife Sciences/Pharma/Healthcare projects and delivering successful outcomes, commercial, clinical, Statistical Models/Machine Learning including Segmentation & predictive modeling, hypothesis testing, multivariate statistical analysis, time series techniques, and optimization.\n\n\n\nGood-to-have\n\n\n\n\nSkills:\nProficiency in Programming languages such as R, Python, SQL, Spark, AWS, Azure, or Google Cloud for deploying and scaling language models, Data Visualization tools like Tableau, Power BI\n\n\n\nExperience:Proven experience (2+ years) in working on Life Sciences/Pharma/Healthcare projects and delivering successful outcomes.\n\n\n\n\nEducational Qualification:Bachelors or Masters degree in Statistics, Data Science, Applied Mathematics, Business Analytics, Computer Science, Information Systems, or other Quantitative field.\n\n\n\nJob\n\n\nSummary\n\nWe are seeking an experienced and visionary - Accenture S&C Global Network - Data & AI practice help our clients grow their business in entirely new ways. Analytics enables our clients to achieve high performance through insights from data - insights that inform better decisions and strengthen customer relationships. From strategy to execution, Accenture works with organizations to develop analytic capabilities - from accessing and reporting on data to predictive modelling - to outperform the competition.\n\n\n\nKey Responsibilities\nSupport delivery of small to medium-sized teams to deliver consulting projects for global clients.\nAn opportunity to work on high-visibility projects with top Pharma clients around the globe.\nPersonalized training modules to develop your strategy & consulting acumen to grow your skills, industry knowledge, and capabilities.\nResponsibilities may include strategy, implementation, process design, and change management for specific modules.\nWork with the team or as an Individual contributor on the project assigned which includes a variety of skills to be utilized from Data Engineering to Data Science\nDevelop assets and methodologies, point-of-view, research, or white papers for use by the team and the larger community.\nWork on variety of projects in Data Modeling, Data Engineering, Data Visualization, Data Science etc.,\nAcquire new skills that have utility across industry groups.\n\n\n\n\n\nAdditional Information\nAbility to solve complex business problems and deliver client delight.\nStrong writing skills to build points of view on current industry trends.\nGood communication, interpersonal, and presentation skills\n\n\nAbout Our Company | Accenture (do not remove the hyperlink)\n\n\nQualification\n\n\n\nExperience:Proven experience (2+ years) in working on Life Sciences/Pharma/Healthcare projects and delivering successful outcomes.\n\n\n\n\nEducational Qualification:Bachelors or Masters degree in Statistics, Data Science, Applied Mathematics, Business Analytics, Computer Science, Information Systems, or other Quantitative field.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['pharmaceutical', 'python', 'sql', 'life sciences', 'aws', 'presentation skills', 'microsoft azure', 'power bi', 'time series', 'machine learning', 'artificial intelligence', 'tableau', 'r', 'data science', 'gcp', 'spark', 'predictive modeling', 'statistical modeling', 'data visualization', 'statistics']",2025-06-13 05:22:42
Data Catalogue - Analyst,AstraZeneca India Pvt. Ltd,1 - 8 years,Not Disclosed,['Chennai'],"Job Title: Data Catalogue Analyst\nCareer Leve : C3\nIntroduction to role:\nAre you ready to make a significant impact in the world of data management? As a Data Catalogue Analyst, youll play a crucial role in ensuring that data is findable, accessible, and fit for use across various business units. Youll be responsible for capturing metadata and developing our data catalogue, supporting the Commercial and Enabling Units business areas. This is your chance to contribute to meaningful work that drives excellence and breakthroughs.\nAccountabilities:\nSupport the Data Catalogue Principal to define Information Asset Registers across business areas to help profile information risk/value\nParticipate in projects to mitigate and control identified priority risk areas\nTake responsibility for nominated markets/business areas, develop domain knowledge and leverage internal customer relationships to respond to localised use cases\nAct as point of contact for nominated business areas or markets\nSupport initiatives to enhance the reusability and transparency of our data by making it available in our global data catalogue\nSupport the capture of user requirements for functionality and usability, and document technical requirements\nWork with IT partners to capture metadata for relevant data sets and lineage, and populate the catalogue\nWork with data stewards and business users to enrich catalogue entries with business data dictionary, business rules, glossaries\nComplete monitoring controls to assure metadata quality remains at a high level\nSupport catalogue principles and data governance leads for tool evaluation and UAT\nEssential Skills/Experience:\nDemonstrable experience of working in a data management, data governance or data engineering domain\nStrong business and system analysis skills\nDemonstrable experience with Data Catalogue, Search and Automation software (Collibra, Informatica, Talend etc)\nAbility to interpret and communicate technical information into business language and in alignment with AZ business\nSolid grasp of metadata harvesting methodologies and ability to create business and technical metadata sets.\nStrong engagement, communication and collaborator management skills, including excellent organizational, presentation and influencing skills\nHigh level of proficiency with common business applications (Excel, Visio, Word, PowerPoint & SAP business user)\nDesirable Skills/Experience:\nDemonstrable experience of working with Commercial or Finance data and systems (Veeva, Reltio, SAP) and consumption\nDomain knowledge of life sciences/pharmaceuticals; manufacturing; corporate finance; or sales & marketing\nExperience with data quality and profiling software\nExperience of working in a complex, diverse global organization\nAstraZeneca offers an environment where you can apply your skills to genuinely impact patients lives. With a focus on innovation and growth, youll be part of a team that challenges norms and embraces intelligent risks. Our collaborative community thrives on sharing knowledge and celebrating successes together. Here, youll find opportunities to learn from diverse perspectives, drive change, and contribute to our digital transformation journey.\nReady to take the next step in your career? Apply now and become a key player in shaping the future at AstraZeneca!\n11-Jun-2025\n11-Jun-2025",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Automation', 'SAP', 'Data management', 'Senior Analyst', 'Life sciences', 'Corporate finance', 'Data quality', 'Visio', 'Monitoring', 'Recruitment']",2025-06-13 05:22:44
Associate Specialist Data Science,Merck Sharp & Dohme (MSD),2 - 7 years,Not Disclosed,['Pune'],"Primary Responsibilities\nSupport in establishing frameworks to standardize, productize and scale existing and new capabilities / analytical solutions\nImplement the vision, roadmap, and best practices for the Data Science Center of Excellence ( CoE ) to align with business goals\nSupport establishing governance frameworks to measure the value of products, standardize data science methodologies, coding practices, and project workflows\nWork with senior CoE members in development and maintenance of best practices for model and algorithm development and design, deployment, and monitoring across the enterprise functions\nCollaborate with product team on product development incorporating Agile framework and latest industry best practices and norms\nSupport in development of MLOps and ModelOps frameworks to streamline the development-to-deployment product pipeline\nDrive innovation by identifying, evaluating, and implementing cutting-edge data science methodologies based on latest published literature\n\nQualifications\nEducation & Work Experience Requiremen ts:\nMaster s degree (relevant field like Economics, Statistics, Mathematics, Operational Research) with 2+ years work experience.\nBachelor s degree (in Engineering or related field, such as Computer Science, Data Science, Statistics, Business, etc.) with at least 3 + years relevant experience\nPrior experience in research publications in reputed journal is a plus\nSkillset:\nCandidates must have -\nStrong programming skills in languages such as Python or R, and SQL with experience in data manipulation and analysis libraries (e.g., pandas, NumPy, scikit-learn, stats models)\nExperience with data science principles, machine learning (supervised and unsupervised) and GenAI algorithms, test-control analysis, propensity score matching etc.\nExposure to product roadmaps, Agile methodologies and backlog management, ensuring iterative and incremental product improvements\nStrong problem solving, business analysis and quantitative skills\nAbility to effectively communicate proposals to key stakeholders\nCandidates are desired but not mandatory to have -\nExperience and familiarity with underlying concepts such as Patient analytics, MMx etc.\nUnderstanding of Pharma commercial landscape will be a plus\nExperience working with healthcare, financial, or enterprise SaaS products\n  Search Firm Representatives Please Read Carefully\nEmployee Status:\nRegular\nRelocation:\nVISA Sponsorship:\nTravel Requirements:\nFlexible Work Arrangements:\nNot Applicable\nShift:\nValid Driving License:\nHazardous Material(s):\n\nRequired Skills:\nBusiness Intelligence (BI), Database Design, Data Engineering, Data Modeling, Data Science, Data Visualization, Machine Learning, Software Development, Stakeholder Relationship Management, Waterfall Model",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Relationship management', 'Business analysis', 'Coding', 'Pharma', 'Analytical', 'Healthcare', 'Business intelligence', 'Analytics', 'Monitoring', 'SQL']",2025-06-13 05:22:45
Data & Analytics Specialist,Roche Diagnostics,5 - 10 years,Not Disclosed,['Pune'],"At Roche you can show up as yourself, embraced for the unique qualities you bring. Our culture encourages personal expression, open dialogue, and genuine connections, where you are valued, accepted and respected for who you are, allowing you to thrive both personally and professionally. This is how we aim to prevent, stop and cure diseases and ensure everyone has access to healthcare today and for generations to come. Join Roche, where every voice matters.\nThe Position The Position\nWe are looking for a Data & Analytics Specialist/ Business Analyst who will join us in the newly setup Integrated Informatics for a journey to drive transformation with data and foster automated and efficient decision making throughout the organisation\nThe Data and Analytics Specialist must be the big-picture thinker who understands the value of data to the organisation, has a strong focus on delivering high value, connecting the dots, investing in right initiatives with reusability at the heart of it.\nIn this position you will be acting as squad lead, have end to end ownership of Product delivery with setting up teams from multiple teams/areas with focus on Lifecycle management of the product\nResponsibilities\nYou will work on various aspects of Analytics Solution Development, Data Management, Governance and Information Architecture including but not limited to:\nCollaborate with business stakeholders to understand their data and analytics needs and develop a product roadmap that aligns with business goals.\nDefine and prioritise product requirements, user stories, and acceptance criteria for data and analytics products and ensure what was agreed gets delivered.\nWork with data engineers and data scientists to develop data pipelines, analytical models, and visualisations that meet business requirements.\nCollaborate with Infrastructure Teams and software developers to ensure that data and analytics products are integrated into existing systems and platforms in a sustainable way that still meets the needs of business to generate the insights necessary to drive their decisions.\nMonitor data and analytics product performance and identify opportunities for improvement.\nStay up-to-date with industry trends and emerging technologies related to data and analytics in the pharmaceutical industry.\nAct as a subject matter expert for data and analytics products and provide guidance to business stakeholders on how to effectively use these products.\nAccountable to Develop and maintain documentation, training materials, and user guides for data and analytics products.\nThe ideal candidate\nBachelors or Masters degree in computer science, information systems, or a related field.\n5+ years of experience in roles such as Senior Data & Analytics Specialist, Data Solutions Lead, Data Architect, or Data Consultant, with a focus on solution design and implementation. Alternatively, 3-4 years of experience in data streams (e.g., Data Science, Data Engineering, Data Governance) combined with a couple of years in Strategic Data Consultancy / Data Product Ownership. Experience in the pharmaceutical or healthcare industry is highly desirable.\nHigh Level understanding of data engineering, data science, Data governance and analytics concepts and technologies.\nExperience working with cross-functional teams, including data engineers, data scientists, and software developers.\nExcellent communication and interpersonal skills.\nStrong analytical and problem-solving skills.\nExperience with agile development methodologies.\nKnowledge of regulatory requirements related to data and analytics in the pharmaceutical industry.\nKnowledge of working with vendor and customer master data for different divisions - Pharmaceuticals, Diagnostic, & Diabetes care.\nUnderstanding of the transparency reporting landscape.\nHands-on experience of working on applications such as Jira, SQL, Postman, SAP GUI, Monday.com, Trello\nProficient in the knowledge of different CRM/Master Data Management systems such as SFDC, Reltio MDM\nUnderstanding data protection laws and consent processes applicable to healthcare professionals and organizations before transparency disclosure.\nWho we are\nAt Roche, more than 100,000 people across 100 countries are pushing back the frontiers of healthcare. Working together, we ve become one of the world s leading research-focused healthcare groups. Our success is built on innovation, curiosity and diversity.\nBasel is the headquarters of the Roche Group and one of its most important centres of pharmaceutical research. Over 10,700 employees from over 100 countries come together at our Basel/Kaiseraugst site, which is one of Roche`s largest sites. Read more.\nBesides extensive development and training opportunities, we offer flexible working options, 18 weeks of maternity leave and 10 weeks of gender independent partnership leave. Our employees also benefit from multiple services on site such as child-care facilities, medical services, restaurants and cafeterias, as well as various employee events.\nWe believe in the power of diversity and inclusion, and strive to identify and create opportunities that enable all people to bring their unique selves to Roche.\nRoche is an Equal Opportunity Employer.\nWho we are\n.\n\nLet s build a healthier future, together.\nRoche is an Equal Opportunity Employer.\n""",Industry Type: Biotechnology,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'SAP', 'Diagnostics', 'HP data protector', 'Analytical', 'Healthcare', 'JIRA', 'Analytics', 'SQL', 'CRM']",2025-06-13 05:22:47
Data & Analytics Specialist,Hoffmann La Roche,5 - 10 years,Not Disclosed,['Pune'],"At Roche you can show up as yourself, embraced for the unique qualities you bring. Our culture encourages personal expression, open dialogue, and genuine connections, where you are valued, accepted and respected for who you are, allowing you to thrive both personally and professionally. This is how we aim to prevent, stop and cure diseases and ensure everyone has access to healthcare today and for generations to come. Join Roche, where every voice matters.\nThe Position The Position\nWe are looking for a Data & Analytics Specialist/ Business Analyst who will join us in the newly setup Integrated Informatics for a journey to drive transformation with data and foster automated and efficient decision making throughout the organisation\nThe Data and Analytics Specialist must be the big-picture thinker who understands the value of data to the organisation, has a strong focus on delivering high value, connecting the dots, investing in right initiatives with reusability at the heart of it.\nIn this position you will be acting as squad lead, have end to end ownership of Product delivery with setting up teams from multiple teams/areas with focus on Lifecycle management of the product\nResponsibilities\nYou will work on various aspects of Analytics Solution Development, Data Management, Governance and Information Architecture including but not limited to:\nCollaborate with business stakeholders to understand their data and analytics needs and develop a product roadmap that aligns with business goals.\nDefine and prioritise product requirements, user stories, and acceptance criteria for data and analytics products and ensure what was agreed gets delivered.\nWork with data engineers and data scientists to develop data pipelines, analytical models, and visualisations that meet business requirements.\nCollaborate with Infrastructure Teams and software developers to ensure that data and analytics products are integrated into existing systems and platforms in a sustainable way that still meets the needs of business to generate the insights necessary to drive their decisions.\nMonitor data and analytics product performance and identify opportunities for improvement.\nStay up-to-date with industry trends and emerging technologies related to data and analytics in the pharmaceutical industry.\nAct as a subject matter expert for data and analytics products and provide guidance to business stakeholders on how to effectively use these products.\nAccountable to Develop and maintain documentation, training materials, and user guides for data and analytics products.\nThe ideal candidate\nBachelors or Masters degree in computer science, information systems, or a related field.\n5+ years of experience in roles such as Senior Data & Analytics Specialist, Data Solutions Lead, Data Architect, or Data Consultant, with a focus on solution design and implementation. Alternatively, 3-4 years of experience in data streams (e.g., Data Science, Data Engineering, Data Governance) combined with a couple of years in Strategic Data Consultancy / Data Product Ownership. Experience in the pharmaceutical or healthcare industry is highly desirable.\nHigh Level understanding of data engineering, data science, Data governance and analytics concepts and technologies.\nExperience working with cross-functional teams, including data engineers, data scientists, and software developers.\nExcellent communication and interpersonal skills.\nStrong analytical and problem-solving skills.\nExperience with agile development methodologies.\nKnowledge of regulatory requirements related to data and analytics in the pharmaceutical industry.\nKnowledge of working with vendor and customer master data for different divisions - Pharmaceuticals, Diagnostic, & Diabetes care.\nUnderstanding of the transparency reporting landscape.\nHands-on experience of working on applications such as Jira, SQL, Postman, SAP GUI, Monday.com, Trello\nProficient in the knowledge of different CRM/Master Data Management systems such as SFDC, Reltio MDM\nUnderstanding data protection laws and consent processes applicable to healthcare professionals and organizations before transparency disclosure.\nWho we are\n.\nBasel is the headquarters of the Roche Group and one of its most important centres of pharmaceutical research. Over 10,700 employees from over 100 countries come together at our Basel/Kaiseraugst site, which is one of Roche`s largest sites. Read more.\nBesides extensive development and training opportunities, we offer flexible working options, 18 weeks of maternity leave and 10 weeks of gender independent partnership leave. Our employees also benefit from multiple services on site such as child-care facilities, medical services, restaurants and cafeterias, as well as various employee events.\nWe believe in the power of diversity and inclusion, and strive to identify and create opportunities that enable all people to bring their unique selves to Roche.\nRoche is an Equal Opportunity Employer.\nWho we are\nA healthier future drives us to innovate. Together, more than 100 000 employees across the globe are dedicated to advance science, ensuring everyone has access to healthcare today and for generations to come. Our efforts result in more than 26 million people treated with our medicines and over 30 billion tests conducted using our Diagnostics products. We empower each other to explore new possibilities, foster creativity, and keep our ambitions high, so we can deliver life-changing healthcare solutions that make a global impact.\n\nLet s build a healthier future, together.\nRoche is an Equal Opportunity Employer.\n""",Industry Type: Biotechnology,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'SAP', 'Diagnostics', 'HP data protector', 'Analytical', 'Healthcare', 'JIRA', 'Analytics', 'SQL', 'CRM']",2025-06-13 05:22:49
Data Scientist-Advanced Analytics,IBM,3 - 7 years,Not Disclosed,['Kochi'],"We are seeking a highly skilled Advanced Analytics Specialist to join our dynamic team. The successful candidate will be responsible for leveraging advanced analytics techniques to derive actionable insights, inform business decisions, and drive strategic initiatives. This role requires a deep understanding of data analysis, statistical modeling, machine learning, and data visualization.\nIn this role, you will be responsible for architecting and delivering AI solutions using cutting-edge technologies, with a strong focus on foundation models and large language models. You will work closely with customers, product managers, and development teams to understand business requirements and design custom AI solutions that address complex challenges. Experience with tools like Github Copilot, Amazon Code Whisperer etc. is desirable.\nSuccess is our passion, and your accomplishments will reflect this, driving your career forward, propelling your team to success, and helping our clients to thrive.\nDay-to-Day Duties:\nProof of Concept (POC) DevelopmentDevelop POCs to validate and showcase the feasibility and effectiveness of the proposed AI solutions. Collaborate with development teams to implement and iterate on POCs, ensuring alignment with customer requirements and expectations.\nHelp in showcasing the ability of Gen AI code assistant to refactor/rewrite and document code from one language to another, particularly COBOL to JAVA through rapid prototypes/ PoC\nDocumentation and Knowledge SharingDocument solution architectures, design decisions, implementation details, and lessons learned. Create technical documentation, white papers, and best practice guides. Contribute to internal knowledge sharing initiatives and mentor new team members.\nIndustry Trends and InnovationStay up to date with the latest trends and advancements in AI, foundation models, and large language models. Evaluate emerging technologies, tools, and frameworks to assess their potential impact on solution design and implementation\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nDevelop and implement advanced analytical models and algorithms to solve complex business problems, analyze large datasets to uncover trends, patterns, and insights that drive business performance.\nCollaborate with cross-functional teams to identify key business challenges and opportunities, Create and maintain data pipelines and workflows to ensure the accuracy and integrity of data, Design and deliver insightful reports and dashboards to communicate findings to stakeholders.\nStay up to date with the latest advancements in analytics, machine learning, and data science. Provide technical expertise and mentorship to junior team members.\nQualificationsBachelor’s or master’s degree in data science, Statistics, Mathematics, Computer Science, or a related field. Proven experience in advanced analytics, data science, or a similar role. Proficiency in programming languages such as Python, R, or SQL. Experience with data visualization tools like Tableau, Power BI, or similar.\nStrong understanding of statistical modelling and machine learning algorithms. Excellent analytical, problem-solving, and critical thinking skills. Ability to communicate complex analytical concepts to non-technical stakeholders. Experience with big data technologies (e.g., Hadoop, Spark) is a plus\n\n\nPreferred technical and professional experience\nFamiliarity with cloud-based analytics platforms (e.g., AWS, Azure).\nKnowledge of natural language processing (NLP) and deep learning techniques.\nExperience with project management and agile methodologies",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analysis', 'machine learning', 'statistical modeling', 'data visualization', 'machine learning algorithms', 'advanced analytics', 'python', 'github', 'natural language processing', 'power bi', 'microsoft azure', 'sql', 'r', 'tableau', 'java', 'data science', 'spark', 'hadoop', 'aws']",2025-06-13 05:22:51
Data Modeler,Accenture,15 - 20 years,Not Disclosed,['Mumbai'],"Project Role :Data Modeler\n\n\n\n\n\nProject Role Description :Work with key business representatives, data owners, end users, application designers and data architects to model current and new data.\n\n\n\nMust have skills :Data Modeling Techniques and Methodologies\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\nProject Role :Data Architect & Modeler\n\nProject Role Description Data Model, Design, build and lead the complex ETL data integration pipelines to meet business process and application requirements. Management Level :9Work Experience :6+ yearsWork Location :AnyMust have skills :Data Architecture Principles\nGood to have skills :Data Modeling, Data Architect, Informatica PowerCenter, Informatica Data Quality, SAP BusinessObjects Data Services, SQL, PL/SQL, SAP HANA DB, MS Azure, Python, ErWin, SAP Power Designer Job :Data Architect, Modeler, and data Integration LeadKey Responsibilities:1) Working on building Data models, Forward and Reverse Engineering.2) Working on Data and design analysis and working with data analysts team on data model design.3) Working on presentations on design, end to end flow and data models.4) Work on new and existing data models using Power designer tools and other designing tools like Visio5) Work with functional SMEs, BAs to review requirements, mapping documents\nTechnical Experience:1) Should have good understanding of ETL design concepts like CDC, SCD, Transpose/ pivot, Updates, Validation2) Should have strong understanding of SQL concepts, Data warehouse concepts and can easily understand data technically and functionally.3) Good understanding of various file formats like xml, delimited, fixed width etc.4) Understand the concepts of data quality, data cleansing, data profiling5) Good to have Python and other new data technologies and cloud exposure.6) Having Insurance background is a plus.\nEducational Qualification :15 years of fulltime education with BE/B Tech or equivalent\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Data Modeling Techniques and Methodologies.- Strong understanding of relational and non-relational database design principles.- Experience with data integration and ETL processes.- Familiarity with data governance and data quality frameworks.- Ability to translate business requirements into technical specifications.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sql', 'data architecture principles', 'data modeling', 'data warehousing concepts', 'sql joins', 'python', 'ms azure', 'sap', 'informatica powercenter', 'informatica data quality', 'data warehousing', 'erwin', 'data architecture', 'plsql', 'modeler', 'hana db', 'etl', 'sap hana', 'data integration']",2025-06-13 05:22:53
Data Modeler,Accenture,15 - 20 years,Not Disclosed,['Mumbai'],"Project Role :Data Modeler\n\n\n\n\n\nProject Role Description :Work with key business representatives, data owners, end users, application designers and data architects to model current and new data.\n\n\n\nMust have skills :Data Building Tool\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Modeler, you will engage with key business representatives, data owners, end users, application designers, and data architects to model both current and new data. Your typical day will involve collaborating with various stakeholders to understand their data needs, analyzing existing data structures, and designing effective data models that support business objectives. You will also be responsible for ensuring that the data models are aligned with best practices and organizational standards, facilitating smooth data integration and accessibility across different systems. This role requires a proactive approach to problem-solving and a commitment to delivering high-quality data solutions that enhance decision-making processes within the organization.\nRoles & Responsibilities:- Expected to be an SME.- Collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Facilitate training sessions and workshops to enhance team capabilities.- Continuously evaluate and improve data modeling processes to ensure efficiency.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Data Building Tool.- Strong understanding of data modeling techniques and methodologies.- Experience with data integration and ETL processes.- Familiarity with database management systems and SQL.- Ability to translate business requirements into technical specifications.\nAdditional Information:- The candidate should have minimum 7.5 years of experience in Data Building Tool.- This position is based in Mumbai.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['sql', 'database management', 'data modeling', 'etl', 'data integration', 'python', 'oracle', 'data analysis', 'data warehousing', 'sme', 'data architecture', 'business intelligence', 'sql server', 'plsql', 'unix shell scripting', 'etl tool', 'modeler', 'informatica', 'unix', 'etl process']",2025-06-13 05:22:55
Data Architect,Accenture,15 - 20 years,Not Disclosed,['Bhubaneswar'],"Project Role :Data Architect\n\n\n\n\n\nProject Role Description :Define the data requirements and structure for the application. Model and design the application data structure, storage and integration.\n\n\n\nMust have skills :Data Architecture Principles\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n18 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Architect, you will define the data requirements and structure for the application. Your typical day involves modeling and designing the application data structure, storage, and integration, ensuring that the data architecture aligns with business objectives and supports efficient data management practices. You will collaborate with various stakeholders to gather requirements and translate them into effective data solutions, while also addressing any challenges that arise in the data architecture process.\nRoles & Responsibilities:- Expected to be a Subject Matter Expert with deep knowledge and experience.- Should have influencing and advisory skills.- Engage with multiple teams and responsible for team decisions.- Expected to provide solutions to problems that apply across multiple teams, and provide solutions to business area problems.- Facilitate workshops and discussions to gather data requirements and ensure alignment with business goals.- Develop and maintain documentation related to data architecture, including data models and integration processes.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Data Architecture Principles.- Strong understanding of data modeling techniques and best practices.- Experience with data integration tools and methodologies.- Knowledge of database management systems and data storage solutions.- Familiarity with data governance and data quality frameworks.\nAdditional Information:- The candidate should have minimum 18 years of experience in Data Architecture Principles.- This position is based at our Mumbai office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data architecture', 'database management', 'data architecture principles', 'data modeling', 'data integration', 'python', 'data management', 'data warehousing', 'power bi', 'sql server', 'sql', 'plsql', 'data quality', 'tableau', 'dwbi', 'data governance', 'etl', 'ssis', 'informatica']",2025-06-13 05:22:57
Data Modeler,Accenture,12 - 15 years,Not Disclosed,['Kolkata'],"Project Role :Data Modeler\n\n\n\n\n\nProject Role Description :Work with key business representatives, data owners, end users, application designers and data architects to model current and new data.\n\n\n\nMust have skills :Data Building Tool\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n12 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Modeler, you will engage with key business representatives, data owners, end users, application designers, and data architects to model both current and new data. Your typical day will involve collaborating with various stakeholders to understand their data needs, analyzing existing data structures, and designing effective data models that support business objectives. You will also be responsible for ensuring that the data models are aligned with best practices and organizational standards, facilitating smooth data integration and accessibility across different systems. This role requires a proactive approach to problem-solving and a commitment to delivering high-quality data solutions that enhance decision-making processes within the organization.\nRoles & Responsibilities:- Expected to be an SME.- Collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Expected to provide solutions to problems that apply across multiple teams.- Facilitate workshops and meetings to gather requirements and feedback from stakeholders.- Develop and maintain comprehensive documentation of data models and architecture.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Data Building Tool.- Strong understanding of data modeling techniques and methodologies.- Experience with data integration and ETL processes.- Familiarity with database management systems and SQL.- Ability to translate business requirements into technical specifications.\nAdditional Information:- The candidate should have minimum 12 years of experience in Data Building Tool.- This position is based at our Kolkata office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sql', 'database management', 'data modeling', 'etl', 'data integration', 'python', 'oracle', 'data analysis', 'data warehousing', 'sme', 'data architecture', 'business intelligence', 'sql server', 'plsql', 'unix shell scripting', 'etl tool', 'modeler', 'informatica', 'unix', 'etl process']",2025-06-13 05:22:59
Data Platform Architect,Accenture,15 - 25 years,Not Disclosed,['Bhubaneswar'],"Project Role :Data Platform Architect\n\n\n\n\n\nProject Role Description :Architects the data platform blueprint and implements the design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\n\n\n\nMust have skills :Data Architecture Principles\n\n\n\n\nGood to have skills :Amazon Web Services (AWS), Teradata Vantage, Data GovernanceMinimum\n\n\n\n15 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Platform Architect, you will architect the data platform blueprint and implement the design, collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\nRoles & Responsibilities:- Expected to be a SME with deep knowledge and experience.- Should have Influencing and Advisory skills.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Expected to provide solutions to problems that apply across multiple teams.- Lead the design and implementation of the data platform architecture.- Collaborate with cross-functional teams to ensure data platform alignment with business objectives.- Provide technical guidance and mentorship to junior team members.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Data Architecture Principles.- Good To Have\n\n\n\n\nSkills:\nExperience with Amazon Web Services (AWS), Teradata Vantage, Data Governance.- Strong understanding of data architecture principles and best practices.- Experience in designing and implementing scalable data solutions.- Knowledge of cloud platforms and data governance frameworks.\nAdditional Information:- The candidate should have a minimum of 15 years of experience in Data Architecture Principles.- This position is based at our Pune office.- A 15 years full-time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['vantage', 'data architecture', 'data architecture principles', 'data governance', 'aws', 'mainframes', 'oracle', 'data warehousing', 'microsoft azure', 'dbms', 'sql server', 'sql', 'jcl', 'java', 'data modeling', 'cobol', 'gcp', 'platform architecture', 'hadoop', 'etl', 'big data', 'informatica', 'vsam']",2025-06-13 05:23:00
Data Analyst - L3,Wipro,3 - 5 years,Not Disclosed,['Hyderabad'],"Role Purpose\nThe purpose of this role is to interpret data and turn into information (reports, dashboards, interactive visualizations etc) which can offer ways to improve a business, thus affecting business decisions.\nDo\n1. Managing the technical scope of the project in line with the requirements at all stages\na. Gather information from various sources (data warehouses, database, data integration and modelling) and interpret patterns and trends\n\n\n\n\nMandatory Skills: Business Analyst/ Data Analyst(Media). Experience: 3-5 Years.",,,,"['Data analysis', 'data validation', 'data mining', 'business analysis', 'data warehousing', 'business analytics', 'dbms', 'dashboards', 'sales', 'analytics reporting', 'reporting tools', 'data integration', 'digital transformation']",2025-06-13 05:23:02
Data Scientist,"Sourced Group, an Amdocs Company",4 - 9 years,Not Disclosed,['Gurugram'],"0px> Who are we?\nIn one sentence\nThis is a hands-on position for a motivated and talented innovator. The Data Scientist performs data mining and develops algorithms that provide insight from data.\nWhat will your job look like?\nYou will be responsible for and perform end-top-end data-based research.\nYou will craft data mining solutions to be implemented and executed with alignment to the planned scope and design coverage and needs/uses, demonstrating knowledge and a broad understanding of E2E business processes and requirements.\nYou will define the data analytics research plan, scope and resources required to meet the objectives of his/her area of ownership.\nYou will identify and analyze new data analytic directions and their potential business impact to determine the accurate prioritization of data analytics activities based on business needs and analytics value.\nYou will identify data sources, supervises the data collection process and crafts the data structure in collaboration with data experts (BI or big-data) and subject matter and business experts. Ensures that data used in the data analysis activities are of the highest quality.\nYou will construct data models (algorithms and formulas) for required business needs and predictions.\nYou will present results, including the preparation of patents and white papers and facilitating presentations during conferences.\nAll you need is...\nPh.D. in Computer Science, Mathematics or Statistics\n4 years experience in tasks related to data analytics\nKnowledge of telecommunications and of the subject area being investigated - advantage\nKnowledge in the product (ACC or other) application knowledge and configuration knowledge\nKnowledge in BSS, billing, Telco and the business processes\nFamiliarity in the Telco Networking - mobile, landline, cable TV, Internet\nknowledge in Oracle SQL\nWhy you will love this job:\nYou will ensure timely resolution or critical issue within the agreed SLA. This includes creating a positive customer support experience and build strong relationships through problem understanding, presenting promptly on progress, and handling customers with a professional demeanour.\nYou will be able to demonstrates an understanding of key business drivers and ensures strategic directions are followed and the organization succeeds\nWe are a dynamic, multi-cultural organization that constantly innovates and empowers our employees to grow. Our people our passionate, daring, and phenomenal teammates that stand by each other with a dedication to creating a diverse, inclusive workplace!\nWe offer a wide range of stellar benefits including health, dental, vision, and life insurance as well as paid time off, sick time, and parental leave!\n",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Data analysis', 'Bss', 'Networking', 'Billing', 'Data collection', 'Customer handling', 'Customer support', 'Data mining', 'Amdocs']",2025-06-13 05:23:04
Advanced Data Science Associate,ZS,0 - 2 years,Not Disclosed,['Pune'],"ZSs Insights & Analytics group partners with clients to design and deliver solutions to help them tackle a broad range of business challenges. Our teams work on multiple projects simultaneously, leveraging advanced data analytics and problem-solving techniques. Our recommendations and solutions are based on rigorous research and analysis underpinned by deep expertise and thought leadership.\nWhat you'll Do\nDevelop advanced and efficient statistically effective algorithms that solve problems of high",,,,"['Text mining', 'Analytical', 'Management consulting', 'Financial planning', 'Machine learning', 'Hypothesis Testing', 'Predictive modeling', 'Data mining', 'big data']",2025-06-13 05:23:06
Advanced Data Science Associate,ZS,0 - 2 years,Not Disclosed,"['Noida', 'Gurugram']","Develop advanced and efficient statistically effective algorithms that solve problems of high dimensionality .\nUtilize technical skills such as hypothesis testing, machine learning and retrieval processes to apply statistical and data mining techniques to identify trends, create figures, and analyze other relevant information.\nCollaborate with clients and other stakeholders at ZS to integrate and effectively communicate analysis findings.\nContribute to the assessment of emerging datasets and technologies that impact our analytical",,,,"['Text mining', 'Analytical', 'Management consulting', 'Financial planning', 'Machine learning', 'Hypothesis Testing', 'Predictive modeling', 'Data mining', 'big data']",2025-06-13 05:23:08
Sr SW Engineer - ML,Visa,2 - 7 years,Not Disclosed,['Bengaluru'],"This position is for a Senior Data Engineer with solid development experience who will focus on creating new capabilities for AI as a Service while maturing our code base and development processes. In this position, you are first a passionate and talented developer that can work in a dynamic environment as a member of Agile Scrum teams. Your strong technical leadership, problem-solving abilities, coding, testing and debugging skills is just a start. You must be dedicated to filling product backlog and delivering production-ready code. You must be willing to go beyond the routine and prepared to do a little bit of everything.\nEssential Functions\nCollaborate with project teams, data science teams and development teams to drive the technical roadmap and guide development and implementation of new data driven business solutions.\nDrive technical standard and best practices, and continuously improve AI Platform engineering scalability.\nArchitecture and design of AI Platform services including Machine Learning Engines, In Memory Computing Systems, Streaming Computing Systems, Distributed Data Systems and etc, in Golang, Java, and Python.\nCoordinate the implementation among development teams to ensure system performance, security, scalability and availability.\nCoaching and mentoring junior team members and evolving team talent pipeline.\n\n\nBasic Qualifications\n2+ years of relevant work experience and a Bachelors degree, OR 5+ years of relevant work experience\n\nPreferred Qualifications\n3 or more years of work experience with a Bachelor s Degree or more than 2 years of work experience with an Advanced Degree (e.g. Masters, MBA, JD, MD)",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Basic', 'data science', 'Agile scrum', 'Architecture', 'Coding', 'Debugging', 'Machine learning', 'Technical leadership', 'Business solutions', 'Python']",2025-06-13 05:23:09
IN Senior Associate AWS DataOps Engineer,PwC Service Delivery Center,4 - 8 years,Not Disclosed,['Bengaluru'],"Not Applicable\nSpecialism\nSAP\nManagement Level\nSenior Associate\n& Summary\nAt PwC, our people in data and analytics engineering focus on leveraging advanced technologies and techniques to design and develop robust data solutions for clients. They play a crucial role in transforming raw data into actionable insights, enabling informed decisionmaking and driving business growth.\n\nIn data engineering at PwC, you will focus on designing and building data infrastructure and systems to enable efficient data processing and analysis. You will be responsible for developing and implementing data pipelines, data integration, and data transformation solutions.\nWhy PWC\nAt PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purposeled and valuesdriven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us .\n& Summary We are looking for a seasoned AWS DataOps Engineer\nResponsibilities\nDesign, implement, and manage scalable data pipelines and ETL processes on AWS. Collaborate with data teams to understand requirements and translate them into robust data solutions. Proven experience with AWS data services such as S3, Redshift, RDS, Glue, and Lambda. Strong understanding of data architecture, data modeling, and data warehousing concepts. Strong programming and scripting skills in languages like Python, SQL, or Shell scripting. Experience with data pipeline and ETL tools, such as Apache Airflow or AWS Data Pipeline. Ensure data quality, integrity, and security through automated testing, monitoring, and alerting systems. Optimize data storage and retrieval using AWS services such as S3, Redshift, RDS, and DynamoDB. Implement data governance and compliance standards to ensure data privacy and security. Automate data integration and deployment processes using tools like AWS Data Pipeline, Glue, and Step Functions. Monitor and troubleshoot data workflows to ensure reliability and performance. Provide technical support and guidance to data teams on best practices for data management and operations.\nMandatory skill sets\n(AWS, Azure, GCP) services such as GCP BigQuery, Dataform AWS Redshift, Python\nPreferred skill sets\nDevops\nYears of experience\nrequired\n48 Years\nEducation qualification BE/B.Tech/MBA/MCA/M.Tech\nEducation\nDegrees/Field of Study required Master of Business Administration, Master of Engineering, Bachelor of Engineering\nDegrees/Field of Study preferred\nRequired Skills\nDevOps\nAccepting Feedback, Accepting Feedback, Active Listening, Agile Scalability, Amazon Web Services (AWS), Analytical Thinking, Apache Airflow, Apache Hadoop, Azure Data Factory, Communication, Creativity, Data Anonymization, Data Architecture, Database Administration, Database Management System (DBMS), Database Optimization, Database Security Best Practices, Databricks Unified Data Analytics Platform, Data Engineering, Data Engineering Platforms, Data Infrastructure, Data Integration, Data Lake, Data Modeling, Data Pipeline {+ 27 more}\nTravel Requirements\nGovernment Clearance Required?",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SAP', 'Data management', 'Data modeling', 'Shell scripting', 'Database administration', 'Agile', 'Apache', 'Technical support', 'SQL', 'Python']",2025-06-13 05:23:11
IN Senior Associate AWS DataOps Engineer,PwC Service Delivery Center,4 - 8 years,Not Disclosed,['Bengaluru'],"Not Applicable\nSpecialism\nSAP\nManagement Level\nSenior Associate\n& Summary\nAt PwC, our people in data and analytics engineering focus on leveraging advanced technologies and techniques to design and develop robust data solutions for clients. They play a crucial role in transforming raw data into actionable insights, enabling informed decisionmaking and driving business growth.\n\nIn data engineering at PwC, you will focus on designing and building data infrastructure and systems to enable efficient data processing and analysis. You will be responsible for developing and implementing data pipelines, data integration, and data transformation solutions.\nWhy PWC\nAt PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purposeled and valuesdriven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us .\n& Summary We are looking for a seasoned AWS DataOps Engineer\nResponsibilities\nDesign, implement, and manage scalable data pipelines and ETL processes on AWS. Collaborate with data teams to understand requirements and translate them into robust data solutions. Proven experience with AWS data services such as S3, Redshift, RDS, Glue, and Lambda. Strong understanding of data architecture, data modeling, and data warehousing concepts. Strong programming and scripting skills in languages like Python, SQL, or Shell scripting. Experience with data pipeline and ETL tools, such as Apache Airflow or AWS Data Pipeline. Ensure data quality, integrity, and security through automated testing, monitoring, and alerting systems. Optimize data storage and retrieval using AWS services such as S3, Redshift, RDS, and DynamoDB. Implement data governance and compliance standards to ensure data privacy and security. Automate data integration and deployment processes using tools like AWS Data Pipeline, Glue, and Step Functions. Monitor and troubleshoot data workflows to ensure reliability and performance. Provide technical support and guidance to data teams on best practices for data management and operations.\nMandatory skill sets\n(AWS, Azure, GCP) services such as GCP BigQuery, Dataform AWS Redshift, Python\nPreferred skill sets\nDevops\nYears of experience\nrequired\n48 Years\nEducation qualification BE/B.Tech/MBA/MCA/M.Tech\nEducation\nDegrees/Field of Study required Master of Business Administration, Master of Engineering, Bachelor of Engineering\nDegrees/Field of Study preferred\nRequired Skills\nDevOps\nAccepting Feedback, Accepting Feedback, Active Listening, Agile Scalability, Amazon Web Services (AWS), Analytical Thinking, Apache Airflow, Apache Hadoop, Azure Data Factory, Communication, Creativity, Data Anonymization, Data Architecture, Database Administration, Database Management System (DBMS), Database Optimization, Database Security Best Practices, Databricks Unified Data Analytics Platform, Data Engineering, Data Engineering Platforms, Data Infrastructure, Data Integration, Data Lake, Data Modeling, Data Pipeline {+ 27 more}\nTravel Requirements\nGovernment Clearance Required?",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SAP', 'Data management', 'Data modeling', 'Shell scripting', 'Database administration', 'Agile', 'Apache', 'Technical support', 'SQL', 'Python']",2025-06-13 05:23:13
Senior Engineer II,AMERICAN EXPRESS,8 - 13 years,Not Disclosed,['Bengaluru'],"Join Team Amex and lets lead the way together.\nAmerican Express is looking for Senior Engineers to contribute to the company s focus on building products, like @Work, to support our large and global corporate clients. @Work helps our clients manage their Corporate Card and Corporate Purchasing Card programs more efficiently online. From performing everyday administrative tasks and account maintenance, to accessing reports and utilizing reconciliation solutions, @Work enables fast, efficient and effective program management resulting in time and cost savings for our clients.",,,,"['Computer science', 'Administration', 'Career development', 'Maven', 'Finance', 'Reconciliation', 'MySQL', 'Workflow', 'Monitoring', 'SQL']",2025-06-13 05:23:15
Senior Murex Front Office & Risk Support Engineer,Synechron,5 - 10 years,Not Disclosed,"['Pune', 'Bengaluru', 'Hinjewadi']","Job Summary\nSynechron is seeking an experienced Murex FO & Risk Support Specialist to join our dynamic team. This role is central to maintaining and supporting Murex platform functionalities related to front-office operations and risk management, with a focus on production support.\nThe individual will collaborate closely with business users and IT teams to resolve complex issues, optimize configurations, and ensure the stability of critical trading and risk systems. By providing expert-level support, this role contributes directly to the organizations ability to manage market and credit risks effectively, deliver timely business insights, and uphold operational resilience.\nSoftware Requirements\nOverall Responsibilities\nTechnical Skills (By Category)\nExperience Requirements\nDay-to-Day Activities\nQualifications\nProfessional Competencies",,,,"['Murex support', 'risk management', 'Risk Support', 'credit risk', 'market risk', 'pricing']",2025-06-13 05:23:16
Sr Manager of Software Engineering,JPMorgan Chase Bank,14 - 20 years,Not Disclosed,['Bengaluru'],"When you mentor and advise multiple technical teams and move financial technologies forward, it s a big challenge with big impact. You were made for this.\n\n\nAs a Senior Manager of Software Engineering at JPMorgan Chase within the Consumer and Community Banking Technology Team, you serve in a leadership role by providing technical coaching and advisory for multiple technical teams, as well as anticipate the needs and potential dependencies of other functions within the firm. As an expert in your field, your insights influence budget and technical considerations to advance operational efficiencies and functionalities.\n\nJob responsibilities\n\n\n\nProvide direction, oversight, and coaching for a team of entry-level to mid-level software engineers working on basic to moderately complex tasks.\n\nBe accountable for decisions affecting team resources, budget, tactical operations, and the execution and implementation of processes and procedures.\n\nLead the design, development, testing, and implementation of data visualization projects to support business objectives.\n\nCollaborate with data analysts, data scientists, and business stakeholders to understand data requirements and translate them into effective visual solutions.\n\nWork in an Agile development environment with team members, including Product Managers, UX Designers, QA Engineers, and other Software Engineers.\n\nValidate the technical feasibility of UI/UX designs and provide regular technical guidance to support business and technical teams, contractors, and vendors.\n\nDevelop secure, high-quality production code, review and debug code written by others, and drive decisions influencing product design, application functionality, and technical operations.\n\nServe as a subject matter expert in one or more areas of focus and actively contribute to the engineering community as an advocate of firmwide frameworks, tools, and practices of the Software Development Life Cycle.\n\nInfluence peers and project decision-makers to consider the use and application of leading-edge technologies.\n\nDevelop and maintain dashboards, reports, and interactive visualizations using tools such as Tableau, ensuring data accuracy and integrity by implementing best practices in data visualization and management.\n\nStay current with industry trends and emerging technologies in data visualization and analytics, communicate complex data insights clearly to various audiences, including senior management, and manage a team of data visualization associates, providing guidance and mentorship to junior team members.\n\n\n\nRequired qualifications, capabilities, and skills\n\n\n\nFormal training or certification in software engineering concepts and 5+ years of applied experience.\n\n5+ Years of experience as a Web/UI Lead Architect\n\nProficiency in Javascript, Typescript, HTML, CSS\n\nExpert knowledge in ReactJs, Redux, React hooks.\n\nStrong understanding of front-end coding and development technologies\n\nHands-on practical experience delivering system design, application development, testing, and operational stability\n\nAdvanced knowledge of software applications and technical processes with considerable in-depth knowledge in UI and Web Technologies\n\nAbility to tackle design and functionality problems independently with little to no oversight\n\nPractical cloud native experience\n\nExperience in Computer Science, Computer Engineering, Mathematics, or a related technical field\n\n\n\nPreferred qualifications, capabilities, and skills\n\n\n\nFull stack development with Node/. NET/Java\n\nFamiliarity with working in event driven environments\n\nA good understanding of cross-browser compatibility issues and their solutions along with Typescript\n\nExperience working with Databases and ability to write SQL queries along with experience with messaging platforms\n\nBachelor s degree in data science, Computer Science, Information Systems, Statistics, or a related field.\n\nProblem solver and solution oriented. Strong written and verbal communication skills. Jira and Agile practices\n\nExperience with big data technologies and machine learning is a plus.",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Front end', 'Coding', 'Javascript', 'Agile', 'System design', 'HTML', 'Application development', 'JIRA', 'Analytics']",2025-06-13 05:23:18
Senior Backend Engineer - Bangalore - Hybrid (Product Based),Trigent Software Solutions,5 - 10 years,25-27.5 Lacs P.A.,['Bengaluru'],"Job Description:\nWe are looking for an experienced Backend Engineer to join our engineering team and contribute to building highly scalable, low-latency, and high-concurrency SaaS applications. The ideal candidate should have deep expertise in Java, cloud technologies (preferably AWS), and experience working with both RDBMS and NoSQL databases. You will be involved in the full software development lifecycle, from design to deployment, ensuring performance, security, and maintainability.\nKey Responsibilities:\nDesign, develop, and maintain high-performance, scalable, and secure backend services\nBuild and maintain RESTful APIs to support client-side applications and services\nWork on transactional and concurrent systems that serve large-scale SaaS platforms\nCollaborate with frontend engineers, architects, and DevOps to build robust cloud-based systems\nEnsure code quality and performance by writing unit/integration tests and performing code reviews\nTroubleshoot, debug, and optimize existing systems for reliability and efficiency\nFollow Agile development practices and participate in daily stand-ups and sprint planning\nMandatory Skills:\n58 years of backend development experience in building SaaS or transactional web applications\nStrong hands-on experience with Java and web application frameworks like Spring, Spring Boot\nExperience working on high-concurrency, low-latency, and high-availability systems\nSolid experience with at least one RDBMS (e.g., PostgreSQL, MySQL) and one NoSQL DB (e.g., MongoDB, Cassandra)\nExpertise in cloud platforms – preferably AWS (e.g., EC2, S3, RDS, Lambda)\nFamiliarity with application servers like Tomcat\nStrong knowledge of system design, data structures, and multithreading\nExperience with RESTful APIs and microservice architectures\nNice to Have:\nKnowledge of Big Data technologies (e.g., Kafka, Hadoop, Spark)\nFamiliarity with containerization tools like Docker and Kubernetes\nExperience with CI/CD tools and cloud deployment pipelines\nExposure to other cloud platforms like GCP or Azure",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Temporary/Contractual","['java', 'RDBMS', 'Saas Product Development', 'Nosql Databases', 'AWS', 'spring', 'tomcat']",2025-06-13 05:23:20
Senior Data Scientist,Epsilon,6 - 9 years,Not Disclosed,['Bengaluru'],"Responsibilities: -\nContribute and build an internal product library that is focused on solving business problems related to prediction & recommendation.\nResearch unfamiliar methodologies, techniques to fine tune existing models in the product suite and, recommend better solutions and/or technologies.\nImprove features of the product to include newer machine learning algorithms in the likes of product recommendation, real time predictions, fraud detection, offer personalization etc\nCollaborate with client teams to on-board data, build models and score predictions.\nParticipate in building automations and standalone applications around machine learning algorithms to enable a One Click solution to getting predictions and recommendations.\nAnalyze large datasets, perform data wrangling operations, apply statistical treatments to filter and fine tune input data, engineer new features and eventually aid the process of building machine learning models.\nRun test cases to tune existing models for performance, check criteria and define thresholds for success by scaling the input data to multifold.\nDemonstrate a basic understanding of different machine learning concepts such as Regression, Classification, Matrix Factorization, K-fold Validations and different algorithms such as Decision Trees, Random Forrest, K-means clustering.\nDemonstrate working knowledge and contribute to building models using deep learning techniques, ensuring robust, scalable and high-performance solutions\nMinimum Qualifications:\nEducation: Master's or PhD in a quantitative discipline (Statistics, Economics, Mathematics, Computer Science) is highly preferred.\nDeep Learning Mastery: Extensive experience with deep learning frameworks (TensorFlow, PyTorch, or Keras) and advanced deep learning projects across various domains, with a focus on multimodal data applications.\nGenerative AI Expertise: Proven experience with generative AI models and techniques, such as RAG, VAEs, Transformers, and applications at scale in content creation or data augmentation.\nProgramming and Big Data: Expert-level proficiency in Python and big data/cloud technologies (Databricks and Spark) with a minimum of 4-5 years of experience.\nRecommender Systems and Real-time Predictions: Expertise in developing sophisticated recommender systems, including the application of real-time prediction frameworks.\nMachine Learning Algorithms: In-depth experience with complex algorithms such as logistic regression, random forest, XGBoost, advanced neural networks, and ensemble methods.\nExperienced with machine learning algorithms such as logistic regression, random forest, XG boost, KNN, SVM, neural network, linear regression, lasso regression and k-means.\nDesirable Qualifications:\nGenerative AI Tools Knowledge: Proficiency with tools and platforms for generative AI (such as OpenAI, Hugging Face Transformers).\nDatabricks and Unity Catalog: Experience leveraging Databricks and Unity Catalog for robust data management, model deployment, and tracking.\nWorking experience in CI/CD tools such as GIT & BitBucket",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Data Engineering', 'Pyspark', 'Azure Aws', 'Generative AI', 'Big Data', 'AWS', 'Data Bricks', 'Deep Learning', 'Python', 'SQL']",2025-06-13 05:23:21
Senior Manager Marketing Data Analytics,Factspan Analytics,9 - 14 years,Not Disclosed,['Bengaluru'],"Position: Senior Manager marketing Analytics\nBengaluru, Karnataka\n\nFactspan Overview: Factspan is a pure play data and analytics services organization. We partner with fortune 500 enterprises to build an analytics center of excellence, generating insights and solutions from raw data to solve business challenges, make strategic recommendations and implement new processes that help them succeed. With offices in Seattle, Washington and Bengaluru, India; we use a global delivery model to service our customers. Our customers include industry leaders from Retail, Financial Services, Hospitality, and technology sectors.",,,,"['Market Mix Modelling', 'People Management Skills', 'Marketing Analytics', 'Mmm', 'Stakeholder Management', 'Delivery Management']",2025-06-13 05:23:23
"Sr. Data Analyst – Tableau, SQL, Snowflake",Int9 Solutions,5 - 7 years,Not Disclosed,['Bengaluru'],"We are looking for a skilled Data Analyst with excellent communication skills and deep expertise in SQL, Tableau, and modern data warehousing technologies. This role involves designing data models, building insightful dashboards, ensuring data quality, and extracting meaningful insights from large datasets to support strategic business decisions.\n\nKey Responsibilities:\nWrite advanced SQL queries to retrieve and manipulate data from cloud data warehouses such as Snowflake, Redshift, or BigQuery.\nDesign and develop data models that support analytics and reporting needs.\nBuild dynamic, interactive dashboards and reports using tools like Tableau, Looker, or Domo.\nPerform advanced analytics techniques including cohort analysis, time series analysis, scenario analysis, and predictive analytics.\nValidate data accuracy and perform thorough data QA to ensure high-quality output.\nInvestigate and troubleshoot data issues; perform root cause analysis in collaboration with BI or data engineering teams.\nCommunicate analytical insights clearly and effectively to stakeholders.\n\nRequired Skills & Qualifications:\nExcellent communication skills are mandatory for this role.\n5+ years of experience in data analytics, BI analytics, or BI engineering roles.\nExpert-level skills in SQL, with experience writing complex queries and building views.\nProven experience using data visualization tools like Tableau, Looker, or Domo.\nStrong understanding of data modeling principles and best practices.\nHands-on experience working with cloud data warehouses such as Snowflake, Redshift, BigQuery, SQL Server, or Oracle.\nIntermediate-level proficiency with spreadsheet tools like Excel, Google Sheets, or Power BI, including functions, pivots, and lookups.\nBachelor's or advanced degree in a relevant field such as Data Science, Computer Science, Statistics, Mathematics, or Information Systems.\nAbility to collaborate with cross-functional teams, including BI engineers, to optimize reporting solutions.\nExperience in handling large-scale enterprise data environments.\nFamiliarity with data governance, data cataloging, and metadata management tools (a plus but not required).",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Snowflake', 'Tableau', 'Data Warehousing', 'Data Analytics', 'SQL', 'Scenario Analysis', 'Cohort Analysis', 'Data Modeling', 'Predictive Analysis', 'Redshift']",2025-06-13 05:23:25
"Sr. Data Analyst – Tableau, SQL, Snowflake",Int9 Solutions,5 - 10 years,Not Disclosed,"['Mumbai', 'Delhi / NCR', 'Bengaluru']","We are looking for a skilled Data Analyst with excellent communication skills and deep expertise in SQL, Tableau, and modern data warehousing technologies. This role involves designing data models, building insightful dashboards, ensuring data quality, and extracting meaningful insights from large datasets to support strategic business decisions.\n\nKey Responsibilities:\nWrite advanced SQL queries to retrieve and manipulate data from cloud data warehouses such as Snowflake, Redshift, or BigQuery.\nDesign and develop data models that support analytics and reporting needs.\nBuild dynamic, interactive dashboards and reports using tools like Tableau, Looker, or Domo.\nPerform advanced analytics techniques including cohort analysis, time series analysis, scenario analysis, and predictive analytics.\nValidate data accuracy and perform thorough data QA to ensure high-quality output.\nInvestigate and troubleshoot data issues; perform root cause analysis in collaboration with BI or data engineering teams.\nCommunicate analytical insights clearly and effectively to stakeholders.\n\nRequired Skills & Qualifications:\nExcellent communication skills are mandatory for this role.\n5+ years of experience in data analytics, BI analytics, or BI engineering roles.\nExpert-level skills in SQL, with experience writing complex queries and building views.\nProven experience using data visualization tools like Tableau, Looker, or Domo.\nStrong understanding of data modeling principles and best practices.\nHands-on experience working with cloud data warehouses such as Snowflake, Redshift, BigQuery, SQL Server, or Oracle.\nIntermediate-level proficiency with spreadsheet tools like Excel, Google Sheets, or Power BI, including functions, pivots, and lookups.\nBachelor's or advanced degree in a relevant field such as Data Science, Computer Science, Statistics, Mathematics, or Information Systems.\nAbility to collaborate with cross-functional teams, including BI engineers, to optimize reporting solutions.\nExperience in handling large-scale enterprise data environments.\nFamiliarity with data governance, data cataloging, and metadata management tools (a plus but not required).\nLocation : - Mumbai, Delhi / NCR, Bengaluru , Kolkata, Chennai, Hyderabad, Ahmedabad, Pune, Remote",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Snowflake', 'Tableau', 'SQL', 'BI Tools', 'Scenario Analysis', 'Cohort Analysis', 'Data Warehousing', 'SQL Server', 'Data Modeling', 'Data Analytics', 'Predictive Analysis', 'Redshift']",2025-06-13 05:23:27
"Senior Manager- Middle and Back Office Data Analyst- ISS,",Fidelity International,10 - 15 years,Not Disclosed,"['Gurugram', 'Bengaluru']","Title: Middle and Back Office Data Analyst - ISS Data (Senior Manager)\nDepartment: Technology\nLocation: Bangalore & Gurgaon (hybrid / flexible working permitted)\nReports To: Middle and Back Office Data Product Owner\nLevel: Senior Manager\nWe re proud to have been helping our clients build better financial futures for over 50 years. How have we achieved this? By working together - and supporting each other - all over the world. So, join our [insert name of team/ business area] team and feel like you re part of something bigger.\nAbout your team\nThe Technology function provides IT services that are integral to running an efficient run-the business operating model and providing change-driven solutions to meet outcomes that deliver on our business strategy. These include the development and support of business applications that underpin our revenue, operational, compliance, finance, legal, marketing and customer service functions. The broader organisation incorporates Infrastructure services that the firm relies on to operate on a day-to-day basis including data centre, networks, proximity services, security, voice, incident management and remediation.\nThe ISS Technology group is responsible for providing Technology solutions to the Investment Solutions & Services (ISS) business (which covers Investment Management, Asset Management Operations & Distribution business units globally)\n\nThe ISS Technology team supports and enhances existing applications as well as designs, builds and procures new solutions to meet requirements and enable the evolving business strategy.\nAs part of this group, a dedicated ISS Data Programme team has been mobilised as a key foundational programme to support the execution of the overarching ISS strategy.\nAbout your role\nThe Middle and Back Office Data Analyst role is instrumental in the creation and execution of a future state design for Fund Servicing & Oversight data across Fidelity s key business areas. The successful candidate will have an in- depth knowledge of data domains that represent Middle and Back-office operations and technology.\nThe role will sit within the ISS Delivery Data Analysis chapter and fully aligned to deliver Fidelity s cross functional ISS Data Programme in Technology, and the candidate will leverage their extensive industry knowledge to build a future state platform in collaboration with Business Architecture, Data Architecture, and business stakeholders.\nThe role is to maintain strong relationships with the various business contacts to ensure a superior service to our clients.\nData Product - Requirements Definition and Delivery of Data Outcomes\nAnalysis of data product requirements to enable business outcomes, contributing to the data product roadmap\nCapture both functional and non-functional data requirements considering the data product and consumers perspectives.\nConduct workshops with both the business and tech stakeholders for requirements gathering, elicitation and walk throughs.\nResponsible for the definition of data requirements, epics and stories within the product backlog and providing analysis support throughout the SDLC.\nResponsible for supporting the UAT cycles, attaining business sign off on outcomes being delivered\nData Quality and Integrity:\nDefine data quality use cases for all the required data sets and contribute to the technical frameworks of data quality.\nAlign the functional solution with the best practice data architecture & engineering principles.\nCoordination and Communication:\nExcellent communication skills to influence technology and business stakeholders globally, attaining alignment and sign off on the requirements.\nCoordinate with internal and external stakeholders to communicate data product deliveries and the change impact to the operating model.\nAn advocate for the ISS Data Programme.\nCollaborate closely with Data Governance, Business Architecture, and Data owners etc.\nConduct workshops within the scrum teams and across business teams, effectively document the minutes and drive the actions.\nAbout you\nAt least 10 years of proven experience as a business/technical/data analyst within technology and/or business changes within the financial services /asset management industry.\nMinimum 5 years as a senior business/technical/data analyst adhering to agile methodology, delivering data solutions using industry leading data platforms such as Snowflake, State Street Alpha Data, Refinitiv Eikon, SimCorp Dimension, BlackRock Aladdin, FactSet etc.\nProven experience. of delivering data driven business outcomes using industry leading data platforms such as Snowflake.\nExcellent knowledge of data life cycle that drives Middle and Back Office capabilities such as trade execution, matching, confirmation, trade settlement, record keeping, accounting, fund & cash positions, custody, collaterals/margin movements, corporate actions , derivations and calculations such as holiday handling, portfolio turnover rates, funds of funds look through .\nIn Depth expertise in data and calculations across the investment industry covering the below.\nAsset-specific data: This includes data related to financial instruments reference data like asset specifications, maintenance records, usage history, and depreciation schedules.\nMarket data: This includes data like security prices, exchange rates, index constituents and licensing restrictions on them.\nABOR & IBOR data: This includes calculation engines covering input data sets, calculations and treatment of various instruments for ABOR and IBOR data leveraging platforms such as Simcorp, Neoxam, Invest1, Charles River, Aladdin etc. Knowledge of TPAs, how data can be structured in a unified way from heterogenous structures.\nShould possess Problem Solving, Attention to detail, Critical thinking.\nTechnical Skills: Excellent hands-on SQL, Advanced Excel, Python, ML (optional) and proven experience and knowledge of data solutions.\nKnowledge of data management, data governance, and data engineering practices\nHands on experience on data modelling techniques such as dimensional, data vault etc.\nWillingness to own and drive things, collaboration across business and tech stakeholders.\nFeel rewarded",Industry Type: Financial Services,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['IT services', 'Data analysis', 'Data management', 'Incident management', 'Scrum', 'Customer service', 'Asset management', 'SDLC', 'SQL', 'Python']",2025-06-13 05:23:29
Senior Flexera Data Analyst,Luxoft,6 - 11 years,Not Disclosed,['Gurugram'],"Internal Data Structures & Modeling\nDesign, maintain, and optimize internal data models and structures within the Flexera environment.\nMap business asset data to Flexeras normalized software models with precision and accuracy.\nEnsure accurate data classification, enrichment, and normalization to support software lifecycle tracking.\nPartner with infrastructure, operations, and IT teams to ingest and reconcile data from various internal systems.\nReporting & Analytics\nDesign and maintain reports and dashboards in Flexera or via external BI tools such as Power BI or Tableau.\nProvide analytical insights on software usage, compliance, licensing, optimization, and risk exposure.\nAutomate recurring reporting processes and ensure timely delivery to business stakeholders.\nWork closely with business users to gather requirements and translate them into meaningful reports and visualizations.\nAutomated Data Feeds & API Integrations\nDevelop and support automated data feeds using Flexera REST/SOAP APIs.\nIntegrate Flexera with enterprise tools (e.g., CMDB, SCCM, ServiceNow, ERP) to ensure reliable and consistent data flow.\nMonitor, troubleshoot, and resolve issues related to data extracts and API communication.\nImplement robust logging, alerting, and exception handling for integration pipelines.\nSkills\nMust have\nMinimum 6+ years of working with Flexera or similar software.\nFlexera Expertise: Strong hands-on experience with Flexera One, FlexNet Manager Suite, or similar tools.\nTechnical Skills:\nProficient in REST/SOAP API development and integration.\nStrong SQL skills and familiarity with data transformation/normalization concepts.\nExperience using reporting tools like Power BI, Tableau, or Excel for data visualization.\nFamiliarity with enterprise systems such as SCCM, ServiceNow, ERP, CMDBs, etc.\nProcess & Problem Solving:\nStrong analytical and troubleshooting skills for data inconsistencies and API failures.\nUnderstanding of license models, software contracts, and compliance requirements.\nNice to have\nSoft Skills: Excellent communication skills to translate technical data into business insights.\nOther\nLanguages\nEnglish: C1 Advanced\nSeniority\nSenior\nRefer a Friend\nPositive work environments and stellar reputations attract and retain top talent. Find out why Luxoft stands apart from the rest.\nRecommend a friend\nRelated jobs View all vacancies\nData Engineer with Neo4j\nData Science\nIndia\nChennai\nData Engineer with Neo4j\nData Science\nIndia\nBengaluru\nData Scientist\nData Science\nIndia\nBengaluru\nGurugram, India\nReq. VR-114544\nData Science\nBCM Industry\n23/05/2025\nReq. VR-114544\nApply for Senior Flexera Data Analyst in Gurugram\n*",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['ERP', 'neo4j', 'Analytical', 'Data structures', 'data visualization', 'SCCM', 'Licensing', 'Analytics', 'Reporting tools', 'SQL']",2025-06-13 05:23:30
Senior Flexera Data Analyst,Luxoft,6 - 11 years,Not Disclosed,['Bengaluru'],"Internal Data Structures & Modeling\nDesign, maintain, and optimize internal data models and structures within the Flexera environment.\nMap business asset data to Flexeras normalized software models with precision and accuracy.\nEnsure accurate data classification, enrichment, and normalization to support software lifecycle tracking.\nPartner with infrastructure, operations, and IT teams to ingest and reconcile data from various internal systems.\nReporting & Analytics\nDesign and maintain reports and dashboards in Flexera or via external BI tools such as Power BI or Tableau.\nProvide analytical insights on software usage, compliance, licensing, optimization, and risk exposure.\nAutomate recurring reporting processes and ensure timely delivery to business stakeholders.\nWork closely with business users to gather requirements and translate them into meaningful reports and visualizations.\nAutomated Data Feeds & API Integrations\nDevelop and support automated data feeds using Flexera REST/SOAP APIs.\nIntegrate Flexera with enterprise tools (e.g., CMDB, SCCM, ServiceNow, ERP) to ensure reliable and consistent data flow.\nMonitor, troubleshoot, and resolve issues related to data extracts and API communication.\nImplement robust logging, alerting, and exception handling for integration pipelines.\nSkills\nMust have\nMinimum 6+ years of working with Flexera or similar software.\nFlexera Expertise: Strong hands-on experience with Flexera One, FlexNet Manager Suite, or similar tools.\nTechnical Skills:\nProficient in REST/SOAP API development and integration.\nStrong SQL skills and familiarity with data transformation/normalization concepts.\nExperience using reporting tools like Power BI, Tableau, or Excel for data visualization.\nFamiliarity with enterprise systems such as SCCM, ServiceNow, ERP, CMDBs, etc.\nProcess & Problem Solving:\nStrong analytical and troubleshooting skills for data inconsistencies and API failures.\nUnderstanding of license models, software contracts, and compliance requirements.\nNice to have\nSoft Skills: Excellent communication skills to translate technical data into business insights.\nOther\nLanguages\nEnglish: C1 Advanced\nSeniority\nSenior\nRefer a Friend\nPositive work environments and stellar reputations attract and retain top talent. Find out why Luxoft stands apart from the rest.\nRecommend a friend\nRelated jobs View all vacancies\nData Engineer with Neo4j\nData Science\nIndia\nChennai\nData Engineer with Neo4j\nData Science\nIndia\nGurugram\nBusiness Analyst\nData Science\nPoland\nRemote Poland\nBengaluru, India\nReq. VR-114544\nData Science\nBCM Industry\n23/05/2025\nReq. VR-114544\nApply for Senior Flexera Data Analyst in Bengaluru\n*",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['ERP', 'neo4j', 'Analytical', 'Data structures', 'data visualization', 'SCCM', 'Licensing', 'Analytics', 'Reporting tools', 'SQL']",2025-06-13 05:23:32
Senior Flexera Data Analyst,Luxoft,6 - 11 years,Not Disclosed,['Chennai'],"Internal Data Structures & Modeling\nDesign, maintain, and optimize internal data models and structures within the Flexera environment.\nMap business asset data to Flexeras normalized software models with precision and accuracy.\nEnsure accurate data classification, enrichment, and normalization to support software lifecycle tracking.\nPartner with infrastructure, operations, and IT teams to ingest and reconcile data from various internal systems.\nReporting & Analytics\nDesign and maintain reports and dashboards in Flexera or via external BI tools such as Power BI or Tableau.\nProvide analytical insights on software usage, compliance, licensing, optimization, and risk exposure.\nAutomate recurring reporting processes and ensure timely delivery to business stakeholders.\nWork closely with business users to gather requirements and translate them into meaningful reports and visualizations.\nAutomated Data Feeds & API Integrations\nDevelop and support automated data feeds using Flexera REST/SOAP APIs.\nIntegrate Flexera with enterprise tools (e.g., CMDB, SCCM, ServiceNow, ERP) to ensure reliable and consistent data flow.\nMonitor, troubleshoot, and resolve issues related to data extracts and API communication.\nImplement robust logging, alerting, and exception handling for integration pipelines.\nSkills\nMust have\nMinimum 6+ years of working with Flexera or similar software.\nFlexera Expertise: Strong hands-on experience with Flexera One, FlexNet Manager Suite, or similar tools.\nTechnical Skills:\nProficient in REST/SOAP API development and integration.\nStrong SQL skills and familiarity with data transformation/normalization concepts.\nExperience using reporting tools like Power BI, Tableau, or Excel for data visualization.\nFamiliarity with enterprise systems such as SCCM, ServiceNow, ERP, CMDBs, etc.\nProcess & Problem Solving:\nStrong analytical and troubleshooting skills for data inconsistencies and API failures.\nUnderstanding of license models, software contracts, and compliance requirements.\nNice to have\nSoft Skills: Excellent communication skills to translate technical data into business insights.\nOther\nLanguages\nEnglish: C1 Advanced\nSeniority\nSenior\nRefer a Friend\nPositive work environments and stellar reputations attract and retain top talent. Find out why Luxoft stands apart from the rest.\nRecommend a friend\nRelated jobs View all vacancies\nData Engineer with Neo4j\nData Science\nIndia\nGurugram\nData Engineer with Neo4j\nData Science\nIndia\nBengaluru\nData Scientist\nData Science\nIndia\nBengaluru\nChennai, India\nReq. VR-114544\nData Science\nBCM Industry\n23/05/2025\nReq. VR-114544\nApply for Senior Flexera Data Analyst in Chennai\n*",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['ERP', 'neo4j', 'Analytical', 'Data structures', 'data visualization', 'SCCM', 'Licensing', 'Analytics', 'Reporting tools', 'SQL']",2025-06-13 05:23:34
Sr. Data Scientist-Stratup-Mid-Size companies Exp.@ Bangalore_Urgent,"A leader in this space, we deliver world...",8 - 13 years,Not Disclosed,['Bengaluru'],"Senior Data Scientist\n\nLocation: Onsite Bangalore\nExperience: 8+ years\n\nRole Overview\n\nWe are seeking a Senior Data Scientist with a strong foundation in machine learning, deep learning, and statistical modeling, with the ability to translate complex operational problems into scalable AI/ML solutions. In addition to core data science responsibilities, the role involves building production-ready backends in Python and contributing to end-to-end model lifecycle management. Exposure to computer vision is a plus, especially for industrial use cases like identification, intrusion detection, and anomaly detection.\n\nKey Responsibilities\n\nDevelop, validate, and deploy machine learning and deep learning models for forecasting, classification, anomaly detection, and operational optimization\nBuild backend APIs using Python (FastAPI, Flask) to serve ML/DL models in production environments\nApply advanced computer vision models (e.g., YOLO, Faster R-CNN) to object detection, intrusion detection, and visual monitoring tasks\nTranslate business problems into analytical frameworks and data science solutions\nWork with data engineering and DevOps teams to operationalize and monitor models at scale\nCollaborate with product, domain experts, and engineering teams to iterate on solution design\nContribute to technical documentation, model explainability, and reproducibility practices\n\n\nRequired Skills\n\nStrong proficiency in Python for data science and backend development\nExperience with ML/DL libraries such as scikit-learn, TensorFlow, or PyTorch\nSolid knowledge of time-series modeling, forecasting techniques, and anomaly detection\nExperience building and deploying APIs for model serving (FastAPI, Flask)\nFamiliarity with real-time data pipelines using Kafka, Spark, or similar tools\nStrong understanding of model validation, feature engineering, and performance tuning\nAbility to work with SQL and NoSQL databases, and large-scale datasets\nGood communication skills and stakeholder engagement experience\n\n\nGood to Have\n\nExperience with ML model deployment tools (MLflow, Docker, Airflow)\nUnderstanding of MLOps and continuous model delivery practices\nBackground in aviation, logistics, manufacturing, or other industrial domains\nFamiliarity with edge deployment and optimization of vision models\n\n\nQualifications\n\nMasters or PhD in Data Science, Computer Science, Applied Mathematics, or related field\n7+ years of experience in machine learning and data science, including end-to-end deployment of models in production",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['scikit-learn', 'time-series modeling', 'ML/DL libraries', 'data science', 'Python', 'Airflow', 'Kafka', 'MLflow', 'logistics', 'anomaly detection', 'aviation', 'SQL', 'PyTorch', 'NoSQL', 'MLOps', 'forecasting techniques', 'Docker', 'manufacturing', 'FastAPI', 'Spark', 'TensorFlow', 'Flask']",2025-06-13 05:23:36
Senior Data Scientist,Cradlepoint,3 - 8 years,Not Disclosed,['Bengaluru'],"Join our Team\nAbout this Opportunity\nThe complexity of running and optimizing the next generation of wireless networks, such as 5G with distributed edge compute, will require Machine Learning (ML) and Artificial Intelligence (AI) technologies. Ericsson is setting up an AI Accelerator Hub in India to fast-track our strategy execution, using Machine Intelligence (MI) to drive thought leadership, automate, and transform Ericsson s offerings and operations. We collaborate with academia and industry to develop state-of-the-art solutions that simplify and automate processes, creating new value through data insights.\nWhat you will do\nAs a Senior Data Scientist, you will apply your knowledge of data science and ML tools backed with strong programming skills to solve real-world problems.\nResponsibilities:\n1. Lead AI/ML features/capabilities in product/business areas\n2. Define business metrics of success for AI/ML projects and translate them into model metrics\n3. Lead end-to-end development and deployment of Generative AI solutions for enterprise use cases\n4. Design and implement architectures for vector search, embedding models, and RAG systems\n5. Fine-tune and evaluate large language models (LLMs) for domain-specific tasks\n6. Collaborate with stakeholders to translate vague problems into concrete Generative AI use cases\n7. Develop and deploy generative AI solutions using AWS services such as SageMaker, Bedrock, and other AWS AI tools. Provide technical expertise and guidance on implementing GenAI models and best practices within the AWS ecosystem.\n8. Develop secure, scalable, and production-grade AI pipelines\n9. Ensure ethical and responsible AI practices\n10. Mentor junior team members in GenAI frameworks and best practices\n11. Stay current with research and industry trends in Generative AI and apply cutting-edge techniques\n12. Contribute to internal AI governance, tooling frameworks, and reusable components\n13. Work with large datasets including petabytes of 4G/5G networks and IoT data\n14. Propose/select/test predictive models and other ML systems\n15. Define visualization and dashboarding requirements with business stakeholders\n16. Build proof-of-concepts for business opportunities using AI/ML\n17. Lead functional and technical analysis to define AI/ML-driven business opportunities\n18. Work with multiple data sources and apply the right feature engineering to AI models\n19. Lead studies and creative usage of new/existing data sources\nWhat you will bring\n1. Bachelors/Masters/Ph.D. in Computer Science, Data Science, AI, ML, Electrical Engineering, or related disciplines from reputed institutes\n2. 3+ years of applied ML/AI production-level experience\n3. Strong programming skills (R/Python)\n4. Proven ability to lead AI/ML projects end-to-end\n5. Strong grounding in mathematics, probability, and statistics\n6. Hands-on experience with data analysis, visualization techniques, and ML frameworks (Python, R, H2O, Keras, TensorFlow, Spark ML)\n7. Experience with semi-structured/unstructured data for AI/ML models\n8. Strong understanding of building AI models using Deep Neural Networks\n9. Experience with Big Data technologies (Hadoop, Cassandra)\n10. Ability to source and combine data from multiple sources for ML models\nPreferred Qualifications:\n1. Good communication skills in English\n2. Certifying MI MOOCs, a plus\n3. Domain knowledge in Telecommunication/IoT, a plus\n4. Experience with data visualization and dashboard creation, a plus\n5. Knowledge of Cognitive models, a plus\n6. Experience in partnering and collaborative co-creation in a global matrix organization.\nWhy join Ericsson\n\n\nWhat happens once you apply\nPrimary country and city: India (IN) || Bangalore\nReq ID: 766481",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Wireless', 'Computer science', 'Data analysis', 'cassandra', 'Neural networks', 'Artificial Intelligence', 'Machine learning', 'Telecommunication', 'data visualization', 'Python']",2025-06-13 05:23:38
Data Analyst - Senior,FedEx,4 - 7 years,Not Disclosed,"['Gurugram', 'Bengaluru']","Role & responsibilities :\n\nAct as a technical expert on complex and specialist subject(s).\nSupport management with the analysis, interpretation and application of complex information, contributing to the achievement of divisional and corporate goals. Supports or leads projects by applying area of expertise.\nLead and implement advanced analytical processes through data/text mining, model development, and prediction to enable informed business decisions.\nApply sound analytical expertise to examine structured and unstructured data from multiple disparate sources to provide insights and recommend high-quality solutions to leadership across levels.\nPlan initiatives from concept to execution with minimal supervision and communicate results to a broad range of audiences. Develops a superior understanding of pricing and revenue management through internal and external sources to creatively solve business problems and lead the team from concept to execution of projects.\nTypically uses data, statistical and quantitative analysis, modeling, and fact-based management to drive decision-making. Provides regular expert consultative advice to senior leadership.\nEffectively shares best practices and fosters knowledge sharing across teams. Provides crossteam and cross-org consultation and supports communities of practice excellence.\n\n\n\nPreferred candidate profile\n\nRelevant experience in analytics/consulting/informatics and statistics\nKey Skills - Data and Business Analytics, Advanced Statistics and Predictive Modelling,\nStakeholder Management, Project Management\nExperience in pricing and revenue management yield management, customer segmentation analytics, revenue impact analytics, etc. is a plus\nExposure to predictive analytics, ML/ AI techniques is an added advantage\nTools - Oracle, SQL Server, Teradata, SAS, Python, Tableau/PowerBI/Spotfire\nGood to have cloud computing, big data, Azure",Industry Type: Courier / Logistics (Logistics Tech),Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Analysis', 'Business Insights', 'Python', 'SQL', 'Power Bi', 'Business Acumen', 'Tableau']",2025-06-13 05:23:40
Data Scientist,Big Oh Tech,4 - 6 years,Not Disclosed,['Noida'],"Key Responsibilities:\n\nDesign, build, and maintain robust and scalable data pipelines to support analytics and reporting needs.\nManage and optimize data lake architectures, with a focus on Apache Atlas for metadata management, data lineage, and governance.\nIntegrate and curate data from multiple structured and unstructured sources to enable advanced analytics.\nCollaborate with data scientists and business analysts to ensure availability of clean, well-structured data.\nImplement data quality, validation, and monitoring processes across data pipelines.\nDevelop and manage Power BI datasets and data models, supporting dashboard and report creation.\nSupport data cataloging and classification using Apache Atlas for enterprise-wide discoverability and compliance.\nEnsure adherence to data security, privacy, and compliance policies.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['advanced analytics', 'metadata', 'Compliance', 'Business Analyst', 'data security', 'power bi', 'Data quality', 'Management', 'Apache', 'Monitoring']",2025-06-13 05:23:41
Python Engineer - ML/Big Query - Hyd/Chennai/Bangalore,People staffing Solutions,5 - 10 years,12-20 Lacs P.A.,"['Hyderabad', 'Chennai', 'Bengaluru']","Key Responsibilities:\nDesign, develop, and maintain scalable and optimized ETL pipelines using Python and SQL.\nWork with Google BigQuery and other cloud-based platforms to build data warehousing solutions.\nDevelop and deploy ML models; collaborate with Data Scientists for productionizing models.\nWrite efficient and optimized SQL queries for large-scale data processing.\nBuild APIs using Flask/Django for machine learning and data applications.\nWork with both SQL and NoSQL databases including Elasticsearch.\nImplement data ingestion using batch and streaming technologies.\nEnsure data quality, integrity, and governance across the data lifecycle.\nAutomate and optimize CI/CD pipelines for data solutions.\nCollaborate with cross-functional teams to gather data requirements and deliver solutions.\nTroubleshoot and monitor data pipelines for seamless operations.\nRequired Skills & Qualifications:\nBachelor's or Master's degree in Computer Science, Engineering, or related field.\n5+ years of experience with Python in a data engineering and/or ML context.\nStrong hands-on experience with SQL, BigQuery, and cloud data platforms (preferably GCP).\nPractical knowledge of ML concepts and experience developing ML models.\nProficiency in frameworks such as Flask and Django.\nExperience with NoSQL databases and data streaming technologies.\nSolid understanding of data modeling, warehousing, and ETL frameworks.\nFamiliarity with CI/CD tools and automation best practices.\nExcellent communication, problem-solving, and collaboration skills.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Django', 'Machine Learning', 'Python', 'SQL', 'Pandas', 'Numpy', 'Ml', 'Flask']",2025-06-13 05:23:43
"AI/ML TESTING-AI, ML, DEEP LEARNING, DATA MINING",Zensar,2 - 7 years,Not Disclosed,['Pune'],"Zensar Technologies is looking for AI/ML TESTING-AI, ML, DEEP LEARNING, DATA MINING, ANALYTICS AI/ML TESTING-AI, ML, DEEP LEARNING, DATA MINING, ANALYTICS to join our dynamic team and embark on a rewarding career journey\n\nDevelops and executes test plans for AI and machine learning models\n\nValidates model accuracy, fairness, performance, and edge-case behavior\n\nImplements automation tools and creates synthetic test datasets\n\nEnsures compliance with model validation protocols and documentation",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Design engineering', 'deep learning', 'Technology consulting', 'Focus', 'Agile', 'Conceptualization', 'Management', 'Data mining', 'Analytics', 'Testing']",2025-06-13 05:23:45
Business Data Analyst,CGI,5 - 8 years,Not Disclosed,['Hyderabad'],"Business Data Analyst - HealthCare\n\nJob Summary\nWe are seeking an experienced and results-driven Business Data Analyst with 5+ years of hands-on experience in data analytics, visualization, and business insight generation. This role is ideal for someone who thrives at the intersection of business and datatranslating complex data sets into compelling insights, dashboards, and strategies that support decision-making across the organization.\nYou will collaborate closely with stakeholders across departments to identify business needs, design and build analytical solutions, and tell compelling data stories using advanced visualization tools.\nKey Responsibilities\nData Analytics & Insights Analyze large and complex data sets to identify trends, anomalies, and opportunities that help drive business strategy and operational efficiency.\n• Dashboard Development & Data Visualization Design, develop, and maintain interactive dashboards and visual reports using tools like Power BI, Tableau, or Looker to enable data-driven decisions.\n• Business Stakeholder Engagement Collaborate with cross-functional teams to understand business goals, define metrics, and convert ambiguous requirements into concrete analytical deliverables.\n• KPI Definition & Performance Monitoring Define, track, and report key performance indicators (KPIs), ensuring alignment with business objectives and consistent measurement across teams.\n• Data Modeling & Reporting Automation Work with data engineering and BI teams to create scalable, reusable data models and automate recurring reports and analysis processes.\n• Storytelling with Data Communicate findings through clear narratives supported by data visualizations and actionable recommendations to both technical and non-technical audiences.\n• Data Quality & Governance Ensure accuracy, consistency, and integrity of data through validation, testing, and documentation practices.\nRequired Qualifications\nBachelor’s or Master’s degree in Business, Economics, Statistics, Computer Science, Information Systems, or a related field.\n• 5+ years of professional experience in a data analyst or business analyst role with a focus on data visualization and analytics.\n• Proficiency in data visualization tools: Power BI, Tableau, Looker (at least one).\n• Strong experience in SQL and working with relational databases to extract, manipulate, and analyze data.\n• Deep understanding of business processes, KPIs, and analytical methods.\n• Excellent problem-solving skills with attention to detail and accuracy.\n• Strong communication and stakeholder management skills with the ability to explain technical concepts in a clear and business-friendly manner.\n• Experience working in Agile or fast-paced environments.\nPreferred Qualifications\nExperience working with cloud data platforms (e.g., Snowflake, BigQuery, Redshift).\n• Exposure to Python or R for data manipulation and statistical analysis.\n• Knowledge of data warehousing, dimensional modeling, or ELT/ETL processes.\n• Domain experience in Healthcare is a plus.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Bigquery', 'Snowflake', 'Data Warehousing', 'Redshift', 'Python', 'ETL']",2025-06-13 05:23:46
Data Scientist,Mastercard,4 - 8 years,Not Disclosed,['Gurugram'],"As consumer preference for digital payments continues to grow, ensuring a seamless and secure consumer experience is top of mind. Optimization Soltions team focuses on tracking of digital performance across all products and regions, understanding the factors influencing performance and the broader industry landscape. This includes delivering data-driven insights and business recommendations, engaging directly with key external stakeholders on implementing optimization solutions (new and existing), and partnering across the organization to drive alignment and ensure action is taken.\n\nThe Role:\n\nWork closely with global optimization solutions team to architect, develop, and maintain advanced reporting and data visualization capabilities on large volumes of data to support data insights and analytical needs across products, markets, and services\nThe candidate for this position will focus on Building solutions using Machine Learning and creating actionable insights to support product optimization and sales enablement.\nPrototype new algorithms, experiment, evaluate and deliver actionable insights.\nDrive the evolution of products with an impact focused on data science and engineering.\nDesigning machine learning systems and self-running artificial intelligence (AI) software to automate predictive models.\nPerform data ingestion, aggregation, and processing on high volume and high dimensionality data to drive and enable data unification and produce relevant insights.\nContinuously innovate and determine new approaches, tools, techniques & technologies to solve business problems and generate business insights & recommendations.\nApply knowledge of metrics, measurements, and benchmarking to complex and demanding solutions.\n\nAll about You\nA superior academic record at a leading university in Computer Science, Data Science, Technology, mathematics, statistics, or a related field or equivalent work experience\nExperience in data management, data mining, data analytics, data reporting, data product development and quantitative analysis\nStrong analytical skills with track record of translating data into compelling insights\nPrior experience working in a product development role.\nknowledge of ML frameworks, libraries, data structures, data modeling, and software architecture.\nproficiency in using Python/Spark, Hadoop platforms & tools (Hive, Impala, Airflow, NiFi), and SQL to build Big Data products & platforms\nExperience with Enterprise Business Intelligence Platform/Data platform ie Tableau, PowerBI is a plus.\nDemonstrated success interacting with stakeholders to understand technical needs and ensuring analyses and solutions meet their needs effectively.\nAbility to build a strong narrative on the business value of products and actively participate in sales enablement efforts.\nAble to work in a fast-paced, deadline-driven environment as part of a team and as an individual contributor.",Industry Type: Financial Services,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Data management', 'Data modeling', 'Information security', 'Machine learning', 'Data structures', 'Data mining', 'Business intelligence', 'SQL', 'Python']",2025-06-13 05:23:48
Data Scientist,Dynamic Yield,5 - 10 years,Not Disclosed,['Gurugram'],"Our Purpose\nMastercard powers economies and empowers people in 200+ countries and territories worldwide. Together with our customers, we re helping build a sustainable economy where everyone can prosper. We support a wide range of digital payments choices, making transactions secure, simple, smart and accessible. Our technology and innovation, partnerships and networks combine to deliver a unique set of products and services that help people, businesses and governments realize their greatest potential.\nTitle and Summary\nData Scientist\nWho is Mastercard?\nMastercard is a global technology company in the payments industry. Our mission is to connect and power an inclusive, digital economy that benefits everyone, everywhere by making transactions safe, simple, smart, and accessible. Using secure data and networks, partnerships, and passion, our innovations and solutions help individuals, financial institutions, governments, and businesses realize their greatest potential.\nOur decency quotient, or DQ, drives our culture and everything we do inside and outside of our company. With connections across more than 210 countries and territories, we are building a sustainable world that unlocks priceless possibilities for all.\n\nOur Team:\nAs consumer preference for digital payments continues to grow, ensuring a seamless and secure consumer experience is top of mind. Optimization Soltions team focuses on tracking of digital performance across all products and regions, understanding the factors influencing performance and the broader industry landscape. This includes delivering data-driven insights and business recommendations, engaging directly with key external stakeholders on implementing optimization solutions (new and existing), and partnering across the organization to drive alignment and ensure action is taken.\nAre you excited about Data Assets and the value they bring to an organization?\nAre you an evangelist for data-driven decision-making?\nAre you motivated to be part of a team that builds large-scale Analytical Capabilities supporting end users across 6 continents?\nDo you want to be the go-to resource for data science & analytics in the company?\n\n\nThe Role:\n\nWork closely with global optimization solutions team to architect, develop, and maintain advanced reporting and data visualization capabilities on large volumes of data to support data insights and analytical needs across products, markets, and services\nThe candidate for this position will focus on Building solutions using Machine Learning and creating actionable insights to support product optimization and sales enablement.\nPrototype new algorithms, experiment, evaluate and deliver actionable insights.\nDrive the evolution of products with an impact focused on data science and engineering.\nDesigning machine learning systems and self-running artificial intelligence (AI) software to automate predictive models.\nPerform data ingestion, aggregation, and processing on high volume and high dimensionality data to drive and enable data unification and produce relevant insights.\nContinuously innovate and determine new approaches, tools, techniques & technologies to solve business problems and generate business insights & recommendations.\nApply knowledge of metrics, measurements, and benchmarking to complex and demanding solutions.\n\nAll about You\nA superior academic record at a leading university in Computer Science, Data Science, Technology, mathematics, statistics, or a related field or equivalent work experience\nExperience in data management, data mining, data analytics, data reporting, data product development and quantitative analysis\nStrong analytical skills with track record of translating data into compelling insights\nPrior experience working in a product development role.\nknowledge of ML frameworks, libraries, data structures, data modeling, and software architecture.\nproficiency in using Python/Spark, Hadoop platforms & tools (Hive, Impala, Airflow, NiFi), and SQL to build Big Data products & platforms\nExperience with Enterprise Business Intelligence Platform/Data platform i.e. Tableau, PowerBI is a plus.\nDemonstrated success interacting with stakeholders to understand technical needs and ensuring analyses and solutions meet their needs effectively.\nAbility to build a strong narrative on the business value of products and actively participate in sales enablement efforts.\nAble to work in a fast-paced, deadline-driven environment as part of a team and as an individual contributor.\nCorporate Security Responsibility\n\nAll activities involving access to Mastercard assets, information, and networks comes with an inherent risk to the organization and, therefore, it is expected that every person working for, or on behalf of, Mastercard is responsible for information security and must:\nAbide by Mastercard s security policies and practices;\nEnsure the confidentiality and integrity of the information being accessed;\nReport any suspected information security violation or breach, and\nComplete all periodic mandatory security trainings in accordance with Mastercard s guidelines.",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Data management', 'Data modeling', 'Information security', 'Machine learning', 'Data structures', 'Data mining', 'Business intelligence', 'SQL', 'Python']",2025-06-13 05:23:50
Be Our Next Data Modeller!!,Zensar,5 - 10 years,Not Disclosed,"['Hyderabad', 'Delhi / NCR']","Proficiency in data modeling tools such as ER/Studio, ERwin or similar.\nDeep understanding of relational database design, normalization/denormalization, and data warehousing principles.\nExperience with SQL and working knowledge of database platforms like Oracle, SQL Server, PostgreSQL, or Snowflake.\nStrong knowledge of metadata management, data lineage, and data governance practices.\nUnderstanding of data integration, ETL processes, and data quality frameworks.\nAbility to interpret and translate complex business requirements into scalable data models.\nExcellent communication and documentation skills to collaborate with cross-functional teams.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Er Studio', 'ERwin', 'SQL', 'Snowflake.', 'Metadata Management', 'PostgreSQL', 'data lineage', 'data governance', 'data warehousing', 'SQL Server', 'Oracle']",2025-06-13 05:23:52
Associate- Referral - Decision Science / Data Science,Axtria,3 - 5 years,Not Disclosed,['Gurugram'],"Position Summary \n\nThis Requisition is for the Employee Referral Campaign.\n\nWe are seeking high-energy, driven, and innovative Data Scientists to join our Data Science Practice to develop new, specialized capabilities for Axtria, and to accelerate the company’s growth by supporting our clients’ commercial & clinical strategies.\n\n Job Responsibilities \n\nBe an Individual Contributor tothe Data Science team and solve real-world problems using cutting-edge capabilities and emerging technologies.\n\nHelp clients translate the business use cases they are trying to crack into data science solutions. Provide genuine assistance to users by advising them on how to leverage Dataiku DSS to implement data science projects, from design to production.\n\nData Source Configuration, Maintenance, Document and maintain work-instructions.\n\nDeep working onmachine learning frameworks such as TensorFlow, Caffe, Keras, SparkML\n\nExpert knowledge in Statistical and Probabilistic methods such as SVM, Decision-Trees, Clustering\n\nExpert knowledge of python data-science and math packages such as NumPy , Pandas, Sklearn\n\nProficiency in object-oriented languages (Java and/or Kotlin),Python and common machine learning frameworks(TensorFlow, NLTK, Stanford NLP, Ling Pipe etc\n\n\n Education \n\nBachelor Equivalent - Engineering\nMaster's Equivalent - Engineering\n\n Work Experience \n\nData Scientist 3-5 years of relevant experience in advanced statistical and mathematical models and predictive modeling using Python. Experience in the data science space prior relevant experience in Artificial intelligence and machine Learning algorithms for developing scalable models supervised and unsupervised techniques likeNLP and deep Learning Algorithms. Ability to build scalable models using Python, R-Studio, R Shiny, PySpark, Keras, and TensorFlow. Experience in delivering data science projects leveraging cloud infrastructure. Familiarity with cloud technology such as AWS / Azure and knowledge of AWS tools such as S3, EMR, EC2, Redshift, and Glue; viz tools like Tableau and Power BI. Relevant experience in Feature Engineering, Feature Selection, and Model Validation on Big Data. Knowledge of self-service analytics platforms such as Dataiku/ KNIME/ Alteryx will be an added advantage.\n\nML Ops Engineering 3-5 years of experience with MLOps Frameworks like Kubeflow, MLFlow, Data Robot, Airflow, etc., experience with Docker and Kubernetes, OpenShift. Prior experience in end-to-end automated ecosystems including, but not limited to, building data pipelines, developing & deploying scalable models, orchestration, scheduling, automation, and ML operations. Ability to design and implement cloud solutions and ability to build MLOps pipelines on cloud solutions (AWS, MS Azure, or GCP). Programming languages like Python, Go, Ruby, or Bash, a good understanding of Linux, knowledge of frameworks such as Keras, PyTorch, TensorFlow, etc. Ability to understand tools used by data scientists and experience with software development and test automation. Good understanding of advanced AI/ML algorithms & their applications.\n\nGen AI :Minimum of 4-6 years develop, test, and deploy Python based applications on Azure/AWS platforms.Must have basic knowledge on concepts of Generative AI / LLMs / GPT.Deep understanding of architecture and work experience on Web Technologies.Python, SQL hands-on experience.Expertise in any popular python web frameworks e.g. flask, Django etc. Familiarity with frontend technologies like HTML, JavaScript, REACT.Be an Individual Contributor in the Analytics and Development team and solve real-world problems using cutting-edge capabilities and emerging technologies based on LLM/GenAI/GPT.Can interact with client on GenAI related capabilities and use cases.",Industry Type: Analytics / KPO / Research,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'gpm', 'machine learning', 'python data', 'statistics', 'kubernetes', 'microsoft azure', 'numpy', 'javascript', 'sql', 'docker', 'pandas', 'tensorflow', 'java', 'django', 'predictive modeling', 'python web framework', 'mathematical modeling', 'pytorch', 'keras', 'aws', 'flask', 'advanced statistical']",2025-06-13 05:23:54
Big Data Developer,Binary Infoways,6 - 10 years,12-22 Lacs P.A.,['Hyderabad'],"AWS (EMR, S3, Glue, Airflow, RDS, Dynamodb, similar)\nCICD (Jenkins or another)\nRelational Databases experience (any)\nNo SQL databases experience (any)\nMicroservices or Domain services or API gateways or similar\nContainers (Docker, K8s, similar)",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'AWS', 'Python', 'Airflow', 'Java', 'Big Data', 'EMR', 'SQL', 'Jenkins', 'Glue', 'SCALA', 'Big Data Technologies', 'Spark']",2025-06-13 05:23:55
Data Entry Job in Big Pharma Company at Borivali East in Mumbai,Big and Reputed Pharma Company of India ...,1 - 5 years,1.5-2.5 Lacs P.A.,['Mumbai (All Areas)'],"Only 1 Year+ experienced candidate in any field, who know basic Word, Excel and computer.\n\nThis is office job and you need to work on word, excel and email for the company\n\n2 Saturday and all Sunday are Holiday\n\nFor query call at 8000044060\n\nRequired Candidate profile\nOnly 1 Year+ experienced candidate in any field, who know basic Word, Excel and computer.\n\nThis is office job and you need to work on word, excel and email.\n\n2 Saturday and all Sunday are Holiday",Industry Type: Pharmaceutical & Life Sciences,Department: Administration & Facilities,"Employment Type: Full Time, Permanent","['Office Work', 'Back Office', 'Computer', 'Data Entry', 'operation', 'Backend', 'Typing', 'Excel', 'Word', 'Computer Operating', 'MS Office']",2025-06-13 05:23:57
Global IT Software Engineer Senior Manager & Chapter Lead,Boston Consulting Group,10 - 15 years,Not Disclosed,['Gurugram'],"To realize our digital transformation, we need to transform our products, experiences, processes, technology and how we operate. Delivering our clients unrivalled experience of exceptional service, value and flexibility is part of our DNA.\nWe are looking for people who are passionate about Digital Products Transformation based on modern technology stack and Gen AI with significant expertise in and Agile ways of working. To execute this transformation, we need people who take the lead in defining standards and guardrails of working and developing expertise further within the teams.\nOur transformation requires a series of efforts across HR space. Over the next few years, you will have the opportunity to lead our most important transformation programs. At the start of your journey, you will focus on talent management solutions related to Staffing and Mobility of our teams.",,,,"['IT services', 'Technical analysis', 'SOA', 'Testing tools', 'Consulting', 'Javascript', 'Performance testing', 'Release management', 'SDLC', 'Analytics']",2025-06-13 05:23:59
Senior Data Manager/ Lead,Codeforce 360,6 - 8 years,Not Disclosed,['Hyderabad'],"Job Description:\nWe are looking for a highly experienced and dynamic Senior Data Manager / Lead to oversee a team of Data Engineers and Data Scientists. This role demands a strong background in data platforms such as Snowflake and proficiency in Python, combined with excellent people management and project leadership skills. While hands-on experience in the technologies is beneficial, the primary focus of this role is on team leadership, strategic planning, and project delivery .\n\nJob Title : Senior Data Manager / Lead\nLocation: Hyderabad (Work From Office)\nShift Timing: 10AM-7PM\nKey Responsibilities:\nLead, mentor, and manage a team of Data Engineers and Data Scientists.\nOversee the design and implementation of data pipelines and analytics solutions using Snowflake and Python.\nCollaborate with cross-functional teams (product, business, engineering) to align data solutions with business goals.\nEnsure timely delivery of projects, with high quality and performance.\nConduct performance reviews, training plans, and support career development for the team.\nSet priorities, allocate resources, and manage workloads within the data team.\nDrive adoption of best practices in data management, governance, and documentation.\nEvaluate new tools and technologies relevant to data engineering and data science.\n\nRequired Skills & Qualifications:\n6+ years of experience in data-related roles, with at least 23 years in a leadership or management position.\nStrong understanding of Snowflake architecture, performance tuning, data sharing, security, etc.\nSolid knowledge of Python for data engineering or data science tasks.\nExperience in leading data migration, ETL/ELT, and analytics projects.\nAbility to translate business requirements into technical solutions.\nExcellent leadership, communication, and stakeholder management skills.\nExposure to tools like Databricks, Dataiku, Airflow, or similar platforms is a plus.\nBachelors or Master’s degree in Computer Science, Engineering, Mathematics, or a related field.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Snowflake', 'Data Bricks', 'Python', 'Airflow', 'Data Migration', 'Dataiku', 'Data Warehousing', 'ETL', 'ELT', 'SQL']",2025-06-13 05:24:01
Data Science_ Lead,Rishabh Software,8 - 13 years,Not Disclosed,"['Ahmedabad', 'Bengaluru', 'Vadodara']","Job Description\n\nWith excellent analytical and problem-solving skills, you should understand business problems of the customers, translate them into scope of work and technical specifications for developing into Data Science projects. Efficiently utilize cutting edge technologies in AI, Generative AI areas and implement solutions for business problems. Good exposure technology platforms for Data Science, AI, Gen AI, cloud with implementation experience. Ability to provide end to end technical solutions leveraging latest AI, Gen AI tools, frameworks for the business problems. This Job requires the following:",,,,"['Data Science', 'gen ai', 'Computer Vision', 'Machine Learning', 'Deep Learning', 'Tensorflow', 'NLP', 'Artificial Intelligence', 'Dl', 'Python']",2025-06-13 05:24:03
Lead SDET | Open Source Data Platform (Onsite),Acceldata,5 - 10 years,Not Disclosed,['Bengaluru'],"About the Role:\nWe are looking for an experienced Lead SDET for our ODP, specializing in ensuring the quality and performance of large-scale data systems.\nIn this role, you will work closely with development and operations teams to design and execute comprehensive test strategies for Open Source Data Platform (ODP), including Hadoop, Spark, Hive, Kafka, and other related technologies. You will focus on test automation, performance tuning, and identifying bottlenecks in distributed data systems.\nYour key responsibilities will include writing test plans, creating automated test scripts, and conducting functional, regression, and performance testing. You will be responsible for identifying and resolving defects, ensuring data integrity, and improving testing processes. Strong collaboration skills are essential as you will be interacting with cross-functional teams and driving quality initiatives. Your work will directly contribute to maintaining high-quality standards for big data solutions and enhancing their reliability at scale.\n\nYou are a great fit for this role if you have\nProven expertise in Quality Engineering, with a strong background in test automation, performance testing, and defect management across multiple data platforms.\nA proactive mindset to define and implement comprehensive test strategies that ensure the highest quality standards are met.\nExperience in working with both functional and non-functional testing, with a particular focus on automated test development.\nA collaborative team player with the ability to effectively work cross-functionally with development teams to resolve issues and deliver timely fixes.\nStrong communication skills with the ability to mentor junior engineers and share knowledge to improve testing practices across the team.\nA commitment to continuous improvement, with the ability to analyze testing processes and recommend enhancements to align with industry best practices.\nAbility to quickly learn new technologies\n\nWhat we look for:\n6-10 years of hands-on experience in quality engineering and quality assurance, focusing on test automation, performance testing, and defect management across multiple data platforms\nProficiency in programming languages such as Java, Python, or Scala for writing test scripts and automating test cases with hands-on experience in developing automated tests using other test automation frameworks, ensuring robust and scalable test suites.\nProven ability to define and execute comprehensive test strategies, including writing test plans, test cases, and scripts for both functional and non-functional testing to ensure predictable delivery of high-quality products and solutions \nExperience with version control systems like Git and CI/CD tools such as Jenkins or GitLab CI to manage code changes and automate test execution within the development pipeline.\nExpertise in identifying, tracking, and resolving defects and issues, collaborating closely with developers and product teams to ensure timely fixes.\nStrong communication skills with the ability to work cross-functionally with development teams and mentor junior team members to improve testing practices and tools.\nAbility to analyze testing processes, recommend improvements and ensure the testing environment aligns with industry best practices, contributing to the overall quality of software.",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Coding', 'API Testing', 'Java', 'Non Functional Testing', 'Ui Automation Testing', 'Database Testing', 'Ui Automation', 'Selenium', 'Functional Testing']",2025-06-13 05:24:05
Enterprise Data Architect - Azure / ETL,Leading Client,10 - 12 years,Not Disclosed,['Bengaluru'],"Key Responsibilities :\n\nAs an Enterprise Data Architect, you will :\n\n- Lead Data Architecture : Design, develop, and implement comprehensive enterprise data architectures, primarily leveraging Azure and Snowflake platforms.\n\n- Data Transformation & ETL : Oversee and guide complex data transformation and ETL processes for large and diverse datasets, ensuring data integrity, quality, and performance.\n\n- Customer-Centric Data Design : Specialize in designing and optimizing customer-centric datasets from various sources, including CRM, Call Center, Marketing, Offline, and Point of Sale systems.\n\n- Data Modeling : Drive the creation and maintenance of advanced data models, including Relational, Dimensional, Columnar, and Big Data models, to support analytical and operational needs.\n\n- Query Optimization : Develop, optimize, and troubleshoot complex SQL and NoSQL queries to ensure efficient data retrieval and manipulation.\n\n- Data Warehouse Management : Apply advanced data warehousing concepts to build and manage high-performing, scalable data warehouse solutions.\n\n- Tool Evaluation & Implementation : Evaluate, recommend, and implement industry-leading ETL tools such as Informatica and Unifi, ensuring best practices are followed.\n\n- Business Requirements & Analysis : Lead efforts in business requirements definition and management, structured analysis, process design, and use case documentation to translate business needs into technical specifications.\n\n- Reporting & Analytics Support : Collaborate with reporting teams, providing architectural guidance and support for reporting technologies like Tableau and PowerBI.\n\n- Software Development Practices : Apply professional software development principles and best practices to data solution delivery.\n\n- Stakeholder Collaboration : Interface effectively with sales teams and directly engage with customers to understand their data challenges and lead them to successful outcomes.\n\n- Project Management & Multi-tasking : Demonstrate exceptional organizational skills, with the ability to manage and prioritize multiple simultaneous customer projects effectively.\n\n- Strategic Thinking & Leadership : Act as a self-managed, proactive, and customer-focused leader, driving innovation and continuous improvement in data architecture.\n\nPosition Requirements :\n\nof strong experience with data transformation & ETL on large data sets.\n\n- Experience with designing customer-centric datasets (i.e., CRM, Call Center, Marketing, Offline, Point of Sale, etc.).\n\n- 5+ years of Data Modeling experience (i.e., Relational, Dimensional, Columnar, Big Data).\n\n- 5+ years of complex SQL or NoSQL experience.\n\n- Extensive experience in advanced Data Warehouse concepts.\n\n- Proven experience with industry ETL tools (i.e., Informatica, Unifi).\n\n- Solid experience with Business Requirements definition and management, structured analysis, process design, and use case documentation.\n\n- Experience with Reporting Technologies (i.e., Tableau, PowerBI).\n\n- Demonstrated experience in professional software development.\n\n- Exceptional organizational skills and ability to multi-task simultaneous different customer projects.\n\n- Strong verbal & written communication skills to interface with sales teams and lead customers to successful outcomes.\n\n- Must be self-managed, proactive, and customer-focused.\n\nTechnical Skills :\n\n- Cloud Platforms : Microsoft Azure\n\n- Data Warehousing : Snowflake\n\n- ETL Methodologies : Extensive experience in ETL processes and tools\n\n- Data Transformation : Large-scale data transformation\n\n- Data Modeling : Relational, Dimensional, Columnar, Big Data\n\n- Query Languages : Complex SQL, NoSQL\n\n- ETL Tools : Informatica, Unifi (or similar enterprise-grade tools)\n\n- Reporting & BI : Tableau, PowerBI",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure', 'Enterprise Architect', 'Data Architect', 'Big Data', 'Snowflake DB', 'Data Modeling', 'Data Warehousing', 'ETL', 'Reporting Tools', 'SQL']",2025-06-13 05:24:06
Lead Azure Data Factory (ADF),Quatrro,3 - 8 years,Not Disclosed,"['Kolkata', 'Mumbai', 'New Delhi', 'Hyderabad', 'Pune', 'Chennai', 'Bengaluru']","To handle all Data/BI responsibilities, including a major part of the work on ADF and team handling.\n  Key Responsibilities:\nData Warehouse Development:\nDesign and implement scalable and efficient data warehouse solutions.\nDevelop complex SQL Server-based solutions, including T-SQL queries, stored procedures, and performance tuning.\nOptimize SQL Server databases, develop T-SQL scripts, and improve query performance.\nETL Development and Maintenance:\nBuild and optimize ETL workflows using Azure Data Factory (ADF) for data integration from multiple sources.\nEnsure high-performance data pipelines for large-scale data processing.\nIntegrate and automate data processes using Azure Functions to extend ETL capabilities.\nCloud Integration:\nImplement cloud-native solutions leveraging Azure SQL Database, Azure Functions, and Synapse Analytics.\nSupport hybrid data integration scenarios combining on-premises and Azure services.\nData Governance and Quality:\nEstablish and maintain robust data quality frameworks and governance standards.\nEnsure consistency, accuracy, and security of data across all platforms.\nLeadership and Collaboration:\nLead a BI and data professionals team, providing mentorship and technical direction.\nPartner with stakeholders to understand business requirements and deliver data-driven solutions.\nDefine project goals, timelines, and resources for successful execution.\nShould be flexible to support multiple IT platforms\nManaging day-to-day activities Jira request, SQL execution, access request, resolving alerts, and updating Tickets",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Performance tuning', 'Back office', 'Data processing', 'Support services', 'Data quality', 'Stored procedures', 'Outsourcing', 'JIRA', 'Analytics', 'SQL']",2025-06-13 05:24:08
MDM Data Analyst / Steward Lead,Gallagher Service Center (GSC),3 - 7 years,Not Disclosed,['Bengaluru'],"Role & responsibilities\n\nThe MDM Analyst / Data Steward works closely with business stakeholders to understand and gather data requirements, develop data models and database designs, and define and implement data standards, policies, and procedures. This role also implements any rules inside of the MDM tool to improve the data, performs deduplication projects to develop golden records, and overall works towards improving the quality of data in the domain assigned.\n\nRequired skills :\nTechnical Skills: Proficiency in MDM tools and technologies such as Informatica MDM, CluedIn, or similar platforms is essential. Familiarity with data modeling, data integration, and data quality control techniques is also important. Experience with data governance platforms like Collibra and Alation can be beneficial1.\nAnalytical Skills: Strong analytical and problem-solving skills are crucial for interpreting and working with large volumes of data. The ability to translate complex business requirements into practical MDM solutions is also necessary.\nData Management: Experience in designing, implementing, and maintaining master data management systems and solutions. This includes conducting data cleansing, data auditing, and data validation activities.\nCommunication and Collaboration: Excellent communication and interpersonal skills to effectively collaborate with business stakeholders, IT teams, and other departments.\nData Governance: In-depth knowledge of data governance, data quality, and data integration principles. The ability to develop and implement data management processes and policies is essential.\nEducational Background: A Bachelor's or Master's degree in Computer Science, Information Systems, Data Science, or a related field is typically required1.\nCertifications: Certification in the MDM domain (e.g., Certified MDM Professional) can be a plus\n\nKey Skills:\nBecome the expert at the assigned domain of data\nUnderstand all source systems feeding into the MDM\nWrite documentation of stewardship for the domain\nDevelop rules and standards for the domain of data\nGenerate measures of improvement to demonstrate to the business the quality of the data\n\nWe are seeking candidates who can join immediately or within a maximum of 30 days' notice.\nMinimum of 3+ years of relevant experience is required.\nCandidates who are willing to relocate to Bangalore or are already based in Bangalore.\nCandidates should be flexible with working UK/US shifts.",Industry Type: Analytics / KPO / Research,Department: Other,"Employment Type: Full Time, Permanent","['Informatica Mdm', 'Data Modeling', 'Data Integration']",2025-06-13 05:24:09
Senior ETL Engineer/Consultant Specialist,Hsbc,3 - 6 years,Not Disclosed,['Hyderabad'],"Some careers shine brighter than others.\nIf you re looking for a career that will help you stand out, join HSBC and fulfil your potential. Whether you want a career that could take you to the top, or simply take you in an exciting new direction, HSBC offers opportunities, support and rewards that will take you further.\nHSBC is one of the largest banking and financial services organisations in the world, with operations in 64 countries and territories. We aim to be where the growth is, enabling businesses to thrive and economies to prosper, and, ultimately, helping people to fulfil their hopes and realise their ambitions.\nWe are currently seeking an experienced professional to join our team in the role of Consultant Specialist\nIn this role you will be\nDesign and Develop ETL Processes: Lead the design and implementation of ETL processes using all kinds of batch/streaming tools to extract, transform, and load data from various sources into GCP.\nCollaborate with stakeholders to gather requirements and ensure that ETL solutions meet business needs.\nData Pipeline Optimization: Optimize data pipelines for performance, scalability, and reliability, ensuring efficient data processing workflows.\nMonitor and troubleshoot ETL processes, proactively addressing issues and bottlenecks.\nData Integration and Management: I ntegrate data from diverse sources, including databases, APIs, and flat files, ensuring data quality and consistency.\nManage and maintain data storage solutions in GCP (e. g. , BigQuery, Cloud Storage) to support analytics and reporting.\nGCP Dataflow Development: Write Apache Beam based Dataflow Job for data extraction, transformation, and analysis, ensuring optimal performance and accuracy.\nCollaborate with data analysts and data scientists to prepare data for analysis and reporting.\nAutomation and Monitoring: Implement automation for ETL workflows using tools like Apache Airflow or Cloud Composer, enhancing efficiency and reducing manual intervention.\nSet up monitoring and alerting mechanisms to ensure the health of data pipelines and compliance with SLAs.\nData Governance and Security: Apply best practices for data governance, ensuring compliance with industry regulations (e. g. , GDPR, HIPAA) and internal policies.\nCollaborate with security teams to implement data protection measures and address vulnerabilities.\nDocumentation and Knowledge Sharing: Document ETL processes, data models, and architecture to facilitate knowledge sharing and onboarding of new team members.\nConduct training sessions and workshops to share expertise and promote best practices within the team.\n\n\n\n\n\n\n\n\n\n\n\nRequirements\n\n\n\nTo be successful in this role, you should meet the following requirements:\nEducation: Bachelor s degree in Computer Science, Information Systems, or a related field.\nExperience: Minimum of 5 years of industry experience in data engineering or ETL development, with a strong focus on Data Stage and GCP.\nProven experience in designing and managing ETL solutions, including data modeling, data warehousing, and SQL development.\nTechnical Skills: Strong knowledge of GCP services (e. g. , BigQuery, Dataflow, Cloud Storage, Pub/Sub) and their application in data engineering.\nExperience of cloud-based solutions, especially in GCP, cloud certified candidate is preferred.\nExperience and knowledge of Bigdata data processing in batch mode and streaming mode, proficient in Bigdata eco systems, e. g. Hadoop, HBase, Hive, MapReduce, Kafka, Flink, Spark, etc.\nFamiliarity with Java Python for data manipulation on Cloud/Bigdata platform.\nAnalytical Skills: Strong problem-solving skills with a keen attention to detail.\nAbility to analyze complex data sets and derive meaningful insights.\nBenefits: Competitive salary and comprehensive benefits package.\nOpportunity to work in a dynamic and collaborative environment on cutting-edge data projects.\nProfessional development opportunities to enhance your skills and advance your career.\nIf you are a passionate data engineer with expertise in ETL processes and a desire to make a significant impact within our organization, we encourage you to apply for this exciting opportunity!",Industry Type: Consumer Electronics & Appliances,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Automation', 'Data modeling', 'HIPAA', 'Data quality', 'Apache', 'Monitoring', 'Analytics', 'Financial services', 'Python']",2025-06-13 05:24:11
"Senior Staff Engineer, Mobile Flutter",Nagarro,10 - 15 years,Not Disclosed,[],"Total Experience 10+ years.\nStrong working experience in Dart and Flutter.\nSolid understanding of Mobile App Architecture (MVVM, BLoC, Provider, GetX).\nExperience with local databases like Sqflite or alternatives.\nHands-on experience in unit testing and test automation for Flutter apps.\nProven experience in building and deploying apps to the App Store and Google Play Store.\nFamiliarity with Git, GitHub/GitLab, CI/CD tools (eg, Jenkins, Bitrise, GitHub Actions).\nDeep knowledge of mobile app lifecycle, design principles, and clean architecture patterns (MVVM, MVC, etc)\nExpertise in mobile app performance optimization and security best practices.\nExperience in API integration , RESTful services, and handling JSON data.\nProficient in Version Control Systems like Git.\nProblem-solving mindset with the ability to tackle complex data engineering challenges.\nStrong communication and teamwork skills, with the ability to mentor and collaborate effectively.\nRESPONSIBILITIES:\nWriting and reviewing great quality code\nUnderstanding the clients business use cases and technical requirements and be able to convert them into technical design which elegantly meets the requirements\nMapping decisions with requirements and be able to translate the same to developers.\nIdentifying different solutions and being able to narrow down the best option that meets the clients requirements.\nDefining guidelines and benchmarks for NFR considerations during project implementation\nWriting and reviewing design document explaining overall architecture, framework, and high-level design of the application for the developers.\nReviewing architecture and design on various aspects like extensibility, scalability, security, design patterns, user experience, NFRs, etc, and ensure that all relevant best practices are followe'd.\nDeveloping and designing the overall solution for defined functional and non-functional requirements; and defining technologies, patterns, and frameworks to materialize it\nUnderstanding and relating technology integration scenarios and applying these learnings in projects\nResolving issues that are raised during code/review, through exhaustive systematic analysis of the root cause, and being able to justify the decision taken.\nCarrying out POCs to make sure that suggested design/technologies meet the requirements\n\n\nBachelor s or master s degree in computer science, Information Technology, or a related field.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'github', 'Version control', 'GIT', 'MVVM', 'JSON', 'MVC', 'Unit testing', 'High level design', 'Information technology']",2025-06-13 05:24:13
Data Engineer,Visa,10 - 14 years,Not Disclosed,['Bengaluru'],"At Visa, the Corporate Information Technology, Billing & Incentives Platforms team, enables Visas revenue growth through flexible pricing engines and global revenue platforms built on next-generation technologies. This includes managing system requirements, evaluating cutting-edge technologies, design, development, integration, quality assurance, implementation and maintenance of corporate revenue applications. The team works closely with business owners of these services to deliver customer developed solutions, as well as implement industry leading packaged software. This team has embarked on a major transformational journey to build and implement best of breed revenue and billing applications to transform our business as well as technology.\nThe candidate should enjoy working on diverse technologies and should be excited to take initiatives to solve complex business problems and get the job done while taking on new challenges. You should thrive in team-oriented and fast-paced environments where each team-member is vital to the overall success of the projects.\nKey Responsibilities\nDevelop and maintain test automation scripts using PySpark for big data applications.\nCollaborate with data engineers and developers to understand data processing workflows and requirements.\nDesign and implement automated tests for data ingestion, processing, and transformation in a Hadoop ecosystem.\nPerform data validation, data integrity, and performance testing for Spark applications.\nUtilize Spark-specific concepts such as RDDs, Data Frames, Datasets, and Spark SQL in test automation.\nCreate and manage CI/CD pipelines for automated testing in a big data environment.\nIdentify, report, and track defects, and work with the development team to resolve issues.\nOptimize and tune Spark jobs for performance and scalability.\nMaintain and update test cases based on new features and changes in the application.\nDocument test plans, test cases, and test results comprehensively.\nPerform QA and manual testing for payments applications, ensuring compliance with business requirements and standards.\nWork with limited direction, usually within a complex environment, to drive delivery of solutions and meet service levels.\nProductively work with stakeholders in multiple countries and time zones.\nWith active engagement, collaboration, effective communication, quality, integrity, and reliable delivery develop and maintain a trusted and valued relationship with the team, customers and business partners.\n\n\nBasic Qualifications\nBachelors degree, OR 3+ years of relevant work experience\n\nPreferred Qualifications\nbachelor s degree in computer science, Information Technology or related field.\nRelevant certifications in Big Data, Spa",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Data validation', 'Manual testing', 'Manager Quality Assurance', 'Billing', 'Performance testing', 'Data processing', 'Test cases', 'Information technology', 'SQL']",2025-06-14 05:03:16
Senior - Data Engineering Professional,KPMG India,2 - 7 years,Not Disclosed,['Bengaluru'],"2 - 7 years of experience in Python\nGood understanding of Big data ecosystems and frameworks such as Hadoop, Spark etc.\nExperience in developing data processing task using PySpark .\nExpertise in at least one popular cloud provider preferably AWS is a plus.\nGood knowledge of any RDBMS/NoSQL database with strong SQL writing skills\nExperience on Datawarehouse tools like Snowflake is a plus.\nExperience with any one ETL tool is a plus\nStrong analytical and problem-solving capability\nExcellent verbal and written communications skills\nClient facing skills: Solid experience working with clients directly, to be able to build trusted relationships with stakeholders\nAbility to collaborate effectively across global teams\nStrong understanding of data structures, algorithm, object-oriented design and design patterns\nExperience in the use of multi-dimensional data, data curation processes, and the measurement/improvement of data quality.\nGeneral knowledge of business processes, data flows and quantitative models that generate or consume data\nIndependent thinker, willing to engage, challenge and learn new technologies\nQualification\nBachelor s degree or master s in computer science or related field.\nCertification from professional bodies is a plus.\nSELECTION PROCESS\nCandidates should expect 3 - 4 rounds of personal or telephonic interviews to assess fitment and communication skills\n.",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Object oriented design', 'RDBMS', 'Analytical', 'Data processing', 'Data structures', 'Data quality', 'Data warehousing', 'SQL', 'Python']",2025-06-14 05:03:18
Data Engineer,Amazon,1 - 6 years,Not Disclosed,['Hyderabad'],"Business Data Technologies (BDT) makes it easier for teams across Amazon to produce, store, catalog, secure, move, and analyze data at massive scale. Our managed solutions combine standard AWS tooling, open-source products, and custom services to free teams from worrying about the complexities of operating at Amazon scale. This lets BDT customers move beyond the engineering and operational burden associated with managing and scaling platforms, and instead focus on scaling the value they can glean from their data, both for their customers and their teams.\n\nWe own the one of the biggest (largest) data lakes for Amazon where 1000 s of Amazon teams can search, share, and store EB (Exabytes) of data in a secure and seamless way; using our solutions, teams around the world can schedule/process millions of workloads on a daily basis. We provide enterprise solutions that focus on compliance, security, integrity, and cost efficiency of operating and managing EBs of Amazon data.\n\n\nCORE RESPONSIBILITIES:\nBe hands-on with ETL to build data pipelines to support automated reporting\nInterface with other technology teams to extract, transform, and load data from a wide variety of data sources\nImplement data structures using best practices in data modeling, ETL/ELT processes, and SQL, Redshift.\nModel data and metadata for ad-hoc and pre-built reporting\nInterface with business customers, gathering requirements and delivering complete reporting solutions\nBuild robust and scalable data integration (ETL) pipelines using SQL, Python and Spark.\nBuild and deliver high quality data sets to support business analyst, data scientists, and customer reporting needs.\nContinually improve ongoing reporting and analysis processes, automating or simplifying self-service support for customers\nParticipate in strategic & tactical planning discussions\n\nA day in the life\nAs a Data Engineer, you will be working with cross-functional partners from Science, Product, SDEs, Operations and leadership to translate raw data into actionable insights for stakeholders, empowering them to make data-driven decisions. Some of the key activities include:\n\nCrafting the Data Flow: Design and build data pipelines, the backbone of our data ecosystem.\nEnsure the integrity of the data journey by implementing robust data quality checks and monitoring processes.\n\nArchitect for Insights: Translate complex business requirements into efficient data models that optimize data analysis and reporting. Automate data processing tasks to streamline workflows and improve efficiency.\n\nBecome a data detective! ensuring data availability and performance 1+ years of data engineering experience\nExperience with SQL\nExperience with data modeling, warehousing and building ETL pipelines\nExperience with one or more query language (e.g., SQL, PL/SQL, DDL, MDX, HiveQL, SparkSQL, Scala)\nExperience with one or more scripting language (e.g., Python, KornShell) Experience with big data technologies such as: Hadoop, Hive, Spark, EMR\nExperience with any ETL tool like, Informatica, ODI, SSIS, BODI, Datastage, etc.\nKnowledge of cloud services such as AWS or equivalent",,,,"['Data analysis', 'Data modeling', 'Datastage', 'PLSQL', 'Data structures', 'Informatica', 'SSIS', 'Open source', 'Monitoring', 'Python']",2025-06-14 05:03:21
Data Engineer,Capgemini,6 - 9 years,Not Disclosed,['Gurugram'],"\nesign, implement, and maintain data pipelines for data ingestion, processing, and transformation in Azure.\nWork together with data scientists and analysts to understand the needs for data and create effective data workflows.\nCreate and maintain data storage solutions including Azure SQL Database, Azure Data Lake, and Azure Blob Storage.\nUtilizing Azure Data Factory or comparable technologies, create and maintain ETL (Extract, Transform, Load) operations.\nImplementing data validation and cleansing procedures will ensure the quality, integrity, and dependability of the data.\nImprove the scalability, efficiency, and cost-effectiveness of data pipelines.\nMonitoring and resolving data pipeline problems will guarantee consistency and availability of the data.\nWorks in the area of Software Engineering, which encompasses the development, maintenance and optimization of software solutions/applications.1. Applies scientific methods to analyse and solve software engineering problems.2. He/she is responsible for the development and application of software engineering practice and knowledge, in research, design, development and maintenance.3. His/her work requires the exercise of original thought and judgement and the ability to supervise the technical and administrative work of other software engineers.4. The software engineer builds skills and expertise of his/her software engineering discipline to reach standard software engineer skills expectations for the applicable role, as defined in Professional Communities.5. The software engineer collaborates and acts as team player with other software engineers and stakeholders.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['azure data lake', 'azure data factory', 'sql', 'azure blob storage', 'sql azure', 'hive', 'azure databricks', 'python', 'data validation', 'pyspark', 'data warehousing', 'power bi', 'data engineering', 'spark', 'data ingestion', 'software engineering', 'hadoop', 'etl', 'big data', 'aws', 'sql database']",2025-06-14 05:03:23
Data Engineer,Accenture,2 - 7 years,Not Disclosed,['Bengaluru'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Google Cloud Data Services\n\n\n\n\nGood to have skills :GCP Dataflow, Data EngineeringMinimum\n\n\n\n2 year(s) of experience is required\n\n\n\n\nEducational Qualification :standard 15 years\n\n\nSummary:As a Data Engineer, you will be responsible for designing, developing, and maintaining data solutions for data generation, collection, and processing. Your role involves creating data pipelines, ensuring data quality, and implementing ETL processes to migrate and deploy data across systems. You will play a crucial part in the data management process.\nRoles & Responsibilities:- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work-related problems.- Develop and maintain data solutions for data generation, collection, and processing.- Create data pipelines to streamline data flow.- Ensure data quality and integrity throughout the data lifecycle.- Implement ETL processes for data migration and deployment.- Collaborate with cross-functional teams to optimize data processes.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Google Cloud Data Services.- Good To Have\n\n\n\n\nSkills:\nExperience with Data Engineering and GCP Dataflow.- Strong understanding of cloud-based data services.- Experience in designing and implementing data pipelines.- Knowledge of ETL processes and data migration techniques.\nAdditional Information:- The candidate should have a minimum of 2 years of experience in Google Cloud Data Services.- This position is based at our Bengaluru office.- A standard 15 years of education is required.\n\nQualification\n\nstandard 15 years",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['gcp', 'etl', 'data services', 'google', 'data engineering', 'hive', 'data management', 'data warehousing', 'data migration', 'business intelligence', 'sql', 'plsql', 'data modeling', 'spark', 'hadoop', 'big data', 'python', 'sql server', 'data quality', 'tableau', 'aws', 'ssis', 'data flow', 'informatica', 'etl process']",2025-06-14 05:03:26
"Senior Staff Engineer, Big Data Engineer",Nagarro,9 - 13 years,Not Disclosed,['India'],"We're Nagarro.\nWe are a Digital Product Engineering company that is scaling in a big way! We build products, services, and experiences that inspire, excite, and delight. We work at scale across all devices and digital mediums, and our people exist everywhere in the world (18000+ experts across 38 countries, to be exact). Our work culture is dynamic and non-hierarchical. We're looking for great new colleagues. That's where you come in!\n\nREQUIREMENTS:\nTotal experience 9+ years.\nExcellent knowledge and experience in Big data engineer.\nStrong working experience with architecture and development in Apache Spark, Spark, Python, Azure Databricks, Data Pipelines, Azure Devops, Kafka, SQL Server/NoSQL.\nStrong expertise in Python, Django Rest Framework, Databricks and PostgreSQL.\nHands on experience in building data pipelines and building data frameworks for unit testing, data lineage tracking, and automation.\nFamiliarity with streaming technologies (e.g., Kafka, Kinesis, Flink).\nExperience with building and maintaining a cloud system.\nFamiliarity with data modeling, data warehousing, and building distributed systems.\nExpertise in Spanner for high-availability, scalable database solutions.\nKnowledge of data governance and security practices in cloud-based environments.\nProblem-solving mindset with the ability to tackle complex data engineering challenges.\nStrong communication and teamwork skills, with the ability to mentor and collaborate effectively.\n\nRESPONSIBILITIES:\nWriting and reviewing great quality code\nUnderstanding the clients business use cases and technical requirements and be able to convert them in to technical design which elegantly meets the requirements\nMapping decisions with requirements and be able to translate the same to developers\nIdentifying different solutions and being able to narrow down the best option that meets the clients requirements\nDefining guidelines and benchmarks for NFR considerations during project implementation\nWriting and reviewing design document explaining overall architecture, framework, and high-level design of the application for the developers\nReviewing architecture and design on various aspects like extensibility, scalability, security, design patterns, user experience, NFRs, etc., and ensure that all relevant best practices are followed\nDeveloping and designing the overall solution for defined functional and non-functional requirements; and defining technologies, patterns, and frameworks to materialize it\nUnderstanding and relating technology integration scenarios and applying these learnings in projects\nResolving issues that are raised during code/review, through exhaustive systematic analysis of the root cause, and being able to justify the decision taken\nCarrying out POCs to make sure that suggested design/technologies meet the requirements",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Big Data', 'Spark', 'Data Bricks', 'Python', 'Pyspark', 'Django Framework', 'Azure Databricks', 'SQL Server']",2025-06-14 05:03:28
Data Engineer SRE - Shell Data Engineer SRE - Shell,Zensar,3 - 6 years,Not Disclosed,"['Hubli', 'Mangaluru', 'Mysuru', 'Bengaluru', 'Belgaum']","Role - Data Engineer / SRE\nMust Have - ADF, Databricks, SQL, Azure Functions\nGood to Have - Python, Azure DevOps, GitHub\nMust have a willingness to work in support and enhancement projects, on incidents, change management etc.,.\nRelevant 6+ yrs experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field.\nExperience in ITSM, worked on incidents, problem tickets, change management.\nExperience with Azure: ADLS, ADF, Databricks, Stream Analytics, SQL DW, COSMOS DB, Analysis Services, Azure Functions.\nExperience with relational SQL and NoSQL databases, including Postgres and Cassandra.\nExperience with object-oriented/object function scripting languages: Python, SQL, Scala, Spark-SQL etc.\nAdvanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.\nExperience building and optimizing big data data pipelines, architectures, and data sets.\nExperience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.\nStrong analytic skills related to working with unstructured datasets.\nBuild processes supporting data transformation, data structures, metadata, dependency, and workload management.\n\nRole - Data Engineer / SRE\nMust Have - ADF, Databricks, SQL, Azure Functions\nGood to Have - Python, Azure DevOps, GitHub\nMust have a willingness to work in support and enhancement projects, on incidents, change management etc.,.\nRelevant 6+ yrs experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field.\nExperience in ITSM, worked on incidents, problem tickets, change management.\nExperience with Azure: ADLS, ADF, Databricks, Stream Analytics, SQL DW, COSMOS DB, Analysis Services, Azure Functions.\nExperience with relational SQL and NoSQL databases, including Postgres and Cassandra.\nExperience with object-oriented/object function scripting languages: Python, SQL, Scala, Spark-SQL etc.\nAdvanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.\nExperience building and optimizing big data data pipelines, architectures, and data sets.\nExperience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.\nStrong analytic skills related to working with unstructured datasets.\nBuild processes supporting data transformation, data structures, metadata, dependency, and workload management.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Change management', 'metadata', 'NoSQL', 'cassandra', 'Data structures', 'Analytics', 'Analysis services', 'SQL', 'Python']",2025-06-14 05:03:31
Data Engineer,Accenture,15 - 20 years,Not Disclosed,['Bengaluru'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Data Modeling Techniques and Methodologies\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n12 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL processes to migrate and deploy data across systems. Your day will involve working on data architecture and collaborating with cross-functional teams to optimize data processes.\nRoles & Responsibilities:- Expected to be an SME.- Collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Expected to provide solutions to problems that apply across multiple teams.- Lead data modeling initiatives to design and implement data structures.- Optimize data storage and retrieval processes.- Develop and maintain data pipelines for efficient data flow.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Data Modeling Techniques and Methodologies.- Strong understanding of database management systems.- Experience with data warehousing and ETL processes.- Knowledge of data governance and compliance.- Hands-on experience with data visualization tools.\nAdditional Information:- The candidate should have a minimum of 12 years of experience in Data Modeling Techniques and Methodologies.- This position is based at our Bengaluru office.PFB Education details- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['database management system', 'data warehousing', 'data modeling', 'data visualization', 'etl', 'hive', 'python', 'data architecture', 'data engineering', 'sql', 'database management', 'data quality', 'tableau', 'spark', 'data governance', 'data structures', 'hadoop', 'big data', 'data flow', 'etl process']",2025-06-14 05:03:33
Data Engineer,Big 4 Accounting Firms,5 - 10 years,Not Disclosed,"['Gurugram', 'Bengaluru', 'Delhi / NCR']","Bachelors or higher degree in Computer Science or a related discipline; or equivalent (minimum 4 years work\nexperience).\n• At least 2+ years of consulting or client service delivery experience on Azure Data Solution\n• At least 2+ years of experience in developing data ingestion, data processing and analytical pipelines\nfor big data, relational databases such as SQL server and data warehouse solutions such as Azure\nSynapse\n• Extensive experience providing practical direction with using Azure Native services.\n• Extensive hands-on experience implementing data ingestion, ETL and data processing using Azure\nservices: ADLS, Azure Data Factory, Azure Functions, Azure Logic App Synapse/DW, Azure SQL DB,\nDatabricks etc.\n• Experience in Data Analysis, data debugging, problem solving skills and business requirement\nunderstanding.\n• Minimum of 2+ years of hands-on experience in Azure and Big Data technologies such as Java, Python, SQL,\nADLS/Blob, PySpark and SparkSQL, Databricks, HD Insight\n• Well versed in DevSecOps and CI/CD deployments\n• Experience in using Big Data File Formats and compression techniques.\n• Experience working with Developer tools such as Azure DevOps, Visual Studio Team Server, Git\n• Experience with private and public cloud architecture, pros/cons and migration considerations.Role & responsibilities\n\n\nPreferred candidate profile",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Azure Data Factory', 'Pyspark', 'Azure Databricks', 'SQL', 'Python']",2025-06-14 05:03:36
Data Engineer,Grid Dynamics,4 - 9 years,Not Disclosed,['Bengaluru'],"Qualifications we are looking for\nMaster/Bachelor degree in Computer Science, Electrical Engineering, Information Systems or other technical discipline; advanced degree preferred.\nMinimum of 7+ years of software development experience (with a concentration in data centric initiatives), with demonstrated expertise in leveraging standard development best practice methodologies.\nMinimum 4+ years of experience in Hadoop using Core Java Programming, Spark, Scala, Hive and Go lang\nExpertise in Object Oriented Programming Language Java\nExperience using CI/CD Process, version control and bug tracking tools.\nExperience in handling very large data volume in Real Time and batch mode.\nExperience with automation of job execution and validation\nStrong knowledge of Database concepts\nStrong team player.\nStrong communication skills with proven ability to present complex ideas and document in a clear and concise way.\nQuick learner; self-starter, detailed and in-depth.",Industry Type: IT Services & Consulting,Department: Other,"Employment Type: Full Time, Permanent","['Data Engineering', 'SCALA', 'Bigdata Technologies', 'Spark']",2025-06-14 05:03:38
Data Engineer,Accenture,12 - 15 years,Not Disclosed,['Bengaluru'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Data Engineering\n\n\n\n\nGood to have skills :Java Enterprise EditionMinimum\n\n\n\n12 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions that facilitate data generation, collection, and processing. Your typical day will involve creating data pipelines, ensuring data quality, and implementing ETL processes to migrate and deploy data across various systems. You will collaborate with cross-functional teams to understand their data needs and provide effective solutions, ensuring that the data infrastructure is robust and scalable to meet the demands of the organization.\nRoles & Responsibilities:- Expected to be an SME.- Collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Expected to provide solutions to problems that apply across multiple teams.- Mentor junior team members to enhance their skills and knowledge in data engineering.- Continuously evaluate and improve data processes to enhance efficiency and effectiveness.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Data Engineering.- Strong understanding of data modeling and database design principles.- Experience with ETL tools and frameworks.- Familiarity with cloud platforms such as AWS, Azure, or Google Cloud.- Knowledge of data warehousing concepts and technologies.\nAdditional Information:- The candidate should have minimum 12 years of experience in Data Engineering.- This position is based at our Bengaluru office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data engineering', 'database design', 'data modeling', 'design principles', 'aws', 'hive', 'python', 'scala', 'microsoft azure', 'data warehousing', 'java collections', 'sql', 'data quality', 'java', 'gcp', 'etl tool', 'spark', 'data warehousing concepts', 'hadoop', 'etl', 'big data', 'etl process']",2025-06-14 05:03:41
Data Engineer,Accenture,15 - 20 years,Not Disclosed,['Bengaluru'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Data Architecture Principles\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n12 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions for data generation, collection, and processing. You will create data pipelines, ensure data quality, and implement ETL processes to migrate and deploy data across systems. Your day will involve working on various data-related tasks and collaborating with teams to optimize data processes.\nRoles & Responsibilities:- Expected to be an SME.- Collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Expected to provide solutions to problems that apply across multiple teams.- Develop innovative data solutions to meet business requirements.- Optimize data pipelines for efficiency and scalability.- Implement data governance policies to ensure data quality and security.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Data Architecture Principles.- Strong understanding of data modeling and database design.- Experience with ETL tools and processes.- Knowledge of cloud platforms and big data technologies.- Good To Have\n\n\n\n\nSkills:\nData management and governance expertise.\nAdditional Information:- The candidate should have a minimum of 12 years of experience in Data Architecture Principles.- This position is based at our Bengaluru office.Education information - - A 15 years full-time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data management', 'data architecture', 'database design', 'data architecture principles', 'data modeling', 'hive', 'python', 'big data technologies', 'cloud platforms', 'data engineering', 'sql', 'data quality', 'etl tool', 'spark', 'data governance', 'hadoop', 'etl', 'big data', 'etl process']",2025-06-14 05:03:43
Data Engineer,Accenture,3 - 8 years,Not Disclosed,['Hyderabad'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Informatica Data Quality\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions that facilitate data generation, collection, and processing. Your typical day will involve creating data pipelines, ensuring data quality, and implementing ETL processes to effectively migrate and deploy data across various systems. You will collaborate with team members to enhance data workflows and contribute to the overall efficiency of data management practices.\nRoles & Responsibilities:- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Assist in the design and implementation of data architecture to support data initiatives.- Monitor and optimize data pipelines for performance and reliability.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Informatica Data Quality.- Strong understanding of data integration techniques and ETL processes.- Experience with data profiling and data cleansing methodologies.- Familiarity with database management systems and SQL.- Knowledge of data governance and data quality best practices.\nAdditional Information:- The candidate should have minimum 3 years of experience in Informatica Data Quality.- This position is based at our Hyderabad office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['informatica data quality', 'sql', 'etl', 'data integration', 'etl process', 'hive', 'python', 'data management', 'data architecture', 'data engineering', 'data cleansing', 'database management', 'profiling', 'spark', 'data governance', 'hadoop', 'big data', 'informatica', 'data profiling']",2025-06-14 05:03:47
Data Engineer,Xoriant,7 - 12 years,Not Disclosed,"['Pune', 'Bengaluru', 'Mumbai (All Areas)']","We are eagerly seeking candidates with 5 to 13 years experience for a Data Engineer / Lead, to join our dynamic team. The ideal candidate will play a pivotal role within the team to who is a skilled professional with exposure to Python, Spark, Hive, AWS. You will collaborate with internal teams to design, develop, deploy, and maintain software applications at scale\nRole: Data Engineer / Lead\nLocation: PAN India\nExperience: 5 to 13 years\nJob type: Full time\nWork type: Hybrid\nData Engineer with minimum 5 years of relevant professional experience\n• Should have expertise in Python Scripting and Big data technologies like Spark, Hive, Presto etc.\n• Experience with AWS services – IAM, EC2, S3, EMR, Lambda Functions, Step Functions, CloudWatch, Redshift, Athena, GLUE etc.\n• Hands-on experience with Databricks.\n• Proficient writing Spark jobs in Pyspark and Scala.\n• Experience writing queries with both SQL and NoSQL DB ((Hive, HBase, MongoDB, Elasticsearch, PostgreSQL etc.)\n• Should have good understanding around python data structures including data frames, datasets, RDD’s etc.\n• Experience in ML – integration of ML models\n• Experience with Data profiling, data migration.\n• Developing Hive UDF and Hive jobs\n• Proven hands-on Software Development experience\n• Experience with test-driven development\n• Preferred experience in the insurance domain\n• Must have good understanding of Data warehousing concepts.\n• Experience using CI/CD tools like GitHub Actions, Jenkins, Azure Devops etc.\n• Experienced working in Agile projects – Sprint planning, grooming and providing estimations.\n• Experience using JIRA, Confluence, VS Code or similar IDE’s, Jupyter notebooks etc.\n• Good communication and collaborative skills with internal and external teams\n• Flexibility and ability to work in onshore/offshore model involving multiple agile teams\n• Mentor and guide junior developers, review code, familiar with estimation techniques using story points\n• Strong analytical and problem-solving skills\nQualification you must require:\nBachelors or master’s with Computer Science or related field",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Snowflake', 'AWS', 'Data Bricks', 'Python']",2025-06-14 05:03:49
Data Engineer,Accenture,15 - 20 years,Not Disclosed,['Bhubaneswar'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :PySpark, Microsoft Azure Databricks, Microsoft Azure Analytics Services, Microsoft Azure Data Services\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions that facilitate data generation, collection, and processing. Your typical day will involve creating data pipelines, ensuring data quality, and implementing ETL processes to migrate and deploy data across various systems. You will collaborate with cross-functional teams to understand data requirements and deliver effective solutions that meet business needs.\nRoles & Responsibilities:- Expected to be an SME.- Collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Mentor junior team members to enhance their skills and knowledge.- Continuously evaluate and improve data processes to enhance efficiency.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in PySpark, Microsoft Azure Databricks, Microsoft Azure Data Services, Microsoft Azure Analytics Services.- Strong experience in designing and implementing data pipelines.- Proficient in data modeling and database design.- Familiarity with data warehousing concepts and technologies.- Experience with data quality and data governance practices.\nAdditional Information:- The candidate should have minimum 5 years of experience in PySpark.- This position is based at our Bengaluru office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['azure databricks', 'data services', 'pyspark', 'microsoft azure', 'data modeling', 'hive', 'python', 'analytics services', 'azure analytics', 'data warehousing', 'data engineering', 'sql', 'database design', 'data quality', 'spark', 'data governance', 'data warehousing concepts', 'etl', 'etl process']",2025-06-14 05:03:51
Data Engineer,Accenture,15 - 20 years,Not Disclosed,['Bengaluru'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Neo4j, Stardog\n\n\n\n\nGood to have skills :JavaMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions that facilitate data generation, collection, and processing. Your typical day will involve creating data pipelines, ensuring data quality, and implementing ETL processes to migrate and deploy data across various systems. You will collaborate with cross-functional teams to understand their data needs and provide effective solutions, ensuring that the data infrastructure is robust and scalable to meet the demands of the organization.\nRoles & Responsibilities:- Expected to be an SME.- Collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Mentor junior team members to enhance their skills and knowledge in data engineering.- Continuously evaluate and improve data processes to enhance efficiency and effectiveness.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Neo4j.- Good To Have\n\n\n\n\nSkills:\nExperience with Java.- Strong understanding of data modeling and graph database concepts.- Experience with data integration tools and ETL processes.- Familiarity with data quality frameworks and best practices.- Proficient in programming languages such as Python or Scala for data manipulation.\nAdditional Information:- The candidate should have minimum 5 years of experience in Neo4j.- This position is based at our Bengaluru office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['scala', 'java', 'data modeling', 'python', 'neo4j', 'hive', 'pyspark', 'data warehousing', 'sql', 'spark', 'hadoop', 'data visualization', 'etl', 'big data', 'data manipulation', 'airflow', 'machine learning', 'data engineering', 'data quality', 'tableau', 'mapreduce', 'kafka', 'sqoop', 'aws', 'etl process']",2025-06-14 05:03:54
Data Engineer,Accenture,3 - 8 years,Not Disclosed,['Navi Mumbai'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :Business AgilityMinimum\n\n\n\n12 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions that facilitate data generation, collection, and processing. Your typical day will involve creating data pipelines, ensuring data quality, and implementing ETL processes to migrate and deploy data across various systems. You will collaborate with cross-functional teams to understand data requirements and deliver effective solutions that meet business needs. Additionally, you will monitor and optimize data workflows to enhance performance and reliability, ensuring that data is accessible and actionable for stakeholders.\nRoles & Responsibilities:- Need Databricks resource with Azure cloud experience- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Collaborate with data architects and analysts to design scalable data solutions.- Implement best practices for data governance and security throughout the data lifecycle.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Good To Have\n\n\n\n\nSkills:\nExperience with Business Agility.- Strong understanding of data modeling and database design principles.- Experience with data integration tools and ETL processes.- Familiarity with cloud platforms and services related to data storage and processing.\nAdditional Information:- The candidate should have minimum 3 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Pune office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data analytics', 'database design', 'data modeling', 'design principles', 'etl', 'hive', 'python', 'data warehousing', 'power bi', 'machine learning', 'data engineering', 'sql server', 'sql', 'data bricks', 'data quality', 'tableau', 'spark', 'data governance', 'hadoop', 'big data', 'aws', 'ssis', 'etl process']",2025-06-14 05:03:57
Data Engineer,Accenture,15 - 20 years,Not Disclosed,['Pune'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Neo4j, Stardog\n\n\n\n\nGood to have skills :JavaMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions that facilitate data generation, collection, and processing. Your typical day will involve creating data pipelines, ensuring data quality, and implementing ETL processes to migrate and deploy data across various systems. You will collaborate with cross-functional teams to understand their data needs and provide effective solutions, ensuring that the data infrastructure is robust and scalable to meet the demands of the organization.\nRoles & Responsibilities:- Expected to be an SME.- Collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Mentor junior team members to enhance their skills and knowledge in data engineering.- Continuously evaluate and improve data processes to enhance efficiency and effectiveness.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Neo4j.- Good To Have\n\n\n\n\nSkills:\nExperience with Java.- Strong understanding of data modeling and graph database concepts.- Experience with data integration tools and ETL processes.- Familiarity with data quality frameworks and best practices.- Proficient in programming languages such as Python or Scala for data manipulation.\nAdditional Information:- The candidate should have minimum 5 years of experience in Neo4j.- This position is based at our Bengaluru office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['scala', 'java', 'data modeling', 'python', 'neo4j', 'hive', 'pyspark', 'data warehousing', 'sql', 'spark', 'hadoop', 'data visualization', 'etl', 'big data', 'data manipulation', 'airflow', 'machine learning', 'data engineering', 'data quality', 'tableau', 'mapreduce', 'kafka', 'sqoop', 'aws', 'etl process']",2025-06-14 05:04:00
Data Engineer,Accenture,5 - 10 years,Not Disclosed,['Bengaluru'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL processes to migrate and deploy data across systems. Your day will involve working on data solutions and ensuring data integrity and quality.\nRoles & Responsibilities:- Expected to be an SME, collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Develop and maintain data pipelines for efficient data processing.- Implement ETL processes to migrate and deploy data across systems.- Ensure data quality and integrity throughout the data solutions.- Collaborate with cross-functional teams to optimize data processes.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Strong understanding of data engineering principles.- Experience with cloud-based data solutions like AWS or Azure.- Knowledge of SQL and NoSQL databases.- Hands-on experience with data modeling and schema design.\nAdditional Information:- The candidate should have a minimum of 5 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Bengaluru office.- A 15 years full-time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analytics', 'data engineering', 'sql', 'etl', 'aws', 'hive', 'python', 'data processing', 'microsoft azure', 'pyspark', 'data warehousing', 'data integrity', 'knowledge of sql', 'nosql', 'database design', 'data quality', 'data modeling', 'spark', 'hadoop', 'big data', 'etl process', 'nosql databases']",2025-06-14 05:04:02
Data Engineer,Accenture,3 - 8 years,Not Disclosed,['Bengaluru'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Informatica MDM\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions that facilitate data generation, collection, and processing. Your typical day will involve creating data pipelines, ensuring data quality, and implementing ETL processes to effectively migrate and deploy data across various systems, contributing to the overall efficiency and reliability of data management within the organization.\nRoles & Responsibilities:- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Collaborate with cross-functional teams to gather requirements and translate them into technical specifications.- Monitor and optimize data pipelines for performance and reliability.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Informatica MDM.- Good To Have\n\n\n\n\nSkills:\nExperience with data warehousing concepts and practices.- Strong understanding of data modeling techniques and best practices.- Familiarity with SQL and database management systems.- Experience in implementing data governance and data quality frameworks.\nAdditional Information:- The candidate should have minimum 3 years of experience in Informatica MDM.- This position is based at our Bengaluru office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data warehousing', 'sql', 'data modeling', 'etl', 'informatica mdm', 'hive', 'python', 'data management', 'data engineering', 'data quality', 'tableau', 'spark', 'mdm', 'data governance', 'data warehousing concepts', 'technical specifications', 'hadoop', 'big data', 'informatica', 'etl process']",2025-06-14 05:04:06
Data Engineer,Kinara Capital,0 - 1 years,3-5 Lacs P.A.,['Bengaluru'],"1\nAbout Company\nKinara Capital is a FinTech NBFC dedicated to driving Financial Inclusion\nin the MSME sector. Our mission is to transform lives, livelihoods, and\nlocal economies by providing fast and flexible loans without property\ncollateral to small business entrepreneurs. Led by a women-majority\nmanagement team, Kinara Capital values diversity and inclusion and\nfosters a collaborative working environment.\nKinara Capital is the only company from India recognized globally by the\nWorld Bank/IFC with a gold award in 2019 as Bank of the Year-Asia’ for\nour innovative work in SME financing. Kinara Capital is an RBI-registered\nSystemically Important NBFC.\nHeadquartered in Bangalore, we have 110 branches across Karnataka,\nGujarat, Maharashtra, Andhra Pradesh, Telangana, Tamil Nadu, and UT\nPuducherry with more than 1000 employees. https://kinaracapital.com/\nTitle:\nData Engineer\nTeam:\nData Warehouse Team\nPurpose of Job:\nThis is a hands-on coding and designing position and we are looking for\nan exceptionally talented data engineer who has exposure in\nimplementing AWS services to build data pipelines, api integration and\ndesigning data warehouse.\nJob Responsibilities:\nExcellent coding skills in Python, PySpark, SQL.\nHave extensive experience in Spark ecosystem and has\nworked on both real time and batch processing\nHave experience in AWS Glue, EMR, DMS, Lambda, S3,\nDynamoDB, Step functions, Airflow, RDS, Aurora etc.\nExperience with modern Database systems such as\nRedshift, Presto, Hive etc.\nWorked on building data lakes in the past on S3 or\nApache Hudi\nSolid understanding of Data Warehousing Concepts\nGood to have experience on tools such as Kafka or Kinesis\nGood to have AWS Developer Associate or Solutions\nArchitect Associate Certification\nQualifications:\nAt least a bachelor’s degree in Science, Engineering, Applied\nMathematics.\nOther Requirements:\nLearning Attitude and good communication skills\nReport to:\nLead Data Engineer\nPlace of work:\nHead office, Bangalore.\nJob Type:\nFull Time\nNo. of Posts:\n2",Industry Type: FinTech / Payments,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Amazon Redshift', 'Data Warehousing', 'Spark', 'Python', 'SQL', 'Snowflake', 'ETL']",2025-06-14 05:04:09
Data Engineer,Capgemini,6 - 9 years,Not Disclosed,['Hyderabad'],"\nDesign, implement, and maintain data pipelines for data ingestion, processing, and transformation in Azure.\nWork together with data scientists and analysts to understand the needs for data and create effective data workflows.\nCreate and maintain data storage solutions including Azure SQL Database, Azure Data Lake, and Azure Blob Storage.\nUtilizing Azure Data Factory or comparable technologies, create and maintain ETL (Extract, Transform, Load) operations.\nImplementing data validation and cleansing procedures will ensure the quality, integrity, and dependability of the data.\nImprove the scalability, efficiency, and cost-effectiveness of data pipelines.\nMonitoring and resolving data pipeline problems will guarantee consistency and availability of the data.\nWorks in the area of Software Engineering, which encompasses the development, maintenance and optimization of software solutions/applications.1. Applies scientific methods to analyse and solve software engineering problems.2. He/she is responsible for the development and application of software engineering practice and knowledge, in research, design, development and maintenance.3. His/her work requires the exercise of original thought and judgement and the ability to supervise the technical and administrative work of other software engineers.4. The software engineer builds skills and expertise of his/her software engineering discipline to reach standard software engineer skills expectations for the applicable role, as defined in Professional Communities.5. The software engineer collaborates and acts as team player with other software engineers and stakeholders.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['azure data lake', 'azure data factory', 'sql', 'azure blob storage', 'sql azure', 'hive', 'azure databricks', 'python', 'data validation', 'pyspark', 'data warehousing', 'power bi', 'data engineering', 'spark', 'data ingestion', 'software engineering', 'hadoop', 'etl', 'big data', 'aws', 'sql database']",2025-06-14 05:04:13
Data Engineer,Accenture,2 - 3 years,Not Disclosed,['Kochi'],"Job Title - Data Engineer Sr.Analyst ACS SONG\n\n\n\nManagement Level:Level 10 Sr. Analyst\n\n\n\nLocation:Kochi, Coimbatore, Trivandrum\n\n\n\nMust have skills:Python/Scala, Pyspark/Pytorch\n\n\n\n\nGood to have skills:Redshift\n\n\n\n\n\n\n\nJob\n\n\nSummary\n\nYoull capture user requirements and translate them into business and digitally enabled solutions across a range of industries. Your responsibilities will include:\n\n\n\nRoles and Responsibilities\n\nDesigning, developing, optimizing, and maintaining data pipelines that adhere to ETL principles and business goals\n\nSolving complex data problems to deliver insights that helps our business to achieve their goals.\n\nSource data (structured unstructured) from various touchpoints, format and organize them into an analyzable format.\n\nCreating data products for analytics team members to improve productivity\n\nCalling of AI services like vision, translation etc. to generate an outcome that can be used in further steps along the pipeline.\n\nFostering a culture of sharing, re-use, design and operational efficiency of data and analytical solutions\n\nPreparing data to create a unified database and build tracking solutions ensuring data quality\n\nCreate Production grade analytical assets deployed using the guiding principles of CI/CD.\n\n\n\n\nProfessional and Technical Skills\n\nExpert in Python, Scala, Pyspark, Pytorch, Javascript (any 2 at least)\n\nExtensive experience in data analysis (Big data- Apache Spark environments), data libraries (e.g. Pandas, SciPy, Tensorflow, Keras etc.), and SQL. 2-3 years of hands-on experience working on these technologies.\n\nExperience in one of the many BI tools such as Tableau, Power BI, Looker.\n\nGood working knowledge of key concepts in data analytics, such as dimensional modeling, ETL, reporting/dashboarding, data governance, dealing with structured and unstructured data, and corresponding infrastructure needs.\n\nWorked extensively in Microsoft Azure (ADF, Function Apps, ADLS, Azure SQL), AWS (Lambda,Glue,S3), Databricks analytical platforms/tools, Snowflake Cloud Datawarehouse.\n\n\n\n\nAdditional Information\n\nExperience working in cloud Data warehouses like Redshift or Synapse\n\nCertification in any one of the following or equivalent\n\nAWS- AWS certified data Analytics- Speciality\n\nAzure- Microsoft certified Azure Data Scientist Associate\n\nSnowflake- Snowpro core- Data Engineer\n\nDatabricks Data Engineering\n\n\nAbout Our Company | Accenture (do not remove the hyperlink)\n\n\n\nQualification\n\n\n\nExperience:3.5 -5 years of experience is required\n\n\n\n\nEducational Qualification:Graduation (Accurate educational details should capture)",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['scala', 'pyspark', 'pytorch', 'python', 'microsoft azure', 'glue', 'amazon redshift', 'sql', 'tensorflow', 'sql azure', 'spark', 'keras', 'big data', 'etl', 'snowflake', 'scipy', 'data analysis', 'azure data lake', 'power bi', 'data engineering', 'javascript', 'data bricks', 'pandas', 'tableau', 'lambda expressions', 'aws']",2025-06-14 05:04:16
Data Engineer,Accenture,15 - 20 years,Not Disclosed,['Hyderabad'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Microsoft Azure Data Services\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n2 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions that facilitate data generation, collection, and processing. Your typical day will involve creating data pipelines, ensuring data quality, and implementing ETL processes to effectively migrate and deploy data across various systems, contributing to the overall efficiency and reliability of data management within the organization.\nRoles & Responsibilities:- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Collaborate with cross-functional teams to gather requirements and deliver data solutions that meet business needs.- Monitor and optimize data pipelines for performance and reliability.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Microsoft Azure Data Services.- Good To Have\n\n\n\n\nSkills:\nExperience with Azure Data Factory, Azure SQL Database, and Azure Synapse Analytics.- Strong understanding of data modeling and database design principles.- Experience with data integration and ETL tools.- Familiarity with data governance and data quality best practices.\nAdditional Information:- The candidate should have minimum 2 years of experience in Microsoft Azure Data Services.- This position is based at our Hyderabad office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data services', 'microsoft azure', 'database design', 'data modeling', 'design principles', 'python', 'data management', 'azure synapse', 'azure data factory', 'data engineering', 'sql', 'data quality', 'sql azure', 'etl tool', 'data governance', 'etl', 'data integration', 'etl process', 'sql database']",2025-06-14 05:04:19
"Data Engineer, Devices",Amazon,5 - 10 years,Not Disclosed,['Noida'],"Are you a highly skilled data engineer and project leaderDo you think big, enjoy complexity and building solutions that scaleAre you curious to know what you could achieve in a company that pushes the boundaries of modern technologyIf you answered yes and you have a background in FinTech you ll love this role and Amazon s data obsessed culture.\n\nAmazon Devices and Services Fintech is the global team that designs and builds the financial planning and analysis tools for wide variety of Amazon s new and established organizations. From Kindle to Ring and even new and exciting companies like Kuiper (our new interstellar satellite play) this team enjoys a wide variety of complex and interesting problem spaces. They are almost like FinTech consultants embedded in Amazon.\n\nThis team are looking for a Data Engineer to build and enhance the businesses finance systems with TM1 at its core. You will manage all aspects from requirements gathering, technical design, development, deployment, and integration to solve budgeting, planning, performance management and reporting challenges\n\n\nDesign and implement next generation financial solutions assisted by almost unlimited access to AWS resources including EC2, RDS, Redshift, Stepfunctions, EMR, Lambda and 3rd party software TM1.\nBuild and deliver high quality data pipelines capable of scaling from running for a single month of data during month end close to 150 and more months when doing restatements.\nContinually improve ongoing reporting and analysis processes and infrastructure, automating or simplifying self-service capabilities for customers.\nDive deep to resolve problems at their root, looking for failure patterns and suggesting and implementing fixes or enhancements.\nPrepare runbooks, methods of procedures, tutorials, training videos on best practices for global delivery.\nSolve unique challenges presented by the massive data volume and diverse data sets working for one of the largest companies in the wo 5+ years data engineering experience.\nExtensive experience writing SQL queries and stored procedures.\nExperience with big data tools and distributed computing.\nFinance experience, exhibiting knowledge of financial reporting, budgeting and forecasting functions and processes.\nBachelors degree. Experience with non-relational databases / data stores (object storage, document or key-value stores, graph databases, column-family databases)\nExperience with AWS technologies like Redshift, S3, AWS Glue, EMR, Kinesis, FireHose, Lambda, and IAM roles and permissions.\nExperience with programming languages such as python, java shell scripts.\nExperience with IBM Planning Analytics/TM1 both scripting processes and writing rules.\nExperience with design delivery of formal training curriculum and programs.\nProject management, scoping, reporting, and scheduling experience.",,,,"['Performance management', 'Financial reporting', 'Project management', 'Financial planning', 'Scheduling', 'Stored procedures', 'Budgeting', 'Forecasting', 'Analytics', 'Python']",2025-06-14 05:04:21
Data Engineer - Python Programming,Leading Client,5 - 7 years,Not Disclosed,['Bengaluru'],"We are looking for a skilled Data Engineer with strong hands-on experience in Clickhouse, Kubernetes, SQL, Python, and FastAPI, along with a good understanding of PostgreSQL.\nThe ideal candidate will be responsible for building and maintaining efficient data pipelines, optimizing query performance, and developing APIs to support scalable data services.\n\n- Design, build, and maintain scalable and efficient data pipelines and ETL processes.\n\n- Develop and optimize Clickhouse databases for high-performance analytics.\n\n- Create RESTful APIs using FastAPI to expose data services.\n\n- Work with Kubernetes for container orchestration and deployment of data services.\n\n- Write complex SQL queries to extract, transform, and analyze data from PostgreSQL and Clickhouse.\n\n- Collaborate with data scientists, analysts, and backend teams to support data needs and ensure data quality.\n\n- Monitor, troubleshoot, and improve performance of data infrastructure.\n\n- Strong experience in Clickhouse - data modeling, query optimization, performance tuning.\n\n- Expertise in SQL - including complex joins, window functions, and optimization.\n\n- Proficient in Python, especially for data processing (Pandas, NumPy) and scripting.\n\n- Experience with FastAPI for creating lightweight APIs and microservices.\n\n- Hands-on experience with PostgreSQL - schema design, indexing, and performance.\n\n- Solid knowledge of Kubernetes managing containers, deployments, and scaling.\n\n- Understanding of software engineering best practices (CI/CD, version control, testing).\n\n- Experience with cloud platforms like AWS, GCP, or Azure.\n\n- Knowledge of data warehousing and distributed data systems.\n\n- Familiarity with Docker, Helm, and monitoring tools like Prometheus/Grafana.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'PostgreSQL', 'Data Modeling', 'Data Warehousing', 'Data Analytics', 'ETL', 'Python', 'SQL']",2025-06-14 05:04:24
Data Engineer,Accenture,3 - 8 years,Not Disclosed,['Hyderabad'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :Business AgilityMinimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions that facilitate data generation, collection, and processing. Your typical day will involve creating data pipelines, ensuring data quality, and implementing ETL processes to migrate and deploy data across various systems. You will collaborate with cross-functional teams to understand data requirements and deliver effective solutions that meet business needs. Additionally, you will monitor and optimize data workflows to enhance performance and reliability, ensuring that data is accessible and actionable for stakeholders.\nRoles & Responsibilities:- Need Databricks resource with Azure cloud experience- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Collaborate with data architects and analysts to design scalable data solutions.- Implement best practices for data governance and security throughout the data lifecycle.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Good To Have\n\n\n\n\nSkills:\nExperience with Business Agility.- Strong understanding of data modeling and database design principles.- Experience with data integration tools and ETL processes.- Familiarity with cloud platforms and services related to data storage and processing.\nAdditional Information:- The candidate should have minimum 3 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Pune office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data analytics', 'database design', 'data modeling', 'design principles', 'etl', 'hive', 'python', 'data warehousing', 'power bi', 'machine learning', 'data engineering', 'sql server', 'sql', 'data bricks', 'data quality', 'tableau', 'spark', 'data governance', 'hadoop', 'big data', 'aws', 'ssis', 'etl process']",2025-06-14 05:04:26
Data Engineer,Accenture,3 - 8 years,Not Disclosed,['Navi Mumbai'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :Business AgilityMinimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions that facilitate data generation, collection, and processing. Your typical day will involve creating data pipelines, ensuring data quality, and implementing ETL processes to migrate and deploy data across various systems. You will collaborate with cross-functional teams to understand data requirements and deliver effective solutions that meet business needs. Additionally, you will monitor and optimize data workflows to enhance performance and reliability, ensuring that data is accessible and actionable for stakeholders.\nRoles & Responsibilities:- Need Databricks resource with Azure cloud experience- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Collaborate with data architects and analysts to design scalable data solutions.- Implement best practices for data governance and security throughout the data lifecycle.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Good To Have\n\n\n\n\nSkills:\nExperience with Business Agility.- Strong understanding of data modeling and database design principles.- Experience with data integration tools and ETL processes.- Familiarity with cloud platforms and services related to data storage and processing.\nAdditional Information:- The candidate should have minimum 3 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Pune office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analytics', 'database design', 'data modeling', 'design principles', 'etl', 'hive', 'python', 'data warehousing', 'power bi', 'machine learning', 'data engineering', 'sql server', 'sql', 'data bricks', 'data quality', 'tableau', 'spark', 'data governance', 'hadoop', 'big data', 'aws', 'ssis', 'etl process']",2025-06-14 05:04:28
Data Engineer,Accenture,3 - 8 years,Not Disclosed,['Indore'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Google BigQuery\n\n\n\n\nGood to have skills :Microsoft SQL Server, Google Cloud Data ServicesMinimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will be responsible for designing, developing, and maintaining data solutions for data generation, collection, and processing. You will create data pipelines, ensure data quality, and implement ETL processes to migrate and deploy data across systems.\nRoles & Responsibilities:- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Develop and maintain data pipelines.- Ensure data quality throughout the data lifecycle.- Implement ETL processes for data migration and deployment.- Collaborate with cross-functional teams to understand data requirements.- Optimize data storage and retrieval processes.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Google BigQuery.- Strong understanding of data engineering principles.- Experience with cloud-based data services.- Knowledge of SQL and database management systems.- Hands-on experience with data modeling and schema design.\nAdditional Information:- The candidate should have a minimum of 3 years of experience in Google BigQuery.- This position is based at our Mumbai office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data engineering', 'sql', 'data modeling', 'bigquery', 'etl', 'schema', 'hive', 'data services', 'python', 'amazon redshift', 'data warehousing', 'google', 'data migration', 'knowledge of sql', 'sql server', 'database design', 'data quality', 'tableau', 'spark', 'etl tool', 'hadoop', 'big data', 'aws', 'etl process']",2025-06-14 05:04:31
Data Engineer,Accenture,3 - 8 years,Not Disclosed,['Bhubaneswar'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :Business AgilityMinimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions that facilitate data generation, collection, and processing. Your typical day will involve creating data pipelines, ensuring data quality, and implementing ETL processes to migrate and deploy data across various systems. You will collaborate with cross-functional teams to understand data requirements and deliver effective solutions that meet business needs. Additionally, you will monitor and optimize data workflows to enhance performance and reliability, ensuring that data is accessible and actionable for stakeholders.\nRoles & Responsibilities:- Need Databricks resource with Azure cloud experience- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Collaborate with data architects and analysts to design scalable data solutions.- Implement best practices for data governance and security throughout the data lifecycle.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Good To Have\n\n\n\n\nSkills:\nExperience with Business Agility.- Strong understanding of data modeling and database design principles.- Experience with data integration tools and ETL processes.- Familiarity with cloud platforms and services related to data storage and processing.\nAdditional Information:- The candidate should have minimum 3 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Pune office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data analytics', 'database design', 'data modeling', 'design principles', 'etl', 'hive', 'python', 'data warehousing', 'power bi', 'machine learning', 'data engineering', 'sql server', 'sql', 'data bricks', 'data quality', 'tableau', 'spark', 'data governance', 'hadoop', 'big data', 'aws', 'ssis', 'etl process']",2025-06-14 05:04:34
Data Engineer,Accenture,3 - 8 years,Not Disclosed,['Chennai'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :Business AgilityMinimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions that facilitate data generation, collection, and processing. Your typical day will involve creating data pipelines, ensuring data quality, and implementing ETL processes to migrate and deploy data across various systems. You will collaborate with cross-functional teams to understand data requirements and deliver effective solutions that meet business needs. Additionally, you will monitor and optimize data workflows to enhance performance and reliability, ensuring that data is accessible and actionable for stakeholders.\nRoles & Responsibilities:- Need Databricks resource with Azure cloud experience- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Collaborate with data architects and analysts to design scalable data solutions.- Implement best practices for data governance and security throughout the data lifecycle.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Good To Have\n\n\n\n\nSkills:\nExperience with Business Agility.- Strong understanding of data modeling and database design principles.- Experience with data integration tools and ETL processes.- Familiarity with cloud platforms and services related to data storage and processing.\nAdditional Information:- The candidate should have minimum 3 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Pune office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analytics', 'database design', 'data modeling', 'design principles', 'etl', 'hive', 'python', 'data warehousing', 'power bi', 'machine learning', 'data engineering', 'sql server', 'sql', 'data bricks', 'data quality', 'tableau', 'spark', 'data governance', 'hadoop', 'big data', 'aws', 'ssis', 'etl process']",2025-06-14 05:04:36
Data Engineer,Accenture,3 - 8 years,Not Disclosed,['Coimbatore'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Talend ETL\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL processes to migrate and deploy data across systems. Be involved in the end-to-end data management process.\nRoles & Responsibilities:- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work-related problems.- Develop and maintain data pipelines for efficient data processing.- Ensure data quality and integrity throughout the data lifecycle.- Implement ETL processes to extract, transform, and load data.- Collaborate with cross-functional teams to optimize data solutions.- Conduct data analysis to identify trends and insights.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Talend ETL.- Strong understanding of data integration and ETL processes.- Experience with data modeling and database design.- Knowledge of SQL and database querying languages.- Hands-on experience with data warehousing concepts.\nAdditional Information:- The candidate should have a minimum of 3 years of experience in Talend ETL.- This position is based at our Hyderabad office.- A 15 years full-time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['sql', 'talend etl', 'etl', 'data integration', 'etl process', 'hive', 'python', 'data analysis', 'data management', 'talend', 'data processing', 'data warehousing', 'knowledge of sql', 'data engineering', 'database design', 'data quality', 'data modeling', 'spark', 'data warehousing concepts', 'hadoop']",2025-06-14 05:04:39
Data Engineer || Paisabazaar || Gurgaon,Paisabazaar,3 - 5 years,Not Disclosed,['Gurugram'],"Qualifications for Data Engineer :\n3+ Years of experience in building and optimizing big data solutions required to fulfill business and technology requirements.\n4+ years of technical expertise in areas of design and implementation using big data technology Hadoop, Hive, Spark, Python/Java.\nStrong analytic skills to understand and create solutions for business use cases.\nEnsure best practices to implement data governance principles, data quality checks on each data layer.",,,,"['Pyspark', 'Data Engineering', 'Hadoop', 'Hive', 'Java', 'Scala Programming', 'Big Data', 'SQL', 'Azure Cloud', 'GCP', 'Spark', 'AWS', 'Python']",2025-06-14 05:04:41
Data Engineer,Accenture,3 - 8 years,Not Disclosed,['Pune'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :Business AgilityMinimum\n\n\n\n12 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions that facilitate data generation, collection, and processing. Your typical day will involve creating data pipelines, ensuring data quality, and implementing ETL processes to migrate and deploy data across various systems. You will collaborate with cross-functional teams to understand data requirements and deliver effective solutions that meet business needs. Additionally, you will monitor and optimize data workflows to enhance performance and reliability, ensuring that data is accessible and actionable for stakeholders.\nRoles & Responsibilities:- Need Databricks resource with Azure cloud experience- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Collaborate with data architects and analysts to design scalable data solutions.- Implement best practices for data governance and security throughout the data lifecycle.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Good To Have\n\n\n\n\nSkills:\nExperience with Business Agility.- Strong understanding of data modeling and database design principles.- Experience with data integration tools and ETL processes.- Familiarity with cloud platforms and services related to data storage and processing.\nAdditional Information:- The candidate should have minimum 3 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Pune office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data analytics', 'database design', 'data modeling', 'design principles', 'etl', 'hive', 'python', 'data warehousing', 'power bi', 'machine learning', 'data engineering', 'sql server', 'sql', 'data bricks', 'data quality', 'tableau', 'spark', 'data governance', 'hadoop', 'big data', 'aws', 'ssis', 'etl process']",2025-06-14 05:04:43
Data Engineer,Accenture,3 - 8 years,Not Disclosed,['Bhubaneswar'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :Business AgilityMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions that facilitate data generation, collection, and processing. Your typical day will involve creating data pipelines, ensuring data quality, and implementing ETL processes to migrate and deploy data across various systems. You will collaborate with cross-functional teams to understand data requirements and deliver effective solutions that meet business needs. Additionally, you will monitor and optimize data workflows to enhance performance and reliability, ensuring that data is accessible and actionable for stakeholders.\nRoles & Responsibilities:- Need Databricks resource with Azure cloud experience- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Collaborate with data architects and analysts to design scalable data solutions.- Implement best practices for data governance and security throughout the data lifecycle.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Good To Have\n\n\n\n\nSkills:\nExperience with Business Agility.- Strong understanding of data modeling and database design principles.- Experience with data integration tools and ETL processes.- Familiarity with cloud platforms and services related to data storage and processing.\nAdditional Information:- The candidate should have minimum 3 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Pune office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analytics', 'database design', 'data modeling', 'design principles', 'etl', 'hive', 'python', 'data warehousing', 'power bi', 'machine learning', 'data engineering', 'sql server', 'sql', 'data bricks', 'data quality', 'tableau', 'spark', 'data governance', 'hadoop', 'big data', 'aws', 'ssis', 'etl process']",2025-06-14 05:04:46
Data Engineer,Accenture,3 - 8 years,Not Disclosed,['Pune'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions that facilitate data generation, collection, and processing. Your typical day will involve creating data pipelines, ensuring data quality, and implementing ETL processes to effectively migrate and deploy data across various systems. You will collaborate with cross-functional teams to understand data requirements and contribute to the overall data strategy of the organization, ensuring that data solutions are efficient, scalable, and aligned with business objectives.\nRoles & Responsibilities:- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Collaborate with stakeholders to gather and analyze data requirements.- Design and implement robust data pipelines to support data processing and analytics.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Strong understanding of data modeling and database design principles.- Experience with ETL tools and data integration techniques.- Familiarity with cloud platforms and services related to data storage and processing.- Knowledge of programming languages such as Python or Scala for data manipulation.\nAdditional Information:- The candidate should have minimum 3 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Pune office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'data analytics', 'database design', 'data modeling', 'design principles', 'hive', 'scala', 'data manipulation', 'data processing', 'pyspark', 'data warehousing', 'data engineering', 'sql', 'data quality', 'tableau', 'etl tool', 'spark', 'hadoop', 'etl', 'big data', 'data integration', 'etl process']",2025-06-14 05:04:49
Data Engineer,Accenture,3 - 8 years,Not Disclosed,['Chennai'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :Business AgilityMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions that facilitate data generation, collection, and processing. Your typical day will involve creating data pipelines, ensuring data quality, and implementing ETL processes to migrate and deploy data across various systems. You will collaborate with cross-functional teams to understand data requirements and deliver effective solutions that meet business needs. Additionally, you will monitor and optimize data workflows to enhance performance and reliability, ensuring that data is accessible and actionable for stakeholders.\nRoles & Responsibilities:- Need Databricks resource with Azure cloud experience- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Collaborate with data architects and analysts to design scalable data solutions.- Implement best practices for data governance and security throughout the data lifecycle.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Good To Have\n\n\n\n\nSkills:\nExperience with Business Agility.- Strong understanding of data modeling and database design principles.- Experience with data integration tools and ETL processes.- Familiarity with cloud platforms and services related to data storage and processing.\nAdditional Information:- The candidate should have minimum 3 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Pune office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data analytics', 'database design', 'data modeling', 'design principles', 'etl', 'hive', 'python', 'data warehousing', 'power bi', 'machine learning', 'data engineering', 'sql server', 'sql', 'data bricks', 'data quality', 'tableau', 'spark', 'data governance', 'hadoop', 'big data', 'aws', 'ssis', 'etl process']",2025-06-14 05:04:51
Data Engineer,Accenture,5 - 10 years,Not Disclosed,['Chennai'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL processes to migrate and deploy data across systems. Your day will involve working on data architecture and engineering tasks to support business operations and decision-making.\nRoles & Responsibilities:- Expected to be an SME, collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Develop and maintain data pipelines for efficient data processing.- Implement ETL processes to ensure seamless data migration and deployment.- Collaborate with cross-functional teams to design and optimize data solutions.- Conduct data quality assessments and implement improvements for data integrity.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Strong understanding of data architecture principles.- Experience in designing and implementing data solutions.- Proficient in SQL and other data querying languages.- Knowledge of cloud platforms such as AWS or Azure.\nAdditional Information:- The candidate should have a minimum of 5 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Chennai office.- A 15 years full-time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analytics', 'sql', 'data architecture principles', 'etl', 'aws', 'hive', 'python', 'data processing', 'airflow', 'microsoft azure', 'pyspark', 'data warehousing', 'data integrity', 'data migration', 'data engineering', 'data quality', 'spark', 'hadoop', 'business operations', 'big data', 'etl process']",2025-06-14 05:04:54
Data Engineer,Accenture,3 - 8 years,Not Disclosed,['Indore'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :Business AgilityMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions that facilitate data generation, collection, and processing. Your typical day will involve creating data pipelines, ensuring data quality, and implementing ETL processes to migrate and deploy data across various systems. You will collaborate with cross-functional teams to understand data requirements and deliver effective solutions that meet business needs. Additionally, you will monitor and optimize data workflows to enhance performance and reliability, ensuring that data is accessible and actionable for stakeholders.\nRoles & Responsibilities:- Need Databricks resource with Azure cloud experience- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Collaborate with data architects and analysts to design scalable data solutions.- Implement best practices for data governance and security throughout the data lifecycle.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Good To Have\n\n\n\n\nSkills:\nExperience with Business Agility.- Strong understanding of data modeling and database design principles.- Experience with data integration tools and ETL processes.- Familiarity with cloud platforms and services related to data storage and processing.\nAdditional Information:- The candidate should have minimum 3 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Pune office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analytics', 'database design', 'data modeling', 'design principles', 'etl', 'hive', 'python', 'data warehousing', 'power bi', 'machine learning', 'data engineering', 'sql server', 'sql', 'data bricks', 'data quality', 'tableau', 'spark', 'data governance', 'hadoop', 'big data', 'aws', 'ssis', 'etl process']",2025-06-14 05:04:56
DE&A - Core - Big Data Engineering - ETL Orchestration DE&A - Core,Zensar,5 - 9 years,Not Disclosed,"['Kolkata', 'Mumbai', 'New Delhi', 'Hyderabad', 'Pune', 'Chennai', 'Bengaluru']","Design of complex ETL interfaces with agnostic tool set for various source/target types (SAP BODS preferred)\nPerformance Tuning & Troubleshooting skills across all technologies used\nStrong in DB and SQLs with decent data modeling skills\nAbility to lead developers and QA and adhere to committed timelines\nAgile experience is preferred\nAbility to work with upstream, downstream, and reporting system stakeholders and convert business requirements into technical requirements (JIRA stories)\n10 years of experience in:\n- ETL tools\n- Repository maintenance\n- Migration of jobs/workflows/dataflows\n- Job monitoring, break fix, user maintenance\n- CDC (Change Data Capture)\n- Oracle Golden Gate\n- Working with different sources/targets (Oracle, SAP, Files as source; SQL Server, file uploads as targets)\nRole Description :\n- Responsible for planning, developing, implementing, and managing data warehouse project deliverables\n- Design documents and high-level data mapping for ETL specifications\n- Create transformations in ETL jobs to achieve data cleansing and standardizations during initial data load\n- Provide support for integration testing, defect fixing, and deployment\nProficient knowledge in:\n- Transformations like Query Push down, SQL Table Comparison, Pivot, Look up, etc.\n- Databases like Oracle, SQL Queries, SQL Server\n- Dimensional modeling, data mining, and data warehouse concepts\nExcellent analytical skills\nKnowledge to transfer data from Oracle, SQL Server tables, and Web Services either incrementally (Delta Load) or full load to the data warehouse on a periodic basis\nTroubleshooting existing ETL jobs and improving performance of existing jobs\nCreating and loading data into aggregate tables using transformations\nAbility to perform tasks individually and independently\nDesign of complex ETL interfaces with agnostic tool set for various source/target types (SAP BODS preferred)\nPerformance Tuning & Troubleshooting skills across all technologies used\nStrong in DB and SQLs with decent data modeling skills\nAbility to lead developers and QA and adhere to committed timelines\nAgile experience is preferred\nAbility to work with upstream, downstream, and reporting system stakeholders and convert business requirements into technical requirements (JIRA stories)\n10 years of experience in:\n- ETL tools\n- Repository maintenance\n- Migration of jobs/workflows/dataflows\n- Job monitoring, break fix, user maintenance\n- CDC (Change Data Capture)\n- Oracle Golden Gate\n- Working with different sources/targets (Oracle, SAP, Files as source; SQL Server, file uploads as targets)\nRole Description :\n- Responsible for planning, developing, implementing, and managing data warehouse project deliverables\n- Design documents and high-level data mapping for ETL specifications\n- Create transformations in ETL jobs to achieve data cleansing and standardizations during initial data load\n- Provide support for integration testing, defect fixing, and deployment\nProficient knowledge in:\n- Transformations like Query Push down, SQL Table Comparison, Pivot, Look up, etc.\n- Databases like Oracle, SQL Queries, SQL Server\n- Dimensional modeling, data mining, and data warehouse concepts\nExcellent analytical skills\nKnowledge to transfer data from Oracle, SQL Server tables, and Web Services either incrementally (Delta Load) or full load to the data warehouse on a periodic basis\nTroubleshooting existing ETL jobs and improving performance of existing jobs\nCreating and loading data into aggregate tables using transformations\nAbility to perform tasks individually and independently",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Performance tuning', 'SAP', 'Data modeling', 'Integration testing', 'Agile', 'data mapping', 'Troubleshooting', 'Data mining', 'JIRA', 'Monitoring']",2025-06-14 05:04:59
Senior Data Engineer,Robert Bosch Engineering and Business Solutions Private Limited,4 - 8 years,Not Disclosed,['Bengaluru'],"As a Data engineer in our team, you work with large scale manufacturing data coming from our globally distributed plants. You will focus on building efficient, scalable data-driven applications.\nThe data sets produced by these applications - whether data streams or data at rest - need to be highly available, reliable, consistent and quality-assured so that they can serve as input to wide range of other use cases and downstream applications.\nWe run these applications on Azure databricks, you will be building applications, you will also contribute to scaling the platform including topics such as automation and observability.\nFinally, you are expected to interact with customers and other technical teams e. g. for requirements clarification definition of data models.\nPrimary responsibilities:\nBe a key contributor to the Bosch hybrid cloud data platform (on-prem cloud)\nDesigning building data pipelines on a global scale, ranging from small to huge datasets\nDesign applications and data models based on deep business understanding and customer requirements\nDirectly work with architects and technical leadership to design implement applications and / or architectural components\nArchitectural proposal and estimation for the application, technical leadership to the team\nCoordination/Collaboration with central teams for tasks and standards\nDevelop data integration workflow in Azure\nDeveloping streaming application using scala.\nIntegrating the end-to-end Azure Databricks pipeline to take data from source systems to target system ensuring the quality and consistency of data.\nDefining data quality and validation checks.\nConfiguring data processing and transformation.\nWriting unit test cases for data pipelines.\nDefining and implementing data quality and validation check.\nTuning pipeline configurations for optimal performance.\nParticipate in Peer review and PR review for the code written by team members",Industry Type: Automobile,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Automation', 'Architecture', 'Technical leadership', 'Data processing', 'Workflow', 'Data quality', 'Test cases', 'Unit testing', 'Downstream']",2025-06-14 05:05:01
Sr. Data Engineer (Bigdata + Java/Python/Go),Visa,2 - 9 years,Not Disclosed,['Bengaluru'],"Visas Data and AI Platform team is at the forefront of transforming the payment industry by harnessing the power of big data, artificial intelligence, and cutting-edge open-source technologies. Our mission is to build scalable, high-performance data platforms and services that enable Visa s businesses to innovate and deliver unparalleled value to our customers worldwide.\nOur Work\nAt Visa, we are dedicated to developing robust data platforms that handle massive volumes of data, ensuring real-time processing, analytics, and low-latency responses. Our team leverages a variety of open-source software to create flexible, efficient, and reliable solutions that meet the dynamic needs of our business operations.\nBig Data Platforms: We design and implement platforms capable of processing and analyzing vast amounts of data generated by millions of transactions daily. These platforms support various use cases, from fraud detection to personalized customer experiences.\nOpen-Source Software: Our development approach heavily relies on open-source technologies, allowing us to build innovative solutions while contributing to the wider tech community. We use and contribute to projects like Apache Hadoop, Apache Druid, Apache Pinot, Apache Spark, Apache Flink, and Apache Kafka, among others.\nCutting-Edge Technologies: To achieve our goals, we incorporate state-of-the-art technologies in our stack. This includes leveraging streaming analytics, real-time data processing, and low-latency services to ensure that our platforms are always at the cutting edge of performance and reliability.\nWho We Are Looking For\nWe are seeking seasoned professionals with a deep understanding and expertise in large-scale distributed systems and platform development using open-source software. Ideal candidates will have:\nExpertise in Distributed Systems: Proven experience in designing, developing, and maintaining large-scale distributed systems.\nOpen-Source Contributions: Active involvement in the open-source community, particularly in streaming, analytical, and low-latency services.\nTechnical Proficiency: Strong proficiency in technologies such as Apache Druid, Apache Pinot, Apache Spark, Apache Flink, Apache Kafka, Temporal, and Yugabyte.\nInnovative Mindset: A passion for innovation and a drive to stay ahead of the curve by continuously exploring and implementing new technologies.\nJoin Us\nIf you are a passionate technologist with a track record of delivering high-quality, scalable solutions and contributing to the open-source community, we invite you to join our team. At Visa, you will have the opportunity to work on groundbreaking projects, collaborate with talented professionals, and make a significant impact on the future of payments.\nBy joining our Data Platform team, you will play a crucial role in enabling Visa s businesses to leverage data and AI to drive innovation, enhance security, and deliver exceptional customer experiences.\nThe candidate will:\nActively participate in all phases of software development lifecycle: analysis, technical design, planning, development, testing/CICD, release, post production/escalation support\nWork on a highly scalable, available and resilient multi-tenant Platforms that can host large scale and highly critical applications.\nDevelop large scale multi-tenant software components on the Platform in an Agile based methodology to provide self-service capabilities.\nCollaborating with partners from business and technology organizations, develop key deliverables for Data Platform Strategy - Scalability, optimization, operations, availability, roadmap.\nBuild partnerships with other technology teams to drive efficiency in their use of platforms and services that PaaS provide\nExpertise in applying the appropriate software engineering patterns to build robust and scalable systems.\nBe a self-starter, work independently and Mentor junior members in the scrum team. Stays current with new and evolving technologies\nBuilds strong commitment within the team to support the appropriate team priorities\nThis is a hybrid position. Expectation of days in office will be confirmed by your Hiring Manager.\n\n\nBasic Qualifications:\n3+ years of relevant work experience with a Bachelor s Degree or at least 2 years of work experience with an Advanced degree (e.g. Masters, MBA, JD, MD) or 0 years of work experience with a PhD, OR 3+ years of relevant wor",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Automation', 'GIT', 'Configuration management', 'Scrum', 'Agile methodology', 'Open source', 'Distribution system', 'Analytics', 'Python']",2025-06-14 05:05:03
IN Senior Associate Azure Data Engineer Data & Analaytics,PwC Service Delivery Center,2 - 5 years,Not Disclosed,['Bengaluru'],"Not Applicable\nSpecialism\nData, Analytics & AI\nManagement Level\nSenior Associate\n& Summary\nAt PwC, our people in data and analytics engineering focus on leveraging advanced technologies and techniques to design and develop robust data solutions for clients. They play a crucial role in transforming raw data into actionable insights, enabling informed decisionmaking and driving business growth.\n\nIn data engineering at PwC, you will focus on designing and building data infrastructure and systems to enable efficient data processing and analysis. You will be responsible for developing and implementing data pipelines, data integration, and data transformation solutions.\n& Summary A career within Data and Analytics services will provide you with the opportunity to help organisations uncover enterprise insights and drive business results using smarter data analytics. We focus on a collection of organisational technology capabilities, including business intelligence, data management, and data assurance that help our clients drive innovation, growth, and change within their organisations in order to keep up with the changing nature of customers and technology. We make impactful decisions by mixing mind and machine to leverage data, understand and navigate risk, and help our clients gain a competitive edge.\nResponsibilities\nDesign, develop, and optimize data pipelines and ETL processes using PySpark or Scala to extract, transform, and load large volumes of structured and unstructured data from diverse sources.\nImplement data ingestion, processing, and storage solutions on Azure cloud platform, leveraging services such as Azure Databricks, Azure Data Lake Storage, and Azure Synapse Analytics.\nDevelop and maintain data models, schemas, and metadata to support efficient data access, query performance, and analytics requirements.\nMonitor pipeline performance, troubleshoot issues, and optimize data processing workflows for scalability, reliability, and costeffectiveness.\nImplement data security and compliance measures to protect sensitive information and ensure regulatory compliance.\nRequirement\nProven experience as a Data Engineer, with expertise in building and optimizing data pipelines using PySpark, Scala, and Apache Spark.\nHandson experience with cloud platforms, particularly Azure, and proficiency in Azure services such as Azure Databricks, Azure Data Lake Storage, Azure Synapse Analytics, and Azure SQL Database.\nStrong programming skills in Python and Scala, with experience in software development, version control, and CI/CD practices.\nFamiliarity with data warehousing concepts, dimensional modeling, and relational databases (e.g., SQL Server, PostgreSQL, MySQL).\nExperience with big data technologies and frameworks (e.g., Hadoop, Hive, HBase) is a plus.\nMandatory skill sets\nSpark, Pyspark, Azure\nPreferred skill sets\nSpark, Pyspark, Azure\nYears of experience required\n4 8\nEducation qualification\nB.Tech / M.Tech / MBA / MCA\nEducation\nDegrees/Field of Study required Bachelor of Technology, Master of Business Administration\nDegrees/Field of Study preferred\nRequired Skills\nMicrosoft Azure\nAccepting Feedback, Accepting Feedback, Active Listening, Agile Scalability, Amazon Web Services (AWS), Analytical Thinking, Apache Airflow, Apache Hadoop, Azure Data Factory, Communication, Creativity, Data Anonymization, Data Architecture, Database Administration, Database Management System (DBMS), Database Optimization, Database Security Best Practices, Databricks Unified Data Analytics Platform, Data Engineering, Data Engineering Platforms, Data Infrastructure, Data Integration, Data Lake, Data Modeling, Data Pipeline {+ 27 more}\nTravel Requirements\nGovernment Clearance Required?",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data management', 'Data modeling', 'Postgresql', 'MySQL', 'Database administration', 'Agile', 'Apache', 'Business intelligence', 'SQL', 'Python']",2025-06-14 05:05:06
PySpark Data Engineer,IT Services & Consulting,6 - 10 years,14-20 Lacs P.A.,"['Hyderabad', 'Bengaluru']","Job Description: PySpark Data Engineer:-\n1. API Development: Design, develop, and maintain robust APIs using FastAPI and RESTful principles for scalable backend systems.\n2. Big Data Processing: Leverage PySpark to process and analyze large datasets efficiently, ensuring optimal performance in big data environments.\n3. Full-Stack Integration: Develop seamless backend-to-frontend feature integrations, collaborating with front-end developers for cohesive user experiences.\n4. CI/CD Pipelines: Implement and manage CI/CD pipelines using GitHub Actions and Azure DevOps to streamline deployments and ensure system reliability.\n5. Containerization: Utilize Docker for building and deploying containerized applications in development and production environments.\n6. Team Leadership: Lead and mentor a team of developers, providing guidance, code reviews, and support to junior team members to ensure high-quality deliverables.\n7. Code Optimization: Write clean, maintainable, and efficient Python code, with a focus on scalability, reusability, and performance.\n8. Cloud Deployment: Deploy and manage applications on cloud platforms like Azure, ensuring high availability and fault tolerance.\n9. Collaboration: Work closely with cross-functional teams, including product managers and designers, to translate business requirements into technical solutions.\n10. Documentation: Maintain thorough documentation for APIs, processes, and systems to ensure transparency and ease of maintenance.\nHighlighted Skillset:-\nBig Data: Strong PySpark skills for processing large datasets.\nDevOps: Proficiency in GitHub Actions, CI/CD pipelines, Azure DevOps, and Docker.\nIntegration: Experience in backend-to-frontend feature connectivity.\nLeadership: Proven ability to lead and mentor development teams.\nCloud: Knowledge of deploying and managing applications in Azure or other cloud environments.\nTeam Collaboration: Strong interpersonal and communication skills for working in cross-functional teams.\nBest Practices: Emphasis on clean code, performance optimization, and robust documentation.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Pyspark', 'Docker', 'Github', 'FastAPI', 'Ci/Cd']",2025-06-14 05:05:08
Data Engineer,Orangemint Technologies,3 - 5 years,20-25 Lacs P.A.,['Bengaluru'],"Company name: PulseData labs Pvt Ltd (captive Unit for URUS, USA)\nAbout URUS We are the URUS family (US), a global leader in products and services for Agritech.\nSENIOR DATA ENGINEER This role is responsible for the design, development, and maintenance of data integration and reporting solutions. The ideal candidate will possess expertise in Databricks and strong skills in SQL Server, SSIS and SSRS, and experience with other modern data engineering tools such as Azure Data Factory. This position requires a proactive and results-oriented individual with a passion for data and a strong understanding of data warehousing principles.\nResponsibilities Data Integration\nDesign, develop, and maintain robust and efficient ETL pipelines and processes on Databricks.\nTroubleshoot and resolve Databricks pipeline errors and performance issues.\nMaintain legacy SSIS packages for ETL processes.\nTroubleshoot and resolve SSIS package errors and performance issues.\nOptimize data flow performance and minimize data latency.\nImplement data quality checks and validations within ETL processes.\nDatabricks Development\nDevelop and maintain Databricks pipelines and datasets using Python, Spark and SQL.\nMigrate legacy SSIS packages to Databricks pipelines.\nOptimize Databricks jobs for performance and cost-effectiveness.\nIntegrate Databricks with other data sources and systems.\nParticipate in the design and implementation of data lake architectures.\nData Warehousing\nParticipate in the design and implementation of data warehousing solutions.\nSupport data quality initiatives and implement data cleansing procedures.\nReporting and Analytics\nCollaborate with business users to understand data requirements for department driven reporting needs.\nMaintain existing library of complex SSRS reports, dashboards, and visualizations.\nTroubleshoot and resolve SSRS report issues, including performance bottlenecks and data inconsistencies.\nCollaboration and Communication\nComfortable in entrepreneurial, self-starting, and fast-paced environment, working both independently and with our highly skilled teams.\nCollaborate effectively with business users, data analysts, and other IT teams.\nCommunicate technical information clearly and concisely, both verbally and in writing.\nDocument all development work and procedures thoroughly.\nContinuous Growth\nKeep abreast of the latest advancements in data integration, reporting, and data engineering technologies.\nContinuously improve skills and knowledge through training and self-learning.\nThis job description reflects managements assignment of essential functions; it does not prescribe or restrict the tasks that may be assigned.\nRequirements\nBachelor's degree in computer science, Information Systems, or a related field.\n2+ years of experience in data integration and reporting.\nExtensive experience with Databricks, including Python, Spark, and Delta Lake.\nStrong proficiency in SQL Server, including T-SQL, stored procedures, and functions.\nExperience with SSIS (SQL Server Integration Services) development and maintenance.\nExperience with SSRS (SQL Server Reporting Services) report design and development.\nExperience with data warehousing concepts and best practices.\nExperience with Microsoft Azure cloud platform and Microsoft Fabric desirable.\nStrong analytical and problem-solving skills.\nExcellent communication and interpersonal skills.\nAbility to work independently and as part of a team.\nExperience with Agile methodologies.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Etl Development', 'Azure Databricks', 'Databricks Engineer', 'Spark', 'SQL Server', 'Data Warehousing', 'Pythonspark']",2025-06-14 05:05:10
Data Engineer - Databricks,KPI Partners,3 - 6 years,Not Disclosed,"['Hyderabad', 'Pune', 'Bengaluru']","About KPI Partners.\nKPI Partners is a leading provider of data analytics solutions, dedicated to helping organizations transform data into actionable insights. Our innovative approach combines advanced technology with expert consulting, allowing businesses to leverage their data for improved performance and decision-making.\n\nJob Description.\nWe are seeking a skilled and motivated Data Engineer with experience in Databricks to join our dynamic team. The ideal candidate will be responsible for designing, building, and maintaining scalable data pipelines and data processing solutions that support our analytics initiatives. You will collaborate closely with data scientists, analysts, and other engineers to ensure the consistent flow of high-quality data across our platforms.",,,,"['python', 'data analytics', 'analytical', 'scala', 'pyspark', 'microsoft azure', 'data warehousing', 'data pipeline', 'data architecture', 'data engineering', 'sql', 'data bricks', 'cloud', 'analytics', 'data quality', 'data modeling', 'gcp', 'teamwork', 'integration', 'aws', 'etl', 'programming', 'communication skills', 'etl scripts']",2025-06-14 05:05:13
nior Data Engineer,M360 Research,5 - 8 years,Not Disclosed,['Bengaluru'],"M3 Global Research, an M3 company, is seeking a Senior Data Engineer to join our data engineering team. This role will focus on building and maintaining robust data pipelines, working closely with stakeholders to ensure data solutions align with business objectives, and utilizing tools like Power BI for data visualization and reporting. The ideal candidate has strong analytical skills, a passion for data-driven decision-making, and excellent communication abilities to work effectively with stakeholders across the organization.\nEssential Duties and Responsibilities:\nDesign, develop, and maintain high-quality, secure data pipelines and processes to manage and transform data efficiently.\nLead the architecture and implementation of data models, schemas, and integrations that support business intelligence and reporting needs.\nCollaborate with cross-functional teams to understand data requirements and deliver optimal data solutions that align with business goals.\nMaintain and enhance data infrastructure, including data warehouses, lakes, and integration tools.\nProvide guidance on best practices for data management, security, and compliance.\nSupport Power BI and other visualization tools, ensuring consistent and reliable access to data insights.\nOversee the delivery of data initiatives, ensuring they meet project milestones, KPIs, and deadlines.\nEssential Job Functions:\nMaintain regular and punctual attendance.\nWork cooperatively and communicate effectively with team members and stakeholders.\nComply with all company policies and procedures.\nSupervisory Responsibility:\nYes\nOutcomes:\nDeliver high-quality, reliable data solutions.\nProvide stakeholders with clear and actionable insights through Power BI reports.\nEnsure data pipelines and ETL processes are optimized and running efficiently.\nFoster strong relationships with stakeholders to ensure their data needs are met.\nCompetencies:\nAttention to detail.\nAnalytical thinking and problem-solving skills.\nStrong communication and interpersonal skills to engage effectively with stakeholders.\nAbility to work in a fast-paced, agile environment.\nKnowledge and Skills:\nProficiency with data engineering tools and technologies (eg, SQL, Python, ETL tools).\nStrong experience with Power BI for data visualization and reporting.\nFamiliarity with cloud-based data platforms (eg, AWS, Azure, Google Cloud).\nExperience with data modeling, data warehousing, and designing scalable data architectures.\nStrong knowledge of database systems (eg, SQL Server, Oracle, PostgreSQL).\nExperience working in an Agile development environment.\nExcellent communication skills to work effectively with both technical and non-technical stakeholders.\nAbility to multi-task and manage multiple projects simultaneously.\nProblem-solving mindset with a desire to continuously improve data processes.\nMinimum Experience:\n5+ years of experience in data engineering or related fields.\n2+ years of experience with Power BI or similar data visualization tools.\nEducation and Training Required:\nbachelors degree in computer science, Data Science, or a related field, or equivalent experience",Industry Type: Analytics / KPO / Research,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data management', 'ISO', 'Analytical', 'Healthcare', 'Market research', 'Oracle', 'Business intelligence', 'Japanese', 'Recruitment', 'SQL']",2025-06-14 05:05:15
Backend Data Engineer,Squarepeg Hires,3 - 8 years,Not Disclosed,['Bengaluru'],"Voicecare AI is a Healthcare Administration General Intelligence (HAGI) company for the back-office and the RCM industry. We are building a safety focused large and small conversational language model for the healthcare industry. Our mission is to dramatically improve access, adherence, and outcomes for the patients and the healthcare workforce through the application of generative AI. We are a venture-backed company partnering with the top healthcare stakeholders in the country.\n\nWe are seeking a Back-end Data Engineer with deep expertise in GCP, strong Python coding skills, and a security-first mindset. This role is critical in designing, implementing, and optimizing secure and scalable data pipelines to support AI-driven healthcare applications. The ideal candidate will have a strong background in cloud data architecture, big data processing, and compliance with healthcare security standards.\n\nKey Responsibilities:\n\nData Pipeline Development & Optimization\nDesign and develop scalable, secure, and high-performance data pipelines on GCP.\nImplement ETL/ELT workflows using tools like Dataflow and BigQuery.\nOptimize data processing for latency, cost-efficiency, and compliance with healthcare standards.\nCloud Data Architecture & GCP Expertise\nArchitect and manage data storage solutions (BigQuery, Cloud Storage, Spanner, Firestore) with a focus on security and performance.\nLeverage GCP-native services to build fault-tolerant, distributed data architectures.\nImplement data governance and access controls using IAM, VPC, and encryption best practices.\nAutomation & Infrastructure as Code (IaC)\nDevelop and maintain Infrastructure as Code (IaC) using Terraform, Deployment Manager, or CloudFormation.\nAutomate data engineering workflows using Python, and CI/CD pipelines.\nPerformance Monitoring & Optimization\nImplement logging, monitoring, and alerting using Cloud Logging, Cloud Monitoring, and other advanced techniques\nContinuously monitor and optimize data queries, storage utilization, and processing speed.\nCollaboration & Cross-Functional Integration\nWork closely with AI/ML engineers, full-stack engineers, quality engineering and data engineers to integrate AI-driven insights into healthcare applications.\nProvide expertise on data modeling, schema design, and efficient query execution.\nSkills & Experience:\nGCP Expertise: Deep experience with BigQuery, Dataflow, Cloud Composer (Airflow), Vertex AI, Pub/Sub, Cloud SQL, Spanner, and Firestore.\nPython Programming: Strong proficiency in Python for data processing, automation, and API development.\nSecurity-First Mindset: Hands-on experience with data encryption, access controls, and regulatory compliance.\nETL/ELT & Data Pipelines: Expertise in building scalable and secure data processing pipelines.\nMonitoring & Optimization: Experience with Cloud Logging, Cloud Monitoring, and SQL performance tuning.\nHas worked across multiple tech-stacks, and thrived in collaborating with Data Science, Frontend/Applications and Infra teams.\nStrong knowledge of database technologies, both SQL and NoSQL and experience in optimizing database performance.\n3 years of hands-on engineering experience, with demonstrated growth\nHas shown entrepreneurial spirit (e.g. worked at an early stage startup, or has started and delivered on a new product from the ground-up).\nHealthcare Industry Knowledge (Preferred): Familiarity with healthcare data standards (FHIR, HL7), compliance regulations, and PHI security best practices.\nQualifications\nBachelor s or Master s degree in Computer Science, Data Engineering, or a related field.\n3+ years of experience in data engineering, cloud architecture, and security-first data processing.\nGCP Certifications preferred (e.g., Professional Data Engineer, Professional Cloud Architect, Security Engineer).\nExperience working in healthcare AI, health tech startups, or enterprise healthcare IT environments is a plus.\nStrong portfolio of cloud-based data engineering projects demonstrating security best practices.\nCandidates located in Bangalore is a plus\nCandidates from top universities is a plus",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Performance tuning', 'Automation', 'Backend', 'Data modeling', 'Coding', 'Infrastructure', 'Healthcare', 'SQL', 'Python']",2025-06-14 05:05:17
Data Engineer,Talent Aspire,2 - 7 years,Not Disclosed,"['Chandigarh', 'Bengaluru']","As the Data Engineer, you will play a pivotal role in shaping our data infrastructure and\nexecuting against our strategy. You will ideate alongside engineering, data and our clients to\ndeploy data products with an innovative and meaningful impact to clients. You will design, build, and maintain scalable data pipelines and workflows on AWS. Additionally, your expertise in AI and machine learning will enhance our ability to deliver smarter, more predictive solutions.\n\nKey Responsibilities\nCollaborate with other engineers, customers to brainstorm and develop impactful data\nproducts tailored to our clients.\nLeverage AI and machine learning techniques to integrate intelligent features into our\nofferings.\nDevelop, and optimize end-to-end data pipelines on AWS\nFollow best practices in software architecture and development.\nImplement effective cost management and performance optimization strategies.\nDevelop and maintain systems using Python, SQL, PySpark, and Django for front-end\ndevelopment.\nWork directly with clients and end-users and address their data needs\nUtilize databases and tools including and not limited to, Postgres, Redshift, Airflow, and\nMongoDB to support our data ecosystem.\nLeverage AI frameworks and libraries to integrate advanced analytics into our solutions.\nQualifications\n\nExperience:\nMinimum of 3 years of experience in data engineering, software development, or\nrelated roles.\nProven track record in designing and deploying AWS cloud infrastructure\nsolutions\nAt least 2 years in data analysis and mining techniques to aid in descriptive and\ndiagnostic insights\nExtensive hands-on experience with Postgres, Redshift, Airflow, MongoDB, and\nreal-time data workflows.\n\nTechnical Skills:\nExpertise in Python, SQL, and PySpark\nStrong background in software architecture and scalable development practices.\nTableau, Metabase or similar viz tools experience\nWorking knowledge of AI frameworks and libraries is a plus.\nLeadership & Communication:\nDemonstrates ownership and accountability for delivery with a strong\ncommitment to quality.\nExcellent communication skills with a history of effective client and end-user\nengagement.\nStartup & Fintech Mindset:\nAdaptability and agility to thrive in a fast-paced, early-stage startup environment.\nPassion for fintech innovation and a strong desire to make a meaningful impact\non the future of finance.",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Engineering', 'performance optimization strategies', 'PySpark', 'Django', 'cost management', 'AWS', 'AI frameworks', 'Python', 'SQL']",2025-06-14 05:05:19
Senior Data Engineering Analyst,Optum,4 - 7 years,Not Disclosed,['Bengaluru'],"Job Description\n\nExperience 4 to 7 years.\nExperience in any ETL tools [e.g. DataStage] with implementation experience in large Data Warehouse\nProficiency in programming languages such as Python etc.\nExperience with data warehousing solutions (e.g., Snowflake, Redshift) and big data technologies (e.g., Hadoop, Spark).\nStrong knowledge of SQL and database management systems.\nFamiliarity with cloud platforms (e.g., AWS, Azure, GCP) and data pipeline orchestration tools (e.g. Airflow).\nProven ability to lead and develop high-performing teams, with excellent communication and interpersonal skills.\nStrong analytical and problem-solving abilities, with a focus on delivering actionable insights.\nResponsibilities\nDesign, develop, and maintain advanced data pipelines and ETL processes using niche technologies.\nCollaborate with cross-functional teams to understand complex data requirements and deliver tailored solutions.\nEnsure data quality and integrity by implementing robust data validation and monitoring processes.\nOptimize data systems for performance, scalability, and reliability.\nDevelop comprehensive documentation for data engineering processes and systems.",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['ETL', 'SQL', 'Python', 'Azure', 'Datastage', 'Snowflake', 'Ab Initio', 'Informatica', 'Teradata', 'AWS']",2025-06-14 05:05:22
Data Engineer,Axis Finance (AFL),7 - 11 years,Not Disclosed,"['Mumbai', 'Mumbai (All Areas)']","Key Responsibilities:\nShould have experience in below\nDesign, develop, and implement a Data Lake House architecture on AWS, ensuring scalability, flexibility, and performance.\nBuild ETL/ELT pipelines for ingesting, transforming, and processing structured and unstructured data.\nCollaborate with cross-functional teams to gather data requirements and deliver data solutions aligned with business needs.\nDevelop and manage data models, schemas, and data lakes for analytics, reporting, and BI purposes.\nImplement data governance practices, ensuring data quality, security, and compliance.\nPerform data integration between on-premise and cloud systems using AWS services.\nMonitor and troubleshoot data pipelines and infrastructure for reliability and scalability.\nSkills and Qualifications:\n7 + years of experience in data engineering, with a focus on cloud data platforms.\nStrong experience with AWS services: S3, Glue, Redshift, Athena, Lambda, IAM, RDS, and EC2.\nHands-on experience in building data lakes, data warehouses, and lake house architectures.\nShould have experience in ETL/ELT pipelines using tools like AWS Glue, Apache Spark, or similar.\nExpertise in SQL and Python or Java for data processing and transformations.\nFamiliarity with data modeling and schema design in cloud environments.\nUnderstanding of data security and governance practices, including IAM policies and data encryption.\nExperience with big data technologies (e.g., Hadoop, Spark) and data streaming services (e.g., Kinesis, Kafka).\nHave lending domain knowledge will be added advantage\nPreferred Skills:\nExperience with Databricks or similar platforms for data engineering.\nFamiliarity with DevOps practices for deploying data solutions on AWS (CI/CD pipelines).\nKnowledge of API integration and cloud data migration strategies.",Industry Type: NBFC,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['pipeline tools', 'lending domain', 'AWS', 'data models', 'spark', 'devops', 'databricks', 'date engineering platforms', 'hadoop', 'data lake house', 'API integration']",2025-06-14 05:05:24
SSAS Data Engineer - MDX / SQL,Leading Client,5 - 7 years,Not Disclosed,['Bengaluru'],"We are looking for an experienced SSAS Data Engineer with strong expertise in SSAS (Tabular and/or Multidimensional Models), SQL, MDX/DAX, and data modeling.\n\nThe ideal candidate will have a solid background in designing and developing BI solutions, working with large datasets, and building scalable SSAS cubes for reporting and analytics.\n\nExperience with ETL processes and reporting tools like Power BI is a strong plus.\n\nKey Responsibilities\n\n- Design, develop, and maintain SSAS models (Tabular and/or Multidimensional).\n\n- Build and optimize MDX or DAX queries for advanced reporting needs.\n\n- Create and manage data models (Star/Snowflake schemas) supporting business KPIs.\n\n- Develop and maintain ETL pipelines for efficient data ingestion (preferably using SSIS or similar tools).\n\n- Implement KPIs, aggregations, partitioning, and performance tuning in SSAS cubes.\n\n- Collaborate with data analysts, business stakeholders, and Power BI teams to deliver accurate and insightful reporting solutions.\n\n- Maintain data quality and consistency across data sources and reporting layers.\n\n- Implement RLS/OLS and manage report security and governance in SSAS and Power BI.\n\nPrimary :\n\nRequired Skills :\n\n- SSAS Tabular & Multidimensional.\n\n- SQL Server (Advanced SQL, Views, Joins, Indexes).\n\n- DAX & MDX.\n\n- Data Modeling & OLAP concepts.\n\nSecondary :\n\n- ETL Tools (SSIS or equivalent).\n\n- Power BI or similar BI/reporting tools.\n\n- Performance tuning & troubleshooting in SSAS and SQL.\n\n- Version control (TFS/Git), deployment best practices.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SSAS', 'MDX', 'Data Engineering', 'Power BI', 'Dimensional Modeling', 'Data Modeling', 'SQL Server', 'ETL', 'SSIS']",2025-06-14 05:05:27
Data Consultant - Databricks Data Engineer,Kyndryl,6 - 10 years,Not Disclosed,['Bengaluru'],"Who We Are\nAt Kyndryl, we design, build, manage and modernize the mission-critical technology systems that the world depends on every day. So why work at Kyndryl? We are always moving forward – always pushing ourselves to go further in our efforts to build a more equitable, inclusive world for our employees, our customers and our communities.\n\nThe Role\nAt Kyndryl, we design, build, manage and modernize the mission-critical technology systems that the world depends on every day. So why work at Kyndryl? We are always moving forward – always pushing ourselves to go further in our efforts to build a more equitable, inclusive world for our employees, our customers and our communities.",,,,"['python', 'sql queries', 'analytical', 'data mining', 'consulting', 'data processing', 'microsoft azure', 'pyspark', 'extraction', 'transformation', 'relational databases', 'stored procedures', 'sql', 'pipeline', 'java', 't', 'load', 'service delivery', 'spark', 'data ingestion', 'debugging', 'troubleshooting', 'client']",2025-06-14 05:05:30
Data Engineer,IT Service based company,5 - 8 years,15-22.5 Lacs P.A.,"['Noida', 'Bengaluru', 'Delhi / NCR']","HI Candidates,\n\nwe have an opportunities with one of the leading IT consulting Group for the data engineer role. Interested candidates can mail their CV's at Abhishek.saxena@mounttalent.com\n\n\nJob Description-\n\nWhat were looking for Data Engineer III with:\n5+ years of experience with ETL Process, Data warehouse architecture\n5+ Years of experience with Azure Data services i.e. ADF, ADLS Gen 2, Azure SQL dB, Synapse, Azure Databricks, Microsoft Fabric\n5+ years of experience designing business intelligence solutions\nStrong proficiency in SQL and Python/pyspark\nImplementation experience of Medallion architecture and delta lake (or lakehouse)\nExperience with cloud-based data platforms, preferably Azure\nFamiliarity with big data technologies and data warehousing concepts\nWorking knowledge of Azure DevOps and CICD (build and release)",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Snowflake', 'ETL', 'Azure Data Factory', 'Azure Synapse', 'Azure Databricks', 'Data Warehousing']",2025-06-14 05:05:32
Databricks Data Engineer,Mumba Technologies,6 - 10 years,22.5-25 Lacs P.A.,['Bengaluru'],"Mandatory Skills & Experience:\n6 to 8 years of experience in data engineering, with strong experience in Oracle\nDWH/ODS environments.\nMinimum 3+ years hands-on experience in Databricks (including PySpark, SQL,\nDelta Lake, Workflows).\nStrong understanding of Lakehouse architecture, cloud data platforms, and big\ndata processing.\n\nProven experience in migrating data warehouse and ETL workloads from Oracle to\ncloud platforms.\nExperience with PL/SQL, query tuning, and reverse engineering legacy systems.\nExposure to Pentaho and/or TIBCO Data Virtualization/Integration tools.\nExperience with CI/CD pipelines, version control (e.g., Git), and automated\ntesting.\nFamiliarity with data governance, security policies, and compliance in cloud\nenvironments.\nStrong communication and documentation skills.\n\nPreferred Skills (Advantage):\nExperience in cloud migration projects (AWS/Azure).\nKnowledge of Delta Lake, Unity Catalog, and Databricks workflows.\nExposure to Kafka for real-time data streaming.\nExperience with ETL tools like Pentaho or Tibco will be an added advantage.\nAWS/Azure/Databricks certifications\nTools & Technologies:\nDatabricks, Oracle, Hadoop (HDFS, Hive, Sqoop), AWS (S3, EMR, Glue, Lamda, RDS)\nPySpark, SQL, Python, Kafka\nCI/CD (Jenkins, GitHub Actions), Orchestration (Airflow, Control-M)\nJIRA, Confluence, Git (GitHub/Bitbucket)\nCloud Certifications (Preferred):\nDatabricks Certified Data Engineer\nAWS Certified Solutions Architect/Developer",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'Delta Lake', 'Databricks', 'Oracle DWH', 'Data warehouse', 'SQL']",2025-06-14 05:05:35
Data Engineer,Clifyx Technology,5 - 10 years,Not Disclosed,['Bengaluru'],"2\nData Engineer\nAzure Synapse/ADF , Workiva\nTo manage and maintain the associated Connector, Chains, Tables and Queries, making updates, as needed, as new metrics or requirements are identified\nDevelop functional and technical requirements for any changes impacting wData (Workiva Data)\nConfigure and unit test any changes impacting wData (connector, chains, tables, queries\nPromote wData changes",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['hive', 'python', 'oracle', 'azure data lake', 'informatica powercenter', 'azure synapse', 'microsoft azure', 'data warehousing', 'power bi', 'azure data factory', 'data engineering', 'sql server', 'sql', 'plsql', 'sql azure', 'spark', 'oracle adf', 'hadoop', 'etl', 'ssis', 'big data', 'informatica', 'unix']",2025-06-14 05:05:38
Data Engineer,BAY Area Technology Solutions,3 - 5 years,Not Disclosed,"['Mumbai', 'Delhi / NCR', 'Bengaluru']","Strong on programming languages like Python, Java\nMust have one cloud hands-on experience (GCP preferred)\nMust have: Experience working with Dockers\nMust have: Environments managing (e.g venv, pip, poetry, etc.)\nMust have: Experience with orchestrators like Vertex AI pipelines, Airflow, etc\nMust have: Data engineering, Feature Engineering techniques\nProficient in either Apache Spark or Apache Beam or Apache Flink\nMust have: Advance SQL knowledge\nMust be aware of Streaming concepts like Windowing , Late arrival , Triggers etc\nShould have hands-on experience on Distributed computing\nShould have working experience on Data Architecture design\nShould be aware of storage and compute options and when to choose what\nShould have good understanding on Cluster Optimisation/ Pipeline Optimisation strategies\nShould have exposure on GCP tools to develop end to end data pipeline for various scenarios (including ingesting data from traditional data bases as well as integration of API based data sources).\nShould have Business mindset to understand data and how it will be used for BI and Analytics purposes.\nShould have working experience on CI/CD pipelines, Deployment methodologies, Infrastructure as a code (eg. Terraform)\nGood to have, Hands-on experience on Kubernetes\nGood to have Vector based Database like Qdrant\nExperience in Working with GCP tools like:\nStorage : CloudSQL , Cloud Storage, Cloud Bigtable, Bigquery, Cloud Spanner, Cloud DataStore, Vector database\nIngest : Pub/Sub, Cloud Functions, AppEngine, Kubernetes Engine, Kafka, Micro services\nSchedule : Cloud Composer, Airflow\nProcessing: Cloud Dataproc, Cloud Dataflow, Apache Spark, Apache Flink\nCI/CD : Bitbucket+Jenkinjs / Gitlab ,Infrastructre as a tool : Terraform\nLocations : Mumbai, Delhi / NCR, Bengaluru , Kolkata, Chennai, Hyderabad, Ahmedabad, Pune, Remote",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Java', 'Docker', 'GCP', 'CI/CD', 'Poetry', 'Apache', 'Python']",2025-06-14 05:05:40
Data Engineer- MS Fabric,InfoCepts,5 - 10 years,Not Disclosed,"['Kolkata', 'Mumbai', 'New Delhi', 'Hyderabad', 'Pune', 'Chennai', 'Bengaluru']","Position: Data Engineer - MS Fabric\nPurpose of the Position: As an MS Fabric Data engineer you will be responsible for designing, implementing, and managing scalable data pipelines. Strong experience in implementation and management of lake House using MS Fabric Azure Tech stack (ADLS Gen2, ADF, Azure SQL) .\nProficiency in data integration techniques, ETL processes and data pipeline architectures. Well versed in Data Quality rules, principles and implementation.\nLocation: Bangalore/ Pune/ Nagpur/ Chennai\nType of Employment: FTE",,,,"['Performance tuning', 'Data modeling', 'Coding', 'XML', 'Scheduling', 'Data quality', 'JSON', 'Business intelligence', 'Data architecture', 'Python']",2025-06-14 05:05:42
"Senior Data Engineer ( T-SQL & SSIS,Data Warehousing & ETL Specialist)",Synechron,5 - 10 years,Not Disclosed,"['Hyderabad', 'Pune', 'Bengaluru']","Job Summary\nSynechron is seeking a highly skilled Senior Data Engineer specializing in T-SQL and SSIS to lead and advance our data integration and warehousing initiatives. In this role, you will design, develop, and optimize complex ETL processes and database solutions to support enterprise data needs. Your expertise will enable efficient data flow, ensure data integrity, and facilitate actionable insights, contributing to our organizations commitment to data-driven decision-making and operational excellence.\nSoftware Requirements\nOverall Responsibilities\nTechnical Skills (By Category)\nExperience Requirements\nDay-to-Day Activities\nQualifications\nProfessional Competencies",,,,"['Data Engineering', 'T-SQL', 'Azure Data Factory', 'query optimization', 'performance tuning', 'database security', 'AWS Glue', 'Data Warehousing', 'SSIS', 'ETL']",2025-06-14 05:05:45
Data Engineer,Supersourcing,4 - 8 years,Not Disclosed,"['Noida', 'Pune', 'Bengaluru']","Job Description-\nWe are hiring a Data Engineer with strong expertise in JAVA, Apache Spark and AWS Cloud. You will design and develop high-performance, scalable applications and data pipelines for cloud-based environments.\n\nKey Skills:\n3+ years in Java (Java 8+), Spring Boot, and REST APIs\n3+ years in Apache Spark (Core, SQL, Streaming)\nStrong hands-on with AWS services: S3, EC2, Lambda, Glue, EMR\nExperience with microservices, CI/CD, and Git\nGood understanding of distributed systems and performance tuning\n\nNice to Have:\nExperience with Kafka, Airflow, Docker, or Kubernetes\nAWS Certification (Developer/Architect)",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Java', 'Spark', 'Pyspark', 'SQL']",2025-06-14 05:05:47
Data Engineer - Ranchi,Teqfocus Consulting,0 - 2 years,Not Disclosed,['Ranchi'],"Job Title: Data Engineer\nExperience: 1+ Years (Freshers with relevant training & certification may apply)\nLocation: Ranchi (Work from Office)\n\nJob Summary:\nWe are looking for a Data Engineer with at least 1 year of hands-on experience in data engineering practices. The ideal candidate will work closely with our data and analytics teams to build robust and scalable data pipelines. Experience with Snowflake is a plus.\n\nKey Responsibilities:\nDesign, build, and maintain data pipelines using modern data engineering tools.\nTransform and clean data from multiple sources for reporting and analytics.\nOptimize data pipelines for performance and scalability.\nCollaborate with cross-functional teams including BI, analytics, and application developers.\nMonitor, troubleshoot, and maintain data workflows.\n\nRequired Skills:\nStrong understanding of data warehousing concepts.\nProficiency in SQL and Python.\nKnowledge of ETL tools and processes.\nFamiliarity with cloud platforms such as AWS, Snowflake, Databricks, Azure or GCP.\nExposure to data visualization tools is a plus.\n\nGood to have any of below certification:\nSnowflake SnowPro Core Certification\nSnowflake Advanced: Data Engineer Certification\nGoogle Cloud Professional Data Engineer\nMicrosoft Certified: Azure Data Engineer Associate\nAWS Certified Data Analytics Specialty\n\nQualifications:\nBachelor's degree in Computer Science, Information Technology, or related field.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Snowflake', 'AWS', 'Data Bricks', 'GCP', 'Microsoft Azure', 'Python']",2025-06-14 05:05:49
Data Engineer,Devdolphins,2 - 7 years,12-24 Lacs P.A.,['Bengaluru'],"Hiring Data Engineer (2+ yrs exp) in Bangalore for onsite client role. Strong in PySpark & Databricks. Exp in ETL/ELT, SQL, data lakes and Azure cloud. Full-time, in-office, immediate joiners only. Salary: 12L - 24L.\n\n\nHealth insurance\nProvident fund",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Pyspark', 'Data Bricks', 'SQL']",2025-06-14 05:05:51
Data Engineer,OSB India,3 - 5 years,Not Disclosed,['Bengaluru'],"Role & responsibilities :\n\n3+ years Data Engineering (SQL, Data warehousing) / ETL (SSIS) development experience is essential\n3 years data design experience in an MI / BI / Analytics environment (Kimball, lake house, data lake) is essential\n3 years experience of working in a structured Change Management project lifecycle is essential",,,,"['SSIS', 'SQL', 'MSBI', 'ETL']",2025-06-14 05:05:54
Data Engineer (C2H),First Mile Consulting,4 - 8 years,Not Disclosed,"['Hyderabad', 'Pune', 'Chennai', 'Bengaluru']","Very strong in python, pyspark and SQL. Good experience in any cloud . They use AWS but any cloud experience is ok. They will train on other things but if candidates have experience with ETL (like AWS Airflow), datalakes like Snowflake",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Temporary/Contractual","['Pyspark', 'Cloud', 'Data engineer', 'Python', 'Sql', 'Airflow']",2025-06-14 05:05:56
Data Engineer - Databricks,KPI Partners,0 - 4 years,Not Disclosed,['Pune'],"About KPI Partners.\nKPI Partners is a leading provider of data analytics solutions, dedicated to helping organizations transform data into actionable insights. Our innovative approach combines advanced technology with expert consulting, allowing businesses to leverage their data for improved performance and decision-making.\n\nJob Description.\nWe are seeking a skilled and motivated Data Engineer with experience in Databricks to join our dynamic team. The ideal candidate will be responsible for designing, building, and maintaining scalable data pipelines and data processing solutions that support our analytics initiatives. You will collaborate closely with data scientists, analysts, and other engineers to ensure the consistent flow of high-quality data across our platforms.",,,,"['Computer science', 'Data modeling', 'Analytical', 'Consulting', 'Data processing', 'Data quality', 'Information technology', 'Analytics', 'SQL', 'Python']",2025-06-14 05:05:58
Big Data Engineer,Amazon,3 - 8 years,Not Disclosed,['Chennai'],"Amazon Retail Financial Intelligence Systems is seeking a seasoned and talented Senior Data Engineer to join the Fortune Platform team. Fortune is a fast growing team with a mandate to build tools to automate profit-and-loss forecasting and planning for the Physical Consumer business. We are building the next generation Business Intelligence solutions using big data technologies such as Apache Spark, Hive/Hadoop, and distributed query engines. As a Data Engineer in Amazon, you will be working in a large, extremely complex and dynamic data environment. You should be passionate about working with big data and are able to learn new technologies rapidly and evaluate them critically. You should have excellent communication skills and be able to work with business owners to translate business requirements into system solutions. You are a self-starter, comfortable with ambiguity, and working in a fast-paced and ever-changing environment. Ideally, you are also experienced with at least one of the programming languages such as Java, C++, Spark/Scala, Python, etc.\n\nMajor Responsibilities:\nWork with a team of product and program managers, engineering leaders, and business leaders to build data architectures and platforms to support business\nDesign, develop, and operate high-scalable, high-performance, low-cost, and accurate data pipelines in distributed data processing platforms\nRecognize and adopt best practices in data processing, reporting, and analysis: data integrity, test design, analysis, validation, and documentation\nKeep up to date with big data technologies, evaluate and make decisions around the use of new or existing software products to design the data architecture\nDesign, build and own all the components of a high-volume data warehouse end to end.\nProvide end-to-end data engineering support for project lifecycle execution (design, execution and risk assessment)\nContinually improve ongoing reporting and analysis processes, automating or simplifying self-service support for customers\nInterface with other technology teams to extract, transform, and load (ETL) data from a wide variety of data sources\nOwn the functional and nonfunctional scaling of software systems in your ownership area.\nImplement big data solutions for distributed computing.\n\n\nAs a DE on our team, you will be responsible for leading the data modelling, database design, and launch of some of the core data pipelines. You will have significant influence on our overall strategy by helping define the data model, drive the database design, and spearhead the best practices to delivery high quality products.\n\nAbout the team\nProfit intelligence systems measures, predicts true profit(/loss) for each item as a result of a specific shipment to an Amazon customer. Profit Intelligence is all about providing intelligent ways for Amazon to understand profitability across retail business. What are the hidden factors driving the growth or profitability across millions of shipments each day\n\nWe compute the profitability of each and every shipment that gets shipped out of Amazon. Guess what, we predict the profitability of future possible shipments too. We are a team of agile, can-do engineers, who believe that not only are moon shots possible but that they can be done before lunch. All it takes is finding new ideas that challenge our preconceived notions of how things should be done. Process and procedure matter less than ideas and the practical work of getting stuff done. This is a place for exploring the new and taking risks.\n\nWe push the envelope in using cloud services in AWS as well as the latest in distributed systems, forecasting algorithms, and data mining. 3+ years of data engineering experience\nExperience with data modeling, warehousing and building ETL pipelines\nExperience with SQL Experience with AWS technologies like Redshift, S3, AWS Glue, EMR, Kinesis, FireHose, Lambda, and IAM roles and permissions\nExperience with non-relational databases / data stores (object storage, document or key-value stores, graph databases, column-family databases)",,,,"['C++', 'Database design', 'Risk assessment', 'Agile', 'Business intelligence', 'Data mining', 'Distribution system', 'SQL', 'Python', 'Data architecture']",2025-06-14 05:06:01
Data Engineer - Python Programming,Leading Client,5 - 7 years,Not Disclosed,['Jaipur'],"We are looking for a skilled Data Engineer with strong hands-on experience in Clickhouse, Kubernetes, SQL, Python, and FastAPI, along with a good understanding of PostgreSQL.\nThe ideal candidate will be responsible for building and maintaining efficient data pipelines, optimizing query performance, and developing APIs to support scalable data services.\n\n- Design, build, and maintain scalable and efficient data pipelines and ETL processes.\n\n- Develop and optimize Clickhouse databases for high-performance analytics.\n\n- Create RESTful APIs using FastAPI to expose data services.\n\n- Work with Kubernetes for container orchestration and deployment of data services.\n\n- Write complex SQL queries to extract, transform, and analyze data from PostgreSQL and Clickhouse.\n\n- Collaborate with data scientists, analysts, and backend teams to support data needs and ensure data quality.\n\n- Monitor, troubleshoot, and improve performance of data infrastructure.\n\n- Strong experience in Clickhouse - data modeling, query optimization, performance tuning.\n\n- Expertise in SQL - including complex joins, window functions, and optimization.\n\n- Proficient in Python, especially for data processing (Pandas, NumPy) and scripting.\n\n- Experience with FastAPI for creating lightweight APIs and microservices.\n\n- Hands-on experience with PostgreSQL - schema design, indexing, and performance.\n\n- Solid knowledge of Kubernetes managing containers, deployments, and scaling.\n\n- Understanding of software engineering best practices (CI/CD, version control, testing).\n\n- Experience with cloud platforms like AWS, GCP, or Azure.\n\n- Knowledge of data warehousing and distributed data systems.\n\n- Familiarity with Docker, Helm, and monitoring tools like Prometheus/Grafana.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'PostgreSQL', 'Data Modeling', 'Data Warehousing', 'Data Analytics', 'ETL', 'Python', 'SQL']",2025-06-14 05:06:03
Data Engineer - Python Programming,Leading Client,5 - 7 years,Not Disclosed,['Indore'],"We are looking for a skilled Data Engineer with strong hands-on experience in Clickhouse, Kubernetes, SQL, Python, and FastAPI, along with a good understanding of PostgreSQL.\nThe ideal candidate will be responsible for building and maintaining efficient data pipelines, optimizing query performance, and developing APIs to support scalable data services.\n\n- Design, build, and maintain scalable and efficient data pipelines and ETL processes.\n\n- Develop and optimize Clickhouse databases for high-performance analytics.\n\n- Create RESTful APIs using FastAPI to expose data services.\n\n- Work with Kubernetes for container orchestration and deployment of data services.\n\n- Write complex SQL queries to extract, transform, and analyze data from PostgreSQL and Clickhouse.\n\n- Collaborate with data scientists, analysts, and backend teams to support data needs and ensure data quality.\n\n- Monitor, troubleshoot, and improve performance of data infrastructure.\n\n- Strong experience in Clickhouse - data modeling, query optimization, performance tuning.\n\n- Expertise in SQL - including complex joins, window functions, and optimization.\n\n- Proficient in Python, especially for data processing (Pandas, NumPy) and scripting.\n\n- Experience with FastAPI for creating lightweight APIs and microservices.\n\n- Hands-on experience with PostgreSQL - schema design, indexing, and performance.\n\n- Solid knowledge of Kubernetes managing containers, deployments, and scaling.\n\n- Understanding of software engineering best practices (CI/CD, version control, testing).\n\n- Experience with cloud platforms like AWS, GCP, or Azure.\n\n- Knowledge of data warehousing and distributed data systems.\n\n- Familiarity with Docker, Helm, and monitoring tools like Prometheus/Grafana.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'PostgreSQL', 'Data Modeling', 'Data Warehousing', 'Data Analytics', 'ETL', 'Python', 'SQL']",2025-06-14 05:06:06
Data Engineer - Python Programming,Leading Client,5 - 7 years,Not Disclosed,['Surat'],"Job Description :\n\nWe are looking for a skilled Data Engineer with strong hands-on experience in Clickhouse, Kubernetes, SQL, Python, and FastAPI, along with a good understanding of PostgreSQL.\nThe ideal candidate will be responsible for building and maintaining efficient data pipelines, optimizing query performance, and developing APIs to support scalable data services.\n\n- Design, build, and maintain scalable and efficient data pipelines and ETL processes.\n\n- Develop and optimize Clickhouse databases for high-performance analytics.\n\n- Create RESTful APIs using FastAPI to expose data services.\n\n- Work with Kubernetes for container orchestration and deployment of data services.\n\n- Write complex SQL queries to extract, transform, and analyze data from PostgreSQL and Clickhouse.\n\n- Collaborate with data scientists, analysts, and backend teams to support data needs and ensure data quality.\n\n- Monitor, troubleshoot, and improve performance of data infrastructure.\n\n- Strong experience in Clickhouse - data modeling, query optimization, performance tuning.\n\n- Expertise in SQL - including complex joins, window functions, and optimization.\n\n- Proficient in Python, especially for data processing (Pandas, NumPy) and scripting.\n\n- Experience with FastAPI for creating lightweight APIs and microservices.\n\n- Hands-on experience with PostgreSQL - schema design, indexing, and performance.\n\n- Solid knowledge of Kubernetes managing containers, deployments, and scaling.\n\n- Understanding of software engineering best practices (CI/CD, version control, testing).\n\n- Experience with cloud platforms like AWS, GCP, or Azure.\n\n- Knowledge of data warehousing and distributed data systems.\n\n- Familiarity with Docker, Helm, and monitoring tools like Prometheus/Grafana.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Python Programming', 'Data Engineering', 'PostgreSQL', 'Data Modeling', 'Data Warehousing', 'Data Analytics', 'ETL', 'Python', 'SQL']",2025-06-14 05:06:08
Data Engineer - Python Programming,Leading Client,5 - 7 years,Not Disclosed,['Chennai'],"We are looking for a skilled Data Engineer with strong hands-on experience in Clickhouse, Kubernetes, SQL, Python, and FastAPI, along with a good understanding of PostgreSQL.\nThe ideal candidate will be responsible for building and maintaining efficient data pipelines, optimizing query performance, and developing APIs to support scalable data services.\n\n- Design, build, and maintain scalable and efficient data pipelines and ETL processes.\n\n- Develop and optimize Clickhouse databases for high-performance analytics.\n\n- Create RESTful APIs using FastAPI to expose data services.\n\n- Work with Kubernetes for container orchestration and deployment of data services.\n\n- Write complex SQL queries to extract, transform, and analyze data from PostgreSQL and Clickhouse.\n\n- Collaborate with data scientists, analysts, and backend teams to support data needs and ensure data quality.\n\n- Monitor, troubleshoot, and improve performance of data infrastructure.\n\n- Strong experience in Clickhouse - data modeling, query optimization, performance tuning.\n\n- Expertise in SQL - including complex joins, window functions, and optimization.\n\n- Proficient in Python, especially for data processing (Pandas, NumPy) and scripting.\n\n- Experience with FastAPI for creating lightweight APIs and microservices.\n\n- Hands-on experience with PostgreSQL - schema design, indexing, and performance.\n\n- Solid knowledge of Kubernetes managing containers, deployments, and scaling.\n\n- Understanding of software engineering best practices (CI/CD, version control, testing).\n\n- Experience with cloud platforms like AWS, GCP, or Azure.\n\n- Knowledge of data warehousing and distributed data systems.\n\n- Familiarity with Docker, Helm, and monitoring tools like Prometheus/Grafana.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Python Programming', 'Data Engineering', 'PostgreSQL', 'Data Modeling', 'Data Warehousing', 'Data Analytics', 'ETL', 'Python', 'SQL']",2025-06-14 05:06:12
Data Engineer - Python Programming,Leading Client,5 - 7 years,Not Disclosed,['Noida'],"Job Description :\n\nWe are looking for a skilled Data Engineer with strong hands-on experience in Clickhouse, Kubernetes, SQL, Python, and FastAPI, along with a good understanding of PostgreSQL.\nThe ideal candidate will be responsible for building and maintaining efficient data pipelines, optimizing query performance, and developing APIs to support scalable data services.\n\n- Design, build, and maintain scalable and efficient data pipelines and ETL processes.\n\n- Develop and optimize Clickhouse databases for high-performance analytics.\n\n- Create RESTful APIs using FastAPI to expose data services.\n\n- Work with Kubernetes for container orchestration and deployment of data services.\n\n- Write complex SQL queries to extract, transform, and analyze data from PostgreSQL and Clickhouse.\n\n- Collaborate with data scientists, analysts, and backend teams to support data needs and ensure data quality.\n\n- Monitor, troubleshoot, and improve performance of data infrastructure.\n\n- Strong experience in Clickhouse - data modeling, query optimization, performance tuning.\n\n- Expertise in SQL - including complex joins, window functions, and optimization.\n\n- Proficient in Python, especially for data processing (Pandas, NumPy) and scripting.\n\n- Experience with FastAPI for creating lightweight APIs and microservices.\n\n- Hands-on experience with PostgreSQL - schema design, indexing, and performance.\n\n- Solid knowledge of Kubernetes managing containers, deployments, and scaling.\n\n- Understanding of software engineering best practices (CI/CD, version control, testing).\n\n- Experience with cloud platforms like AWS, GCP, or Azure.\n\n- Knowledge of data warehousing and distributed data systems.\n\n- Familiarity with Docker, Helm, and monitoring tools like Prometheus/Grafana.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Python Programming', 'Data Engineering', 'PostgreSQL', 'Data Modeling', 'Data Warehousing', 'Data Analytics', 'ETL', 'Python', 'SQL']",2025-06-14 05:06:14
Data Engineer - Python Programming,Leading Client,5 - 7 years,Not Disclosed,['Lucknow'],"We are looking for a skilled Data Engineer with strong hands-on experience in Clickhouse, Kubernetes, SQL, Python, and FastAPI, along with a good understanding of PostgreSQL.\nThe ideal candidate will be responsible for building and maintaining efficient data pipelines, optimizing query performance, and developing APIs to support scalable data services.\n\n- Design, build, and maintain scalable and efficient data pipelines and ETL processes.\n\n- Develop and optimize Clickhouse databases for high-performance analytics.\n\n- Create RESTful APIs using FastAPI to expose data services.\n\n- Work with Kubernetes for container orchestration and deployment of data services.\n\n- Write complex SQL queries to extract, transform, and analyze data from PostgreSQL and Clickhouse.\n\n- Collaborate with data scientists, analysts, and backend teams to support data needs and ensure data quality.\n\n- Monitor, troubleshoot, and improve performance of data infrastructure.\n\n- Strong experience in Clickhouse - data modeling, query optimization, performance tuning.\n\n- Expertise in SQL - including complex joins, window functions, and optimization.\n\n- Proficient in Python, especially for data processing (Pandas, NumPy) and scripting.\n\n- Experience with FastAPI for creating lightweight APIs and microservices.\n\n- Hands-on experience with PostgreSQL - schema design, indexing, and performance.\n\n- Solid knowledge of Kubernetes managing containers, deployments, and scaling.\n\n- Understanding of software engineering best practices (CI/CD, version control, testing).\n\n- Experience with cloud platforms like AWS, GCP, or Azure.\n\n- Knowledge of data warehousing and distributed data systems.\n\n- Familiarity with Docker, Helm, and monitoring tools like Prometheus/Grafana.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Python Programming', 'Data Engineering', 'PostgreSQL', 'Data Modeling', 'Data Warehousing', 'Data Analytics', 'ETL', 'Python', 'SQL']",2025-06-14 05:06:17
Data Engineer - Python Programming,Leading Client,5 - 7 years,Not Disclosed,['Hyderabad'],"We are looking for a skilled Data Engineer with strong hands-on experience in Clickhouse, Kubernetes, SQL, Python, and FastAPI, along with a good understanding of PostgreSQL.\nThe ideal candidate will be responsible for building and maintaining efficient data pipelines, optimizing query performance, and developing APIs to support scalable data services.\n\n- Design, build, and maintain scalable and efficient data pipelines and ETL processes.\n\n- Develop and optimize Clickhouse databases for high-performance analytics.\n\n- Create RESTful APIs using FastAPI to expose data services.\n\n- Work with Kubernetes for container orchestration and deployment of data services.\n\n- Write complex SQL queries to extract, transform, and analyze data from PostgreSQL and Clickhouse.\n\n- Collaborate with data scientists, analysts, and backend teams to support data needs and ensure data quality.\n\n- Monitor, troubleshoot, and improve performance of data infrastructure.\n\n- Strong experience in Clickhouse - data modeling, query optimization, performance tuning.\n\n- Expertise in SQL - including complex joins, window functions, and optimization.\n\n- Proficient in Python, especially for data processing (Pandas, NumPy) and scripting.\n\n- Experience with FastAPI for creating lightweight APIs and microservices.\n\n- Hands-on experience with PostgreSQL - schema design, indexing, and performance.\n\n- Solid knowledge of Kubernetes managing containers, deployments, and scaling.\n\n- Understanding of software engineering best practices (CI/CD, version control, testing).\n\n- Experience with cloud platforms like AWS, GCP, or Azure.\n\n- Knowledge of data warehousing and distributed data systems.\n\n- Familiarity with Docker, Helm, and monitoring tools like Prometheus/Grafana.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Python', 'Data Engineering', 'GCP', 'PostgreSQL', 'Clickhouse', 'Data Modeling', 'Data Warehousing', 'ETL', 'AWS', 'SQL', 'Kubernetes']",2025-06-14 05:06:19
Lead Data Engineer,Acuity Knowledge Partners,8 - 13 years,Not Disclosed,"['Pune', 'Gurugram', 'Bengaluru']","Preferred candidate profile\n\n9+ years of overall experience in software development with a focus on data projects using Python, PySpark, and associated frameworks.\nProven experience as a Data Engineer with experience in Azure cloud.\nExperience implementing solutions using Azure cloud services, Azure Data Factory, Azure Lake Gen 2, Azure Databases, Azure Data Fabric, API Gateway management, Azure Functions.",,,,"['Pyspark', 'Azure Cloud', 'Python', 'SQL']",2025-06-14 05:06:21
Data Engineer - Python Programming,Leading Client,5 - 7 years,Not Disclosed,['Kolkata'],"Job Description :\n\nWe are looking for a skilled Data Engineer with strong hands-on experience in Clickhouse, Kubernetes, SQL, Python, and FastAPI, along with a good understanding of PostgreSQL.\nThe ideal candidate will be responsible for building and maintaining efficient data pipelines, optimizing query performance, and developing APIs to support scalable data services.\n\n- Design, build, and maintain scalable and efficient data pipelines and ETL processes.\n\n- Develop and optimize Clickhouse databases for high-performance analytics.\n\n- Create RESTful APIs using FastAPI to expose data services.\n\n- Work with Kubernetes for container orchestration and deployment of data services.\n\n- Write complex SQL queries to extract, transform, and analyze data from PostgreSQL and Clickhouse.\n\n- Collaborate with data scientists, analysts, and backend teams to support data needs and ensure data quality.\n\n- Monitor, troubleshoot, and improve performance of data infrastructure.\n\n- Strong experience in Clickhouse - data modeling, query optimization, performance tuning.\n\n- Expertise in SQL - including complex joins, window functions, and optimization.\n\n- Proficient in Python, especially for data processing (Pandas, NumPy) and scripting.\n\n- Experience with FastAPI for creating lightweight APIs and microservices.\n\n- Hands-on experience with PostgreSQL - schema design, indexing, and performance.\n\n- Solid knowledge of Kubernetes managing containers, deployments, and scaling.\n\n- Understanding of software engineering best practices (CI/CD, version control, testing).\n\n- Experience with cloud platforms like AWS, GCP, or Azure.\n\n- Knowledge of data warehousing and distributed data systems.\n\n- Familiarity with Docker, Helm, and monitoring tools like Prometheus/Grafana.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Python Programming', 'Data Engineering', 'PostgreSQL', 'Data Modeling', 'Data Warehousing', 'Data Analytics', 'ETL', 'Python', 'SQL']",2025-06-14 05:06:25
Data Engineer - Python Programming,Leading Client,5 - 7 years,Not Disclosed,['Kanpur'],"We are looking for a skilled Data Engineer with strong hands-on experience in Clickhouse, Kubernetes, SQL, Python, and FastAPI, along with a good understanding of PostgreSQL.\nThe ideal candidate will be responsible for building and maintaining efficient data pipelines, optimizing query performance, and developing APIs to support scalable data services.\n\n- Design, build, and maintain scalable and efficient data pipelines and ETL processes.\n\n- Develop and optimize Clickhouse databases for high-performance analytics.\n\n- Create RESTful APIs using FastAPI to expose data services.\n\n- Work with Kubernetes for container orchestration and deployment of data services.\n\n- Write complex SQL queries to extract, transform, and analyze data from PostgreSQL and Clickhouse.\n\n- Collaborate with data scientists, analysts, and backend teams to support data needs and ensure data quality.\n\n- Monitor, troubleshoot, and improve performance of data infrastructure.\n\n- Strong experience in Clickhouse - data modeling, query optimization, performance tuning.\n\n- Expertise in SQL - including complex joins, window functions, and optimization.\n\n- Proficient in Python, especially for data processing (Pandas, NumPy) and scripting.\n\n- Experience with FastAPI for creating lightweight APIs and microservices.\n\n- Hands-on experience with PostgreSQL - schema design, indexing, and performance.\n\n- Solid knowledge of Kubernetes managing containers, deployments, and scaling.\n\n- Understanding of software engineering best practices (CI/CD, version control, testing).\n\n- Experience with cloud platforms like AWS, GCP, or Azure.\n\n- Knowledge of data warehousing and distributed data systems.\n\n- Familiarity with Docker, Helm, and monitoring tools like Prometheus/Grafana.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Python', 'Data Engineering', 'PostgreSQL', 'Clickhouse', 'Data Modeling', 'Data Warehousing', 'ETL', 'Kubernetes', 'SQL']",2025-06-14 05:06:28
Data Engineer,Ortseam Technologies,5 - 10 years,Not Disclosed,[],"Job Description\n\nJob Title: Offshore Data Engineer\nBase Location: Bangalore\nWork Mode: Remote\nExperience: 5+ Years\n\nJob Description: We are looking for a skilled Offshore Data Engineer with strong experience in Python, SQL, and Apache Beam. Familiarity with Java is a plus. The ideal candidate should be self-driven, collaborative, and able to work in a fast-paced environment.\n\nKey Responsibilities:\n\nDesign and implement reusable, scalable ETL frameworks using Apache Beam and GCP Dataflow.\nDevelop robust data ingestion and transformation pipelines using Python and SQL.\nIntegrate Kafka for real-time data streams alongside batch workloads.\nOptimize pipeline performance and manage costs within GCP services.\nWork closely with data analysts, data architects, and product teams to gather and understand data requirements.\nManage and monitor BigQuery datasets, tables, and partitioning strategies.\nImplement error handling, resiliency, and observability mechanisms across pipeline components.\nCollaborate with DevOps teams to enable automated delivery (CI/CD) for data pipeline components.\n\nRequired Skills:\n\n5+ years of hands-on experience in Data Engineering or Software Engineering.\nProficiency in Python and SQL.\nGood understanding of Java (for reading or modifying codebases).\nExperience building ETL pipelines with Apache Beam and Google Cloud Dataflow.\nHands-on experience with Apache Kafka for stream processing.\nSolid understanding of BigQuery and data modeling on GCP.\nExperience with GCP services (Cloud Storage, Pub/Sub, Cloud Compose, etc.).\n\nGood to Have:\n\nExperience building reusable ETL libraries or framework components.\nKnowledge of data governance, data quality checks, and pipeline observability.\nFamiliarity with Apache Airflow or Cloud Composer for orchestration.\nExposure to CI/CD practices in a cloud-native environment (Docker, Terraform, etc.).\n\nTech stack: \n\nPython, SQL, Java, GCP (BigQuery, Pub/Sub, Cloud Storage, Cloud Compose, Dataflow), Apache Beam, Apache Kafka, Apache Airflow, CI/CD (Docker, Terraform)",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Bigquery', 'Kafka', 'SQL', 'GCP', 'Python', 'Airflow', 'Java', 'Pubsub', 'Data Engineering', 'Gcp Dataflow', 'Cloud Storage', 'Data Pipeline', 'Data Flow', 'Cicd Pipeline', 'Apache Beam', 'Google Cloud Platforms']",2025-06-14 05:06:30
Data Engineer (5-10 Years) | @ Banking | Bangalore & Mumbai,Net Connect,5 - 10 years,Not Disclosed,"['Bengaluru', 'Mumbai (All Areas)']","Job Summary\nWe are seeking a skilled Data Engineer to design, develop, and optimize scalable data pipelines and infrastructure. The ideal candidate will have expertise in relational databases, data modeling, cloud migration, and automation, working closely with cross-functional teams to drive data-driven decision-making.\n\nKey Responsibilities",,,,"['Data Modeling', 'ETL', 'Scripting', 'SQL', 'Azure Data Factory', 'Informatica']",2025-06-14 05:06:32
Senior Data Engineer- (Big data and Data Pipelines),Findem,5 - 9 years,Not Disclosed,"['New Delhi', 'Bengaluru']","What is Findem:\n\nFindem is the only talent data platform that combines 3D data with AI. It automates and consolidates top-of-funnel activities across your entire talent ecosystem, bringing together sourcing, CRM, and analytics into one place. Only 3D data connects people and company data over time - making an individual s entire career instantly accessible in a single click, removing the guesswork, and unlocking insights about the market and your competition no one else can. Powered by 3D data, Findem s automated workflows across the talent lifecycle are the ultimate competitive advantage. Enabling talent teams to deliver continuous pipelines of top, diverse candidates while creating better talent experiences, Findem transforms the way companies plan, hire, and manage talent. Learn more at www.findem.ai\n\nExperience - 5 - 9 years\n\nWe are looking for an experienced Big Data Engineer, who will be responsible for building, deploying and managing various data pipelines, data lake and Big data processing solutions using Big data and ETL technologies.\n\nLocation- Delhi, India\nHybrid- 3 days onsite\nResponsibilities\nBuild data pipelines, Big data processing solutions and data lake infrastructure using various Big data and ETL technologies\nAssemble and process large, complex data sets that meet functional non-functional business requirements ETL from a wide variety of sources like MongoDB, S3, Server-to-Server, Kafka etc., and processing using SQL and big data technologies\nBuild analytical tools to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics Build interactive and ad-hoc query self-serve tools for analytics use cases\nBuild data models and data schema for performance, scalability and functional requirement perspective Build processes supporting data transformation, metadata, dependency and workflow management\nResearch, experiment and prototype new tools/technologies and make them successful\nSkill Requirements\nMust have-Strong in Python/Scala\nMust have experience in Big data technologies like Spark, Hadoop, Athena / Presto, Redshift, Kafka etc\nExperience in various file formats like parquet, JSON, Avro, orc etc\nExperience in workflow management tools like airflow Experience with batch processing, streaming and message queues\nAny of visualization tools like Redash, Tableau, Kibana etc\nExperience in working with structured and unstructured data sets\nStrong problem solving skills\nGood to have\nExposure to NoSQL like MongoDB\nExposure to Cloud platforms like AWS, GCP, etc\nExposure to Microservices architecture\nExposure to Machine learning techniques\nThe role is full-time and comes with full benefits. We are globally headquartered in the San Francisco Bay Area with our India headquarters in Bengaluru.\n\nEqual Opportunity",Industry Type: Management Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SAN', 'metadata', 'Prototype', 'Machine learning', 'Schema', 'JSON', 'Analytics', 'SQL', 'CRM', 'Python']",2025-06-14 05:06:34
Data Engineer-Pyspark,A leading technology services and consul...,5 - 10 years,Not Disclosed,"['Pune', 'Chennai', 'Bengaluru']","Our client is Global IT Service & Consulting Organization\nExp-5+ yrs\n\nSkil Apache Spark\n\nLocation- Bangalore, Hyderabad, Pune, Chennai, Coimbatore, Gr. Noida\n\n\nExcellent Knowledge on Spark; The professional must have a thorough understanding Spark framework, Performance Tuning etc\nExcellent Knowledge and hands-on experience of at least 4+ years in Scala or PySpark\nExcellent Knowledge of the Hadoop eco System- Knowledge of Hive mandatory\nStrong Unix and Shell Scripting Skills\nExcellent Inter-personal skills and for experienced candidates Excellent leadership skills\nMandatory for anyone to have Good knowledge of any of the CSPs like Azure,AWS or GCP; Certifications on Azure will be additional Pl",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Pyspark', 'Data Engineering', 'Cloud', 'Microsoft Azure', 'Python', 'GCP', 'data engineer', 'Hadoop', 'AWS']",2025-06-14 05:06:36
Data Engineer,Hunch,2 - 7 years,Not Disclosed,['New Delhi'],"What is Hunch\nHunch is a dating app that helps you land a date without swiping like a junkie. Designed for people tired of mindless swiping and commodified matchmaking, Hunch leverages a powerful AI-engine to help users find meaningful connections by focusing on personality over just looks. With 2M+ downloads and a 4.4-star rating, Hunch is going viral in the US by challenging the swipe-left/right norm of traditional apps. Hunch is a Series A funded ($23 Million) startup building the future of social discovery in a post-AI world.\n\nLink to our fundraising announcement\n\nKey offerings of Hunch:\n\n\nSwipe Less, Vibe More: Curated profiles, cutting the clutter of endless swiping.\n\nPersonality Matters: Opinion-based, belief-based, and thought-based compatibility rather than just focusing on looks.\n\nEvery Match, Verified: No bots, no catfishing just real, trustworthy connections\n\nMatch Scores: Our AI shows compatibility percentages, helping users identify their 100% vibe match.\n\n\nWere looking for a highly motivated and skilled Data Engineer. Youll design, build, and optimize our robust data infrastructure. Youll also develop scalable data pipelines, ensure data quality, and collaborate closely with our machine learning teams. Were looking for someone passionate about data who thrives in a dynamic environment. If you enjoy tackling complex challenges with cutting-edge technologies, we encourage you to apply.\n\nWhat Youll Do:\n\nArchitect Optimize Data Infrastructure: Design, implement, and maintain highly scalable data infrastructure. This includes processes for auto-scaling and easy maintainability of our data pipelines.\n\nDevelop Deploy Data Pipelines: Lead the design, implementation, testing, and deployment of resilient data pipelines. These pipelines will ingest, transform, and process large datasets efficiently.\n\nEmpower ML Workflows: Partner with Machine Learning Engineers to understand their specific data needs. This includes providing high-quality data for model training and ensuring low-latency data delivery for real-time inference. Ensure seamless data flow and efficient integration with ML models.\n\nEnsure Data Integrity: Establish and enforce robust systems and processes. These will ensure comprehensive data quality assurance, validation, and reliability across the entire data lifecycle.\n\n\nWhat Youll Bring:\n\nExperience: A minimum of 2+ years of professional experience in data engineering. You should have a proven track record of delivering solutions in a production environment.\n\nData Storage Expertise: Hands-on experience with relational databases (e.g., PostgreSQL, MySQL, Redshift) and cloud object storage (e.g., S3) is required. Experience with distributed file systems (e.g., HDFS) and NoSQL databases is a plus.\n\nBig Data Processing: Demonstrated proficiency with big data processing platforms and frameworks. Examples include Hadoop, Spark, Hive, Presto, and Trino.\n\nPipeline Orchestration Messaging: Practical experience with key data pipeline tools. This includes message queues (e.g., Kafka, Kinesis), workflow orchestrators (e.g., dbt, Airflow), change data capture (e.g., Debezium), and ETL services (e.g., AWS Glue ETL).\n\nProgramming Prowess: Strong programming skills in Python and SQL are essential. Proficiency in at least one JVM-based language (e.g., Java, Scala) is also required.\n\nML Acumen: A solid understanding of machine learning workflows. This includes data preparation and feature engineering concepts.\n\nInnovation Agility: You should be a creative problem-solver. Youll need a proactive approach to experimenting with new technologies.\n\n\nWhat we have to offer\n\nCompetitive financial rewards + annual PLI (Performance Linked Incentives).\n\nMeritocracy-driven, candid, and diverse culture.\n\nEmployee benefits like Medical Insurance\n\nOne annual all expenses paid by company trip for all employees to bond\n\nAlthough we work from our office in New Delhi, we are flexible in our style and approach\n\n\nLife @Hunch\n\nWork Culture: At Hunch we take our work seriously but don t take ourselves too seriously. Everyone is encouraged to think as owners and not renters, and we prefer to let builders build, empowering people to pursue independent ideas.\n\nImpact: Your work will shape the future of social engagement and connect people around the world.\n\nCollaboration: Join a diverse team of creative minds and be part of a supportive community.\n\nGrowth: We invest in your development and provide opportunities for continuous learning.\n\nBacked by Global Investors: Hunch is a Series A funded startup, backed by Hashed, AlphaWave, Brevan Howard and Polygon Studios\n\nExperienced Leadership: Hunch is founded by a trio of industry veterans - Ish Goel (CEO), Nitika Goel (CTO), and Kartic Rakhra (CMO) - serial entrepreneurs with the last exit from Nexus Mutual, a web3 consumer-tech startup.",Industry Type: Internet,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Manager Quality Assurance', 'NoSQL', 'Postgresql', 'MySQL', 'Machine learning', 'Data processing', 'Data quality', 'SQL', 'Python']",2025-06-14 05:06:38
Senior - AWS Data Engineering,KPMG India,4 - 8 years,Not Disclosed,['Gurugram'],"KPMG India is looking for Senior - AWS Data Engineering to join our dynamic team and embark on a rewarding career journey Designs and builds scalable data pipelines using AWS servicesOptimizes data ingestion, storage, and processingCollaborates with data scientists and analystsEnsures performance, security, and compliance",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Networking', 'Focus', 'Manager Technology', 'professional services', 'AWS', 'international clients']",2025-06-14 05:06:40
Data Engineer Sr. Analyst,Accenture,2 - 3 years,Not Disclosed,['Kochi'],"Job Title - + +\n\n\n\nManagement Level:\n\n\n\nLocation:Kochi, Coimbatore, Trivandrum\n\n\n\nMust have skills:Python/Scala, Pyspark/Pytorch\n\n\n\n\nGood to have skills:Redshift\n\n\n\nJob\n\n\nSummary\n\nYoull capture user requirements and translate them into business and digitally enabled solutions across a range of industries. Your responsibilities will include:\n\n\n\nRoles and Responsibilities\nDesigning, developing, optimizing, and maintaining data pipelines that adhere to ETL principles and business goals\nSolving complex data problems to deliver insights that helps our business to achieve their goals.\nSource data (structured unstructured) from various touchpoints, format and organize them into an analyzable format.\nCreating data products for analytics team members to improve productivity\nCalling of AI services like vision, translation etc. to generate an outcome that can be used in further steps along the pipeline.\nFostering a culture of sharing, re-use, design and operational efficiency of data and analytical solutions\nPreparing data to create a unified database and build tracking solutions ensuring data quality\nCreate Production grade analytical assets deployed using the guiding principles of CI/CD.\n\n\nProfessional and Technical Skills\nExpert in Python, Scala, Pyspark, Pytorch, Javascript (any 2 at least)\nExtensive experience in data analysis (Big data- Apache Spark environments), data libraries (e.g. Pandas, SciPy, Tensorflow, Keras etc.), and SQL. 2-3 years of hands-on experience working on these technologies.\nExperience in one of the many BI tools such as Tableau, Power BI, Looker.\nGood working knowledge of key concepts in data analytics, such as dimensional modeling, ETL, reporting/dashboarding, data governance, dealing with structured and unstructured data, and corresponding infrastructure needs.\nWorked extensively in Microsoft Azure (ADF, Function Apps, ADLS, Azure SQL), AWS (Lambda,Glue,S3), Databricks analytical platforms/tools, Snowflake Cloud Datawarehouse.\n\n\n\n\nAdditional Information\nExperience working in cloud Data warehouses like Redshift or Synapse\nCertification in any one of the following or equivalent\nAWS- AWS certified data Analytics- Speciality\nAzure- Microsoft certified Azure Data Scientist Associate\nSnowflake- Snowpro core- Data Engineer\nDatabricks Data Engineering\n\nQualification\n\n\n\nExperience:3.5 -5 years of experience is required\n\n\n\n\nEducational Qualification:Graduation (Accurate educational details should capture)",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['scala', 'pyspark', 'pytorch', 'python', 'microsoft azure', 'glue', 'amazon redshift', 'sql', 'tensorflow', 'sql azure', 'spark', 'keras', 'big data', 'etl', 'scipy', 'snowflake', 'data analysis', 'azure data lake', 'power bi', 'data engineering', 'javascript', 'pandas', 'data bricks', 'tableau', 'lambda expressions', 'aws']",2025-06-14 05:06:43
Data Engineer,Konrad Group,3 - 8 years,Not Disclosed,['Gurugram'],"About The Role\nAs a Data Engineer you ll be tasked with designing, building, and maintaining scalable data platforms and pipelines. Your deep knowledge of data platforms such as Azure Fabric, Databricks, and Snowflake will be essential as you collaborate closely with data analysts, scientists, and other engineers to ensure reliable, secure, and efficient data solutions.\nWhat You ll Do\nDesign, build, and manage robust data pipelines and data architectures.\nImplement solutions leveraging platforms such as Azure Fabric, Databricks, and Snowflake.\nOptimize data workflows, ensuring reliability, scalability, and performance.\nCollaborate with internal stakeholders to understand data needs and deliver tailored solutions.\nEnsure data security and compliance with industry standards and best practices.\nPerform data modelling, data extraction, transformation, and loading (ETL/ELT).\nIdentify and recommend innovative solutions to enhance data quality and analytics capabilities.\nQualifications\nBachelor s degree or higher in Computer Science, Data Engineering, Information Technology, or a related field.\nAt least 3 years of professional experience as a Data Engineer or similar role.\nProficiency in data platforms such as Azure Fabric, Databricks, and Snowflake.\nHands-on experience with data pipeline tools, cloud services, and storage solutions.\nStrong programming skills in SQL, Python, or related languages.\nExperience with big data technologies and concepts (Spark, Hadoop, Kafka).\nExcellent analytical, troubleshooting, and problem-solving skills.\nAbility to effectively communicate technical concepts clearly to non-technical stakeholders.\nNice to have\nCertifications related to Azure Data Engineering, Databricks, or Snowflake.\nFamiliarity with DevOps practices and CI/CD pipelines.\nPerks and Benefits\nComprehensive Health Wellness Benefits Package\nSocials, Outings Retreats\nRetirement Planning\nParental Leave Program\nCulture of Learning Development\nFlexible Working Hours\nWork from Home Flexibility\nService Recognition Programs",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Analytical', 'Genetics', 'Data quality', 'Troubleshooting', 'Information technology', 'Analytics', 'SQL', 'Python', 'Data extraction']",2025-06-14 05:06:45
Data Engineer - Python Programming,Leading Client,5 - 7 years,Not Disclosed,['Pune'],"We are looking for a skilled Data Engineer with strong hands-on experience in Clickhouse, Kubernetes, SQL, Python, and FastAPI, along with a good understanding of PostgreSQL.\nThe ideal candidate will be responsible for building and maintaining efficient data pipelines, optimizing query performance, and developing APIs to support scalable data services.\n\n- Design, build, and maintain scalable and efficient data pipelines and ETL processes.\n\n- Develop and optimize Clickhouse databases for high-performance analytics.\n\n- Create RESTful APIs using FastAPI to expose data services.\n\n- Work with Kubernetes for container orchestration and deployment of data services.\n\n- Write complex SQL queries to extract, transform, and analyze data from PostgreSQL and Clickhouse.\n\n- Collaborate with data scientists, analysts, and backend teams to support data needs and ensure data quality.\n\n- Monitor, troubleshoot, and improve performance of data infrastructure.\n\n- Strong experience in Clickhouse - data modeling, query optimization, performance tuning.\n\n- Expertise in SQL - including complex joins, window functions, and optimization.\n\n- Proficient in Python, especially for data processing (Pandas, NumPy) and scripting.\n\n- Experience with FastAPI for creating lightweight APIs and microservices.\n\n- Hands-on experience with PostgreSQL - schema design, indexing, and performance.\n\n- Solid knowledge of Kubernetes managing containers, deployments, and scaling.\n\n- Understanding of software engineering best practices (CI/CD, version control, testing).\n\n- Experience with cloud platforms like AWS, GCP, or Azure.\n\n- Knowledge of data warehousing and distributed data systems.\n\n- Familiarity with Docker, Helm, and monitoring tools like Prometheus/Grafana.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'PostgreSQL', 'Data Modeling', 'Data Warehousing', 'Data Analytics', 'ETL', 'Python', 'SQL']",2025-06-14 05:06:47
Senior Data Engineer,Ford,3 - 12 years,Not Disclosed,['Chennai'],"The Service360 Senior Data Engineer will be the trusted data advisor in GDI&A (Global Data Insights & Analytics) supporting the following teams: Ford Pro, FCSA (Ford Customer Service Analytics) and FCSD (Ford Customer Service Division) Business. This is an exciting opportunity that provides the Data Engineer a well-rounded experience. The position requires translation of the customer s Analytical needs into specific data products that should be built in the GCP environment by collaborating with the Product Owners, Technical Anchor and the Customers\n\nExperience working in GCP native (or equivalent) services like Big Query, Google Cloud Storage, PubSub, Dataflow, Dataproc etc.\n\nExperience working with Airflow for scheduling and orchestration of data pipelines.\n\nExperience working with Terraform to provision Infrastructure as Code.\n\n2 + years professional development experience in Java or Python.\n\nBachelor s degree in computer science or related scientific field.\n\nExperience in analysing complex data, organizing raw data, and integrating massive datasets from multiple data sources to build analytical domains and reusable data products.\n\nExperience in working with architects to evaluate and productionalize data pipelines for data ingestion, curation, and consumption.\n\nExperience in working with stakeholders to formulate business problems as technical data requirements, identify and implement technical solutions while ensuring key business drivers are captured in collaboration with product management.\n\n\nWork on a small agile team to deliver curated data products for the Product Organization.\n\nWork effectively with fellow data engineers, product owners, data champions and other technical experts.\n\nMinimum of 5 years of experience with progressive responsibilities in software development\n\nMinimum of 3 years of experience defining product vision, strategy, product roadmaps and creating and managing backlogs\n\nExperience wrangling, transforming and visualizing large data sets from multiple sources, using a variety of tools\n\nProficiency in SQL is a must have skill\n\nExcellent written and verbal communication skills\n\nMust be comfortable presenting to and interacting with cross-functional teams and customers\n\nDemonstrate technical knowledge and communication skills with the ability to advocate for well-designed solutions.\n\nDevelop exceptional analytical data products using both streaming and batch ingestion patterns on Google Cloud Platform with solid data warehouse principles.\n\nBe the Subject Matter Expert in Data Engineering with a focus on GCP native services and other well integrated third-party technologies.\n\nArchitect and implement sophisticated ETL pipelines, ensuring efficient data integration into Big Query from diverse batch and streaming sources.\n\nSpearhead the development and maintenance of data ingestion and analytics pipelines using cutting-edge tools and technologies, including Python, SQL, and DBT/Data form.\n\nEnsure the highest standards of data quality and integrity across all data processes.\n\nData workflow management using Astronomer and Terraform for cloud infrastructure, promoting best practices in Infrastructure as Code\n\nRich experience in Application Support in GCP. Experienced in data mapping, impact analysis, root cause analysis, and document data lineage to support robust data governance.\n\nDevelop comprehensive documentation for data engineering processes, promoting knowledge sharing and system maintainability.\n\nUtilize GCP monitoring tools to proactively address performance issues and ensure system resilience, while providing expert production support.\n\nProvide strategic guidance and mentorship to team members on data transformation initiatives, championing data utility within the enterprise.",Industry Type: Auto Components,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Product management', 'Computer science', 'Application support', 'Production support', 'Agile', 'Scheduling', 'Customer service', 'Analytics', 'SQL', 'Python']",2025-06-14 05:06:50
Data Engineer - Python Programming,Leading Client,5 - 7 years,Not Disclosed,"['Mumbai', 'Any Location']","We are looking for a skilled Data Engineer with strong hands-on experience in Clickhouse, Kubernetes, SQL, Python, and FastAPI, along with a good understanding of PostgreSQL.\nThe ideal candidate will be responsible for building and maintaining efficient data pipelines, optimizing query performance, and developing APIs to support scalable data services.\n\n- Design, build, and maintain scalable and efficient data pipelines and ETL processes.\n\n- Develop and optimize Clickhouse databases for high-performance analytics.\n\n- Create RESTful APIs using FastAPI to expose data services.\n\n- Work with Kubernetes for container orchestration and deployment of data services.\n\n- Write complex SQL queries to extract, transform, and analyze data from PostgreSQL and Clickhouse.\n\n- Collaborate with data scientists, analysts, and backend teams to support data needs and ensure data quality.\n\n- Monitor, troubleshoot, and improve performance of data infrastructure.\n\n- Strong experience in Clickhouse - data modeling, query optimization, performance tuning.\n\n- Expertise in SQL - including complex joins, window functions, and optimization.\n\n- Proficient in Python, especially for data processing (Pandas, NumPy) and scripting.\n\n- Experience with FastAPI for creating lightweight APIs and microservices.\n\n- Hands-on experience with PostgreSQL - schema design, indexing, and performance.\n\n- Solid knowledge of Kubernetes managing containers, deployments, and scaling.\n\n- Understanding of software engineering best practices (CI/CD, version control, testing).\n\n- Experience with cloud platforms like AWS, GCP, or Azure.\n\n- Knowledge of data warehousing and distributed data systems.\n\n- Familiarity with Docker, Helm, and monitoring tools like Prometheus/Grafana.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'PostgreSQL', 'Data Modeling', 'Data Warehousing', 'Data Analytics', 'ETL', 'Python', 'SQL']",2025-06-14 05:06:52
Data Engineer - Python Programming,Leading Client,5 - 7 years,Not Disclosed,['Nagpur'],"We are looking for a skilled Data Engineer with strong hands-on experience in Clickhouse, Kubernetes, SQL, Python, and FastAPI, along with a good understanding of PostgreSQL.\nThe ideal candidate will be responsible for building and maintaining efficient data pipelines, optimizing query performance, and developing APIs to support scalable data services.\n\n- Design, build, and maintain scalable and efficient data pipelines and ETL processes.\n\n- Develop and optimize Clickhouse databases for high-performance analytics.\n\n- Create RESTful APIs using FastAPI to expose data services.\n\n- Work with Kubernetes for container orchestration and deployment of data services.\n\n- Write complex SQL queries to extract, transform, and analyze data from PostgreSQL and Clickhouse.\n\n- Collaborate with data scientists, analysts, and backend teams to support data needs and ensure data quality.\n\n- Monitor, troubleshoot, and improve performance of data infrastructure.\n\n- Strong experience in Clickhouse - data modeling, query optimization, performance tuning.\n\n- Expertise in SQL - including complex joins, window functions, and optimization.\n\n- Proficient in Python, especially for data processing (Pandas, NumPy) and scripting.\n\n- Experience with FastAPI for creating lightweight APIs and microservices.\n\n- Hands-on experience with PostgreSQL - schema design, indexing, and performance.\n\n- Solid knowledge of Kubernetes managing containers, deployments, and scaling.\n\n- Understanding of software engineering best practices (CI/CD, version control, testing).\n\n- Experience with cloud platforms like AWS, GCP, or Azure.\n\n- Knowledge of data warehousing and distributed data systems.\n\n- Familiarity with Docker, Helm, and monitoring tools like Prometheus/Grafana.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Python Programming', 'Data Engineering', 'PostgreSQL', 'Data Modeling', 'Data Warehousing', 'Data Analytics', 'ETL', 'Python', 'SQL']",2025-06-14 05:06:54
Data Engineer - Python Programming,Leading Client,5 - 7 years,Not Disclosed,['Ahmedabad'],"We are looking for a skilled Data Engineer with strong hands-on experience in Clickhouse, Kubernetes, SQL, Python, and FastAPI, along with a good understanding of PostgreSQL.\nThe ideal candidate will be responsible for building and maintaining efficient data pipelines, optimizing query performance, and developing APIs to support scalable data services.\n\n- Design, build, and maintain scalable and efficient data pipelines and ETL processes.\n\n- Develop and optimize Clickhouse databases for high-performance analytics.\n\n- Create RESTful APIs using FastAPI to expose data services.\n\n- Work with Kubernetes for container orchestration and deployment of data services.\n\n- Write complex SQL queries to extract, transform, and analyze data from PostgreSQL and Clickhouse.\n\n- Collaborate with data scientists, analysts, and backend teams to support data needs and ensure data quality.\n\n- Monitor, troubleshoot, and improve performance of data infrastructure.\n\n- Strong experience in Clickhouse - data modeling, query optimization, performance tuning.\n\n- Expertise in SQL - including complex joins, window functions, and optimization.\n\n- Proficient in Python, especially for data processing (Pandas, NumPy) and scripting.\n\n- Experience with FastAPI for creating lightweight APIs and microservices.\n\n- Hands-on experience with PostgreSQL - schema design, indexing, and performance.\n\n- Solid knowledge of Kubernetes managing containers, deployments, and scaling.\n\n- Understanding of software engineering best practices (CI/CD, version control, testing).\n\n- Experience with cloud platforms like AWS, GCP, or Azure.\n\n- Knowledge of data warehousing and distributed data systems.\n\n- Familiarity with Docker, Helm, and monitoring tools like Prometheus/Grafana.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Python Programming', 'Data Engineering', 'PostgreSQL', 'Data Modeling', 'Data Warehousing', 'Data Analytics', 'ETL', 'Python', 'SQL']",2025-06-14 05:06:56
Data Engineer - Python Programming,Leading Client,5 - 7 years,Not Disclosed,['Delhi / NCR'],"We are looking for a skilled Data Engineer with strong hands-on experience in Clickhouse, Kubernetes, SQL, Python, and FastAPI, along with a good understanding of PostgreSQL.\nThe ideal candidate will be responsible for building and maintaining efficient data pipelines, optimizing query performance, and developing APIs to support scalable data services.\n\n- Design, build, and maintain scalable and efficient data pipelines and ETL processes.\n\n- Develop and optimize Clickhouse databases for high-performance analytics.\n\n- Create RESTful APIs using FastAPI to expose data services.\n\n- Work with Kubernetes for container orchestration and deployment of data services.\n\n- Write complex SQL queries to extract, transform, and analyze data from PostgreSQL and Clickhouse.\n\n- Collaborate with data scientists, analysts, and backend teams to support data needs and ensure data quality.\n\n- Monitor, troubleshoot, and improve performance of data infrastructure.\n\n- Strong experience in Clickhouse - data modeling, query optimization, performance tuning.\n\n- Expertise in SQL - including complex joins, window functions, and optimization.\n\n- Proficient in Python, especially for data processing (Pandas, NumPy) and scripting.\n\n- Experience with FastAPI for creating lightweight APIs and microservices.\n\n- Hands-on experience with PostgreSQL - schema design, indexing, and performance.\n\n- Solid knowledge of Kubernetes managing containers, deployments, and scaling.\n\n- Understanding of software engineering best practices (CI/CD, version control, testing).\n\n- Experience with cloud platforms like AWS, GCP, or Azure.\n\n- Knowledge of data warehousing and distributed data systems.\n\n- Familiarity with Docker, Helm, and monitoring tools like Prometheus/Grafana.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Python Programming', 'Data Engineering', 'PostgreSQL', 'Data Modeling', 'Data Warehousing', 'Data Analytics', 'ETL', 'Python', 'SQL']",2025-06-14 05:06:59
Hadoop Data Engineer/ Senior Software Engineer,Hsbc,2 - 11 years,Not Disclosed,['Pune'],"Some careers shine brighter than others.\nIf you re looking for a career that will help you stand out, join HSBC and fulfil your potential. Whether you want a career that could take you to the top, or simply take you in an exciting new direction, HSBC offers opportunities, support and rewards that will take you further.\nHSBC is one of the largest banking and financial services organisations in the world, with operations in 64 countries and territories. We aim to be where the growth is, enabling businesses to thrive and economies to prosper, and, ultimately, helping people to fulfil their hopes and realise their ambitions.\nWe are currently seeking an experienced professional to join our team in the role of Software Engineer\nIn this role you will be\nExpertise in Scala-Spark/Python-Spark development and should be able to Work with Agile application dev team to implement data strategies.\nDesign and implement scalable data architectures to support the banks data needs.\nDevelop and maintain ETL (Extract, Transform, Load) processes.\nEnsure the data infrastructure is reliable, scalable, and secure.\nOversee the integration of diverse data sources into a cohesive data platform.\nEnsure data quality, data governance, and compliance with regulatory requirements.\nMonitor and optimize data pipeline performance.\nTroubleshoot and resolve data-related issues promptly.\nImplement monitoring and alerting systems for data processes.\nTroubleshoot and resolve technical issues optimizing system performance ensuring reliability.\nCreate and maintain technical documentation for new and existing system ensuring that information is accessible to the team.\nImplementing and monitoring solutions that identify both system bottlenecks and production issues.\n\n\n\n\n\n\n\n\n\n\nRequirements\n\n\n\nTo be successful in this role, you should meet the following requirements:\nExperience in data engineering or related field and hands-on experience of building and maintenance of ETL Data pipelines\nGood experience in Designing and Developing Spark Applications using Scala or Python.\nGood experience with database technologies (SQL, NoSQL), data warehousing solutions, and big data technologies (Hadoop, Spark)\nProficiency in programming languages such as Python, Java, or Scala.\nOptimization and Performance Tuning of Spark Applications\nGIT Experience on creating, merging and managing Repos.\nPerform unit testing and performance testing.\nGood understanding of ETL processes and data pipeline orchestration tools like Airflow, Control-M.\nStrong problem-solving skills and ability to work under pressure.\nExcellent communication and interpersonal skills.",Industry Type: Consumer Electronics & Appliances,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Performance tuning', 'Performance testing', 'Agile', 'Control-M', 'Data quality', 'Unit testing', 'Financial services', 'SQL', 'Python', 'Technical documentation']",2025-06-14 05:07:01
Data Engineer Sr. Analyst,Accenture,5 - 7 years,Not Disclosed,['Kochi'],"Job Title - + +\n\n\n\nManagement Level:\n\n\n\nLocation:Kochi, Coimbatore, Trivandrum\n\n\n\nMust have skills:Databricks including Spark-based ETL, Delta Lake\n\n\n\n\nGood to have skills:Pyspark\n\n\n\nJob\n\n\nSummary\n\nWe are seeking a highly skilled and experienced Senior Data Engineer to join our growing Data and Analytics team. The ideal candidate will have deep expertise in Databricks and cloud data warehousing, with a proven track record of designing and building scalable data pipelines, optimizing data architectures, and enabling robust analytics capabilities. This role involves working collaboratively with cross-functional teams to ensure the organization leverages data as a strategic asset. Your responsibilities will include:\n\n\n\nRoles and Responsibilities\nDesign, build, and maintain scalable data pipelines and ETL processes using Databricks and other modern tools.\nArchitect, implement, and manage cloud-based data warehousing solutions on Databricks (Lakehouse Architecture)\nDevelop and maintain optimized data lake architectures to support advanced analytics and machine learning use cases.\nCollaborate with stakeholders to gather requirements, design solutions, and ensure high-quality data delivery.\nOptimize data pipelines for performance and cost efficiency.\nImplement and enforce best practices for data governance, access control, security, and compliance in the cloud.\nMonitor and troubleshoot data pipelines to ensure reliability and accuracy.\nLead and mentor junior engineers, fostering a culture of continuous learning and innovation.\nExcellent communication skills\nAbility to work independently and along with client based out of western Europe.\n\n\n\nProfessional and Technical Skills\n3.5-5 years of experience in Data Engineering roles with a focus on cloud platforms.\nProficiency in Databricks, including Spark-based ETL, Delta Lake, and SQL.\nStrong experience with one or more cloud platforms (AWS preferred).\nHandson Experience with Delta lake, Unity Catalog, and Lakehouse architecture concepts.\nStrong programming skills in Python and SQL; experience with Pyspark a plus.\nSolid understanding of data modeling concepts and practices (e.g., star schema, dimensional modeling).\nKnowledge of CI/CD practices and version control systems (e.g., Git).\nFamiliarity with data governance and security practices, including GDPR and CCPA compliance.\n\n\n\n\nAdditional Information\nExperience with Airflow or similar workflow orchestration tools.\nExposure to machine learning workflows and MLOps.\nCertification in Databricks, AWS\nFamiliarity with data visualization tools such as Power BI\n\n(do not remove the hyperlink)Qualification\n\n\n\nExperience:3.5 -5 years of experience is required\n\n\n\n\nEducational Qualification:Graduation (Accurate educational details should capture)",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data warehousing', 'sql', 'data modeling', 'python', 'data bricks', 'hive', 'kubernetes', 'catalog', 'pyspark', 'data architecture', 'docker', 'ansible', 'git', 'java', 'spark', 'devops', 'hadoop', 'etl', 'big data', 'data lake', 'airflow', 'power bi', 'cloud platforms', 'machine learning', 'data engineering', 'aws']",2025-06-14 05:07:04
Data Engineer - Senior Analyst,Accenture,2 - 3 years,Not Disclosed,['Kochi'],"Job Title - + +\n\n\n\nManagement Level:\n\n\n\nLocation:Kochi, Coimbatore, Trivandrum\n\n\n\nMust have skills:Python/Scala, Pyspark/Pytorch\n\n\n\n\nGood to have skills:Redshift\n\n\n\nExperience:3.5 -5 years of experience is required\n\n\n\n\nEducational Qualification:Graduation (Accurate educational details should capture)\n\n\n\nJob\n\n\nSummary\n\nYoull capture user requirements and translate them into business and digitally enabled solutions across a range of industries. Your responsibilities will include:\n\n\n\nRoles and Responsibilities\nDesigning, developing, optimizing, and maintaining data pipelines that adhere to ETL principles and business goals\nSolving complex data problems to deliver insights that helps our business to achieve their goals.\nSource data (structured unstructured) from various touchpoints, format and organize them into an analyzable format.\nCreating data products for analytics team members to improve productivity\nCalling of AI services like vision, translation etc. to generate an outcome that can be used in further steps along the pipeline.\nFostering a culture of sharing, re-use, design and operational efficiency of data and analytical solutions\nPreparing data to create a unified database and build tracking solutions ensuring data quality\nCreate Production grade analytical assets deployed using the guiding principles of CI/CD.\n\n\nProfessional and Technical Skills\nExpert in Python, Scala, Pyspark, Pytorch, Javascript (any 2 at least)\nExtensive experience in data analysis (Big data- Apache Spark environments), data libraries (e.g. Pandas, SciPy, Tensorflow, Keras etc.), and SQL. 2-3 years of hands-on experience working on these technologies.\nExperience in one of the many BI tools such as Tableau, Power BI, Looker.\nGood working knowledge of key concepts in data analytics, such as dimensional modeling, ETL, reporting/dashboarding, data governance, dealing with structured and unstructured data, and corresponding infrastructure needs.\nWorked extensively in Microsoft Azure (ADF, Function Apps, ADLS, Azure SQL), AWS (Lambda,Glue,S3), Databricks analytical platforms/tools, Snowflake Cloud Datawarehouse.\n\n\n\n\nAdditional Information\nExperience working in cloud Data warehouses like Redshift or Synapse\nCertification in any one of the following or equivalent\nAWS- AWS certified data Analytics- Speciality\nAzure- Microsoft certified Azure Data Scientist Associate\nSnowflake- Snowpro core- Data Engineer\nDatabricks Data Engineering\n\nQualification\n\n\n\nExperience:3.5 -5 years of experience is required\n\n\n\n\nEducational Qualification:Graduation (Accurate educational details should capture)",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['scala', 'pyspark', 'pytorch', 'python', 'microsoft azure', 'glue', 'amazon redshift', 'sql', 'tensorflow', 'sql azure', 'spark', 'keras', 'big data', 'etl', 'scipy', 'snowflake', 'data analysis', 'azure data lake', 'power bi', 'data engineering', 'javascript', 'pandas', 'data bricks', 'tableau', 'lambda expressions', 'aws']",2025-06-14 05:07:06
Data Engineer,Konrad Group,5 - 10 years,Not Disclosed,['Gurugram'],"About The Role\nAs a Senior Data Engineer you ll be tasked with designing, building, and maintaining scalable data platforms and pipelines. Your deep knowledge of data platforms such as Azure Fabric, Databricks, and Snowflake will be essential as you collaborate closely with data analysts, scientists, and other engineers to ensure reliable, secure, and efficient data solutions.\nWhat You ll Do\nDesign, build, and manage robust data pipelines and data architectures.\nImplement solutions leveraging platforms such as Azure Fabric, Databricks, and Snowflake.\nOptimize data workflows, ensuring reliability, scalability, and performance.\nCollaborate with internal stakeholders to understand data needs and deliver tailored solutions.\nEnsure data security and compliance with industry standards and best practices.\nPerform data modelling, data extraction, transformation, and loading (ETL/ELT).\nIdentify and recommend innovative solutions to enhance data quality and analytics capabilities.\nQualifications\nBachelor s degree or higher in Computer Science, Data Engineering, Information Technology, or a related field.\n5+ years of professional experience as a Data Engineer or similar role.\nProficiency in data platforms such as Azure Fabric, Databricks, and Snowflake.\nHands-on experience with data pipeline tools, cloud services, and storage solutions.\nStrong programming skills in SQL, Python, or related languages.\nExperience with big data technologies and concepts (Spark, Hadoop, Kafka).\nExcellent analytical, troubleshooting, and problem-solving skills.\nAbility to effectively communicate technical concepts clearly to non-technical stakeholders.\nNice to have\nCertifications related to Azure Data Engineering, Databricks, or Snowflake.\nFamiliarity with DevOps practices and CI/CD pipelines",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Analytical', 'Genetics', 'Data quality', 'Troubleshooting', 'Information technology', 'Analytics', 'SQL', 'Python', 'Data extraction']",2025-06-14 05:07:09
Senior Data Engineer,Taskus,5 - 10 years,Not Disclosed,['Chennai'],"What We Offer:\nWhat youll do\nDevelop, maintain, and enhance new data sources and tables, contributing to data engineering efforts to ensure comprehensive and efficient data architecture.\nServes as the liaison between Data Engineer team and the Airport operation teams, developing new data sources and overseeing enhancements to existing database; being one of the main contact points for data requests, metadata, and statistical analysis",,,,"['BPO', 'Project management', 'Analytical', 'Social media', 'Javascript', 'HTML', 'Data quality', 'Outsourcing', 'Gaming', 'SQL']",2025-06-14 05:07:11
"Senior Manager, Senior Data Engineer",Merck Sharp & Dohme (MSD),6 - 11 years,Not Disclosed,['Hyderabad'],"Senior Manager, Data Engineer\nThe Opportunity\nBased in Hyderabad, join a global healthcare biopharma company and be part of a 130- year legacy of success backed by ethical integrity, forward momentum, and an inspiring mission to achieve new milestones in global healthcare.\nBe part of an organisation driven by digital technology and data-backed approaches that support a diversified portfolio of prescription medicines, vaccines, and animal health products.\nDrive innovation and execution excellence. Be a part of a team with passion for using data, analytics, and insights to drive decision-making, and which creates custom software, allowing us to tackle some of the worlds greatest health threats.\nOur Technology Centers focus on creating a space where teams can come together to deliver business solutions that save and improve lives. An integral part of our company s IT operating model, Tech Centers are globally distributed locations where each IT division has employees to enable our digital transformation journey and drive business outcomes. These locations, in addition to the other sites, are essential to supporting our business and strategy.\nA focused group of leaders in each Tech Center helps to ensure we can manage and improve each location, from investing in growth, success, and well-being of our people, to making sure colleagues from each IT division feel a sense of belonging to managing critical emergencies. And together, we must leverage the strength of our team to collaborate globally to optimize connections and share best practices across the Tech Centers.\nRole Overview\nResponsibilities\nDesigns, builds, and maintains data pipeline architecture - ingest, process, and publish data for consumption.\nBatch processes collected data, formats data in an optimized way to bring it analyze-ready\nEnsures best practices sharing and across the organization\nEnables delivery of data-analytics projects\nDevelops deep knowledge of the companys supported technology; understands the whole complexity/dependencies between multiple teams, platforms (people, technologies)\nCommunicates intensively with other platform/competencies to comprehend new trends and methodologies being implemented/considered within the company ecosystem\nUnderstands the customer and stakeholders business needs/priorities and helps building solutions that support our business goals\nEstablishes and manages the close relationship with customers/stakeholders\nHas overview of the date engineering market development to be able to come up/explore new ways of delivering pipelines to increase their value/contribution\nBuilds community of practice leveraging experience from delivering complex analytics projects\nIs accountable for ensuring that the team delivers solutions with high quality standards, timeliness, compliance and excellent user experience\nContributes to innovative experiments, specifically to idea generation, idea incubation and/or experimentation, identifying tangible and measurable criteria\nQualifications:\nBachelor s degree in Computer Science, Data Science, Information Technology, Engineering or a related field.\n3+ plus years of experience as a Data Engineer or in a similar role, with a strong portfolio of data projects.\n3+ plus years experience SQL skills, with the ability to write and optimize queries for large datasets.\n1+ plus years experience and proficiency in Python for data manipulation, automation, and pipeline development.\nExperience with Databricks including creating notebooks and utilizing Spark for big data processing.\nStrong experience with data warehousing solution (such as Snowflake), including schema design and performance optimization.\nExperience with data governance and quality management tools, particularly Collibra DQ.\nStrong analytical and problem-solving skills, with an attention to detail.\nSAP Basis experience working on SAP S/4HANA deployments on Cloud platforms (example: AWS, GCP or Azure).\nOur technology teams operate as business partners, proposing ideas and innovative solutions that enable new organizational capabilities. We collaborate internationally to deliver services and solutions that help everyone be more productive and enable innovation.\nWho we are:\nWhat we look for:\n#HYDIT\nCurrent Employees apply HERE\nCurrent Contingent Workers apply HERE\nSearch Firm Representatives Please Read Carefully\nEmployee Status:\nRegular\nRelocation:\nVISA Sponsorship:\nTravel Requirements:\nFlexible Work Arrangements:\nHybrid\nShift:\nValid Driving License:\nHazardous Material(s):\n\nRequired Skills:\nBusiness, Business, Business Intelligence (BI), Business Management, Contractor Management, Cost Reduction, Database Administration, Database Optimization, Data Engineering, Data Flows, Data Infrastructure, Data Management, Data Modeling, Data Optimization, Data Quality, Data Visualization, Design Applications, ETL Tools, Information Management, Management Process, Operating Cost Reduction, Senior Program Management, Social Collaboration, Software Development, Software Development Life Cycle (SDLC) {+ 1 more}\n\nPreferred Skills:\nJob Posting End Date:\n08/13/2025\n*A job posting is effective until 11:59:59PM on the day BEFORE the listed job posting end date. Please ensure you apply to a job posting no later than the day BEFORE the job posting end date.",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Automation', 'Data management', 'Data modeling', 'Analytical', 'Healthcare', 'Data processing', 'Business intelligence', 'Information technology', 'Analytics', 'SQL']",2025-06-14 05:07:13
Senior Data Engineer - Azure,Blend360 India,6 - 11 years,Not Disclosed,['Hyderabad'],"Job Description\nAs a Senior Data Engineer, your role is to spearhead the data engineering teams and elevate the team to the next level! You will be responsible for laying out the architecture of the new project as well as selecting the tech stack associated with it. You will plan out the development cycles deploying AGILE if possible and create the foundations for good data stewardship with our new data products!\nYou will also set up a solid code framework that needs to be built to purpose yet have enough flexibility to adapt to new business use cases tough but rewarding challenge!\n\nResponsibilities\nCollaborate with several stakeholders to deeply understand the needs of data practitioners to deliver at scale\nLead Data Engineers to define, build and maintain Data Platform\nWork on building Data Lake in Azure Fabric processing data from multiple sources\nMigrating existing data store from Azure Synapse to Azure Fabric\nImplement data governance and access control\nDrive development effort End-to-End for on-time delivery of high-quality solutions that conform to requirements, conform to the architectural vision, and comply with all applicable standards.\nPresent technical solutions, capabilities, considerations, and features in business terms. Effectively communicate status, issues, and risks in a precise and timely manner.\nFurther develop critical initiatives, such as Data Discovery, Data Lineage and Data Quality\nLeading team and Mentor junior resources\nHelp your team members grow in their role and achieve their career aspirations\nBuild data systems, pipelines, analytical tools and programs\nConduct complex data analysis and report on results",Industry Type: Advertising & Marketing,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Access control', 'Data analysis', 'Team leading', 'Architecture', 'Analytical', 'Agile', 'data governance', 'Data processing', 'Mentor', 'Data quality']",2025-06-14 05:07:15
Senior Data Engineer,Randstad Global,5 - 8 years,Not Disclosed,['Hyderabad'],"Key Responsibilities\n\nDesign, develop, and maintain data pipelines and ETL processes focused on data transformation using SQL, Python, and DBT.\nDevelop complex SQL queries for data extraction, transformation, and loading (ETL), with focus on aggregation, cleansing, and modeling.\nLeverage Python to automate transformation tasks and implement custom logic in data workflows.\nBuild and manage DBT models for reusable, maintainable, and scalable data pipelines.\nAssemble reusable and scalable datasets aligned with business needs.\nEnsure data accuracy, consistency, and completeness through thorough validation, testing, and documentation.\nDevelop and maintain data storage solutions and implement transformation logic for analysis and reporting.\nCollaborate with cross-functional stakeholders to understand data requirements and provide actionable insights.\nTroubleshoot and resolve data-related technical issues and identify opportunities to improve data quality and reliability.\nDocument technical specifications and data transformation processes.\nAdhere to information security policies, ensure data compliance with PII, GDPR, and other relevant regulations.\nRequired Skills & Experience\n\nHands-on experience with Google Cloud Platform (GCP) and Google BigQuery.\nStrong expertise in SQL, database design, and query optimization.\nProven experience in designing and implementing ETL pipelines and data transformation flows.\nTechnical proficiency in data modeling, data mining, and segmentation techniques.\nExcellent numerical, analytical, and problem-solving skills.\nStrong attention to detail and a critical mindset for evaluating information.\nEffective stakeholder management and ability to handle multiple priorities.\nExcellent verbal and written communication skills.\nAbility to self-manage workload and work collaboratively within a team.\nDesirable Skills\n\nProficiency in SQL and Python programming.\nHands-on experience with DBT and version control tools like GitLab.\nFamiliarity with complex financial data models.\nExperience with data visualization tools such as Tableau or Google Data Studio.\nExposure to Salesforce and Salesforce Einstein Analytics.\nUnderstanding of Agile/Scrum methodologies.\nBachelor's degree in Computer Science, Information Technology, or a related field.\nData engineering certification and basic knowledge of AI/ML fundamentals are a plus.",Industry Type: Recruitment / Staffing,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Python', 'Airflow', 'Google Cloud Platforms', 'ETL', 'SQL']",2025-06-14 05:07:17
Senior PySpark Data Engineer,Synechron,7 - 12 years,Not Disclosed,"['Pune', 'Hinjewadi']","Job Summary\nSynechron is seeking an experienced and technically proficient Senior PySpark Data Engineer to join our data engineering team. In this role, you will be responsible for developing, optimizing, and maintaining large-scale data processing solutions using PySpark. Your expertise will support our organizations efforts to leverage big data for actionable insights, enabling data-driven decision-making and strategic initiatives.\nSoftware Requirements\nRequired Skills:",,,,"['PySpark', 'S3', 'Unix operating systems', 'Spark SQL', 'Luigi', 'HDFS', 'AWS EMR', 'Apache Airflow', 'Hive', 'Linux', 'Azure HDInsight', 'Apache Kafka', 'AWS']",2025-06-14 05:07:20
Senior Data Engineer - Azure,Blend360 India,6 - 11 years,Not Disclosed,['Hyderabad'],"Job Description\nAs a Senior Data Engineer, your role is to spearhead the data engineering teams and elevate the team to the next level! You will be responsible for laying out the architecture of the new project as well as selecting the tech stack associated with it. You will plan out the development cycles deploying AGILE if possible and create the foundations for good data stewardship with our new data products!\nYou will also set up a solid code framework that needs to be built to purpose yet have enough flexibility to adapt to new business use cases tough but rewarding challenge!\n\nResponsibilities\nCollaborate with several stakeholders to deeply understand the needs of data practitioners to deliver at scale\nLead Data Engineers to define, build and maintain Data Platform\nWork on building Data Lake in Azure Fabric processing data from multiple sources\nMigrating existing data store from Azure Synapse to Azure Fabric\nImplement data governance and access control\nDrive development effort End-to-End for on-time delivery of high-quality solutions that conform to requirements, conform to the architectural vision, and comply with all applicable standards.\nPresent technical solutions, capabilities, considerations, and features in business terms. Effectively communicate status, issues, and risks in a precise and timely manner.\nFurther develop critical initiatives, such as Data Discovery, Data Lineage and Data Quality\nLeading team and Mentor junior resources\nHelp your team members grow in their role and achieve their career aspirations\nBuild data systems, pipelines, analytical tools and programs\nConduct complex data analysis and report on results\n\n\nQualifications\n5+ Years of Experience as a data engineer or similar role in Azure Synapses, ADF or\nrelevant exp in Azure Fabric\nDegree in Computer Science, Data Science, Mathematics, IT, or similar fie",Industry Type: Advertising & Marketing,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Data analysis', 'Data modeling', 'Analytical', 'Agile', 'data governance', 'Data quality', 'Data mining', 'SQL', 'Python']",2025-06-14 05:07:22
Cognitive Data and Analytics Data Engineer,Dell Technologies,3 - 6 years,Not Disclosed,"['Hyderabad', 'Gurugram', 'Bengaluru']","Senior Analyst, Data Engineer\nData Science is all about breaking new ground to enable businesses to answer their most urgent questions. Pioneering massively parallel data-intensive analytic processing, our mission is to develop a whole new approach to generating meaning and value from petabyte-scale data sets and shape brand new methodologies, tools, statistical methods and models. What s more, we are in collaboration with leading academics, industry experts and highly skilled engineers to equip our customers to generate sophisticated new insights from the biggest of big data.\nJoin us to do the best work of your career and make a profound social impact as a Data Engineering Sr. Analyst on our Data Engineering Team in Bangalore / Hyderabad/Gurgaon.\nWhat you ll achieve\nAs a Data Engineer, you will build and support leading edge AI-Fueled, Data-Driven business solutions within Services Parts, Services Repair and Services Engineering Space.\n\nYou will:\nInteract with business leaders and internal customers to create and establish design standards and assurance processes for software, systems and applications development to ensure compatibility and operability.\nAnalyze business goals, defines project scope and identifies functional and technical requirements to produce accurate business insights.\nDevelop fundamentally new approaches to generate meaning from data, creating specifications for reports and analysis based on business needs.\nSupport existing AI and Analytics products by ensuring optimal operations and enhance for new functionalities.\n\nTake the first step towards your dream career\nEvery Dell Technologies team member brings something unique to the table. Here s what we are looking for with this role:\n\nEssential Requirements\nUses proactive outcomes-driven mindset\nUses detailed analytical data skills\nUses hands on experience developing coding data and analytics solutions\nUses hands on knowledge in SQL\nUses hands on knowledge in Python, Java, or other modern programming language\nDesirable Requirements\nTeradata macro programming\nData management concepts\n\n\nApplication closing date: 30th June 25",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Assurance', 'Data management', 'Coding', 'Senior Analyst', 'Analytical', 'Programming', 'Teradata', 'Business solutions', 'SQL', 'Python']",2025-06-14 05:07:24
Cognitive Data and Analytics Data Engineer,Dell Technologies,3 - 6 years,Not Disclosed,"['Hyderabad', 'Gurugram', 'Bengaluru']","Data Engineering Advisor\nData Science is all about breaking new ground to enable businesses to answer their most urgent questions. Pioneering massively parallel data-intensive analytic processing, our mission is to develop a whole new approach to generating meaning and value from petabyte-scale data sets and shape brand new methodologies, tools, statistical methods and models. What s more, we are in collaboration with leading academics, industry experts and highly skilled engineers to equip our customers to generate sophisticated new insights from the biggest of big data.\nJoin us to do the best work of your career and make a profound social impact as a Data Engineering Advisor on our Data Engineering Team in Bangalore /Hyderabad/Gurgaon.\n\nWhat you ll achieve\nAs a Data Engineer, you will build leading edge AI-Fueled, Data-Driven business solutions within Services Parts, Services Repair and Services Engineering Space.\n\nYou will:\nInteract with business leaders and internal customers to create and establish design standards and assurance processes for software, systems and applications development to ensure compatibility and operability.\nAnalyze business goals, defines project scope and identifies functional and technical requirements to produce accurate business insights.\nDevelop fundamentally new approaches to generate meaning from data, creating specifications for reports and analysis based on business needs.\nSupport existing AI and Analytics products by ensuring optimal operations and enhance for new functionalities.\n\nTake the first step towards your dream career\nEvery Dell Technologies team member brings something unique to the table. Here s what we are looking for with this role:\n\nEssential Requirements\nUses proactive outcomes-driven mindset\nUses detailed analytical data skills\nUses hands on experience developing coding data and analytics solutions\nUses hands on expert knowledge in SQL\nUses hands on expert knowledge in Python, Java, or other modern programming language\nDesirable Requirements\nTeradata macro programming\nData management concepts\n\n\nApplication closing date: 30th June 25",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Assurance', 'Data management', 'Coding', 'Analytical', 'Programming', 'Engineering Manager', 'Teradata', 'Business solutions', 'SQL', 'Python']",2025-06-14 05:07:26
Data Engineer,Luxoft,5 - 10 years,Not Disclosed,['Pune'],"Project description\nYou'll be working in the GM Business Analytics team located in Pune. The successful candidate will be a member of the global Distribution team, which has team members in London and Pune.\n\nWe work as part of a global team providing analytical solutions for IB distribution/sales people. Solutions deployed should be extensible globally with minimal localization.\n\nResponsibilities\n\nAre you passionate about data and analyticsAre you keen to be part of the journey to modernize a data warehouse/ analytics suite of application(s). Do you take pride in the quality of software delivered for each development iteration\n\nWe're looking for someone like that to join us and\n\nbe a part of a high-performing team on a high-profile project.\n\nsolve challenging problems in an elegant way\n\nmaster state-of-the-art technologies\n\nbuild a highly responsive and fast updating application in an Agile & Lean environment\n\napply best development practices and effectively utilize technologies\n\nwork across the full delivery cycle to ensure high-quality delivery\n\nwrite high-quality code and adhere to coding standards\n\nwork collaboratively with diverse team(s) of technologists\n\nYou are:\n\nCurious and collaborative, comfortable working independently, as well as in a team\n\nFocused on delivery to the business\n\nStrong in analytical skills. For example, the candidate must understand the key dependencies among existing systems in terms of the flow of data among them. It is essential that the candidate learns to understand the 'big picture' of how IB industry/business functions.\n\nAble to quickly absorb new terminology and business requirements\n\nAlready strong in analytical tools, technologies, platforms, etc. The candidate must also demonstrate a strong desire for learning and self-improvement.\n\nOpen to learning home-grown technologies, support current state infrastructure and help drive future state migrations. imaginative and creative with newer technologies\n\nAble to accurately and pragmatically estimate the development effort required for specific objectives\n\nYou will have the opportunity to work under minimal supervision to understand local and global system requirements, design and implement the required functionality/bug fixes/enhancements. You will be responsible for components that are developed across the whole team and deployed globally.\n\nYou will also have the opportunity to provide third-line support to the application's global user community, which will include assisting dedicated support staff and liaising with the members of other development teams directly, some of which will be local and some remote.\n\nSkills\nMust have\n\nA bachelor's or master's degree, preferably in Information Technology or a related field (computer science, mathematics, etc.), focusing on data engineering.\n\n5+ years of relevant experience as a data engineer in Big Data is required.\n\nStrong Knowledge of programming languages (Python / Scala) and Big Data technologies (Spark, Databricks or equivalent) is required.\n\nStrong experience in executing complex data analysis and running complex SQL/Spark queries.\n\nStrong experience in building complex data transformations in SQL/Spark.\n\nStrong knowledge of Database technologies is required.\n\nStrong knowledge of Azure Cloud is advantageous.\n\nGood understanding and experience with Agile methodologies and delivery.\n\nStrong communication skills with the ability to build partnerships with stakeholders.\n\nStrong analytical, data management and problem-solving skills.\n\nNice to have\n\nExperience working on the QlikView tool\n\nUnderstanding of QlikView scripting and data model\n\nOther\n\nLanguages\n\nEnglishC1 Advanced\n\nSeniority\n\nSenior",Industry Type: Legal,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analysis', 'data management', 'big data technologies', 'sql', 'spark', 'python', 'scala', 'mathematics', 'business analytics', 'data engineering', 'azure cloud', 'qlikview', 'data bricks', 'computer science', 'database creation', 'data transformation', 'agile', 'big data', 'agile methodology']",2025-06-14 05:07:29
Data Engineer,Infoobjects Inc.,3 - 6 years,Not Disclosed,['Jaipur'],"Role & responsibilities:\nDesign, develop, and maintain robust ETL/ELT pipelines to ingest and process data from multiple sources.\nBuild and maintain scalable and reliable data warehouses, data lakes, and data marts.\nCollaborate with data scientists, analysts, and business stakeholders to understand data needs and deliver solutions.\nEnsure data quality, integrity, and security across all data systems.\nOptimize data pipeline performance and troubleshoot issues in a timely manner.\nImplement data governance and best practices in data management.\nAutomate data validation, monitoring, and reporting processes.\n\n\n\nPreferred candidate profile:\nBachelor's or Masters degree in Computer Science, Engineering, Information Systems, or related field.\nProven experience (X+ years) as a Data Engineer or similar role.\nStrong programming skills in Python, Java, or Scala.\nProficiency with SQL and working knowledge of relational databases (e.g., PostgreSQL, MySQL).\nHands-on experience with big data technologies (e.g., Spark, Hadoop).\nFamiliarity with cloud platforms such as AWS, GCP, or Azure (e.g., S3, Redshift, BigQuery, Data Factory).\nExperience with orchestration tools like Airflow or Prefect.\nKnowledge of data modeling, warehousing, and architecture design principles.\nStrong problem-solving skills and attention to detail.\n\nPerks and benefits\nFree Meals\nPF and Gratuity\nMedical and Term Insurance",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SCALA', 'Kafka', 'AWS', 'Python', 'Pyspark', 'Java', 'Postgresql', 'Hadoop', 'Spark', 'ETL', 'SQL']",2025-06-14 05:07:31
Data Engineer,ESolutioninc,4 - 9 years,Not Disclosed,"['Navi Mumbai', 'Pune', 'Mumbai (All Areas)']","Job Description :\n\nJob Overview:\nWe are seeking a highly skilled Data Engineer with expertise in SQL, Python, Data Warehousing, AWS, Airflow, ETL, and Data Modeling. The ideal candidate will be responsible for designing, developing, and maintaining robust data pipelines, ensuring efficient data processing and integration across various platforms. This role requires strong problem-solving skills, an analytical mindset, and a deep understanding of modern data engineering frameworks.Key Responsibilities:\nDesign, develop, and optimize scalable data pipelines and ETL processes to support business intelligence, analytics, and operational data needs.\nBuild and maintain data models (conceptual, logical, and physical) to enhance data storage, retrieval, and transformation efficiency.\nDevelop, test, and optimize complex SQL queries for efficient data extraction, transformation, and loading (ETL).\nImplement and manage data warehousing solutions (e.g., Snowflake, Redshift, BigQuery) for structured and unstructured data storage.\nWork with AWS, Azure, and cloud-based data solutions to build high-performance data ecosystems.\nUtilize Apache Airflow for orchestrating workflows and automating data pipeline execution.\nCollaborate with cross-functional teams to understand business data requirements and ensure alignment with data strategies.\nEnsure data integrity, security, and compliance with governance policies and best practices.\nMonitor, troubleshoot, and improve the performance of existing data systems for scalability and reliability.\nStay updated with emerging data engineering technologies, frameworks, and best practices to drive continuous improvement.\nRequired Skills & Qualifications:\nProficiency in SQL for query development, performance tuning, and optimization.\nStrong Python programming skills for data processing, automation, and scripting.\nHands-on experience with ETL development, data integration, and transformation workflows.\nExpertise in data modeling for efficient database and data warehouse design.\nExperience with cloud platforms such as AWS (S3, Redshift, Lambda), Azure, or GCP.\nWorking knowledge of Airflow or similar workflow orchestration tools.\nFamiliarity with Big Data frameworks like Hadoop or Spark (preferred but not mandatory).\nStrong problem-solving skills and ability to work in a fast-paced, dynamic environment.Role & responsibilities\n\n\nPreferred candidate profile",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Advanced Python', 'Pyspark', 'Microsoft Azure', 'Data Modeling', 'Data Warehousing', 'ETL', 'Elt', 'Advanced SQl', 'AWS', 'SQL']",2025-06-14 05:07:34
Data Engineer,Invitusdata,1 - 4 years,Not Disclosed,['Chennai'],"Strong understanding of Data Warehousing concepts and data modeling techniques and columnar databases\n\nStrong Hands on experience in Data Analysis, preprocessing and validation of the engineered dataset in SQL/NoSQL databases\n\nHands-on Experience in data cleansing,processing and loading using Big-data tools mainly (Py)Spark and Hive/Presto\n\nExposure to any workflow scheduling & Orchestration tools Airflow/Prefect/SQS would be helpful\n\nComfortable working with any programming/scripting language like Python(Preferred)/Scala/Shell scripting\n\nStrong data engineering skills on any Cloud Platform is essential(Preferred: AWS)\n\nExposure to data lake and data lake house would be an added advantage\n\nSolid Understanding of data structures and algorithms\n\nKnowledge of distributed/MPP systems pertaining to data storage and computing\n\nAbility to effectively communicate with both business and technical teams\n\nGood interpersonal skills and positive attitude\n\nExperience in working with agile methodology in a fast paced environment\n\n""] , keyResponsibilities:Design , Develop and implement ETL data pipelines that load data into an information product that helps the organization in reaching strategic goals\n\nWork on ingesting, storing, cleansing, processing and analyzing large data sets from heterogeneous data sources\n\nFinalizing the scope of the system and delivering Big Data solutions\n\nTranslate complex technical and functional requirements into detailed designs\n\nSchedule, orchestrate and Implement data pipelines and processes that scale with increase in data volume\n\nInvestigate and analyze alternative solutions to data storing, processing etc\n\nto ensure most streamlined approaches are implemented\n\nCollaborate with business consultants, data scientists, and application developers to develop analytics solutions\n\nHelp define data governance policies and support data versioning processes\n\nMaintain security & data privacy working closely with the Data Protection Officer internally\n\nAnalyze a vast number of data stores and uncover insights\n\nCreate data tools for business team members that assist them in analysis that gives them the competitive edge\n\nMonitor data application performance for potential bottlenecks and resolve performance issues\n\nIdentify and implement cost-saving strategies to reduce ongoing Big Data and Cloud costs/expenses",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data analysis', 'Data modeling', 'Shell scripting', 'Data structures', 'Workflow', 'Scheduling', 'Agile methodology', 'Analytics', 'SQL', 'Python']",2025-06-14 05:07:36
Data Engineer - SSIS - 5+ Years - Gurugram (Hybrid),Crescendo Global,5 - 10 years,Not Disclosed,['Gurugram'],"Data Engineer - SSIS - 5+ Years - Gurugram (Hybrid)\n\nAre you a skilled Data Engineer with expertise in SSIS and 5+ years of experience? Do you have a passion for analytics and want to work in a hybrid setup in Gurugram? Our client is seeking a talented individual to join their team and contribute to their data engineering projects.\n\nLocation : Gurugram (Hybrid)\n\nYour Future EmployerOur client is a leading organization in the analytics domain, known for fostering an inclusive and diverse work environment. They are committed to providing their employees with opportunities for growth and development.\n\nResponsibilities\nDesign, develop, and maintain data pipelines using SSIS for efficient data processing\nCollaborate with cross-functional teams to understand data requirements and provide effective data solutions\nOptimize data pipelines for performance and scalability\nEnsure data quality and integrity throughout the data engineering process\n\nRequirements\n5+ years of experience in data engineering with a strong focus on SSIS\nProficiency in data warehousing concepts and ETL processes\nHands-on experience with SQL databases and data modeling4.Strong analytical and problem-solving skills\nBachelor's degree in Computer Science, Engineering, or related field\n\nWhat's in it for you : In this role, you will have the opportunity to work on challenging projects and enhance your expertise in data engineering. The organization offers a competitive compensation package and a supportive work environment where your contributions are valued.\n\nReach us : If you feel this opportunity is well aligned with your career progression plans, please feel free to reach me with your updated profile at rohit.kumar@crescendogroup.in\n\nDisclaimer : Crescendo Global specializes in Senior to C-level niche recruitment. We are passionate about empowering job seekers and employers with an engaging memorable job search and leadership hiring experience. Crescendo Global does not discriminate on the basis of race, religion, color, origin, gender, sexual orientation, age, marital status, veteran status or disability status.\n\nNote : We receive a lot of applications on a daily basis so it becomes a bit difficult for us to get back to each candidate. Please assume that your profile has not been shortlisted in case you don't hear back from us in 1 week. Your patience is highly appreciated.\n\nScammers can misuse Crescendo Globals name for fake job offers. We never ask for money, purchases, or system upgrades. Verify all opportunities at www.crescendo-global.com and report fraud immediately. Stay alert!\n\nProfile keywords : Data Engineer, SSIS, Data Warehousing, ETL, SQL, Analytics",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Engineering', 'SSIS', 'SQL', 'Analytics']",2025-06-14 05:07:38
Data Engineer,Konrad Group,3 - 7 years,15-30 Lacs P.A.,['Gurugram( Sector 42 Gurgaon )'],"Who We Are\n\nKonrad is a next generation digital consultancy. We are dedicated to solving complex business problems for our global clients with creative and forward-thinking solutions. Our employees enjoy a culture built on innovation and a commitment to creating best-in-class digital products in use by hundreds of millions of consumers around the world. We hire exceptionally smart, analytical, and hard working people who are lifelong learners.\nAbout The Role\nAs a Data Engineer youll be tasked with designing, building, and maintaining scalable data platforms and pipelines. Your deep knowledge of data platforms such as Azure Fabric, Databricks, and Snowflake will be essential as you collaborate closely with data analysts, scientists, and other engineers to ensure reliable, secure, and efficient data solutions.\n\nWhat Youll Do\n\nDesign, build, and manage robust data pipelines and data architectures.\nImplement solutions leveraging platforms such as Azure Fabric, Databricks, and Snowflake.\nOptimize data workflows, ensuring reliability, scalability, and performance.\nCollaborate with internal stakeholders to understand data needs and deliver tailored solutions.\nEnsure data security and compliance with industry standards and best practices.\nPerform data modelling, data extraction, transformation, and loading (ETL/ELT).\nIdentify and recommend innovative solutions to enhance data quality and analytics capabilities.\n\nQualifications\n\nBachelors degree or higher in Computer Science, Data Engineering, Information Technology, or a related field.\nAt least 3 years of professional experience as a Data Engineer or similar role.\nProficiency in data platforms such as Azure Fabric, Databricks, and Snowflake.\nHands-on experience with data pipeline tools, cloud services, and storage solutions.\nStrong programming skills in SQL, Python, or related languages.\nExperience with big data technologies and concepts (Spark, Hadoop, Kafka).\nExcellent analytical, troubleshooting, and problem-solving skills.\nAbility to effectively communicate technical concepts clearly to non-technical stakeholders.\nAdvanced English\n\nNice to have\n\nCertifications related to Azure Data Engineering, Databricks, or Snowflake.\nFamiliarity with DevOps practices and CI/CD pipelines.\n\nPerks and Benefits\n\nComprehensive Health & Wellness Benefits Package \nSocials, Outings & Retreats\nCulture of Learning & Development\nFlexible Working Hours\nWork from Home Flexibility\nService Recognition Programs\n\nKonrad is committed to maintaining a diverse work environment and is proud to be an equal opportunity employer. All qualified applicants, regardless of race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status will receive consideration for employment. If you have any accessibility requirements or concerns regarding the hiring process or employment with us, please notify us so we can provide suitable accommodation.\nWhile we sincerely appreciate all applications, only those candidates selected for an interview will be contacted.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Hadoop', 'Azure Data Factory', 'Azure Databricks', 'Spark', 'Fabric', 'Python']",2025-06-14 05:07:40
Data Engineer,Luxoft,5 - 8 years,Not Disclosed,['Pune'],"Project description\nAre you passionate about leveraging the latest technologies for strategic changeDo you enjoy problem solving in clever waysAre you organized enough to drive change across complex data systemsIf so, you could be the right person for this role.\nAs an experienced data engineer, you will join a global data analytics team in our Group Chief Technology Officer / Enterprise Architecture organization supporting our strategic initiatives which ranges from portfolio health to integration.\n\nResponsibilities\n\nHelp Group Enterprise Architecture team to develop our suite of EA tools and workbenches\n\nWork in the development team to support the development of portfolio health insights\n\nBuild data applications from cloud infrastructure to visualization layer\n\nProduce clear and commented code\n\nProduce clear and comprehensive documentation\n\nPlay an active role with technology support teams and ensure deliverables are completed or escalated on time\n\nProvide support on any related presentations, communications, and trainings\n\nBe a team player, working across the organization with skills to indirectly manage and influence\n\nBe a self-starter willing to inform and educate others\n\nSkills\nMust have\n\nB.Sc./M.Sc. degree in computing or similar\n\n5-8+ years' experience as a Data Engineer, ideally in a large corporate environment\n\nIn-depth knowledge of SQL and data modelling/data processing\n\nStrong experience working with Microsoft Azure\n\nExperience with visualisation tools like PowerBI (or Tableau, QlikView or similar)\n\nExperience working with Git, JIRA, GitLab\n\nStrong flair for data analytics\n\nStrong flair for IT architecture and IT architecture metrics\n\nExcellent stakeholder interaction and communication skills\n\nUnderstanding of performance implications when making design decisions to deliver performant and maintainable software.\n\nExcellent end-to-end SDLC process understanding.\n\nProven track record of delivering complex data apps on tight timelines\n\nFluent in English both written and spoken.\n\nPassionate about development with focus on data and cloud\n\nAnalytical and logical, with strong problem solving skills\n\nA team player, comfortable with taking the lead on complex tasks\n\nAn excellent communicator who is adept in, handling ambiguity and communicating with both technical and non-technical audiences\n\nComfortable with working in cross-functional global teams to effect change\n\nPassionate about learning and developing your hard and soft professional skills\n\nNice to have\n\nExperience working in the financial industry\n\nExperience in complex metrics design and reporting\n\nExperience in using artificial intelligence for data analytics\n\nOther\n\nLanguages\n\nEnglishC1 Advanced\n\nSeniority\n\nSenior",Industry Type: Legal,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analytics', 'data processing', 'microsoft azure', 'sql', 'data modeling', 'screening', 'it architecture', 'hiring', 'power bi', 'hrsd', 'knowledge of sql', 'data engineering', 'artificial intelligence', 'sourcing', 'qlikview', 'talent acquisition', 'tableau', 'git', 'recruitment', 'gitlab', 'sdlc', 'jira']",2025-06-14 05:07:42
Data Engineer For a US based IT Company based in Hyderabad,GLOBAL INSTITUTE FOR STAFFING & TRAINING...,5 - 10 years,16-20 Lacs P.A.,['Hyderabad( Kokapeta Village )'],"We are Hiring Data Engineer for a US based IT Company Based in Hyderabad. Candidates with minimum 5 Years of experience in Data Engineering can apply.\n\nThis job is for 1 year contract only\n\nJob Title: Data Engineer\nLocation: Hyderabad\nCTC: Upto 20 LPA\nExperience: 5+ Years\n\nJob Overview:\nWe are looking for a seasoned Senior Data Engineer with deep hands-on experience in Talend and IBM DataStage to join our growing enterprise data team. This role will focus on designing and optimizing complex data integration solutions that support enterprise-wide analytics, reporting, and compliance initiatives.\nIn this senior-level position, you will collaborate with data architects, analysts, and key stakeholders to facilitate large-scale data movement, enhance data quality, and uphold governance and security protocols.\n\nKey Responsibilities:\nDevelop, maintain, and enhance scalable ETL pipelines using Talend and IBM DataStage\nPartner with data architects and analysts to deliver efficient and reliable data integration solutions\nReview and optimize existing ETL workflows for performance, scalability, and reliability\nConsolidate data from multiple sourcesboth structured and unstructuredinto data lakes and enterprise platforms\nImplement rigorous data validation and quality assurance procedures to ensure data accuracy and integrity\nAdhere to best practices for ETL development, including source control and automated deployment\nMaintain clear and comprehensive documentation of data processes, mappings, and transformation rules\nSupport enterprise initiatives around data migration, modernization, and cloud transformation\nMentor junior engineers and participate in code reviews and team learning sessions\nRequired Qualifications:\nMinimum 5 years of experience in data engineering or ETL development\nProficient with Talend (Open Studio and/or Talend Cloud) and IBM DataStage\nStrong skills in SQL, data profiling, and performance tuning\nExperience handling large datasets and complex data workflows\nSolid understanding of data warehousing, data modeling, and data lake architecture\nFamiliarity with version control systems (e.g., Git) and CI/CD pipelines\nStrong analytical and troubleshooting skills\nEffective verbal and written communication, with strong documentation habits\n\nPreferred Qualifications:\nPrior experience in banking or financial services\nExposure to cloud platforms such as AWS, Azure, or Google Cloud\nKnowledge of data governance tools (e.g., Collibra, Alation)\nAwareness of data privacy regulations (e.g., GDPR, CCPA)\nExperience working in Agile/Scrum environments\n\nFor further assistance contact/whatsapp: 9354909518 or write to priya@gist.org.in",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Temporary/Contractual","['Data Engineering', 'Kafka', 'Snowflake', 'Java', 'Spark', 'mongo', 'Ci/Cd', 'Data Pipeline', 'Agile', 'Scrum', 'Data Modeling', 'Talend', 'Oracle', 'AWS', 'Data Governance', 'Gdpr', 'azure', 'Python', 'Shell Scripting', 'Postgresql', 'Ibm Datastage', 'Workflow', 'Data Framework', 'GCPA', 'SQL', 'GIT', 'Amazon Redshift', 'GCP', 'Data Lake', 'Data Warehousing', 'ETL']",2025-06-14 05:07:44
Data Engineer,Atyeti,2 - 4 years,Not Disclosed,['Pune'],"Role & responsibilities\n\nDevelop and Maintain Data Pipelines: Design, develop, and manage scalable ETL pipelines to process large datasets using PySpark, Databricks, and other big data technologies.\nData Integration and Transformation: Work with various structured and unstructured data sources to build efficient data workflows and integrate them into a central data warehouse.\nCollaborate with Data Scientists & Analysts: Work closely with the data science and business intelligence teams to ensure the right data is available for advanced analytics, machine learning, and reporting.",,,,"['Azure Synapse', 'Pyspark', 'ETL', 'Python']",2025-06-14 05:07:47
Data Engineer - Python/PySpark,Ekloud Inc,6 - 8 years,Not Disclosed,['Kolkata'],"Job Summary :\nWe are seeking an experienced Data Engineer with strong expertise in Databricks, Python, PySpark, and Power BI, along with a solid background in data integration and the modern Azure ecosystem. The ideal candidate will play a critical role in designing, developing, and implementing scalable data engineering\nsolutions and pipelines.\nKey Responsibilities :\n- Design, develop, and implement robust data solutions using Azure Data Factory, Databricks, and related data engineering tools.\n- Build and maintain scalable ETL/ELT pipelines with a focus on performance and reliability.\n- Write efficient and reusable code using Python and PySpark.\n- Perform data cleansing, transformation, and migration across various platforms.\n- Work hands-on with Azure Data Factory (ADF) for at least 1.5 to 2 years.\n- Develop and optimize SQL queries, stored procedures, and manage large data sets using SQL Server, T-SQL, PL/SQL, etc.\n- Collaborate with cross-functional teams to understand business requirements and provide data-driven solutions.\n- Engage directly with clients and business stakeholders to gather requirements, suggest optimal solutions, and ensure successful delivery.\n- Work with Power BI for basic reporting and data visualization tasks.\n- Apply strong knowledge of data warehousing concepts, modern data platforms, and cloud-based analytics.\n- Adhere to coding standards and best practices, including thorough documentation and testing (unit, integration, performance).\n- Support the operations, maintenance, and enhancement of existing data pipelines and architecture.\n- Estimate tasks and plan release cycles effectively.\nRequired Technical Skills :\n- Languages & Frameworks : Python, PySpark\n- Cloud & Tools : Azure Data Factory, Databricks, Azure ecosystem\n- Databases : SQL Server, T-SQL, PL/SQL\n- Reporting & BI Tools : Power BI (PBI)\n- Data Concepts : Data Warehousing, ETL/ELT, Data Cleansing, Data Migration\n- Other : Version control, Agile methodologies, good problem-solving skills\nPreferred Qualifications :\n- Experience with coding in Pysense within Databricks (added advantage)\n- Solid understanding of cloud data architecture and analytics processes\n- Ability to independently initiate and lead conversations with business stakeholders",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Azure Data Factory', 'Power BI', 'Data Pipeline', 'PySpark', 'Azure Databricks', 'Data Visualization', 'Data Warehousing', 'ETL', 'Python', 'SQL']",2025-06-14 05:07:49
Data Engineer,Dslr Technologies,4 - 6 years,Not Disclosed,['Gurugram'],"Job Title: Data Engineer\nLocation: Gurugram (WFO)\nExperience: 46 years\nDepartment: Engineering / Data & Analytics\n\nAbout Aramya\nAt Aramya, were redefining fashion for Indias underserved Gen X/Y women, offering size-inclusive, comfortable, and stylish ethnic wear at affordable prices. Launched in 2024, weve already achieved 40 Cr in revenue in our first year, driven by a unique blend of data-driven design, in-house manufacturing, and a proprietary supply chain.\n\nToday, with an ARR of 100 Cr, were scaling rapidly with ambitious growth plans for the future.\nOur vision is bold to build the most loved fashion and lifestyle brands across the world while empowering individuals to express themselves effortlessly. Backed by marquee investors like Accel and Z47, we’re on a mission to make high-quality ethnic wear accessible to every woman.\nWe’ve built a community of loyal customers who love our weekly design launches, impeccable quality, and value-for-money offerings. With a fast-moving team driven by creativity, technology, and customer obsession, Aramya is more than a fashion brand—it’s a movement to celebrate every woman’s unique journey.\n\nRole Overview\nWe’re looking for a results-driven Data Engineer who will play a key role in building and scaling our data infrastructure. This individual will own our end-to-end data pipelines, backend services for analytics, and infrastructure automation—powering real-time decision-making across our business.\nThis is a high-impact role for someone passionate about data architecture, cloud engineering, and creating a foundation for scalable insights in a fast-paced D2C environment.\nKey Responsibilities\nDesign, build, and manage scalable ETL/ELT pipelines using tools like Apache Airflow, Databricks, or Spark.\nOwn and optimize data lakes and data warehouses on AWS Redshift (or Snowflake/BigQuery).\nDevelop robust and scalable backend APIs using Python (FastAPI/Django/Flask) or Node.js.\nIntegrate third-party data sources (APIs, SFTP, flat files) and ensure data validation and consistency.\nEnsure high availability, observability, and fault-tolerance of data systems via logging, monitoring, and alerting.\nCollaborate with analysts, product managers, and business stakeholders to gather requirements and define data contracts.\nImplement Infrastructure-as-Code using tools like Terraform or AWS CDK to automate data workflows and provisioning.\n\nMust-Have Skills\nProficiency in SQL and data modeling for both OLTP and OLAP systems.\nStrong Python skills, with demonstrated experience in both backend and data engineering use cases.\nHands-on experience with Databricks, Apache Spark, and AWS Redshift.\nExperience in Airflow, dbt, or other workflow orchestration tools.\nWorking knowledge of REST APIs, backend architectures, and microservices.\nFamiliarity with Docker, Git, and CI/CD pipelines.\nExperience working on AWS cloud (S3, Lambda, ECS/Fargate, CloudWatch, etc.).\nNice-to-Have Skills\nExperience with streaming platforms like Kafka, Flink, or Kinesis.\nExposure to Snowflake, BigQuery, or Delta Lake.\nUnderstanding of data governance and PII handling best practices.\nExperience with GraphQL, gRPC, or event-driven architectures.\nFamiliarity with data observability tools like Monte Carlo, Great Expectations, or Datafold.\nPrior experience in D2C, e-commerce, or high-growth startup environments.\nQualifications\nBachelor’s degree in Computer Science, Data Engineering, or related technical discipline.\n4–6 years of experience in data engineering roles with strong backend and cloud integration exposure.",Industry Type: Textile & Apparel (Fashion),Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Engineering', 'ETL', 'Python', 'Rest Api Development', 'Spark', 'Apache', 'AWS']",2025-06-14 05:07:51
Data Engineer,Agilisium,4 - 6 years,Not Disclosed,"['Hyderabad', 'Gurugram']","We are looking for a skilled Data Engineer with strong expertise in Python, PySpark, SQL, and AWS to join our data engineering team. The ideal candidate will be responsible for building scalable data pipelines, transforming large datasets, and enabling data-driven decision-making across the organization.\n\nRole & responsibilities",,,,"['Pyspark', 'AWS', 'Python', 'SQL', 'Delta Lake', 'Data Bricks']",2025-06-14 05:07:53
Data Engineer,Diverse Lynx,3 - 6 years,Not Disclosed,['Noida'],"Responsibilities\nAs part of the Client delivery team, your primary role would be to interface with the client for quality assurance, issue resolution and ensuring high customer satisfaction.\nYou will understand requirements, create and review designs, validate the architecture and ensure high levels of service offerings to clients in the technology domain.\nYou will participate in project estimation, provide inputs for solution delivery, conduct technical risk planning, perform code reviews and unit test plan reviews.\nYou will lead and guide your teams towards developing optimized high quality code deliverables, continual knowledge management and adherence to the organizational guidelines and processes.\nYou would be a key contributor to building efficient programs/ systems and if you think you fit right in to help our clients navigate their next in their digital transformation journey, this is the place for you!If you think you fit right in to help our clients navigate their next in their digital transformation journey, this is the place for you!",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Manager Quality Assurance', 'Project estimation', 'Architecture', 'Customer satisfaction', 'Test planning', 'Issue resolution', 'Unit testing', 'Management', 'Solution delivery', 'digital transformation']",2025-06-14 05:07:56
Microsoft Fabric Data engineer,Bahwan CyberTek,12 - 14 years,20-30 Lacs P.A.,"['Indore', 'Hyderabad']","Microsoft Fabric Data engineer\n\nCTC Range 12 14 Years\nLocation – Hyderabad/Indore\nNotice Period - Immediate\n* Primary Skill\n\nMicrosoft Fabric\nSecondary Skill 1\n\nAzure Data Factory (ADF)\n12+ years of experience in Microsoft Azure Data Engineering for analytical projects.\nProven expertise in designing, developing, and deploying high-volume, end-to-end ETL pipelines for complex models, including batch, and real-time data integration frameworks using Azure, Microsoft Fabric and Databricks.\nExtensive hands-on experience with Azure Data Factory, Databricks (with Unity Catalog), Azure Functions, Synapse Analytics, Data Lake, Delta Lake, and Azure SQL Database for managing and processing large-scale data integrations.\nExperience in Databricks cluster optimization and workflow management to ensure cost-effective and high-performance processing.\nSound knowledge of data modelling, data governance, data quality management, and data modernization processes.\nDevelop architecture blueprints and technical design documentation for Azure-based data solutions.\nProvide technical leadership and guidance on cloud architecture best practices, ensuring scalable and secure solutions.\nKeep abreast of emerging Azure technologies and recommend enhancements to existing systems.\nLead proof of concepts (PoCs) and adopt agile delivery methodologies for solution development and delivery.\nwww.yash.com\n\n'Information transmitted by this e-mail is proprietary to YASH Technologies and/ or its Customers and is intended for use only by the individual or entity to which it is addressed, and may contain information that is privileged, confidential or exempt from disclosure under applicable law. If you are not the intended recipient or it appears that this mail has been forwarded to you without proper authority, you are notified that any use or dissemination of this information in any manner is strictly prohibited. In such cases, please notify us immediately at info@yash.com and delete this mail from your records.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Databricks', 'Azure Data Factory', 'Azure Synapse', 'Microsoft Fabric', 'Microsoft Azure Data Engineering', 'Pyspark', 'Data Bricks', 'SQL', 'Azure Data Lake', 'Microsoft Azure', 'Spark', 'ETL', 'Python']",2025-06-14 05:07:58
Data Engineer,Mobile Programming,5 - 10 years,Not Disclosed,['Mumbai'],"Candidate Skill:Technical Skills - Data Engineering | ETL | SQL | Python | AWS | Azure | Google Cloud | Hadoop | Spark | Kafka | Data Warehousing | Data Modeling | NoSQL | Data Quality\nWe are looking for an experienced Data Engineer to join our team in Mumbai. As a Data Engineer, you will be responsible for designing, building, and maintaining efficient data pipelines that transform raw data into actionable insights. You will work closely with data scientists and analysts to ensure that data is accessible, reliable, and optimized for analysis. Your role will also involve handling large datasets, ensuring data quality, and implementing data processing frameworks.\nKey Responsibilities:Design and build scalable data pipelines for processing, transforming, and integrating large datasets.Develop and maintain ETL processes to extract, transform, and load data from multiple sources into the data warehouse.Collaborate with data scientists and analysts to ensure that the data is optimized for analysis and modeling.Ensure the quality, integrity, and security of data throughout its lifecycle.\nWork with cloud-based technologies for data storage and processing (AWS, Azure, GCP).Implement data processing frameworks for efficient handling of structured and unstructured data.Troubleshoot and resolve issues related to data pipelines and workflows.Automate data integration processes and ensure data consistency and accuracy across systems.\nRequired Skills:\n5+ years of experience in Data Engineering with hands-on experience in data pipeline development.Strong expertise in ETL processes, data integration, and data warehousing.Proficiency in SQL, Python, and other programming languages for data manipulation.Experience with cloud technologies such as AWS, Azure, or Google Cloud.Knowledge of big data technologies like Hadoop, Spark, or Kafka is a plus.Strong understanding of data modeling, data quality, and data governance.\nFamiliarity with NoSQL databases (e.g., MongoDB, Cassandra) and relational databases.Strong analytical and problem-solving skills with the ability to work with large, complex datasets.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Azure', 'Hadoop', 'Kafka', 'SQL', 'Data Quality', 'NoSQL', 'GCP', 'Spark', 'Data Warehousing', 'Data Modeling', 'ETL', 'AWS', 'Python']",2025-06-14 05:08:00
Data Engineer,Shyftlabs,5 - 10 years,Not Disclosed,['Noida'],"Position Overview\nWe are looking for an experienced Lead Data Engineer to join our dynamic team. If you are passionate about building scalable software solutions, and work collaboratively with cross-functional teams to define requirements and deliver solutions we would love to hear from you.\nJob Responsibilities:\nDevelop and maintain data pipelines and ETL/ELT processes using Python\nDesign and implement scalable, high-performance applications\nWork collaboratively with cross-functional teams to define requirements and deliver solutions\nDevelop and manage near real-time data streaming solutions using Pub, Sub or Beam.\nContribute to code reviews, architecture discussions, and continuous improvement initiatives\nMonitor and troubleshoot production systems to ensure reliability and performance\nBasic Qualifications:\n5+ years of professional software development experience with Python\nStrong understanding of software engineering best practices (testing, version control, CI/CD)\nExperience building and optimizing ETL/ELT processes and data pipelines\nProficiency with SQL and database concepts\nExperience with data processing frameworks (e.g., Pandas)\nUnderstanding of software design patterns and architectural principles\nAbility to write clean, well-documented, and maintainable code\nExperience with unit testing and test automation\nExperience working with any cloud provider (GCP is preferred)\nExperience with CI/CD pipelines and Infrastructure as code\nExperience with Containerization technologies like Docker or Kubernetes\nBachelors degree in Computer Science, Engineering, or related field (or equivalent experience)\nProven track record of delivering complex software projects\nExcellent problem-solving and analytical thinking skills\nStrong communication skills and ability to work in a collaborative environment\nPreferred Qualifications:\nExperience with GCP services, particularly Cloud Run and Dataflow\nExperience with stream processing technologies (Pub/Sub)\nFamiliarity with big data technologies (Airflow)\nExperience with data visualization tools and libraries\nKnowledge of CI/CD pipelines with Gitlab and infrastructure as code with Terraform\nFamiliarity with platforms like Snowflake, Bigquery or Databricks,.\nGCP Data engineer certification",Industry Type: Management Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Software design', 'Version control', 'Analytical', 'Data processing', 'Unit testing', 'data visualization', 'Continuous improvement', 'SQL', 'Python']",2025-06-14 05:08:02
Data Engineer with API Development,Ntrix Innovations,7 - 12 years,Not Disclosed,[],"Job Title -Data Engineer with API Development\nExp - 7+ years\nLocation- Remote\nShift timing- 11am to 8:30pm\nWork type: Contract\n\n\n7+ Years of Overall IT experience.\n3+ Years of experience - Azure Architecture / Engineering (one or more of: Azure Functions, App Services, API Development)\n3+ Years of experience - Development (ex. Python, C#, Go or other)\n1+ CI/CD Experience (GitHub preferred)\n2+ Years of API Development experience (Creating APIs and Consuming APIs)\n\nNice to Have:\n\n- Service Bus\n- Terraform\n- ADF\n- Data Bricks\n- Spark\n- Scala",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Temporary/Contractual","['C#', 'Data Engineering', 'API Development', 'Python', 'github', 'Azure Architecture', 'Azure engineer', 'Terraform', 'Azure Functions', 'Azure App Service', 'cicd', 'azure data factory']",2025-06-14 05:08:05
MDM Data Engineer,Amgen Inc,4 - 9 years,Not Disclosed,['Hyderabad'],"ABOUT THE ROLE\nRole Description:\nWe are seeking an experienced MDM Senior Data Engineer with 6-9 years of experience and expertise in backend engineering to work closely with business on development and operations of our Master Data Management (MDM) platforms, with hands-on experience in Informatica or Reltio and data engineering experience.\nThis role will also involve guiding junior data engineers/analysts, and quality experts to deliver high-performance, scalable, and governed MDM solutions that align with enterprise data strategy.",,,,"['Data Engineering', 'Unix', 'data modeling', 'PySpark', 'Informatica', 'AWS cloud', 'Reltio', 'Python', 'SQL']",2025-06-14 05:08:07
Data Engineer--Operations,Robert Bosch Engineering and Business Solutions Private Limited,2 - 6 years,Not Disclosed,['Bengaluru'],"As a Data engineer in Operations, you will work on the operational management, monitoring, and support of scalable data pipelines running in Azure Databricks, Hadoop and Radium. You will ensure the reliability, performance, and availability of data workflows and maintain production environments. You will collaborate closely with data engineers, architects, and platform teams to implement best practices in data pipeline operations and incident management to ensure data availability and data completeness.\nPrimary responsibilities:\nOperational support and incident management for Azure Databricks, Hadoop, Radium data pipelines.\nCollaborating with data engineering and platform teams to define and enforce operational standards, SLAs, and best practices.\nDesigning and implementing monitoring, alerting, and logging solutions for Azure Databricks pipelines.\nCoordinating with central teams to ensure compliance with organizational operational standards and security policies.\nDeveloping and maintaining runbooks, SOPs, and troubleshooting guides for pipeline issues.\nManaging the end-to-end lifecycle of data pipeline incidents, including root cause analysis and remediation.\nOverseeing pipeline deployments, rollbacks, and change management using CI/CD tools such as Azure DevOps.\nEnsuring data quality and validation checks are effectively monitored in production.\nWorking closely with platform and infrastructure teams to address pipeline and environment-related issues.\nProviding technical feedback and mentoring junior operations engineers.\nConducting peer reviews of operational scripts and automation code.\nAutomating manual operational tasks using Scala and Python scripts.\nManaging escalations and coordinating critical production issue resolution.\nParticipating in post-mortem reviews and continuous improvement initiatives for data pipeline operations.",Industry Type: Automobile,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Automation', 'Change management', 'SCALA', 'Incident management', 'Data quality', 'Troubleshooting', 'Operations', 'Monitoring', 'Python']",2025-06-14 05:08:09
Data Engineer II,Flipkart,1 - 3 years,Not Disclosed,['Bengaluru'],"Skills Required :\nKafka, Spark Streaming. Proficiency in one of the programming languages preferably Java, Scala or Python.\nEducation/Qualification :\nBachelor's Degree in Computer Science, Engineering, Technology or related field\nDesirable Skills :\nKafka, Spark Streaming. Proficiency in one of the programming languages preferably Java, Scala or Python.",,,,"['Data Engineering', 'Scala', 'Kafka', 'Python', 'java', 'Spark Streaming']",2025-06-14 05:08:12
Data Engineer II,Amazon,3 - 8 years,Not Disclosed,['Bengaluru'],"Amazon s Consumer Payments organization is seeking a highly quantitative, experienced Data Engineer to drive growth through analytics, automation of data pipelines, and enhancement of self-serve experiences. . You will succeed in this role if you are an organized self-starter who can learn new technologies quickly and excel in a fast-paced environment. In this position, you will be a key contributor and sparring partner, developing analytics and insights that global executive management teams and business leaders will use to define global strategies and deep dive businesses.\nYou will be part the team that is focused on acquiring new merchants from around the world to payments around the world. The position is based in India but will interact with global leaders and teams in Europe, Japan, US, and other regions. You should be highly analytical, resourceful, customer focused, team oriented, and have an ability to work independently under time constraints to meet deadlines. You will be comfortable thinking big and diving deep. A proven track record in taking on end-to-end ownership and successfully delivering results in a fast-paced, dynamic business environment is strongly preferred.\nResponsibilities include but not limited to:\nDesign, develop, implement, test, and operate large-scale, high-volume, high-performance data structures for analytics and Reporting.\nImplement data structures using best practices in data modeling, ETL/ELT processes, and SQL, AWS Redshift, and OLAP technologies, Model data and metadata for ad hoc and pre-built reporting.\nWork with product tech teams and build robust and scalable data integration (ETL) pipelines using SQL, Python and Spark.\nContinually improve ongoing reporting and analysis processes, automating or simplifying self-service support for customers.\nInterface with business customers, gathering requirements and delivering complete reporting solutions.\nCollaborate with Analysts, Business Intelligence Engineers and Product Managers to implement algorithms that exploit rich data sets for statistical analysis, and machine learning.\nParticipate in strategic tactical planning discussions, including annual budget processes.\nCommunicate effectively with product / business / tech-teams / other Data teams.\n3+ years of data engineering experience\nExperience with data modeling, warehousing and building ETL pipelines Experience with AWS technologies like Redshift, S3, AWS Glue, EMR, Kinesis, FireHose, Lambda, and IAM roles and permissions\nExperience with non-relational databases / data stores (object storage, document or key-value stores, graph databases, column-family databases)",,,,"['Automation', 'metadata', 'Data modeling', 'Machine learning', 'Data structures', 'OLAP', 'Business intelligence', 'Analytics', 'SQL', 'Python']",2025-06-14 05:08:14
Data Engineer,Tek Ninjas,8 - 13 years,Not Disclosed,['Pune'],"Must have:-\n\n• Strong communicator, from making presentations to technical writing\n• Experience in financial domain preferably in the areas of Financial accounting and Risk.\n• Strong data analysis and validation experience with attention to detail and understanding patterns\n• Good understanding of Credit Risk, workflow and reporting solutions\n• Have a flair for technology and are adept at leveraging the latest tools and technologies to increase team productivity and collaboration across dispersed teams\n• Experience in Python, SQL and PL/SQL (Oracle) and writing queries, scripts.\n• A real passion for and experience of Agile working practices, with a strong desire to work with baked in quality subject areas such as TDD, BDD, test automation and DevOps principles\n• Experience using DevOps toolsets like GitLab\n• Has prior experience of working on databases and usage of SQL to perform data analysis, preferably cloud technologies\n• Familiar with Azure Native Cloud services, software design and enterprise integration patterns.\n• Experience working with / exposure to NLP, Gen AI, ML and data modelling projects, a plus\n• Has prior experience of working on IT projects and has knowledge and experience of software development life cycle using Agile methodology\n• Is well structured, very reliable and dedicated; has high attention to detail, has ability to handle a significant number of dependencies and issues ensuring nothing is missed out.\n• Has excellent written and verbal communication skills, inter-personal and negotiation skills\n• Ability to work as part of a global team with multiple delivery teams and engage with stakeholders at various levels\n• Able to challenge the status quo and if required have the ability to push-back demands from stakeholders with right justification\n• Someone with a general ability to pick up information quickly and turn around deliverables under pressure\n• A team player with excellent people management skills\n• Takes ownership of tasks assigned to ultimate resolution\n• Accuracy and timeliness of delivering solutions using the best IT standards and practices",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Gen Ai', 'AWS', 'Machine Learning', 'Python', 'SQL', 'Generative Ai', 'Data Engineering', 'Ml']",2025-06-14 05:08:16
Azure Databricks Data Engineer / Developer,Infosys,2 - 5 years,Not Disclosed,['Bengaluru'],"Educational Requirements\nMCA,MSc,Bachelor of Engineering,BBA,BCom,BSc\nService Line\nData & Analytics Unit\nResponsibilities\nA day in the life of an Infoscion\nAs part of the Infosys delivery team, your primary role would be to interface with the client for quality assurance, issue resolution and ensuring high customer satisfaction.\nYou will understand requirements, create and review designs, validate the architecture and ensure high levels of service offerings to clients in the technology domain.\nYou will participate in project estimation, provide inputs for solution delivery, conduct technical risk planning, perform code reviews and unit test plan reviews.\nYou will lead and guide your teams towards developing optimized high quality code deliverables, continual knowledge management and adherence to the organizational guidelines and processes.\nYou would be a key contributor to building efficient programs/ systems and if you think you fit right in to help our clients navigate their next in their digital transformation journey, this is the place for you!If you think you fit right in to help our clients navigate their next in their digital transformation journey, this is the place for you!\nAdditional Responsibilities:\nKnowledge of more than one technology\nBasics of Architecture and Design fundamentals\nKnowledge of Testing tools\nKnowledge of agile methodologies\nUnderstanding of Project life cycle activities on development and maintenance projects\nUnderstanding of one or more Estimation methodologies, Knowledge of Quality processes\nBasics of business domain to understand the business requirements\nAnalytical abilities, Strong Technical Skills, Good communication skills\nGood understanding of the technology and domain\nAbility to demonstrate a sound understanding of software quality assurance principles, SOLID design principles and modelling methods\nAwareness of latest technologies and trends\nExcellent problem solving, analytical and debugging skills\nTechnical and Professional Requirements:\nPrimary skills:Technology->Cloud Platform->Azure Development & Solution Architecting\nPreferred Skills:\nTechnology->Cloud Platform->Azure Development & Solution Architecting",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Databricks', 'Azure Development', 'Testing tools', 'agile methodologies', 'debugging', 'Cloud Platform', 'software quality assurance']",2025-06-14 05:08:19
Data Engineer-Data Platforms,IBM,2 - 5 years,Not Disclosed,['Bengaluru'],"As a Data Engineer at IBM, you'll play a vital role in the development, design of application, provide regular support/guidance to project teams on complex coding, issue resolution and execution.\n\nYour primary responsibilities include:\nLead the design and construction of new solutions using the latest technologies, always looking to add business value and meet user requirements.\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\n\n\n\n\nPreferred technical and professional experience",,,,"['python', 'data processing', 'pyspark', 'data modeling', 'spark', 'data analysis', 'microsoft azure', 'data warehousing', 'cloud platforms', 'numpy', 'data engineering', 'sql', 'pandas', 'spring boot', 'etl pipelines', 'java', 'gcp', 'kafka', 'data warehousing concepts', 'hadoop', 'big data', 'aws', 'etl']",2025-06-14 05:08:22
Deputy Manager - Data Engineer - Analytics,IBM,2 - 7 years,Not Disclosed,['Bengaluru'],"Develop, test and support future-ready data solutions for customers across industry verticals\nDevelop, test, and support end-to-end batch and near real-time data flows/pipelines\nDemonstrate understanding in data architectures, modern data platforms, big data, analytics, cloud platforms, data governance and information management and associated technologies\nCommunicates risks and ensures understanding of these risks.\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\n\n\nPreferred technical and professional experience",,,,"['information management', 'cloud platforms', 'data architecture', 'data governance', 'big data', 'schema', 'python', 'data analytics', 'datastage', 'microsoft azure', 'warehouse', 't-sql', 'data engineering', 'ansible', 'docker', 'sql', 'java', 'devops', 'linux', 'olap', 'jenkins', 'shell scripting', 'etl', 'aws']",2025-06-14 05:08:25
"Data Engineer II, SCOT - AIM",Amazon,3 - 8 years,Not Disclosed,['Bengaluru'],"SCOTs Automated Inventory Management (AIM) team seeks talented individuals passionate about solving complex problems and driving impactful business decisions for our executives. The AIM team owns critical Tier 1 metrics for Amazon Retail stores, providing key insights to improve store health monitoring. We focus on enhancing selection, product availability, inventory efficiency, and inventory readiness to fulfill customer orders (FastTrack) while enabling accelerated delivery Speed Fulfillment Worldwide. This improves both Customer Experience (CX) and Long-Term Free Cash Flow (LTFCF) outcomes. Our approach involves creating standardized, scalable, and automated systems and tools to identify and reduce supply chain defects in our systems and inputs, while driving operational leverage and scaling.\n\nAs a Data Engineer, you will analyze large-scale business data, solve real-world problems, and develop metrics and business cases to delight our customers worldwide. You will work closely with Scientists, Engineers, and Product Managers to build scalable, high-impact products, architect data pipelines, and transform data into actionable insights to manage business at scale. We are looking for people who are motivated by thinking big, moving fast, and exploring business insights. If you love to implement solutions to hard problems while working hard, having fun, and making history, this may be the opportunity for you.\n\nAbout the team\nSupply Chain Optimization Technologies (SCOT) is the name of a complex group of systems designed to make the best decisions when it comes to forecasting, buying, placing, and shipping inventory. Functionally these teams work together to drive in-stock, drive placement, drive inventory removal and manage the customer experience.\n\n3+ years of data engineering experience\n4+ years of SQL experience\nExperience with data modeling, warehousing and building ETL pipelines Experience with AWS technologies like Redshift, S3, AWS Glue, EMR, Kinesis, FireHose, Lambda, and IAM roles and permissions\nExperience with non-relational databases / data stores (object storage, document or key-value stores, graph databases, column-family databases)\nExperience with big data technologies such as: Hadoop, Hive, Spark, EMR",,,,"['Supply chain', 'data engineer ii', 'Data modeling', 'Inventory management', 'Cash flow', 'Customer experience', 'Forecasting', 'Operations', 'Monitoring', 'SQL']",2025-06-14 05:08:27
Data Engineer-Data Platforms-Google,IBM,5 - 7 years,Not Disclosed,['Bengaluru'],"A career in IBM Consulting is rooted by long-term relationships and close collaboration with clients across the globe.You'll work with visionaries across multiple industries to improve the hybrid cloud and Al journey for the most innovative and valuable companies in the world. Your ability to accelerate impact and make meaningful change for your clients is enabled by our strategic partner ecosystem and our robust technology platforms across the IBM portfolio; including Software and Red Hat.\n\nIn your role, you will be responsible for:\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\n\n\nPreferred technical and professional experience",,,,"['sql', 'java', 'html', 'python', 'javascript', 'hive', 'css', 'confluence', 'aem', 'data warehousing', 'apache sling', 'jquery', 'gen', 'git', 'gcp', 'spark', 'jenkins', 'bigquery', 'data transformation', 'hadoop', 'big data', 'etl', 'jira', 'cloud sql', 'maven', 'airflow', 'osgi', 'granite', 'agile', 'sqoop', 'aws']",2025-06-14 05:08:30
Data Engineer-Data Platforms-Google,IBM,5 - 7 years,Not Disclosed,['Bengaluru'],"Skilled Multiple GCP services - GCS, BigQuery, Cloud SQL, Dataflow, Pub/Sub, Cloud Run, Workflow, Composer, Error reporting, Log explorer etc.\nMust have Python and SQL work experience & Proactive, collaborative and ability to respond to critical situation\nAbility to analyse data for functional business requirements & front face customer\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\n\n\nPreferred technical and professional experience",,,,"['sql', 'gcp', 'bigquery', 'cloud sql', 'python', 'hive', 'gen', 'java', 'postgresql', 'spark', 'linux', 'mysql', 'hadoop', 'big data', 'pubsub', 'airflow', 'application engine', 'machine learning', 'sql server', 'dataproc', 'cloud storage', 'bigtable', 'agile', 'sqoop', 'aws', 'data flow']",2025-06-14 05:08:33
Gcp Data Engineer,Tata Consultancy Services,7 - 10 years,Not Disclosed,"['Pune', 'Chennai', 'Bengaluru']","As a GCP data engineer the colleague should be able to designs scalable data architectures on Google Cloud Platform, using services like Big Query and Dataflow. They write and maintain code (Python, Java), ensuring efficient data models and seamless ETL processes. Quality checks and governance are implemented to maintain accurate and reliable data.\nSecurity is a priority, enforcing measures for storage, transmission, and processing, while ensuring compliance with data protection standards. Collaboration with cross-functional teams is key for understanding diverse data requirements. Comprehensive documentation is maintained for data processes, pipelines, and architectures.",,,,"['Big query', 'GCP', 'Bigquery', 'GCP Data Engineer - GCP', 'Python', 'Cloud Composer', 'Groovy', 'Data flow', 'SQL', 'air flow']",2025-06-14 05:08:35
Supply Chain Analyst - Data Engineering,TE Connectivity,5 - 10 years,Not Disclosed,['Bengaluru'],,,,,"['azure databricks', 'supply chain', 'azure data factory', 'data engineering', 'data modeling', 'azure data warehouse', 'python', 'ssas', 'power bi', 'warehouse', 'dashboards', 'supply', 'sql server', 'sql', 'transportation', 'sql server integration services', 'data center', 'spark', 'ssrs', 'etl', 'ssis', 'msbi']",2025-06-14 05:08:38
Data Engineer,Mobio Solutions,3 - 5 years,Not Disclosed,['Ahmedabad'],"Job Overview:\nWe are looking for a skilled and experienced Data Engineer to join our team. The ideal candidate will have a strong background in Azure Data Factory, Databricks, Pyspark, Python , Azure SQL and other Azure cloud services, and will be responsible for building and managing scalable data pipelines, data lakes, and data warehouses . Experience with Azure Synapse Analytics, Microsoft Fabric or PowerBI will be considered a strong advantage.\nKey Responsibilities:\nDesign, develop, and manage robust and scalable ETL/ELT pipelines using Azure Data Factory and Databricks\nWork with PySpark and Python to transform and process large datasets\nBuild and maintain data lakes and data warehouses on Azure Cloud\nCollaborate with data architects, analysts, and stakeholders to gather and translate requirements into technical solutions\nEnsure data quality, consistency, and integrity across systems\nOptimize performance and cost of data pipelines and cloud infrastructure\nImplement best practices for security, governance, and monitoring of data pipelines\nMaintain and document data workflows and architecture\nRequired Skills & Qualifications:\n3-5 years of experience in Data Engineering\nStrong hands-on experience with:\nAzure Data Factory (ADF)\nAzure Databricks\nAzure SQL\nPySpark and Python\nAzure Storage (Blob, Data Lake Gen2)\nHands-on experience with data warehouse/Lakehouse/data lake architecture\nFamiliarity with Delta Lake, MLflow, and Unity Catalog is a plus\nGood understanding of SQL and performance tuning\nKnowledge of CI/CD in Azure for data pipelines\nExcellent problem-solving skills and ability to work independently\nPreferred Skills:\nExperience with Azure Synapse Analytics\nFamiliarity with Microsoft Fabric\nWorking knowledge of Power BI for data visualization and dashboarding\nExposure to DevOps and infrastructure as code (IaC) in Azure\nUnderstanding of data governance and security best practices\nDatabricks certification (e.g., Databricks Certified Data Engineer Associate/Professional)",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Performance tuning', 'Cloud', 'data governance', 'Data quality', 'data visualization', 'microsoft', 'Analytics', 'Monitoring', 'SQL', 'Python']",2025-06-14 05:08:40
Data Engineer,Hinduja Tech,6 - 10 years,Not Disclosed,['Pune'],"Education: Bachelors or masters degree in computer science, Information Technology, Engineering, or a related field.Experience: 6-10 years\n8+ years of experience in data engineering or a related field.\nStrong hands-on experience with Azure Databricks, Spark, Python/Scala, CICD, Scripting for data processing.\nExperience working in multiple file formats like Parquet, Delta, and Iceberg.\nKnowledge of Kafka or similar streaming technologies for real-time data ingestion.",,,,"['Data Engineer', 'Azure Databricks', 'ETL', 'Pyspark', 'AWS', 'Python', 'SQL']",2025-06-14 05:08:42
Data Engineer,Digital Aptech,3 - 8 years,Not Disclosed,['Kolkata'],"Data Engineer || Product Based MNC (Direct Payroll) || Kolkata Location\nRole & responsibilities :\nUnderstands requirements and is involved in the discussions relating to technical and functional design of the sprint/ module/project\nDesign and implement end-to-end data solutions (storage, integration, processing, and visualization) in Azure.\nUsed various sources to ingest data into Azure Data Factory ,Azure Data Lake Storage (ADLS) such as SQL Server, Excel, Oracle, SQL Azure etc.\nExtract data from one database and load it into another\nBuild data architecture for ingestion, processing, and surfacing of data for large-scale applications\nUse many different scripting languages, understanding the nuances and benefits of each, to combine systems\nResearch and discover new methods to acquire data, and new applications for existing data\nWork with other members of the data team, including data architects, data analysts, and data scientists\nPrepare data sets for analysis and interpretation\nPerform statistical analysis and fine-tuning using test results\nCreate libraries and extend the existing frameworks\nCreate design documents basis discussions and assists in providing technical solutions for the business process\nPreferred candidate profile\nIn-depth understanding of database management systems, online analytical processing (OLAP) and ETL (Extract, transform, load) framework\n3+ years of overall experience with Azure, Data Factory and .Net\nStrong in Data factory and should be able to create manual and auto trigger pipelines\nShould be able to create, update, edit and delete ETL jobs in Azure Synapse Analytics\nRecreate existing application logic and functionality in the Azure Data Lake, Data Factory, SQL Database and SQL data warehouse environment.\nKnowledge of SQL queries, SQL Server Reporting Services (SSRS) and SQL Server Integration Services (SSIS)\nProven abilities to take initiative and be innovative\nAnalytical mind with a problem-solving aptitude\n10 LPA - 15 LPA Apply for this position Allowed Type(s): .pdf, .doc, .docx By using this form you agree with the storage and handling of your data by this website. * Your Next Step Towards Success Starts Here Why Choose Us\nFree Expert Consultation\nHave an idea but unsure how to execute it? Our industry experts offer free feasibility checks, expert advice, and actionable strategies tailored to your goals at no cost!\nComplimentary Technical Project Manager\nEvery project comes with a Complimentary Technical Project Manager to ensure smooth project management, offer valuable development guidance and keep everything on track.",Industry Type: Internet,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Business process', 'Payroll', 'Project management', 'Consulting', 'SSRS', 'OLAP', 'HTML', 'SSIS', 'Analytics', 'Data architecture']",2025-06-14 05:08:44
DATA ENGINEER,Svitla Systems,4 - 9 years,Not Disclosed,[],"Svitla Systems Inc. is looking for a Data Engineer (with ML/AI Experience) for a full-time position (40 hours per week) in India . Our client is the world s largest travel guidance platform, helping hundreds of millions each month become better travelers, from planning to booking to taking a trip. Travelers across the globe use the site and app to discover where to stay, what to do, and where to eat based on guidance from those who have been there before.\nWith more than 1 billion reviews and opinions from nearly 8 million businesses, travelers turn to clients to find deals on accommodations, book experiences, and reserve tables at delicious restaurants. They discover great places nearby as a travel guide company, available in 43 markets and 22 languages.\nAs a member of the Data Platform Enterprise Services Team, you will collaborate with engineering and business stakeholders to build, optimize, maintain, and secure the full data vertical, including tracking instrumentation, information architecture, ETL pipelines, and tooling that provide key analytics insights for business-critical decisions at the highest levels of product, finance, sales, CRM, marketing, data science, and more, all in a dynamic environment of continuously modernizing tech stack including highly scalable architecture, cloud-based infrastructure, and real-time responsiveness.\nRequirements:\nBS/MS in Computer Science or related field.\n4+ years of experience in data engineering or software development.\nProven data design and modeling with large datasets ( star/snowflake schema , SCDs, etc.).\nStrong SQL skills and ability to query large datasets.\nExperience with modern cloud data warehouses : Snowflake , BigQuery , etc.\nETL development experience: SLA , performance , and monitoring .\nFamiliarity with BI tools and semantic layer principles (e.g., Looker , Tableau ).\nUnderstanding of CI/CD , testing , documentation practices.\nComfortable in a fast-paced , dynamic environment .\nAbility to collaborate cross-functionally and communicate with technical/non-technical peers.\nStrong data investigation and problem-solving abilities.\nComfortable with ambiguity and focused on clean, maintainable data architecture .\nDetail-oriented with a strong sense of ownership .\nNice to Have\nExperience with data governance , data transformation tools .\nPrior work with e-commerce platforms .\nExperience with Airflow , Dagster , Monte Carlo , or Knowledge Graphs .\nResponsibilities:\nCollaborate with stakeholders from multiple teams to collect business requirements and translate them into technical data model solutions .\nDesign, build, and maintain efficient, scalable, and reusable data models in cloud data warehouses (e.g., Snowflake , BigQuery ).\nTransform data from many sources into clean, curated, standardized, and trustworthy data products .\nBuild data pipelines and ETL processes handling terabytes of data .\nAnalyze data using SQL and dashboards; ensure models align with business needs.\nEnsure data quality through testing, observability tools , and monitoring .\nTroubleshoot complex data issues , validate assumptions, and trace anomalies.\nParticipate in code reviews and help improve data development standards .\nWe offer\nUS and EU projects based on advanced technologies.\nCompetitive compensation based on skills and experience.\nAnnual performance appraisals.\nRemote-friendly culture and no micromanagement.\nPersonalized learning program tailored to your interests and skill development.\nBonuses for article writing, public talks, other activities.\n15 PTO days, 10 national holidays.\nFree webinars, meetups and conferences organized by Svitla.\nFun corporate celebrations and activities.\nAwesome team, friendly and supportive community!",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Article writing', 'Cloud', 'Schema', 'Instrumentation', 'Data quality', 'Analytics', 'Monitoring', 'CRM', 'SQL']",2025-06-14 05:08:46
S&C Global Network - AI - CG&S - Data Engineer Consultant,Accenture,3 - 7 years,Not Disclosed,['Bengaluru'],"Job Title:Industry & Function AI Data Engineer + S&C GN\n\n\n\nManagement Level:09 - Consultant\n\n\n\nLocation:Primary - Bengaluru, Secondary - Gurugram\n\n\n\nMust-Have Skills:Data Engineering expertise, Cloud platforms:AWS, Azure, GCP, Proficiency in Python, SQL, PySpark and ETL frameworks\n\n\n\nGood-to-Have Skills:LLM Architecture, Containerization tools:Docker, Kubernetes, Real-time data processing tools:Kafka, Flink, Certifications like AWS Certified Data Analytics Specialty, Google Professional Data Engineer,Snowflake,DBT,etc.\n\n\n\nJob\n\n\nSummary:\n\nAs a Data Engineer, you will play a critical role in designing, implementing, and optimizing data infrastructure to power analytics, machine learning, and enterprise decision-making. Your work will ensure high-quality, reliable data is accessible for actionable insights. This involves leveraging technical expertise, collaborating with stakeholders, and staying updated with the latest tools and technologies to deliver scalable and efficient data solutions.\n\n\n\n\nRoles & Responsibilities:\nBuild and Maintain Data Infrastructure:Design, implement, and optimize scalable data pipelines and systems for seamless ingestion, transformation, and storage of data.\nCollaborate with Stakeholders:Work closely with business teams, data analysts, and data scientists to understand data requirements and deliver actionable solutions.\nLeverage Tools and Technologies:Utilize Python, SQL, PySpark, and ETL frameworks to manage large datasets efficiently.\nCloud Integration:Develop secure, scalable, and cost-efficient solutions using cloud platforms such as Azure, AWS, and GCP.\nEnsure Data Quality:Focus on data reliability, consistency, and quality using automation and monitoring techniques.\nDocument and Share Best Practices:Create detailed documentation, share best practices, and mentor team members to promote a strong data culture.\nContinuous Learning:Stay updated with the latest tools and technologies in data engineering through professional development opportunities.\n\n\n\n\n\nProfessional & Technical\n\n\n\n\nSkills:\n\nStrong proficiency in programming languages such as Python, SQL, and PySpark\nExperience with cloud platforms (AWS, Azure, GCP) and their data services\nFamiliarity with ETL frameworks and data pipeline design\nStrong knowledge of traditional statistical methods, basic machine learning techniques.\nKnowledge of containerization tools (Docker, Kubernetes)\nKnowing LLM, RAG & Agentic AI architecture\nCertification in Data Science or related fields (e.g., AWS Certified Data Analytics Specialty, Google Professional Data Engineer)\n\n\n\n\n\nAdditional Information:\n\nThe ideal candidate has a robust educational background in data engineering or a related field and a proven track record of building scalable, high-quality data solutions in the Consumer Goods sector.\n\nThis position offers opportunities to design and implement cutting-edge data systems that drive business transformation, collaborate with global teams to solve complex data challenges and deliver measurable business outcomes and enhance your expertise by working on innovative projects utilizing the latest technologies in cloud, data engineering, and AI.\n\n\n\nAbout Our Company | Accenture\n\nQualification\n\n\n\nExperience:Minimum 3-7 years in data engineering or related fields, with a focus on the Consumer Goods Industry\n\n\n\n\nEducational Qualification:Bachelors or Masters degree in Computer Science, Information Systems, Engineering, or a related field",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'pyspark', 'data engineering', 'sql', 'machine learning algorithms', 'kubernetes', 'snowflake', 'data analytics', 'microsoft azure', 'cloud platforms', 'machine learning', 'apache flink', 'artificial intelligence', 'docker', 'pipeline', 'data science', 'gcp', 'kafka', 'aws', 'etl', 'etl scripts']",2025-06-14 05:08:49
Data Engineer,Smartavya Analytica,3 - 5 years,Not Disclosed,['Pune'],"Job Title: Data Engineer\nLocation: Pune, India (On-site)\nExperience: 3 5 years\nEmployment Type: Full-time\n\nJob Summary\nWe are looking for a hands-on Data Engineer who can design and build modern Lakehouse solutions on Microsoft Azure. You will own data ingestion from source-system APIs through Azure Data Factory into OneLake, curate bronze silver gold layers on Delta Lake, and deliver dimensional models that power analytics at scale.",,,,"['Azure Data Factory', 'Azure Synapse', 'Adls Gen2', 'Data Lake', 'Fabric']",2025-06-14 05:08:51
Data Engineer,Fortune India 500 Chemicals Firm,12 - 18 years,Not Disclosed,['Mumbai (All Areas)'],"Skills:\nData Management: Expertise in data warehousing, SQL/NoSQL, cloud platforms (AWS, Azure, GCP)\nETL Tools: Proficient in Informatica, Talend, Azure Data Factory\nModelling: Strong in dimensional modelling, star/snowflake schema\nGovernance & Compliance: Knowledge of GDPR, HIPAA, data governance frameworks\nLanguages: T-SQL, PL/SQL\nSoft Skills: Effective communicator, strong analytical and problem-solving skills\nKey Responsibilities:\nArchitecture: Designed scalable, high-performance data warehouse architectures and data models\nETL & Integration: Led ETL design/development for structured/unstructured data across platforms\nGovernance: Defined data quality standards and collaborated on data governance policy implementation\nCollaboration: Interfaced with BI, data science, and business teams to align data strategies\nPerformance & Security: Optimized queries/ETL jobs and ensured data security and compliance\nDocumentation: Maintained standards and documentation for architecture, ETL, and workflows",Industry Type: Chemicals,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Warehousing', 'GCP', 'Snowflake', 'Microsoft Azure', 'Dimensional Modeling', 'Data Modeling', 'ETL', 'AWS']",2025-06-14 05:08:53
Sales Excellence - COE - Data Engineering Specialist,Accenture,5 - 8 years,Not Disclosed,['Bengaluru'],"Job Title -\n\n\n\nSales Excellence - COE - Data Engineering Specialist\n\n\n\nManagement Level:\n\n\n\n9-Team Lead/Consultant\n\n\n\nLocation:\n\n\n\nMumbai, MDC2C\n\n\n\nMust-have skills:Sales\n\n\n\n\nGood to have skills:Data Science, SQL, Automation, Machine Learning\n\n\n\nJob\n\n\nSummary:\n\nApply deep statistical tools and techniques to find relationships between variables\n\n\n\n\nRoles & Responsibilities:\n\n- Apply deep statistical tools and techniques to find relationships between variables.\n\n- Develop intellectual property for analytical methodologies and optimization techniques.\n\n- Identify data requirements and develop analytic solutions to solve business issues.\n\nJob Title - Analytics & Modelling Specialist\n\nManagement Level :9-Specialist\n\nLocation:Bangalore/ Gurgaon/Hyderabad/Mumbai\n\nMust have skills:Python, Data Analysis, Data Visualization, SQL\nGood to have skills:Machine Learning\n\nJob\n\n\nSummary:\n\nThe Center of Excellence (COE) makes sure that the sales and pricing methods and offerings of Sales Excellence are effective.\n\n- The COE supports salespeople through its business partners and Analytics and Sales Operations teams.\n\nThe Data Engineer helps manage data sources and environments, utilizing large data sets and maintaining their integrity to create models and apps that deliver insights to the organization.\nRoles & Responsibilities:\n\nBuild and manage data models that bring together data from different sources.\n\nHelp consolidate and cleanse data for use by the modeling and development teams.\n\nStructure data for use in analytics applications.\n\nLead a team of Data Engineers effectively.\nProfessional & Technical\n\n\n\n\nSkills:\nA bachelors degree or equivalent\n\nTotal experience Range:5-8 years in the relevant field\n\nA minimum of 3 years of GCP experience with exposure to machine learning/data science\n\nExperience in configuration the machine learning workflow in GCP.\n\nA minimum of 5 years Advanced SQL knowledge and experience working with relational databases\n\nA minimum of 3 years Familiarity and hands on experience in different SQL objects like stored procedures, functions, views etc.,\n\nA minimum of 3 years Building of data flow components and processing systems to extract, transform, load and integrate data from various sources.\n\nA minimum of 3 years Hands on experience in advanced excel topics such as cube functions, VBA Automation, Power Pivot etc.\n\nA minimum of 3 years Hands on experience in Python\nAdditional Information:\n\nUnderstanding of sales processes and systems.\n\nMasters degree in a technical field.\n\nExperience with quality assurance processes.\n\nExperience in project management.\n\nYou May Also Need:\n\nAbility to work flexible hours according to business needs.\n\nMust have good internet connectivity and a distraction-free environment for working at home, in accordance with local guidelines.\n\n\n\n\nProfessional & Technical\n\n\n\n\nSkills:\n\n\n- Relevant experience in the required domain.\n\n- Strong analytical, problem-solving, and communication skills.\n\n- Ability to work in a fast-paced, dynamic environment.\n\n\n\n\nAdditional Information:\n\n- Opportunity to work on innovative projects.\n\n- Career growth and leadership exposure.\n\n\n\n\n\nAbout Our Company | AccentureQualification\n\n\n\nExperience:8 to 10 Years\n\n\n\n\nEducational Qualification:\n\n\n\nB.Com",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'data analysis', 'sales', 'sql', 'data visualization', 'hive', 'advance sql', 'ssas', 'dbms', 'machine learning', 'data engineering', 'power pivot', 'sql server', 'vba automation', 'data science', 'gcp', 'spark', 'advanced excel', 'hadoop', 'ssis', 'etl', 'big data', 'data flow', 'sql joins']",2025-06-14 05:08:55
Cloud Data Engineer,Wipro,8 - 12 years,Not Disclosed,"['Hyderabad', 'Pune', 'Bengaluru']",Role: Cloud Data Engineer\nLocation: Wipro PAN India\nHybrid 3 days in Wipro office\n\nJD:\nStrong - SQL\nStrong - Python,,,,"['SQL', 'Python', 'AZURE', 'GCP', 'AWS']",2025-06-14 05:08:57
Lead Data Engineer,Jio,6 - 11 years,Not Disclosed,['Mumbai (All Areas)'],"Overview of the Company:\nJio Platforms Ltd. is a revolutionary Indian multinational tech company, often referred to as India's biggest startup, headquartered in Mumbai. Launched in 2019, it's the powerhouse behind Jio, India's largest mobile network with over 400 million users. But Jio Platforms is more than just telecom. It's a comprehensive digital ecosystem, developing cutting-edge solutions across media, entertainment, and enterprise services through popular brands like JioMart, JioFiber, and JioSaavn.\nJoin us at Jio Platforms and be part of a fast-paced, dynamic environment at the forefront of India's digital transformation. Collaborate with brilliant minds to develop next-gen solutions that empower millions and revolutionize industries.\n\nTeam Overview:\nThe Data Platforms Team is the launchpad for a data-driven future, empowering the Reliance Group of Companies. We're a passionate group of experts architecting an enterprise-scale data mesh to unlock the power of big data, generative AI, and ML modelling across various domains. We don't just manage data we transform it into intelligent actions that fuel strategic decision-making. Imagine crafting a platform that automates data flow, fuels intelligent insights, and empowers the organization that's what we do.\nJoin our collaborative and innovative team, and be a part of shaping the future of data for India's biggest digital revolution! About the role.\n\nTitle: Lead Data Engineer\nLocation: Mumbai\n\nResponsibilities:\nEnd-to-End Data Pipeline Development: Design, build, optimize, and maintain robust data pipelines across cloud, on-premises, or hybrid environments, ensuring performance, scalability, and seamless data flow.\nReusable Components & Frameworks: Develop reusable data pipeline components and contribute to the team's data pipeline framework evolution.\nData Architecture & Solutions: Contribute to data architecture design, applying data modelling, storage, and retrieval expertise.\nData Governance & Automation: Champion data integrity, security, and efficiency through metadata management, automation, and data governance best practices.\nCollaborative Problem Solving: Partner with stakeholders, data teams, and engineers to define requirements, troubleshoot, optimize, and deliver data-driven insights.\nMentorship & Knowledge Transfer: Guide and mentor junior data engineers, fostering knowledge sharing and professional growth.\n\nQualification Details:\nEducation: Bachelor's degree or higher in Computer Science, Data Science, Engineering, or a related technical field.\nCore Programming: Excellent command of a primary data engineering language (Scala, Python, or Java) with a strong foundation in OOPS and functional programming concepts.\nBig Data Technologies: Hands-on experience with data processing frameworks (e.g., Hadoop, Spark, Apache Hive, NiFi, Ozone, Kudu), ideally including streaming technologies (Kafka, Spark Streaming, Flink, etc.).\nDatabase Expertise: Excellent querying skills (SQL) and strong understanding of relational databases (e.g., MySQL, PostgreSQL). Experience with NoSQL databases (e.g., MongoDB, Cassandra) is a plus.\nEnd-to-End Pipelines: Demonstrated experience in implementing, optimizing, and maintaining complete data pipelines, integrating varied sources and sinks including streaming real-time data.\nCloud Expertise: Knowledge of Cloud Technologies like Azure HDInsights, Synapse, EventHub and GCP DataProc, Dataflow, BigQuery.\nCI/CD Expertise: Experience with CI/CD methodologies and tools, including strong Linux and shell scripting skills for automation.\n\nDesired Skills & Attributes:\nProblem-Solving & Troubleshooting: Proven ability to analyze and solve complex data problems, troubleshoot data pipeline issues effectively.\nCommunication & Collaboration: Excellent communication skills, both written and verbal, with the ability to collaborate across teams (data scientists, engineers, stakeholders).\nContinuous Learning & Adaptability: A demonstrated passion for staying up-to-date with emerging data technologies and a willingness to adapt to new tools.",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Azure Cloud', 'Kafka', 'Python']",2025-06-14 05:08:59
Azure Data Engineer,Zensar,5 - 10 years,15-25 Lacs P.A.,"['Hyderabad', 'Pune', 'Bengaluru']","We are looking for a skilled Data Engineer with expertise in Azure Data Factory, ADLS, SQL, PySpark, Python and Azure Databricks to design, build and optimize data pipelines. Also ensuring efficient data ingestion, transformation and storage solutions.\n\nKey Responsibilities:\n\nDesing and Develop data pipelines using Azure Data Factory and SSIS\nManage Cloud Data Storage and Processing (AWS S3, ADLS etc)\nWrite complex SQL queries, optimize performance\nProcess large datasets using PySpark\nDevelop scripts for data processing, automation and API integration using Python\nDevelop Databrikcs notebook, manage workflows and implement delta lake for data transactions\nPipeline orchestration and Monitoring\nKnowledge on CI/CD using Azure DevOps/Github",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Data Factory', 'Azure Data Lake', 'Azure Databricks', 'Pyspark']",2025-06-14 05:09:01
Data Engineer-Data Platforms-Google,IBM,2 - 5 years,Not Disclosed,['Hyderabad'],"As an Associate Software Developer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\n\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\n\nIn this role, your responsibilities may include:\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\n\n\nPreferred technical and professional experience",,,,"['elastic search', 'gcp', 'splunk', 'hadoop', 'big data', 'hive', 'python', 'data management', 'presentation skills', 'microsoft azure', 'machine learning', 'javascript', 'sql', 'docker', 'java', 'git', 'spark', 'linux', 'jenkins', 'html', 'mysql', 'aws']",2025-06-14 05:09:04
Data Engineer-Business Intelligence,IBM,3 - 8 years,Not Disclosed,['Pune'],"Provide expertise in analysis, requirements gathering, design, coordination, customization, testing and support of reports, in client’s environment\nDevelop and maintain a strong working relationship with business and technical members of the team\nRelentless focus on quality and continuous improvement\nPerform root cause analysis of reports issues\nDevelopment / evolutionary maintenance of the environment, performance, capability and availability.\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\n\n\n\nPreferred technical and professional experience",,,,"['informatica powercenter', 'microsoft azure', 'mis', 'bi dw', 'etl', 'datastage', 'bi', 'azure data factory', 'cognos', 'root cause analysis', 'business intelligence', 'sql', 'dw', 'ibm datastage', 'troubleshooting', 'debugging', 'agile', 'data visualization', 'aws', 'data integration', 'informatica', 'unix']",2025-06-14 05:09:06
Data Engineer-Data Platforms,IBM,2 - 5 years,Not Disclosed,['Mumbai'],"As a Data Engineer at IBM, you'll play a vital role in the development, design of application, provide regular support/guidance to project teams on complex coding, issue resolution and execution.\nYour primary responsibilities include\nLead the design and construction of new solutions using the latest technologies, always looking to add business value and meet user requirements.\nStrive for continuous improvements by testing the build solution and working under an agile framework.\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\n\n\n\n\nPreferred technical and professional experience",,,,"['python', 'data processing', 'pyspark', 'data modeling', 'spark', 'data analysis', 'microsoft azure', 'data warehousing', 'cloud platforms', 'numpy', 'data engineering', 'sql', 'pandas', 'spring boot', 'etl pipelines', 'java', 'gcp', 'kafka', 'data warehousing concepts', 'hadoop', 'big data', 'aws', 'etl']",2025-06-14 05:09:09
Data Engineer-Data Integration,IBM,2 - 5 years,Not Disclosed,['Pune'],"As Data Engineer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\n\n\nPreferred technical and professional experience",,,,"['information management', 'data warehousing', 'business intelligence', 'etl', 'data integration', 'python', 'data management', 'informatica powercenter', 'power bi', 'relational sql', 'data migration', 'azure cloud', 'sql', 'unix shell scripting', 'java', 'etl tool', 'big data', 'informatica', 'unix']",2025-06-14 05:09:11
Senior Data Engineer,Grid Dynamics,8 - 13 years,15-25 Lacs P.A.,['Bengaluru'],"We are looking for an enthusiastic and technology-proficient Big Data Engineer, who is eager to participate in the design and implementation of a top-notch Big Data solution to be deployed at massive scale.\nOur customer is one of the world's largest technology companies based in Silicon Valley with operations all over the world. On this project we are working on the bleeding-edge of Big Data technology to develop high performance data analytics platform, which handles petabytes datasets.\nEssential functions\nParticipate in design and development of Big Data analytical applications.\nDesign, support and continuously enhance the project code base, continuous integration pipeline, etc.\nWrite complex ETL processes and frameworks for analytics and data management.\nImplement large-scale near real-time streaming data processing pipelines.\nWork inside the team of industry experts on the cutting edge Big Data technologies to develop solutions for deployment at massive scale.\nQualifications\nStrong coding experience with Scala, Spark,Hive, Hadoop.\nIn-depth knowledge of Hadoop and Spark, experience with data mining and stream processing technologies (Kafka, Spark Streaming, Akka Streams).\nUnderstanding of the best practices in data quality and quality engineering.\nExperience with version control systems, Git in particular.\nDesire and ability for quick learning of new tools and technologies.\nWould be a plus\nKnowledge of Unix-based operating systems (bash/ssh/ps/grep etc.).\nExperience with Github-based development processes.\nExperience with JVM build systems (SBT, Maven, Gradle).\nWe offer\nOpportunity to work on bleeding-edge projects\nWork with a highly motivated and dedicated team\nCompetitive salary\nFlexible schedule\nBenefits package - medical insurance, sports\nCorporate social events\nProfessional development opportunities\nWell-equipped office\nAbout us\nGrid Dynamics (NASDAQ: GDYN) is a leading provider of technology consulting, platform and product engineering, AI, and advanced analytics services. Fusing technical vision with business acumen, we solve the most pressing technical challenges and enable positive business outcomes for enterprise companies undergoing business transformation. A key differentiator for Grid Dynamics is our 8 years of experience and leadership in enterprise AI, supported by profound expertise and ongoing investment in data, analytics, cloud & DevOps, application modernization and customer experience. Founded in 2006, Grid Dynamics is headquartered in Silicon Valley with offices across the Americas, Europe, and India.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Hive', 'SCALA', 'Hadoop', 'Big Data', 'Spark']",2025-06-14 05:09:13
Data Engineer-Data Integration,IBM,2 - 5 years,Not Disclosed,['Pune'],"As Data Engineer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\n\nIn this role, your responsibilities may include:\n\n\n \n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\n\n\nPreferred technical and professional experience",,,,"['information management', 'data warehousing', 'business intelligence', 'etl', 'data integration', 'python', 'informatica powercenter', 'power bi', 'relational sql', 'data migration', 'azure cloud', 'sql', 'elastic search', 'unix shell scripting', 'splunk', 'agile', 'big data', 'informatica']",2025-06-14 05:09:16
Big Data Engineer,Rarr Technologies,6 - 8 years,Not Disclosed,['Bengaluru'],"Job description\nProven experience working with data pipelines ETL BI regardless of the technology\nProven experience working with AWS including at least 3 of RedShift S3 EMR Cloud Formation DynamoDB RDS lambda\nBig Data technologies and distributed systems one of Spark Presto or Hive\nPython language scripting and object oriented\nFluency in SQL for datawarehousing RedShift in particular is a plus\nGood undesrtanding on datawarehousing and Data modelling concepts\nFamiliar with GIT Linux CICD pipelines is a plus\nStrong systemsprocess orientation with demonstrated analytical thinking organization skills and problemsolving skills\nAbility to selfmanage prioritize and execute tasks in a demanding environment\nStrong consultancy orientation and experience with the ability to form collaborative productive working relationships across diverse teams and cultures is a must\nWillingness and ability to train and teach others\nAbility to facilitate meetings and follow up with resulting action items\nPython Scripting, Etl, Big Data, Aws",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['GIT', 'Linux', 'Analytical', 'big data', 'Distribution system', 'AWS', 'Data warehousing', 'SQL', 'Python', 'Scripting']",2025-06-14 05:09:18
Senior Developer / Lead Data Engineer - Incorta,KPI Partners,4 - 9 years,Not Disclosed,"['Hyderabad', 'Pune', 'Bengaluru']","About Us:\nKPI Partners is a leading provider of data analytics and performance management solutions, dedicated to helping organizations harness the power of their data to drive business success. Our team of experts is at the forefront of the data revolution, delivering innovative solutions to our clients. We are currently seeking a talented and experienced Senior Developer / Lead Data Engineer with expertise in Incorta to join our dynamic team.\n\n",,,,"['python', 'oracle', 'data analytics', 'analytical', 'scala', 'pyspark', 'microsoft azure', 'cloud platforms', 'data pipeline', 'relational databases', 'data engineering', 'sql server', 'sql', 'analytics', 'java', 'data modeling', 'collaboration', 'data integration tools', 'mysql', 'etl', 'aws', 'programming', 'communication skills']",2025-06-14 05:09:21
Data Engineer Specialist,Accenture,3 - 4 years,Not Disclosed,['Kochi'],"Job Title - + +\n\n\n\nManagement Level :\n\n\n\nLocation:Kochi, Coimbatore, Trivandrum\n\n\n\nMust have skills:Python, Pyspark\n\n\n\n\nGood to have skills:Redshift\n\n\n\nJob\n\n\nSummary: We are seeking a highly skilled and experienced Senior Data Engineer to join our growing Data and Analytics team. The ideal candidate will have deep expertise in Databricks and cloud data warehousing, with a proven track record of designing and building scalable data pipelines, optimizing data architectures, and enabling robust analytics capabilities. This role involves working collaboratively with cross-functional teams to ensure the organization leverages data as a strategic asset. Your responsibilities will include:\n\n\n\n\nRoles & Responsibilities\nDesign, build, and maintain scalable data pipelines and ETL processes using Databricks and other modern tools.\nArchitect, implement, and manage cloud-based data warehousing solutions on Databricks (Lakehouse Architecture)\nDevelop and maintain optimized data lake architectures to support advanced analytics and machine learning use cases.\nCollaborate with stakeholders to gather requirements, design solutions, and ensure high-quality data delivery.\nOptimize data pipelines for performance and cost efficiency.\nImplement and enforce best practices for data governance, access control, security, and compliance in the cloud.\nMonitor and troubleshoot data pipelines to ensure reliability and accuracy.\nLead and mentor junior engineers, fostering a culture of continuous learning and innovation.\nExcellent communication skills\nAbility to work independently and along with client based out of western Europe\n\n\n\n\nProfessional & Technical\n\n\n\n\nSkills:\nDesigning, developing, optimizing, and maintaining data pipelines that adhere to ETL principles and business goals\nSolving complex data problems to deliver insights that helps our business to achieve their goals.\nSource data (structured unstructured) from various touchpoints, format and organize them into an analyzable format.\nCreating data products for analytics team members to improve productivity\nCalling of AI services like vision, translation etc. to generate an outcome that can be used in further steps along the pipeline.\nFostering a culture of sharing, re-use, design and operational efficiency of data and analytical solutions\nPreparing data to create a unified database and build tracking solutions ensuring data quality\nCreate Production grade analytical assets deployed using the guiding principles of CI/CD.\n\n\n\nProfessional and Technical Skills\nExpert in Python, Scala, Pyspark, Pytorch, Javascript (any 2 at least)\nExtensive experience in data analysis (Big data- Apache Spark environments), data libraries (e.g. Pandas, SciPy, Tensorflow, Keras etc.), and SQL. 3-4 years of hands-on experience working on these technologies.\nExperience in one of the many BI tools such as Tableau, Power BI, Looker.\nGood working knowledge of key concepts in data analytics, such as dimensional modeling, ETL, reporting/dashboarding, data governance, dealing with structured and unstructured data, and corresponding infrastructure needs.\nWorked extensively in Microsoft Azure (ADF, Function Apps, ADLS, Azure SQL), AWS (Lambda,Glue,S3), Databricks analytical platforms/tools, Snowflake Cloud Datawarehouse.\n\n\n\n\nAdditional Information\nExperience working in cloud Data warehouses like Redshift or Synapse\nCertification in any one of the following or equivalent\nAWS- AWS certified data Analytics- Speciality\nAzure- Microsoft certified Azure Data Scientist Associate\nSnowflake- Snowpro core- Data Engineer\nDatabricks Data Engineering\n\nQualification\n\n\n\nExperience:5-8 years of experience is required\n\n\n\n\nEducational Qualification:Graduation (Accurate educational details should capture)",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['scala', 'pyspark', 'pytorch', 'python', 'data bricks', 'glue', 'amazon redshift', 'data warehousing', 'sql', 'tensorflow', 'sql azure', 'spark', 'keras', 'big data', 'etl', 'snowflake', 'scipy', 'data analysis', 'azure data lake', 'microsoft azure', 'power bi', 'javascript', 'pandas', 'tableau', 'lambda expressions', 'aws']",2025-06-14 05:09:23
Tech Lead-Data Engineering,Ameriprise Financial,7 - 10 years,Not Disclosed,['Hyderabad'],"Key Responsibilities\nDesign and develop high-volume, data engineering solutions for mission-critical systems with quality.\nMaking enhancements to various applications that meets business and auditing requirements.\nResearch and evaluate alternative solutions and make recommendations on improving the product to meet business and information risk requirements.\nEvaluate service level issues and suggested enhancements to diagnose and address underlying system problems and inefficiencies.\nParticipate in full development lifecycle activities for the product (coding, testing, release activities).\nSupport Release activities on weekends as required.\nSupport any application issues reported during weekends.\nCoordinating day-To-day activities for multiple projects with onshore and offshore team members. Ensuring the availability of platform in lower environments\n\nRequired Qualifications\n7+ years of overall IT experience, which includes hands on experience in Big Data technologies.\nMandatory - Hands on experience in Python and PySpark.\nBuild pySpark applications using Spark Dataframes in Python using Jupyter notebook and PyCharm(IDE).\nWorked on optimizing spark jobs that processes huge volumes of data.\nHands on experience in version control tools like Git.\nWorked on Amazon s Analytics services like Amazon EMR, Amazon Athena, AWS Glue.\nWorked on Amazon s Compute services like Amazon Lambda, Amazon EC2 and Amazon s Storage service like S3 and few other services like SNS.\nExperience/knowledge of bash/shell scripting will be a plus.\nHas built ETL processes to take data, copy it, structurally transform it etc. involving a wide variety of formats like CSV, TSV, XML and JSON.\nExperience in working with fixed width, delimited , multi record file formats etc.\nGood to have knowledge of datawarehousing concepts - dimensions, facts, schemas- snowflake, star etc.\nHave worked with columnar storage formats- Parquet,Avro,ORC etc. Well versed with compression techniques - Snappy, Gzip.\nGood to have knowledge of AWS databases (atleast one) Aurora, RDS, Redshift, ElastiCache, DynamoDB.\nHands on experience in tools like Jenkins to build, test and deploy the applications\nAwareness of Devops concepts and be able to work in an automated release pipeline environment.\nExcellent debugging skills.\n\nPreferred Qualifications\nExperience working with US Clients and Business partners.\nKnowledge on Front end frameworks.\nExposure to BFSI domain is a good to have.\nHands on experience on any API Gateway and management platform.\nAWMPO AWMPS Presidents Office\nTechnology",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Front end', 'Version control', 'Coding', 'XML', 'Shell scripting', 'Debugging', 'JSON', 'Asset management', 'Analytics', 'Python']",2025-06-14 05:09:25
Azure Cloud Data Engineering Consultant,Optum,7 - 10 years,17-27.5 Lacs P.A.,['Gurugram'],"Primary Responsibilities:\nDesign and develop applications and services running on Azure, with a strong emphasis on Azure Databricks, ensuring optimal performance, scalability, and security.\nBuild and maintain data pipelines using Azure Databricks and other Azure data integration tools.\nWrite, read, and debug Spark, Scala, and Python code to process and analyze large datasets.\nWrite extensive query in SQL and Snowflake\nImplement security and access control measures and regularly audit Azure platform and infrastructure to ensure compliance.\nCreate, understand, and validate design and estimated effort for given module/task, and be able to justify it.\nPossess solid troubleshooting skills and perform troubleshooting of issues in different technologies and environments.\nImplement and adhere to best engineering practices like design, unit testing, functional testing automation, continuous integration, and delivery.\nMaintain code quality by writing clean, maintainable, and testable code.\nMonitor performance and optimize resources to ensure cost-effectiveness and high availability.\nDefine and document best practices and strategies regarding application deployment and infrastructure maintenance.\nProvide technical support and consultation for infrastructure questions.\nHelp develop, manage, and monitor continuous integration and delivery systems.\nTake accountability and ownership of features and teamwork.\nComply with the terms and conditions of the employment contract, company policies and procedures, and any directives.\nRequired Qualifications:\nB.Tech/MCA (Minimum 16 years of formal education)\nOverall 7+ years of experience.\nMinimum of 3 years of experience in Azure (ADF), Databricks and DevOps.\n5 years of experience in writing advanced level SQL.\n2-3 years of experience in writing, reading, and debugging Spark, Scala, and Python code.\n3 or more years of experience in architecting, designing, developing, and implementing cloud solutions on Azure.\nProficiency in programming languages and scripting tools.\nUnderstanding of cloud data storage and database technologies such as SQL and NoSQL.\nProven ability to collaborate with multidisciplinary teams of business analysts, developers, data scientists, and subject-matter experts.\nFamiliarity with DevOps practices and tools, such as continuous integration and continuous deployment (CI/CD) and Teraform.\nProven proactive approach to spotting problems, areas for improvement, and performance bottlenecks.\nProven excellent communication, writing, and presentation skills.\nExperience in interacting with international customers to gather requirements and convert them into solutions using relevant skills.\nPreferred Qualifications:\nKnowledge of AI/ML or LLM (GenAI).\nKnowledge of US Healthcare domain and experience with healthcare data.\nExperience and skills with Snowflake.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Data Factory', 'Azure Databricks', 'ETL', 'SQL', 'Python', 'Airflow', 'Pyspark', 'Snowflake', 'SCALA', 'Spark', 'Data Bricks']",2025-06-14 05:09:27
DE&A - Core - Cloud Data Engineering - Snowflake Data Engineering,Zensar,5 - 9 years,Not Disclosed,"['Kolkata', 'Mumbai', 'New Delhi', 'Hyderabad', 'Pune', 'Chennai', 'Bengaluru']","Designing and implementing data processing systems using Microsoft Fabric, Azure Data Analytics, Databricks and other distributed frameworks\n(ex\n\nHadoop, Spark, Snowflake, Airflow)\n\nWriting efficient and scalable code to process, transform, and clean large volumes of structured and unstructured data\nDesigning data pipelines: Snowflake Data Cloud uses data pipelines to ingest data into its system from sources like databases, cloud storage, or streaming platforms\n\nA Snowflake Data Engineer designs, builds, and fine-tunes these pipelines to make sure that all data is loaded into Snowflake correctly\n\nDesigning and implementing data processing systems using Microsoft Fabric, Azure Data Analytics, Databricks and other distributed frameworks\n(ex\n\nHadoop, Spark, Snowflake, Airflow)\n\nWriting efficient and scalable code to process, transform, and clean large volumes of structured and unstructured data\nDesigning data pipelines: Snowflake Data Cloud uses data pipelines to ingest data into its system from sources like databases, cloud storage, or streaming platforms\n\nA Snowflake Data Engineer designs, builds, and fine-tunes these pipelines to make sure that all data is loaded into Snowflake correctly",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['spark', 'Hadoop', 'cloud storage', 'Data processing', 'Data analytics', 'microsoft']",2025-06-14 05:09:29
Sr Data Engineer - Lead,Clifyx Technology,7 - 11 years,Not Disclosed,['Bengaluru'],"Developing Scala Spark pipelines that are resilient, modular and tested.\nHelp automate and scale governance through technology enablement\nEnable users finding the ""right data for the ""right use case\nParticipate in identifying and proposing solutions to data quality issues, and data management solutions\nSupport technical implementation of solutions through data pipeline development\nMaintain technical processes and procedures for data management\nVery good understanding of MS Azure Data Lake and associated setups\nETL knowledge to build semantic layers for reporting\nCreation / modification of pipelines based on source and target systems\nUser and access Management and Training end users",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Training', 'Usage', 'Data management', 'Access management', 'spark', 'Billing', 'SCALA', 'Manager Technology', 'Data quality', 'Testing']",2025-06-14 05:09:31
Lead Data Engineer - Azure,Blend360 India,7 - 12 years,Not Disclosed,['Hyderabad'],"Job Description\nAs a Lead Data Engineer, your role is to spearhead the data engineering teams and elevate the team to the next level! You will be responsible for laying out the architecture of the new project as well as selecting the tech stack associated with it. You will plan out the development cycles deploying AGILE if possible and create the foundations for good data stewardship with our new data products!\nYou will also set up a solid code framework that needs to be built to purpose yet have enough flexibility to adapt to new business use cases tough but rewarding challenge!\n\nResponsibilities\nCollaborate with several stakeholders to deeply understand the needs of data practitioners to deliver at scale\nLead Data Engineers to define, build and maintain Data Platform\nWork on building Data Lake in Azure Fabric processing data from multiple sources\nMigrating existing data store from Azure Synapse to Azure Fabric\nImplement data governance and access control\nDrive development effort End-to-End for on-time delivery of high-quality solutions that conform to requirements, conform to the architectural vision, and comply with all applicable standards.\nPresent technical solutions, capabilities, considerations, and features in business terms. Effectively communicate status, issues, and risks in a precise and timely manner.\nFurther develop critical initiatives, such as Data Discovery, Data Lineage and Data Quality\nLeading team and Mentor junior resources\nHelp your team members grow in their role and achieve their career aspirations\nBuild data systems, pipelines, analytical tools and programs\nConduct complex data analysis and report on results\n\n\nQualifications\n7+ Years of Experience as a data engineer or similar role in Azure Synapses, ADF or\nrelevant exp in Azure Fabric\nDegree in Computer Science, Data Science, Mathematics, IT, or similar fiel",Industry Type: Advertising & Marketing,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Data analysis', 'Data modeling', 'Analytical', 'Agile', 'data governance', 'Data quality', 'Data mining', 'SQL', 'Python']",2025-06-14 05:09:34
Data Engineering Professional,Agilisium,4 - 6 years,Not Disclosed,['Chennai'],"Experience - 4+ years\n\nProgramming Side:\nStrong programming skill on Python\nStrong skill on SQL and any one of the Database/Data warehouses\nKnowledge on Spark coding and Architecture\nSound knowledge on ETL flow, Good Knowledge on Databricks.\nExperience on any cloud(AWS/Azure/GCP)",,,,"['Data Engineering', 'python', 'Kinesis', 'Kafka', 'data warehousing', 'dbms', 'etl', 'sql']",2025-06-14 05:09:36
Data Engineer-Data Platforms,IBM,5 - 10 years,Not Disclosed,['Navi Mumbai'],"As a Big Data Engineer, you will develop, maintain, evaluate, and test big data solutions. You will be involved in data engineering activities like creating pipelines/workflows for Source to Target and implementing solutions that tackle the clients needs.\n\nYour primary responsibilities include:\nDesign, build, optimize and support new and existing data models and ETL processes based on our clients business requirements.\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\n\n\nPreferred technical and professional experience",,,,"['scala', 'hadoop spark', 'spark', 'big data', 'python', 'hive', 'cloudera', 'pyspark', 'sql', 'java', 'git', 'postgresql', 'devops', 'jenkins', 'data ingestion', 'mysql', 'hadoop', 'etl', 'hbase', 'data analysis', 'dynamo db', 'oozie', 'microsoft azure', 'impala', 'data engineering', 'lambda expressions', 'kafka', 'sqoop', 'aws']",2025-06-14 05:09:39
Big Data Engineer - Python/PySpark,Qcentrio,6 - 10 years,Not Disclosed,['Bengaluru'],"Work Location : Bangalore (CV Ramen Nagar location)\n\nNotice Period : Immediate - 30 days\n\nMandatory Skills : Big Data, Python, SQL, Spark/Pyspark, AWS Cloud\n\nJD and required Skills & Responsibilities :\n\n- Actively participate in all phases of the software development lifecycle, including requirements gathering, functional and technical design, development, testing, roll-out, and support.\n\n- Solve complex business problems by utilizing a disciplined development methodology.\n\n- Produce scalable, flexible, efficient, and supportable solutions using appropriate technologies.\n\n- Analyse the source and target system data. Map the transformation that meets the requirements.\n\n- Interact with the client and onsite coordinators during different phases of a project.\n\n- Design and implement product features in collaboration with business and Technology stakeholders.\n\n- Anticipate, identify, and solve issues concerning data management to improve data quality.\n\n- Clean, prepare, and optimize data at scale for ingestion and consumption.\n\n- Support the implementation of new data management projects and re-structure the current data architecture.\n\n- Implement automated workflows and routines using workflow scheduling tools.\n\n- Understand and use continuous integration, test-driven development, and production deployment frameworks.\n\n- Participate in design, code, test plans, and dataset implementation performed by other data engineers in support of maintaining data engineering standards.\n\n- Analyze and profile data for the purpose of designing scalable solutions.\n\n- Troubleshoot straightforward data issues and perform root cause analysis to proactively resolve product issues.\n\nRequired Skills :\n\n- 5+ years of relevant experience developing Data and analytic solutions.\n\n- Experience building data lake solutions leveraging one or more of the following AWS, EMR, S3, Hive & PySpark\n\n- Experience with relational SQL.\n\n- Experience with scripting languages such as Python.\n\n- Experience with source control tools such as GitHub and related dev process.\n\n- Experience with workflow scheduling tools such as Airflow.\n\n- In-depth knowledge of AWS Cloud (S3, EMR, Databricks)\n\n- Has a passion for data solutions.\n\n- Has a strong problem-solving and analytical mindset\n\n- Working experience in the design, Development, and test of data pipelines.\n\n- Experience working with Agile Teams.\n\n- Able to influence and communicate effectively, both verbally and in writing, with team members and business stakeholders\n\n- Able to quickly pick up new programming languages, technologies, and frameworks.\n\n- Bachelor's degree in computer science",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Python', 'Hive', 'PySpark', 'Hadoop', 'Big Data', 'Agile', 'Spark', 'AWS EMR', 'SQL']",2025-06-14 05:09:41
Azure Senior Data Engineers,IT Services & Consulting,5 - 9 years,14-24 Lacs P.A.,['Bengaluru'],"Job Title: Senior Data Engineer Azure\nLocation: Bengaluru\nExperience: 6+ years (3+ years on Azure data services preferred)\nDepartment: Data Engineering / IT\nRoles and Responsibilities:\nThe Data Engineer will work on data engineering projects for various business units, focusing on delivery of complex data management solutions by leveraging industry best practices. They work with the project team to build the most efficient data pipelines and data management solutions that make data easily available for consuming applications and analytical solutions. A Data engineer is expected to possess strong technical skills.\nKey Characteristics\nTechnology champion who constantly pursues skill enhancement and has inherent curiosity to understand work from multiple dimensions.\nInterest and passion in Big Data technologies and appreciates the value that can be brought in with an effective data management solution.\nHas worked on real data challenges and handled high volume, velocity, and variety of data.\nExcellent analytical & problem-solving skills, willingness to take ownership and resolve technical challenges.\nContributes to community building initiatives like CoE, CoP.\nMandatory skills:\nAzure - Master\nELT - Skill\nData Modeling - Skill\nData Integration & Ingestion - Skill\nData Manipulation and Processing - Skill\nGITHUB, Action, Azure DevOps - Skill\nData factory, Databricks, SQL DB, Synapse, Stream Analytics, Glue, Airflow, Kinesis, Redshift, SonarQube, PyTest - Skill\nOptional skills:\nExperience in project management, running a scrum team.\nExperience working with BPC, Planning.\nExposure to working with external technical ecosystem.\nMKDocs documentation",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Live streaming data', 'Pyspark', 'SCALA', 'Data Bricks', 'SQL', 'batch processing data']",2025-06-14 05:09:43
Data Engineer-Business Intelligence,IBM,5 - 10 years,Not Disclosed,['Hyderabad'],"Provide expertise in analysis, requirements gathering, design, coordination, customization, testing and support of reports, in client’s environment\nDevelop and maintain a strong working relationship with business and technical members of the team\nRelentless focus on quality and continuous improvement\nPerform root cause analysis of reports issues\nDevelopment / evolutionary maintenance of the environment, performance, capability and availability.\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\n\n\nPreferred technical and professional experience",,,,"['advance sql', 'sql', 'bi tools', 'debugging', 'troubleshooting', 'python', 'data analysis', 'data analytics', 'bi', 'data warehousing', 'power bi', 'business analysis', 'machine learning', 'business intelligence', 'sql server', 'qlikview', 'tableau', 'r', 'data visualization', 'etl', 'ssis']",2025-06-14 05:09:45
Data Engineer-Data Platforms,IBM,2 - 5 years,Not Disclosed,['Pune'],"As a Data Engineer at IBM, you'll play a vital role in the development, design of application, provide regular support/guidance to project teams on complex coding, issue resolution and execution.\nYour primary responsibilities include:\nLead the design and construction of new solutions using the latest technologies, always looking to add business value and meet user requirements.\nStrive for continuous improvements by testing the build solution and working under an agile framework.\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\n\n\n\n\nPreferred technical and professional experience",,,,"['python', 'data processing', 'pyspark', 'data modeling', 'spark', 'data analysis', 'microsoft azure', 'data warehousing', 'cloud platforms', 'numpy', 'data engineering', 'sql', 'pandas', 'spring boot', 'etl pipelines', 'java', 'gcp', 'kafka', 'data warehousing concepts', 'hadoop', 'big data', 'aws', 'etl']",2025-06-14 05:09:48
"Data Engineer II, TFAW/Sherlock",Amazon,3 - 8 years,Not Disclosed,['Hyderabad'],"As a Data Engineer you will be working on building and maintaining complex data pipelines, assemble large and complex datasets to generate business insights and to enable data driven decision making and support the rapidly growing and dynamic business demand for data.\nYou will have an opportunity to collaborate and work with various teams of Business analysts, Managers, Software Dev Engineers, and Data Engineers to determine how best to design, implement and support solutions. You will be challenged and provided with tremendous growth opportunity in a customer facing, fast paced, agile environment.\n\n\nDesign, implement and support an analytical data platform solutions for data driven decisions and insights\nDesign data schema and operate internal data warehouses SQL/NOSQL database systems\nWork on different data model designs, architecture, implementation, discussions and optimizations\nInterface with other teams to extract, transform, and load data from a wide variety of data sources using AWS big data technologies like EMR, RedShift, Elastic Search etc.\nWork on different AWS technologies such as S3, RedShift, Lambda, Glue, etc.. and Explore and learn the latest AWS technologies to provide new capabilities and increase efficiency\nWork on data lake platform and different components in the data lake such as Hadoop, Amazon S3 etc.\nWork on SQL technologies on Hadoop such as Spark, Hive, Impala etc..\nHelp continually improve ongoing analysis processes, optimizing or simplifying self-service support for customers\nMust possess strong verbal and written communication skills, be self-driven, and deliver high quality results in a fast-paced environment.\nRecognize and adopt best practices in reporting and analysis: data integrity, test design, analysis, validation, and documentation.\nEnjoy working closely with your peers in a group of talented engineers and gain knowledge.\nBe enthusiastic about building deep domain knowledge on various Amazon s business domains.\nOwn the development and maintenance of ongoing metrics, reports, analyses, dashboards, etc. to drive key business decisions. 3+ years of data engineering experience\n4+ years of SQL experience\nExperience with data modeling, warehousing and building ETL pipelines Experience with AWS technologies like Redshift, S3, AWS Glue, EMR, Kinesis, FireHose, Lambda, and IAM roles and permissions\nExperience with non-relational databases / data stores (object storage, document or key-value stores, graph databases, column-family databases)",,,,"['NoSQL', 'data engineer ii', 'Business Analyst', 'Analytical', 'Schema', 'Test design', 'Agile', 'data integrity', 'Design analysis', 'SQL']",2025-06-14 05:09:50
Data Engineer-Data Platforms,IBM,2 - 5 years,Not Disclosed,['Navi Mumbai'],"As a Data Engineer at IBM, you'll play a vital role in the development, design of application, provide regular support/guidance to project teams on complex coding, issue resolution and execution.\n\nYour primary responsibilities include:\nLead the design and construction of new solutions using the latest technologies, always looking to add business value and meet user requirements.\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\n\n\n\n\nPreferred technical and professional experience",,,,"['python', 'data processing', 'pyspark', 'data modeling', 'spark', 'data analysis', 'microsoft azure', 'data warehousing', 'cloud platforms', 'numpy', 'data engineering', 'sql', 'pandas', 'spring boot', 'etl pipelines', 'java', 'gcp', 'kafka', 'data warehousing concepts', 'hadoop', 'big data', 'aws', 'etl']",2025-06-14 05:09:53
Data Engineer-Data Platforms,IBM,2 - 5 years,Not Disclosed,['Mumbai'],"Experience with Scala object-oriented/object function Strong SQL background.\nExperience in Spark SQL, Hive, Data Engineer.\nSQL Experience with data pipelines & Data Lake Strong background in distributed comp.\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\n\n\nPreferred technical and professional experience",,,,"['hive', 'scala', 'sql', 'spark', 'data lake', 'amazon redshift', 'pyspark', 'data warehousing', 'emr', 'java', 'data modeling', 'mysql', 'hadoop', 'big data', 'etl', 'python', 'microsoft azure', 'machine learning', 'data engineering', 'sql server', 'nosql', 'amazon ec2', 'kafka', 'sqoop', 'aws']",2025-06-14 05:09:55
Cloud Data Engineer - GCP,Synechron,2 - 3 years,Not Disclosed,"['Hyderabad', 'Gachibowli']","Job Summary\nSynechron is seeking a highly motivated and skilled Senior Cloud Data Engineer GCP to join our cloud solutions team. In this role, you will collaborate closely with clients and internal stakeholders to design, implement, and manage scalable, secure, and high-performance cloud-based data solutions on Google Cloud Platform (GCP). You will leverage your technical expertise to ensure the integrity, security, and efficiency of cloud data architectures, enabling the organization to derive maximum value from cloud data assets. This role contributes directly to our mission of delivering innovative digital transformation solutions and supports the organizations strategic objectives of scalable and sustainable cloud infrastructure.\nSoftware Requirements\nOverall Responsibilities\nTechnical Skills (By Category)\nExperience Requirements\nDay-to-Day Activities\nQualifications\nProfessional Competencies",,,,"['GCP', 'Jenkins', 'Java', 'NoSQL', 'Bash scripts', 'Data Studio', 'Data Management', 'CI/CD', 'Apache Beam', 'MongoDB', 'Cloud Build']",2025-06-14 05:09:58
SNOWFLAKE DATA ENGINEER,Capgemini,6 - 11 years,Not Disclosed,['Chennai'],"Your Role \n\nWorks in the area of Software Engineering, which encompasses the development, maintenance and optimization of software solutions/applications.\n1. Applies scientific methods to analyse and solve software engineering problems.\n2. He/she is responsible for the development and application of software engineering practice and knowledge, in research, design, development and maintenance.\n3. His/her work requires the exercise of original thought and judgement and the ability to supervise the technical and administrative work of other software engineers.\n4. The software engineer builds skills and expertise of his/her software engineering discipline to reach standard software engineer skills expectations for the applicable role, as defined in Professional Communities.\n5. The software engineer collaborates and acts as team player with other software engineers and stakeholders.\nWorks in the area of Software Engineering, which encompasses the development, maintenance and optimization of software solutions/applications.1. Applies scientific methods to analyse and solve software engineering problems.2. He/she is responsible for the development and application of software engineering practice and knowledge, in research, design, development and maintenance.3. His/her work requires the exercise of original thought and judgement and the ability to supervise the technical and administrative work of other software engineers.4. The software engineer builds skills and expertise of his/her software engineering discipline to reach standard software engineer skills expectations for the applicable role, as defined in Professional Communities.5. The software engineer collaborates and acts as team player with other software engineers and stakeholders.\n\n Your Profile \n4+ years of experience in data architecture, data warehousing, and cloud data solutions.\nMinimum 3+ years of hands-on experience with End to end Snowflake implementation.\nExperience in developing data architecture and roadmap strategies with knowledge to establish data governance and quality frameworks within Snowflake\nExpertise or strong knowledge in Snowflake best practices, performance tuning, and query optimisation.\nExperience with cloud platforms like AWS or Azure and familiarity with Snowflakes integration with these environments.\nStrong knowledge in at least one cloud(AWS or Azure) is mandatory\nSolid understanding of SQL, Python, and scripting for data processing and analytics.\nExperience in leading teams and managing complex data migration projects.\nStrong communication skills, with the ability to explain technical concepts to non-technical stakeholders.\n\nKnowledge on new Snowflake features,AI capabilities and industry trends to drive innovation and continuous improvement.\n\n  \n\n Skills (competencies) \n\nVerbal Communication",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['snowflake', 'python', 'microsoft azure', 'sql', 'aws', 'head hunting', 'screening', 'performance tuning', 'data processing', 'data warehousing', 'hrsd', 'data architecture', 'data migration', 'sourcing', 'talent acquisition', 'it recruitment', 'technical recruitment', 'recruitment', 'data governance']",2025-06-14 05:10:00
"Data Engineer, Alexa AI Developer Tech",Amazon,3 - 8 years,Not Disclosed,['Pune'],"Alexa+ is our next-generation assistant powered by generative AI. Alexa+ is more conversational, smarter, personalized, and gets things done.\n\nOur goal is make Alexa+ an instantly familiar personal assistant that is always ready to help or entertain on any device. At the core of this vision is Alexa AI Developer Tech, a close-knit team that s dedicated to providing software developers with the tools, primitives, and services they need to easily create engaging customer experiences that expand the wealth of information, products and services available on Alexa+.\n\nYou will join a growing organization working on top technology using Generative AI and have an enormous opportunity to make an impact on the design, architecture, and implementation of products used every day, by people you know.\n\nWe re working hard, having fun, and making history; come join us!\n\n\nWork with a team of product and program managers, engineering leaders, and business leaders to build data architectures and platforms to support business\nDesign, develop, and operate high-scalable, high-performance, low-cost, and accurate data pipelines in distributed data processing platforms\nRecognize and adopt best practices in data processing, reporting, and analysis: data integrity, test design, analysis, validation, and documentation\nKeep up to date with big data technologies, evaluate and make decisions around the use of new or existing software products to design the data architecture\nDesign, build and own all the components of a high-volume data warehouse end to end.\nProvide end-to-end data engineering support for project lifecycle execution (design, execution and risk assessment)\nContinually improve ongoing reporting and analysis processes, automating or simplifying self-service support for customers\nInterface with other technology teams to extract, transform, and load (ETL) data from a wide variety of data sources\nOwn the functional and nonfunctional scaling of software systems in your ownership area. *Implement big data solutions for distributed computing.\n\nAbout the team\nAlexa AI Developer Tech is an organization within Alexa on a mission to empower developers to create delightful and engaging experiences by making Alexa more natural, accurate, conversational, and personalized. 3+ years of data engineering experience\n4+ years of SQL experience\nExperience with data modeling, warehousing and building ETL pipelines Experience with AWS technologies like Redshift, S3, AWS Glue, EMR, Kinesis, FireHose, Lambda, and IAM roles and permissions\nExperience with non-relational databases / data stores (object storage, document or key-value stores, graph databases, column-family databases)",,,,"['Data modeling', 'Business design', 'Risk assessment', 'Test design', 'Architectural design', 'Data processing', 'data integrity', 'Design analysis', 'SQL', 'Data architecture']",2025-06-14 05:10:02
Senior Data Engineer,Grab,4 - 9 years,Not Disclosed,['Bengaluru'],"Lending team at Grab is dedicated to building safe, secure, and loan products catering to all user segments across SEA. Our mission is to promote financial inclusion and support underbanked partners across the region. Data plays a pivotal role in our lending operations, guiding decisions across credit assessment, collections, reporting, and beyond\nYou will report to the Lead Data Engineer. This role is based in Bangalore.\nGet to Know the Role:\nAs the Data engineer in the Lending Data Engineering team, you will work with data modellers, product analytics, product managers, software engineers and business stakeholders across the SEA in understanding the business and data requirements. You will build and manage the data asset, including acquisition, storage, processing and use channels, and using some of the most scalable and resilient open source big data technologies like Flink, Airflow, Spark, Kafka, Trino and more on cloud infrastructure. You are encouraged to think out of the box and have fun exploring the latest patterns and designs.\nThe Critical Tasks You will Perform:\nDevelop scalable, reliable ETL pipelines to ingest data from diverse sources.\nBuild expertise in real-time data availability to support accurate real-time metric definitions.\nImplement data quality checks and governance best practices for data cleansing, assurance, and ETL operations. Use existing data platform tools to set up and manage pipelines.\nImprove data infrastructure performance to ensure, reliable insights for decision-making. Design next-gen data lifecycle management tools/frameworks for batch, real-time, API-based, and serverless use cases.\nBuild solutions using AWS services like Glue, Redshift, Athena, Lambda, S3, Step Functions, EMR, and Kinesis.\nUse tools like Amazon MSK/Kinesis for real-time data processing and metric tracking.\nRead more\nSkills you need\nEssential Skills Youll Need:\n4+ years of experience building scalable, secure, distributed, and data pipelines.\nProficiency in Python, Scala, or Java for data engineering solutions.\nKnowledge of big data technologies like Flink, Spark, Trino, Airflow, Kafka, and AWS services (EMR, Glue, Redshift, Kinesis, and Athena).\nSolid experience with SQL, data modelling, and schema design.\nHands-on with AWS storage and compute services (S3, DynamoDB, Athena, and Redshift Spectrum).\nExperience working with NoSQL, Columnar, and Relational databases.\nCurious and eager to explore new data technologies and solutions.\nFamiliarity with in-house and AWS-native tools for efficient pipeline development.\nDesign event-driven architectures using SNS, SQS, Lambda, or similar serverless technologies.\nExperience with data structures, algorithms, or ML concepts.\nRead more\nWhat we offer\nAbout Grab and Our Workplace\nGrab is Southeast Asias leading superapp. From getting your favourite meals delivered to helping you manage your finances and getting around town hassle-free, weve got your back with everything. In Grab, purpose gives us joy and habits build excellence, while harnessing the power of Technology and AI to deliver the mission of driving Southeast Asia forward by economically empowering everyone, with heart, hunger, honour, and humility.\nRead more\nLife at Grab\nLife at Grab\nWe care about your well-being at Grab, here are some of the global benefits we offer:\nWe have your back with Term Life Insurance and comprehensive Medical Insurance.\nWith GrabFlex, create a benefits package that suits your needs and aspirations.\nCelebrate moments that matter in life with loved ones through Parental and Birthday leave , and give back to your communities through Love-all-Serve-all (LASA) volunteering leave\nWe have a confidential Grabber Assistance Programme to guide and uplift you and your loved ones through lifes challenges.\nWhat We Stand For at Grab\nWe are committed to building an inclusive and equitable workplace that enables diverse Grabbers to grow and perform at their best. As an equal opportunity employer, we consider all candidates fairly and equally regardless of nationality, ethnicity, religion, age, gender identity, sexual orientation, family commitments, physical and mental impairments or disabilities, and other attributes that make them unique.\nRead more",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Assurance', 'NoSQL', 'Schema', 'Data structures', 'Data processing', 'Data quality', 'Open source', 'Analytics', 'SQL', 'Python']",2025-06-14 05:10:05
Senior Data Engineer,PulseData labs Pvt Ltd,7 - 10 years,Not Disclosed,['Bengaluru'],"Company name: PulseData labs Pvt Ltd (captive Unit for URUS, USA)\n\nAbout URUS\nWe are the URUS family (US), a global leader in products and services for Agritech.\n\nSENIOR DATA ENGINEER\nThis role is responsible for the design, development, and maintenance of data integration and reporting solutions. The ideal candidate will possess expertise in Databricks and strong skills in SQL Server, SSIS and SSRS, and experience with other modern data engineering tools such as Azure Data Factory. This position requires a proactive and results-oriented individual with a passion for data and a strong understanding of data warehousing principles.\n\nResponsibilities\nData Integration\nDesign, develop, and maintain robust and efficient ETL pipelines and processes on Databricks.\nTroubleshoot and resolve Databricks pipeline errors and performance issues.\nMaintain legacy SSIS packages for ETL processes.\nTroubleshoot and resolve SSIS package errors and performance issues.\nOptimize data flow performance and minimize data latency.\nImplement data quality checks and validations within ETL processes.\nDatabricks Development\nDevelop and maintain Databricks pipelines and datasets using Python, Spark and SQL.\nMigrate legacy SSIS packages to Databricks pipelines.\nOptimize Databricks jobs for performance and cost-effectiveness.\nIntegrate Databricks with other data sources and systems.\nParticipate in the design and implementation of data lake architectures.\nData Warehousing\nParticipate in the design and implementation of data warehousing solutions.\nSupport data quality initiatives and implement data cleansing procedures.\nReporting and Analytics\nCollaborate with business users to understand data requirements for department driven reporting needs.\nMaintain existing library of complex SSRS reports, dashboards, and visualizations.\nTroubleshoot and resolve SSRS report issues, including performance bottlenecks and data inconsistencies.\nCollaboration and Communication\nComfortable in entrepreneurial, self-starting, and fast-paced environment, working both independently and with our highly skilled teams.\nCollaborate effectively with business users, data analysts, and other IT teams.\nCommunicate technical information clearly and concisely, both verbally and in writing.\nDocument all development work and procedures thoroughly.\nContinuous Growth\nKeep abreast of the latest advancements in data integration, reporting, and data engineering technologies.\nContinuously improve skills and knowledge through training and self-learning.\nThis job description reflects managements assignment of essential functions; it does not prescribe or restrict the tasks that may be assigned.\n\nRequirements\nBachelor's degree in computer science, Information Systems, or a related field.\n7+ years of experience in data integration and reporting.\nExtensive experience with Databricks, including Python, Spark, and Delta Lake.\nStrong proficiency in SQL Server, including T-SQL, stored procedures, and functions.\nExperience with SSIS (SQL Server Integration Services) development and maintenance.\nExperience with SSRS (SQL Server Reporting Services) report design and development.\nExperience with data warehousing concepts and best practices.\nExperience with Microsoft Azure cloud platform and Microsoft Fabric desirable.\nStrong analytical and problem-solving skills.\nExcellent communication and interpersonal skills.\nAbility to work independently and as part of a team.\nExperience with Agile methodologies.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Etl Development', 'Azure Databricks', 'Spark', 'SQL Server', 'Databricks Engineer', 'Data Warehousing', 'Pythonspark']",2025-06-14 05:10:07
Senior Data Engineer,Roku,5 - 10 years,Not Disclosed,['Bengaluru'],"id=""job_description_2_0"">\nRoku is the #1 TV streaming platform in the U.S., Canada, and Mexico, and weve set our sights on powering every television in the world. Roku pioneered streaming to the TV. Our mission is to be the TV streaming platform that connects the entire TV ecosystem. We connect consumers to the content they love, enable content publishers to build and monetize large audiences, and provide advertisers unique capabilities to engage consumers.\nFrom your first day at Roku, youll make a valuable - and valued - contribution. Were a fast-growing public company where no one is a bystander. We offer you the opportunity to delight millions of TV streamers around the world while gaining meaningful experience across a variety of disciplines.\nAbout the team\nThe mission of Rokus Data Engineering team is to develop a world-class big data platform so that internal and external customers can leverage data to grow their businesses. Data Engineering works closely with business partners and Engineering teams to collect metrics on existing and new initiatives that are critical to business success. As Senior Data Engineer working on Device metrics, you will design data models & develop scalable data pipelines to capturing different business metrics across different Roku Devices.\nAbout the role\nRoku pioneered streaming to the TV. We connect users to the streaming content they love, enable content publishers to build and monetise large audiences, and provide advertisers with unique capabilities to engage consumers. Roku streaming players and Roku TV models are available around the world through direct retail sales and licensing arrangements with TV brands and pay-TV operators.With tens of million players sold across many countries, thousands of streaming channels and billions of hours watched over the platform, building scalable, highly available, fault-tolerant, big data platform is critical for our success.This role is based in Bangalore, India and requires hybrid working, with 3 days in the office.\nWhat youll be doing\nBuild highly scalable, available, fault-tolerant distributed data processing systems (batch and streaming systems) processing over 10s of terabytes of data ingested every day and petabyte-sized data warehouse\nBuild quality data solutions and refine existing diverse datasets to simplified data models encouraging self-service\nBuild data pipelines that optimise on data quality and are resilient to poor quality data sources\nOwn the data mapping, business logic, transformations and data quality\nLow level systems debugging, performance measurement & optimization on large production clusters\nParticipate in architecture discussions, influence product roadmap, and take ownership and responsibility over new projects\nMaintain and support existing platforms and evolve to newer technology stacks and architectures\nWere excited if you have\nExtensive SQL Skills\nProficiency in at least one scripting language, Python is required\nExperience in big data technologies like HDFS, YARN, Map-Reduce, Hive, Kafka, Spark, Airflow, Presto, etc.\nProficiency in data modeling, including designing, implementing, and optimizing conceptual, logical, and physical data models to support scalable and efficient data architectures.\nExperience with AWS, GCP, Looker is a plus\nCollaborate with cross-functional teams such as developers, analysts, and operations to execute deliverables\n5+ years professional experience as a data or software engineer\nBS in Computer Science; MS in Computer Science preferred\n#LI-AR3\nBenefits\nRoku is committed to offering a diverse range of benefits as part of our compensation package to support our employees and their families. Our comprehensive benefits include global access to mental health and financial wellness support and resources. Local benefits include statutory and voluntary benefits which may include healthcare (medical, dental, and vision), life, accident, disability, commuter, and retirement options (401(k)/pension). Our employees can take time off work for vacation and other personal reasons to balance their evolving work and life needs. Its important to note that not every benefit is available in all locations or for every role. For details specific to your location, please consult with your recruiter.\nThe Roku Culture\nWe have a unique culture that we are proud of. We think of ourselves primarily as problem-solvers, which itself is a two-part idea. We come up with the solution, but the solution isnt real until it is built and delivered to the customer. That penchant for action gives us a pragmatic approach to innovation, one that has served us well since 2002.\nTo learn more about Roku, our global footprint, and how weve grown, visit https: / / www.weareroku.com / factsheet .\nBy providing your information, you acknowledge that you have read our Applicant Privacy Notice and authorize Roku to process your data subject to those terms.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Data modeling', 'Debugging', 'Healthcare', 'Data processing', 'Data quality', 'data mapping', 'Licensing', 'SQL', 'Python']",2025-06-14 05:10:09
Specialist Digital Supply Chain Datamodeler/Data Engineer,Merck Sharp & Dohme (MSD),4 - 8 years,Not Disclosed,['Hyderabad'],"Our company is an innovative, global healthcare leader that is committed to improving health and well-being around the world with a diversified portfolio of prescription medicines, vaccines and animal health products. We continue to focus our research on conditions that affect millions of people around the world - diseases like Alzheimers, diabetes and cancer - while expanding our strengths in areas like vaccines and biologics.\nOur ability to excel depends on the integrity, knowledge, imagination, skill, diversity and teamwork of an individual like you. To this end, we strive to create an environment of mutual respect, encouragement and teamwork. As part of our global team, you ll have the opportunity to collaborate with talented and dedicated colleagues while developing and expanding your career.\nAs a Digital Supply Chain Data Modeler/Engineer, you will work as a member of the Digital Manufacturing Division team supporting Enterprise Orchestration Platform. You will be responsible for identifying, assessing, and solving complex business problems related to manufacturing and supply chain. You will receive training to achieve this, and you ll be amazed at the diversity of opportunities to develop your potential and grow professionally. You will collaborate with business stakeholders and determine analytical capabilities that will enable the creation of Insights-focused solutions that align to business needs and ensure that delivery of these solutions meet quality requirements.\nThe Opportunity\nBased in Hyderabad, joining a global healthcare biopharma company and be part of a 130- year legacy of success backed by ethical integrity, forward momentum, and an inspiring mission to achieve new milestones in global healthcare.\nBe part of an organization driven by digital technology and data-backed approaches that support a diversified portfolio of prescription medicines, vaccines, and animal health products.\nDrive innovation and execution excellence. Be a part of a team with passion for using data, analytics, and insights to drive decision-making, and which creates custom software, allowing us to tackle some of the worlds greatest health threats.\nOur Technology Centers focus on creating a space where teams can come together to deliver business solutions that save and improve lives. An integral part of our company s IT operating model, Tech Centers are globally distributed locations where each IT division has employees to enable our digital transformation journey and drive business outcomes. These locations, in addition to the other sites, are essential to supporting our business and strategy.\nA focused group of leaders in each Tech Centre helps to ensure we can manage and improve each location, from investing in growth, success, and well-being of our people, to making sure colleagues from each IT division feel a sense of belonging to managing critical emergencies. And together, we must leverage the strength of our team to collaborate globally to optimize connections and share best practices across the Tech Centers.\nAs Data modeler lead, you will be responsible for following\nDeliver divisional analytics initiatives with primary focus on datamodeling for all analytics, advanced analytics and AI/ML uses cases e,g Self Services , Business Intelligence & Analytics, Data exploration, Data Wrangling etc.\nHost and lead requirement/process workshop to understand the requirements of datamodeling .\nAnalysis of business requirements and work with architecture team to deliver & contribute to feasibility analysis, implementation plans and high-level estimates.\nBased on business process and analysis of data sources, deliver detailed ETL design with mapping of data model covering all areas of Data warehousing for all analytics use cases .\nCreation of data model & transformation mapping in modeling tool and deploy in databases including creation of schedule orchestration jobs .\nDeployment of Data modeling configuration to Target systems (SIT , UAT & Prod ) .\nUnderstanding of Product owership and management.\nLead Data model as a product for focus areas of Digital supply chain domain.\nCreation of required SDLC documentation as per project requirements.\nOptimization/industrialization of existing database and data transformation solution\nPrepare and update Data modeling and Data warehousing best practices along with foundational platforms.\nWork very closely with foundational product teams, Business, vendors and technology support teams to build team to deliver business initiatives .\nPosition Qualifications :\nEducation Minimum Requirement: - B.S. or M.S. in IT, Engineering, Computer Science, or related field.\nRequired Experience and Skills**:\n5+ years of relevant work experience, with demonstrated expertise in Data modeling in DWH, Data Mesh or any analytics related implementation; experience in implementing end to end DWH solutions involving creating design of DWH and deploying the solution\n3+ years of experience in creating logical & Physical data model in any modeling tool ( SAP Power designer, WhereScape etc ).\nExperience in creating data modeling standards, best practices and Implementation process.\nHigh Proficiency in Information Management, Data Analysis and Reporting Requirement Elicitation\nExperience working with extracting business rules to develop transformations, data lineage, and dimension data modeling\nExperience working with validating legacy and developed data model outputs\nDevelopment experience using WhereScape and various similar ETL/Data Modeling tools\nExposure to Qlik or similar BI dashboarding applications\nHas advanced knowledge of SQL and data transformation practices\nHas deep understanding of data modelling and preparation of optimal data structures\nIs able to communicate with business, data transformation team and reporting team\nHas knowledge of ETL methods, and a willingness to learn ETL technologies\nCan fluently communicate in English\nExperience in Redshift or similar databases using DDL, DML, Query optimization, Schema management, Security, etc\nExperience with Airflow or similar various orchestration tools\nExposure to CI/CD tools\nExposure to AWS modules such as S3, AWS Console, Glue, Spectrum, etc management\nIndependently support business discussions, analyze, and develop/deliver code\nPreferred Experience and Skills:\nExperience working on projects where Agile methodology is leveraged Understanding of data management best practices and data analytics Ability to lead requirements sessions with clients and project teams Strong leadership, verbal and written communication skills with ability to articulate results and issues to internal and client teams Demonstrated experience in the Life Science space Exposure to SAP and Rapid Response domain data is a plus",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['CVS', 'Data analysis', 'SAP', 'Database administration', 'Data structures', 'Healthcare', 'Business intelligence', 'Information technology', 'SDLC', 'SQL']",2025-06-14 05:10:11
AWS Data engineer,Tata Consultancy Services,5 - 10 years,Not Disclosed,"['Hyderabad', 'Pune', 'Chennai']","Airflow Data Engineer in AWS platform\nJob Title Apache Airflow Data Engineer\nROLE” as per TCS Role\nMaster • 4-8 years of experience in AWS, Apache Airflow (on Astronomer platform), Python, Pyspark, SQL • Good hands-on knowledge on SQL and Data Warehousing life cycle is an absolute requirement. • Experience in creating data pipelines and orchestrating using Apache Airflow • Significant experience with data migrations and development of Operational Data Stores, Enterprise Data Warehouses, Data Lake and Data Marts. • Good to have: Experience with cloud ETL and ELT in one of the tools like DBT/Glue/EMR or Matillion or any other ELT tool • Excellent communication skills to liaise with Business & IT stakeholders. • Expertise in planning execution of a project and efforts estimation. • Exposure to working in Agile ways of working.",,,,"['Airflow', 'Pyspark', 'ETL', 'AWS', 'Python', 'Hive', 'AWS Glue', 'EMR', 'ETL Tool', 'Lambda', 'Athena', 'RedShift']",2025-06-14 05:10:13
Senior Data Engineer,Blackline,6 - 11 years,Not Disclosed,['Bengaluru'],"Get to Know Us:\n\nIts fun to work in a company where people truly believe in what theyre doing!\nAt BlackLine, were committed to bringing passion and customer focus to the business of enterprise applications.\nSince being founded in 2001, BlackLine has become a leading provider of cloud software that automates and controls the entire financial close process. Our vision is to modernize the finance and accounting function to enable greater operational effectiveness and agility, and we are committed to delivering innovative solutions and services to empower accounting and finance leaders around the world to achieve Modern Finance.\nBeing a best-in-class SaaS Company, we understand that bringing in new ideas and innovative technology is mission critical. At BlackLine we are always working with new, cutting edge technology that encourages our teams to learn something new and expand their creativity and technical skillset that will accelerate their careers.\nWork, Play and Grow at BlackLine!\n\nMake Your Mark:\n\nAs a member of the Data BI Engineering team you will primarily focus on advancing our Enterprise Data Platform to allow the organization to make data-driven decisions. The successful candidate will work closely with cross-functional teams to identify business requirements, design, and develop data models, data warehouses, and data visualization solutions that help support the organizations strategic goals.\nThe Senior Data Engineer will work in a dynamic environment and will be required to stay current with the latest trends and technologies in the business intelligence field. The ideal candidate will be able to pick up business domain and internal process knowledge and leverage that knowledge to think strategically, communicate effectively, and manage multiple projects simultaneously.\nThe team is also responsible for administering tools and platforms around reporting, analytics, and data visualization while promoting best practices. The role requires a strong combination of technical expertise, leadership skills, and a deep understanding of data engineering principles and best practices. We are looking for a driven, detail-oriented, and passionate engineer to come to join our team.\n\nYoull Get To:\n\nProvide technical expertise and leadership in technology direction, road-mapping, architecture definition, design, development, and delivery of enterprise-class solutions while adhering to timelines, coding standards, requirements, and quality.\nArchitect, design, develop, test, troubleshoot, debug, optimize, scale, perform the capacity planning, deploy, maintain, and improve software applications, driving the delivery of high-quality value and features to Blackline s customers.\nWork collaboratively across the company to design, communicate and further assist with adoption of best practices in architecture and implementation.\nDeliver robust architectural solutions for complex design problems.\nImplement, refine, and enforce data engineering best practices to ensure that delivered features meet performance, security, and maintainability expectations.\nResearch, test, benchmark, and evaluate new tools and technologies, and recommend ways to implement them in data platform. Identify and create solutions that are likely to contribute to the development of new company concepts while keeping in mind the business strategy, short- and long-term roadmap, and architectural considerations to support them in a highly scalable and easy extensible manner.\nActively participate in research, development, support, management, and other company initiatives designing solutions to optimally address current and future business requirements and infrastructure plans.\nInspire a forward-thinking team of developers, acting as an agent of change and evangelist for a quality-first culture within the organization. Mentor and coach key technical staff and guide them to solutions on complex design issues.\nAct as a conduit for questions and information flow when those outside of Engineering have ideas for new technology applications.\nSpeak in terms relevant to audience, translating technical concepts into non-technical language and vice versa. Facilitate consensus building while striving for win/win scenarios and elicit value-add contributions from all team members in group settings.\nMaintain a strong sense of business value and return on investment in planning, design, and communication.\nProactively identify issues, bottlenecks, gaps, or other areas of concern or opportunity and work to either directly affect change, or advocate for that change by working with peers and leadership to build consensus and act.\nPerform critical maintenance, deployment, and release support activities, including occasional off-hours support.\n\nWhat Youll Bring:\n\nBachelors or Masters degree in Computer Science, Data Science, or a related field. 6+ years as a data engineer. 6+ years of experience using RDBMS, SQL, NoSQL, Python, Java, or other programming languages is a plus. 6+ years of experience designing, developing, testing, and implementing Extract, Transform and Load (ELT/ETL) solutions using enterprise ELT/ETL tools and Open source. 3+ years working experience with SQL and familiarity with Snowflake data warehouse, strong working knowledge in stored procedures, CTEs, and UDFs, RBAC Knowledge of data integration and data quality best practices Familiarity with data security and privacy regulations. Experience in working in a startup-type environment, good team player, and can work independently with minimal supervision Experience with cloud-native architecture and data solutions. Strong working knowledge in data modeling, data partitioning, and query optimization Demonstrated knowledge of development processes and agile methodologies. Strong analytical and interpersonal skills, comfortable presenting complex ideas in simple terms. Proficient in managing large volumes of data. Strong analytical and interpersonal skills, comfortable presenting complex ideas in simple terms. Strong communication and collaboration skills, with the ability to work effectively with cross-functional teams. Experience in providing technical support and troubleshooting for data-related issues. Expertise with at least one cloud environment and building cloud native data services. Prior experience driving data governance, quality, security initiatives.\nWe re Even More Excited If You Have:\n\nExperience with Google Cloud or similar cloud provider\nSignificant experience with open source platforms and technologies.\nExperience with data science and machine learning tools and technologies is a plus.\n\nThrive at BlackLine Because You Are Joining:\n\nA technology-based company with a sense of adventure and a vision for the future. Every door at BlackLine is open. Just bring your brains, your problem-solving skills, and be part of a winning team at the worlds most trusted name in Finance Automation!\nA culture that is kind, open, and accepting. Its a place where people can embrace what makes them unique, and the mix of cultural backgrounds and varying interests cultivates diverse thought and perspectives.\nA culture where BlackLiners continued growth and learning is empowered. BlackLine offers a wide variety of professional development seminars and inclusive affinity groups to celebrate and support our diversity.\nBlackLine is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to sex, gender identity or expression, race, ethnicity, age, religious creed, national origin, physical or mental disability, ancestry, color, marital status, sexual orientation, military or veteran status, status as a victim of domestic violence, sexual assault or stalking, medical condition, genetic information, or any other protected class or category recognized by applicable equal employment opportunity or other similar laws.\nBlackLine recognizes that the ways we work and the workplace itself have shifted. We innovate in a workplace that optimizes a combination of virtual and in-person interactions to maximize collaboration and nurture our culture. Candidates who live within a reasonable commute to one of our offices will work in the office at least 2 days a week.\nBachelors or Masters degree in Computer Science, Data Science, or a related field. 6+ years as a data engineer. 6+ years of experience using RDBMS, SQL, NoSQL, Python, Java, or other programming languages is a plus. 6+ years of experience designing, developing, testing, and implementing Extract, Transform and Load (ELT/ETL) solutions using enterprise ELT/ETL tools and Open source. 3+ years working experience with SQL and familiarity with Snowflake data warehouse, strong working knowledge in stored procedures, CTEs, and UDFs, RBAC Knowledge of data integration and data quality best practices Familiarity with data security and privacy regulations. Experience in working in a startup-type environment, good team player, and can work independently with minimal supervision Experience with cloud-native architecture and data solutions. Strong working knowledge in data modeling, data partitioning, and query optimization Demonstrated knowledge of development processes and agile methodologies. Strong analytical and interpersonal skills, comfortable presenting complex ideas in simple terms. Proficient in managing large volumes of data. Strong analytical and interpersonal skills, comfortable presenting complex ideas in simple terms. Strong communication and collaboration skills, with the ability to work effectively with cross-functional teams. Experience in providing technical support and troubleshooting for data-related issues. Expertise with at least one cloud environment and building cloud native data services. Prior experience driving data governance, quality, security initiatives.",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Automation', 'Data modeling', 'RDBMS', 'Coding', 'Agile', 'Business intelligence', 'Open source', 'Technical support', 'SQL', 'Python']",2025-06-14 05:10:16
"Scala Data Engr/Sr Data Pipeline Python, Spark Streaming German MNC",German MNC,6 - 9 years,Not Disclosed,['Bangalore/Bengaluru( Electronic City )'],"Full time with top German MNC for location Bangalore - Experience on SCALA is a must\n\nJob Overview:\nTo work on development, monitoring and maintenance of Data pipelines across clusters.\nPrimary responsibilities:\nDevelop, Monitor and Maintain data pipeline for various plants.\nCreate and maintain optimal data pipeline architecture.\nAssemble large, complex data sets that meet functional / non-functional business requirements.\nIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability.\nWork with stakeholders including the Data officers and stewards to assist with data-related technical issues and support their data infrastructure needs.\nWork on incidents highlighted by the data officers.\nIncident diagnosis, routing, evaluation & resolution.\nAnalyze the root cause of incidents.\nCreate incident closure report.\nQualifications\nQualifications\nBachelors degree in Computer Science, Electronics & Communication Engineering, a related technical field, or equivalent practical experience.\n6-8 years of experience in Spark, Scala software development.\nExperience in large-scale software development.\nExcellent software engineering skills (i.e., data structures, algorithms, software design).\nExcellent problem-solving, investigative, and troubleshooting skills.\nExperience in Kafka is mandatory\nAdditional Information\nSkills\nSelf-starter and empowered professional with strong execution and project management capabilities\nAbility to collaborate effectively, well developed inter personal relationships with all levels in the organization and outside contacts.\nOutstanding written and verbal communication skills.\nHigh Collaboration & a perseverance to drive performance & change\nAdditional information\nKey Competencies-\nDistributed computing systems\nExperience with CI/CD tools such as Jenkins or Github Actions\nExperience with Python programming\nWorking knowledge of Docker & Kubernetes\nExperience in developing data pipelines using spark & scala.\nExperience in debugging pipeline issues.\nExperience in writing python and shell scripts.\nIn-Depth Knowledge of SQL and Other Database Solutions\nHaving a strong understanding of Apache Hadoop-based analytics\nHands on experience on InteliJ, Github /Bitbucket, HUE.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Data Pipeline', 'SCALA', 'Spark Streaming', 'Python', 'Azure', 'Pipeline']",2025-06-14 05:10:18
Hiring | Data Engineer- Azure | Sr Analyst/ Consultant | Bangalore,Global MNC,5 - 10 years,Not Disclosed,['Bengaluru'],"Skill required: Data Engineers- Azure\nDesignation: Sr Analyst/ Consultant\nJob Location: Bengaluru\nQualifications: BE/BTech\nYears of Experience: 4 - 11 Years\n\nOVERALL PURPOSE OF JOB\nUnderstand client requirements and build ETL solution using Azure Data Factory, Azure Databricks & PySpark. Build solution in such a way that it can absorb clients change request very easily. Find innovative ways to accomplish tasks and handle multiple projects simultaneously and independently. Works with Data & appropriate teams to effectively source required data. Identify data gaps and work with client teams to effectively communicate the findings to stakeholders/clients.\n\nResponsibilities:\nDevelop ETL solution to populate Centralized Repository by integrating data from various data sources.\nCreate Data Pipelines, Data Flow, Data Model according to the business requirement.\nProficient in implementing all transformations according to business needs.\nIdentify data gaps in data lake and work with relevant data/client teams to get necessary data required for dashboarding/reporting.\nStrong experience working on Azure data platform, Azure Data Factory, Azure Data Bricks.\nStrong experience working on ETL components and scripting languages like PySpark, Python.\nExperience in creating Pipelines, Alerts, email notifications, and scheduling jobs.\nExposure on development/staging/production environments.\nProviding support in creating, monitoring and troubleshooting the scheduled jobs.\nEffectively work with client and handle client interactions.\n\nSkills Required:\nBachelors' degree in Engineering or Science or equivalent graduates with at least 4-11 years of overall experience in data management including data integration, modeling & optimization.\nMinimum 4 years of experience working on Azure cloud, Azure Data Factory, Azure Databricks.\nMinimum 3-4 years of experience in PySpark, Python, etc. for data ETL.\nIn-depth understanding of data warehouse, ETL concept and modeling principles.\nStrong ability to design, build and manage data.\nStrong understanding of Data integration.\nStrong Analytical and problem-solving skills.\nStrong Communication & client interaction skills.\nAbility to design database to store huge data necessary for reporting & dashboarding.\nAbility and willingness to acquire knowledge on the new technologies, good analytical and interpersonal skills\nwith ability to interact with individuals at all levels.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Azure Data Factory', 'Data Management', 'Data Engineer', 'Azure Data Platform', 'Azure Data Bricks', 'Pyspark', 'Data Modelling', 'ETL', 'Python']",2025-06-14 05:10:20
Senior Data Engineer -Bangalore,Happiest Minds Technologies,6 - 10 years,Not Disclosed,['Bengaluru'],"Job Overview:\nThe primary purpose of this role is to translate business requirements and functional specifications into logical program designs and to deliver dashboards, schema, data pipelines, and software solutions. This includes developing, configuring, or modifying data components within various complex business and/or enterprise application solutions in various computing environments. You will partner closely with multiple Business partners, Product Owners, Data Strategy, Data Platform, Data Science and Machine Learning (MLOps) teams to drive innovative data products for end users. Additionally, you will help shape overall solution & data products, develop scalable solutions through best-in-class engineering practices.",,,,"['NoSQL', 'big data systems', 'Data Pipeline', 'MongoDB', 'SQL', 'Hive', 'GIT', 'Hadoop', 'Kafka', 'Agile', 'MQL', 'Ci/Cd']",2025-06-14 05:10:22
Data Engineer-Data Platforms-AWS,IBM,4 - 9 years,Not Disclosed,['Kochi'],"As Data Engineer, you will develop, maintain, evaluate and test big data solutions. You will be involved in the development of data solutions using Spark Framework with Python or Scala on Hadoop and Azure Cloud Data Platform\n\nResponsibilities:\nExperienced in building data pipelines to Ingest, process, and transform data from files, streams and databases. Process the data with Spark, Python, PySpark and Hive, Hbase or other NoSQL databases on Azure Cloud Data Platform or HDFS\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\n\n\nPreferred technical and professional experience",,,,"['python', 'big data technologies', 'data engineering', 'sql', 'spark', 'hive', 'cloudera', 'scala', 'pyspark', 'microsoft azure', 'azure data factory', 'azure hdinsight', 'azure cloud', 'sql server', 'nosql', 'data bricks', 'apache', 'kafka', 'hadoop', 'big data', 'aws', 'cloud computing', 'hbase', 'nosql databases']",2025-06-14 05:10:24
Senior Manufacturing Data Engineer,Analog Devices,3 - 8 years,Not Disclosed,['Bengaluru'],"About Analog Devices\nAnalog Devices, Inc. (NASDAQ: ADI ) is a global semiconductor leader that bridges the physical and digital worlds to enable breakthroughs at the Intelligent Edge. ADI combines analog, digital, and software technologies into solutions that help drive advancements in digitized factories, mobility, and digital healthcare, combat climate change, and reliably connect humans and the world. With revenue of more than $9 billion in FY24 and approximately 24,000 people globally, ADI ensures todays innovators stay Ahead of Whats Possible . Learn more at www.analog.com and on LinkedIn and Twitter (X) .\nResponsibilities\nAs a Senior Manufacturing Data Engineer, you will collaborate with cross-functional teams, including data analysts, product managers, data scientists, and engineers, to deliver impactful data solutions for semiconductor manufacturing and automation.\nTranslate business and functional requirements into scalable data solutions aligned with Data Architecture guidelines for manufacturing and test environments.\nDevelop and maintain data pipelines to process manufacturing data, including formats such as STDF files, wafer metrology data, wafer sort results, tool logs, MES data, and FDC data, ensuring seamless integration into enterprise data systems.\nWrite scripts and programs to parse, extract, and transform data from diverse sources, improving data accessibility and quality across systems.\nSupport the integration of manufacturing and test data into cloud-based platforms such as Snowflake and Azure Data Lake, enabling advanced analytics and scalable processing.\nContribute to the standardization and optimization of data flows from tool logs, wafer tracking systems, and test equipment to improve reporting and analytics accuracy.\nPartner with senior team members to enhance data quality and completeness across MES, SPC, and test systems.\nPerform root cause analysis on data issues, ensuring timely resolution and adherence to SLA requirements.\nAssist in building subject matter expertise in MES platforms like Camstar/OpCenter and PROMIS, SPC tools, and data analytics platforms.\nParticipate in the development of frameworks for data pipeline observability, including alerting and monitoring systems.\nQualifications\n3+ years in Data Engineering, Data Science, or Data Analytics roles, with a preference for candidates with experience in semiconductor manufacturing.\nDegree in Computer Science, Electrical Engineering, Computer Engineering, or a related field.\nProficiency in SQL and Python.\nExperience with data modeling, ELT processes, and data integration techniques.\nFamiliarity with big data tools (e.g., Spark, Hadoop), containerization (e.g., Kubernetes), and cloud platforms like Snowflake, Azure, and DBT.\nExposure to analytics and visualization tools like Spotfire, Power BI, or Tableau.\nBasic understanding of statistical methods and techniques for anomaly detection and root cause analysis.\nExperience with scripting and programming for data parsing and transformation.",Industry Type: Consumer Electronics & Appliances,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Automation', 'Analog', 'Metrology', 'Healthcare', 'Licensing', 'Monitoring', 'Analytics', 'SQL', 'Python']",2025-06-14 05:10:27
AWS Data Engineer,Redserv Global Solutions,3 - 6 years,10-15 Lacs P.A.,[],"Roles and Responsibilities:\n\nWe are looking for an experienced AWS Cloud Data Engineer to join our Data Science & Analytics team to build, optimize, and maintain cloud-based data solutions. The ideal candidate will possess strong technical knowledge in data engineering on AWS, expertise in data integration, pipeline creation, performance optimization, and a strong understanding of DevOps methodologies.\nDesign, develop, and deploy scalable, high-performance data pipelines on AWS and scalable AWS infrastructure solutions.",,,,"['Data Engineer', 'AWS', 'Python']",2025-06-14 05:10:29
Snowflake Data Engineer,LatentView,3 - 6 years,Not Disclosed,"['Chennai', 'Bengaluru']","Job Description:\n\nWe are seeking a highly experienced and skilled Senior Data Engineer to join our dynamic team. This role requires hands-on experience with databases such as Snowflake and Teradata, as well as advanced knowledge in various data science and AI techniques. The successful candidate will play a pivotal role in driving data-driven decision-making and innovation within our organization.",,,,"['Data Engineering', 'Snowflake', 'Tableau', 'Data Warehousing', 'Data Modeling', 'ETL']",2025-06-14 05:10:31
IN Senior Associate GenAI S/W Engineer- Data and Analytics,PwC Service Delivery Center,1 - 7 years,Not Disclosed,['Bengaluru'],"Not Applicable\nSpecialism\nData, Analytics & AI\nManagement Level\nSenior Associate\n& Summary\nAt PwC, our people in data and analytics engineering focus on leveraging advanced technologies and techniques to design and develop robust data solutions for clients. They play a crucial role in transforming raw data into actionable insights, enabling informed decisionmaking and driving business growth.\n\nIn data engineering at PwC, you will focus on designing and building data infrastructure and systems to enable efficient data processing and analysis. You will be responsible for developing and implementing data pipelines, data integration, and data transformation solutions.\nWhy PWC\n& Summary\nJob Overview\nWe are seeking a highly skilled and versatile polyglot Full Stack Developer with expertise in modern frontend and backend technologies, cloudbased solutions, AI/ML and Gen AI. The ideal candidate will have a strong foundation in fullstack development, cloud platforms (preferably Azure), and handson experience in Gen AI, AI and machine learning technologies.\nKey Responsibilities\nDevelop and maintain web applications using Angular / React.js , .NET , and Python .\nDesign, deploy, and optimize Azure native PaaS and SaaS services, including but not limited to Function Apps , Service Bus , Storage Accounts , SQL Databases , Key vaults, ADF, Data Bricks and REST APIs with Open API specifications.\nImplement security best practices for data in transit and rest. Authentication best practices SSO, OAuth 2.0 and Auth0.\nUtilize Python for developing data processing and advanced AI/ML models using libraries like pandas , NumPy , scikitlearn and Langchain , Llamaindex , Azure OpenAI SDK\nLeverage Agentic frameworks like Crew AI, Autogen etc.\nWell versed with RAG and Agentic Architecture.\nStrong in Design patterns Architectural, Data, Object oriented\nLeverage azure serverless components to build highly scalable and efficient solutions.\nCreate, integrate, and manage workflows using Power Platform , including Power Automate , Power Pages , and SharePoint .\nApply expertise in machine learning , deep learning , and Generative AI to solve complex problems.\nPrimary Skills\nProficiency in React.js , .NET , and Python .\nStrong knowledge of Azure Cloud Services , including serverless architectures and data security.\nExperience with Python Data Analytics libraries\npandas\nNumPy\nscikitlearn\nMatplotlib\nSeaborn\nExperience with Python Generative AI Frameworks\nLangchain\nLlamaIndex\nCrew AI\nAutoGen\nFamiliarity with REST API design , Swagger documentation , and authentication best practices .\nSecondary Skills\nExperience with Power Platform tools such as Power Automate, Power Pages, and SharePoint integration.\nKnowledge of Power BI for data visualization (preferred).\nPreferred Knowledge Areas Nice to have\nIndepth understanding of Machine Learning , deep learning, supervised, unsupervised algorithms.\nMandatory skill sets\nAI, ML\nPreferred skill sets\nAI, ML\nYears of experience required\n3 7 years\nEducation qualification\nBE/BTECH, ME/MTECH, MBA, MCA\nEducation\nDegrees/Field of Study required Bachelor of Technology, Master of Business Administration, Bachelor of Engineering, Master of Engineering\nDegrees/Field of Study preferred\nRequired Skills\nGame AI\nAccepting Feedback, Accepting Feedback, Active Listening, Agile Scalability, Amazon Web Services (AWS), Analytical Thinking, Apache Airflow, Apache Hadoop, Azure Data Factory, Communication, Creativity, Data Anonymization, Data Architecture, Database Administration, Database Management System (DBMS), Database Optimization, Database Security Best Practices, Databricks Unified Data Analytics Platform, Data Engineering, Data Engineering Platforms, Data Infrastructure, Data Integration, Data Lake, Data Modeling, Data Pipeline {+ 28 more}\nTravel Requirements\nGovernment Clearance Required?",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Front end', 'Architecture', 'Data modeling', 'data security', 'Machine learning', 'data visualization', 'Apache', 'SQL', 'Python', 'Data architecture']",2025-06-14 05:10:33
Gcp Data Engineer,TELUS International,5 - 10 years,17-30 Lacs P.A.,[],"JD - Required skills:\n5+ years of industry experience in the field of Data Engineering support and enhancement.\nProficient in Google Cloud Platform (GCP) services such as Dataflow, BigQuery, Cloud Storage and Pub/Sub.\nStrong understanding of data pipeline architectures and ETL processes.\nExperience with Python programming language in terms of data processing.\nKnowledge of SQL and experience with relational databases.",,,,"['Data Pipeline', 'Bigquery', 'Data Flow', 'Gcp Cloud', 'Python', 'SQL']",2025-06-14 05:10:36
Senior / Lead Data Engineer,Quincy Compressor,5 - 8 years,Not Disclosed,['Pune'],"Senior / Lead Data Engineer Job Details | our company Search by Keyword Search by Location Select how often (in days) to receive an alert: Select how often (in days) to receive an alert: Senior / Lead Data Engineer Research and Development Atlas Copco (India) Private Ltd. Date of posting: Jun 11, 2025 Passionate people create exceptional things\nDid you know that the solutions we develop are a key part of most industriesElectronics, medical research, renewable energy, food production, infrastructure and many more.\nWe re everywhere! Working with us means working with the latest technologies and groundbreaking, sustainable innovations. With our inclusive and caring environment, you get the support and inspiration you need to grow.\nHere, your ideas are embraced, and you never stop learning. Interested in being part of our team\nYour Role\nAtlas Copco is a global, industrial company based in Stockholm, Sweden, with almost 40, 000 employees and customers in more than 180 countries. Atlas Copco has been driven by an innovative spirit ever since the start in 1873.\nDid you know that Atlas Copco is the largest Air Compressor manufacturer in the world with its compressors being used in industries ranging from hospitals to offshore oil drillsAre you looking to be part of an organization which believes in investing in its people and contributing towards their organic growthWould you be interested to be part of the organization which is tasked with establishing, consolidating and unifying data and analytics services across the business areas of Atlas CopcoAre you an experienced data engineer who would like to use their experience to build and maintain enterprise-production-grade data pipelines, considering business requirements and platform standardsThis might be the perfect opportunity for you to make a difference.\nWho are we\nThe Data Analytics is the competence within Atlas Copcos Global Engineering Center India Air Power (GECIA) responsible for establishing, consolidating, and unifying data and analytics services across the business areas of Atlas Copco. We strive to become the business partner for trusted, transparent, and robust enterprise data to enable data driven decision making.\nYou would be a part of Data Analytics Team of Global Engineering Center (GECIA) of Atlas Copco India Ltd and supporting CTIS from our Hinjewadi location in Pune, India. As a Data Engineer within our Data Anlytics Competence team, based out of Pune, you will lead the management and organisation of data for our Sales and Marketing leading indicators team which is focused on creating data and analytics solution related to the leading indicators space.\nWhat are we looking for\nWe are looking for minimum 5-8 years of experienced Data Engineers to build and maintain enterprise, production-grade data pipelines, considering business requirements and data platform standards. You will be expected to liaise with internal customers/stakeholders, understand their requirements, and propose solutions that align with their expectations. While you will be part of the local team at Atlas Copco, Pune office, you will also have the opportunity to be part of global project deliveries with potential on-site work if required.\nAs a key member of the highly motivated Data Analytics Competence Team, your work will enable team members to deliver ""first time right"" application delivery.\nPrincipal Duties and Responsibilities\nYou will be responsible for the management and organization of data for our applications\nYou will handle ETL processes, and manage data pipelines for a variety of applications, these will include shop floor applications, enterprise applications and home-grown applications developed within the team.\nYou will manage the data cloud infrastructure in Azure Data Lakehouse. You will also manage data in SAP using both SAP HANA and API connections\nYou will interact with the customers/stakeholders to understand their requirements and propose solutions.\nYou will work closely with Application Owners and be responsible for end-to-end processes/deployment.\nYou will document technical functional application aspects and release notes to facilitate the aftercare of the application\nYou ll be expected to work across the data spectrum and not be afraid to support the team in data analytics and data science activities if your skillset allows whilst prioritising your key projects.\nImportant areas of Expertise\nSkills Experience:\nWe are looking for minimum 5-8 years of experienced Data EngineersThe ideal candidate will have a good blend of business and technical skills. Specific requirements for this position include: General skills: You have a Maters degree in computer science of equivalent by experience The ideal candidate has strong and demonstrable hands-on experience with data engineering on cloud-based data platforms\nRequired Skills\nStrong working knowledge of ETL processes, and data pipeline management SAP SAP HANA\nStrong capability with Python\nGood understanding of low code workflow automation tools (Power Automate) Strong SQL skills and ability to both design, build and extract data from SQL databases Excellent English communication skills (written, oral), with good listening capabilities. Good technical analytical, debugging, and problem-solving skills\nHas a reasonable balance between getting the job done vs technical debt\nEnjoys enabling seamless deployments in a fast-moving environment.\nEffective team player working in a team; willingness to put the needs of the team over their own\nExperience with Microsoft Power Platform low-code tools (Power Apps/Power Automate/Dataverse)\nPreferred Skills\nExperience with product development for the Microsoft Azure platform Experience with product development life cycle would be a plus Experience with agile development methodology (Scrum) Functional analysis skills and experience (Use cases, UML) is an asset. Experience in NoSQL databases would be an asset.\nExperience in data science activities such as advanced analytics and machine learning would be an asset.\n\nCompetences:\nYou have a passion for innovation and technologies, combined with strong technical and analytical skills.\nYou are customer oriented, enthusiastic, and professional\nYou have a can-do mindset, hands-on approach and are decisive. You are not afraid of making errors and are willing to learn by doing\nYou are a strong communicator with excellent collaboration skills and are customer focused\nYou have self-drive and passion You are flexible and prepared to work outside of business hours if required to meet a deadline.\nYou have a proactive attitude and always strive for continuous improvement\nYou are able to cooperate with different levels in the organization, with different people and cultures\nYou can maintain good relations with external parties\nYou are stress resistant.\nYou are result oriented and quality focused on terms of processes.\nIn return, we offer you What we offer\nFlexible working hours\nA modern infrastructure providing you with the latest tools and technologies at your disposal\nA challenging environment which contributes to your constant learning and professional growth\nTo be part of the data competence team which is constantly growing\nDepending on the country you are enrolled in group healthcare insurance plans covering your medical needs\nA chance to become part of a global, innovative company, supporting sustainable productivity\nYou get the opportunity to bring revolutionary ideas fostering innovation and execute qualified ideas\nA friendly culture with immense professional and personal development, education and opportunities for career growth\nFree access to LinkedIn Learning and many other internal and external trainings\nAtlas Copco offers trainings on a regular basis to acquire new skills\nFriendly and open culture of Swedish company. Very high visibility in the organization with ""No door"" culture, you can always talk to anyone in the organization\nLast Day to Apply\n25/06/2025\nDiverse by nature and inclusive by choice\nBright ideas come from all of us. The more unique perspectives we embrace, the more innovative we are. Together we build a culture where difference is valued and we share a deep sense of purpose and belonging.",Industry Type: Industrial Equipment / Machinery,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Enterprise applications', 'UML', 'Debugging', 'Healthcare', 'Scrum', 'Continuous improvement', 'SQL', 'Python']",2025-06-14 05:10:38
Gcp Data Engineer,V2soft,3 - 8 years,Not Disclosed,"['Chennai', 'Sholinganallur']","Position Description:Bachelors Degree 2+Years in GCP Services - Biq Query, Data Flow, Dataproc, DataPlex,DataFusion, Terraform, Tekton, Cloud SQL, Redis Memory, Airflow, Cloud Storage 2+ Years inData Transfer Utilities 2+ Years in Git / any other version control tool 2+ Years in Confluent Kafka1+ Years of Experience in API Development 2+ Years in Agile Framework 4+ years of strongexperience in python, Pyspark development. 4+ years of shell scripting to develop the adhoc jobsfor data importing/exportingSkills Required:Google Cloud Platform - Biq Query, Data Flow, Dataproc, Data Fusion, TERRAFORM, Tekton,Cloud SQL, AIRFLOW, POSTGRES, Airflow PySpark, Python, API",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['GCP', 'Python', 'Airflow', 'Pyspark', 'Terraform']",2025-06-14 05:10:40
Senior / Lead Data Engineer,Atlas,1 - 2 years,Not Disclosed,['Pune'],"1. Designing and developing data pipelines: Lead data engineers are responsible for designing and developing data pipelines that move data from various sources to storage and processing systems.\n2. Building and maintaining data infrastructure: Lead data engineers are responsible for building and maintaining data infrastructure, such as data warehouses, data lakes, and data marts.\n3. Ensuring data quality and integrity: Lead data engineers are responsible for ensuring data quality and integrity, by setting up data validation processes and implementing data quality checks.\n4. Managing data storage and retrieval: Lead data engineers are responsible for managing data storage and retrieval, by designing and implementing data storage systems, such as NoSQL databases or Hadoop clusters.\n5. Developing and maintaining data models: Lead data engineers are responsible for developing and maintaining data models, such as data dictionaries and entity-relationship diagrams, to ensure consistency in data architecture.\n6. Managing data security and privacy: Lead data engineers are responsible for managing data security and privacy, by implementing security measures, such as access controls and encryption, to protect sensitive data.\n7. Leading and managing a team: Lead data engineers may be responsible for leading and managing a team of data engineers, providing guidance and support for their work.",Industry Type: Industrial Equipment / Machinery,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent",['Senior Lead'],2025-06-14 05:10:42
Senior Software / Data Engineer,Splore Human Centric Ai,4 - 9 years,Not Disclosed,['Bengaluru'],"What is Splore\n\n\n\nSplore is redefining how enterprises harness the power of generative AI and multi-agent systems. We work closely with established partners across industries like finance, legal, and tech, enabling them to solve real-world challenges and drive productivity. We integrate state-of-the-art AI technologies into existing business workflows, offering end-to-end solutions that enhance decision-making and streamline operations.\n\n\n\nBacked by industry leaders Temasek and Menyala, and powered by a team of AI and machine learning experts, Splore delivers AI applications to stay ahead in a rapidly evolving, data-driven landscape.\n\na data-driven world, providing precise, actionable insights that drive businesses to operate more effectively and make smarter decisions.\n\n\n\nWhat is the role\n\n\nAs the Senior Software / Data Engineer, you will act as the primary contact for the Engineering Director and Product Managers in steering platform development. You will drive the creation of our core / data platform and improve engineering effectiveness and processes by scaling the platform across target businesses. Overseeing a team of 2-4 engineers, you will build a strong engineering / data culture driving fast product innovation. Guiding and designing technical architecture, you will manage sprints and releases for the team.\n\n\n\nResponsibilities\n\n\nIn this role, you will:\n\n\nLead Architecture and Design: Architect, design, and oversee the implementation of various components of the core/data platform, ensuring integration with technologies developed by our AI/ML team. APIs for Front-End App, leveraging Cloud, Cloud Native, Edge Computing, and AI/ML technologies.\n\n\n\nMetrics and Optimization: Define key performance metrics for the platform, establish processes for tracking these metrics, and continuously optimize platform performance.\n\nEngineering Processes: Implement and refine engineering processes, utilizing the most effective tools and methodologies to enhance development efficiency and data management.\n\nCross-Functional Collaboration: Work closely with Product Management and Engineering teams to align AI, platform capabilities, and solutions with business and technical requirements.\n\nPlatform Development Lifecycle Management: Oversee the entire lifecycle of platform development, from conception through deployment and maintenance.\n\nTechnical Leadership and Oversight: Provide technical guidance to engineers, code reviews, ensuring high-quality technical delivery and effective problem-solving during implementation phases.\n\nArchitectural and Non-Functional Considerations: Ensure that the platform architecture adequately addresses key non-functional requirements such as high availability, scalability, maintainability, and extensibility\n\n\n\n\nAttributes\n\n\n\nWe are looking for a Senior Software / Data Engineer with the following:\n\n\nDealing with Ambiguity - You thrive in navigating dynamic environments, making informed decisions amid evolving scenarios and comfortably embracing uncertainty.\n\nCollaborates - Close partnership with our Product, Design and Engineering teams will be key in building our search engine!\n\nNimble Learning - Were looking for someone who thrives in a startup environment. Youre not afraid to get your hands dirty and learn through experimentation when faced with fresh challenges. Youre always on the pulse of the latest ML trends and immersing yourself in new technologies.\n\nFunctional/ Technical Skills - Demonstrated expertise in Backend Engineering with over 4 years of experience designing and building large-scale distributed systems and data platforms.\n\nProficient in Java and Python, with strong working knowledge of databases and modern UI technologies; experience with React is a plus.\n\nDeep understanding of technical architecture, system design, and software development best practices.\n\nProven track record of mentoring and leading engineers across various experience levels.\n\nHands-on experience with Data Engineering, platform development, data pipelines, real-time streaming, Data Lake architecture, and DevOps practices.\n\nBackground in startup environments, particularly in Data/AI startups or SaaS companies.\n\nFamiliarity with the MCP (Model Context Protocol) framework is a strong advantage.\n\nA passion for AI and emerging technologies is highly valued.",Industry Type: Internet,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Product management', 'Backend', 'Front end', 'Data management', 'Machine learning', 'Technical leadership', 'System design', 'Product design', 'Distribution system', 'Python']",2025-06-14 05:10:44
Lead Data Engineer,Whats On India Media,10 - 16 years,Not Disclosed,"['Bengaluru', 'Mumbai (All Areas)']","Gracenote, a Nielsen company, is dedicated to connecting audiences to the\nentertainment they love, powering a better media future for all people. Gracenote is the\ncontent data business unit of Nielsen that powers innovative entertainment experiences for\nthe worlds leading media companies. Our entertainment metadata and connected IDs\ndeliver advanced content navigation and discovery to connect consumers to the content\nthey love and discover new ones.\nGracenotes industry-leading datasets cover TV programs, movies, sports, music and\npodcasts in 80 countries and 35 languages.Common identifiers Universally adopted by the\nworlds leading media companies to deliver powerful cross-media entertainment\nexperiences. Machine driven, human validated best-in-class data and images fuel new\nsearch and discovery experiences across every screen.\nGracenote's Data Organization is a dynamic and innovative group that is essential in\ndelivering business outcomes through data, insights, predictive & prescriptive analytics. An\nextremely motivated team that values creativity, experimentation through continuous\nlearning in an agile and collaborative manner. From designing, developing and maintaining\ndata architecture that satisfies our business goals to managing data governance and\nregion-specific regulations, the data team oversees the whole data lifecycle.\nRole Overview:\nWe are seeking an experienced Senior Data Engineer with 10-12 years of experience to join\nour Video engineering team with Gracenote - a NielsenIQ Company. In this role, you will\ndesign, build, and maintain our data processing systems and pipelines. You will work closely\nwith Product managers, Architects, analysts, and other stakeholders to ensure data is\naccessible, reliable, and optimized for Business, analytical and operational needs.\n\nKey Responsibilities:\n\nDesign, develop, and maintain scalable data pipelines and ETL processes\nArchitect and implement data warehousing solutions and data lakes\nOptimize data flow and collection for cross-functional teams\nBuild infrastructure required for optimal extraction, transformation, and loading of data\nEnsure data quality, reliability, and integrity across all data systems\nCollaborate with data scientists and analysts to help implement models and algorithms\nIdentify, design, and implement internal process improvements: automating manual\nprocesses, optimizing data delivery, etc.\nCreate and maintain comprehensive technical documentation\nMentor junior engineers and provide technical leadership\nEvaluate and integrate new data management technologies and tools\nImplement Optimization strategies to enable and maintain sub second latency.\nOversee Data infrastructure to ensure robust deployment and monitoring of the\npipelines and processes.\nStay ahead of emerging trends in Data, cloud, integrating new research into practical\napplications.\nMentor and grow a team of junior data engineers.\n\nRequired qualification and Skills:\n\nExpert-level proficiency in Python, SQL, and big data tools (Spark, Kafka, Airflow).\nBachelor's degree in Computer Science, Engineering, or related field; Master's degree\npreferred\nExpert knowledge of SQL and experience with relational databases (e.g., PostgreSQL,\nRedshift, TIDB, MySQL, Oracle, Teradata)\nExtensive experience with big data technologies (e.g., Hadoop, Spark, Hive, Flink)\nProficiency in at least one programming language such as Python, Java, or Scala\nExperience with data modeling, data warehousing, and building ETL pipelines\nStrong knowledge of data pipeline and workflow management tools (e.g., Airflow, Luigi,\nNiFi)\nExperience with cloud platforms (AWS, Azure, or GCP) and their data services. AWS\nPreferred\nHands on Experience with building streaming pipelines with flink, Kafka, Kinesis. Flink\nPreferred.\nUnderstanding of data governance and data security principles\nExperience with version control systems (e.g., Git) and CI/CD practices\nProven leadership skills in grooming data engineering teams.\nPreferred Skills\nExperience with containerization and orchestration tools (Docker, Kubernetes)\nBasic knowledge of machine learning workflows and MLOps\nExperience with NoSQL databases (MongoDB, Cassandra, etc.)\nFamiliarity with data visualization tools (Tableau, Power BI, etc.)\nExperience with real-time data processing\nKnowledge of data governance frameworks and compliance requirements (GDPR, CCPA,\netc.)\nExperience with infrastructure-as-code tools (Terraform, CloudFormation)Role & responsibilities",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'Kafka Streams', 'AWS', 'GCP', 'Azure Cloud', 'Postgresql', 'MySQL']",2025-06-14 05:10:47
Senior Developer / Lead Data Engineer - Incorta,KPI Partners,5 - 10 years,Not Disclosed,['Pune'],"About Us:\n\n\nKPI Partners is a leading provider of data analytics and performance management solutions, dedicated to helping organizations harness the power of their data to drive business success. Our team of experts is at the forefront of the data revolution, delivering innovative solutions to our clients. We are currently seeking a talented and experienced Senior Developer / Lead Data Engineer with expertise in Incorta to join our dynamic team.",,,,"['Performance management', 'Data modeling', 'Analytical', 'MySQL', 'SCALA', 'Data quality', 'data visualization', 'Analytics', 'Python']",2025-06-14 05:10:49
Engineer Data,Empower Pragati,2 - 6 years,Not Disclosed,['Bengaluru'],"Our vision for the future is based on the idea that transforming financial lives starts by giving our people the freedom to transform their own. We have a flexible work environment, and fluid career paths. We not only encourage but celebrate internal mobility. We also recognize the importance of purpose, well-being, and work-life balance. Within Empower and our communities, we work hard to create a welcoming and inclusive environment, and our associates dedicate thousands of hours to volunteering for causes that matter most to them.\nChart your own path and grow your career while helping more customers achieve financial freedom. Empower Yourself.\nThis role supports Empower s data and AI strategy, with a focus on building Responsible AI capabilities. The Data Engineer will design and implement scalable, ethical, and secure data pipelines and infrastructure that underpin AI/ML systems, ensuring high-quality data flows into model development, testing, monitoring, and governance workflows. The candidate will work across cloud (AWS) and on-premises environments, contributing to the lifecycle of data used for Responsible AI tooling, including bias detection, model transparency, and compliance tracking.\nESSENTIAL FUNCTIONS:\nDesign, build, and maintain data pipelines that support model development, testing, and monitoring, with a focus on AI governance and traceability.\nCollaborate with cross-functional teams (including Data Scientists, ML Engineers, and Risk) to understand data needs for AI use cases.\nIntegrate data quality, lineage, and metadata tracking into ETL pipelines to support Responsible AI workflows.\nSupport ingestion and transformation of structured and unstructured data (including NLP datasets) for AI model training and evaluation.\nDesign with compliance in mind: integrate secure handling of PII and support auditability in data flows.\nParticipate in technical design discussions focused on enabling transparency, fairness, and explainability in data workflows.\nTroubleshoot and resolve performance and data quality issues in distributed AI pipelines.\nContribute to reusable libraries or templates to support standardized data practices across AI projects.\nQUALIFICATIONS:\nBachelor s Degree in Computer Science , Information Systems, or related field.\n2-6 years of experience in data engineering, preferably in AI/ML environments.\nStrong Python and SQL skills with experience in data pipeline orchestration (e.g., Airflow, Step Functions).\nExperience with Big Data frameworks (e.g., Spark, Hadoop) and streaming data platforms (e.g., Kafka).\nExperience working in AWS environments with services like S3, Glue, Redshift, SageMaker, and Lake Formation.\nFamiliarity with machine learning workflows and data requirements (e.g., training/test splits, data versioning, feature stores).\nExperience integrating data validation, data lineage, or metadata tools (e.g., Great Expectations, Apache Atlas, Amundsen).\nUnderstanding of Responsible AI principles and experience supporting data aspects of fairness, bias, explainability, or model monitoring is a strong plus.\nExperience with JIRA and Agile methodologies.\nExperience in financial services or other highly regulated environments preferred.\nThis job description is not intended to be an exhaustive list of all duties, responsibilities and qualifications of the job. The employer has the right to revise this job description at any time. You will be evaluated in part based on your performance of the responsibilities and/or tasks listed in this job description. You may be required perform other duties that are not included on this job description. The job description is not a contract for employment, and either you or the employer may terminate employment at any time, for any reason.\nWe are an equal opportunity employer with a commitment to diversity. All individuals, regardless of personal characteristics, are encouraged to apply. All qualified applicants will receive consideration for employment without regard to age, race, color, national origin, ancestry, sex, sexual orientation, gender, gender identity, gender expression, marital status, pregnancy, religion, physical or mental disability, military or veteran status, genetic information, or any other status protected by applicable state or local law.",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data validation', 'development testing', 'Machine learning', 'Agile', 'model development', 'Data quality', 'Financial services', 'Monitoring', 'SQL']",2025-06-14 05:10:51
Senior Data Engineer,Thinkapps Solutions,5 - 10 years,Not Disclosed,"['Kolkata', 'Hyderabad', 'Pune', 'Ahmedabad', 'Chennai', 'Bengaluru', 'Delhi / NCR', 'Mumbai (All Areas)']","•Experience in MongoDB database is important, other databases of SQL Server, PostgreSQL knowledge also required. Require understand & modifying the database objects based on business request. Must have deep understanding of preparing complex queries,",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['MongoDB', 'Data Bricks', 'PostgreSQL', 'SQL']",2025-06-14 05:10:53
Senior Data Engineer,ANS Group,2 - 6 years,Not Disclosed,['Ahmedabad'],"ANS Group is looking for Senior Data Engineer\nThe job responsibilities of a Senior Data Engineer may include:1\n\nDesigning and implementing scalable and reliable data pipelines, data models, and data infrastructure for processing large and complex datasets\n\n2\n\nDeveloping and maintaining databases, data warehouses, and data lakes that store and manage the organization's data\n\n3\n\nDeveloping and implementing data integration and ETL (Extract, Transform, Load) processes to ensure that data flows smoothly and accurately between different systems and data sources\n\n4\n\nEnsuring data quality, consistency, and accuracy through data profiling, cleansing, and validation\n\n5\n\nBuilding and maintaining data processing and analytics systems that support business intelligence, machine learning, and other data-driven applications\n\n6\n\nOptimizing the performance and scalability of data systems and infrastructure to ensure that they can handle the organization's growing data needs\n\nTo be a successful Senior Data Engineer, one must have in-depth knowledge of database architecture, data modeling, data integration, and ETL processes\n\nThey should also be proficient in programming languages such as Python, Java, or SQL and have experience working with big data technologies like Hadoop, Spark, and NoSQL databases\n\nStrong communication and leadership skills",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'data warehousing', 'data pipeline', 'machine learning', 'data engineering', 'sql', 'nosql', 'database design', 'data quality', 'java', 'data modeling', 'spark', 'leadership', 'hadoop', 'etl', 'data integration', 'programming', 'data lake', 'etl process', 'communication skills', 'data profiling']",2025-06-14 05:10:56
Senior Data Engineer,Ensemble Health Partners,5 - 9 years,Not Disclosed,[],"Company Overview:\nAs Ensemble Health Partners Company, we're at the forefront of innovation, leveraging cutting-edge technology to drive meaningful impact in the Revenue Cycle Management landscape. Our future-forward technology combines tightly integrated data ingestion, workflow automation and business intelligence solutions on a modern cloud architecture. We have the second-largest share in the RCM space in the US Market with 10000+ professionals working in the organization. With 10 Technology Patents in our name, we believe the best results come from a combination of skilled and experienced team, proven and repeatable processes, and modern and flexible technologies. As a leading player in the industry, we offer an environment that fosters growth, creativity, and collaboration, where your expertise will be valued, and your contributions will make a difference.\n\nRole & responsibilities :\nExperience : 5-9 Years\nLocation : remote/wfh\n\nPosition Summary :\nDesign and maintain scalable data pipelines, manage ETL processes and data warehouses, ensure data quality and governance, collaborate with cross-functional teams, support machine learning deployment, lead projects, mentor juniors, work with big data and cloud technologies, and bring expertise in Spark, Databricks, Streaming/Reactive/Event-driven systems, Agentic programming, and LLM application development.\n\nRequired Skills :\nSpark, Databricks, Streaming/Reactive /Event driven, Agentic programming & LLM Application Experience\n5+ years of coding experience with Microsoft SQL.\n3+ years working with big data technologies including but not limited to Databricks, Apache Spark, Python, Microsoft Azure (Data Factory, Dataflows, Azure Functions, Azure Service Bus) with a willingness and ability to learn new ones\nExcellent understanding of engineering fundamentals: testing automation, code reviews, telemetry, iterative delivery and DevOps\nExperience with polyglot storage architectures including relational, columnar, key-value, graph or equivalent\nExperience with Delta tables as well as Parquet files stored in ADLS\nExperience delivering applications using componentized and distributed architectures using event driven patterns\nDemonstrated ability to communicate effectively to both technical and non-technical, globally distributed audiences\nSolid foundations in formal architecture, design patterns and best practices\nExperience working with healthcare datasets\n\nWhy Join US?\nWe adapt emerging technologies to practical uses to deliver concrete solutions that bring maximum impact to providers bottom line. We currently have 10 Technology Patents in our name.\nWe offer you a great organization to work for, where you will get to do best work of your career and grow with the team that is shaping the future of Revenue Cycle Management.\nWe have our strong focus on Learning and development. We have the best Industry standard professional development policies to support the learning goals of our associates.\nWe have flexible/ remote working/ working from home options\nBenefits\nHealth Benefits and Insurance Coverage for family and parents. Accidental Insurance for the associate.\nCompliant with all Labor Laws- Maternity benefits, Paternity Leaves.\nCompany Swags- Welcome Packages, Work Anniversary Kits\nExclusive Referral Policy\nProfessional Development Program and Reimbursements.\nRemote work flexibility to work from home.\nPlease share your resume on yash.arora@ensemblehp.com with current ctc, expected ctc, notice period.",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Data Factory', 'Pyspark', 'Azure Functions', 'Azure Databricks']",2025-06-14 05:10:58
Lead Data Engineer - SQL and AWS,Gartner for HR,6 - 11 years,Not Disclosed,['Gurugram'],"About the Role:\nGartner is looking for passionate and motivated Lead Data Engineers who are excited to foray into new technologies and help build / maintain data driven components for realizing business needs. This role is in Gartner Product Delivery Organization (PDO). PDO Data Engineering teams are high velocity agile teams responsible for developing and maintaining components crucial to customer-facing channels, data science teams, and reporting & analysis. These components include but are not limited to Spark jobs, REST APIs, AI/ML model training & inference, MLOps / devops pipelines, data transformation & quality pipelines, data lake & data catalogs, data streams etc.\nWhat you will do :\nAbility to lead and execute a mix of small/medium sized projects simultaneously\nOwns success, takes responsibility for successful delivery of solutions from development to production.\nMentor and guide team members\nExplore and create POC of new technologies / frameworks\nShould have significant experience working directly with Business users in problem solving\nExcellent Communication and Prioritization skills.\nShould be able to interact and coordinate well with other developers / teams to resolve operational issues.\nShould be self-motivated and a fast learner to ramp up quickly with a fair amount of help from team members.\nMust be able to estimate development tasks with high accuracy and deliver on time with high quality while following coding guidelines and best practices.\nIdentify systemic operational issues and resolve them.\nWhat you will need\n6+ years of post-college experience in data engineering, API development or related fields\nMust have\nDemonstrated experience in data engineering, data science, or machine learning. Experience working with data platforms - building and maintaining ETL flows and data stores for ML and reporting applications.\nSkills to transform data, prepare it for analysis, and analyze it - including structured and unstructured data\nAbility to transform business needs into technical solutions\nDemonstrated experience of cloud platforms (AWS, Azure, GCP, etc.)\nExperience with languages such as Python, Java, SQL\nExperience with tools such as Apache Spark, Databricks, AWS EMR\nExperience with Kanban or Agile Scrum development\nExperience with REST API development\nExperience collaboration tools such as Git, Jenkins, Jira, Confluence\nExperience with Data modeling and Database schema / table design\n#LI-SP7",Industry Type: Analytics / KPO / Research,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['GIT', 'Data modeling', 'Coding', 'Machine learning', 'Schema', 'Mentor', 'JIRA', 'SQL', 'Python', 'Recruitment']",2025-06-14 05:11:00
"Senior data engineer - Python, Pyspark, AWS - 5+ years Gurgaon",One of the largest insurance providers.,5 - 10 years,Not Disclosed,['Gurugram'],"Senior data engineer - Python, Pyspark, AWS - 5+ years Gurgaon\n\nSummary: An excellent opportunity for someone having a minimum of five years of experience with expertise in building data pipelines. A person must have experience in Python, Pyspark and AWS.\n\nLocation- Gurgaon (Hybrid)\n\nYour Future Employer- One of the largest insurance providers.\n\nResponsibilities-\nTo design, develop, and maintain large-scale data pipelines that can handle large datasets from multiple sources.\nReal-time data replication and batch processing of data using distributed computing platforms like Spark, Kafka, etc.\nTo optimize the performance of data processing jobs and ensure system scalability and reliability.\nTo collaborate with DevOps teams to manage infrastructure, including cloud environments like AWS.\nTo collaborate with data scientists, analysts, and business stakeholders to develop tools and platforms that enable advanced analytics and reporting.\n\nRequirements-\nHands-on experience with AWS services such as S3, DMS, Lambda, EMR, Glue, Redshift, RDS (Postgres) Athena, Kinesics, etc.\nExpertise in data modeling and knowledge of modern file and table formats.\nProficiency in programming languages such as Python, PySpark, and SQL/PLSQL for implementing data pipelines and ETL processes.\nExperience data architecting or deploying Cloud/Virtualization solutions (Like Data Lake, EDW, Mart ) in the enterprise.\nCloud/hybrid cloud (preferably AWS) solution for data strategy for Data lake, BI and Analytics.\nWhat is in for you-\nA stimulating working environment with equal employment opportunities.\nGrowing of skills while working with industry leaders and top brands.\nA meritocratic culture with great career progression.\n\nReach us- If you feel that you are the right fit for the role please share your updated CV at randhawa.harmeen@crescendogroup.in\n\nDisclaimer- Crescendo Global specializes in Senior to C-level niche recruitment. We are passionate about empowering job seekers and employers with an engaging memorable job search and leadership hiring experience. Crescendo Global does not discriminate on the basis of race, religion, color, origin, gender, sexual orientation, age, marital status, veteran status, or disability status.",Industry Type: Insurance,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Pyspark', 'Data Pipeline', 'AWS', 'Data Ingestion', 'Data Engineering', 'Data Processing']",2025-06-14 05:11:03
Senior Data Engineer,Suzva Software Technologies,6 - 8 years,Not Disclosed,"['Mumbai', 'Delhi / NCR', 'Bengaluru']","JobOpening Senior Data Engineer (Remote, Contract 6 Months)\nRemote | Contract Duration: 6 Months | Experience: 6-8 Years\n\nWe are hiring a Senior Data Engineer for a 6-month remote contract position. The ideal candidate is highly skilled in building scalable data pipelines and working within the Azure cloud ecosystem, especially Databricks, ADF, and PySpark. You'll work closely with cross-functional teams to deliver enterprise-level data engineering solutions.\n\n#KeyResponsibilities\nBuild scalable ETL pipelines and implement robust data solutions in Azure.\n\nManage and orchestrate workflows using ADF, Databricks, ADLS Gen2, and Key Vaults.\n\nDesign and maintain secure and efficient data lake architecture.\n\nWork with stakeholders to gather data requirements and translate them into technical specs.\n\nImplement CI/CD pipelines for seamless data deployment using Azure DevOps.\n\nMonitor data quality, performance bottlenecks, and scalability issues.\n\nWrite clean, organized, reusable PySpark code in an Agile environment.\n\nDocument pipelines, architectures, and best practices for reuse.\n\n#MustHaveSkills\nExperience: 6+ years in Data Engineering\n\nTech Stack: SQL, Python, PySpark, Spark, Azure Databricks, ADF, ADLS Gen2, Azure DevOps, Key Vaults\n\nCore Expertise: Data Warehousing, ETL, Data Pipelines, Data Modelling, Data Governance\n\nAgile, SDLC, Containerization (Docker), Clean coding practices\n\n#GoodToHaveSkills\nEvent Hubs, Logic Apps\n\nPower BI\n\nStrong logic building and competitive programming background\n\nMode: Remote\n\nDuration: 6 Months\nLocations : Mumbai, Delhi / NCR, Bengaluru , Kolkata, Chennai, Hyderabad, Ahmedabad, Pune, Remote",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Databricks', 'Data Modelling', 'Data Pipelines', 'ADF', 'PySpark', 'Data Warehousing', 'ETL', 'Data Governance']",2025-06-14 05:11:05
Senior Data Engineer,Tata Elxsi,8 - 13 years,25-40 Lacs P.A.,['Bengaluru'],"*Must-Have Skills:*\n\n* Azure Databricks / PySpark hands-on\n* SQL/PL-SQL advanced level\n* Snowflake – 2+ years\n* Spark/Data pipeline development – 2+ years\n* Azure Repos / GitHub, Azure DevOps\n* Unix Shell Scripting\n* Cloud technology experience\n\n*Key Responsibilities:*\n\n1. *Design, build, and manage data pipelines using Azure Databricks, PySpark, and Snowflake.\n2. *Analyze and resolve production issues (Tier 2 support with weekend/on-call rotation).\n3. *Write and optimize complex SQL/PL-SQL queries.\n4. *Collaborate on low-level and high-level design for data solutions.\n5. *Document all project deliverables and support deployment.\n\nGood to Have:\nKnowledge of Oracle, Qlik Replicate, GoldenGate, Hadoop\nJob scheduler tools like Control-M or Airflow\n\nBehavioral:\nStrong problem-solving & communication skills",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Pyspark', 'Azure Databricks', 'Cloud Technologies', 'Snowflake', 'Etl Development', 'Data Engineering', 'Github', 'Data Pipeline Development', 'Unix Shell Scripting', 'SQL', 'Apache Spark', 'Azure Repo', 'PLSQL', 'Oracle', 'Azure Devops']",2025-06-14 05:11:07
"Lead Engineer, Data Engineering (J2EE/Angular/React/React Full Stack)",S&P Global Market Intelligence,10 - 15 years,Not Disclosed,"['Mumbai', 'Maharastra']","About the Role:\nGrade Level (for internal use): 11\nThe Team\nYou will be an expert contributor and part of the Rating Organizations Data Services Product Engineering Team. This team, who has a broad and expert knowledge on Ratings organizations critical data domains, technology stacks and architectural patterns, fosters knowledge sharing and collaboration that results in a unified strategy. All Data Services team members provide leadership, innovation, timely delivery, and the ability to articulate business value. Be a part of a unique opportunity to build and evolve S&P Ratings next gen analytics platform.\nResponsibilities:\nArchitect, design, and implement innovative software solutions to enhance S&P Ratings' cloud-based analytics platform.\nMentor a team of engineers (as required), fostering a culture of trust, continuous growth, and collaborative problem-solving.\nCollaborate with business partners to understand requirements, ensuring technical solutions align with business goals.\nManage and improve existing software solutions, ensuring high performance and scalability.\nParticipate actively in all Agile scrum ceremonies, contributing to the continuous improvement of team processes.\nProduce comprehensive technical design documents and conduct technical walkthroughs.\nExperience & Qualifications:\nBachelors degree in computer science, Information Systems, Engineering, equivalent or more is required\nProficient with software development lifecycle (SDLC) methodologies like Agile, Test-driven development\n10+ years of experience with 4+ years designing/developing enterprise products, modern tech stacks and data platforms\n4+ years of hands-on experience contributing to application architecture & designs, proven software/enterprise integration design patterns and full-stack knowledge including modern distributed front end and back-end technology stacks\n5+ years full stack development experience in modern web development technologies, Java/J2EE, UI frameworks like Angular, React, SQL, Oracle, NoSQL Databases like MongoDB\nExperience designing transactional/data warehouse/data lake and data integrations with Big data eco system leveraging AWS cloud technologies\nThorough understanding of distributed computing\nPassionate, smart, and articulate developer\nQuality first mindset with a strong background and experience with developing products for a global audience at scale\nExcellent analytical thinking, interpersonal, oral and written communication skills with strong ability to influence both IT and business partners\nSuperior knowledge of system architecture, object-oriented design, and design patterns.\nGood work ethic, self-starter, and results-oriented\nExcellent communication skills are essential, with strong verbal and writing proficiencies\nExp. with Delta Lake systems like Databricks using AWS cloud technologies and PySpark is a plus\nAdditional Preferred Qualifications:\nExperience working AWS\nExperience with SAFe Agile Framework\nBachelor's/PG degree in Computer Science, Information Systems or equivalent.\nHands-on experience contributing to application architecture & designs, proven software/enterprise integration design principles\nAbility to prioritize and manage work to critical project timelines in a fast-paced environment\nExcellent Analytical and communication skills are essential, with strong verbal and writing proficiencies\nAbility to train and mentor",Industry Type: Banking,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'AWS cloud technologies', 'PySpark', 'J2EE', 'React Full Stack', 'Databricks', 'React', 'Angular']",2025-06-14 05:11:09
Data Streaming Engineer,Data Streaming Engineer,5 - 10 years,Not Disclosed,"['Pune', 'Chennai', 'Bengaluru']","Hello Candidates,\n\nWe are Hiring !!\n\nJob Position - Data Streaming Engineer\nExperience - 5+ years\nLocation - Mumbai, Pune , Chennai , Bangalore\nWork mode - Hybrid ( 3 days WFO)\n\nJOB DESCRIPTION\n\nRequest for Data Streaming Engineer Data Streaming @ offshore :\n• Flink , Python Language.\n• Data Lake Systems. (OLAP Systems).\n• SQL (should be able to write complex SQL Queries)\n• Orchestration (Apache Airflow is preferred).\n• Hadoop (Spark and Hive: Optimization of Spark and Hive apps).\n• Snowflake (good to have).\n• Data Quality (good to have).\n• File Storage (S3 is good to have)\n\nNOTE - Candidates can share their resume on - shrutia.talentsketchers@gmail.com",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Flink', 'Apache Airflow', 'Data Quality', 'Hadoop', 'Snowflake', 'Data Lake', 'orchastration', 'Python', 'SQL']",2025-06-14 05:11:12
Data Engineer / Sr. Data Entry Specialist,Techwaukee,2 - 4 years,Not Disclosed,[],Role Purpose\nThe purpose of this role is to execute the process and drive the performance of the team on the key metrices of the process.\nJob Details\nCountry/Region: India\nEmployment Type: Remote\nWork Type: Contract,,,,"['Product management', 'ERP', 'Data modeling', 'XML', 'MySQL', 'JSON', 'Informatica', 'Information technology', 'SQL', 'Python']",2025-06-14 05:11:14
AWS Data Engineer,Redserv Global Solutions,3 - 6 years,10-15 Lacs P.A.,[],"Roles and Responsibilities:\n\nWe are looking for an experienced AWS Cloud Data Engineer to join our Data Science & Analytics team to build, optimize, and maintain cloud-based data solutions. The ideal candidate will possess strong technical knowledge in data engineering on AWS, expertise in data integration, pipeline creation, performance optimization, and a strong understanding of DevOps methodologies.",,,,"['Data Engineer', 'AWS', 'Python']",2025-06-14 05:11:16
Senior Data Engineer,Guidehouse,5 - 10 years,Not Disclosed,['Chennai'],"Job Posting\nWhat You Will Do:\nDesign, develop, and maintain robust, scalable, and efficient data pipelines and ETL/ELT processes.\nLead and execute data engineering projects from inception to completion, ensuring timely delivery and high quality.\nBuild and optimize data architectures for operational and analytical purposes.\nCollaborate with cross-functional teams to gather and define data requirements.\nImplement data quality, data governance, and data security practices.\nManage and optimize cloud-based data platforms (Azure\\AWS).\nDevelop and maintain Python/PySpark libraries for data ingestion, Processing and integration with both internal and external data sources.\nDesign and optimize scalable data pipelines using Azure data factory and Spark(Databricks)\nWork with stakeholders, including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.\nDevelop frameworks for data ingestion, transformation, and validation.\nMentor junior data engineers and guide best practices in data engineering.\nEvaluate and integrate new technologies and tools to improve data infrastructure.\nEnsure compliance with data privacy regulations (HIPAA, etc.).\nMonitor performance and troubleshoot issues across the data ecosystem.\nAutomated deployment of data pipelines using GIT hub actions \\ Azure devops\nWhat You Will Need:\nBachelors or masters degree in computer science, Information Systems, Statistics, Math, Engineering, or related discipline.\nMinimum 5 + years of solid hands-on experience in  data engineering and cloud services.\nExtensive working experience with advanced SQL and deep understanding of SQL.\nGood Experience in Azure data factory (ADF), Databricks , Python and PySpark.\nGood experience in modern data storage concepts data lake, lake house.\nExperience in other cloud services (AWS) and data processing technologies will be added advantage.\nAbility to enhance , develop and resolve defects in ETL process using cloud services.\nExperience handling large volumes (multiple terabytes) of incoming data from clients and 3rd party sources in various formats such as text, csv, EDI X12 files and access database.\nExperience with software development methodologies (Agile, Waterfall) and version control tools\nHighly motivated, strong problem solver, self-starter, and fast learner with demonstrated analytic and quantitative skills.\nGood communication skill.\nWhat Would Be Nice To Have:\nAWS ETL Platform – Glue , S3\nOne or more programming languages such as Java, .Net\nExperience in US health care domain and insurance claim processing.",Industry Type: Management Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Pyspark', 'databricks', 'SQL', 'azure']",2025-06-14 05:11:18
Senior Data Engineer,Infraveo Technologies,3 - 6 years,Not Disclosed,[],"We are seeking a Senior Data Engineer (PostgreSQL and d SQL syntax) to join our team.\n\nResponsibilties:\nWork closely with other talented database professionals & software engineer. Our vetting process means you can count on your team members to know what they are talking about.\nWork from home or work remotely from anywhere.\nFlexible work schedule: meaning you can work regular hours or whenever you work best\nWork-life balance is essential and highly valued at company. If you choose to work more than 40 hours, youll be compensated for the\nextra work.\nWork on interesting projects solving complex business problems with custom software.\n100 hours per year to focus on your professional development. We invest in your growth.\nProfit sharing bonus means as were successful, youre successful.\nVariable compensation opportunities for being part of on-call team rotation and performing client focused work outside of normal business hours.\nExcellent benefits package including medical insurance, dental, vision, 401(k) matching, FSA, disability, life insurance, and paid parental leave.\nRequirements\nBasic understanding of data modeling design patterns such as 3NF and star-schema.\nData warehouse design experience using medallion architecture and/or snowflake methodologies.\nExperience with Microsoft Fabric, Synapse or Snowflake.\nReporting tools such as Power BI.\nETL tools such as Azure Data Factory.\nPerformance tuning databases and long running queries.\nStored procedure development.\nDatabase migrations and/or database server consolidations, including Azure SQL solutions.\nConfigure Azure SQL databases, elastic pools, and Managed Instances.\nPerform health checks, assessments, and security audits for client databases.\nImplement replication and/or log shipping for disaster recovery or reporting scenarios.\nEvaluate disk I/O and network throughput for SQL performance.\n\nNice to Have Experience:\n\nAdvanced SQL syntax.\nSQL Server installation & configuration in production environments using best practices.\nAvailability Group & failover cluster configuration.\nExposure to MySQL and/or PostgreSQL environments.\nFamiliarity with Red Gate SQL Monitor or similar tools.\nRight-sizing VM for SQL Server.\nDevelop and maintain robust backup, recovery, and maintenance plans.\nHolding Microsoft certification(s) in the Data & AI solutions field are a plus.\nMicrosoft Certified: Fabric Analytics Engineer Associate (DP-600).\nMicrosoft Certified: Fabric Data Engineer Associate (DP-700).\nMicrosoft Certified: Power BI Data Analyst Associate (PL-300).\nMicrosoft Certified: Administering Microsoft Azure SQL Solutions (DP-300).",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Performance tuning', 'Data modeling', 'Postgresql', 'MySQL', 'Database', 'power bi', 'Medical insurance', 'microsoft', 'Warehouse design', 'SQL']",2025-06-14 05:11:20
Senior Data engineer,Wissen Technology,10 - 17 years,Not Disclosed,['Pune'],"Wissen Technology is Hiring fo r Senior Data engineer\n\nAbout Wissen Technology:\nWissen Technology is a globally recognized organization known for building solid technology teams, working with major financial institutions, and delivering high-quality solutions in IT services. With a strong presence in the financial industry, we provide cutting-edge solutions to address complex business challenges\n\nRole Overview : We are looking for a highly skilled and experienced Senior Data Engineer to join our growing data engineering team in Bangalore . In this role, you will be responsible for designing, developing, and maintaining scalable and efficient data pipelines and data architectures. You will collaborate closely with cross-functional teams to ensure data accessibility, reliability, and performance to support business intelligence, analytics, and data science initiatives\n\nExperience : 9-13 Years\nLocation: Bangalore\nKey Responsibilities\nDesign , build, and maintain scalable and reliable data pipelines for ingesting, transforming, and loading structured and unstructured data from diverse sources.\nDevelop optimized SQL queries and modular scripts for data manipulation and transformation.\nCreate and maintain data models and schemas to support advanced analytics, reporting, and operational processes.\nAutomate routine data engineering tasks and data quality checks using scripting (Python, Shell, etc.) and orchestration tools.\nCollaborate with engineering, product, and data science teams to understand data requirements and deliver high-quality solutions.\nAct as a liaison between engineering and business teams to translate business needs into technical solutions.\nImplement monitoring, alerting, and troubleshooting mechanisms for data pipelines and dashboards to ensure data integrity and availability.\nDefine and implement best practices for data validation, data governance, and compliance.\nManage test data and QA environments, supporting test data management processes.\nWork in an Agile environment and contribute to continuous improvement initiatives across the data engineering landscape.\n\n\nRequired Skills and Qualification\nBachelors or masters degree in computer science , Information Systems, or a related field.\n9+ years of professional experience in data engineering or a related field.\nStrong expertise in SQL development and performance tuning for both RDBMS and cloud-based databases.\nHands-on experience with cloud platforms, particularly AWS (e.g., S3, Glue, Lambda, Redshift, EMR).\nExperience with modern data warehouse technologies like Snowflake and Amazon Redshift .\nStrong background in data modeling , ETL/ELT architecture, and data warehousing concepts.\nProficiency in programming languages such as Python , Scala , or Java .\nExperience working with",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Telecom', 'Performance tuning', 'Manager Quality Assurance', 'Data management', 'Consulting', 'Manager Technology', 'Healthcare', 'Business intelligence', 'Monitoring', 'Python']",2025-06-14 05:11:22
Senior Azure Data Engineer,Bit Wave Solutions,7 - 12 years,Not Disclosed,[],"Position : Sr Azure Data Engineer\nLocation: Remote\nTime : CET Time\nRole & responsibilities\n\nWe are seeking a highly skilled Senior Data Engineer to join our dynamic team. The ideal candidate will have extensive experience in Microsoft Azure, Fabric Azure SQL, Azure Synapse, Python, and Power BI. Knowledge of Oracle DB and data replication tools will be preferred. This role involves designing, developing, and maintaining robust data pipelines and ensuring efficient data processing and integration across various platforms.\n\nCandidate understands stated needs & requirements of the stakeholders and produce high quality deliverables Monitors own work to ensure delivery within the desired performance standards. Understands the importance of delivery within expected time, budget and quality standards and displays concern in case of deviation. Good communication skills and a team player\n\nDesign and Development: Architect, develop, and maintain scalable data pipelines using Microsoft Fabric and Azure services, including Azure SQL and Azure Synapse.\n\nData Integration: Integrate data from multiple sources, ensuring data consistency, quality, and availability using data replication tools.\nData Management: Manage and optimize databases, ensuring high performance and reliability.\nETL Processes: Develop and maintain ETL processes to transform data into actionable insights.\nData Analysis: Use Python and other tools to analyze data, create reports, and provide insights to support business decisions.\nVisualization: Develop and maintain dashboards and reports in Power BI to visualize complex data sets.\nPerformance Tuning: Optimize database performance and troubleshoot any issues related to data processing and integration\n\nPreferred candidate profile\n\nMinimum 7 years of experience in data engineering or a related field.\nProven experience with Microsoft Azure services, Fabrics including Azure SQL and Azure Synapse.\nStrong proficiency in Python for data analysis and scripting.\nExtensive experience with Power BI for data visualization.\nKnowledge of Oracle DB and experience with data replication tools.\nProficient in SQL and database management.\nExperience with ETL tools and processes.\nStrong understanding of data warehousing concepts and architectures.\nFamiliarity with cloud-based data platforms and services.\nAnalytical Skills: Ability to analyze complex data sets and provide actionable insights.\nProblem-Solving: Strong problem-solving skills and the ability to troubleshoot data-related issues.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Azure Data Factory', 'Azure Synapse', 'Microsoft Fabric', 'Python', 'Power Bi', 'Powerapps', 'SQL']",2025-06-14 05:11:25
Sr. Data Engineer - Snowflake,Freelancer Monika,5 - 10 years,15-25 Lacs P.A.,['Pune'],"Role & responsibilities\nDesigned and implemented end-to-end data pipeline using DBT, Snowflake  \nCreated and structure DBT models like staging, transformation, marts, YAML configurations for models and tests, dbt  seeds.  \n\nHands-on experience on DBT Jinja templating, macro development, dbt jobs and snapshot management for Slowly  changing dimensions.  \n\nDevelop python script for data cleaning, transformation and automation of repetitive task.  \nExperienced in loading structured and semi-structured data from AWS S3 to Snowflake by designing file formats,  \nconfiguring storage integration, and automating data loads using Snow pipe.  \nDesigned scalable incremental models for handling large datasets, reducing resource usage\n\nPreferred candidate profile\nCandidate must have 5+ Yrs experience.\nEarly joiner, who can join within a month",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Snowflake', 'DBT', 'Data Engineer', 'Snowflake Sql', 'Snow Flake Schema', 'Data Build Tool', 'Snowflake Db', 'ETL', 'SQL']",2025-06-14 05:11:27
"Walk-In Senior Data Engineer - DataStage, Azure & Power BI",Net Connect,6 - 10 years,5-11 Lacs P.A.,['Hyderabad( Madhapur )'],"Greetings from NCG!\n\nWe have a opening for Snowflake Developer role in Hyderabad office!\nBelow JD for your reference\n\nJob Description:\n\nWe are hiring an experienced Senior Data Engineer with strong expertise in IBM DataStage, , and . The ideal candidate will be responsible for end-to-end data integration, transformation, and reporting solutions that drive business decisions.",,,,"['Azure Data Factory', 'Datastage', 'Etl Datastage']",2025-06-14 05:11:29
Sr. AWS Data Engineer,Agilisium,6 - 10 years,14-24 Lacs P.A.,['Hyderabad'],"Role & responsibilities\nJob Title: Data Engineer\nYears of experience: 6 to 10 years (Minimum 5 years of relevant experience)\nWork Mode: Work From Office Hyderabad\nNotice Period-Immediate to 30 Days only\nKey Skills:\nPython, SQL, AWS, Spark, Databricks - (Mandate)",,,,"['Pyspark', 'AWS', 'Python', 'SQL', 'Amazon Redshift', 'Data Bricks', 'Elastic Search']",2025-06-14 05:11:31
Sr Data Engineer,Algoleap Technologies,10 - 15 years,Not Disclosed,['Hyderabad'],"We are looking for a Senior Data Engineer to lead the design and implementation of scalable data infrastructure and engineering practices. This role will be critical in laying down the architectural foundations for advanced analytics and AI/ML use cases across global business units. Youll work closely with the Data Science Lead, Product Manager, and other cross-functional stakeholders to ensure data systems are robust, secure, and future-ready.\nKey Responsibilities:",,,,"['orchestration', 'Architecture', 'GCP', 'Data modeling', 'Machine learning', 'Data processing', 'Data quality', 'Open source', 'SQL', 'Python']",2025-06-14 05:11:33
Senior Data Engineer - Azure,Blend360 India,6 - 11 years,Not Disclosed,['Hyderabad'],"As a Senior Data Engineer, your role is to spearhead the data engineering teams and elevate the team to the next level! You will be responsible for laying out the architecture of the new project as well as selecting the tech stack associated with it. You will plan out the development cycles deploying AGILE if possible and create the foundations for good data stewardship with our new data products!\nYou will also set up a solid code framework that needs to be built to purpose yet have enough flexibility to adapt to new business use cases tough but rewarding challenge!\n\nResponsibilities\nCollaborate with several stakeholders to deeply understand the needs of data practitioners to deliver at scale\nLead Data Engineers to define, build and maintain Data Platform\nWork on building Data Lake in Azure Fabric processing data from multiple sources\nMigrating existing data store from Azure Synapse to Azure Fabric\nImplement data governance and access control\nDrive development effort End-to-End for on-time delivery of high-quality solutions that conform to requirements, conform to the architectural vision, and comply with all applicable standards.\nPresent technical solutions, capabilities, considerations, and features in business terms. Effectively communicate status, issues, and risks in a precise and timely manner.\nFurther develop critical initiatives, such as Data Discovery, Data Lineage and Data Quality\nLeading team and Mentor junior resources\nHelp your team members grow in their role and achieve their career aspirations\nBuild data systems, pipelines, analytical tools and programs\nConduct complex data analysis and report on results",Industry Type: Industrial Automation,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Access control', 'Data analysis', 'Team leading', 'Architecture', 'Analytical', 'Agile', 'data governance', 'Data processing', 'Mentor', 'Data quality']",2025-06-14 05:11:35
Data Engineer sr Associate,Product base company,8 - 13 years,Not Disclosed,['Hyderabad( Gachibowli )'],"Bachelors degree in computer science, engineering, or a related field. Master’s degree preferred.\nData: 5+ years of experience with data analytics and data warehousing. Sound knowledge of data warehousing concepts.\nSQL: 5+ years of hands-on experience on SQL and query optimization for data pipelines.\nELT/ETL: 5+ years of experience in Informatica/ 3+ years of experience in IICS/IDMC\nMigration Experience: Experience Informatica on prem to IICS/IDMC migration\nCloud: 5+ years’ experience working in AWS cloud environment\nPython: 5+ years of hands-on experience of development with Python\nWorkflow: 4+ years of experience in orchestration and scheduling tools (e.g. Apache Airflow)\nAdvanced Data Processing: Experience using data processing technologies such as Apache Spark or Kafka\nTroubleshooting: Experience with troubleshooting and root cause analysis to determine and remediate potential issues\nCommunication: Excellent communication, problem-solving and organizational and analytical skills\nAble to work independently and to provide leadership to small teams of developers.\nReporting: Experience with data reporting (e.g. MicroStrategy, Tableau, Looker) and data cataloging tools (e.g. Alation)\nExperience in Design and Implementation of ETL solutions with effective design and optimized performance, ETL Development with industry standard recommendations for jobs recovery, fail over, logging, alerting mechanisms. Role & responsibilities\n\n\nPreferred candidate profile",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Airflow', 'Unix', 'IICS', 'Informatica', 'Python', 'Informatica Cloud', 'Data Engineer', 'AWS', 'SQL']",2025-06-14 05:11:37
Senior AWS Data Engineer,Exavalu,7 - 12 years,Not Disclosed,[],"7-12 years of experience using AWS Data Landscape and data Ingestion pipeline.\nAble to understand and explain the data ingestion from different sources like file, database, applications etc\nBuild and enhance the Python, PySpark Based Framework for ingestion.Data engineering experience in AWS Data Services like Glue, EMR, Airflow, CloudWatch, Lambda, Step functions, Event triggers.\nAble to work as a senior engineer with sole interaction point with different business functional teams.\nRequirements\n7 to 12 years of experience in ETL and Data Engineering roles.\nAWS Glue, PySpark, and Amazon Redshift.\nStrong command of SQL and procedural programming in cloud or enterprise databases.\nDeep understanding of data warehousing concepts and data modeling.\nProven ability to deliver efficient, we'll-documented, and scalable data pipelines on AWS.\nFamiliarity with Airflow, AWS Lambda, and other orchestration tools is a plus.\nAWS Certification (eg, AWS Data Analytics Specialty) is an advantage.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data services', 'Data modeling', 'amazon redshift', 'Database', 'Programming', 'Data analytics', 'AWS', 'Data warehousing', 'SQL', 'Python']",2025-06-14 05:11:40
Senior Azure Data Engineer,Cloud Angles Digital Transformation,8 - 12 years,Not Disclosed,['Hyderabad'],"Job Summary:\nWe are seeking a highly skilled Data Engineer with expertise in leveraging Data Lake architecture and the Azure cloud platform to develop, deploy, and optimise data-driven solutions. . You will play a pivotal role in transforming raw data into actionable insights, supporting strategic decision-making across the organisation.\nResponsibilities\nDesign and implement scalable data science solutions using Azure Data Lake, Azure Data Bricks, Azure Data Factory and related Azure services.\nDevelop, train, and deploy machine learning models to address business challenges.\nCollaborate with data engineering teams to optimise data pipelines and ensure seamless data integration within Azure cloud infrastructure.\nConduct exploratory data analysis (EDA) to identify trends, patterns, and insights.\nBuild predictive and prescriptive models to support decision-making processes.\nExpertise in developing end-to-end Machine learning lifecycle utilizing crisp-DM which includes of data collection, cleansing, visualization, preprocessing, model development, model validation and model retraining\nProficient in building and implementing RAG systems that enhance the accuracy and relevance of model outputs by integrating retrieval mechanisms with generative models.\nEnsure data security, compliance, and governance within the Azure cloud ecosystem.\nMonitor and optimise model performance and scalability in production environments.\nPrepare clear and concise documentation for developed models and workflows.\nSkills Required:\nGood experience in using Pyspark, Python, MLops (Optional), ML flow (Optional), Azure Data Lake Storage. Unity Catalog\nWorked and utilized data from various RDBMS like MYSQL, SQL Server, Postgres and NoSQL databases like MongoDB, Cassandra, Redis and graph DB like Neo4j, Grakn.\nProven experience as a Data Engineer with a strong focus on Azure cloud platform and Data Lake architecture.\nProficiency in Python, Pyspark,\nHands-on experience with Azure services such as Azure Data Lake, Azure Synapse Analytics, Azure Machine Learning, Azure Databricks, and Azure Functions.\nStrong knowledge of SQL and experience in querying large datasets from Data Lakes.\nFamiliarity with data engineering tools and frameworks for data ingestion and transformation in Azure.\nExperience with version control systems (e.g., Git) and CI/CD pipelines for machine learning projects.\nExcellent problem-solving skills and the ability to work collaboratively in a team environment.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Data Factory', 'Azure Data Engineering', 'Azure Databricks', 'Pyspark', 'Azure Data Lake', 'Python']",2025-06-14 05:11:42
Senior Data Engineer,Ntrix Innovations,6 - 9 years,Not Disclosed,"['Hyderabad', 'Kondapur']","Manadatory Skills:\n• Airflow,\n• python,\n• AWS and\n• Big Data technologies like Spark",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Airflow', 'Spark', 'AWS', 'Python']",2025-06-14 05:11:44
Senior Data Engineer,Avalara Technologies,6 - 11 years,Not Disclosed,['Pune'],"What You'll Do\n\nThe Global Analytics & Insights (GAI) team is looking for a Senior Data Engineer to lead our build of the data infrastructure for Avalara's core data assets- empowering us with accurate, data to lead data backed decisions. As A Senior Data Engineer, you will help architect, implement, and maintain our data infrastructure using Snowflake, dbt (Data Build Tool), Python, Terraform, and Airflow. You will immerse yourself in our financial, marketing, and sales data to become an expert of Avalara's domain. You will have deep SQL experience, an understanding of modern data stacks and technology, a desire to build things the right way using modern software principles, and experience with data and all things data related.\n\nWhat Your Responsibilities Will Be\n\nWhat You'll Need to be Successful",,,,"['Data Engineering', 'salesforce', 'continuous integration', 'snowflake', 'advance sql', 'python', 'git', 'infrastructure', 'ci/cd', 'terraform', 'sql']",2025-06-14 05:11:46
Senior Data Engineer - Azure,Blend360 India,5 - 11 years,Not Disclosed,['Hyderabad'],"As a Senior Data Engineer, your role is to spearhead the data engineering teams and elevate the team to the next level! You will be responsible for laying out the architecture of the new project as well as selecting the tech stack associated with it. You will plan out the development cycles deploying AGILE if possible and create the foundations for good data stewardship with our new data products!\nYou will also set up a solid code framework that needs to be built to purpose yet have enough flexibility to adapt to new business use cases tough but rewarding challenge!\n\nResponsibilities\nCollaborate with several stakeholders to deeply understand the needs of data practitioners to deliver at scale\nLead Data Engineers to define, build and maintain Data Platform\nWork on building Data Lake in Azure Fabric processing data from multiple sources\nMigrating existing data store from Azure Synapse to Azure Fabric\nImplement data governance and access control\nDrive development effort End-to-End for on-time delivery of high-quality solutions that conform to requirements, conform to the architectural vision, and comply with all applicable standards.\nPresent technical solutions, capabilities, considerations, and features in business terms. Effectively communicate status, issues, and risks in a precise and timely manner.\nFurther develop critical initiatives, such as Data Discovery, Data Lineage and Data Quality\nLeading team and Mentor junior resources\nHelp your team members grow in their role and achieve their career aspirations\nBuild data systems, pipelines, analytical tools and programs\nConduct complex data analysis and report on results\n\n\n5+ Years of Experience as a data engineer or similar role in Azure Synapses, ADF or\nrelevant exp in Azure Fabric\nDegree in Computer Science, Data Science, Mathematics, IT, or similar field\nMust have experience e",Industry Type: Industrial Automation,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Data analysis', 'Data modeling', 'Analytical', 'Agile', 'data governance', 'Data quality', 'Data mining', 'SQL', 'Python']",2025-06-14 05:11:48
Senior Databricks Data Engineer,"Creative Capsule, LLC",5 - 10 years,Not Disclosed,['Panaji'],"This position will primarily be responsible for designing, developing, and maintaining robust ETL/ELT pipelines for data ingestion, transformation, and storage. The role also involves designing and developing scalable data solutions.\nYou will work with a team responsible for ensuring the availability, reliability, and performance of data systems and requires a good understanding of infrastructure management, cost optimization, and performance tuning in Databricks environments. The candidate will design, develop, and maintain scalable data pipelines of Databricks on cloud platforms (Databricks, Azure, AWS, or GCP).\nThe person will also work with a collaborative team responsible for driving client performance by combining data-driven insights and strategic thinking to solve business challenges. The candidate should have strong organizational, critical thinking, and communication skills to interact effectively with stakeholders.\nResponsibilities:\nDesign, develop, and manage end-to-end data pipelines on Databricks using Spark, Delta Lake, and related technologies\nImplement and optimize ETL/ELT workflows for ingesting, transforming, and storing large volumes of structured and semi-structured data\nManage and monitor Databricks infrastructure, including cluster configuration, auto-scaling, job execution, and resource utilization\nIdentify and implement cost-saving strategies across Databricks workspaces through efficient pipeline design, job scheduling, and infrastructure tuning\nOrchestrate workflows using tools like Apache Airflow, Azure Data Factory, or similar orchestration frameworks\nCollaborate with data architects and platform engineers to ensure efficient data architecture and performance tuning\nMonitor data quality, integrity, and lineage across all pipelines and proactively troubleshoot data issues\nDevelop, maintain, and optimize data models and storage layers that support BI/reporting and analytics use cases\nPartner with business stakeholders to translate business needs into technical specifications and scalable data solutions\nMaintain comprehensive documentation of pipeline architecture, configurations, and operational best practices\nTechnical Qualifications:\nExperience in infrastructure management on Databricks: cluster setup, scaling, security roles, workspace configuration\nExperience in cost optimization techniques in Databricks (e.g., job cluster vs. all-purpose cluster usage, cost/performance tradeoffs)\nExperience with orchestration tools such as Apache Airflow, Azure Data Factory, or AWS Glue Workflows\nExperience with relational (e.g., PostgreSQL, SQL Server) and NoSQL (e.g., MongoDB) databases\nStrong proficiency in Apache Spark and Delta Lake within the Databricks ecosystem\nProficient in Python and SQL for data processing and pipeline development\nFamiliarity with data warehousing, data lakes, and distributed data processing frameworks\nUnderstanding of BI tools like Power BI, Tableau, or Looker is a plus\nPersonal Skills:\nStrong analytical skills: ability to read business requirements, analyze problems, and propose solutions\nAbility to identify alternatives and find optimal solutions\nAbility to follow through and ensure logical implementation\nQuick learner with the ability to adapt to new concepts and software\nAbility to work effectively in a team environment\nStrong time management skills, capable of handling multiple tasks and competing deadlines\nEffective written and verbal communication skills\nEducation and Work Experience:\nBackground in Computer Science, Information Technology, Data Science, or a related field preferred\nMinimum 5 years of experience in Data Engineering with at least 2 years of hands-on experience with Databricks (Azure, AWS, or GCP)\nCertification in Databricks, Azure Data Engineering, or any related data technology is an added advantage",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Performance tuning', 'Infrastructure management', 'Postgresql', 'Scheduling', 'Data quality', 'Information technology', 'Analytics', 'SQL', 'Python']",2025-06-14 05:11:50
Senior Data Engineer with Dot Net,Swits Digital,4 - 7 years,Not Disclosed,['Gurugram'],"Job Title: Senior Data Engineer with .NET\n\nExperience: 6+ Years\n\nLocation: Gurgaon, India\n\n\n\nRequired Skills:\n\n\n\n\n5+ years of experience in distributed computing (Spark) and software development\n\n\n\n3+ years of hands-on experience in Spark-Scala\n\n\n\n5+ years of experience in Data Engineering\n\n\n\n5+ years of experience in Python\n\n\n\n5+ years of experience in .NET Core\n\n\n\nExperience with Dapper, SQL, XUnit, NUnit, and RabbitMQ\n\n\n\nProficiency in working with databases (preferably Postgres)\n\n\n\nSolid understanding of Object-Oriented Programming principles\n\n\n\nExperience in Agile development environments (Scrum/Kanban)\n\n\n\nExperience with version control tools (preferably Git)\n\n\n\nCI/CD pipeline experience\n\n\n\nStrong exposure to automated testing including Integration/Delta, Load, and Performance testing\n\n\n\n\nGood to Have Skills:\n\n\n\n\nExperience with Docker and containerized deployments\n\n\n\nExperience with Kubernetes\n\n\n\nFamiliarity with Airflow\n\n\n\nExperience working in cloud environments (GCP and Azure)\n\n\n\nExposure to TeamCity CI and Octopus Deploy",Industry Type: Recruitment / Staffing,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Automation testing', 'GIT', 'Version control', 'spark', 'Agile development', 'Performance testing', 'Deployment', 'Object oriented programming', 'SQL', 'Python']",2025-06-14 05:11:52
Senior .NET Backend Developer - Azure Data Engineering Experience,Suzva Software Technologies,5 - 10 years,Not Disclosed,['Hyderabad'],"We Are Hiring: Senior .NET Backend Developer with Azure Data Engineering Experience\nJob Location: Hyderabad, India\nWork Mode: Onsite Only\nExperience: Minimum 6+ Years\nQualification: B.Tech, B.E, MCA, M.Tech\n\nRole Overview\nWe are seeking an experienced .NET Backend Developer with strong Azure Data Engineering skills to join our growing team in Hyderabad. You will work closely with cross-functional teams to build scalable backend systems, modern APIs, and data pipelines using cutting-edge tools like Azure Databricks and MS Fabric.\n\nTechnical Skills (Must-Have)\nStrong hands-on experience in C, SQL Server, and OOP Concepts\n\nProficiency with .NET Core, ASP.NET Core, Web API, Entity Framework (v6 or above)\n\nStrong understanding of Microservices Architecture\n\nExperience with Azure Cloud technologies including Data Engineering, Azure Databricks, MS Fabric, Azure SQL, Blob Storage, etc.\n\nExperience with Snowflake or similar cloud data platforms\n\nExperience working with NoSQL databases\n\nSkilled in Database Performance Tuning and Design Patterns\n\nWorking knowledge of Agile methodologies\n\nAbility to write reusable libraries and modular, maintainable code\n\nExcellent verbal and written communication skills (especially with US counterparts)\n\nStrong troubleshooting and debugging skills\n\nNice to Have Skills\nExperience with Angular, MongoDB, NPM\n\nFamiliarity with Azure DevOps CI/CD pipelines for build and release configuration\n\nSelf-starter attitude with strong analytical and problem-solving abilities\n\nWillingness to work extra hours when needed to meet tight deadlines\n\nWhy Join Us\nWork with a passionate, high-performing team\n\nOpportunity to grow your technical and leadership skills in a dynamic environment\n\nBe part of global digital transformation initiatives with top-tier clients\n\nExposure to real-world enterprise data systems\n\nOpportunity to work on cutting-edge Azure and cloud technologies\n\nPerformance-based growth & internal mobility opportunities\n\nTags\nDotNetDeveloper BackendDeveloper AzureDataEngineering Databricks MSFabric Snowflake Microservices CSharpJobs HyderabadJobs FullTimeJob HiringNow EntityFramework ASPNetCore CloudEngineering SQLJobs DevOps DotNetCore BackendJobs SuzvaCareers DataPlatformDeveloper SoftwareJobsIndia",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['.Net', 'Web API', 'C', 'Data Engineering', 'Azure Databricks', 'MS Fabric', 'SQL Server', 'Azure SQL', 'Backend', 'Azure Data Engineering', 'ASP.NET Core', 'Blob Storage', 'Entity Framework', 'OOP Concepts', '.NET Core']",2025-06-14 05:11:55
Data Engineer I,Swiss Re,1 - 3 years,Not Disclosed,['Bengaluru'],"About the Role:\nAs a Data Engineer, you will be responsible for implementing data pipelines and analytics\nsolutions to support key decision-making processes in our Life Health Reinsurance business. You will become part of a project that is leveraging cutting edge technology that applies Big Data and Machine Learning to solve new and emerging problems for Swiss Re. You will be expected to gain a full understanding of the reinsurance data and business logic required to deliver analytics solutions.\nKey responsibilities include:\nWork closely with Product Owners and Engineering Leads to understand requirements and evaluate the implementation effort.\nDevelop and maintain scalable data transformation pipelines\nImplement analytics models and visualizations to provide actionable data insights\nCollaborate within a global development team to design and deliver solutions.\nAbout the Team:\nLife Health Data Analytics Engineering is a key tech partner for our Life Health Reinsurance division, supporting in the transformation of the data landscape and the creation of innovative analytical products and capabilities. A large globally distributed team working in an agile development landscape, we deliver solutions to make better use of our reinsurance data and enhance our ability to make data-driven decisions across the business value chain.\nAbout You:\nAre you eager to disrupt the industry with us and make an impactDo you wish to have your talent recognized and rewardedThen join our growing team and become part of the next wave of data\ninnovation. Key qualifications include:\nBachelors degree level or equivalent in Computer Science, Data Science or similar discipline\nAt least 1-3 years of experience working with large scale software systems\nProficient in Python/PySpark\nProficient in SQL (Spark SQL preferred)\nPalantir Foundry experience is a strong plus.\nExperience working with large data sets on enterprise data platforms and distributed computing (Spark/Hive/Hadoop preferred)\nExperience with JavaScript/HTML/CSS a plus\nExperience working in a Cloud environment such as AWS or Azure is a plus\nStrong analytical and problem-solving skills\nEnthusiasm to work in a global and multicultural environment of internal and external professionals\nStrong interpersonal and communication skills, demonstrating a clear and articulate standard of written and verbal communication in complex environments\nAbout Swiss Re\n.\n\n\n\nIf you are an experienced professional returning to the workforce after a career break, we encourage you to apply for open positions that match your skills and experience.\nKeywords:\nReference Code: 134085",Industry Type: Insurance,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Reinsurance', 'data science', 'spark', 'Analytical', 'Machine learning', 'Javascript', 'HTML', 'SQL', 'Python']",2025-06-14 05:11:57
Jr Data Engineer,BMW Techworks India,4 - 6 years,Not Disclosed,"['Chennai', 'Bengaluru']","What awaits you/ Job Profile\nAnalyze, Organize data and Build data systems with multiple data sources\nWorking closely with business and market team for the Data analysis.\nWhat should you bring along\nBuild data systems and pipelines\nStandardize the data assets which maximize data reusability\nEvaluate business needs and objectives\nImplementing data pipeline to ingest and cleanse raw data from various source systems including SaaS, Cloud based and On-prep databases\nCombine raw information from different sources\nExplore ways to enhance data quality and reliability\nImplementing functional requirements including incremental loads, data snapshots and identity resolution\nOptimizing data pipelines to improve performance and cost, while ensuring a high quality of data within the data lake:\nMonitoring services and jobs for cost and performance, ensuring continual operations of data pipelines, and fixing of defects.\nConstantly looking for opportunities to optimize data pipelines to improve performance.\nMust have technical skill\nMust have coding skills in Spark/Pyspark, Python and SQL\nMust have Knowledge of AWS tools, Glue, Athena ,step functions and S3\nGood to have technical skills\nGood knowledge of CI/CD (Github / Github Actions, Terraform)\nKnowledge on Agile methodologies.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data analysis', 'github', 'Coding', 'Agile', 'Data quality', 'Cost', 'AWS', 'Monitoring', 'SQL', 'Python']",2025-06-14 05:11:59
"Data Engineer, Product Analytics",Meta,2 - 7 years,Not Disclosed,['Bengaluru'],"Apply to this job\nAs a Data Engineer at Meta, you will shape the future of people-facing and business-facing products we build across our entire family of applications (Facebook, Instagram, Messenger, WhatsApp, Reality Labs, Threads). Your technical skills and analytical mindset will be utilized designing and building some of the worlds most extensive data sets, helping to craft experiences for billions of people and hundreds of millions of businesses worldwide.In this role, you will collaborate with software engineering, data science, and product management teams to design/build scalable data solutions across Meta to optimize growth, strategy, and user experience for our 3 billion plus users, as well as our internal employee community.You will be at the forefront of identifying and solving some of the most interesting data challenges at a scale few companies can match. By joining Meta, you will become part of a world-class data engineering community dedicated to skill development and career growth in data engineering and beyond.Data Engineering: You will guide teams by building optimal data artifacts (including datasets and visualizations) to address key questions. You will refine our systems, design logging solutions, and create scalable data models. Ensuring data security and quality, and with a focus on efficiency, you will suggest architecture and development approaches and data management standards to address complex analytical problems.Product leadership: You will use data to shape product development, identify new opportunities, and tackle upcoming challenges. Youll ensure our products add value for users and businesses, by prioritizing projects, and driving innovative solutions to respond to challenges or opportunities.Communication and influence: You wont simply present data, but tell data-driven stories. You will convince and influence your partners using clear insights and recommendations. You will build credibility through structure and clarity, and be a trusted strategic partner.\nData Engineer, Product Analytics Responsibilities\nCollaborate with engineers, product managers, and data scientists to understand data needs, representing key data insights in a meaningful way\nDesign, build, and launch collections of sophisticated data models and visualizations that support multiple use cases across different products or domains\nDefine and manage Service Level Agreements for all data sets in allocated areas of ownership\nSolve challenging data integration problems, utilizing optimal Extract, Transform, Load (ETL) patterns, frameworks, query techniques, sourcing from structured and unstructured data sources\nImprove logging\nAssist in owning existing processes running in production, optimizing complex code through advanced algorithmic concepts\nOptimize pipelines, dashboards, frameworks, and systems to facilitate easier development of data artifacts\nInfluence product and cross-functional teams to identify data opportunities to drive impact\nMinimum Qualifications\nBachelors degree in Computer Science, Computer Engineering, relevant technical field, or equivalent\n2+ years of experience where the primary responsibility involves working with data. This could include roles such as data analyst, data scientist, data engineer, or similar positions\n2+ years of experience with SQL, ETL, data modeling, and at least one programming language (e.g., Python, C++, C#, Scala or others.)\nPreferred Qualifications\nMasters or Ph.D degree in a STEM field\nAbout Meta\n.\n\n\nEqual Employment Opportunity\n.\n\nMeta is committed to providing reasonable accommodations for qualified individuals with disabilities and disabled veterans in our job application procedures. If you need assistance or an accommodation due to a disability, fill out the Accommodations request form .",Industry Type: Internet,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Product management', 'Computer science', 'C++', 'Data management', 'Data modeling', 'data security', 'Analytical', 'Analytics', 'SQL', 'Python']",2025-06-14 05:12:01
AWS Data Engineer,Redserv Global Solutions,3 - 6 years,10-15 Lacs P.A.,[],"Roles and Responsibilities:\n\nWe are looking for an experienced AWS Cloud Data Engineer to join our Data Science & Analytics team to build, optimize, and maintain cloud-based data solutions. The ideal candidate will possess strong technical knowledge in data engineering on AWS, expertise in data integration, pipeline creation, performance optimization, and a strong understanding of DevOps methodologies.",,,,"['Data Engineer', 'AWS', 'Python']",2025-06-14 05:12:03
Azure Data Engineer-ADF,Ltimindtree,6 - 11 years,Not Disclosed,"['Hyderabad', 'Pune', 'Bengaluru']","Hello,\nGreetings from LTIMindtree!\nWe are Hiring for Azure Data Engineer-ADF (PAN India)!\n\nJob Description\nNotice Period:- 0 to 30 Days only\nExperience:- 5 to 12 Years\nInterview Mode:- 2 rounds (One round is F2F)\nHybrid (2-3 WFO)\nBrief Description of Role\nJob Summary:\nDetailed JD\nAzure Data Factory ADF\nExtensive experience in designing developing and maintaining data pipelines using Azure Data Factory\nProficiency in creating and managing data flows activities and triggers in ADF\nAzure Cloud Services\nStrong knowledge of Azure services such as Azure Data Lake Storage ADLS Azure Synapse Analytics and Azure SQL Database\nExperience with Azure DevOps for CICD pipeline implementation\nData Integration and ETL\nExpertise in data integration and ETL processes including data ingestion transformation and loading\nFamiliarity with various data sources and sinks including SQL and NoSQL databases",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Data Factory', 'Datafactory', 'ADF']",2025-06-14 05:12:05
AWS Data Engineer,Redserv Global Solutions,3 - 6 years,10-15 Lacs P.A.,[],"Roles and Responsibilities:\n\nWe are looking for an experienced AWS Cloud Data Engineer to join our Data Science & Analytics team to build, optimize, and maintain cloud-based data solutions. The ideal candidate will possess strong technical knowledge in data engineering on AWS, expertise in data integration, pipeline creation, performance optimization, and a strong understanding of DevOps methodologies.",,,,"['Data Engineer', 'AWS', 'Python']",2025-06-14 05:12:08
AWS Data Engineer,Redserv Global Solutions,3 - 6 years,10-15 Lacs P.A.,[],"Roles and Responsibilities:\n\nWe are looking for an experienced AWS Cloud Data Engineer to join our Data Science & Analytics team to build, optimize, and maintain cloud-based data solutions. The ideal candidate will possess strong technical knowledge in data engineering on AWS, expertise in data integration, pipeline creation, performance optimization, and a strong understanding of DevOps methodologies.",,,,"['Data Engineer', 'AWS', 'Python']",2025-06-14 05:12:10
PySpark Data Engineer (Only Immediate Joiner),Adecco,5 - 9 years,12-22 Lacs P.A.,"['Hyderabad', 'Bengaluru']","Position : PySpark Data Engineer\nLocation : Bangalore / Hyderabad\nExperience : 5 to 9 Yrs\nJob Type : On Role\n\nJob Description: PySpark Data Engineer:-\n1. API Development: Design, develop, and maintain robust APIs using FastAPI and RESTful principles for scalable backend systems.\n2. Big Data Processing: Leverage PySpark to process and analyze large datasets efficiently, ensuring optimal performance in big data environments.\n3. Full-Stack Integration: Develop seamless backend-to-frontend feature integrations, collaborating with front-end developers for cohesive user experiences.\n4. CI/CD Pipelines: Implement and manage CI/CD pipelines using GitHub Actions and Azure DevOps to streamline deployments and ensure system reliability.\n5. Containerization: Utilize Docker for building and deploying containerized applications in development and production environments.\n6. Team Leadership: Lead and mentor a team of developers, providing guidance, code reviews, and support to junior team members to ensure high-quality deliverables.\n7. Code Optimization: Write clean, maintainable, and efficient Python code, with a focus on scalability, reusability, and performance.\n8. Cloud Deployment: Deploy and manage applications on cloud platforms like Azure, ensuring high availability and fault tolerance.\n9. Collaboration: Work closely with cross-functional teams, including product managers and designers, to translate business requirements into technical solutions.\n10. Documentation: Maintain thorough documentation for APIs, processes, and systems to ensure transparency and ease of maintenance.\nHighlighted Skillset:-\nBig Data: Strong PySpark skills for processing large datasets.\nDevOps: Proficiency in GitHub Actions, CI/CD pipelines, Azure DevOps, and Docker.\nIntegration: Experience in backend-to-frontend feature connectivity.\nLeadership: Proven ability to lead and mentor development teams.\nCloud: Knowledge of deploying and managing applications in Azure or other cloud environments.\nTeam Collaboration: Strong interpersonal and communication skills for working in cross-functional teams.\nBest Practices: Emphasis on clean code, performance optimization, and robust documentation.\n\nInterested candidates kindly share your CV and below details to usha.sundar@adecco.com\n1) Present CTC (Fixed + VP) -\n2) Expected CTC -\n3) No. of years experience -\n4) Notice Period -\n5) Offer-in hand -\n6) Reason of Change -\n7) Present Location -",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Pyspark', 'Cicd Methodology', 'FastAPI', 'API', 'Python', 'Cloud Deployment', 'Data Engineering', 'Docker', 'Big Data', 'Github Actions', 'Performance Tuning', 'Azure Devops']",2025-06-14 05:12:12
PySpark Azure Data Engineer,Our Client is American multinational inf...,5 - 10 years,16-25 Lacs P.A.,"['Hyderabad', 'Bengaluru']","PySpark Data Engineer:-\n\nJob Description:\n\n1. API Development: Design, develop, and maintain robust APIs using FastAPI and RESTful principles for scalable backend systems.\n2. Big Data Processing: Leverage PySpark to process and analyze large datasets efficiently, ensuring optimal performance in big data environments.\n3. Full-Stack Integration: Develop seamless backend-to-frontend feature integrations, collaborating with front-end developers for cohesive user experiences.\n4. CI/CD Pipelines: Implement and manage CI/CD pipelines using GitHub Actions and Azure DevOps to streamline deployments and ensure system reliability.\n5. Containerization: Utilize Docker for building and deploying containerized applications in development and production environments.\n6. Team Leadership: Lead and mentor a team of developers, providing guidance, code reviews, and support to junior team members to ensure high-quality deliverables.\n7. Code Optimization: Write clean, maintainable, and efficient Python code, with a focus on scalability, reusability, and performance.\n8. Cloud Deployment: Deploy and manage applications on cloud platforms like Azure, ensuring high availability and fault tolerance.\n9. Collaboration: Work closely with cross-functional teams, including product managers and designers, to translate business requirements into technical solutions.\n10. Documentation: Maintain thorough documentation for APIs, processes, and systems to ensure transparency and ease of maintenance\n\n\nHighlighted Skillset:-\nBig Data: Strong PySpark skills for processing large datasets.\nDevOps: Proficiency in GitHub Actions, CI/CD pipelines, Azure DevOps, and Docker.\nIntegration: Experience in backend-to-frontend feature connectivity.\nLeadership: Proven ability to lead and mentor development teams.\nCloud: Knowledge of deploying and managing applications in Azure or other cloud environments.\nTeam Collaboration: Strong interpersonal and communication skills for working in cross-functional teams.\nBest Practices: Emphasis on clean code, performance optimization, and robust documentation\n\n\nShare updated resume at siddhi.pandey@adecco.com or whatsapp at 6366783349",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'CI/CD pipelines', 'GitHub Actions', 'Docker', 'Azure Devops', 'FastAPI', 'RESTful principles']",2025-06-14 05:12:14
Tech. PM - Data Engineering-Data Analytics@ Gurgaon/Blore_Urgent,A global leader in delivering innovative...,5 - 10 years,Not Disclosed,"['Gurugram', 'Bengaluru']","Job Title - Technical Project Manager\n\nLocation - Gurgaon/ Bangalore\n\nNature of Job - Permanent\n\nDepartment - data analytics\n\nWhat you will be doing\n\n\nDemonstrated client servicing and business analytics skills with at least 5 - 9 years of experience as data engineer, BI developer, data analyst, technical project manager, program manager etc.\nTechnical project management- drive BRD, project scope, resource allocation, team\ncoordination, stakeholder communication, UAT, Prod fix, change requests, project governance\nSound knowledge of banking industry (payments, retail operations, fraud etc.)\nStrong ETL experience or experienced Teradata developer\nManaging team of business analysts, BI developers, ETL developers to ensure that projects are completed on time\nResponsible for providing thought leadership and technical advice on business issues\nDesign methodological frameworks and solutions.\n\n\nWhat were looking for\n\n\nBachelors/masters degree in computer science/data science/AI/statistics, Certification in Gen AI. Masters degree Preferred.\nManage multiple projects, at a time, from inception to delivery\nSuperior problem-solving, analytical, and quantitative skills\nEntrepreneurial mindset, coupled with a “can do” attitude\nDemonstrated ability to collaborate with cross-functional, cross-border teams and coach / mentor colleagues.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Technical Project Manager', 'Data Engineering', 'multiple projects', 'Technical project management', 'Data Analytics', 'project scope', 'ETL Pipeline', 'team coordination', 'resource allocation', 'Prod fix', 'drive BRD', 'program manager', 'Big data']",2025-06-14 05:12:16
Azure Data Engineer,Arcadis,3 - 8 years,Not Disclosed,['Bengaluru'],"ARCADISis looking for Azure Data Engineer with a passion to drive and execute Digital to the core of everything we do. We firmly believe in Everything Digital, Digital Everything. We are transforming, we are reimagining the industry and we are reimagining how communities and nations can help becoming more sustainable places to live for today and future generations.\nTechnology is the core and integral part of what we do, all the way for empowering Arcadians to harnessing power of data and AI/ML for sensors, IIOT and Advanced Drones, the technology teams are Dreaming Big and Delivering on future. As part of our Technology drive, we are looking for on-board talented and passionate Azure data engineers across multiple locations in North America.\nRole accountabilities:\nPossess excellent design and coding skills and a zeal for owning the complete SDLC of building applications in a DevOps environment\nYou are excited about working with Azure Data Platform\nchallenges while building the next wave of software engineering solutions\nCollaborating with and across Agile teams to design, develop, test, implement, and support technical solutions in Microsoft Azure Data Platform\nLeading the craftsmanship, security, availability, resilience, and scalability of your solutions\nVery strong on database concepts, data modelling, stored procedures, complex query writing, performance optimization of SQL queries.\nStrong experience in\nT-SQL, SSIS, SSAS, SSRS\nAzure Data Factory\nAzure Data Lake Store\nAzure Data Lake Analytics (Good to have, not mandatory)\nAzure SQL DB\nAzure SQL DW\nAzure Analysis Services, DAX\nAzure Data Bricks with Python/Scala\nExperience in building end to end solution using Azure data analytics platform.\nExperience in building generic framework solution which can be reused for upcoming similar use cases.\nExperience in building Azure data analytics solutions with DevOps (CI/CD) approach.\nExperience in using TFS, Azure Repos.\nMentor peers to gain expertise on Azure data platform solutions skills.\nExperience in developing, maintaining, publishing, and supporting dashboards using Power BI.\nStrong experience in publishing dashboards to Power BI service, using Power BI gateways, Power BI Report Server & Power BI Embedded\nQualifications & Experience:\nBasic Qualifications:\nBachelor in Engineering/Math/Statistics/Econometrics or related discipline\nShould have 3-8 years of experience in MSBI with relevant hands-on experience in Azure Data Platform (must) for a minimum of 3 years.\nPreferred Qualifications:\nMasters or Minor in Computer Science\n3+ years of experience developing Data Engineering solutions\nArchitecture, design experience with good knowledge of data model design & their implementation.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure', 'T-SQL', 'Azure Data Factory', 'SSAS', 'SSRS', 'Azure SQL DB', 'SSIS', 'Azure SQL DW']",2025-06-14 05:12:18
Snowflake Data Engineer,Tredence,6 - 10 years,Not Disclosed,"['Gurugram', 'Chennai', 'Bengaluru']",Role & responsibilities\n\nCollaborate with DW/BI leads to understand new ETL pipeline development requirements\nTriage issues to find gaps in existing pipelines and fix the issues\nWork with business to understand the need in reporting layer and develop data model to fulfill reporting needs\nHelp joiner team members to resolve issues and technical challenges.\nDrive technical discussion with client architect and team members,,,,"['Snowflake', 'Snowpipe', 'AWS', 'DBT']",2025-06-14 05:12:20
PySpark Azure Data Engineer,American multinational information techn...,5 - 10 years,16-25 Lacs P.A.,"['Hyderabad', 'Bengaluru']","Urgent Hiring for PySpark Data Engineer:-\n\nJob Location- Bangalore and Hyderabad\n\nExp- 5yrs-9yrs\n\nShare CV Mohini.sharma@adecco.com OR Call 9740521948\n\n\nJob Description:\n\n1. API Development: Design, develop, and maintain robust APIs using FastAPI and RESTful principles for scalable backend systems.\n2. Big Data Processing: Leverage PySpark to process and analyze large datasets efficiently, ensuring optimal performance in big data environments.\n3. Full-Stack Integration: Develop seamless backend-to-frontend feature integrations, collaborating with front-end developers for cohesive user experiences.\n4. CI/CD Pipelines: Implement and manage CI/CD pipelines using GitHub Actions and Azure DevOps to streamline deployments and ensure system reliability.\n5. Containerization: Utilize Docker for building and deploying containerized applications in development and production environments.\n6. Team Leadership: Lead and mentor a team of developers, providing guidance, code reviews, and support to junior team members to ensure high-quality deliverables.\n7. Code Optimization: Write clean, maintainable, and efficient Python code, with a focus on scalability, reusability, and performance.\n8. Cloud Deployment: Deploy and manage applications on cloud platforms like Azure, ensuring high availability and fault tolerance.\n9. Collaboration: Work closely with cross-functional teams, including product managers and designers, to translate business requirements into technical solutions.\n10. Documentation: Maintain thorough documentation for APIs, processes, and systems to ensure transparency and ease of maintenance\n\n\nHighlighted Skillset:-\nBig Data: Strong PySpark skills for processing large datasets.\nDevOps: Proficiency in GitHub Actions, CI/CD pipelines, Azure DevOps, and Docker.\nIntegration: Experience in backend-to-frontend feature connectivity.\nLeadership: Proven ability to lead and mentor development teams.\nCloud: Knowledge of deploying and managing applications in Azure or other cloud environments.\nTeam Collaboration: Strong interpersonal and communication skills for working in cross-functional teams.\nBest Practices: Emphasis on clean code, performance optimization, and robust documentation",Industry Type: Financial Services (Asset Management),Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'CI/CD pipelines', 'GitHub Actions', 'Docker', 'Azure Devops', 'FastAPI', 'RESTful principles']",2025-06-14 05:12:22
Data Consultant-GCP Data Engineer,Kyndryl,10 - 15 years,Not Disclosed,['Bengaluru'],"Who We Are\nAt Kyndryl, we design, build, manage and modernize the mission-critical technology systems that the world depends on every day. So why work at Kyndryl? We are always moving forward – always pushing ourselves to go further in our efforts to build a more equitable, inclusive world for our employees, our customers and our communities.\n\nThe Role\nAt Kyndryl, we design, build, manage and modernize the mission-critical technology systems that the world depends on every day. So why work at Kyndryl? We are always moving forward – always pushing ourselves to go further in our efforts to build a more equitable, inclusive world for our employees, our customers and our communities.",,,,"['kubernetes', 'components', 'analytical', 'data', 'metadata management', 'pyspark', 'master data management', 'sql', 'docker', 'cloud', 'gcp', 'looker', 'bigquery', 'etl', 'programming', 'snowflake', 'python', 'data analytics', 'airflow', 'machine learning', 'data engineering', 'dataproc', 'nosql', 'data bricks', 'data quality', 'mdm', 'data flow']",2025-06-14 05:12:25
Data Engineer - GCP,Happiest Minds Technologies,5 - 10 years,8-18 Lacs P.A.,"['Pune', 'Bengaluru']","Key Responsibilities:\n• Data Pipeline Development: Designing, building, and maintaining robust data pipelines to move data from various sources (e.g., databases, external APIs, logs) to centralized data systems, such as data lakes or warehouses.\n• Data Integration: Integrating data from multiple sources and ensuring it's processed in a consistent, usable format. This involves transforming, cleaning, and validating data to meet the needs of products, analysts and data scientists.\n• Database Management: Creating, managing, and optimizing databases for storing large amounts of structured and unstructured data. Ensuring high availability, scalability, and security of data storage solutions.\nIdentifying and resolving issues related to the speed and efficiency of data systems. This could include optimizing queries, storage systems, and improving overall system architecture.\n• : Automating routine tasks, such as data extraction, transformation, and loading (ETL), to ensure smooth data flows with minimal manual intervention.\n• : Working closely with Work closely with product managers, UX/UI designers, and other stakeholders to understand data requirements and ensure data is in the right format for analysis and modeling.\n• : Ensuring data integrity and compliance with data governance policies, including data quality standards, privacy regulations (e.g., GDPR), and security protocols.\n: Continuously monitoring data pipelines and databases for any disruptions or errors and troubleshooting any issues that arise to ensure continuous data flow.\n• : Staying up to date with emerging data tools, technologies, and best practices in order to improve data systems and infrastructure.\n• : Documenting data systems, pipeline processes, and data architectures, providing clear instructions for the team to follow, and ensuring that the architecture is understandable for stakeholders.",,,,"['Data Engineering', 'gcp', 'Python', 'SQL', 'Pyspark', 'Bigquery', 'Google Cloud Platforms']",2025-06-14 05:12:28
Expert Cloud Data Engineer - (AWS/Python/Pyspark/Kafka/SQL),BMW Techworks India,9 - 13 years,15-27.5 Lacs P.A.,"['Chennai', 'Bengaluru']","Job Description\nRole Expert Cloud Data Engineer\nLocation Pune/Chennai\nExperience: 9 to 12 years and above.\nWhat awaits you/ Job Profile\nYou will lead strategic data engineering initiatives, ensuring that cloud data solutions align with business objectives. You will oversee large-scale projects, mentor teams, and drive innovation in cloud-based data engineering.\nKey Responsibilities:\nArchitect and implement enterprise-level data solutions that are scalable and secure.\nLead cross-functional teams to execute large-scale data projects.\nDevelop data governance strategies and lifecycle management best practices.\nExplore and integrate cutting-edge cloud and AI technologies to enhance data capabilities.\nMentor junior and senior engineers, fostering a culture of technical excellence.\nConduct high-level data analysis and make strategic recommendations.\nDrive cost optimization and performance tuning across cloud-based data platforms.\nWhat should you bring along\n8+ years of experience in data engineering, AWS Cloud Architecture, and DevOps.\nStrong understanding of data security, compliance, and governance in cloud environments.\nAbility to evaluate and implement emerging technologies in data engineering.\nExcellent knowledge of Terraform and GitHub Actions.\nExperienced in being in the lead of a feature team.\nMust have technical skill\nAdvanced Python, PySpark, SQL, Java/Scala (preferred)\nAWS (advanced expertise in Glue, Redshift, Athena, Kinesis, EMR), Cloud Architect Certification\nKafka, Flink, Spark Streaming\nGood to have technical skills\nTerraform, AWS CloudFormation\nGitHub Actions, Jenkins, Kubernetes\nIntegration of ML models into cloud data pipelines\nData Governance & Security: Role-based access control (RBAC), encryption, compliance frameworks",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'Terraform', 'Kafka', 'AWS', 'Python']",2025-06-14 05:12:30
Azure Data Engineer,JRD Systems,7 - 12 years,Not Disclosed,"['Hyderabad', 'Bengaluru']","Cloud Data Engineer\n\nThe Cloud Data Engineer will be responsible for developing the data lake platform and all applications on Azure cloud. Proficiency in data engineering, data modeling, SQL, and Python programming is essential. The Data Engineer will provide design and development solutions for applications in the cloud.\nEssential Job Functions:\nUnderstand requirements and collaborate with the team to design and deliver projects.\nDesign and implement data lake house projects within Azure.\nDevelop application lifecycle utilizing Microsoft Azure technologies.\nParticipate in design, planning, and necessary documentation.\nEngage in Agile ceremonies including daily standups, scrum, retrospectives, demos, and code reviews.\nHands-on experience with Python/SQL development and Azure data pipelines.\nCollaborate with the team to develop and deliver cross-functional products.\nKey Skills:\na. Data Engineering and SQL\nb. Python\nc. PySpark\nd. Azure Data Lake and ADF\ne. Databricks\nf. CI/CD\ng. Strong communication\nOther Responsibilities:\nDocument and maintain project artifacts.\nMaintain comprehensive knowledge of industry standards, methodologies, processes, and best practices.\nComplete training as required for Privacy, Code of Conduct, etc.\nPromptly report any known or suspected loss, theft, or unauthorized disclosure or use of PI to the General Counsel/Chief Compliance Officer or Chief Information Officer.\nAdhere to the company's compliance program.\nSafeguard the company's intellectual property, information, and assets.\nOther duties as assigned.\nMinimum Qualifications and Job Requirements:\nBachelor's degree in Computer Science.\n7 years of hands-on experience in designing and developing distributed data pipelines.\n5 years of hands-on experience in Azure data service technologies.\n5 years of hands-on experience in Python, SQL, Object-oriented programming, ETL, and unit testing.\nExperience with data integration with APIs, Web services, Queues.\nExperience with Azure DevOps and CI/CD as well as agile tools and processes including JIRA, Confluence.\n*Required: Azure data engineering associate and databricks data engineering certification",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'Delta Table', 'Azure Databricks', 'SQL', 'Python', 'SCALA', 'Big Data', 'Kafka', 'Azure Data Lake', 'Spark', 'ETL', 'Data Bricks']",2025-06-14 05:12:32
Azure Data Engineer ( Azure Databricks),Apex One,4 - 8 years,Not Disclosed,"['Hyderabad', 'Bengaluru']","Job Summary\nWe are seeking a skilled Azure Data Engineer with 4 years of overall experience, including at least 2 years of hands-on experience with Azure Databricks (Must). The ideal candidate will have strong expertise in building and maintaining scalable data pipelines and working across cloud-based data platforms.\nKey Responsibilities\nDesign, develop, and optimize large-scale data pipelines using Azure Data Factory, Azure Databricks, and Azure Synapse.\nImplement data lake solutions and work with structured and unstructured datasets in Azure Data Lake Storage (ADLS).\nCollaborate with data scientists, analysts, and engineering teams to design and deliver end-to-end data solutions.\nDevelop ETL/ELT processes and integrate data from multiple sources.\nMonitor, debug, and optimize workflows for performance and cost-efficiency.\nEnsure data governance, quality, and security best practices are maintained.\nMust-Have Skills\n4+ years of total experience in data engineering.\n2+ years of experience with Azure Databricks (PySpark, Notebooks, Delta Lake).\nStrong experience with Azure Data Factory, Azure SQL, and ADLS.\nProficient in writing SQL queries and Python/Scala scripting.\nUnderstanding of CI/CD pipelines and version control systems (e.g., Git).\nSolid grasp of data modeling and warehousing concepts.",Industry Type: Management Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure', 'Azure Data Factory', 'SQL queries', 'PySpark', 'Delta Lake', 'Azure Databricks', 'Notebooks', 'Azure SQL']",2025-06-14 05:12:34
Aws Data Engineer,Agilisium,4 - 8 years,Not Disclosed,"['Chennai', 'Bengaluru']",Years of experience: 4 to 8 years (with minimum 4 years of relevant experience)\n\nTech Stack :\nAWS\nPython\nSQL\nPyspark,,,,"['Pyspark', 'AWS', 'Python', 'SQL']",2025-06-14 05:12:36
Azure Data Engineer,Hiring for Leading MNC Company!!,5 - 8 years,Not Disclosed,"['Gurugram', 'Bengaluru']","Warm Greetings from SP Staffing!!\n\nRole:Azure Data Engineer\nExperience Required :5 to 8 yrs\nWork Location :Bangalore/Gurgaon\n\nRequired Skills,\nAzure Databricks, ADF, Pyspark/SQL\n\nInterested candidates can send resumes to nandhini.spstaffing@gmail.com",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Data Factory', 'Azure Databricks', 'Pyspark', 'Datafactory', 'ADF', 'Azure Data Lake', 'Data Bricks', 'Python', 'SQL', 'ADB']",2025-06-14 05:12:38
Senior Data Engineer,Tanasvi Technologies,9 - 14 years,9.6-24 Lacs P.A.,['Visakhapatnam'],"Responsibilities:\n* Design, develop & maintain data pipelines using PySpark, SQL & DBs.\n* Collaborate with cross-functional teams on project\ndelivery.\n*Strong in Databricks, PySpark, SQL\n* Databricks certification is mandatory\n*Location: Remote",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'Data Bricks', 'SQL']",2025-06-14 05:12:40
Data Engineer - Financial Analytics Specialist,RWS Group,5 - 8 years,Not Disclosed,['Bengaluru'],"Job Purpose -\nWe are seeking a Data Engineer Financial Analytics Specialist to join our Data and Analytics team at RWS. This role combines advanced technical skills in SQL and ETL with analytical thinking and financial business acumen. You will be instrumental in designing and implementing data transformations and pipelines to support complex business requirements, particularly in financial data systems. This is a hands-on technical role with a significant focus on problem-solving, mathematical reasoning, and creating solutions for intricate business processes.\nThis position is ideal for someone with a quantitative background in computer science, mathematics, or a related field, who thrives on developing innovative solutions, optimizing data workflows, and ensuring data accuracy. We are looking for a candidate who is self-motivated, detail-oriented, and possesses exceptional technical skills, alongside a strong sense of ownership and accountability. The primary focus of this role will be financial data projects within a collaborative and dynamic team environment.\n\nAbout Group Technology-\nGroup Technology enables the organization to achieve its strategic direction whilst driving shareholder value. The division establishes common standards and IT governance across the business. It further develops and manages core applications enabling smooth operational running of the organization across all functions. We drive and deliver future roadmaps aligned to the overall strategic direction of the business. Group Technology support services to over 7500 end users across the globe, manage the information security operation and safeguard all our assets. Our core Group Technology functions include Technical Architecture, Network & Voice, IT Security, Service Delivery, Solutions Delivery and Asset Management. Group Technology has a global presence across all regions with over 400 staff.\nOur Data Engineering team is responsible for developing, building, maintaining, and managing data pipelines. This requires working with large datasets, databases, and the software used to analyse them including cloud systems like AWS or Azure.\n\nJob Overview\nKey Responsibilities\nWe are seeking a Data Engineer Financial Analytics Specialist to join our Data and Analytics team at RWS. This role combines advanced technical skills in SQL and ETL with analytical thinking and financial business acumen. You will be instrumental in designing and implementing data transformations and pipelines to support complex business requirements, particularly in financial data systems. This is a hands-on technical role with a significant focus on problem-solving, mathematical reasoning, and creating solutions for intricate business processes.\nThis position is ideal for someone with a quantitative background in computer science, mathematics, or a related field, who thrives on developing innovative solutions, optimizing data workflows, and ensuring data accuracy. We are looking for a candidate who is self-motivated, detail-oriented, and possesses exceptional technical skills, alongside a strong sense of ownership and accountability. The primary focus of this role will be financial data projects within a collaborative and dynamic team environment.\nKey Responsibilities\nWrite advanced T-SQL queries, stored procedures, and functions to handle complex data transformations, ensuring optimal performance and scalability.\nPerform query performance optimization, including indexing strategies and tuning for large datasets.\nDesign, develop, and maintain ETL/ELT pipelines using tools like SSIS, ADF, or equivalent, ensuring seamless data integration and transformation.\nBuild and automate data workflows to support accurate and efficient data operations.\nCollaborate with stakeholders to understand financial business processes and requirements.\nDevelop tailored data solutions to address challenges such as cost allocation, weighted distributions, and revenue recognition using mathematical and logical reasoning.\nConduct root cause analysis for data issues, resolving them swiftly to ensure data accuracy and reliability.\nApply critical thinking and problem-solving to transform business logic into actionable data workflows.\nPartner with cross-functional teams to ensure data solutions align with business objectives.\nDocument data workflows, pipelines, and processes to support ongoing maintenance and scalability.\nRequired Skills and Experiences\n\nMinimum 5+ years of experience require in the similar role or same skillset\nAdvanced proficiency in T-SQL and SQL Server, including tools like SSMS, SQL Profiler, and SQL Agent.\nProven experience developing and optimizing complex queries, stored procedures, and ETL pipelines.\nProficiency with SSIS for data integration and transformation.\nStrong critical thinking and mathematical reasoning skills to design efficient solutions for business problems.\nDemonstrated ability to handle complex financial data challenges, including proportional distributions and other mathematical transformations.\nSolid understanding of financial concepts and processes, enabling seamless translation of business needs into data solutions.\nAbility to work effectively with business stakeholders, translating requirements into technical solutions.\nExcellent interpersonal and communication skills, fostering collaboration across teams.\nFamiliarity with Python, C#, or similar scripting languages for backend integration.\nExperience with cloud ETL tools such as Azure Data Factory (ADF), Fivetran, or Airbyte.\nKnowledge of data governance and compliance principles.\n\nShift Timings-\nThere will be UK shift timings - 1:30 PM 9:30 PM (IST)\n\nRWS Values -\nGet the 3Ps right Partner, Pioneer, Progress and well Deliver together as One RWS.\nFor further information, please visit: RWS\nRWS embraces DEI and promotes equal opportunity, we are an Equal Opportunity Employer and prohibit discrimination and harassment of any kind. RWS is committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment. All employment decisions at RWS are based on business needs, job requirements and individual qualifications, without regard to race, religion, nationality, ethnicity, sex, age, disability, or sexual orientation. RWS will not tolerate discrimination based on any of these characteristics\nRecruitment Agencies: RWS Holdings PLC does not accept agency resumes. Please do not forward any unsolicited resumes to any RWS employees. Any unsolicited resume received will be treated as the property of RWS and Terms & Conditions associated with the use of such resume will be considered null and void.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['T-SQL', 'Analytical Skills', 'Finance', 'SSIS', 'SQL', 'Critical Thinking', 'Azure', 'Analytical Ability', 'SSRS', 'ETL']",2025-06-14 05:12:42
GCP Data Engineer,TVS Next,3 - 5 years,Not Disclosed,['Bengaluru'],"What you’ll be doing:\nAssist in developing machine learning models based on project requirements\nWork with datasets by preprocessing, selecting appropriate data representations, and ensuring data quality.\nPerforming statistical analysis and fine-tuning using test results.\nSupport training and retraining of ML systems as needed.\nHelp build data pipelines for collecting and processing data efficiently.",,,,"['kubernetes', 'pyspark', 'data pipeline', 'sql', 'docker', 'cloud', 'tensorflow', 'java', 'spark', 'gcp', 'pytorch', 'bigquery', 'programming', 'ml', 'cloud sql', 'cd', 'python', 'airflow', 'cloud spanner', 'cloud pubsub', 'application engine', 'machine learning', 'apache flink', 'data engineering', 'dataproc', 'kafka', 'cloud storage', 'terraform', 'bigtable']",2025-06-14 05:12:45
Senior Data Engineer,Binary Infoways,5 - 9 years,12-19.2 Lacs P.A.,['Hyderabad'],"Responsibilities:\n* Design, develop & maintain data pipelines using Airflow, Python & SQL.\n* Optimize performance through Spark & Splunk analytics.\n* Collaborate with cross-functional teams on big data initiatives.\n* AWS",Industry Type: BPM / BPO,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Airflow', 'Big Data Technologies', 'ETL', 'AWS', 'Python', 'Glue', 'Snowflake', 'Spark', 'Splunk', 'SQL']",2025-06-14 05:12:47
Specialist Data/AI Engineering,ATT Communication Services,3 - 4 years,Not Disclosed,['Bengaluru'],"Overall Purpose\nThis position will interact on a consistent basis with other developers, architects, data product owners and source systems. This position requires multifaceted candidates who have experience in data analysis, visualization, good hands-on experience with BI Tools and relational databases, experience in data warehouse architecture (traditional and cloud).\nKey Roles and Responsibilities\nDevelop, understand, and enhance code in traditional data warehouse environments, data lake, and cloud environments like Snowflake, Azure, Databricks\nBuild new end-to-end business intelligence solutions. This includes data extraction, ETL processes applied on data to derive useful business insights, and best representing this data through dashboards.\nWrite complex SQL queries used to transform data using Python/Unix shell scripting\nUnderstand business requirements and create visual reports and dashboards using Power BI or Tableau.\nUpskill to different technologies, understand existing products and programs in place\nWork with other development and operations teams.\nFlexible with shifts and occasional weekend support.\nKey Competencies\nFull life-cycle experience on enterprise software development projects.\nExperience in relational databases/ data marts/data warehouses and complex SQL programming.\nExtensive experience in ETL, shell or python scripting, data modelling, analysis, and preparation\nExperience in Unix/Linux system, files systems, shell scripting.\nGood to have knowledge on any cloud platforms like AWS, Azure, Snowflake, etc.\nGood to have experience in BI Reporting tools Power BI or Tableau\nGood problem-solving and analytical skills used to resolve technical problems.\nMust possess a good understanding of business requirements and IT strategies.\nAbility to work independently but must be a team player. Should be able to drive business decisions and take ownership of their work.\nExperience in presentation design, development, delivery, and good communication skills to present analytical results and recommendations for action-oriented data driven decisions and associated operational and financial impacts.\nRequired/Desired Skills\nCloud Platforms - Azure, Snowflake, Databricks, Delta lake (Required 3-4 years)\nRDBMS and Data Warehousing (Required 7-10 Years)\nSQL Programming and ETL (Required 7-10 Years)\nUnix/Linux shell scripting (Required 2-3 years)\nPower BI / Tableau (Desired 3 years)\nPython or any other programming language (Desired 4 years)\nEducation & Qualifications\nUniversity Degree in Computer Science and/or Analytics\nMinimum Experience required: 7-10 years in relational database design & development, ETL development\nJob ID R-68650 Date posted 06/11/2025",Industry Type: Telecom / ISP,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Unix', 'Linux', 'RDBMS', 'Database design', 'Shell scripting', 'Business intelligence', 'Unix shell scripting', 'Analytics', 'SQL', 'Python']",2025-06-14 05:12:49
Azure Data Engineer,Jurist Associates,5 - 10 years,7-12 Lacs P.A.,"['Kochi', 'Hyderabad', 'Bengaluru']","Design, build, and maintain scalable and efficient data pipelines using Azure services such as Azure Data Factory (ADF), Azure Databricks, and Azure Synapse Analytics. Develop and optimize ETL/ELT workflows for ingestion, cleansing, transformation,\n\nRequired Candidate profile\nStrong understanding of data warehouse architecture, data lakes, and big data frameworks. Candidates who have atleast 5 years of experience should only apply.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Data Engineering', 'Azure Data Factory', 'GenAI Tools', 'Azure Data Warehouse', 'Azure data lake Gen2', 'Azure Synapse Analytics', 'Azure Databricks', 'Azure Data Lake', 'Nosql Databases', 'Data Modeling', 'Semantic Analytics', 'ML/DL Models']",2025-06-14 05:12:51
Data Engineer-Having Stratup-Mid-Size company Exp.@ Bangalore_Urgent,"As a leader in this space, we deliver wo...",8 - 13 years,Not Disclosed,['Bengaluru'],"Data Engineer\n\nLocation: Bangalore - Onsite\nExperience: 8 - 15 years\nType: Full-time\n\nRole Overview\n\nWe are seeking an experienced Data Engineer to build and maintain scalable, high-performance data pipelines and infrastructure for our next-generation data platform. The platform ingests and processes real-time and historical data from diverse industrial sources such as airport systems, sensors, cameras, and APIs. You will work closely with AI/ML engineers, data scientists, and DevOps to enable reliable analytics, forecasting, and anomaly detection use cases.\nKey Responsibilities\nDesign and implement real-time (Kafka, Spark/Flink) and batch (Airflow, Spark) pipelines for high-throughput data ingestion, processing, and transformation.\nDevelop data models and manage data lakes and warehouses (Delta Lake, Iceberg, etc) to support both analytical and ML workloads.\nIntegrate data from diverse sources: IoT sensors, databases (SQL/NoSQL), REST APIs, and flat files.\nEnsure pipeline scalability, observability, and data quality through monitoring, alerting, validation, and lineage tracking.\nCollaborate with AI/ML teams to provision clean and ML-ready datasets for training and inference.\nDeploy, optimize, and manage pipelines and data infrastructure across on-premise and hybrid environments.\nParticipate in architectural decisions to ensure resilient, cost-effective, and secure data flows.\nContribute to infrastructure-as-code and automation for data deployment using Terraform, Ansible, or similar tools.\n\n\nQualifications & Required Skills\n\nBachelors or Master’s in Computer Science, Engineering, or related field.\n6+ years in data engineering roles, with at least 2 years handling real-time or streaming pipelines.\nStrong programming skills in Python/Java and SQL.\nExperience with Apache Kafka, Apache Spark, or Apache Flink for real-time and batch processing.\nHands-on with Airflow, dbt, or other orchestration tools.\nFamiliarity with data modeling (OLAP/OLTP), schema evolution, and format handling (Parquet, Avro, ORC).\nExperience with hybrid/on-prem and cloud platforms (AWS/GCP/Azure) deployments.\nProficient in working with data lakes/warehouses like Snowflake, BigQuery, Redshift, or Delta Lake.\nKnowledge of DevOps practices, Docker/Kubernetes, Terraform or Ansible.\nExposure to data observability, data cataloging, and quality tools (e.g., Great Expectations, OpenMetadata).\nGood-to-Have\nExperience with time-series databases (e.g., InfluxDB, TimescaleDB) and sensor data.\nPrior experience in domains such as aviation, manufacturing, or logistics is a plus.\n\nRole & responsibilities\n\n\nPreferred candidate profile",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['aviation', 'Data Modeling', 'Python', 'OLAP', 'Cloud', 'ORC', 'logistics', 'Avro', 'Terraform', 'Snowflake', 'manufacturing', 'AWS', 'Parquet', 'Java', 'Azure', 'BigQuery', 'Data', 'Redshift', 'SQL', 'TimescaleDB', 'GCP', 'InfluxDB', 'dbt', 'Ansible', 'OLTP', 'Kubernetes']",2025-06-14 05:12:54
Senior Data Engineer,Dfcs Technologies,8 - 13 years,Not Disclosed,['Chennai'],"Architect & Build Scalable Systems: Design and implement a petabyte-scale lakehouse\nArchitectures to unify data lakes and warehouses. Real-Time Data Engineering: Develop and optimize streaming pipelines using Kafka, Pulsar, and Flink.\n\nRequired Candidate profile\nData engineering experience with large-scale systems• Expert proficiency in Java for data-intensive applications. Handson experience with lakehouse architectures, stream processing, & event streaming",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Java', 'Terraform', 'Delta Lake', 'Data Engineer', 'Streaming', 'Streaming Framework', 'Pyspark', 'Apache Iceberg', 'lakehouse', 'Apache Flink', 'Kafka', 'Spark Streaming', 'Stream Processing', 'Spark Core', 'Apache Storm', 'Apache Pulsar', 'Kafta', 'Data Pipeline', 'Streaming Kafka', 'Clickhouse', 'Spark', 'AWS', 'Kafka Streams', 'Streams']",2025-06-14 05:12:56
Azure Data Engineer,Fortune India 500 IT Services Firm,4 - 8 years,5-12 Lacs P.A.,"['Hyderabad', 'Chennai', 'Bengaluru']",Hiring for Azure Data Engineer and also the client is looking for Immediate joiners who can join in 30 days.,Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Temporary/Contractual","['Azure Data Factory', 'Pyspark', 'Azure Databricks', 'Python', 'Azure Synapse', 'Azure Data Lake', 'SQL']",2025-06-14 05:12:58
Gcp Data Engineer,Saama Technologies,3 - 8 years,Not Disclosed,"['Pune', 'Chennai', 'Coimbatore']","We are looking for immediate joiners only.\nPosition: GCP Data Engineer\nWe are seeking a skilled and experienced GCP Data Engineer to join our dynamic team. The ideal candidate will have a strong background in Google Cloud Platform (GCP), BigQuery, Dataform, and data warehouse concepts. Experience with Airflow/Cloud Composer and cloud computing knowledge will be a significant advantage.\nResponsibilities:\n- Designing, developing, and maintaining data pipelines and workflows on the Google Cloud Platform.",,,,"['Pyspark', 'GCP', 'Python', 'SQL', 'Google Cloud Platforms']",2025-06-14 05:13:01
"Data Engineer - Snowflake, Azure Data Factory (ADF)",Suzva Software Technologies,0 - 1 years,Not Disclosed,['Mumbai'],"We are seeking an experienced Data Engineer to join our team for a 6-month contract assignment. The ideal candidate will work on data warehouse development, ETL pipelines, and analytics enablement using Snowflake, Azure Data Factory (ADF), dbt, and other tools.\n\nThis role requires strong hands-on experience with data integration platforms, documentation, and pipeline optimizationespecially in cloud environments such as Azure and AWS.\n\n#KeyResponsibilities\nBuild and maintain ETL pipelines using Fivetran, dbt, and Azure Data Factory\n\nMonitor and support production ETL jobs\n\nDevelop and maintain data lineage documentation for all systems\n\nDesign data mapping and documentation to aid QA/UAT testing\n\nEvaluate and recommend modern data integration tools\n\nOptimize shared data workflows and batch schedules\n\nCollaborate with Data Quality Analysts to ensure accuracy and integrity of data flows\n\nParticipate in performance tuning and improvement recommendations\n\nSupport BI/MDM initiatives including Data Vault and Data Lakes\n\n#RequiredSkills\n7+ years of experience in data engineering roles\n\nStrong command of SQL, with 5+ years of hands-on development\n\nDeep experience with Snowflake, Azure Data Factory, dbt\n\nStrong background with ETL tools (Informatica, Talend, ADF, dbt, etc.)\n\nBachelor's in CS, Engineering, Math, or related field\n\nExperience in healthcare domain (working with PHI/PII data)\n\nFamiliarity with scripting/programming (Python, Perl, Java, Linux-based environments)\n\nExcellent communication and documentation skills\n\nExperience with BI tools like Power BI, Cognos, etc.\n\nOrganized, self-starter with strong time-management and critical thinking abilities\n\n#NiceToHave\nExperience with Data Lakes and Data Vaults\n\nQA & UAT alignment with clear development documentation\nMulti-cloud experience (especially Azure, AWS)",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Java', 'Azure', 'Power BI', 'UAT', 'Perl', 'QA', 'Azure Data Factory', 'Linux', 'Cognos', 'Snowflake', 'ETL', 'AWS', 'Python']",2025-06-14 05:13:03
Data Platform Engineer,Accenture,3 - 8 years,Not Disclosed,['Bengaluru'],"Project Role :Data Platform Engineer\n\n\n\n\n\nProject Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Platform Engineer, you will assist with the data platform blueprint and design, collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. You will play a crucial role in shaping the data platform components.\nRoles & Responsibilities:- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Collaborate with Integration Architects and Data Architects to design and implement data platform components.- Ensure seamless integration between various systems and data models.- Develop and maintain data platform blueprints.- Optimize data platform performance and scalability.- Provide technical guidance and support to team members.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Strong understanding of data platform architecture and design principles.- Experience with cloud-based data platforms like AWS or Azure.- Hands-on experience with data integration tools and technologies.- Knowledge of data governance and security best practices.\nAdditional Information:- The candidate should have a minimum of 3 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Bengaluru office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analytics', 'microsoft azure', 'platform architecture', 'design principles', 'aws', 'kubernetes', 'c++', 'oracle', 'enterprise architecture', 'microservices', 'docker', 'infrastructure architecture', 'java', 'data modeling', 'gcp', 'design patterns', 'data governance', 'agile', 'hadoop']",2025-06-14 05:13:05
Data Platform Engineer,Accenture,12 - 15 years,Not Disclosed,['Bengaluru'],"Project Role :Data Platform Engineer\n\n\n\n\n\nProject Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\n\n\n\nMust have skills :Collibra Data Governance\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n12 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Platform Engineer, you will assist with the data platform blueprint and design, encompassing the relevant data platform components. Your typical day will involve collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models, while also engaging in discussions to refine and enhance the overall data architecture. You will be involved in various stages of the data platform lifecycle, ensuring that all components work harmoniously to support the organization's data needs and objectives.\nRoles & Responsibilities:- Expected to be an SME.- Collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Expected to provide solutions to problems that apply across multiple teams.- Facilitate knowledge sharing sessions to enhance team capabilities and foster a culture of continuous improvement.- Monitor and evaluate team performance, providing constructive feedback to ensure alignment with project goals.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Collibra Data Governance.- Strong understanding of data governance frameworks and best practices.- Experience with data integration tools and techniques.- Familiarity with data modeling concepts and methodologies.- Ability to analyze and interpret complex data sets to inform decision-making.\nAdditional Information:- The candidate should have minimum 12 years of experience in Collibra Data Governance.- This position is based at our Bengaluru office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'data architecture', 'sql', 'data modeling', 'data governance', 'data analysis', 'oracle', 'data management', 'data warehousing', 'business analysis', 'machine learning', 'business intelligence', 'javascript', 'sql server', 'data quality', 'tableau', 'java', 'html', 'mysql', 'etl', 'informatica']",2025-06-14 05:13:08
Lead Security Engineer (Data Protection),Flipkart,7 - 11 years,Not Disclosed,['Bengaluru'],"Skills Required :\n7+ years of experience in the field of information security. Strong technical expertise in DLP and data classification methodologies\nDesirable Skills :\nWork done in AI, automations",Industry Type: Courier / Logistics,,,"['AI', 'automations', 'Data Protection', 'information security', 'DLP']",2025-06-14 05:13:10
Mid-Level Data Engineer,BMW Techworks India,6 - 8 years,Not Disclosed,"['Chennai', 'Bengaluru']","What awaits you/ Job Profile\nProvide estimates for requirements, analyses and develop as per the requirement.\nDeveloping and maintaining data pipelines and ETL (Extract, Transform, Load) processes to extract data efficiently and reliably from various sources, transform it into a usable format, and load it into the appropriate data repositories.\nCreating and maintaining logical and physical data models that align with the organizations data architecture and business needs. This includes defining data schemas, tables, relationships, and indexing strategies for optimal data retrieval and analysis.\nCollaborating with cross-functional teams and stakeholders to ensure data security, privacy, and compliance with regulations.\nCollaborate with downstream application to understand their needs and build the data storage and optimize as per their need.\nWorking closely with other stakeholders and Business to understand data requirements and translate them into technical solutions.\nFamiliar with Agile methodologies and have prior experience working with Agile teams using Scrum/Kanban\nLead Technical discussions with customers to find the best possible solutions.\nProactively identify and implement opportunities to automate tasks and develop reusable frameworks.\nOptimizing data pipelines to improve performance and cost, while ensuring a high quality of data within the data lake.\nMonitoring services and jobs for cost and performance, ensuring continual operations of data pipelines, and fixing of defects.\nConstantly looking for opportunities to optimize data pipelines to improve performance\nWhat should you bring along\nMust Have:\nHand on Expertise of 6- 8 years in AWS services like S3, Lambda, Glue, Athena, RDS, Step functions, SNS, SQS, API Gateway, Security, Access and Role permissions, Logging and monitoring Services.\nGood hand on knowledge on Python, Spark, Hive and Unix, AWS CLI\nPrior experience in working with streaming solution like Kafka\nPrior experience in implementing different file storage types like Delta-lake / Ice-berg.\nExcellent knowledge in Data modeling and Designing ETL pipeline.\nMust have strong knowledge in using different databases such as MySQL, Oracle and Writing complex queries.\nStrong experience working in a continuous integration and Deployment process.\nNice to Have:\nHand on experience in the Terraform, GIT, GIT Actions. CICD pipeline and Amazon Q.\nMust have technical skill\nPyspark, AWS ,SQL, Kafka, Glue, IAM. S3, Lambda, Step Function, Athena\nGood to have Technical skills\nTerraform, GIT, GIT Actions. CICD pipeline , AI",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Unix', 'Data modeling', 'data security', 'MySQL', 'Agile', 'Scrum', 'Oracle', 'Monitoring', 'SQL', 'Python']",2025-06-14 05:13:12
Data Platform Engineer,Accenture,5 - 10 years,Not Disclosed,['Pune'],"Project Role :Data Platform Engineer\n\n\n\n\n\nProject Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Platform Engineer, you will assist with the data platform blueprint and design, collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. You will play a crucial role in the development and maintenance of the data platform components, contributing to the overall success of the project.\nRoles & Responsibilities:- Expected to be an SME, collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Assist with the data platform blueprint and design.- Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.- Develop and maintain data platform components.- Contribute to the overall success of the project.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Strong understanding of statistical analysis and machine learning algorithms.- Experience with data visualization tools such as Tableau or Power BI.- Hands-on implementing various machine learning algorithms such as linear regression, logistic regression, decision trees, and clustering algorithms.- Solid grasp of data munging techniques, including data cleaning, transformation, and normalization to ensure data quality and integrity.\nAdditional Information:- The candidate should have a minimum of 5 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Bengaluru office.- 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analytics', 'tableau', 'machine learning algorithms', 'statistics', 'data munging', 'python', 'natural language processing', 'power bi', 'machine learning', 'sql', 'data bricks', 'data quality', 'r', 'data modeling', 'data science', 'predictive modeling', 'text mining', 'logistic regression']",2025-06-14 05:13:14
Data Platform Engineer,Accenture,7 - 12 years,Not Disclosed,['Bengaluru'],"Project Role :Data Platform Engineer\n\n\n\n\n\nProject Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Platform Engineer, you will assist with the data platform blueprint and design, collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. You will play a crucial role in shaping the data platform components.\nRoles & Responsibilities:- Expected to be an SME, collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Lead the implementation of data platform solutions.- Conduct performance tuning and optimization of data platform components.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Strong understanding of cloud-based data platforms.- Experience in designing and implementing data pipelines.- Knowledge of data governance and security best practices.\nAdditional Information:- The candidate should have a minimum of 7.5 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Bengaluru office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['hive', 'data analytics', 'data modeling', 'spark', 'data governance', 'python', 'amazon redshift', 'data warehousing', 'microsoft azure', 'emr', 'machine learning', 'sql', 'nosql', 'amazon ec2', 'java', 'kafka', 'mysql', 'hadoop', 'sqoop', 'big data', 'aws', 'etl']",2025-06-14 05:13:17
Data Platform Engineer,Accenture,7 - 12 years,Not Disclosed,['Bengaluru'],"Project Role :Data Platform Engineer\n\n\n\n\n\nProject Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification : Engineering graduate preferably Computer Science graduate 15 years of full time education\n\n\nSummary:As a Data Platform Engineer, you will be responsible for assisting with the blueprint and design of the data platform components using Databricks Unified Data Analytics Platform. Your typical day will involve collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\nRoles & Responsibilities:- Assist with the blueprint and design of the data platform components using Databricks Unified Data Analytics Platform.- Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.- Develop and maintain data pipelines and ETL processes using Databricks Unified Data Analytics Platform.- Design and implement data security and access controls for the data platform.- Troubleshoot and resolve issues related to the data platform and data pipelines.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nExperience with Databricks Unified Data Analytics Platform.- Must To Have\n\n\n\n\nSkills:\nStrong understanding of data modeling and database design principles.- Good To Have\n\n\n\n\nSkills:\nExperience with cloud-based data platforms such as AWS or Azure.- Good To Have\n\n\n\n\nSkills:\nExperience with data security and access controls.- Good To Have\n\n\n\n\nSkills:\nExperience with data visualization tools such as Tableau or Power BI.\nAdditional Information:- The candidate should have a minimum of 7.5 years of experience in Databricks Unified Data Analytics Platform.- The ideal candidate will possess a strong educational background in computer science or a related field, along with a proven track record of delivering impactful data-driven solutions.- This position is based at our Bangalore, Hyderabad, Chennai and Pune Offices.- Mandatory office (RTO) for 2- 3 days and have to work on 2 shifts (Shift A- 10:00am to 8:00pm IST and Shift B - 12:30pm to 10:30 pm IST)\n\nQualification\n\nEngineering graduate preferably Computer Science graduate 15 years of full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['database design', 'data modeling', 'data analytics', 'microsoft azure', 'design principles', 'hive', 'sql', 'java', 'spark', 'design patterns', 'oops', 'mysql', 'hadoop', 'etl', 'big data', 'c#', 'rest', 'python', 'data security', 'power bi', 'javascript', 'sql server', 'data bricks', 'tableau', 'kafka', 'sqoop', 'aws']",2025-06-14 05:13:19
Aws Data Engineer,Hiring for Leading MNC Company,4 - 6 years,Not Disclosed,"['Hyderabad', 'Pune', 'Bengaluru']","Warm Greetings from SP Staffing!!\n\nRole:AWS Data Engineer\nExperience Required :4 to 6 yrs\nWork Location :Bangalore/Pune/Hyderabad/Chennai\n\nRequired Skills,\n\nPyspark\nAWS Glue\n\nInterested candidates can send resumes to nandhini.spstaffing@gmail.com",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'Aws Glue', 'python', 'EMR', 'Aws Emr', 'AWS Data Engineer', 'Aws Lambda', 'Lakehouse', 'spark', 'Data Engineer', 'ETL', 'AWS', 'Athena', 'gateway']",2025-06-14 05:13:22
Hadoop & UNIX Shell Scripting Engineer (Data Management),Synechron,5 - 10 years,Not Disclosed,['Bengaluru'],"Job Summary\nSynechron is seeking a dedicated and technically skilled Hadoop Shell Scripting Engineer to manage and optimize our Hadoop ecosystem. The role involves developing automation utilities, troubleshooting complex issues, and collaborating with vendors for platform enhancements. Your expertise will directly support enterprise data processing, performance tuning, and cloud migration initiatives, ensuring reliable and efficient data infrastructure that aligns with organizational goals.\nSoftware Requirements\nOverall Responsibilities\nTechnical Skills (By Category)\nExperience Requirements\nDay-to-Day Activities\nQualifications\nProfessional Competencies",,,,"['Hadoop', 'Java', 'Automation', 'DevOps', 'Data Management', 'CI/CD', 'Performance Tuning', 'UNIX Shell Scripting', 'Python', 'SQL']",2025-06-14 05:13:24
Lead Data Engineer (Immediate joiner),Decision Point,4 - 9 years,15-30 Lacs P.A.,"['Gurugram', 'Chennai']","Role & responsibilities\n• Assume ownership of Data Engineering projects from inception to completion.\nImplement fully operational Unified Data Platform solutions in production environments using technologies like Databricks, Snowflake, Azure Synapse etc.\nShowcase proficiency in Data Modelling and Data Architecture\nUtilize modern data transformation tools such as DBT (Data Build Tool) to streamline and automate data pipelines (nice to have).",,,,"['Pyspark', 'Azure Databricks', 'SQL', 'Azure Synapse', 'Python', 'Etl Pipelines', 'Airflow', 'Bigquery', 'Advance Sql', 'Azure Cloud', 'GCP', 'Data Modeling', 'Data Architecture', 'AWS']",2025-06-14 05:13:27
Lead Data Engineer,Wiser Solutions,2 - 3 years,Not Disclosed,['Vadodara'],"Job Description\nWhen looking to buy a product, whether it is in a brick and mortar store or online, it can be hard enough to find one that not only has the characteristics you are looking for but is also at a price that you are willing to pay. It can also be especially frustrating when you finally find one, but it is out of stock. Likewise, brands and retailers can have a difficult time getting the visibility they need to ensure you have the most seamless experience as possible in selecting their product. We at Wiser believe that shoppers should have this seamless experience, and we want to do that by providing the brands and retailers the visibility they need to make that belief a reality.\nOur goal is to solve a messy problem elegantly and cost effectively. Our job is to collect, categorize, and analyze lots of structured and semi-structured data from lots of different places every day (whether it s 20 million+ products from 500+ websites or data collected from over 300,000 brick and mortar stores across the country). We help our customers be more competitive by discovering interesting patterns in this data they can use to their advantage, while being uniquely positioned to be able to do this across both online and instore.\nWe are looking for a lead-level software engineer to lead the charge on a team of like-minded individuals responsible for developing the data architecture that powers our data collection process and analytics platform. If you have a passion for optimization, scaling, and integration challenges, this may be the role for you.\nWhat You Will Do\nThink like our customers - you will work with product and engineering leaders to define data solutions that support customers business practices.\nDesign/develop/extend our data pipeline services and architecture to implement your solutions - you will be collaborating on some of the most important and complex parts of our system that form the foundation for the business value our organization provides\nFoster team growth - provide mentorship to both junior team members and evangelizing expertise to those on others.\nImprove the quality of our solutions - help to build enduring trust within our organization and amongst our customers by ensuring high quality standards of the data we manage\nOwn your work - you will take responsibility to shepherd your projects from idea through delivery into production\nBring new ideas to the table - some of our best innovations originate within the team\nTechnologies We Use\nLanguages: SQL, Python\nInfrastructure: AWS, Docker, Kubernetes, Apache Airflow, Apache Spark, Apache Kafka, Terraform\nDatabases: Snowflake, Trino/Starburst, Redshift, MongoDB, Postgres, MySQL\nOthers: Tableau (as a business intelligence solution)\n\n\nQualifications\nBachelors/Master s degree in Computer Science or relevant technical degree\n10+ years of professional software engineering experience\nStrong proficiency with data languages such as Python a",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Linux', 'MySQL', 'Agile', 'Data structures', 'OLAP', 'Apache', 'Business intelligence', 'SQL', 'Python']",2025-06-14 05:13:29
Specialist Data/AI Engineering,ATT Communication Services,3 - 4 years,Not Disclosed,['Bengaluru'],"Overall Purpose\nThis position will interact on a consistent basis with other developers, architects, data product owners and source systems. This position requires multifaceted candidates who have experience in data analysis, visualization, good hands-on experience with BI Tools and relational databases, experience in data warehouse architecture (traditional and cloud).\nKey Roles and Responsibilities\nDevelop, understand, and enhance code in traditional data warehouse environments, data lake, and cloud environments like Snowflake, Azure, Databricks\nBuild new end-to-end business intelligence solutions. This includes data extraction, ETL processes applied on data to derive useful business insights, and best representing this data through dashboards.\nWrite complex SQL queries used to transform data using Python/Unix shell scripting\nUnderstand business requirements and create visual reports and dashboards using Power BI or Tableau.\nUpskill to different technologies, understand existing products and programs in place\nWork with other development and operations teams.\nFlexible with shifts and occasional weekend support.\nKey Competencies\nFull life-cycle experience on enterprise software development projects.\nExperience in relational databases/ data marts/data warehouses and complex SQL programming.\nExtensive experience in ETL, shell or python scripting, data modelling, analysis, and preparation\nExperience in Unix/Linux system, files systems, shell scripting.\nGood to have knowledge on any cloud platforms like AWS, Azure, Snowflake, etc.\nGood to have experience in BI Reporting tools Power BI or Tableau\nGood problem-solving and analytical skills used to resolve technical problems.\nMust possess a good understanding of business requirements and IT strategies.\nAbility to work independently but must be a team player. Should be able to drive business decisions and take ownership of their work.\nExperience in presentation design, development, delivery, and good communication skills to present analytical results and recommendations for action-oriented data driven decisions and associated operational and financial impacts.\nRequired/Desired Skills\nCloud Platforms - Azure, Snowflake, Databricks, Delta lake (Required 3-4 years)\nRDBMS and Data Warehousing (Required 7-10 Years)\nSQL Programming and ETL (Required 7-10 Years)\nUnix/Linux shell scripting (Required 2-3 years)\nPower BI / Tableau (Desired 3 years)\nPython or any other programming language (Desired 4 years)\nEducation & Qualifications\nUniversity Degree in Computer Science and/or Analytics\nMinimum Experience required: 7-10 years in relational database design & development, ETL development\nJob ID R-68650 Date posted 06/11/2025",Industry Type: Telecom / ISP,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Unix', 'Linux', 'RDBMS', 'Database design', 'Shell scripting', 'Business intelligence', 'Unix shell scripting', 'Analytics', 'SQL', 'Python']",2025-06-14 05:13:31
Site Reliability Engineer - Cloud & Data Center Networking,IBM,5 - 10 years,Not Disclosed,['Bengaluru'],"- Manage, optimise and resolve issues observed in the VPC environment through detailed debugging\n\n- Manage extremely good timelines for the events and actions taken in our incident records for future reference for improvements\n\n- Manage and optimize underlay network infrastructure including routing, switching, and physical connectivity in data centers\n\n- Collaborate with cloud architects to troubleshoot and resolve networking issues across the Cloud Infrastructure\n\n- Monitor network performance and proactively resolve issues using tools like Splunk, AppNeta or equivalent\n\n- Document procedures and call out anomalies when observed during run-book executions\n\n- Also improve run-books as and when issues are discovered\n\n- Participate in on-call rotations and incident response as needed\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\n\n- Bachelor’s degree in Computer Science, Information Technology, or related field (or equivalent experience).\n\n- 5+ years of experience in enterprise networking, with a strong focus on both cloud and data center networking environments\n\n- Proficiency in overlay technologiesVXLAN, NVGRE, VPC\n\n- Strong understanding of underlay technologiesBGP, OSPF, MPLS, Ethernet, Spine-Leaf architectures\n\n- Relevant certifications (e.g., CCNP, CCIE, AWS Advanced Networking) are a plus\n\n\nPreferred technical and professional experience\n\n- Exposure to container networking (Kubernetes)\n\n- Familiarity with network automation tools (e.g., Ansible, Terraform, Python scripting)",,,,"['networking', 'ospf', 'ethernet', 'data center', 'mpls', 'container', 'kubernetes', 'python', 'network infrastructure', 'reliability', 'site reliability engineering', 'ansible', 'docker', 'automation tools', 'java', 'devops', 'linux', 'jenkins', 'splunk', 'cloud infrastructure', 'debugging', 'terraform', 'aws']",2025-06-14 05:13:33
Gcp Data Engineer,Cloudxtreme,8 - 13 years,16-31 Lacs P.A.,['Gurugram'],"Role & responsibilities\n\nGCP; Big Data; ETL - Big Data / Data Warehousing\nBig query, data proc, data flow, composer\nLooking forGCP Developerwith below mandatory skills and requirementsMandatory Skills::\nBigQuery,Cloud Storage, Cloud Pub/Sub, Dataflow, Dataproc,Composer• 6+ years in cloud infrastructure and designing data pipeline, specifically in GCP•\nProficiency in programming languages Python, SQL•Proven experience in designing and implementing cloud-native applications and microservices on GCP.\nHands-on experience with CI/CD tools like Jenkins and Github Action•In-depth understanding of GCP networking, IAM policies, and security best practices.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Temporary/Contractual","['GCP', 'Big Data', 'ETL']",2025-06-14 05:13:36
Data Platform Engineer,Accenture,3 - 8 years,Not Disclosed,['Bengaluru'],"Project Role :Data Platform Engineer\n\n\n\n\n\nProject Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Platform Engineer, you will assist with the data platform blueprint and design, collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. You will play a crucial role in the development and maintenance of the data platform components, contributing to the overall success of the project.\nRoles & Responsibilities:- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Assist with the data platform blueprint and design.- Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.- Develop and maintain data platform components.- Contribute to the overall success of the project.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Strong understanding of statistical analysis and machine learning algorithms.- Experience with data visualization tools such as Tableau or Power BI.- Hands-on implementing various machine learning algorithms such as linear regression, logistic regression, decision trees, and clustering algorithms.- Solid grasp of data munging techniques, including data cleaning, transformation, and normalization to ensure data quality and integrity.\nAdditional Information:- The candidate should have a minimum of 3 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Hyderabad office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analytics', 'tableau', 'machine learning algorithms', 'statistics', 'data munging', 'python', 'data analysis', 'natural language processing', 'power bi', 'machine learning', 'sql', 'data quality', 'r', 'data modeling', 'data science', 'predictive modeling', 'text mining', 'logistic regression']",2025-06-14 05:13:38
Data Platform Engineer,Accenture,3 - 8 years,Not Disclosed,['Chennai'],"Project Role :Data Platform Engineer\n\n\n\n\n\nProject Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Platform Engineer, you will assist with the data platform blueprint and design, encompassing the relevant data platform components. You will collaborate with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models. Your typical day will involve working on the data platform blueprint and design, collaborating with architects, and ensuring seamless integration between systems and data models.\nRoles & Responsibilities:- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Assist with the data platform blueprint and design.- Collaborate with Integration Architects and Data Architects.- Ensure cohesive integration between systems and data models.- Implement data platform components.- Troubleshoot and resolve data platform issues.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Strong understanding of statistical analysis and machine learning algorithms.- Experience with data visualization tools such as Tableau or Power BI.- Hands-on implementing various machine learning algorithms such as linear regression, logistic regression, decision trees, and clustering algorithms.- Solid grasp of data munging techniques, including data cleaning, transformation, and normalization to ensure data quality and integrity.\nAdditional Information:- The candidate should have a minimum of 3 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Bengaluru office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analytics', 'tableau', 'machine learning algorithms', 'statistics', 'data munging', 'python', 'natural language processing', 'power bi', 'data architecture', 'machine learning', 'sql', 'data quality', 'r', 'data modeling', 'data science', 'predictive modeling', 'text mining']",2025-06-14 05:13:40
Lead Data Engineer,Transunion,5 - 10 years,Not Disclosed,['Chennai'],"TransUnions Job Applicant Privacy Notice\nWhat Well Bring:\nData Pipeline Engineer at Orion project are embedded within our engineering teams and support the development and operation.\nWhat Youll Bring:\nLead Data Engineer\nWhat We Offer\nWe are looking for an individual to be part of an autonomous, cross-functional agile/scrum team where everyone shares responsibility for all aspects of the work.\nThis is a hybrid position and involves regular performance of job responsibilities virtually as well as in-person at an assigned TU office location for a minimum of two days a week.\nLead Developer, Software Development",,,,"['orchestration', 'Data management', 'Coding', 'GCP', 'Postgresql', 'Agile', 'Data processing', 'big data', 'Analytics', 'SQL']",2025-06-14 05:13:42
AWS Data Engineer,SoulPage IT,4 - 9 years,Not Disclosed,['Hyderabad'],"About Us:\nSoulpage IT Solutions is an AI/ML technology company based in Hyderabad, specializing in cutting-edge solutions for data engineering, AI, and cloud-based analytics. We are looking for an experienced Data Engineer with expertise in Big Data technologies, Python, and PySpark to join our team.\nKey Responsibilities:\nDevelop and optimize PySpark applications using Spark DataFrames in Python. Work on large-scale data processing, ensuring performance optimization for high-volume data pipelines .\nImplement best practices for version control using Git.\nWork with Amazon Analytics services such as Amazon EMR, Amazon Athena, and AWS Glue .\nUtilize Amazon Compute services including AWS Lambda, EC2 , and storage solutions like S3, SNS .\nWork with columnar storage formats like Parquet, Avro, ORC , and apply compression techniques ( Snappy, Gzip ).\nGood to have experience with data warehousing concepts (dimensions, facts, schemas - Star, Snowflake ).\nGood to have experience with AWS databases such as Aurora, RDS, Redshift, ElastiCache, or DynamoDB .\nRequirements:\n4+ years of experience in IT, with strong hands-on expertise in Big Data technologies .\nMandatory: Hands-on experience in Python and PySpark .\nExperience working on AWS ecosystem , including EMR, Athena, Glue, Lambda, EC2, S3, and SNS .\nStrong understanding of data modeling and warehousing concepts .\nProficiency in working with structured and semi-structured data\nAbility to optimize Spark jobs for better performance and efficiency .\nWhy Join Us?\nWork on cutting-edge AI & ML projects in a fast-growing technology company.\nBe part of an innovative and collaborative work environment.\nCareer growth opportunities in Big Data, AI, and Cloud technologies .\nCompetitive salary and benefits.\nApplication Process:\nInterested candidates can send their resumes to [email protected] with the subject line:\nApplication for AWS Data Engineer\nWe look forward to welcoming passionate individuals to our team!",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Warehouse', 'GIT', 'Data modeling', 'spark', 'Cloud', 'Manager Technology', 'big data', 'AWS', 'Data warehousing', 'Python']",2025-06-14 05:13:45
Gcp Data Engineer,Estuate Software,6 - 11 years,Not Disclosed,['Hyderabad'],"Design, build,\nJob Title: Data Engineer / Integration Engineer\nJob Summary:\nWe are seeking a highly skilled Data Engineer / Integration Engineer to join our team. The ideal candidate will have expertise in Python, workflow orchestration, cloud platforms (GCP/Google BigQuery), big data frameworks (Apache Spark or similar), API integration, and Oracle EBS. The role involves designing, developing, and maintaining scalable data pipelines, integrating various systems, and ensuring data quality and consistency across platforms. Knowledge of Ascend.io is a plus.\nKey Responsibilities:\nDesign, build, and maintain scalable data pipelines and workflows.\nDevelop and optimize ETL/ELT processes using Python and workflow automation tools.\nImplement and manage data integration between various systems, including APIs and Oracle EBS.\nWork with Google Cloud Platform (GCP) or Google BigQuery (GBQ) for data storage, processing, and analytics.\nUtilize Apache Spark or similar big data frameworks for efficient data processing.\nDevelop robust API integrations for seamless data exchange between applications.\nEnsure data accuracy, consistency, and security across all systems.\nMonitor and troubleshoot data pipelines, identifying and resolving performance issues.\nCollaborate with data analysts, engineers, and business teams to align data solutions with business goals.\nDocument data workflows, processes, and best practices for future reference.\nRequired Skills & Qualifications:\nStrong proficiency in Python for data engineering and workflow automation.\nExperience with workflow orchestration tools (e.g., Apache Airflow, Prefect, or similar).\nHands-on experience with Google Cloud Platform (GCP) or Google BigQuery (GBQ).\nExpertise in big data processing frameworks, such as Apache Spark.\nExperience with API integrations (REST, SOAP, GraphQL) and handling structured/unstructured data.\nStrong problem-solving skills and ability to optimize data pipelines for performance.\nExperience working in an agile environment with CI/CD processes.\nStrong communication and collaboration skills.\nPreferred Skills & Nice-to-Have:\nExperience with Ascend.io platform for data pipeline automation.\nKnowledge of SQL and NoSQL databases.\nFamiliarity with Docker and Kubernetes for containerized workloads.\nExposure to machine learning workflows is a plus.\nWhy Join Us?\nOpportunity to work on cutting-edge data engineering projects.\nCollaborative and dynamic work environment.\nCompetitive compensation and benefits.\nProfessional growth opportunities with exposure to the latest technologies.\n\nHow to Apply:\nInterested candidates can apply by sending their resume to 8892751405 / deekshith.naidu@estuate.com or through Naukri",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Airflow', 'GCP', 'Bigquery', 'SQL']",2025-06-14 05:13:47
Data Platform Engineer,Accenture,7 - 12 years,Not Disclosed,['Bengaluru'],"Project Role :Data Platform Engineer\n\n\n\n\n\nProject Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Platform Engineer, you will assist with the data platform blueprint and design, collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. You will play a crucial role in the development and maintenance of the data platform components, contributing to the overall success of the project.\nRoles & Responsibilities:- Expected to be an SME, collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Assist with the data platform blueprint and design.- Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.- Develop and maintain data platform components.- Contribute to the overall success of the project.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Strong understanding of statistical analysis and machine learning algorithms.- Experience with data visualization tools such as Tableau or Power BI.- Hands-on implementing various machine learning algorithms such as linear regression, logistic regression, decision trees, and clustering algorithms.- Solid grasp of data munging techniques, including data cleaning, transformation, and normalization to ensure data quality and integrity.\nAdditional Information:- The candidate should have a minimum of 7.5 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Bengaluru office.- 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analytics', 'tableau', 'machine learning algorithms', 'statistics', 'data munging', 'python', 'natural language processing', 'power bi', 'machine learning', 'sql', 'data bricks', 'data quality', 'r', 'data modeling', 'data science', 'predictive modeling', 'text mining', 'logistic regression']",2025-06-14 05:13:50
Data Platform Engineer,Accenture,7 - 12 years,Not Disclosed,['Pune'],"Project Role :Data Platform Engineer\n\n\n\n\n\nProject Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Platform Engineer, you will assist with the data platform blueprint and design, collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. You will play a crucial role in the development and maintenance of the data platform components, contributing to the overall success of the project.\nRoles & Responsibilities:- Expected to be an SME, collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Assist with the data platform blueprint and design.- Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.- Develop and maintain data platform components.- Contribute to the overall success of the project.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Strong understanding of statistical analysis and machine learning algorithms.- Experience with data visualization tools such as Tableau or Power BI.- Hands-on implementing various machine learning algorithms such as linear regression, logistic regression, decision trees, and clustering algorithms.- Solid grasp of data munging techniques, including data cleaning, transformation, and normalization to ensure data quality and integrity.\nAdditional Information:- The candidate should have a minimum of 7.5 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Bengaluru office.- 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analytics', 'tableau', 'machine learning algorithms', 'statistics', 'data munging', 'python', 'natural language processing', 'power bi', 'machine learning', 'sql', 'data bricks', 'data quality', 'r', 'data modeling', 'data science', 'predictive modeling', 'text mining', 'logistic regression']",2025-06-14 05:13:52
Lead Data Engineer,Moreyeahs,7 - 9 years,Not Disclosed,['Indore'],"We are seeking a highly skilled and experienced Lead Data Engineer (7+ years) to join our dynamic team. As a Lead Data Engineer, you will play a crucial role in designing, developing, and maintaining our data infrastructure. You will be responsible for ensuring the efficient and reliable collection, storage, and transformation of large-scale data to support business intelligence, analytics, and data-driven decision-making.\n\nKey Responsibilities :\n\nData Architecture & Design :\n- Lead the design and implementation of robust data architectures that support data warehousing (DWH), data integration, and analytics platforms.\n- Develop and maintain ETL (Extract, Transform, Load) pipelines to ensure the efficient processing of large datasets.\n\nETL Development :\n- Design, develop, and optimize ETL processes using tools like Informatica Power Center, Intelligent Data Management Cloud (IDMC), or custom Python scripts.\n- Implement data transformation and cleansing processes to ensure data quality and consistency across the enterprise.\n\nData Warehouse Development :\n- Build and maintain scalable data warehouse solutions using Snowflake, Databricks, Redshift, or similar technologies.\n\n- Ensure efficient storage, retrieval, and processing of structured and semi-structured data.\n\nBig Data & Cloud Technologies :\n- Utilize AWS Glue and PySpark for large-scale data processing and transformation.\n- Implement and manage data pipelines using Apache Airflow for orchestration and scheduling.\n- Leverage cloud platforms (AWS, Azure, GCP) for data storage, processing, and analytics.\n\nData Management & Governance :\n- Establish and enforce data governance and security best practices.\n- Ensure data integrity, accuracy, and availability across all data platforms.\n- Implement monitoring and alerting systems to ensure data pipeline reliability.\n\nCollaboration & Leadership :\n\n- Work closely with data Stewards, analysts, and business stakeholders to understand data requirements and deliver solutions that meet business needs.\n- Mentor and guide junior data engineers, fostering a culture of continuous learning and development within the team.\n- Lead data-related projects from inception to delivery, ensuring alignment with business objectives and timelines.\n\nDatabase Management :\n- Design and manage relational databases (RDBMS) to support transactional and analytical workloads.\n- Optimize SQL queries for performance and scalability across various database platforms.\n\nRequired Skills & Qualifications :\nEducation: Bachelors or Masters degree in Computer Science, Information Systems, Engineering, or a related field.\n\nExperience :\n- Minimum of 7+ years of experience in data engineering, ETL, and data warehouse development.\n- Proven experience with ETL tools like Informatica Power Center or IDMC.\n- Strong proficiency in Python and PySpark for data processing.\n- Experience with cloud-based data platforms such as AWS Glue, Snowflake, Databricks, or Redshift.\n- Hands-on experience with SQL and RDBMS platforms (e.g., Oracle, MySQL, PostgreSQL).\n- Familiarity with data orchestration tools like Apache Airflow.\nTechnical Skills :\n- Advanced knowledge of data warehousing concepts and best practices.\n- Strong understanding of data modeling, schema design, and data governance.\n- Proficiency in designing and implementing scalable ETL pipelines.\n- Experience with cloud infrastructure (AWS, Azure, GCP) for data storage and processing.\n\nSoft Skills :\n- Excellent communication and collaboration skills.\n- Ability to lead and mentor a team of engineers.\n- Strong problem-solving and analytical thinking abilities.\n- Ability to manage multiple projects and prioritize tasks effectively.\n\nPreferred Qualifications :\n- Experience with machine learning workflows and data science tools.\n- Certification in AWS, Snowflake, Databricks, or relevant data engineering technologies.\n- Experience with Agile methodologies and DevOps practices.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data modeling', 'RDBMS', 'MySQL', 'Agile', 'Informatica', 'Oracle', 'Apache', 'Business intelligence', 'Analytics', 'Python']",2025-06-14 05:13:55
Lead Data Engineer,Shyftlabs,5 - 10 years,Not Disclosed,['Noida'],"Position Overview\nWe are looking for an experienced Lead Data Engineer to join our dynamic team. If you are passionate about building scalable software solutions, and work collaboratively with cross-functional teams to define requirements and deliver solutions we would love to hear from you.\nJob Responsibilities:\nDevelop and maintain data pipelines and ETL/ELT processes using Python\nDesign and implement scalable, high-performance applications\nWork collaboratively with cross-functional teams to define requirements and deliver solutions\nDevelop and manage near real-time data streaming solutions using Pub, Sub or Beam.\nContribute to code reviews, architecture discussions, and continuous improvement initiatives\nMonitor and troubleshoot production systems to ensure reliability and performance\nBasic Qualifications:\n5+ years of professional software development experience with Python\nStrong understanding of software engineering best practices (testing, version control, CI/CD)\nExperience building and optimizing ETL/ELT processes and data pipelines\nProficiency with SQL and database concepts\nExperience with data processing frameworks (e.g., Pandas)\nUnderstanding of software design patterns and architectural principles\nAbility to write clean, well-documented, and maintainable code\nExperience with unit testing and test automation\nExperience working with any cloud provider (GCP is preferred)\nExperience with CI/CD pipelines and Infrastructure as code\nExperience with Containerization technologies like Docker or Kubernetes\nBachelors degree in Computer Science, Engineering, or related field (or equivalent experience)\nProven track record of delivering complex software projects\nExcellent problem-solving and analytical thinking skills\nStrong communication skills and ability to work in a collaborative environment\nPreferred Qualifications:\nExperience with GCP services, particularly Cloud Run and Dataflow\nExperience with stream processing technologies (Pub/Sub)\nFamiliarity with big data technologies (Airflow)\nExperience with data visualization tools and libraries\nKnowledge of CI/CD pipelines with Gitlab and infrastructure as code with Terraform\nFamiliarity with platforms like Snowflake, Bigquery or Databricks,.\nGCP Data engineer certification",Industry Type: Management Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Software design', 'Version control', 'Analytical', 'Data processing', 'Unit testing', 'data visualization', 'Continuous improvement', 'SQL', 'Python']",2025-06-14 05:13:57
Data Engineer - AWS & Python,Agilisium,6 - 10 years,Not Disclosed,['Saidapet'],"Key Responsibilities:\nDesign, develop, and optimizedata pipelinesandETL workflowsusingAWS Glue, AWS Lambda, and Apache Spark.\nImplement big data processingsolutions leveraging AWS EMR and AWS Redshift.\nDevelop and maintain data lakes and data warehouses on AWS (S3, Redshift, RDS).\nEnsure data quality, integrity, and governance usingAWS Glue Data Catalog and AWS Lake Formation.",,,,"['Python', 'continuous integration', 'cloud services', 'performance tuning', 'amazon redshift', 'ci/cd', 'kinesis', 'aws lambda', 'sql', 'aws glue', 'database design', 'git', 'data modeling', 'aws cloud', 'spark', 'kafka', 'aws']",2025-06-14 05:13:59
Lead Data Engineer,anblicks,6 - 8 years,Not Disclosed,['Ahmedabad'],"Job Summary:\nWe are seeking a Senior Data Engineer with hands-on experience building scalable data pipelines using Microsoft Fabric. The role focuses on delivering ingestion, transformation, and enrichment workflows across medallion architecture.\nKey Responsibilities:\nDevelop and maintain data pipelines using Microsoft Fabric Data Factory and OneLake.\nDesign and build ingestion and transformation pipelines for structured and unstructured data.\nImplement frameworks for metadata tagging, version control, and batch tracking.\nEnsure security, quality, and compliance of data pipelines.\nContribute to CI/CD integration, observability, and documentation.\nCollaborate with data architects and analysts to meet business requirements.\nQualifications:\n6+ years of experience in data engineering; 2+ years working on Microsoft Fabric or Azure Data services.\nHands-on with tools like Azure Data Factory, Fabric, Databricks, or Synapse.\nStrong SQL and data processing skills (e.g., PySpark, Python).\nExperience with data cataloging, lineage, and governance frameworks.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['metadata', 'data services', 'Version control', 'Compliance', 'Architecture', 'Data processing', 'microsoft', 'SQL', 'Python']",2025-06-14 05:14:02
Lead Data Engineer - Azure,Blend360 India,7 - 12 years,Not Disclosed,['Hyderabad'],"As a Lead Data Engineer, your role is to spearhead the data engineering teams and elevate the team to the next level! You will be responsible for laying out the architecture of the new project as well as selecting the tech stack associated with it. You will plan out the development cycles deploying AGILE if possible and create the foundations for good data stewardship with our new data products!\nYou will also set up a solid code framework that needs to be built to purpose yet have enough flexibility to adapt to new business use cases tough but rewarding challenge!\n\nResponsibilities\nCollaborate with several stakeholders to deeply understand the needs of data practitioners to deliver at scale\nLead Data Engineers to define, build and maintain Data Platform\nWork on building Data Lake in Azure Fabric processing data from multiple sources\nMigrating existing data store from Azure Synapse to Azure Fabric\nImplement data governance and access control\nDrive development effort End-to-End for on-time delivery of high-quality solutions that conform to requirements, conform to the architectural vision, and comply with all applicable standards.\nPresent technical solutions, capabilities, considerations, and features in business terms. Effectively communicate status, issues, and risks in a precise and timely manner.\nFurther develop critical initiatives, such as Data Discovery, Data Lineage and Data Quality\nLeading team and Mentor junior resources\nHelp your team members grow in their role and achieve their career aspirations\nBuild data systems, pipelines, analytical tools and programs\nConduct complex data analysis and report on results\n\n\n7+ Years of Experience as a data engineer or similar role in Azure Synapses, ADF or\nrelevant exp in Azure Fabric\nDegree in Computer Science, Data Science, Mathematics, IT, or similar field\nMust have experience e",Industry Type: Industrial Automation,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Data analysis', 'Data modeling', 'Analytical', 'Agile', 'data governance', 'Data quality', 'Data mining', 'SQL', 'Python']",2025-06-14 05:14:04
Data Platform Engineer,Accenture,5 - 10 years,Not Disclosed,['Pune'],"Project Role :Data Platform Engineer\n\n\n\n\n\nProject Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :A Engineering graduate preferably Computer Science graduate 15 years of full time education\n\n\nSummary:Overall 7+ years of experience In Industry including 4 Years of experience As Developer using Big Data Technologies like Databricks/Spark and Hadoop Ecosystems - Hands on experience on Unified Data Analytics with Databricks, Databricks Workspace User Interface, Managing Databricks Notebooks, Delta Lake with Python, Delta Lake with Spark SQL - Good understanding of Spark Architecture with Databricks, Structured Streaming.\nSetting Up cloud platform with Databricks, Databricks Workspace- Working knowledge on distributed processing, data warehouse concepts, NoSQL, huge amount of data processing, RDBMS, Testing, Data management principles, Data mining and Data modellingAs a Data Platform Engineer, you will be responsible for assisting with the blueprint and design of the data platform components using Databricks Unified Data Analytics Platform. Your typical day will involve collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\nRoles & Responsibilities:- Assist with the blueprint and design of the data platform components using Databricks Unified Data Analytics Platform.- Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.- Develop and maintain data pipelines using Databricks Unified Data Analytics Platform.- Troubleshoot and resolve issues related to data pipelines and data platform components.- Ensure data quality and integrity by implementing data validation and testing procedures.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nExperience with Databricks Unified Data Analytics Platform.- Must To Have\n\n\n\n\nSkills:\nStrong understanding of data modeling and database design principles.- Good To Have\n\n\n\n\nSkills:\nExperience with Apache Spark and Hadoop.- Good To Have\n\n\n\n\nSkills:\nExperience with cloud-based data platforms such as AWS or Azure.- Proficiency in programming languages such as Python or Java.- Experience with data integration and ETL tools such as Apache NiFi or Talend.\nAdditional Information:- The candidate should have a minimum of 5 years of experience in Databricks Unified Data Analytics Platform.- The ideal candidate will possess a strong educational background in computer science, software engineering, or a related field, along with a proven track record of delivering impactful data-driven solutions.- This position is based at our Chennai, Bengaluru, Hyderabad and Pune office.\n\nQualification\n\nA Engineering graduate preferably Computer Science graduate 15 years of full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data analytics', 'database design', 'data modeling', 'software engineering', 'design principles', 'python', 'rdbms', 'data management', 'talend', 'microsoft azure', 'nosql', 'spark programming', 'data bricks', 'data quality', 'java', 'apache nifi', 'spark', 'hadoop', 'big data', 'aws', 'etl', 'data integration']",2025-06-14 05:14:06
Data Engineering Automation Tester,Swits Digital,3 - 6 years,Not Disclosed,['Gurugram'],"Job Title: Data Engineering Automation Tester\n\nExperience: 6+ Years\n\nLocation: Gurgaon, India\n\n\nMandatory Skills:\n\n\n\n\nStrong experience in distributed computing (Spark) and software development.\n\n\n\nProficiency in working with databases (preferably Postgres).\n\n\n\nSolid understanding of Object-Oriented Programming and development principles.\n\n\n\nExperience in Agile development methodologies (Scrum/Kanban).\n\n\n\nHands-on experience with version control tools (preferably Git).\n\n\n\nExposure to CI/CD pipelines.\n\n\n\nStrong background in automated testing including Integration/Delta, Load, and Performance testing.\n\n\n\nExtensive experience in database testing (preferably Postgres).\n\n\n\n\nGood to Have Skills:\n\n\n\n\nExposure to Docker and containerized environments.\n\n\n\nExperience with Spark-Scala.\n\n\n\nKnowledge of Data Engineering principles.\n\n\n\nExperience in Python and .NET Core.\n\n\n\nFamiliarity with Kubernetes.\n\n\n\nExperience with Airflow.\n\n\n\nWorking knowledge of cloud platforms (GCP and Azure).\n\n\n\nExperience with TeamCity CI and Octopus Deploy.",Industry Type: Recruitment / Staffing,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Database testing', 'Automation', 'Automation testing', 'GIT', 'Version control', 'spark', 'Agile development', 'Performance testing', 'Object oriented programming', 'Python']",2025-06-14 05:14:09
"Azure Data Engineering (Synapse, ADF, T-SQL) - Technical Lead",BILVANTIS TECHNOLOGIES,7 - 12 years,Not Disclosed,['Hyderabad'],"We are looking for a highly self-motivated individual with Azure Data Engineering (Synapse, ADF, T-SQL) as a Technical Lead:\nWork Experience:\nExperience should have 8 Years to 12 Years of Azure Data Engineering.\n7+ years of experience in ETL and Data Warehousing development.\nExperience with data modelling and ETL. Strong hands-on experience in ETL process preferably Microsoft technologies.\nKnowledge and experience in Azure Synapse Analytics, Azure Synapse DWH, building pipelines in AZURE Synapse platform and TSQL and Azure Data Factory.\nExperience in data warehouse design and maintenance is plus.\nExperience in agile development processes using Jira and Confluence.\nUnderstanding on the SDLC.\nUnderstanding on the Agile methodologies.\nCommunication with customer and producing the Daily status report.\nShould have good oral and written communication.\nShould be proactive and adaptive.\nSkills and languages:\n\nProficiency in written and spoken English; working knowledge of another UN language would be an asset.\nExpected Deliverables:\nCandidate should help IST in data extraction process from ERP to Datawarehouse.\nStrong knowledge on TSQL and writing Stored procedures.\nShould help building data models in different area of ERP based data sets.\nCandidate should build pipeline to establish integration of data flow among different sources of IT applications.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['T-SQL', 'ERP', 'Agile', 'Technical Lead', 'Stored procedures', 'JIRA', 'Data warehousing', 'SDLC', 'Analytics', 'Data extraction']",2025-06-14 05:14:11
Data Engineer - R&D Precision Medicine,Amgen Inc,1 - 3 years,Not Disclosed,['Hyderabad'],"What you will do\nIn this vital role, you will be responsible for the end-to-end development of an enterprise analytics and data mastering solution using Databricks and Power BI. This role requires expertise in both data architecture and analytics, with the ability to create scalable, reliable, and impactful enterprise solutions that research cohort-building and advanced research pipeline. The ideal candidate will have experience creating and surfacing large unified repositories of human data, based on integrations from multiple repositories and solutions, and be extraordinarily skilled with data analysis and profiling.",,,,"['Data Engineering', 'CDC', 'Power BI', 'Data Analysis', 'ETL', 'ITIL', 'AWS', 'data warehouses']",2025-06-14 05:14:13
DBA DATA Engineer || 12 Lakhs CTC,Robotics Technologies,5 - 10 years,12 Lacs P.A.,['Hyderabad( Banjara hills )'],"Dear Candidate,\nWe are seeking a skilled and experienced DBA Data Engineer to join our growing data team. The ideal candidate will play a key role in designing, implementing, and maintaining our databases and data pipeline architecture. You will collaborate with software engineers, data analysts, and DevOps teams to ensure efficient data flow, data integrity, and optimal database performance across all systems. This role requires a strong foundation in database administration, SQL performance tuning, data modeling, and experience with both on-prem and cloud-based environments.\n\nRequirements:\nBachelors degree in Computer Science, Information Systems, or a related field.\n5+ years of experience in database administration and data engineering.\nProven expertise in RDBMS (Oracle, MySQL, and PostgreSQL) and NoSQL systems (MongoDB, Cassandra).\nExperience managing databases in cloud environments (AWS, Azure, or GCP).\nProficiency in ETL processes and tools (e.g., Apache NiFi, Talend, Informatica, AWS Glue).\nStrong experience with scripting languages such as Python, Bash, or PowerShell.\n\nDBA Data Engineer Roles & Responsibilities:\nDesign and maintain scalable and high-performance database architectures.\nMonitor and optimize database performance using tools like CloudWatch, Oracle Enterprise Manager, pgAdmin, Mongo Compass, or Dynatrace.\nDevelop and manage ETL/ELT pipelines to support business intelligence and analytics.\nEnsure data integrity and security through best practices in backup, recovery, and encryption.\nAutomate regular database maintenance tasks using scripting and scheduled jobs.\nImplement high availability, failover, and disaster recovery strategies.\nConduct performance tuning of queries, stored procedures, indexes, and table structures.\nCollaborate with DevOps to automate database deployments using CI/CD and IaC tools (e.g., Terraform, AWS CloudFormation).\nDesign and implement data models, including star/snowflake schemas for data warehousing.\nDocument data flows, data dictionaries, and database configurations.\nManage user access and security policies using IAM roles or database-native permissions.\nAnalyze existing data systems and propose modernization or migration plans (on-prem to cloud, SQL to NoSQL, etc.).\nUse AWS RDS, Amazon Redshift, Azure SQL Database, or Google BigQuery as needed.\nStay up-to-date with emerging database technologies and make recommendations.\n\nMust-Have Skills:\nDeep knowledge of SQL and database performance tuning.\nHands-on experience with database migrations and replication strategies.\nFamiliarity with data governance, data quality, and compliance frameworks (GDPR, HIPAA, etc.).\nStrong problem-solving and troubleshooting skills.\nExperience with data streaming platforms such as Apache Kafka, AWS Kinesis, or Apache Flink is a plus.\nExperience with data lake and data warehouse architectures.\nExcellent communication and documentation skills.\n\nSoft Skills:\nProblem-Solving: Ability to analyze complex problems and develop effective solutions.\nCommunication Skills: Strong verbal and written communication skills to effectively collaborate with cross-functional teams.\nAnalytical Thinking: Ability to think critically and analytically to solve technical challenges.\nTime Management: Capable of managing multiple tasks and deadlines in a fast-paced environment.\nAdaptability: Ability to quickly learn and adapt to new technologies and methodologies.\n\nInterview Mode : F2F for who are residing in Hyderabad / Zoom for other states\nLocation : 43/A, MLA Colony,Road no 12, Banjara Hills, 500034\nTime : 2 - 4pm",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Database Administration', 'Database Migration', 'Dba Skills', 'High Availability', 'Db Administration', 'Hadr', 'Database Security', 'Disaster Recovery', 'Dr Testing', 'DR', 'Luw', 'Db Upgrade', 'Brtools', 'UDDI', 'Os Migration', 'Dbase', 'DBMS', 'IBM DB2', 'Db Migration', 'Disaster Recovery Planning', 'Db Dba', 'Backup And Recovery']",2025-06-14 05:14:15
Cloud Data Engineer,Luxoft,7 - 10 years,20-25 Lacs P.A.,"['Hyderabad', 'Pune']",Role & responsibilities\n\n\n• Azure Cloud\n• FinOps / Cloud cost efficiencies\n• Azure CosmosDB / SQL\n• Terraform / IaC\n• PowerShell / Bash\n• Linux\n• DevOps skills CI/CD\n• Automation\n• UBS processes / tooling\n• Grid DataSynpase\n\n\nPreferred candidate profile,Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['PowerShell', 'Terraform', 'Linux', 'Azure Cloud', 'Azure CosmosDB', 'DevOps', 'Grid DataSynpase']",2025-06-14 05:14:17
MDM Associate Data Engineer,Amgen Inc,2 - 5 years,Not Disclosed,['Hyderabad'],"We are seeking an MDM Associate Data Engineerwith 25 years of experience to support and enhance our enterprise MDM (Master Data Management) platforms using Informatica/Reltio. This role is critical in delivering high-quality master data solutions across the organization, utilizing modern tools like Databricks and AWS to drive insights and ensure data reliability. The ideal candidate will have strong SQL, data profiling, and experience working with cross-functional teams in a pharma environment.To succeed in this role, the candidate must have strong data engineering experience along with MDM knowledge, hence the candidates having only MDM experience are not eligible for this role. Candidate must have data engineering experience on technologies like (SQL, Python,",,,,"['MDM', 'Informatica MDM', 'SQL queries', 'AWS cloud services', 'Reltio MDM platforms', 'data modeling', 'PySpark', 'IDQ', 'Databricks', 'data stewardship processes', 'JIRA', 'Python']",2025-06-14 05:14:19
MDM Associate Data Engineer,Amgen Inc,1 - 6 years,Not Disclosed,['Hyderabad'],"We are seeking an MDM Associate Data Engineer with 2-- 5 years of experience to support and enhance our enterprise MDM (Master Data Management) platforms using Informatica/Reltio. This role is critical in delivering high-quality master data solutions across the organization, utilizing modern tools like Databricks and AWS to drive insights and ensure data reliability. The ideal candidate will have strong SQL, data profiling, and experience working with cross-functional teams in a pharma environment. To succeed in this role, the candidate must have strong data engineering experience along with MDM knowledge, hence the candidates having only MDM experience are not eligible for this role. Candidate must have data engineering experience on technologies like (SQL, Python, PySpark , Databricks, AWS etc ), along with knowledge of MDM (Master Data Management)",,,,"['Python', 'AZURE', 'data modeling', 'Confluence', 'PySpark', 'MDM', 'IDQ', 'data governance', 'Databricks', 'JIRA', 'AWS', 'SQL']",2025-06-14 05:14:22
Data Engineer (With Testing & Validation Experience),Mcsquared Ai,8 - 13 years,25-30 Lacs P.A.,[],"Job Title: Data Engineer (With Testing & Validation Experience)\nJob Summary:\nWe are looking for a Data Engineer with strong testing experience to join our team. You will design, build, and test data pipelines while ensuring data quality and reliability. The role involves managing the full development lifecycle from design to testing and deployment. Experience with cloud platforms is required; knowledge of Palantir Foundry (Contour, Code Repo, and Data Lineage) is a strong advantage.\nKey Responsibilities:\nBuild and maintain data pipelines using Python, PySpark, and SQL.\nDesign and implement unit tests, regression tests, and validations to ensure data accuracy.\nManage test plans, automation, and execution across the data lifecycle.\nWork closely with analysts, engineers, and business teams to gather requirements and deliver solutions.\nMaintain data quality and consistency across platforms.\nUse AWS for data processing, storage, and integrations.\nDocument work using tools like Confluence and Jira.\nCommunicate testing outcomes and data quality metrics to stakeholders.\nTechnical Skills:\nMust Have:\nPython, PySpark, SQL.\nTesting experience (unit testing, regression testing, QA practices).\nAWS cloud experience.\nStrong problem-solving and analytical skills.\nGood communication and teamwork.\nNice to Have:\nExperience with Palantir Foundry (Contour, Code Repo, Data Lineage).\nFamiliarity with Confluence, Jira, and CI/CD tools.\nExposure to data orchestration and monitoring tools.\nSoft Skills:\nStrong analytical and problem-solving abilities.\nEffective verbal and written communication skills.\nCollaborative mindset with a proactive approach.\nAdaptability to fast-paced and evolving project environments.\nSelf-starter with a high level of accountability and ownership.\nQualifications & Certifications:\nPreferred certifications:\nAWS Certified Data Analytics or Solutions Architect\nPalantir Foundry Certification\nISTQB or equivalent testing certifications",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Pyspark', 'python', 'Data Testing', 'Data Pipeline Developement', 'SQl', 'Data Validation', 'Regression Testing', 'Unit Testing', 'Test Automation', 'Data Quality Management', 'ETL', 'AWS']",2025-06-14 05:14:24
Data Engineering Trainee,Lanxess,1 - 3 years,Not Disclosed,['Thane'],"Contract Type: Regular 12 months\nIf the chemistry is right, we can make a difference at LANXESS: speed up sports, make beverages last longer, add more color to leisure time and much more.\nAs a leading specialty chemicals group, we develop and produce chemical intermediates, additives, specialty chemicals and high-tech plastics. With more than 13,000 employees. Be part of it!\nJob Highlights\nPlease shortly describe the most important, appealing 5 to 8 tasks of this position:",,,,"['Training', 'Engineer Trainee', 'SAP', 'Business reporting', 'data science', 'Finance', 'Javascript', 'Analytics', 'SQL', 'Python']",2025-06-14 05:14:26
AWS Data Engineer,Exavalu,2 - 5 years,Not Disclosed,[],"Exavalu is looking for AWS Data Engineer to join our dynamic team and embark on a rewarding career journey\nBe responsible for the planning, implementation, and growth of the AWS cloud infrastructure\nBuild, release, and manage the configuration of all production systems\nManage a continuous integration and deployment methodology for server-based technologies\nWork alongside architecture and engineering teams to design and implement any scalable software services\nEnsure necessary system security by using best in class cloud security solutionsImplement continuous integration/continuous delivery (CI/CD) pipelines when necessary\nRecommend process and architecture improvements\nTroubleshoot the system and solve problems across all platform and application domains\nOversee pre-production acceptance testing to ensure the high quality of a companys services and products",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent",['AWS Data Engineer'],2025-06-14 05:14:29
Data Engineering Trainee,Lanxess,1 - 3 years,Not Disclosed,['Thane'],"Contract Type: Regular 12 months\nIf the chemistry is right, we can make a difference at LANXESS: speed up sports, make beverages last longer, add more color to leisure time and much more.\nAs a leading specialty chemicals group, we develop and produce chemical intermediates, additives, specialty chemicals and high-tech plastics. With more than 13,000 employees. Be part of it!\nJob Highlights",,,,"['Training', 'Engineer Trainee', 'SAP', 'Business reporting', 'data science', 'Javascript', 'Analytics', 'SQL', 'Python']",2025-06-14 05:14:31
Azure Data Engineer,Arges Global,2 - 5 years,8-18 Lacs P.A.,['Pune( Baner )'],"Scope of Work:\nCollaborate with the lead Business / Data Analyst to gather and analyse business requirements for data processing and reporting solutions.\nMaintain and run existing Python code, ensuring smooth execution and troubleshooting any issues that arise.\nDevelop new features and enhancements for data processing, ingestion, transformation, and report building.\nImplement best coding practices to improve code quality, maintainability, and efficiency.\nWork within Microsoft Fabric to manage data integration, warehousing, and analytics, ensuring optimal performance and reliability.\nSupport and maintain CI/CD workflows using Git-based deployments or other automated deployment tools, preferably in Fabric.\nDevelop complex business rules and logic in Python to meet functional specifications and reporting needs.\nParticipate in an agile development environment, providing feedback, iterating on improvements, and supporting continuous integration and delivery processes.\nRequirements:\nThis person will be an individual contributor responsible for programming, maintenance support, and troubleshooting tasks related to data movement, processing, ingestion, transformation, and report building.\nAdvanced-level Python developer.\nModerate-level experience in working in Microsoft Fabric environment (at least one and preferably two or more client projects in Fabric).\nWell-versed with understanding of modelling, databases, data warehousing, data integration, and technical elements of business intelligence technologies.\nAbility to understand business requirements and translate them into functional specifications for reporting applications.\nExperience in GIT-based deployments or other CI/CD workflow options, preferably in Fabric.\nStrong verbal and written communication skills.\nAbility to perform in an agile environment where continual development is prioritized.\nWorking experience in the financial industry domain and familiarity with financial accounting terms and statements like general ledger, balance sheet, and profit & loss statements would be a plus.\nAbility to create Power BI dashboards, KPI scorecards, and visual reports would be a plus.\nDegree in Computer Science or Information Systems, along with a good understanding of financial terms or working experience in banking/financial institutions, is preferred.",Industry Type: Financial Services (Asset Management),Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Power Bi', 'Microsoft Azure', 'Python', 'Azure Data Factory', 'Microsoft Fabric', 'Azure Databricks', 'Azure Data Lake']",2025-06-14 05:14:33
Python Data Engineer,MNC IT,12 - 14 years,15-18 Lacs P.A.,"['Pune', 'Mumbai (All Areas)']","Bachelor of Engineering or equivalent.\n• Interested in learning new technologies and practices, reuse strategic platforms and standards, evaluate options, and make decisions with long-term sustainability in mind\nMust have\n• Strong communicator, from making presentations to technical writing\n• Experience in financial domain preferably in the areas of Financial accounting and Risk.\n• Strong data analysis and validation experience with attention to detail and understanding patterns\n• Good understanding of Credit Risk, workflow and reporting solutions\n• Have a flair for technology and are adept at leveraging the latest tools and technologies to increase team productivity and collaboration across dispersed teams\n• Experience in Python, SQL and PL/SQL (Oracle) and writing queries, scripts.\n• A real passion for and experience of Agile working practices, with a strong desire to work with baked in quality subject areas such as TDD, BDD, test automation and DevOps principles\n• Experience using DevOps toolsets like GitLab\n• Has prior experience of working on databases and usage of SQL to perform data analysis, preferably cloud technologies\n• Familiar with Azure Native Cloud services, software design and enterprise integration patterns.\n• Experience working with / exposure to NLP, Gen AI, ML and data modelling projects, a plus\n• Has prior experience of working on IT projects and has knowledge and experience of software development life cycle using Agile methodology\n• Is well structured, very reliable and dedicated; has high attention to detail, has ability to handle a significant number of dependencies and issues ensuring nothing is missed out.\n• Has excellent written and verbal communication skills, inter-personal and negotiation skills\n• Ability to work as part of a global team with multiple delivery teams and engage with stakeholders at various levels\n• Able to challenge the status quo and if required have the ability to push-back demands from stakeholders with right justification\n• Someone with a general ability to pick up information quickly and turn around deliverables under pressure\n• A team player with excellent people management skills\n• Takes ownership of tasks assigned to ultimate resolution\n• Accuracy and timeliness of delivering solutions using the best IT standards and practices",Industry Type: IT Services & Consulting,Department: Project & Program Management,"Employment Type: Full Time, Permanent","['PLSQL', 'Devops', 'SQL', 'Azure Native Cloud']",2025-06-14 05:14:35
Azure Data Engineer,Black and white Business Solution,4 - 7 years,10-17 Lacs P.A.,['Pune'],"Hi\nGreeting for the Day!\nWe found your profile suitable for the below opening, kindly go through the JD and reach out to us if you are interested.\nAbout Us\nIncorporated in 2006, We are an 18 year old recruitment and staffing company, we are a provider of manpower for some of the fortune 500 companies for junior/ Middle/ Executive talent.\nAbout Client\nHiring for One of the Most Prestigious Multinational Corporations!\nJob Description\nJob Title :\nAzure Data Engineer\n\nQualification :\nAny Graduate or Above\n\nRelevant Experience :\n4+Yrs\n\nRequired skills:\n\nAzure Databricks\nPython\nPySpark\nSQL\nAZURE Cloud\nPowerBI {Basic+Debug}\n\nLocation :\nPune\n\nCTC Range :\n10-17 (Lakhs Per Annum)\n\nMode of Work :\nHybrid\nJoel.\nIT Staff.\nBlack and White Business solutions PVT Ltd\nBangalore, Karnataka, INDIA.\n8067432416 I joel.manivasan@blackwhite.in I www.blackwhite.in",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'Power Bi', 'Azure Databricks', 'Python']",2025-06-14 05:14:37
GCP Data Engineer,Swits Digital,2 - 7 years,Not Disclosed,['Chennai'],"Job Title: GCP Data Engineer\nLocation: Chennai, India\nJob type: FTE\nMandatory Skills: Google Cloud Platform - Biq Query, Data Flow, Dataproc, Data Fusion, TERRAFORM, Tekton,Cloud SQL, AIRFLOW, POSTGRES, Airflow PySpark, Python, API\nJob Description:\n2+Years in GCP Services, Biq Query, Data Flow, Dataproc, DataPlex,DataFusion, Terraform, Tekton, Cloud SQL, Redis Memory, Airflow, Cloud Storage\n2+ Years inData Transfer Utilities\n2+ Years in Git / any other version control tool\n2+ Years in Confluent Kafka\n1+ Years of Experience in API Development\n2+ Years in Agile Framework\n4+ years of strong experience in python, Pyspark development.\n4+ years of shell scripting to develop the adhoc jobsfor data importing/exporting",Industry Type: Recruitment / Staffing,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Version control', 'GIT', 'GCP', 'Shell scripting', 'query', 'Agile', 'cloud storage', 'SQL', 'Python']",2025-06-14 05:14:40
Azure Data Engineer,Randomtrees,2 - 6 years,Not Disclosed,['Chennai'],"We are seeking a highly skilled and motivated Azure Data Engineer to join our growing data team. In this role, you will be responsible for designing, developing, and maintaining scalable and robust data pipelines and data solutions within the Microsoft Azure ecosystem. You will work closely with data scientists, analysts, and business stakeholders to understand data requirements and translate them into effective data architectures. The ideal candidate will have a strong background in data warehousing, ETL/ELT processes, and a deep understanding of Azure data services.\nResponsibilities:",,,,"['Azure Data Factory', 'Pyspark', 'Azure Databricks', 'SQL']",2025-06-14 05:14:42
Data Engineering Specialist,Sanofi,7 - 11 years,Not Disclosed,['Hyderabad'],"About the job\nWe are seeking an experienced Data Engineering Specialist interested in challenging the status quo to ensure the seamless creation and operation of the data pipelines that are needed by Sanofis advanced analytic, AI and ML initiatives for the betterment of our global patients and customers.\nSanofi has recently embarked into a vast and ambitious digital transformation program. A cornerstone of this roadmap is the acceleration of its data transformation and of the adoption of artificial intelligence (AI) and machine learning (ML) solutions, to accelerate R&D, manufacturing and commercial performance and bring better drugs and vaccines to patients faster, to improve health and save lives\nMain Responsibilities:\nEstablish technical designs to meet Sanofi requirements aligned with the architectural and Data standards\nOwnership of the entire back end of the application, including the design, implementation, test, and troubleshooting of the core application logic, databases, data ingestion and transformation, data processing and orchestration of pipelines, APIs, CI/CD integration and other processes\nFine-tune and optimize queries using Snowflake platform and database techniques\nOptimize ETL/data pipelines to balance performance, functionality, and other operational requirements.\nAssess and resolve data pipeline issues to ensure performance and timeliness of execution\nAssist with technical solution discovery to ensure technical feasibility.\nAssist in setting up and managing CI/CD pipelines and development of automated tests\nDeveloping and managing microservices using python\nConduct peer reviews for quality, consistency, and rigor for production level solution\nDesign application architecture for efficient concurrent user handling, ensuring optimal performance during high usage periods\nOwn all areas of the product lifecycle: design, development, test, deployment, operation, and support\nAbout you\nQualifications:\n5+ years of relevant experience developing backend, integration, data pipelining, and infrastructure\nExpertise in database optimization and performance improvement\nExpertise in Python, PySpark, and Snowpark\nExperience data warehousing and object-relational database (Snowflake and PostgreSQL) and writing efficient SQL queries\nExperience in cloud-based data platforms (Snowflake, AWS)\nProficiency in developing robust, reliable APIs using Python and FastAPI Framework\nExpert in ELT and ETL & experience working with large data sets and performance and query optimization. IICS is a plus\nUnderstanding of data structures and algorithms\nUnderstanding of DBT is a plus\nExperience in modern testing framework (SonarQube, K6 is a plus)\nStrong collaboration skills, willingness to work with others to ensure seamless integration of the server-side and client-side\nKnowledge of DevOps best practices and associated tools is a plus, especially in the setup, configuration, maintenance, and troubleshooting of associated tools:\nContainers and containerization technologies (Kubernetes, Argo, Red Hat OpenShift)\nInfrastructure as code (Terraform)\nMonitoring and Logging (CloudWatch, Grafana)\nCI/CD Pipelines (JFrog Artifactory)\nScripting and automation (Python, GitHub, Github actions)\nExperience with JIRA & Confluence\nWorkflow orchestration (Airflow)\nMessage brokers (RabbitMQ)\nEducation: Bachelor’s degree in computer science, engineering, or similar quantitative field of study\nWhy choose us?\nBring the miracles of science to life alongside a supportive, future-focused team.\nDiscover endless opportunities to grow your talent and drive your career, whether it’s through a promotion or lateral move, at home or internationally.\nEnjoy a thoughtful, well-crafted rewards package that recognizes your contribution and amplifies your impact.\nTake good care of yourself and your family, with a wide range of health and wellbeing benefits including high-quality healthcare, prevention and wellness programs and at least 14 weeks’ gender-neutral parental leave.\nOpportunity to work in an international environment, collaborating with diverse business teams and vendors, working in a dynamic team, and fully empowered to propose and implement innovative ideas.\nPursue Progress. Discover Extraordinary.\nProgress doesn’t happen without people – people from different backgrounds, in different locations, doing different roles, all united by one thing: a desire to make miracles happen. You can be one of those people. Chasing change, embracing new ideas and exploring all the opportunities we have to offer. Let’s pursue progress. And let’s discover extraordinary together.\nAt Sanofi, we provide equal opportunities to all regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, or gender identity.\nWatch our ALL IN video and check out our Diversity Equity and Inclusion actions at sanofi.com!\nLanguages: English is a must",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Engineering', 'Snowflake', 'AWS', 'Python', 'Fast Api']",2025-06-14 05:14:44
Data Engineering Specialist,Investec Global Services,4 - 6 years,Not Disclosed,['Mumbai'],"The Offshore Data Engineering Lead will be responsible for overseeing the data and application development efforts which support our Microsoft Data Mesh Platform. Working as a part of the Investec Central Data Team, the candidate will be responsible for leading development on solutions and applications that support our data domain teams with creation of data products. This role involves driving technical initiatives, exploring new technologies, and enhancing engineering practices within the data teams in-line with the group engineering strategy. The Data Engineering Lead will be a key driver for Investecs move to Microsoft Fabric and other enablement data quality, data management and data orchestration technologies.\nKey Roles and Responsibilities:\nLead the development and implementation of data and custom application solutions that support the creation of data products across various data domains.\nDesign, build, and maintain data pipelines using Microsoft Azure Data Platform, Microsoft Fabric and Databricks technologies.\nEnsure data quality, integrity, and security within the data mesh architecture.\nShare group engineering context with the CIO and engineers within the business unit continuously.\nDrive engineering efficiency and enable teams to deliver high-quality software quickly within the business unit\nCultivate a culture focused on security, risk management, and best practices in engineering\nActively engage with the data domain teams, business units and wider engineering community to promote knowledge sharing\nSpearhead technical projects and innovation within the business units engineering teams and contribute to group engineering initiatives\nAdvance the technical skills of the engineering community and mentor engineers within the business unit\nEnhance the stability, performance, and security of the business units systems.\nDevelop and promote exceptional engineering documentation and practices\nBuild a culture of development and mentorship within the central data team\nProvide guidance on technology and engineering practices\nActively encourages creating Investec open-source software where appropriate within the business unit\nActively encourages team members within the business unit to speak at technical conferences based on the work being done\nCore Skills and Knowledge:\nProven experience in data engineering, with a strong focus on Microsoft Data Platform technologies, including Azure Data Factory, Azure SQL Database, and Databricks\nProficiency in programming languages such as C# and/or Python, with experience in application development being a plus\nExperience with CI/CD pipelines, Azure, and Azure DevOps\nStrong experience and knowledge with PySpark and SQL with the ability to create solutions using Microsoft Fabric\nAbility to create solutions that query and work with web APIs\nIn-depth knowledge of Azure, containerisation, and Kubernetes\nStrong understanding of data architecture concepts, particularly data mesh principles\nExcellent problem-solving skills and the ability to work independently as a self-starter\nStrong communication and collaboration skills, with the ability to work effectively in a remote team environment\nRelevant degree in Computer Science, Data Engineering, or a related field is preferred",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Data management', 'Agile', 'Engineering Manager', 'Application development', 'microsoft', 'Open source', 'Risk management', 'SQL', 'Python']",2025-06-14 05:14:46
Principal Data Engineer,Horizon Therapeutics,5 - 18 years,Not Disclosed,['Hyderabad'],"Career Category Engineering Job Description\nJoin Amgen s Mission of Serving Patients\nAt Amgen, if you feel like you re part of something bigger, it s because you are. Our shared mission to serve patients living with serious illnesses drives all that we do.\nSince 1980, we ve helped pioneer the world of biotech in our fight against the world s toughest diseases. With our focus on four therapeutic areas -Oncology, Inflammation, General Medicine, and Rare Disease- we reach millions of patients each year. As a member of the Amgen team, you ll help make a lasting impact on the lives of patients as we research, manufacture, and deliver innovative medicines to help people live longer, fuller happier lives.\nOur award-winning culture is collaborative, innovative, and science based. If you have a passion for challenges and the opportunities that lay within them, you ll thrive as part of the Amgen team. Join us and transform the lives of patients while transforming your career.\nWhat you will do\nLet s do this. Let s change the world. In this vital role you will manage and oversee the development of robust Data Architectures, Frameworks, Data product Solutions, while mentoring and guiding a small team of data engineers. You will be responsible for leading the development, implementation, and management of enterprise-level data data engineering frameworks and solutions that support the organizations data-driven strategic initiatives. You will continuously strive for innovation in the technologies and practices used for data engineering and build enterprise scale data frameworks and expert data engineers. This role will closely collaborate with counterparts in US and EU. You will collaborate with cross-functional teams, including platform, functional IT, and business stakeholders, to ensure that the solutions that are built align with business goals and are scalable, secure, and efficient.\nRoles Responsibilities:\nArchitect Implement of scalable, high-performance Modern Data Engineering solutions (applications) that include data analysis, data ingestion, storage, data transformation (data pipelines), and analytics.\nEvaluate the new trends in data engineering area and build rapid prototypes\nBuild Data Solution Architectures and Frameworks to accelerate the Data Engineering processes\nBuild frameworks to improve the re-usability, reduce the development time and cost of data management governance\nIntegrate AI into data engineering practices to bring efficiency through automation\nBuild best practices in Data Engineering capability and ensure their adoption across the product teams\nBuild and nurture strong relationships with stakeholders, emphasizing value-focused engagement and partnership to align data initiatives with broader business goals.\nLead and motivate a high-performing data engineering team to deliver exceptional results.\nProvide expert guidance and mentorship to the data engineering team, fostering a culture of innovation and best practices.\nCollaborate with counterparts in US and EU and work with business functions, functional IT teams, and others to understand their data needs and ensure the solutions meet the requirements.\nEngage with business stakeholders to understand their needs and priorities, ensuring that data and analytics solutions built deliver real value and meet business objectives.\nDrive adoption of the data and analytics solutions by partnering with the business stakeholders and functional IT teams in rolling out change management, trainings, communications, etc.\nTalent Growth People Leadership: Lead, mentor, and manage a high-performing team of engineers, fostering an environment that encourages learning, collaboration, and innovation. Focus on nurturing future leaders and providing growth opportunities through coaching, training, and mentorship.\nRecruitment Team Expansion: Develop a comprehensive talent strategy that includes recruitment, retention, onboarding, and career development and build a diverse and inclusive team that drives innovation, aligns with Amgens culture and values, and delivers business priorities\nOrganizational Leadership: Work closely with senior leaders within the function and across the Amgen India site to align engineering goals with broader organizational objectives and demonstrate leadership by contributing to strategic discussions\nWhat we expect of you\nWe are all different, yet we all use our unique contributions to serve patients. The [vital attribute] professional we seek is a [type of person] with these qualifications.\nBasic Qualifications:\nMaster s degree and 8 to 10 years of computer science and engineering preferred, other Engineering fields will be considered OR\nBachelor s degree and 12 to 14 years of computer science and engineering preferred, other Engineering fields will be considered OR\nDiploma and 16 to 18 years of computer science and engineering preferred, other Engineering fields will be considered\n10+ years of experience in Data Engineering, working in COE development or product building\n5+ years of experience in leading enterprise scale data engineering solution development.\nExperience building enterprise scale data lake, data fabric solutions on cloud leveraging modern approaches like Data Mesh\nDemonstrated proficiency in leveraging cloud platforms (AWS, Azure, GCP) for data engineering solutions. Strong understanding of cloud architecture principles and cost optimization strategies.\nHands-on experience using Databricks, Snowflake, PySpark, Python, SQL\nProven ability to lead and develop high-performing data engineering teams.\nStrong problem-solving, analytical, and critical thinking skills to address complex data challenges.\nPreferred Qualifications:\nExperience in Integrating AI with Data Engineering and building AI ready data lakes\nPrior experience in data modeling especially star-schema modeling concepts.\nFamiliarity with ontologies, information modeling, and graph databases.\nExperience working with agile development methodologies such as Scaled Agile.\nExperienced with software engineering best-practices, including but not limited to version control (Git, Subversion, etc.), CI/CD (Jenkins, Maven etc.), automated unit testing, and Dev Ops.\nEducation and Professional Certifications\nSAFe for Teams certification (preferred)\nDatabricks certifications\nAWS cloud certification\nSoft Skills:\nExcellent analytical and troubleshooting skills.\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nTeam-oriented, with a focus on achieving team goals\nStrong presentation and public speaking skills.\nWhat you can expect of us\nAs we work to develop treatments that take care of others, we also work to care for your professional and personal growth and well-being. From our competitive benefits to our collaborative culture, we ll support your journey every step of the way.\nIn addition to the base salary, Amgen offers competitive and comprehensive Total Rewards Plans that are aligned with local industry standards.\nApply now\nfor a career that defies imagination\nObjects in your future are closer than they appear. Join us.\ncareers.amgen.com\nAs an organization dedicated to improving the quality of life for people around the world, Amgen fosters an inclusive environment of diverse, ethical, committed and highly accomplished people who respect each other and live the Amgen values to continue advancing science to serve patients. Together, we compete in the fight against serious disease.\nAmgen is an Equal Opportunity employer and will consider all qualified applicants for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, disability status, or any other basis protected by applicable law.\nWe will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation.\n.",Industry Type: Biotechnology,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Automation', 'Data analysis', 'Data management', 'Analytical', 'Troubleshooting', 'Analytics', 'SQL', 'Python', 'Recruitment']",2025-06-14 05:14:48
Azure Data Engineer,SoulPage IT,3 - 8 years,Not Disclosed,['Hyderabad'],"Azure Data Engineer - Soulpage IT Solutions\nHome\nAzure Data Engineer\nMarch 6, 2025\nPosition: Azure Data Engineer\nSkill set: Azure Databricks and Data Lake implementation\nExperience: 3+ years\nNotice Period: Immediate Immediate to 15 days\nLocation: WFO, Hyderabad\nJob Type : Full-Time\nPositions: 2\nJob Summary:\nWe are looking for a highly skilled Azure Data Engineer with expertise in Azure Databricks and Data Lake implementation to design, develop, and optimize our data pipelines. The engineer will be responsible for integrating data from multiple sources, ensuring data is cleaned, standardized, and normalized for ML model building. This role involves working closely with stakeholders to understand data requirements and ensuring seamless data flow across different platforms.\nKey Responsibilities: Data Lake & Pipeline Development\nDesign, develop, and implement scalable Azure Data Lake solutions .\nBuild robust ETL/ELT pipelines using Azure Databricks, Data Factory, and Synapse Analytics .\nOptimize data ingestion and processing from multiple structured and unstructured sources.\nImplement data cleaning, standardization, and normalization processes to ensure high data quality.\nImplement best practices for data governance, security, and compliance .\nOptimize data storage and retrieval for performance and cost-efficiency.\nMonitor and troubleshoot data pipelines, ensuring minimal downtime.\nWork closely with data scientists, analysts, and business stakeholders to define data needs.\nMaintain thorough documentation for data pipelines, transformations, and integrations.\nAssist in developing ML-ready datasets by ensuring consistency across integrated data sources.\nRequired Skills & Qualifications:\n3+ years of experience in data engineering, with a focus on Azure cloud technologies .\nExpertise in Azure Databricks, Data Factory, Data Lake\nStrong proficiency in Python, SQL, and PySpark for data processing and transformations.\nUnderstanding of ML data preparation workflows , including feature engineering and data normalization.\nKnowledge of data security and governance principles .\nExperience in optimizing ETL pipelines for scalability and performance.\nStrong analytical and problem-solving skills.\nExcellent written and verbal communication skills.\nPreferred Qualifications:\nAzure Certifications Azure Data Engineer Associate, Azure Solutions Architect .\nWhy Join Us?\nWork on cutting-edge Azure cloud and data technologies .\nCollaborate with a dynamic and innovative team solving complex data challenges.\nCompetitive compensation and career growth opportunities.\nApplication Process:\nInterested candidates can send their resumes to [email protected] with the subject line:\nApplication for Azure Data Engineer\nWe look forward to welcoming passionate individuals to our team!",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data security', 'Analytical', 'Cloud', 'data governance', 'Data processing', 'Data quality', 'Analytics', 'Solution Architect', 'SQL', 'Python']",2025-06-14 05:14:51
Azure Data Engineer,Addnectar Solutions,6 - 10 years,Not Disclosed,"['Chennai', 'Coimbatore', 'Mumbai (All Areas)']","We have an urgent requirement for\nRole: (Senior Azure Data Engineer)\n\nExperience: 6 years.\n\nNotice Period: 0-15 days Max\n\nPosition: C2H\n\nShould be able to work in Flexible timing.\n\nCommunication should be excellent.\n\nMust Have:\nStrong understanding of ADF, Azure, Databricks, PySpark,\nStrong understanding of SQL, ADO, PowerBI, Unity Catalog is mandatory",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Part Time, Temporary/Contractual","['Pyspark', 'Azure', 'ADF', 'PowerBI', 'SQL', 'Azure Data Factory', 'Databricks']",2025-06-14 05:14:53
Azure Data Engineer (6 plus years) @ chennai,Changepond,6 - 11 years,Not Disclosed,['Chennai'],"Role & responsibilities\nData Engineer, with working on data migration projects.\nExperience with Azure data stack, including Data Lake Storage, Synapse Analytics, ADF, Azure Databricks, and Azure ML.\nSolid knowledge of Python, PySpark and other Python packages\nFamiliarity with ML workflows and collaboration with data science teams.\nStrong understanding of data governance, security, and compliance in financial domains.",,,,"['Azure Data Factory', 'Azure Databricks', 'Python', 'Pyspark', 'Data Engineering', 'Migration', 'Synapse Analytics', 'Machine Learning']",2025-06-14 05:14:56
Aws Data Engineer,Astrosoft Technologies,2 - 4 years,6.5-10 Lacs P.A.,['Hyderabad( Gachibowli )'],"Company: AstroSoft Technologies (https://www.astrosofttech.com/)\nAstrosoft is an award-winning company that specializes in the areas of Data, Analytics, Cloud, AI/ML, Innovation, Digital. We have a customer first mindset and take extreme ownership in delivering solutions and projects for our customers and have consistently been recognized by our clients as the premium partner to work with. We bring to bear top tier talent, a robust and structured project execution framework, our significant experience over the years and have an impeccable record in delivering solutions and projects for our clients.\nFounded in 2004, Headquarters in FL,USA, Corporate Office - India, Hyderabad\nBenefits from Astrosoft Technologies\nH1B Sponsorship (Depends on Project & Performance)\nLunch & Dinner (Every day)\nHealth Insurance Coverage- Group\nIndustry Standards Leave Policy\nSkill Enhancement Certification\nHybrid Mode\nRole & responsibilities\nJob Title: AWS Engineer\nRequired Skills:\nMinimum of 2+ years direct experience with AWS Data Engineer\nStrong Experience in AWS Services like Redshift & ETL Glue, Spark, Python, Lambda,Kafka,S3, EMR Etc experience is a must.\nMonitoring tools - Cloudwatch\nExperience in Development & Support Projects as well.\nStrong verbal and written communication skills\nStrong experience and understanding of streaming architecture and development practices using kafka, spark, flink etc,\nStrong AWS development experience using S3, SNS, SQS, MWAA (Airflow) Glue, DMS and EMR.\nStrong knowledge of one or more programing languages Python/Java/Scala (ideally Python)\nExperience using Terraform to build IAC components in AWS.\nStrong experience with ETL Tools in AWS; ODI experience is as plus.\nStrong experience with Database Platforms: Oracle, AWS Redshift\nStrong experience in SQL tuning, tuning ETL solutions, physical optimization of databases.\nVery familiar with SRE concepts which includes evaluating and implementing monitoring and observability tools like Splunk, Data Dog, CloudWatch and other job, log or dashboard concepts for customer support and application health checks.\nAbility to collaborate with our business partners to understand and implement their requirements.\nExcellent interpersonal skills and be able to build consensus across teams.\nStrong critical thinking and ability to think out-of-the box.\nSelf-motivated and able to perform under pressure.\nAWS certified (preferred)\nQualifications:\nEducation: Bachelor's degree in Computer Science, Information Technology, or a related field (or equivalent work experience).\nExperience:\nProven experience as an AWS Support Engineer or in a similar role.\nHands-on experience with a wide range of AWS services (e.g., EC2, S3, RDS, Lambda, CloudFormation, IAM).\nSoft Skills:\nExcellent communication and interpersonal skills.\nStrong problem-solving and analytical skills.\nAbility to work independently and as part of a team.\nCustomer-focused mindset with a commitment to delivering high-quality support.\nWhat We Offer:\nCompetitive salary and benefits package.\nOpportunities for professional growth and development.\nA collaborative and supportive work environment.\nAccess to the latest AWS technologies and training resources.\nIf you are passionate about cloud technology and enjoy helping customers solve complex technical challenges, we would love to hear from you!\nAcknowledge the mail with your updated cv,\nJulekha.Gousiya@astrosofttech.com\n\n\n\nDetails As we discussed, Please revert with your acknowledgment.\nTotal Experience-\nAws\nRedshift\nGlue-\nCurrent Location-\nCurrent Company-\nC-CTC-\nEx-CTC –\nOffer –\nNP –\nReady to Relocate Hyderabad (Y/N) – Yes\n(Hybrid) –(Y/N) - Yes",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['AWS', 'Glue', 'redshift', 'Spark']",2025-06-14 05:14:58
DBA DATA Engineer || 12 Lakhs CTC,Robotics Technologies,6 - 10 years,12 Lacs P.A.,['Hyderabad( Banjara hills )'],"Dear Candidate,\nWe are seeking a skilled and experienced DBA Data Engineer to join our growing data team. The ideal candidate will play a key role in designing, implementing, and maintaining our databases and data pipeline architecture. You will collaborate with software engineers, data analysts, and DevOps teams to ensure efficient data flow, data integrity, and optimal database performance across all systems. This role requires a strong foundation in database administration, SQL performance tuning, data modeling, and experience with both on-prem and cloud-based environments.\n\nRequirements:\nBachelors degree in Computer Science, Information Systems, or a related field.\n6+ years of experience in database administration and data engineering.\nProven expertise in RDBMS (Oracle, MySQL, and PostgreSQL) and NoSQL systems (MongoDB, Cassandra).\nExperience managing databases in cloud environments (AWS, Azure, or GCP).\nProficiency in ETL processes and tools (e.g., Apache NiFi, Talend, Informatica, AWS Glue).\nStrong experience with scripting languages such as Python, Bash, or PowerShell.\n\nDBA Data Engineer Roles & Responsibilities:\nDesign and maintain scalable and high-performance database architectures.\nMonitor and optimize database performance using tools like CloudWatch, Oracle Enterprise Manager, pgAdmin, Mongo Compass, or Dynatrace.\nDevelop and manage ETL/ELT pipelines to support business intelligence and analytics.\nEnsure data integrity and security through best practices in backup, recovery, and encryption.\nAutomate regular database maintenance tasks using scripting and scheduled jobs.\nImplement high availability, failover, and disaster recovery strategies.\nConduct performance tuning of queries, stored procedures, indexes, and table structures.\nCollaborate with DevOps to automate database deployments using CI/CD and IaC tools (e.g., Terraform, AWS CloudFormation).\nDesign and implement data models, including star/snowflake schemas for data warehousing.\nDocument data flows, data dictionaries, and database configurations.\nManage user access and security policies using IAM roles or database-native permissions.\nAnalyze existing data systems and propose modernization or migration plans (on-prem to cloud, SQL to NoSQL, etc.).\nUse AWS RDS, Amazon Redshift, Azure SQL Database, or Google BigQuery as needed.\nStay up-to-date with emerging database technologies and make recommendations.\n\nMust-Have Skills:\nDeep knowledge of SQL and database performance tuning.\nHands-on experience with database migrations and replication strategies.\nFamiliarity with data governance, data quality, and compliance frameworks (GDPR, HIPAA, etc.).\nStrong problem-solving and troubleshooting skills.\nExperience with data streaming platforms such as Apache Kafka, AWS Kinesis, or Apache Flink is a plus.\nExperience with data lake and data warehouse architectures.\nExcellent communication and documentation skills.\n\nSoft Skills:\nProblem-Solving: Ability to analyze complex problems and develop effective solutions.\nCommunication Skills: Strong verbal and written communication skills to effectively collaborate with cross-functional teams.\nAnalytical Thinking: Ability to think critically and analytically to solve technical challenges.\nTime Management: Capable of managing multiple tasks and deadlines in a fast-paced environment.\nAdaptability: Ability to quickly learn and adapt to new technologies and methodologies.\n\nInterview Mode : F2F for who are residing in Hyderabad / Zoom for other states\nLocation : 43/A, MLA Colony,Road no 12, Banjara Hills, 500034\nTime : 2 - 4pm",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Database Administration', 'Database Migration', 'Dba Skills', 'High Availability', 'Db Administration', 'Hadr', 'Database Security', 'Disaster Recovery', 'Dr Testing', 'DR', 'Luw', 'Db Upgrade', 'Brtools', 'UDDI', 'Os Migration', 'Dbase', 'DBMS', 'IBM DB2', 'Db Migration', 'Disaster Recovery Planning', 'Db Dba', 'Backup And Recovery']",2025-06-14 05:15:00
Quality Engineer-Data,IBM,2 - 5 years,Not Disclosed,['Kochi'],The ability to be a team player\nThe ability and skill to train other people in procedural and technical topics\nStrong communication and collaboration skills\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\n\n\nPreferred technical and professional experience,,,,"['azure databricks', 'sql queries', 'sql', 'data quality', 'stakeholder management', 'hive', 'python', 'azure data lake', 'scala', 'pyspark', 'microsoft azure', 'data warehousing', 'power bi', 'azure data factory', 'sql server', 'tableau', 'sql azure', 'spark', 'hadoop', 'big data', 'etl', 'ssis']",2025-06-14 05:15:03
Snowflake Data Engineer,Prudent Globaltech Solutions,7 - 12 years,16-27.5 Lacs P.A.,['Hyderabad( Madhapur )'],"Job Description Data Engineer\nWe are seeking a highly skilled Data Engineer with extensive experience in Snowflake, Data Build Tool (dbt), Snaplogic, SQL Server, PostgreSQL, Azure Data Factory, and other ETL tools. The ideal candidate will have a strong ability to optimize SQL queries and a good working knowledge of Python. A positive attitude and excellent teamwork skills are essential.\n\nRole & responsibilities\nData Pipeline Development: Design, develop, and maintain scalable data pipelines using Snowflake, DBT, Snaplogic, and ETL tools.",,,,"['Snowflake', 'Azure Data Factory', 'ADF', 'Data Engineer', 'ETL', 'SQL']",2025-06-14 05:15:05
Lead Infrastructure Engineer - Data Platforms,General Mills,6 - 10 years,Not Disclosed,['Mumbai'],"Position Title\nLead Infrastructure Engineer - Data Platforms\nFunction/Group\nDigital and Technology\nLocation\nMumbai\nShift Timing\nGeneral\nRole Reports to\nDT Manager Cloud Data Platforms\nRemote/Hybrid/in-Office\nHybrid\nABOUT GENERAL MILLS\nWe make food the world loves: 100 brands. In 100 countries. Across six continents. With iconic brands like Cheerios, Pillsbury, Betty Crocker, Nature Valley, and H agen-Dazs, we ve been serving up food the world loves for 155 years (and counting). Each of our brands has a unique story to tell.\nHow we make our food is as important as the food we make. Our values are baked into our legacy and continue to accelerate\nus into the future as an innovative force for good. General Mills was founded in 1866 when Cadwallader Washburn boldly bought the largest flour mill west of the Mississippi. That pioneering spirit lives on today through our leadership team who upholds a vision of relentless innovation while being a force for good. For more details check out http://www.generalmills.com\nGeneral Mills India Center (GIC) is our global capability center in Mumbai that works as an extension of our global organization delivering business value, service excellence and growth, while standing for good for our planet and people.\nWith our team of 1800+ professionals, we deliver superior value across the areas of Supply chain (SC) , Digital Technology (DT) Innovation, Technology Quality (ITQ), Consumer and Market Intelligence (CMI), Sales Strategy Intelligence (SSI) , Global Shared Services (GSS) , Finance Shared Services (FSS) and Human Resources Shared Services (HRSS).For more details check out https://www.generalmills.co.in\nWe advocate for advancing equity and inclusion to create more equitable workplaces and a better tomorrow.\nJOB OVERVIEW\nFunction Overview\nThe Digital and Technology team at General Mills stands as the largest and foremost unit, dedicated to exploring the latest trends and innovations in technology while leading the adoption of cutting-edge technologies across the organization. Collaborating closely with global business teams, the focus is on understanding business models and identifying opportunities to leverage technology for increased efficiency and disruption. The teams expertise spans a wide range of areas, including AI/ML, Data Science, IoT, NLP, Cloud, Infrastructure, RPA and Automation, Digital Transformation, Cyber Security, Blockchain, SAP S4 HANA and Enterprise Architecture. The MillsWorks initiative embodies an agile@scale delivery model, where business and technology teams operate cohesively in pods with a unified mission to deliver value for the company. Employees working on significant technology projects are recognized as Digital Transformation change agents.\nThe team places a strong emphasis on service partnerships and employee engagement with a commitment to advancing equity and supporting communities. In fostering an inclusive culture, the team values individuals passionate about learning and growing with technology, exemplified by the Work with Heartphilosophy, emphasizing results over facetime. Those intrigued by the prospect of contributing to the digital transformation journey of a Fortune 500 company are encouraged to explore more details about the function through the provided Link\nPurpose of the role\nThe Digital and Technology team of General Mills India Centre is looking for a MS SQL Server / Cloud database administrator (DBA) with Operations-oriented/DevOps skill set to support Cloud/On Prem databases. This opportunity is in a fast-paced environment as we migrate and refactor enterprise-scale databases from our on-premises datacenters to a cloud environment. The ideal candidate will have experience operating in a fast-paced, complex, and multi-platform environment and will contribute to the strategic direction of our database infrastructure.\nKEY ACCOUNTABILITIES\nProvide technical leadership for migrating existing Microsoft SQL Server and NoSQL Databases to Public Cloud, developing and owning migration processes, documentation, and tooling. This includes defining strategies and roadmaps for database migrations, ensuring alignment with overall IT strategy and leveraging automation wherever possible to streamline the process.\nExpertly administer and manage mission-critical, complex, and high-volume Database Platforms in a 24/7 environment, implementing and maintaining automated monitoring and alerting systems to proactively identify and address potential issues.\nProactively identify and address potential database performance bottlenecks, proposing and implementing automated solutions to optimize database efficiency and scalability. This includes developing and deploying automated scripts for performance tuning and optimization.\nLead the design and implementation of highly available and scalable database solutions in the cloud (GCP preferred), ensuring compliance with security and governance standards and utilizing Infrastructure as Code (IaC) for automated provisioning and management of database infrastructure.\nAdminister and troubleshoot SQL Server, PostgreSQL, MySQL, and NoSQL DBs (MongoDB), implementing best practices and resolving complex performance issues through automated scripting and tooling.\nDevelop and maintain comprehensive documentation for database systems, processes, and procedures.\nChampion the adoption of DevOps practices, including CI/CD, infrastructure as code (Terraform), and automation (Ansible, Python, PowerShell) to streamline database management and deployment. Actively participate in the development and improvement of automated deployment pipelines.\nCollaborate with application stakeholders to understand their database requirements and provide technical guidance on database design and optimization, emphasizing automation opportunities to improve development workflows.\nContribute to the development and implementation of database security policies and procedures, ensuring compliance with industry best practices and leveraging automation for security auditing and vulnerability management.\nActively participate in Agile development processes, contributing to sprint planning, daily stand-ups, and retrospectives, focusing on automation opportunities to improve team efficiency and reduce manual effort.\nMentor and guide junior team members, fostering a culture of knowledge sharing and continuous learning, particularly around automation and DevOps practices.\nStay abreast of emerging technologies and trends in database administration and cloud computing, recommending and implementing innovative solutions to improve database performance and reliability, with a focus on automation and efficiency gains.\nMINIMUM QUALIFICATIONS\n9+ years of hands-on experience with leading design, refactoring, or migration of databases in cloud infrastructures and services for at least one of Microsoft Azure, Amazon Web Services, and Google GCP (preferred). Experience with automating database migration processes is highly desirable.\nExperience maintaining and administering with CI/CD tools such as Ansible, GitHub, and Artifactory in a cloud environment and developing/writing scripts using advanced DevOps languages such as Python, PowerShell. Demonstrated ability to design and implement automated workflows.\nExperience working with infrastructure as code such as Terraform, or equivalent. Proven ability to automate infrastructure provisioning and management.\nExperience with concepts, processes tools required for cloud adoption including cloud security, governance, and integration. Experience with automating security and compliance tasks is a plus.\nExperience with SQL Server AlwaysOn and Windows clustering. Experience with automating the management of high-availability clusters is preferred.\nExperience with agile techniques and methods.\nFamiliarity with user expectations for cloud Databases (to be able to design user-centric engineering services e.g., provisioning self-service workflow). Experience with automating user provisioning and access management is a plus.\nWorking knowledge of DevOps, Agile development processes, exploration, and POCs.\nAbility to work collaboratively across functional team boundaries.\nPREFERRED QUALIFICATIONS\nGood understanding hands-on experience of Linux OS and Windows Server (2012+).\nExperience working with high availability, disaster recovery, backup strategies, and server tuning strategies including parameters, resources, contention, etc.\nExcellent interpersonal and communication skills.\nAbility to work in a fast-paced team environment.\nFlexibility, reliability, initiative, responsibility, and a can-domentality.",Industry Type: Food Processing,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Automation', 'MS SQL', 'SAP', 'Linux', 'MySQL', 'Infrastructure', 'Workflow', 'Windows', 'SQL', 'Python']",2025-06-14 05:15:07
Software Engineer Data Privacy,Bajaj Finserv Health,4 - 6 years,Not Disclosed,['Pune( Viman Nagar )'],"Job Description:\nKnowledgeable and proactive Data Privacy engineer to manage privacy and data protection initiatives in compliance with Indian data privacy regulations, including the Digital Personal Data Protection Act, 2023 (DPDP Act).\nThis role will work closely with legal, IT, security, HR, and business teams to implement privacy by design and foster a strong culture of privacy data protection\n\nKey Objectives/Responsibilities of this Role:\nImplement and oversee the data privacy program implementation in accordance with the Privacy Laws and requirements (DPDP Act, 2023 and other applicable Indian laws)\nDocument and maintain records of processing activities\nDraft, review, and update privacy notices, policies, consent mechanisms, and contractual clauses for data processing.\nConduct and review Data Protection Impact Assessments (DPIAs) and help identify and mitigate privacy risks.\nTrain and educate employees on Data privacy requirements and best practices.\nPerform regular compliance audits and assessments across internal departments and third-party vendors.\nDevelop and deliver training and awareness programs to ensure employees understand their responsibilities under the DPDP Act and internal privacy policies.\nHandle and ensure timely response to data principal rights requests (access, correction, erasure, grievance redressal) as per Privacy Law requirements\nCollaborate with legal and IT/security teams to ensure privacy by design and default in new processes, technologies, and products.\nStay updated with latest developments in data protection laws and regulations.\nSupport incident response and breach notification processes as per regulatory timelines and requirements.\nAssist the Data Privacy Officer in preparing reports for internal leadership or regulators, as required.\nServe as a subject matter expert on Indian data privacy regulations and provide guidance to cross-functional teams.\n\nMandatory Skillset & Experience:\nMinimum 4+ years of relevant experience in information security, compliance, or legal roles with focus on data privacy\nIn-depth knowledge of the Data Protection and Privacy Regulations, and awareness of related Indian IT regulations.\nExperienced in using Data protection management tools and softwares\nUnderstanding of global data privacy frameworks (GDPR, etc.) is a plus.\nStrong analytical and problem-solving skills with a practical approach to risk and compliance.\nAbility to communicate complex privacy topics in a clear and business-friendly manner.\nExperience working with privacy compliance tools and risk management systems.\n\nBehavioral Skills:\nSelf-driven and proactive.\nPlan and execute projects in a timely fashion meeting the project timelines\nExperienced in coordinating activities across different teams and stakeholders\nContinuous process improvement / quality assurance experience is a plus\nProactively identify potential data privacy issues and problems in business processes\n\nPreferred Qualification:\nBachelors or Masters degree in law, Technology, Compliance, Risk Management, or a related field.\n\nPreferred Certifications:\nDCPP (Data Protection Certified Professional - India)\nCIPP/A, CIPM (IAPP certifications)\nISO/IEC 27701, ISO 27001 (Good to have)",Industry Type: Medical Services / Hospital,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Data Protection Manager', 'Privacy Regulation', 'Data Privacy Law', 'Data Privacy', 'Gdpr']",2025-06-14 05:15:10
Cloud Data Platform Engineer - C,Capgemini,3 - 7 years,Not Disclosed,['Mumbai'],"\nA Data Platform Engineer specialises in the design, build, and maintenance of cloud-based data infrastructure and platforms for data-intensive applications and services. They develop Infrastructure as Code and manage the foundational systems and tools for efficient data storage, processing, and management. This role involves architecting robust and scalable cloud data infrastructure, including selecting and implementing suitable storage solutions, data processing frameworks, and data orchestration tools. Additionally, a Data Platform Engineer ensures the continuous evolution of the data platform to meet changing data needs and leverage technological advancements, while maintaining high levels of data security, availability, and performance. They are also tasked with creating and managing processes and tools that enhance operational efficiency, including optimising data flow and ensuring seamless data integration, all of which are essential for enabling developers to build, deploy, and operate data-centric applications efficiently.\n\n - Grade Specific \nAn expert on the principles and practices associated with data platform engineering, particularly within cloud environments, and demonstrates proficiency in specific technical areas related to cloud-based data infrastructure, automation, and scalability.Key responsibilities encompass:Team Leadership and ManagementSupervising a team of platform engineers, with a focus on team dynamics and the efficient delivery of cloud platform solutions.Technical Guidance and Decision-MakingProviding technical leadership and making pivotal decisions concerning platform architecture, tools, and processes. Balancing hands-on involvement with strategic oversight.Mentorship and Skill DevelopmentGuiding team members through mentorship, enhancing their technical proficiencies, and nurturing a culture of continual learning and innovation in platform engineering practices.In-Depth Technical ProficiencyPossessing a comprehensive understanding of platform engineering principles and practices, and demonstrating expertise in crucial technical areas such as cloud services, automation, and system architecture.Community ContributionMaking significant contributions to the development of the platform engineering community, staying informed about emerging trends, and applying this knowledge to drive enhancements in capability.\n\n Skills (competencies)",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['cloud services', 'cloud platform', 'cloud data flow', 'system architecture', 'data flow', 'hive', 'snowflake', 'python', 'airflow', 'microsoft azure', 'data warehousing', 'data engineering', 'sql', 'docker', 'spark', 'gcp', 'kafka', 'hadoop', 'sqoop', 'bigquery', 'big data', 'aws', 'etl', 'data integration']",2025-06-14 05:15:12
"Data Center Engineering Operations, Data Center Engineering Operations",Amazon,2 - 8 years,Not Disclosed,['Hyderabad'],"AWS Infrastructure Services owns the design, planning, delivery, and operation of all AWS global infrastructure. In other words, we re the people who keep the cloud running. We support all AWS data centers and all of the servers, storage, networking, power, and cooling equipment that ensure our customers have continual access to the innovation they rely on. We work on the most challenging problems, with thousands of variables impacting the supply chain and we re looking for talented people who want to help.\n\nYou ll join a diverse team of software, hardware, and network engineers, supply chain specialists, security experts, operations managers, and other vital roles. You ll collaborate with people across AWS to help us deliver the highest standards for safety and security while providing seemingly infinite capacity at the lowest possible cost for our customers. And you ll experience an inclusive culture that welcomes bold ideas and empowers you to own them to completion.\n\n\nThe Data Center Engineering Operations Engineer is responsible for ensuring all electrical, mechanical, and fire/life safety equipment within the data center operates within contracted parameters. This role includes risk management, infrastructure maintenance, vendor management, and performance reporting.\n\nKey responsibilities include:\n\nManaging on-site contractors, sub-contractors, and vendors to ensure all work complies with established practices, procedures, and local legislation.\nEstablishing performance benchmarks, conducting analyses, and preparing reports on all aspects of data center facility infrastructure operations and maintenance.\nCollaborating with DCO managers (IT), business leaders, and operating partners to coordinate projects, manage capacity, and optimize plant safety, performance, reliability, sustainability, and efficiency.\nSupporting rack installation and ensuring proper power and cooling provisioning.\nImplementing and overseeing preventative and corrective maintenance of critical infrastructure.\nDeveloping and maintaining metrics for reporting on facility performance and efficiency.\nEnsuring compliance with safety standards and environmental regulations.\nParticipating in continuous improvement initiatives to enhance data center operations\n\nA day in the life\nAssist in troubleshooting of facility and rack-level events within internal Service Level Agreements (SLA).\nPerform rack installs, rack decommissioning, and facility management.\nProvide operational readings and key performance indicators to make sure uptime is maintained.\nResponsible for the on-site management of contractors, sub-contractors and vendors, ensuring that all work performed is in accordance with established practices, procedures local legislation.\nPerform and oversee maintenance and operations on all electrical, mechanical, and fire/life safety equipment within the data center. Work schedule changes depending on specific site needs. Shifts can be up to 12-hours and may rotate on a predefined schedule. Some locations have on-call rotations\n\nAbout the team\nAbout AWS\nDiverse Experiences\nAmazon values diverse experiences. Even if you do not meet all of the preferred qualifications and skills listed in the job description, we encourage candidates to apply. If your career is just starting, hasn t followed a traditional path, or includes alternative experiences, don t let it stop you from applying.",,,,"['Procurement', 'Substation', 'Automation', 'Switchgear', 'Pumps', 'Valves', 'Heat exchangers', 'Gas turbine', 'Troubleshooting', 'SCADA']",2025-06-14 05:15:14
"Level 2 Market Data Support Engineer (Windows, Linux, SQL, IM)",Synechron,3 - 7 years,Not Disclosed,['Pune'],"Job Summary\nSynechron is seeking a technically skilled and proactive Level 2 Market Data Support Engineer specializing in Market Data operations to join our dedicated back-office support team. This role is pivotal in maintaining the stability, security, and efficiency of our market data systems and related infrastructure. The ideal candidate will deliver advanced technical support, troubleshoot complex issues, and contribute to process improvements, ensuring seamless access and data accuracy in a highly regulated financial environment.\nThis position provides an opportunity to work with cross-functional teams, implement best practices, and support critical business functions. It requires a combination of technical expertise, analytical acumen, and effective communication skills to sustain the integrity of our market data platform and ensure high service standards.",,,,"['Market Data Support', 'Batch Processing Systems', 'ServiceNow', 'Git', 'Linux', 'Windows Server management', 'Unix/Linux', 'ITSM tools', 'Windows', 'Incident Management', 'SQL']",2025-06-14 05:15:17
Hadoop Data Engineer,Envision Technology Solutions,3 - 8 years,5-15 Lacs P.A.,"['New Delhi', 'Hyderabad', 'Gurugram']","Primary Skill – Hadoop, Hive, Python, SQL, Pyspark/Spark.\nLocation –Hyderabad / Gurgaon;",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Hadoop', 'Hive', 'Spark', 'Python', 'SQL']",2025-06-14 05:15:19
Data Platform Engineer,Accenture,3 - 8 years,Not Disclosed,['Pune'],"Project Role :Data Platform Engineer\n\n\n\n\n\nProject Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Platform Engineer, you will assist with the data platform blueprint and design, collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. You will play a crucial role in the development and maintenance of the data platform components, contributing to the overall success of the project.\nRoles & Responsibilities:- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Assist with the data platform blueprint and design.- Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.- Develop and maintain data platform components.- Contribute to the overall success of the project.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Strong understanding of statistical analysis and machine learning algorithms.- Experience with data visualization tools such as Tableau or Power BI.- Hands-on implementing various machine learning algorithms such as linear regression, logistic regression, decision trees, and clustering algorithms.- Solid grasp of data munging techniques, including data cleaning, transformation, and normalization to ensure data quality and integrity.\nAdditional Information:- The candidate should have a minimum of 3 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Pune office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analytics', 'tableau', 'machine learning algorithms', 'statistics', 'data munging', 'python', 'data analysis', 'natural language processing', 'power bi', 'machine learning', 'sql', 'data quality', 'r', 'data modeling', 'data science', 'predictive modeling', 'text mining', 'logistic regression']",2025-06-14 05:15:21
GCP Data Engineer,Product based Fortune Global 500 MNC in ...,4 - 8 years,6-16 Lacs P.A.,"['Hyderabad', 'Chennai']","Role & responsibilities\nBachelors degree or four or more years of work experience.\nFour or more years of work experience.\nExperience with Data Warehouse concepts and Data Management life cycle.\nExperience in any DBMS\nExperience in Shell scripting, Spark, Scala.\nExperience in GCP/Big Query, composer, Airflow.\nExperience in real time streaming\nExperience in DevOps",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['GCP', 'Bigquery', 'Cloud Storage', 'Pubsub', 'Data Flow', 'Dataproc', 'Spark', 'Composing', 'Dataprep', 'Python']",2025-06-14 05:15:23
Data Platform Engineer,Accenture,2 - 7 years,Not Disclosed,['Pune'],"Project Role :Data Platform Engineer\n\n\n\n\n\nProject Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Platform Engineer, you will assist with the data platform blueprint and design, collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. You will play a crucial role in the development and maintenance of the data platform components, contributing to the overall success of the project.\nRoles & Responsibilities:- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Assist with the data platform blueprint and design.- Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.- Develop and maintain data platform components.- Contribute to the overall success of the project.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Strong understanding of statistical analysis and machine learning algorithms.- Experience with data visualization tools such as Tableau or Power BI.- Hands-on implementing various machine learning algorithms such as linear regression, logistic regression, decision trees, and clustering algorithms.- Solid grasp of data munging techniques, including data cleaning, transformation, and normalization to ensure data quality and integrity.\nAdditional Information:- The candidate should have a minimum of 2 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Pune office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analytics', 'tableau', 'machine learning algorithms', 'statistics', 'data munging', 'python', 'data analysis', 'natural language processing', 'power bi', 'machine learning', 'sql', 'data quality', 'r', 'data modeling', 'data science', 'predictive modeling', 'text mining', 'logistic regression']",2025-06-14 05:15:25
Data Platform Engineer,Accenture,7 - 12 years,Not Disclosed,['Hyderabad'],"Project Role :Data Platform Engineer\n\n\n\n\n\nProject Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Platform Engineer, you will assist with the data platform blueprint and design, collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. You will play a crucial role in the development and maintenance of the data platform components, contributing to the overall success of the organization.\nRoles & Responsibilities:- Expected to be an SME, collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Assist with the data platform blueprint and design.- Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.- Develop and maintain data platform components.- Contribute to the overall success of the organization.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Strong understanding of statistical analysis and machine learning algorithms.- Experience with data visualization tools such as Tableau or Power BI.- Hands-on implementing various machine learning algorithms such as linear regression, logistic regression, decision trees, and clustering algorithms.- Solid grasp of data munging techniques, including data cleaning, transformation, and normalization to ensure data quality and integrity.\nAdditional Information:- The candidate should have a minimum of 7.5 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Hyderabad office.- 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analytics', 'tableau', 'machine learning algorithms', 'statistics', 'data munging', 'python', 'natural language processing', 'power bi', 'machine learning', 'sql', 'data bricks', 'data quality', 'r', 'data modeling', 'data science', 'predictive modeling', 'text mining', 'logistic regression']",2025-06-14 05:15:28
Data Platform Engineer,Accenture,7 - 12 years,Not Disclosed,['Hyderabad'],"Project Role :Data Platform Engineer\n\n\n\n\n\nProject Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification : Engineering graduate preferably Computer Science graduate 15 years of full time education\n\n\nSummary:As a Data Platform Engineer, you will assist with the data platform blueprint and design, collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. You will play a crucial role in shaping the data platform components.\nRoles & Responsibilities:- Expected to be an SME- Collaborate and manage the team to perform- Responsible for team decisions- Engage with multiple teams and contribute on key decisions- Provide solutions to problems for their immediate team and across multiple teams- Lead data platform blueprint and design- Implement data platform components effectively- Ensure seamless integration between systems and data models\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform- Strong understanding of data platform architecture- Experience in data integration and data modeling- Knowledge of cloud platforms like AWS or Azure- Hands-on experience with SQL and NoSQL databases\nAdditional Information:- The candidate should have a minimum of 7.5 years of experience in Databricks Unified Data Analytics Platform- This position is based at our Hyderabad office- An Engineering graduate preferably Computer Science graduate with 15 years of full-time education is required\n\nQualification\n\nEngineering graduate preferably Computer Science graduate 15 years of full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['sql', 'data modeling', 'data analytics', 'platform architecture', 'aws', 'kubernetes', 'continuous integration', 'openshift', 'docker', 'microservices', 'iot', 'java', 'git', 'gcp', 'devops', 'linux', 'jenkins', 'mysql', 'hadoop', 'big data', 'microsoft azure', 'cloud platforms', 'nosql', 'agile', 'data integration']",2025-06-14 05:15:30
Data Platform Engineer,Accenture,3 - 8 years,Not Disclosed,['Pune'],"Project Role :Data Platform Engineer\n\n\n\n\n\nProject Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Platform Engineer, you will assist with the data platform blueprint and design, collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. You will play a crucial role in shaping the data platform components.\nRoles & Responsibilities:- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Collaborate with Integration Architects and Data Architects to design and implement data platform components.- Ensure seamless integration between various systems and data models.- Develop and maintain data platform blueprints.- Provide technical expertise in data platform design and implementation.- Troubleshoot and resolve data platform related issues.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Strong understanding of data platform architecture and design principles.- Experience in implementing and optimizing data pipelines.- Knowledge of cloud-based data solutions.- Hands-on experience with data platform security and compliance measures.\nAdditional Information:- The candidate should have a minimum of 3 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Pune office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analytics', 'java', 'data modeling', 'platform architecture', 'design principles', 'hive', 'python', 'oracle', 'enterprise architecture', 'microsoft azure', 'data warehousing', 'sql', 'docker', 'infrastructure architecture', 'spark', 'gcp', 'design patterns', 'hadoop', 'agile', 'aws', 'big data']",2025-06-14 05:15:33
Data Platform Engineer,Accenture,7 - 12 years,Not Disclosed,['Chennai'],"Project Role :Data Platform Engineer\n\n\n\n\n\nProject Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification : Engineering graduate preferably Computer Science graduate 15 years of full time education\n\n\nSummary:As a Data Platform Engineer, you will be responsible for assisting with the blueprint and design of the data platform components using Databricks Unified Data Analytics Platform. Your typical day will involve collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\nRoles & Responsibilities:- Assist with the blueprint and design of the data platform components using Databricks Unified Data Analytics Platform.- Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.- Develop and maintain data pipelines using Databricks Unified Data Analytics Platform.- Design and implement data security and access controls using Databricks Unified Data Analytics Platform.- Troubleshoot and resolve issues related to data platform components using Databricks Unified Data Analytics Platform.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nExperience with Databricks Unified Data Analytics Platform.- Must To Have\n\n\n\n\nSkills:\nStrong understanding of data platform components and architecture.- Good To Have\n\n\n\n\nSkills:\nExperience with cloud-based data platforms such as AWS or Azure.- Good To Have\n\n\n\n\nSkills:\nExperience with data security and access controls.- Good To Have\n\n\n\n\nSkills:\nExperience with data pipeline development and maintenance.\nAdditional Information:- The candidate should have a minimum of 7.5 years of experience in Databricks Unified Data Analytics Platform.- The ideal candidate will possess a strong educational background in computer science or a related field, along with a proven track record of delivering impactful data-driven solutions.-This position is based at our Bangalore, Hyderabad, Chennai and Pune Offices.- Mandatory office (RTO) for 2- 3 days and have to work on 2 shifts (Shift A- 10:00am to 8:00pm IST and Shift B - 12:30pm to 10:30 pm IST)\n\nQualification\n\nEngineering graduate preferably Computer Science graduate 15 years of full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data analytics', 'data security', 'microsoft azure', 'data bricks', 'aws', 'hive', 'amazon redshift', 'pyspark', 'data warehousing', 'emr', 'sql', 'java', 'data modeling', 'spark', 'mysql', 'hadoop', 'big data', 'etl', 'python', 'machine learning', 'sql server', 'nosql', 'pipeline', 'amazon ec2', 'kafka', 'sqoop']",2025-06-14 05:15:36
Data Platform Engineer,Accenture,7 - 12 years,Not Disclosed,['Chennai'],"Project Role :Data Platform Engineer\n\n\n\n\n\nProject Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification : Engineering graduate preferably Computer Science graduate 15 years of full time education\n\n\nSummary:As a Data Platform Engineer, you will be responsible for assisting with the blueprint and design of the data platform components using Databricks Unified Data Analytics Platform. Your typical day will involve collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\nRoles & Responsibilities:- Assist with the blueprint and design of the data platform components using Databricks Unified Data Analytics Platform.- Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.- Develop and maintain data pipelines using Databricks Unified Data Analytics Platform.- Design and implement data security and access controls using Databricks Unified Data Analytics Platform.- Troubleshoot and resolve issues related to data platform components using Databricks Unified Data Analytics Platform.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nExperience with Databricks Unified Data Analytics Platform.- Must To Have\n\n\n\n\nSkills:\nStrong understanding of data modeling and database design principles.- Good To Have\n\n\n\n\nSkills:\nExperience with cloud-based data platforms such as AWS or Azure.- Good To Have\n\n\n\n\nSkills:\nExperience with data security and access controls.- Good To Have\n\n\n\n\nSkills:\nExperience with data pipeline development and maintenance.\nAdditional Information:- The candidate should have a minimum of 7.5 years of experience in Databricks Unified Data Analytics Platform.- The ideal candidate will possess a strong educational background in computer science or a related field, along with a proven track record of delivering impactful data-driven solutions.- This position is based at our Chennai office.\n\nQualification\n\nEngineering graduate preferably Computer Science graduate 15 years of full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['database design', 'data modeling', 'data analytics', 'microsoft azure', 'design principles', 'hive', 'data warehousing', 'sql', 'java', 'spark', 'design patterns', 'mysql', 'hadoop', 'big data', 'etl', 'rest', 'python', 'data security', 'javascript', 'sql server', 'data bricks', 'pipeline', 'kafka', 'sqoop', 'aws']",2025-06-14 05:15:39
Data Platform Engineer,Accenture,7 - 12 years,Not Disclosed,['Chennai'],"Project Role :Data Platform Engineer\n\n\n\n\n\nProject Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification : Engineering graduate preferably Computer Science graduate 15 years of full time education\n\n\nSummary:As a Data Platform Engineer, you will be responsible for assisting with the blueprint and design of the data platform components using Databricks Unified Data Analytics Platform. Your typical day will involve collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\nRoles & Responsibilities:- Assist with the blueprint and design of the data platform components using Databricks Unified Data Analytics Platform.- Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.- Develop and maintain data pipelines using Databricks Unified Data Analytics Platform.- Design and implement data security and access controls using Databricks Unified Data Analytics Platform.- Troubleshoot and resolve issues related to data platform components using Databricks Unified Data Analytics Platform.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nExperience with Databricks Unified Data Analytics Platform.- Good To Have\n\n\n\n\nSkills:\nExperience with other big data technologies such as Hadoop, Spark, and Kafka.- Strong understanding of data modeling and database design principles.- Experience with data security and access controls.- Experience with data pipeline development and maintenance.- Experience with troubleshooting and resolving issues related to data platform components.\nAdditional Information:- The candidate should have a minimum of 7.5 years of experience in Databricks Unified Data Analytics Platform.- The ideal candidate will possess a strong educational background in computer science or a related field, along with a proven track record of delivering impactful data-driven solutions.- This position is based at our Bangalore, Hyderabad, Chennai and Pune Offices.- Mandatory office (RTO) for 2- 3 days and have to work on 2 shifts (Shift A- 10:00am to 8:00pm IST and Shift B - 12:30pm to 10:30 pm IST)\n\nQualification\n\nEngineering graduate preferably Computer Science graduate 15 years of full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['database design', 'data modeling', 'spark', 'data analytics', 'design principles', 'hive', 'sql', 'java', 'design patterns', 'oops', 'mysql', 'hadoop', 'big data', 'etl', 'c#', 'rest', 'python', 'microsoft azure', 'javascript', 'sql server', 'data bricks', 'amazon ec2', 'kafka', 'troubleshooting', 'sqoop', 'aws']",2025-06-14 05:15:42
Data Platform Engineer,Accenture,7 - 12 years,Not Disclosed,['Pune'],"Project Role :Data Platform Engineer\n\n\n\n\n\nProject Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification : Engineering graduate preferably Computer Science graduate 15 years of full time education\n\n\nSummary:As a Data Platform Engineer, you will be responsible for assisting with the blueprint and design of the data platform components using Databricks Unified Data Analytics Platform. Your typical day will involve collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\nRoles & Responsibilities:- Assist with the blueprint and design of the data platform components using Databricks Unified Data Analytics Platform.- Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.- Develop and maintain data pipelines and ETL processes using Databricks Unified Data Analytics Platform.- Design and implement data security and access controls for the data platform.- Troubleshoot and resolve issues related to the data platform and data pipelines.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nExperience with Databricks Unified Data Analytics Platform.- Must To Have\n\n\n\n\nSkills:\nStrong understanding of data modeling and database design principles.- Good To Have\n\n\n\n\nSkills:\nExperience with cloud-based data platforms such as AWS or Azure.- Good To Have\n\n\n\n\nSkills:\nExperience with data security and access controls.- Good To Have\n\n\n\n\nSkills:\nExperience with data visualization tools such as Tableau or Power BI.\nAdditional Information:- The candidate should have a minimum of 7.5 years of experience in Databricks Unified Data Analytics Platform.- The ideal candidate will possess a strong educational background in computer science or a related field, along with a proven track record of delivering impactful data-driven solutions.- This position is based at our Bangalore, Hyderabad, Chennai and Pune Offices.- Mandatory office (RTO) for 2- 3 days and have to work on 2 shifts (Shift A- 10:00am to 8:00pm IST and Shift B - 12:30pm to 10:30 pm IST)\n\nQualification\n\nEngineering graduate preferably Computer Science graduate 15 years of full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['database design', 'data modeling', 'data analytics', 'microsoft azure', 'design principles', 'hive', 'sql', 'java', 'spark', 'design patterns', 'oops', 'mysql', 'hadoop', 'etl', 'big data', 'c#', 'rest', 'python', 'data security', 'power bi', 'javascript', 'sql server', 'data bricks', 'tableau', 'kafka', 'sqoop', 'aws']",2025-06-14 05:15:45
Remote Data Visualization Engineer 36Lakhs CTC|| Kandi Srinivasa Reddy,Integra Technologies,8 - 11 years,35-37.5 Lacs P.A.,"['Kolkata', 'Ahmedabad', 'Bengaluru']","Dear Candidate,\nWe are seeking a Data Visualization Engineer to turn complex data into clear, engaging visual insights that empower business decisions. This role involves working closely with analysts and stakeholders to build interactive dashboards and reports.\n\nKey Responsibilities:\nDesign and develop visualizations using tools like Power BI, Tableau, or Looker.\nTranslate data into compelling stories and business insights.\nOptimize dashboard performance and usability for end users.\nCollaborate with data engineering and analytics teams.\nImplement visualization standards and data governance practices.\nRequired Skills & Qualifications:\nProficiency in data visualization tools (Power BI, Tableau, D3.js).\nStrong knowledge of SQL and data modeling.\nUnderstanding of UX principles in data presentation.\nExperience working with large datasets and cloud-based data platforms.\nKnowledge of scripting languages (Python, R) is a plus.\nSoft Skills:\nStrong troubleshooting and problem-solving skills.\nAbility to work independently and in a team.\nExcellent communication and documentation skills.\n\nNote: If interested, please share your updated resume and preferred time for a discussion. If shortlisted, our HR team will contact you.\n\nKandi Srinivasa Reddy\nDelivery Manager\nIntegra Technologies",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Tableau', 'Quicksight', 'Data Visualization', 'Dashboard Development', 'Business Intelligence', 'Power Bi', 'Bi Tools', 'Dashboarding', 'Business Objects', 'Reporting Tools', 'SQL', 'Microstrategy', 'Dashboards', 'QlikView']",2025-06-14 05:15:48
Data Governance Analyst,HCLTech,4 - 6 years,Not Disclosed,"['Noida', 'Vijayawada', 'Chennai']","Key Responsibilities:\nUnderstand all of our data definitions and nuances (e.g., attribution window)\nMaintain and update glossary; Advise various internal stakeholders\nConsult data visualization team and analysts on what data to use for new reports and analyses\nRecommend development or enhancement of datasets for reporting and analyses questions\nDevelop and implement data governance policies and procedures to ensure data quality, availability, and integrity.",,,,"['Data Management', 'ETL', 'Data Governance', 'SQL', 'Python', 'Data Engineering', 'Data Modeling', 'Data Warehousing', 'Data Governance Analyst']",2025-06-14 05:15:50
Data Engineer+ Subject Matter Expert- Data Mining,Newton School,0 - 2 years,Not Disclosed,['Pune'],"Newton School of Technology is on a mission to transform technology education and bridge the employability gap. As India s first impact university, we are committed to revolutionizing\nlearning, empowering students, and shaping the future of the tech industry. Backed by\nrenowned professionals and industry leaders, we aim to solve the employability challenge\nand create a lasting impact on society. We are currently looking for a Data Engineer + Subject Matter Expert - Data Mining to join our Computer Science Department. This is a full-time academic role focused on data mining, analytics, and teaching/mentoring students in core data science and engineering topics.\n\nKey Responsibilities:\nDevelop and deliver comprehensive and engaging lectures for the undergraduate\n""Data Mining"", BigData and Data Analytics courses, covering the full syllabus from\nfoundational concepts to advanced techniques.\nInstruct students on the complete data lifecycle, including data preprocessing,\ncleaning, transformation, and feature engineering.\nTeach the theory, implementation, and evaluation of a wide range of algorithms for\nClassification, Association rules mining, Clustering and Anomaly Detections.\nDesign and facilitate practical lab sessions and assignments that provide students\nwith hands-on experience using modern data tools and software.\nDevelop and grade assessments, including assignments, projects, and examinations,\nthat effectively measure the Course Learning Objectives (CLOs).\nMentor and guide students on projects, encouraging them to work with real-world or\nbenchmark datasets (e.g., from Kaggle).\nStay current with the latest advancements, research, and industry trends in data\nengineering and machine learning to ensure the curriculum remains relevant and\ncutting-edge.\nContribute to the academic and research environment of the department and the\nuniversity.\n\nRequired Qualifications:\nA Ph.D. (or a Masters degree with significant, relevant industry experience) in\nComputer Science, Data Science, Artificial Intelligence, or a closely related field.\nDemonstrable expertise in the core concepts of data engineering and machine\nlearning as outlined in the syllabus.\nStrong practical proficiency in Python and its data science ecosystem, specifically\nScikit-learn, Pandas, NumPy, and visualization libraries (e.g., Matplotlib, Seaborn).\nProven experience in teaching, preferably at the undergraduate level, with an ability to\nmake complex topics accessible and engaging.\nExcellent communication and interpersonal skills.\n\nPreferred Qualifications:\nA strong record of academic publications in reputable data mining, machine learning,\nor AI conferences/journals.\nPrior industry experience as a Data Scientist, Big Data Engineer, Machine Learning\nEngineer, or in a similar role.\nExperience with big data technologies (e.g., Spark, Hadoop) and/or deep learning\nframeworks (e.g., TensorFlow, PyTorch).\nExperience in mentoring student teams for data science competitions or hackathons.\n\nPerks & Benefits:\nCompetitive salary packages aligned with industry standards.\nAccess to state-of-the-art labs and classroom facilities.\nTo know more about us, feel free to explore our website: Newton School of Technology\nWe look forward to the possibility of having you join our academic team and help shape the\nfuture of tech education!\nNewton School of Technology is on a mission to transform technology education and bridge the employability gap. As India s first impact university,\n...",Industry Type: Education / Training,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Interpersonal skills', 'data science', 'Artificial Intelligence', 'Machine learning', 'Manager Technology', 'Data analytics', 'Subject Matter Expert', 'Data mining', 'Python']",2025-06-14 05:15:54
Software Data Science Engineer,Johnson Controls,2 - 5 years,Not Disclosed,['Pune'],"Job Title Software and Data Science Engineer\nJob Summary We are seeking a highly motivated and analytical Data Scientist to join our team. The ideal candidate will possess a strong engineering background, a passion for solving technical problems, and the ability to work collaboratively with both technical and non-technical stakeholders. You will play a key role in developing and maintaining our platform, utilizing large-scale data to derive insights and drive business value, while working on both front-end and back-end components.\nWhat We Value",,,,"['Computer science', 'C++', 'Backend', 'Front end', 'data science', 'Data management', 'Analytical', 'Javascript', 'Data structures', 'Python']",2025-06-14 05:15:58
Software Data Operations Engineer (MS+0),MAQ Software,2 - 4 years,Not Disclosed,"['Noida', 'Mumbai', 'Hyderabad']","MAQ LLC d.b.a MAQ Software has multiple openings at Redmond, WA for:\n\nSoftware Data Operations Engineer (MS+0)\n\nWill support data management projects to include architecting, programming, testing and modifying software to meet customer specifications. Deploy, configure, implement, test reports, analyze databases for errors and fix them. Automate user test scenarios, configure, debug and fix errors in cloud-based infrastructure and dashboards to meet customer needs. Must be able to travel temporarily to client sites and or relocate throughout the United States.\n\nRequirements: Masters Degree or foreign equivalent in Computer Science, Computer Applications, Computer Information Systems, Information Technology or related field.\n\nBenefits: Standard Employee Benefits.\n\nThe position qualifies for Employee Referral program.\n\nSend resume to 2027 152nd Avenue NE, Redmond, WA 98052, Attn: H.R. Manager.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Operations', 'Data Engineering', 'test scenarios', 'configuration', 'Data Management']",2025-06-14 05:16:01
Big Data Lead,Hexaware Technologies,8 - 13 years,18-25 Lacs P.A.,"['Chennai', 'Bengaluru', 'Mumbai (All Areas)']","As an Azure Data Engineer, we are looking for candidates who possess expertise in the following:\nDatabricks\nData Factory\nSQL\nPyspark/Spark\n\nRoles and Responsibilities:",,,,"['Databricks', 'Sql', 'Python']",2025-06-14 05:16:05
Remote Data Platform Engineer 57LakhsCTC|| Srinivasa Reddy Kandi,Integra Technologies,12 - 15 years,55-60 Lacs P.A.,"['Ahmedabad', 'Chennai', 'Bengaluru']","Dear Candidate,\nWe are hiring a Data Platform Engineer to build and maintain scalable, secure, and reliable data infrastructure for analytics and real-time processing.\n\nKey Responsibilities:\nDesign and manage data pipelines, storage layers, and ingestion frameworks.\nBuild platforms for batch and streaming data processing (Spark, Kafka, Flink).\nOptimize data systems for scalability, fault tolerance, and performance.\nCollaborate with data engineers, analysts, and DevOps to enable data access.\nEnforce data governance, access controls, and compliance standards.\nRequired Skills & Qualifications:\nProficiency with distributed data systems (Hadoop, Spark, Kafka, Airflow).\nStrong SQL and experience with cloud data platforms (Snowflake, BigQuery, Redshift).\nKnowledge of data warehousing, lakehouse, and ETL/ELT pipelines.\nExperience with infrastructure as code and automation.\nFamiliarity with data quality, security, and metadata management.\nSoft Skills:\nStrong troubleshooting and problem-solving skills.\nAbility to work independently and in a team.\nExcellent communication and documentation skills.\nNote: If interested, please share your updated resume and preferred time for a discussion. If shortlisted, our HR team will contact you.\n\nSrinivasa Reddy Kandi\nDelivery Manager\nIntegra Technologies",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Airflow', 'Data Engineering', 'Spark', 'Python', 'Streaming', 'Apache Flink', 'Kafka', 'Apache', 'Spark Streaming', 'Apache Nifi', 'Stream Processing', 'Apache Storm', 'Apache Pulsar', 'Data Pipeline', 'Streams']",2025-06-14 05:16:08
Senior Software Engineer: Data Science,Enphase Energy,3 - 5 years,Not Disclosed,['Bengaluru'],"Description\nOur mission at Enphase Energy is to advance a sustainable future for all.\n\nToday, our intelligent microinverters, which turn sunlight into an affordable, safe, reliable, and scalable source of energy, work with virtually every solar panel made, and when paired with our award-winning smart battery technology, we engineer one of the industrys best-performing clean energy systems. To date, we have installed more than 48 million microinverters on more than 2.5 million systems across 140 countries and well over 50 thousand homes use our energy storage products.\n\nLike our customers, our innovative teams are also worldwide, making Enphase Energy a truly global company. We are one of the fastest growing and most dynamic energy companies in the world. Nimble and acutely focused on developing ground-breaking solar energy management technology, each of our teams has a shared goal of creating a carbon-free future.\nDo you want to help change the worldLearn more about the role:\nFor our Customer Experience team, we seek Hands-On AI/M L Staff Engineer who can work on designing implementing high quality scalable AI/ML applications and platforms, while providing technical leadership/mentoring to a small team of talented developers in agile environment. Your ability to lead the architecture, design, and implementation of maintainable, high-quality, and high-performing Machine Learning systems and AI applications is essential for success in this role.\nProvide hands-on technical expertise to design, engineer, deploy, and deliver highly scalable machine learning applications. Drive improvements in technical architecture, standards, and processes. Drive engineering excellence while managing/mentoring talented team of developers in agile environment. Work closely with product management and other stakeholders for system design and delivery.\nWhat you will be doing:\nUnderstanding the customer experience business use cases and technical requirements and being able to convert them into a technical design that elegantly meets the requirements.\nImplementation of sophisticated analytics programs, machine learning, and statistical methods to prepare enterprise data for use in predictive and prescriptive modeling.\nAccountable for the data science platform design in addition to the use of case-based application solution design.\nWork on complex unstructured datasets using advanced statistical and analytical methods.\nWork closely with other technical and operational functions to gather, organize and analyze datasets to extract meaningful insights.\nWho you are and what you bring:\nMS or Bachelors in Computer Science, Math, Machine Learning, or a relevant field with 4 + yrs. of experience in industry and/or academic research\nStrong Experience in Python for Data Science, Data Science on AWS, Data Science solutions, Communication and Collaboration, Statistics Probability.\nAbility to design and implement workflows of Linear Logistic Regression, Ensemble Models (Random Forest, Boosting) using R/Python or Optimisation methodologies\nDemonstrable competency in Probability Statistics, ability to use ideas of Data Distributions, Hypothesis Testing, and other Statistical Tests.\nDemonstrable competency in Data Visualisation using the Python/R Data Science Stack.\nHands-on experience in using statistical and analytical techniques to complex business problems.\nHands-on experience in solving regression, prediction, classification, clustering, neural networks, and Bayesian problems.\nAdvanced knowledge of statistical techniques, machine learning algorithms, Bayesian Models, data mining, and text mining.\nExperience in handling large datasets on cloud and on-premises setup, using distributed computing.\nAble to understand various data structures and common methods in data transformation.\nStrong Programming background and expertise in building models in languages like Python, R, Scala, etc.\nGood knowledge of visual techniques for data analysis and presentation skills\nStrong troubleshooting skills in different disparate technologies and environments\nEnthusiastic about different areas of work and exploring new technologies\nClarity of thought and strong communication skills to effectively pitch solutions\nAbility to explore and grasp new technologies\nMentoring your team members in projects and helping them keep up with new technologies\nEmpowering the team members to be solution providers and enable a flat environment where every ones point of view is considered, and feedback is encouraged.",Industry Type: Telecom / ISP,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Product management', 'Data analysis', 'Agile', 'Data structures', 'System design', 'Troubleshooting', 'Data mining', 'Analytics', 'Python']",2025-06-14 05:16:12
Data Science & AI Engineer,Blue Altair,5 - 8 years,Not Disclosed,['Pune'],"Greetings from Blue Altair!\nJob Overview:\nWe are seeking an experienced and highly skilled Data Science and AI Engineer to join our dynamic team. The ideal candidate will have 5+ years of experience working on cutting-edge data science and AI technologies across various cloud platforms with a strong focus to work on LLMs and SLMs. The role demands a professional capable of performing in a client-facing environment, as well as mentoring and guiding junior team members.\n\nTitle: Consultant/Sr. Consultant - Data Science Engineer\nExperience: 5-8 years\nLocation: Pune/Bangalore (Hybrid)\n\nRoles and responsibilities:\nDevelop, implement, and optimize machine learning models and AI algorithms to solve complex business problems.\nDesign, build, and fine-tune AI models, particularly focusing on LLMs and SLMs, using state-of-the-art techniques and architectures.\nApply advanced techniques in prompt engineering, model fine-tuning, and optimization to tailor models for specific business needs.\nDeploy and manage machine learning models and pipelines on cloud platforms (AWS, GCP, Azure, etc.).\nWork closely with clients to understand their data and AI needs and provide tailored solutions.\nCollaborate with cross-functional teams to integrate AI solutions into broader software architectures.\nMentor junior team members and provide guidance in implementing best practices in data science and AI development.\nStay up-to-date with the latest trends and advancements in data science, AI, and cloud technologies.\nPrepare technical documentation and present insights to both technical and non-technical stakeholders.\n\nRequirement:\n5+ years of experience in data science, machine learning, and AI technologies.\nProven experience working with cloud platforms such as Google Cloud, Microsoft Azure, or AWS.\nExpertise in programming languages such as Python, R, Julia, and AI frameworks like TensorFlow, PyTorch, Scikit-learn, Hugging face Transformers.\nKnowledge of data visualization tools (e.g., Matplotlib, Seaborn, Tableau)\nSolid understanding of data engineering concepts including ETL, data pipelines, and databases (SQL, NoSQL).\nExperience with MLOps practices and deployment of models in production environments.\nFamiliarity with NLP (Natural Language Processing) tasks and working with large-scale datasets.\nHands-on experience with generative AI models like GPT, Gemini, Claude, Mistral etc.\nClient-facing experience with strong communication skills to manage and engage stakeholders.\nStrong problem-solving skills and analytical mindset.\nAbility to work independently and as part of a team and mentor and provide technical leadership to junior team members.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['LLMs', 'Artificial Intelligence', 'MLOps', 'RAG', 'Natural Language Processing', 'Neural Networks', 'LLM', 'Machine Learning', 'AI Models', 'Data Science', 'PyTorch', 'SLM', 'AI Automation']",2025-06-14 05:16:16
Data & Analytics Engineering Manager,Avalara Technologies,10 - 15 years,Not Disclosed,['Pune'],"What You'll Do\n\nThe Global Analytics & Insights (GAI) team is seeking a Data & Analytics Engineering Manager to lead our team in designing, developing, and maintaining data pipelines and analytics infrastructure. As a Data & Analytics Engineering Manager, you will play a pivotal role in empowering a team of engineers to build and enhance analytics applications and a modern data platform using Snowflake, dbt (Data Build Tool), Python, Terraform, and Airflow. You will become an expert in Avalaras financial, marketing, sales, and operations data. The ideal candidate will have deep SQL experience, an understanding of modern data stacks and technology, demonstrated leadership and mentoring experience, and an ability to drive innovation and manage complex projects. This position will report to Senior Manager.\n\nWhat Your Responsibilities Will Be\nMentor a team of data engineers, providing guidance and support to ensure a high level of quality and career growth\nLead a team of data engineers in the development and maintenance of data pipelines, data modelling, code reviews and data products\nCollaborate with cross-functional teams to understand requirements and translate them into scalable data solutions\nDrive innovation and continuous improvements within the data engineering team\nBuild maintainable and scalable processes and playbooks to ensure consistent delivery and quality across projects\nDrive adoption of best practices in data engineering and data modelling\nBe the visible lead of the team- coordinate communication, releases, and status to various stakeholders\nWhat You'll Need to be Successful\nBachelor's degree in Computer Science, Engineering, or related field\n10+ years experience in data engineering field, with deep SQL knowledge\n2+ years management experience, including direct technical reports\n5+ years experience with data warehousing concepts and technologies\n4+ years of working with Git, and demonstrated experience using these tools to facilitate growth of engineers\n4+ years working with Snowflake\n3+ years working with dbt (dbt core preferred)\nPreferred Qualifications:\nSnowflake, Dbt, AWS Certified\n3+ years working with Infrastructure as Code, preferably Terraform\n2+ years working with CI CD, and demonstrated ability to build and operate pipelines\nExperience and understanding of Snowflake administration and security principles\nDemonstrated experience with Airflow",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Terraform', 'Snowflake administration', 'Snowflake', 'Dbt', 'AWS']",2025-06-14 05:16:19
"Lead Data Analytics Engineer - Power BI, Snowflake, SQL, Python",Avalara Technologies,8 - 13 years,Not Disclosed,['Pune'],"What You'll Do\n\nThe Data Science Engineering team is looking for a Lead Data Analytics Engineer\nto join our team! You should be and gather our requirements, understanding complex product, business, and engineering challenges, composing and prioritizing research projects, and then building them in partnership with cloud engineers and architects, and using the work of our data engineering team. You have deep SQL experience, an understanding of modern data stacks and technology, experience with data and all things data-related, and experience guiding a team through technical and design challenges. You will report into the Sr. Manager, Cloud Software Engineering and be a part of the larger Data Engineering team.\n\nWhat Your Responsibilities Will Be",,,,"['Data Analytics', 'Power BI', 'Snowflake', 'data storytelling', 'data warehousing', 'Data Modeling', 'AWS', 'SQL', 'Python']",2025-06-14 05:16:21
Sr. Azure Data Platform Engineer,EMTA Infotech,5 - 10 years,18-30 Lacs P.A.,[],"Role Title: Sr. Azure Data Platform Engineer\nLocation: India\n\n1 remote role and 5 WFO Noida location candidates need to have L2 and L3 Support experience with the below)\n\nWe are seeking an Azure Data Platform Engineer with a strong focus on Administration and hands-on experience in Azure platform engineering services.\nIdeal candidates should have expertise in administering services such as:\nAzure Key Vault\n\nFunction App & Logic App\nEvent Hub\n\nApp Services\nAzure Data Factory (Administration)\n\nAzure Monitor & Log Analytics\nAzure Databricks (Administration)\n\nETL processes\nCosmos DB (Administration)\n\nAzure DevOps & CI/CD pipelines\nAzure Synapse Analytics (Administration)\n\nPython / Shell scripting\nAzure Data Lake Storage (ADLS)\n\nAzure Kubernetes Service (AKS)\n\nAdditional knowledge of Tableau and Power BI would be a plus.\n\nAlso, candidates should have hands-on experience managing and ensuring the stability, security, and performance of these platforms, with a focus on automation, monitoring, and incident management.\n\nProficient in distributed system architectures and Azure Data Engineering services like Event Hub, Data Factory, ADLS Gen2, Cosmos DB, Synapse, Databricks, APIM, Function App , Logic App, and App Services\nImplement, and manage infrastructure using IaC tools such as Azure Resource Manager (ARM) templates and Terraform.\nManage containerized applications using Docker and orchestrate them with Azure Kubernetes Service (AKS).\nSet up and manage monitoring, logging, and alerting systems using Azure Monitor, Log Analytics, and Application Insights.\nImplement disaster recovery (DR) strategies, backups, and failover mechanisms for critical workloads.\nAutomate infrastructure provisioning, scaling, and management for high availability and efficiency.\nExperienced in managing and maintaining clusters across Development, Test, Preproduction, and Production environments on Azure.\nSkilled in defining, scheduling, and monitoring job flows, with proactive alert setup.\nAdept at troubleshooting failed jobs in azure tools like Databricks and Data Factory, performing root cause analysis, and applying corrective measures.\nHands-on experience with distributed streaming tools like Event Hub.\nExpertise in designing and managing backup and disaster recovery solutions using Infrastructure as Code (IaC) with Terraform.\nStrong experience in automating processes using Python, shell scripting, and working with Jenkins and Azure DevOps.\nProficient in designing and maintaining Azure CI/CD pipelines for seamless code integration, testing, and deployment.\nExperienced in monitoring and troubleshooting VM resources such as memory, CPU, OS, storage, and network.\nSkilled at monitoring applications and advising developers on improving job and workflow performance.\nCapable of reviewing and resolving log file issues for system and application components.\nAdaptable to evolving technologies, with a strong sense of responsibility and accomplishment.\nKnowledgeable in agile methodologies for software delivery.\n5-15 years of experience with Azure and cloud platforms, leveraging cloud-native tools to build, manage, and optimize secure, scalable solutions.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Key Vault', 'Azure Databricks', 'Azure Data Factory', 'Azure Synapse Analytics', 'Azure Devops', 'Adls Gen2', 'Function Apps', 'Shell Scripting', 'Azure Logic Apps', 'Azure Monitoring', 'Iac', 'Cosmos', 'Log Analytics', 'APIM', 'Terraform', 'Event Hub', 'CI/CD', 'App Service Plan', 'Etl Process', 'ARM', 'Kubernetes']",2025-06-14 05:16:23
Data Visualization Engineer,Maimsd Technology,4 - 7 years,Not Disclosed,['Bengaluru'],"Role : Data Visualization Engineer\n\nLocation : Bangalore, Pune\n\nExperience : 4 - 7 Yrs\n\nEmployment Type : Full Time, Permanent\n\nWorking mode : Regular\n\nNotice Period : Immediate - 15 Day\n\nAbout the Role :\n\nWe are seeking a skilled Data Visualization Engineer to join our team and transform raw data into actionable insights. You will play a crucial role in designing, developing, and maintaining interactive dashboards and reports using Power BI, Power Apps, Power Query, and Power Automate.\n\nResponsibilities :\n\n- Data Visualization : Create compelling and informative dashboards and reports using Power BI, effectively communicating complex data to stakeholders.\n\n- Data Integration : Import data from various sources (e.g., databases, spreadsheets, APIs) using Power Query and ensure data quality and consistency.\n\n- Data Modeling : Develop robust data models in Power BI to support complex analysis and reporting requirements.\n\n- Power Apps Development : Create custom applications using Power Apps to enable data-driven decision-making and automate workflows.\n\n- Power Automate Integration : Automate repetitive tasks and workflows using Power Automate to improve efficiency and reduce errors.\n\n- Cloud Data Analytics : Leverage cloud-based data analytics platforms to process and analyze large datasets.\n\n- Collaboration : Work closely with data analysts, data scientists, and business users to understand their requirements and deliver effective visualizations.\n\nQualifications :\n\nExperience : 4-7 years of experience in data visualization and business intelligence.\n\nTechnical Skills :\n\n- Proficiency in Power BI, Power Apps, Power Query, and Power Automate.\n\n- Strong understanding of data modeling and ETL processes.\n\n- Experience working with cloud-based data analytics platforms.\n\n- Familiarity with SQL and other data query languages.\n\nSoft Skills :\n\n- Excellent communication and interpersonal skills.\n\n- Strong problem-solving and analytical abilities.\n\n- Attention to detail and ability to deliver high-quality work",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Visualization Engineer', 'Reporting Analytics', 'Power BI', 'Dashboard Design', 'Data Visualization', 'Power Automate', 'Data Modeling', 'ETL', 'Power Query M', 'Data Integration', 'SQL']",2025-06-14 05:16:26
Software Data Operations Engineer,MAQ Software,5 - 7 years,Not Disclosed,"['Noida', 'Mumbai', 'Hyderabad']","Software Data Operations Engineer (BS+5)\n\nResponsible for analyzing, developing and managing cloud computing architecture for data-oriented applications. Work with cross functional team of business analysts, software programmers, engineers to define and own solution architecture. Work with clients to design a full-stack solution using software that fully integrates with customers existing cloud infrastructure from data flow, security, DevOps & other standpoints. Evaluate technical risks, costs and map out mitigation strategies. Create well-informed cloud strategy and administer the adaption process. Develop & organize cloud systems & work closely with IT security to monitor the companys cloud privacy. Develop new specs, documentation; & partake in the development of technical procedures & user support guides. Direct infrastructure movement procedure, including bulk application transfers into cloud environment. Design & manage server systems locally & on Amazon Web Services & Microsoft Azure cloud-based Data operations. Evaluate & monitor cloud applications, hardware & software at regular intervals. Carry out debugging, troubleshooting, modifications & unit testing of custom solutions built on client platform. Must be able to travel temporarily to client sites & or relocate throughout the United States.\n\nRequirements: Bachelors Degree or foreign equivalent in Computer Science, Computer Engineering, Information Technology or related field with 5 years of work experience in job offered, Software Engineer, Development Operations Engineer, Data Engineer, Programmer Analyst, Business Analytics or related job.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data operations', 'data analysis', 'data analytics', 'advanced excel', 'pivot table', 'sql']",2025-06-14 05:16:28
Staff Software Engineer - Cloud Data Pipeline,Calix,12 - 15 years,Not Disclosed,['Bengaluru'],"This position is based in Bangalore, India.\n\nCalix is leading a service provider transformation to deliver a differentiated subscriber experience around the Smart Home and Business, while monetizing their network using Role based Cloud Services, Telemetry, Analytics, Automation, and the deployment of Software Driven Adaptive networks.\nAs part of a high performing global team, the right candidate will play a significant role as Calix Cloud Data Engineer involved in architecture design, implementation, technical leadership in data ingestion, extraction, and transformation domain.\nResponsibilities and Duties:\nTechnical leadership in all phases of software design and development in meeting requirements of service stability, reliability, scalability, and security.\nHiring, training, provide technical direction and lead discussions and coordinate deliverables across multiple engineering teams globally.\nWork closely with Cloud product owners to understand, analyze product requirements, provide feedback, coordinate resources and deliver a complete solution.\nDrive evaluation and selection of best fit, efficient, cost-effective solution stack for the Calix Cloud data platform.\nDrive development of scalable data pipeline infrastructure and services for enabling operationally efficient analytics solutions for Calix Cloud suite of products.\nCreate and extend data lake solution to enable data science workbenches and implement quality systems to ensure data quality, consistency, security, compliance, and lineage.\nDrive continuous optimization of the data pipelines with automation, and tools.\nHave a Test first mindset and use modern DevSecOps practices for Agile development.\nCollaborate with senior leadership to translate platform opportunities into an actionable roadmap, track progress, and deliver new platform capabilities on-time and on-budget.\nTriage and resolve customer escalations and technical issues.\nQualifications:\n12 + years of highly technical, hands-on software engineering experience with at least 7 years of Cloud based solution development\n3+ experience leading and mentoring engineering team with strong technical direction and delivering high quality software on schedule, including delivery for large, cross-functional projects and working with geographically distributed teams\nStrong, creative problem-solving skills and ability to abstract and share details to create meaningful articulation.\nPassionate about delivering high quality software solutions and enabling automation in all phases.\nGood understanding of big data engineering challenges and proven experience with data platform engineering (batch and streaming, ingestion, storage, processing, management, governance, integration, consumption patterns)\nExperience in designing and performance tuning batch-based, low latency real-time streaming and event-based data solutions (Kafka, Spark, Flink or similar frameworks).\nPractical experience of architecting with GCP Cloud platform and services and especially the Data ecosystem: BigQuery, Datastream, DataProc, Composer etc.\nDeep understanding of Data Cataloging, Data Governance, Data Privacy principles and frameworks to integrate into the Data engineering flows.\nAdvanced knowledge of Data Lake technologies, data storage formats (Parquet, ORC, Avro) and query engines and associated concepts for consumption layers.\nExperience implementing solutions that adhere to best practices and guidelines for different privacy and compliance practices around data (GDPR, CCPA).\nHands on expert level on one or more of the following programming languages - Python, Java\nOrganized and goal-focused, ability to deliver in a fast-paced environment.\nBS degree in Computer Science, Engineering, Mathematics, or relevant industry standard experience to match",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Performance tuning', 'Automation', 'Software design', 'Quality systems', 'data governance', 'Data quality', 'data privacy', 'Analytics', 'Python']",2025-06-14 05:16:30
Staff Software Engineer - Cloud Infrastructure-Network Data Ingestion,Calix,3 - 7 years,Not Disclosed,['Bengaluru'],"Calix is leading a service provider transformation to deliver a differentiated subscriber experience around the Smart Home and Business, while monetizing their network using Role based Cloud Services, Telemetry, Analytics, Automation, and the deployment of Software Driven Adaptive networks.\n\nAs part of a high performing global engineering team, the right candidate will play a critical role in expanding the Calix Cloud Infrastructure capabilities and be part of a team that leads the effort defining and architecting a world class in-home eco-system. Calix Cloud empowers service providers with solutions for Insights and real time data to support delivery of the best customer experiences, improve operational efficiency and drive market penetration.\nResponsibilities\nDesign, develop and maintain backend infrastructure, workflows, and services for collection, processing, analysis, correlation, and monitoring in Calix Cloud\nDevelop solutions to support onboarding, partner integrations, managing, collecting, and analyzing data from large scale deployment of home networks and access network systems and make them available as insights for various BSP user roles.\nWork closely with Cloud product owners to understand, analyze product requirements, provide feedback, and deliver a complete solution.\nTechnical leadership of software design in meeting requirements of service stability, reliability, scalability, and security\nParticipate and drive technical discussions within engineering group in all phases of the SDLC: review requirements, produce design documents, participate in peer reviews, produce test plans, support QA team, provide internal training and support TAC team.\nSupport test strategy and automation in both end-to-end solution and functional testing.\nCustomer facing engineering role in debugging and resolving field issues.\nQualifications:\n10+ years of highly technical, hands-on software engineering experience delivering quality software releases.\nIndependent and Self driven and works in a Team.\nStrong, creative problem-solving skills and ability to abstract and share details to create meaningful articulation.\nAbility to drive technical discussions across x-functional teams.\nStrong Implementation background in distributed design, data consumption patterns, and pipelines and experience in designing real-time streaming and event-based data solutions\nProficient in design and implementation of microservices-based, API/Endpoint architectures\nStrong background in designing and developing event-based / pub-sub workflows & data ingestion solutions. Proficiency and hands on experience with Kafka at scale (or similar) desired.\nGood Experience with load balancers, WebSocket, MQTT and similar technologies at different layers for efficient data abstraction and transfer for large scale data connections / large flow of data\nGood understanding of implementation and deployment of Cloud based solutions (preferably GCP)\nStrong background in transactional databases and good understanding and experience with no-SQL datastores and working in defining optimal data models.\nGood understanding of Networking concepts.\nExpert in Go. Proficiency in other languages like Java, Python/JavaScript a plus.\nOrganized and goal-focused, ability to deliver in a fast-paced environment.\nEducation:\nBS degree in Computer Science, engineering, or mathematics or equivalent experience.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Automation', 'Software design', 'Functional testing', 'Debugging', 'Javascript', 'SDLC', 'Analytics', 'Monitoring', 'SQL', 'Python']",2025-06-14 05:16:32
Azure Data Factory (ADF) Engineer,Arieotech Solutions,3 - 5 years,Not Disclosed,['Pune'],"We are seeking an experienced Azure Data Factory Engineer to design, develop, and manage data pipelines using Azure Data Factory. The ideal candidate will possess hands-on expertise in ADF components and activities, and have practical knowledge of incremental data loading, file management, API integration, and cloud storage solutions. This role involves automating data workflows, optimizing performance, and ensuring the seamless flow of data within our cloud environment.\nKey Responsibilities:\nDesign and Develop Data Pipelines: Build and maintain scalable data pipelines using Azure Data Factory, ensuring efficient and reliable data movement and transformation.\nIncremental Data Loads: Implement and manage incremental data loading processes to ensure that only updated or new data is processed, optimizing data pipeline performance and reducing resource consumption.\nFile Management: Handle data ingestion and management from various file sources, including CSV, JSON, and Parquet formats, ensuring data accuracy and consistency.\nAPI Integration: Develop and configure data pipelines to interact with RESTful APIs for data extraction and integration, handling authentication and data retrieval processes effectively.\nCloud Storage Management: Work with Azure Blob Storage and Azure Data Lake Storage to manage and utilize cloud storage solutions, ensuring data is securely stored and easily accessible.\nADF Automation: Leverage Azure Data Factory s automation capabilities to schedule and monitor data workflows, ensuring timely execution and error-free operations.\nPerformance Optimization: Continuously monitor and optimize data pipeline performance, troubleshoot issues, and implement best practices to enhance efficiency.\nCollaboration: Work closely with data engineers, analysts, and other stakeholders to gather requirements, provide technical guidance, and ensure successful data integration solutions.\nQualifications:\nEducational Background: Bachelor s degree in Computer Science, Information Technology, or a related field (B. E, B.Tech, MCA, MCS). Advanced degrees or certifications are a plus.\nExperience: Minimum 3-5 years of hands-on experience with Azure Data Factory, including designing and implementing complex data pipelines.\nTechnical Skills:\nStrong knowledge of ADF components and activities, including datasets, pipelines, data flows, and triggers.\nProficiency in incremental data loading techniques and optimization strategies.\nExperience working with various file formats and handling large-scale data files.\nProven ability to integrate and interact with APIs for data retrieval and processing.\nHands-on experience with Azure Blob Storage and Azure Data Lake Storage.\nFamiliarity with ADF automation features and scheduling.\nSoft Skills:\nStrong problem-solving and analytical skills.\nExcellent communication and collaboration abilities.\nAbility to work independently and manage multiple tasks effectively.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Analytical skills', 'Automation', 'Storage management', 'cloud storage', 'JSON', 'Scheduling', 'Management', 'Information technology', 'Data extraction']",2025-06-14 05:16:34
Big Data Developer/ Senior Big Data Developer,Grid Dynamics,5 - 10 years,Not Disclosed,"['Hyderabad', 'Chennai', 'Bengaluru']","About us\nGrid Dynamics (NASDAQ: GDYN) is a leading provider of technology consulting, platform and product engineering, AI, and advanced analytics services. Fusing technical vision with business acumen, we solve the most pressing technical challenges and enable positive business outcomes for enterprise companies undergoing business transformation. A key differentiator for Grid Dynamics is our 8 years of experience and leadership in enterprise AI, supported by profound expertise and ongoing investment in data, analytics, cloud & DevOps, application modernization and customer experience. Founded in 2006, Grid Dynamics is headquartered in Silicon Valley with offices across the Americas, Europe, and India.\n\nRole & responsibilities\nWe are looking for an enthusiastic and technology-proficient Big Data Engineer, who is eager to participate in the design and implementation of a top-notch Big Data solution to be deployed at massive scale.\nOur customer is one of the world's largest technology companies based in Silicon Valley with operations all over the world. On this project we are working on the bleeding-edge of Big Data technology to develop high performance data analytics platform, which handles petabytes datasets.\nEssential functions\nParticipate in design and development of Big Data analytical applications.\nDesign, support and continuously enhance the project code base, continuous integration pipeline, etc.\nWrite complex ETL processes and frameworks for analytics and data management.\nImplement large-scale near real-time streaming data processing pipelines.\nWork inside the team of industry experts on the cutting edge Big Data technologies to develop solutions for deployment at massive scale.\nQualifications\nStrong coding experience with Scala, Spark,Hive, Hadoop.\nIn-depth knowledge of Hadoop and Spark, experience with data mining and stream processing technologies (Kafka, Spark Streaming, Akka Streams).\nUnderstanding of the best practices in data quality and quality engineering.\nExperience with version control systems, Git in particular.\nDesire and ability for quick learning of new tools and technologies.\nWould be a plus\nKnowledge of Unix-based operating systems (bash/ssh/ps/grep etc.).\nExperience with Github-based development processes.\nExperience with JVM build systems (SBT, Maven, Gradle).\nWe offer\nOpportunity to work on bleeding-edge projects\nWork with a highly motivated and dedicated team\nCompetitive salary\nFlexible schedule\nBenefits package - medical insurance, sports\nCorporate social events\nProfessional development opportunities\nWell-equipped office",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Big Data', 'SCALA', 'Hadoop', 'Spark']",2025-06-14 05:16:37
Senior Data Governance Engineer,Aviatrix Systems,5 - 10 years,Not Disclosed,['Bengaluru'],"As a Senior Data Governance Engineer with 5+ years of relevant experience, you will be instrumental in designing, building, and maintaining Aviatrixs data governance framework. This position will have a significant impact on our data infrastructure by promoting best practices, enhancing data transparency, and establishing policies that enable seamless cross-functional collaboration. You will work independently with various teams to implement data governance solutions and ensure our data meets the highest standards of quality and compliance. The role requires flexibility to align with USA working hours (including until midnight IST) to effectively collaborate with global teams.",,,,"['Python', 'SQL', 'Azure Cloud']",2025-06-14 05:16:39
Oracle BRM Data Migration Engineer,Techstar Group,5 - 10 years,Not Disclosed,"['Noida', 'Hyderabad', 'Bengaluru']","Work Location : Hyderabad, Bangalore, Noida, Pune\nQualifications and Skills :\n- Proven expertise in Oracle BRM (Mandatory skill) with a strong understanding of its architecture and modules to effectively manage data migration processes.\n\n- Hands-on experience in data migration activities, particularly with Oracle BRM, ensuring high efficiency and accuracy throughout migration projects.\n\n- Knowledge in SQL for querying and managing databases, crucial for data migration and integration tasks.\n\n- Strong knowledge of ETL tools and processes for efficient data extraction, transformation, and loading from various sources.\n\n- Ability to perform detailed data mapping, ensuring logical transformation and compatibility between source and target system data structures.\n\n- Experience in data cleansing techniques to ensure data integrity and consistency throughout the migration process.\n\n- Understanding of data quality principles and practices, essential to maintain high standards of data accuracy and dependability.\n\n- Proficiency in scripting for automation of data migration tasks, enhancing efficiency and reducing potential for errors.\n\n- Excellent analytical and problem-solving skills to identify and address data-related challenges and opportunities.\n\n- Handling the execution of the data migration and validations.\n\n- Handle the develop Migration strategy documents and techniques. Execute data integrity testing post migration.\n\n- Understanding BRM : Having a working knowledge of BRM data migration components, the BRM 12 schema, and the data model\n\n- Data migration strategy : Developing a migration strategy and implementation plan\n\n- Data loading : Being able to load data and integrate it with systems\n\n- Post-migration analysis : Performing post-migration analysis on events, invoices, open items, bills, and dunning\n\n- Data reconciliation : Developing scripts to reconcile migrated data\n\n- Working Knowledge of all the BRM Data migration components.\n\n- Must have hands-on in BRM to verify the sanity of the Data migration.\n\n- Advantage - Programming skills on Java technologies. Exp. in C/C++, Oracle 12c/19c, PL/SQL, PCM Java, BRM Webservice, Scripting language (perl/python)\n\nRoles and Responsibilities :\n\n- Analyze client data and formulating effective data migration plans tailored to Oracle BRM specifications.\n\n- Collaborate with cross-functional teams to gather and interpret data migration requirements accurately.\n\n- Develop and implement efficient data migration scripts and processes, ensuring minimal disruption to business operations.\n\n- Conduct thorough testing and validation of data migration outputs to guarantee data accuracy and conformity.\n\n- Monitor and troubleshoot migration activities to ensure seamless execution and rectify any issues promptly.\n\n- Document data migration processes, maps, and transformations for knowledge sharing and continuous improvement.\n\n- Liaise with stakeholders to present progress updates and discuss ongoing improvements to data migration practices.\n\n- Contribute to the development of data migration best practices and reusable frameworks within the organization.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Oracle BRM', 'Java', 'Data Quality', 'Oracle Apps', 'C++', 'PL/SQL', 'Data Migration', 'Data reconciliation', 'ETL', 'SQL']",2025-06-14 05:16:41
"SENIOR, DATA SCIENTIST",Walmart,3 - 8 years,Not Disclosed,['Bengaluru'],"Position Summary...\nWhat youll do...\nAbout Team\nThe Catalog Data Science Team at Walmart Global Tech is focused on using the latest research in generative AI (GenAI), artificial intelligence (AI), machine learning (ML), statistics, deep learning, computer vision and optimization to implement solutions that ensure Walmart s product catalog is accurate, complete, and optimized for customer experience. Our team tackles complex data science and ML engineering challenges related to product classification, attribute extraction, trust & safety, and catalog optimization, empowering next-generation retail use cases.\nThe Data Science and ML Engineering community at Walmart Global Tech is active in most of the Hack events, utilizing the petabytes of data at our disposal, to build some of the coolest ideas. All the work we do at Walmart Global Tech will eventually benefit our operations & our associates, helping Customers Save Money to Live Better.\nWhat youll do:\nAs a Senior Data Scientist - ML Engineer, you ll have the opportunity to:\nDrive research initiatives and proof-of-concepts that push the state of the art in generative AI and large-scale machine learning.\nDesign and implement high-throughput, low-latency AI/ML pipelines and microservices that operate at global scale.\nOversee data ingestion, model training, evaluation, deployment and monitoring-ensuring performance, quality and reliability.\nCustomize and optimize LLMs for specific business use cases, balancing accuracy, latency and cost.\nPrototype novel generative AI solutions, integrate advancements into production, and collaborate with research partners.\nChampion best practices in data quality, lineage, governance and cost optimization across ML pipelines.\nMentor a team of ML engineers, establish coding standards, conduct design reviews, and foster a culture of continuous improvement.\nPresent your team s work at top-tier AI/ML conferences, publish scientific papers, and cultivate partnerships with universities and research labs.\nWhat youll bring\nPhD in Computer Science, Statistics, Applied Mathematics or related field with 3+ years experience in ML engineering-or Master s with 6+ years or Bachelor s with 8+ years.\nProven track record of leading and scaling AI/ML products in production environments.\nDeep expertise in generative AI, large-scale model deployment, and fine-tuning of transformer-based architectures.\nStrong programming skills in Python, or equivalent, and experience with big data frameworks (Spark, Hadoop) and ML platforms (TensorFlow, PyTorch).\nDemonstrated history of scientific publications or patents in AI/ML.\nExcellent communication skills, a growth mindset, and the ability to drive cross-functional collaboration.\nAbout Walmart Global Tech\n.\n.\nFlexible, hybrid work\n.\nBenefits\n.\nBelonging\n.\n.\nEqual Opportunity Employer\nWalmart, Inc., is an Equal Opportunities Employer - By Choice. We believe we are best equipped to help our associates, customers and the communities we serve live better when we really know them. That means understanding, respecting and valuing unique styles, experiences, identities, ideas and opinions - while being inclusive of all people.\nMinimum Qualifications...\nMinimum Qualifications:Option 1- Bachelors degree in Statistics, Economics, Analytics, Mathematics, Computer Science, Information Technology, or related field and 3 years experience in an analytics related field. Option 2- Masters degree in Statistics, Economics, Analytics, Mathematics, Computer Science, Information Technology, or related field and 1 years experience in an analytics related field. Option 3 - 5 years experience in an analytics or related field.\nPreferred Qualifications...\nPrimary Location...\n\n\n",Industry Type: Retail,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Prototype', 'Networking', 'Coding', 'Machine learning', 'Continuous improvement', 'Information technology', 'Monitoring', 'Analytics', 'Python']",2025-06-14 05:16:44
"SENIOR, DATA SCIENTIST",Walmart,1 - 5 years,Not Disclosed,['Bengaluru'],"The Catalog Data Science Team at Walmart Global Tech is focused on using the latest research in generative AI (GenAI), artificial intelligence (AI), machine learning (ML), statistics, deep learning, computer vision and optimization to implement solutions that ensure Walmart s product catalog is accurate, complete, and optimized for customer experience. Our team tackles complex data science and ML engineering challenges related to product classification, attribute extraction, trust & safety, and catalog optimization, empowering next-generation retail use cases.\nThe Data Science and ML Engineering community at Walmart Global Tech is active in most of the Hack events, utilizing the petabytes of data at our disposal, to build some of the coolest ideas. All the work we do at Walmart Global Tech will eventually benefit our operations & our associates, helping Customers Save Money to Live Better.\nWhat youll do:\nAs a Senior Data Scientist - ML Engineer, you ll have the opportunity to:\nDrive research initiatives and proof-of-concepts that push the state of the art in generative AI and large-scale machine learning.\nDesign and implement high-throughput, low-latency AI/ML pipelines and microservices that operate at global scale.\nOversee data ingestion, model training, evaluation, deployment and monitoring-ensuring performance, quality and reliability.\nCustomize and optimize LLMs for specific business use cases, balancing accuracy, latency and cost.\nPrototype novel generative AI solutions, integrate advancements into production, and collaborate with research partners.\nChampion best practices in data quality, lineage, governance and cost optimization across ML pipelines.\nMentor a team of ML engineers, establish coding standards, conduct design reviews, and foster a culture of continuous improvement.\nPresent your team s work at top-tier AI/ML conferences, publish scientific papers, and cultivate partnerships with universities and research labs.\nWhat youll bring\nPhD in Computer Science, Statistics, Applied Mathematics or related field with 3+ years experience in ML engineering-or Master s with 6+ years or Bachelor s with 8+ years.\nProven track record of leading and scaling AI/ML products in production environments.\nDeep expertise in generative AI, large-scale model deployment, and fine-tuning of transformer-based architectures.\nStrong programming skills in Python, or equivalent, and experience with big data frameworks (Spark, Hadoop) and ML platforms (TensorFlow, PyTorch).\nDemonstrated history of scientific publications or patents in AI/ML.\nExcellent communication skills, a growth mindset, and the ability to drive cross-functional collaboration",Industry Type: Retail,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Prototype', 'Networking', 'Coding', 'Machine learning', 'Continuous improvement', 'Information technology', 'Monitoring', 'Analytics', 'Python']",2025-06-14 05:16:47
Data Science Engineer,ACL Digital,3 - 7 years,12-16 Lacs P.A.,['Chennai'],"Work Location: Chennai\n\nPlease share your updated profile to sugantha.krishnan@acldigital.com\n\nRole & responsibilities\nResponsibilities:\n- Collecting, cleaning, and preprocessing data from various sources\n- Developing and applying machine learning algorithms and statistical models to solve business problems\n- Conducting exploratory data analysis and data visualization\n- Collaborating with cross-functional teams to identify business opportunities and develop data-driven solutions\n- Developing and deploying data pipelines and ETL processes\n- Communicating insights and findings to technical and non-technical stakeholders\n- Evaluating the effectiveness and accuracy of models and recommending improvements\n- Keeping up-to-date with the latest trends and technologies in the field of data science\n\nQualifications\n- Minimum of 3 years' Relevant experience with Bachelors' or Masters' degree in STEM (Science, Technology, Engineering, and Mathematics)\n- Work experience where data science, data engineering and application development was a major part of your work in automating software deployments and following a continuous delivery and deployment model\n- Experience with mathematical & statistical modeling, Cloud environment, APIs, Kubernetes\n- Solid experience in program coding with Python, SQL,or R Programming\n- Experience in visualization tools like Power BI\n- Experience in data science, machine learning or artificial intelligence technologies",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['R', 'Dash', 'Python', 'R shiny']",2025-06-14 05:16:50
"SENIOR, DATA SCIENTIST",Walmart,1 - 5 years,Not Disclosed,['Bengaluru'],"Position Summary...\nWhat youll do...\nAbout Team\nThe Catalog Data Science Team at Walmart Global Tech is focused on using the latest research in generative AI (GenAI), artificial intelligence (AI), machine learning (ML), statistics, deep learning, computer vision and optimization to implement solutions that ensure Walmart s product catalog is accurate, complete, and optimized for customer experience. Our team tackles complex data science and ML engineering challenges related to product classification, attribute extraction, trust & safety, and catalog optimization, empowering next-generation retail use cases.\nThe Data Science and ML Engineering community at Walmart Global Tech is active in most of the Hack events, utilizing the petabytes of data at our disposal, to build some of the coolest ideas. All the work we do at Walmart Global Tech will eventually benefit our operations & our associates, helping Customers Save Money to Live Better.\nWhat youll do:\nAs a Senior Data Scientist for Walmart Global Tech, you ll have the opportunity to\nDesign, develop, and deploy AI/ML, NLP, LLM models into production environments with a focus on reliability and scalability\nIntegrate data science solutions into current business processes.\nDevelop and recommend process standards and best practices in Machine Learning as applicable to the retail industry.\nSpearhead collaborations with other senior team members and stakeholders, leveraging your data science expertise to drive strategic decision-making and optimize business operations\nPromote and support company policies, procedures, mission, values, and standards of ethics and integrity.\nWhat youll bring:\nQualifications\nPhD with >3 years of relevant experience / 4-year bachelor s degree with > 8 years of experience / Master s degree with > 6 years of experience. Educational qualifications should be preferably in Computer Science or a strongly quantitative discipline.\nDemonstrated history of strong hands-on experience in AI/ML modelling\nPrior Experience in building Vision-based models\nProven records of scientific publications or intellectual property generation\nPrior Experience in programming skills across data science, statistical analysis, big data and ML stack\nStrong communication skills with inclination to high ownership and commitment\nProven track record of delivering high-impact AI/ML solutions to Production\nMandatory Skills: Machine Learning, NLP, Computer Vision , Python, R\nAdditional Qualifications: Good to have experience in areas such as Graph Neural Networks, LLM Optimization\nAbout Walmart Global Tech\n.\n.\nFlexible, hybrid work\n.\nBenefits\n.\nBelonging\n.\n.\nEqual Opportunity Employer\nWalmart, Inc., is an Equal Opportunities Employer - By Choice. We believe we are best equipped to help our associates, customers and the communities we serve live better when we really know them. That means understanding, respecting and valuing unique styles, experiences, identities, ideas and opinions - while being inclusive of all people.\nMinimum Qualifications...\nMinimum Qualifications:Option 1- Bachelors degree in Statistics, Economics, Analytics, Mathematics, Computer Science, Information Technology, or related field and 3 years experience in an analytics related field. Option 2- Masters degree in Statistics, Economics, Analytics, Mathematics, Computer Science, Information Technology, or related field and 1 years experience in an analytics related field. Option 3 - 5 years experience in an analytics or related field.\nPreferred Qualifications...\nPrimary Location...",Industry Type: Retail,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Computer vision', 'Networking', 'Neural networks', 'Artificial Intelligence', 'Intellectual property', 'Machine learning', 'Information technology', 'Analytics', 'Python']",2025-06-14 05:16:53
JavaScript Engineer For Training AI Data,G2i Inc,3 - 8 years,Not Disclosed,[],"Evaluating the quality of AI-generated code, including human-readable summaries of your rationale.\nBuilding and evaluating React components, hooks, and modern JavaScript solutions.\nSolving coding problems and writing functional and efficient JavaScript/React code.\nWriting robust test cases to confirm code works efficiently and effectively.\nCreating instructions to help others and reviewing code before it goes into the model.\nEngaging in a variety of projects, from evaluating code snippets to developing full mobile applications using chatbots.\nPay Rates\nCompensation rates average at $30/hr and can go up to $50+/hr. Expectations are 15+ hours per week; however, there is no upper limit. You can work as much as you want and will be paid weekly per hour of work done on the platform.\nContract Length\nThis is a long-term contract with no end date. We expect to have work for the next 2 years. You can end the contract at any time, but we hope you will commit to 12 months of work.\nFlexible Schedules\nDevelopers can set their own hours. Ideal candidates will be interested in spending 40 hours a week. You will be assigned to projects, so strong performers will adapt to the urgency of projects and stay engaged, but we are incredibly flexible on working hours. You can take a 3-hour lunch with no problem. Instead of tracking your hours, you are paid according to time spent on the platform, calculated in the coding exercises.\nInterview Process\nApply using this Ashby form.\nIf you seem like a good fit, well send an async RLHF code review that will take 35 minutes and must be finished within 72 hours of us sending it.\nYou ll receive credentials to the RLHF platform. We re doing regular calls to answer any further questions about onboarding, as well as providing a support team at your disposal.\nYou ll perform a simulated production-level task (RLHF task) on the platform. This will be the final stage, which will ultimately determine your leveling and which project you ll be assigned. Successful completion of this process provides you with an opportunity to work on projects as they become available.\nTech Stack Priorities\nThe current priority for this team is frontend engineers who are well versed in JavaScript, React, and modern web development frameworks and libraries.\nRequired Qualifications\n3+ years of experience in a software engineering/software development role.\nStrong proficiency with JavaScript/React and frontend development.\nComplete fluency in the English language.\nAbility to articulate complex technical concepts clearly and engagingly.\nExcellent attention to detail and ability to maintain consistency in writing. Solid understanding of grammar, punctuation, and style guidelines.\nNice To Haves:\nBachelors or Masters degree in Computer Science.\nExperience with modern JavaScript frameworks and libraries (Next.js, Vue, Angular).\nFamiliarity with frontend testing frameworks (Jest, React Testing Library, Cypress).\nKnowledge of state management solutions (Redux, Context API, MobX).\nExperience with TypeScript and modern frontend tooling.\nRecognized accomplishments or contributions to the coding community or in projects.\nProven analytical skills with an ability to approach problems creatively.\nAdept communication skills, especially when understanding and discussing project requirements.\nA commitment to continuous learning and staying updated with the latest coding advancements and best practices.\nEnthusiasm for teaching AI models and experience with technical writing!",Industry Type: Software Product,Department: Other,"Employment Type: Full Time, Permanent","['Computer science', 'Analytical skills', 'Front end', 'Technical writing', 'Coding', 'Web development', 'Javascript', 'Test cases', 'Mobile applications', 'Software engineering']",2025-06-14 05:16:56
Data Modeler,Synechron,0 - 2 years,Not Disclosed,['Bengaluru'],"Synechron is seeking a knowledgeable and proactive Data Modeler to guide the design and development of data structures that support our clients' business objectives. In this role, you will collaborate with cross-functional teams to translate business requirements into scalable and efficient data models, ensuring data accuracy, consistency, and integrity. You will contribute to creating sustainable and compliant data architectures that leverage emerging technologies such as cloud, IoT, mobile, and blockchain. Your work will be instrumental in enabling data-driven decision-making and operational excellence across projects.Software Required",,,,"['data modeling', 'modeling tools', 'relational databases', 'scrum', 'agile', 'confluence', 'hipaa', 'data warehousing', 'data architecture', 'erwin', 'sql', 'git', 'gcp', 'mysql', 'etl', 'mongodb', 'jira', 'python', 'oracle', 'microsoft azure', 'sql server', 'nosql', 'gdpr', 'cassandra', 'aws', 'data integration', 'sdlc']",2025-06-14 05:16:59
Software Data Operations Engineer (BS+2),MAQ Software,2 - 5 years,Not Disclosed,['Noida'],"MAQ LLC d.b.a MAQ Software hasmultiple openings at Redmond, WA for:\nSoftware Data Operations Engineer (BS+2)\n\nResponsible for gathering & analyzing business requirements from customers. Implement,test and integrate software applications for use by customers. Develop &review cost effective data architecture to ensure appropriateness with currentindustry advances in data management, cloud & user experience. Automateuser test scenarios, debug & fix errors in cloud-based data infrastructure,reporting applications to meet customer needs. Must be able to traveltemporarily to client sites and or relocate throughout the United States.\n\nRequirements:Bachelors Degree or foreign equivalent in Computer Science, ComputerApplications, Computer Information Systems, Information Technology or relatedfield with two years of work experience in job offered, software engineer, systemsanalyst or related job.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Software Data Operations', 'software applications', 'data management', 'Data Operations', 'data architecture', 'data infrastructure']",2025-06-14 05:17:02
"Staff Engr - Data Science(Advanced OOPS,Python,PySpark,Databricks)",The TJX Companies Inc,5 - 10 years,Not Disclosed,['Hyderabad'],"TJX Companies\nAt TJX Companies, every day brings new opportunities for growth, exploration, and achievement. You ll be part of our vibrant team that embraces diversity, fosters collaboration, and prioritizes your development. Whether you re working in our four global Home Offices, Distribution Centers or Retail Stores TJ Maxx, Marshalls, Homegoods, Homesense, Sierra, Winners, and TK Maxx, you ll find abundant opportunities to learn, thrive, and make an impact. Come join our TJX family a Fortune 100 company and the world s leading off-price retailer.\nJob Description:\nAbout TJX:\nAt TJX, is a Fortune 100 company that operates off-price retailers of apparel and home fashions. TJX India - Hyderabad is the IT home office in the global technology organization of off-price apparel and home fashion retailer TJX, established to deliver innovative solutions that help transform operations globally. At TJX, we strive to build a workplace where our Associates contributions are welcomed and are embedded in our purpose to provide excellent value to our customers every day. At TJX India, we take a long-term view of your career. We have a high-performance culture that rewards Associates with career growth opportunities, preferred assignments, and upward career advancement. We take well-being very seriously and are committed to offering a great work-life balance for all our Associates.\nWhat you ll discover\nInclusive culture and career growth opportunities\nA truly Global IT Organization that collaborates across North America, Europe, Asia and Australia, click here to learn more\nChallenging, collaborative, and team-based environment\nWhat you ll do\nThe Global Supply Chain - Logistics Team is responsible for managing various supply chain logistics related solutions within TJX IT. The organization delivers capabilities that enrich the customer experience and provide business value. We seek a motivated, talented Staff E ngineer with good understanding of cloud base, database and BI concepts to help architect enterprise reporting solutions across global buying, planning and allocations .\nWhat you ll need\nThe Global Supply Chain - Logistics Team thrives on strong relationships with our business partners and working diligently to address their needs which supports TJX growth and operational stability. On this tightly knit and fast-paced solution delivery team you will be constantly challenged to stretch and think outside the box .\nYou will be working with product teams, architecture and business partners to strategically plan and deliver the product features by connecting the technical and business worlds. You will need to break down complex problems into steps that drive product development while keeping product quality and security as the priority. You will be responsible for most architecture, design and technical decisions within the assigned scope.\nKey Responsibilities:\nDesign, develop, test and deploy AI solutions using Azure AI services to meet business requirements , working collaboratively with architects and other engineers.\nTrain, fine-tune, and evaluate AI models, including large language models (LLMs), ensuring they meet performance criteria and integrate seamlessly into new or existing solutions.\nDevelop and integrate APIs to enable smooth interaction between AI models and other applications, facilitating efficient model serving.\nCollaborate effectively with cross-functional teams, including data scientists, software engineers, and business stakeholders, to deliver comprehensive AI solutions.\nOptimize AI and ML model performance through techniques such as hyperparameter tuning and model compression to enhance efficiency and effectiveness.\nMonitor and maintain AI systems, providing technical support and troubleshooting to ensure continuous operation and reliability.\nCreate comprehensive documentation for AI solutions, including design documents, user guides, and operational procedures, to support development and maintenance.\nStay updated with the latest advancements in AI, machine learning, and cloud technologies, demonstrating a commitment to continuous learning and improvement.\nDesign, code, deploy, and support software components, working collaboratively with AI architects and engineers to build impactful systems and services.\nLead medium complex initiatives, prioritizing and assigning tasks, providing guidance, and resolving issues to ensure successful project delivery.\nMinimum Qualifications\nBachelors degree in computer science, engineering, or related field\n8 + years of experience in data /software engineering, design, implementation and architecture.\nAt least 5+ years of hands-on experience in developing AI/ML solutions, with a focus on deploying them in a cloud environment .\nDeep understanding of AI and ML algorithms with focus on Operations Research / Optimization knowledge ( preferably M etaheuristics / Genetic Algorithms) .\nStrong programming skills in Python with advanced OOPS concepts.\nGood understanding of structured, semi structured, and unstructured data, Data modelling, Data analysis, ETL and ELT .\nProficiency with Databricks & PySpark .\nExperience with MLOps practices including CI/CD for machine learning models.\nKnowledge of security best practices for deploying AI solutions, including data encryption and access control.\nKnowledge of ethical considerations in AI, including bias detection and mitigation strategies.\nThis role operates in an Agile/Scrum environment and requires a solid understanding of the full software lifecycle, including functional requirement gathering, design and development, testing of software applications, and documenting requirements and technical specifications.\nFully Owns Epics with decreasing guidance. Takes initiative through identifying gaps and opportunities.\nStrong communication and influence skills. Solid team leadership with mentorship skills\nAbility to understand the work environment and competing priorities in conjunction with developing/meeting project goals .\nShows a positive, open-minded, and can-do attitude .\nExperience in the following technologies:\nAdvanced Python programming ( OOPS)\nOperations Research / Optimization knowledge ( preferably M etaheuristics / Genetic Algorithms)\nDatabricks with Pyspark\nAzure / Cloud knowledge\nGithub / version control\nFunctional knowledge on Supply Chain / Logistics is preferred.\nIn addition to our open door policy and supportive work environment, we also strive to provide a competitive salary and benefits package. TJX considers all applicants for employment without regard to race, color, religion, gender, sexual orientation, national origin, age, disability, gender identity and expression, marital or military status, or based on any individuals status in any group or class protected by applicable federal, state, or local law. TJX also provides reasonable accommodations to qualified individuals with disabilities in accordance with the Americans with Disabilities Act and applicable state and local law.\nAddress:\nSalarpuria Sattva Knowledge City, Inorbit Road\nLocation:\nAPAC Home Office Hyderabad IN",Industry Type: Retail,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Supply chain', 'Data analysis', 'Operations research', 'Architecture', 'OOPS', 'Cloud', 'Machine learning', 'Technical support', 'Python', 'Logistics']",2025-06-14 05:17:06
Senior Data Scientist,Ericsson,3 - 8 years,Not Disclosed,['Bengaluru'],"About this Opportunity\nThe complexity of running and optimizing the next generation of wireless networks, such as 5G with distributed edge compute, will require Machine Learning (ML) and Artificial Intelligence (AI) technologies. Ericsson is setting up an AI Accelerator Hub in India to fast-track our strategy execution, using Machine Intelligence (MI) to drive thought leadership, automate, and transform Ericsson s offerings and operations. We collaborate with academia and industry to develop state-of-the-art solutions that simplify and automate processes, creating new value through data insights.\n\nAs a Senior Data Scientist, you will apply your knowledge of data science and ML tools backed with strong programming skills to solve real-world problems.\nResponsibilities:\n1. Lead AI/ML features/capabilities in product/business areas\n2. Define business metrics of success for AI/ML projects and translate them into model metrics\n3. Lead end-to-end development and deployment of Generative AI solutions for enterprise use cases\n4. Design and implement architectures for vector search, embedding models, and RAG systems\n5. Fine-tune and evaluate large language models (LLMs) for domain-specific tasks\n6. Collaborate with stakeholders to translate vague problems into concrete Generative AI use cases\n7. Develop and deploy generative AI solutions using AWS services such as SageMaker, Bedrock, and other AWS AI tools. Provide technical expertise and guidance on implementing GenAI models and best practices within the AWS ecosystem.\n8. Develop secure, scalable, and production-grade AI pipelines\n9. Ensure ethical and responsible AI practices\n10. Mentor junior team members in GenAI frameworks and best practices\n11. Stay current with research and industry trends in Generative AI and apply cutting-edge techniques\n12. Contribute to internal AI governance, tooling frameworks, and reusable components\n13. Work with large datasets including petabytes of 4G/5G networks and IoT data\n14. Propose/select/test predictive models and other ML systems\n15. Define visualization and dashboarding requirements with business stakeholders\n16. Build proof-of-concepts for business opportunities using AI/ML\n17. Lead functional and technical analysis to define AI/ML-driven business opportunities\n18. Work with multiple data sources and apply the right feature engineering to AI models\n19. Lead studies and creative usage of new/existing data sources",,,,"['Wireless', 'Computer science', 'Data analysis', 'cassandra', 'Neural networks', 'Artificial Intelligence', 'Machine learning', 'Telecommunication', 'data visualization', 'Python']",2025-06-14 05:17:08
Data Warehouse Engineer (SSIS),Tek Experts,9 - 10 years,Not Disclosed,['New Delhi'],"We re searching for a Data Warehouse Engineer with a proven track record of developing Business Intelligence solutions to drive key outcomes. The role is responsible for different layers of the data hierarchy - including database design, data collection and storage techniques, to a deep understanding of data transformation tools and methodologies, the provisioning and managing of analytical databases, and building infrastructures that bring machine learning capabilities into production. The role will operate within the Business Analytics unit under the Business Analytics Global Manager.\nAt TeKnowledge , your work makes an impact from day one. We partner with organizations to deliver AI-First Expert Technology Services that\ndrive meaningful impact in AI, Customer Experience, and Cybersecurity. We turn complexity into clarity and potential into progress in a place where people lead and tech empowers.\nYou ll be part of a diverse and inclusive team where trust, teamwork, and shared success fuel everything we do. We push boundaries, using advanced technologies to solve complex challenges for clients around the world.\nHere, your work drives real change, and your ideas help shape the future of technology. We invest in you with top-tier training, mentorship, and career development ensuring you stay ahead in an ever-evolving world.\nWhy You ll Enjoy It Here:\nBe Part of Something Big A growing company where your contributions matter.\nMake an Immediate Impact Support groundbreaking technologies with real-world results.\nWork on Cutting-Edge Tech AI, cybersecurity, and next-gen digital solutions.\nThrive in an Inclusive Team A culture built on trust, collaboration, and respect.\nWe Care Integrity, empathy, and purpose guide every decision.\nWe re looking for innovators, problem-solvers, and experts ready to drive change and grow with us.\nWe Are TeKnowledge. Where People Lead and Tech Empowers.\n\nResponsibilities\n\nEnd to end development of data models, including gathering the data sources, developing ETL processes, and developing front end data model solutions for our business to help our users make smarter decisions.\nFull responsibility on our DWH including infrastructure, data modeling, audit logging, etc.\nBuilding automated validation processes to ensure data integrity.\nOptimizing processes run-time and solving problems in a scalable manner.\nTaking full ownership of designing, building and deployment of data products.\nCollaborating with Backend, Data Science, Data Analysis, and Product teams.\n\nQualifications\n\nBachelor s degree in Information Systems, Industrial Engineering, or equivalent experience is required.\nProfessional fluency in English is essential, both written and spoken.\nThree or more years of experience in BI solutions development (DWH, ETL) as well as in SQL and ETL tools like SSIS is required.\nExperience with building automated validation processes to ensure data integrity is considered an advantage.\nExperience in data modeling, working ETL tools and methodology, as well as with BI reporting tools is required.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data analysis', 'Backend', 'Front end', 'Database design', 'Business analytics', 'Machine learning', 'HTML', 'SSIS', 'Business intelligence', 'SQL']",2025-06-14 05:17:11
Hiring For Big Data Lead with Infogain,Infogain,12 - 15 years,Not Disclosed,['Singapore'],"BIG Data Lead\n\nExperience: 12-15 Years\nLocation: Singapore\nNotice Period: 0-15 Days\n\nJob Description:\nMandatory Skills :\n(SQL Server / Oracle / DB2 / Netezza) at-least good working knowledge in 2 of these DB\nApache Spark Streaming or Apache Flink\nKafka\nNOSQL databases - Cosmos DB, Document DB\nSpark, Dataframe API\nHive (HQL)\nScripting language Shell or bash\nCI CD\nExperience with at least one Cloud Infra provider (Azure/AWS)\nGood to have Skills :\nCertifications related to Data and Analytics",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Big Data', 'Spark', 'Data Modeling', 'Azure', 'Hive', 'Apache Flink', 'Hadoop', 'Kafka', 'Spark Streaming', 'AWS.', 'Shell Python']",2025-06-14 05:17:14
Data Migration Engineer,Echo It Solutions,3 - 8 years,Not Disclosed,['Gurugram'],"experienced SSIS Developer to design, develop, deploy, and maintain ETL solutions using SSIS. extensive hands-on experience in data migration, data transformation, integration workflows between multiple systems, Oracle Cloud Infrastructure.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Migration', 'Oracle Cloud Infrastructure', 'SSIS', 'ETL']",2025-06-14 05:17:16
Data Analytics Engineer,Automotive Industry,5 - 6 years,Not Disclosed,['Chennai'],"Position: Data Analytics Engineer\nExp: 5 -6 years\nNP: Immediate - 30 days\nQualification: B.tech\nLocation: Chennai Hybrid\nPrimary: Google Cloud Platform ,Python skills and Big Data pipeline\nSecondary: Big Query SQ, coding, testing, implementing, debugging workflows and apps\nKindly share your updated resume to aishwarya_s@onwardgroup.com\nKindly fill the below details\nTotal Exp:\nRelevant Exp:\nNotice Period: CTC:\nECTC:\nIf servicing NP, Last working Day, offered location & CTC:\nAvailable for Video modes interview on Weekdays (Y/N) :\nPAN Number:\nName as Per PAN Card:\nDate of Birth:\nAlternative Contact No:\nReason for Job Change:",Industry Type: Automobile,Department: Engineering - Hardware & Networks,"Employment Type: Full Time, Permanent","['GCP', 'Bigquery', 'Google Cloud Platform', 'Query SQ', 'Python']",2025-06-14 05:17:18
"CAD, Engineering Isometric Data Specialist",BP INCORPORATE INTERNATIONAL.,5 - 10 years,Not Disclosed,['Pune'],"We are a global energy business involved in every aspect of the energy system. We are working towards delivering light, heat, and mobility to millions of people every day. We are one of the very few companies equipped to lead some of the challenges that matter for the future. We have a real contribution to make to the worlds ambition of a low-carbon future. Join us and chip in to what we can accomplish together. You can participate in our new ambition to become a net zero company by 2050 or sooner and help the world get to net zero.\nWould you like to discover how our diverse, hardworking people are guiding the way in making energy cleaner and better - and how you can play your part in our outstanding team?\nJoin our Finance Team and advance your career as a Data Operations - CAD, Engineering Isometric Data Senior Analyst\nKey Accountability\nModifying 2D drawings and 3D Models of engineering and manufacturing design by using CAD or similar software in accordance to bp or industry standards associated to assets and projects across P&O globally.\nCalculating the dimensions and other physical components of the design to ensure the technical drawing has the accurate scale as per bp standards\nAdhering with engineering subject areas accountable for the content of the 2D drawing and 3D models to review and approve modifications prior to 2D drawings being issued as approved documents or 3D Models being published in ALIM.\nAssess the quality of data, documents and models in ALIM proposing corrections to engineering to improve integrity of the design associated to assets and projects across P&O globally.\nImplement effective controls for internal delegation, audit and control of the quality of engineering data, documents and models in ALIM based on engineering standards and procedures and through working with the Engineering Data Field Lead and Engineering.\nAssesses and handles risks around the use of engineering data, documents and models stored in ALIM based on the quality assessment.\nChip in to reports on the consolidated status of quality and performance criteria as set by the Engineering Data, Doc and Model Delivery and Engineering Data, Doc and Model Field Lead to effective decision making.\nEnsures that data, document are presented in accordance with policies, standards, procedures, guidelines etc.\nContinuously improves IMD engineering data, document and model standards, tools, systems and processes through the Class Library and ALIM change request process as per the findings of assessments.\nBalances problems and issues, leading resolutions, corrective actions, lessons learned and the collection and dissemination of relevant information. Responds to a broad range of service requests for support by providing information to fulfill requests or enable resolution.\nProvides first line investigation and diagnosis of issues associated to the use of ALIM and promptly allocates unresolved issues as appropriate. Assists with the development of standards, and applies these to run, monitor, report, resolve or advance issues.\nEducation and Qualification\nBachelor s Degree or equivalent experience in Engineering (Preferably Mechanical, Electrical, Electrical instrumentation)\n5+ years of work experience in document controller, engineering data and in leading a client-service oriented function with experience in management of large corporate initiatives/projects, critical thinking, relationship management and processes\nDirect process management experience including standard methodologies, driving innovation, continuous improvement, technologies, processes, procedures and tools\nExperience in JDE, ALIM, Sales Force, Hazid Tool, SAP or other CRM systems\nAbility to analyze raw data/input images, drawing conclusions and developing recommendations. Knowledge on MRO spares, Oil & Gas Equipments and spare parts\nItem identification, classification coding, usage of parametric data (attributes), application of standards. Enrichment: Searching part from the manufacturer website and extract the required information Cleansing & normalize the extracted data & update it in the client approved template\nExposure to UNSPSC, taxonomy creation, schema mapping and defining attribute structure. working cross culturally and in an international environment\nWe are an equal opportunity employer and value diversity at our company! We do not discriminate based on race, religion, colour, national origin, sex, gender, gender expression, sexual orientation, age, marital status, veteran status, or disability status.\nWe will ensure that individuals with disabilities are provided with reasonable accommodation to participate in the job application or interview process, to perform crucial job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation .\n\nTravel Requirement\nNegligible travel should be expected with this role\n\nRelocation Assistance:\nThis role is eligible for relocation within country\n\nRemote Type:\nThis position is a hybrid of office/remote working\n\nSkills:\nAgility core practices, Agility core practices, Analytical Thinking, Asset Life Cycle Management, Automation system digital security, Commercial Acumen, Commissioning, start-up and handover, Communication, Competency Management, Creativity and Innovation, Digital fluency, Earthing and grounding systems, Electrical operational safety, Factory acceptance testing, Financial Management, Fire and gas philosophy, Functional Safety, Hazard Identification, Hazard identification and protection layer definition, Instrument and protective systems, Management of change, Network Technologies, Safe and reliable operations, Safety critical equipment, Site Acceptance Testing {+ 1 more}",Industry Type: Oil & Gas,Department: Other,"Employment Type: Full Time, Permanent","['Automation', 'SAP', 'Coding', 'Analytical', 'CAD', 'Instrumentation', 'Continuous improvement', 'Risk management', 'Auditing', 'CRM']",2025-06-14 05:17:20
Data Visualization Engineer,Maimsd Technology,4 - 7 years,Not Disclosed,['Pune'],"Experience : 4 - 7 Yrs\n\nEmployment Type : Full Time, Permanent\n\nWorking mode : Regular\n\nNotice Period : Immediate - 15 Day\n\nAbout the Role :\n\nWe are seeking a skilled Data Visualization Engineer to join our team and transform raw data into actionable insights. You will play a crucial role in designing, developing, and maintaining interactive dashboards and reports using Power BI, Power Apps, Power Query, and Power Automate.\n\nResponsibilities :\n\n- Data Visualization : Create compelling and informative dashboards and reports using Power BI, effectively communicating complex data to stakeholders.\n\n- Data Integration : Import data from various sources (e.g., databases, spreadsheets, APIs) using Power Query and ensure data quality and consistency.\n\n- Data Modeling : Develop robust data models in Power BI to support complex analysis and reporting requirements.\n\n- Power Apps Development : Create custom applications using Power Apps to enable data-driven decision-making and automate workflows.\n\n- Power Automate Integration : Automate repetitive tasks and workflows using Power Automate to improve efficiency and reduce errors.\n\n- Cloud Data Analytics : Leverage cloud-based data analytics platforms to process and analyze large datasets.\n\n- Collaboration : Work closely with data analysts, data scientists, and business users to understand their requirements and deliver effective visualizations.\n\nQualifications :\n\nExperience : 4-7 years of experience in data visualization and business intelligence.\n\nTechnical Skills :\n\n- Proficiency in Power BI, Power Apps, Power Query, and Power Automate.\n\n- Strong understanding of data modeling and ETL processes.\n\n- Experience working with cloud-based data analytics platforms.\n\n- Familiarity with SQL and other data query languages.\n\nSoft Skills :\n\n- Excellent communication and interpersonal skills.\n\n- Strong problem-solving and analytical abilities.\n\n- Attention to detail and ability to deliver high-quality work",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Visualization', 'Reporting Analytics', 'Power BI', 'Dashboard Design', 'Power Automate', 'Data Modeling', 'ETL', 'Power Query M', 'Data Integration', 'SQL']",2025-06-14 05:17:22
Senior Professional Data Science,Next Gear India,8 - 10 years,Not Disclosed,['Bengaluru'],"Job Description\nDiagnose model performance issues; research, implement and test alternative options; prepare analyses; and deploy solutions.\nConduct complex statistical analyses of real estate, mortgage, and insurance related data.\nApply machine learning and data mining techniques to discover patterns from large and complex datasets.\nMake forecasts and predictions based on modeling and analysis of multiple economic and human factors.  \nPrepare and maintain programs and documentation for models.\nPerform modeling and analysis using various data mining, machine learning, statistical, and mathematical techniques.  \nDrive the re-engineering of current production models and their prototypes as needed.\nLead and manage quantitative and qualitative research projects independently.\nAssess and communicate research results to internal stakeholders.\nUnderstand business requirements and conceptualize analytic solutions to those business problems.\nRegularly engage cross functional teams to research, define, and implement analytic solutions within a broad business function.\nTroubleshoot and drive solutions on a wide variety of highly complex problems.\nResearch and test GenAI best practices and solutions for emerging issues, as appropriate.\nJob Qualifications\nMasters degree in Economics, Mathematics, Computer Science, Data Science, Machine Learning, Statistics, Physics or related field.\nAt least 8 years industry or academic experience in AI, machine learning techniques, data mining, statistical modeling and/or pattern recognition model building for clients on real world problems\nProficiency in network analysis and graph theory is a plus\nExperience building and maintaining models in a cloud environment\nProficiency in at least one of the following programming languages: Python, Java, R, C++, Scala\nMust have big data expertise, such as Spark, Hadoop, Hive, etc.\nExperience integrating GenAI into production solutions a plus.\nTeam player, willing to assist and jump into a project at any point as needed\nDetail oriented; strong presentation skills, experienced with client interaction. Ability to communicate effectively and in a professional manner to key stakeholders and explain business needs to technical personnel. Solid interpersonal skills. Must be able to communicate clearly to partners and colleagues verbally and in written form.\nAble to manage multiple, complex projects simultaneously.\nAble to effectively present analytic insights and modeling techniques internally and externally.",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Java', 'C++', 'machine learning techniques', 'R', 'Python', 'GenAI', 'Scala', 'data mining', 'Hadoop', 'AI', 'statistical modeling pattern recognition', 'Hive', 'Spark']",2025-06-14 05:17:24
Big Data Developer,Techstar Group,7 - 10 years,Not Disclosed,['Hyderabad'],"Responsibilities of the Candidate :\n\n- Be responsible for the design and development of big data solutions. Partner with domain experts, product managers, analysts, and data scientists to develop Big Data pipelines in Hadoop\n\n- Be responsible for moving all legacy workloads to a cloud platform\n\n- Work with data scientists to build Client pipelines using heterogeneous sources and provide engineering services for data PySpark science applications\n\n- Ensure automation through CI/CD across platforms both in cloud and on-premises\n\n- Define needs around maintainability, testability, performance, security, quality, and usability for the data platform\n\n- Drive implementation, consistent patterns, reusable components, and coding standards for data engineering processes\n\n- Convert SAS-based pipelines into languages like PySpark, and Scala to execute on Hadoop and non-Hadoop ecosystems\n\n- Tune Big data applications on Hadoop and non-Hadoop platforms for optimal performance\n\n- Apply an in-depth understanding of how data analytics collectively integrate within the sub-function as well as coordinate and contribute to the objectives of the entire function.\n\n- Produce a detailed analysis of issues where the best course of action is not evident from the information available, but actions must be recommended/taken.\n\n- Assess risk when business decisions are made, demonstrating particular consideration for the firm's reputation and safeguarding Citigroup, its clients, and assets, by driving compliance with applicable laws, rules, and regulations, adhering to Policy, applying sound ethical judgment regarding personal behavior, conduct, and business practices, and escalating, managing and reporting control issues with transparency\n\nRequirements :\n\n- 6+ years of total IT experience\n\n- 3+ years of experience with Hadoop (Cloudera)/big data technologies\n\n- Knowledge of the Hadoop ecosystem and Big Data technologies Hands-on experience with the Hadoop eco-system (HDFS, MapReduce, Hive, Pig, Impala, Spark, Kafka, Kudu, Solr)\n\n- Experience in designing and developing Data Pipelines for Data Ingestion or Transformation using Java Scala or Python.\n\n- Experience with Spark programming (Pyspark, Scala, or Java)\n\n- Hands-on experience with Python/Pyspark/Scala and basic libraries for machine learning is required.\n\n- Proficient in programming in Java or Python with prior Apache Beam/Spark experience a plus.\n\n- Hand on experience in CI/CD, Scheduling and Scripting\n\n- Ensure automation through CI/CD across platforms both in cloud and on-premises\n\n- System level understanding - Data structures, algorithms, distributed storage & compute\n\n- Can-do attitude on solving complex business problems, good interpersonal and teamwork skills",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Big Data', 'Hive', 'Data Engineering', 'Data Pipeline', 'PySpark', 'Hadoop', 'Kafka', 'HDFS', 'Spark', 'Python']",2025-06-14 05:17:26
S&C Global Network - AI - Life Sciences -Data Science Sr. Manager,Accenture,11 - 15 years,Not Disclosed,['Bengaluru'],"JR:\n\n\n\nR00229254\n\n\n\nExperience:\n\n\n\n11-15 Years\n\n\n\n\nEducational Qualification:\n\n\n\nBachelors or Masters degree in Statistics, Data Science, Applied Mathematics, Business Analytics, Computer Science, Information Systems, or other Quantitative field.\n\n\n\n---------------------------------------------------------------------\n\n\n\nJob Title -\n\n\n\nS&C Global Network - AI - Healthcare Analytics - Senior Manager\n\n\n\nManagement Level:\n\n\n\n6-Senior Manager\n\n\n\nLocation:\n\n\n\nBangalore/Gurgaon\n\n\n\nMust-have skills:R,Phython,SQL,Spark,Tableau ,Power BI\n\n\n\n\nGood to have skills:Ability to leverage design thinking, business process optimization, and stakeholder management skills.\n\n\n\nJob\n\n\nSummary:\n\nThis role involves driving strategic initiatives, managing business transformations, and leveraging industry expertise to create value-driven solutions.\n\n\n\n\nRoles & Responsibilities:\n\nProvide strategic advisory services, conduct market research, and develop data-driven recommendations to enhance business performance.\n\nAs part of our Data & AI practice, you will join a worldwide network of smart and driven colleagues experienced in leading AI/ML/Statistical tools, methods and applications. From data to analytics and insights to actions, our forward-thinking consultants provide analytically-informed, issue-based insights at scale to help our clients improve outcomes and achieve high performance.\n\n\n\nWHATS IN IT FOR YOU\nAn opportunity to work on high-visibility projects with top Pharma clients around the globe.\nPotential to Co-create with leaders in strategy, industry experts, enterprise function practitioners, and business intelligence professionals to shape and recommend innovative solutions that leverage emerging technologies.\nAbility to embed responsible business into everythingfrom how you service your clients to how you operate as a responsible professional.\nPersonalized training modules to develop your strategy & consulting acumen to grow your skills, industry knowledge, and capabilities.\nOpportunity to thrive in a culture that is committed to accelerating equality for all. Engage in boundaryless collaboration across the entire organization.\n\n\n\n\nWhat you would do in this role\nLead proposals, and business development efforts and coordinate with other colleagues to cross-sell/ up-sell Life Sciences offerings to existing as well as potential clients.\nLead client discussions, developing new industry Point of View (PoV), re-usable assets (tools)\nCollaborate closely with cross-functional teams including Data engineering, technology, and business stakeholders to identify opportunities for leveraging data to drive business solutions.\nLead and manage teams to deliver transformative and innovative client projects.\nGuide teams on analytical and AI methods and approaches\nManage client relationships to foster trust, deliver value, and build the Accenture brand\nDrive consulting practice innovation and thought leadership in your area of specialization\nSupport strategies and operating models focused on some business units and assess likely competitive responses. Also, assess implementation readiness and points of greatest impact.\nExecute a transformational change plan aligned with the clients business strategy and context for change. Engage stakeholders in the change journey and build commitment to change.\n\n\n\n\n\nProfessional & Technical\n\n\n\n\nSkills:\n\n\n- Relevant experience in the required domain.\n\n- Strong analytical, problem-solving, and communication skills.\n\n- Ability to work in a fast-paced, dynamic environment.\nProven experience in cross-sell/ up-sell\nLeverage ones hands-on experience of working across one or more of these areas such as real-world evidence data, R&D clinical data, and digital marketing data.\nExperience with handling Datasets like Komodo, RAVE, IQVIA, Truven, Optum, SHS, Specialty Pharmacy, PSP, etc.\nExperience in building and deployment of Statistical Models/Machine Learning including Segmentation & predictive modeling, hypothesis testing, multivariate statistical analysis, time series techniques, and optimization.\nExcellent analytical and problem-solving skills, with a data-driven mindset.\nAbility to solve complex business problems and deliver client delight.\nStrong writing skills to build points of view on current industry trends.\nGood Client handling skills; able to demonstrate thought leadership & problem-solving skills.\n\n\n\n\n\nAdditional Information:\n\n- Opportunity to work on innovative projects.\n\n- Career growth and leadership exposure.\n\n\n\n\n\nAbout Our Company | AccentureQualification\n\n\n\nExperience:\n\n\n\n11-15 Years\n\n\n\n\nEducational Qualification:\n\n\n\nBachelors or Masters degree in Statistics, Data Science, Applied Mathematics, Business Analytics, Computer Science, Information Systems, or other Quantitative field.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'power bi', 'sql', 'tableau', 'r', 'hypothesis testing', 'time series', 'business analytics', 'machine learning', 'data engineering', 'business intelligence', 'artificial intelligence', 'data science', 'computer science', 'spark', 'predictive modeling', 'segmentation', 'statistics', 'ml']",2025-06-14 05:17:29
Electrical Design Engineer - Data Center,Deerns,8 - 15 years,Not Disclosed,['Thane'],"Requirements:\nQualification: BE/Masters in Electrical Engineering.\nExperience: 8-15 years in electrical system design for facilities.\nExpertise:\nFull electrical system design, including HT LT calculations.\nExperience in data centers, telecom, mission-critical sectors, RD labs, hospitals, and industrial facilities.\nEnd-to-end project involvement from concept to commissioning.\nTechnical lead experience with power systems in these industries.\nFamiliarity with RIBA Plan of Work and Indian engineering regulations.\nStrong knowledge of IS, ISO, and local/international electrical codes.\nAbility to manage schedules, budgets, and team priorities.\nProblem-solving mindset and mentoring capability.\nProactive communicator with a willingness to support team growth.",Industry Type: Engineering & Construction,Department: Construction & Site Engineering,"Employment Type: Full Time, Permanent","['Electrical engineering', 'Telecom', 'Electrical design', 'ISO', 'System design', 'Technical Lead', 'Management', 'Electricals']",2025-06-14 05:17:31
Electrical Design Engineer - Data Center,Deerns,8 - 15 years,Not Disclosed,"['Mumbai', 'Nagpur', 'Thane', 'Nashik', 'Pune', 'Aurangabad']","Location: Mumbai, India. Working from Thane office\nJob Status: Permanent\nSector: Data Centres, Heatlhcare, Electronics part of Life Sciences & High-Tech Unit\nSalary: Competitive and negotiable\nJob Title: Senior Electrical Engineer\n\nDeerns Spectrum Private Limited is the joint venture between Deerns Groep B.V. and Spectrum PharmaTECH Consultants Private Limited. As a fully integrated business of the globally operating Deerns Group, we specialise in high-end engineering services and are committed to delivering world-class engineering services across India, South Asia, and East Africa. Together, we offer comprehensive advisory, design, and engineering solutions for mission-critical facilities requiring specialized conditions, such as data centers, hospitals, laboratories, and clean rooms. With a unique combination of deep technical expertise and extensive local market knowledge, Deerns and Spectrum are dedicated to serving clients across key sectors, including data centers, healthcare, life sciences, and microelectronics.\n\nDeerns is a company that was founded 96+ years ago. We specialize in Mechanical, Electrical and Piping (MEP) advisory, design and engineering particularly for mission critical and high-tech building systems and industrial facilities. We have 600+ employees across 17 offices in 10 countries.\n\nWe re experiencing an exciting period of growth, and we have opportunities available for a talented and innovative individual to join our fast-growing office at a senior level and looking to take their career on to the next step.\n\nRoles & Responsibilities:\nAttend and/or lead client, design team, contractor, and site meetings.\nDevelop concept and detailed designs for electrical systems, including power distribution, backup systems, lighting, and controls, ensuring compliance with local and international standards (e.g., IS, IEC, ISO).\nPrepare schematics, specifications, calculations, equipment schedules, and reports to meet industry and legal requirements.\nCoordinate electrical designs with other disciplines, ensuring seamless integration.\nMaintain commercial awareness of projects, tracking hours against agreed fees and addressing design variations before allocating additional time.\nOversee and review the work of electrical and graduate electrical engineers.\nValidate contractor designs, specifications, and installations to ensure they meet client requirements and regulatory standards.\nConduct QA/QC inspections on-site and produce associated reports.\nLead client meetings, design reviews, and site inspections to ensure project objectives are met.\nProvide technical leadership and expert solutions, acting as the primary point of contact for clients, contractors, and internal teams.\nProficient in Microsoft Office, AutoCAD, Dialux, Amtech, with Revit, SKM, or Easypower skills as an advantage.\nParticipate in witness testing activities on-site and prepare related reports.\nPrimarily office-based, with occasional site visits (domestic and international).\nCapable of leading projects independently with minimal input from the management team.\nJob requirements\nRequirements:\nQualification: BE/Masters in Electrical Engineering.\nExperience: 8-15 years in electrical system design for facilities.\nExpertise:\nFull electrical system design, including HT & LT calculations.\nExperience in data centers, telecom, mission-critical sectors, R&D labs, hospitals, and industrial facilities.\nEnd-to-end project involvement from concept to commissioning.\nTechnical lead experience with power systems in these industries.\nFamiliarity with RIBA Plan of Work and Indian engineering regulations.\nStrong knowledge of IS, ISO, and local/international electrical codes.\nAbility to manage schedules, budgets, and team priorities.\nProblem-solving mindset and mentoring capability.\nProactive communicator with a willingness to support team growth.",Industry Type: Engineering & Construction,Department: Construction & Site Engineering,"Employment Type: Full Time, Permanent","['Engineering services', 'Telecom', 'Electrical design', 'ISO', 'AutoCAD', 'Power distribution', 'MEP', 'System design', 'Healthcare', 'REVIT']",2025-06-14 05:17:33
AWS Data Engineer_ Capgemini_ Pan India Location,Capgemini,5 - 10 years,Not Disclosed,"['Chennai', 'Bengaluru', 'Mumbai (All Areas)']","Role & responsibilities\nAWS S3, Glue, API Gateway, Crawler, Athena, , Lambda, Dynamic DB, Redshift is an advantage\nExperience/knowledge with streaming technologies is must preferably Kafka\nShould have knowledge/experience with SQL\nGood analytical skills\nFamiliar working on Linux platforms\nHave good understanding on pros and cons and the cost impact of the AWS services being leveraged\nGood Communication skills.\n\nPrimary Skills\nAWS S3, Glue, Athena, Python or Pyspark\nShould have knowledge/experience with SQL\n\nSecondary Skills\nExcellent verbal and written communication and interpersonal skills\nAbility to work independently and within a team environment",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Lamda', 'AWS', 'Athena', 'SQL', 'Python', 'S', 'Amazon Redshift', 'Aws Glue']",2025-06-14 05:17:35
Data Bricks,PwC India,7 - 12 years,Not Disclosed,['Bengaluru'],"Job Summary:\n\nWe are seeking a talented Data Engineer with strong expertise in Databricks, specifically in Unity Catalog, PySpark, and SQL, to join our data team. Youll play a key role in building secure, scalable data pipelines and implementing robust data governance strategies using Unity Catalog.\n\nKey Responsibilities:",,,,"['DataBricks', 'Data Bricks', 'Pyspark', 'Delta Lake', 'Databricks Engineer', 'Unity Catalog', 'SQL']",2025-06-14 05:17:37
Senior Analyst-Data Analytics,AMERICAN EXPRESS,9 - 11 years,Not Disclosed,['Gurugram'],"Here, your voice and ideas matter, your work makes an impact, and together, you will help us define the future of American Express.\nHow will you make an impact in this role?\nYou will be responsible for delivery of highly impactful analytics to understand and optimize our commercial acquisition site experience and increase digital conversion.\nDeliver strategic analytics focused on digital acquisition and membership experiences.\nDefine and build key KPIs to monitor the channel/product/ platform health and success",,,,"['Mining', 'Computer science', 'Career development', 'Finance', 'Analytical', 'Data processing', 'Wellness', 'Analytics', 'SQL', 'Python']",2025-06-14 05:17:40
Data Scientist-Artificial Intelligence,IBM,5 - 7 years,Not Disclosed,['Bengaluru'],"Work with broader team to build, analyze and improve the AI solutions.\nYou will also work with our software developers in consuming different enterprise applications\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\n\n\nPreferred technical and professional experience",,,,"['algorithms', 'python', 'data analytics', 'tableau', 'ml', 'hive', 'data analysis', 'natural language processing', 'pyspark', 'data warehousing', 'machine learning', 'artificial intelligence', 'sql', 'pandas', 'deep learning', 'java', 'data science', 'spark', 'kafka', 'hadoop', 'big data', 'aws', 'etl']",2025-06-14 05:17:42
Senior Analyst - Data Governance & Management,AMERICAN EXPRESS,3 - 7 years,Not Disclosed,['Gurugram'],"Here, your voice and ideas matter, your work makes an impact, and together, you will help us define the future of American Express.\nAt American Express, you ll be recognized for your contributions, leadership, and impact every colleague has the opportunity to share in the company s success. Together, we ll win as a team, striving to uphold our company values and powerful backing promise to provide the world s best customer experience every day. And we ll do it with the utmost integrity, and in an environment where everyone is seen, heard and feels like they belong.\nJoin Team Amex and lets lead the way together.",,,,"['Career development', 'metadata', 'Manager Quality Assurance', 'Data management', 'Finance', 'Shell scripting', 'Wellness', 'Data quality', 'Risk management', 'SQL']",2025-06-14 05:17:44
S&C GN - Tech Strategy & Advisory - Data On Cloud - Consultant,Accenture,6 - 10 years,Not Disclosed,['Bengaluru'],"Practice Overview:\n\nSkill/Operating Group\nTechnology Consulting\n\nLevel\nConsultant\n\nLocation\nGurgaon / Mumbai / Bangalore / Kolkata / Pune\n\nTravel Percentage\nExpected Travel could be anywhere between 0-100%\n\nPrincipal Duties\n\nAnd Responsibilities:\nWorking closely with our clients, Consulting professionals design, build and implement strategies that can help enhance business performance. They develop specialized expertisestrategic, industry, functional, technicalin a diverse project environment that offers multiple opportunities for career growth.\n\nThe opportunities to make a difference within exciting client initiatives are limitless in this ever-changing business landscape.\n\nHere are just a few of your day-to-day responsibilities.\nArchitect large scale data lake, DW, and Delta Lake on cloud solutions using AWS, Azure, GCP, Ali Cloud, Snowflake, Hadoop, or Cloudera\nDesign Data Mesh Strategy and Architecture\nBuild strategy and roadmap for data migration to cloud\nEstablish Data Governance Strategy & Operating Model\nImplementing programs/interventions that prepare the organization for implementation of new business processes\nDeep understanding of Data and Analytics platforms, data integration w/ cloud\nProvide thought leadership to the downstream teams for developing offerings and assets\nIdentifying, assessing, and solvingcomplex business problemsfor area of responsibility, where analysis of situations or data requires an in-depth evaluation of variable factors\nOverseeing the production and implementation of solutions covering multiple cloud technologies, associated Infrastructure / application architecture, development, and operating models\nCalled upon to apply your solid understanding of Data, Data on cloud and disruptive technologies.\nDriving enterprise business, application, and integration architecture\nHelping solve key business problems and challenges by enabling a cloud-based architecture transformation, painting a picture of, and charting a journey from the current state to a to-be enterprise environment\nAssisting our clients to build the required capabilities for growth and innovation to sustain high performance\nManaging multi-disciplinary teams to shape, sell, communicate, and implement programs\nExperience in participating in client presentations & orals for proposal defense etc.\nExperience in effectively communicating the target state, architecture & topology on cloud to clients\n\nQualifications:\nBachelors degree\nMBA Degree from Tier-1 College (Preferable)\n6-10 years of large-scale consulting experience and/or working with hi tech companies in data architecture, data governance, data mesh, data security and management.\nCertified on DAMA (Data Management) or Azure Data Architecture or Google Cloud Data Analytics or AWS Data Analytics\n\nExperience:\nWe are looking for experienced professionals with Data strategy, data architecture, data on cloud, data modernization, data operating model and data security experience across all stages of the innovation spectrum, with a remit to build the future in real-time.\nThe candidate should have practical industry expertise in one of these areas - Financial Services, Retail, consumer goods, Telecommunications, Life Sciences, Transportation, Hospitality, Automotive/Industrial, Mining and Resources.\n\nKey Competencies and\n\nSkills:\nThe right candidate should have competency and skills aligned to one or more of these archetypes -\n\nData SME - Experience in deal shaping & strong presentation skills, leading proposal experience, customer orals; technical understanding of data platforms, data on cloud strategy, data strategy, data operating model, change management of data transformation programs, data modeling skills.\n\nData on Cloud Architect - Technical understanding of data platform strategy for data on cloud migrations, big data technologies, experience in architecting large scale data lake and DW on cloud solutions. Experience one or more technologies in this space:AWS, Azure, GCP, AliCloud, Snowflake, Hadoop, Cloudera\n\nData Strategy - Data Capability Maturity Assessment, Data & Analytics / AI Strategy, Data Operating Model & Governance, Data Hub Enablement, Data on Cloud Strategy, Data Architecture Strategy\n\nData Transformation Lead - Understanding of data supply chain and data platforms on cloud, experience in conducting alignment workshops, building value realization framework for data transformations, program management experience\n\nExceptional interpersonal and presentation skills - ability to convey technology and business value propositions to senior stakeholders\nCapacity to develop high impact thought leadership that articulates a forward-thinking view of the market\nOther desired skills -\nStrong desire to work in technology-driven business transformation\nStrong knowledge of technology trends across IT and digital and how they can be applied to companies to address real world problems and opportunities.\nComfort conveying both high-level and detailed information, adjusting the way ideas are presented to better address varying social styles and audiences.\nLeading proof of concept and/or pilot implementations and defining the plan to scale implementations across multiple technology domains\nFlexibility to accommodate client travel requirements\nPublished Thought leadership Whitepapers, POVs",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['presentation skills', 'microsoft azure', 'gcp', 'aws', 'data integration', 'cloudera', 'snowflake', 'data analytics', 'data security', 'data architecture', 'data migration', 'financial services', 'data modeling', 'enterprise business', 'data governance', 'life sciences', 'hadoop', 'data on cloud']",2025-06-14 05:17:46
Data Techology Senior Associate,MSCI Services,4 - 8 years,Not Disclosed,['Pune'],"As data engineers, we build scalable systems to process data in various formats and volumes, ranging from megabytes to terabytes. Our systems perform quality checks, match data across various sources, and release it in multiple formats. We leverage the latest technologies, sources, and tools to process the data. Some of the exciting technologies we work with include Snowflake, Databricks, and Apache Spark.\nYour skills and experience that will help you excel\nCore Java, Spring Boot, Apache Spark, Spring Batch, Python. Exposure to sql databases like Oracle, Mysql, Microsoft Sql is a must. Any experience / knowledge / certification on Cloud technology preferrably Microsoft Azure or Google cloud platform is good to have. Exposures to non sql databases like Neo4j or Document database is again good to have.\n  What we offer you\nTransparent compensation schemes and comprehensive employee benefits, tailored to your location, ensuring your financial security, health, and overall we'llbeing.\nFlexible working arrangements, advanced technology, and collaborative workspaces.\nA culture of high performance and innovation where we experiment with new ideas and take responsibility for achieving results.\nA global network of talented colleagues, who inspire, support, and share their expertise to innovate and deliver for our clients.\nGlobal Orientation program to kickstart your journey, followe'd by access to our Learning@MSCI platform, LinkedIn Learning Pro and tailored learning opportunities for ongoing skills development.\nMulti-directional career paths that offer professional growth and development through new challenges, internal mobility and expanded roles.\nWe actively nurture an environment that builds a sense of inclusion belonging and connection, including eight Employee Resource Groups. All Abilities, Asian Support Network, Black Leadership Network, Climate Action Network, Hola! MSCI, Pride & Allies, Women in Tech, and Women s Leadership Forum.",Industry Type: Financial Services,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['CVS', 'Core Java', 'Bloomberg', 'spring batch', 'MySQL', 'Oracle', 'Analytics', 'Downstream', 'Python', 'Recruitment']",2025-06-14 05:17:49
Senior Lead Data Architect,JPMorgan Chase Bank,11 - 19 years,Not Disclosed,['Hyderabad'],"You strive to be an essential member of a diverse team of visionaries dedicated to making a lasting impact. Don t pass up this opportunity to collaborate with some of the brightest minds in the field and deliver best-in-class solutions to the industry.\n\n\nAs a Senior Lead Data Architect at JPMorgan Chase within the Consumer and Community Banking Data Technology, you are an integral part of a team that works to develop high-quality data architecture solutions for various software applications, platform and data products. Drive significant business impact and help shape the global target state architecture through your capabilities in multiple data architecture domains.\n\nJob responsibilities\n\n\nRepresents the data architecture team at technical governance bodies and provides feedback regarding proposed improvements regarding data architecture governance practices\n\nEvaluates new and current technologies using existing data architecture standards and frameworks\n\nRegularly provides technical guidance and direction to support the business and its technical teams, contractors, and vendors\n\nDesign secure, high-quality, scalable solutions and reviews architecture solutions designed by others\n\nDrives data architecture decisions that impact data product platform design, application functionality, and technical operations and processes\n\nServes as a function-wide subject matter expert in one or more areas of focus\n\nActively contributes to the data engineering community as an advocate of firmwide data frameworks, tools, and practices in the Software Development Life Cycle\n\nInfluences peers and project decision-makers to consider the use and application of leading-edge technologies\n\nAdvises junior architects and technologists\n\n\nRequired qualifications, capabilities, and skills\n\n\n7+ years of hands-on practical experience delivering data architecture and system designs, data engineer, testing, and operational stability\n\nAdvanced knowledge of architecture, applications, and technical processes with considerable in-depth knowledge in data architecture discipline and solutions (e. g. , data modeling, native cloud data services, business intelligence, artificial intelligence, machine learning, data domain driven design, etc. )\n\nPractical cloud based data architecture and deployment experience, preferably AWS\n\nPractical SQL development experiences in cloud native relational databases, e. g. Snowflake, Athena, Postgres\n\nAbility to deliver various types of data models with multiple deployment targets, e. g. conceptual, logical and physical data models deployed as an operational vs. analytical data stores\n\nAdvanced in one or more data engineering disciplines, e. g. streaming, ELT, event processing\n\nAbility to tackle design and functionality problems independently with little to no oversight\n\nAbility to evaluate current and emerging technologies to select or recommend the best solutions for the future state data architecture\n\n\nPreferred qualifications, capabilities, and skills\n\n\nFinancial services experience, card and banking a big plus\n\nPractical experience in modern data processing technologies, e. g. , Kafka streaming, DBT, Spark, Airflow, etc.\n\nPractical experience in data mesh and/or data lake\n\nPractical experience in machine learning/AI with Python development a big plus\n\nPractical experience in graph and semantic technologies, e. g. RDF, LPG, Neo4j, Gremlin\n\nKnowledge of architecture assessments frameworks, e. g. Architecture Trade off Analysis",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data modeling', 'Analytical', 'Artificial Intelligence', 'Banking', 'Machine learning', 'Data processing', 'Business intelligence', 'Financial services', 'Python', 'Data architecture']",2025-06-14 05:17:51
S&C GN - Data&AI - CMT Eng - Consultant,Accenture,2 - 7 years,Not Disclosed,['Bengaluru'],"Job Title - S&C Global Network - AI - CMT DE- Consultant\n\n\n\nManagement Level:9- Consultant\n\n\n\nLocation:Open\n\n\n\nMust-have skills:Data Engineering\n\n\n\n\nGood to have skills:Ability to leverage design thinking, business process optimization, and stakeholder management skills.\n\n\n\nJob\n\n\nSummary:\n\nWe are looking for a passionate and results-driven\n\n\n\nData Engineerto join our growing data team. You will be responsible for designing, building, and maintaining scalable data pipelines and infrastructure that support data-driven decision-making across the organization.\n\n\n\n\nRoles & Responsibilities:\n\nDesign, build, and maintain robust, scalable, and efficient data pipelines (ETL/ELT).\nWork with structured and unstructured data across a wide variety of data sources.\nCollaborate with data analysts, data scientists, and business stakeholders to understand data requirements.\nOptimize data systems and architecture for performance, scalability, and reliability.\nMonitor data quality and support initiatives to ensure clean, accurate, and consistent data.\nDevelop and maintain data models and metadata.\nImplement and maintain best practices in data governance, security, and compliance.\n\n\n\n\nProfessional & Technical\n\n\n\n\nSkills:\n\n2+ years in data engineering or related fields\nProficiency in SQL and experience with relational databases (e.g., PostgreSQL, MySQL).\nStrong programming skills in Python, Scala, or Java.\nExperience with big data technologies such as Spark, Hadoop, or Hive.\nFamiliarity with cloud platforms like AWS, Azure, or GCP, especially services like S3, Redshift, BigQuery, or Azure Data Lake.\nExperience with orchestration tools like Airflow, Luigi, or similar.\nSolid understanding of data warehousing concepts and data modeling techniques.\nGood problem-solving skills and attention to detail.\nExperience with modern data stack tools like dbt, Snowflake, or Databricks.\nKnowledge of CI/CD pipelines and version control (e.g., Git).\nExposure to containerization (Docker, Kubernetes) and infrastructure as code (Terraform, CloudFormation).\n\n\n\n\nAdditional Information: - The ideal candidate will possess a strong educational background in quantitative discipline and experience in working with Hi-Tech clients\n\n- This position is based at our Bengaluru (preferred) and other AI Accenture locations.\n\n\n\n\n\nAbout Our Company | Accenture\n\nQualification\n\n\n\nExperience:4+ years\n\n\n\n\nEducational Qualification:Btech/ BE",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'scala', 'data engineering', 'sql', 'java', 'hive', 'continuous integration', 'kubernetes', 'snowflake', 'amazon redshift', 'airflow', 'microsoft azure', 'ci/cd', 'aws cloudformation', 'docker', 'data bricks', 'data modeling', 'spark', 'gcp', 'data warehousing concepts', 'terraform', 'hadoop', 'aws']",2025-06-14 05:17:53
Senior/Lead Data Scientist,Tiger Analytics,6 - 11 years,Not Disclosed,"['Hyderabad', 'Chennai', 'Bengaluru']","Role & responsibilities\n\nCurious about the role? What your typical day would look like?As a Senior Data Scientist, your work is a combination of hands-on contribution to Loreum Ipsum, Loreum Ipsum, etc. More specifically, this will involve:\nLead and contribute to developing sophisticated machine learning models, predictive analytics, and statistical analyses to solve complex business problems.",,,,"['Data Science', 'Time Series Analysis', 'Machine Learning', 'Python', 'Time Series Forecasting', 'Regression', 'Clustering', 'neural nets', 'Optimization', 'SQL']",2025-06-14 05:17:56
S&C GN - Tech Strategy & Advisory - Data On Cloud - Consultant,Accenture,6 - 10 years,Not Disclosed,"['Mumbai', 'Pune', 'Bengaluru']","Location\n\nGurgaon/Mumbai/Bangalore/Kolkata/Pune\n\nTravel Percentage\n\nExpected Travel could be anywhere between 0-100%\n\nPrincipal Duties\n\nAnd Responsibilities:\n\nWorking closely with our clients, Consulting professionals design, build and implement strategies that can help enhance business performance. They develop specialized expertisestrategic, industry, functional, technicalin a diverse project environment that offers multiple opportunities for career growth.\n\nThe opportunities to make a difference within exciting client initiatives are limitless in this ever-changing business landscape.\n\nHere are just a few of your day-to-day responsibilities.\nArchitect large scale data lake, DW, and Delta Lake on cloud solutions using AWS, Azure, GCP, Ali Cloud, Snowflake, Hadoop, or Cloudera\nDesign Data Mesh Strategy and Architecture\nBuild strategy and roadmap for data migration to cloud\nEstablish Data Governance Strategy & Operating Model\nImplementing programs/interventions that prepare the organization for implementation of new business processes\nDeep understanding of Data and Analytics platforms, data integration w/ cloud\nProvide thought leadership to the downstream teams for developing offerings and assets\nIdentifying, assessing, and solvingcomplex business problemsfor area of responsibility, where analysis of situations or data requires an in-depth evaluation of variable factors\nOverseeing the production and implementation of solutions covering multiple cloud technologies, associated Infrastructure / application architecture, development, and operating models\nCalled upon to apply your solid understanding of Data, Data on cloud and disruptive technologies.\nDriving enterprise business, application, and integration architecture\nHelping solve key business problems and challenges by enabling a cloud-based architecture transformation, painting a picture of, and charting a journey from the current state to a to-be enterprise environment\nAssisting our clients to build the required capabilities for growth and innovation to sustain high performance\nManaging multi-disciplinary teams to shape, sell, communicate, and implement programs\nExperience in participating in client presentations & orals for proposal defense etc.\nExperience in effectively communicating the target state, architecture & topology on cloud to clients\nQualification\n\n\nQualifications:\nBachelors degree\nMBA Degree from Tier-1 College (Preferable)\n6-10 years of large-scale consulting experience and/or working with hi tech companies in data architecture, data governance, data mesh, data security and management.\nCertified on DAMA (Data Management) or Azure Data Architecture or Google Cloud Data Analytics or AWS Data Analytics\nExperience:\nWe are looking for experienced professionals with Data strategy, data architecture, data on cloud, data modernization, data operating model and data security experience across all stages of the innovation spectrum, with a remit to build the future in real-time.\nThe candidate should have practical industry expertise in one of these areas - Financial Services, Retail, consumer goods, Telecommunications, Life Sciences, Transportation, Hospitality, Automotive/Industrial, Mining and Resources.\nKey Competencies and\n\nSkills:\nThe right candidate should have competency and skills aligned to one or more of these archetypes -\nData SME - Experience in deal shaping & strong presentation skills, leading proposal experience, customer orals; technical understanding of data platforms, data on cloud strategy, data strategy, data operating model, change management of data transformation programs, data modeling skills.\nData on Cloud Architect - Technical understanding of data platform strategy for data on cloud migrations, big data technologies, experience in architecting large scale data lake and DW on cloud solutions. Experience one or more technologies in this space:AWS, Azure, GCP, AliCloud, Snowflake, Hadoop, Cloudera\nData Strategy - Data Capability Maturity Assessment, Data & Analytics / AI Strategy, Data Operating Model & Governance, Data Hub Enablement, Data on Cloud Strategy, Data Architecture Strategy\nData Transformation Lead - Understanding of data supply chain and data platforms on cloud, experience in conducting alignment workshops, building value realization framework for data transformations, program management experience\nExceptional interpersonal and presentation skills - ability to convey technology and business value propositions to senior stakeholders\nCapacity to develop high impact thought leadership that articulates a forward-thinking view of the market\nOther desired skills -\nStrong desire to work in technology-driven business transformation\nStrong knowledge of technology trends across IT and digital and how they can be applied to companies to address real world problems and opportunities.\nComfort conveying both high-level and detailed information, adjusting the way ideas are presented to better address varying social styles and audiences.\nLeading proof of concept and/or pilot implementations and defining the plan to scale implementations across multiple technology domains\nFlexibility to accommodate client travel requirements\nPublished Thought leadership Whitepapers, POVs",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['Data On Cloud', 'Data Management', 'AWS Data Analytics', 'Azure Data Architecture', 'Data Governance Strategy', 'Data Analytics', 'data integration']",2025-06-14 05:17:58
"Senior Python Developer (Machine Learning,Data Analysis,Visualization)",Synechron,3 - 5 years,Not Disclosed,"['Pune', 'Hinjewadi']","Software Requirements\nRequired Skills:\nProficiency in Python (version 3.6+) with experience in data analysis, manipulation, and scripting\nKnowledge of SQL for data extraction, transformation, and database querying\nExperience with data visualization tools such as PowerBI, Tableau, or QlikView\nFamiliarity with AI and Machine Learning frameworks such as TensorFlow, Keras, PyTorch, or equivalent",,,,"['Python', 'PostgreSQL', 'MySQL', 'Data Analysis', 'Data Visualization', 'Oracle', 'ETL', 'Machine Learning']",2025-06-14 05:18:01
IN_Manager_Azure Data Engineer_Data & Analytics_Advisory_Bangalore,PwC Service Delivery Center,4 - 8 years,Not Disclosed,['Bengaluru'],"Not Applicable\nSpecialism\nSAP\n& Summary\nAt PwC, our people in data and analytics engineering focus on leveraging advanced technologies and techniques to design and develop robust data solutions for clients. They play a crucial role in transforming raw data into actionable insights, enabling informed decisionmaking and driving business growth.\n\nIn data engineering at PwC, you will focus on designing and building data infrastructure and systems to enable efficient data processing and analysis. You will be responsible for developing and implementing data pipelines, data integration, and data transformation solutions.\nAt PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purposeled and valuesdriven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us .\n& Summary A career within Data and Analytics services will provide you with the opportunity to help organisations uncover enterprise insights and drive business results using smarter data analytics. We focus on a collection of organisational technology capabilities, including business intelligence, data management, and data assurance that help our clients drive innovation, growth, and change within their organisations in order to keep up with the changing nature of customers and technology. We make impactful decisions by mixing mind and machine to leverage data, understand and navigate risk, and help our clients gain a competitive edge.\nResponsibilities\nDesign, develop, and optimize data pipelines and ETL processes using PySpark or Scala to extract, transform, and load large volumes of structured and unstructured data from diverse sources. Implement data ingestion, processing, and storage solutions on Azure cloud platform, leveraging services such as Azure Databricks, Azure Data Lake Storage, and Azure Synapse Analytics. Develop and maintain data models, schemas, and metadata to support efficient data access, query performance, and analytics requirements. Monitor pipeline performance, troubleshoot issues, and optimize data processing workflows for scalability, reliability, and costeffectiveness. Implement data security and compliance measures to protect sensitive information and ensure regulatory compliance. Requirement Proven experience as a Data Engineer, with expertise in building and optimizing data pipelines using PySpark, Scala, and Apache Spark. Handson experience with cloud platforms, particularly Azure, and proficiency in Azure services such as Azure Databricks, Azure Data Lake Storage, Azure Synapse Analytics, and Azure SQL Database. Strong programming skills in Python and Scala, with experience in software development, version control, and CI/CD practices. Familiarity with data warehousing concepts, dimensional modeling, and relational databases (e.g., SQL Server, PostgreSQL, MySQL).\nExperience with big data technologies and frameworks (e.g., Hadoop, Hive, HBase) is a plus.\nMandatory skill sets\nSpark, Pyspark, Azure\nPreferred skill sets\nSpark, Pyspark, Azure\nYears of experience required\n4 8\nEducation qualification\nB.Tech / M.Tech / MBA / MCA\nEducation\nDegrees/Field of Study required Bachelor of Engineering, Master of Business Administration, Master of Engineering\nDegrees/Field of Study preferred\nRequired Skills\nPython (Programming Language)\nAccepting Feedback, Accepting Feedback, Active Listening, Agile Scalability, Amazon Web Services (AWS), Analytical Thinking, Apache Airflow, Apache Hadoop, Azure Data Factory, Coaching and Feedback, Communication, Creativity, Data Anonymization, Data Architecture, Database Administration, Database Management System (DBMS), Database Optimization, Database Security Best Practices, Databricks Unified Data Analytics Platform, Data Engineering, Data Engineering Platforms, Data Infrastructure, Data Integration, Data Lake, Data Modeling {+ 32 more}\nNo",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['SAP', 'Data management', 'Data modeling', 'Postgresql', 'MySQL', 'Database administration', 'Agile', 'Apache', 'Business intelligence', 'Python']",2025-06-14 05:18:03
Senior Data Scientist,Capgemini,5 - 9 years,Not Disclosed,['Gurugram'],"At Capgemini Invent, we believe difference drives change. As inventive transformation consultants, we blend our strategic, creative and scientific capabilities,collaborating closely with clients to deliver cutting-edge solutions. Join us to drive transformation tailored to our client's challenges of today and tomorrow.Informed and validated by science and data. Superpowered by creativity and design. All underpinned by technology created with purpose.\n\n \n\nYour role \n\nAs a Senior Data Scientist, you are expected to develop and implement Artificial Intelligence based solutions across various disciplines for the Intelligent Industry vertical of Capgemini Invent. You are expected to work as an individual contributor or along with a team to help design and develop ML/NLP models as per the requirement. You will work closely with the Product Owner, Systems Architect and other key stakeholders right from conceptualization till the implementation of the project. You should take ownership while understanding the client requirement, the data to be used, security & privacy needs and the infrastructure to be used for the development and implementation.\n\nThe candidate will be responsible for executing data science projects independently to deliver business outcomes and is expected to demonstrate domain expertise, develop, and execute program plans and proactively solicit feedback from stakeholders to identify improvement actions. This role requires a strong technical background, excellent problem-solving skills, and the ability to work collaboratively with stakeholders from different functional and business teams.\nThe role also requires the candidate to collaborate on ML asset creation and eager to learn and impart trainings to fellow data science professionals. We expect thought leadership from the candidate, especially on proposing to build a ML/NLP asset based on expected industry requirements. Experience in building Industry specific (e.g. Manufacturing, R&D, Supply Chain, Life Sciences etc), production ready AI Models using microservices and web-services is a plus.\n\nProgramming Languages Python NumPy, SciPy, Pandas, MatPlotLib, Seaborne\nDatabases RDBMS (MySQL, Oracle etc.), NoSQL Stores (HBase, Cassandra etc.)\nML/DL Frameworks SciKitLearn, TensorFlow (Keras), PyTorch,\nBig data ML Frameworks - Spark (Spark-ML, Graph-X), H2O\nCloud Azure/AWS/GCP\n\n \n\nYour Profile \n\nPredictive and Prescriptive modelling using Statistical and Machine Learning algorithms including but not limited to Time Series, Regression, Trees, Ensembles, Neural-Nets (Deep & Shallow CNN, LSTM, Transformers etc.). Experience with open-source OCR engines like Tesseract, Speech recognition, Computer Vision, face recognition, emotion detection etc. is a plus.\nUnsupervised learning Market Basket Analysis, Collaborative Filtering, Dimensionality Reduction, good understanding of common matrix decomposition approaches like SVD. Various Clustering approaches Hierarchical, Centroid-based, Density-based, Distribution-based, Graph-based clustering like Spectral.\nNLP Information Extraction, Similarity Matching, Sentiment Analysis, Text Clustering, Semantic Analysis, Document Summarization, Context Mapping/Understanding, Intent Classification, Word Embeddings, Vector Space Models, experience with libraries like NLTK, Spacy, Stanford Core-NLP is a plus. Usage of Transformers for NLP and experience with LLMs like (ChatGPT, Llama) and usage of RAGs (vector stores like LangChain & LangGraps), building Agentic AI applications.\nModel Deployment ML pipeline formation, data security and scrutiny check and ML-Ops for productionizing a built model on-premises and on cloud.\n\nRequired Qualifications\nMasters degree in a quantitative field such as Mathematics, Statistics, Machine Learning, Computer Science or Engineering or a bachelors degree with relevant experience.\nGood experience in programming with languages such as Python/Java/Scala, SQL and experience with data visualization tools like Tableau or Power BI.\n\nPreferred Experience\nExperienced in Agile way of working, manage team effort and track through JIRA\nExperience in Proposal, RFP, RFQ and pitch creations and delivery to the big forum.\nExperience in POC, MVP, PoV and assets creations with innovative use cases\nExperience working in a consulting environment is highly desirable.\nPresupposition\n\nHigh Impact client communication\nThe job may also entail sitting as well as working at a computer for extended periods of time. Candidates should be able to effectively communicate by telephone, email, and face to face.\n\n \n\nWhat you will love about working here \nWe recognize the significance of flexible work arrangements to provide support. Be it remote work, or flexible work hours, you will get an environment to maintain healthy work life balance.\nAt the heart of our mission is your career growth. Our array of career growth programs and diverse professions are crafted to support you in exploring a world of opportunities.\nEquip yourself with valuable certifications in the latest technologies such as Generative AI.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['numpy', 'sql', 'java', 'python', 'pandas', 'scala', 'poc', 'nltk', 'dl', 'artificial intelligence', 'tensorflow', 'spacy', 'spark', 'gcp', 'pytorch', 'keras', 'mysql', 'hbase', 'ml', 'jira', 'scipy', 'rdbms', 'oracle', 'mvp', 'microsoft azure', 'power bi', 'nosql', 'tableau', 'cassandra', 'matplotlib', 'agile', 'aws']",2025-06-14 05:18:05
S&C GN - Tech Strategy & Advisory - Data On Cloud - Consultant,Accenture,6 - 10 years,Not Disclosed,"['Kolkata', 'Pune', 'Bengaluru']","Practice Overview:\n\nSkill/Operating Group\n\nTechnology Consulting\n\nLevel\n\nConsultant\n\nLocation\n\nGurgaon/Mumbai/Bangalore/Kolkata/Pune\n\nTravel Percentage\n\nExpected Travel could be anywhere between 0-100%\n\nPrincipal Duties\n\nAnd Responsibilities:\n\nWorking closely with our clients, Consulting professionals design, build and implement strategies that can help enhance business performance. They develop specialized expertisestrategic, industry, functional, technicalin a diverse project environment that offers multiple opportunities for career growth.\n\nThe opportunities to make a difference within exciting client initiatives are limitless in this ever-changing business landscape.\n\nHere are just a few of your day-to-day responsibilities.\nArchitect large scale data lake, DW, and Delta Lake on cloud solutions using AWS, Azure, GCP, Ali Cloud, Snowflake, Hadoop, or Cloudera\nDesign Data Mesh Strategy and Architecture\nBuild strategy and roadmap for data migration to cloud\nEstablish Data Governance Strategy & Operating Model\nImplementing programs/interventions that prepare the organization for implementation of new business processes\nDeep understanding of Data and Analytics platforms, data integration w/ cloud\nProvide thought leadership to the downstream teams for developing offerings and assets\nIdentifying, assessing, and solvingcomplex business problemsfor area of responsibility, where analysis of situations or data requires an in-depth evaluation of variable factors\nOverseeing the production and implementation of solutions covering multiple cloud technologies, associated Infrastructure / application architecture, development, and operating models\nCalled upon to apply your solid understanding of Data, Data on cloud and disruptive technologies.\nDriving enterprise business, application, and integration architecture\nHelping solve key business problems and challenges by enabling a cloud-based architecture transformation, painting a picture of, and charting a journey from the current state to a to-be enterprise environment\nAssisting our clients to build the required capabilities for growth and innovation to sustain high performance\nManaging multi-disciplinary teams to shape, sell, communicate, and implement programs\nExperience in participating in client presentations & orals for proposal defense etc.\nExperience in effectively communicating the target state, architecture & topology on cloud to clients\nQualification\n\n\nQualifications:\nBachelors degree\nMBA Degree from Tier-1 College (Preferable)\n6-10 years of large-scale consulting experience and/or working with hi tech companies in data architecture, data governance, data mesh, data security and management.\nCertified on DAMA (Data Management) or Azure Data Architecture or Google Cloud Data Analytics or AWS Data Analytics\nExperience:\nWe are looking for experienced professionals with Data strategy, data architecture, data on cloud, data modernization, data operating model and data security experience across all stages of the innovation spectrum, with a remit to build the future in real-time.\nThe candidate should have practical industry expertise in one of these areas - Financial Services, Retail, consumer goods, Telecommunications, Life Sciences, Transportation, Hospitality, Automotive/Industrial, Mining and Resources.\nKey Competencies and\n\nSkills:\nThe right candidate should have competency and skills aligned to one or more of these archetypes -\nData SME - Experience in deal shaping & strong presentation skills, leading proposal experience, customer orals; technical understanding of data platforms, data on cloud strategy, data strategy, data operating model, change management of data transformation programs, data modeling skills.\nData on Cloud Architect - Technical understanding of data platform strategy for data on cloud migrations, big data technologies, experience in architecting large scale data lake and DW on cloud solutions. Experience one or more technologies in this space:AWS, Azure, GCP, AliCloud, Snowflake, Hadoop, Cloudera\nData Strategy - Data Capability Maturity Assessment, Data & Analytics / AI Strategy, Data Operating Model & Governance, Data Hub Enablement, Data on Cloud Strategy, Data Architecture Strategy\nData Transformation Lead - Understanding of data supply chain and data platforms on cloud, experience in conducting alignment workshops, building value realization framework for data transformations, program management experience\nExceptional interpersonal and presentation skills - ability to convey technology and business value propositions to senior stakeholders\nCapacity to develop high impact thought leadership that articulates a forward-thinking view of the market\nOther desired skills -\nStrong desire to work in technology-driven business transformation\nStrong knowledge of technology trends across IT and digital and how they can be applied to companies to address real world problems and opportunities.\nComfort conveying both high-level and detailed information, adjusting the way ideas are presented to better address varying social styles and audiences.\nLeading proof of concept and/or pilot implementations and defining the plan to scale implementations across multiple technology domains\nFlexibility to accommodate client travel requirements\nPublished Thought leadership Whitepapers, POVs",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Cloud', 'Azure', 'GCP', 'Snowflake', 'Hadoop', 'Google Cloud Data Analytics', 'Ali Cloud', 'AWS Data Analytics', 'Azure Data Architecture', 'AWS']",2025-06-14 05:18:08
Data Scientist Sr. Analyst,Accenture,5 - 10 years,Not Disclosed,['Kochi'],"Job Title - + +\n\n\n\nManagement Level:\n\n\n\nLocation:Kochi, Coimbatore, Trivandrum\n\n\n\nMust have skills:Big Data, Python or R\n\n\n\n\nGood to have skills:Scala, SQL\n\n\n\nJob\n\n\nSummary\n\nA Data Scientist is expected to be hands-on to deliver end to end vis a vis projects undertaken in the Analytics space. They must have a proven ability to drive business results with their data-based insights. They must be comfortable working with a wide range of stakeholders and functional teams. The right candidate will have a passion for discovering solutions hidden in large data sets and working with stakeholders to improve business outcomes.\n\n\n\nRoles and Responsibilities\nIdentify valuable data sources and collection processes\nSupervise preprocessing of structured and unstructured data\nAnalyze large amounts of information to discover trends and patterns for insurance industry.\nBuild predictive models and machine-learning algorithms\nCombine models through ensemble modeling\nPresent information using data visualization techniques\nCollaborate with engineering and product development teams\nHands-on knowledge of implementing various AI algorithms and best-fit scenarios\nHas worked on Generative AI based implementations\n\n\n\nProfessional and Technical Skills\n3.5-5 years experience in Analytics systems/program delivery; at least 2 Big Data or Advanced Analytics project implementation experience\nExperience using statistical computer languages (R, Python, SQL, Pyspark, etc.) to manipulate data and draw insights from large data sets; familiarity with Scala, Java or C++\nKnowledge of a variety of machine learning techniques (clustering, decision tree learning, artificial neural networks, etc.) and their real-world advantages/drawbacks\nKnowledge of advanced statistical techniques and concepts (regression, properties of distributions, statistical tests and proper usage, etc.) and experience with applications\nHands on experience in Azure/AWS analytics platform (3+ years)\nExperience using variations of Databricks or similar analytical applications in AWS/Azure\nExperience using business intelligence tools (e.g. Tableau) and data frameworks (e.g. Hadoop)\nStrong mathematical skills (e.g. statistics, algebra)\nExcellent communication and presentation skills\nDeploying data pipelines in production based on Continuous Delivery practices.\n\n\n\n\nAdditional Information\nMulti Industry domain experience\nExpert in Python, Scala, SQL\nKnowledge of Tableau/Power BI or similar self-service visualization tools\nInterpersonal and Team skills should be top notch\nNice to have leadership experience in the past\nQualification\n\n\n\nExperience:3.5 -5 years of experience is required\n\n\n\n\nEducational Qualification:Graduation (Accurate educational details should capture)",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'scala', 'sql', 'r', 'big data', 'advanced analytics', 'mathematics', 'data manipulation', 'presentation skills', 'microsoft azure', 'pyspark', 'power bi', 'machine learning', 'javascript', 'aws kinesis', 'tableau', 'decision tree', 'java', 'hadoop', 'data visualization', 'aws', 'statistics']",2025-06-14 05:18:10
S&C GN - Data&AI - Life Sciences - Consultant,Accenture,4 - 9 years,Not Disclosed,['Bengaluru'],"Management Level:Ind&Func AI Decision Science Consultant\n\n\n\n\nJob Location:Bangalore / Gurgaon\n\n\n\nMust-have\n\n\n\n\nSkills:\nExcellent understanding of Pharma data sets commercial, clinical, Leverage ones hands on experience of working across one or more of these areas such as real-world evidence data, Statistical Models/Machine Learning including Segmentation & predictive modeling, hypothesis testing, multivariate statistical analysis, time series techniques, and optimization.\n\n\n\nGood-to-have\n\n\n\n\nSkills:\nProgramming languages such as R, Python, SQL, Spark, AWS, Azure, or Google Cloud for deploying and scaling language models, Data Visualization tools like Tableau, Power BI.\n\n\n\nExperience:Proven experience (4+ years) in working on Life Sciences/Pharma/Healthcare projects and delivering successful outcomes.\n\n\n\n\nEducational Qualification:Bachelors or Masters degree in Statistics, Data Science, Applied Mathematics, Business Analytics, Computer Science, Information Systems, or other Quantitative field.\n\n\n\nJob\n\n\nSummary\n\nThis role involves driving strategic initiatives, managing business transformations, and leveraging industry expertise to create value-driven solutions. Provide strategic advisory services, conduct market research, and develop data-driven recommendations to enhance business performance.\n\n\n\nKey Responsibilities\nAn opportunity to work on high-visibility projects with top Pharma clients around the globe.\nPersonalized training modules to develop your strategy & consulting acumen to grow your skills, industry knowledge, and capabilities.\nProvide Subject matter expertise in various sub-segments of the LS industry.\nSupport delivery of small to medium-sized teams to deliver consulting projects for global clients.\nResponsibilities may include strategy, implementation, process design, and change management for specific modules.\nWork with the team or as an Individual contributor on the project assigned which includes a variety of skills to be utilized from Data Engineering to Data Science\nDevelop assets and methodologies, point-of-view, research, or white papers for use by the team and the larger community.\nAcquire new skills that have utility across industry groups.\nSupport strategies and operating models focused on some business units and assess likely competitive responses. Also, assess implementation readiness and points of greatest impact.\n\n\n\n\n\nAdditional Information\nProficient in Excel, MS Word, PowerPoint, etc.\nAbility to solve complex business problems and deliver client delight.\nStrong writing skills to build points of view on current industry trends.\nGood communication, interpersonal, and presentation skills\n\n\nAbout Our Company | Accenture (do not remove the hyperlink)\n\nQualification\n\n\n\nExperience:Proven experience (4+ years) in working on Life Sciences/Pharma/Healthcare projects and delivering successful outcomes.\n\n\n\n\nEducational Qualification:Bachelors or Masters degree in Statistics, Data Science, Applied Mathematics, Business Analytics, Computer Science, Information Systems, or other Quantitative field.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['pharmaceutical', 'python', 'sql', 'life sciences', 'aws', 'microsoft azure', 'power bi', 'time series', 'machine learning', 'data engineering', 'artificial intelligence', 'tableau', 'r', 'data science', 'gcp', 'spark', 'predictive modeling', 'statistical modeling', 'data visualization', 'statistics']",2025-06-14 05:18:13
S&C Global Network - AI - CG&S - Consultant Data Science,Accenture,4 - 8 years,Not Disclosed,['Bengaluru'],"Job Title: Industry & Function AI Decision Science Consultant S&C Global Network\n\n\n\nManagement Level: 9 Consultant\n\n\n\nLocation: Primary - Bengaluru, Secondary - Gurugram\n\n\n\nMust-Have\n\n\n\n\nSkills:\nData Science, AI, ML, Experience with cloud platforms such as AWS, Azure, or Google Cloud, Hands-on experience in programming languages like Python, R, PySpark, and SQL\n\n\n\nGood-to-Have\n\n\n\n\nSkills:\nDeep Learning Techniques (e.g. RNN, CNN), Visualization tools like Power BI and Tableau, Exposure to tools like ChatGPT, Llama 2, Hugging Face, etc.\n\n\n\nJob\n\n\nSummary:\n\nAs an Industry & Function AI Decision Science Consultant, you will leverage your expertise in data science and Consumer Goods domain knowledge to design and deliver AI-driven solutions. Your role will include strategic analysis, project delivery, solution development, and technical execution to empower businesses with actionable insights and enable automated and augmented decision-making.\n\n\n\n\nRoles & Responsibilities:\nConduct strategic analysis of the AI, analytics, and data maturity landscape for clients in the Consumer Goods domain\nLead data science engagements, manage delivery teams, and build innovative AI capabilities\nDevelop and implement advanced analytics solutions tailored to client requirements\nUtilize languages like Python, PySpark, R, and SQL for data wrangling and machine learning model development\nLeverage cloud technologies (Azure, AWS, GCP) to integrate and implement AI solutions\nTranslate complex data into compelling narratives for effective data storytelling\nMentor junior team members and contribute to thought leadership\n\n\n\n\n\nProfessional & Technical\n\n\n\n\nSkills:\n\nProficiency in Python, R, PySpark, and SQL\nStrong knowledge of traditional statistical methods, machine learning techniques, and deep learning\nHands-on experience in Consumer Goods & Services domain\nCloud integration skills with platforms like AWS, Azure, or Google Cloud\nExperience with optimization techniques (exact and evolutionary)\nCertifications like AWS Certified Data Analytics Specialty or Google Professional Data Engineer\nFamiliarity with visualization tools like Tableau and Power BI\nExposure to large language models (e.g., ChatGPT, Llama 2)\nFamiliarity with version control systems like Git.\n\n\n\n\n\nAdditional Information:\nThe ideal candidate will have a strong educational background in data science, computer science, or a related field, along with a proven track record of delivering impactful AI-driven solutions in the Consumer Goods industry.\n\n\n\n\nAbout Our Company | Accenture\n\nQualification\n\n\n\nExperience: Minimum 4-8 years of hands-on experience in data science with a focus on the Consumer Goods industry\n\n\n\n\nEducational Qualification:Bachelors or Masters degree in Statistics, Economics, Mathematics, Computer Science, or equivalent degree with Data Science specialization (from a premier institute)",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'artificial intelligence', 'deep learning', 'data science', 'ml', 'advanced analytics', 'cnn', 'data analytics', 'pyspark', 'microsoft azure', 'power bi', 'machine learning', 'sql', 'r', 'tableau', 'git', 'rnn', 'gcp', 'machine learning algorithms', 'aws', 'consumer goods', 'statistics']",2025-06-14 05:18:15
Data Science,Global Banking Organization,5 - 10 years,Not Disclosed,['Bengaluru'],"Key Skills: Machine Learning, Data Science, Azure, Python, Hadoop.\nRoles and Responsibilities:\nStrong understanding of Math, Statistics, and the theoretical foundations of Statistical & Machine Learning, including Parametric and Non-parametric models.\nApply advanced data mining techniques to curate, process, and transform raw data into reliable datasets.\nUse various statistical techniques and ML methods to perform predictive modeling/classification for problems related to clients, distribution, sales, client profiles, and segmentation, and provide actionable insights for business decision-making.\nDemonstrate expertise in the full Machine Learning lifecycle--feature engineering, training, validation, scaling, deployment, scoring, monitoring, and feedback loops.\nProficiency in Python visualization libraries such as matplotlib and seaborn.\nExperience with cloud computing infrastructure like Azure, including Machine Learning Studio, Azure Data Factory, Synapse, Python, and PySpark.\nAbility to develop, test, and deploy models on cloud/web platforms.\nExcellent knowledge of Deep Learning Architectures, including Convolutional Neural Networks and Transformer/LLM Foundation Models.\nStrong expertise in supervised and adversarial learning techniques.\nRobust working knowledge of deep learning frameworks such as TensorFlow, Keras, and PyTorch.\nExcellent Python coding skills.\nExperience with version control tools (Git, GitHub/GitLab) and data version control.\nExperience in end-to-end model deployment and productionization.\nDemonstrated proficiency in deploying, scaling, and optimizing ML models in production environments with low latency, high availability, and cost efficiency.\nSkilled in model interpretability and CI/CD for ML using tools like MLflow and Kubeflow, with the ability to implement automated monitoring, logging, and retraining strategies.\nExperience Requirement:\n5-12 years of experience in designing and deploying deep learning and machine learning solutions.\nProven track record of delivering AI/ML solutions in real-world business applications at scale.\nHands-on experience working in cross-functional teams including data engineers, product managers, and business stakeholders.\nExperience mentoring junior data scientists and providing technical leadership within a data science team.\nExperience working with big data tools and environments such as Hadoop, Spark, or Databricks is a plus.\nPrior experience in managing model lifecycle in enterprise production environments including drift detection and retraining pipelines.\nEducation: B.Tech.",Industry Type: Financial Services,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Azure', 'Hadoop.', 'Machine Learning', 'Python']",2025-06-14 05:18:17
S&C GN - Tech Strategy & Advisory - Data Architect - Manager,Accenture,10 - 15 years,Not Disclosed,"['Mumbai', 'Gurugram', 'Bengaluru']","Data Strategy & Data Governance Manager\n\nJoin our team in Technology Strategy for an exciting career opportunity to enable our most strategic clients to realize exceptional business value from technology\n\nPractice: Technology Strategy & Advisory, Capability Network\n\nAreas of Work: Data Strategy\n\nLevel: Manager\n\n| Location: Bangalore / Gurgaon / Mumbai / Pune / Chennai / Hyderabad / Kolkata |\n\nYears of Exp: 10 to 15 years\n\nExplore an Exciting Career at Accenture\n\nDo you believe in creating an impactAre you a problem solver who enjoys working on transformative strategies for global clientsAre you passionate about being part of an inclusive, diverse and collaborative culture\n\nThen, this is the right place for you! Welcome to a host of exciting global opportunities in Accenture Technology Strategy & Advisory.\n\nThe Practice- A Brief Sketch:\n\nThe Technology Strategy & Advisory Practice is a part of and focuses on the clients most strategic priorities. We help clients achieve growth and efficiency through innovative R&D transformation, aimed at redefining business models using agile methodologies.\n\nAs part of this high performing team, you will work on the scaling Data & Analyticsand the data that fuels it allto power every single person and every single process. You will part of our global team of experts who work on the right scalable solutions and services that help clients achieve your business objectives faster.\nBusiness Transformation: Assessment of Data & Analytics potential and development of use cases that can transform business\nTransforming Businesses: Envisioning and Designing customized, next-generation data and analytics products and services that help clients shift to new business models designed for today's connectedlandscape of disruptive technologies\nFormulation of Guiding Principles and Components: Assessing impact to clients technology landscape/ architecture and ensuring formulation of relevant guiding principles and platform components.\nProduct and Frameworks :Evaluate existing data and analytics products and frameworks available and develop options for proposed solutions.\n\nBring your best skills forward to excel in the role:\nLeverage your\n\nknowledge of technology trends across Data & Analytics and how they can be applied to address real world problems and opportunities. Interact with client stakeholders to\n\nunderstand their Data & Analytics problems, priority use-cases, define a problem statement, understand the scope of the engagement, and also drive projects to deliver value to the client Design & guide development of\n\nEnterprise-wide Data & Analytics strategy for our clients that includes Data & Analytics Architecture, Data on Cloud, Data Quality, Metadata and Master Data strategy Establish framework for effective\n\nData Governance across multispeed implementations. Define data ownership, standards, policies and associated processes Define a\n\nData & Analytics operating model to manage data across organization . Establish processes around effective data management ensuring Data Quality & Governance standards as well as roles for Data Stewards\n\nBenchmark against global research benchmarks and leading industry peers to understand current & recommend Data & Analytics solutions Conduct\n\ndiscovery workshops and design sessions to elicit Data & Analytics opportunities and client pain areas. Develop and Drive\n\nData Capability Maturity Assessment, Data & Analytics Operating Model & Data Governance exercises for clients A fair understanding of\n\ndata platform strategy for data on cloud migrations, big data technologies, large scale data lake and DW on cloud solutions. Utilize\n\nstrong expertise & certification in any of the Data & Analytics on Cloud platforms Google, Azure or AWS\n\nCollaborate with business experts for business understanding, working with other consultants and platform engineers for solutions and with technology teams for prototyping and client implementations. Create expert content and\n\nuse advanced presentation, public speaking, content creation and communication skills for C-Level discussions.\n\nDemonstrate strong understanding of a specific industry , client or technology and function as an expert to advise senior leadership.\n\nManage budgeting and forecasting activities and build financial proposals\n\nQualification\n\nYour experience counts!\nMBA from a tier 1 institute\n5 7 years of Strategy Consulting experience at a consulting firm\n3+ years of experience on projects showcasing skills across these capabilities- Data Capability Maturity Assessment, Data & Analytics Strategy, Data Operating Model & Governance, Data on Cloud Strategy, Data Architecture Strategy\nAt least 2 years of experience on architecting or designing solutions for any two of these domains - Data Quality, Master Data (MDM), Metadata, data lineage, data catalog.\nExperience in one or more technologies in the data governance space:Collibra, Talend, Informatica, SAP MDG, Stibo, Alteryx, Alation etc.\n3+ years of experience in designing end-to-end Enterprise Data & Analytics Strategic Solutions leveraging Cloud & Non-Cloud platforms like AWS, Azure, GCP, AliCloud, Snowflake, Hadoop, Cloudera, Informatica, Snowflake, Palantir\nDeep Understanding of data supply chain and building value realization framework for data transformations\n3+ years of experience leading or managing teams effectively including planning/structuring analytical work, facilitating team workshops, and developing Data & Analytics strategy recommendations as well as developing POCs\nFoundational understanding of data privacy is desired\nMandatory knowledge of IT & Enterprise architecture concepts through practical experience and knowledge of technology trends e.g. Mobility, Cloud, Digital, Collaboration\nA strong understanding in any of the following industries is preferred:Financial Services, Retail, Consumer Goods, Telecommunications, Life Sciences, Transportation, Hospitality, Automotive/Industrial, Mining and Resources or equivalent domains\nCDMP Certification from DAMA preferred\nCloud Data & AI Practitioner Certifications (Azure, AWS, Google) desirable but not essential",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Collibra', 'Informatica', 'Stibo', 'SAP MDG', 'Talend', 'Google', 'Azure', 'Palantir', 'Cloudera', 'Alteryx', 'Alation', 'Hadoop', 'AliCloud', 'GCP', 'Snowflake', 'AWS']",2025-06-14 05:18:19
"STAFF, DATA SCIENTIST",Walmart,5 - 10 years,Not Disclosed,['Bengaluru'],"Position Summary...\nWhat youll do...\nAbout Team\nThe Catalog Data Science Team at Walmart Global Tech is focused on using the latest research in generative AI (GenAI), artificial intelligence (AI), machine learning (ML), statistics, deep learning, computer vision and optimization to implement solutions that ensure Walmart s product catalog is accurate, complete, and optimized for customer experience. Our team tackles complex data science and ML engineering challenges related to product classification, attribute extraction, trust & safety, and catalog optimization, empowering next-generation retail use cases.\nThe Data Science and ML Engineering community at Walmart Global Tech is active in most of the Hack events, utilizing the petabytes of data at our disposal, to build some of the coolest ideas. All the work we do at Walmart Global Tech will eventually benefit our operations & our associates, helping Customers Save Money to Live Better.\nWhat youll do:\nWe are looking for a Staff Machine Learning Engineer who can help build large scale AI/ML/Optimization products. Expected qualities include ability to build, deploy, maintain and troubleshoot large scale systems.\nAs a Staff ML Engineer, you ll have the opportunity to\nDrive research initiatives and proof-of-concepts that push the state of the art in generative AI and large-scale machine learning.\nDesign and implement high-throughput, low-latency AI/ML pipelines and microservices that operate at global scale.\nOversee data ingestion, model training, evaluation, deployment and monitoring-ensuring performance, quality and reliability.\nCustomize and optimize LLMs for specific business use cases, balancing accuracy, latency and cost.\nPrototype novel generative AI solutions, integrate advancements into production, and collaborate with research partners.\nChampion best practices in data quality, lineage, governance and cost optimization across ML pipelines.\nMentor a team of ML engineers, establish coding standards, conduct design reviews, and foster a culture of continuous improvement.\nPresent your team s work at top-tier AI/ML conferences, publish scientific papers, and cultivate partnerships with universities and research labs.\nWhat youll bring:\nPhD in Computer Science, Statistics, Applied Mathematics or related field with 5+ years experience in ML engineering-or Master s with 8+ years or Bachelor s with 10+ years.\nProven track record of leading and scaling AI/ML products in production environments.\nDeep expertise in generative AI, large-scale model deployment, and fine-tuning of transformer-based architectures.\nStrong programming skills in Python, or equivalent, and experience with big data frameworks (Spark, Hadoop) and ML platforms (TensorFlow, PyTorch).\nDemonstrated history of scientific publications or patents in AI/ML.\nExcellent communication skills, a growth mindset, and the ability to drive cross-functional collaboration.\nAbout Walmart Global Tech\n.\n.\nFlexible, hybrid work\n.\nBenefits\n.\nBelonging\n.\n.\nEqual Opportunity Employer\nWalmart, Inc., is an Equal Opportunities Employer - By Choice. We believe we are best equipped to help our associates, customers and the communities we serve live better when we really know them. That means understanding, respecting and valuing unique styles, experiences, identities, ideas and\nMinimum Qualifications...\nMinimum Qualifications:Option 1: Bachelors degree in Statistics, Economics, Analytics, Mathematics, Computer Science, Information Technology or related field and 4 years experience in an analytics related field. Option 2: Masters degree in Statistics, Economics, Analytics, Mathematics, Computer Science, Information Technology or related field and 2 years experience in an analytics related field. Option 3: 6 years experience in an analytics or related field.\nPreferred Qualifications...\nPrimary Location...",Industry Type: Retail,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Prototype', 'Networking', 'Coding', 'Machine learning', 'Continuous improvement', 'Information technology', 'Monitoring', 'Analytics', 'Python']",2025-06-14 05:18:22
Data Scientist,Ford,1 - 8 years,Not Disclosed,['Chennai'],"Ford/GDIA Mission and Scope:\n\n\nCreating the future of smart mobility requires the highly intelligent use of data, metrics, and analytics. That s where you can make an impact as part of our Global Data Insight & Analytics team. We are the trusted advisers that enable Ford to clearly see business conditions, customer needs, and the competitive landscape. With our support, key decision-makers can act in meaningful, positive ways. Join us and use your data expertise and analytical skills to drive evidence-based, timely decision-making.\n\nThe Global Data Insights and Analytics (GDI&A) department at Ford Motors Company is looking for qualified people who can develop scalable solutions to complex real-world problems using Machine Learning, Big Data, Statistics, Econometrics, and Optimization. The goal of GDI&A is to drive evidence-based decision making by providing insights from data. Applications for GDI&A include, but are not limited to, Connected Vehicle, Smart Mobility, Advanced Operations, Manufacturing, Supply chain, Logistics, and Warranty Analytics.\n\nAbout the Role:\n\nYou will be part of the FCSD analytics team, playing a critical role in leveraging data science to drive significant business impact within Ford Customer Service Division. As a Data Scientist, you will translate complex business challenges into data-driven solutions. This involves partnering closely with stakeholders to understand problems, working with diverse data sources (including within GCP), developing and deploying scalable AI/ML models, and communicating actionable insights that deliver measurable results for Ford.\nQualifications:\n\n\nAt least 2 years of relevant professional experience applying data science techniques to solve business problems. This includes demonstrated hands-on proficiency with SQL and Python.\n\nBachelors or Masters degree in a quantitative field (e. g. , Statistics, Computer Science, Mathematics, Engineering, Economics).\n\nHands-on experience in conducting statistical data analysis (EDA, forecasting, clustering, hypothesis testing, etc. ) and applying machine learning techniques (Classification/Regression, NLP, time-series analysis, etc. ).\n\n\n\n\n\nTechnical Skills:\n\n\nProficiency in SQL, including the ability to write and optimize queries for data extraction and analysis.\n\nProficiency in Python for data manipulation (Pandas, NumPy), statistical analysis, and implementing Machine Learning models (Scikit-learn, TensorFlow, PyTorch, etc. ).\n\nWorking knowledge in a Cloud environment (GCP, AWS, or Azure) is preferred for developing and deploying models.\n\nExperience with version control systems, particularly Git.\n\nNice to have: Exposure to Generative AI / Large Language Models (LLMs).\n\n\n\n\n\nFunctional Skills:\n\n\nProven ability to understand and formulate business problem statements.\n\nAbility to translate Business Problem statements into data science problems.\n\nStrong problem-solving ability, with the capacity to analyze complex issues and develop effective solutions.\n\nExcellent verbal and written communication skills, with a demonstrated ability to translate complex technical information and results into simple, understandable language for non-technical audiences.\n\nStrong business engagement skills, including the ability to build relationships, collaborate effectively with stakeholders, and contribute to data-driven decision-making.\n\n\n\nBuild an in-depth understanding of the business domain and data sources, demonstrating strong business acumen.\n\nExtract, analyze, and transform data using SQL for insights.\n\nApply statistical methods and develop ML models to solve business problems.\n\nDesign and implement analytical solutions, contributing to their deployment, ideally leveraging Cloud environments.\n\nWork closely and collaboratively with Product Owners, Product Managers, Software Engineers, and Data Engineers within an agile development environment.\n\nIntegrate and operationalize ML models for real-world impact.\n\nMonitor the performance and impact of deployed models, iterating as needed.\n\nPresent findings and recommendations effectively to both technical and non-technical audiences to inform and drive business decisions.",Industry Type: Auto Components,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Supply chain', 'Data analysis', 'Analytical', 'Customer service', 'Econometrics', 'Forecasting', 'Analytics', 'SQL', 'Logistics', 'Data extraction']",2025-06-14 05:18:25
Data Scientist,Ford,1 - 8 years,Not Disclosed,['Chennai'],"Ford/GDIA Mission and Scope:\n\n\nCreating the future of smart mobility requires the highly intelligent use of data, metrics, and analytics. That s where you can make an impact as part of our Global Data Insight & Analytics team. We are the trusted advisers that enable Ford to clearly see business conditions, customer needs, and the competitive landscape. With our support, key decision-makers can act in meaningful, positive ways. Join us and use your data expertise and analytical skills to drive evidence-based, timely decision-making.\n\nThe Global Data Insights and Analytics (GDI&A) department at Ford Motors Company is looking for qualified people who can develop scalable solutions to complex real-world problems using Machine Learning, Big Data, Statistics, Econometrics, and Optimization. The goal of GDI&A is to drive evidence-based decision making by providing insights from data. Applications for GDI&A include, but are not limited to, Connected Vehicle, Smart Mobility, Advanced Operations, Manufacturing, Supply chain, Logistics, and Warranty Analytics.\n\nAbout the Role:\n\nYou would be part of FCSD analytics team.\n\nAs a Data Scientist on the team, you will collaborate within the team and work with business partners to understand business problems and explore data from various sources in GCP-Data Factory, wrangle them to develop solutions using AI/ML algorithms to provide actionable insights that deliver key results to Ford.\n\nThe potential candidate should have hands-on experience in building statistical/machine learning models adhering to the best practices of development and deployment in cloud environment. This role requires a solid problem-solving skill, business acumen, and passion for leveraging data science/AI skills to drive business results.\nQualifications:\n\n\nAt least 2 years of relevant work experience in solving business problems using data science\n\nBachelors/master s degree in quantitative domain, Statistics, Computer science, Mathematics, Engineering with MBA from a premier institute (BE, MS, MBA, BSc/MSc -Computer science/Statistics) or any other equivalent\n\n2+ years of experience with SQL, Python delivering analytical solutions in production environment.\n\nAt least 1 year of experience working in Cloud environment (GCP or AWS or Azure)\n\n2+ years of experience in conducting statistical data analysis (EDA, forecasting, clustering, etc. , ) and machine learning techniques (Classification/Regression, NLP)\n\n\nTechnical Skills:\n\n\nProficient in BigQuery/SQL, Python\n\nAdvanced SQL knowledge to handle large data, optimize queries.\n\nWorking knowledge in GCP environment (Big Query, Vertex AI) to develop and deploy machine Learning models\n\nNice to have: Exposure to Gen AI/LLM\n\n\nFunctional Skills:\n\n\nUnderstanding and formulating business problem statements\n\nConvert Business Problem statement into data science problems.\n\nSelf-motivated with excellent verbal and written skills\n\nHighly credible in organizational, time management and decision making.\n\nExcellent Problem-Solving and Interpersonal skills\n\nJob Responsibilities\n\n\nBuild an in-depth understanding of the business domain and data sources.\n\nExtract, Analyse data from database/data warehouse to gain insights, discover trends and patterns with clear objectives in mind.\n\nDesign and implement scalable analytical solutions in Google cloud environment.\n\nWork closely with Product Owner, Product Manager, Software engineers and Data engineers to build products in agile environment.\n\nOperationalize AI/ML/LLM models by integrating with upstream and downstream business processes.\n\nCommunicate results to business teams through effective presentations.\n\nWork with business partners through problem formulation, data management, solutions development, operationalization, and solutions management\n\nIdentify opportunities to build analytical solutions driving business value, leveraging various data sources.",Industry Type: Auto Components,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Supply chain', 'Computer science', 'Data analysis', 'Data management', 'Machine learning', 'Agile', 'Analytics', 'SQL', 'Python', 'Logistics']",2025-06-14 05:18:27
Data Scientist-Artificial Intelligence,IBM,3 - 7 years,Not Disclosed,['Bengaluru'],"As an Associate Data Scientist at IBM, you will work to solve business problems using leading edge and open-source tools such as Python, R, and TensorFlow, combined with IBM tools and our AI application suites. You will prepare, analyze, and understand data to deliver insight, predict emerging trends, and provide recommendations to stakeholders.\n\nIn your role, you may be responsible for\nImplementing and validating predictive and prescriptive models and creating and maintaining statistical models with a focus on big data & incorporating machine learning. techniques in your projects\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\n\n\nPreferred technical and professional experience",,,,"['python', 'scikit-learn', 'tensorflow', 'pytorch', 'keras', 'natural language processing', 'neural networks', 'predictive', 'huggingface', 'machine learning', 'prototype', 'artificial intelligence', 'sql', 'pandas', 'deep learning', 'r', 'java', 'cobol', 'data science', 'matplotlib', 'big data', 'statistics']",2025-06-14 05:18:29
Data Analysis - English Specialist Data Analysis - English Specialist,Zensar,2 - 6 years,Not Disclosed,"['Kolkata', 'Mumbai', 'New Delhi', 'Hyderabad', 'Pune', 'Chennai', 'Bengaluru']","Zensar Technologies is looking for Data Analysis - English Specialist Data Analysis - English Specialist to join our dynamic team and embark on a rewarding career journey A Data Analyst is responsible for collecting, analyzing, and interpreting data to identify trends, patterns, and insights that can drive informed business decisions\n\nThey play a crucial role in helping organizations understand their data, derive actionable insights, and optimize processes\n\nHere is a general job description for a Data Analyst:Data Collection and Preparation: Collect and gather relevant data from various sources, such as databases, spreadsheets, and external systems\n\nClean, validate, and transform the data to ensure accuracy and consistency\n\nData Analysis and Interpretation: Apply statistical techniques, data mining methods, and visualization tools to analyze large datasets\n\nIdentify trends, patterns, and correlations within the data and generate insights to support decision-making\n\nReporting and Visualization: Create clear and concise reports, dashboards, and visual representations of data using data visualization tools, such as Tableau, Power BI, or Excel\n\nPresent findings to stakeholders in a compelling and understandable manner\n\nData Quality and Integrity: Ensure data integrity and accuracy by conducting data validation, resolving discrepancies, and monitoring data quality\n\nImplement measures to maintain data privacy, security, and compliance with regulatory requirements\n\nBusiness Needs Assessment: Collaborate with stakeholders to understand their data analysis requirements and translate them into actionable analytics projects\n\nIdentify key performance indicators (KPIs) and metrics to measure business performance and success\n\nData-Driven Decision Making: Assist in making data-driven decisions by providing insights and recommendations based on data analysis\n\nSupport strategic planning, operational improvements, and process optimizations based on data-driven insights\n\nData Modeling and Forecasting: Develop and maintain data models, predictive models, and forecasting models to anticipate trends, predict outcomes, and support future planning\n\nUtilize statistical software, programming languages, or machine learning techniques as necessary\n\nContinuous Improvement: Stay updated with the latest data analysis techniques, tools, and trends\n\nContinuously improve data analysis processes, methodologies, and automation to enhance efficiency and effectiveness\n\nand Communication: Work closely with cross-functional teams, such as business analysts, data engineers, and data scientists, to align data analysis efforts with organizational goals\n\nCommunicate findings, insights, and recommendations to non-technical stakeholders in a clear and understandable manner\n\nDocumentation and Knowledge Sharing: Document data analysis methodologies, processes, and findings for future reference\n\nShare knowledge and best practices with the team to promote a culture of learning and data-driven decision-making\n\nSkills and Qualifications:Strong analytical skills with the ability to manipulate and analyze complex datasets\n\nProficiency in data analysis tools such as SQL, Excel, Python, R, or similar tools\n\nExperience with data visualization tools such as Tableau, Power BI, or similar tools\n\nKnowledge of statistical analysis techniques and methodologies\n\nFamiliarity with data modeling, predictive modeling, and forecasting techniques\n\nUnderstanding of database concepts and query languages\n\nExcellent attention to detail and problem-solving abilities\n\nStrong communication and presentation skills\n\nAbility to work independently and collaborate in a team environment\n\nFamiliarity with data privacy, security, and regulatory compliance\n\nPrior experience in data analysis or a related field is preferred",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Design engineering', 'Data analysis', 'Technology consulting', 'Focus', 'Agile', 'Conceptualization', 'Management']",2025-06-14 05:18:32
Data Science Assoc Analyst,Pepsico,2 - 4 years,Not Disclosed,['Hyderabad'],"Overview \n\nData Science Team works in developing Machine Learning (ML) and Artificial Intelligence (AI) projects. Specific scope of this role is to develop ML solution in support of ML/AI projects using big analytics toolsets in a CI/CD environment. Analytics toolsets may include DS tools/Spark/Databricks, and other technologies offered by Microsoft Azure or open-source toolsets. This role will also help automate the end-to-end cycle with Azure Pipelines.\n\nYou will be part of a collaborative interdisciplinary team around data, where you will be responsible of our continuous delivery of statistical/ML models. You will work closely with process owners, product owners and final business users. This will provide you the correct visibility and understanding of criticality of your developments.\n\n \n\n\n",,,,"['python', 'pyspark', 'machine learning', 'ml', 'statistics', 'hive', 'continuous integration', 'github', 'supply chain', 'natural language processing', 'ci/cd', 'microsoft azure', 'apache pig', 'artificial intelligence', 'sql', 'docker', 'data bricks', 'git', 'spark', 'gcp', 'devops', 'oracle adf', 'jenkins', 'aws']",2025-06-14 05:18:34
AVP Data Management Analyst,Wells Fargo,2 - 7 years,Not Disclosed,['Bengaluru'],"About this role:\nWells Fargo is seeking a Data Management Analyst\n\nIn this role, you will:\nParticipate in less complex analysis to identify and remediate data quality or integrity issues and to identify and remediate process or control gaps\nAdhere to data governance standards and procedures",,,,"['Data Management', 'Agile Methodology', 'Funds Transfer Pricing', 'Financial Data Mapping', 'Big Data Query Techniques', 'Lineage Tracing', 'Data warehousing', 'Data Governance', 'Jira', 'Market Risks', 'SQL']",2025-06-14 05:18:36
S&C GN - Data&AI - Life Sciences - Analyst,Accenture,2 - 7 years,Not Disclosed,['Bengaluru'],"Management Level:Ind & Func AI Decision Science Analyst\n\n\n\n\nJob Location:Bangalore / Gurgaon\n\n\n\nMust-have\n\n\n\n\nSkills:\nLife Sciences/Pharma/Healthcare projects and delivering successful outcomes, commercial, clinical, Statistical Models/Machine Learning including Segmentation & predictive modeling, hypothesis testing, multivariate statistical analysis, time series techniques, and optimization.\n\n\n\nGood-to-have\n\n\n\n\nSkills:\nProficiency in Programming languages such as R, Python, SQL, Spark, AWS, Azure, or Google Cloud for deploying and scaling language models, Data Visualization tools like Tableau, Power BI\n\n\n\nJob\n\n\nSummary\n\nWe are seeking an experienced and visionary - Accenture S&C Global Network - Data & AI practice help our clients grow their business in entirely new ways. Analytics enables our clients to achieve high performance through insights from data - insights that inform better decisions and strengthen customer relationships. From strategy to execution, Accenture works with organizations to develop analytic capabilities - from accessing and reporting on data to predictive modelling - to outperform the competition.\n\n\n\nKey Responsibilities\nSupport delivery of small to medium-sized teams to deliver consulting projects for global clients.\nAn opportunity to work on high-visibility projects with top Pharma clients around the globe.\nPersonalized training modules to develop your strategy & consulting acumen to grow your skills, industry knowledge, and capabilities.\nResponsibilities may include strategy, implementation, process design, and change management for specific modules.\nWork with the team or as an Individual contributor on the project assigned which includes a variety of skills to be utilized from Data Engineering to Data Science\nDevelop assets and methodologies, point-of-view, research, or white papers for use by the team and the larger community.\nWork on variety of projects in Data Modeling, Data Engineering, Data Visualization, Data Science etc.,\nAcquire new skills that have utility across industry groups.\n\n\n\n\n\nAdditional Information\nAbility to solve complex business problems and deliver client delight.\nStrong writing skills to build points of view on current industry trends.\nGood communication, interpersonal, and presentation skills\n\n\nAbout Our Company | Accenture (do not remove the hyperlink)\n\n\nQualification\n\n\n\nExperience:Proven experience (2+ years) in working on Life Sciences/Pharma/Healthcare projects and delivering successful outcomes.\n\n\n\n\nEducational Qualification:Bachelors or Masters degree in Statistics, Data Science, Applied Mathematics, Business Analytics, Computer Science, Information Systems, or other Quantitative field.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['pharmaceutical', 'python', 'sql', 'life sciences', 'aws', 'presentation skills', 'microsoft azure', 'power bi', 'time series', 'machine learning', 'artificial intelligence', 'tableau', 'r', 'data science', 'gcp', 'spark', 'predictive modeling', 'statistical modeling', 'data visualization', 'statistics']",2025-06-14 05:18:39
S&C GN - Data&AI - Retail - Consultant,Accenture,4 - 9 years,Not Disclosed,['Bengaluru'],"Job Title - Retail Specialized Data Scientist Level 9 SnC GN Data & AI\n\n\n\nManagement Level:09 - Consultant\n\n\n\nLocation:Bangalore / Gurgaon / Mumbai / Chennai / Pune / Hyderabad / Kolkata\n\n\n\nMust have skills:\nA solid understanding of retail industry dynamics, including key performance indicators (KPIs) such as sales trends, customer segmentation, inventory turnover, and promotions.\nStrong ability to communicate complex data insights to non-technical stakeholders, including senior management, marketing, and operational teams.\nMeticulous in ensuring data quality, accuracy, and consistency when handling large, complex datasets.\nGather and clean data from various retail sources, such as sales transactions, customer interactions, inventory management, website traffic, and marketing campaigns.\nStrong proficiency in Python for data manipulation, statistical analysis, and machine learning (libraries like Pandas, NumPy, Scikit-learn).\nExpertise in supervised and unsupervised learning algorithms\nUse advanced analytics to optimize pricing strategies based on market demand, competitor pricing, and customer price sensitivity.\n\n\n\n\nGood to have skills:\nFamiliarity with big data processing platforms like Apache Spark, Hadoop, or cloud-based platforms such as AWS or Google Cloud for large-scale data processing.\nExperience with ETL (Extract, Transform, Load) processes and tools like Apache Airflow to automate data workflows.\nFamiliarity with designing scalable and efficient data pipelines and architecture.\nExperience with tools like Tableau, Power BI, Matplotlib, and Seaborn to create meaningful visualizations that present data insights clearly.\n\n\nJob\n\n\nSummary: The Retail Specialized Data Scientist will play a pivotal role in utilizing advanced analytics, machine learning, and statistical modeling techniques to help our retail business make data-driven decisions. This individual will work closely with teams across marketing, product management, supply chain, and customer insights to drive business strategies and innovations. The ideal candidate should have experience in retail analytics and the ability to translate data into actionable insights.\n\n\n\n\nRoles & Responsibilities:\nLeverage Retail Knowledge:Utilize your deep understanding of the retail industry (merchandising, customer behavior, product lifecycle) to design AI solutions that address critical retail business needs.\nGather and clean data from various retail sources, such as sales transactions, customer interactions, inventory management, website traffic, and marketing campaigns.\nApply machine learning algorithms, such as classification, clustering, regression, and deep learning, to enhance predictive models.\nUse AI-driven techniques for personalization, demand forecasting, and fraud detection.\nUse advanced statistical methods help optimize existing use cases and build new products to serve new challenges and use cases.\nStay updated on the latest trends in data science and retail technology.\nCollaborate with executives, product managers, and marketing teams to translate insights into business actions.\n\n\n\n\nProfessional & Technical Skills:\nStrong analytical and statistical skills.\nExpertise in machine learning and AI.\nExperience with retail-specific datasets and KPIs.\nProficiency in data visualization and reporting tools.\nAbility to work with large datasets and complex data structures.\nStrong communication skills to interact with both technical and non-technical stakeholders.\nA solid understanding of the retail business and consumer behavior.\nProgramming Languages:Python, R, SQL, Scala\nData Analysis Tools:Pandas, NumPy, Scikit-learn, TensorFlow, Keras\nVisualization Tools:Tableau, Power BI, Matplotlib, Seaborn\nBig Data Technologies:Hadoop, Spark, AWS, Google Cloud\nDatabases:SQL, NoSQL (MongoDB, Cassandra)\n\n\n\n\nAdditional Information: -\n\nQualification\n\n\n\nExperience:Minimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification:Bachelors or Master's degree in Data Science, Statistics, Computer Science, Mathematics, or a related field.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'machine learning', 'artificial intelligence', 'data visualization', 'statistics', 'algorithms', 'data manipulation', 'scikit-learn', 'scala', 'numpy', 'unsupervised learning', 'sql', 'pandas', 'tensorflow', 'spark', 'consumer behavior', 'keras', 'hadoop', 'aws', 'reporting tools', 'retail business']",2025-06-14 05:18:41
Enterprise Data Operations Analyst,Pepsico,4 - 9 years,Not Disclosed,['Gurugram'],"Overview \n\nAs a member of the data engineering team, you will be the key technical expert developing and overseeing PepsiCo's data product build & operations and drive a strong vision for how data engineering can proactively create a positive impact on the business. You'll be an empowered member of a team of data engineers who build data pipelines into various source systems, rest data on the PepsiCo Data Lake, and enable exploration and access for analytics, visualization, machine learning, and product development efforts across the company. As a member of the data engineering team, you will help lead the development of very large and complex data applications into public cloud environments directly impacting the design, architecture, and implementation of PepsiCo's flagship data products around topics like revenue management, supply chain, manufacturing, and logistics. You will work closely with process owners, product owners and business users. You'll be working in a hybrid environment with in-house, on-premise data sources as well as cloud and remote systems and help grow DevOps and DataOps culture.\n\n \n\n\n",,,,"['azure policy', 'azure devops', 'arm templates', 'api', 'rest', 'artifactory', 'kubernetes', 'sonar', 'docker', 'ansible', 'azure cli', 'pcf', 'git', 'gcp', 'devops', 'powershell', 'paas', 'jenkins', 'python', 'microsoft azure', 'groovy', 'automation engineering', 'infrastructure', 'scrum', 'terraform', 'agile', 'aws']",2025-06-14 05:18:43
IN_Director_Senior Data Architect_Data & Analytics_Advisory_ Bangalore,PwC Service Delivery Center,12 - 18 years,Not Disclosed,['Bengaluru'],"Not Applicable\nSpecialism\nData, Analytics & AI\nManagement Level\nDirector\n& Summary\n\n\n.\n& Summary We are seeking an experienced Senior Data Architect to lead the design and development of our data architecture, leveraging cloudbased technologies, big data processing frameworks, and DevOps practices. The ideal candidate will have a strong background in data warehousing, data pipelines, performance optimization, and collaboration with DevOps teams.\nResponsibilities\n1. Design and implement endtoend data pipelines using cloudbased services (AWS/ GCP/Azure) and conventional data processing frameworks.\n2. Lead the development of data architecture, ensuring scalability, security, and performance.\n3. Collaborate with crossfunctional teams, including DevOps, to design and implement data lakes, data warehouses, and data ingestion/extraction processes. 4. Develop and optimize data processing workflows using PySpark, Kafka, and other big data processing frameworks.\n5. Ensure data quality, integrity, and security across all data pipelines and architectures.\n6. Provide technical leadership and guidance to junior team members.\n7. Design and implement data load strategies, data partitioning, and data storage solutions.\n8. Collaborate with stakeholders to understand business requirements and develop data solutions to meet those needs.\n9. Work closely with DevOps team to ensure seamless integration of data pipelines with overall system architecture.\n10. Participate in design and implementation of CI/CD pipelines for data workflows.\nDevOps Requirements\n1. Knowledge of DevOps practices and tools, such as Jenkins, GitLab CI/CD, or Apache Airflow.\n2. Experience with containerization using Docker.\n3. Understanding of infrastructure as code (IaC) concepts using tools like Terraform or AWS CloudFormation.\n4. Familiarity with monitoring and logging tools, such as Prometheus, Grafana, or ELK Stack.\nRequirements\n1. 1214 years of experience for Senior Data Architect in data architecture, data warehousing, and big data processing.\n2. Strong expertise in cloudbased technologies (AWS/ GCP/ Azure) and data processing frameworks (PySpark, Kafka, Flink , Beam etc.).\n3. Experience with data ingestion, data extraction, data warehousing, and data lakes.\n4. Strong understanding of performance optimization, data partitioning, and data storage solutions.\n5. Excellent leadership and communication skills.\n6. Experience with NoSQL databases is a plus.\nMandatory skill sets\n1. Experience with agile development methodologies.\n2. Certification in cloudbased technologies (AWS / GCP/ Azure) or data processing frameworks.\n3. Experience with data governance, data quality, and data security.\nPreferred skill sets\nKnowledge of AgenticAI and GenAI is added advantage\nYears of experience required\n12 to 18 years\nEducation qualification\nGraduate Engineer or Management Graduate\nEducation\nDegrees/Field of Study required Bachelor of Engineering\nDegrees/Field of Study preferred\nRequired Skills\nAWS Devops\nAccepting Feedback, Accepting Feedback, Active Listening, Analytical Reasoning, Analytical Thinking, Application Software, Business Data Analytics, Business Management, Business Technology, Business Transformation, Coaching and Feedback, Communication, Creativity, Documentation Development, Embracing Change, Emotional Regulation, Empathy, Implementation Research, Implementation Support, Implementing Technology, Inclusion, Influence, Innovation, Intellectual Curiosity, Learning Agility {+ 28 more}\nTravel Requirements\nGovernment Clearance Required?",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['System architecture', 'Business transformation', 'GCP', 'Analytical', 'Consulting', 'Data processing', 'Data quality', 'Operations', 'Monitoring', 'Data architecture']",2025-06-14 05:18:45
Senior Data Scientist | Snowflakes | Tableau | AI/ML,Cisco,0 - 2 years,Not Disclosed,['Bengaluru'],"Job posting may be removed earlier if the position is filled or if a sufficient number of applications are received.\n\nMeet the Team\n\nWe are a dynamic and innovative team of Data Engineers, Data Architects, and Data Scientists based in Bangalore, India. Our mission is to harness the power of data to provide actionable insights that empower executives to make informed, data-driven decisions. By analyzing and interpreting complex datasets, we enable the organization to understand the health of the business and identify opportunities for growth and improvement.\n\nYour Impact\n\nWe are seeking a highly experienced and skilled Senior Data Scientist to join our dynamic team. The ideal candidate will possess deep expertise in machine learning models, artificial intelligence (AI), generative AI, and data visualization. Proficiency in Tableau and other visualization tools is essential. This role requires hands-on experience with databases such as Snowflake and Teradata, as well as advanced knowledge in various data science and AI techniques. The successful candidate will play a pivotal role in driving data-driven decision-making and innovation within our organization.\n\nKey Responsibilities\nKey Technologies &\n\nMinimum Qualifications\nPreferred Qualifications (Provide up to five (5) bullet points these can include soft skills)",,,,"['machine learning', 'artificial intelligence', 'sql', 'tableau', 'data visualization', 'snowflake', 'scipy', 'python', 'scikit-learn', 'data warehousing', 'numpy', 'pandas', 'tensorflow', 'data integration tools', 'matplotlib', 'pytorch', 'keras', 'machine learning algorithms', 'etl', 'nosql databases']",2025-06-14 05:18:48
DE&A - AIML - Data Science - Data Science (Other) DE&A - AIML,Zensar,15 - 18 years,Not Disclosed,"['Kolkata', 'Mumbai', 'New Delhi', 'Hyderabad', 'Pune', 'Chennai', 'Bengaluru']","Experience: 15+ years overall | Minimum 10 full-cycle AI/ML project implementations , including GenAI experience\nRole Summary:\nWe are seeking a AI Architect to lead strategic AI transformation initiatives. This role demands deep hands-on experience in AI, Machine Learning (ML), and Generative AI (GenAI) , along with the ability to engage directly with C-level stakeholders , align technical delivery with business objectives, and drive enterprise-wide adoption of advanced AI solutions.\nThe ideal candidate is a techno-strategic leader who can take AI/ML/GenAI projects from ideation to production building architectures, leading cross-functional teams, and ensuring regulatory and operational alignment in BFSI environments.\nKey\nConsulting & Business Alignment\nPartner with senior business and IT leadership , including CIOs, CDOs, and COOs , to identify high-impact use cases across retail banking, insurance, credit, and capital markets.\nTranslate complex BFSI challenges into technically feasible and scalable AI/ML/GenAI solutions.\nCreate strategic roadmaps, capability assessments, and PoV/PoC execution plans that align with business KPIs and regulatory needs.\nSolution Architecture & Delivery Leadership\nDesign and lead delivery of AI/ML/GenAI pipelines covering data ingestion, model training, validation, deployment, and monitoring.\nBuild and scale GenAI-based solutions like LLM-driven chatbots, intelligent document processing, RAG pipelines, summarization tools , and virtual assistants.\nArchitect cloud-native AI platforms using AWS (SageMaker, Bedrock) , Azure (ML, OpenAI) , or GCP (Vertex AI, BigQuery, LangChain) .\nDefine and implement MLOps and LLMOps frameworks for versioning, retraining, CI/CD, and production observability.\nEnsure adherence to Responsible AI principles , including explainability, bias mitigation, auditability, and regulatory compliance\nEngineering & Integration\nWork closely with data engineering teams to acquire, transform, and pipeline data from core banking systems, CRMs, claims systems, and real-time feeds.\nDesign architecture for data lakes, feature stores, and vector databases supporting AI and GenAI use cases.\nEnable seamless integration of AI capabilities into enterprise workflows, customer platforms, and decision engines via APIs and microservices.\nRequired Skills & Experience:\n15+ years of experience in AI/ML, data engineering, and cloud architecture.\nMinimum of 10 end-to-end AI/ML project implementations from use case discovery through to productionization.\nProven expertise in: (Any One)\nAI/ML frameworks : scikit-learn, XGBoost, TensorFlow, PyTorch\nGenAI/LLM platforms : OpenAI, Cohere, Mistral, LangChain, Hugging Face, vector DBs (Pinecone, FAISS, Chroma)\nCloud platforms : AWS, Azure, GCP - including AI/ML & GenAI native services\nMLOps/LLMOps tools : MLflow, Kubeflow, SageMaker Pipelines, Vertex AI Pipelines\nStrong experience with data security, governance, model risk management , and AI compliance frameworks relevant to BFSI.\nAbility to lead large cross-functional teams and engage both technical teams and senior stakeholders.\nExperience: 15+ years overall | Minimum 10 full-cycle AI/ML project implementations , including GenAI experience\nRole Summary:\nWe are seeking a AI Architect to lead strategic AI transformation initiatives. This role demands deep hands-on experience in AI, Machine Learning (ML), and Generative AI (GenAI) , along with the ability to engage directly with C-level stakeholders , align technical delivery with business objectives, and drive enterprise-wide adoption of advanced AI solutions.\nThe ideal candidate is a techno-strategic leader who can take AI/ML/GenAI projects from ideation to production building architectures, leading cross-functional teams, and ensuring regulatory and operational alignment in BFSI environments.\nKey\nConsulting & Business Alignment\nPartner with senior business and IT leadership , including CIOs, CDOs, and COOs , to identify high-impact use cases across retail banking, insurance, credit, and capital markets.\nTranslate complex BFSI challenges into technically feasible and scalable AI/ML/GenAI solutions.\nCreate strategic roadmaps, capability assessments, and PoV/PoC execution plans that align with business KPIs and regulatory needs.\nSolution Architecture & Delivery Leadership\nDesign and lead delivery of AI/ML/GenAI pipelines covering data ingestion, model training, validation, deployment, and monitoring.\nBuild and scale GenAI-based solutions like LLM-driven chatbots, intelligent document processing, RAG pipelines, summarization tools , and virtual assistants.\nArchitect cloud-native AI platforms using AWS (SageMaker, Bedrock) , Azure (ML, OpenAI) , or GCP (Vertex AI, BigQuery, LangChain) .\nDefine and implement MLOps and LLMOps frameworks for versioning, retraining, CI/CD, and production observability.\nEnsure adherence to Responsible AI principles , including explainability, bias mitigation, auditability, and regulatory compliance\nEngineering & Integration\nWork closely with data engineering teams to acquire, transform, and pipeline data from core banking systems, CRMs, claims systems, and real-time feeds.\nDesign architecture for data lakes, feature stores, and vector databases supporting AI and GenAI use cases.\nEnable seamless integration of AI capabilities into enterprise workflows, customer platforms, and decision engines via APIs and microservices.\nRequired Skills & Experience:\n15+ years of experience in AI/ML, data engineering, and cloud architecture.\nMinimum of 10 end-to-end AI/ML project implementations from use case discovery through to productionization.\nProven expertise in: (Any One)\nAI/ML frameworks : scikit-learn, XGBoost, TensorFlow, PyTorch\nGenAI/LLM platforms : OpenAI, Cohere, Mistral, LangChain, Hugging Face, vector DBs (Pinecone, FAISS, Chroma)\nCloud platforms : AWS, Azure, GCP - including AI/ML & GenAI native services\nMLOps/LLMOps tools : MLflow, Kubeflow, SageMaker Pipelines, Vertex AI Pipelines\nStrong experience with data security, governance, model risk management , and AI compliance frameworks relevant to BFSI.\nAbility to lead large cross-functional teams and engage both technical teams and senior stakeholders.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Solution architecture', 'Architect', 'Bfsi', 'data security', 'Consulting', 'Machine learning', 'Risk management', 'Operations', 'Monitoring', 'Core banking']",2025-06-14 05:18:51
Data Scientist - Python / Machine Learning,Blueberry Unicorn Services,6 - 11 years,Not Disclosed,"['Hyderabad', 'Bengaluru']","Working Hours : 2PM to 11PM IST\n\nMid-Level ML Engineers / Data Scientist Role : (4-5 years of experience )\n\n- Experience processing, filtering, and presenting large quantities (100K to Millions of rows) of data using Pandas and PySpark\n\n- Experience with statistical analysis, data modeling, machine learning, optimizations, regression modeling and forecasting, time series analysis, data mining, and demand modeling.\n\n- Experience applying various machine learning techniques and understanding the key parameters that affect their performance.\n\n- Experience with Predictive analytics (e.g., forecasting, time-series, neural networks) and Prescriptive analytics (e.g., stochastic optimization, bandits, reinforcement learning).\n\n- Experience with Python and Python packages like NumPy, Pandas and deep learning frameworks like TensorFlow, Pytorch and Keras\n\n- Experience in Big Data ecosystem with frameworks like Spark, PySpark , Unstructured DBs like Elasticsearch and MongoDB\n\n- Proficiency with TABLEAU or other web-based interfaces to create graphic-rich customizable plots, charts data maps etc.\n\n- Able to write SQL scripts for analysis and reporting (Redshift, SQL, MySQL).\n\n- Previous experience in ML, data scientist or optimization engineer role with a large technology company.\n\n- Experience in an operational environment developing, fast-prototyping, piloting, and launching analytic products.\n\n- Ability to develop experimental and analytic plans for data modeling processes, use of strong baselines, ability to accurately determine cause and effect relations.\n\n- Experience in creating data driven visualizations to describe an end-to-end system.\n\n- Excellent written and verbal communication skills. The role requires effective communication with colleagues from computer science, operations research, and business backgrounds.\n\n- Bachelors or Masters in Artificial Intelligence, Computer Science, Statistics, Applied Math, or a related field.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Machine Learning', 'Data Science', 'Data Scientist', 'Artificial Intelligence', 'Data Management', 'Big Data', 'Data Modeling', 'Spark', 'Numpy', 'Python', 'Predictive Analytics']",2025-06-14 05:18:53
S&C Global Network - AI - CG&S - Consultant Data Science,Accenture,4 - 8 years,Not Disclosed,['Bengaluru'],"Job Title: Industry & Function AI Decision Science Consultant S&C Global Network\n\n\n\nManagement Level: 9 Consultant\n\n\n\nLocation: Primary - Bengaluru, Secondary - Gurugram\n\n\n\nMust-Have\n\n\n\n\nSkills:\nData Science, AI, ML, Experience with cloud platforms such as AWS, Azure, or Google Cloud, Hands-on experience in programming languages like Python, R, PySpark, and SQL\n\n\n\nGood-to-Have\n\n\n\n\nSkills:\nDeep Learning Techniques (e.g. RNN, CNN), Visualization tools like Power BI and Tableau, Exposure to tools like ChatGPT, Llama 2, Hugging Face, etc.\n\n\n\nJob\n\n\nSummary:\n\nAs an Industry & Function AI Decision Science Consultant, you will leverage your expertise in data science and Consumer Goods domain knowledge to design and deliver AI-driven solutions. Your role will include strategic analysis, project delivery, solution development, and technical execution to empower businesses with actionable insights and enable automated and augmented decision-making.\n\n\n\n\nRoles & Responsibilities:\nConduct strategic analysis of the AI, analytics, and data maturity landscape for clients in the Consumer Goods domain\nLead data science engagements, manage delivery teams, and build innovative AI capabilities\nDevelop and implement advanced analytics solutions tailored to client requirements\nUtilize languages like Python, PySpark, R, and SQL for data wrangling and machine learning model development\nLeverage cloud technologies (Azure, AWS, GCP) to integrate and implement AI solutions\nTranslate complex data into compelling narratives for effective data storytelling\nMentor junior team members and contribute to thought leadership\n\n\n\n\n\nProfessional & Technical\n\n\n\n\nSkills:\n\nProficiency in Python, R, PySpark, and SQL\nStrong knowledge of traditional statistical methods, machine learning techniques, and deep learning\nHands-on experience in Consumer Goods & Services domain\nCloud integration skills with platforms like AWS, Azure, or Google Cloud\nExperience with optimization techniques (exact and evolutionary)\nCertifications like AWS Certified Data Analytics Specialty or Google Professional Data Engineer\nFamiliarity with visualization tools like Tableau and Power BI\nExposure to large language models (e.g., ChatGPT, Llama 2)\nFamiliarity with version control systems like Git.\n\n\n\n\n\nAdditional Information:\nThe ideal candidate will have a strong educational background in data science, computer science, or a related field, along with a proven track record of delivering impactful AI-driven solutions in the Consumer Goods industry.\n\n\n\n\nAbout Our Company | Accenture\n\nQualification\n\n\n\nExperience: Minimum 4-8 years of hands-on experience in data science with a focus on the Consumer Goods industry\n\n\n\n\nEducational Qualification:Bachelors or Masters degree in Statistics, Economics, Mathematics, Computer Science, or equivalent degree with Data Science specialization (from a premier institute)",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'artificial intelligence', 'deep learning', 'data science', 'ml', 'advanced analytics', 'cnn', 'data analytics', 'pyspark', 'microsoft azure', 'power bi', 'machine learning', 'sql', 'r', 'tableau', 'git', 'rnn', 'gcp', 'machine learning algorithms', 'aws', 'consumer goods', 'statistics']",2025-06-14 05:18:55
S&C Global Network - AI - CG&S - Consultant Data Science,Accenture,4 - 8 years,Not Disclosed,['Bengaluru'],"Job Title: Industry & Function AI Decision Science Consultant S&C Global Network\n\n\n\nManagement Level: 9 Consultant\n\n\n\nLocation: Primary - Bengaluru, Secondary - Gurugram\n\n\n\nMust-Have\n\n\n\n\nSkills:\nData Science, AI, ML, Experience with cloud platforms such as AWS, Azure, or Google Cloud, Hands-on experience in programming languages like Python, R, PySpark, and SQL\n\n\n\nGood-to-Have\n\n\n\n\nSkills:\nDeep Learning Techniques (e.g. RNN, CNN), Visualization tools like Power BI and Tableau, Exposure to tools like ChatGPT, Llama 2, Hugging Face, etc.\n\n\n\nJob\n\n\nSummary:\n\nAs an Industry & Function AI Decision Science Consultant, you will leverage your expertise in data science and Consumer Goods domain knowledge to design and deliver AI-driven solutions. Your role will include strategic analysis, project delivery, solution development, and technical execution to empower businesses with actionable insights and enable automated and augmented decision-making.\n\n\n\n\nRoles & Responsibilities:\nConduct strategic analysis of the AI, analytics, and data maturity landscape for clients in the Consumer Goods domain\nLead data science engagements, manage delivery teams, and build innovative AI capabilities\nDevelop and implement advanced analytics solutions tailored to client requirements\nUtilize languages like Python, PySpark, R, and SQL for data wrangling and machine learning model development\nLeverage cloud technologies (Azure, AWS, GCP) to integrate and implement AI solutions\nTranslate complex data into compelling narratives for effective data storytelling\nMentor junior team members and contribute to thought leadership\n\n\n\n\n\nProfessional & Technical\n\n\n\n\nSkills:\n\nProficiency in Python, R, PySpark, and SQL\nStrong knowledge of traditional statistical methods, machine learning techniques, and deep learning\nHands-on experience in Consumer Goods & Services domain\nCloud integration skills with platforms like AWS, Azure, or Google Cloud\nExperience with optimization techniques (exact and evolutionary)\nCertifications like AWS Certified Data Analytics Specialty or Google Professional Data Engineer\nFamiliarity with visualization tools like Tableau and Power BI\nExposure to large language models (e.g., ChatGPT, Llama 2)\nFamiliarity with version control systems like Git.\n\n\n\n\n\nAdditional Information:\nThe ideal candidate will have a strong educational background in data science, computer science, or a related field, along with a proven track record of delivering impactful AI-driven solutions in the Consumer Goods industry.\n\n\n\n\nAbout Our Company | Accenture\n\nQualification\n\n\n\nExperience: Minimum 4-8 years of hands-on experience in data science with a focus on the Consumer Goods industry\n\n\n\n\nEducational Qualification:Bachelors or Masters degree in Statistics, Economics, Mathematics, Computer Science, or equivalent degree with Data Science specialization (from a premier institute)",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'artificial intelligence', 'deep learning', 'data science', 'ml', 'advanced analytics', 'cnn', 'data analytics', 'pyspark', 'microsoft azure', 'power bi', 'machine learning', 'sql', 'r', 'tableau', 'git', 'rnn', 'gcp', 'machine learning algorithms', 'aws', 'consumer goods', 'statistics']",2025-06-14 05:18:58
Data Analysis - Japanese Specialist Data Analysis,Zensar,3 - 7 years,Not Disclosed,"['Kolkata', 'Mumbai', 'New Delhi', 'Hyderabad', 'Pune', 'Chennai', 'Bengaluru']","Zensar Technologies is looking for Data Analysis - Japanese Specialist Data Analysis - Japanese Specialist to join our dynamic team and embark on a rewarding career journey A Data Analyst is responsible for collecting, analyzing, and interpreting data to identify trends, patterns, and insights that can drive informed business decisions\n\nThey play a crucial role in helping organizations understand their data, derive actionable insights, and optimize processes\n\nHere is a general job description for a Data Analyst:Data Collection and Preparation: Collect and gather relevant data from various sources, such as databases, spreadsheets, and external systems\n\nClean, validate, and transform the data to ensure accuracy and consistency\n\nData Analysis and Interpretation: Apply statistical techniques, data mining methods, and visualization tools to analyze large datasets\n\nIdentify trends, patterns, and correlations within the data and generate insights to support decision-making\n\nReporting and Visualization: Create clear and concise reports, dashboards, and visual representations of data using data visualization tools, such as Tableau, Power BI, or Excel\n\nPresent findings to stakeholders in a compelling and understandable manner\n\nData Quality and Integrity: Ensure data integrity and accuracy by conducting data validation, resolving discrepancies, and monitoring data quality\n\nImplement measures to maintain data privacy, security, and compliance with regulatory requirements\n\nBusiness Needs Assessment: Collaborate with stakeholders to understand their data analysis requirements and translate them into actionable analytics projects\n\nIdentify key performance indicators (KPIs) and metrics to measure business performance and success\n\nData-Driven Decision Making: Assist in making data-driven decisions by providing insights and recommendations based on data analysis\n\nSupport strategic planning, operational improvements, and process optimizations based on data-driven insights\n\nData Modeling and Forecasting: Develop and maintain data models, predictive models, and forecasting models to anticipate trends, predict outcomes, and support future planning\n\nUtilize statistical software, programming languages, or machine learning techniques as necessary\n\nContinuous Improvement: Stay updated with the latest data analysis techniques, tools, and trends\n\nContinuously improve data analysis processes, methodologies, and automation to enhance efficiency and effectiveness\n\nand Communication: Work closely with cross-functional teams, such as business analysts, data engineers, and data scientists, to align data analysis efforts with organizational goals\n\nCommunicate findings, insights, and recommendations to non-technical stakeholders in a clear and understandable manner\n\nDocumentation and Knowledge Sharing: Document data analysis methodologies, processes, and findings for future reference\n\nShare knowledge and best practices with the team to promote a culture of learning and data-driven decision-making\n\nSkills and Qualifications:Strong analytical skills with the ability to manipulate and analyze complex datasets\n\nProficiency in data analysis tools such as SQL, Excel, Python, R, or similar tools\n\nExperience with data visualization tools such as Tableau, Power BI, or similar tools\n\nKnowledge of statistical analysis techniques and methodologies\n\nFamiliarity with data modeling, predictive modeling, and forecasting techniques\n\nUnderstanding of database concepts and query languages\n\nExcellent attention to detail and problem-solving abilities\n\nStrong communication and presentation skills\n\nAbility to work independently and collaborate in a team environment\n\nFamiliarity with data privacy, security, and regulatory compliance\n\nPrior experience in data analysis or a related field is preferred",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Design engineering', 'Data analysis', 'Technology consulting', 'Focus', 'Agile', 'Conceptualization', 'Management', 'Japanese']",2025-06-14 05:19:00
Big data Developer,Diverse Lynx,3 - 5 years,Not Disclosed,['Bengaluru'],"Total Yrs. of Experience 8+ Yrs Relevant Yrs. of experience 4+ Yrs Detailed JD (Roles and Responsibilities)\nGather operational Client on business processes and policies from multiple sources\nPrepare periodical and ad-hoc reports using operational data\nDevelop semantic core to align data with business processes\nSupport operations teams work streams for data processing, analysis and reporting\nAnalyse data and Create dashboards for the senior management\nDesign and implement optimal processes\nRegression testing of the releases\nMandatory skills\nBig Data : Spark, Hive, DataBricks\nLanguage : SQL, JAVA /Python\nBI Analytics: Power BI (DAX), Tableau , Dataiku\nOperating System : Unix\nExperience with Data Migration, Data Engineering, Data Analysis\nDesired/ Secondary skills\nBig Data : SCALA, HADOOP\nTools: DB Visualizer, JIRA, GIT, Bitbucket, Control-M\nStrong problem-solving skills and the ability to work independently and in a team environment.\nExcellent communication skills and the ability to work effectively with cross-functional teams.\nDomain eCommerce / Retail Max Vendor Rate in Per Day (Currency in relevance to work location) 11000 INR/Day Delivery Anchor for tracking the sourcing statistics, technical evaluation, interviews and feedback etc. Prem_dason@infosys.com Work Location given in ECMS ID Bangalore (Hyd, Pune, Trivandrum locations also ok)",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Unix', 'Data analysis', 'Data migration', 'Control-M', 'Data processing', 'Regression testing', 'Operations', 'Analytics', 'SQL', 'Python']",2025-06-14 05:19:03
Data Modeler,Accenture,15 - 20 years,Not Disclosed,['Bengaluru'],"Project Role :Data Modeler\n\n\n\n\n\nProject Role Description :Work with key business representatives, data owners, end users, application designers and data architects to model current and new data.\n\n\n\nMust have skills :Microsoft Power Business Intelligence (BI)\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Modeler, you will engage with key business representatives, data owners, end users, application designers, and data architects to model both current and new data. Your typical day will involve collaborating with various stakeholders to understand their data needs, analyzing existing data structures, and designing effective data models that support business objectives. You will also be responsible for ensuring that the data models are aligned with best practices and meet the requirements of the organization, facilitating seamless data integration and accessibility across different platforms.\nRoles & Responsibilities:- Expected to be an SME.- Collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Facilitate training sessions for junior team members to enhance their understanding of data modeling.- Continuously evaluate and improve data modeling processes to ensure efficiency and effectiveness.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Microsoft Power Business Intelligence (BI).- Strong understanding of data modeling concepts and best practices.- Experience with data integration techniques and tools.- Familiarity with database management systems and SQL.- Ability to communicate complex data concepts to non-technical stakeholders.\nAdditional Information:- The candidate should have minimum 5 years of experience in Microsoft Power Business Intelligence (BI).- This position is based at our Bengaluru office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['business intelligence', 'sql', 'database management', 'data modeling', 'data integration', '3d modeling', 'tekla structures', '3ds max', 'python', '3d modeler', 'oracle', 'texturing', 'bi', 'data warehousing', 'sme', 'photoshop', 'autocad', 'sql server', 'maya', 'plsql', 'modeler', 'etl', 'tekla', 'informatica']",2025-06-14 05:19:06
Data Architect - Adobe Experience Platform,Ekloud Inc,10 - 12 years,Not Disclosed,['New Delhi'],"About the Role :\n\nWe are seeking a highly experienced and results-driven AEP Data Architect to join our team. The ideal candidate will possess deep expertise in data architecture, transformation, and modeling across large, complex datasets, particularly within customer-centric domains such as CRM, marketing, and sales systems.\n\nThis role requires hands-on experience with Adobe Experience Platform (AEP), advanced ETL processes, and the ability to translate business needs into scalable data solutions.\n\nKey Responsibilities :\n\n- Design, develop, and implement robust data architecture solutions within the Adobe Experience Platform (AEP).\n\n- Transform and model large-scale datasets, ensuring they are customer-centric and optimized for business intelligence and marketing use cases.\n\n- Define and manage business requirements, process designs, and use cases in collaboration with stakeholders.\n\n- Develop and maintain data pipelines using industry-standard ETL tools (e.g., Informatica, Unifi).\n\n- Create and manage relational, dimensional, columnar, and big data models.\n\n- Write complex SQL or NoSQL queries for data extraction, transformation, and analysis.\n\n- Collaborate with business teams to deliver actionable insights using reporting tools like Tableau and Power BI.\n\n- Lead data-related discussions with cross-functional teams including Sales, Marketing, and Engineering.\n\n- Provide strategic guidance and support for data governance, data quality, and integration best practices.\n\n- Manage multiple projects simultaneously with a strong focus on delivery, accuracy, and customer satisfaction.\n\nRequired Qualifications :\n\n- 10+ years of experience in data transformation and ETL processes on large datasets.\n\n- 5+ years of hands-on data modeling experience across various paradigms (relational, dimensional, big data, etc.).\n\n- Proven expertise in writing complex SQL or NoSQL queries.\n\n- In-depth knowledge of advanced data warehousing concepts.\n\n- Strong understanding of customer-centric data domains including CRM, Call Center, Marketing, POS, and Offline data.\n\n- Proficiency in ETL tools such as Informatica or Unifi.\n\n- Experience with data visualization tools such as Tableau and Power BI.\n\n- Strong analytical and problem-solving skills with a detail-oriented mindset.\n\n- Excellent verbal and written communication skills; ability to interface with both technical and business teams.\n\n- Demonstrated ability to work independently, proactively, and in a customer-focused manner.\n\n- Bachelors or Masters degree in Computer Science, Information Systems, Data Science, or a related field.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Adobe Experience Platform', 'Data Engineering', 'Data Pipeline', 'Data Architect', 'Big Data', 'Informatica', 'Data Modeling', 'ETL', 'SQL']",2025-06-14 05:19:08
Data Scientist,Ford,1 - 8 years,Not Disclosed,['Chennai'],"The Global Data Insights and Analytics (GDI&A) department at Ford Motors Company is looking for qualified people who can develop scalable solutions to complex real-world problems using Machine Learning, Big Data, Statistics, Econometrics, and Optimization. The candidate should possess the ability to translate a business problem into an analytical problem, identify the relevant data sets needed for addressing the analytical problem, recommend, implement, and validate the best suited analytical algorithm(s), and generate/deliver insights to stakeholders. Candidates are expected to regularly refer to research papers and be at the cutting-edge with respect to algorithms, tools, and techniques. The role is that of an individual contributor; however, the candidate is expected to work in project teams of 2 to 3 people and interact with Business partners on regular basis.\n\nMasters degree in computer science, Operational research, Statistics, Applied mathematics, or in any other engineering discipline.\n\nProficient in querying and analyzing large datasets using BigQuery on GCP. Strong Python skills for data wrangling and automation.\n\n2+ years of hands-on experience in Python programming for data analysis, machine learning, and with libraries such as NumPy, Pandas, Matplotlib, Scikit-learn, TensorFlow, PyTorch, NLTK, spaCy, and Gensim.\n\n2+ years of experience with both supervised and unsupervised machine learning techniques.\n\n2+ years of experience with data analysis and visualization using Python packages such as Pandas, NumPy, Matplotlib, Seaborn, or data visualization tools like Dash or QlikSense.\n\n1+ years experience in SQL programming language and relational databases.\n\n\nUnderstand business requirements and analyze datasets to determine suitable approaches to meet analytic business needs and support data-driven decision-making by FCSD business team\n\nDesign and implement data analysis and ML models, hypotheses, algorithms and experiments to support data-driven decision-making\n\nApply various analytics techniques like data mining, predictive modeling, prescriptive modeling, math, statistics, advanced analytics, machine learning models and algorithms, etc. ; to analyze data and uncover meaningful patterns, relationships, and trends\n\nDesign efficient data loading, data augmentation and data analysis techniques to enhance the accuracy and robustness of data science and machine learning models, including scalable models suitable for automation\n\nResearch, study and stay updated in the domain of data science, machine learning, analytics tools and techniques etc. ; and continuously identify avenues for enhancing analysis efficiency, accuracy and robustness",Industry Type: Auto Components,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Automation', 'Data analysis', 'Analytical', 'Machine learning', 'Predictive modeling', 'data visualization', 'Data mining', 'SQL', 'Python']",2025-06-14 05:19:10
Data Product Owner,Capgemini,9 - 14 years,Not Disclosed,['Hyderabad'],"\n\nThe Product Owner III will be responsible for defining and prioritizing features and user stories, outlining acceptance criteria, and collaborating with cross-functional teams to ensure successful delivery of product increments. This role requires strong communication skills to effectively engage with stakeholders, gather requirements, and facilitate product demos.\n\nThe ideal candidate should have a deep understanding of agile methodologies, experience in the insurance sector, and possess the ability to translate complex needs into actionable tasks for the development team.\n\n Key Responsibilities: \nDefine and communicate the  vision, roadmap, and backlog  for data products.\nManages team backlog items and prioritizes based on business value.\nPartners with the business owner to understand needs, manage scope and add/eliminate user stories while contributing heavy influence to build an effective strategy.\nTranslate business requirements into scalable data product features.\nCollaborate with data engineers, analysts, and business stakeholders to prioritize and deliver impactful solutions.\nChampion  data governance , privacy, and compliance best practices.\nAct as the voice of the customer to ensure usability and adoption of data products.\nLead Agile ceremonies (e.g., backlog grooming, sprint planning, demos) and maintain a clear product backlog.\nMonitor data product performance and continuously identify areas for improvement.\nSupport the integration of AI/ML solutions and advanced analytics into product offerings.\n\n\n  \n\n Required Skills & Experience: \nProven experience as a Product Owner, ideally in data or analytics domains.\nStrong understanding of  data engineering ,  data architecture , and  cloud platforms  (AWS, Azure, GCP).\nFamiliarity with  SQL , data modeling, and modern data stack tools (e.g., Snowflake, dbt, Airflow).\nExcellent  stakeholder management  and communication skills across technical and non-technical teams.\nStrong  business acumen  and ability to align data products with strategic goals.\nExperience with  Agile/Scrum methodologies  and working in cross-functional teams.\nAbility to  translate data insights into compelling stories and recommendations .\n\n\n",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data architecture', 'business acumen', 'data engineering', 'stakeholder management', 'agile methodology', 'snowflake', 'advanced analytics', 'microsoft azure', 'cloud platforms', 'user stories', 'demo', 'sql', 'data modeling', 'gcp', 'sprint planning', 'scrum', 'product performance', 'agile', 'aws', 'ml']",2025-06-14 05:19:13
Data Scientist-Advanced Analytics,IBM,3 - 7 years,Not Disclosed,['Kochi'],"We are seeking a highly skilled Advanced Analytics Specialist to join our dynamic team. The successful candidate will be responsible for leveraging advanced analytics techniques to derive actionable insights, inform business decisions, and drive strategic initiatives. This role requires a deep understanding of data analysis, statistical modeling, machine learning, and data visualization.\nIn this role, you will be responsible for architecting and delivering AI solutions using cutting-edge technologies, with a strong focus on foundation models and large language models. You will work closely with customers, product managers, and development teams to understand business requirements and design custom AI solutions that address complex challenges. Experience with tools like Github Copilot, Amazon Code Whisperer etc. is desirable.\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\n\n\nPreferred technical and professional experience",,,,"['data analysis', 'machine learning', 'statistical modeling', 'data visualization', 'machine learning algorithms', 'advanced analytics', 'python', 'github', 'natural language processing', 'power bi', 'microsoft azure', 'sql', 'r', 'tableau', 'java', 'data science', 'spark', 'hadoop', 'aws']",2025-06-14 05:19:15
Associate Specialist Data Science,Merck Sharp & Dohme (MSD),1 - 3 years,Not Disclosed,['Pune'],"We are seeking candidates with prior experience in the healthcare analytics or consulting sectors, prior hands-on experience in Data Science (building end-to-end ML models). It is preferred that you have a good understanding of Physician and Patient-level data (PLD) from leading vendors such as IQVIA, Komodo, and Optum. Familiarity with HCP Analytics, PLD analytics, concepts like persistence, compliance, line of therapy, etc., or Segmentation & Targeting is highly desirable. You will be part of a dynamic team that collaborates with our partners across therapeutic areas. Furthermore, effective communication skills are crucial, as this role requires interfacing with executive and business stakeholders.\nWho you are:\nYou understand the foundations of statistics and machine learning and can work in high performance computing/cloud environments, with experience/knowledge in aspects across statistical analysis, machine learning, model development, data engineering, data visualization, and data interpretation\nYou are self-motivated, and have demonstrated abilities to think independently as a data scientist\nYou structure your data science approach according to the necessary task, while appropriately applying the correct level of model complexity to the problem at hand\nYou have an agile mindset of continuous learning and will focus on integrating enterprise value into team culture\nYou are kind, collaborative, and capable of seeking and giving candid feedback that effectively contributes to a more seamless day-to-day execution of tasks\nKey Responsibilities:\nUnderstand the business requirements and support the manager to translate those to analytical problem statements.\nImplement the solution steps through SQL/Python, appropriate ML techniques without rigorous handholding.\nFollow technical requirements (Datasets, business rules, technical architecture) and industry best practices in every task.\nCollaborate with cross-functional teams to design and implement solutions that meet business requirements.\nPresent the findings to US DS stakeholders in a clear and concise manner and address feedback.\nAdopt a continuous learning mindset, both technical and functional.\nDevelop deep expertise in therapeutic area, with clear focus on commercial aspects.\nMinimum Qualifications:\nBachelor s degree with at least 1-3 years industry experience\nStrong Python/R, SQL, Excel skills\nStrong foundations of statistics and machine learning\nPreferred Qualifications:\nAdvanced degree in STEM (MS, MBA, PhD)\n2-3 years experience in healthcare analytics and consulting\nFamiliarity with Physician and Patient-Level data (e.g., claims, electronic health records) and data from common healthcare data vendors (IQVIA, Optum, Komodo, etc.)\nExperience in HCP & Patient Level Data analytics (e.g., HCP Segmentation & targeting, Patient Cohorts, knowledge of Lines of Therapy, Persistency, Compliance, etc.)\nProficiency in Data Science Concepts, Microsoft Excel and PowerPoint, and familiarity with Dataiku\nCurrent Employees apply HERE\nCurrent Contingent Workers apply HERE\nSearch Firm Representatives Please Read Carefully\nEmployee Status:\nRegular .",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Relationship management', 'Claims', 'Data modeling', 'Pharma', 'Analytical', 'Consulting', 'Healthcare', 'healthcare analytics', 'Business intelligence', 'SQL']",2025-06-14 05:19:17
S&C GN - Data&AI - Life Sciences - Analyst,Accenture,2 - 7 years,Not Disclosed,['Gurugram'],"Management Level:Ind & Func AI Decision Science Analyst\n\n\n\n\nJob Location:Bangalore / Gurgaon\n\n\n\nMust-have\n\n\n\n\nSkills:\nLife Sciences/Pharma/Healthcare projects and delivering successful outcomes, commercial, clinical, Statistical Models/Machine Learning including Segmentation & predictive modeling, hypothesis testing, multivariate statistical analysis, time series techniques, and optimization.\n\n\n\nGood-to-have\n\n\n\n\nSkills:\nProficiency in Programming languages such as R, Python, SQL, Spark, AWS, Azure, or Google Cloud for deploying and scaling language models, Data Visualization tools like Tableau, Power BI\n\n\n\nExperience:Proven experience (2+ years) in working on Life Sciences/Pharma/Healthcare projects and delivering successful outcomes.\n\n\n\n\nEducational Qualification:Bachelors or Masters degree in Statistics, Data Science, Applied Mathematics, Business Analytics, Computer Science, Information Systems, or other Quantitative field.\n\n\n\nJob\n\n\nSummary\n\nWe are seeking an experienced and visionary - Accenture S&C Global Network - Data & AI practice help our clients grow their business in entirely new ways. Analytics enables our clients to achieve high performance through insights from data - insights that inform better decisions and strengthen customer relationships. From strategy to execution, Accenture works with organizations to develop analytic capabilities - from accessing and reporting on data to predictive modelling - to outperform the competition.\n\n\n\nKey Responsibilities\nSupport delivery of small to medium-sized teams to deliver consulting projects for global clients.\nAn opportunity to work on high-visibility projects with top Pharma clients around the globe.\nPersonalized training modules to develop your strategy & consulting acumen to grow your skills, industry knowledge, and capabilities.\nResponsibilities may include strategy, implementation, process design, and change management for specific modules.\nWork with the team or as an Individual contributor on the project assigned which includes a variety of skills to be utilized from Data Engineering to Data Science\nDevelop assets and methodologies, point-of-view, research, or white papers for use by the team and the larger community.\nWork on variety of projects in Data Modeling, Data Engineering, Data Visualization, Data Science etc.,\nAcquire new skills that have utility across industry groups.\n\n\n\n\n\nAdditional Information\nAbility to solve complex business problems and deliver client delight.\nStrong writing skills to build points of view on current industry trends.\nGood communication, interpersonal, and presentation skills\n\n\nAbout Our Company | Accenture (do not remove the hyperlink)\n\n\nQualification\n\n\n\nExperience:Proven experience (2+ years) in working on Life Sciences/Pharma/Healthcare projects and delivering successful outcomes.\n\n\n\n\nEducational Qualification:Bachelors or Masters degree in Statistics, Data Science, Applied Mathematics, Business Analytics, Computer Science, Information Systems, or other Quantitative field.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['pharmaceutical', 'python', 'sql', 'life sciences', 'aws', 'presentation skills', 'microsoft azure', 'power bi', 'time series', 'machine learning', 'artificial intelligence', 'tableau', 'r', 'data science', 'gcp', 'spark', 'predictive modeling', 'statistical modeling', 'data visualization', 'statistics']",2025-06-14 05:19:20
Sr SW Engineer - ML,Visa,2 - 7 years,Not Disclosed,['Bengaluru'],"This position is for a Senior Data Engineer with solid development experience who will focus on creating new capabilities for AI as a Service while maturing our code base and development processes. In this position, you are first a passionate and talented developer that can work in a dynamic environment as a member of Agile Scrum teams. Your strong technical leadership, problem-solving abilities, coding, testing and debugging skills is just a start. You must be dedicated to filling product backlog and delivering production-ready code. You must be willing to go beyond the routine and prepared to do a little bit of everything.\nEssential Functions\nCollaborate with project teams, data science teams and development teams to drive the technical roadmap and guide development and implementation of new data driven business solutions.\nDrive technical standard and best practices, and continuously improve AI Platform engineering scalability.\nArchitecture and design of AI Platform services including Machine Learning Engines, In Memory Computing Systems, Streaming Computing Systems, Distributed Data Systems and etc, in Golang, Java, and Python.\nCoordinate the implementation among development teams to ensure system performance, security, scalability and availability.\nCoaching and mentoring junior team members and evolving team talent pipeline.\n\n\nBasic Qualifications\n2+ years of relevant work experience and a Bachelors degree, OR 5+ years of relevant work experience\n\nPreferred Qualifications\n3 or more years of work experience with a Bachelor s Degree or more than 2 years of work experience with an Advanced Degree (e.g. Masters, MBA, JD, MD)",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Basic', 'data science', 'Agile scrum', 'Architecture', 'Coding', 'Debugging', 'Machine learning', 'Technical leadership', 'Business solutions', 'Python']",2025-06-14 05:19:22
S&C GN - Data&AI - Life Sciences - Consultant,Accenture,4 - 9 years,Not Disclosed,['Gurugram'],"Management Level:Ind&Func AI Decision Science Consultant\n\n\n\n\nJob Location:Bangalore / Gurgaon\n\n\n\nMust-have\n\n\n\n\nSkills:\nExcellent understanding of Pharma data sets commercial, clinical, Leverage ones hands on experience of working across one or more of these areas such as real-world evidence data, Statistical Models/Machine Learning including Segmentation & predictive modeling, hypothesis testing, multivariate statistical analysis, time series techniques, and optimization.\n\n\n\nGood-to-have\n\n\n\n\nSkills:\nProgramming languages such as R, Python, SQL, Spark, AWS, Azure, or Google Cloud for deploying and scaling language models, Data Visualization tools like Tableau, Power BI.\n\n\n\n\nJob\n\n\nSummary\n\nThis role involves driving strategic initiatives, managing business transformations, and leveraging industry expertise to create value-driven solutions. Provide strategic advisory services, conduct market research, and develop data-driven recommendations to enhance business performance.\n\n\n\nKey Responsibilities\nAn opportunity to work on high-visibility projects with top Pharma clients around the globe.\nPersonalized training modules to develop your strategy & consulting acumen to grow your skills, industry knowledge, and capabilities.\nProvide Subject matter expertise in various sub-segments of the LS industry.\nSupport delivery of small to medium-sized teams to deliver consulting projects for global clients.\nResponsibilities may include strategy, implementation, process design, and change management for specific modules.\nWork with the team or as an Individual contributor on the project assigned which includes a variety of skills to be utilized from Data Engineering to Data Science\nDevelop assets and methodologies, point-of-view, research, or white papers for use by the team and the larger community.\nAcquire new skills that have utility across industry groups.\nSupport strategies and operating models focused on some business units and assess likely competitive responses. Also, assess implementation readiness and points of greatest impact.\n\n\n\n\n\nAdditional Information\nProficient in Excel, MS Word, PowerPoint, etc.\nAbility to solve complex business problems and deliver client delight.\nStrong writing skills to build points of view on current industry trends.\nGood communication, interpersonal, and presentation skills\n\n\nAbout Our Company | Accenture (do not remove the hyperlink)\n\nQualification\n\n\n\nExperience:Proven experience (4+ years) in working on Life Sciences/Pharma/Healthcare projects and delivering successful outcomes.\n\n\n\n\nEducational Qualification:Bachelors or Masters degree in Statistics, Data Science, Applied Mathematics, Business Analytics, Computer Science, Information Systems, or other Quantitative field.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['pharmaceutical', 'python', 'sql', 'life sciences', 'aws', 'microsoft azure', 'power bi', 'time series', 'machine learning', 'data engineering', 'artificial intelligence', 'tableau', 'r', 'data science', 'gcp', 'spark', 'predictive modeling', 'statistical modeling', 'data visualization', 'statistics']",2025-06-14 05:19:25
Sr Manager of Software Engineering,JPMorgan Chase Bank,14 - 20 years,Not Disclosed,['Bengaluru'],"When you mentor and advise multiple technical teams and move financial technologies forward, it s a big challenge with big impact. You were made for this.\n\n\nAs a Senior Manager of Software Engineering at JPMorgan Chase within the Consumer and Community Banking Technology Team, you serve in a leadership role by providing technical coaching and advisory for multiple technical teams, as well as anticipate the needs and potential dependencies of other functions within the firm. As an expert in your field, your insights influence budget and technical considerations to advance operational efficiencies and functionalities.\n\nJob responsibilities\n\n\n\nProvide direction, oversight, and coaching for a team of entry-level to mid-level software engineers working on basic to moderately complex tasks.\n\nBe accountable for decisions affecting team resources, budget, tactical operations, and the execution and implementation of processes and procedures.\n\nLead the design, development, testing, and implementation of data visualization projects to support business objectives.\n\nCollaborate with data analysts, data scientists, and business stakeholders to understand data requirements and translate them into effective visual solutions.\n\nWork in an Agile development environment with team members, including Product Managers, UX Designers, QA Engineers, and other Software Engineers.\n\nValidate the technical feasibility of UI/UX designs and provide regular technical guidance to support business and technical teams, contractors, and vendors.\n\nDevelop secure, high-quality production code, review and debug code written by others, and drive decisions influencing product design, application functionality, and technical operations.\n\nServe as a subject matter expert in one or more areas of focus and actively contribute to the engineering community as an advocate of firmwide frameworks, tools, and practices of the Software Development Life Cycle.\n\nInfluence peers and project decision-makers to consider the use and application of leading-edge technologies.\n\nDevelop and maintain dashboards, reports, and interactive visualizations using tools such as Tableau, ensuring data accuracy and integrity by implementing best practices in data visualization and management.\n\nStay current with industry trends and emerging technologies in data visualization and analytics, communicate complex data insights clearly to various audiences, including senior management, and manage a team of data visualization associates, providing guidance and mentorship to junior team members.\n\n\n\nRequired qualifications, capabilities, and skills\n\n\n\nFormal training or certification in software engineering concepts and 5+ years of applied experience.\n\n5+ Years of experience as a Web/UI Lead Architect\n\nProficiency in Javascript, Typescript, HTML, CSS\n\nExpert knowledge in ReactJs, Redux, React hooks.\n\nStrong understanding of front-end coding and development technologies\n\nHands-on practical experience delivering system design, application development, testing, and operational stability\n\nAdvanced knowledge of software applications and technical processes with considerable in-depth knowledge in UI and Web Technologies\n\nAbility to tackle design and functionality problems independently with little to no oversight\n\nPractical cloud native experience\n\nExperience in Computer Science, Computer Engineering, Mathematics, or a related technical field\n\n\n\nPreferred qualifications, capabilities, and skills\n\n\n\nFull stack development with Node/. NET/Java\n\nFamiliarity with working in event driven environments\n\nA good understanding of cross-browser compatibility issues and their solutions along with Typescript\n\nExperience working with Databases and ability to write SQL queries along with experience with messaging platforms\n\nBachelor s degree in data science, Computer Science, Information Systems, Statistics, or a related field.\n\nProblem solver and solution oriented. Strong written and verbal communication skills. Jira and Agile practices\n\nExperience with big data technologies and machine learning is a plus.",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Front end', 'Coding', 'Javascript', 'Agile', 'System design', 'HTML', 'Application development', 'JIRA', 'Analytics']",2025-06-14 05:19:27
Hiring Senior Data Scientist/ Data Science Manager,Tredence,5 - 10 years,Not Disclosed,"['Kolkata', 'Pune', 'Bengaluru']","Job Description:\n\nGraduate degree in a quantitative field (CS, statistics, applied mathematics, machine learning, or related discipline)\n• Good programming skills in Python with strong working knowledge of Pythons numerical, data analysis, or AI frameworks such as NumPy, Pandas, Scikit-learn, etc.\n• Experience with LMs (Llama (1/2/3), T5, Falcon, Langchain or framework similar like Langchain)\n• Candidate must be aware of entire evolution history of NLP (Traditional Language Models to Modern Large Language Models), training data creation, training set-up and finetuning",,,,"['Data Science', 'Tensorflow', 'Pytorch', 'generative', 'python', 'Artificial Intelligence', 'Natural Language Processing', 'Machine Learning', 'Deep Learning']",2025-06-14 05:19:30
Data Architect,Accenture,15 - 20 years,Not Disclosed,['Bhubaneswar'],"Project Role :Data Architect\n\n\n\n\n\nProject Role Description :Define the data requirements and structure for the application. Model and design the application data structure, storage and integration.\n\n\n\nMust have skills :Data Architecture Principles\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n18 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Architect, you will define the data requirements and structure for the application. Your typical day involves modeling and designing the application data structure, storage, and integration, ensuring that the data architecture aligns with business objectives and supports efficient data management practices. You will collaborate with various stakeholders to gather requirements and translate them into effective data solutions, while also addressing any challenges that arise in the data architecture process.\nRoles & Responsibilities:- Expected to be a Subject Matter Expert with deep knowledge and experience.- Should have influencing and advisory skills.- Engage with multiple teams and responsible for team decisions.- Expected to provide solutions to problems that apply across multiple teams, and provide solutions to business area problems.- Facilitate workshops and discussions to gather data requirements and ensure alignment with business goals.- Develop and maintain documentation related to data architecture, including data models and integration processes.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Data Architecture Principles.- Strong understanding of data modeling techniques and best practices.- Experience with data integration tools and methodologies.- Knowledge of database management systems and data storage solutions.- Familiarity with data governance and data quality frameworks.\nAdditional Information:- The candidate should have minimum 18 years of experience in Data Architecture Principles.- This position is based at our Mumbai office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data architecture', 'database management', 'data architecture principles', 'data modeling', 'data integration', 'python', 'data management', 'data warehousing', 'power bi', 'sql server', 'sql', 'plsql', 'data quality', 'tableau', 'dwbi', 'data governance', 'etl', 'ssis', 'informatica']",2025-06-14 05:19:32
Data Platform Architect,Accenture,15 - 25 years,Not Disclosed,['Bhubaneswar'],"Project Role :Data Platform Architect\n\n\n\n\n\nProject Role Description :Architects the data platform blueprint and implements the design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\n\n\n\nMust have skills :Data Architecture Principles\n\n\n\n\nGood to have skills :Amazon Web Services (AWS), Teradata Vantage, Data GovernanceMinimum\n\n\n\n15 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Platform Architect, you will architect the data platform blueprint and implement the design, collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\nRoles & Responsibilities:- Expected to be a SME with deep knowledge and experience.- Should have Influencing and Advisory skills.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Expected to provide solutions to problems that apply across multiple teams.- Lead the design and implementation of the data platform architecture.- Collaborate with cross-functional teams to ensure data platform alignment with business objectives.- Provide technical guidance and mentorship to junior team members.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Data Architecture Principles.- Good To Have\n\n\n\n\nSkills:\nExperience with Amazon Web Services (AWS), Teradata Vantage, Data Governance.- Strong understanding of data architecture principles and best practices.- Experience in designing and implementing scalable data solutions.- Knowledge of cloud platforms and data governance frameworks.\nAdditional Information:- The candidate should have a minimum of 15 years of experience in Data Architecture Principles.- This position is based at our Pune office.- A 15 years full-time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['vantage', 'data architecture', 'data architecture principles', 'data governance', 'aws', 'mainframes', 'oracle', 'data warehousing', 'microsoft azure', 'dbms', 'sql server', 'sql', 'jcl', 'java', 'data modeling', 'cobol', 'gcp', 'platform architecture', 'hadoop', 'etl', 'big data', 'informatica', 'vsam']",2025-06-14 05:19:35
Senior Consultant - Data Science,Arcadis,8 - 13 years,Not Disclosed,['Bengaluru'],"Qualification & Experience:\nMinimum of 8 years of experience as a Data Scientist/Engineer with demonstrated expertise in data engineering and cloud computing technologies.\nTechnical Responsibilities\nExcellent proficiency in Python, with a strong focus on developing advanced skills.\nExtensive exposure to NLP and image processing concepts.\nProficient in version control systems like Git.\nIn-depth understanding of Azure deployments.\nExpertise in OCR, ML model training, and transfer learning.\nExperience working with unstructured data formats such as PDFs, DOCX, and images. O\nStrong familiarity with data science best practices and the ML lifecycle.\nStrong experience with data pipeline development, ETL processes, and data engineering tools such as Apache Airflow, PySpark, or Databricks.\nFamiliarity with cloud computing platforms like Azure, AWS, or GCP, including services like Azure Data Factory, S3, Lambda, and BigQuery.\nTool Exposure: Advanced understanding and hands-on experience with Git, Azure, Python, R programming and data engineering tools such as Snowflake, Databricks, or PySpark.\nData mining, cleaning and engineering: Leading the identification and merging of relevant data sources, ensuring data quality, and resolving data inconsistencies.\nCloud Solutions Architecture: Designing and deploying scalable data engineering workflows on cloud platforms such as Azure, AWS, or GCP.\nData Analysis: Executing complex analyses against business requirements using appropriate tools and technologies.\nSoftware Development: Leading the development of reusable, version-controlled code under minimal supervision.\nBig Data Processing: Developing solutions to handle large-scale data processing using tools like Hadoop, Spark, or Databricks.\nPrincipal Duties & Key Responsibilities:\nLeading data extraction from multiple sources, including PDFs, images, databases, and APIs.\nDriving optical character recognition (OCR) processes to digitize data from images.\nApplying advanced natural language processing (NLP) techniques to understand complex data.\nDeveloping and implementing highly accurate statistical models and data engineering pipelines to support critical business decisions and continuously monitor their performance.\nDesigning and managing scalable cloud-based data architectures using Azure, AWS, or GCP services.\nCollaborating closely with business domain experts to identify and drive key business value drivers.\nDocumenting model design choices, algorithm selection processes, and dependencies.\nEffectively collaborating in cross-functional teams within the CoE and across the organization.\nProactively seeking opportunities to contribute beyond assigned tasks.\nRequired Competencies:\nExceptional communication and interpersonal skills.\nProficiency in Microsoft Office 365 applications.\nAbility to work independently, demonstrate initiative, and provide strategic guidance.\nStrong networking, communication, and people skills.\nOutstanding organizational skills with the ability to work independently and as part of a team.\nExcellent technical writing skills.\nEffective problem-solving abilities.\nFlexibility and adaptability to work flexible hours as required.\nKey competencies / Values:\nClient Focus: Tailoring skills and understanding client needs to deliver exceptional results.\nExcellence: Striving for excellence defined by clients, delivering high-quality work.\nTrust: Building and retaining trust with clients, colleagues, and partners.\nTeamwork: Collaborating effectively to achieve collective success.\nResponsibility: Taking ownership of performance and safety, ensuring accountability.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Big Data Processing', 'Software Development', 'Data cleaning', 'Data engineering', 'Data mining', 'cloud computing', 'OCR']",2025-06-14 05:19:38
S&C Global Network - AI - Life Sciences -Data Science Consultant,Accenture,4 - 9 years,Not Disclosed,['Gurugram'],"Job Title -\n\n\n\nS&C Global Network - AI - Healthcare Analytics - Consultant\n\n\n\nManagement Level:\n\n\n\n9-Team Lead/Consultant\n\n\n\nLocation:\n\n\n\nBangalore/Gurgaon\n\n\n\nMust-have skills:R,Phython,SQL,Spark,Tableau ,Power BI\n\n\n\n\nGood to have skills:Ability to leverage design thinking, business process optimization, and stakeholder management skills.\n\n\n\nJob\n\n\nSummary:\n\nThis role involves driving strategic initiatives, managing business transformations, and leveraging industry expertise to create value-driven solutions.\n\n\n\n\nRoles & Responsibilities:\n\nProvide strategic advisory services, conduct market research, and develop data-driven recommendations to enhance business performance.\n\n\n\nWHATS IN IT FOR YOU\nAn opportunity to work on high-visibility projects with top Pharma clients around the globe.\nPotential to Co-create with leaders in strategy, industry experts, enterprise function practitioners, and business intelligence professionals to shape and recommend innovative solutions that leverage emerging technologies.\nAbility to embed responsible business into everythingfrom how you service your clients to how you operate as a responsible professional.\nPersonalized training modules to develop your strategy & consulting acumen to grow your skills, industry knowledge, and capabilities.\nOpportunity to thrive in a culture that is committed to accelerating equality for all. Engage in boundaryless collaboration across the entire organization.\n\n\n\n\nWhat you would do in this role\nSupport delivery of small to medium-sized teams to deliver consulting projects for global clients.\nResponsibilities may include strategy, implementation, process design, and change management for specific modules.\nWork with the team or as an Individual contributor on the project assigned which includes a variety of skills to be utilized from Data Engineering to Data Science\nProvide Subject matter expertise in various sub-segments of the LS industry.\nDevelop assets and methodologies, point-of-view, research, or white papers for use by the team and the larger community.\nAcquire new skills that have utility across industry groups.\nSupport strategies and operating models focused on some business units and assess likely competitive responses. Also, assess implementation readiness and points of greatest impact.\nCo-lead proposals, and business development efforts and coordinate with other colleagues to create consensus-driven deliverables.\nExecute a transformational change plan aligned with the clients business strategy and context for change. Engage stakeholders in the change journey and build commitment to change.\nMake presentations wherever required to a known audience or client on functional aspects of his or her domain.\n\n\n\nWho are we looking for\nBachelors or Masters degree in Statistics, Data Science, Applied Mathematics, Business Analytics, Computer Science, Information Systems, or other Quantitative field.\nProven experience (4+ years) in working on Life Sciences/Pharma/Healthcare projects and delivering successful outcomes.\nExcellent understanding of Pharma data sets commercial, clinical, RWE (Real World Evidence) & EMR (Electronic medical records)\nLeverage ones hands on experience of working across one or more of these areas such as real-world evidence data, R&D clinical data, digital marketing data.\nHands-on experience with handling Datasets like Komodo, RAVE, IQVIA, Truven, Optum etc.\nHands-on experience in building and deployment of Statistical Models/Machine Learning including Segmentation & predictive modeling, hypothesis testing, multivariate statistical analysis, time series techniques, and optimization.\nProficiency in Programming languages such as R, Python, SQL, Spark, etc.\nAbility to work with large data sets and present findings/insights to key stakeholders; Data management using databases like SQL.\nExperience with any of the cloud platforms like AWS, Azure, or Google Cloud for deploying and scaling language models.\nExperience with any of the Data Visualization tools like Tableau, Power BI, Qlikview, Spotfire is good to have.\nExcellent analytical and problem-solving skills, with a data-driven mindset.\nProficient in Excel, MS Word, PowerPoint, etc.\nAbility to solve complex business problems and deliver client delight.\nStrong writing skills to build points of view on current industry trends.\nGood communication, interpersonal, and presentation skills\n\n\n\n\n\nProfessional & Technical\n\n\n\n\nSkills:\n\n\n- Relevant experience in the required domain.\n\n- Strong analytical, problem-solving, and communication skills.\n\n- Ability to work in a fast-paced, dynamic environment.\n\n\n\n\nAdditional Information:\n\n- Opportunity to work on innovative projects.\n\n- Career growth and leadership exposure.\n\n\n\n\n\nAbout Our Company | Accenture\nQualification\n\n\n\nExperience:\n\n\n\n4-8 Years\n\n\n\n\nEducational Qualification:\n\n\n\nBachelors or Masters degree in Statistics, Data Science, Applied Mathematics, Business Analytics, Computer Science, Information Systems, or other Quantitative field.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'sql', 'tableau', 'r', 'spark', 'spotfire', 'power bi', 'microsoft azure', 'time series', 'emr', 'machine learning', 'data engineering', 'artificial intelligence', 'qlikview', 'data science', 'gcp', 'predictive modeling', 'segmentation', 'life sciences', 'data visualization', 'aws', 'statistics']",2025-06-14 05:19:40
Senior Murex Front Office & Risk Support Engineer,Synechron,5 - 10 years,Not Disclosed,"['Pune', 'Bengaluru', 'Hinjewadi']","Job Summary\nSynechron is seeking an experienced Murex FO & Risk Support Specialist to join our dynamic team. This role is central to maintaining and supporting Murex platform functionalities related to front-office operations and risk management, with a focus on production support.\nThe individual will collaborate closely with business users and IT teams to resolve complex issues, optimize configurations, and ensure the stability of critical trading and risk systems. By providing expert-level support, this role contributes directly to the organizations ability to manage market and credit risks effectively, deliver timely business insights, and uphold operational resilience.\nSoftware Requirements\nOverall Responsibilities\nTechnical Skills (By Category)\nExperience Requirements\nDay-to-Day Activities\nQualifications\nProfessional Competencies",,,,"['Murex support', 'risk management', 'Risk Support', 'credit risk', 'market risk', 'pricing']",2025-06-14 05:19:43
Senior Consultant Data Science,Arcadis,8 - 13 years,Not Disclosed,['Bengaluru'],"Qualification & Experience:\nMinimum of 8 years of experience as a Data Scientist/Engineer with demonstrated expertise in data engineering and cloud computing technologies.\nTechnical Responsibilities\nExcellent proficiency in Python, with a strong focus on developing advanced skills.\nExtensive exposure to NLP and image processing concepts.\nProficient in version control systems like Git.\nIn-depth understanding of Azure deployments.\nExpertise in OCR, ML model training, and transfer learning.\nExperience working with unstructured data formats such as PDFs, DOCX, and images. O\nStrong familiarity with data science best practices and the ML lifecycle.\nStrong experience with data pipeline development, ETL processes, and data engineering tools such as Apache Airflow, PySpark, or Databricks.\nFamiliarity with cloud computing platforms like Azure, AWS, or GCP, including services like Azure Data Factory, S3, Lambda, and BigQuery.\nTool Exposure: Advanced understanding and hands-on experience with Git, Azure, Python, R programming and data engineering tools such as Snowflake, Databricks, or PySpark.\nData mining, cleaning and engineering: Leading the identification and merging of relevant data sources, ensuring data quality, and resolving data inconsistencies.\nCloud Solutions Architecture: Designing and deploying scalable data engineering workflows on cloud platforms such as Azure, AWS, or GCP.\nData Analysis: Executing complex analyses against business requirements using appropriate tools and technologies.\nSoftware Development: Leading the development of reusable, version-controlled code under minimal supervision.\nBig Data Processing: Developing solutions to handle large-scale data processing using tools like Hadoop, Spark, or Databricks.\nPrincipal Duties & Key Responsibilities:\nLeading data extraction from multiple sources, including PDFs, images, databases, and APIs.\nDriving optical character recognition (OCR) processes to digitize data from images.\nApplying advanced natural language processing (NLP) techniques to understand complex data.\nDeveloping and implementing highly accurate statistical models and data engineering pipelines to support critical business decisions and continuously monitor their performance.\nDesigning and managing scalable cloud-based data architectures using Azure, AWS, or GCP services.\nCollaborating closely with business domain experts to identify and drive key business value drivers.\nDocumenting model design choices, algorithm selection processes, and dependencies.\nEffectively collaborating in cross-functional teams within the CoE and across the organization.\nProactively seeking opportunities to contribute beyond assigned tasks.\nRequired Competencies:\nExceptional communication and interpersonal skills.\nProficiency in Microsoft Office 365 applications.\nAbility to work independently, demonstrate initiative, and provide strategic guidance.\nStrong networking, communication, and people skills.\nOutstanding organizational skills with the ability to work independently and as part of a team.\nExcellent technical writing skills.\nEffective problem-solving abilities.\nFlexibility and adaptability to work flexible hours as required.\nKey competencies / Values:\nClient Focus: Tailoring skills and understanding client needs to deliver exceptional results.\nExcellence: Striving for excellence defined by clients, delivering high-quality work.\nTrust: Building and retaining trust with clients, colleagues, and partners.\nTeamwork: Collaborating effectively to achieve collective success.\nResponsibility: Taking ownership of performance and safety, ensuring accountability.\nPeople: Creating an inclusive environment that fosters individual growth and development.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Azure Data Factory', 'S3', 'Apache Airflow', 'Azure', 'BigQuery', 'Git', 'PySpark', 'Lambda']",2025-06-14 05:19:45
Data Modeler,Accenture,12 - 15 years,Not Disclosed,['Kolkata'],"Project Role :Data Modeler\n\n\n\n\n\nProject Role Description :Work with key business representatives, data owners, end users, application designers and data architects to model current and new data.\n\n\n\nMust have skills :Data Building Tool\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n12 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Modeler, you will engage with key business representatives, data owners, end users, application designers, and data architects to model both current and new data. Your typical day will involve collaborating with various stakeholders to understand their data needs, analyzing existing data structures, and designing effective data models that support business objectives. You will also be responsible for ensuring that the data models are aligned with best practices and organizational standards, facilitating smooth data integration and accessibility across different systems. This role requires a proactive approach to problem-solving and a commitment to delivering high-quality data solutions that enhance decision-making processes within the organization.\nRoles & Responsibilities:- Expected to be an SME.- Collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Expected to provide solutions to problems that apply across multiple teams.- Facilitate workshops and meetings to gather requirements and feedback from stakeholders.- Develop and maintain comprehensive documentation of data models and architecture.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Data Building Tool.- Strong understanding of data modeling techniques and methodologies.- Experience with data integration and ETL processes.- Familiarity with database management systems and SQL.- Ability to translate business requirements into technical specifications.\nAdditional Information:- The candidate should have minimum 12 years of experience in Data Building Tool.- This position is based at our Kolkata office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sql', 'database management', 'data modeling', 'etl', 'data integration', 'python', 'oracle', 'data analysis', 'data warehousing', 'sme', 'data architecture', 'business intelligence', 'sql server', 'plsql', 'unix shell scripting', 'etl tool', 'modeler', 'informatica', 'unix', 'etl process']",2025-06-14 05:19:48
Senior Consultant Data Science,Arcadis,8 - 13 years,Not Disclosed,['Bengaluru'],"Qualification & Experience:\nMinimum of 8 years of experience as a Data Scientist/Engineer with demonstrated expertise in data engineering and cloud computing technologies.\nTechnical Responsibilities\nExcellent proficiency in Python, with a strong focus on developing advanced skills.\nExtensive exposure to NLP and image processing concepts.\nProficient in version control systems like Git.\nIn-depth understanding of Azure deployments.\nExpertise in OCR, ML model training, and transfer learning.\nExperience working with unstructured data formats such as PDFs, DOCX, and images. O\nStrong familiarity with data science best practices and the ML lifecycle.\nStrong experience with data pipeline development, ETL processes, and data engineering tools such as Apache Airflow, PySpark, or Databricks.\nFamiliarity with cloud computing platforms like Azure, AWS, or GCP, including services like Azure Data Factory, S3, Lambda, and BigQuery.\nTool Exposure: Advanced understanding and hands-on experience with Git, Azure, Python, R programming and data engineering tools such as Snowflake, Databricks, or PySpark.\nData mining, cleaning and engineering: Leading the identification and merging of relevant data sources, ensuring data quality, and resolving data inconsistencies.\nCloud Solutions Architecture: Designing and deploying scalable data engineering workflows on cloud platforms such as Azure, AWS, or GCP.\nData Analysis: Executing complex analyses against business requirements using appropriate tools and technologies.\nSoftware Development: Leading the development of reusable, version-controlled code under minimal supervision.\nBig Data Processing: Developing solutions to handle large-scale data processing using tools like Hadoop, Spark, or Databricks.\nPrincipal Duties & Key Responsibilities:\nLeading data extraction from multiple sources, including PDFs, images, databases, and APIs.\nDriving optical character recognition (OCR) processes to digitize data from images.\nApplying advanced natural language processing (NLP) techniques to understand complex data.\nDeveloping and implementing highly accurate statistical models and data engineering pipelines to support critical business decisions and continuously monitor their performance.\nDesigning and managing scalable cloud-based data architectures using Azure, AWS, or GCP services.\nCollaborating closely with business domain experts to identify and drive key business value drivers.\nDocumenting model design choices, algorithm selection processes, and dependencies.\nEffectively collaborating in cross-functional teams within the CoE and across the organization.\nProactively seeking opportunities to contribute beyond assigned tasks.\nRequired Competencies:\nExceptional communication and interpersonal skills.\nProficiency in Microsoft Office 365 applications.\nAbility to work independently, demonstrate initiative, and provide strategic guidance.\nStrong networking, communication, and people skills.\nOutstanding organizational skills with the ability to work independently and as part of a team.\nExcellent technical writing skills.\nEffective problem-solving abilities.\nFlexibility and adaptability to work flexible hours as required.\nKey competencies / Values:\nClient Focus: Tailoring skills and understanding client needs to deliver exceptional results.\nExcellence: Striving for excellence defined by clients, delivering high-quality work.\nTrust: Building and retaining trust with clients, colleagues, and partners.\nTeamwork: Collaborating effectively to achieve collective success.\nResponsibility: Taking ownership of performance and safety, ensuring accountability.\nPeople: Creating an inclusive environment that fosters individual growth and development.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Azure Data Factory', 'S3', 'Big Query', 'Azure', 'GCP', 'Hadoop', 'Spark', 'Databricks', 'ETL', 'Lambda', 'AWS']",2025-06-14 05:19:50
Senior Consultant - Data Analytics,Arcadis,5 - 10 years,Not Disclosed,['Bengaluru'],"Qualifications and Experience:\nBS/MS or BCA/MCA or bachelors/masters degree in Math, Statistics, Computer Science, Engineering, or another technical field.\nExperience required: 5+ years.\nExperience in Data engineering with Python\\R\nExperience in SQL, MS-SQL Server, or other relational databases.\nAzure Cloud Service experience\\AWS\\Google Could Service (Optional)\nAPI automated Data extraction pipeline\nExperience in developing and maintaining integrated visualization reports in PowerBI (Optional)\nExperience with software deployment project lifecycle phases - requirements gathering, planning, testing, delivery, enhancements, support.\nExperience in Project Management (Preferred)\nExceptional communication skills and fluency in English (professional level).\nKey Skills / Attributes\nExceptional analytical and problem-solving skills, strong attention to detail, organization skills, and work ethic.\nSelf-motivated and team-oriented, with the ability to work successfully both independently and within a team.\nAbility to balance and address new challenges as they arise and an eagerness to take ownership of tasks.\nDrive to succeed and grow a career in the Project/Program Management\nPrincipal Duties & Key Responsibilities\nKey Duties & Responsibilities\nThis role will require to work independently or in team to solve data problems with unstructured data.\nCollaborate with other team members and other disciplines to deliver project requirements.\nWork independently to complete allocated activities to meet timeframe and quality objectives and meeting or exceeding client expectations.\nDevelop effective materials for clients, making sure that their messages are clearly conveyed through the appropriate channel, using the language that is suitable for the intended audience and readers, and would induce the desired response.\nActively contribute to Arcadis Global Communities of Practice relevant to Project Management, Data Visualization, and Power Platform, through knowledge shares and case study presentations.\nActively contribute to the Digital Advisory community of practice, through development of integrated solutions that embed GEC capabilities into core advisory business.\nData Engineering, Management, and Visualisation\nExperience in manipulating, transforming, and analysing data sets that are raw, large, and complex.\nDemonstrates ability to plan, gather, analyse, and document user and business information.\nIncorporates, integrates, and interfaces technical knowledge with business / systems requirements.\nUnderstanding of all aspects of an implementation project including, but not limited to planning, analysis and design, configuration, development, conversions, system testing, cutover and production support.\nProduce written deliverables for requirement specifications and support documentation: process mapping, meeting minutes, glossaries, data dictionary, technical design, system testing and implementation activities.\nCollect and organize data, data warehouse reports, spreadsheets, and databases for analytical reporting.\nStrong on database concepts, data modelling, stored procedures, complex query writing, performance optimization of SQL queries.\nExperience in creating automated data extraction pipeline from various sources like API, Databases in various formats.\nA problem solving, solution driven mindset with the ability to innovate within the constraints of a project time/cost/quality.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Analytics', 'project management', 'python', 'sql queries', 'performance tuning', 'power bi', 'triggers', 'data engineering', 'sql server', 'azure cloud', 'stored procedures', 'sql', 'plsql', 'data extraction', 'data modeling', 'database creation', 'ssrs', 'data visualization', 'ssis', 'statistics']",2025-06-14 05:19:53
Data Modeler,Accenture,15 - 20 years,Not Disclosed,['Mumbai'],"Project Role :Data Modeler\n\n\n\n\n\nProject Role Description :Work with key business representatives, data owners, end users, application designers and data architects to model current and new data.\n\n\n\nMust have skills :Data Building Tool\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Modeler, you will engage with key business representatives, data owners, end users, application designers, and data architects to model both current and new data. Your typical day will involve collaborating with various stakeholders to understand their data needs, analyzing existing data structures, and designing effective data models that support business objectives. You will also be responsible for ensuring that the data models are aligned with best practices and organizational standards, facilitating smooth data integration and accessibility across different systems. This role requires a proactive approach to problem-solving and a commitment to delivering high-quality data solutions that enhance decision-making processes within the organization.\nRoles & Responsibilities:- Expected to be an SME.- Collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Facilitate training sessions and workshops to enhance team capabilities.- Continuously evaluate and improve data modeling processes to ensure efficiency.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Data Building Tool.- Strong understanding of data modeling techniques and methodologies.- Experience with data integration and ETL processes.- Familiarity with database management systems and SQL.- Ability to translate business requirements into technical specifications.\nAdditional Information:- The candidate should have minimum 7.5 years of experience in Data Building Tool.- This position is based in Mumbai.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['sql', 'database management', 'data modeling', 'etl', 'data integration', 'python', 'oracle', 'data analysis', 'data warehousing', 'sme', 'data architecture', 'business intelligence', 'sql server', 'plsql', 'unix shell scripting', 'etl tool', 'modeler', 'informatica', 'unix', 'etl process']",2025-06-14 05:19:55
Data Modeler,Accenture,15 - 20 years,Not Disclosed,['Mumbai'],"Project Role :Data Modeler\n\n\n\n\n\nProject Role Description :Work with key business representatives, data owners, end users, application designers and data architects to model current and new data.\n\n\n\nMust have skills :Data Modeling Techniques and Methodologies\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\nProject Role :Data Architect & Modeler\n\nProject Role Description Data Model, Design, build and lead the complex ETL data integration pipelines to meet business process and application requirements. Management Level :9Work Experience :6+ yearsWork Location :AnyMust have skills :Data Architecture Principles\nGood to have skills :Data Modeling, Data Architect, Informatica PowerCenter, Informatica Data Quality, SAP BusinessObjects Data Services, SQL, PL/SQL, SAP HANA DB, MS Azure, Python, ErWin, SAP Power Designer Job :Data Architect, Modeler, and data Integration LeadKey Responsibilities:1) Working on building Data models, Forward and Reverse Engineering.2) Working on Data and design analysis and working with data analysts team on data model design.3) Working on presentations on design, end to end flow and data models.4) Work on new and existing data models using Power designer tools and other designing tools like Visio5) Work with functional SMEs, BAs to review requirements, mapping documents\nTechnical Experience:1) Should have good understanding of ETL design concepts like CDC, SCD, Transpose/ pivot, Updates, Validation2) Should have strong understanding of SQL concepts, Data warehouse concepts and can easily understand data technically and functionally.3) Good understanding of various file formats like xml, delimited, fixed width etc.4) Understand the concepts of data quality, data cleansing, data profiling5) Good to have Python and other new data technologies and cloud exposure.6) Having Insurance background is a plus.\nEducational Qualification :15 years of fulltime education with BE/B Tech or equivalent\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Data Modeling Techniques and Methodologies.- Strong understanding of relational and non-relational database design principles.- Experience with data integration and ETL processes.- Familiarity with data governance and data quality frameworks.- Ability to translate business requirements into technical specifications.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sql', 'data architecture principles', 'data modeling', 'data warehousing concepts', 'sql joins', 'python', 'ms azure', 'sap', 'informatica powercenter', 'informatica data quality', 'data warehousing', 'erwin', 'data architecture', 'plsql', 'modeler', 'hana db', 'etl', 'sap hana', 'data integration']",2025-06-14 05:19:58
Data Cleansing & Integration Specialist,European Company,5 - 10 years,10-20 Lacs P.A.,"['Chennai', 'Bengaluru', 'Mumbai (All Areas)']","Data Cleansing & Integration Project Delivery:\n\nExecute high visibility data programs as assigned by the Data Cleansing Manager.\nUtilize SAP data load solutions such as SAP Migration Cockpit and LSMW for data loading and template creation.\nFDO Data Change Management Methodology:\n\nAssist in defining data cleansing approaches using Mass Change functionality.\nDevelop and prepare data cleansing strategies.\nData Cleansing & Integration Technical Guidance:\n\nUnderstand SAP landscape and data flow to underlying/consumed systems to prevent data synchronization issues.\nData Quality:\n\nCollaborate with the Data Quality (DQ) team to define DQ rules and enhance visibility of existing data quality.\nData Governance:\n\nWork with the Data Governance (DG) team to ensure proper governance before implementing system changes.\nConduct necessary data load testing in test systems.\nData Sourcing:\n\nMaintain and update the data catalogue/data dictionary, creating a defined list of data sources indicating the best versions (golden copies).\nData Ingestion:\n\nCollaborate with DG and project teams on data harmonization by integrating data from multiple sources.\nDevelop sustainable integration routines and methods.\nQualifications:\n\nExperience: Minimum of 6 years in data-related disciplines such as data management, quality, and cleansing.\nTechnical Skills:\nProven experience in delivering data initiatives (cleansing, integration, migrations) using established technical data change methodologies.\nProficiency in handling large data sets with tools like Microsoft Excel and Power BI.\nExperience with SAP native migration and cleansing tools such as SAP Migration Cockpit, LSMW, and MASS.\nKnowledge of Master Data Management in SAP MDG, SAP ECC, and associated data structures.\nCollaboration: Ability to work effectively with internal cross-functional teams.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Cleansing', 'Data Ingestion', 'SAP', 'Data Enrichment', 'Taxonomy', 'Metadata', 'MRO', 'Data Integration', 'FDO', 'LSMW', 'Data Migration', 'Data Profiling', 'Data Classification', 'Data Processing', 'Data Governance', 'ECC', 'ISO', 'Unspsc', 'eClass', 'Data Quality', 'Cockpit', 'mdm', 'Master Data Management', 'Data Services', 'Cleansing']",2025-06-14 05:20:00
Data Scientist,Mastercard,4 - 8 years,Not Disclosed,['Gurugram'],"As consumer preference for digital payments continues to grow, ensuring a seamless and secure consumer experience is top of mind. Optimization Soltions team focuses on tracking of digital performance across all products and regions, understanding the factors influencing performance and the broader industry landscape. This includes delivering data-driven insights and business recommendations, engaging directly with key external stakeholders on implementing optimization solutions (new and existing), and partnering across the organization to drive alignment and ensure action is taken.\n\nThe Role:\n\nWork closely with global optimization solutions team to architect, develop, and maintain advanced reporting and data visualization capabilities on large volumes of data to support data insights and analytical needs across products, markets, and services\nThe candidate for this position will focus on Building solutions using Machine Learning and creating actionable insights to support product optimization and sales enablement.\nPrototype new algorithms, experiment, evaluate and deliver actionable insights.\nDrive the evolution of products with an impact focused on data science and engineering.\nDesigning machine learning systems and self-running artificial intelligence (AI) software to automate predictive models.\nPerform data ingestion, aggregation, and processing on high volume and high dimensionality data to drive and enable data unification and produce relevant insights.\nContinuously innovate and determine new approaches, tools, techniques & technologies to solve business problems and generate business insights & recommendations.\nApply knowledge of metrics, measurements, and benchmarking to complex and demanding solutions.\n\nAll about You\nA superior academic record at a leading university in Computer Science, Data Science, Technology, mathematics, statistics, or a related field or equivalent work experience\nExperience in data management, data mining, data analytics, data reporting, data product development and quantitative analysis\nStrong analytical skills with track record of translating data into compelling insights\nPrior experience working in a product development role.\nknowledge of ML frameworks, libraries, data structures, data modeling, and software architecture.\nproficiency in using Python/Spark, Hadoop platforms & tools (Hive, Impala, Airflow, NiFi), and SQL to build Big Data products & platforms\nExperience with Enterprise Business Intelligence Platform/Data platform ie Tableau, PowerBI is a plus.\nDemonstrated success interacting with stakeholders to understand technical needs and ensuring analyses and solutions meet their needs effectively.\nAbility to build a strong narrative on the business value of products and actively participate in sales enablement efforts.\nAble to work in a fast-paced, deadline-driven environment as part of a team and as an individual contributor.",Industry Type: Financial Services,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Data management', 'Data modeling', 'Information security', 'Machine learning', 'Data structures', 'Data mining', 'Business intelligence', 'SQL', 'Python']",2025-06-14 05:20:02
Data Analyst - Senior,FedEx,4 - 7 years,Not Disclosed,"['Gurugram', 'Bengaluru']","Role & responsibilities :\n\nAct as a technical expert on complex and specialist subject(s).\nSupport management with the analysis, interpretation and application of complex information, contributing to the achievement of divisional and corporate goals. Supports or leads projects by applying area of expertise.\nLead and implement advanced analytical processes through data/text mining, model development, and prediction to enable informed business decisions.\nApply sound analytical expertise to examine structured and unstructured data from multiple disparate sources to provide insights and recommend high-quality solutions to leadership across levels.\nPlan initiatives from concept to execution with minimal supervision and communicate results to a broad range of audiences. Develops a superior understanding of pricing and revenue management through internal and external sources to creatively solve business problems and lead the team from concept to execution of projects.\nTypically uses data, statistical and quantitative analysis, modeling, and fact-based management to drive decision-making. Provides regular expert consultative advice to senior leadership.\nEffectively shares best practices and fosters knowledge sharing across teams. Provides crossteam and cross-org consultation and supports communities of practice excellence.\n\n\n\nPreferred candidate profile\n\nRelevant experience in analytics/consulting/informatics and statistics\nKey Skills - Data and Business Analytics, Advanced Statistics and Predictive Modelling,\nStakeholder Management, Project Management\nExperience in pricing and revenue management yield management, customer segmentation analytics, revenue impact analytics, etc. is a plus\nExposure to predictive analytics, ML/ AI techniques is an added advantage\nTools - Oracle, SQL Server, Teradata, SAS, Python, Tableau/PowerBI/Spotfire\nGood to have cloud computing, big data, Azure",Industry Type: Courier / Logistics (Logistics Tech),Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Analysis', 'Business Insights', 'Python', 'SQL', 'Power Bi', 'Business Acumen', 'Tableau']",2025-06-14 05:20:04
Data Scientist,Dynamic Yield,5 - 10 years,Not Disclosed,['Gurugram'],"Our Purpose\nMastercard powers economies and empowers people in 200+ countries and territories worldwide. Together with our customers, we re helping build a sustainable economy where everyone can prosper. We support a wide range of digital payments choices, making transactions secure, simple, smart and accessible. Our technology and innovation, partnerships and networks combine to deliver a unique set of products and services that help people, businesses and governments realize their greatest potential.\nTitle and Summary\nData Scientist\nWho is Mastercard?\nMastercard is a global technology company in the payments industry. Our mission is to connect and power an inclusive, digital economy that benefits everyone, everywhere by making transactions safe, simple, smart, and accessible. Using secure data and networks, partnerships, and passion, our innovations and solutions help individuals, financial institutions, governments, and businesses realize their greatest potential.\nOur decency quotient, or DQ, drives our culture and everything we do inside and outside of our company. With connections across more than 210 countries and territories, we are building a sustainable world that unlocks priceless possibilities for all.\n\nOur Team:\nAs consumer preference for digital payments continues to grow, ensuring a seamless and secure consumer experience is top of mind. Optimization Soltions team focuses on tracking of digital performance across all products and regions, understanding the factors influencing performance and the broader industry landscape. This includes delivering data-driven insights and business recommendations, engaging directly with key external stakeholders on implementing optimization solutions (new and existing), and partnering across the organization to drive alignment and ensure action is taken.\nAre you excited about Data Assets and the value they bring to an organization?\nAre you an evangelist for data-driven decision-making?\nAre you motivated to be part of a team that builds large-scale Analytical Capabilities supporting end users across 6 continents?\nDo you want to be the go-to resource for data science & analytics in the company?\n\n\nThe Role:\n\nWork closely with global optimization solutions team to architect, develop, and maintain advanced reporting and data visualization capabilities on large volumes of data to support data insights and analytical needs across products, markets, and services\nThe candidate for this position will focus on Building solutions using Machine Learning and creating actionable insights to support product optimization and sales enablement.\nPrototype new algorithms, experiment, evaluate and deliver actionable insights.\nDrive the evolution of products with an impact focused on data science and engineering.\nDesigning machine learning systems and self-running artificial intelligence (AI) software to automate predictive models.\nPerform data ingestion, aggregation, and processing on high volume and high dimensionality data to drive and enable data unification and produce relevant insights.\nContinuously innovate and determine new approaches, tools, techniques & technologies to solve business problems and generate business insights & recommendations.\nApply knowledge of metrics, measurements, and benchmarking to complex and demanding solutions.\n\nAll about You\nA superior academic record at a leading university in Computer Science, Data Science, Technology, mathematics, statistics, or a related field or equivalent work experience\nExperience in data management, data mining, data analytics, data reporting, data product development and quantitative analysis\nStrong analytical skills with track record of translating data into compelling insights\nPrior experience working in a product development role.\nknowledge of ML frameworks, libraries, data structures, data modeling, and software architecture.\nproficiency in using Python/Spark, Hadoop platforms & tools (Hive, Impala, Airflow, NiFi), and SQL to build Big Data products & platforms\nExperience with Enterprise Business Intelligence Platform/Data platform i.e. Tableau, PowerBI is a plus.\nDemonstrated success interacting with stakeholders to understand technical needs and ensuring analyses and solutions meet their needs effectively.\nAbility to build a strong narrative on the business value of products and actively participate in sales enablement efforts.\nAble to work in a fast-paced, deadline-driven environment as part of a team and as an individual contributor.\nCorporate Security Responsibility\n\nAll activities involving access to Mastercard assets, information, and networks comes with an inherent risk to the organization and, therefore, it is expected that every person working for, or on behalf of, Mastercard is responsible for information security and must:\nAbide by Mastercard s security policies and practices;\nEnsure the confidentiality and integrity of the information being accessed;\nReport any suspected information security violation or breach, and\nComplete all periodic mandatory security trainings in accordance with Mastercard s guidelines.",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Data management', 'Data modeling', 'Information security', 'Machine learning', 'Data structures', 'Data mining', 'Business intelligence', 'SQL', 'Python']",2025-06-14 05:20:06
Be Our Next Data Modeller!!,Zensar,5 - 10 years,Not Disclosed,"['Hyderabad', 'Delhi / NCR']","Proficiency in data modeling tools such as ER/Studio, ERwin or similar.\nDeep understanding of relational database design, normalization/denormalization, and data warehousing principles.\nExperience with SQL and working knowledge of database platforms like Oracle, SQL Server, PostgreSQL, or Snowflake.\nStrong knowledge of metadata management, data lineage, and data governance practices.\nUnderstanding of data integration, ETL processes, and data quality frameworks.\nAbility to interpret and translate complex business requirements into scalable data models.\nExcellent communication and documentation skills to collaborate with cross-functional teams.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Er Studio', 'ERwin', 'SQL', 'Snowflake.', 'Metadata Management', 'PostgreSQL', 'data lineage', 'data governance', 'data warehousing', 'SQL Server', 'Oracle']",2025-06-14 05:20:08
Senior Data Lead,Aqb Solutions,7 - 10 years,20-27.5 Lacs P.A.,['Kolkata'],"We are looking for a Senior Data Lead to lead enterprise-level data modernization and innovation. In this highly strategic role, you will design scalable, secure, and future-ready data architectures, modernize legacy systems, and provide trusted technical leadership across both technology and business teams. This is a unique opportunity to make a company-wide impact by influencing data strategy and enabling smarter, faster decision-making through data.\n\nKey Responsibilities\n\nArchitect & Design: Lead the development of robust, scalable data models, data management systems, and integration frameworks to ensure enterprise-wide data accuracy, consistency, and security.\nDomain Expertise: Act as a subject matter expert across key business functions such as Supply Chain, Product Engineering, Sales & Marketing, Manufacturing, Finance, and Legal.\nModernization Leadership: Drive the transformation of legacy systems and manage end-to-end cloud migrations with minimal business disruption.\nCollaboration: Partner with data engineers, scientists, analysts, and IT leaders to build high-performance, scalable data pipelines and transformation solutions.\nGovernance & Compliance: Establish and maintain data governance frameworks including metadata repositories, data dictionaries, and data lineage documentation.\nStrategic Advisory: Provide guidance on data architecture best practices, technology selection, and roadmap alignment to senior leadership and cross-functional teams.\nMentorship: Serve as a mentor and thought leader to junior data professionals, fostering a culture of innovation, knowledge sharing, and technical excellence.\nInnovation & Trends: Stay abreast of emerging technologies in cloud, data platforms, and AI/ML to identify and implement innovative solutions.\nCommunication: Translate complex technical concepts into clear, actionable insights for technical and non-technical audiences alike.\nRequired Qualifications\n10+ years of experience in data architecture, engineering, or enterprise data management roles.\nDemonstrated success leading large-scale data initiatives in life sciences or other highly regulated industries.\nDeep expertise in modern data architecture paradigms such as Data Lakehouse, Data Mesh, or Data Fabric.\nStrong hands-on experience with cloud platforms like AWS, Azure, or Google Cloud Platform (GCP).\nProficiency in data modeling, ETL/ELT frameworks, and enterprise integration patterns.\nDeep understanding of data governance, metadata management, master data management (MDM), and data quality practices.\nExperience with tools and platforms including but not limited to:\nData Integration: Informatica, Talend\nData Governance: Collibra\nModeling/Transformation: dbt\nCloud Platforms: Snowflake, Databricks\nExcellent problem-solving skills with the ability to translate business requirements into scalable data solutions.\nExceptional communication skills and experience engaging with both executive stakeholders and engineering teams.\nPreferred Qualifications (Nice to Have)\nExperience with AI/ML data pipelines or real-time streaming architectures.\nCertifications in cloud technologies (e.g., AWS Certified Solutions Architect, Azure Data Engineer).\nFamiliarity with regulatory frameworks such as GxP, HIPAA, or GDPR.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Architecture', 'Data Migration', 'Data Management', 'Data Modeling', 'Data Governance', 'Data Integration']",2025-06-14 05:20:11
"Hiring Alert:- Customer Data Platform & Integration, Google Cloud Data",Fortune Global 500 IT Services Firm,12 - 22 years,Not Disclosed,"['Bengaluru', 'Delhi / NCR', 'Mumbai (All Areas)']","Professional & Technical Skills:\n- Must To Have Skills: Proficiency in Customer Data Platform & Integration, Google Cloud Data Services.\n- Strong understanding of data integration and data management principles.\n- Experience in architecting and implementing scalable and secure data solutions.\n- Knowledge of cloud-based data services and technologies.\n- Hands-on experience with data modeling and database design.",Industry Type: Analytics / KPO / Research,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Integration', 'Data Engineering', 'implementation', 'Customer data platform', 'Cdp']",2025-06-14 05:20:13
"Hiring Alert- Architect (Big Data, Apache Spark, Pyspark)",Leading Global MNC,12 - 22 years,Not Disclosed,['Chennai'],"We have an urgent requirement for the role- with Leading MNC based out at chennai Location\n\nJD- Architect (Big Data)\nProfessional & Technical Skills:\n- Must To Have Skills: Proficiency in Apache Spark, PySpark\n- Strong understanding of distributed computing and parallel processing.\n- Experience with data processing frameworks like Hadoop and Spark.\n- Hands-on experience with programming languages like Java or Scala.\n- Knowledge of SQL and database systems.\n- Familiarity with cloud platforms like AWS or Azure.\n- Experience with version control systems like Git.\n- Good To Have Skills: Experience with machine learning algorithms and libraries.\n- Knowledge of data streaming technologies like Kafka\n- Understanding of containerization technologies like Docker or Kubernetes.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Pyspark', 'Big Data', 'Spark', 'Hive', 'Hadoop']",2025-06-14 05:20:15
Big Data Developer,Binary Infoways,6 - 10 years,12-22 Lacs P.A.,['Hyderabad'],"AWS (EMR, S3, Glue, Airflow, RDS, Dynamodb, similar)\nCICD (Jenkins or another)\nRelational Databases experience (any)\nNo SQL databases experience (any)\nMicroservices or Domain services or API gateways or similar\nContainers (Docker, K8s, similar)",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'AWS', 'Python', 'Airflow', 'Java', 'Big Data', 'EMR', 'SQL', 'Jenkins', 'Glue', 'SCALA', 'Big Data Technologies', 'Spark']",2025-06-14 05:20:17
Sr. Data Scientist-Stratup-Mid-Size companies Exp.@ Bangalore_Urgent,"A leader in this space, we deliver world...",8 - 13 years,Not Disclosed,['Bengaluru'],"Senior Data Scientist\n\nLocation: Onsite Bangalore\nExperience: 8+ years\n\nRole Overview\n\nWe are seeking a Senior Data Scientist with a strong foundation in machine learning, deep learning, and statistical modeling, with the ability to translate complex operational problems into scalable AI/ML solutions. In addition to core data science responsibilities, the role involves building production-ready backends in Python and contributing to end-to-end model lifecycle management. Exposure to computer vision is a plus, especially for industrial use cases like identification, intrusion detection, and anomaly detection.\n\nKey Responsibilities\n\nDevelop, validate, and deploy machine learning and deep learning models for forecasting, classification, anomaly detection, and operational optimization\nBuild backend APIs using Python (FastAPI, Flask) to serve ML/DL models in production environments\nApply advanced computer vision models (e.g., YOLO, Faster R-CNN) to object detection, intrusion detection, and visual monitoring tasks\nTranslate business problems into analytical frameworks and data science solutions\nWork with data engineering and DevOps teams to operationalize and monitor models at scale\nCollaborate with product, domain experts, and engineering teams to iterate on solution design\nContribute to technical documentation, model explainability, and reproducibility practices\n\n\nRequired Skills\n\nStrong proficiency in Python for data science and backend development\nExperience with ML/DL libraries such as scikit-learn, TensorFlow, or PyTorch\nSolid knowledge of time-series modeling, forecasting techniques, and anomaly detection\nExperience building and deploying APIs for model serving (FastAPI, Flask)\nFamiliarity with real-time data pipelines using Kafka, Spark, or similar tools\nStrong understanding of model validation, feature engineering, and performance tuning\nAbility to work with SQL and NoSQL databases, and large-scale datasets\nGood communication skills and stakeholder engagement experience\n\n\nGood to Have\n\nExperience with ML model deployment tools (MLflow, Docker, Airflow)\nUnderstanding of MLOps and continuous model delivery practices\nBackground in aviation, logistics, manufacturing, or other industrial domains\nFamiliarity with edge deployment and optimization of vision models\n\n\nQualifications\n\nMasters or PhD in Data Science, Computer Science, Applied Mathematics, or related field\n7+ years of experience in machine learning and data science, including end-to-end deployment of models in production",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['scikit-learn', 'time-series modeling', 'ML/DL libraries', 'data science', 'Python', 'Airflow', 'Kafka', 'MLflow', 'logistics', 'anomaly detection', 'aviation', 'SQL', 'PyTorch', 'NoSQL', 'MLOps', 'forecasting techniques', 'Docker', 'manufacturing', 'FastAPI', 'Spark', 'TensorFlow', 'Flask']",2025-06-14 05:20:20
Senior Data Scientist,Cradlepoint,3 - 8 years,Not Disclosed,['Bengaluru'],"Join our Team\nAbout this Opportunity\nThe complexity of running and optimizing the next generation of wireless networks, such as 5G with distributed edge compute, will require Machine Learning (ML) and Artificial Intelligence (AI) technologies. Ericsson is setting up an AI Accelerator Hub in India to fast-track our strategy execution, using Machine Intelligence (MI) to drive thought leadership, automate, and transform Ericsson s offerings and operations. We collaborate with academia and industry to develop state-of-the-art solutions that simplify and automate processes, creating new value through data insights.\nWhat you will do\nAs a Senior Data Scientist, you will apply your knowledge of data science and ML tools backed with strong programming skills to solve real-world problems.\nResponsibilities:\n1. Lead AI/ML features/capabilities in product/business areas\n2. Define business metrics of success for AI/ML projects and translate them into model metrics\n3. Lead end-to-end development and deployment of Generative AI solutions for enterprise use cases\n4. Design and implement architectures for vector search, embedding models, and RAG systems\n5. Fine-tune and evaluate large language models (LLMs) for domain-specific tasks\n6. Collaborate with stakeholders to translate vague problems into concrete Generative AI use cases\n7. Develop and deploy generative AI solutions using AWS services such as SageMaker, Bedrock, and other AWS AI tools. Provide technical expertise and guidance on implementing GenAI models and best practices within the AWS ecosystem.\n8. Develop secure, scalable, and production-grade AI pipelines\n9. Ensure ethical and responsible AI practices\n10. Mentor junior team members in GenAI frameworks and best practices\n11. Stay current with research and industry trends in Generative AI and apply cutting-edge techniques\n12. Contribute to internal AI governance, tooling frameworks, and reusable components\n13. Work with large datasets including petabytes of 4G/5G networks and IoT data\n14. Propose/select/test predictive models and other ML systems\n15. Define visualization and dashboarding requirements with business stakeholders\n16. Build proof-of-concepts for business opportunities using AI/ML\n17. Lead functional and technical analysis to define AI/ML-driven business opportunities\n18. Work with multiple data sources and apply the right feature engineering to AI models\n19. Lead studies and creative usage of new/existing data sources\nWhat you will bring\n1. Bachelors/Masters/Ph.D. in Computer Science, Data Science, AI, ML, Electrical Engineering, or related disciplines from reputed institutes\n2. 3+ years of applied ML/AI production-level experience\n3. Strong programming skills (R/Python)\n4. Proven ability to lead AI/ML projects end-to-end\n5. Strong grounding in mathematics, probability, and statistics\n6. Hands-on experience with data analysis, visualization techniques, and ML frameworks (Python, R, H2O, Keras, TensorFlow, Spark ML)\n7. Experience with semi-structured/unstructured data for AI/ML models\n8. Strong understanding of building AI models using Deep Neural Networks\n9. Experience with Big Data technologies (Hadoop, Cassandra)\n10. Ability to source and combine data from multiple sources for ML models\nPreferred Qualifications:\n1. Good communication skills in English\n2. Certifying MI MOOCs, a plus\n3. Domain knowledge in Telecommunication/IoT, a plus\n4. Experience with data visualization and dashboard creation, a plus\n5. Knowledge of Cognitive models, a plus\n6. Experience in partnering and collaborative co-creation in a global matrix organization.\nWhy join Ericsson\n\n\nWhat happens once you apply\nPrimary country and city: India (IN) || Bangalore\nReq ID: 766481",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Wireless', 'Computer science', 'Data analysis', 'cassandra', 'Neural networks', 'Artificial Intelligence', 'Machine learning', 'Telecommunication', 'data visualization', 'Python']",2025-06-14 05:20:22
Lead Software Engineer / LMTS,Salesforce,10 - 15 years,Not Disclosed,['Bengaluru'],"To get the best candidate experience, please consider applying for a maximum of 3 roles within 12 months to ensure you are not duplicating efforts.\nJob Category\nSoftware Engineering\nJob Details\nAbout Salesforce\nAbout Salesforce\nWe re Salesforce, the Customer Company, inspiring the future of business with AI+ Data +CRM. Leading with our core values, we help companies across every industry blaze new trails and connect with customers in a whole new way. And, we empower you to be a Trailblazer, too driving your performance and career growth, charting new paths, and improving the state of the world. If you believe in business as the greatest platform for change and in companies doing well and doing good- you ve come to the right place.\nRole Description\nSalesforce has immediate opportunities for software developers who want their lines of code to have significant and measurable positive impact for users, the companys bottom line, and the industry. You will be working with a group of world-class engineers to build the breakthrough features our customers will love, adopt, and use while keeping our trusted CRM platform stable and scalable. The software engineer role at Salesforce encompasses architecture, design, implementation, and testing to ensure we build products right and release them with high quality.\nCode review, mentoring junior engineers, and providing technical guidance to the team (depending on the seniority level) We pride ourselves on writing high-quality, maintainable code that strengthens the stability of the product and makes our lives easier. We embrace the hybrid model and celebrate the individual strengths of each team member while encouraging everyone on the team to grow into the best version of themselves. We believe that autonomous teams with the freedom to make decisions will empower the individuals, the product, the company, and the customers they serve to thrive.\nYour Impact\nAs a Lead Backend Software Engineer, your job responsibilities will include:\nBuild new and exciting components in an ever-growing and evolving market technology to provide scale and efficiency.\nDevelop high-quality, production-ready code that can be used by millions of users of our cloud platform.\nMake design decisions on the basis of performance, scalability, and future expansion.\nWork in a Hybrid Engineering model and contribute to all phases of SDLC including design, implementation, code reviews, automation, and testing of the features.\nBuild efficient components/algorithms on a microservice multi-tenant SaaS cloud environment\nCode review, mentoring junior engineers, and providing technical guidance to the team (depending on the seniority level)\nRequired Skills:\nMastery of multiple programming languages and platforms\n10+ years of software development experience\nDeep knowledge of object-oriented programming and other scripting languages: Java, Python, Scala C#, Go, Node.JS and C++.\nStrong SQL skills and experience and experience with relational and non-relational databases e.g. (Postgress / Trino / redshift / Mongo).\nExperience with developing SAAS products over public cloud infrastructure - AWS/Azure/GCP.\nProven experience designing and developing distributed systems at scale.\nProficiency in queues, locks, scheduling, event-driven architecture, and workload distribution, along with a deep understanding of relational database and non-relational databases.\nA deeper understanding of software development best practices and demonstrate leadership skills.\nDegree or equivalent relevant experience required. Experience will be evaluated based on the core competencies for the role (e.g. extracurricular leadership roles, military experience, volunteer roles, work experience, etc.)\nPreferred Skills:\nExperience with Big-Data/ML and S3\nHands-on experience with Streaming technologies like Kafka\nExperience with Elastic Search\nExperience with Terraform, Kubernetes, Docker\nExperience working in a high-paced and rapidly growing multinational organization\n\nBENEFITS & PERKS\nComprehensive benefits package including well-being reimbursement, generous parental leave, adoption assistance, fertility benefits, and more!\nWorld-class enablement and on-demand training with Trailhead.com\nExposure to executive thought leaders and regular 1:1 coaching with leadership\nVolunteer opportunities and participation in our 1:1:1 model for giving back to the community\nFor more details, visit https: / / www.salesforcebenefits.com /\nAccommodations\nIf you require assistance due to a disability applying for open positions please submit a request via this Accommodations Request Form .\nPosting Statement\nAt Salesforce we believe that the business of business is to improve the state of our world. Each of us has a responsibility to drive Equality in our communities and workplaces. We are committed to creating a workforce that reflects society through inclusive programs and initiatives such as equal pay, employee resource groups, inclusive benefits, and more. Learn more about Equality at www.equality.com and explore our company benefits at www.salesforcebenefits.com .\nSalesforce welcomes all.",Industry Type: Internet,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['C++', 'Automation', 'Backend', 'Scheduling', 'Distribution system', 'SDLC', 'SQL', 'CRM', 'Python', 'Salesforce']",2025-06-14 05:20:24
Senior Manager Marketing Data Analytics,Factspan Analytics,9 - 14 years,Not Disclosed,['Bengaluru'],"Position: Senior Manager marketing Analytics\nBengaluru, Karnataka\n\nFactspan Overview: Factspan is a pure play data and analytics services organization. We partner with fortune 500 enterprises to build an analytics center of excellence, generating insights and solutions from raw data to solve business challenges, make strategic recommendations and implement new processes that help them succeed. With offices in Seattle, Washington and Bengaluru, India; we use a global delivery model to service our customers. Our customers include industry leaders from Retail, Financial Services, Hospitality, and technology sectors.",,,,"['Market Mix Modelling', 'People Management Skills', 'Marketing Analytics', 'Mmm', 'Stakeholder Management', 'Delivery Management']",2025-06-14 05:20:26
Big Data Developer,Nitor,6 - 11 years,15-30 Lacs P.A.,['Pune'],"Hi ,\n\nGreetings from Nitor infotech!\n\nPlease find below the Company  Job Description with this mail\n\n\n6+ years candidates only apply\nImmediate joiners Preferred\nPython ,Hadoop,hive,spark and SQL are mandatory\n\n\n\nJob Description:\n\nPrimary Skills:\n•6+ years of Data Engineering experience.\n•4+ years of experience in Big Data technologies (e.g. Spark, Hive, Hadoop, etc.).\n•Strong Experience designing and implementing data pipelines.\n•Excellent knowledge of Data engineering concepts and best practices.\n•Proven ability to lead, mentor, inspire and support more junior team members.\n•Able to lead technical deliverables autonomously and lead more junior data engineers.\n•Strong attention to detail and working according to best practices.\n•Experience in designing solution using batch data processing methods, real-time streams, ETL processes and Business Intelligence tools.\n•Experience designing Logical Data Model and Physical Data Models including data warehouse and data mart designs.\n•Strong SQL knowledge & experience (T-SQL, working with SQL Server, SSMS)\n•Apache Spark: Advanced proficiency with Spark, including PySpark and SparkSQL, for distributed data processing.\n•Working knowledge of Apache Hive\n•Proficiency in Python, Pandas, PySpark (Scala knowledge is a plus).\n•Knowledge of Delta Lake concepts and common data formats, Lakehouse architecture.\n•Source control with Git.\n•Expertise in designing and implementing scalable data pipelines and ETL processes using the GCP data stack including: BigQuery, Dataflow, Pub/Sub, Cloud Storage, Cloud Composer, Cloud Functions, Dataproc (spark)\n•Apache Airflow - Expertise in building and managing ETL workflows using Airflow, including DAG creation, scheduling, and error handling.\n•Knowledge of CI/CD concepts, experience designing CI/CD for data pipelines.\nSecondary Skills:\n•Experience with Streaming services such as Kafka is a plus.\n•R & Sparklyr experience is a plus\n•Knowledge of MLOps concepts, AI/ML life-cycle management, Mlflow\n•Expertise in writing complex, highly optimized queries across large data sets to write data pipelines and data processing layers.\n•Jenkins\n\nCandidate Profile:\nDesign, build, test and deploy innovative Big Data solutions at scale.\nExtract, Clean, transform, and analyse vast amounts of raw data from various Data Sources.\nBuild data pipelines and API integrations with various internal systems.\nWork Across all stages of Data Lifecycle\nImplement best practices across all Data Analytics Processes\nEstimate effort, identify risks, and plan execution.\nProactively monitor, identify, and escalate issues or root causes of systemic issues.\nEnable data scientists, business, and product partners to fully leverage our platform.\nEngage with business stakeholders to understand client requirements and build technical solutions and delivery plans.\nEvaluate and communicate technical risks effectively and ensure assignments delivery in scheduled time with desired quality.\nProvide end to end big data solution and design details to data engineering teams.\nExcellent analytical & problem-solving skills\nExcellent communication skills, experience communicating with Snr. Business stakeholders\nLeading technical delivery on use-cases, able to plan and delegate tasks to more junior team members, oversee the work from inception to final product. \n\nKey Required Skills:\nApache Airflow, Kafka, SQL, Data Engineering, CI CD pipelines , Big Data, Apache Spark, Logical Data Model, Physical Data Model\n\nWish you all the best!\nThanks & Regards,\nVIGNESH\n6379146150",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Hive', 'Hadoop', 'Spark', 'Python', 'SQL']",2025-06-14 05:20:29
Data Modeler - SQL/ Erwin,Leading Client,7 - 10 years,Not Disclosed,['Bengaluru'],"Job Title : Data Engineer / Data Modeler.\n\nLocation : Remote (India).\n\nEmployment Type : Contract (Remote).\n\nExperience Required : 7+ Years.\n\nJob Summary :\n\nWe are looking for a highly skilled Data Engineer / Data Modeler with strong experience in Snowflake, DBT, and GCP to support our data infrastructure and modeling initiatives.\n\nThe ideal candidate should possess excellent SQL skills, hands-on experience with Erwin Data Modeler, and a strong background in modern data architectures and data modeling techniques.\n\nKey Responsibilities :\n\n- Design and implement scalable data models using Snowflake and Erwin Data Modeler.\n\n- Create, maintain, and enhance data pipelines using DBT and GCP (BigQuery, Cloud Storage, Dataflow).\n\n- Perform reverse engineering on existing systems (e.g., Sailfish/DDMS) using DBeaver or similar tools to understand and rebuild data models.\n\n- Develop efficient SQL queries and stored procedures for data transformation, quality, and validation.\n\n- Collaborate with business analysts and stakeholders to gather data requirements and convert them into physical and logical models.\n\n- Ensure performance tuning, security, and optimization of the Snowflake data warehouse.\n\n- Document metadata, data lineage, and business logic behind data structures and flows.\n\n- Participate in code reviews, enforce coding standards, and provide best practices for data modeling and governance.\n\nMust-Have Skills :\n\n- Snowflake architecture, schema design, and data warehouse experience.\n\n- DBT (Data Build Tool) for data transformation and pipeline development.\n\n- Strong expertise in SQL (query optimization, complex joins, window functions, etc.)\n\n- Hands-on experience with Erwin Data Modeler (logical and physical modeling).\n\n- Experience with GCP (BigQuery, Cloud Composer, Cloud Storage).\n\n- Experience in reverse engineering legacy systems like Sailfish or DDMS using DBeaver.\n\nGood To Have :\n\n- Experience with CI/CD tools and DevOps for data environments.\n\n- Familiarity with data governance, security, and privacy practices.\n\n- Exposure to Agile methodologies and working in distributed teams.\n\n- Knowledge of Python for data engineering tasks and orchestration scripts.\n\nSoft Skills :\n\n- Excellent problem-solving and analytical skills.\n\n- Strong communication and stakeholder management.\n\n- Self-driven with the ability to work independently in a remote setup.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Modeling', 'Data Quality', 'Data Build Tool', 'Data Security', 'Snowflake DB', 'Data Warehousing', 'Data Analytics', 'Data Governance', 'Erwin', 'SQL']",2025-06-14 05:20:31
Data Science Analyst (Standard),Infogain,3 - 8 years,Not Disclosed,['Gurugram'],"Job Description: Data Scientist\n1. Expertise in Data Science & AI/ML: 3-8 years experience designing, developing, and deploying scalable AI/ML solutions for Big Data, with proficiency in Python, SQL, TensorFlow, PyTorch, Scikit-learn, and Big Data ML libraries (e.g., Spark MLlib).\n2. Cloud Proficiency: Proven experience with cloud-based Big Data services (GCP preferred, AWS/Azure a plus) for AI/ML model deployment and Big Data pipelines.; understanding of data modeling, warehousing, and ETL in Big Data contexts.\n3. Analytical & Communication Skills: Ability to extract actionable insights from large datasets, apply statistical methods, and effectively communicate complex findings to both technical and non-technical audiences (visualization skills a plus).\n4. Educational Background: Bachelors or Masters degree in a quantitative field (Computer Science, Data Science, Engineering, Statistics, Mathematics).\nEXPERIENCE\n3-4.5 Years\nSKILLS\nPrimary Skill: Data Science\nSub Skill(s): Data Science\nAdditional Skill(s): Python, Data Science, SQL, TensorFlow, Pytorch",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'data services', 'data science', 'Data modeling', 'GCP', 'Analytical', 'Cloud', 'big data', 'SQL', 'Python']",2025-06-14 05:20:33
Lead Azure Data Factory (ADF),Quatrro,3 - 8 years,Not Disclosed,"['Kolkata', 'Mumbai', 'New Delhi', 'Hyderabad', 'Pune', 'Chennai', 'Bengaluru']","To handle all Data/BI responsibilities, including a major part of the work on ADF and team handling.\n  Key Responsibilities:\nData Warehouse Development:\nDesign and implement scalable and efficient data warehouse solutions.\nDevelop complex SQL Server-based solutions, including T-SQL queries, stored procedures, and performance tuning.\nOptimize SQL Server databases, develop T-SQL scripts, and improve query performance.\nETL Development and Maintenance:\nBuild and optimize ETL workflows using Azure Data Factory (ADF) for data integration from multiple sources.\nEnsure high-performance data pipelines for large-scale data processing.\nIntegrate and automate data processes using Azure Functions to extend ETL capabilities.\nCloud Integration:\nImplement cloud-native solutions leveraging Azure SQL Database, Azure Functions, and Synapse Analytics.\nSupport hybrid data integration scenarios combining on-premises and Azure services.\nData Governance and Quality:\nEstablish and maintain robust data quality frameworks and governance standards.\nEnsure consistency, accuracy, and security of data across all platforms.\nLeadership and Collaboration:\nLead a BI and data professionals team, providing mentorship and technical direction.\nPartner with stakeholders to understand business requirements and deliver data-driven solutions.\nDefine project goals, timelines, and resources for successful execution.\nShould be flexible to support multiple IT platforms\nManaging day-to-day activities Jira request, SQL execution, access request, resolving alerts, and updating Tickets",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Performance tuning', 'Back office', 'Data processing', 'Support services', 'Data quality', 'Stored procedures', 'Outsourcing', 'JIRA', 'Analytics', 'SQL']",2025-06-14 05:20:36
Enterprise Data Architect - Azure / ETL,Leading Client,10 - 12 years,Not Disclosed,['Bengaluru'],"Key Responsibilities :\n\nAs an Enterprise Data Architect, you will :\n\n- Lead Data Architecture : Design, develop, and implement comprehensive enterprise data architectures, primarily leveraging Azure and Snowflake platforms.\n\n- Data Transformation & ETL : Oversee and guide complex data transformation and ETL processes for large and diverse datasets, ensuring data integrity, quality, and performance.\n\n- Customer-Centric Data Design : Specialize in designing and optimizing customer-centric datasets from various sources, including CRM, Call Center, Marketing, Offline, and Point of Sale systems.\n\n- Data Modeling : Drive the creation and maintenance of advanced data models, including Relational, Dimensional, Columnar, and Big Data models, to support analytical and operational needs.\n\n- Query Optimization : Develop, optimize, and troubleshoot complex SQL and NoSQL queries to ensure efficient data retrieval and manipulation.\n\n- Data Warehouse Management : Apply advanced data warehousing concepts to build and manage high-performing, scalable data warehouse solutions.\n\n- Tool Evaluation & Implementation : Evaluate, recommend, and implement industry-leading ETL tools such as Informatica and Unifi, ensuring best practices are followed.\n\n- Business Requirements & Analysis : Lead efforts in business requirements definition and management, structured analysis, process design, and use case documentation to translate business needs into technical specifications.\n\n- Reporting & Analytics Support : Collaborate with reporting teams, providing architectural guidance and support for reporting technologies like Tableau and PowerBI.\n\n- Software Development Practices : Apply professional software development principles and best practices to data solution delivery.\n\n- Stakeholder Collaboration : Interface effectively with sales teams and directly engage with customers to understand their data challenges and lead them to successful outcomes.\n\n- Project Management & Multi-tasking : Demonstrate exceptional organizational skills, with the ability to manage and prioritize multiple simultaneous customer projects effectively.\n\n- Strategic Thinking & Leadership : Act as a self-managed, proactive, and customer-focused leader, driving innovation and continuous improvement in data architecture.\n\nPosition Requirements :\n\nof strong experience with data transformation & ETL on large data sets.\n\n- Experience with designing customer-centric datasets (i.e., CRM, Call Center, Marketing, Offline, Point of Sale, etc.).\n\n- 5+ years of Data Modeling experience (i.e., Relational, Dimensional, Columnar, Big Data).\n\n- 5+ years of complex SQL or NoSQL experience.\n\n- Extensive experience in advanced Data Warehouse concepts.\n\n- Proven experience with industry ETL tools (i.e., Informatica, Unifi).\n\n- Solid experience with Business Requirements definition and management, structured analysis, process design, and use case documentation.\n\n- Experience with Reporting Technologies (i.e., Tableau, PowerBI).\n\n- Demonstrated experience in professional software development.\n\n- Exceptional organizational skills and ability to multi-task simultaneous different customer projects.\n\n- Strong verbal & written communication skills to interface with sales teams and lead customers to successful outcomes.\n\n- Must be self-managed, proactive, and customer-focused.\n\nTechnical Skills :\n\n- Cloud Platforms : Microsoft Azure\n\n- Data Warehousing : Snowflake\n\n- ETL Methodologies : Extensive experience in ETL processes and tools\n\n- Data Transformation : Large-scale data transformation\n\n- Data Modeling : Relational, Dimensional, Columnar, Big Data\n\n- Query Languages : Complex SQL, NoSQL\n\n- ETL Tools : Informatica, Unifi (or similar enterprise-grade tools)\n\n- Reporting & BI : Tableau, PowerBI",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure', 'Enterprise Architect', 'Data Architect', 'Big Data', 'Snowflake DB', 'Data Modeling', 'Data Warehousing', 'ETL', 'Reporting Tools', 'SQL']",2025-06-14 05:20:38
Lead SDET | Open Source Data Platform (Onsite),Acceldata,5 - 10 years,Not Disclosed,['Bengaluru'],"About the Role:\nWe are looking for an experienced Lead SDET for our ODP, specializing in ensuring the quality and performance of large-scale data systems.\nIn this role, you will work closely with development and operations teams to design and execute comprehensive test strategies for Open Source Data Platform (ODP), including Hadoop, Spark, Hive, Kafka, and other related technologies. You will focus on test automation, performance tuning, and identifying bottlenecks in distributed data systems.\nYour key responsibilities will include writing test plans, creating automated test scripts, and conducting functional, regression, and performance testing. You will be responsible for identifying and resolving defects, ensuring data integrity, and improving testing processes. Strong collaboration skills are essential as you will be interacting with cross-functional teams and driving quality initiatives. Your work will directly contribute to maintaining high-quality standards for big data solutions and enhancing their reliability at scale.\n\nYou are a great fit for this role if you have\nProven expertise in Quality Engineering, with a strong background in test automation, performance testing, and defect management across multiple data platforms.\nA proactive mindset to define and implement comprehensive test strategies that ensure the highest quality standards are met.\nExperience in working with both functional and non-functional testing, with a particular focus on automated test development.\nA collaborative team player with the ability to effectively work cross-functionally with development teams to resolve issues and deliver timely fixes.\nStrong communication skills with the ability to mentor junior engineers and share knowledge to improve testing practices across the team.\nA commitment to continuous improvement, with the ability to analyze testing processes and recommend enhancements to align with industry best practices.\nAbility to quickly learn new technologies\n\nWhat we look for:\n6-10 years of hands-on experience in quality engineering and quality assurance, focusing on test automation, performance testing, and defect management across multiple data platforms\nProficiency in programming languages such as Java, Python, or Scala for writing test scripts and automating test cases with hands-on experience in developing automated tests using other test automation frameworks, ensuring robust and scalable test suites.\nProven ability to define and execute comprehensive test strategies, including writing test plans, test cases, and scripts for both functional and non-functional testing to ensure predictable delivery of high-quality products and solutions \nExperience with version control systems like Git and CI/CD tools such as Jenkins or GitLab CI to manage code changes and automate test execution within the development pipeline.\nExpertise in identifying, tracking, and resolving defects and issues, collaborating closely with developers and product teams to ensure timely fixes.\nStrong communication skills with the ability to work cross-functionally with development teams and mentor junior team members to improve testing practices and tools.\nAbility to analyze testing processes, recommend improvements and ensure the testing environment aligns with industry best practices, contributing to the overall quality of software.",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Coding', 'API Testing', 'Java', 'Non Functional Testing', 'Ui Automation Testing', 'Database Testing', 'Ui Automation', 'Selenium', 'Functional Testing']",2025-06-14 05:20:41
Senior Data Modeler - Technical Lead,BILVANTIS TECHNOLOGIES,7 - 10 years,Not Disclosed,['Hyderabad'],"Senior Data Modeler - Technical Lead:\n\nSenior Data Modeler designs and optimizes enterprise data models and data warehouse architectures for analytics and reporting. They collaborate with engineers and analysts to ensure scalability, data integrity, and governance best practices. Expertise in dimensional modeling, SQL optimization, and cloud data warehouses is essential.\n\nEXPERIENCE REQUIREMENTS:\n\nBachelor s degree in computer science, Information Systems, or a related field.\n8+ years of experience in data modeling, data architecture, and enterprise DWH development.\nStrong expertise in at least one Columnar MPP Cloud Data Warehouse (Snowflake, Azure Synapse, Redshift).\nExperience with ETL/ELT tools (Azure Data Factory, DBT, Fivetran) to support data model implementation.\nAdvanced SQL development skills, including query optimization, stored procedures, and indexing strategies.\nHands-on experience in dimensional modeling, data vault modeling, and OLAP architectures.\nExperience working with data modeling tools (Erwin, DBT, SQL DB Modeler, or similar).\nFamiliarity with data governance frameworks, data lineage tracking, and MDM solutions.\nExperience in agile development processes using Jira and Confluence.\n\nPreferred Qualifications:\n\nKnowledge of Python for data transformation and automation.\nDomain expertise in healthcare data (provider credentialing, claims processing, payer networks).\n\nKEY RESPONSIBILITIES:\n\nData Modeling Architecture:\nDesign and develop conceptual, logical, and physical data models to support enterprise-wide DWH, data marts, and OLAP cubes for analytics and reporting.\nImplement dimensional modeling (star/snowflake schemas) for analytical workloads and normalized models (3NF) for operational data stores (ODS).\nOptimize data structures for performance, scalability, and maintainability, ensuring they support complex analytical queries.\nCreate and maintain data dictionaries, metadata, and ER diagrams to document models effectively.\nDefine slowly changing dimensions (SCD), surrogate keys, fact tables, and aggregates to enhance analytical reporting.\nData Warehouse Development Optimization:\nArchitect and implement Enterprise Data Warehousing (EDW) solutions to consolidate and centralize business data.\nWork with Data Engineers to develop ETL/ELT strategies for ingesting, transforming, and storing data efficiently in cloud DWH environments.\nOptimize data partitioning, clustering, indexing, and query performance tuning within Snowflake, Redshift, or Azure Synapse.\nDevelop and maintain data pipelines to support real-time and batch data processing.\nDefine data retention, archiving, and purging strategies to optimize storage costs.\nData Governance, Quality, Compliance:\nEstablish data governance best practices to ensure accuracy, consistency, and security across all data assets.\nImplement data lineage tracking, cataloging, and MDM (Master Data Management) strategies to improve data discoverability.\nWork with compliance teams to enforce security, access control, and regulatory compliance (HIPAA, GDPR, SOC 2, etc.) in data management.\nDevelop data quality frameworks to monitor anomalies, enforce validation rules, and handle missing or inconsistent data.\nDocumentation Knowledge Sharing:\nCreate and maintain comprehensive documentation, including data flow diagrams, transformation logic, mapping documents, and ETL specifications.\nConduct data model reviews, knowledge-sharing sessions, and training for engineering and analytics teams.\nStay updated with industry trends in DWH architectures, cloud data platforms, and data modeling tools.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Performance tuning', 'Automation', 'SOC', 'Technical Lead', 'Data structures', 'Healthcare', 'Stored procedures', 'Analytics', 'SQL', 'Python']",2025-06-14 05:20:43
Compliance-Associate-Software Engineering,Goldman Sachs,2 - 5 years,Not Disclosed,['Bengaluru'],"YOUR IMPACT\nAre you passionate about developing mission-critical, high quality software solutions, using cutting-edge technology, in a dynamic environment?\nOUR IMPACT\nWe are Compliance Engineering, a global team of more than 300 engineers and scientists who work on the most complex, mission-critical problems.\nWe:\nbuild and operate a suite of platforms and applications that prevent, detect, and mitigate regulatory and reputational risk across the firm.\nhave access to the latest technology and to massive amounts of structured and unstructured data.\nleverage modern frameworks to build responsive and intuitive front end and Big Data applications.\nThe firm is making a significant investment to uplift and rebuild the Compliance application portfolio in 2023.\nTo achieve this Compliance Engi neering is looking to fill several full stack developers roles across different teams.\nHOW YOU WILL FULFILL YOUR POTENTIAL\nAs a member of our team, you will:\npartner globally with sponsors, users and engineering colleagues across multiple divisions to create end-to-end solutions,\nlearn from experts,\nleverage various technologies depending on the team including; Java, JavaScript, TypeScript, React, APIs, GraphQL, Elastic Search, Kafka, Kubernetes, Machine Learning\nbe able to innovate and incubate new ideas,\nhave an opportunity to work on a broad range of problems, often dealing with large data sets, including real-time processing, messaging, workflow and UI/UX\nbe involved in the full life cycle; defining, designing, implementing, testing, deploying, and maintaining software across our products.\nQUALIFICATIONS\nA successful candidate will possess the following attributes:\nA Bachelors or Masters degree in Computer Science, Computer Engineering, or a similar field of study.\nExpertise in Java development.\nExperience in automated testing and SDLC concepts.\nThe ability (and tenacity) to clearly express ideas and arguments in meetings and on paper.\nExperience in some of the following is desired and can set you apart from other candidates :\nUI/UX development\nAPI design, such as to create interconnected services,\nmessage buses or real time processing,\nrelational databases\nknowledge of the financial industry and compliance or risk functions,\ninfluencing stakeholders.",Industry Type: Banking,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Front end', 'Machine learning', 'Javascript', 'Developer', 'Workflow', 'HTML', 'Investment banking', 'Investment management', 'SDLC']",2025-06-14 05:20:45
"Business Intelligence Engineer, AOP Team",Amazon,1 - 6 years,Not Disclosed,['Bengaluru'],"*** The role is for 1 year term in Amazon\n\n\nAre you interested in applying your strong quantitative analysis and big data skills to world-changing problems? Are you interested in driving the development of methods, models and systems for strategy planning, transportation and fulfillment network? If so, then this is the job for you.\n\nOur team is responsible for creating core analytics tech capabilities, platforms development and data engineering. We develop scalable analytics applications across APAC, MENA and LATAM. We standardize and optimize data sources and visualization efforts across geographies, builds up and maintains the online BI services and data mart. You will work with professional software development managers, data engineers, business intelligence engineers and product managers using rigorous quantitative approaches to ensure high quality data tech products for our customers around the world, including India, Australia, Brazil, Mexico, Singapore and Middle East.\n\nAmazon is growing rapidly and because we are driven by faster delivery to customers, a more efficient supply chain network, and lower cost of operations, our main focus is in the development of strategic models and automation tools fed by our massive amounts of available data. You will be responsible for building these models/tools that improve the economics of Amazon s worldwide fulfillment networks in emerging countries as Amazon increases the speed and decreases the cost to deliver products to customers. You will identify and evaluate opportunities to reduce variable costs by improving fulfillment center processes, transportation operations and scheduling, and the execution to operational plans.\n\nMajor responsibilities include:\n\nTranslating business questions and concerns into specific analytical questions that can be answered with available data using BI tools; produce the required data when it is not available.\nWriting SQL queries and automation scripts\nEnsure data quality throughout all stages of acquisition and processing, including such areas as data sourcing/collection, ground truth generation, normalization, transformation, cross-lingual alignment/mapping, etc.\n\nCommunicate proposals and results in a clear manner backed by data and coupled with actionable conclusions to drive business decisions.\nCollaborate with colleagues from multidisciplinary science, engineering and business backgrounds.\nDevelop efficient data querying and modeling infrastructure.\nManage your own process. Prioritize and execute on high impact projects, triage external requests, and ensure to deliver projects in time.\nUtilizing code (SQL, Python, R, Scala, etc.) for analyzing data and building data marts 3+ years of analyzing and interpreting data with Redshift, Oracle, NoSQL etc. experience\nExperience with data visualization using Tableau, Quicksight, or similar tools\nExperience with data modeling, warehousing and building ETL pipelines\nExperience in Statistical Analysis packages such as R, SAS and Matlab\nExperience using SQL to pull data from a database or data warehouse and scripting experience (Python) to process data for modeling Experience with AWS solutions such as EC2, DynamoDB, S3, and Redshift\nExperience in data mining, ETL, etc. and using databases in a business environment with large-scale, complex datasets",,,,"['Supply chain', 'SAS', 'Data modeling', 'Scheduling', 'Oracle', 'Business intelligence', 'Data mining', 'MATLAB', 'Analytics', 'Python']",2025-06-14 05:20:47
"Engineer, Staff/Manager",Qualcomm,11 - 16 years,Not Disclosed,['Bengaluru'],"Synechron is seeking a knowledgeable and proactive Data Modeler to guide the design and development of data structures that support our clients' business objectives. In this role, you will collaborate with cross-functional teams to translate business requirements into scalable and efficient data models, ensuring data accuracy, consistency, and integrity. You will contribute to creating sustainable and compliant data architectures that leverage emerging technologies such as cloud, IoT, mobile, and blockchain. Your work will be instrumental in enabling data-driven decision-making and operational excellence across projects.Software Required\n\nSkills:\nStrong understanding of data modeling concepts, methodologies, and tools Experience with data modeling for diverse technology platforms including cloud, mobile, IoT, and blockchain Familiarity with database management systems (e.g., relational, NoSQL) Knowledge of SDLC and Agile development practices Proficiency in modeling tools such as ERwin, PowerDesigner, or similar Preferred Skills:\nExperience with data integration tools and ETL processes Knowledge of data governance and compliance standards Familiarity with cloud platforms (AWS, Azure, GCP) and how they impact data architectureOverall Responsibilities Collaborate with business analysts, data engineers, and stakeholders to understand data requirements and translate them into robust data models Design logical and physical data models optimized for performance, scalability, and maintainability Develop and maintain documentation for data structures, including data dictionaries and metadata Conduct reviews of data models and code to ensure adherence to quality standards and best practices Assist in designing data security and privacy measures in alignment with organizational policies Stay informed about emerging data modeling trends and incorporate best practices into project delivery Support data migration, integration, and transformation activities as needed Provide technical guidance and mentorship related to data modeling standardsTechnical Skills (By Category) Data Modeling & Data Management: EssentialLogical/physical data modeling, ER diagrams, data dictionaries PreferredDimensional modeling, data warehousing, master data management Programming Languages: PreferredSQL (expertise in writing complex queries) OptionalPython, R for data analysis and scripting Databases & Data Storage Technologies: EssentialRelational databases (e.g., Oracle, SQL Server, MySQL) PreferredNoSQL (e.g., MongoDB, Cassandra), cloud-native data stores Cloud Technologies: PreferredBasic understanding of cloud data solutions (AWS, Azure, GCP) Frameworks & Libraries: Not typically required, but familiarity with data integration frameworks is advantageous Development Tools & Methodologies: EssentialData modeling tools (ERwin, PowerDesigner), version control (Git), Agile/Scrum workflows Security & Compliance: Knowledge of data security best practices, regulatory standards like GDPR, HIPAAExperience Minimum of 8+ years of direct experience in data modeling, data architecture, or related roles Proven experience designing data models for complex systems across multiple platforms (cloud, mobile, IoT, blockchain) Experience working in Agile environments using tools like JIRA, Confluence, Git Preference for candidates with experience supporting data governance and data quality initiativesNoteEquivalent demonstrated experience in relevant projects or certifications can qualify candidates.Day-to-Day Activities Participate in daily stand-ups and project planning sessions Collaborate with cross-functional teams to understand and analyze business requirements Create, review, and refine data models and associated documentation Develop data schemas, dictionaries, and standards to ensure consistency Support data migration, integration, and performance tuning activities Conduct peer reviews and provide feedback on data models and solutions Keep current with the latest industry developments in data architecture and modeling Troubleshoot and resolve data-related technical issuesQualifications Bachelors or Masters degree in Computer Science, Data Science, Information Technology, or related fields Demonstrated experience with data modeling tools and techniques in diverse technological environments Certifications related to data modeling, data management, or cloud platforms (preferred)Professional Competencies Strong analytical and critical thinking skills to develop optimal data solutions Effective communication skills for translating technical concepts to non-technical stakeholders Ability to work independently and in collaborative team environments Skilled problem solver able to handle complex data challenges Adaptability to rapidly evolving technologies and project requirements Excellent time management and prioritization skills to deliver quality outputs consistently",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data modeling', 'modeling tools', 'relational databases', 'scrum', 'agile', 'confluence', 'hipaa', 'data warehousing', 'data architecture', 'erwin', 'sql', 'git', 'gcp', 'mysql', 'etl', 'mongodb', 'jira', 'python', 'oracle', 'microsoft azure', 'sql server', 'nosql', 'gdpr', 'cassandra', 'aws', 'data integration', 'sdlc']",2025-06-14 05:20:49
Sr. Data Analyst,Slintel,3 - 8 years,Not Disclosed,[],"Our Mission:\n6sense is on a mission to revolutionize how B2B organizations create revenue by predicting customers most likely to buy and recommending the best course of action to engage anonymous buying teams. 6sense Revenue AI is the only sales and marketing platform to unlock the ability to create, manage and convert high-quality pipeline to revenue.\nOur People:\nPeople are the heart and soul of 6sense. We serve with passion and purpose. We live by our Being 6sense values of Accountability, Growth Mindset, Integrity, Fun and One Team. Every 6sensor plays a part in de ning the future of our industry-leading technology. 6sense is a place where difference-makers roll up their sleeves, take risks, act with integrity, and measure success by the value we create for our customers.\nWe want 6sense to be the best chapter of your career.\nPosition Overview:\nWe are seeking a highly skilled Sr. Data Analyst to focus on backend data support and governance. The ideal candidate will have a strong background in data engineering principles, SQL, and data modeling within modern cloud data platforms. This individual will play a key role in building and maintaining a scalable and trusted data infrastructure that supports reporting across the customer journey. A working knowledge of data governance frameworks and the ability to collaborate cross-functionally is essential.\nKey Responsibilities:\nBuild, maintain, and optimize data pipelines and models within our data warehouse to enable trusted downstream analytics.\nDevelop scalable, clean, and joinable datasets to support reporting across sales, marketing, customer success, and finance functions.\nCollaborate closely with RevOps, data engineering, and analytics stakeholders to ensure data is structured and aligned to business needs.\nSupport data governance by enforcing data definitions, naming conventions, and ownership models.\nMonitor and improve data quality, lineage, and integrity through proactive checks and documentation.\nTranslate raw data into reusable, governed tables and metrics to support self-service and centralized reporting use cases.\nAssist in standardizing metrics and business definitions to drive consistent reporting across systems and teams.\nQualifications:\nBachelor s degree in Computer Science, Data Science, Information Systems, or a related field. Master s preferred.\n3+ years of experience in a data analytics, analytics engineering, or backend reporting role.\nExpert-level SQL skills and experience working with cloud data warehouses (Snowflake, Redshift, BigQuery, etc.).\nSolid understanding of dimensional modeling, data architecture, and ELT pipeline development.\nFamiliarity with data governance tools, policies, or best practices.\nExperience with BI platforms (Looker, Tableau, Power BI, Sigma) is a plus.\nStrong organizational and communication skills; ability to translate technical requirements into business impact.\nOur Benefits:\nFull-time employees can take advantage of health coverage, paid parental leave, generous paid time-off and holidays, quarterly self-care days off, and stock options. We ll make sure you have the equipment and support you need to work and connect with your teams, at home or in one of our o ces.\nWe have a growth mindset culture that is represented in all that we do, from onboarding through to numerous learning and development initiatives including access to our LinkedIn Learning platform. Employee well-being is also top of mind for us. We host quarterly wellness education sessions to encourage self care and personal growth. From wellness days to ERG-hosted events, we celebrate and energize all 6sense employees and their backgrounds.\nEqual Opportunity Employer:\n6sense is an Equal Employment Opportunity and Affirmative Action Employers. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender perception or identity, national origin, age, marital status, protected veteran status, or disability status. .\nWe are aware of recruiting impersonation attempts that are not affiliated with 6sense in any way. A ll email communications from 6sense will originate from the @6sense.com domain . We will not initially contact you via text message and will never request payments . If you are uncertain whether you have been contacted by an official 6sense employee, reach out to jobs@ 6sense.com",Industry Type: Internet,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Backend', 'Data modeling', 'Senior Data Analyst', 'data governance', 'Wellness', 'Data quality', 'Downstream', 'SQL', 'Data architecture']",2025-06-14 05:20:52
"Manager, Platform Engineering Bengaluru, India",Cargill,4 - 9 years,Not Disclosed,['Bengaluru'],"Manager, Platform Engineering\nJob ID 308786 Date posted 06/11/2025 Location : Bengaluru, India Category DIGITAL TECHNOLOGY AND DATA (DTD) Job Status Salaried Full Time\n\n\nJob Purpose and Impact\nThe Supervisor II, Platform Engineering job sets goals and objectives for the achievement of operational results for the Digital Integration Platform team responsible for designing, developing and maintaining digital technology infrastructure to support information technology platforms and services. This job oversees the delivery against project planning and prioritization, coordinates collaboration with cross functional teams, and allocates resources effectively and efficiently. The job also leads the team to implement best in class industry standards to continuously improve the development process.\n\n\nKey Accountabilities\nPROJECT MANAGEMENT: Oversees the implementation and delivery of software projects, including resource allocation, to ensure they are completed on time and within scope.\nTECHNICAL GUIDANCE: Leads the team to apply internal software deployment platform, continuous integration or continuous delivery pipeline and twelve factor development methodology to automate the deployment process, ensuring smooth and reliable releases.\nQUALITY ASSURANCE: Leads rigorous testing, code reviews, and adherence to best in class industry standards to ensure the quality and performance of software applications.\nPROCESS IMPROVEMENT: Suggests continuous improvement initiatives and leads the implementation of approved standards to improve software development and deployment processes and operational excellence, applying test driven development as needed.\nCOLLABORATION: Coordinates collaboration with product managers, designers and other cross functional teams to gather requirements, set priorities and deliver resolutions to meet business objectives.\nDOCUMENTATION: Leads and reviews the creation and maintenance of comprehensive documentation for software applications, deployment processes and system configurations.\nSTAKEHOLDER MANAGEMENT: Maintains partnership with key internal and external stakeholders, understanding their needs and enabling effective communication to assure project alignment and success.\nTEAM MANAGEMENT: Manages team members to achieve the organization s goals, by ensuring productivity, communicating performance expectations, creating goal alignment, giving and seeking feedback, providing coaching, measuring progress and holding people accountable, supporting employee development, recognizing achievement and lessons learned, and developing enabling conditions for talent to thrive in an inclusive team culture.\n\n\nQualifications\nMinimum requirement of 4 years of relevant work experience. Typically reflects 5 years or more of relevant exp.\nPriori experience as a middleware / digital integration engineer p erform ing platform an d integration pipeline engineering leveraging advanced cloud technologies and diverse coding languages.\nLeading geographically distributed engineering teams across a large global organization\nDeveloping and managing strategic partnerships across both digital and business facing stakeholders\nTrack record of leading architecture strategies and execution across a diverse digital and data technology landscape\nExperience developing and leading transformation strategies regarding to people, process, and technology\nThorough understanding of industry trends and best practices related to integration platform engineering of robust, performant, and cost effective solutions\nProven record h elp ing drive the adoption of new technologies and methods within the digital integration team and be a role model and mentor for data engineers.",Industry Type: Food Processing,Department: Project & Program Management,"Employment Type: Full Time, Permanent","['Manager Quality Assurance', 'Team management', 'Operational excellence', 'Coding', 'Project management', 'Process improvement', 'Project planning', 'Continuous improvement', 'Stakeholder management', 'Information technology']",2025-06-14 05:20:54
Data Techology Senior Associate,MSCI Services,2 - 8 years,Not Disclosed,['Pune'],"The Data Technology team at MSCI is responsible for meeting the data requirements across various business areas, including Index, Analytics, and Sustainability. Our team collates data from multiple sources such as vendors (e.g., Bloomberg, Reuters), website acquisitions, and web scraping (e.g., financial news sites, company websites, exchange websites, filings). This data can be in structured or semi-structured formats. We normalize the data, perform quality checks, assign internal identifiers, and release it to downstream applications.\nYour Key Responsibilities\nAs data engineers, we build scalable systems to process data in various formats and volumes, ranging from megabytes to terabytes. Our systems perform quality checks, match data across various sources, and release it in multiple formats. We leverage the latest technologies, sources, and tools to process the data. Some of the exciting technologies we work with include Snowflake, Databricks, and Apache Spark.\nYour skills and experience that will help you excel\nCore Java, Spring Boot, Apache Spark, Spring Batch, Python. Exposure to sql databases like Oracle, Mysql, Microsoft Sql is a must. Any experience / knowledge / certification on Cloud technology preferrably Microsoft Azure or Google cloud platform is good to have. Exposures to non sql databases like Neo4j or Document database is again good to have.\nAbout MSCI\nWhat we offer you\nTransparent compensation schemes and comprehensive employee benefits, tailored to your location, ensuring your financial security, health, and overall wellbeing.\nFlexible working arrangements, advanced technology, and collaborative workspaces.\nA culture of high performance and innovation where we experiment with new ideas and take responsibility for achieving results.\nA global network of talented colleagues, who inspire, support, and share their expertise to innovate and deliver for for ongoing skills development.\nMulti-directional career paths that offer professional growth and development through new challenges, internal mobility and expanded roles.\nWe actively nurture an environment that builds a sense of inclusion belonging and connection, including eight Employee Resource Groups. All Abilities, Asian Support Network, Black Leadership Network, Climate Action Network, Hola! MSCI, Pride & Allies, Women in Tech, and Women s Leadership Forum.\n.\nMSCI Inc. is an equal opportunity employer. It is the policy of the firm to ensure equal employment opportunity without discrimination or harassment on the basis of race, color, religion, creed, age, sex, gender, gender identity, sexual orientation, national origin, citizenship, disability, marital and civil partnership/union status, pregnancy (including unlawful discrimination on the basis of a legally protected parental leave), veteran status, or any other characteristic protected by law. MSCI is also committed to working with and providing reasonable accommodations to individuals with disabilities. If you are an individual with a disability and would like to request a reasonable accommodation for . Please note, this e-mail is intended only for individuals who are requesting a reasonable workplace accommodation; it is not intended for other inquiries.\nTo all recruitment agencies\n.\nNote on recruitment scams",Industry Type: NGO / Social Services / Industry Associations,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['CVS', 'Core Java', 'Bloomberg', 'spring batch', 'MySQL', 'Oracle', 'Analytics', 'Downstream', 'Python', 'Recruitment']",2025-06-14 05:20:57
Performance Test Engineer,Cognizant,5 - 10 years,Not Disclosed,"['Hyderabad', 'Chennai', 'Bengaluru']","1. Total of 6+ years of experience with a minimum 3+ years of proven hands-on software testing experience.\n2. Strong experience in performance testing of SAP applications. Candidate should have an experience working on Brown or Green field S/4 HANA implementations\n3. Strong knowledge of one or more programming languages (e.g., Python, Perl, ABAP, Java, C++).\n4. Experience in performance testing tools\nLoadRunner Enterprise, Apache JMeter, SoapUI, Postman.",,,,"['Performance Testing', 'Load Runner', 'SAP', 'JMeter']",2025-06-14 05:20:59
Senior ETL Engineer/Consultant Specialist,Hsbc,3 - 6 years,Not Disclosed,['Hyderabad'],"Some careers shine brighter than others.\nIf you re looking for a career that will help you stand out, join HSBC and fulfil your potential. Whether you want a career that could take you to the top, or simply take you in an exciting new direction, HSBC offers opportunities, support and rewards that will take you further.\nHSBC is one of the largest banking and financial services organisations in the world, with operations in 64 countries and territories. We aim to be where the growth is, enabling businesses to thrive and economies to prosper, and, ultimately, helping people to fulfil their hopes and realise their ambitions.\nWe are currently seeking an experienced professional to join our team in the role of Consultant Specialist\nIn this role you will be\nDesign and Develop ETL Processes: Lead the design and implementation of ETL processes using all kinds of batch/streaming tools to extract, transform, and load data from various sources into GCP.\nCollaborate with stakeholders to gather requirements and ensure that ETL solutions meet business needs.\nData Pipeline Optimization: Optimize data pipelines for performance, scalability, and reliability, ensuring efficient data processing workflows.\nMonitor and troubleshoot ETL processes, proactively addressing issues and bottlenecks.\nData Integration and Management: I ntegrate data from diverse sources, including databases, APIs, and flat files, ensuring data quality and consistency.\nManage and maintain data storage solutions in GCP (e. g. , BigQuery, Cloud Storage) to support analytics and reporting.\nGCP Dataflow Development: Write Apache Beam based Dataflow Job for data extraction, transformation, and analysis, ensuring optimal performance and accuracy.\nCollaborate with data analysts and data scientists to prepare data for analysis and reporting.\nAutomation and Monitoring: Implement automation for ETL workflows using tools like Apache Airflow or Cloud Composer, enhancing efficiency and reducing manual intervention.\nSet up monitoring and alerting mechanisms to ensure the health of data pipelines and compliance with SLAs.\nData Governance and Security: Apply best practices for data governance, ensuring compliance with industry regulations (e. g. , GDPR, HIPAA) and internal policies.\nCollaborate with security teams to implement data protection measures and address vulnerabilities.\nDocumentation and Knowledge Sharing: Document ETL processes, data models, and architecture to facilitate knowledge sharing and onboarding of new team members.\nConduct training sessions and workshops to share expertise and promote best practices within the team.\n\n\n\n\n\n\n\n\n\n\n\nRequirements\n\n\n\nTo be successful in this role, you should meet the following requirements:\nEducation: Bachelor s degree in Computer Science, Information Systems, or a related field.\nExperience: Minimum of 5 years of industry experience in data engineering or ETL development, with a strong focus on Data Stage and GCP.\nProven experience in designing and managing ETL solutions, including data modeling, data warehousing, and SQL development.\nTechnical Skills: Strong knowledge of GCP services (e. g. , BigQuery, Dataflow, Cloud Storage, Pub/Sub) and their application in data engineering.\nExperience of cloud-based solutions, especially in GCP, cloud certified candidate is preferred.\nExperience and knowledge of Bigdata data processing in batch mode and streaming mode, proficient in Bigdata eco systems, e. g. Hadoop, HBase, Hive, MapReduce, Kafka, Flink, Spark, etc.\nFamiliarity with Java Python for data manipulation on Cloud/Bigdata platform.\nAnalytical Skills: Strong problem-solving skills with a keen attention to detail.\nAbility to analyze complex data sets and derive meaningful insights.\nBenefits: Competitive salary and comprehensive benefits package.\nOpportunity to work in a dynamic and collaborative environment on cutting-edge data projects.\nProfessional development opportunities to enhance your skills and advance your career.\nIf you are a passionate data engineer with expertise in ETL processes and a desire to make a significant impact within our organization, we encourage you to apply for this exciting opportunity!",Industry Type: Consumer Electronics & Appliances,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Automation', 'Data modeling', 'HIPAA', 'Data quality', 'Apache', 'Monitoring', 'Analytics', 'Financial services', 'Python']",2025-06-14 05:21:01
Senior Engineer II,AMERICAN EXPRESS,7 - 12 years,Not Disclosed,['Chennai'],"At American Express, you ll be recognized for your contributions, leadership, and impact every colleague has the opportunity to share in the company s success. Together, we ll win as a team, striving to uphold our company values and powerful backing promise to provide the world s best customer experience every day. And we ll do it with the utmost integrity, and in an environment where everyone is seen, heard and feels like they belong.\nJoin Team Amex and lets lead the way together.\nAs part of our diverse tech team, you can architect, code and ship software that makes us an essential part of our customers digital lives. Here, you can work alongside talented engineers in an open, supportive, inclusive environment where your voice is valued, and you make your own decisions on what tech to use to solve challenging problems. Amex offers a range of opportunities to work with the latest technologies and encourages you to back the broader engineering community through open source. And because we understand the importance of keeping your skills fresh and relevant, we give you dedicated time to invest in your professional development. Find your place in technology on #TeamAmex.",,,,"['Performance tuning', 'Automation', 'Coding', 'XML', 'Agile', 'PLSQL', 'Data structures', 'JSON', 'Open source', 'Python']",2025-06-14 05:21:03
"Associate Staff Engineer, Frontend React",Nagarro,5 - 7 years,Not Disclosed,['Bengaluru'],"We're Nagarro.\n\nWe are a Digital Product Engineering company that is scaling in a big way! We build products, services, and experiences that inspire, excite, and delight. We work at scale across all devices and digital mediums, and our people exist everywhere in the world (18000+ experts across 38 countries, to be exact). Our work culture is dynamic and non-hierarchical. We're looking for great new colleagues. That's where you come in!\n\nREQUIREMENTS:\nTotal Experience 5+ years.\nHands on working experience in front-end or full-stack development experience, with building production apps in React.js and Next.js.\nHands-on expertise writing unit/integration tests for React components (Jest, React Testing Library, etc.)\nSolid grasp of state-management patterns and libraries (Redux, React Context, Zustand, etc.).\nStrong understanding of RESTful APIs, asynchronous programming (Promises, async/await), and modern build tools (Webpack, Vite, or Turbopack).\nPractical experience with Git, pull-request workflows, and collaborative development tools (GitHub, GitLab, Bitbucket).\nAdvanced proficiency in JavaScript (ES6+) and TypeScript.\nProblem-solving mindset with the ability to tackle complex data engineering challenges. \nStrong communication and teamwork skills, with the ability to mentor and collaborate effectively.\n\nRESPONSIBILITIES:\nWriting and reviewing great quality code\nUnderstanding the clients business use cases and technical requirements and be able to convert them in to technical design which elegantly meets the requirements\nMapping decisions with requirements and be able to translate the same to developers.\nIdentifying different solutions and being able to narrow down the best option that meets the clients requirements.\nDefining guidelines and benchmarks for NFR considerations during project implementation\nWriting and reviewing design document explaining overall architecture, framework, and high-level design of the application for the developers.\nReviewing architecture and design on various aspects like extensibility, scalability, security, design patterns, user experience, NFRs, etc., and ensure that all relevant best practices are followed.\nDeveloping and designing the overall solution for defined functional and non-functional requirements; and defining technologies, patterns, and frameworks to materialize it\nUnderstanding and relating technology integration scenarios and applying these learnings in projects\nResolving issues that are raised during code/review, through exhaustive systematic analysis of the root cause, and being able to justify the decision taken.\nCarrying out POCs to make sure that suggested design/technologies meet the requirements.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Typescript', 'Javascript', 'React.Js']",2025-06-14 05:21:05
Senior Manager Technology - US Commercial Data & Analytics,Amgen Inc,8 - 10 years,Not Disclosed,['Hyderabad'],"What you will do\nIn this vital role you will lead the engagement model between Amgen's Technology organization and our global business partners in Commercial Data & Analytics. We seek a technology leader with a passion for innovation and a collaborative working style that partners effectively with business and technology leaders. Are you interested in building a team that consistently delivers business value in an agile model using technologies such as AWS, Databricks, Airflow, and Tableau Come join our team!\nRoles & Responsibilities:",,,,"['Analytics', 'Power BI', 'Big Data', 'Data Lake', 'Databricks', 'Tableau', 'ETL', 'Data Integration']",2025-06-14 05:21:08
Senior Data Scientist,Codetru Software Solutions,8 - 12 years,Not Disclosed,['Hyderabad'],"Senior Data ScientistLocation: Hyderabad, IndiaExperience: 8-10 YearsWe are seeking a highly motivated, driven, and experienced Senior Data Scientist to join our dynamic team. As a go-getter with a passion for uncovering insights from complex data, you will play a pivotal role in shaping our data strategy and driving business decisions. The ideal candidate is a proactive problem-solver who thrives in a fast-paced environment and possesses a deep understanding of the entire data lifecycle, from extraction to model deployment.\n\nNote: This is purely a Technical role not Managerial.\n\nKey Responsibilities\nLead and execute end-to-end data science projects, from conception and data collection to model building and delivering actionable insights.\nDesign, build, and maintain robust and scalable ETL pipelines to process large volumes of structured and unstructured data from our data lake.\nUtilize advanced SQL and Python (Pandas, NumPy) for data extraction, manipulation, and in-depth analysis to identify critical trends, patterns, and opportunities.\nDevelop and implement a variety of machine learning algorithms, such as regression, classification, clustering, and forecasting models, to solve key business challenges.\nCreate compelling and intuitive data visualizations and dashboards using tools like Tableau or Power BI to communicate complex findings to both technical and non-technical stakeholders.\nMentor junior data scientists and contribute to the team's technical growth and best practices.\n\nMust-Have Qualifications & Skills\n\nExperience: A minimum of 8-10 years of hands-on experience in a data science or related role.\nSQL and Visualization: Expert-level proficiency in SQL for complex querying and proven experience with data visualization tools such as Tableau, Power BI, or Looker.\nData Engineering: Strong, hands-on experience building and managing ETL processes and working extensively within a data lake environment.\nPython and Data Analysis: Mastery of Python and its core data science libraries, especially Pandas, for data wrangling, exploration, and identifying hidden patterns.\nMachine Learning: In-depth theoretical knowledge and practical application of various ML algorithms, including supervised and unsupervised learning techniques. A portfolio of successfully deployed models is a strong plus.\nAttitude: A proactive, self-starting go-getter with excellent problem-solving skills and the drive to take ownership of projects from start to finish.",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Pandas', 'Fast Api', 'Machine Learning', 'Numpy', 'Python', 'Power Bi', 'Tableau', 'SQL', 'Flask']",2025-06-14 05:21:11
Snowflake Databricks Engineer,Oracle,4 - 7 years,Not Disclosed,['Bengaluru'],"Snowflake Development:\nDesign, build, and optimize ETL pipelines within and outside of Snowflake.\nDevelop scripts using languages like Python, Unix, or other relevant technologies for data loading, extraction, and transformation.\nCreate and maintain views, stored procedures, and other database objects in Snowflake.\nOptimize query performance and troubleshoot data-related issues.\nDatabricks Integration:\nCollaborate with data scientists and analysts to integrate Snowflake with Databricks.\nBuild and maintain highly scalable data pipelines using Databricks, Azure, AWS, or other cloud platforms.\nDesign and implement ETL/ELT data pipelines to extract, process, and transform data from various sources.\nWork on data insights, machine learning models, and fraud detection within the Databricks environment.\nData Quality and Security:\nImplement extensive data quality checks to ensure high-quality data.\nDefine and enforce data security measures within both Snowflake and Databricks.\nMonitor and manage access control for data assets.\nCollaboration and Communication:\nCollaborate with business leaders to understand organizational goals and align data engineering efforts.\nCommunicate data trends, insights, and recommendations to business executives.\nQualifications:\nBachelor s degree in Computer Science, Information Systems, or a related field.\nExp Level- 4-7 Years\nProven experience as a Data Engineer, with a focus on Snowflake and Databricks.\nStrong proficiency in SQL and Snowflake s Snow SQL.\nFamiliarity with cloud platforms (Azure, AWS, or Google Cloud).\nExperience with ETL/ELT processes, data modeling, and performance optimization.\nExcellent problem-solving skills and attention to detail.\nEffective communication and collaboration abilities.\nAdditional Notes:\nThis role offers an exciting opportunity to work at the intersection of Snowflake and Databricks, leveraging the strengths of both platforms.\nYou ll contribute to building scalable, reliable, and secure data solutions that drive business insights and innovation.\nIdeal to have some background knowledge around Finance / Investment Banking / Fixed Income / OCIO Business\nCareer Level - IC2",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Unix', 'Computer science', 'Data modeling', 'data security', 'Fixed income', 'Data quality', 'Investment banking', 'Stored procedures', 'SQL', 'Python']",2025-06-14 05:21:13
"Senior Staff Engineer, Frontend React",Nagarro,10 - 15 years,Not Disclosed,[],"Total Experience 10+ years.\nHands on working experience in front-end or full-stack development experience, with building production apps in React.js and Next.js.\nHands-on expertise writing unit/integration tests for React components (Jest, React Testing Library, etc)\nSolid grasp of state-management patterns and libraries (Redux, React Context, Zustand, etc).\nStrong understanding of RESTful APIs, asynchronous programming (Promises, async/await), and modern build tools (Webpack, Vite, or Turbopack).\nPractical experience with Git, pull-request workflows, and collaborative development tools (GitHub, GitLab, Bitbucket).\nAdvanced proficiency in JavaScript (ES6+) and TypeScript.\nProblem-solving mindset with the ability to tackle complex data engineering challenges.\nStrong communication and teamwork skills, with the ability to mentor and collaborate effectively.\nRESPONSIBILITIES:\nWriting and reviewing great quality code\nUnderstanding the clients business use cases and technical requirements and be able to convert them in to technical design which elegantly meets the requirements\nMapping decisions with requirements and be able to translate the same to developers.\nIdentifying different solutions and being able to narrow down the best option that meets the clients requirements.\nDefining guidelines and benchmarks for NFR considerations during project implementation\nWriting and reviewing design document explaining overall architecture, framework, and high-level design of the application for the developers.\nReviewing architecture and design on various aspects like extensibility, scalability, security, design patterns, user experience, NFRs, etc, and ensure that all relevant best practices are followe'd.\nDeveloping and designing the overall solution for defined functional and non-functional requirements; and defining technologies, patterns, and frameworks to materialize it\nUnderstanding and relating technology integration scenarios and applying these learnings in projects\nResolving issues that are raised during code/review, through exhaustive systematic analysis of the root cause, and being able to justify the decision taken.\nCarrying out POCs to make sure that suggested design/technologies meet the requirements.\n\n\nBachelor s or master s degree in computer science, Information Technology, or a related field.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'github', 'Usage', 'GIT', 'Project implementation', 'Architecture', 'Scalability', 'Technical design', 'High level design', 'Information technology']",2025-06-14 05:21:15
Senior Data Scientist,Growby Exx Services,5 - 10 years,Not Disclosed,['Ahmedabad'],"Growexx is looking for smart and passionateData Scientist/Analyst, who will empower Marketing, Product, Sales teams to make strategic, data-driven decisions.\nKey Responsibilities\nMine, process, and analyse hit/event level web, product, sales, and digital marketing data. \nLeverage LLMs (Large Language Models) and traditional machine learning to mine, process, and analyze web, product, sales, and digital marketing event-level data.\nDevelop and fine-tune LLM-driven solutions for tasks such as text summarization, customer support automation, personalization, and user journey understanding.\nBuild and deploy predictive models and ML algorithms across structured and unstructured customer profile, journey, and usage datasets.\nDeploy LLM and ML models into production environments for activation across websites, product applications, and sales/marketing channels.\nDesign and implement model activation strategies, including A/B testing plans, benchmarking studies, and measurement of final business impact.\nConduct comprehensive evaluation of LLMs, including performance benchmarking (accuracy, latency, token usage, cost), prompt effectiveness testing, fine-tuning impact analysis, and safety/bias assessments.\nDesign, build, and deploy LLM-based agentic systems using frameworks such as LangChain, AutoGen, CrewAI, or custom orchestration for complex workflows (e.g., multi-agent collaboration, function-calling pipelines, dynamic task execution).\nIntegrate LLM agents with APIs, internal knowledge bases, retrieval systems (RAG architectures), and external tools to enable autonomous or semi-autonomous decision-making.\nPartner with data engineering teams to enhance and maintain the Customer360 data model, including creating new feature engineering requirements, improving taxonomy, and identifying and resolving data quality issues.\nCollaborate with cross-functional teams (Enterprise Data Warehouse, Salesforce MOPS, IT, Product, Marketing) to continuously improve data integration and quality for advanced modeling use cases.\nBuild a deep understanding of business models, objectives, challenges, and opportunities by working closely with leadership and key stakeholders.\nDocument model methodologies, evaluation frameworks, agent workflows, deployment architectures, and post-activation performance results in a structured and reproducible format.\nStay current with advancements in LLMs, agentic AI, retrieval-augmented generation (RAG), and ML technologies to recommend and implement innovative solutions.\nKey Skills\nExperience using Python, SciKit, SQL, Snowflake, product usage data, Jupyter Notebooks, Amazon SageMaker, Airflow, Github.\nProficient in data mining, advanced statistical analysis, feature engineering, and mathematical modeling.\nDeep experience with machine learning techniques including supervised, unsupervised, reinforcement learning, causal inference, and predictive modeling.\nSkilled across the full ML lifecycle: data preparation, feature creation and selection, model training, hyperparameter tuning, evaluation, and deployment for inference/prediction.\nExtensive hands-on experience with cookie-level advertising and digital marketing data (Google Ads, Bing, Epsilon, LinkedIn, Facebook) for demand generation KPIs such as ROAS, CTRs, impressions, multi-touch attribution (MTA).\nProven experience designing, fine-tuning, evaluating, and deploying Large Language Models (LLMs) and generative AI applications.\nExperience designing and deploying agentic systems using frameworks such as LangChain, AutoGen, CrewAI, and custom function-calling pipelines.\nExpertise integrating LLM agents with APIs, knowledge bases, retrieval systems (RAG architecture), and orchestrating dynamic multi-agent workflows.\nStrong understanding of evaluation metrics for LLMs, including prompt testing, token optimization, bias/safety analysis, latency, and cost benchmarks.\nDeep familiarity with cookie-level web and product behavior data (usage metrics, conversion funnels, bounce rates, sessions, hits/events, journey optimization).\nExpertise in designing and executing A/B, multivariate, and lift tests to measure activated ML/LLM model performance across digital and offline channels.\nSkilled in gathering business requirements, translating them into ML use cases, and clearly communicating methodologies and results to both technical and non-technical stakeholders.\nContinuous learner, keeping up-to-date with the latest advances in transformers, generative AI models, retrieval-augmented generation (RAG), and agentic AI frameworks.\nPreferred: practical experience in an engineering capacity building, testing, deploying, and optimizing ensemble ML and LLM solutions in production environments.\nEducation and Experience\nB Tech or B. E. (Computer Science / Information Technology)\n5 + years as a Data Scientist or similar roles.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Generative Ai', 'LLM', 'Machine Learning', 'Deep Learning']",2025-06-14 05:21:17
Engineering IT Software Solutions Manager,Qualcomm,7 - 12 years,Not Disclosed,['Bengaluru'],"General Summary:\nQualcomm is enabling a world where everyone and everything can be intelligently connected. Qualcomm 5G and AI innovations are the power behind the connected intelligent edge. Youll find our technologies behind and inside the innovations that deliver significant value across multiple industries and to billions of people every day.\nQualcomm engineering teams rely heavily on the latest High Performance Computing (HPC) technologies to design and develop new products using electronic design automation (EDA) tools. This role offers an exciting opportunity to manage and deliver a portfolio of distributed software solutions and services for core engineering teams. You will gain experience leading a portfolio of critical projects while building scalable and fault-tolerant software solutions that are deployed on some of the largest supercomputing infrastructures across the globe.\nMinimum Qualifications:\n7+ years of IT-related work experience with a Bachelor's degree.\nOR\n9+ years of IT-related work experience without a Bachelors degree.\n\n4+ years in a leadership role in projects/programs.\nWhat are we looking for?\nEngineering Data Analytics & Applications (EDAAP) team is looking for an experienced software development manager preferably with exposure to HPC technologies. The team handles development of software and analytics solutions enabling High Performance Compute grid and large-scale, distributed applications. They work on components and services for HPC infrastructure optimization, hardware IP management systems, petabyte-scale cloud data platforms, development of machine learning solutions, data pipelines, and operational insights.\nThis role will lead a team of about 30 software developers and data engineers working on a portfolio of software products and analytics being developed by the team. The ideal candidate would be a seasoned Software Development Manager experienced in engaging with business and technical stakeholders, understanding complex problem statements, and proposing value-driven software and analytics solutions.\nWhat will you do?\nThis roles responsibilities include:\nLead and manage a team of software developers, data engineers and project manager, providing mentorship and guidance to foster professional growth.\nProvide technical expertise across a portfolio of software development and analytics projects, creating designs and performing code reviews.\nIdentify opportunities and deliver solutions for EDA workflow optimizations.\nSet and manage team priorities in line with organizational goals and objectives, working closely with diverse set of stakeholders in Engineering & Infrastructure Services.\nOversee the entire software development lifecycle, from planning and design to implementation, testing, and deployment for a portfolio of products and services developed by the team.\nCollaborate with global teams to define project requirements, scope, and deliverables.\nEnsure the delivery of high-quality software solutions and analytics that meet business objectives and customer needs.\nImplement best practices for software development, including coding standards, code reviews, and automated testing.\nManage project timelines and resources to ensure successful project completion.\nStay updated with the latest industry trends and technologies to drive continuous improvement and innovation.\nBuild a culture of collaboration, accountability, and continuous learning within the team.\nWhat do we want to see?\nThe ideal candidate will be able to demonstrate some of the following skills:\n14+ years of hands-on experience in large-scale distributed software engineering and analytics, with at least 4 years in a leadership role\nStrong proficiency in programming languages such as Java, C++, Python, Rust or similar.\nExpertise in software lifecycle management, version control, and CI/CD best practices for quality, agility and security, data engineering and analytics\nProven ability to manage multiple projects and conflicting priorities.\nExperience with public cloud environments such as AWS, Azure or Google Cloud\nExperience with microservices architecture and containerization\nFamiliarity with EDA and semiconductor design process\nAbility to explain technical concepts and analysis implications in a clear manner to a wide audience.\nExposure to HPC technologies is a plus.\nBachelors or Masters in Computer Science or related field",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['CI/CD', 'Java', 'C++', 'Azure', 'Rust', 'microservices architecture', 'AWS', 'Python', 'Google Cloud']",2025-06-14 05:21:19
Software Development Engineer,Accenture,5 - 10 years,Not Disclosed,['Bengaluru'],"Project Role :Software Development Engineer\n\n\n\n\n\nProject Role Description :Analyze, design, code and test multiple components of application code across one or more clients. Perform maintenance, enhancements and/or development work.\n\n\n\nMust have skills :PySpark\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :BE\nProject Role :Software Development Engineer\n\nProject Role Description :Analyze, design, code and test multiple components of application code across one or more clients. Perform maintenance, enhancements and/or development work.\nMust have Skills :PySparkGood to Have Skills :No Industry SpecializationJob :Key Responsibilities :Overall 8 years of experience working in Data Analytics projects, Work on client projects to deliver AWS, PySpark, Databricks based Data engineering Analytics solutions Build and operate very large data warehouses or data lakes ETL optimization, designing, coding, tuning big data processes using Apache Spark Build data pipelines applications to stream and process datasets at low latencies Show efficiency in handling data - tracking data lineage, ensuring data quality, and improving discoverability of data.\nTechnical Experience :Minimum of 2 years of experience in Databricks engineering solutions on any of the Cloud platforms using PySpark Minimum of 5 years of experience years of experience in ETL, Big Data/Hadoop and data warehouse architecture delivery Minimum 3 year of Experience in one or more programming languages Python, Java, Scala Experience using airflow for the data pipelines in min 1 project 2 years of experience developing CICD pipelines using GIT, Jenkins, Docker, Kubernetes, Shell Scripting, Terraform. Must be able to understand ETL technologies and translate into Cloud (AWS, Azure, Google Cloud) native tools or Pyspark.\nProfessional Attributes :1 Should have involved in data engineering project from requirements phase to delivery 2 Good communication skill to interact with client and understand the requirement 3 Should have capability to work independently and guide the team.\nEducational Qualification:Additional Info :\n\nQualification\n\nBE",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['scala', 'pyspark', 'java', 'python', 'etl process', 'kubernetes', 'data warehousing', 'docker', 'git', 'spark', 'gcp', 'jenkins', 'shell scripting', 'hadoop', 'big data', 'etl', 'data analytics', 'microsoft azure', 'cloud platforms', 'warehouse', 'data engineering', 'data bricks', 'terraform', 'aws', 'ci cd pipeline']",2025-06-14 05:21:22
"Senior Staff Engineer, Mobile Flutter",Nagarro,10 - 15 years,Not Disclosed,[],"Total Experience 10+ years.\nStrong working experience in Dart and Flutter.\nSolid understanding of Mobile App Architecture (MVVM, BLoC, Provider, GetX).\nExperience with local databases like Sqflite or alternatives.\nHands-on experience in unit testing and test automation for Flutter apps.\nProven experience in building and deploying apps to the App Store and Google Play Store.\nFamiliarity with Git, GitHub/GitLab, CI/CD tools (eg, Jenkins, Bitrise, GitHub Actions).\nDeep knowledge of mobile app lifecycle, design principles, and clean architecture patterns (MVVM, MVC, etc)\nExpertise in mobile app performance optimization and security best practices.\nExperience in API integration , RESTful services, and handling JSON data.\nProficient in Version Control Systems like Git.\nProblem-solving mindset with the ability to tackle complex data engineering challenges.\nStrong communication and teamwork skills, with the ability to mentor and collaborate effectively.\nRESPONSIBILITIES:\nWriting and reviewing great quality code\nUnderstanding the clients business use cases and technical requirements and be able to convert them into technical design which elegantly meets the requirements\nMapping decisions with requirements and be able to translate the same to developers.\nIdentifying different solutions and being able to narrow down the best option that meets the clients requirements.\nDefining guidelines and benchmarks for NFR considerations during project implementation\nWriting and reviewing design document explaining overall architecture, framework, and high-level design of the application for the developers.\nReviewing architecture and design on various aspects like extensibility, scalability, security, design patterns, user experience, NFRs, etc, and ensure that all relevant best practices are followe'd.\nDeveloping and designing the overall solution for defined functional and non-functional requirements; and defining technologies, patterns, and frameworks to materialize it\nUnderstanding and relating technology integration scenarios and applying these learnings in projects\nResolving issues that are raised during code/review, through exhaustive systematic analysis of the root cause, and being able to justify the decision taken.\nCarrying out POCs to make sure that suggested design/technologies meet the requirements\n\n\nBachelor s or master s degree in computer science, Information Technology, or a related field.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'github', 'Version control', 'GIT', 'MVVM', 'JSON', 'MVC', 'Unit testing', 'High level design', 'Information technology']",2025-06-14 05:21:24
"Senior Staff Engineer, Mobile - Flutter",Nagarro,10 - 15 years,Not Disclosed,[],"Total Experience 10+ years.\nStrong working experience in Dart and Flutter.\nSolid understanding of Mobile App Architecture (MVVM, BLoC, Provider, GetX).\nExperience with local databases like Sqflite or alternatives.\nHands-on experience in unit testing and test automation for Flutter apps.\nProven experience in building and deploying apps to the App Store and Google Play Store.\nFamiliarity with Git, GitHub/GitLab, CI/CD tools (eg, Jenkins, Bitrise, GitHub Actions).\nDeep knowledge of mobile app lifecycle, design principles, and clean architecture patterns (MVVM, MVC, etc)\nExpertise in mobile app performance optimization and security best practices.\nExperience in API integration , RESTful services, and handling JSON data.\nProficient in Version Control Systems like Git.\nProblem-solving mindset with the ability to tackle complex data engineering challenges.\nStrong communication and teamwork skills, with the ability to mentor and collaborate effectively.\nRESPONSIBILITIES:\nWriting and reviewing great quality code\nUnderstanding the clients business use cases and technical requirements and be able to convert them into technical design which elegantly meets the requirements\nMapping decisions with requirements and be able to translate the same to developers.\nIdentifying different solutions and being able to narrow down the best option that meets the clients requirements.\nDefining guidelines and benchmarks for NFR considerations during project implementation\nWriting and reviewing design document explaining overall architecture, framework, and high-level design of the application for the developers.\nReviewing architecture and design on various aspects like extensibility, scalability, security, design patterns, user experience, NFRs, etc, and ensure that all relevant best practices are followe'd.\nDeveloping and designing the overall solution for defined functional and non-functional requirements; and defining technologies, patterns, and frameworks to materialize it\nUnderstanding and relating technology integration scenarios and applying these learnings in projects\nResolving issues that are raised during code/review, through exhaustive systematic analysis of the root cause, and being able to justify the decision taken.\nCarrying out POCs to make sure that suggested design/technologies meet the requirements\n\n\nBachelor s or master s degree in computer science, Information Technology, or a related field.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'github', 'Version control', 'GIT', 'MVVM', 'JSON', 'MVC', 'Unit testing', 'High level design', 'Information technology']",2025-06-14 05:21:26
"Senior Specialist, Java/Angular JS Engineer",Merck Sharp & Dohme (MSD),5 - 15 years,Not Disclosed,['Hyderabad'],"Based in Hyderabad, join a global healthcare biopharma company and be part of a 130- year legacy of success backed by ethical integrity, forward momentum, and an inspiring mission to achieve new milestones in global healthcare.\nBe part of an organization driven by digital technology and data-backed approaches that support a diversified portfolio of prescription medicines, vaccines, and animal health products.\nDrive innovation and execution excellence. Be a part of a team with passion for using data, analytics, and insights to drive decision-making, and which creates custom software, allowing us to tackle some of the worlds greatest health threats.\nOur Technology Centers focus on creating a space where teams can come together to deliver business solutions that save and improve lives. An integral part of our company s IT operating model, Tech Centers are globally distributed locations where each IT division has employees to enable our digital transformation journey and drive business outcomes. These locations, in addition to the other sites, are essential to supporting our business and strategy.\nA focused group of leaders in each Tech Center helps to ensure we can manage and improve each location, from investing in growth, success, and well-being of our people, to making sure colleagues from each IT division feel a sense of belonging to managing critical emergencies. And together, we must leverage the strength of our team to collaborate globally to optimize connections and share best practices across the Tech Centers.\nRole Overview\nWe are seeking a highly skilled and experienced Senior Multistack Java Developer to join our team and technically lead a group of talented developers. This role will involve close collaboration with a large team, focusing on enhancing, developing, and maintaining enterprise-grade applications. The ideal candidate will have 8-15 years of strong hands-on experience in hardcore development, expertise in Java,and front-end frameworks like React and Angular, in a continuous release environment. This position also requires proactive leadership, and expertise in managing production outages and troubleshooting.\nAs a Software Engineer focusing on Java/Angular JS you will design, develop, and maintain software systems. This role involves both creative and analytical skills to solve complex problems and create efficient, reliable software. You will use your expertise in requirements analysis, programming languages, software development methodologies, and tools to build and deliver software products that meet the needs of businesses, organizations, or end- users. You will work with other engineers, product managers and delivery leads, to design systems, determine functional and non-functional needs and implement solutions accordingly. You should be ready to work independently as well as in a team.\nWhat will you do in this role\nDesign, code, verify, test, document, amend and refactor moderately complex applications and software configurations for deployment in collaboration with cross-disciplinary teams across various regions worldwide.\nDesign test cases and test scripts under own direction, mapping back to pre-determined criteria, recording and reporting test outcomes. Participate in requirement, design and specification reviews. Perform manual and automation testing.\nElicit requirements for systems and software life cycle working practices and automation. Prepare design options for the working environment of methods, procedures, techniques, tools, and people. Utilize systems and software life cycle working practices for software components and micro-services. Deploy automation to achieve well-engineered and secure outcome.\nWork within a matrix organizational structure, reporting to both the functional manager and the Product manager.\nParticipate in Product planning, execution, and delivery, ensuring alignment with both functional and Product goals.\nLead and mentor a team of Java Microservices and front-end developers, providing technical guidance, code reviews, and fostering a culture of continuous learning.\nStay actively involved in development tasks, ensuring the delivery of high-quality, maintainable code.\nDesign and develop scalable systems using Java, Spring Framework, and Spring Boot for backend development.\nDevelop rich, responsive user interfaces using React, Angular to deliver seamless user experiences.\nEnsure optimal integration of front-end and back-end components for customer-facing applications\nDevelop and consume Microservices, REST/SOAP APIs, and XML web services.\nUtilize tools like Splunk, ELK, and database queries to monitor, troubleshoot, and resolve system issues.\nLeverage tools like Jenkins, Ansible, and Terraform for automated deployments.\nWork on cloud platforms like AWS, implementing and supporting cloud-based solutions.\nWhat should you have\nBachelor s degree in information technology, Computer Science or any Technology stream.\n~ 8-15 years of experience. 5+ years of experience with Java and Java frameworks and libraries (such as Spring Framework, Spring Boot, Hibernate, JEE, JDBC, JMS, JMX), relational databases, Kafka/RabbitMQ, and deployment to application servers.\nDevelop and maintain microservices-based applications using Spring Boot.\nDesign and implement RESTful APIs for seamless communication between services.\nExperience with enterprise-level databases both SQL and No-SQL such as Oracle, PostgreSQL, MongoDB etc\nAutomated testing frameworks (JUnit, TestNG, Selenium, etc.)\nMonitoring and log tools (Splunk, ELK, Prometheus, Grafana)\nFamiliarity with AWS Cloud, CI/CD pipelines (Jenkins, GitLab CI, etc.), and automation tools such as Jenkins, Ansible, and Terraform.\nProficiency in cloud platforms (e.g., AWS, Azure, Google Cloud) and containerization technologies (e.g., Docker, Kubernetes)..\nExcellent debugging, troubleshooting, and analytical skills.\nEffective verbal and written communication skills.\nFamiliarity with modern product development practices - Agile, Scrum, test driven development, UX, design thinking.\nHands-on experience with DevOps practices (Git, infrastructure as code, observability, continuous integration/continuous deployment - CI/CD).\nCloud-native, ideally AWS certified.\nProduct and customer-centric approach\nGood to Have Skills:\nExperience with any front-end technologies like React, Angular for building responsive user interfaces.\nUnderstanding of security best practices and data protection methodologies.\nExperience with Agile methodologies and tools like Jira or Trello.\nCertification of any of public cloud (AWS, GCP, Azure, OCI)\nA passionate commitment to learning about business domains and emerging technologies.\nOur technology teams operate as business partners, proposing ideas and innovative solutions that enable new organizational capabilities. We collaborate internationally to deliver services and solutions that help everyone be more productive and enable innovation.\nWho we are\nFor more than a century, we have been inventing for life, bringing forward medicines and vaccines for many of the worlds most challenging diseases. Today, our company continues to be at the forefront of research to deliver innovative health solutions and advance the prevention and treatment of diseases that threaten people and animals around the world.\nWhat we look for\nImagine getting up in the morning for a job as important as helping to save and improve lives around the world. Here, you have that opportunity. You can put your empathy, creativity, digital mastery, or scientific genius to work in collaboration with a diverse group of colleagues who pursue and bring hope to countless people who are battling some of the most challenging diseases of our time. Our team is constantly evolving, so if you are among the intellectually curious, join us and start making your impact today\nCurrent Employees apply HERE\nCurrent Contingent Workers apply HERE\nSearch Firm Representatives Please Read Carefully\nEmployee Status:\nRegular\nRelocation:\nVISA Sponsorship:\nTravel Requirements:\nFlexible Work Arrangements:\nHybrid\nShift:\nValid Driving License:\nHazardous Material(s):\n\nRequired Skills:\nAnimal Vaccination, Computer Science, Data Engineering, Data Visualization, Design, Design Applications, DevOps, Digital Technology, Digital Transformation, JavaScript, PostgreSQL, Requirements Analysis, Social Collaboration, Software Configurations, Software Development, Software Development Life Cycle (SDLC), Solution Architecture, Spring Framework, System Designs, Systems Integration, Systems Troubleshooting, Technical Consulting, Test Automation, Testing, Web Development",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Solution architecture', 'JMS', 'Hibernate', 'Front end', 'XML', 'Consulting', 'Healthcare', 'data visualization', 'Oracle', 'SQL']",2025-06-14 05:21:29
Support Engineer I,Amazon,1 - 6 years,Not Disclosed,['Bengaluru'],"Amazon Payment Merchant Category team at Amazon India Development Center, Bangalore is looking for a Support Engineer to help scale and build the next generation of Payments platform and product from the ground up. This is a rare opportunity to be part of a team that will be responsible for building successful, sustainable and strategic business for Amazon, from the ground up!\nYou will get the opportunity to code on almost all key pages on brand new Payments stack building features and improving business metrics. This team will work on diverse technology stack from SOA, UI frameworks, big-data and Gen AI.\nThe ideal candidate will be working to shape the product and will be actively involved in defining key product features that impact the business. You will work to evolve the design and implementation of the products owned by this team. You will be responsible to set up and hold a high software quality bar in a highly technical team of Software Engineers.\n\n\nWork closely with senior engineers to design, implement and deploy applications that impact the Amazon.in business with an emphasis on Mobile, Payments, and e-Commerce website development.\nOwn the delivery of an integral piece of a system or application.\nManagement and execution against project plans and delivery commitments\nAssist directly and indirectly in the continual hiring and development of technical talent.\nCreate and execute appropriate quality plans, project plans, test strategies and processes for development activities in concert with business and project management efforts.\nThe ideal candidate will be a leader, quick learner and be able to work independently. He/she should be able to operate in a very fast paced environment where time to hit market is super critical. The candidate will need to balance technical leadership and savvy with strong business judgment to make the right decisions about technology choices. 1+ years of software development, or 1+ years of technical support experience\nExperience troubleshooting and debugging technical systems\nExperience scripting in modern program languages Knowledge of computer science fundamentals such as object-oriented design, operating systems, algorithms, data structures, and complexity analysis",,,,"['Computer science', 'Object oriented design', 'Application management', 'SOA', 'Project management', 'Debugging', 'Data structures', 'software quality', 'Troubleshooting', 'Technical support']",2025-06-14 05:21:31
Software Engineer III,Walmart,2 - 7 years,Not Disclosed,['Bengaluru'],"Position Summary...\nWhat youll do...\nAbout the Team:\nAs a member of the Buybox prod Eng. group, you ll be responsible for driving site/customer impacting production incidents, with tactical/long-term solutions identified, working with cross functional teams. You ll independently handle high impact, critical software/systems monitoring issues, troubleshoot business and production issues, developing automations/tools leading to Operational excellence. As a member of the team, you ll be able to say that you work for the world s largest retailer and contribute to the development to best-in-class methodologies that impacted perception and drastically changed business as we know it.\nWhat You ll Do:\nSupporting java full stack backend application system components in a massively scalable, high performance, multi-tenant, international eCommerce platform with multiple micro-services deployed in cloud environment, root causing every reactive/proactive production issues.\nLeads and participates in medium- to large-scale, complex, cross-functional projects\nPartners with architects and development leads to come up with high level design to accelerate omni customer experience, recommending out-of-box engineering best practices.\nPro-Actively identifies areas to drive automation/speed/innovation\nTroubleshoots business and production issues by gathering information (for example, issue, impact, criticality, possible root cause); performing root cause analysis to reduce future issues; engaging support teams to assist in the resolution of issues; developing solutions; driving the development of an action plan; performing actions as designated in the plan; interpreting the results to determine further action; and completing online documentation.\nProvides support to the business by responding to user questions, concerns, and issues (for example, technical feasibility, implementation strategies); researching and identifying needed solutions; determining implementation designs; providing guidance regarding implications of new and enhanced systems; identifying short and long term solutions; and directing users to appropriate contacts for issues outside of associates domain.\nAssists in providing guidance to small groups of 5 to 6 engineers, including offshore associates, for assigned Engineering projects by proving pertinent documents, directions, examples, and timeline.\nDemonstrates up-to-date expertise and applies this to the development, execution, and improvement of action plans by providing expert advice and guidance to others in the application of information and best practices; supporting and aligning efforts to meet customer and business needs; and building commitment for perspectives and rationales.\nModels compliance with company policies and procedures and supports company mission, values, and standards of ethics and integrity by incorporating these into the development and implementation/Support of business plans; using the Open Door Policy; and demonstrating and assisting others with how to apply these in executing business processes and practices.\nProvides and supports the implementation of business solutions by building relationships and partnerships with key stakeholders; identifying business needs; determining and carrying out necessary processes and practices; monitoring progress and results; recognizing and capitalizing on improvement opportunities; and adapting to competing demands, organizational changes, and new responsibilities.\nWhat you ll Bring ...\nExpertise in Java/Spring based Rest web services and Python Experience development and its maintenance\nExpertise in AI ops to build based on LLM\nStrong Analytical thinking/troubleshooting in complex ecosystem problem solving skills\nExpertise in Application container handling using Kubernetes and Docker\nExperience in Asynchronous inter micro-service communication Messaging channels like Kafka\nExperience in Reviewing and tuning the SQL scripts for data handling, in Cassandra, Big Data\nExperience in Developing Spark jobs, Java utilities, Python scripts, for operational success\nHands on experience debugging 5xx and 4xx service errors\nAbility to work with application developers to find root cause analysis, tactical and permanent solutions to critical business and customer impacting issues\nExcellent Written and Oral communication skills\nExperience in trouble shooting production incidents on eCommerce retailer site and identify root cause, working with all business and Engineering stakeholders to bring it to closure with action items and define timeline for tactical and permanent fixes.\nAbility to drive critical production incident calls, communicating up to the point & summarizing action plans for each owners and follow-up until closure\nExperience with creating Alerting and Monitoring Dashboards, as Subject Matter Expert\nStrong in developing Innovation strategies, processes, automation, failover experience\nExposure to driving the execution of multiple business plans and projects\nExperience in managing infrastructure scaling, setup and decommissioning\nStrong Hands-on experience in both public (Azure/GCP) and private cloud experience, planning and driving efficiencies\nExperience with best in class analytics & monitoring platform like Grafana/Dynatrace/MMS/Splunk metrics and dashboards\nExperience with Machine Learning model\nAbility to take right priority decision and run the operational excellence with innovative ideas, without much guidance/supervision\nDocumenting SOPs for repetitive issues, building knowledge base articles for team s benefit\nAbout Walmart Global Tech\n.\n.\nFlexible, hybrid work\n.\nBenefits\n.\nBelonging\n.\n.\nEqual Opportunity Employer\nWalmart, Inc., is an Equal Opportunities Employer - By Choice. We believe we are best equipped to help our associates, customers and the communities we serve live better when we really know them. That means understanding, respecting and valuing unique styles, experiences, identities, ideas and opinions - while being inclusive of all people.\nMinimum Qualifications...\nMinimum Qualifications:Option 1: Bachelors degree in computer science, information technology, engineering, information systems, cybersecurity, or related area and 2years experience in software engineering or related area at a technology, retail, or data-driven company.\n\nOption 2: 4 years experience in software engineering or related area at a technology, retail, or data-driven company.\nPreferred Qualifications...\nCertification in Security+, Network+, GISF, GSEC, CISSP, or CCSP, Master s degree in Computer Science, Information Technology, Engineering, Information Systems, Cybersecurity, or related area\nPrimary Location...",Industry Type: Retail,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Automation', 'Networking', 'Debugging', 'Machine learning', 'Information technology', 'Analytics', 'Monitoring', 'SQL', 'Python']",2025-06-14 05:21:34
"Data Scientist,VP",NatWest Markets,10 - 12 years,Not Disclosed,"['Gurugram', 'Bengaluru']","Join us as a Data Scientist\nIn this role, you ll drive and embed the design and implementation of data science tools and methods, which harness our data to drive market-leading purpose customer solutions\nDay-to-day, you ll act as a subject matter expert and articulate advanced data and analytics opportunities, bringing them to life through data visualisation\nIf you re ready for a new challenge, and are interested in identifying opportunities to support external customers by using your data science expertise, this could be the role for you\nWere offering this role at vice president level\nWhat you ll do\nWe re looking for someone to understand the requirements and needs of our business stakeholders. You ll develop good relationships with them, form hypotheses, and identify suitable data and analytics solutions to meet their needs and to achieve our business strategy.\nYou ll be maintaining and developing external curiosity around new and emerging trends within data science, keeping up to date with emerging trends and tooling and sharing updates within and outside of the team.\nYou ll also be responsible for:\nProactively bringing together statistical, mathematical, machine-learning and software engineering skills to consider multiple solutions, techniques, and algorithms\nImplementing ethically sound models end-to-end and applying software engineering and a product development lens to complex business problems\nWorking with and leading both direct reports and wider teams in an Agile way within multi-disciplinary data to achieve agreed project and Scrum outcomes\nUsing your data translation skills to work closely with business stakeholders to define business questions, problems or opportunities that can be supported through advanced analytics\nSelecting, building, training, and testing complex machine models, considering model valuation, model risk, governance, and ethics throughout to implement and scale models\nThe skills you ll need\nTo be successful in this role, you ll need evidence of project implementation and work experience gained in a data-analysis-related field as part of a multi-disciplinary team. We ll also expect you to hold an undergraduate or a master s degree in Data science, Statistics, Computer science, or related field .\nYou ll also need an experience of 10 years with statistical software, database languages, big data technologies, cloud environments and machine learning on large data sets. And we ll look to you to bring the ability to demonstrate leadership, self-direction and a willingness to both teach others and learn new techniques.\nAdditionally, you ll need:\nExperience of deploying machine learning models into a production environment\nProficiency in Python and relevant libraries such as Pandas, NumPy, Scikit-learn coupled with experience in data visualisation tools.\nExtensive work experience with AWS Sage maker , including expertise in statistical data analysis, machine learning models, LLMs, and data management principles\nEffective verbal and written communication skills , the ability to adapt communication style to a specific audience and mentoring junior team members",Industry Type: Banking,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Data analysis', 'data science', 'Data management', 'Machine learning', 'Agile', 'Scrum', 'SAGE', 'Business strategy', 'Python']",2025-06-14 05:21:36
Quality Engineer (Tester),Accenture,3 - 8 years,Not Disclosed,['Bengaluru'],"Project Role :Quality Engineer (Tester)\n\n\n\n\n\nProject Role Description :Enables full stack solutions through multi-disciplinary team planning and ecosystem integration to accelerate delivery and drive quality across the application lifecycle. Performs continuous testing for security, API, and regression suite. Creates automation strategy, automated scripts and supports data and environment configuration. Participates in code reviews, monitors, and reports defects to support continuous improvement activities for the end-to-end testing process.\n\n\n\nMust have skills :Selenium\n\n\n\n\nGood to have skills :Payments Fundamentals, Java Standard Edition, Microsoft SQL ServerMinimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Quality Engineer (Tester), you will enable full stack solutions through multi-disciplinary team planning and ecosystem integration to accelerate delivery and drive quality across the application lifecycle. You will perform continuous testing for security, API, and regression suite, create automation strategy, automated scripts, and support data and environment configuration. Participate in code reviews, monitor, and report defects to support continuous improvement activities for the end-to-end testing process.\nRoles & Responsibilities:- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Collaborate with cross-functional teams to ensure quality throughout the software development lifecycle.- Develop and execute automated test scripts for regression testing.- Analyze test results and provide feedback to the development team.- Identify and report software defects to assist in maintaining product quality.- Implement and maintain test automation frameworks for efficient testing processes.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Selenium.- Good To Have\n\n\n\n\nSkills:\nExperience with Java Standard Edition, Microsoft SQL Server, Payments Fundamentals.- Strong understanding of test automation principles and best practices.- Experience in creating and maintaining automated test scripts.- Knowledge of software testing methodologies and tools.- Ability to troubleshoot and debug issues in test automation scripts.\n""Data Testing (primary skill ) and UI automation testing (secondary skill). Candidate should be having experience in both primary(80% -85%) and secondary skill (20%-15%). Must have skills:Big data testing with python and spark Cloud testing hands-on (AWS is preferred)Experience in SPARK_SQL (Not looking for RDBMS SQL) Python scripting experienceUI Automation testing experience ""- Cypress, JavaScript, Payments Fundamental.- Java, SQL, API integration, Selenium and Cucumber framework.Exposure working on TeamCity, JIRA & GIT.- Expertise on Java, Spring, PL/SQL, SQL, Multithreading, Unit TestingExposure working on TeamCity, JIRA & GITShould have a basic understanding of Equities Derivatives\nAdditional Information:- The candidate should have a minimum of 3 years of experience in Selenium.- This position is based at our Pune office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'software testing', 'sql', 'spring', 'java', 'rdbms', 'ui automation testing', 'api integration', 'automation testing', 'big data testing', 'cloud testing', 'javascript', 'sql server', 'plsql', 'cypress', 'java standard edition', 'git', 'selenium', 'spark', 'multithreading', 'teamcity', 'aws', 'jira']",2025-06-14 05:21:38
Data Consultant-Elastic Developer,Kyndryl,10 - 15 years,Not Disclosed,['Bengaluru'],"Who We Are\nAt Kyndryl, we design, build, manage and modernize the mission-critical technology systems that the world depends on every day. So why work at Kyndryl? We are always moving forward – always pushing ourselves to go further in our efforts to build a more equitable, inclusive world for our employees, our customers and our communities.\n\nThe Role\nAs an ELK (Elastic, Logstash & Kibana) Data Engineer, you would be responsible for developing, implementing, and maintaining the ELK stack-based solutions for Kyndryl’s clients. This role would be responsible to develop efficient and effective, data & log ingestion, processing, indexing, and visualization for monitoring, troubleshooting, and analysis purposes.",,,,"['kubernetes', 'analytical', 'logstash', 'elastic search', 'scripting', 'aix', 'automation', 'gcp', 'devops', 'linux', 'writing', 'aix environment', 'debugging', 'shell scripting', 'mysql', 'prometheus', 'kibana', 'python', 'elk', 'beats', 'microsoft azure', 'nosql', 'servicenow', 'grafana', 'microsoft windows', 'troubleshooting', 'splunk', 'aws']",2025-06-14 05:21:41
Software Engineer III,JPMorgan Chase Bank,8 - 13 years,Not Disclosed,['Bengaluru'],"As an Experienced Software Engineer at JPMorgan Chase within the Global Technology team, you serve as member of an agile team to design and deliver trusted market-leading technology products in a secure, stable, and scalable way. Depending on the team that you join, you could be developing mobile features that give our customers and clients more control over how they bank with us, strategizing on how big data can make our trading systems quicker, creating the next innovation in payments for merchants, or supporting the integration of our private and public cloud platforms.\n\nJob Responsibilities\nParticipates in, design and develop scalable and resilient systems using Java to contribute to continual, iterative improvements for product teams\nExecutes software solutions, design, development, and technical troubleshooting\nCreates secure and high-quality production code and maintains algorithms that run synchronously with appropriate systems\nProduces or contributes to architecture and design artifacts for applications while ensuring design constraints are met by software code development\nGathers, analyzes, synthesizes, and develops visualizations and reporting from large, diverse data sets in service of continuous improvement of software applications and systems\nIdentifies hidden problems and patterns in data and uses these insights to drive improvements to coding hygiene and system architecture\nContributes to software engineering communities of practice and events that explore new and emerging technologies\nAdds to team culture of diversity, equity, inclusion, and respect\nRequired qualifications, capabilities, and skills\nMin 8+ years strong hands on experience as a JAVA Programmer\nHands-on practical experience in system design, application development, testing and operational stability\nProficient in coding in Java language\nExperience in developing, debugging, and maintaining code in a large corporate environment with one or more modern programming languages and database querying languages\nOverall knowledge of the Software Development Life Cycle\nUnderstanding of agile methodologies such as CI/CD, Application Resiliency, and Security\nKnowledge of software applications and technical processes within a technical discipline (eg, cloud, artificial intelligence, machine learning, mobile, etc)\nPreferred qualifications, capabilities, and skills\nFamiliarity with modern front-end technologies\nExposure to cloud technologies",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['System architecture', 'Front end', 'Coding', 'Artificial Intelligence', 'Debugging', 'Machine learning', 'Agile', 'System design', 'Application development', 'Troubleshooting']",2025-06-14 05:21:43
"Software Development Engineer II, RISC",Amazon,3 - 8 years,Not Disclosed,['Bengaluru'],"Want to join a team that protects and improves the buyer experience of millions Amazon customers and builds earths most customer-centric sellers daily using innovative technology including machine learning, data mining and big data analytics, cloud computing services, and highly available/scalable distributed systems that support hundreds of millions of transactions across the globe\n\nWe have an exciting opportunity with the Regulatory Intelligence, Safety, and Compliance (RISC) engineering team, to architect and build next-generation engineering systems to quickly and accurately identify and mitigate product safety issues and potential risks to the customer experience.\n\nAs a Software Development Engineer, you will work with your team of highly skilled software, data, and ML engineers to invent, design, build and manage highly scalable distributed systems that provide availability, scalability and latency guarantees. You will work with your internal customers to balance customer requirements with team requirements and help your team and business evolve, by working with LLMs and large data sets. You will be using the latest AI, AWS and industry technologies to deliver a one-stop risk identification and remediation ecosystem for Amazon, keeping our customers safe and products compliant, building the software creating the world s most trustworthy data set on everything companies and customers need to know related to the safety and compliance of products and chains.\n\nEach and every person buying, selling, or handling Amazon products will be your customer.\n\nAs a member of this growing team, you ll be able to build the groundwork and influence its direction for the years to come. Our work cuts across various disciplines from delivering an awesome user experience via great UI/UX, to building massively scalable backend systems to support the most high-traffic pages on Amazon.com, to analytical and feedback systems which give us data-driven customer insights, to using machine learning and AI to influence recommendations and marketing. If you have a passion for consumer-facing applications, and are obsessed with customer experience, we want you!\n\nIf you d like to make a real-world difference by working hard, having fun, and making history, this is the team for you!\n\n\nIn this role you will:\nHelp define the system architecture, own and implement specific components, and help shape the overall experience\nCollaborate closely with product managers, UX designers, and other SDE team members to help define the scope of the product\nTake responsibility for technical problem solving, creatively meeting product objectives, and developing best practices\nDemonstrate cross-functional resource interaction to accomplish your goals\nWrite high-quality, efficient, testable code in Java and other object-oriented languages\nDesign Amazon-scale tools to facilitate internal business\nBuild highly available, secure, and low-latency systems\nMentor other developers\nFind out what it takes to engineer systems for ""Amazon Scale""\nDesign and build microservices\nOwn and operate the systems that you build based on real-time customer data and demanding service-level agreements\nContribute to planning, design, implementation, testing, operations, and process improvement\n\nA day in the life\nHigh-level designs, cross-team alignment, long-term architectural roadmap and technical strategy, understanding the business domain and proposing solutions to address customer and business problems, helping scope and analyze product requirements, mentorship, reviewing CRs, writing high-quality code to be an example for the team. 3+ years of non-internship professional software development experience\n2+ years of non-internship design or architecture (design patterns, reliability and scaling) of new and existing systems experience\n3+ years of Video Games Industry (supporting title Development, Release, or Live Ops) experience\nExperience programming with at least one software programming language 3+ years of full software development life cycle, including coding standards, code reviews, source control management, build processes, testing, and operations experience\nBachelors degree in computer science or equivalent",,,,"['Computer science', 'System architecture', 'Cloud computing', 'Backend', 'Coding', 'Analytical', 'Machine learning', 'Data mining', 'Internship', 'Distribution system']",2025-06-14 05:21:46
Principal Data Solution Architect,Goodyear,12 - 16 years,Not Disclosed,['Hyderabad'],"A\nRoles & Responsibilities:\nResponsible for designing/building data products, logical data layers, data streams, algorithms and reporting systems (i.e. dashboards, front ends). Secure correct designed solution, performance and scalability Considering appropriate cost control. Link data product design with DevOps and Infrastructure. Act as a reference within and outside DEA team.\nBe a technical partner to Data Engineer(s) regarding digital product implementation. Responsible for developing visualizations for complex data sets. Provide guidance to internal DEA associates, IT and business users on data solutions available and related guidelines/standards.\nFamiliarity with neuro-linguistic programming (NLP) and other advanced techniques to simplify interfaces. Work across data analytics projects to provide support in data analytics methodology, processes and standards. Create/Deliver user training.\nResponsible for user acceptance testing (in collaboration with business/demand owners). Work closely with internal team members, on/off-shore contractors and strategic business unit (SBU) IT/business associates to develop guiding principles/best practices for determining solution architecture that will be needed for a particular information requirement.\nDevelop an in-depth understanding of DEA data for communication and support of business/SBU support. Work with the SBU counterparts to develop/manage roles and processes for on-going user support and solution architecture administration. Participate/Assist in conducting user group meetings\nSkills Required:\nProven success interfacing with the business community and identifying business requirements\nMinimum 5+ years of experience in developing advanced data pipelines in Python & SQL.\nStrong experience of configuring and deploying AWS infrastructure using Terraform, etc.\nStrong experience of developing DevOps pipelines using GitHub / GitHub Actions.\nPrior experience of working with Snowflake and/or other Data Warehouses.\nSome exposure to AI/ML based applications is good to have.\nExcellent analytical and problem-solving skill\nExpertise in data warehouse concepts, methodology and technology\nExcellent interpersonal/communication skills - Ability to work in a dynamic team environment and be comfortable/credible interacting with both technical/business organizations and executive management\nThe highest-level of personal ethics - Ability to keep sensitive information confidential - Unquestionable integrity and character\nKnowledgeable in current and possible future policies, practices, trends, technologies and information\nUnderstands and works with the organization's mission, operations, structure and goals\nWorking knowledge of Goodyear operational systems (e.g. SAP/R3) - Desired",Industry Type: Auto Components (Tyre),Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Snowflake', 'AWS', 'Python', 'Terraform', 'Github', 'Artificial Intelligence', 'AI', 'Machine Learning', 'SQL', 'ML']",2025-06-14 05:21:48
Starburst Engineer,Luxoft,0 - 4 years,Not Disclosed,['Pune'],"Project description\nYou will be working in a global team that manages and performs a global technical control.\nYou'll be joining Assets Management team which is looking after asset management data foundation and operates a set of in-house developed tooling.\nAs an IT engineer you'll play an important role in ensuring the development methodology is followed, and lead technical design discussions with the architects. Our culture centers around partnership with our businesses, transparency, accountability and empowerment, and passion for the future.\n\nResponsibilities\n\nDesign, develop, and maintain scalable data solutions using Starburst.\n\nCollaborate with cross-functional teams to integrate Starburst with existing data sources and tools.\n\nOptimize query performance and ensure data security and compliance.\n\nImplement monitoring and alerting systems for data platform health.\n\nStay updated with the latest developments in data engineering and analytics.\n\nSkills\nMust have\n\nBachelor's degree or Masters in a related technical field; or equivalent related professional experience.\n\nPrior experience as a Software Engineer applying new engineering principles to improve existing systems including leading complex, well defined projects.\n\nStrong knowledge of Big-Data Languages including:\n\nSQL\n\nHive\n\nSpark/Pyspark\n\nPresto\n\nPython\n\nStrong knowledge of Big-Data Platforms, such as:o The Apache Hadoop ecosystemo AWS EMRo Qubole or Trino/Starburst\n\nGood knowledge and experience in cloud platforms such as AWS, GCP, or Azure.\n\nContinuous learner with the ability to apply previous experience and knowledge to quickly master new technologies.\n\nDemonstrates the ability to select among technology available to implement and solve for need.\n\nAble to understand and design moderately complex systems.\n\nUnderstanding of testing and monitoring tools.\n\nAbility to test, debug, fix issues within established SLAs.\n\nExperience with data visualization tools (e.g., Tableau, Power BI).\n\nUnderstanding of data governance and compliance standards.\n\nNice to have\n\nData Architecture & EngineeringDesign and implement efficient and scalable data warehousing solutions using Azure Databricks and Microsoft Fabric.\n\nBusiness Intelligence & Data VisualizationCreate insightful Power BI dashboards to help drive business decisions.\n\nOther\n\nLanguages\n\nEnglishC1 Advanced\n\nSeniority\n\nSenior",Industry Type: Legal,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['hive', 'python', 'pyspark', 'sql', 'spark', 'azure databricks', 'microsoft azure', 'data warehousing', 'power bi', 'data architecture', 'business intelligence', 'data engineering', 'dashboards', 'tableau', 'apache', 'gcp', 'debugging', 'hadoop', 'data visualization', 'aws', 'engineering design', 'presto']",2025-06-14 05:21:50
Quality Engineer (Tester),Accenture,5 - 10 years,Not Disclosed,['Bengaluru'],"Project Role :Quality Engineer (Tester)\n\n\n\n\n\nProject Role Description :Enables full stack solutions through multi-disciplinary team planning and ecosystem integration to accelerate delivery and drive quality across the application lifecycle. Performs continuous testing for security, API, and regression suite. Creates automation strategy, automated scripts and supports data and environment configuration. Participates in code reviews, monitors, and reports defects to support continuous improvement activities for the end-to-end testing process.\n\n\n\nMust have skills :Data Warehouse ETL Testing\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Quality Engineer (Tester), you will enable full stack solutions through multi-disciplinary team planning and ecosystem integration to accelerate delivery and drive quality across the application lifecycle. Your typical day will involve performing continuous testing for security, API, and regression suite. You will create automation strategy, automated scripts, and support data and environment configuration. Additionally, you will participate in code reviews, monitor, and report defects to support continuous improvement activities for the end-to-end testing process.\nRoles & Responsibilities:- Expected to be an SME, collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Conduct thorough testing of data warehouse ETL processes.- Develop and execute test cases, test plans, and test scripts.- Identify and document defects, issues, and risks.- Collaborate with cross-functional teams to ensure quality standards are met.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Data Warehouse ETL Testing.- Strong understanding of SQL and database concepts.- Experience with ETL tools such as Informatica or Talend.- Knowledge of data warehousing concepts and methodologies.- Experience in testing data integration, data migration, and data transformation processes.\nAdditional Information:- The candidate should have a minimum of 5 years of experience in Data Warehouse ETL Testing.- This position is based at our Bengaluru office.- 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data warehousing', 'data warehouse etl testing', 'sql', 'database creation', 'etl', 'software testing', 'test case execution', 'talend', 'test cases', 'data migration', 'etl testing', 'quality engineering', 'quality testing', 'test planning', 'informatica', 'data integration']",2025-06-14 05:21:53
Software Development Engineer,Accenture,15 - 20 years,Not Disclosed,['Coimbatore'],"Project Role :Software Development Engineer\n\n\n\n\n\nProject Role Description :Analyze, design, code and test multiple components of application code across one or more clients. Perform maintenance, enhancements and/or development work.\n\n\n\nMust have skills :SAP Data Migration\n\n\n\n\nGood to have skills :SAP HANA CloudMinimum\n\n\n\n12 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Software Development Engineer, you will analyze, design, code, and test multiple components of application code across one or more clients. You will also perform maintenance, enhancements, and/or development work.\nRoles & Responsibilities:- Expected to be an SME.- Collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Expected to provide solutions to problems that apply across multiple teams.- Lead and mentor junior team members.- Conduct code reviews and ensure code quality.- Stay updated with the latest technologies and trends.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in SAP Data Migration.- Good To Have\n\n\n\n\nSkills:\nExperience with SAP HANA Cloud.- Strong understanding of data migration processes.- Experience in analyzing, designing, and testing software components.- Knowledge of SAP systems and architecture.- Ability to troubleshoot and resolve technical issues efficiently.\nAdditional Information:- The candidate should have a minimum of 12 years of experience in SAP Data Migration.- This position is based at our Bengaluru office.- A 15 years full-time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data migration', 'sap hana', 'sap data migration', 'sap', 'software development', 'lsmw', 'sap sd', 'data warehousing', 'sql', 'sap s hana', 'idocs', 'java', 'bapi', 'sap data services', 'sap mm', 'code review', 'etl', 'idoc', 'cransoft', 'sap bods', 'sql server', 'data quality', 'bods', 'sap abap', 'data integration']",2025-06-14 05:21:56
Software Development Engineer,Accenture,3 - 8 years,Not Disclosed,['Bengaluru'],"Project Role :Software Development Engineer\n\n\n\n\n\nProject Role Description :Analyze, design, code and test multiple components of application code across one or more clients. Perform maintenance, enhancements and/or development work.\n\n\n\nMust have skills :SAP BTP Datasphere\n\n\n\n\nGood to have skills :SAP BTP Data Warehouse CloudMinimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Software Development Engineer, you will analyze, design, code, and test multiple components of application code across one or more clients. You will perform maintenance, enhancements, and/or development work, contributing to the overall success of the projects.\nRoles & Responsibilities:- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work-related problems.- Collaborate with team members to analyze, design, and develop software solutions.- Conduct code reviews and provide feedback to ensure code quality.- Troubleshoot and debug software applications to resolve issues.- Stay updated with industry trends and technologies to suggest improvements.- Document software specifications, user manuals, and technical documentation.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in SAP BTP Datasphere.- Strong understanding of SAP BTP Data Warehouse Cloud.- Experience with data integration and data modeling.- Knowledge of cloud computing principles and services.- Familiarity with Agile development methodologies.\nAdditional Information:- The candidate should have a minimum of 3 years of experience in SAP BTP Datasphere.- This position is based at our Bengaluru office.- A 15 years full-time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data modeling', 'cloud computing', 'sap', 'warehouse', 'agile', 'css', 'c++', 'sql', 'spring', 'java', 'git', 'code review', 'html', 'mysql', 'data structures', 'c#', 'python', 'software development', 'c', 'business analysis', 'javascript', 'sql server', 'troubleshooting', 'technical documentation', 'data integration']",2025-06-14 05:21:58
Software Development Engineer,Accenture,3 - 8 years,Not Disclosed,['Bengaluru'],"Project Role :Software Development Engineer\n\n\n\n\n\nProject Role Description :Analyze, design, code and test multiple components of application code across one or more clients. Perform maintenance, enhancements and/or development work.\n\n\n\nMust have skills :Microsoft Azure Data Services\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Software Development Engineer, you will engage in a dynamic work environment where you will analyze, design, code, and test various components of application code across multiple clients. Your day will involve collaborating with team members to ensure the successful implementation of software solutions, while also performing maintenance and enhancements to existing applications. You will be responsible for delivering high-quality code and contributing to the overall success of the projects you are involved in, ensuring that client requirements are met effectively and efficiently.\nRoles & Responsibilities:- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Collaborate with cross-functional teams to gather requirements and translate them into technical specifications.- Conduct code reviews to ensure adherence to best practices and coding standards.\nProfessional & Technical\n\n\n\n\nSkills:\n-Must have experience on Azure Synapse and Pyspark.- Must To Have\n\n\n\n\nSkills:\nProficiency in Microsoft Azure Data Services.- Strong understanding of cloud computing concepts and architecture.- Experience with data integration and ETL processes.- Familiarity with database management systems and data modeling.- Ability to troubleshoot and resolve technical issues efficiently.-Must have experience on Azure Synapse.\nAdditional Information:- The candidate should have minimum 3 years of experience in Microsoft Azure Data Services.- This position is based at our Bengaluru office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data services', 'azure synapse', 'microsoft azure', 'pyspark', 'etl', 'software development', 'ssas', 'power bi', 'azure data factory', 'sql server', 'sql', 'database management', 'data modeling', 'ssrs', 'code review', 'technical specifications', 'cloud computing', 'ssis', 'data integration', 'msbi']",2025-06-14 05:22:00
Data Scientist- Credit Risk Modelling,TransOrg,3 - 5 years,Not Disclosed,"['Bengaluru', 'Mumbai (All Areas)']","Domain: Retail Banking / Credit Cards\nLocation: Mumbai/ Bengaluru\nExperience: 3-5 years\nIndustry: Banking / Financial Services (Mandatory)\n\nWhy would you like to join us?\nTransOrg Analytics specializes in Data Science, Data Engineering and Generative AI, providing advanced analytics solutions to industry leaders and Fortune 500 companies across India, US, APAC and the Middle East. We leverage data science to streamline, optimize, and accelerate our clients' businesses.\nVisit at www.transorg.com to know more about us.\n\nWhat do we expect from you?\nBuild and validate credit risk models, including application scorecards and behavior scorecards (B-score), aligned with business and regulatory requirements.\nUse advanced machine learning algorithms such as Logistic Regression, XGBoost, and Clustering to develop interpretable and high-performance models.\nTranslate business problems into data-driven solutions using robust statistical and analytical methods.\nCollaborate with cross-functional teams including credit policy, risk strategy, and data engineering to ensure effective model implementation and monitoring.\nMaintain clear, audit-ready documentation for all models and comply with internal model governance standards.\nTrack and monitor model performance, proactively suggesting recalibrations or enhancements as needed.\n\nWhat do you need to excel at?\nWriting efficient and scalable code in Python, SQL, and PySpark for data processing, feature engineering, and model training.\nWorking with large-scale structured and unstructured data in a fast-paced, banking or fintech environment.\nDeploying and managing models using MLFlow, with a strong understanding of version control and model lifecycle management.\nUnderstanding retail banking products, especially credit card portfolios, customer behavior, and risk segmentation.\nCommunicating complex technical outcomes clearly to non-technical stakeholders and senior management.\nApplying a structured problem-solving approach and delivering insights that drive business value.\nWhat are we looking for?\nBachelors or masters degree in Statistics, Mathematics, Computer Science, or a related quantitative field.\n35 years of experience in credit risk modelling, preferably in retail banking or credit cards.\nHands-on expertise in Python, SQL, PySpark, and experience with MLFlow or equivalent MLOps tools.\nDeep understanding of machine learning techniques including Logistic Regression, XGBoost, and Clustering.\nProven experience in developing Application Scorecards and behavior Scorecards using real-world banking data.\nStrong documentation and compliance orientation, with an ability to work within regulatory frameworks.\nCuriosity, accountability, and a passion for solving real-world problems using data.\nCloud Knowledge, JIRA, GitHub(good to have)",Industry Type: Analytics / KPO / Research,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Natural Language Processing', 'Machine Learning', 'Python', 'SQL', 'Predictive Modeling', 'Logistic Regression', 'Linear Regression', 'Time Series Analysis', 'Statistical Modeling', 'Credit Risk Modelling', 'Deep Learning', 'Predictive Analytics']",2025-06-14 05:22:03
Databricks Unified Data Analytics Platform- Pan India,Fortune Global 500 IT Services Firm,8 - 13 years,Not Disclosed,['Bhubaneswar'],"Project Role : Application Developer\nProject Role Description : Design, build and configure applications to meet business process and application requirements.\nMust have skills : Databricks Unified Data Analytics Platform\nGood to have skills : NA\nMinimum 7.5 year(s) of experience is required\nEducational Qualification : 15 years of fulltime education or above\nSummary:\nAs an Application Developer, you will be responsible for designing, building, and configuring applications to meet business process and application requirements using Databricks Unified Data Analytics Platform. Your typical day will involve working with the development team, analyzing business requirements, and developing solutions to meet those requirements.\n\nRoles & Responsibilities:\n- Design, develop, and maintain applications using Databricks Unified Data Analytics Platform.\n- Collaborate with cross-functional teams to analyze business requirements and develop solutions to meet those requirements.\n- Develop and maintain technical documentation related to the application development process.\n- Ensure that all applications are developed according to industry standards and best practices.\n- Provide technical support and troubleshooting for applications developed using Databricks Unified Data Analytics Platform.\n\nProfessional & Technical Skills:\n- Must To Have Skills: Experience with Databricks Unified Data Analytics Platform.\n- Good To Have Skills: Experience with other big data technologies such as Hadoop, Spark, and Hive.\n- Strong understanding of software development principles and methodologies.\n- Experience with programming languages such as Python, Java, or Scala.\n- Experience with database technologies such as SQL and NoSQL.\n- Experience with version control systems such as Git or SVN.\n\nAdditional Information:\n- The candidate should have a minimum of 3 years of experience in Databricks Unified Data Analytics Platform.\n- The ideal candidate will possess a strong educational background in computer science or a related field, along with a proven track record of delivering impactful data-driven solutions.\n- This position is based at our Bengaluru office.\n15 years of fulltime education or above",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Databricks Unified Data Analytics Platform', 'Java', 'Hive', 'Hadoop', 'big data', 'Python']",2025-06-14 05:22:05
"Software Development Engineer III, RISC",Amazon,5 - 10 years,Not Disclosed,['Bengaluru'],"Are you interested in joining a team that safeguards and enhances the buying experience of Amazon customers worldwide while empowering sellers worldwide to build the most customer-centric marketplace on EarthOur team leverages a host of technologies including machine learning, data mining, big data analytics, cloud computing, and highly scalable distributed systems to support scalable global transactions every day.\n\nWe have an exciting opportunity with the Regulatory Intelligence, Safety, and Compliance (RISC) engineering team, to architect and build next-generation engineering systems to quickly and accurately identify and mitigate product safety issues and potential risks to the customer experience.\n\nAs a Software Development Engineer, you will collaborate with a team of talented software, data, and machine learning engineers to design, develop, and manage highly scalable distributed systems. These systems are engineered to ensure high availability, low latency, and seamless scalability safeguarding customers and ensuring product compliance. Your work will contribute to building the worlds most trusted data platform for all safety and compliance information related to products and supply chains.\n\nEach and every person buying, selling, or handling Amazon products will be your customer.\n\nAs a member of this growing team, you ll be able to build the groundwork and influence its direction for the years to come. Our work cuts across various disciplines from delivering an awesome user experience via great UI/UX, to building massively scalable backend systems to support the most high-traffic pages on Amazon.com, to analytical and feedback systems which give us data-driven customer insights, to using machine learning and AI to influence recommendations and marketing. If you have a passion for consumer-facing applications, and are obsessed with customer experience, we want you!\n\nIf you d like to make a real-world difference by working hard, having fun, and making history, this is the team for you!\n\n\nIn this role you will:\n\nHelp define the system architecture, own and implement specific components, and help shape the overall experience\nCollaborate closely with product managers, UX designers, and other SDE team members to help define the scope of the product\nTake responsibility for technical problem solving, creatively meeting product objectives, and developing best practices\nDemonstrate cross-functional resource interaction to accomplish your goals\nWrite high-quality, efficient, testable code in Java and other object-oriented languages\nDesign Amazon-scale tools to facilitate internal business\nBuild highly available, secure, and low-latency systems\nMentor other developers\nFind out what it takes to engineer systems for ""Amazon Scale""\nDesign and build microservices\nOwn and operate the systems that you build based on real-time customer data and demanding service-level agreements\nContribute to planning, design, implementation, testing, operations, and process improvement\n\nA day in the life\nHigh-level designs, cross-team alignment, long-term architectural roadmap and technical strategy, understanding the business domain and proposing solutions to address customer and business problems, helping scope and analyze product requirements, mentorship, reviewing CRs, writing high-quality code to be an example for the team. 5+ years of non-internship professional software development experience\n5+ years of programming with at least one software programming language experience\n5+ years of leading design or architecture (design patterns, reliability and scaling) of new and existing systems experience\nExperience as a mentor, tech lead or leading an engineering team 5+ years of full software development life cycle, including coding standards, code reviews, source control management, build processes, testing, and operations experience\nBachelors degree in computer science or equivalent",,,,"['Computer science', 'System architecture', 'Cloud computing', 'Backend', 'Coding', 'Analytical', 'Machine learning', 'Data mining', 'Internship', 'Distribution system']",2025-06-14 05:22:07
Data Architect,Arcadis,7 - 12 years,Not Disclosed,['Bengaluru'],"Individual Accountabilities\nCollaboration\nCollaborates with domain architects in the DSS, OEA, EUS, and HaN towers and if appropriate, the respective business stakeholders in architecting data solutions for their data service needs.\nCollaborates with the Data Engineering and Data Software Engineering teams to effectively communicate the data architecture to be implemented. Contributes to prototype or proof of concept efforts.\nCollaborates with InfoSec organization to understand corporate security policies and how they apply to data solutions.\nCollaborates with the Legal and Data Privacy organization to understand the latest policies so they may be incorporated into every data architecture solution.\nSuggest architecture design with Ontologies, MDM team.\nTechnical skills & design\nSignificant experience working with structured and unstructured data at scale and comfort with a variety of different stores (key-value, document, columnar, etc.) as well as traditional RDBMS and data warehouses.\nDeep understanding of modern data services in leading cloud environments, and able to select and assemble data services with maximum cost efficiency while meeting business requirements of speed, continuity, and data integrity.\nCreates data architecture artifacts such as architecture diagrams, data models, design documents, etc.\nGuides domain architect on the value of a modern data and analytics platform.\nResearch, design, test, and evaluate new technologies, platforms and third-party products.\nWorking experience with Azure Cloud, Data Mesh, MS Fabric, Ontologies, MDM, IoT, BI solution and AI would be greater assets.\nExpert troubleshoot skills and experience.\nLeadership\nMentors aspiring data architects typically operating in data engineering and software engineering roles.\nKey shared accountabilities\nLeads medium to large data services projects.\nProvides technical partnership to product owners\nShared stewardship, with domains architects, of the Arcadis data ecosystem.\nActively participates in Arcadis Tech Architect community.\nKey profile requirements\nMinimum of 7 years of experience in designing and implementing modern solutions as part of variety of data ingestion and transformation pipelines\nMinimum of 5 years of experience with best practice design principles and approaches for a range of application styles and technologies to help guide and steer decisions.\nExperience working in large scale development and cloud environment.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Architecture', 'data modeling', 'Azure Cloud', 'RDBMS', 'data ingestion', 'MS Fabric', 'data transformation']",2025-06-14 05:22:09
Digital Marketing Data Science Manager,Abinbev Gcc Services,6 - 10 years,Not Disclosed,['Bengaluru'],"AB InBev GCC was incorporated in 2014 as a strategic partner for Anheuser-Busch InBev. The center leverages the power of data and analytics to drive growth for critical business functions such as operations, finance, people, and technology. The teams are transforming Operations through Tech and Analytics.\nDo You Dream Big?\nWe Need You.\n\nJob Description",,,,"['NLP', 'Python', 'SQL', 'R', 'Power Bi', 'MFDL', 'Azure Cloud']",2025-06-14 05:22:12
Data Architect,Diverse Lynx,8 - 13 years,Not Disclosed,['Bengaluru'],"Data Architect-\nTotal Yrs. of Experience* 15+ Relevant Yrs. of experience* 8+ Detailed JD *(Roles and Responsibilities)\nLeadership qualities and ability to lead a team of 8 data engineers + PowerBI resources\nShould be able to engage with business users and IT to provide consultation on data and visualization needs\nExcellent communication, articulation, and presentation skills\nExposure to data architecture, ETL architecture\nDesign, develop, and maintain scalable data pipelines using Python, ADF, and Databricks\nImplement ETL process to extract, transform, and load data from various sources into Snowflake\nEnsure data is processed efficiently and is made available for analytics and reporting\n8+ years of experience in data engineering, with a focus on Python, ADF, Snowflake, Databricks, and ETL processes.\nProficiency in SQL and experience with cloud-based data storage and processing.\nStrong problem-solving skills and the ability to work in a fast-paced environment\nExperience with Agile methodologies and working in a collaborative team environment.\nCertification in Snowflake, Azure, or other relevant technologies is an added advantage\nBachelors degree in computer science engineering, Information Systems or equivalent field\nMandatory skills* Python, Snowflake, Azure Data Factory, Databricks, SQL Desired skills* 1. Strong Oral and written communication\n2. Proactive and accountable of the deliverables quality and timely submission Domain* Retail Work Location* India\nLocation- PAN India\nYrs of Exp-15+Yrs",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Retail', 'Focus', 'Data Architect', 'Cloud', 'Agile', 'Analytics', 'SQL', 'Python', 'Data architecture']",2025-06-14 05:22:14
Data Scientist - GEN AI,Rarr Technologies,6 - 8 years,Not Disclosed,['Bengaluru'],"At least 6 - 8 years overall experience\nAt least 4-5 years experience in Machine Learning / Deep Learning\nExtensive experience working on NLP\nConversant with Python programming\nGood knowledge of and experience working on Generative AI\nAdept in Prompt Engineering\nShould have a good understanding of microservices architecture and Data Engineering\nShould be able to understand user needs and map with corresponding technologies\nShould be able to architect Generative AI based solutions\nShould be able to guide the team technically\nShould be able to highlight technical limitations and shortcomings well in advance and suggest alternative approaches / solutions\nShould have working experience on Azure Cognitive Services\nExperience working on client facing roles and direct client interactions\nGood communication skills\nAi Ml, Nlp & Deep Learning, Data Science",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['deep learning', 'Architect', 'data science', 'Architecture', 'Machine learning', 'Programming', 'Python', 'microservices']",2025-06-14 05:22:16
Data Scientist,Barclays,1 - 7 years,Not Disclosed,['Pune'],"Join us as a Data Scientist at Barclays, where youll take part in the evolution of our digital landscape, driving innovation and excellence. Youll harness cutting-edge technology to revolutionise our digital offerings, ensuring unparalleled customer experiences. As a part of the Service Operations team, you will deliver technology stack, using strong analytical and problem solving skills to understand the business requirements and deliver quality solutions. Youll be working on complex technical problems that will involve detailed analytical skills and analysis. This will be done in conjunction with fellow engineers, business analysts and business stakeholders.\nTo be successful as a Data Scientist you should have experience with:\nEssential Skills\nSolid understanding of machine learning concepts and model deployment.\nPrior experience in a data science role, indicating a strong foundation in the field.\nAdvanced coding proficiency in Python, with the ability to design, test, and correct complex scripts.\nProficiency in SQL, for managing and manipulating data.\nWorking in an Agile manner and leading Agile teams using Jira.\nSome other highly valued skills include:\nExcellent modelling skills, as evidenced by an advanced degree or significant experience.\nStrong quantitative and statistical skills, enabling logical and methodical problem-solving.\nGood understanding and experience of big data technologies and the underlying approach.\nGood interpersonal skills for maintaining relationships with multiple business areas, including senior leadership and compliance.\nAbility to manage laterally and upwards across multiple discipline technical areas.\nVersion control using Bitbucket, Gitlab, etc.\nCloud experience (AWS, Azure or GCP)\nYou may be assessed on key critical skills relevant for success in role, such as risk and controls, change and transformation, business acumen, strategic thinking and digital and technology, as well as job-specific technical skills.\nThis role is based in Pune.\nPurpose of the role\nTo use innovative data analytics and machine learning techniques to extract valuable insights from the banks data reserves, leveraging these insights to inform strategic decision-making, improve operational efficiency, and drive innovation across the organisation.\nAccountabilities\nIdentification, collection, extraction of data from various sources, including internal and external sources.\nPerforming data cleaning, wrangling, and transformation to ensure its quality and suitability for analysis.\nDevelopment and maintenance of efficient data pipelines for automated data acquisition and processing.\nDesign and conduct of statistical and machine learning models to analyse patterns, trends, and relationships in the data.\nDevelopment and implementation of predictive models to forecast future outcomes and identify potential risks and opportunities.\nCollaborate with business stakeholders to seek out opportunities to add value from data through Data Science.\nAssistant Vice President Expectations\nTo advise and influence decision making, contribute to policy development and take responsibility for operational effectiveness. Collaborate closely with other functions/ business divisions.\nLead a team performing complex tasks, using well developed professional knowledge and skills to deliver on work that impacts the whole business function. Set objectives and coach employees in pursuit of those objectives, appraisal of performance relative to objectives and determination of reward outcomes\nIf the position has leadership responsibilities, People Leaders are expected to demonstrate a clear set of leadership behaviours to create an environment for colleagues to thrive and deliver to a consistently excellent standard. The four LEAD behaviours are: L - Listen and be authentic, E - Energise and inspire, A - Align across the enterprise, D - Develop others.\nOR for an individual contributor, they will lead collaborative assignments and guide team members through structured assignments, identify the need for the inclusion of other areas of specialisation to complete assignments. They will identify new directions for assignments and/ or projects, identifying a combination of cross functional methodologies or practices to meet required outcomes.\nConsult on complex issues; providing advice to People Leaders to support the resolution of escalated issues.\nIdentify ways to mitigate risk and developing new policies/procedures in support of the control and governance agenda.\nTake ownership for managing risk and strengthening controls in relation to the work done.\nPerform work that is closely related to that of other areas, which requires understanding of how areas coordinate and contribute to the achievement of the objectives of the organisation sub-function.\nCollaborate with other areas of work, for business aligned support areas to keep up to speed with business activity and the business strategy.\nEngage in complex analysis of data from multiple sources of information, internal and external sources such as procedures and practises (in other areas, teams, companies, etc). to solve problems creatively and effectively.\nCommunicate complex information. Complex information could include sensitive information or information that is difficult to communicate because of its content or its audience.\nInfluence or convince stakeholders to achieve outcomes.",Industry Type: Financial Services,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Coding', 'Analytical', 'Machine learning', 'Agile', 'Business strategy', 'Assistant Vice President', 'Operations', 'SQL', 'Service operations', 'Python']",2025-06-14 05:22:18
GCP Cloud Engineer,Ford,0 - 3 years,Not Disclosed,['Chennai'],"Responsibilities:\n\n\nInfrastructure Provisioning and Management:\n\nSet up and configure cloud projects/Data Integration Tools and accounts using Google Cloud Platform (GCP).\n\n\nConfiguring Cloud and Data Integration Solutions:\n\nConfigure Data integration tools like Astronomer/GKE/App Integration/Airflow and Composer\n\n\nData Pipeline Orchestration:\n\nSet up and configure data pipeline orchestration tools such as Astronomer, Composer, and Airflow.\n\nManage and schedule workflows using Directed Acyclic Graphs (DAGs) to ensure efficient data processing.\n\n\nMonitoring and Incident Response:\n\nMaintain the availability of systems and services in production by setting service-level objectives (SLOs), service-level agreements (SLAs), and service-level indicators (SLIs).\n\nSet up monitoring and alerting systems for cloud services, data pipelines, and AI solutions.\n\nRespond to incidents, perform root cause analysis, and collaborate with teams to improve incident response practices.\n\n\nSecurity and Compliance:\n\nEnsure access and identity management in Google Cloud and data pipeline projects.\n\nImplement security measures to protect cloud-based data, services, and AI/ML workloads.\n\nComply with global privacy laws and organizational policies.\n\n\nGenAI Solution Development Delivery:\n\nLead the development, testing, and deployment of GenAI solutions.\n\n\nCollaboration and Communication:\n\nCollaborate with cross-functional teams to design and implement innovative security solutions that align with industry best practices and regulatory requirements.\n\nCommunicate effectively with management and users about the state and performance of cloud services, data pipelines, and AI solutions.\n\n\nContinuous Improvement:\n\nAutomate infrastructure provisioning with tools like Terraform and Tekton.\n\nEnhance visibility and monitoring capabilities for cloud services, data pipelines, and GenAI solutions.\n\nImplement improvements based on feedback and incident reviews\n\n\nOnCall Support:\n\n\n\nAble to support Oncall during weekends\n\nQualifications\n\n\nEducation: Bachelors degree in Computer Science, Information Technology, or a related field.\n\nExperience: Proven experience in cloud engineering, data pipeline orchestration, and AI solution development, specifically with Google Cloud Platform (GCP) and tools like Astronomer, Composer, Airflow.\n\nProgramming Language : Python, Terraform, Google cloud, Tekton, Gen AI and LLM Models.\n\nSkills:\n\nProficiency in GCP services and data pipeline orchestration tools.\n\nStrong understanding of IAM roles, policies, and security best practices.\n\nExperience with infrastructure as code tools like Terraform.\n\nExcellent problem-solving and troubleshooting skills.\n\n\n\nStrong communication and collaboration skills.\nResponsibilities:\n\n\nInfrastructure Provisioning and Management:\n\nSet up and configure cloud projects/Data Integration Tools and accounts using Google Cloud Platform (GCP).\n\n\nConfiguring Cloud and Data Integration Solutions:\n\nConfigure Data integration tools like Astronomer/GKE/App Integration/Airflow and Composer\n\n\nData Pipeline Orchestration:\n\nSet up and configure data pipeline orchestration tools such as Astronomer, Composer, and Airflow.\n\nManage and schedule workflows using Directed Acyclic Graphs (DAGs) to ensure efficient data processing.\n\n\nMonitoring and Incident Response:\n\nMaintain the availability of systems and services in production by setting service-level objectives (SLOs), service-level agreements (SLAs), and service-level indicators (SLIs).\n\nSet up monitoring and alerting systems for cloud services, data pipelines, and AI solutions.\n\nRespond to incidents, perform root cause analysis, and collaborate with teams to improve incident response practices.\n\n\nSecurity and Compliance:\n\nEnsure access and identity management in Google Cloud and data pipeline projects.\n\nImplement security measures to protect cloud-based data, services, and AI/ML workloads.\n\nComply with global privacy laws and organizational policies.\n\n\nGenAI Solution Development Delivery:\n\nLead the development, testing, and deployment of GenAI solutions.\n\n\nCollaboration and Communication:\n\nCollaborate with cross-functional teams to design and implement innovative security solutions that align with industry best practices and regulatory requirements.\n\nCommunicate effectively with management and users about the state and performance of cloud services, data pipelines, and AI solutions.\n\n\nContinuous Improvement:\n\nAutomate infrastructure provisioning with tools like Terraform and Tekton.\n\nEnhance visibility and monitoring capabilities for cloud services, data pipelines, and GenAI solutions.\n\nImplement improvements based on feedback and incident reviews\n\n\nOnCall Support:\n\nAble to support Oncall during weekends",Industry Type: Auto Components,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Service level', 'Cloud Services', 'GCP', 'Data processing', 'Troubleshooting', 'Information technology', 'Monitoring', 'Python', 'Identity management']",2025-06-14 05:22:21
Site Reliability Engineer Lead,Optum,9 - 14 years,30-37.5 Lacs P.A.,['Hyderabad'],"Primary Responsibilities:\nDesign, implement, and maintain scalable, reliable, and secure infrastructure on AWS and EKS\nDevelop and manage observability and monitoring solutions using Datadog, Splunk, and Kibana\nCollaborate with development teams to ensure high availability and performance of microservices-based applications\nAutomate infrastructure provisioning, deployment, and monitoring using Infrastructure as Code (IaC) and CI/CD pipelines",,,,"['AWS services', 'EKS', 'Splunk', 'microservices', 'Python', 'Terraform', 'Ansible', 'Bash', 'Datadog']",2025-06-14 05:22:23
GCP Cloud Engineer,Ford,0 - 3 years,Not Disclosed,['Chennai'],"Responsibilities:\nInfrastructure Provisioning and Management:\nSet up and configure cloud projects/Data Integration Tools and accounts using Google Cloud Platform (GCP).\nConfiguring Cloud and Data Integration Solutions:\nConfigure Data integration tools like Astronomer/GKE/App Integration/Airflow and Composer\nData Pipeline Orchestration:\nSet up and configure data pipeline orchestration tools such as Astronomer, Composer, and Airflow.\nManage and schedule workflows using Directed Acyclic Graphs (DAGs) to ensure efficient data processing.\nMonitoring and Incident Response:\nMaintain the availability of systems and services in production by setting service-level objectives (SLOs), service-level agreements (SLAs), and service-level indicators (SLIs).\nSet up monitoring and alerting systems for cloud services, data pipelines, and AI solutions.\nRespond to incidents, perform root cause analysis, and collaborate with teams to improve incident response practices.\nSecurity and Compliance:\nEnsure access and identity management in Google Cloud and data pipeline projects.\nImplement security measures to protect cloud-based data, services, and AI/ML workloads.\nComply with global privacy laws and organizational policies.\nGenAI Solution Development & Delivery:\nLead the development, testing, and deployment of GenAI solutions.\nCollaboration and Communication:\nCollaborate with cross-functional teams to design and implement innovative security solutions that align with industry best practices and regulatory requirements.\nCommunicate effectively with management and users about the state and performance of cloud services, data pipelines, and AI solutions.\nContinuous Improvement:\nAutomate infrastructure provisioning with tools like Terraform and Tekton.\nEnhance visibility and monitoring capabilities for cloud services, data pipelines, and GenAI solutions.\nImplement improvements based on feedback and incident reviews\nOnCall Support:\nAble to support Oncall during weekends\nQualifications\nEducation: Bachelors degree in Computer Science, Information Technology, or a related field.\nExperience: Proven experience in cloud engineering, data pipeline orchestration, and AI solution development, specifically with Google Cloud Platform (GCP) and tools like Astronomer, Composer, Airflow.\nProgramming Language : Python, Terraform, Google cloud, Tekton, Gen AI and LLM Models.\nSkills:\nProficiency in GCP services and data pipeline orchestration tools.\nStrong understanding of IAM roles, policies, and security best practices.\nExperience with infrastructure as code tools like Terraform.\nExcellent problem-solving and troubleshooting skills.\nStrong communication and collaboration skills.\nResponsibilities:\nInfrastructure Provisioning and Management:\nSet up and configure cloud projects/Data Integration Tools and accounts using Google Cloud Platform (GCP).\nConfiguring Cloud and Data Integration Solutions:\nConfigure Data integration tools like Astronomer/GKE/App Integration/Airflow and Composer\nData Pipeline Orchestration:\nSet up and configure data pipeline orchestration tools such as Astronomer, Composer, and Airflow.\nManage and schedule workflows using Directed Acyclic Graphs (DAGs) to ensure efficient data processing.\nMonitoring and Incident Response:\nMaintain the availability of systems and services in production by setting service-level objectives (SLOs), service-level agreements (SLAs), and service-level indicators (SLIs).\nSet up monitoring and alerting systems for cloud services, data pipelines, and AI solutions.\nRespond to incidents, perform root cause analysis, and collaborate with teams to improve incident response practices.\nSecurity and Compliance:\nEnsure access and identity management in Google Cloud and data pipeline projects.\nImplement security measures to protect cloud-based data, services, and AI/ML workloads.\nComply with global privacy laws and organizational policies.\nGenAI Solution Development & Delivery:\nLead the development, testing, and deployment of GenAI solutions.\nCollaboration and Communication:\nCollaborate with cross-functional teams to design and implement innovative security solutions that align with industry best practices and regulatory requirements.\nCommunicate effectively with management and users about the state and performance of cloud services, data pipelines, and AI solutions.\nContinuous Improvement:\nAutomate infrastructure provisioning with tools like Terraform and Tekton.\nEnhance visibility and monitoring capabilities for cloud services, data pipelines, and GenAI solutions.\nImplement improvements based on feedback and incident reviews\nOnCall Support:\nAble to support Oncall during weekends",Industry Type: Automobile,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Service level', 'Cloud Services', 'GCP', 'Data processing', 'Troubleshooting', 'Information technology', 'Monitoring', 'Python', 'Identity management']",2025-06-14 05:22:25
Engineer III,AMERICAN EXPRESS,0 - 4 years,Not Disclosed,['Chennai'],"Here, your voice and ideas matter, your work makes an impact, and together, you will help us define the future of American Express.\nHow will you make an impact in this role?\nFunction Description:\nAmerican Express is embarking on an exciting transformation driven by an energetic new team of high performers. This group is nimble and creative with the power to shape our technology and product roadmap. If you have the talent and desire to deliver innovative digital and servicing products at a rapid pace, serving our customers seamlessly across physical, digital, mobile, and social media, join our transformation team! You will be part of a fast-paced, entrepreneurial team responsible for delivering projects platform supporting our global customer base.",,,,"['Unix', 'Computer science', 'Career development', 'Linux', 'Analytical', 'Social media', 'HTTP', 'Application development', 'Continuous improvement', 'Apache']",2025-06-14 05:22:27
Data Analyst,Growing Soonicorn in Auto Components Spa...,3 - 5 years,Not Disclosed,['Bengaluru'],"Dear Candidates,\n\nGreetings!!\n\nWe are hiring for one of the Globalized Product Based & Motor Vehicle Manufacturing MNC.\n\nJob Type: FTE\nJob Role:- Data Analyst\nExperience: 3 to 5 Years\nLocation: Bangalore\nWork Mode: Work from office\nNotice Period: Immediate to 30 days\nBudget: As Per Market Standards\nMandatory Skills:- Azure Data bricks, Power BI, Tableau, SQL, Python\n\n\n\nInterested candidates can share their updated resume on Gurpreet@selectiveglobalsearch.com",Industry Type: Electronic Components / Semiconductors,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Power Bi', 'Azure Databricks', 'Tableau', 'SQL', 'Python', 'Data Visualization']",2025-06-14 05:22:29
Data Analyst-Having Stratup-Mid-Size companies Exp.@ Bangalore_Urgent,"A leader in this space, we deliver world...",8 - 13 years,Not Disclosed,['Bengaluru'],"Data Analyst\n\nLocation: Bangalore\nExperience: 8 - 15 Yrs\nType: Full-time\n\nRole Overview\n\nWe are seeking a skilled Data Analyst to support our platform powering operational intelligence across airports and similar sectors. The ideal candidate will have experience working with time-series datasets and operational information to uncover trends, anomalies, and actionable insights. This role will work closely with data engineers, ML teams, and domain experts to turn raw data into meaningful intelligence for business and operations stakeholders.\n\nKey Responsibilities\n\nAnalyze time-series and sensor data from various sources\nDevelop and maintain dashboards, reports, and visualizations to communicate key metrics and trends.\nCorrelate data from multiple systems (vision, weather, flight schedules, etc) to provide holistic insights.\nCollaborate with AI/ML teams to support model validation and interpret AI-driven alerts (e.g., anomalies, intrusion detection).\nPrepare and clean datasets for analysis and modeling; ensure data quality and consistency.\nWork with stakeholders to understand reporting needs and deliver business-oriented outputs.\n\n\nQualifications & Required Skills\n\nBachelors or Masters degree in Data Science, Statistics, Computer Science, Engineering, or a related field.\n5+ years of experience in a data analyst role, ideally in a technical/industrial domain.\nStrong SQL skills and proficiency with BI/reporting tools (e.g., Power BI, Tableau, Grafana).\nHands-on experience analyzing structured and semi-structured data (JSON, CSV, time-series).\nProficiency in Python or R for data manipulation and exploratory analysis.\nUnderstanding of time-series databases or streaming data (e.g., InfluxDB, Kafka, Kinesis).\nSolid grasp of statistical analysis and anomaly detection methods.\nExperience working with data from industrial systems or large-scale physical infrastructure.\n\n\nGood-to-Have Skills\n\nDomain experience in airports, smart infrastructure, transportation, or logistics.\nFamiliarity with data platforms (Snowflake, BigQuery, Custom-built using open-source).\nExposure to tools like Airflow, Jupyter Notebooks and data quality frameworks.\nBasic understanding of AI/ML workflows and data preparation requirements.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Kafka', 'SQL', 'airports', 'InfluxDB', 'Airflow', 'structured Data', 'time-series', 'JSON', 'Tableau', 'Grafana', 'R', 'AI/ML', 'Kinesis', 'Snowflake', 'time-series databases', 'Data Preparation', 'Python', 'smart infrastructure', 'BigQuery', 'streaming data', 'Power BI', 'CSV', 'transportation', 'logistic', 'reporting tools']",2025-06-14 05:22:32
Data Modeler - SQL / Erwin,Leading Client,7 - 10 years,Not Disclosed,[],"Employment Type : Contract (Remote).\n\nJob Summary :\n\nWe are looking for a highly skilled Data Engineer / Data Modeler with strong experience in Snowflake, DBT, and GCP to support our data infrastructure and modeling initiatives.\n\nThe ideal candidate should possess excellent SQL skills, hands-on experience with Erwin Data Modeler, and a strong background in modern data architectures and data modeling techniques.\n\nKey Responsibilities :\n\n- Design and implement scalable data models using Snowflake and Erwin Data Modeler.\n\n- Create, maintain, and enhance data pipelines using DBT and GCP (BigQuery, Cloud Storage, Dataflow).\n\n- Perform reverse engineering on existing systems (e.g., Sailfish/DDMS) using DBeaver or similar tools to understand and rebuild data models.\n\n- Develop efficient SQL queries and stored procedures for data transformation, quality, and validation.\n\n- Collaborate with business analysts and stakeholders to gather data requirements and convert them into physical and logical models.\n\n- Ensure performance tuning, security, and optimization of the Snowflake data warehouse.\n\n- Document metadata, data lineage, and business logic behind data structures and flows.\n\n- Participate in code reviews, enforce coding standards, and provide best practices for data modeling and governance.\n\nMust-Have Skills :\n\n- Snowflake architecture, schema design, and data warehouse experience.\n\n- DBT (Data Build Tool) for data transformation and pipeline development.\n\n- Strong expertise in SQL (query optimization, complex joins, window functions, etc.)\n\n- Hands-on experience with Erwin Data Modeler (logical and physical modeling).\n\n- Experience with GCP (BigQuery, Cloud Composer, Cloud Storage).\n\n- Experience in reverse engineering legacy systems like Sailfish or DDMS using DBeaver.\n\nGood To Have :\n\n- Experience with CI/CD tools and DevOps for data environments.\n\n- Familiarity with data governance, security, and privacy practices.\n\n- Exposure to Agile methodologies and working in distributed teams.\n\n- Knowledge of Python for data engineering tasks and orchestration scripts.\n\nSoft Skills :\n\n- Excellent problem-solving and analytical skills.\n\n- Strong communication and stakeholder management.\n\n- Self-driven with the ability to work independently in a remote setup.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Modeling', 'Data Quality', 'GCP', 'Snowflake architecture', 'Data Security', 'Data Warehousing', 'Data Governance', 'Erwin', 'SQL']",2025-06-14 05:22:34
Senior Software Engineer (Java/Python and Database),Dynamic Yield,3 - 8 years,Not Disclosed,['Pune'],"Our Purpose\nMastercard powers economies and empowers people in 200+ countries and territories worldwide. Together with our customers, we re helping build a sustainable economy where everyone can prosper. We support a wide range of digital payments choices, making transactions secure, simple, smart and accessible. Our technology and innovation, partnerships and networks combine to deliver a unique set of products and services that help people, businesses and governments realize their greatest potential.\nTitle and Summary\nSenior Software Engineer (Java/Python and Database)\nOverview\n\nWe are the global technology company behind the world s fastest payments processing network. We are a vehicle for commerce, a connection to financial systems for the previously excluded, a technology innovation lab, and the home of Priceless . We ensure every employee has the opportunity to be a part of something bigger and to change lives. We believe as our company grows, so should you. We believe in connecting everyone to endless, priceless possibilities.\n\nOur team within Mastercard - Services:\n\nThe Services org is a key differentiator for Mastercard, providing the cutting-edge services that are used by some of the worlds largest organizations to make multi-million dollar decisions and grow their businesses. Focused on thinking big and scaling fast around the globe, this agile team is responsible for end-to-end solutions for a diverse global customer base. Centered on data-driven technologies and innovation, these services include payments-focused consulting, loyalty and marketing programs, business Test & Learn experimentation, and data-driven information and risk management services.\n\n\n\nData Analytics and AI Solutions (DAAI) Program:\n\nWithin the D&S Technology Team, the DAAI program is a relatively new program that is comprised of a rich set of products that provide accurate perspectives on Portfolio Optimization, and Ad Insights. Currently, we are enhancing our customer experience with new user interfaces, moving to API and web application-based data publishing to allow for seamless integration in other Mastercard products and externally, utilizing new data sets and algorithms to further analytic capabilities, and generating scalable big data processes.\n\nWe are looking for an innovative data engineer who will lead the technical design and development of an Analytic Foundation. The Analytic Foundation is a suite of individually commercialized analytical capabilities (think prediction as a service, matching as a service or forecasting as a service) that also includes a comprehensive data platform. These services will be offered through a series of APIs that deliver data and insights from various points along a central data store. This individual will partner closely with other areas of the business to build and enhance solutions that drive value for our customers.\n\nEngineers work in small, flexible teams. Every team member contributes to designing, building, and testing features. The range of work you will encounter varies from building intuitive, responsive UIs to designing backend data models, architecting data flows, and beyond. There are no rigid organizational structures, and each team uses processes that work best for its members and projects.\n\nHere are a few examples of products in our space:\n\nPortfolio Optimizer (PO) is a solution that leverages Mastercard s data assets and analytics to allow issuers to identify and increase revenue opportunities within their credit and debit portfolios.\nAd Insights uses anonymized and aggregated transaction insights to offer targeting segments that have high likelihood to make purchases within a category to allow for more effective campaign planning and activation.\nRole\n\nAs a Senior Software Engineer, you will:\n\nLead the scoping, design and implementation of complex features\nLead and push the boundaries of analytics and powerful, scalable applications\nDesign and implement intuitive, responsive UIs that allow issuers to better understand data and analytics\nBuild and maintain analytics and data models to enable performant and scalable products\nEnsure a high-quality code base by writing and reviewing performant, well-tested code\nMentor junior software engineers and teammates\nDrive innovative improvements to team development processes\nPartner with Product Managers and Customer Experience Designers to develop a deep understanding of users and use cases and apply that knowledge to scoping and building new modules and features\nCollaborate across teams with exceptional peers who are passionate about what they do\n\n\nAll about you / Ideal Candidate Qualifications\n\n6+ years of full stack engineering experience in an agile production environment\nExperience leading the design and implementation of complex features in full-stack applications\nExperience leading a large project and working with other developers\nStrong technologist eager to learn new technologies and frameworks. The following is a plus:\nProficiency with .NET/C#, React, Redux, Typescript, Java JDK 8, Tomcat, Spring Boot, Spring Security, Maven, Hibernate / JPA, REST, and SQL Server or other object-oriented languages, front-end frameworks, and/or relational database technologies\nSolid experience with RESTful APIs and JSON/SOAP based API\nExperience with SQL, Multi-threading, Message Queuing & Distributed Systems.\nExperience with Design Patterns.\nExpertise in Junit or other automated unit testing frameworks.\nKnowledge of Splunk or other alerting and monitoring solutions.\nFluent in the use of Git, Jenkins.\nKnowledge of cloud native development such as cloud foundry, AWS, etc.\nCustomer-centric development approach\nPassion for analytical / quantitative problem solving\nAbility to identify and implement improvements to team development processes\nStrong collaboration skills with experience collaborating across many people, roles, and geographies\nMotivation, creativity, self-direction, and desire to thrive on small project teams\nSuperior academic record with a degree in Computer Science or related technical field\nStrong written and verbal English communication skills\nCorporate Security Responsibility\n\nAll activities involving access to Mastercard assets, information, and networks comes with an inherent risk to the organization and, therefore, it is expected that every person working for, or on behalf of, Mastercard is responsible for information security and must:\nAbide by Mastercard s security policies and practices;\nEnsure the confidentiality and integrity of the information being accessed;\nReport any suspected information security violation or breach, and\nComplete all periodic mandatory security trainings in accordance with Mastercard s guidelines.",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Maven', 'Hibernate', 'Tomcat', 'Information security', 'Consulting', 'Agile', 'JSON', 'Unit testing', 'SQL', 'Python']",2025-06-14 05:22:36
Senior Software Engineer (Java/Python and Database),Mastercard,6 - 11 years,Not Disclosed,['Pune'],"We are the global technology company behind the world s fastest payments processing network. We are a vehicle for commerce, a connection to financial systems for the previously excluded, a technology innovation lab, and the home of Priceless . We ensure every employee has the opportunity to be a part of something bigger and to change lives. We believe as our company grows, so should you. We believe in connecting everyone to endless, priceless possibilities.\n\nOur team within Mastercard - Services:\n\nThe Services org is a key differentiator for Mastercard, providing the cutting-edge services that are used by some of the worlds largest organizations to make multi-million dollar decisions and grow their businesses. Focused on thinking big and scaling fast around the globe, this agile team is responsible for end-to-end solutions for a diverse global customer base. Centered on data-driven technologies and innovation, these services include payments-focused consulting, loyalty and marketing programs, business Test & Learn experimentation, and data-driven information and risk management services.\n\nData Analytics and AI Solutions (DAAI) Program:\n\nWithin the D&S Technology Team, the DAAI program is a relatively new program that is comprised of a rich set of products that provide accurate perspectives on Portfolio Optimization, and Ad Insights. Currently, we are enhancing our customer experience with new user interfaces, moving to API and web application-based data publishing to allow for seamless integration in other Mastercard products and externally, utilizing new data sets and algorithms to further analytic capabilities, and generating scalable big data processes.\n\nWe are looking for an innovative data engineer who will lead the technical design and development of an Analytic Foundation. The Analytic Foundation is a suite of individually commercialized analytical capabilities (think prediction as a service, matching as a service or forecasting as a service) that also includes a comprehensive data platform. These services will be offered through a series of APIs that deliver data and insights from various points along a central data store. This individual will partner closely with other areas of the business to build and enhance solutions that drive value for our customers.\n\nEngineers work in small, flexible teams. Every team member contributes to designing, building, and testing features. The range of work you will encounter varies from building intuitive, responsive UIs to designing backend data models, architecting data flows, and beyond. There are no rigid organizational structures, and each team uses processes that work best for its members and projects.\n\nHere are a few examples of products in our space:\n\nPortfolio Optimizer (PO) is a solution that leverages Mastercard s data assets and analytics to allow issuers to identify and increase revenue opportunities within their credit and debit portfolios.\nAd Insights uses anonymized and aggregated transaction insights to offer targeting segments that have high likelihood to make purchases within a category to allow for more effective campaign planning and activation.\nRole\n\nAs a Senior Software Engineer, you will:\n\nLead the scoping, design and implementation of complex features\nLead and push the boundaries of analytics and powerful, scalable applications\nDesign and implement intuitive, responsive UIs that allow issuers to better understand data and analytics\nBuild and maintain analytics and data models to enable performant and scalable products\nEnsure a high-quality code base by writing and reviewing performant, we'll-tested code\nMentor junior software engineers and teammates\nDrive innovative improvements to team development processes\nPartner with Product Managers and Customer Experience Designers to develop a deep understanding of users and use cases and apply that knowledge to scoping and building new modules and features\nCollaborate across teams with exceptional peers who are passionate about what they do\n\nAll about you / Ideal Candidate Qualifications\n\n6+ years of full stack engineering experience in an agile production environment\nExperience leading the design and implementation of complex features in full-stack applications\nExperience leading a large project and working with other developers\nStrong technologist eager to learn new technologies and frameworks. The following is a plus:\nProficiency with .NET/C#, React, Redux, Typescript, Java JDK 8, Tomcat, Spring Boot, Spring Security, Maven, Hibernate / JPA, REST, and SQL Server or other object-oriented languages, front-end frameworks, and/or relational database technologies\nSolid experience with RESTful APIs and JSON/SOAP based API\nExperience with SQL, Multi-threading, Message Queuing & Distributed Systems.\nExperience with Design Patterns.\nExpertise in Junit or other automated unit testing frameworks.\nKnowledge of Splunk or other alerting and monitoring solutions.\nFluent in the use of Git, Jenkins.\nKnowledge of cloud native development such as cloud foundry, AWS, etc\nCustomer-centric development approach\nPassion for analytical / quantitative problem solving\nAbility to identify and implement improvements to team development processes\nStrong collaboration skills with experience collaborating across many people, roles, and geographies\nMotivation, creativity, self-direction, and desire to thrive on small project teams\nSuperior academic record with a degree in Computer Science or related technical field\nStrong written and verbal English communication skills",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Maven', 'Hibernate', 'Tomcat', 'Information security', 'Consulting', 'Agile', 'JSON', 'Unit testing', 'SQL', 'Python']",2025-06-14 05:22:38
Application Software Engineer,Transasia Bio Medicals,2 - 5 years,Not Disclosed,['Bangalore Rural'],"Design, develop and implement application software solutions on Linux using Python, C, C++ for innovative medical devices.\n\nRole & responsibilities\nDesign and Development of Application Software as per requirements.\nWriting clean, high-quality, high-performance, maintainable code while incorporating good design practices.\nDevelop and support software including applications, hardware and sensor data integration, interfaces, and new functionality enhancements.\nCoordinate cross-functionally to ensure project meets business objectives and compliance standards.\nSupport test and deployment of new products and features.\nParticipate in code reviews.\nREQUIREMENT PROFILE: (Qualification, No of yrs exp)\na. Bachelors / Masters degree in Computer Science / Information Science, or related engineering streams.\nb. 4+ years of industry experience in concept to delivery of application and system software, preferably on Single Board Computers running on Linux OS.\nCOMPETENCIES: (knowledge, skills and attitudes required to perform job):\nSkills: C++ / C (must), Linux (highly desirable), LVGL (desirable), Experience/Exposure to Yocto & Poky (nice to have), Multi-threaded applications / IPC Design & Development (Desirable).\nDesirable: Experience in Linux, Embedded Systems Interfacing; with a solid Math background.\nDesirable: Exposure and Experience in Object Oriented Design\nKnowledge and practice of applying Design Patterns - nice to have.\nExperience with Computer Vision, Image Processing Algorithms and Stereo Vision; and exposure to Kinematics & Robotics are desirable - nice to have.\n2+ years of relevant work experience.\nBachelor's degree in Computer Science (or related field).\nExperience with Agile or Scrum software development methodologies.\nAbility to plan, organize, and prioritize work.",Industry Type: Pharmaceutical & Life Sciences,Department: Healthcare & Life Sciences,"Employment Type: Full Time, Permanent","['Linux', 'C++ / C', 'LVGL', 'Multi-threaded applications / IPC Design & Development', 'Yocto & Poky']",2025-06-14 05:22:40
Data Modeler - SQL/Erwin,Leading Client,7 - 10 years,Not Disclosed,['Noida'],"Job Title : Data Engineer / Data Modeler.\n\nLocation : Remote (India).\n\nEmployment Type : Contract (Remote).\n\nExperience Required : 7+ Years.\n\nJob Summary :\n\nWe are looking for a highly skilled Data Engineer / Data Modeler with strong experience in Snowflake, DBT, and GCP to support our data infrastructure and modeling initiatives.\n\nThe ideal candidate should possess excellent SQL skills, hands-on experience with Erwin Data Modeler, and a strong background in modern data architectures and data modeling techniques.\n\nKey Responsibilities :\n\n- Design and implement scalable data models using Snowflake and Erwin Data Modeler.\n\n- Create, maintain, and enhance data pipelines using DBT and GCP (BigQuery, Cloud Storage, Dataflow).\n\n- Perform reverse engineering on existing systems (e.g., Sailfish/DDMS) using DBeaver or similar tools to understand and rebuild data models.\n\n- Develop efficient SQL queries and stored procedures for data transformation, quality, and validation.\n\n- Collaborate with business analysts and stakeholders to gather data requirements and convert them into physical and logical models.\n\n- Ensure performance tuning, security, and optimization of the Snowflake data warehouse.\n\n- Document metadata, data lineage, and business logic behind data structures and flows.\n\n- Participate in code reviews, enforce coding standards, and provide best practices for data modeling and governance.\n\nMust-Have Skills :\n\n- Snowflake architecture, schema design, and data warehouse experience.\n\n- DBT (Data Build Tool) for data transformation and pipeline development.\n\n- Strong expertise in SQL (query optimization, complex joins, window functions, etc.)\n\n- Hands-on experience with Erwin Data Modeler (logical and physical modeling).\n\n- Experience with GCP (BigQuery, Cloud Composer, Cloud Storage).\n\n- Experience in reverse engineering legacy systems like Sailfish or DDMS using DBeaver.\n\nGood To Have :\n\n- Experience with CI/CD tools and DevOps for data environments.\n\n- Familiarity with data governance, security, and privacy practices.\n\n- Exposure to Agile methodologies and working in distributed teams.\n\n- Knowledge of Python for data engineering tasks and orchestration scripts.\n\nSoft Skills :\n\n- Excellent problem-solving and analytical skills.\n\n- Strong communication and stakeholder management.\n\n- Self-driven with the ability to work independently in a remote setup.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SQL/Erwin', 'Data Quality', 'Data Build Tool', 'Data Security', 'Snowflake DB', 'Data Modeling', 'Data Warehousing', 'Data Analytics', 'Data Governance', 'Erwin', 'SQL']",2025-06-14 05:22:42
Data Architect,Thinkapps Solutions,10 - 20 years,Not Disclosed,"['Kolkata', 'Hyderabad', 'Pune', 'Ahmedabad', 'Chennai', 'Bengaluru', 'Delhi / NCR', 'Mumbai (All Areas)']","• Proficient in Databricks Data Engg.\n• Knowledge in ETL concepts.\n• Proficient in writing SQL queries, performance tuning.\n• Knowledge in python script writing.\n• Good Communication skill.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Databricks', 'ETL', 'SQL', 'Python']",2025-06-14 05:22:44
Data Modeler - SQL/Erwin,Leading Client,7 - 10 years,Not Disclosed,[],"Employment Type : Contract (Remote).\n\nExperience Required : 7+ Years.\n\nJob Summary :\n\nWe are looking for a highly skilled Data Engineer / Data Modeler with strong experience in Snowflake, DBT, and GCP to support our data infrastructure and modeling initiatives.\n\nThe ideal candidate should possess excellent SQL skills, hands-on experience with Erwin Data Modeler, and a strong background in modern data architectures and data modeling techniques.\n\nKey Responsibilities :\n\n- Design and implement scalable data models using Snowflake and Erwin Data Modeler.\n\n- Create, maintain, and enhance data pipelines using DBT and GCP (BigQuery, Cloud Storage, Dataflow).\n\n- Perform reverse engineering on existing systems (e.g., Sailfish/DDMS) using DBeaver or similar tools to understand and rebuild data models.\n\n- Develop efficient SQL queries and stored procedures for data transformation, quality, and validation.\n\n- Collaborate with business analysts and stakeholders to gather data requirements and convert them into physical and logical models.\n\n- Ensure performance tuning, security, and optimization of the Snowflake data warehouse.\n\n- Document metadata, data lineage, and business logic behind data structures and flows.\n\n- Participate in code reviews, enforce coding standards, and provide best practices for data modeling and governance.\n\nMust-Have Skills :\n\n- Snowflake architecture, schema design, and data warehouse experience.\n\n- DBT (Data Build Tool) for data transformation and pipeline development.\n\n- Strong expertise in SQL (query optimization, complex joins, window functions, etc.)\n\n- Hands-on experience with Erwin Data Modeler (logical and physical modeling).\n\n- Experience with GCP (BigQuery, Cloud Composer, Cloud Storage).\n\n- Experience in reverse engineering legacy systems like Sailfish or DDMS using DBeaver.\n\nGood To Have :\n\n- Experience with CI/CD tools and DevOps for data environments.\n\n- Familiarity with data governance, security, and privacy practices.\n\n- Exposure to Agile methodologies and working in distributed teams.\n\n- Knowledge of Python for data engineering tasks and orchestration scripts.\n\nSoft Skills :\n\n- Excellent problem-solving and analytical skills.\n\n- Strong communication and stakeholder management.\n\n- Self-driven with the ability to work independently in a remote setup.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SQL', 'Data Quality', 'Data Build Tool', 'Data Security', 'Snowflake DB', 'Data Modeling', 'Data Warehousing', 'Data Analytics', 'Data Governance', 'Erwin']",2025-06-14 05:22:46
Enterprise Data Architect - Azure/ETL,Leading Client,10 - 12 years,Not Disclosed,['Kanpur'],"Key Responsibilities :\n\nAs an Enterprise Data Architect, you will :\n\n- Lead Data Architecture : Design, develop, and implement comprehensive enterprise data architectures, primarily leveraging Azure and Snowflake platforms.\n\n- Data Transformation & ETL : Oversee and guide complex data transformation and ETL processes for large and diverse datasets, ensuring data integrity, quality, and performance.\n\n- Customer-Centric Data Design : Specialize in designing and optimizing customer-centric datasets from various sources, including CRM, Call Center, Marketing, Offline, and Point of Sale systems.\n\n- Data Modeling : Drive the creation and maintenance of advanced data models, including Relational, Dimensional, Columnar, and Big Data models, to support analytical and operational needs.\n\n- Query Optimization : Develop, optimize, and troubleshoot complex SQL and NoSQL queries to ensure efficient data retrieval and manipulation.\n\n- Data Warehouse Management : Apply advanced data warehousing concepts to build and manage high-performing, scalable data warehouse solutions.\n\n- Tool Evaluation & Implementation : Evaluate, recommend, and implement industry-leading ETL tools such as Informatica and Unifi, ensuring best practices are followed.\n\n- Business Requirements & Analysis : Lead efforts in business requirements definition and management, structured analysis, process design, and use case documentation to translate business needs into technical specifications.\n\n- Reporting & Analytics Support : Collaborate with reporting teams, providing architectural guidance and support for reporting technologies like Tableau and PowerBI.\n\n- Software Development Practices : Apply professional software development principles and best practices to data solution delivery.\n\n- Stakeholder Collaboration : Interface effectively with sales teams and directly engage with customers to understand their data challenges and lead them to successful outcomes.\n\n- Project Management & Multi-tasking : Demonstrate exceptional organizational skills, with the ability to manage and prioritize multiple simultaneous customer projects effectively.\n\n- Strategic Thinking & Leadership : Act as a self-managed, proactive, and customer-focused leader, driving innovation and continuous improvement in data architecture.\n\nPosition Requirements :\n\nof strong experience with data transformation & ETL on large data sets.\n\n- Experience with designing customer-centric datasets (i.e., CRM, Call Center, Marketing, Offline, Point of Sale, etc.).\n\n- 5+ years of Data Modeling experience (i.e., Relational, Dimensional, Columnar, Big Data).\n\n- 5+ years of complex SQL or NoSQL experience.\n\n- Extensive experience in advanced Data Warehouse concepts.\n\n- Proven experience with industry ETL tools (i.e., Informatica, Unifi).\n\n- Solid experience with Business Requirements definition and management, structured analysis, process design, and use case documentation.\n\n- Experience with Reporting Technologies (i.e., Tableau, PowerBI).\n\n- Demonstrated experience in professional software development.\n\n- Exceptional organizational skills and ability to multi-task simultaneous different customer projects.\n\n- Strong verbal & written communication skills to interface with sales teams and lead customers to successful outcomes.\n\n- Must be self-managed, proactive, and customer-focused.\n\nTechnical Skills :\n\n- Cloud Platforms : Microsoft Azure\n\n- Data Warehousing : Snowflake\n\n- ETL Methodologies : Extensive experience in ETL processes and tools\n\n- Data Transformation : Large-scale data transformation\n\n- Data Modeling : Relational, Dimensional, Columnar, Big Data\n\n- Query Languages : Complex SQL, NoSQL\n\n- ETL Tools : Informatica, Unifi (or similar enterprise-grade tools)\n\n- Reporting & BI : Tableau, PowerBI",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure', 'Enterprise Architect', 'Data Architect', 'Big Data', 'Snowflake DB', 'Data Modeling', 'Data Warehousing', 'ETL', 'Reporting Tools', 'SQL']",2025-06-14 05:22:48
"Independent Contractor, Application and Data Integration",Myers Holum,6 - 11 years,Not Disclosed,[],"Principal Developer, Application and Data Integration\nMyers-Holum is expanding operations, and is actively seeking experienced Principal Developers with strong application integration and data integration experience to play a pivotal role in the expansion of the NS90 Practice.\nResponsibilities :\nHands-on design, development, deployment, and maintenance of integration processes between applications using Oracle Integration Cloud (OIC), Dell Boomi, or other data integration/pipelining tools.\nManaging, monitoring, sustaining, troubleshooting, and supporting existing integrations using Oracle Integration Cloud, Dell Boomi platform, or other relevant tools.\nDevelop custom data integration solutions to ingest data from multiple sources (cloud and on-premise) using various data integration/pipelining tools, Oracle Analytics Cloud (OAC), and Oracle Autonomous Data Warehouse (ADW)\nDesign, build, and test custom data warehouse solutions using platforms such as Oracle NetSuite Analytics Warehouse (NSAW)\nIntegrate with REST API endpoints, and applications like Netsuite.\nCollaborate with cross-functional teams to define and determine data migration and integration requirements.\nEstablish best practices in services development, integration of applications, and govern detailed designs generated by vendors.\nMonitor and report on progress in completing engagements and accomplishing goals.\nInterface with internal/external technical experts to drive system solutions.\nTranslate business requirements into technical specifications.\nProactively address potential and/or current integration challenges to meet or exceed established timelines, deliver services within budget constraints, and seek ways to reduce application-related costs.\nLead product implementation from start to finish, acting as a trusted advisor to stakeholders and end users.\nConduct product training and technical workshops with clients/end users.\nQualifications and Skills :\n6+ years of relevant Consulting or Industry Experience.\n6+ years of experience providing end-to-end integration solutions development for enterprise software or hosted high-tech services.\n4+ years experience working with the Oracle Integration Cloud (OIC) or Dell Boomi Platform.\n1+ years experience working with Netsuite.\n3+ years experience with data management using Oracle or other relational databases.\n3+ years experience with data warehouse and data integration projects, using ETL/data integration/data pipelining tools.\nProficiency in at least one version of SQL: Oracle (preferred), SQLServer, MySQL, PostgreSQL, Snowflake.\nExperience with OIC (Oracle Integration Cloud) is highly preferred.\nExperience using REST APIs to get data from cloud data sources is preferred.\nKnowledge of ERP data and business processes.\nStrong written and verbal communication skills, as well as business analysis skills\n10+ years of relevant professional experience (overall).\n6+ years experience with data management using Oracle or other relational databases.\nProficiency with at least one cloud or on-premise data integration/data pipelining tool such as Fivetran, Stitch, Talend, Glue, Nexla, Informatica, Oracle Data Integrator.\nWhy Become an MHIer\nYour Life at Myers-Holum & What you Can Expect:\nDo you desire collaboration? Are you ready to shape your future and positively influence change for your customers? If so, then its time to join the MHI Team where you can own your individuality and collaborate with other curious and thought-provoking minds. Discover what you are capable of by paving your path through MHI using your expertise and discovering your true potential.\nEvery MHIer is committed to our mission and operating principles; We remain curious as we lead with a genuine interest in people, ideas, and the unknown. We remain humble in knowing we can change how we currently do things and it is our ability to learn and grow that makes us a success. We remain resilient in understanding that success is not linear and that through reflection and a steadfast passion for betterment, we can continue our mindful and purposeful growth.\nA little about Us\nWe ve been around for 40 years. You ll get stability & growth with us . Myers-Holum is a technology and management consulting firm that was founded in 1981 and continues to grow year after year. Today we have over 400 remote employees sitting across ten countries; United States, Canada, Mexico, Chile, Uruguay, Philippines, Vietnam, India & Pakistan.\nThe Partnerships we ve built. Our cutting-edge technology partners include Oracle NetSuite, Stripe, Google Cloud Platform, Zone Billing, Celigo, and Boomi all working with us to provide the best customer experience throughout each implementation.\nOur Structure. We re a boutique firm with a strong internal framework. Our powerful model includes robust Sales, Solution Architecture, Resourcing & Enablement, Consulting, Project Management, Managed Services & Development, and Integration departments with subject matter experts sitting in each function.\nOur Employees. We re a company that recognizes we re only as strong as our people and each person who joins our firm contributes to our well-being. We strive to ensure all employees have a career path within Myers-Holum for both upward and lateral opportunities while maintaining a healthy work-life balance.\nWe re growing and making waves along the way, join us on this journey!\nOur Benefits\nMHI offers a competitive base salary, incentive pay, and comprehensive benefits including medical, dental, vision, retirement plan, and paid time off. Full package offerings are based on the Country in which you reside.\nCompany access to training and full coverage for certification/exam fees in a variety of products and professional skill sets\nCareer growth and upward mobility to meet your long-term career goals with mid-year and annual reviews scheduled on a recurring basis\nMHI invests in our employee s health by offering a company medical insurance policy that covers 100% of premiums for the individual employee and manager s dependents with a ZERO deductible in the United States\nRemote working opportunity when not traveling for client requirements with full access to the team through technology\nOur Process:\nOur interview process is efficient and provides you with an opportunity to showcase your strengths, current abilities and share your future career aspirations while learning whether Myers-Holum would be a good fit for you. You will be paired with a member of our recruiting team who will handle all of the logistics as well as provide thoughtful feedback after each touchpoint to ensure you are kept within the loop from start to finish. Our recruiting timeline can be flexible to align with your circumstances; our typical timeline can take 2-4 weeks to complete however this can be adjusted or expedited depending on your needs. You can expect the following touchpoints when interviewing with MHI\n30-minute call with our recruiting team\n60-minute video call with a MHI Team Lead, Delivery Manager, or Practice Manager\n90-minute interactive Case Study session with a Delivery Manager or Practice Manager\nExecutive leadership review and decision making\nWe are dedicated to fostering a workplace environment that upholds and promotes equal employment opportunity, free from discrimination on the basis of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity, or Veteran status. Our compensation ranges and packages are based on a wide array of factors unique to each candidate. It is not a common practice or guaranteed for an individual to be hired at or near the top of the range and compensation decisions are determined using a wide array of factors unique to each candidate, including but not limited to; skill set, years & depth of experience, certifications, and specific location due to cost of living & labor considerations; All of which will be considered during the interview and offer process. Salary ranges and incentive plans will differ in other countries in which MHI pays local currency.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['ERP', 'Business analysis', 'Project management', 'MySQL', 'Consulting', 'Billing', 'Informatica', 'Oracle', 'Troubleshooting', 'Analytics']",2025-06-14 05:22:50
Data QA Lead,Codvo,8 - 13 years,Not Disclosed,[],"Role: Data QA Lead\nExperience Required- 8+ Years\nLocation- India/Remote\n\nCompany Overview\nAt Codvo.ai, software and people transformations go hand-in-hand. We are a global empathy-led technology services company. Product innovation and mature software engineering are part of our core DNA. Respect, Fairness, Growth, Agility, and Inclusiveness are the core values that we aspire to live by each day.\nWe continue to expand our digital strategy, design, architecture, and product management capabilities to offer expertise, outside-the-box thinking, and measurable results.\nThe Data Quality Analyst is responsible for ensuring the quality, accuracy, and consistency of data within the Customer and Loan Master Data API solution. This role will work closely with data owners, data modelers, and developers to identify and resolve data quality issues.\nKey Responsibilities\nLead and manage end-to-end ETL/data validation activities.\nDesign test strategy, plans, and scenarios for source-to-target validation.\nBuild automated data validation frameworks (SQL/Python/Great Expectations).\nIntegrate tests with CI/CD pipelines (Jenkins, Azure DevOps).\nPerform data integrity, transformation logic, and reconciliation checks.\nCollaborate with Data Engineering, Product, and DevOps teams.\nDrive test metrics reporting, defect triage, and root cause analysis.\nMentor QA team members and ensure process adherence.\nMust-Have Skills\n8+ years in QA with 4+ years in ETL testing.\nStrong SQL and database testing experience.\nProficiency with ETL tools (Airbyte, DBT, Informatica, etc.).\nAutomation using Python or similar scripting language.\nSolid understanding of data warehousing, SCD, deduplication.\nExperience with large datasets and structured/unstructured formats.\nPreferred Skills\nKnowledge of data orchestration tools (Prefect, Airflow).\nFamiliarity with data quality/observability tools.\nExperience with big data systems (Spark, Hive).\nHands-on with test data generation (Faker, Mockaroo).\nSub-Department:\nTesting",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Product management', 'Automation', 'Data validation', 'orchestration', 'Test strategy', 'Reconciliation', 'Data quality', 'Informatica', 'SQL', 'Python']",2025-06-14 05:22:53
Data Quality Analyst,Yallas Technology Solutions Opc,5 - 10 years,Not Disclosed,[],"Title: Data Quality Analyst/Developer\nDuration: 6 months to 1 year contract\nLocation:  Remote\nNotice period - Immediate to 7 days\nUAN /EPFO Report Required\n\nWork Experience:\n5 + years of this experience - Experience doing Data Emendation\nDesign/Develop Rules, monitoring mechanisms, notification\nDesign/Develop UI, Workflows, security\nDesign/Develop analytics (overall DQ reporting, usage statistics, etc).\nDesign/Develop migration activities to migrate existing DQ assets between our existing DQ platform and new DQ platform.\nDesign integration with MDM & Catalog (as needed)\nMonitor system performance and suggest optimization strategies (as needed).\nWork with DT to maintain system - patches, backups, etc.\nWork with LYB's Data Stewards to support their governance activities.\nTesting\n\nThe DQ Analyst/Developer should have experience with IMDC (for the sake of our example) cloud DQ and observability, JSON (depending on tool) Deep SQL skills, Integration tools/methodologies - API as well as ETL, Data Analysis, Snowflake or Databricks knowledge (for lineage), Power BI (nice to have), SAP ECC knowledge (nice to have), experience with cloud platforms (Azure, AWS, Google).\nIf you are interested please share required details along with resume\nFull Name:\nCurrent or Previous organization:\nCurrent Location:\nTotal Experience:\nRelevant experience as Python Developer:\nhow many years of experience In Azure, AWS, Google\nHow many years of experience in UI, Workflows, security\nWorking as full time or contract:\nReason for job change:\nAny other offers inhand:\nCurrent CTC:\nexpected CTC:\nNotice Period:\nemail id:\ncontact Number :\nDomain name:\nare you ok to work Cotractual role?:\nshare your aadhar or pan card for the verification",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Temporary/Contractual","['Data quality analyst', 'cloud data quality', 'Azure', 'data quality developer', 'JSON', 'google', 'Informatica', 'AWS']",2025-06-14 05:22:55
DevOps/MLOps Engineer,Lam Research,8 - 13 years,Not Disclosed,['Bengaluru'],"The Group You ll Be A Part Of\nGlobal Information System - Enterprise Analytics and Engineering - DevOps\nThe Impact You ll Make\nDevelops, analyzes and maintains tools that support and automate processes for hardware or software product release. Writes installation scripts and programs for installation of products. Works with project teams to determine an appropriate build schedule and then initiates the build and packaging process. Compiles and assembles software from source code. Ensures tools are stored in source control. Supports developers for software package registry and delivery. Works with others to complete analysis, evaluations and design alternatives and to implement process improvements. Works with development engineers to correct build errors.\nWhat You ll Do\nApplication Ownership and Management: Take ownership of critical DevOps and development tools. This includes upgrades, performance optimization, access management, configuration, and vulnerability management.\nContractor Leadership: Collaborate with and lead outsourced managed services teams, ensuring accountability for application performance and maintenance.\nCI/CD Pipeline Development: Design, implement, and optimize CI/CD pipelines for various applications written in C/C++, Java, JavaScript, Node.js, Angular, Python, and other languages.\nDevSecOps Initiatives: Integrate security scanning tools into pipelines, and lead DevSecOps efforts across the organization.\nAutomation Development: Create custom automations using various product-specific APIs to generate reports, metrics, and enhance workflows.\nCloud Infrastructure Management: Work extensively with Azure cloud infrastructure, including services like Azure Databricks and Azure ML.\nAI and MLOps Support: Contribute to AI and MLOps initiatives, working with tools like Azure Databricks and OpenAI.\nWho We re Looking For\nWe are seeking a highly skilled DevOps Engineer to join our team and support our growing DevOps function. This role offers an exciting opportunity to work with a diverse set of applications and technologies, while driving automation and efficiency across our organization.\nMinimum of 8 years of related experience with a Bachelors degree in Computer Science, Software Engineering, or related field or 6 years and a Master s degree; or a PhD with 3 years experience; or equivalent experience.\nPreferred Qualifications\nRequired Skills and Qualifications:\nStrong programming skills in Python and Linux bash scripting\nExtensive experience with DevOps practices and tools\nProficiency in developing and maintaining CI/CD pipelines\nAbility to quickly learn and effectively utilize product-specific APIs\nFamiliarity with cloud platforms, particularly Azure\nExperience with containerization technologies (e.g., Docker, Kubernetes)\nStrong analytical and problem-solving skills\nExcellent communication abilities, both written and verbal\nAbility to quickly adapt to new technologies and learn independently\nBachelors degree in Computer Science, Software Engineering, or related field\nPreferred Qualifications:\nExperience with common DevOps tools and platforms (e.g., Jenkins, Artifactory, SonarQube)\nKnowledge of security best practices and DevSecOps principles\nFamiliarity with AI and MLOps concepts and tools (e.g., Azure Databricks, OpenAI)\nExperience with big data processing and analytics platforms\nExperience leading or mentoring other team members or contractors\nOur Commitment\nWe believe it is important for every person to feel valued, included, and empowered to achieve their full potential. By bringing unique individuals and viewpoints together, we achieve extraordinary results.\nLam Research (""Lam"" or the ""Company"") is an equal opportunity employer. Lam is committed to and reaffirms support of equal opportunity in employment and non-discrimination in employment policies, practices and procedures on the basis of race, religious creed, color, national origin, ancestry, physical disability, mental disability, medical condition, genetic information, marital status, sex (including pregnancy, childbirth and related medical conditions), gender, gender identity, gender expression, age, sexual orientation, or military and veteran status or any other category protected by applicable federal, state, or local laws. It is the Companys intention to comply with all applicable laws and regulations. Company policy prohibits unlawful discrimination against applicants or employees.\nLam offers a variety of work location models based on the needs of each role. Our hybrid roles combine the benefits of on-site collaboration with colleagues and the flexibility to work remotely and fall into two categories - On-site Flex and Virtual Flex. On-site Flex you ll work 3+ days per week on-site at a Lam or customer/supplier location, with the opportunity to work remotely for the balance of the week. Virtual Flex you ll work 1-2 days per week on-site at a Lam or customer/supplier location, and remotely the rest of the time.",Industry Type: Electronic Components / Semiconductors,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'C++', 'Automation', 'Linux', 'Infrastructure management', 'Access management', 'Flex', 'Packaging', 'Analytics', 'Python']",2025-06-14 05:22:57
Sap Customer Data Platform / Customer Data cloud Consultant,CMMI Level 5 Company!,7 - 12 years,Not Disclosed,"['Hyderabad', 'Pune', 'Bengaluru']","Work Location: Pune, Bangalore, Kolkata, Hyderabad, Gurugram\nExperience: 7+yrs\n\nSAP Customer Data Platform Expert\nJob description\nHighlights the need for a professional skilled in customer data management, integration, and personalized marketing, with expertise in SAP CDP.\nThe focus is on optimizing customer insights, ensuring smooth data integration, and providing actionable analytics to improve customer engagement across multiple channels.\n\nSAP Customer Data Cloud Expert\nJob description\nThe need for a professional skilled in management of SAPs Customer Data Cloud, ensuring seamless integration of customer data across systems, enhancing personalization, and ensuring data privacy compliance. The ideal candidate would have expertise in customer data management, SAP integrations, and a strong understanding of privacy regulations, enabling them to drive data-driven decision-making and improve customer engagement.\n\nPlease share your updated profile to suganya@spstaffing.in if interested.",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['SAP CDC', 'SAP CDP', 'Sap Customer Data Platform', 'Sap Customer Data cloud']",2025-06-14 05:23:00
