job_title,company,experience,salary,locations,description,industry,department,employment_type,skills,scraped_at
Data Scientist,AMERICAN EXPRESS,1 - 3 years,15-18 Lacs P.A.,"['Gurugram', 'Delhi / NCR']","You Lead the Way. Weve Got Your Back.\n\nWith the right backing, people and businesses have the power to progress in incredible ways. When you join Team Amex, you become part of a global and diverse community of colleagues with an unwavering commitment to back our customers, communities and each other. Here, youll learn and grow as we help you create a career journey thats unique and meaningful to you with benefits, programs, and flexibility that support you personally and professionally.",,,,"['Machine Learning', 'Python', 'SQL', 'Natural Language Processing', 'Ml']",2025-06-10 15:14:05
Data Scientist-Artificial Intelligence,IBM,3 - 7 years,Not Disclosed,['Pune'],"As an Associate Data Scientist at IBM, you will work to solve business problems using leading edge and open-source tools such as Python, R, and TensorFlow, combined with IBM tools and our AI application suites. You will prepare, analyze, and understand data to deliver insight, predict emerging trends, and provide recommendations to stakeholders.\n\nIn your role, you may be responsible for:\nImplementing and validating predictive and prescriptive models and creating and maintaining statistical models with a focus on big data & incorporating machine learning. techniques in your projects\nWriting programs to cleanse and integrate data in an efficient and reusable manner\nWorking in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors\nCommunicating with internal and external clients to understand and define business needs and appropriate modelling techniques to provide analytical solutions.\nEvaluating modelling results and communicating the results to technical and non-technical audiences\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nProof of Concept (POC) DevelopmentDevelop POCs to validate and showcase the feasibility and effectiveness of the proposed AI solutions.\nCollaborate with development teams to implement and iterate on POCs, ensuring alignment with customer requirements and expectations.\nHelp in showcasing the ability of Gen AI code assistant to refactor/rewrite and document code from one language to another, particularly COBOL to JAVA through rapid prototypes/ PoC\nDocument solution architectures, design decisions, implementation details, and lessons learned.\nCreate technical documentation, white papers, and best practice guides\n\n\nPreferred technical and professional experience\nStrong programming skills, with proficiency in Python and experience with AI frameworks such as TensorFlow, PyTorch, Keras or Hugging Face.\nUnderstanding in the usage of libraries such as SciKit Learn, Pandas, Matplotlib, etc. Familiarity with cloud platforms\nExperience and working knowledge in COBOL & JAVA would be preferred",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'scikit-learn', 'tensorflow', 'pytorch', 'keras', 'natural language processing', 'neural networks', 'predictive', 'huggingface', 'machine learning', 'prototype', 'artificial intelligence', 'sql', 'pandas', 'deep learning', 'r', 'java', 'cobol', 'data science', 'matplotlib', 'big data', 'statistics']",2025-06-10 15:14:08
Data Scientist - L3,Wipro,3 - 5 years,Not Disclosed,['Ramdurg'],"Role Purpose\nThe purpose of the role is to define, architect and lead delivery of machine learning and AI solutions.\nDo\n1. Demand generation through support in Solution development\na. Support Go-To-Market strategy\ni. Contribute to development solutions, proof of concepts aligned to key offerings to enable solution led sales\nb. Collaborate with different colleges and institutes for research initiatives and provide data science courses\n2. Revenue generation through Building & operationalizing Machine Learning, Deep Learning solutions\na. Develop Machine Learning / Deep learning models for decision augmentation or for automation solutions\nb. Collaborate with ML Engineers, Data engineers and IT to evaluate ML deployment options\n3. Team Management\na. Talent Management\ni. Support on boarding and training to enhance capability & effectiveness\nDeliver\n\nNo.Performance ParameterMeasure\n1.Demand generation# PoC supported\n2.Revenue generation through deliveryTimeliness, customer success stories, customer use cases\n3.Capability Building & Team Management# Skills acquired\n\n\nMandatory Skills: Data Analysis.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Analysis', 'python', 'team management', 'natural language processing', 'scikit-learn', 'ml deployment', 'machine learning', 'data engineering', 'artificial intelligence', 'sql', 'deep learning', 'tensorflow', 'data science', 'predictive modeling', 'statistical modeling', 'ml']",2025-06-10 15:14:10
Data Scientist,Wipro,2 - 7 years,Not Disclosed,['Bengaluru'],"Role Purpose\nThe purpose of this role is to design, test and maintain software programs for operating systems or applications which needs to be deployed at a client end and ensure its meet 100% quality assurance parameters\nDo\nInstrumental in understanding the requirements and design of the product/ software\nDevelop software solutions by studying information needs, studying systems flow, data usage and work processes\nInvestigating problem areas followed by the software development life cycle\nFacilitate root cause analysis of the system issues and problem statement\nIdentify ideas to improve system performance and impact availability\nAnalyze client requirements and convert requirements to feasible design\nCollaborate with functional teams or systems analysts who carry out the detailed investigation into software requirements\nConferring with project managers to obtain information on software capabilities\nPerform coding and ensure optimal software/ module development\nDetermine operational feasibility by evaluating analysis, problem definition, requirements, software development and proposed software\nDevelop and automate processes for software validation by setting up and designing test cases/scenarios/usage cases, and executing these cases\nModifying software to fix errors, adapt it to new hardware, improve its performance, or upgrade interfaces.\nAnalyzing information to recommend and plan the installation of new systems or modifications of an existing system\nEnsuring that code is error free or has no bugs and test failure\nPreparing reports on programming project specifications, activities and status\nEnsure all the codes are raised as per the norm defined for project / program / account with clear description and replication patterns\nCompile timely, comprehensive and accurate documentation and reports as requested\nCoordinating with the team on daily project status and progress and documenting it\nProviding feedback on usability and serviceability, trace the result to quality risk and report it to concerned stakeholders\nStatus Reporting and Customer Focus on an ongoing basis with respect to project and its execution\nCapturing all the requirements and clarifications from the client for better quality work\nTaking feedback on the regular basis to ensure smooth and on time delivery\nParticipating in continuing education and training to remain current on best practices, learn new programming languages, and better assist other team members.\nConsulting with engineering staff to evaluate software-hardware interfaces and develop specifications and performance requirements\nDocument and demonstrate solutions by developing documentation, flowcharts, layouts, diagrams, charts, code comments and clear code\nDocumenting very necessary details and reports in a formal way for proper understanding of software from client proposal to implementation\nEnsure good quality of interaction with customer w.r.t. e-mail content, fault report tracking, voice calls, business etiquette etc\nTimely Response to customer requests and no instances of complaints either internally or externally\nStakeholder Interaction\n\nStakeholder Type\nStakeholder Identification\nPurpose of Interaction\nInternal\nLead Software Developer and Project Manager\nRegular reporting & updates\nSoftware Developers\nFor work coordination and support in providing testing solutions\nExternal\nClients\nProvide apt solutions and support as per the requirement\nDisplay\nLists the competencies required to perform this role effectively:\nFunctional Competencies/ Skill\nLeveraging Technology Knowledge of current and upcoming technology along with expertise in programming (automation, tools and systems) to build efficiencies and effectiveness in own function/ Client organization Competent\nProcess Excellence - Ability to follow the standards and norms to produce consistent results, provide effective control and reduction of risk Expert\nTechnical knowledge knowledge of various programming languages, tools, quality management standards and processes - Expert\n\nCompetency Levels\nFoundation\nKnowledgeable about the competency requirements. Demonstrates (in parts) frequently with minimal support and guidance.\nCompetent\nConsistently demonstrates the full range of the competency without guidance. Extends the competency to difficult and unknown situations as well.\nExpert\nApplies the competency in all situations and is serves as a guide to others as well.\nMaster\nCoaches others and builds organizational capability in the competency area. Serves as a key resource for that competency and is recognised within the entire organization.\nBehavioral Competencies\nFormulation & Prioritization\nInnovation\nManaging Complexity\nExecution Excellence\nPassion for Results\nDeliver / No. / Performance Parameter / Measure -\n1. Continuous Integration, Deployment & Monitoring of Software\n100% error free on boarding & implementation, throughput %, Adherence to the schedule/ release plan\n2. Quality & CSAT\nOn-Time Delivery, Manage software, Troubleshoot queries\nCustomer experience, completion of assigned certifications for skill upgradation\n3. MIS & Reporting\n100% on time MIS & report generation\nMandatory Skills: Python, GenAI, AWS\nPreferred Skills: NLP , AI/ML , LLM\nArchitect and implement Al solutions utilizing cutting-edge technologies like LLM, Langchain, and Machine Learning.\nAIML solution development in Azure using Python\nAbility to build and finetune the model to improve the performance\nCreate own technology if off-the-shelf technology is not solving the problem. E.g changes to traditional RAG approaches, finetune LLM, create architectures.\nUse experience and advise leadership and team of data scientists best approaches, architectures for complex ML use cases.\nLead from the front, responsible for coding, designing, and ensuring best practices & frameworks are adhered by the team.\nCreate end to end AI systems with responsible AI principles\nDevelop data pipelines using SQL to extract and transform data from Snowflake for Al model training and inference.\nPossess expertise in Natural Language Processing (NLP) & GenAI to integrate text-based data sources into the Al architecture.\nCollaborate with data scientists and engineers to ensure seamless integration of Al components into existing systems.\nResponsible for continuous communication about the team progress to keystakeholder.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'GenAI', 'NLP', 'Artificial Intelligence', 'LLM', 'AWS', 'Machine Learning', 'Python']",2025-06-10 15:14:13
Data Scientist - L3,Wipro,3 - 5 years,Not Disclosed,['Bengaluru'],"Role Purpose\n\nThe purpose of the role is to define, architect and lead delivery of machine learning and AI solutions.\n\nDo\n\n1. Demand generation through support in Solution development\n\na. Support Go-To-Market strategy\n\ni. Contribute to development solutions, proof of concepts aligned to key offerings to enable solution led sales\n\nb. Collaborate with different colleges and institutes for research initiatives and provide data science courses\n\n\n\na. Develop Machine Learning / Deep learning models for decision augmentation or for automation solutions\n\nb. Collaborate with ML Engineers, Data engineers and IT to evaluate ML deployment options\n\n\n\na. Talent Management\n\ni. Support on boarding and training to enhance capability & effectiveness",,,,"['Data Science', 'deep learning', 'data analysis', 'machine learning', 'data engineering']",2025-06-10 15:14:16
Data Scientist-Artificial Intelligence,IBM,3 - 7 years,Not Disclosed,['Bengaluru'],"As an Data Engineer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\n\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\n\n In this role, your responsibilities may include: \nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviours’.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modelling results\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nProof of Concept (POC) DevelopmentDevelop POCs to validate and showcase the feasibility and effectiveness of the proposed AI solutions.\nHelp in showcasing the ability of Gen AI code assistant to refactor/rewrite and document code from one language to another\nDocument solution architectures, design decisions, implementation details, and lessons learned.\nStay up to date with the latest trends and advancements in AI, foundation models, and large language models.\nEvaluate emerging technologies, tools, and frameworks to assess their potential impact on solution design and implementation\n\n\nPreferred technical and professional experience\nExperience and working knowledge in COBOL & JAVA would be preferred\nHaving experience in Code generation, code matching & code translation leveraging LLM capabilities would be a Big plus\nDemonstrate a growth mindset to understand clients' business processes and challenges",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['elastic search', 'java', 'proof of concept', 'cobol', 'splunk', 'targetlink', 'simulink', 'data management', 'stateflow', 'sil', 'big data', 'can bus', 'matlab', 'python', 'c', 'predictive', 'machine learning', 'presales', 'autosar', 'code generation', 'rtw', 'rfi', 'embedded c', 'model based development', 'rfp']",2025-06-10 15:14:19
Data Scientist,Capgemini,4 - 7 years,Not Disclosed,['Pune'],"\n\nWorks in the area of Software Engineering, which encompasses the development, maintenance and optimization of software solutions/applications.\n1. Applies scientific methods to analyse and solve software engineering problems.\n2. He/she is responsible for the development and application of software engineering practice and knowledge, in research, design, development and maintenance.\n3. His/her work requires the exercise of original thought and judgement and the ability to supervise the technical and administrative work of other software engineers.\n4. The software engineer builds skills and expertise of his/her software engineering discipline to reach standard software engineer skills expectations for the applicable role, as defined in Professional Communities.\n5. The software engineer collaborates and acts as team player with other software engineers and stakeholders.\nWorks in the area of Software Engineering, which encompasses the development, maintenance and optimization of software solutions/applications.1. Applies scientific methods to analyse and solve software engineering problems.2. He/she is responsible for the development and application of software engineering practice and knowledge, in research, design, development and maintenance.3. His/her work requires the exercise of original thought and judgement and the ability to supervise the technical and administrative work of other software engineers.4. The software engineer builds skills and expertise of his/her software engineering discipline to reach standard software engineer skills expectations for the applicable role, as defined in Professional Communities.5. The software engineer collaborates and acts as team player with other software engineers and stakeholders.\n\n - Grade Specific \nIs highly respected, experienced and trusted. Masters all phases of the software development lifecycle and applies innovation and industrialization. Shows a clear dedication and commitment to business objectives and responsibilities and to the group as a whole. Operates with no supervision in highly complex environments and takes responsibility for a substantial aspect of Capgeminis activity. Is able to manage difficult and complex situations calmly and professionally. Considers the bigger picture when making decisions and demonstrates a clear understanding of commercial and negotiating principles in less-easy situations. Focuses on developing long term partnerships with clients. Demonstrates leadership that balances business, technical and people objectives. Plays a significant part in the recruitment and development of people.\n\n Skills (competencies) \n\nVerbal Communication",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'natural language processing', 'machine learning', 'deep learning', 'data science', 'gsm', 'rf engineering', 'data analysis', 'artificial intelligence', 'r', 'rf optimization', '3g', 'predictive modeling', 'lte', 'rf planning', 'statistics', 'drive test']",2025-06-10 15:14:22
Data Scientist - Cybersecurity,Visa,2 - 7 years,Not Disclosed,['Bengaluru'],"As part of Cyber Threat Analytics and Research team (CTAR) , you will leverage cutting-edge technologies to perform statistical profiling, inference, classification, clustering and predictive analysis. As a key member of the technical team, you will create and implement sophisticated machine learning models to help derive new insights to defend against cyber-attacks. You will be working with a large variety of data sets, cutting-edge security technologies, and world-class operation teams to create awesome analytics for security and other business units.\n  Essential Functions:\nAnalyze cyber event logs using Spark and big data technologies and develop deeper insights into products using advanced statistical methods.\nFormulate cyber threat scenarios into technical data problems and develop high fidelity models to capture unseen threats\nDevise and implement deep learning models for building user behavior profiles. This includes data acquisition, feature engineering, model development, and deployment.\nConduct feature engineering on various data sources to build and enrich feature store\nFine tune open source LLM to detect anomalous user behavior\nLeverage Generative AI to perform RAG for helping improve Cyber investigation efficiency\nAs a member of the CTAR team, you will work closely with other data scientists and data engineers to build, design, engineer, and develop analytical software and services that deliver security functionality and improve security efficiency and capabilities through automation.\nAssist in shaping overall direction, life-cycle management, and leadership for Information Security architecture and technology related to Visa.\nCommunicate clean and persuasive data directly to end users, leadership, and other stakeholders, technical and non-technical.\n\n\n\nBasic Qualifications:\n2+ years of relevant work experience and a Bachelors degree, OR 5+ years of relevant work experience\n\nPreferred Qualifications:\n3 or more years of work experience with a Bachelor s Degree or more than 2 years of work experience with an Advanced Degree (e.g. Masters, MBA, JD, MD)\nSolid background and hands on experiences with building Machine learning, deep learning and AI models\nExperience with Generative AI/LLM\nExcellent understanding of algorithms and data structures and proficiency in Python and SQL.\nExperience working with large datasets using tools and Hadoop, Spark, or Hive\nExcellent analytic and problem-solving capability combined with ambition to solve hard problems\nStrong communications skills and ability to collaborate\nHighly driven, resourceful and results oriented\nGood team player and excellent interpersonal skills\nDemonstrated ability to lead and navigate through ambiguity",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Automation', 'Information security', 'Analytical', 'Machine learning', 'Data structures', 'Open source', 'Analytics', 'Monitoring', 'SQL', 'Python']",2025-06-10 15:14:24
Data Scientist - L3,Wipro,3 - 6 years,Not Disclosed,['Bengaluru'],"Wipro Limited (NYSEWIT, BSE507685, NSEWIPRO) is a leading technology services and consulting company focused on building innovative solutions that address clients’ most complex digital transformation needs. Leveraging our holistic portfolio of capabilities in consulting, design, engineering, and operations, we help clients realize their boldest ambitions and build future-ready, sustainable businesses. With over 230,000 employees and business partners across 65 countries, we deliver on the promise of helping our customers, colleagues, and communities thrive in an ever-changing world. For additional information, visit us at www.wipro.com.\n About The Role  \n\nRole Purpose\n\nThe purpose of the role is to define, architect and lead delivery of machine learning and AI solutions.\n\n ? \n\nDo\n\n1. Demand generation through support in Solution development\n\na. Support Go-To-Market strategy\n\ni. Contribute to development solutions, proof of concepts aligned to key offerings to enable solution led sales\n\nb. Collaborate with different colleges and institutes for research initiatives and provide data science courses\n\n2. Revenue generation through Building & operationalizing Machine Learning, Deep Learning solutions\n\na. Develop Machine Learning / Deep learning models for decision augmentation or for automation solutions\n\nb. Collaborate with ML Engineers, Data engineers and IT to evaluate ML deployment options\n\n3. Team Management\n\na. Talent Management\n\ni. Support on boarding and training to enhance capability & effectiveness\n\n ? \n\nDeliver\n\n\nNo.\n\nPerformance Parameter\n\nMeasure 1. Demand generation # PoC supported 2. Revenue generation through delivery Timeliness, customer success stories, customer use cases 3. Capability Building & Team Management # Skills acquired\n\n\n ? \n\n ? \nMandatory\n\nSkills:\nData Analysis.\n\nExperience3-5 Years.\n\nReinvent your world. We are building a modern Wipro. We are an end-to-end digital transformation partner with the boldest ambitions. To realize them, we need people inspired by reinvention. Of yourself, your career, and your skills. We want to see the constant evolution of our business and our industry. It has always been in our DNA - as the world around us changes, so do we. Join a business powered by purpose and a place that empowers you to design your own reinvention. Come to Wipro. Realize your ambitions. Applications from people with disabilities are explicitly welcome.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analysis', 'machine learning', 'deep learning', 'data science', 'ml', 'python', 'natural language processing', 'scikit-learn', 'neural networks', 'ml deployment', 'data engineering', 'artificial intelligence', 'sql', 'tensorflow', 'r', 'predictive modeling', 'statistical modeling', 'statistics']",2025-06-10 15:14:27
Data Scientist,Simplify Healthcare,5 - 10 years,Not Disclosed,"['Pune( Hadapsar, Kharadi, Keshav Nagar, Vishrantwadi, Dhanori, Mundhwa, Viman Nagar )']","Engineer/Sr. Engineer Data Science\nLocation: Pune, India\n\nCompany Overview:\nSimplify Healthcare is one of the fastest-growing healthcare technology solutions providers serving the US health insurance (Payer) industry. Headquartered in Chicago with a Global Delivery Centre in Pune, we are trusted by 65+ payer organizations and supported by a team of 800+ professionals.\nWe specialize in delivering SaaS-based enterprise software solutions focused on product and benefits configuration, provider lifecycle management, and more. In 2023, we launched Simplify Health Cloud, our flagship Payer Platform, establishing our position as a leader in cloud-native, low-code configurable platforms for the healthcare sector.\nWith our strategic acquisition of Virtical.ai in 2024, we’re accelerating innovation through AI integration, particularly in areas such as LLMs, conversational AI, and cloud-based intelligence. Our proprietary Simplify App Fabric™ enables fast, secure, and low-code development for modern Payer solutions.\nOur innovation has earned us repeated recognition in Deloitte Technology Fast 500™, Inc. 5000, and reports by IDC and Gartner.",,,,"['Speech Recognition', 'Artificial Intelligence', 'Natural Language Processing', 'Conversational Ai', 'Chatbot', 'Cognitive Services', 'Text Mining', 'Machine Learning', 'Azure Cognitive Services']",2025-06-10 15:14:30
Data Scientist-Artificial Intelligence,IBM,3 - 7 years,Not Disclosed,['Bengaluru'],"An AI Data Scientist at IBM is not just a job title - it’s a mindset. You’ll leverage the watsonx,AWS Sagemaker,Azure Open AI platform to co-create AI value with clients, focusing on technology patterns to enhance repeatability and delight clients.\n\nWe are seeking an experienced and innovative AI Data Scientist to be specialized in foundation models and large language models. In this role, you will be responsible for architecting and delivering AI solutions using cutting-edge technologies, with a strong focus on foundation models and large language models. You will work closely with customers, product managers, and development teams to understand business requirements and design custom AI solutions that address complex challenges. Experience with tools like Github Copilot, Amazon Code Whisperer etc. is desirable.\n\nSuccess is our passion, and your accomplishments will reflect this, driving your career forward, propelling your team to success, and helping our clients to thrive.\n\nDay-to-Day Duties:\nProof of Concept (POC) DevelopmentDevelop POCs to validate and showcase the feasibility and effectiveness of the proposed AI solutions. Collaborate with development teams to implement and iterate on POCs, ensuring alignment with customer requirements and expectations.\nHelp in showcasing the ability of Gen AI code assistant to refactor/rewrite and document code from one language to another, particularly COBOL to JAVA through rapid prototypes/ PoC\nDocumentation and Knowledge SharingDocument solution architectures, design decisions, implementation details, and lessons learned. Create technical documentation, white papers, and best practice guides. Contribute to internal knowledge sharing initiatives and mentor new team members.\nIndustry Trends and InnovationStay up to date with the latest trends and advancements in AI, foundation models, and large language models. Evaluate emerging technologies, tools, and frameworks to assess their potential impact on solution design and implementation\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nStrong programming skills, with proficiency in Python and experience with AI frameworks such as TensorFlow, PyTorch, Keras or Hugging Face. Understanding in the usage of libraries such as SciKit Learn, Pandas, Matplotlib, etc. Familiarity with cloud platforms (e.g. Kubernetes, AWS, Azure, GCP) and related services is a plus.\nExperience and working knowledge in COBOL & JAVA would be preferred\no Having experience in Code generation, code matching & code translation leveraging LLM capabilities would be a Big plus (e.g. Amazon Code Whisperer, Github Copilot etc.) Soft\n\nSkills:\nExcellent interpersonal and communication skills. Engage with stakeholders for analysis and implementation. Commitment to continuous learning and staying updated with advancements in the field of AI.\nGrowth mindsetDemonstrate a growth mindset to understand clients' business processes and challenges.\nExperience in python and pyspark will be added advantage\n\n\nPreferred technical and professional experience\nExperienceProven experience in designing and delivering AI solutions, with a focus on foundation models, large language models, exposure to open source, or similar technologies. Experience in natural language processing (NLP) and text analytics is highly desirable. Understanding of machine learning and deep learning algorithms.\nStrong track record in scientific publications or open-source communities\nExperience in full AI project lifecycle, from research and prototyping to deployment in production environments",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'machine learning', 'tensorflow', 'pytorch', 'keras', 'kubernetes', 'github', 'natural language processing', 'scikit-learn', 'pyspark', 'microsoft azure', 'artificial intelligence', 'text analytics', 'pandas', 'deep learning', 'java', 'code generation', 'cobol', 'gcp', 'matplotlib', 'aws']",2025-06-10 15:14:34
Data Scientist,"NTT DATA, Inc.",2 - 5 years,Not Disclosed,['Bengaluru'],"Your day at NTT DATA\nThe Senior Data Scientist is an advanced subject matter expert, tasked with taking accountability in the adoption of data science and analytics within the organization.\n\nThe primary responsibility of this role is to participate in the creation and delivery of data-driven solutions that add business value using statistical models, machine learning algorithms, data mining, and visualization techniques.\n\nWhat youll be doing\n\nKey Responsibilities:\nDesigns, develops, and programs methods, processes, and systems to consolidate and analyze unstructured, diverse big data sources to generate actionable insights and solutions for client services and product enhancement.\nDesigns and enhances data collection procedures to include information that is relevant for building analytic systems.\nResponsible for ensuring that data used for analysis is processed, cleaned and, integrally verified and build algorithms necessary to find meaningful answers.\nDesigns and codes software programs, algorithms, and automated processes to cleanse, integrate and evaluate large datasets from multiple disparate sources\nProvides meaningful insights from large data and metadata sources; interprets and communicates insights and findings from analysis and experiments to product, service, and business managers.\nDirects scalable and highly available applications leveraging the latest tools and technologies.\nAccountable for creatively visualizing and effectively communicating results of data analysis, insights, and ideas in a variety of formats to key decision-makers within the business.\nCreates SQL queries for the analysis of data and visualizes the output of the models.\nResponsible for ensuring that industry standards best practices are applied to development activities.\nKnowledge and Attributes:\nAdvanced understanding of data modelling, statistical methods and machine learning techniques.\nStrong ability to thrive in a dynamic, fast-paced environment.\nStrong quantitative and qualitative analysis skills.\nDesire to acquire more knowledge to keep up to speed with the ever-evolving field of data science.\nCuriosity to sift through data to find answers and more insights.\nAdvanced understanding of the information technology industry within a matrixed organization and the typical business problems such organizations face.\nStrong ability to translate technical findings clearly and fluently to non-technical team business stakeholders to enable informed decision-making.\nStrong ability to create a storyline around the data to make it easy to interpret and understand.\nSelf-driven and able to work independently yet acts as a team player.\nAcademic Qualifications and Certifications:\nBachelors degree or equivalent in Data Science, Business Analytics, Mathematics, Economics, Engineering, Computer Science or a related field.\nRelevant programming certification preferred.\nAgile certification preferred.\nRequired Experience:\nAdvanced demonstrated experience in a data science position in a corporate environment and/or related industry.\nAdvanced demonstrated experience in statistical modelling and data modelling, machine learning, data mining, unstructured data analytics, natural language processing.\nAdvanced demonstrated experience in programming languages (R, Python, etc.).\nAdvanced demonstrated experience working with and creating data architectures.\nAdvanced demonstrated experience with extracting, cleaning, and transforming data and working with data owners to understand the data.\nAdvanced demonstrated experience visualizing and/or presenting data for stakeholder use and reuse across the business.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data science', 'R', 'data modelling', 'data mining', 'statistical modelling', 'machine learning', 'Python', 'SQL']",2025-06-10 15:14:36
Data Scientist-Artificial Intelligence,IBM,5 - 7 years,Not Disclosed,['Mumbai'],"Work with broader team to build, analyze and improve the AI solutions.\nYou will also work with our software developers in consuming different enterprise applications\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nResource should have 5-7 years of experience. Sound knowledge of Python and should know how to use the ML related services.\nProficient in Python with focus on Data Analytics Packages.\nStrategy Analyse large, complex data sets and provide actionable insights to inform business decisions. Strategy Design and implementing data models that help in identifying patterns and trends. Collaboration Work with data engineers to optimize and maintain data pipelines.\nPerform quantitative analyses that translate data into actionable insights and provide analytical, data-driven decision-making. Identify and recommend process improvements to enhance the efficiency of the data platform. Develop and maintain data models, algorithms, and statistical models\n\n\nPreferred technical and professional experience\nExperience with conversation analytics. Experience with cloud technologies\nExperience with data exploration tools such as Tableu",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['algorithms', 'python', 'data analytics', 'tableau', 'ml', 'hive', 'data analysis', 'natural language processing', 'pyspark', 'data warehousing', 'machine learning', 'artificial intelligence', 'sql', 'pandas', 'deep learning', 'java', 'data science', 'spark', 'kafka', 'hadoop', 'big data', 'aws', 'etl']",2025-06-10 15:14:39
Data Scientist,Evnek,2 - 3 years,Not Disclosed,['Bengaluru'],"Job Title: Data Scientist OpenCV\nExperience: 23 Years\nLocation: Bangalore\nNotice Period: Immediate JoinersOnly\n\nJob Overview\nWe are looking for a passionate and driven Data Scientistwith a strong foundation in computer vision, image processing, and OpenCV. Thisrole is ideal for professionals with 23 years of experience who are excitedabout working on real-world visual data problems and eager to contribute toimpactful projects in a collaborative environment.\nKey Responsibilities\nDevelop and implement computer vision solutionsusing OpenCV and Python.\nWork on tasks including object detection,recognition, tracking, and image/video enhancement.\nClean, preprocess, and analyze large image andvideo datasets to extract actionable insights.\nCollaborate with senior data scientists andengineers to deploy models into production pipelines.\nContribute to research and proof-of-conceptprojects in the field of computer vision and machine learning.\nPrepare clear documentation for models,experiments, and technical processes.\nRequired Skills\nProficient in OpenCV and image/video processingtechniques.\nStrong coding skills in Python, with familiarityin libraries such as NumPy, Pandas, Matplotlib.\nSolid understanding of basic machine learningand deep learning concepts.\nHands-on experience with Jupyter Notebooks;exposure to TensorFlow or PyTorch is a plus.\nExcellent analytical, problem-solving, anddebugging skills.\nEffective communication and collaborationabilities.\n  Preferred Qualifications\nBachelor\\u2019s degree in computer science, DataScience, Electrical Engineering, or a related field.\nPractical exposure through internships oracademic projects in computer vision or image analysis.\nFamiliarity with cloud platforms (AWS, GCP,Azure) is an added advantage.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Electrical engineering', 'Computer vision', 'deep learning', 'data science', 'Coding', 'Opencv', 'Analytical', 'Machine learning', 'Debugging', 'Python']",2025-06-10 15:14:42
Specialist Data Scientist,NICE,8 - 11 years,Not Disclosed,['Pune'],"So, what’s the role all about?\nNICE provides state-of-the-art enterprise level AI and analytics for all forms of business communications between speech and digital.   We are a world class research team developing new algorithms and approaches to help companies with solving critical issues such as identifying their best performing agents, preventing fraud, categorizing customer issues, and determining overall customer satisfaction.  If you have interacted with a major contact center in the last decade, it is very likely we have processed your call. \nThe research group partners with all areas of NICE’s business to scale out the delivery of new technology and AI models to customers around the world that are tailored to their company, industry, and language needs.",,,,"['python', 'confluence', 'natural language processing', 'presentation skills', 'big data technologies', 'pyspark', 'microsoft azure', 'bert', 'machine learning', 'sql', 'tensorflow', 'data science', 'gcp', 'pytorch', 'machine learning algorithms', 'aws', 'big data', 'communication skills', 'statistics', 'jira']",2025-06-10 15:14:45
Data Scientist - SCB,Wipro,8 - 10 years,Not Disclosed,['Chennai'],"About The Role  \n\nRole Purpose\n\nThe purpose of the role is to create exceptional architectural solution design and thought leadership and enable delivery teams to provide exceptional client engagement and satisfaction.\n\n\n ? \n\nMandatory Skills\n\nData Science, ML, DL, NLP or Computer Vision, Python, Tensorflow, Pytorch, Django, PostgreSQL\n\nPreferred Skills\n\nGen AI, LLM, RAG, Lanchain, Vector DB, Azure Cloud, MLOps, Banking exposure\n\n\n ? \n\n \n\n3.Competency Building and Branding  \nEnsure completion of necessary trainings and certifications\nDevelop Proof of Concepts (POCs),case studies, demos etc. for new growth areas based on market and customer research\nDevelop and present a point of view of Wipro on solution design and architect by writing white papers, blogs etc.\nAttain market referencability and recognition through highest analyst rankings, client testimonials and partner credits\nBe the voice of Wipro’s Thought Leadership by speaking in forums (internal and external)\nMentor developers, designers and Junior architects in the project for their further career development and enhancement\nContribute to the architecture practice by conducting selection interviews etc\n\n\n ? \n\nMandatory\nStrong understanding of Data Science, machine learning and deep learning principles and algorithms.\nProficiency in programming languages such as Python, TensorFlow, and PyTorch.\nAbility to work with large datasets and knowledge of data preprocessing techniques.\nStrong Backend Python developer\nExperience in applying machine learning techniques,\n\nNatural Language Processing or Computer Vision using TensorFlow, Pytorch\nBuild and deploy end to end ML models and leverage metrics to support predictions, recommendations, search, and growth strategies\nExpert in applying ML techniques such asclassification, clustering, deep learning, optimization methods, supervised and unsupervised techniques\nOptimize model performance and scalability for real-time inference and deployment.\nExperiment with different hyperparameters and model configurations to improve AI model quality.\nEnsure AI ML solutions are developed, and validations are performed in accordance with Responsible AI guidelines.\n\n\n ? \n\n \n\n4.Team Management  \n\n Resourcing  \nAnticipating new talent requirements as per the market/ industry trends or client requirements\nHire adequate and right resources for the team\nTalent Management\nEnsure adequate onboarding and training for the team members to enhance capability & effectiveness\nBuild an internal talent pool and ensure their career progression within the organization\nManage team attrition\nDrive diversity in leadership positions\nPerformance Management\nSet goals for the team, conduct timely performance reviews and provide constructive feedback to own direct reports\nEnsure that the Performance Nxt is followed for the entire team\nEmployee Satisfaction and Engagement\nLead and drive engagement initiatives for the team\nTrack team satisfaction scores and identify initiatives to build engagement within the team\nMandatory\n\nSkills:\nGenerative AI.\n\nExperience8-10 Years.\n\nReinvent your world. We are building a modern Wipro. We are an end-to-end digital transformation partner with the boldest ambitions. To realize them, we need people inspired by reinvention. Of yourself, your career, and your skills. We want to see the constant evolution of our business and our industry. It has always been in our DNA - as the world around us changes, so do we. Join a business powered by purpose and a place that empowers you to design your own reinvention. Come to Wipro. Realize your ambitions. Applications from people with disabilities are explicitly welcome.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'natural language processing', 'machine learning', 'deep learning', 'tensorflow', 'algorithms', 'dl', 'azure cloud', 'sql', 'gen', 'django', 'data science', 'postgresql', 'predictive modeling', 'computer vision', 'pytorch', 'statistical modeling', 'vector', 'clustering', 'db', 'ml', 'statistics']",2025-06-10 15:14:47
Data Scientist / Machine Learning Professional,Infosys,6 - 8 years,Not Disclosed,['Bengaluru'],"Responsibilities\nJob Title:\nLead Analyst - Data Science (Intermediate/Senior)Role Summary:\nWe are looking for candidates to build and implement analytics solutions to our esteemed clients. The incumbent should have strong aptitude for numbers, experience in any domain and willingness to learn some cutting edge technologies\nTechnical and Professional Requirements:\nOther key to have Skills:\nSQL knowledge and experience working with relational databases.\nUnderstanding of any one of domain (Eg: Retail, Supply chain, Logistics, Manufacturing).\nUnderstanding of the project lifecycles: waterfall and agile.\nSoft Skills:\nStrong verbal and written communication skills with the ability to work well in a team.\nStrong customer focus, ownership, urgency and drive.\nAbility to handle multiple, competing priorities in a fast-paced environment.\nWork well with the team members to maintain high credibility\nWork Experience:\nOverall 6-8 of years of experience in Data Analytics, Data Science and Machine Learning.\nEducational Requirements (any of the following):\nBachelor of Engineering/Bachelor of Technology in any stream with consistent academic track record.\nBachelor's degree in a quantitative discipline (e.g., Statistics, Economics, Mathematics, Marketing Analytics) or significant relevant coursework with consistent academic track record.\nPreferred Skills:\nTechnology->Data Science->Machine Learning\nAdditional Responsibilities:\nAdditional Academic Qualification (good to have):\nMasters in any area related to Science, Mathematics, Statistics, Economy and Finance with consistent academic track record.\nPhD in any stream.Location:\nBangalore, Pune, Hyderabad, Chennai, Trivandrum, Mysore\nEducational Requirements\nMCA,MSc,Bachelor of Engineering,BBA,BCom\nService Line\nData & Analytics Unit\n* Location of posting is subject to business requirements",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Machine Learning', 'Data Science', 'waterfall', 'Data Scientist', 'agile', 'Data Analytics', 'relational databases', 'SQL']",2025-06-10 15:14:50
Data Scientist - AI,Standard Chartered,3 - 7 years,Not Disclosed,['Bengaluru'],"We are looking for a Data Scientist with strong data and analytical skills, you will be tasked with managing all aspects any new work/project assessments thoroughly from inception to delivery. You will need to build close working relationships with COO business and CDO in order to gather specific modelling requirements and define and scope the work. In addition, as Data Scientist you will also work closely with internal staff, clients and 3rd parties.\n\nKey Responsibilities\nDesign Machine Learning, Natural Language and Decision Optimisation applications architecture.\n\nDesign ML, NLP and DO models, algorithms for predictive prescriptive analytics.\n\nTranslate business requirements to reporting dashboard and analytics.\n\nPerform advanced analytics and statistical modelling on structured and unstructured data.\n\nWork with software engineers to integrate and deploy AI applications.\n\nAssist in the creation of data definitions for new database file/table development and/or changes to existing ones as needed for analysis.\n\nSupport the business with clear and insightful analysis applying advanced modelling techniques leveraging the data at hand.\n\nRespond to and resolve data mining performance issues.\n\nMonitor data mining system performance and implement efficiency improvements.\n\nDevelop and implement change management plans to ensure successful adoption of AI solutions across the organization.\n\nProvide training and support to business users to help them understand and leverage AI tools and technologies.\n\nFoster a culture of innovation and continuous improvement by promoting the benefits of AI and encouraging experimentation.\n\nWork with senior management to develop and refine the banks AI strategy, ensuring alignment with overall business objectives.\n\nIdentify and prioritize AI use cases that have the potential to deliver significant business value.\n\nDevelop business cases for AI projects, including cost-benefit analysis, risk assessment, and ROI estimation.\n\nSkills and Experience\n8+ Years of Software development experience using . Net framework or Java\n\nCompleted real world ML, NLP and DO projects using R or Python\n\nExtensive experience in SQL and NoSQL database design, queries and stored procedures\n\nHands-on experience with Windows Server, Azure, AWS and Git\n\nHighly proficient in data visualisation tools. . .\n\nDocumentation - Requirements/Use Cases/Business Rules/User Stories, Etc.\n\nReport Types - Gap Analysis/Problem Analysis/Initial Assessments, Etc.\n\nProcess/Data Modelling - As Is/To Be/ Visio/Enterprise Architect/System Architect, Etc.\n\nStrong query language skills (SQL, Hive, ETL, Hadoop, Spark, R, Python)\n\nGood experience with Business Intelligence tools and Decision Support Systems\n\nStrong data analysis skills using Hive, Spark, R, Python, Dremio, MicroStrategy and Tableau.\n\nProven experience in working with key stakeholders within the business.\n\nProven problem-solving skills\n\nWorkshop Facilitation\n\nQualifications\nMaster or PHD in Mathematics, Statistics, Knowledge Engineering or Data science.\n\n\nreal world exposure on ML, NLP and DO projects using R or Python\n\nAbout Standard Chartered\nWere an international bank, nimble enough to act, big enough for impact. For more than 170 years, weve worked to make a positive difference for our clients, communities, and each other. We question the status quo, love a challenge and enjoy finding new opportunities to grow and do better than before. If youre looking for a career with purpose and you want to work for a bank making a difference, we want to hear from you. You can count on us to celebrate your unique talents and we cant wait to see the talents you can bring us.\n\n\nOur purpose, to drive commerce and prosperity through our unique diversity, together with our brand promise, to be here for good are achieved by how we each live our valued behaviours. When you work with us, youll see how we value difference and advocate inclusion.\n\n\nTogether we:\n\n\n\nDo the right thing and are assertive, challenge one another, and live with integrity, while putting the client at the heart of what we do\n\nNever settle, continuously striving to improve and innovate, keeping things simple and learning from doing well, and not so well\n\nAre better together, we can be ourselves, be inclusive, see more good in others, and work collectively to build for the long term\n\nWhat we offer\nIn line with our Fair Pay Charter, we offer a competitive salary and benefits to support your mental, physical, financial and social wellbeing.\n\n\n\nCore bank funding for retirement savings, medical and life insurance, with flexible and voluntary benefits available in some locations.\n\nTime-off including annual leave, parental/maternity (20 weeks), sabbatical (12 months maximum) and volunteering leave (3 days), along with minimum global standards for annual and public holiday, which is combined to 30 days minimum.\n\nFlexible working options based around home and office locations, with flexible working patterns.\n\nProactive wellbeing support through Unmind, a market-leading digital wellbeing platform, development courses for resilience and other human skills, global Employee Assistance Programme, sick leave, mental health first-aiders and all sorts of self-help toolkits\n\nA continuous learning culture to support your growth, with opportunities to reskill and upskill and access to physical, virtual and digital learning.\n\nBeing part of an inclusive and values driven organisation, one that embraces and celebrates our unique diversity, across our teams, business functions and geographies - everyone feels respected and can realise their full potential.\n\n\n\n\n\n\n\n\n\n\nwww. sc. com/careers\n\n\n\n\n\n\n\n\n\n25999",Industry Type: Banking,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Microstrategy', 'Data analysis', 'Change management', 'Database design', 'Windows', 'Stored procedures', 'Business intelligence', 'Data mining', 'SQL', 'Python']",2025-06-10 15:14:52
Data Scientist,Global Banking Organization,15 - 18 years,Not Disclosed,['Gurugram'],"Key Skills: Pyspark, Python, Data Visualization\nRoles and Responsibilities:\nBuild and deploy predictive and event-based models to improve marketing effectiveness.\nLead the use of data science techniques to derive customer insights and drive decision-making.\nDesign and develop advanced feature engineering pipelines and feature stores for Marketing use cases.\nLeverage online and offline customer data, digital interactions, and demographics to develop targeted marketing strategies.\nCollaborate with stakeholders including data scientists, engineers, architects, and business teams to deliver impactful data science solutions.\nDeliver insights using advanced data visualization tools and effectively communicate findings to senior stakeholders.\nContinuously explore and implement new analytics methodologies to solve emerging business challenges.\nPromote a culture of data innovation, standardization, and reusability across the Marketing Data Science team.\nSkills Required:\nStrong enthusiasm for analytics and data science with a continuous learning mindset.\nProven experience in solving business problems using machine learning and statistical methods.\nStrategic thinking with a customer-centric focus.\nExcellent problem-solving and documentation skills.\nStrong interpersonal, collaboration, and stakeholder management skills.\nAbility to clearly articulate complex technical concepts to non-technical audiences.\nProficient in:\nPython, PySpark, SQL\nSpark MLlib, Data Science Libraries (e.g., Scikit-learn, XGBoost, etc.)\nData visualization tools (e.g., Tableau, Power BI, Plotly, Matplotlib)\nCloud platforms (Databricks preferred; AWS, Azure, or GCP acceptable)\nSource code control using GitHub\nRequired Experience:\n15+ years of experience in leading data science or machine learning teams.\nProven track record of deploying scalable ML models and delivering measurable business outcomes.\nHands-on experience working with structured, semi-structured, and unstructured data.\nStrong background in customer and marketing data analytics.\nDeep understanding of AI/ML trends and their business applications.\nExperience presenting insights and data-driven recommendations to senior stakeholders.\nDesirable Experience:\nExperience with Adobe Analytics or other digital analytics tools.\nPrior work in financial services or customer lifecycle marketing analytics.\nDevelopment and implementation of AI/ML roadmaps.\nExperience driving continuous improvement in data science practice and innovation culture.\nEducation:\nBachelor's or Master's degree in Statistics, Mathematics, Computer Science, Economics, Engineering, or related fields.\nEvidence of ongoing professional development through certifications or short courses in Data Science or AI.",Industry Type: Financial Services,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Pyspark', 'Data Visualization', 'Python']",2025-06-10 15:14:55
MLOps Engineer / Data Scientist,CEVA Logistics,3 - 7 years,Not Disclosed,['Mumbai'],"CEVA Logistics provides global supply chain solutions to connect people, products, and providers all around the world\nPresent in 170+ countries and with more than 110,000 employees spread over 1,500 sites, we are proud to be a Top 5 global 3PL\nWe believe that our employees are the key to our success\nWe want to engage and empower our diverse, global team to co-create value with our customers through our solutions in contract logistics and air, ocean, ground, and finished vehicle transport\nThat is why CEVA Logistics offers a dynamic and exceptional work environment that fosters personal growth, innovation, and continuous improvement\nDARE TO GROW! Join CEVA Logistics, and you will be part of a team that values imagination and continued learning and is committed to excellence in everything we do\nJoin us in our mission to shape the future of global logistics\nAs we continue growing at a fast pace, will you Dare to Grow with us\nJoin the forefront of AI-driven logistics innovation as a MLOps Engineer/Data Scientist at CEVA Logistics, a global leader in supply chain solutions\nYOUR ROLE\nAs a MLOps Engineer/Data scientist, you will play a pivotal role in CEVA LogisticsGlobal IT Data & Digital organization, reporting directly to the Global IT Data & Digital BI & Advanced Analytics AI & Data Science Manager\nThis hybrid role combines the expertise of data science with operational practices focused on the industrialization and deployment of AI and machine learning solutions at scale\nYou will collaborate with cross-functional teams in IT, Corporate Support Functions, Business Development, and Operations to drive the development and industrialization of machine learning models\nYour work will ensure that AI-driven insights and use cases are seamlessly integrated, scalable, and continuously optimized to provide ongoing value to CEVA Logistics and its customers\nThis position requires expertise not only in data science and machine learning but also in the processes that turn innovative solutions into robust, enterprise-grade systems\nThis position is open in Spain (Madrid / Barcelona) or Poland (Warsaw) or India (Mumbai)\nWHAT ARE YOU GOING TO DO\nExpertise in data science and machine learning techniques with experience in designing, building, and deploying models to solve business challenges\nProficiency in industrializing use cases, focusing on turning proof-of-concept models into full-scale, production-ready AI solutions that are both scalable and sustainable\nHands-on experience with MLOps tools and technologies (such as Kubernetes, Docker, Jenkins, MLflow, TensorFlow, etc-) for automating the deployment, monitoring, and maintenance of machine learning models\nStrong experience with cloud platforms (AWS, Google Cloud, Azure) and modern deployment architectures for AI/ML workloads\nAdvanced proficiency in Python, SQL, and experience with big data technologies to work with large datasets\nStrong understanding of model governance, model monitoring, and model retraining to ensure AI solutions deliver consistent and ongoing business value\nAbility to work collaboratively across cross-functional teams to ensure smooth transitions from development to deployment and operationalization\nExcellent communication skills to explain complex technical concepts to non-technical stakeholders and influence business strategies through AI-driven insights\nWHAT ARE WE LOOKING FOR\nA minimum of 5 years of professional experience with proven results in AI, Data Science & MLOps\nMasters degree in computer science, Data Science, Engineering, Mathematics, or a related field\nAdvanced certifications in AI, Data Science, Machine Learning, or MLOps would be plus\nProven ability to collaborate effectively with cross-functional teams, including business stakeholders, IT, and operations, to translate business needs into technical solutions\nStrong written and verbal communication skills, with the ability to explain complex technical concepts to non-technical audiences\nTechnical Expertise:\nData Science:\nSolid understanding of machine learning algorithms, statistical methods, and predictive modeling\nProficiency in Python and libraries such as TensorFlow, Keras, scikit-learn, XGBoost, and PyTorch\nMLOps:\nHands-on experience implementing MLOps practices for model deployment, automation, and continuous integration/continuous delivery (CI/CD)\nFamiliarity with tools like MLflow, Kubeflow, Jenkins, Docker, Kubernetes, and Terraform for automating machine learning pipelines\nData Integration & Management:\nProficient in using Snowflake for data integration, storage, and processing at scale\nExperience with Dataiku for creating and managing end-to-end data science workflows\nBig Data Technologies:\nFamiliarity with big data platforms and tools to process and analyze large datasets\nData Visualization & Reporting:\nExpertise in using Qlik Sense, Streamlite, or other BI, webapp & data visualization tools for developing interactive and insightful dashboards and reports to communicate complex results to non-technical stakeholders\nModel Development & Industrialization:\nExperience in transitioning machine learning models from prototype to production at scale, ensuring robustness, scalability, and reliability in real-world applications\nFamiliarity with best practices in model versioning, testing, monitoring, and retraining to maintain model accuracy and performance over time\nCloud Platforms:\nStrong experience with cloud-based platforms like AWS, Google Cloud and Azure for deploying, managing, and scaling AI/ML models\nCollaboration & Communication:\nProven ability to collaborate effectively with cross-functional teams, including business stakeholders, IT, and operations, to translate business needs into technical solutions\nStrong written and verbal communication skills, with the ability to explain complex technical concepts to non-technical audiences\nProblem Solving & Innovation:\nStrong analytical and problem-solving skills, with a creative mindset for leveraging AI and machine learning to solve business problems and drive innovation\nAbility to stay up to date with the latest developments in AI, machine learning, and MLOps, and apply cutting-edge techniques to business challenges\nSoft Skills:\nSelf-motivated, adaptable, and able to thrive in a fast-paced and dynamic work environment\nStrong attention to detail with a focus on quality, accuracy, and reliability in all work\nEffective time management and organizational skills, with the ability to handle multiple projects simultaneously and meet deadlines\nAdditional Information:\nThis position offers the opportunity to work with both existing tools at CEVA and new, modern tools available in the Cloud, leveraging our newly developed CEVA Data Platform\nThe role requires a proactive individual with a strong commitment to driving data-driven strategies and solutions within the organization\nWHAT DO WE HAVE TO OFFER\nWith a genuine culture of recognition, we want our employees to grow, develop and be part of our journey\nYou have access to the CEVA academy for training\nYou receive healthcare benefits, reimbursement of the transportation card (50%) and meal vouchers for each working day\nWe are a team in every sense, and we support each other and work collaboratively to achieve our goals together\nIt is our goal that you will be compensated for your hard work and commitment, so if youd like to work for one of the top Logistics providers in the world then lets work together to help you find your new role",Industry Type: Courier / Logistics,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['model monitoring', 'snowflake', 'python', 'predictive modeling', 'cloud platforms', 'machine learning algorithms', 'sql', 'communication skills']",2025-06-10 15:14:58
Data Scientist I,Affle,1 - 5 years,Not Disclosed,"['Mumbai', 'Gurugram', 'Bengaluru']","We are seeking a talented and motivated Data Scientist with 1-3 years of experience to join our Data Science team. If you have a strong passion for data science, expertise in machine learning, and experience working with large-scale datasets, we want to hear from you.\nAs a Data Scientist at RevX, you will play a crucial role in developing and implementing machine learning models to drive business impact. You will work closely with teams across data science, engineering, product, and campaign management to build predictive models, optimize algorithms, and deliver actionable insights. Your work will directly influence business strategy, product development, and campaign optimization.\n\nMajor Responsibilities:\nDevelop and implement machine learning models, particularly neural networks, decision trees, random forests, and XGBoost, to solve complex business problems.\nWork on deep learning models and other advanced techniques to enhance predictive accuracy and model performance.\nAnalyze and interpret large, complex datasets using Python, SQL, and big data technologies to derive meaningful insights.\nCollaborate with cross-functional teams to design, build, and deploy end-to-end data science solutions, including data pipelines and model deployment frameworks.\nUtilize advanced statistical techniques and machine learning methodologies to optimize business strategies and outcomes.\nEvaluate and improve model performance, calibration, and deployment strategies for real-time applications.\nPerform clustering, segmentation, and other unsupervised learning techniques to discover patterns in large datasets.\nConduct A/B testing and other experimental designs to validate model performance and business strategies.\nCreate and maintain data visualizations and dashboards using tools such as matplotlib, seaborn, Grafana, and Looker to communicate findings.\nProvide technical expertise in handling big data, data warehousing, and cloud-based platforms like Google Cloud Platform (GCP).\n\nRequired Experience/Skills:\nBachelors or Masters degree in Data Science, Computer Science, Statistics,\nMathematics, or a related field.\n1-3 years of experience in data science or machine learning roles.\nStrong proficiency in Python for machine learning, data analysis, and deep learning applications.\nExperience in developing, deploying, and monitoring machine learning models, particularly neural networks, and other advanced algorithms.\nExpertise in handling big data technologies, with experience in tools such as BigQuery and cloud platforms (GCP preferred).\nAdvanced SQL skills for data querying and manipulation from large datasets.\nExperience in data visualization tools like matplotlib, seaborn, Grafana, and Looker.\nStrong understanding of A/B testing, statistical tests, experimental design, and methodologies.\nExperience in clustering, segmentation, and other unsupervised learning techniques.\nStrong problem-solving skills and the ability to work with complex datasets and machine learning pipelines.\nExcellent communication skills, with the ability to explain complex technical concepts to non-technical stakeholders.\n\nPreferred Skills:\nExperience with deep learning frameworks such as TensorFlow or PyTorch.\nFamiliarity with data warehousing concepts and big data tools.\nKnowledge of MLOps practices, including model deployment, monitoring, and management.\nExperience with business intelligence tools and creating data-driven dashboards.\nUnderstanding of reinforcement learning, natural language processing (NLP), or other advanced AI techniques.\n  Education:\nBachelor of Engineering or similar degree from any reputed University.",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['advance sql', 'python', 'data analysis', 'big data technologies', 'data warehousing', 'random forest', 'machine learning', 'sql', 'deep learning', 'ab testing', 'tensorflow', 'seaborn', 'data science', 'grafana', 'gcp', 'design', 'matplotlib', 'pytorch', 'bigquery', 'big data', 'xgboost', 'communication skills']",2025-06-10 15:15:00
F2F Drive-14th June-Bangalore LTIM - Data Scientist,Ltimindtree,6 - 11 years,Not Disclosed,"['Hyderabad', 'Pune', 'Bengaluru']",DS\n\nKey Responsibilities\nCombine expertise in mathematics statistics computer science and domain knowledge to create AIML models to solve various business challenges\nCollaborate closely with the AI Technical Manager and GCC Petro technical professionals and data engineers to integrate models into the business framework\nIdentify and frame opportunities to apply advanced analytics modeling and related technologies to data to help businesses gain insight and improve decision making workflow and automation\nUnderstand and communicate the value of proposed opportunity with team members and other stakeholders\nIdentify needed data and appropriate technology to solve identified business challenges\nClean data and develop and test models\nEstablish the life cycle management process for models\nProvide technical mentoring in modeling and analytics technologies the specifics of the modeling process and general consulting skills\nDrive innovation in AIML to enhance capabilities in data driven decision making\nAligns with team on shared goals and outcomes recognizes others contributions and work collaboratively seek diverse perspectives\nTakes actions to develop self and others beyond existing skillset\nEncourages innovative ideas adapts to change and changing technologies\nUnderstand and communicate data insights and model behaviors to stakeholders with varying levels of technical expertise\nRequired Qualification\nMinimum 5 years of experience in designing and developing AIML models and or various optimization algorithms 5 to 9 years of experience\nSolid foundation in mathematics probability and statistics with demonstrated depth of knowledge and experience in advanced analytics and data science methodologies eg supervised and unsupervised learning statistics data science model development\nProficiency in Python and working knowledge of cloud AIML services Azure Machine Learning and Databricks preferred\nDomain knowledge relevant to the energy sector and working knowledge of Oil and Gas value chain eg upstream midstream or downstream and associated business workflows\nProven ability to frame data science opportunities leverage standard foundational tools and Azure services to perform exploratory data analysis for purposes of data cleaning and discovery visualize data and identify actions to reach needed results\nAbility to quickly assess current state and apply technical concepts across cross functional business workflows\nExperience with driving successful execution deliverables and accountabilities to meet quality and schedule goals\nAbility to translate complex data into actionable insights that drive business val\nDemonstrated ability to engage and establish collaborative relationships both inside and outside immediate workgroup at various organizational levels across functional and geographic boundaries to achieve desired outcomes\nDemonstrated ability to adjust behavior based on feedback and provide feedback to other\nTeam oriented mindset with effective communication skills and the ability to work collaboratively\nStrong problem solving skills and attention to detail\nExcellent communication and collaboration skills,Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Azure Databricks', 'Machine Learning', 'GCP', 'Data Scientist', 'Python', 'Predictive Modeling', 'Azure', 'Generative AI', 'Natural Language Processing', 'Deep Learning', 'Data Science', 'Azure Machine Learning', 'Computer Vision', 'AWS']",2025-06-10 15:15:03
Data Scientist,Dwplacesolutions,3 - 5 years,Not Disclosed,['Bengaluru'],We are seeking an experienced Data Scientist to join our team.\nThe ideal candidate will have a strong background in developing and deploying\nconversational AI solutions using Large Language Models (LLMs) and RASA\nframework.,Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Machine Learning', 'Tensorflow', 'R', 'Artificial Intelligence', 'Natural Language Processing', 'Neural Networks', 'Chatbot', 'Deep Learning', 'Python']",2025-06-10 15:15:06
Data Scientist I - Newton,Affle,2 - 5 years,Not Disclosed,['Gurugram'],"The Data Scientist is crucial in leveraging data to derive meaningful insights and solutions for complex business problems. This individual will lead and guide the data science team in developing advanced analytical models, algorithms, and statistical analyses. They will collaborate with cross-functional teams to identify\nopportunities for leveraging data-driven solutions, making strategic decisions, and enhancing overall business performance. The Data Scientist will be responsible for designing and implementing machine learning models, conducting data exploration, and communicating findings to non-technical stakeholders.\nPrimary Responsibilities:\nCollaborate with business stakeholders to understand and translate their goals into data science initiatives.\nLead the development and implementation of machine learning models and algorithms to place ads in the context of cutting-edge privacy frameworks efficiently.\nDevelop strategies to optimize budget allocation in scenarios with high cardinality and uncertainty.\nConduct exploratory data analysis to discover complex data sets' patterns, trends, and insights.\nCommunicate complex analytical findings in a clear and actionable manner to non-technical stakeholders.\nStay abreast of the latest advancements in data science, machine learning, and relevant technologies.\nDevelop and train ML/DL models for forecasting/classification\nBuild AI agents RAG-based systems\nLeverage data-driven insights and predictive modeling to build PoCs.\nLeverage LangChain, LangGraph, or similar for orchestration\nUse SQL for data analysis, feature engineering, and reporting\nRequired Skills:\nQualification in a quantitative field such as Computer Science, Statistics, Physics, or Mathematics.\nExcellent problem-solving skills.\nStrong coding skills.\n2+ years of relevant work experience in data science and machine learning.\nSolid background in statistical analysis, hypothesis testing, and experimental design.\nProficiency in Python and SQL\nPreferred GCP Platform\nExposure to RAG, LLMs, and agentic workflows - Finetuning Evaluation\nFamiliar with LangChain, LangGraph, or similar toolkits\nPlus points if you have experience with Apple Ads (formerly ASA) or have worked in the AdTech space.\nEffective communication skills with the ability to convey technical concepts to non-technical audiences",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['algorithms', 'python', 'modeling', 'hypothesis testing', 'mathematics', 'forecasting', 'dl', 'predictive', 'problem solving', 'machine learning', 'sql', 'coding', 'data science', 'gcp', 'predictive modeling', 'design', 'reporting', 'statistics', 'communication skills', 'ml']",2025-06-10 15:15:08
Data Scientist- Deep Learning & Model Building,Axtria,4 - 9 years,Not Disclosed,"['Noida', 'Gurugram', 'Delhi / NCR']","Axtria: -Axtria is a global provider of award-winning cloud software and data analytics to the life sciences industry. Axtria enables life sciences organizations to transform the product commercialization journey and deliver much-improved healthcare outcomes for patients worldwide. We are acutely aware that our work impacts millions of people and are incredibly passionate about the improvement we can bring to patients lives.\nOur focus is on delivering solutions that help pharmaceutical, medical device, and diagnostics companies complete the journey from data to insights to action and get superior returns from their investments. As a participant of the United Nations Global Compact, Axtria is committed to aligning strategies and operations with universal principles on human rights, labor, environment, and anti-corruption and taking actions that advance societal goals.\nOur people are our core strength, and they make us proud of our work; this has helped us grow exponentially and make tremendous strides toward developing great products. It is for the grit, thinking of an entrepreneur, and a family-like environment where each member is valued and treasured that we are growing rapidly.\nFor more information, visit www.axtria.com.\nJob Title: - Associate Manager/Project Leader Model Building\nRequisition ID:\nJob Location: -Gurgaon/Bangalore/Pune/Hyderabad\nJob Summary: -\nData Scientist with good hands-on experience of 3+ years in developing state of the art and scalable Machine Learning models and their operationalization, leveraging off-the-shelf workbench production.\nJob Responsibilities: -\n1. Hands on experience in Python data-science and math packages such as NumPy, Pandas, Sklearn, Seaborn, PyCaret, Matplotlib\n2. Proficiency in Python and common Machine Learning frameworks (TensorFlow, NLTK, Stanford NLP, PyTorch, Ling Pipe, Caffe, Keras, SparkML and OpenAI etc.)\n3. Experience of working in large teams and using collaboration tools like GIT, Jira and Confluence\n4. Good understanding of any of the cloud platform – AWS, Azure or GCP\n5. Understanding of Commercial Pharma landscape and Patient Data / Analytics would be a huge plus\n6. Should have an attitude of willingness to learn, accepting the challenging environment and confidence in delivering the results within timelines. Should be inclined towards self motivation and self-driven to find solutions for problems.\n7. Should be able to mentor and guide mid to large sized teams under him/her\nJob Requirements: -\n1. Strong experience on Spark with Scala/Python/Java\n2. Strong proficiency in building/training/evaluating state of the art machine learning models and its deployment\n3. Proficiency in Statistical and Probabilistic methods such as SVM, Decision-Trees, Bagging and Boosting Techniques, Clustering\n4. Proficiency in Core NLP techniques like Text Classification, Named Entity Recognition (NER), Topic Modeling, Sentiment Analysis, etc. Understanding of Generative AI / Large Language Models / Transformers would be a plus\nQualification: -\n- B-Tech or BE in Computer Science / Computer Applications from Tier 1-2 college with 3+ years of proven experience in the field of Advanced Analytics or Machine Learning\nOR\n- Master’s degree in Machine Learning / Statistics / Econometrics, or related discipline from Tier 1-2 college with 3+ years of experience\nMust have Skills: -\nReal-world experience in implementing machine learning/statistical/econometric models/advanced algorithms\nBreadth of machine learning domain knowledge\nExperience in application of machine learning algorithms (classification, regression, deep learning, NLP, etc.)\nExperience with a ML/data-centric programming language (such as Python, Scala, or R) and ML libraries (pandas, numpy, scikit-learn, etc.)\nExperience with Apache Hadoop / Spark (or equivalent cloud-computing/map-reduce framework)\nSkills that give you an edge: -\nStrong analytical skills to solve and model complex business requirements are a plus. With life sciences or pharma background.\nWe will provide– (Employee Value Proposition)\nOffer an inclusive environment that encourages diverse perspectives and ideas\nDeliver challenging and unique opportunities to contribute to the success of a transforming organization\nOpportunity to work on technical challenges that may impact across geographies\nVast opportunities for self-development: online Axtria Institute, knowledge sharing opportunities globally, learning opportunities through external certifications\nSponsored Tech Talks & Hackathons\nPossibility to relocate to any Axtria office for short and long-term projects\nBenefit package:\n-Health benefits\n-Retirement benefits\n-Paid time off\n-Flexible Benefits\n-Hybrid /FT Office/Remote\nAxtria is an equal-opportunity employer that values diversity and inclusiveness in the workplace.\nWho we are\nAxtria 14 years journey\nAxtria, Great Place to Work\nLife at Axtria\nAxtria Diversity",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Natural Language Processing', 'Model Building', 'Deep Learning']",2025-06-10 15:15:11
Data Scientist,Top Rated Firm in IT Services Domain,3 - 8 years,5-7 Lacs P.A.,['Hyderabad( Nanakramguda )'],"Key Responsibilities:\nDesign and develop machine learning models and algorithms to solve business problems\nWrite clean, efficient, and reusable Python code for data processing and model deployment\nCollaborate with data engineers and product teams to integrate models into production systems\nAnalyze large datasets to derive insights, trends, and patterns\nEvaluate model performance and continuously improve through retraining and tuning\nCreate dashboards, reports, and data visualizations as needed\nMaintain documentation and ensure code quality and version control\n\nPreference\nMust have hands-on experience in building, training, and deploying AI/ML models using relevant frameworks and tools within a Linux environment.\nStrong proficiency in Python with hands-on experience in data science libraries (NumPy, Pandas, Scikit-learn, TensorFlow/PyTorch, etc.)\nExperience working with Hugging Face Transformers, spaCy, ChatGPT (OpenAI APIs), and DeepSeek LLMs for building NLP or generative AI solutions\nSolid understanding of machine learning, statistics, and data modeling\nExperience with data preprocessing, feature engineering, and model evaluation\nFamiliarity with SQL and working with structured/unstructured data\nKnowledge of APIs, data pipelines, and cloud platforms (AWS, GCP, or Azure) is a plus",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Python', 'Tensorflow', 'Pyspark', 'Artificial Intelligence', 'Natural Language Processing', 'Jupyter Notebook', 'Machine Learning', 'Scikit-Learn', 'Numpy', 'SQL', 'Pytorch', 'Pandas', 'AWS']",2025-06-10 15:15:13
Data Analyst / Data Scientist,Nybl,1 - 4 years,Not Disclosed,[],"nybl is looking for our next generation of data scientists. We pride ourselves on growing our team and are always looking for the brightest talent to join us. Attitude is the most important trait we are looking for above all else.\n\nYou will be working on transforming data into intelligence by developing innovative Artificial Intelligence (AI) solutions and integrating them with cutting-edge Internet of Things (IoT) technologies. Candidates must prove that they have the will, determination and ambition to be part of a team thats going to be the next Camel of the Middle East.\n\nresponsibilities\n\nwork closely with nybl to identify issues and use data to propose solutions for effective decision making\nbuild algorithms and design experiments to merge, manage, interrogate and extract data to supply tailored reports to colleagues, customers or the wider organisation\nuse machine learning tools and statistical techniques to produce solutions to problems\ntest data mining models to select the most appropriate ones for use on a project\nmaintain clear and coherent communication, both verbal and written, to understand data needs and report results\ncreate clear reports that tell compelling stories about how customers or clients work with the business\nassess the effectiveness of data sources and data-gathering techniques and improve data collection methods\nhorizon scan to stay up to date with the latest technology, techniques and methods\nconduct research from which youll develop prototypes and proof of concepts\nstay curious and enthusiastic about using algorithms to solve problems and enthuse others to see the benefit of your work.\n\nrequired skills, abilities, education, and experience\n\nExperience and knowledge in statistical and data mining techniques using (e.g., python, R, SQL)\nExperience and knowledge in applying advance Machine Learning techniques (e.g., Neural networks, supervised and unsupervised ML, computer vision and image processing, text analysis)\nExperience and knowledge in big data analysis and management and distributed computing tools (e.g., Hadoop, Hive, Spark)\nExperience and knowledge in one or more programming languages (C, C#, Java)\nExperience and knowledge in web development frameworks (javascript, React, node.js).\nExperience analyzing data from 3rd party providers: (e.g., Google Analytics, Site Catalyst, Facebook Insights)\nExperience visualizing/presenting data for stakeholders using: Periscope, Business Objects, D3, ggplot, etc.\nExperience working with and creating data architectures.\nKnowledge of a variety of machine learning techniques (clustering, decision tree learning, artificial neural networks, etc.) and their real-world advantages/drawbacks.\nKnowledge of advanced statistical techniques and concepts (regression, properties of distributions, statistical tests and proper usage, etc.) and experience with applications.\nExcellent written and verbal communication skills for coordinating across teams.\nA drive to learn and master new technologies and techniques.\nWillingness to learn new technology.\nAble to work independently on researching solutions and applying findings.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data analysis', 'Business objects', 'Google Analytics', 'Image processing', 'Web development', 'Machine learning', 'Data collection', 'Data mining', 'SQL', 'Python']",2025-06-10 15:15:16
Data Scientist,Ltimindtree,6 - 11 years,Not Disclosed,"['Pune', 'Chennai', 'Bengaluru']","We are looking for Data Scientist with GenAI solution & Python Development experience.\nExperience - 5+ Years\nLocation - Bangalore\nMandatory Skills - Data Science, Gen AI, Python, RAG. AI/ML, NLP, Azure, AWS.\n\nPFB JD\nResponsibilities:\nImplement and optimize AIML models and algorithms with a focus on Agentic AI and Retrieval Augmented Generation RAG\nImplement scalable machine learning models and algorithms to solve complex challenges\nResearch and integrate the latest AI tools and frameworks into existing systems\nStay uptodate with advancements in AIML research and apply them to practical problems\nExperiment with and implement Generative AI technologies across multiple domains\nLeverage cloud platforms like Google Cloud AWS and Azure for scalable AIML deployments\nUtilize opensource large language models LLMs such as LLama3 in AI application development\n\nKey Skills\nStrong understanding of Agentic AI concepts and applications\nHandson experience with RetrievalAugmented Generation RAG models\nExpertise in machine learning and deep learning frameworks eg TensorFlow PyTorch\nExperience with Large language models eg GPT4 LLama Azure OpenAI and integrating them into production systems\nProficiency in programming languages such as Python and familiarity with libraries like Hugging Face Transformers langchain and llama index\nExperience in building and deploying AIML solutions using cloud platforms Azure\nKnowledge of data retrieval natural language processing NLP and reinforcement learning techniques\nStrong problemsolving skills and ability to work in a fastpaced collaborative environment\nExperience with model optimization finetuning and inference strategies to improve system performance\n\nPreferred Skills\nExperience with advanced NLP tasks like text generation summarization and question answering familiarity with reinforcement learning algorithms and their application in intelligent agent design\nFamiliarity with containerization technologies eg Docker Kubernetes\nBackground in computer vision and image generation techniques",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Gen AI', 'Machine Learning']",2025-06-10 15:15:19
Data Scientist,Reyna Solutions,5 - 10 years,12-18 Lacs P.A.,"['Pune', 'Chennai', 'Bengaluru']","Hello,\n\nWe are looking for Data Scientist role to work with one of the American Multinational based at Bangalore/Chennai/Pune/Gurgaon/Hyderabad.\nKey Responsibilities:\n\n• R Programming Transition:\nAnalyze and understand the existing application to effectively transition it to R.\nRe-write or optimize existing code to ensure efficiency and scalability in R.\nTransform R models to Python as needed.\n• R Shiny Dashboard Development:\nDesign, develop, and implement interactive and visually appealing R Shiny Dashboards.\nIntegrate data sources and ensure real-time data updates in the dashboards.\nCollaborate with stakeholders to gather requirements and provide dashboard solutions that meet business needs.\n• Cloud Server Hosting:\nSet up and configure AWS for hosting R Shiny Dashboards.\nEnsure secure, scalable, and reliable AWS hosting environments.\n• Project Leadership and Collaboration:\nCollaborate with cross-functional teams including data scientists, analysts, and IT staff.\nRequired Skills and Qualifications:\n• Bachelors or Masters degree in Computer Science, Data Science, Statistics, or related field.\n• Proven experience in R programming and developing R Shiny applications and strong python\nskills.\n• Strong understanding of data manipulation and visualization in R.\n• Experience with cloud platforms such as AWS, Google Cloud, or Azure, specifically in deploying R Shiny Dashboards.\n• Familiarity with DevOps practices, including CI/CD pipelines and version control (e.g., Git).\n• Excellent problem-solving skills and ability to work independently and as part of a team.\n• Strong communication skills to effectively collaborate with technical and non-technical stakeholders.\n\nNotice - Immediate/15 days/1 Month\n\nInterested candidates may apply on saritha.nair@reynasolutions.com\n\nRegards,\nSarita Nair",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['R Shiny', 'Shiny', 'R Program']",2025-06-10 15:15:21
Data Scientist,Alethe Labs,2 - 4 years,Not Disclosed,['Gurugram'],"Alethe Labs is looking for Data Scientist to join our dynamic team and embark on a rewarding career journey.\n\nUndertaking data collection, preprocessing and analysis\nBuilding models to address business problems\nPresenting information using data visualization techniques\nIdentify valuable data sources and automate collection processes\nUndertake preprocessing of structured and unstructured data\nAnalyze large amounts of information to discover trends and patterns\nBuild predictive models and machine - learning algorithms\nCombine models through ensemble modeling\nPresent information using data visualization techniques\nPropose solutions and strategies to business challenges\nCollaborate with engineering and product development teams",Industry Type: Oil & Gas,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Image processing', 'Data processing']",2025-06-10 15:15:23
Data Scientist,Accelon Inc,4 - 9 years,Not Disclosed,"['Chennai', 'Bengaluru']","Data Scientist\nBengaluru/Chennai (Hybrid also possible)\n\nJob Description\nLooking for Data scientists with product analytics and technically skills, hands on development experience in Python, statistical techniques, SQL and basic Data visualization with lot of attention to granular details in data., Self-motivated, team player, with curiosity to learn and grow, exhibiting strong resolve and challenge the status quo, to strive for excellence in everything they do, with good professional and academic track record to be part of a team that deals with most innovative payments processing team that deals transactions at scale.\n\nCandidate Requirements\nTop must-have skills\nSQL\nPython\nTableau\nStatistics\nAB Testing\n\nYour way to impact\n\nWhat do you need to bring-\nStrong 5+ Years of experience in Python and SQL skills with a lot of attention to granular details in data\nExpertise in stitching together findings to convey coherent insights with statistical significance and analysis\nUse A/B Testing, analytics, market research, usability studies, and competitive analysis to drive decision making.\nExceptional oral and written communication skills\nAbility to perform deep dive analyses on key business trends from multiple perspectives and package the insights into easily consumable presentations and documents\nEngage in problem solving with the product and various merchant facing teams to determine performance trends and root causes.\nSupport new product launches for merchants by ensuring constant monitoring is in place to track product performance.\nPartner with varies internal teams to ensure that the merchants analytic needs are well addressed.\nCreate close relationships with stakeholders to anticipate & answer the questions that might be asked both by internal and external parties.\nDegrees or certifications required.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Tableau', 'Ab Testing', 'Data Analytics', 'SQL', 'Python']",2025-06-10 15:15:26
Data Scientist @ bangalore,MKS Vision,7 - 12 years,Not Disclosed,"['Hyderabad', 'Coimbatore']","MKS Vision Pvt Ltd\n\nAbout us:\nMKS Vision is a full spectrum of Information Technology and engineering service provider. We exist to provide increased efficiencies and flexibility that accelerate business performance by adapting the latest cutting-edge technologies for our customers. Our services bring tangible benefits to our customers. MKS Vision will assist you in adopting global services.\nWebsite: https://www.mksvision.com/\nJob Location: Coimbatore/Hyderabad\nRisk Data Scientist\nKnowledge of lending industry analytical processes related to (credit underwriting, collections, etc.)\nExperienced in the data science lifecycle (model specification, development, deployment, and validation)\nProficient in the use of modeling and machine learning techniques (logistic regression, gradient boosting, etc.), in SAS, Python or R\nStrong working exp in Power BI.\nProficient in SQL for data extraction, manipulation and cleanup, and the development of modeling datasets for development\nExperience in conducting data studies and retro studies using internal and external data for model validation\nExperience in developing project presentations across the project lifecycle (project specification, development, conclusions, and recommendations)\nExperience in development and maintenance of model documentation\nKnowledge of lending data systems and data structures (credit applications, loan origination, collections, payments, dialers, credit bureau data)\nAbility to generate analytical insights form model data, including identification of candidate variables, and development of new features.\nPreferred minimum 7+ years of experience, BS degree on computer science, management information systems, statistics, data science, etc., or similar experience.",Industry Type: FinTech / Payments,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'SAS', 'Credit Risk Modelling', 'Python', 'SQL', 'R', 'Power Bi', 'Risk Analytics', 'ETL', 'Data Standards', 'Risk Modeling', 'Credit Risk Analysis']",2025-06-10 15:15:28
Data Scientist,Qua Xigma IT Solutions Pvt Ltd,2 - 7 years,Not Disclosed,['Tirupati'],"Position Overview:\nWe are seeking a collaborative and analytical Data Scientist who can bridge the gap between business needs and data science capabilities. In this role, you will lead and support projects that apply machine learning, AI, and statistical modeling to generate actionable insights and drive business value.\n\nKey Responsibilities:\nCollaborate with stakeholders to define and translate business challenges into data science solutions.\nConduct in-depth data analysis on structured and unstructured datasets.\nBuild, validate, and deploy machine learning models to solve real-world problems.\nDevelop clear visualizations and presentations to communicate insights.\nDrive end-to-end project delivery, from exploration to production.\nContribute to team knowledge sharing and mentorship activities.\n\nMust-Have Skills:\n3+ years of progressive experience in data science, applied analytics, or a related quantitative role, demonstrating a proven track record of delivering impactful data-driven solutions.\nExceptional programming proficiency in Python, including extensive experience with core libraries such as Pandas, NumPy, Scikit-learn, NLTK and XGBoost.\nExpert-level SQL skills for complex data extraction, transformation, and analysis from various relational databases.\nDeep understanding and practical application of statistical modeling and machine learning techniques, including but not limited to regression, classification, clustering, time series analysis, and dimensionality reduction.\nProven expertise in end-to-end machine learning model development lifecycle, including robust feature engineering, rigorous model validation and evaluation (e.g., A/B testing), and model deployment strategies.\nDemonstrated ability to translate complex business problems into actionable analytical frameworks and data science solutions, driving measurable business outcomes.\nProficiency in advanced data analysis techniques, including Exploratory Data Analysis (EDA), customer segmentation (e.g., RFM analysis), and cohort analysis, to uncover actionable insights.\nExperience in designing and implementing data models, including logical and physical data modeling, and developing source-to-target mappings for robust data pipelines.\nExceptional communication skills, with the ability to clearly articulate complex technical findings, methodologies, and recommendations to diverse business stakeholders (both technical and non-technical audiences).\nExperience in designing and implementing data models, including logical and physical data modeling, and developing source-to-target mappings for robust data pipelines.\nExceptional communication skills, with the ability to clearly articulate complex technical findings, methodologies, and recommendations to diverse business stakeholders (both technical and non-technical audiences).\n\nGood-to-Have Skills:\nExperience with cloud platforms (Azure, AWS, GCP) and specific services like Azure ML, Synapse, Azure Kubernetes and Databricks.\nFamiliarity with big data processing tools like Apache Spark or Hadoop.\nExposure to MLOps tools and practices (e.g., MLflow, Docker, Kubeflow) for model lifecycle management.\nKnowledge of deep learning libraries (TensorFlow, PyTorch) or experience with Generative AI (GenAI) and Large Language Models (LLMs).\nProficiency with business intelligence and data visualization tools such as Tableau, Power BI, or Plotly.\nExperience working within Agile project delivery methodologies.\n\nCompetencies:\nTech Savvy - Anticipating and adopting innovations in business-building digital and technology applications.\nSelf-Development - Actively seeking new ways to grow and be challenged using both formal and informal development channels.\nAction Oriented - Taking on new opportunities and tough challenges with a sense of urgency, high energy, and enthusiasm.\nCustomer Focus - Building strong customer relationships and delivering customer-centric solutions.\nOptimizes Work Processes - Knowing the most effective and efficient processes to get things done, with a focus on continuous improvement.\n\nWhy Join Us?\nBe part of a collaborative and agile team driving cutting-edge AI and data engineering solutions.\nWork on impactful projects that make a difference across industries.\nOpportunities for professional growth and continuous learning.\nCompetitive salary and benefits package.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Machine Learning', 'Python', 'SQL', 'End-to-End ML Development', 'Data Analysis', 'Data Modeling & ETL']",2025-06-10 15:15:29
Data Scientist,Synergy Maritime,3 - 5 years,Not Disclosed,['Chennai'],Role & responsibilities\n\nDesign ML design and Ops stack considering the various trade-offs.\nStatistical Analysis and fundamentals\nMLOPS frameworks design and implementation \nModel Evaluation best practices -Train and retrain systems when necessary.\nExtend existing ML libraries and frameworks -Keep abreast of developments in the field.,,,,"['Data Science', 'Docker', 'Artificial Intelligence', 'Hadoop', 'Big Data Technologies', 'Machine Learning', 'Deep Learning', 'Kubernetes', 'SQL']",2025-06-10 15:15:32
Data Scientist,Digital Convergence Technologies,2 - 5 years,Not Disclosed,['Pune'],"Job Title: Data Scientist\nLocation: Work from Office (Hybrid)\nJob Location: Magarpatta, Pune\nShift timing: 11 am to 8 pm\n\nData Scientist:\nAnalyse complex datasets to uncover patterns, trends, and insights that drive strategic business decisions.\nDevelop predictive models, machine learning algorithms, and statistical analyses to solve business problems and improve campaign or operational performance.\nCollaborate with business and technical teams to define data science use cases and translate business requirements into analytical solutions.\nCommunicate findings and recommendations through visualizations, presentations, and reports tailored to both technical and non-technical audiences.\nWork with large volumes of structured and unstructured data using tools such as Python, SAS, SQL, and big data platforms.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['SAS', 'Python', 'SQL', 'Big Data Technologies']",2025-06-10 15:15:34
Data Scientist,Virtana Corp,3 - 8 years,Not Disclosed,"['Pune', 'Chennai']","Position Overview:\nWe are seeking a Data Scientist Engineer with experience bringing highly scalable enterprise SaaS applications to market. This is a uniquely impactful opportunity to help drive our business forward and directly contribute to long-term growth at Virtana.\nIf you thrive in a fast-paced environment, take initiative, embrace proactivity and collaboration, and you re seeking an environment for continuous learning and improvement, we d love to hear from you!\nVirtana is a remote first work environment so you ll be able to work from the comfort of your home while collaborating with teammates on a variety of connectivity tools and technologies.\nJob Location- Pune/ Chennai/ Remote\nYou can schedule with us through Calendly at https: / / calendly.com / bimla-dhirayan / zoom-meeting-virtana\nRole Responsibilities:\nResearch and test machine learning approaches for analyzing large-scale distributed computing applications.\nImplement different models AI and ML algorithms for prototype and production systems.\nTest and refine the models and algorithms with live customer data to improve accuracy and efficacy.\nWork with other functional teams to integrate implemented systems into the SaaS platform\nSuggest innovative and creative concepts and ideas that would improve the overall platform\nQualifications:\nThe ideal candidate must have the following qualifications:\n3+ years experience in practical implementation and deployment of ML based systems preferred.\nBS/B Tech or M Tech/ MS (preferred) in Applied Mathematics or Statistics, or CS/Engineering with strong mathematical/statistical background\nStrong quantitative and analytical skills, especially statistical and ML techniques, including familiarity with different supervised and unsupervised learning algorithms\nImplementation experiences and deep knowledge of Classification, Time Series Analysis, Pattern Recognition, Reinforcement Learning, Deep Learning, Dynamic Programming and Optimization\nExperience in working on modeling graph structures related to spatiotemporal systems\nProgramming skills in Python\nExperience in developing and deploying on cloud (AWS or Google or Azure)\nExperience in understanding and usage of LLM models and Prompt engineering is preferred.\nGood verbal and written communication skills\nFamiliarity with well-known ML frameworks such as Pandas, Keras and TensorFlow",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Usage', 'Prototype', 'Time series analysis', 'Artificial Intelligence', 'IT operations management', 'Machine learning', 'Cloud', 'Cash flow', 'Pattern recognition', 'Python']",2025-06-10 15:15:36
Data Scientist,Go Digital Technology Consulting,3 - 5 years,Not Disclosed,"['Pune', 'Mumbai (All Areas)']","Job Title: Data Scientist Electric Load Forecasting\nLocation: Mumbai / Pune\nJob Type: Full-time \nExperience: 3-5 years\n About the Role:\nWe are seeking a highly motivated Data Scientist – Forecasting with a strong passion for energy, technology, and data-driven decision-making. In this role, you will be responsible for developing and refining energy load forecasting models, analyzing customer demand patterns, and improving forecasting accuracy using advanced time series analysis and machine learning techniques. Your insights will directly support risk management, operational planning, and strategic decision-making across the company. \n\nIf you thrive in a fast-paced, dynamic environment and enjoy solving complex data science challenges, we’d love to hear from you! \n\nKey Responsibilities:\nDevelop and enhance energy load forecasting models using time series forecasting, statistical modeling, and machine learning techniques. \nAnalyze historical and real-time energy consumption data to identify trends and improve forecasting accuracy. \nInvestigate discrepancies between forecasted and actual energy usage, providing actionable insights. \nAutomate data pipelines and forecasting workflows to streamline processes across departments. \nMonitor day-over-day forecast variations and communicate key insights to stakeholders. \nWork closely with internal teams and external vendors to refine forecasting methodologies. \nPerform scenario analysis to assess seasonal patterns, anomalies, and market trends. \nContinuously optimize forecasting models, leveraging techniques like ARIMA, Prophet, LSTMs, and regression-based models. \n\nQualifications & Skills:\n3-5 years of experience in data science, preferably in energy load forecasting, demand prediction, or a related field. \nStrong expertise in time series analysis, forecasting algorithms, and statistical modeling. \nProficiency in Python, with experience using libraries such as pandas, NumPy, scikit-learn, statsmodels, and TensorFlow/PyTorch. \nExperience working with SQL and handling large datasets. \nHands-on experience with forecasting models like ARIMA, SARIMA, Prophet, LSTMs, XGBoost, and random forests. \nFamiliarity with feature engineering, anomaly detection, and seasonality analysis. \nStrong analytical and problem-solving skills with a data-driven mindset. \nExcellent communication skills, with the ability to translate technical findings into business insights. \nAbility to work independently and collaboratively in a fast-paced, dynamic environment. \nStrong attention to detail, time management, and organizational skills. \n\nPreferred Qualifications (Nice to Have):\nExperience working with energy market data, smart meter analytics, or grid forecasting. \nKnowledge of cloud platforms (AWS) for deploying forecasting models. \nExperience with big data technologies such as Spark or Hadoop.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Time Series Analysis', 'Data Scientist', 'Tensorflow', 'Arima', 'SARIMA', 'Pandas', 'Big Data', 'Numpy', 'Scikit-Learn', 'Prophet', 'Python', 'SQL']",2025-06-10 15:15:40
Senior Associate Data Scientist,"NTT DATA, Inc.",2 - 5 years,Not Disclosed,['Bengaluru'],"Your day at NTT DATA\nThe Data Scientist is a seasoned subject matter expert, tasked with participating in the adoption of data science and analytics within the organization.\n\nThe primary responsibility of this role is to participate in the creation and delivery of data-driven solutions that add business value using statistical models, machine learning algorithms, data mining, and visualization techniques.\n\nWhat youll be doing\n\nKey Responsibilities:\nDesigns, develops, and programs methods, processes, and systems to consolidate and analyze unstructured, diverse big data sources to generate actionable insights and solutions for client services and product enhancement.\nDesigns and enhances data collection procedures to include information that is relevant for building analytic systems.\nAccountable for ensuring that data used for analysis is processed, cleaned and, integrally verified and build algorithms necessary to find meaningful answers.\nDesigns and codes software programs, algorithms, and automated processes to cleanse, integrate and evaluate large datasets from multiple disparate sources.\nAccountable for providing meaningful insights from large data and metadata sources; interprets and communicates insights and findings from analysis and experiments to product, service, and business managers.\nAccountable for performing analysis using programming languages or statistical packages such as Python, pandas etc.\nDesigns scalable and highly available applications leveraging the latest tools and technologies.\nAccountable for creatively visualizing and effectively communicating results of data analysis, insights, and ideas in a variety of formats to key decision-makers within the business.\nCreates SQL queries for the analysis of data and visualize the output of the models.\nCreates documentation around processes and procedures and manages code reviews.\nAccountable for ensuring that industry standards best practices are applied to development activities.\nKnowledge and Attributes:\nSeasoned in data modelling, statistical methods and machine learning techniques.\nAbility to thrive in a dynamic, fast-paced environment.\nQuantitative and qualitative analysis skills.\nDesire to acquire more knowledge to keep up to speed with the ever-evolving field of data science.\nCuriosity to sift through data to find answers and more insights.\nGood understanding of the information technology industry within a matrixed organization and the typical business problems such organizations face.\nAbility to translate technical findings clearly and fluently to non-technical team business stakeholders to enable informed decision-making.\nAbility to create a storyline around the data to make it easy to interpret and understand.\nSelf-driven and able to work independently yet acts as a team player.\nAble to apply data science principles through a business lens.\nDesire to create strategies and solutions that challenge and expand the thinking of peers and business stakeholders.\nAcademic Qualifications and Certifications:\nBachelors degree or equivalent in Data Science, Business Analytics, Mathematics, Economics, Engineering, Computer Science or a related field.\nRelevant programming (Python) certification preferred.\nAgile certification preferred.\nRequired Experience:\nSeasoned experience in a data science position in a corporate environment and/or related industry.\nSeasoned experience in statistical modelling and data modelling, machine learning, data mining, unstructured data analytics, natural language processing.\nSeasoned experience in programming languages (Python, etc.).\nSeasoned experience working in databases (MySQL, Microsoft SQL Server, Azure Synapse, MongoDB)\nSeasoned experience working with and creating data architectures.\nSeasoned experience with extracting, cleaning, and transforming data and working with data owners to understand the data.\nSeasoned experience visualizing and/or presenting data for stakeholder use and reuse across the business.\nSeasoned experience on working with API (creating and using APIs)\nAutomation experience using Python scripting, UIPath, Selenium, PowerAutomate.\nSeasoned experience working on Linux operating system (Ubuntu)",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Azure Synapse', 'Microsoft SQL Server', 'MySQL', 'Python scripting', 'PowerAutomate', 'Linux operating system', 'MongoDB', 'Selenium', 'Python', 'UIPath']",2025-06-10 15:15:43
Senior Data Scientist,"NTT DATA, Inc.",2 - 6 years,Not Disclosed,['Bengaluru'],"Your day at NTT DATA\nThe Senior Data Scientist is an advanced subject matter expert, tasked with taking accountability in the adoption of data science and analytics within the organization.\n\nThe primary responsibility of this role is to participate in the creation and delivery of data-driven solutions that add business value using statistical models, machine learning algorithms, data mining, and visualization techniques.\n\nKey responsibilities:\nDesigns, develops, and programs methods, processes, and systems to consolidate and analyze unstructured, diverse big data sources to generate actionable insights and solutions for client services and product enhancement.\nDesigns and enhances data collection procedures to include information that is relevant for building analytic systems.\nResponsible for ensuring that data used for analysis is processed, cleaned and, integrally verified and build algorithms necessary to find meaningful answers.\nDesigns and codes software programs, algorithms, and automated processes to cleanse, integrate and evaluate large datasets from multiple disparate sources\nProvides meaningful insights from large data and metadata sources; interprets and communicates insights and findings from analysis and experiments to product, service, and business managers.\nDirects scalable and highly available applications leveraging the latest tools and technologies.\nAccountable for creatively visualizing and effectively communicating results of data analysis, insights, and ideas in a variety of formats to key decision-makers within the business.\nCreates SQL queries for the analysis of data and visualizes the output of the models.\nResponsible for ensuring that industry standards best practices are applied to development activities.\n\nTo thrive in this role, you need to have:\nAdvanced understanding of data modelling, statistical methods and machine learning techniques.\nStrong ability to thrive in a dynamic, fast-paced environment.\nStrong quantitative and qualitative analysis skills.\nDesire to acquire more knowledge to keep up to speed with the ever-evolving field of data science.\nCuriosity to sift through data to find answers and more insights.\nAdvanced understanding of the information technology industry within a matrixed organization and the typical business problems such organizations face.\nStrong ability to translate technical findings clearly and fluently to non-technical team business stakeholders to enable informed decision-making.\nStrong ability to create a storyline around the data to make it easy to interpret and understand.\nSelf-driven and able to work independently yet acts as a team player.\n\nAcademic qualifications and certifications:\nBachelors degree or equivalent in Data Science, Business Analytics, Mathematics, Economics, Engineering, Computer Science or a related field.\nRelevant programming certification preferred.\nAgile certification preferred.\n\nRequired experience:\nAdvanced demonstrated experience in a data science position in a corporate environment and/or related industry.\nAdvanced demonstrated experience in statistical modelling and data modelling, machine learning, data mining, unstructured data analytics, natural language processing.\nAdvanced demonstrated experience in programming languages (R, Python, etc.).\nAdvanced demonstrated experience working with and creating data architectures.\nAdvanced demonstrated experience with extracting, cleaning, and transforming data and working with data owners to understand the data.\nAdvanced demonstrated experience visualizing and/or presenting data for stakeholder use and reuse across the business.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'data analytics', 'natural language processing', 'data modeling', 'data mining', 'statistical modeling', 'data architecture', 'machine learning']",2025-06-10 15:15:46
"Sr Data Scientist, Supply Chain AI Delivery",Kimberly-Clark Corporation,4 - 9 years,Not Disclosed,['Bengaluru'],"You we're made to do this work: designing new technologies, diving into data, optimizing digital experiences, and constantly developing better, faster ways to get results. You want to be part of a performance culture dedicated to building technology for a purpose that matters. You want to work in an environment that promotes sustainability, inclusion, we'llbeing, and career development. In this role, you'll help us deliver better care for billions of people around the world. It starts with YOU.\n  In this role, you will:",,,,"['Procurement', 'Supply chain', 'Data analysis', 'Configuration management', 'Flex', 'Data processing', 'Monitoring', 'SQL', 'Logistics', 'Python']",2025-06-10 15:15:49
Chief Analytics Office (CAO) - Data Scientist,IBM,2 - 4 years,Not Disclosed,['Bengaluru'],"Role Overview: \n\nAs a Data Scientist within IBM's Chief Analytics Office, you will support AI-driven projects across the enterprise. You will apply your technical skills in AI, machine learning, and data analytics to assist in implementing data-driven solutions that align with business goals. This role involves working with team members to translate data insights into actionable recommendations.\n\n  \n\n Key Responsibilities: \n Technical Execution and Leadership: \nDevelop and deploy AI models and data analytics solutions.\nSupport the implementation and optimization of AI-driven strategies per business stakeholder requirements.\nHelp refine data-driven methodologies for transformation projects.\n Data Science and AI: \nDesign and implement machine learning solutions and statistical models, from problem formulation through deployment, to analyze complex datasets and generate actionable insights.\nLearn and utilize cloud platforms to ensure the scalability of AI solutions.\nLeverage reusable assets and apply IBM standards for data science and development.\n Project Support: \nLead and contribute to various stages of AI and data science projects, from data exploration to model development.\nMonitor project timelines and help resolve technical challenges.\nDesign and implement measurement frameworks to benchmark AI solutions, quantifying business impact through KPIs.\n Collaboration: \nEnsure alignment to stakeholders’ strategic direction and tactical needs.\nWork with data engineers, software developers, and other team members to integrate AI solutions into existing systems.\nContribute technical expertise to cross-functional teams.\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\n\n Education: Bachelor’s or Master’s in Computer Science, Data Science, Statistics, or a related field is required; an advanced degree strongly preferred\n\n Experience: \n2-4 yearsof experience in data science, AI, or analytics with a focus on implementing data-driven solutions\nExperience with data cleaning, data analysis, A/B testing, and data visualization\nExperience with AI technologies through coursework or projects\n\n\n Technical\n\nSkills:\n \nProficiency in SQL and Python for performing data analysis and developing machine learning models\nKnowledge of common machine learning algorithms and frameworkslinear regression, decision trees, random forests, gradient boosting (e.g., XGBoost, LightGBM), neural networks, and deep learning frameworks such as TensorFlow and PyTorch\nExperience with cloud-based platforms and data processing frameworks\nUnderstanding of large language models (LLMs)\nFamiliarity with IBM’s watsonx product suite\nFamiliarity with object-oriented programming\n\n\n Analytical\n\nSkills:\n \nStrong problem-solving abilities and eagerness to learn\nAbility to work with datasets and derive insights\n\n\n Other : \nGood communication skills, with the ability to explain technical concepts clearly\nEnthusiasm for learning and applying new technologies\nStrong project management skills, with the ability to balance multiple initiatives, prioritize tasks effectively, and meet deadlines in a fast-paced environment\nSuccessful completion of Coding Assessment\n\n\nPreferred technical and professional experience\n\nAdvanced degree strongly preferred",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'project management', 'artificial intelligence', 'sql', 'data science', 'data analytics', 'data analysis', 'neural networks', 'random forest', 'machine learning', 'data cleansing', 'tensorflow', 'ab testing', 'pytorch', 'statistical modeling', 'data visualization', 'statistics']",2025-06-10 15:15:52
Data Scientist Lead - L1,Wipro,4 - 9 years,Not Disclosed,['Bengaluru'],"About The Role  \n\nRole Purpose\n\nThe purpose of the role is to define, architect and lead delivery of machine learning and AI solutions\n\n ? \n\nDo\n\n1. Demand generation through support in Solution development\n\na. Support Go-To-Market strategy\n\ni. Collaborate with sales, pre-sales &consulting team to assist in creating solutions and propositions for proactive demand generation\n\nii. Contribute to development solutions, proof of concepts aligned to key offerings to enable solution led sales\n\nb. Collaborate with different colleges and institutes for recruitment, joint research initiatives and provide data science courses\n\n2. Revenue generation through Building & operationalizing Machine Learning, Deep Learning solutions\n\na. Develop Machine Learning / Deep learning models for decision augmentation or for automation solutions\n\nb. Collaborate with ML Engineers, Data engineers and IT to evaluate ML deployment options\n\nc. Integrate model performance management tools into the current business infrastructure\n\n3. Team Management\n\na. Resourcing\n\ni. Support recruitment process to on-board right resources for the team\n\nb. Talent Management\n\ni. Support on boarding and training for the team members to enhance capability & effectiveness\n\nii. Manage team attrition\n\nc. Performance Management\n\ni. Conduct timely performance reviews and provide constructive feedback to own direct reports\n\nii. Be a role model to team for five habits\n\niii. Ensure that the Performance Nxt is followed for the entire team\n\nd. Employee Satisfaction and Engagement\n\ni. Lead and drive engagement initiatives for the team\n\n ? \n\nDeliver\n\n\nNo.\n\nPerformance Parameter\n\nMeasure 1. Demand generation Order booking 2. Revenue generation through delivery Timeliness, customer success stories, customer use cases 3. Capability Building & Team Management % trained on new skills, Team attrition %\n\n\n ? \n\n ? \nReinvent your world. We are building a modern Wipro. We are an end-to-end digital transformation partner with the boldest ambitions. To realize them, we need people inspired by reinvention. Of yourself, your career, and your skills. We want to see the constant evolution of our business and our industry. It has always been in our DNA - as the world around us changes, so do we. Join a business powered by purpose and a place that empowers you to design your own reinvention. Come to Wipro. Realize your ambitions. Applications from people with disabilities are explicitly welcome.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['team management', 'machine learning', 'deep learning', 'data science', 'performance management', 'python', 'natural language processing', 'neural networks', 'ml deployment', 'data engineering', 'artificial intelligence', 'sql', 'tensorflow', 'predictive modeling', 'statistical modeling', 'ml']",2025-06-10 15:15:55
Data Scientist Lead - L1,Wipro,5 - 8 years,Not Disclosed,['Gurugram'],"About The Role  \n\nRole Purpose\n\nThe purpose of the role is to define, architect and lead delivery of machine learning and AI solutions\n\n ? \n\nDo\n\n1. Demand generation through support in Solution development\n\na. Support Go-To-Market strategy\n\ni. Collaborate with sales, pre-sales &consulting team to assist in creating solutions and propositions for proactive demand generation\n\nii. Contribute to development solutions, proof of concepts aligned to key offerings to enable solution led sales\n\nb. Collaborate with different colleges and institutes for recruitment, joint research initiatives and provide data science courses\n\n2. Revenue generation through Building & operationalizing Machine Learning, Deep Learning solutions\n\na. Develop Machine Learning / Deep learning models for decision augmentation or for automation solutions\n\nb. Collaborate with ML Engineers, Data engineers and IT to evaluate ML deployment options\n\nc. Integrate model performance management tools into the current business infrastructure\n\n3. Team Management\n\na. Resourcing\n\ni. Support recruitment process to on-board right resources for the team\n\nb. Talent Management\n\ni. Support on boarding and training for the team members to enhance capability & effectiveness\n\nii. Manage team attrition\n\nc. Performance Management\n\ni. Conduct timely performance reviews and provide constructive feedback to own direct reports\n\nii. Be a role model to team for five habits\n\niii. Ensure that the Performance Nxt is followed for the entire team\n\nd. Employee Satisfaction and Engagement\n\ni. Lead and drive engagement initiatives for the team\n\n ? \n\nDeliver\n\n\nNo.\n\nPerformance Parameter\n\nMeasure 1. Demand generation Order booking 2. Revenue generation through delivery Timeliness, customer success stories, customer use cases 3. Capability Building & Team Management % trained on new skills, Team attrition %\n\n\n ? \n\n ? \nMandatory\n\nSkills:\nPython for Data Science.\n\nExperience5-8 Years.\n\nReinvent your world. We are building a modern Wipro. We are an end-to-end digital transformation partner with the boldest ambitions. To realize them, we need people inspired by reinvention. Of yourself, your career, and your skills. We want to see the constant evolution of our business and our industry. It has always been in our DNA - as the world around us changes, so do we. Join a business powered by purpose and a place that empowers you to design your own reinvention. Come to Wipro. Realize your ambitions. Applications from people with disabilities are explicitly welcome.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'machine learning', 'deep learning', 'data science', 'ml', 'data analytics', 'natural language processing', 'neural networks', 'predictive analytics', 'ml deployment', 'data engineering', 'artificial intelligence', 'sql', 'r', 'predictive modeling', 'statistical modeling', 'statistics']",2025-06-10 15:15:59
"Lead Data Scientist, Operations || Mumbai || Max 38 LPA",Argus India Price Reporting Services,5 - 10 years,20-35 Lacs P.A.,"['Mumbai Suburban', 'Navi Mumbai', 'Mumbai (All Areas)']","Lead Data Scientist, Operations\nMumbai, India\n\nAbout Argus:\n\nArgus is the leading independent provider of market intelligence to the global energy and commodity markets. We offer essential price assessments, news, analytics, consulting services, data science tools and industry conferences to illuminate complex and opaque commodity markets.\nHeadquartered in London with 1,500 staff, Argus is an independent media organisation with 30 offices in the worlds principal commodity trading hubs.\nCompanies, trading firms and governments in 160 countries around the world trust Argus data to make decisions, analyse situations, manage risk, facilitate trading and for long-term planning. Argus prices are used as trusted benchmarks around the world for pricing transportation, commodities and energy.\nFounded in 1970, Argus remains a privately held UK-registered company owned by employee shareholders and global growth equity firm General Atlantic.\nWhat were looking for:\nJoin our Generative AI team to lead a new group in India, focused on creating and maintaining AI-ready data. As the point of contact in Mumbai, you will guide the local team and ensure seamless collaboration with our global counterparts. Your contributions will directly impact the development of innovative solutions used by industry leaders worldwide, supporting text and numerical data extraction, curation, and metadata enhancements to accelerate development and ensure rapid response times. You will play a pivotal role in transforming how our data are seamlessly integrated with AI systems, paving the way for the next generation of customer interactions.\n\nWhat will you be doing:\n\nLead and Develop the Team: Oversee a team of data scientists in Mumbai. Mentoring and guiding junior team members, fostering their professional growth and development.\nStrategic Planning: Develop and implement strategic plans for data science projects, ensuring alignment with the company's goals and objectives.\nAI-Ready Data Development: Design, develop, and maintain high-quality AI-ready datasets, ensuring data integrity, usability, and scalability to support advanced Generative AI models.\nAdvanced Data Processing: Drive hands-on efforts in complex data extraction, cleansing, and curation for diverse text and numerical datasets. Implement sophisticated metadata enrichment strategies to enhance data utility and accessibility for AI systems.\nAlgorithm Implementation & Optimization: Implement and optimize state-of-the-art algorithms and pipelines for efficient data processing, feature engineering, and data transformation tailored for LLM and GenAI applications.\nGenAI Application Development: Apply and integrate frameworks like LangChain and Hugging Face Transformers to build modular, scalable, and robust Generative AI data pipelines and applications.\nPrompt Engineering Application: Apply advanced prompt engineering techniques to optimize LLM performance for specific data extraction, summarization, and generation tasks, working closely with the Lead's guidance.\nLLM Evaluation Support: Contribute to the systematic evaluation of Large Language Models (LLMs) outputs, analysing quality, relevance, and accuracy, and supporting the implementation of LLM-as-a-judge frameworks.\nRetrieval-Augmented Generation (RAG) Contribution: Actively contribute to the implementation and optimization of RAG systems, including working with embedding models, vector databases, and, where applicable, knowledge graphs, to enhance data retrieval for GenAI.\nTechnical Leadership: Act as a technical leader and subject matter expert for junior data scientists, providing guidance on best practices in coding and PR reviews, data handling, and GenAI methodologies.\nCross-Functional Collaboration: Collaborate effectively with global data science teams, engineering, and product stakeholders to integrate data solutions and ensure alignment with broader company objectives.\nOperational Excellence: Troubleshoot and resolve data-related issues promptly to minimize potential disruptions, ensuring high operational efficiency and responsiveness.\nDocumentation & Code Quality: Produce clean, well-documented, production-grade code, adhering to best practices for version control and software engineering.\n\nSkills and Experience:\n\nLeadership Experience: Proven track record in leading and mentoring data science teams, with a focus on strategic planning and operational excellence.\nAcademic Background: Advanced degree in AI, statistics, mathematics, computer science, or a related field.\nProgramming and Frameworks: 5+ years of hands-on experience with Python, TensorFlow or PyTorch, and NLP libraries such as spaCy and Hugging Face.\nGenAI Tools: 2+ years of Practical experience with LangChain, Hugging Face Transformers, and embedding models for building GenAI applications.\nPrompt Engineering: Deep expertise in prompt engineering, including prompt tuning, chaining, and optimization techniques.\nLLM Evaluation: Experience evaluating LLM outputs, including using LLM-as-a-judge methodologies to assess quality and alignment.\nRAG and Knowledge Graphs: Practical understanding and experience using vector databases. In addition, familiarity with graph-based RAG architectures and the use of knowledge graphs to enhance retrieval and reasoning would be a strong plus.\nCloud: 2+ years of experience with Gemini/OpenAI models and cloud platforms such as AWS, Google Cloud, or Azure. Proficient with Docker for containerization.\nData Engineering: Strong understanding of data extraction, curation, metadata enrichment, and AI-ready dataset creation.\nCollaboration and Communication: Excellent communication skills and a collaborative mindset, with experience working across global teams.\n\nWhats in it for you:\n\nCompetitive salary\nHybrid Working Policy (3 days in Mumbai office/ 2 days WFH once fully inducted)\nGroup healthcare scheme\n18 days annual leave\n8 days of casual leave\nExtensive internal and external training\n\nHours:\n\nThis is a full-time position operating under a hybrid model, with three days in the office and up to two days working remotely.\nThe team supports Argus key business processes every day, as such you will be required to work on a shift-based rota with other members of the team supporting the business until 8pm. Typically support hours run from 11am to 8pm with each member of the team participating up to 2/3 times a week.\n\nFor more details about the company and to apply please make sure you send your CV and cover letter via our website: www.argusmedia.com/en/careers/open-positions\nBy submitting your job application, you automatically acknowledge and consent to the collection, use and/or disclosure of your personal data to the Company. Argus is an equal opportunity employer. We welcome and encourage diversity in the workplace regardless of race, gender, sexual orientation, gender identity, disability or veteran status.",Industry Type: Analytics / KPO / Research,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Huggingface', 'Langchain', 'Spacy', 'Python', 'TensorFlow', 'Pytorch']",2025-06-10 15:16:01
Associate Data Scientist,Visa,3 - 8 years,Not Disclosed,['Bengaluru'],"We are seeking a Data Scientist to join our data science team in Bangalore. This individual contributor role will be responsible for driving data-driven decision making across the organization.\n  They will collaborate with business stakeholders, understand their problems, and design data-based solutions for them. Key responsibilities include:\n1. Develop and implement machine learning models, perform data analysis, and create predictive models to support the Client Services teams.\n2. Collaborate with team members and stakeholders to understand their data science needs and design solutions in collaboration/guidance from team members.\n3. Conduct research to identify new methods and techniques to improve existing models.\n4. Create visualizations to communicate complex data and insights in a clear and effective manner.\n5. Ensure data quality throughout all stages of acquisition and processing, including data cleaning, normalization, and transformation.\n6. Maintain clear and coherent communication, both verbal and written, to understand data needs and report results\n\n\nBasic Qualifications\nBachelors degree, OR 3+ years of relevant work experience\n\nPreferred Qualifications\nBachelors degree, OR 3+ years of relevant work experience\n1 or more years of work experience with a Bachelors Degree or an Advanced Degree (e.g. Masters, MBA, JD, MD) in data science, machine learning, AI, or a related field.\nProficient in Python, R, SQL, and other data science tools.\nKnowledge of big data platforms like Hadoop, Spark, or similar technologies.\nKnowledgeable in machine learning, generative AI techniques and algorithms.\nFamiliarity with Large Language Models such as OpenAIs GPT-3, and open-source models like Falcon or Llama2.\nProficient in Machine Learning Operations (MLOps), including deployment and maintenance of machine learning models.\nExcellent problem-solving skills and ability to think critically.\nStrong communication skills to clearly articulate the message with team members and stakeholders\nKnowledge of cloud platforms like AWS, GCP, or Azure is good to have.\nFamiliarity with data visualization tools like Tableau, PowerBI, or similar.\nAbility to work in a team and independently as required.\nFamiliarity with containerization technologies like Docker or Kubernetes.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data analysis', 'data science', 'GCP', 'Machine learning', 'Data quality', 'data visualization', 'Open source', 'big data', 'SQL', 'Python']",2025-06-10 15:16:03
Data Scientist-Advanced Analytics,IBM,3 - 7 years,Not Disclosed,['Bengaluru'],"As an Data Engineer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\n\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\n\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviour’s.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modelling results\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nProof of Concept (POC) DevelopmentDevelop POCs to validate and showcase the feasibility and effectiveness of the proposed AI solutions.\nHelp in showcasing the ability of Gen AI code assistant to refactor/rewrite and document code from one language to another\nDocument solution architectures, design decisions, implementation details, and lessons learned.\nStay up to date with the latest trends and advancements in AI, foundation models, and large language models.\nEvaluate emerging technologies, tools, and frameworks to assess their potential impact on solution design and implementation\n\n\nPreferred technical and professional experience\nExperience and working knowledge in COBOL & JAVA would be preferred\nHaving experience in Code generation, code matching & code translation leveraging LLM capabilities would be a Big plus\nDemonstrate a growth mindset to understand clients' business processes and challenges",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['elastic search', 'java', 'code generation', 'cobol', 'splunk', 'matlab', 'time series analysis', 'financial analysis', 'simulink', 'python', 'c', 'predictive analytics', 'analytics data', 'stateflow', 'machine learning', 'financial projections', 'sil', 'embedded c', 'clustering', 'big data']",2025-06-10 15:16:06
"Senior Data Scientist, Operations || Mumbai || 29 LPA",Argus India Price Reporting Services,5 - 10 years,20-25 Lacs P.A.,"['Mumbai Suburban', 'Navi Mumbai', 'Mumbai (All Areas)']","Senior Data Scientist, Operations\nMumbai, India\nAbout Argus:\n\nArgus is the leading independent provider of market intelligence to the global energy and commodity markets. We offer essential price assessments, news, analytics, consulting services, data science tools and industry conferences to illuminate complex and opaque commodity markets.\nHeadquartered in London with 1,500 staff, Argus is an independent media organisation with 30 offices in the worlds principal commodity trading hubs.\nCompanies, trading firms and governments in 160 countries around the world trust Argus data to make decisions, analyse situations, manage risk, facilitate trading and for long-term planning. Argus prices are used as trusted benchmarks around the world for pricing transportation, commodities and energy.\nFounded in 1970, Argus remains a privately held UK-registered company owned by employee shareholders and global growth equity firm General Atlantic.\n\nWhat were looking for:\nJoin our Generative AI team as a Senior Data Scientist, reporting directly to the Lead Data Scientist in India. You will play a crucial role in building, optimizing, and maintaining AI-ready data infrastructure for advanced Generative AI applications. Your focus will be on hands-on implementation of cutting-edge data extraction, curation, and metadata enhancement techniques for both text and numerical data. You will be a key contributor to the development of innovative solutions, ensuring rapid iteration and deployment, and supporting the Lead in achieving the team's strategic goals.\n\nWhat will you be doing:\nAI-Ready Data Development: Design, develop, and maintain high-quality AI-ready datasets, ensuring data integrity, usability, and scalability to support advanced Generative AI models.\nAdvanced Data Processing: Drive hands-on efforts in complex data extraction, cleansing, and curation for diverse text and numerical datasets. Implement sophisticated metadata enrichment strategies to enhance data utility and accessibility for AI systems.\nAlgorithm Implementation & Optimization: Implement and optimize state-of-the-art algorithms and pipelines for efficient data processing, feature engineering, and data transformation tailored for LLM and GenAI applications.\nGenAI Application Development: Apply and integrate frameworks like LangChain and Hugging Face Transformers to build modular, scalable, and robust Generative AI data pipelines and applications.\nPrompt Engineering Application: Apply advanced prompt engineering techniques to optimize LLM performance for specific data extraction, summarization, and generation tasks, working closely with the Lead's guidance.\nLLM Evaluation Support: Contribute to the systematic evaluation of Large Language Models (LLMs) outputs, analysing quality, relevance, and accuracy, and supporting the implementation of LLM-as-a-judge frameworks.\nRetrieval-Augmented Generation (RAG) Contribution: Actively contribute to the implementation and optimization of RAG systems, including working with embedding models, vector databases, and, where applicable, knowledge graphs, to enhance data retrieval for GenAI.\nTechnical Mentorship: Act as a technical mentor and subject matter expert for junior data scientists, providing guidance on best practices in coding and PR reviews, data handling, and GenAI methodologies.\nCross-Functional Collaboration: Collaborate effectively with global data science teams, engineering, and product stakeholders to integrate data solutions and ensure alignment with broader company objectives.\nOperational Excellence: Troubleshoot and resolve data-related issues promptly to minimize potential disruptions, ensuring high operational efficiency and responsiveness.\nDocumentation & Code Quality: Produce clean, well-documented, production-grade code, adhering to best practices for version control and software engineering.\n\nSkills and Experience:\nAcademic Background: Advanced degree in AI, statistics, mathematics, computer science, or a related field.\nProgramming and Frameworks: 2+ years of hands-on experience with Python, TensorFlow or PyTorch, and NLP libraries such as spaCy and Hugging Face.\nGenAI Tools: 1+ years Practical experience with LangChain, Hugging Face Transformers, and embedding models for building GenAI applications.\nPrompt Engineering: Deep expertise in prompt engineering, including prompt tuning, chaining, and optimization techniques.\nLLM Evaluation: Experience evaluating LLM outputs, including using LLM-as-a-judge methodologies to assess quality and alignment.\nRAG and Knowledge Graphs: Practical understanding and experience using vector databases. In addition, familiarity with graph-based RAG architectures and the use of knowledge graphs to enhance retrieval and reasoning would be a strong plus.\nCloud: 2+ years of experience with Gemini/OpenAI models and cloud platforms such as AWS, Google Cloud, or Azure. Proficient with Docker for containerization.\nData Engineering: Strong understanding of data extraction, curation, metadata enrichment, and AI-ready dataset creation.\nCollaboration and Communication: Excellent communication skills and a collaborative mindset, with experience working across global teams.\n\nWhats in it for you:\nCompetitive salary\nHybrid Working Policy (3 days in Mumbai office/ 2 days WFH once fully inducted)\nGroup healthcare scheme\n18 days annual leave\n8 days of casual leave\nExtensive internal and external training\n\nHours:\nThis is a full-time position operating under a hybrid model, with three days in the office and up to two days working remotely.\nThe team supports Argus key business processes every day, as such you will be required to work on a shift-based rota with other members of the team supporting the business until 8pm. Typically support hours run from 11am to 8pm with each member of the team participating up to 2/3 times a week.\n\nFor more details about the company and to apply please make sure you send your CV and cover letter via our website: www.argusmedia.com/en/careers/open-positions\nBy submitting your job application, you automatically acknowledge and consent to the collection, use and/or disclosure of your personal data to the Company. Argus is an equal opportunity employer. We welcome and encourage diversity in the workplace regardless of race, gender, sexual orientation, gender identity, disability or veteran status.",Industry Type: Analytics / KPO / Research,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Pytorch', 'Artificial Intelligence', 'LangChain', 'hugging face', 'Spacy', 'Tensorflow']",2025-06-10 15:16:09
Lead Data Scientist,TEKsystems,8 - 13 years,25-40 Lacs P.A.,['Bengaluru'],"Job Title: Lead Data Scientist\nLocation: Bangalore Urban, India (Hybrid)\nKey Responsibilities:\nLead AI/ML initiatives across multiple projects.\nCollaborate with other data scientists and cross-functional teams.\nTranslate business and technical requirements into actionable data science solutions.\nPropose and implement optimized machine learning models and data-driven strategies.\nEnsure quality and scalability of AI/ML solutions.\n\nRequired Qualifications:\nStrong background in machine learning, deep learning, and data science.\nProficiency in Python and ML libraries (e.g., TensorFlow, PyTorch, Scikit-learn).\nExperience with large-scale data processing and model deployment.\nExcellent problem-solving and communication skills.\nPrior experience in leading data science teams or projects.\n\nPreferred Skills:\nExperience with cloud platforms (AWS, GCP, or Azure).\nFamiliarity with MLOps practices and tools.\nKnowledge of computer vision or NLP is a plus.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'python', 'Machine Learning', 'gen ai', 'Statistics', 'Deep Learning', 'Classic Algorithms']",2025-06-10 15:16:12
Data Scientist-Advanced Analytics,IBM,3 - 7 years,Not Disclosed,['Pune'],"We are seeking a highly skilled Advanced Analytics Specialist to join our dynamic team. The successful candidate will be responsible for leveraging advanced analytics techniques to derive actionable insights, inform business decisions, and drive strategic initiatives. This role requires a deep understanding of data analysis, statistical modelling, machine learning, and data visualization.\nIn this role, you will be responsible for architecting and delivering AI solutions using cutting-edge technologies, with a strong focus on foundation models and large language models. You will work closely with customers, product managers, and development teams to understand business requirements and design custom AI solutions that address complex challenges. Experience with tools like Github Copilot, Amazon Code Whisperer etc. is desirable.\nSuccess is our passion, and your accomplishments will reflect this, driving your career forward, propelling your team to success, and helping our clients to thrive.\nDay-to-Day Duties:\nProof of Concept (POC) DevelopmentDevelop POCs to validate and showcase the feasibility and effectiveness of the proposed AI solutions. Collaborate with development teams to implement and iterate on POCs, ensuring alignment with customer requirements and expectations.\nHelp in showcasing the ability of Gen AI code assistant to refactor/rewrite and document code from one language to another, particularly COBOL to JAVA through rapid prototypes/ PoC\nDocumentation and Knowledge SharingDocument solution architectures, design decisions, implementation details, and lessons learned. Create technical documentation, white papers, and best practice guides. Contribute to internal knowledge sharing initiatives and mentor new team members.\nIndustry Trends and InnovationStay up to date with the latest trends and advancements in AI, foundation models, and large language models. Evaluate emerging technologies, tools, and frameworks to assess their potential impact on solution design and implementation\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nDevelop and implement advanced analytical models and algorithms to solve complex business problems, analyze large datasets to uncover trends, patterns, and insights that drive business performance.\nCollaborate with cross-functional teams to identify key business challenges and opportunities, Create and maintain data pipelines and workflows to ensure the accuracy and integrity of data, Design and deliver insightful reports and dashboards to communicate findings to stakeholders.\nStay up to date with the latest advancements in analytics, machine learning, and data science. Provide technical expertise and mentorship to junior team members.\nQualificationsBachelor’s or master’s degree in data science, Statistics, Mathematics, Computer Science, or a related field. Proven experience in advanced analytics, data science, or a similar role. Proficiency in programming languages such as Python, R, or SQL. Experience with data visualization tools like Tableau, Power BI, or similar.\nStrong understanding of statistical modelling and machine learning algorithms. Excellent analytical, problem-solving, and critical thinking skills. Ability to communicate complex analytical concepts to non-technical stakeholders. Experience with big data technologies (e.g., Hadoop, Spark) is a plus\n\n\nPreferred technical and professional experience\nFamiliarity with cloud-based analytics platforms (e.g., AWS, Azure).\nKnowledge of natural language processing (NLP) and deep learning techniques.\nExperience with project management and agile methodologies",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analysis', 'machine learning', 'statistical modeling', 'data visualization', 'machine learning algorithms', 'advanced analytics', 'python', 'github', 'natural language processing', 'power bi', 'microsoft azure', 'sql', 'tableau', 'r', 'java', 'data science', 'spark', 'hadoop', 'aws']",2025-06-10 15:16:16
Senior Data Scientist,Tractable,5 - 10 years,Not Disclosed,['Noida'],"Who we are\nTractable is an Artificial Intelligence company bringing the speed and insight of Applied AI to visual assessment. Trained on millions of data points, our AI-powered solutions connect everyone involved in insurance, repairs, and sales of homes and cars - helping people work faster and smarter, while reducing friction and waste.\nFounded in 2014, Tractable is now the AI tool of choice for world-leading insurance and automotive companies. Our solutions unlock the potential of Applied AI to transform the whole recovery ecosystem, from assessing damage and accelerating claims and repairs to recycling parts. They help make response to recovery up to ten times faster - even after full-scale disasters like floods and hurricanes.\nTractable has a world-class culture, backed up by our team, making us a global employer of choice!\nWere a diverse team, uniting individuals of over 40 different nationalities and from varied backgrounds, with machine learning researchers and motor engineers collaborating together on a daily basis. We empower each team member to have tangible impact and grow their own scope by intentionally building a culture centred around collaboration, transparency, autonomy and continuous learning.\nWere seeking a Senior Data Scientist to lead the delivery of real-world AI solutions across industries like insurance and automotive. Youll drive end-to-end ML development from scalable pipelines to production deployment while mentoring teammates and collaborating with enterprise partners to deliver business impact.\nYour impact\nArchitect Solutions: Design and optimise scalable ML systems, focusing on computer vision and NLP/LLM applications\nBuild & Deploy Models: Develop and productionise deep learning models using Python and modern ML frameworks\nLead Cross-Functionally: Align research, engineering, and product goals through hands-on leadership\nMentor & Guide: Support junior team members and promote best practices in ML development\nEngage Customers: Collaborate directly with enterprise clients to ensure seamless integration of AI solutions\nDrive Innovation: Evaluate and implement new AI tools and methodologies to enhance performance and scalability\nWhat youll need to be successful\nApplied AI Expertise: 5+ years building ML systems, especially in computer vision or NLP/LLM\nStrong Python & ML Skills: Proficiency in Python and ML libraries (e.g. PyTorch, TensorFlow)\nProduction Experience: Hands-on experience deploying ML models in production environments\nCloud & Data Engineering: Knowledge of cloud platforms (ideally AWS) and scalable data pipelines\nCollaboration & Communication: Ability to work across teams and communicate complex ideas clearly\nProblem-Solving Mindset: Analytical thinking and a focus on continuous improvement\nPreferred experience\nIndustry Impact: Experience applying AI in insurance, automotive, or similar sectors\nLLM/NLP Expertise: Background in large language models or conversational AI\nScalable Product Delivery: Proven success scaling ML solutions in production\nLeadership: Track record of mentorship and technical leadership in data science teams\nDiversity commitment\nAt Tractable, we are committed to building a diverse team and inclusive workplace where people s varied backgrounds and experiences are valued and recognised.\nWe encourage applications from candidates of all backgrounds and offer equal opportunities without discrimination.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer vision', 'Claims', 'Analytical', 'Artificial Intelligence', 'Machine learning', 'Technical leadership', 'Mentor', 'Continuous improvement', 'Automotive', 'Python']",2025-06-10 15:16:18
Principal AI Architect,Wabtec,10 - 15 years,Not Disclosed,['Bengaluru'],"How will you make a difference?\n\nWe are seeking a collaborative and highly motivated Principal AI Architect to lead our AI team, drive innovation, and enhance customer experiences through impactful artificial intelligence solutions. As a member of the Wabtec IT Data & Analytics (DnA) Team, you will be responsible for:\n\nProviding strategic leadership and direction in the development, articulation and implementation, of a comprehensive AI/ML/ Data Transformation Roadmap for Wabtec aligned with the overall business objectives.\nWorking with other AI champions in Wabtec evaluating AI tools/ technologies/ frameworks, champion adoption in different projects and demonstrate value for business and customers.\nActively collaborating with various stakeholders to align AI initiatives in Cloud Computing environments (e.g., AWS, Azure, OCI) with business goals.\nProviding technical oversight on AI projects to drive performance output to meet KPI metrics in Productivity and Quality.\nServing as contact and interface with external partners and industry leaders for collaborations in AI/LLM/ Generative AI.\nArchitecting and deploying scalable AI solutions that integrate seamlessly with existing business and IT infrastructure.\nDesign and architect AI as a service, to enable collaboration btw multiple teams in delivering AI solutions\nOptimizing state-of-the-art algorithms in distributed environments\nCreate clear and concise communications/recommendations for senior leadership review related to AI strategic business plans and initiatives.\nStaying abreast of advancements in AI, machine learning, and data science to continuously innovate and improve solutions and bring the external best practices for adoption in Wabtec\nImplementing best practices for AI designing, testing, deployment, and maintenance\nDiving deep into complex business problems and immerse yourself in Wabtec data & outcomes.\nMentoring a team of data scientists, fostering growth and performance.\nDeveloping AI governance frameworks with ethical AI practices and ensuring compliance with data protection regulations and ensuring responsible AI development.\n\nWhat do we want to know about you?\n\nYou must have:\nThe minimum qualifications for this role include:\n\nPh.D., M.S., or Bachelor's degree in Statistics, Machine Learning, Operations Research, Computer Science, Economics, or a related quantitative field\n5+ years of experience developing and supporting AI products in a production environment with 12+ years of proven relevant experience\n8+ years of experience managing and leading data science teams initiatives at enterprise level\nProfound knowledge of modern AI and Generative AI technologies\nExtensive experience in designing, implementing, and maintaining AI systems\nEnd-to-end expertise in AI/ML project lifecycle, from conception to large-scale production deployment\nProven track record as an Architect with cloud computing environments (e.g., AWS, Azure, OCI) and distributed computing platforms, including containerized deployments using technologies such as Amazon EKS (Elastic Kubernetes Service)\nExpertise with Hands-On experience into Python, AWS AI tech-stack (Bedrock Services, Foundation models, Textract, Kendra, Knowledge Bases, Guard rails, Agents etc.), ML Flow, Image Processing, NLP/Deep Learning, PyTorch /TensorFlow, LLMs integration with applications.\n\nPreferred qualifications for this role include:\nProven track record in building and leading high-performance AI teams, with expertise in hiring, coaching, and developing engineering leaders, data scientists, and ML engineers\nDemonstrated ability to align team vision with strategic business goals, driving impactful outcomes across complex product suites for diverse, global customers\nStrong stakeholder management skills, adept at influencing and unifying cross-functional teams to achieve successful project outcomes\nExtensive hands-on experience with enterprise-level Python development, PyData stack, Big Data technologies, and machine learning model deployment at scale\nProficiency in cutting-edge AI technologies, including generative AI, open-source frameworks, and third-party solutions (e.g., OpenAI)\nMastery of data science infrastructure and tools, including code versioning (Git), containerization (Docker), and modern AI/ML tech stacks\nPreferred: AWS with AWS AI services.\n\nWe would love it if you had:\n\nFluent with experimental design and the ability to identify, compute and validate the appropriate metrics to measure success\nDemonstrated success working in a highly collaborative technical environment (e.g., code sharing, using revision control, contributing to team discussions/workshops, and collaborative documentation)\nPassion and aptitude for turning complex business problems into concrete hypotheses that can be answered through rigorous data analysis and experimentation\nDeep expertise in analytical storytelling and stellar communications skills\nDemonstrated success mentoring junior teammates & helping develop peers\n\nWhat will your typical day look like?\n\nStakeholder Engagement: Collaborate with our Internal stakeholders to understand their needs, update on a specific project progress, and align our AI initiatives with business goals.\nUse Generative AI and machine learning techniques and build LLM Models & fine-tuning, Image processing, NLP, model integration with new/existing applications, and improve model performance/accuracy along with cost effective solutions.\nSupport AI Team: Guide and mentor the AI team, resolving technical issues and provide suggestions.\nReporting & Strategy: Generate and present reports to senior leadership, develop strategic insights, and stay updated on industry trends.\nBuilding AI roadmap for Wabtec and discussion with senior leadership\nTraining, Development & Compliance: Organize training sessions, manage resources efficiently, ensure data accuracy, security, and compliance with best practices.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Artificial Intelligence', 'Cloud Architecture', 'Eks', 'Machine Learning', 'Aiml']",2025-06-10 15:16:21
Sr. Data Scientist-Stratup-Mid-Size companies Exp.@ Bangalore_Urgent,"A leader in this space, we deliver world...",8 - 13 years,Not Disclosed,['Bengaluru'],"Senior Data Scientist\n\nLocation: Onsite Bangalore\nExperience: 8+ years\n\nRole Overview\n\nWe are seeking a Senior Data Scientist with a strong foundation in machine learning, deep learning, and statistical modeling, with the ability to translate complex operational problems into scalable AI/ML solutions. In addition to core data science responsibilities, the role involves building production-ready backends in Python and contributing to end-to-end model lifecycle management. Exposure to computer vision is a plus, especially for industrial use cases like identification, intrusion detection, and anomaly detection.\n\nKey Responsibilities\n\nDevelop, validate, and deploy machine learning and deep learning models for forecasting, classification, anomaly detection, and operational optimization\nBuild backend APIs using Python (FastAPI, Flask) to serve ML/DL models in production environments\nApply advanced computer vision models (e.g., YOLO, Faster R-CNN) to object detection, intrusion detection, and visual monitoring tasks\nTranslate business problems into analytical frameworks and data science solutions\nWork with data engineering and DevOps teams to operationalize and monitor models at scale\nCollaborate with product, domain experts, and engineering teams to iterate on solution design\nContribute to technical documentation, model explainability, and reproducibility practices\n\n\nRequired Skills\n\nStrong proficiency in Python for data science and backend development\nExperience with ML/DL libraries such as scikit-learn, TensorFlow, or PyTorch\nSolid knowledge of time-series modeling, forecasting techniques, and anomaly detection\nExperience building and deploying APIs for model serving (FastAPI, Flask)\nFamiliarity with real-time data pipelines using Kafka, Spark, or similar tools\nStrong understanding of model validation, feature engineering, and performance tuning\nAbility to work with SQL and NoSQL databases, and large-scale datasets\nGood communication skills and stakeholder engagement experience\n\n\nGood to Have\n\nExperience with ML model deployment tools (MLflow, Docker, Airflow)\nUnderstanding of MLOps and continuous model delivery practices\nBackground in aviation, logistics, manufacturing, or other industrial domains\nFamiliarity with edge deployment and optimization of vision models\n\n\nQualifications\n\nMasters or PhD in Data Science, Computer Science, Applied Mathematics, or related field\n7+ years of experience in machine learning and data science, including end-to-end deployment of models in production",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['scikit-learn', 'time-series modeling', 'ML/DL libraries', 'data science', 'Python', 'Airflow', 'Kafka', 'MLflow', 'logistics', 'anomaly detection', 'aviation', 'SQL', 'PyTorch', 'NoSQL', 'MLOps', 'forecasting techniques', 'Docker', 'manufacturing', 'FastAPI', 'Spark', 'TensorFlow', 'Flask']",2025-06-10 15:16:24
Senior/Lead Data Scientist,Tiger Analytics,6 - 11 years,Not Disclosed,"['Hyderabad', 'Chennai', 'Bengaluru']","Role & responsibilities\n\nCurious about the role? What your typical day would look like?As a Senior Data Scientist, your work is a combination of hands-on contribution to Loreum Ipsum, Loreum Ipsum, etc. More specifically, this will involve:\nLead and contribute to developing sophisticated machine learning models, predictive analytics, and statistical analyses to solve complex business problems.",,,,"['Data Science', 'Time Series Analysis', 'Machine Learning', 'Python', 'Time Series Forecasting', 'Regression', 'Clustering', 'neural nets', 'Optimization', 'SQL']",2025-06-10 15:16:27
Lead - Data Scientist,Cognitio Analytics,6 - 8 years,Not Disclosed,['Gurugram'],"Senior Analyst\nGurugram (Hybrid)\n2 To 4 years\n+ Job Description\nApply\nCore responsibilities include:\nOwning and executing distinct work streams within larger analytics engagement\nDelivering insights based on complex data analysis, within relevant verticals (insurance, health care, banking, etc.)\nHands on experience in data manipulation/processing skills using Python.\nExperience in exploratory data analysis and feature engineering\nMust have strong capabilities in problem solving, managing own work diligently, thoroughly documenting own work, succinctly communicating analysis process and outcomes, as well as effectively working with clients\nBasic understanding of at least one business area and its components (Healthcare, Insurance, Banking, Telecommunications, Logistics)\nFamiliarity with / Exposure on cloud engineering (preferred)\nAbility to translate technical information to non-technical stakeholders and vice versa\nStrong verbal and written communications skills\nActively seeks information to clarify customer needs to deliver better experience\nActs promptly to ensure customer needs are fulfilled",Industry Type: Management Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Basic', 'Data analysis', 'data manipulation', 'Senior Analyst', 'Banking', 'Cloud', 'Healthcare', 'Analytics', 'Logistics', 'Python']",2025-06-10 15:16:30
Lead Data Scientist - Media Mix Modelling (MMM),Blend360 India,7 - 10 years,Not Disclosed,['Hyderabad'],"Job Description\nWe are seeking a highly skilled Data Scientist/Analyst with expertise in Media Mix Modeling (MMM) to join our dynamic analytics team. The ideal candidate will play a critical role in providing actionable insights and optimizing marketing spend across various channels by leveraging statistical models and data-driven techniques.\nKey Responsibilities:\nDevelop and implement Media Mix Models to optimize marketing spend across different channels (e.g., TV, digital, radio, print, etc.).\nAnalyze historical data to understand the impact of marketing efforts and determine the effectiveness of different media channels.\nCollaborate with marketing and business teams to translate business objectives into quantitative analyses and actionable insights.\nBuild predictive models to forecast the impact of future marketing activities and recommend budget allocation.\nPresent and communicate complex findings in a clear, concise, and actionable manner to both technical and non-technical stakeholders.\nPerform deep-dive analyses of marketing campaigns and customer data to identify trends, opportunities, and areas for improvement.\nEnsure data integrity, accuracy, and consistency in all analyses and models.\nStay up-to-date with the latest trends and advancements in media mix modeling, marketing analytics, and data science.\nCollaborate with cross-functional teams including Data Engineering, Marketing, and Business Intelligence to ensure seamless data flow and integration.\nCreate and maintain documentation for all models, methodologies, and analysis processes.\n\n\nQualifications\nBachelor s or Master s degree in Data Science, Statistics, Economics, Mathematics, or a related field.\nProven experience (6+ years) working in Media Mix Modeling (MMM) and/or marketing analytics.",Industry Type: Advertising & Marketing,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data analysis', 'Analytical', 'Social media', 'Consulting', 'data integrity', 'Regression analysis', 'Business intelligence', 'CRM', 'SQL', 'Python']",2025-06-10 15:16:32
Senior / Lead Data Scientist - Time Series,Tiger Analytics,5 - 9 years,Not Disclosed,"['Hyderabad', 'Chennai', 'Bengaluru']","* 6-10 years of total DS and model development experience\n* A passion for writing high-quality code (Python), and the code should be modular, scalable, and end-end project execution while planning an active hands-on role\n* Having good problem-solving skills is essential, and it is equally important to have in-depth knowledge to solve complex problems effectively.\n* Comprehensive knowledge of the regression and classification concepts and mathematical backend along with SQL\n* Experience in Time Series Forecasting Idea about other machine learning techniques (clustering, regression, ensemble learning, neural nets, time series, optimizations etc.) to their real-world problems",,,,"['Data scientist', 'Regression', 'Time Series', 'Machine Learning', 'Python', 'Forecasting']",2025-06-10 15:16:35
Senior Data Scientist,Fastenal,4 - 9 years,Not Disclosed,['Bengaluru'],"Job Description:\nWe are seeking a highly skilled and experienced Senior Data Scientist to join our team. The ideal candidate will have a strong background in data science, machine learning, and statistical analysis. As a Senior Data Scientist, you will be responsible for leading data-driven projects, developing predictive models, and providing actionable insights to drive business decisions.\n\nKey Responsibilities:",,,,"['Data Science', 'SQL', 'Pyspark', 'R', 'Python']",2025-06-10 15:16:38
Senior Data Scientist- EV,VecT,5 - 8 years,Not Disclosed,['Delhi / NCR'],"Key Responsibilities:\n\nLead the data science efforts across the platformdefining architecture, reusable modeling components, and integration with product workflows\nDesign and implement ML models to predict driver income using IoT and operational data\nDevelop AI/ML-based models to help EV financing, leasing, fleet cos, to manage non performing assets\nBuild predictive and preventive maintenance models to reduce vehicle downtime\nDevelop battery degradation models to support lifecycle management and warranty analytics\nManage and mentor a team of Data Scientists and analysts\nCollaborate cross-functionally with Product, Engineering, and Operations teams to ensure models are scalable and production-ready\nDefine and implement best practices for data quality, model governance, and continuous monitoring\n\nRequirements:\n\n5+ years of experience in data science, with at least 2 years in a managerial or leadership role\nStrong experience in building and deploying machine learning models in production environments\nProficiency in Python, SQL, and ML frameworks (e.g., scikit-learn, XGBoost, TensorFlow)\nFamiliarity with IoT data and experience in the mobility or EV sector is a strong plus\nDemonstrated ability to translate business problems into data solutions\nStrong leadership, communication, and stakeholder management skills\nExceptional analytical and problem-solving skills.",Industry Type: Automobile (Electric Vehicle (EV)),Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'XGBoost', 'Data analysts', 'machine learning', 'TensorFlow', 'Python', 'SQL']",2025-06-10 15:16:41
Senior Data Scientist,Comviva Technology,2 - 5 years,Not Disclosed,['Gurugram'],"Job Description for Sr Data Scientist (relevant experience: 2 to 5 years)\n\nWhat we look for:\nExpertise in Big Data/ML: Should be able to build cutting edge credit/churn/usage models using advanced algorithms in a Big data/Machine Learning environment. Should have hands on experience and track record of delivering projects in individual capacity.\n- Process oriented: Should help in building a process that maximizes operating efficiency while maintaining risk across multiple lending cycles. There needs to be an obsession with collecting and analyzing data to drive business iterations and improvements.\n- Willingness to go above and beyond: For start-ups the responsibilities and needs of the business change quickly. Were looking for someone who is not afraid to take on calculated risks and can deal with ambiguity.\n\nJob Responsibilities:\n- Develop innovative credit risk / churn / usage models using mobile wallet transaction data, Call data, Telecom usage data, customer bureau data etc\n- Partner with the Data Engineering team to define the required data pipelines to build and enhance the feature bank (foundational capability) to build/deploy the various ML algorithms both for batch and real time use cases\n- Collaborate with credit policy/portfolio mgmt. team to drive P&L outcomes.\n- Building reports for model monitoring and drive enhancements\nRequired Qualifications & Skills:\nSolid expertise in end-to-end risk model lifecycle management (develop, deploy, monitor)\nPrevious hands on in credit, fraud, churn model development and deployment\nPrevious experience in PD/EAD/LGD model development/validation\nExperience in CSI/PSI model monitoring process\nHands on experience in data extraction using SQL/Pyspark SQL; data cleaning, feature creation and building models using Py Spark/Python on Spark; Scale will be a plus.\nPrevious exposure to below algorithms (preferably multiple):\nLogistic Regression\nRandom forest\nXGBOOST\nMarkov Chain\nPSI/CSI for model monitoring\nStrategy performance tracking and swap in / swap out analysis\n- Strong entrepreneurial drive\n- Good to have:\no Good business understanding of the fintech/consumer finance space\no Experience in working with credit card / personal lending space, esp fintech\no hands on experience in working with Telecom data",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Telecom', 'Logistic regression', 'Consumer finance', 'Machine learning', 'model development', 'Deployment', 'Business understanding', 'SQL', 'Python', 'Data extraction']",2025-06-10 15:16:44
Lead Data Scientist - RevX,Affle,5 - 9 years,Not Disclosed,"['Mumbai', 'Gurugram', 'Bengaluru']","Research and Problem-Solving: Identify and frame business problems, conduct exploratory data analysis, and propose innovative data science solutions tailored to business needs.\nLeadership & Communication: Serve as a technical referent for the research team, driving high-impact, high-visibility initiatives. Effectively communicate complex scientific concepts to senior stakeholders, ensuring insights are actionable for both technical and non-technical audiences. Mentor and develop scientists within the team, fostering growth and technical excellence.\nAlgorithm Development: Design, optimize, and implement advanced machine learning algorithms, including neural networks, ensemble models (XGBoost, random forests), and clustering techniques.\nEnd-to-End Project Ownership: Lead the development, deployment, and monitoring of machine learning models and data pipelines for large-scale applications.\nModel Optimization and Scalability: Focus on optimizing algorithms for performance and scalability, ensuring robust, well-calibrated models suitable for real-time environments.\nA/B Testing and Validation: Design and execute experiments, including A/B testing, to validate model effectiveness and business impat.\nBig Data Handling: Leverage tools like BigQuery, advanced SQL, and cloud platforms (e.g., GCP) to process and analyze large datasets.\nCollaboration and Mentorship: Work closely with engineering, product, and campaign management teams, while mentoring junior data scientists in best practices and advanced techniques.\nData Visualization: Create impactful visualizations using tools like matplotlib, seaborn, Looker, and Grafana to communicate insights effectively to stakeholders.\nRequired Experience/Skills\n5–8 years of hands-on experience in data science or machine learning roles.\n2+ years leading data science projects in AdTech\nStrong hands-on skills in Advanced Statistics, Machine Learning, and Deep Learning.\nDemonstrated ability to implement and optimize neural networks and other advanced ML models.\nProficiency in Python for developing machine learning models, with a strong grasp of TensorFlow or PyTorch.\nExpertise handling large datasets using advanced SQL and big data tools like BigQuery\nIn-depth knowledge of MLOps pipelines, from data preprocessing to deployment and monitoring.\nStrong background in A/B testing, statistical analysis, and experimental design.\nProven capability in clustering, segmentation, and unsupervised learning methods.\nStrong problem-solving and analytical skills with a focus on delivering business value.\nEducation:\nA Master’s in Data Science, Computer Science, Mathematics, Statistics, or a related field is preferred. A Bachelor's degree with exceptional experience will also be considered.",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['advance sql', 'python', 'adtech', 'neural networks', 'random forest', 'machine learning', 'sql', 'pipeline', 'deep learning', 'tensorflow', 'ab testing', 'seaborn', 'data science', 'grafana', 'design', 'matplotlib', 'pytorch', 'bigquery', 'big data', 'xgboost', 'machine learning algorithms', 'statistics', 'ml']",2025-06-10 15:16:47
Mega Walk In | TELUS Digital | Annotation Analyst | For Freshers,TELUS International,0 - 2 years,2.5-3 Lacs P.A.,"['Gandhinagar', 'Ahmedabad']","Job Description:\nWe are looking for motivated and detail-oriented Annotation Analysts who are fresh graduates eager to start their careers in the AI and data annotation field. As an Annotation Analyst, you will play a crucial role in training machine learning models by accurately labeling and annotating data such as images, text, audio, or video.\n\nResponsibilities:",,,,"['Fresher', 'Cad Software', 'Backend', 'Annotation Analyst', 'CAD', 'Back Office Operations', 'Aiml', 'Fresher Hiring', 'Data Annotation', 'Annotation', 'Freshers']",2025-06-10 15:16:50
Data Scientist 2,Fission Labs,4 - 8 years,20-30 Lacs P.A.,['Hyderabad'],"Company Name - Fission Labs\n\nApply Here - https://app.fabrichq.ai/jobs/0e46cebe-8b91-4a96-9061-950b66dc4d54\n\nAbout Us:\nHeadquartered in Sunnyvale, with offices in Dallas & Hyderabad, Fission Labs is a leading\nsoftware development company, specializing in crafting flexible, agile, and scalable solutions\nthat propel businesses forward.With a comprehensive range of services, including product development, cloud engineering, big data analytics, QA, DevOps consulting, and AI/ML solutions, we empower clients to achieve sustainable digital transformation that aligns seamlessly with their business goals.\n\nKey Responsibilities\nDesign and architect complex Generative AI solutions using AWS technologies\nDevelop advanced AI architectures incorporating state-of-the-art GenAI technologies\nCreate and implement Retrieval Augmented Generation (RAG) and GraphRAG solutions\nArchitect scalable AI systems using AWS Bedrock and SageMaker\nDesign and implement agentic AI systems with advanced reasoning capabilities\nDevelop custom AI solutions leveraging vector databases and advanced machine learning techniques\nEvaluate and integrate emerging GenAI technologies and methodologies\n\nTechnical Expertise Requirements\n\nGenerative AI Technologies\nExpert-level understanding of:\nRetrieval Augmented Generation (RAG)\nVector Database architectures\nAgentic AI design principles\n\nAWS AI Services\nComprehensive expertise in:\nAWS Bedrock\nAmazon SageMaker\nAWS AI/ML services ecosystem\nCloud-native AI solution design\n\nTechnical Skills\nAdvanced Python programming for AI/ML applications",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['LoRA', 'Vector Database', 'RAG', 'LLM', 'Python', 'GraphRag']",2025-06-10 15:16:52
Proactive Hiring For Databricks,HCLTech,5 - 10 years,Not Disclosed,"['Noida', 'Chennai', 'Bengaluru']","Responsibilities\nLead the design, development, and implementation of big data solutions using Apache Spark and Databricks.\nArchitect and optimize data pipelines and workflows to process large volumes of data efficiently.\nUtilize Databricks features such as Delta Lake, Databricks SQL, and Databricks Workflows to enhance data processing and analytics capabilities.",,,,"['apache spark', 'Databricks Engineer', 'SQL']",2025-06-10 15:16:56
Data Analyst,Capgemini,4 - 6 years,Not Disclosed,['Bengaluru'],"Overview\nWe are seeking a highly motivated Data Analyst with strong technical and analytical skills to join our ADAS (Advanced Driver Assistance Systems) team. This role involves working with large-scale data from vehicle systems to drive insights, support data science initiatives, and contribute to the development of safer and smarter automotive technologies.\n\nResponsibilities:\nPerform data cleansing, aggregation, and analysis on large, complex datasets related to ADAS components and systems.\nBuild, maintain, and update dashboards and data visualizations to communicate insights effectively (Power BI preferred).\nDevelop and optimize data pipelines and ETL processes.\nCreate and maintain technical documentation, including data catalogs and process documentation.\nCollaborate with cross-functional teams including data scientists, software engineers, and system engineers.\nContribute actively to the internal data science community by sharing knowledge, tools, and best practices.\nWork independently on assigned projects, managing priorities and delivering results in a dynamic, unstructured environment.Required Qualifications:\nBachelors degree or higher in Computer Science, Data Science, or a related field.\nMinimum 3 years of experience in the IT industry, with at least 2 years in data analytics or data engineering roles.\nProficient in Python or Pyspark with solid software development fundamentals.\nStrong experience with SQL and relational databases.\nHands-on experience with data science, data engineering, or machine learning techniques.\nKnowledge of data modeling, data warehousing concepts, and ETL processes.\nFamiliarity with data visualization tools (Power BI preferred).\nBasic understanding of cloud platforms such as Azure or AWS.\nFundamental knowledge of ADAS functionalities is a plus.\nStrong problem-solving skills, self-driven attitude, and the ability to manage projects independently.Preferred Skills:\nExperience in automotive data or working with sensor data (e.g., radar, lidar, cameras).\nFamiliarity with agile development methodologies.\nUnderstanding of big data tools and platforms such as Databricks or Spark. Works in the area of Software Engineering, which encompasses the development, maintenance and optimization of software solutions/applications.1. Applies scientific methods to analyse and solve software engineering problems.2. He/she is responsible for the development and application of software engineering practice and knowledge, in research, design, development and maintenance.3. His/her work requires the exercise of original thought and judgement and the ability to supervise the technical and administrative work of other software engineers.4. The software engineer builds skills and expertise of his/her software engineering discipline to reach standard software engineer skills expectations for the applicable role, as defined in Professional Communities.5. The software engineer collaborates and acts as team player with other software engineers and stakeholders.- Grade SpecificIs fully competent in it's own area and has a deep understanding of related programming concepts software design and software development principles. Works autonomously with minimal supervision. Able to act as a key contributor in a complex environment, lead the activities of a team for software design and software development. Acts proactively to understand internal/external client needs and offers advice even when not asked. Able to assess and adapt to project issues, formulate innovative solutions, work under pressure and drive team to succeed against its technical and commercial goals. Aware of profitability needs and may manage costs for specific project/work area. Explains difficult concepts to a variety of audiences to ensure meaning is understood. Motivates other team members and creates informal networks with key contacts outside own area.Skills (competencies)Verbal Communication",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'software development', 'pyspark', 'relational databases', 'sql', 'data analytics', 'software design', 'data warehousing', 'microsoft azure', 'power bi', 'machine learning', 'data engineering', 'data bricks', 'data science', 'data modeling', 'spark', 'adas', 'agile', 'etl', 'aws', 'big data']",2025-06-10 15:16:58
Lead Data Scientist,Tothr,8 - 12 years,20-22.5 Lacs P.A.,"['Chennai', 'Bengaluru']","Experience working closely with other data scientists, data engineers' software engineers, data managers and business partners.\n7+ years in designing, planning, prototyping, productionizing, maintaining\nknowledge in Python, Go, Java,\nSQL knowledge",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Statistical Modeling', 'Python Development', 'Machine Learning', 'Deep Learning', 'Generative Ai', 'Advance Sql']",2025-06-10 15:17:01
Pyspark Engineer,Barclays,0 - 6 years,Not Disclosed,['Pune'],"Join us as a Pyspark Engineer at Barclays, responsible for supporting the successful delivery of location strategy projects to plan, budget, agreed quality and governance standards. Youll spearhead the evolution of our digital landscape, driving innovation and excellence. You will harness cutting-edge technology to revolutionize our digital offerings, ensuring unparalleled customer experiences.\nTo be successful as a Pyspark Engineer you should have experience with:\nPyspark\nAWS\nSnowflake\nDatawarehouse technologies\nSome other highly valued skills may include:\nDevOps tools\nAirflow\nIceberg\nAgile Methodologies\nYou may be assessed on key critical skills relevant for success in role, such as risk and controls, change and transformation, business acumen, strategic thinking and digital and technology, as well as job-specific technical skills.\nThis role is based out of Pune.\nPurpose of the role\nTo build and maintain the systems that collect, store, process, and analyse data, such as data pipelines, data warehouses and data lakes to ensure that all data is accurate, accessible, and secure.\nAccountabilities\nBuild and maintenance of data architectures pipelines that enable the transfer and processing of durable, complete and consistent data.\nDesign and implementation of data warehoused and data lakes that manage the appropriate data volumes and velocity and adhere to the required security measures.\nDevelopment of processing and analysis algorithms fit for the intended data complexity and volumes.\nCollaboration with data scientist to build and deploy machine learning models.\nAssistant Vice President Expectations\nTo advise and influence decision making, contribute to policy development and take responsibility for operational effectiveness. Collaborate closely with other functions/ business divisions.\nLead a team performing complex tasks, using well developed professional knowledge and skills to deliver on work that impacts the whole business function. Set objectives and coach employees in pursuit of those objectives, appraisal of performance relative to objectives and determination of reward outcomes\nIf the position has leadership responsibilities, People Leaders are expected to demonstrate a clear set of leadership behaviours to create an environment for colleagues to thrive and deliver to a consistently excellent standard. The four LEAD behaviours are: L - Listen and be authentic, E - Energise and inspire, A - Align across the enterprise, D - Develop others.\nOR for an individual contributor, they will lead collaborative assignments and guide team members through structured assignments, identify the need for the inclusion of other areas of specialisation to complete assignments. They will identify new directions for assignments and/ or projects, identifying a combination of cross functional methodologies or practices to meet required outcomes.\nConsult on complex issues; providing advice to People Leaders to support the resolution of escalated issues.\nIdentify ways to mitigate risk and developing new policies/procedures in support of the control and governance agenda.\nTake ownership for managing risk and strengthening controls in relation to the work done.\nPerform work that is closely related to that of other areas, which requires understanding of how areas coordinate and contribute to the achievement of the objectives of the organisation sub-function.\nCollaborate with other areas of work, for business aligned support areas to keep up to speed with business activity and the business strategy.\nEngage in complex analysis of data from multiple sources of information, internal and external sources such as procedures and practises (in other areas, teams, companies, etc). to solve problems creatively and effectively.\nCommunicate complex information. Complex information could include sensitive information or information that is difficult to communicate because of its content or its audience.\nInfluence or convince stakeholders to achieve outcomes.",Industry Type: Financial Services,Department: Other,"Employment Type: Full Time, Permanent","['Machine learning', 'Service excellence', 'Agile', 'Manager Technology', 'Manual', 'Business strategy', 'Assistant Vice President', 'Individual Contributor', 'Operations', 'Data warehousing']",2025-06-10 15:17:03
Sr. Data Scientist - Chennai,Teamplus Staffing Solution,10 - 17 years,25-40 Lacs P.A.,['Chennai( T Nagar )'],"LLMs - OpenAI, Gemini, CoPilot etc.\nGood Knowledge of RAG Pipeline Architectures\nFine/Prompt/Instruction Tuning of LLMs\nmachine learning frameworks (like Keras or PyTorch) and libraries (like scikit-learn).\ncloud services (GCP, Azure, AWS).",Industry Type: Emerging Technologies (AI/ML),Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Data Scientist', 'Azure', 'Artificial Intelligence', 'CoPilot', 'LLM', 'Machine Learning', 'Deep Learning', 'Pytorch', 'GCP', 'RAG', 'OpenAI', 'Keras', 'AWS', 'Gemini']",2025-06-10 15:17:05
Lead Data Scientist,PLAYSIMPLE,2 - 5 years,Not Disclosed,['Bengaluru'],"Were looking for Data Scientists to join our Central Analytics team. Ita fast-paced, high-adrenaline job, with plenty to learn. If you enjoy crunching numbers, are never satisfied with the products around you, and have always wanted to make things better than the best, youll love this job.\n\nWhatrequired of you\n\nYou will work closely with product leaders as a data-driven advisor and partner on strategic issues.\nWill work collaboratively with game teams to deliver actionable insights into our games to further increase user acquisition, engagement and\nmonetization.\nWill proactively perform a wide range of analyses to identify trends, issues, and opportunities across games help us continue to improve gameplay.\nWill answer business-related questions through exploratory data analyses and ad-hoc reporting.\n\nRequirements\nB.Tech/M.Tech/PhD in Computer Science/Statistics with demonstrated experience in ML/Statistics\nHands on experience in Python/Spark for data crunching, data visualization and machine learning.\nSQL skills: Experience querying large, complex datasets from a data lake/warehouse.\nAbility to execute research projects, and generate practical results and recommendations.\nThe candidate should enjoy both working with people and undertaking rigorous statistical analyses.\nKnowledge of Deep Learning is highly desirable",Industry Type: Film / Music / Entertainment,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'deep learning', 'Warehouse', 'spark', 'Machine learning', 'data visualization', 'Statistics', 'Analytics', 'SQL', 'Python']",2025-06-10 15:17:07
Project Leader (MCX) - Customer CoE,Bain,6 - 8 years,Not Disclosed,['New Delhi'],"Lead consulting projects (critical)\nDrive end-to-end delivery of projects for clients, ensuring clear objectives, timelines, and deliverables are met (along with inputs from Manager/ Senior Manager)\nDay-to-day work planning and team management. Take responsibility for scoping and delegating work streams; monitor and balance workloads within the team of analysts and associates\nServe as the main point of contact for clients/ Bain case teams, providing strategic insights and ensuring client satisfaction.",,,,"['Data analysis', 'Team management', 'Primary research', 'Analytical', 'Consulting', 'Intellectual property', 'SEM', 'SEO', 'Financial services', 'Secondary research']",2025-06-10 15:17:10
Senior Data Scientist,Oportun,3 - 6 years,Not Disclosed,[],"We are growing our world-class team of mission-driven, entrepreneurial Data Scientists who are passionate about broadening financial inclusion by untapping insights from non-traditional data. Be part of the team responsible for developing and enhancing Oportun's core intellectual property used in scoring risk for underbanked consumers that lack a traditional credit bureau score.\n\nIn this role you will be on the cutting edge working with large and diverse (i.e. data from dozens of sources including transactional, mobile, utility, and other financial services) alternative data sets and utilize machine learning and statistical modeling to build scores and strategies for managing risk, collection/loss mitigation, and fraud. You will also drive growth and optimize marketing spend across channels by leveraging alternative data to help predict which consumers would likely be interested in Oportuns affordable, credit building loan product.\n\nRESPONSIBILITIES\n\nDevelop data products and machine learning models used in Risk, Fraud, Collections, and portfolio management, and provide frictionless customer experience for various products and services Oportun provides.\nBuild accurate and automated monitoring tools which can help us to keep a close eye on the performance of the models and rules.\nBuild model deployment platform which can shorten the time of implementing new models.\nBuild end-to-end reusable pipelines from data acquisition to model output delivery.\nLead initiatives to drive business value from start to finish including project planning,\ncommunication, and stakeholder management.\nLead discussions with Compliance, Bank Partners, and Model Risk Management teams to\nfacilitate the Model Governance Activities such as Model Validations and Monitoring\n\n\nREQUIREMENTS\nA relentless problem solver and out of the box thinker with a proven track record of driving business results in a timely manner\nMasters degree or PhD in Statistics, Mathematics, Computer Science, Engineering or Economics or other quantitative discipline (Bachelor’s degree with significant relevant experience will be considered).\nHands on experience leveraging machine learning techniques such as Gradient Boosting, Logistic Regression and Neural Network to solve real world problems\n3+ years of hands-on experience with data extraction, cleaning, analysis and building reusable data pipelines; Proficient in SQL, Spark SQL and/or Hive\n3+ years of experience in leveraging modern machine learning toolset and programming languages such as Python\nExcellent written and oral communication skills\nStrong stakeholder management and project management skills\nComfortable in a high-growth, fast-paced, agile environment\nExperience working with AWS EMR, Sage-maker or other cloud-based platforms is a plus\nExperience with HDFS, Hive, Shell script and other big data tools is a plus",Industry Type: FinTech / Payments,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['cloud', 'ML model building', 'Statistics', 'Python', 'sql']",2025-06-10 15:17:14
Opportunity | Senior Lead DS (Consulting) | Tavant India,Tavant Technologies,11 - 21 years,Not Disclosed,"['Noida', 'Hyderabad/Secunderabad', 'Bangalore/Bengaluru']","Dear candidate,\n\nWe found your profile suitable for our current opening, please go through the below JD for better understanding of the role,\n\nJob Description :\nRole : Technical Architect/Senior Technical Architect\nExp : 10-25 years\nMode of work : Hybrid Model\nWork Location : Hyderabad/Bangalore/Noida/Pune/Kolkata\nRole Overview:\nWe are looking for an experienced Senior Lead Data Scientist / ML Engineer with a strong blend of pre-sales expertise, team leadership, and technical proficiency across classical machine learning, deep learning, and generative AI. You will engage in high-level client discussions, drive technical sales strategies, and lead a team to design and implement cutting-edge ML solutions. This is a strategic role requiring both thought leadership and hands-on technical contributions.\nRoles & Responsibilities\nKey Responsibilities\nPre-Sales & Client Engagement\nCollaborate with the sales and business development teams to identify client needs and formulate AI/ML solutions.\nPresent technical concepts, project proposals, and proof-of-concepts (POCs) to prospects and clients.\nTranslate complex client requirements into actionable project scopes, estimates, and technical proposals.\nLeadership & Team Management\nProvide direction, mentorship, and performance feedback to a team of data scientists and ML engineers.\nEstablish best practices in solution design, code reviews, model validation, and production deployment.\nDrive the strategic roadmap for AI initiatives,ensuring alignment with organizational goals and market trends.\nClassical Machine Learning & Statistical Modeling\nApply classical machine learning techniques (e.g.,regression, clustering, decision trees, ensemble methods) to solve diverse business problems.\nDesign and optimize data pipelines, feature engineering processes, and model selection strategies.\nEnsure robust model evaluation, tuning, and performance monitoring in production environments\nDeep Learning & Generative AI\nDevelop and maintain deep learning models using frameworks such as TensorFlow or PyTorch for tasks like computer vision, NLP, or recommendation systems.\nExplore and build solutions leveraging generative AI (GANs, VAEs, or transformer-based architectures) for innovative product features and services.\nChampion research and experimentation with state-of-the-art AI models, staying ahead of industry advances.\nProject Delivery & MLOps\nLead end-to-end ML project lifecycles, from data exploration and model development to deployment and post-launch maintenance.\nImplement MLOps best practices (CI/CD,containerization, model versioning) on cloud or on-premise infrastructures.\nCollaborate with DevOps and engineering teams to integrate ML solutions seamlessly into existing systems.\nStakeholder Management & Communication\nServe as a key technical advisor to executive leadership, product managers, and client teams.\nCommunicate complex AI/ML findings in clear,actionable terms to both technical and non-technical audiences.\nAdvocate data-driven decision-making and foster a culture of innovation within the organization.\nRequired Qualifications\nEducation & Experience\nMasters or PhD in Computer Science, Data Science, Engineering, or a related field is preferred.\n12+ years of relevant industry experience in data science or ML engineering, with 5+ years in a leadership or management capacity.\nTechnical Expertise\nPre-Sales: Demonstrated experience in client-facing roles, solutioning, and proposal development.\nClassical ML: Skilled in traditional algorithms (regression, classification, clustering, etc.) and statistical methods.\nDeep Learning: Hands-on expertise with frameworks (e.g., TensorFlow, PyTorch) for CNNs, RNNs, transformer architectures, etc.\nGenerative AI: Practical exposure to GANs, VAEs, or large language models, with a track record of building generative models.\nMLOps: Familiarity with CI/CD pipelines, Docker/Kubernetes, and cloud platforms (AWS, Azure, GCP).\nLeadership & Communication\nProven ability to mentor and lead data science/ML engineering teams to meet project goals.\nExceptional communication skills for presenting to clients, stakeholders, and executive leadership.\nExperience in agile methodologies and project management, balancing multiple projects simultaneously.\nPreferred / Bonus Skills\nExperience in big data ecosystems (Spark, Hadoop) for large-scale data processing.\nBackground in NLP, computer vision, or recommendation systems.\nKnowledge of DevOps tools (Jenkins, GitLab CI, Terraform) for infrastructure automation.\nTrack record of published research or contributions to open-source AI projects.\n\nPlease check below link for organisation details,\nhttps://www.tavant.com/\n\nIf interested , please drop your resume to dasari.gowri@tavant.com\nRegards\nDasari Krishna Gowri\nAssociate Manager - HR\nwww.tavant.com",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'NLP', 'machine learning', 'client engagement', 'Classification', 'Presales']",2025-06-10 15:17:17
GenAI Developer,Nokia,5 - 9 years,Not Disclosed,['Bengaluru'],"GenAI Developer is a senior software engineer role focused on building AI-powered applications using natural language processing (NLP) and generative AI models. Develop and code in C# or Python, work on prompt engineering and cloud platforms like Azure. Work in an agile environment and collaborate with cross-functional teams.\n\n You have: \nBachelor's or Master's degree in Computer Science, Engineering, AI, or a related field.\nOverall 5- 9 years of work experience in C# or Python development where natural language processing was a key component.\nAtleast 1 year of experience in developing GenAI applications using prompt engineering, in-context learning etc\nPreferably Certified Cloud Developer (either Microsoft, IBM Cloud, AWS, Google)\nGood understanding of orchestrating frameworks like Semantic Kernel, AutoGen, Langchain, Llamaindex etc.\nWork with an Agile mindset to create value across projects of multiple scopes and scale\n\n It would be nice if you also had: \n1-2+ years of experience in participating in data and analytics initiatives in a corporate environment or a growing, data-driven startup\nUnderstanding of advanced analytics, including data management, different types of machine learning approaches and machine learning pipelines\nUnderstanding of fine tuning large language models\n\nDesign and develop generative AI solutions that align with business goals and optimize performance.\nCollaborate with product managers, data scientists, and designers to create AI-powered applications tailored to customer needs.\nBuild and refine natural language processing models, ensuring seamless user interaction through intuitive interfaces.\nIntegrate generative AI models into applications and manage end-to-end software life cycles.\nUtilize C# or Python for backend development, focusing on server-side applications and API integrations.\nStay updated on AI advancements to continuously enhance application performance and capabilities.\nLead documentation efforts and manage production support issues to ensure system reliability.\nFoster an agile, collaborative environment that encourages innovative problem-solving and creative solutions.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['c#', 'python', 'natural language processing', 'python development', 'machine learning', 'advanced analytics', 'css', 'c++', 'data management', 'c', 'kernel', 'backend development', 'artificial intelligence', 'javascript', 'jquery', 'sql', 'production support', 'java', 'linux', 'html', 'api', 'agile', 'aws']",2025-06-10 15:17:20
Specialist Data Science,Merck Sharp & Dohme (MSD),5 - 7 years,Not Disclosed,"['Hyderabad', 'Pune']","Specialist - Data Science\nAt our company we are leveraging analytics and technology, as we invent for life on behalf of patients around the world. We are seeking those who have a passion for using data, analytics, and insights to drive decision making, that will allow us to tackle some of the world s greatest health threats.\nWithin our commercial Insights, Analytics, and Data organization we are transforming to better power decision-making across our end-to-end commercialization process, from business development to late lifecycle management. As we endeavor, we are seeking a dynamic talent to serve in the role of Analyst - Data Science.\nThis role involves working with our partners in different Therapeutic areas (e.g. Oncology, Vaccines, Pharma & Rare Disease, etc.) and Domain areas (HCP Analytics, Patient Analytics, Segmentation & targeting, Market Access, etc.) across the organization to help create scalable and production-grade analytics solutions, ranging from data visualization and reporting to advanced statistical and AI/ML models.\nYou will work in one of the three therapeutic areas of Brand Strategy and Performance Analytics - Oncology/Vaccines/Pharma & Rare Disease, where you will play a pivotal role in leveraging your statistical and machine learning expertise to address critical business challenges and derive insights to drive key decisions. Working alongside experienced data scientists and business analysts, you will have the opportunity to collaborate in translating business queries into analytical problems, employing your critical thinking, problem-solving, statistical, machine learning, and data visualization skills to deliver impactful solutions.\nWe are seeking candidates with prior experience in the healthcare analytics or consulting sectors, prior hands-on experience in Data Science (building end-to-end ML models). It is preferred that you have a good understanding of Physician and Patient-level data (PLD) from leading vendors such as IQVIA, Komodo, and Optum. Familiarity with HCP Analytics, PLD analytics, concepts like persistence, compliance, line of therapy, etc., or Segmentation & Targeting is highly desirable. You will be part of a dynamic team that collaborates with our partners across therapeutic areas. Furthermore, effective communication skills are crucial, as this role requires interfacing with executive and business stakeholders.\nWho you are:\nYou understand the foundations of statistics and machine learning and can work in high performance computing/cloud environments, with experience/knowledge in aspects across statistical analysis, machine learning, model development, data engineering, data visualization, and data interpretation\nYou are self-motivated, and have demonstrated abilities to think independently as a data scientist\nYou structure your data science approach according to the necessary task, while appropriately applying the correct level of model complexity to the problem at hand\nYou have an agile mindset of continuous learning and will focus on integrating enterprise value into team culture\nYou are kind, collaborative, and capable of seeking and giving candid feedback that effectively contributes to a more seamless day-to-day execution of tasks\nKey responsibilities:\nLead a team of Analysts - Data Science to solve complex business problems.\nLead the team in understanding the business requirements and translating them into analytical problem statements.\nDefine technical requirements (datasets, business rules, technical architecture), provide technical direction to the team and manage end-to-end projects\nCollaborate with cross-functional teams to design and implement solutions that meet business requirements\nPresent the findings to senior business stakeholders in a clear and concise manner\nManage and mentor junior data scientists through technical and professional guidance, trainings etc.\nDevelop deep expertise in the therapeutic area of interest, contribute to thought leadership in the domain through publications and conference presentations.\nMinimum Qualifications:\nBachelor s degree with 5-7 years industry experience\nProficiency in Python/R & SQL\nExperience in healthcare analytics or consulting sectors\nExperience working with real world evidence (RWE) and patient level data (PLD) from leading vendors such as IQVIA, Komodo, Optum etc.\nExperience in HCP Analytics, Segmentation and Targeting and Patient Level Data analytics (e.g., creating Patient Cohorts, knowledge of Lines of Therapy, Persistency, Compliance, etc.)\nExperience in leading small sized teams\nStrong Python/R, SQL, Excel skills\nStrong foundations of statistics and machine learning\nPreferred Qualifications:\nAdvanced degree in STEM (MS, MBA, PhD)\nExperience in Oncology/Vaccine/Pharma & Rare Diseases therapeutic area commercial analytics\nKnowledge of statistics, data science and machine learning & commercial\nExperience of supporting End to End Project Management\nCurrent Employees apply HERE\nCurrent Contingent Workers apply HERE\nSearch Firm Representatives Please Read Carefully\nEmployee Status:\nRegular\nRelocation:\nVISA Sponsorship:\nTravel Requirements:\nFlexible Work Arrangements:\nHybrid\nShift:\nValid Driving License:\nHazardous Material(s):\n\nRequired Skills:\nBusiness Intelligence (BI), Database Design, Data Engineering, Data Modeling, Data Science, Data Visualization, Machine Learning, Software Development, Stakeholder Relationship Management, Waterfall Model\n\nPreferred Skills:\nJob Posting End Date:\n07/31/2025\n*A job posting is effective until 11:59:59PM on the day BEFORE the listed job posting end date. Please ensure you apply to a job posting no later than the day BEFORE the job posting end date.",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Relationship management', 'data science', 'Data modeling', 'Project management', 'Pharma', 'Analytical', 'Consulting', 'Agile', 'Business intelligence', 'SQL']",2025-06-10 15:17:22
Data Scientist- Lead,Inspiration Manpower Consultancy.,5 - 8 years,Not Disclosed,"['Pune', 'Bengaluru']","Job Opportunity: Data Scientist- Lead\nExperience: 5+ Years\nMandate Skills: Data Science, Natural Language Processing (NLP) and Large Language model (LLM) landscape, Python, cloud platforms such as AWS, Azure or GCP .\nLocation: Bengaluru, Pune\nNotice Period: 30 days",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Large Language Model', 'Natural Language Processing', 'AWS', 'Python', 'GCP', 'Microsoft Azure']",2025-06-10 15:17:25
Senior Data Scientist,Virtana Corp,6 - 11 years,Not Disclosed,"['Pune', 'Chennai']","Position Overview:\nWe are seeking a Senior Data Scientist Engineer with experience bringing highly scalable enterprise SaaS applications to market. This is a uniquely impactful opportunity to help drive our business forward and directly contribute to long-term growth at Virtana.\nIf you thrive in a fast-paced environment, take initiative, embrace proactivity and collaboration, and you re seeking an environment for continuous learning and improvement, we d love to hear from you!\nVirtana is a remote first work environment so you ll be able to work from the comfort of your home while collaborating with teammates on a variety of connectivity tools and technologies.\nRole Responsibilities:\nResearch and test machine learning approaches for analyzing large-scale distributed computing applications.\nDevelop production-ready implementations of proposed solutions across different models AI and ML algorithms, including testing on live customer data to improve accuracy, efficacy, and robustness\nWork closely with other functional teams to integrate implemented systems into the SaaS platform\nSuggest innovative and creative concepts and ideas that would improve the overall platform.\nJob Location - Pune, Chennai or Remote\nQualifications:\nThe ideal candidate must have the following qualifications:\n6 + years experience in practical implementation and deployment of large customer-facing ML based systems.\nMS or M Tech (preferred) in applied mathematics/statistics; CS or Engineering disciplines are acceptable but must have with strong quantitative and applied mathematical skills\nIn-depth working, beyond coursework, familiarity with classical and current ML techniques, both supervised and unsupervised learning techniques and algorithms\nImplementation experiences and deep knowledge of Classification, Time Series Analysis, Pattern Recognition, Reinforcement Learning, Deep Learning, Dynamic Programming and Optimization\nExperience in working on modeling graph structures related to spatiotemporal systems\nProgramming skills in Python is a must\nExperience in understanding and usage of LLM models and Prompt engineering is preferred.\nExperience in developing and deploying on cloud (AWS or Google or Azure)\nGood verbal and written communication skills\nFamiliarity with well-known ML frameworks such as Pandas, Keras, TensorFlow\nAbout Virtana:\nVirtana delivers the industry s only unified software multi-cloud management platform that allows organizations to monitor infrastructure, de-risk cloud migrations, and reduce cloud costs by 25% or more.\nOver 200 Global 2000 enterprise customers, such as AstraZeneca, Dell, Salesforce, Geico, Costco, Nasdaq, and Boeing, have valued Virtana s software solutions for over a decade.\nOur modular platform for hybrid IT digital operations includes Infrastructure Performance Monitoring and Management (IPM), Artificial Intelligence for IT Operations (AIOps), Cloud Cost Management (Fin Ops), and Workload Placement Readiness Solutions. Virtana is simplifying the complexity of hybrid IT environments with a single cloud-agnostic platform across all the categories listed above. The $30B IT Operations Management (ITOM) Software market is ripe for disruption, and Virtana is uniquely positioned for success.\nCompany Profitable Growth and Recognition\nIn FY2023 (Fiscal year ending January 2023), Virtana earned:\nBest CEO, Best CEO for Women, and Best CEO for Diversity by Comparably\nTwo years in a row YoY Profitable Annual Recurring Revenue (ARR) Growth\nTwo consecutive years of +EBITDA, 78% YoY EBITDA growth, or 20% of Revenue\nPositive Cash Flow, 171% YoY cash flow growth\n\nYou can schedule with us through Calendly at https: / / calendly.com / bimla-dhirayan / zoom-meeting-virtana",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Usage', 'Time series analysis', 'Artificial Intelligence', 'IT operations management', 'Machine learning', 'Cloud', 'Cash flow', 'Pattern recognition', 'Performance monitoring', 'Python']",2025-06-10 15:17:28
Senior Data Scientist,Go Digital Technology Consulting,4 - 7 years,Not Disclosed,"['Pune', 'Mumbai (All Areas)']","Job Title: Sr. Data Scientist\nLocation: Mumbai / Pune\nJob Type: Full-time\nExperience: 4-7 years\n\nAbout the Role:\nWe are seeking a Sr. Data Scientist with experience in Computer Vision to join our team. You will be responsible for building and maintaining models including but not limited to Vision Transformer neural networks. This role involves collaborating with data engineers and backend developers to deliver quality AI solutions.\n\nResponsibilities:\nDesign and implement end-to-end machine learning workflows for image and computer vision applications, from data collection to model deployment.\nCollaborate with cross-functional teams, including data engineers, product managers, and domain experts, to define and prioritize machine learning initiatives.\nDocument technical designs and model specifications, ensuring clarity and accessibility for stakeholders and team members.\nEnsure adherence to best practices in model development, deployment, and monitoring, in alignment with the overall AI strategy.\nMonitor model performance and implement strategies for continuous improvement and retraining as needed.\nDevelop scalable and efficient deep learning models using PyTorch, optimizing for performance and resource utilization.\n\nQualifications:\nBachelors degree in Computer Science or a related field.\n4-7 years of hands-on experience in developing and deploying machine learning models, particularly in computer vision tasks.\nProficient in using PyTorch for developing deep learning models, with a strong understanding of CNNs, transfer learning, vision transformers, and data augmentation techniques.\nSolid understanding of computer vision concepts, including image classification, object detection, and image segmentation.\nStrong programming skills in Python, with experience in data manipulation libraries such as NumPy and Pandas.\nExperience with version control systems like Git.\nExcellent analytical and problem-solving skills, strong communication abilities, and a collaborative mindset.\n\nPreferred Qualifications:\nExperience with cloud platforms (e.g., AWS, GCP, Azure) and their ML services, particularly those related to model deployment and GPU training.\nUnderstanding of MLOps principles and practices, including model monitoring, versioning, and governance.\nKnowledge of GPU computing and tools for managing GPU resources (e.g., CUDA, cuDNN).",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Senior Data Scientist', 'Computer Vision', 'Pytorch', 'Pandas', 'Machine Learning', 'Numpy', 'Python']",2025-06-10 15:17:31
Lead Data Scientist,Kanini Software Solutions,12 - 20 years,Not Disclosed,"['Pune', 'Chennai', 'Bengaluru']","Lead - Data Scientist\n\nAbout the Role:\nWe are seeking a highly skilled and GCP-certified AI/ML Engineer with expertise in Generative AI models, Natural Language Processing (NLP), and cloud-native development. This role involves designing and deploying scalable ML solutions, building robust APIs, and integrating AI capabilities into enterprise applications. The ideal candidate will also have a solid background in software engineering and DevOps practices.",,,,"['Data Science', 'Genrative Ai', 'Natural Language Processing', 'Machine Learning', 'Artificial Intelligence']",2025-06-10 15:17:33
Senior Data Scientist - YouAppi,Affle,3 - 6 years,Not Disclosed,"['Gurugram', 'Bengaluru']","3+ years of experience in data science roles, working with tabular data in large-scale projects. Experience in feature engineering and working with methods such as XGBoost, LightGBM, factorization machines , and similar algorithms.\nExperience in adtech or fintech industries is a plus. Familiarity with clickstream data, predictive modeling for user engagement, or bidding optimization is highly advantageous.\nMS or PhD in mathematics, computer science, physics, statistics, electrical engineering, or a related field.\nProficiency in Python (3.9+), with experience in scientific computing and machine learning tools (e.g., NumPy, Pandas, SciPy, scikit-learn, matplotlib, etc.). Familiarity with deep learning frameworks (such as TensorFlow or PyTorch) is a plus.\nStrong expertise in applied statistical methods, A/B testing frameworks, advanced experiment design, and interpreting complex experimental results.\nExperience querying and processing data using SQL and working with distributed data storage solutions (e.g., AWS Redshift, Snowflake, BigQuery, Athena, Presto, MinIO, etc.).\nExperience in budget allocation optimization, lookalike modeling, LTV prediction, or churn analysis is a plus.\nAbility to manage multiple projects, prioritize tasks effectively, and maintain a structured approach to complex problem-solving.\nExcellent communication and collaboration skills to work effectively with both technical and business teams.",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['deep learning frameworks', 'snowflake', 'scipy', 'python', 'modeling', 'adtech', 'scikit-learn', 'amazon redshift', 'user acquisition', 'numpy', 'sql', 'pandas', 'tensorflow', 'testing frameworks', 'predictive modeling', 'design', 'matplotlib', 'pytorch', 'athena', 'bigquery', 'communication skills', 'presto']",2025-06-10 15:17:35
Data Engineer,Blend360 India,5 - 10 years,Not Disclosed,['Hyderabad'],"Job Description\nWe are looking for an experienced Senior Data Engineer with a strong foundation in Python, SQL, and Spark , and hands-on expertise in AWS, Databricks . In this role, you will build and maintain scalable data pipelines and architecture to support analytics, data science, and business intelligence initiatives. You ll work closely with cross-functional teams to drive data reliability, quality, and performance.\nResponsibilities:\nDesign, develop, and optimize scalable data pipelines using Databricks in AWS such as Glue, S3, Lambda, EMR, Databricks notebooks, workflows and jobs.\nBuilding data lake in WS Databricks.\nBuild and maintain robust ETL/ELT workflows using Python and SQL to handle structured and semi-structured data.\nDevelop distributed data processing solutions using Apache Spark or PySpark .\nPartner with data scientists and analysts to provide high-quality, accessible, and well-structured data.\nEnsure data quality, governance, security, and compliance across pipelines and data stores.\nMonitor, troubleshoot, and improve the performance of data systems and pipelines.\nParticipate in code reviews and help establish engineering best practices.\nMentor junior data engineers and support their technical development.\n\n\nQualifications\nRequirements\nBachelors or masters degree in computer science, Engineering, or a related field.\n5+ years of hands-on experience in data",Industry Type: Advertising & Marketing,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Automation', 'Version control', 'GIT', 'Workflow', 'Data quality', 'Business intelligence', 'Analytics', 'SQL', 'Python']",2025-06-10 15:17:37
Senior AWS Data Engineer,Genspark,5 - 10 years,Not Disclosed,"['Chennai', 'Coimbatore', 'Bengaluru']","Job Summary:\nWe are looking for a highly skilled Senior AWS Data Engineer to design, develop, and lead enterprise-grade data solutions on the AWS cloud. This position requires a blend of deep AWS technical proficiency, hands-on PySpark experience, and the ability to engage with business stakeholders in solution design. The ideal candidate will build scalable, secure, and high-performance data platforms using AWS-native tools and best practices.\n\nRole & responsibilities:\nDesign and implement scalable AWS cloud-native data architectures, including data lakes, warehouses, and streaming pipelines\nDevelop ETL/ELT pipelines using AWS Glue (PySpark/Scala), Lambda, and Step Functions\nOptimize Redshift-based data warehouses including schema design, data distribution, and materialized views\nLeverage Athena, Glue Data Catalog, and S3 for efficient serverless query patterns\nImplement IAM-based data access control, lineage tracking, and encryption for secure data workflows\nAutomate infrastructure and data deployments using CDK, Terraform, or CloudFormation\nDrive data modelling standards (Star/Snowflake, 3NF, Data Vault) and ensure data quality and governance\nCollaborate with data scientists, DevOps, and business stakeholders to deliver end-to-end data solutions\nMentor junior engineers and lead code reviews and architecture discussions\nParticipate in client-facing activities including requirements gathering, technical proposal preparation, and solution demos\n\nMust-Have Qualifications:\nAWS Expertise: Proven hands-on experience with AWS Glue, Redshift, Athena, S3, Lake Formation, Kinesis, Lambda, Step Functions, EMR, and Cloud Watch\nPySpark & Big Data: Minimum 2 years of hands-on PySpark/Spark experience for large-scale data processing\nETL/ELT Engineering:Expertise in Python, dbt, or similar automation frameworks\nData Modelling: Proficiency in designing and implementing normalized and dimensional models\nPerformance Optimization:Ability to tune Spark jobs with custom partitioning, broadcast joins, and memory management\nCI/CD & Automation: Experience with GitHub Actions, Code Pipeline, or similar tools\nConsulting & Pre-sales: Prior exposure to client-facing roles including proposal drafting and cost estimation\nGood-to-Have Skills:\nKnowledge of Iceberg, Hudi, or Delta Lake file formats\nExperience with Athena Federated Queries and AWS OpenSearch\nFamiliarity with Data Zone, Data Brew, and data profiling tools\nUnderstanding of compliance frameworks like GDPR, HIPAA, SOC2\nBI integration skills using Power BI, Quick Sight, or Tableau\nKnowledge of event-driven architectures (e.g., Kinesis, MSK, Lambda)\nExposure to lake house or data mesh architectures\nExperience with Lucid chart, Miro, or other documentation/storyboarding tools\n\nWhy Join Us?\nWork on cutting-edge AWS data platforms\nCollaborate with a high-performing team of engineers and architects\nOpportunity to lead key client engagements and shape large-scale solutions\nFlexible work environment and strong learning culture",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Temporary/Contractual","['Pyspark', 'Big Data', 'AWS Expertise', 'Data Modeling', 'ETL/ELT Engineering', 'Automation', 'CI/CD']",2025-06-10 15:17:40
Associate Analyst - Data Engineer,Pepsico,2 - 7 years,Not Disclosed,['Hyderabad'],"Overview\n\nPepsiCo operates in an environment undergoing immense and rapid change. Big-data and digital technologies are driving business transformation that is unlocking new capabilities and business innovations in areas like eCommerce, mobile experiences and IoT. The key to winning in these areas is being able to leverage enterprise data foundations built on PepsiCo s global business scale to enable business insights, advanced analytics, and new product development. PepsiCo s Data Management and Operations team is tasked with the responsibility of developing quality data collection processes, maintaining the integrity of our data foundations, and enabling business leaders and data scientists across the company to have rapid access to the data they need for decision-making and innovation.\nMaintain a predictable, transparent, global operating rhythm that ensures always-on access to high-quality data for stakeholders across the company.\nResponsible for day-to-day data collection, transportation, maintenance/curation, and access to the PepsiCo corporate data asset\nWork cross-functionally across the enterprise to centralize data and standardize it for use by business, data science or other stakeholders.\nIncrease awareness about available data and democratize access to it across the company.\nAs a data engineer, you will be the key technical expert building PepsiCo's data productsto drive a strong vision. You'll be empowered to create data pipelines into various source systems, rest data on the PepsiCo Data Lake, and enable exploration and access for analytics, visualization, machine learning, and product development efforts across the company.\nAs a member of the data engineering team, you will help developingvery large and complex data applications into public cloud environments directly impacting the design, architecture, and implementation of PepsiCo's flagship data products around topics like revenue management, supply chain, manufacturing, and logistics. You will work closely with process owners, product owners and business users. You'll be working in a hybrid environment with in-house, on-premises data sources as well as cloud and remote systems.\n\nResponsibilities\nAct as a subject matter expert across different digital projects.\nOversee work with internal clients and external partners to structure and store data into unified taxonomies and link them together with standard identifiers.\nManage and scale data pipelines from internal and external data sources to support new product launches and drive data quality across data products.\nBuild and own the automation and monitoring frameworks that captures metrics and operational KPIs for data pipeline quality and performance.\nResponsible for implementing best practices around systems integration, security, performance, and data management.\nEmpower the business by creating value through the increased adoption of data, data science and business intelligence landscape.\nCollaborate with internal clients (data science and product teams) to drive solutioning and POC discussions.\nEvolve the architectural capabilities and maturity of the data platform by engaging with enterprise architects and strategic internal and external partners.\nDevelop and optimize procedures to productionalize data science models.\nDefine and manage SLA s for data products and processes running in production.\nSupport large-scale experimentation done by data scientists.\nPrototype new approaches and build solutions at scale.\nResearch in state-of-the-art methodologies.\nCreate documentation for learnings and knowledge transfer.\nCreate and audit reusable packages or libraries.\n\nQualifications\n4+ years of overall technology experience that includes at least 3+ years of hands-on software development, data engineering, and systems architecture.\n3+ years of experience with Data Lake Infrastructure, Data Warehousing, and Data Analytics tools.\n3+ years of experience in SQL optimization and performance tuning, and development experience in programming languages like Python, PySpark, Scala etc.).\n2+ years in cloud data engineering experience in Azure.\nFluent with Azure cloud services. Azure Certification is a plus.\nExperience in Azure Log Analytics\nExperience with integration of multi cloud services with on-premises technologies.\nExperience with data modelling, data warehousing, and building high-volume ETL/ELT pipelines.\nExperience with data profiling and data quality tools like Apache Griffin, Deequ, and Great Expectations.\nExperience building/operatinghighly available, distributed systems of data extraction, ingestion, and processing of large data sets.\nExperience with at least one MPP database technology such as Redshift, Synapse or Snowflake.\nExperience with Azure Data Factory, Azure Databricks and Azure Machine learning tools.\nExperience with Statistical/ML techniques is a plus.\nExperience with building solutions in the retail or in the supply chain space is a plus.\nExperience with version control systems like Github and deployment & CI tools.\nWorking knowledge of agile development, including DevOps and DataOps concepts.\nB Tech/ BA/ BS in Computer Science, Math, Physics, or other technical fields.\nSkills, Abilities, Knowledge:\nExcellent communication skills, both verbal and written, along with the ability to influence and demonstrate confidence in communications with senior level management.\nStrong change manager. Comfortable with change, especially that which arises through company growth.\nAbility to understand and translate business requirements into data and technical requirements.\nHigh degree of organization and ability to manage multiple, competing projects and priorities simultaneously.\nPositive and flexible attitude to enable adjusting to different needs in an ever-changing environment.\nStrong organizational and interpersonal skills; comfortable managing trade-offs.\nFoster a team culture of accountability, communication, and self-management.\nProactively drives impact and engagement while bringing others along.\nConsistently attain/exceed individual and team goals.\n4+ years of overall technology experience that includes at least 3+ years of hands-on software development, data engineering, and systems architecture.\n3+ years of experience with Data Lake Infrastructure, Data Warehousing, and Data Analytics tools.\n3+ years of experience in SQL optimization and performance tuning, and development experience in programming languages like Python, PySpark, Scala etc.).\n2+ years in cloud data engineering experience in Azure.\nFluent with Azure cloud services. Azure Certification is a plus.\nExperience in Azure Log Analytics\nExperience with integration of multi cloud services with on-premises technologies.\nExperience with data modelling, data warehousing, and building high-volume ETL/ELT pipelines.\nExperience with data profiling and data quality tools like Apache Griffin, Deequ, and Great Expectations.\nExperience building/operatinghighly available, distributed systems of data extraction, ingestion, and processing of large data sets.\nExperience with at least one MPP database technology such as Redshift, Synapse or Snowflake.\nExperience with Azure Data Factory, Azure Databricks and Azure Machine learning tools.\nExperience with Statistical/ML techniques is a plus.\nExperience with building solutions in the retail or in the supply chain space is a plus.\nExperience with version control systems like Github and deployment & CI tools.\nWorking knowledge of agile development, including DevOps and DataOps concepts.\nB Tech/BA/BS in Computer Science, Math, Physics, or other technical fields.",Industry Type: Beverage,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Engineering', 'Data Lake Infrastructure', 'Azure Data Factory', 'PySpark', 'Scala', 'Azure Log Analytics', 'Azure Databricks', 'Data Warehousing', 'Data Analytics', 'data collection', 'Python', 'SQL']",2025-06-10 15:17:43
Data Engineer,Visa,3 - 8 years,Not Disclosed,['Bengaluru'],"We are seeking a Data Engineer with a strong background in data engineering. This role involves managing system requirements, design, development, integration, quality assurance, implementation, and maintenance of corporate applications.\nWork with product owners, business stakeholders and internal teams to understand business requirements and desired business outcomes.\nAssist in scoping and designing analytic data assets, implementing modelled attributes and contributing to brainstorming sessions.\nBuild and maintain a robust data engineering process to develop and implement self-serve data and tools for Visa s product management teams and data scientists.\nFind opportunities to create, automate and scale repeatable analyses or build self-service tools for business users.\nExecute data engineering projects ranging from small to large either individually or as part of a project team.\nSet the benchmark in the team for good data engineering practices and assist leads and architects in solution design.\nExhibit a passion for optimizing existing solutions and making incremental improvements.\nThis is a hybrid position. Expectation of days in office will be confirmed by your hiring manager.\n\n\nBasic Qualification\n-Bachelors degree, OR 3+ years of relevant work experience\n\nPreferred Qualification\n-Minimum of 1 years experience in building data engineering pipelines.\n-Design and coding skills with Big Data technologies like Hadoop, Spark, Hive and Map reduce.\n-Mastery in Pyspark or Scala.\n-Expertise in any programming like Java or Python. Knowing OOP concepts like inheritance, polymorphism and implementing Design Patterns in programming is needed.\n-Experience with cloud platforms like AWS, GCP, or Azure is good to have.\n-Excellent problem-solving skills and ability to think critically.\n-Experience with any one ETL tool like Informatica, SSIS, Pentaho or Azure Data Factory.\n-Knowledge of successful design, and development of data driven real time and batch systems.\n-Experience in data warehousing and an expert in any one of the RDBMS like SQL Server, Oracle, etc.\n-Nice to have reporting skills on PowerBI/Tableau/QlikView.\n-Strong understanding of cloud architecture and service offerings including compute, storage, databases, networking, AI, and ML.\n-Passionate about delivering zero defect code that meets or exceeds the proposed defect SLA and have high sense of accountability for quality and timelines on deliverables.\n-Experience developing as part of Agile/Scrum team is preferred and hands on with Jira.\n-Understanding basic CI/ CD functionality and Git concepts is must.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Product management', 'Manager Quality Assurance', 'Networking', 'RDBMS', 'Coding', 'Informatica', 'Oracle', 'SSIS', 'SQL', 'Python']",2025-06-10 15:17:45
Mega Walk In | TELUS Digital | Annotation Analyst | @Gandhinagar,TELUS International,0 - 2 years,2.5-3 Lacs P.A.,"['Gandhinagar', 'Ahmedabad']",Responsibilities:\nAnnotate and label datasets accurately using specialized tools and guidelines\nReview and correct existing annotations to ensure data quality\nCollaborate with machine learning engineers and data scientists to understand annotation requirements\nFollow detailed instructions and apply judgment to edge cases and ambiguous data\nMeet project deadlines and maintain high levels of accuracy and efficiency,,,,"['Fresher', 'Cad Software', 'Backend', 'Annotation Analyst', 'CAD', 'Back Office Operations', 'Aiml', 'Fresher Hiring', 'Data Annotation', 'Annotation', 'Freshers']",2025-06-10 15:17:48
Senior Manager - Insights ( Advanced Analytics ),Pepsico,10 - 14 years,Not Disclosed,['Hyderabad'],"Overview\n\nProvide advanced analytics support for the LATAMSectorin the Insights Services Center; a part of the broader Global Business Services function in Hyderabad, India.\nThis role will help to enable accelerated growth for PepsiCo by developing custom descriptive analytics approaches for LATAM to drive deeper understanding of business performance driversboth atthe National / Regional / State /City level.\nPrimary responsibilities include developing/enhancing statistical models to address key business questions associated with Growth DriverModeling, Pricing, Assortment, Market Structure, Innovation Forecasting, and Category Growth Forecasts, Portfolio growth model, Ambition related assignmentsfor the key markets of LATAM. Additionally, this role will alsosupport the consumer insight team by collating learnings from a variety of sources, to help inform the development of future insights strategies for various brands with in Bevareages / Snacks .\nThe role will have short-term responsibilities for knowledge transfer from the Vendors and effectively establishing business process and communication methods with teams they support; both are crucial requirements to enabling the organization to deliver answers to on-going business questions\n\nResponsibilities\n\nFunctional Responsibilities:\nSupport delivery of descriptive and predictive analytics as defined by the SLA (Service Level Agreement) within the LATAM Business Service Service Center\nExecute deep descriptive analytics of business performance and drivers to supplement standard reporting and inform data-driven decisions\nIdentify, assess, and visualize key market share drivers for LATAM Categories, as a growth catalyst to prioritize and enable brand planning across portfolio\nSupport LATAMregion s annual SKU optimization process for the portfolio; analyzing impact by channel, customers and region as needed based on HQ delivered recommendations & targets\nAdvise and share Advanced Analytics models and best practices with Regional Category Managers to leverage and build Advanced Analytics capability.\nDevelop, maintain, and apply statistical models to business questions - including forecasting, price sensitivities/corridors, drivers analysis, market structure, etc.\nForecast market growthleveraging (PGM an internal tool) on an annual basis to inform PEP s long-term expectations for growth\nCollate and format consumer learnings from custom insight outputs, sales performace reporting, industry periodicals, and social listening resources, etc to help inform and develop future consumer insights strategies\nProvide responsestoad-hoc follow-ups when double-click (additional questions) required with tables/charts for relevant data\nSupport relationships with the key end-user stakeholdersin LATAMand region offices\nOwn flawless execution of analytics exercises\nResponsible for managing multiple priorities; being able to manage deadlines and deliverables\nLead communication withBusiness Partners and potentially end-users on matters such as available capacity, changes of scope of existing projects and planning of future projects\nDeliver outputs in line with the agreed timelines andformats\nFlag and monitor any business risks related to delivering the requested outputs\nPartner with stakeholders and service center leadership to develop and finetune internal COE processes (work-flow mapping, pain-points and bottlenecks management) both related to service delivery and internal center operations\nImprove existing processes based on frequent Business Partner & end-user feedback loop\nPeople and Project Management Responsibilities\nLeading a team of 4-5 data scientists and modelers\nDevelop new skillsets among team members related to advanced analytics by conducting training and period review\nPromotes cross functional seamless communication, sharing know how and creating same quality of experience across sector markets\n\nQualifications\n10-14 years of experience in the field of analytics\nCritical experience as Data Scientist and Programming in Python & BI skills, basic statistics like regression, types of distribution, expertise in Nielsen, knowledge of media sufficiency/MMM is good to have\nExperience either in boutique analytics consulting or Captive analytics teams\nHave been hands-on modeler in the past specifically in market mix /attribution modeling, CPG domain preferred.\nExperience in translating business problem into analytics framework and vice versa\nExperience in leading mid-sized teams of data scientists and modelers\nWell versed in statistical techniques\nEducational Background- BE/B TECH/ MSC Economics or Statistics, MBA preferred for Analytics translator\nHigh degree of familiarity with CPG and Food & Beverage industry data sources, including Nielsen/IRi (POS and HH panel), Global Data, Kantar Worldpanel, etc.\nDeep understanding of CPG industry business performance outputs and causal measures, their relationships, and how to address business questions in a robust way that translates to simple outputs\nExperience with Agile development methods, SQL, R, Python desired\nStrong collaborator: Interested and motivated by working with others. Actively creates and participates in opportunities to co-create solutions across markets; will be willing and able to embrace Responsive Ways of Working\nCan easily see how insights/research learning can improve businesses impact and frames up reporting efforts and opportunities to inform business decisions\nProven analytics, shopper research experience, or consumer insights experience applying statistics to CPG industry business problems\nOperational experience from business servicing sector and/or consulting experience would be a plus\nLanguage: English Fluent.",Industry Type: Beverage,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Advanced Analytics', 'R', 'Project Management', 'Agile development', 'BI', 'statistical modeling', 'Python', 'SQL']",2025-06-10 15:17:50
Scrum Lead- Project Management,Metlife,8 - 12 years,Not Disclosed,['Hyderabad'],"Position Summary\n\nMetLife established a Global capability center (MGCC) in India to scale and mature Data & Analytics, technology capabilities in a cost-effective manner and make MetLife future ready. The center is integral to Global Technology and Operations with a with a focus to protect & build MetLife IP, promote reusability and drive experimentation and innovation. The Data & Analytics team in India mirrors the Global D&A team with an objective to drive business value through trusted data, scaled capabilities, and actionable insights. The operating models consists of business aligned data officers- US, Japan and LatAm & Corporate functions enabled by enterprise COEs- data engineering, data governance and data science.\n\nRole Value Proposition:\nDriven by passion and a zeal to succeed, we are looking for accomplished Program Manager to structure, plan and handle multiple projects with minimum supervision and will be responsible for successful completion of projects supporting MGCC and US D&A leadership with various strategic initiatives in development and successful implementation of governance and process excellence practices.\nThis position would be responsible for complete adherence of the projects and its objectives and support all aspects of project management. This role will support development of best practices, processes and framework to achieve standardization and streamlining across various initiatives.\n\nJob Responsibilities\nServe as analytics program manager on data, analytics projects and POCs working with data engineers, business analysts, data scientists, IT teams, vendors, executive leaders, and business stakeholders\nDrive transparency leveraging tech stack and data, own progress reporting and proactively communicate status\nDrive delivery of projects using Agile methodology for data and analytics programs\nFacilitate scrum ceremonies including Sprint planning, Daily stand ups, sprint reviews and retrospectives\nResponsible for defining relevant program metrics, status reports and continuous measurement of program portfolio best practices\nLead, coach, support and mentor junior team members\nInteract with senior leadership teams across Data and analytics, IT and business teams.\n\nKnowledge, Skills and Abilities\nBachelors degree. Technology/IT specialization is preferred.\nMBA is a preferred qualification\n8-12+ years of progressive experience in project/program management role with proven people influencing experience including with virtual and global teams\nAgile project management/delivery experience is a must preferably with Data and Analytics background\nProficient in MS Office suite: Excel, PowerPoint, Project.\nUnderstanding of analytical tool stack, Azure Devboards, Jira, SharePoint is a plus\nCSM, SAFe Agilist certifications are preferred\nAbility to identify risks to project success and recommend course of action to prevent risk from negatively impacting the project; Effectively recognize when to escalate issues and options to senior management for resolution\nSuperior solutioning techniques, organizational skills and ability to manage multiple ongoing projects.\nExcellent collaboration and communication skills, both written and verbal\nDemonstrated competency with cross-group collaboration, organizational agility, and analytical planning\nStrong leadership & negotiation skills.",Industry Type: Insurance,Department: Project & Program Management,"Employment Type: Full Time, Permanent","['Agile Methodology', 'Sprint Planning', 'scrum', 'Azure Devops', 'Agile Framework', 'Backlog Refinement', 'Scrum Development', 'Sprint Review', 'Retrospective']",2025-06-10 15:17:53
Data Scientist - Computational Scientist / Applied Mathematician,Gitcs India,5 - 7 years,Not Disclosed,"['Hyderabad', 'Chennai', 'Bengaluru']","Role: Data Scientists - Computational Scientist / Applied Mathematician\nLocation: Bangalore\nMust Have:\nMSc/PhD in Physics, Stats, Math, Comp Bio, CS, etc.\nStrong Python + Jupyter skills\nExperience with real-world biological data\nEager to learn biology & make real patient impact\nBonus: Genomics, sequencing tech, cloud (AWS/GCP), Rust/C++, Snakemake, ML models\nWork with biologists, chemists & ML scientists to analyze experiments & build predictive tools.\n\nAbout Role\nYou have strong mathematical foundations and a passion for understanding & analyzing data. You have a solid understanding of how to analyze noisy, real-world, data. In addition to a strong background in statistics, you are also capable of programming a computer to perform data analysis tasks. You want to impact people by treating genetic diseases and are capable of learning biology that is crucial to the data analysis task.\nYou enjoy close-knit teamwork and a highly interdisciplinary intellectual environment where you will work side-by-side with both computational and bench scientists. You continually learn and challenge yourself with scientific pursuits. You take pride in rigorous pursuit of science.\n\nRequired Qualifications\nMSc or PhD in a quantitative discipline such as physics, statistics, economics, applied mathematics, computer science, computation genomics / biology or related field\nFamiliarity with analyzing real world data and quantifying uncertainty\nGood understanding of Python is a required, knowledge of Rust / C++ are nice to have\nVery comfortable with Juptyer ecosystem\nAbility to independently master unfamiliar topics, especially in biological sciences and data analysis\nDesire to make a difference to patients with rare disease\n\nHelpful Qualifications\nStrong foundation in algorithms of scientific computing: steepest descent, Metropolis-Hastings, Runge-Kutta, Krylov space methods, etc\nFluent and comfortable working across local and cloud environments and tools (AWS, GCP, kubernetes, docker)\nUnderstanding of sequencing technologies (Illumina, PacBio, Oxford Nanopore)\nKnowledge of standard software tools in transcriptomics, genomics, NGS assays & biological data analysis\nUnderstanding of Linux, make, snakemake, git, etc.\nExposure in distributed computing (Spark, Dask etc.)",Industry Type: Biotechnology,Department: Research & Development,"Employment Type: Full Time, Permanent","['Cloud Platforms', 'Research Analysis', 'Python', 'C++', 'Rust', 'Docker', 'Jupyter Ecosystem', 'GCP', 'Statistical Analysis Software', 'Bioinformatics', 'AWS', 'Machine Learning']",2025-06-10 15:17:55
Product Design Internship,Kaleideo,4 months duration,Unpaid,['Bengaluru'],"About SatSure:\n\nSatSure is a deep tech, decision Intelligence company that works primarily at the nexus of agriculture, infrastructure, and climate action creating an impact for the other millions, focusing on the developing world. We want to make insights from earth observation data accessible to all.\n\nJoin us to be at the forefront of building a deep tech company in India that solves problems worldwide.\n\nWhat You ll Do\n\nDive deep into the why behind every screen your designs will be rooted in user needs, behavior, and real problems.\nSketch fast, prototype faster expect whiteboard jams (physical or FigJam), lo-fid wireframes, and hi-fid UIs in Figma.\nDesign clear, intuitive interfaces for complex problems you ll learn how to simplify the messy, not decorate the obvious.\nWork closely with product managers, engineers, data scientists, and business folks your work will be part of real products, not parked in a folder.\nDocument your thought process, design decisions, and learnings because clarity is half the craft.\nBe part of feedback loops, user research, usability testing, and design reviews this is where your ideas evolve and sharpen.\n\nYou re a Great Fit If\n\nYou re in your 3rd or 4th year of design school (or a recent graduate) and itching to solve real problems through design.\nYou live in Figma, but care more about user needs than just pixel-perfect screens.\nYou ask questions, stay curious, and don t settle for surface-level thinking.\nYou ve got a portfolio that shows your process not just pretty UIs, but your reasoning, mistakes, and how you learned from them.\nYou re a great communicator you explain your ideas clearly and listen just as well.\nYou take initiative, don t wait for permission, and love collaborating in messy, ambiguous spaces.\n\nBonus Points If\n\nYou ve worked on data-heavy or workflow-based tools.\nYou ve contributed to or maintained a design system.\nYou re curious about geospatial, climate, or agri-tech products and designing for emerging markets excites you.\nYou write things down ideas, assumptions, feedback, or just what you learned this week.\n\nWhy Intern at SatSure\n\nWe work on problems that don t have ready-made solutions. From helping farmers get better advice, to making satellite data usable for businesses we design with impact, not just interfaces.\nYou ll be mentored, challenged, supported, and most importantly trusted. You won t just shadow someone. You ll be designing, learning by doing, and growing every week.",Industry Type: Defence & Aerospace,"Department: UX, Design & Architecture","Employment Type: Full Time, Permanent","['Process design', 'User research', 'Prototype', 'PDF', 'Intern', 'Infrastructure', 'Workflow', 'Product design', 'Internship', 'Usability testing']",2025-06-10 15:17:57
Datascientist with GenAI-2,Wipro,4 - 7 years,Not Disclosed,['Bengaluru'],"Wipro Limited (NYSEWIT, BSE507685, NSEWIPRO) is a leading technology services and consulting company focused on building innovative solutions that address clients’ most complex digital transformation needs. Leveraging our holistic portfolio of capabilities in consulting, design, engineering, and operations, we help clients realize their boldest ambitions and build future-ready, sustainable businesses. With over 230,000 employees and business partners across 65 countries, we deliver on the promise of helping our customers, colleagues, and communities thrive in an ever-changing world. For additional information, visit us at www.wipro.com.\n About The Role  \n\nDo\n\nResearch, design, develop, and modify computer vision and machine learning. algorithms and models, leveraging experience with technologies such as Caffe, Torch, or TensorFlow.\nShape product strategy for highly contextualized applied ML/AI solutions by engaging with customers, solution teams, discovery workshops and prototyping initiatives.\nHelp build a high-impact ML/AI team by supporting recruitment, training and development of team members.\nServe as evangelist by engaging in the broader ML/AI community through research, speaking/teaching, formal collaborations and/or other channels. Knowledge & Abilities- Designing integrations of and tuning machine learning & computer vision algorithms - Research and prototype techniques and algorithms for object detection and recognition - Convolutional neural networks (CNN) for performing image classification and object detection.\nFamiliarity with Embedded Vision Processing systems - Open source tools & platforms - Statistical Modeling, Data Extraction, Analysis, - Construct, train, evaluate and tune neural networks\n\nMandatory\n\nSkills:\nOne or more of the following:\nJava, C++, Python Deep Learning frameworks such as Caffe OR Torch OR TensorFlow, and image/video vision library like OpenCV, Clarafai, Google Cloud Vision etc Supervised & Unsupervised Learning Developed feature learning, text mining, and prediction models (e.g., deep learning, collaborative filtering, SVM, and random forest) on big data computation platform (Hadoop, Spark, HIVE, and Tableau) *One or more of the followingTableau, Hadoop, Spark, HBase, Kafka Experience- 2-5 years of work or educational experience in Machine Learning or Artificial Intelligence - Creation and application of Machine Learning algorithms to a variety of real-world problems with large datasets.\nBuilding scalable machine learning systems and data-driven products working with cross functional teams - Working w/ cloud services like AWS, Microsoft, IBM, and Google Cloud - Working w/ one or more of the followingNatural Language Processing, text understanding, classification, pattern recognition, recommendation systems, targeting systems, ranking systems or similar Nice to Have- Contribution to research communities and/or efforts, including publishing papers at conferences such as NIPS, ICML, ACL, CVPR, etc.\n\nEducation:\nBA/BS (advanced degree preferable) in Computer Science, Engineering or related technical field or equivalent practical experience Wipro is an Equal Employment Opportunity employer and makes all employment and employment-related decisions without regard to a person's race, sex, national origin, ancestry, disability, sexual orientation, or any other status protected by applicable law Product and Services Sales Manager\n\n ? \n\n ? \n\n ? \n\n ? \nReinvent your world. We are building a modern Wipro. We are an end-to-end digital transformation partner with the boldest ambitions. To realize them, we need people inspired by reinvention. Of yourself, your career, and your skills. We want to see the constant evolution of our business and our industry. It has always been in our DNA - as the world around us changes, so do we. Join a business powered by purpose and a place that empowers you to design your own reinvention. Come to Wipro. Realize your ambitions. Applications from people with disabilities are explicitly welcome.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['artificial intelligence', 'tensorflow', 'spark', 'python', 'machine learning', 'hive', 'c++', 'neural networks', 'deep learning', 'data extraction', 'java', 'gcp', 'statistical modeling', 'text mining', 'hadoop', 'big data', 'hbase', 'ml', 'cnn', 'caffe', 'google', 'tableau', 'kafka', 'computer vision', 'aws', 'opencv']",2025-06-10 15:17:59
Associate Director - AIML,HARMAN,12 - 15 years,Not Disclosed,['Bengaluru'],"As a technology leader that is rapidly on the move, HARMAN is filled with people who are focused on making life better. Innovation, inclusivity and teamwork are a part of our DNA. When you add that to the challenges we take on and solve together, you'll discover that at HARMAN you can grow, make a difference and be proud of the work you do everyday.\n  Introduction: - Harman Tech Solutions (HTS)\nwe're a global, multi-disciplinary team that s putting the innovative power of technology to work and transforming tomorrow. At HARMAN HTS, you solve challenges by creating innovative solutions.",,,,"['IT services', 'Coding', 'Agile', 'Healthcare', 'microsoft', 'MATLAB', 'Information technology', 'Automotive', 'SQL', 'Python']",2025-06-10 15:18:02
Product Design Internship,Satsure Analytics,4 months duration,Unpaid,['Bengaluru'],"About SatSure:\n\nSatSure is a deep tech, decision Intelligence company that works primarily at the nexus of agriculture, infrastructure, and climate action creating an impact for the other millions, focusing on the developing world. We want to make insights from earth observation data accessible to all.\n\nJoin us to be at the forefront of building a deep tech company in India that solves problems worldwide.\n\nWhat You ll Do\n\nDive deep into the why behind every screen your designs will be rooted in user needs, behavior, and real problems.\nSketch fast, prototype faster expect whiteboard jams (physical or FigJam), lo-fid wireframes, and hi-fid UIs in Figma.\nDesign clear, intuitive interfaces for complex problems you ll learn how to simplify the messy, not decorate the obvious.\nWork closely with product managers, engineers, data scientists, and business folks your work will be part of real products, not parked in a folder.\nDocument your thought process, design decisions, and learnings because clarity is half the craft.\nBe part of feedback loops, user research, usability testing, and design reviews this is where your ideas evolve and sharpen.\n\nYou re a Great Fit If\n\nYou re in your 3rd or 4th year of design school (or a recent graduate) and itching to solve real problems through design.\nYou live in Figma, but care more about user needs than just pixel-perfect screens.\nYou ask questions, stay curious, and don t settle for surface-level thinking.\nYou ve got a portfolio that shows your process not just pretty UIs, but your reasoning, mistakes, and how you learned from them.\nYou re a great communicator you explain your ideas clearly and listen just as well.\nYou take initiative, don t wait for permission, and love collaborating in messy, ambiguous spaces.\n\nBonus Points If\n\nYou ve worked on data-heavy or workflow-based tools.\nYou ve contributed to or maintained a design system.\nYou re curious about geospatial, climate, or agri-tech products and designing for emerging markets excites you.\nYou write things down ideas, assumptions, feedback, or just what you learned this week.\n\nWhy Intern at SatSure?\n\nWe work on problems that don t have ready-made solutions. From helping farmers get better advice, to making satellite data usable for businesses we design with impact, not just interfaces.\nYou ll be mentored, challenged, supported, and most importantly trusted. You won t just shadow someone. You ll be designing, learning by doing, and growing every week.\n\nSound like you?\n\nSend us your resume and portfolio (Notion/PDF/Figma whatever shows your process best).\n\nPlease fill out this google form to tell us more about yourself: https: / / forms.gle / W4SbTnYQem6Y9GAK6\n\nWe re excited to meet designers who think deeply, design with care, and are obsessed with making things better.\n\nLocation: Bangalore\nDuration-3-4months",Industry Type: Defence & Aerospace,"Department: UX, Design & Architecture","Employment Type: Full Time, Permanent","['Process design', 'User research', 'Prototype', 'PDF', 'Intern', 'Infrastructure', 'Workflow', 'Product design', 'Internship', 'Usability testing']",2025-06-10 15:18:04
IT Developer (Full Stack Developer),Medtronic,6 - 10 years,Not Disclosed,['Pune'],"At Medtronic you can begin a life-long career of exploration and innovation, while helping champion healthcare access and equity for all. Youll lead with purpose, breaking down barriers to innovation in a more connected, compassionate world.\n\nA Day in the Life\n\nOur Global Diabetes Capability Center in Pune is expanding to serve more people living with diabetes globally. Our state-of-the-art facility is dedicated to transforming diabetes management through innovative solutions and technologies that reduce the burden of living with diabetes.\nWe are seeking a highly skilled Full-Stack Developer with a strong focus on front-end UI/UX development using modern JavaScript frameworks. In this role, you will collaborate with cross-functional teams to build intuitive, responsive, and interactive web applications that drive our data-driven products.",,,,"['React.Js', 'AWS', 'Reacts Js', 'Javascript', 'React JS developer', 'React Development']",2025-06-10 15:18:07
Lead Data Scientist - Media Mix Modelling (MMM),Blend360 India,7 - 10 years,Not Disclosed,['Hyderabad'],"We are seeking a highly skilled Data Scientist/Analyst with expertise in Media Mix Modeling (MMM) to join our dynamic analytics team. The ideal candidate will play a critical role in providing actionable insights and optimizing marketing spend across various channels by leveraging statistical models and data-driven techniques.\nKey Responsibilities:\nDevelop and implement Media Mix Models to optimize marketing spend across different channels (e.g., TV, digital, radio, print, etc.).\nAnalyze historical data to understand the impact of marketing efforts and determine the effectiveness of different media channels.\nCollaborate with marketing and business teams to translate business objectives into quantitative analyses and actionable insights.\nBuild predictive models to forecast the impact of future marketing activities and recommend budget allocation.\nPresent and communicate complex findings in a clear, concise, and actionable manner to both technical and non-technical stakeholders.\nPerform deep-dive analyses of marketing campaigns and customer data to identify trends, opportunities, and areas for improvement.\nEnsure data integrity, accuracy, and consistency in all analyses and models.\nStay up-to-date with the latest trends and advancements in media mix modeling, marketing analytics, and data science.\nCollaborate with cross-functional teams including Data Engineering, Marketing, and Business Intelligence to ensure seamless data flow and integration.\nCreate and maintain documentation for all models, methodologies, and analysis processes.\n\n\nBachelor s or Master s degree in Data Science, Statistics, Economics, Mathematics, or a related field.\nProven experience (6+ years) working in Media Mix Modeling (MMM) and/or marketing analytics.\nStrong proficiency in stati",Industry Type: Industrial Automation,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data analysis', 'Analytical', 'Social media', 'Consulting', 'data integrity', 'Regression analysis', 'Business intelligence', 'CRM', 'SQL', 'Python']",2025-06-10 15:18:09
Lead Data Scientist,Camp Systems Internationals Inc,5 - 10 years,Not Disclosed,['Hyderabad'],"Job Description: We are seeking a talented and experienced Data Scientist to join our dynamic team. The ideal candidate will have a strong background in data analysis, machine learning, statistical modeling, and artificial intelligence. Experience with Natural Language Processing (NLP) is desirable. Experience delivering products that incorporate AI/ML, familiarity with Cloud Services such as AWS highly desirable.\nRequired Skills/Qualifications :\n- 3-12 years of experience in AI/ML related work\n- Extensive experience in Python",,,,"['Al', 'Machine Learning', 'Gen AI', 'Linear Regression', 'Python', 'Tensorflow', 'CNN', 'Azure', 'Logistic Regression', 'Lstm', 'LLMs', 'Natural Language Processing', 'Numpy', 'Scikit-Learn', 'Pytorch', 'RNN', 'MLOps', 'Kedro', 'Cloud', 'Rag', 'AWS']",2025-06-10 15:18:11
Opportunity | Gen AI Architect | Tavant India,Tavant Technologies,12 - 22 years,Not Disclosed,"['Noida', 'Hyderabad/Secunderabad', 'Bangalore/Bengaluru']","Dear candidate,\n\nWe found your profile suitable for our current opening, please go through the below JD for better understanding of the role,\n\nJob Description :\nRole : Lead Data Scientist / TA\nExp : 12 - 20 years\nMode of work : Hybrid Model (3daya WFO)\nWork Location : Hyderabad/Bangalore/Noida/Pune/Kolkata\nJob Description:\nWe are looking for a highly experienced Lead Data Scientist / ML Engineer to drive innovation in Generative AI (GenAI), search technologies, recommendation engines, and agentic AI systems. This role demands both technical excellence and strategic vision, combining deep hands-on expertise in AI/ML with proven leadership in building and scaling data science teams.\nKey Responsibilities:\n1. Technical & Strategic Leadership\nDefine and execute AI strategies aligned with business goals.\nLead and mentor a high-performing team of data scientists and ML engineers.\nDrive architectural decisions and innovation in AI-powered products.\n2. Generative & Agentic AI\nBuild and deploy GenAI models for text generation and content automation.\nDevelop agentic AI systems with autonomous task planning and decision-making capabilities.\nApply latest deep learning advancements (transformers, diffusion models, etc.) to real-world use cases.\n3. Search & Recommendation Systems\nDesign scalable, high-performance search and recommendation platforms.\nImplement ML models for ranking, personalization, and relevance optimization.\nLeverage real-time feedback loops and experimentation (A/B testing).\n4. Large Language Models (LLMs)\nFine-tune and operationalize LLMs (e.g., GPT, BERT) for NLP tasks.\nWork closely with product teams to integrate LLMs into applications.\nEstablish best practices for LLMOps, including prompt engineering and monitoring.\n5. Machine Learning Engineering & MLOps\nBuild robust ML pipelines for model development, deployment, and monitoring.\nImplement MLOps using CI/CD, Docker, Kubernetes, and cloud platforms (AWS/GCP/Azure).\nEnsure responsible AI practices and data governance compliance.\n6. Collaboration & Stakeholder Engagement\nTranslate business needs into ML solutions in partnership with product and engineering teams.\nCommunicate insights and recommendations to stakeholders across levels.\nPromote a culture of data-driven decision-making.\n7. Research & Innovation\nStay updated on the latest in AI/ML and contribute to R&D efforts.\nEncourage experimentation, POCs, and technical publications.\nRequired Qualifications:\nMasters or PhD in Computer Science, AI, Data Science, or related field.\n12+ years of experience in data science/ML, with 5+ years in leadership roles.\nStrong experience in:\nGenerative AI (e.g., GANs, VAEs, transformer models)\nSearch & Recommendation Systems\nLLMs & NLP using frameworks like Hugging Face, TensorFlow, PyTorch\nAgentic AI and autonomous systems\nML Engineering & MLOps\nLeadership skills with proven success in managing cross-functional teams.\nStrong communication and stakeholder management abilities.\nPreferred Skills:\nExperience with big data tools (Spark, Hadoop, Kafka).\nDevOps tools: Jenkins, GitLab CI, Terraform.\nExposure to edge computing and distributed systems.\nContributions to open-source AI/ML projects or research publications.\n\nPlease check below link for organisation details https://www.tavant.com/\nIf interested , please drop your resume to dasari.gowri@tavant.com\nRegards\nDasari Krishna Gowri\nAssociate Manager - HR\nwww.tavant.com",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Gen AI', 'Search Engine', 'LLM', 'Recommendation', 'Natural Language Processing', 'Machine Learning']",2025-06-10 15:18:13
Job opening For Manager Data Science @ GlobalData-Bengaluru,Globaldata,8 - 12 years,Not Disclosed,['Bengaluru( Koramangala )'],"Hello,\n\nGreetings from GlobalData!!!\n\nJob opening for Data Science Manager role @ GD-Bengaluru\nJob Criteria :-\nQualification: B.Tech/BE/MCA/M.Tech/M.Sc-(Computers)",,,,"['Natural Language Processing', 'Machine Learning', 'Generative Ai Tools', 'Large Language Model', 'Python', 'Tensorflow', 'Artificial Intelligence', 'Statistics', 'Deep Learning', 'Data Science', 'Agentic AI', 'Pytorch', 'Pandas']",2025-06-10 15:18:16
Lead Consultant / Principal Consultant - Informatica MDM SaaS,Genpact,5 - 10 years,11-21 Lacs P.A.,"['Kolkata', 'Hyderabad', 'Bengaluru']","Ready to build the future with AI?\nAt Genpact, we dont just keep up with technology—we set the pace. AI and digital innovation are redefining industries, and we’re leading the charge. Genpact’s AI Gigafactory, our industry-first accelerator, is an example of how we’re scaling advanced technology solutions to help global enterprises work smarter, grow faster, and transform at scale. From large-scale models to agentic AI, our breakthrough solutions tackle companies’ most complex challenges.\nIf you thrive in a fast-moving, innovation-driven environment, love building and deploying cutting-edge AI solutions, and want to push the boundaries of what’s possible, this is your moment.",,,,"['Informatica MDM SaaS', 'IDMC', 'informatica MDM']",2025-06-10 15:18:18
Senior Data Engineer,Impetus Technologies,5 - 10 years,Not Disclosed,['United Arab Emirates'],"The Opportunity:We are seeking a highly motivated and technically strong Module Lead Software Engineer with significant expertise in Python, PySpark, and Palantir Foundry. In this role, you will be responsible for the end-to-end technical ownership, design, and delivery of a specific module or component within our enterprise data platform. You will combine hands-on development with technical leadership, ensuring the highest standards of code quality, performance, and reliability.\n\nKey Responsibilities:\n\nModule Technical Leadership & Ownership: Take full technical ownership of a specific module or component within the data platform on Palantir Foundry. This includes defining its technical roadmap, architecture, design patterns, and ensuring its integration into the broader data ecosystem.\nHands-on Development and Complex Problem Solving: Act as a lead individual contributor, developing sophisticated data pipelines, transformations, and applications using Python and PySpark within Palantir Foundry's various tools (e.g., Code Workbook, Pipeline Builder). Tackle the most challenging technical problems and implement core functionalities for the module.\nQuality Assurance and Best Practices Advocacy: Drive and enforce high standards for code quality, test coverage, documentation, and operational excellence within your module. Conduct rigorous code reviews, provide constructive feedback, and mentor engineers within your immediate scope to elevate their technical skills.\nCross-Functional Collaboration and Module Integration: Collaborate extensively with other module leads, architects, data scientists, and business stakeholders to ensure seamless integration of your module's deliverables. Proactively identify and manage technical dependencies and ensure the module aligns with overall project goals and architectural vision.\n• Performance Optimization and Troubleshooting: Continuously monitor and optimize the performance of your module's data pipelines and applications. Efficiently troubleshoot and resolve complex technical issues, data quality concerns, and system failures specific to your module.\n\nRequired Qualifications:\n\nExperience: 6-8 years of progressive experience in software development with a strong focus on data engineering.\nPython Proficiency: Expert-level proficiency in Python, including advanced programming concepts, data structures, and performance optimization techniques.\nPySpark Expertise: Strong experience with PySpark for large-scale distributed data processing, transformations, and analytics.\nPalantir Foundry: Proven, hands-on experience designing, developing, and deploying solutions within Palantir Foundry is essential.\nDeep familiarity with Foundry's data integration capabilities, Code Workbook, Pipeline Builder, Data Health checks, and Ontology modeling.\nExperience with Foundry's approach to data governance and versioning.\nSQL Skills: Excellent SQL skills for complex data querying, manipulation, and optimization.\nData Warehousing/Lakes: Solid understanding of data warehousing concepts, data lake architectures, and ETL/ELT principles.\nCloud Platforms: Experience with at least one major cloud platform (AWS, Azure, GCP), particularly with data-related services.\nVersion Control: Strong experience with Git and collaborative development workflows.\n\nPreferred Qualifications (Nice-to-Have):\n\nExperience mentoring or leading small technical teams/pods.\nFamiliarity with containerization technologies (Docker, Kubernetes).\nExperience with streaming data technologies (e.g., Kafka, Kinesis).\nUnderstanding of CI/CD pipelines for data solutions.\nKnowledge of data governance, data quality, and metadata management best practices.\nExperience in [specific industry, e.g., Financial Services, Manufacturing, Healthcare].",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'Data Engineering', 'Palantir Foundry', 'Python', 'Spark', 'Data Warehousing', 'SQL']",2025-06-10 15:18:21
Senior Backend Engineer,Welocalize,5 - 8 years,Not Disclosed,['Noida'],"Role Summary\nThe Senior Backend Engineer (Python & Node.js) is responsible for leading the design, implementation, and optimization of scalable machine learning infrastructure. This role ensures that AI/ML models are efficiently deployed, managed, and monitored in production environments while providing mentorship and technical leadership to junior engineers.\n\nYou can apply using the link below.",,,,"['Node.Js', 'Python', 'Cloud Development', 'GCP', 'Azure Cloud', 'Microservice Based Architecture', 'AWS']",2025-06-10 15:18:23
Senior GenAI Data Engineer,"NTT DATA, Inc.",2 - 5 years,Not Disclosed,"['New Delhi', 'Chennai', 'Bengaluru']","Your day at NTT DATA\nSenior GenAI Data Engineer\nWe are seeking an experienced Senior Data Engineer to join our team in delivering cutting-edge Generative AI (GenAI) solutions to clients. The successful candidate will be responsible for designing, developing, and deploying data pipelines and architectures that support the training, fine-tuning, and deployment of LLMs for various industries. This role requires strong technical expertise in data engineering, problem-solving skills, and the ability to work effectively with clients and internal teams.\nWhat you'll be doing\nKey Responsibilities:\nDesign, develop, and manage data pipelines and architectures to support GenAI model training, fine-tuning, and deployment\nData Ingestion and Integration: Develop data ingestion frameworks to collect data from various sources, transform, and integrate it into a unified data platform for GenAI model training and deployment.\nGenAI Model Integration: Collaborate with data scientists to integrate GenAI models into production-ready applications, ensuring seamless model deployment, monitoring, and maintenance.\nCloud Infrastructure Management: Design, implement, and manage cloud-based data infrastructure (e.g., AWS, GCP, Azure) to support large-scale GenAI workloads, ensuring cost-effectiveness, security, and compliance.\nWrite scalable, readable, and maintainable code using object-oriented programming concepts in languages like Python, and utilize libraries like Hugging Face Transformers, PyTorch, or TensorFlow\nPerformance Optimization: Optimize data pipelines, GenAI model performance, and infrastructure for scalability, efficiency, and cost-effectiveness.\nData Security and Compliance: Ensure data security, privacy, and compliance with regulatory requirements (e.g., GDPR, HIPAA) across data pipelines and GenAI applications.\nClient Collaboration: Collaborate with clients to understand their GenAI needs, design solutions, and deliver high-quality data engineering services.\nInnovation and R&D: Stay up to date with the latest GenAI trends, technologies, and innovations, applying research and development skills to improve data engineering services.\nKnowledge Sharing: Share knowledge, best practices, and expertise with team members, contributing to the growth and development of the team.\nRequirements:\nBachelors degree in computer science, Engineering, or related fields (Master's recommended)\nExperience with vector databases (e.g., Pinecone, Weaviate, Faiss, Annoy) for efficient similarity search and storage of dense vectors in GenAI applications\n5+ years of experience in data engineering, with a strong emphasis on cloud environments (AWS, GCP, Azure, or Cloud Native platforms)\nProficiency in programming languages like SQL, Python, and PySpark\nStrong data architecture, data modeling, and data governance skills\nExperience with Big Data Platforms (Hadoop, Databricks, Hive, Kafka, Apache Iceberg), Data Warehouses (Teradata, Snowflake, BigQuery), and lakehouses (Delta Lake, Apache Hudi)\nKnowledge of DevOps practices, including Git workflows and CI/CD pipelines (Azure DevOps, Jenkins, GitHub Actions)\nExperience with GenAI frameworks and tools (e.g., TensorFlow, PyTorch, Keras)\nNice to have:\nExperience with containerization and orchestration tools like Docker and Kubernetes\nIntegrate vector databases and implement similarity search techniques, with a focus on GraphRAG is a plus\nFamiliarity with API gateway and service mesh architectures\nExperience with low latency/streaming, batch, and micro-batch processing\nFamiliarity with Linux-based operating systems and REST APIs\nLocation: Delhi or Bangalore\nWorkplace type:\nHybrid Working",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['GenAI', 'hive', 'continuous integration', 'kubernetes', 'ci/cd', 'pyspark', 'data architecture', 'sql', 'docker', 'tensorflow', 'git', 'data modeling', 'gcp', 'devops', 'linux', 'jenkins', 'pytorch', 'keras', 'hadoop', 'bigquery', 'python', 'microsoft azure', 'data engineering', 'data bricks', 'data governance', 'aws']",2025-06-10 15:18:25
Application Architect-Customer & Experience Transformation,IBM,6 - 11 years,Not Disclosed,['Bengaluru'],"Involved in all aspects of a project and has dual ability to maintain the broad vision required fordesign development of a project, including strategic thinking and leadership. Oversees the intricatedetails of a project from inception through launch.\nHas a strong understanding of the Client'sbusiness, industry, economic model, organizational trends and customer needs in order to lead withrelevant digital marketing solutions.\nPartner across teams to recommend and determine appropriate project execution models (waterfalland/or agile practices) and determine project engagement type (Retainer, Fixed Fee, Time andMaterials) as part of solutioning. Establishes and maintains a Center of excellence for every projectwith Client, and/or Subject Matter Experts (SMEs). Serves as a central point of contact for projectestimates, utilizing department leads and SMEs to determine estimates for their team's activities.\nFacilitates the creation of accurate project plans with clearly defined milestones, deliverables andtasks. Work with department SMEs to determine department level deliverables and create resourceallocation/staffing plans for the lifecycle of the project. Experience with IBM Design Thinking, Agile,DevOps, Scrum, SAFe, LeSS and SDLC.\nDesign major aspects of the architecture of an application including components, UI, middlewareand databases.\nProvide technical leadership to the application development team.\nProficient in performing design and code reviews.\nEnsure application design standards are maintained.\nCreate and maintain documentations surrounding software architecture, application design\nprocesses, component integrations, testing guidelines etc.\nResponsible for training developers and refining the technical expertise.\nProblem solving skills to effectively identify and develop architectural systems that meet the needsof clients\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nFor a given scope area, evaluate architectural models and perform/drive in-depth analysis ofsystems, data flows processes, and KPIs/metrics about the current state\nDevelop understanding of business processes, data flows, strategy, and long-term thinking to comeup with an end-state architecture for large and complex systems\nFull-stack software Architecture Expertise, Designing and Developing full stack modules andcomponents for web applications (frontend and backend services)\nWorking experience on MEAN (Mongo, Express, Angular, Node), MERN (Mongo, Express, React, Node)stacks\nConsumer Web Development Experience for High-Traffic, Public Facing web applications\n\n\nPreferred technical and professional experience\nExperience in working with AB Test frameworks such as Optimizely, Experience in using front end monitoring tools to troubleshoot errors and recognize performance bottlenecks\nExperience in designing UX for simplifying user experience and dashboards for viewing high volume of information\nExpertise in hosting and configuring Data Annotation tools, defining meta data for media types such as Images, Audio, Video, model-based data capture, Preferred Experience in Playing liaison role with ML Engineers, Data Scientists, Data Analysts to translate business requirements to conceptual designs",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['configuring', 'hosting', 'data annotation', 'code review', 'data flow', 'css', 'technical leadership', 'application design', 'mern', 'bootstrap', 'javascript', 'application development', 'react.js', 'node', 'angular', 'application architectures', 'mean', 'devops', 'scrum', 'le', 'html', 'agile', 'mongodb', 'sdlc']",2025-06-10 15:18:27
Analyst,Wipro,0 - 4 years,Not Disclosed,['Gurugram'],"About The Role  \n\nJob titleAnalyst – Investment Management – Structured Finance\n\nBandB1 - Analyst\nExperience RangeMinimum 1 year\nShift Timings-2:00 PM - 11:30 PM IST\n\nJob Summary\nWe are seeking a highly motivated Investment Management Analyst to join our Asset Based Finance (ABF) team within the Structured Finance Group. As an integral part of our team, you will gain exposure to a diverse portfolio of Asset-Backed Securities (ABS) investments and play a pivotal role in underwriting, monitoring, and managing these deals for our institutional clients.\n\nResponsibilities:\nDeal ManagementCollaborate closely with ABF Deal Analysts to assist in investment management and monitoring activities for structured finance deals.\nIC MemosSupport the deal team in updating Investment Committee approval memos, ensuring a comprehensive understanding of each deal's intricacies.\nTerm sheetsAnalyze closing documents such as credit agreements, indentures and note purchase agreements and set up processes for analyzing and monitoring the deal post-closing.\nDue DiligencePerform pre-deal due diligence and stratify the collateral pool using Python to assess risk and investment potential.\nSurveillance and ReportingUpdate surveillance data and create one-pagers for presentations to senior management for both Public and Private deals.\nCashflow ModelingDevelop and update cashflow models for structured finance deals. Monitor key metrics to assess risk and expected returns. Conduct stress case scenarios and analyze their impact on repayments.\nCollateral AnalysisCreate and update asset-level one-pagers for collateral analysis, assisting in underwriting and post-deal monitoring.\nAsset ValuationTrack and review revaluation events for underlying assets, ensuring accurate pricing of collateral.\nSector-Level AnalysisUpdate monthly sector-level presentations and pro-actively highlight potential issues to senior management.\nCovenant MonitoringContinuously monitor covenants and key performance indicators (KPIs) at the deal level, thereby assist the Business Analytics team to run portfolio-level analysis.\nAd Hoc ProjectsUndertake multiple ad hoc projects as requested by senior management to assess the impact of macro events.\nClient request/ CompliancesEnsure compliance with investor requirements from an investment management perspective.\nData and ToolsLeverage advanced tools such as Intex and Python for in-depth analysis. Utilize Tableau, Street Diligence and Sigma for enhanced data visualization.\n\nSkills Required:\nMBA in Finance, CFA, or CA qualification.\nExperience in analyzing Asset Backed Finance deal structures and collateral is a plus.\nStrong analytical and quantitative skills.\nProficiency in Microsoft Office tools (MS Excel, MS PowerPoint, and MS Word).\nAbility to summarize complex information succinctly and efficiently.\nExcellent written and verbal communication skills.\nAbility to manage multiple projects in a fast-paced environment.\nDetail-oriented with a commitment to accuracy and precision.\nAbility to work independently and collaboratively while demonstrating high sense of ownership and accountability.\nThorough understanding of basic financial concepts and the ability to critically implement them.\n\n ? \n\n \n\nDeliver\n NoPerformance ParameterMeasure1ProcessNo. of cases resolved per day, compliance to process and quality standards, meeting process level SLAs, Pulse score, Customer feedback2Self- ManagementProductivity, efficiency, absenteeism, Training Hours, No of technical training completed",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'tableau', 'ms office tools', 'quantitative', 'finance', 'analysts', 'risk management', 'due diligence', 'mortgage advisors', 'collaterals', 'business analytics', 'accounting', 'process compliance', 'underwriting', 'investment management', 'data visualization', 'asset', 'pricing', 'cfa']",2025-06-10 15:18:30
Artificial Intelligence Developer,Infosys,5 - 10 years,Not Disclosed,"['Hyderabad', 'Bengaluru', 'Thiruvananthapuram']","Role & responsibilities\nArchitect and implement AI/ML solutions tailored to client-specific business problems using Gen AI (e.g., LLMs like GPT, Gemini, Mistral) and traditional ML models (e.g., scikit-learn, XGBoost).\nCollaborate with client stakeholders to understand requirements, define problem statements, and translate them into scalable AI/ML solutions.\nPresent demos and proof-of-concepts (POCs) to clients, showcasing the value of AI/ML in real-world scenarios\nMentor and guide a cross-functional team of data scientists, ML engineers, and developers\nDrive best practices in model development, deployment, and monitoring\nStay abreast of the latest advancements in Gen AI and ML, and evaluate their applicability to client use cases\nContribute to internal knowledge bases and reusable solution accelerators\nPreferred candidate profile\nStrong programming skills in Python and experience with ML libraries (e.g., TensorFlow, PyTorch, scikit-learn).\nHands-on experience with Gen AI platforms and LLMs (e.g., OpenAI, Gemini, LLaMA, Mistral).\nProven track record of deploying ML models in production environments (cloud/on-prem).\nExperience with API development, data pipelines, and dashboarding tools like Streamlit.\nFamiliarity with DevOps and MLOps practices for model lifecycle management.\nExcellent communication and stakeholder management skills.\nTools:\nLLM, SLM, Vector DB, Graph DB, Airflow, MLFlow, MLOps tools, NLP, KG, ML models (Regression, Clustering, Classification etc), LangChain, LangGraph, AutoGen",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Agentic Ai', 'Knowledge Graphs', 'Generative Ai', 'Machine Learning', 'Deep Learning', 'RAG', 'Multimodal models', 'Computer Vision']",2025-06-10 15:18:33
Analyst - L3,Wipro,0 - 4 years,Not Disclosed,['Bengaluru'],"Wipro Limited (NYSEWIT, BSE507685, NSEWIPRO) is a leading technology services and consulting company focused on building innovative solutions that address clients’ most complex digital transformation needs. Leveraging our holistic portfolio of capabilities in consulting, design, engineering, and operations, we help clients realize their boldest ambitions and build future-ready, sustainable businesses. With over 230,000 employees and business partners across 65 countries, we deliver on the promise of helping our customers, colleagues, and communities thrive in an ever-changing world. For additional information, visit us at www.wipro.com.\n About The Role  \n\n ? \n\n ? \n\n ? \n\n ? \nReinvent your world. We are building a modern Wipro. We are an end-to-end digital transformation partner with the boldest ambitions. To realize them, we need people inspired by reinvention. Of yourself, your career, and your skills. We want to see the constant evolution of our business and our industry. It has always been in our DNA - as the world around us changes, so do we. Join a business powered by purpose and a place that empowers you to design your own reinvention. Come to Wipro. Realize your ambitions. Applications from people with disabilities are explicitly welcome.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['project management', 'program management', 'business analysis', 'agile', 'digital transformation', 'management consulting', 'financial analysis', 'business development', 'change management', 'product management', 'delivery management', 'stakeholder management', 'business consulting']",2025-06-10 15:18:35
Lab45 - AI Scientist,Wipro,2 - 7 years,Not Disclosed,['Bengaluru'],"About The Role  \n\nJob TitleAI Scientist\n\nPosition Overview:\n\nWe are seeking a talented and experienced Core AI Algorithm Developer to join our Lab45 AI Platform Team, Wipro. We are looking for candidates with 4 to 10 years of hands-on experience in developing cutting-edge AI algorithms, such as in Generative AI, LLM, Deep Learning, Unsupervised AI, etc. along with expertise in Python, TensorFlow, PyTorch, PySpark, distributed computing, Statistics, and cloud technologies. Candidate should have strong foundation in AI and good coding skills.\n\nKey Responsibilities:\nDevelop and implement state-of-the-art AI algorithms and models to solve complex problems in diverse domains.\nCollaborate with cross-functional teams to understand business requirements and translate them into scalable AI production-grade solutions.\nWork with large datasets to extract insights, optimize algorithms, and enhance model performance.\nContribute to the creation of intellectual property (IP) through patents, research papers, and innovative solutions.\nStay abreast of the latest advancements in AI research and technologies and apply them to enhance our AI offerings.\nCollaborate with cross-functional teams to understand business requirements, gather feedback, and iterate on AI solutions.\nStrong communication and interpersonal skills, with the ability to effectively collaborate with cross-functional teams.\n\n\nQualifications:\nMaster's, or Ph.D. degree (preferred) in Computer Science, Artificial Intelligence, Machine Learning, or related field.\n4 to 10 years of proven experience in developing cutting-edge AI algorithms and solutions.\nStrong proficiency in Python programming and familiarity with TensorFlow, PyTorch, PySpark, etc.\nExperience with distributed computing and cloud platforms (e.g., Azure, AWS, GCP).\nDemonstrated ability to work with large datasets and optimize algorithms for scalability and efficiency.\nExcellent problem-solving skills and a strong understanding of AI concepts and techniques.\nProven track record of delivering high-quality, innovative solutions and contributing to IP creation (e.g., patents, research papers).\nStrong communication and collaboration skills, with the ability to work effectively in a team environment.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'pyspark', 'distributed computing', 'tensorflow', 'pytorch', 'c++', 'c', 'microsoft azure', 'machine learning', 'cloud technologies', 'artificial intelligence', 'docker', 'deep learning', 'r', 'java', 'gcp', 'cutting', 'data structures', 'hadoop', 'aws', 'big data', 'cloud computing', 'statistics']",2025-06-10 15:18:37
Gen AI Lead,Infosys,5 - 9 years,Not Disclosed,['Hubli'],"Job Overview:\nWe are seeking an accomplished Generative AI candidate to spearhead the development, implementation, and optimization of Generative AI solutions. Primary Skill Set:Generative AI Expertise: Good understanding of various Generative AI techniques, including GANs, VAEs, and other relevant architectures. Proven experience in applying these techniques to real-world problems for tasks such as image and text generation. Conversant with Gen AI development tools like Prompt engineering, Langchain, Semantic Kernels, Function calling. Exposure to both API based and opens source LLMs based solution design.\nTechnical Proficiency:\nMachine learning algorithms: Linear regression, logistic regression, decision trees, random forests, support vector machines, neural networks\nData science tools: NumPy, SciPy, Pandas, Matplotlib, TensorFlow, Keras\nCloud computing platforms: AWS, Azure, GCP\nNatural language processing (NLP): Transformer models, attention mechanisms, word embeddings\nComputer vision: Convolutional neural networks, recurrent neural networks, object detection\nRobotics: Reinforcement learning, motion planning, control systems\nData ethics: Bias in machine learning, fairness in algorithms\nResponsible AI: Should have proficient knowledge in Responsible AI and Data Privacy principles to ensure ethical data handling, transparency, and accountability in all stages of AI development. Must demonstrate a commitment to upholding privacy standards, mitigating bias, and fostering trust within data-driven initiatives.\nLLM Pipeline Creation: Strong experience in designing data pipelines, including data preprocessing, feature extraction, and model integration. Familiarity with best practices for creating efficient and scalable pipelines.\nSecondary Skill Set:Software Development: Proficiency in software development practices, version control systems (e.g., Git), and collaborative coding environments. Understanding of agile methodologies is advantageous.\nTesting and Deployment: Familiarity with testing methodologies for AI models, including unit testing, integration testing, and model validation. Experience in deploying models to production environments.\nWorkflow Optimization: Knowledge of workflow optimization techniques and tools to enhance development speed and efficiency. Understanding of CI/CD (Continuous Integration/Continuous Deployment) principles.\nResponsibilities:\nPrompt Engineering: Spearhead the design and development of prompt engineering strategies to influence and control the output of Generative AI models. Optimize prompts for desired results.\nPipeline Design: Design end-to-end data pipelines that encompass data preprocessing, feature engineering, model training, and deployment. Ensure pipelines are efficient, scalable, and well-documented.\nTechnical Review: Review the technical outputs generated by the team, including code, models, and pipelines. Ensure high-quality and maintainable solutions that adhere to best practices.\nTesting and Validation: Implement testing methodologies to validate the performance and accuracy of Generative AI models. Develop and execute unit tests, integration tests, and validation strategies.\nDeployment Strategy: Collaborate with DevOps and deployment teams to deploy trained models into production environments. Ensure smooth integration and monitor performance post-deployment.\nWorkflow Optimization: Identify opportunities to optimize development workflows, enhance productivity, and streamline processes. Implement tools and practices to improve efficiency.\nCollaboration: Interface with cross-functional teams, including data scientists, architects, and business stakeholders. Collaborate on solution design, implementation, and project milestones.\nDocumentation: Maintain comprehensive documentation of technical designs, code, and workflows. Ensure documentation is up-to-date, accessible, and understandable for team members.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Gen AI', 'Natural Language Processing', 'Machine Learning', 'Deep Learning', 'Python']",2025-06-10 15:18:40
AI Engineer,Staedean Solutions,0 - 2 years,Not Disclosed,['Hyderabad'],"As an AI/ML Engineer, you will be a core contributor to the implementation of intelligent, production-ready solutions that integrate seamlessly with Microsoft platforms. Working closely with our AI Architect, Data Scientist, and Product Owners, you will bring AI concepts to life building robust pipelines, interfaces, and integrations for ERP and business applications powered by large language models (LLMs), Azure AI, and Copilot. Your responsibilities will include:\nImplement intelligent AI-driven solutions, including LLM-powered agents, chat interfaces, and decision-support tools.\nIntegrate AI capabilities with Microsoft platforms including Azure AI, Azure ML, Power Platform, and Dataverse.\nEnhance Microsoft Dynamics 365 ERP (Finance Supply Chain) with embedded AI features and Copilot experiences.\nBuild scalable, modular data pipelines on Azure using e.g. Data Factory, Synapse Analytics, and other Microsoft integration tools.\nDesign and maintain reusable AI components (e.g., prompt templates, embeddings, RAG pipelines)\nAutomate data collection, preprocessing, evaluation, and retraining workflows.\nAssist with monitoring, evaluation, and optimization of AI models in production environments\nWrite clean, maintainable code and contribute to shared AI engineering infrastructure.\nCollaborate cross-functionally to deliver AI functionality as part of larger product solutions\nWhat You Need to Succeed:\nProven experience in AI/ML applications, ideally in enterprise or ERP settings\nHands-on experience with Azure AI services, Copilot, and ERP systems, preferably Microsoft Dynamics 365 or similar platforms\nFamiliarity with Power Platform, Power BI, and Dataverse\nStrong Python skills for backend logic, data processing, and model orchestration\nExperience building modular pipelines, APIs, and workflows in cloud environments\nUnderstanding of prompt engineering, RAG (Retrieval-Augmented Generation) and fine-tuning, and LLM evaluation best practices\nAbility to work independently and take ownership of projects while meeting deadlines\nStrong collaboration and communication skills you can align with architects, developers, and business stakeholders\nBonus: Experience with MLOps, DevOps, CI/CD, and monitoring tools\nWhy You Should Apply:\nBe Part of a Dynamic Community: Our supportive and vibrant environment ensures your contributions truly matter. Youll work with passionate professionals who are dedicated to making a difference.\nDrive Innovation and Excellence: As a STAEDEAN, you ll be at the forefront of innovation, developing solutions that transform industries and drive sustainable impact.\nGrow and Thrive: We are committed to fostering a culture of continuous improvement and shared success. Whether youre an experienced professional or just starting your career, youll find ample opportunities to develop your skills, take on new challenges, and grow.\nMake a Meaningful Impact: Your work at STAEDEAN will have a real impact on our customers, partners, and the world. Together, we strive to achieve extraordinary things, pushing the boundaries to create a better future.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Supply chain', 'ERP', 'Backend', 'Business transformation', 'orchestration', 'Data collection', 'Data processing', 'Microsoft Dynamics', 'Analytics', 'Python']",2025-06-10 15:18:43
Lab45 - AI Scientist,Wipro,2 - 7 years,Not Disclosed,['Bengaluru'],"? \n\nDo\n\n1. Instrumental in understanding the requirements and design of the product/ software\nDevelop software solutions by studying information needs, studying systems flow, data usage and work processes\nInvestigating problem areas followed by the software development life cycle\nFacilitate root cause analysis of the system issues and problem statement\nIdentify ideas to improve system performance and impact availability\nAnalyze client requirements and convert requirements to feasible design\nCollaborate with functional teams or systems analysts who carry out the detailed investigation into software requirements\nConferring with project managers to obtain information on software capabilities",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['rtd', 'root cause analysis', 'software development life cycle', 'control valves', 'instrumentation', 'temperature gauge', 'transmitters', 'software testing', 'plc', 'sharepoint', 'javascript', 'salesforce', 'thermocouple', 'dcs', 'loop checking', 'level switch']",2025-06-10 15:18:45
DevOps Engineer,Barclays,1 - 9 years,Not Disclosed,['Pune'],"Join us as a DevOps Engineer at Barclays, where you will be responsible for supporting the successful delivery of location strategy projects to plan, budget, agreed quality and governance standards. Youll spearhead the evolution of our digital landscape, driving innovation and excellence. You will harness cutting-edge technology to revolutionise our digital offerings, ensuring unparalleled customer experiences.\nTo be successful as a DevOps Engineer you should have experience with:\nExperience of Application or Technical Support.\nGood Knowledge on Release, Deployment and Change management process (ITIL).\nGood Knowledge in UNIX/Linux/Windows systems.\nGood Knowledge of Cloud services like AWS along with the tools like Terraform, Chef, etc.\nSome other highly valued skills may include:\nExtensive hands-on on AWS and tools like Chef and terraform\nHands-on knowledge on Unix systems\nYou may be assessed on key critical skills relevant for success in role, such as risk and controls, change and transformation, business acumen, strategic thinking and digital and technology, as well as job-specific technical skills.\nThis role is based out of Pune.\nPurpose of the role\nTo build and maintain the systems that collect, store, process, and analyse data, such as data pipelines, data warehouses and data lakes to ensure that all data is accurate, accessible, and secure.\nAccountabilities\nBuild and maintenance of data architectures pipelines that enable the transfer and processing of durable, complete and consistent data.\nDesign and implementation of data warehoused and data lakes that manage the appropriate data volumes and velocity and adhere to the required security measures.\nDevelopment of processing and analysis algorithms fit for the intended data complexity and volumes.\nCollaboration with data scientist to build and deploy machine learning models.\nAnalyst Expectations\nTo perform prescribed activities in a timely manner and to a high standard consistently driving continuous improvement.\nRequires in-depth technical knowledge and experience in their assigned area of expertise\nThorough understanding of the underlying principles and concepts within the area of expertise\nThey lead and supervise a team, guiding and supporting professional development, allocating work requirements and coordinating team resources.\nIf the position has leadership responsibilities, People Leaders are expected to demonstrate a clear set of leadership behaviours to create an environment for colleagues to thrive and deliver to a consistently excellent standard. The four LEAD behaviours are: L - Listen and be authentic, E - Energise and inspire, A - Align across the enterprise, D - Develop others.\nOR for an individual contributor, they develop technical expertise in work area, acting as an advisor where appropriate.\nWill have an impact on the work of related teams within the area.\nPartner with other functions and business areas.\nTakes responsibility for end results of a team s operational processing and activities.\nEscalate breaches of policies / procedure appropriately.\nTake responsibility for embedding new policies/ procedures adopted due to risk mitigation.\nAdvise and influence decision making within own area of expertise.\nTake ownership for managing risk and strengthening controls in relation to the work you own or contribute to. Deliver your work and areas of responsibility in line with relevant rules, regulation and codes of conduct.\nMaintain and continually build an understanding of how own sub-function integrates with function, alongside knowledge of the organisations products, services and processes within the function.\nDemonstrate understanding of how areas coordinate and contribute to the achievement of the objectives of the organisation sub-function.\nMake evaluative judgements based on the analysis of factual information, paying attention to detail.\nResolve problems by identifying and selecting solutions through the application of acquired technical experience and will be guided by precedents.\nGuide and persuade team members and communicate complex / sensitive information.\nAct as contact point for stakeholders outside of the immediate function, while building a network of contacts outside team and external to the organisation.",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Unix', 'Change management', 'Linux', 'Senior Analyst', 'devops', 'Machine learning', 'Windows', 'Continuous improvement', 'Operations', 'Technical support']",2025-06-10 15:18:47
Data Architect,CGI,8 - 12 years,15-30 Lacs P.A.,['Hyderabad'],"Job Title: Data Architect / Data Modeler\nExperience Level: 8+ Years\nLocation: Hyderabad\nJob Summary\nWe are seeking a highly experienced Data Architect to join our growing Data & Analytics team. This role demands a strategic thinker and technical expert who can design and build robust, scalable, and efficient data solutions. You will play a critical role in architecting end-to-end data pipelines, designing optimized data models, and delivering business-centric data infrastructure using cutting-edge technologies such as Python, PySpark, SQL, Snowflake, and/or Databricks.\nThe ideal candidate will have a deep understanding of data engineering best practices and a proven track record of enabling data-driven decision-making through innovative and scalable data solutions.\nKey Responsibilities\nArchitect & Design Scalable Data Pipelines\nLead the design and implementation of high-performance, scalable, and maintainable data pipelines that support batch and real-time processing.\nData Modeling & Data Architecture\nDesign and implement optimized data models and database schemas to support analytics, reporting, and machine learning use cases.\nCloud Data Platforms\nDevelop and manage modern cloud-based data architectures using platforms like Snowflake or Databricks, ensuring performance, security, and cost-efficiency.\nData Integration & ETL Development\nBuild robust ETL/ELT workflows to ingest, transform, and provision data from a variety of internal and external sources.\nCollaboration with Stakeholders\nWork closely with data analysts, data scientists, product managers, and business leaders to translate business requirements into technical specifications and data solutions.\nData Quality & Governance\nImplement and advocate for best practices in data quality, security, compliance, lineage, and governance.\nPerformance Optimization\nOptimize data storage and query performance using advanced SQL, partitioning, indexing, caching strategies, and compute resource tuning.\nMentorship & Best Practices\nProvide mentorship to junior engineers, establish coding standards, and contribute to the growth and maturity of the data engineering practice.\nRequired Qualifications\nBachelors or Master’s degree in Computer Science, Engineering, Data Science, or a related field.\n8+ years of experience in data engineering or related roles.\nStrong expertise in Python and PySpark for data processing and transformation.\nProficient in advanced SQL with a deep understanding of query optimization and performance tuning.\nHands-on experience with Snowflake and/or Databricks in a production environment.\nExperience in designing and implementing data warehouses and data lakes.\nSolid understanding of distributed computing frameworks, big data ecosystems, and modern data architecture patterns.\nExperience with CI/CD, version control systems (e.g., Git), and workflow orchestration tools (e.g., Airflow, dbt, etc.).\nStrong communication skills with the ability to clearly articulate technical concepts to non-technical stakeholders.Role & responsibilities\n\n\nPreferred candidate profile",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'pyspark', 'sql', 'snowflake', 'Data Architecture']",2025-06-10 15:18:50
Dev Ops Engineer,Barclays,1 - 9 years,Not Disclosed,['Pune'],"Join us as a Senior DevOps Engineer at Barclays, where you will be responsible for supporting the successful delivery of location strategy projects to plan, budget, agreed quality and governance standards. Youll spearhead the evolution of our digital landscape, driving innovation and excellence. You will harness cutting-edge technology to revolutionise our digital offerings, ensuring unparalleled customer experiences.\nTo be successful as a Senior DevOps Engineer you should have experience with:\nExperience of Application or Technical Support.\nGood Knowledge on Release, Deployment and Change management process (ITIL).\nGood Knowledge in UNIX/Linux/Windows systems.\nGood Knowledge of Cloud services like AWS along with the tools like Terraform, Chef, etc.\nSome other highly valued skills may include:\nExtensive hands-on on AWS and tools like Chef and terraform\nHands-on knowledge on Unix systems\nYou may be assessed on key critical skills relevant for success in role, such as risk and controls, change and transformation, business acumen, strategic thinking and digital and technology, as well as job-specific technical skills.\nThis role is based out of Pune.\nPurpose of the role\nTo build and maintain the systems that collect, store, process, and analyse data, such as data pipelines, data warehouses and data lakes to ensure that all data is accurate, accessible, and secure.\nAccountabilities\nBuild and maintenance of data architectures pipelines that enable the transfer and processing of durable, complete and consistent data.\nDesign and implementation of data warehoused and data lakes that manage the appropriate data volumes and velocity and adhere to the required security measures.\nDevelopment of processing and analysis algorithms fit for the intended data complexity and volumes.\nCollaboration with data scientist to build and deploy machine learning models.\nAssistant Vice President Expectations\nTo advise and influence decision making, contribute to policy development and take responsibility for operational effectiveness. Collaborate closely with other functions/ business divisions.\nLead a team performing complex tasks, using well developed professional knowledge and skills to deliver on work that impacts the whole business function. Set objectives and coach employees in pursuit of those objectives, appraisal of performance relative to objectives and determination of reward outcomes\nIf the position has leadership responsibilities, People Leaders are expected to demonstrate a clear set of leadership behaviours to create an environment for colleagues to thrive and deliver to a consistently excellent standard. The four LEAD behaviours are: L - Listen and be authentic, E - Energise and inspire, A - Align across the enterprise, D - Develop others.\nOR for an individual contributor, they will lead collaborative assignments and guide team members through structured assignments, identify the need for the inclusion of other areas of specialisation to complete assignments. They will identify new directions for assignments and/ or projects, identifying a combination of cross functional methodologies or practices to meet required outcomes.\nConsult on complex issues; providing advice to People Leaders to support the resolution of escalated issues.\nIdentify ways to mitigate risk and developing new policies/procedures in support of the control and governance agenda.\nTake ownership for managing risk and strengthening controls in relation to the work done.\nPerform work that is closely related to that of other areas, which requires understanding of how areas coordinate and contribute to the achievement of the objectives of the organisation sub-function.\nCollaborate with other areas of work, for business aligned support areas to keep up to speed with business activity and the business strategy.\nEngage in complex analysis of data from multiple sources of information, internal and external sources such as procedures and practises (in other areas, teams, companies, etc). to solve problems creatively and effectively.\nCommunicate complex information. Complex information could include sensitive information or information that is difficult to communicate because of its content or its audience.\nInfluence or convince stakeholders to achieve outcomes.",Industry Type: Financial Services,Department: Other,"Employment Type: Full Time, Permanent","['Unix', 'Change management', 'Linux', 'Machine learning', 'Service excellence', 'Windows', 'Business strategy', 'Assistant Vice President', 'Operations', 'Technical support']",2025-06-10 15:18:52
Gen AI Developer,Infosys,3 - 5 years,Not Disclosed,['Bengaluru'],"Responsibilities\n* Design, develop, and deploy AI models and systems using Python, Gen AI, Agentic AI, and other relevant technologies\n* Collaborate with cross-functional teams to identify business problems and develop AI-powered solutions\n* Work with large datasets to train and test AI models, and optimize their performance\n* Apply Prompt Engineering techniques to improve the overall performance of the AI systems\n* Troubleshoot and debug AI systems to ensure optimal performance and reliability\n* Provide technical guidance and expertise to ensure high-quality deliverables\n* Collaborate with the cross functional teams\n* Participate in code reviews and contribute to the improvement of the overall code quality\nTechnical and Professional Requirements:\nPreferred Qualifications:\n* Certification in AI or a related field, such as Certified Data Scientist or Certified AI Engineer* Experience with other AI frameworks and technologies, such as TensorFlow, PyTorch, or Keras\n* Familiarity with containerization using Docker and Kubernetes\n* Experience with agile development methodologies and version control systems, such as Git\n* Strong understanding of software design patterns, principles, and best practices\n* Experience with DevOps practices, such as continuous integration and continuous deployment\n* Knowledge of cloud computing platforms, such as Azure / GCP / AWS\nPreferred Skills:\nTechnology->Analytics - Solutions->SQL Server - Analytics\nTechnology->Machine Learning->Generative AI->retrieval augmented generation (rag)\nTechnology->Machine Learning->Python\nTechnology->Programing language C->go lang\nAdditional Responsibilities:\nRequired Qualifications:* Bachelor's or Master's degree in Computer Science, Engineering, or a related field (B.E/B.Tech/M.E/M.Tech/MCA)\n* At least 3-5 years of experience in software development, with at least 2 years of hands on experience in Gen AI development\n* Strong proficiency in Python programming language\n* Hands on experience with LangChain, Gen AI, Agentic AI, and Prompt Engineering\n* Excellent problem-solving skills, with the ability to analyze complex problems and develop creative solutions\n* Strong communication and teamwork skills, with the ability to collaborate with cross-functional teams Keywords: LangChain, Python, Fast/Flask API, Gen AI, Agentic AI, Prompt Engineering, Machine Learning, SQL, Kafka\nEducational Requirements\nBachelor of Engineering\nService Line\nInformation Systems\n* Location of posting is subject to business requirements",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Gen AI', 'kubernetes', 'continuous integration', 'python', 'software development', 'microsoft azure', 'machine learning', 'artificial intelligence', 'sql server', 'docker', 'sql', 'software design and development', 'gen', 'tensorflow', 'git', 'gcp', 'devops', 'kafka', 'pytorch', 'keras', 'agile', 'api', 'aws', 'flask']",2025-06-10 15:18:55
Associate Data Engineer,"NTT DATA, Inc.",1 - 3 years,Not Disclosed,"['New Delhi', 'Chennai', 'Bengaluru']","Your day at NTT DATA\nWe are seeking an experienced Data Engineer to join our team in delivering cutting-edge Generative AI (GenAI) solutions to clients. The successful candidate will be responsible for designing, developing, and deploying data pipelines and architectures that support the training, fine-tuning, and deployment of LLMs for various industries. This role requires strong technical expertise in data engineering, problem-solving skills, and the ability to work effectively with clients and internal teams.\n\nWhat youll be doing\n\nKey Responsibilities:\nDesign, develop, and manage data pipelines and architectures to support GenAI model training, fine-tuning, and deployment\nData Ingestion and Integration: Develop data ingestion frameworks to collect data from various sources, transform, and integrate it into a unified data platform for GenAI model training and deployment.\nGenAI Model Integration: Collaborate with data scientists to integrate GenAI models into production-ready applications, ensuring seamless model deployment, monitoring, and maintenance.\nCloud Infrastructure Management: Design, implement, and manage cloud-based data infrastructure (e.g., AWS, GCP, Azure) to support large-scale GenAI workloads, ensuring cost-effectiveness, security, and compliance.\nWrite scalable, readable, and maintainable code using object-oriented programming concepts in languages like Python, and utilize libraries like Hugging Face Transformers, PyTorch, or TensorFlow\nPerformance Optimization: Optimize data pipelines, GenAI model performance, and infrastructure for scalability, efficiency, and cost-effectiveness.\nData Security and Compliance: Ensure data security, privacy, and compliance with regulatory requirements (e.g., GDPR, HIPAA) across data pipelines and GenAI applications.\nClient Collaboration: Collaborate with clients to understand their GenAI needs, design solutions, and deliver high-quality data engineering services.\nInnovation and R&D: Stay up to date with the latest GenAI trends, technologies, and innovations, applying research and development skills to improve data engineering services.\nKnowledge Sharing: Share knowledge, best practices, and expertise with team members, contributing to the growth and development of the team.\n\nBachelors degree in computer science, Engineering, or related fields (Masters recommended)\nExperience with vector databases (e.g., Pinecone, Weaviate, Faiss, Annoy) for efficient similarity search and storage of dense vectors in GenAI applications\n5+ years of experience in data engineering, with a strong emphasis on cloud environments (AWS, GCP, Azure, or Cloud Native platforms)\nProficiency in programming languages like SQL, Python, and PySpark\nStrong data architecture, data modeling, and data governance skills\nExperience with Big Data Platforms (Hadoop, Databricks, Hive, Kafka, Apache Iceberg), Data Warehouses (Teradata, Snowflake, BigQuery), and lakehouses (Delta Lake, Apache Hudi)\nKnowledge of DevOps practices, including Git workflows and CI/CD pipelines (Azure DevOps, Jenkins, GitHub Actions)\nExperience with GenAI frameworks and tools (e.g., TensorFlow, PyTorch, Keras)\nNice to have:\nExperience with containerization and orchestration tools like Docker and Kubernetes\nIntegrate vector databases and implement similarity search techniques, with a focus on GraphRAG is a plus\nFamiliarity with API gateway and service mesh architectures\nExperience with low latency/streaming, batch, and micro-batch processing\nFamiliarity with Linux-based operating systems and REST APIs",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Apache Iceberg', 'Faiss', 'PySpark', 'Kafka', 'Pinecone', 'GitHub Actions', 'Snowflake', 'Apache Hudi', 'AWS', 'Azure DevOps', 'Python', 'Azure', 'BigQuery', 'Hadoop', 'Annoy', 'Teradata', 'SQL', 'Jenkins', 'Hive', 'Cloud Native platforms', 'GCP', 'Delta Lake', 'Databricks', 'Weaviate']",2025-06-10 15:18:59
Lead Technical Architect,Merkle B2b,7 - 12 years,Not Disclosed,['Mumbai'],"Job Description:\nBusiness Title\nLead Technical Architect\nYears of Experience\n> 7 Years\nMust have skills\n1. Database ( SQL server / SnowFlake / Teradata / Redshift / Vertica / Oracle / Big query / Azure DW etc)\n2. ETL tool (Talend, Informatica, IICS (Informatica cloud) )\n3. Experience in Cloud computing (one or more of AWS, Azure, GCP)\n4. Python, UNIX shell scripting, Project & resource management\n5. SVN, JIRA, Automation workflow (Apache Airflow, Tidal, Tivoli or similar)\nGood to have skills\n1. PySpark, Big Query, Familiar with NoSQL such as MongoDB etc\n2. Client-facing skills\nJob Descreption\nThe Technical Lead / Technical Consultant is a core role and focal point of the project team responsible for the whole technical solution and managing the day to day delivery. The role will focus on the technical solution architecture, detailed technical design, coaching of the development/implementation team and governance of the technical delivery. Technical ownership of the solution from bid inception through implementation to client delivery, followed by after sales support and best practise advice. Interactions with internal stakeholder s and clients to explain technology solutions and a clear understanding of client s business requirements through which to guide optimal design to meet their needs.\nKey responsibiltes\nAbility to design simple to medium data solutions for clients by using cloud architecture using AWS/GCP\nStrong understanding of DW, data mart, data modelling, data structures, databases, and data ingestion and transformation.\nWorking knowledge of ETL as well as database skills\nWorking knowledge of data modelling, data structures, databases, and ETL processes\nStrong understand of relational and non-relational databases and when to use them\nLeadership and communication skills to collaborate with local leadership as well as our global teams\nTranslating technical requirements into ETL/ SQL application code\nDocument project architecture, explain detailed design to team and create low level to high level design\nCreate technical documents for ETL and SQL developments using Visio, PowerPoint and other MS Office package\nWill need to engage with Project Managers, Business Analysts and Application DBA to implement ETL Solutions\nPerform mid to complex level tasks independently\nSupport Client, Data Scientists and Analytical Consultants working on marketing solution\nWork with cross functional internal team and external clients\nStrong project Management and organization skills. Ability to lead 1 - 2 projects of team size 2 - 3 team members.\nCode management systems which includes Code review, deployment, cod\nWork closely with the QA / Testing team to help identify/implement defect reduction initiatives\nWork closely with the Architecture team to make sure Architecture standards and principles are followed during development\nPerforming Proof of Concepts on new platforms/ validate proposed solutions\nWork with the team to establish and reinforce disciplined software development, processes, standards, and error recovery procedures are deployed\nMust understand software development methodologies including waterfall and agile\nDistribute and manage SQL development Work across the team\nEducation Qualification\nBachelor s or Master Degree in Computer Science\nShift timing\nGMT (UK Shift) - 2 PM to 11 PM\nLocation:\nMumbai\nBrand:\nMerkle\nTime Type:\nFull time\nContract Type:\nPermanent",Industry Type: Management Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Automation', 'Project management', 'Agile', 'Data structures', 'Workflow', 'Informatica', 'Oracle', 'Apache', 'Teradata', 'Python']",2025-06-10 15:19:01
Generative AI Architect,Safran Engineering Services,7 - 12 years,Not Disclosed,['Bengaluru'],"Role & responsibilities\n\nWe are seeking a highly experienced Generative AI Architect to lead the design and development of an AI platform that leverages Large Language Models (LLMs) and other cutting-edge AI technologies.\nIn this role, you will be responsible for architecting scalable systems that process unstructured data (such as PDFs of technical documents) and generate structured outputs.\nYou will collaborate with data scientists, engineers, and product teams to build an end-to-end AI-driven platform for real-world applications.\n\n\nSkills Required:\n\nGenAI (LMM) Technology & Architecture\nCloud (AWS) Based solution dev. (EC2, AWS Bedrock, EKS, microservices)\nLLM & Transformer based models - Selection, integration and optimization\nData pipeline engineering\nProduct security",Industry Type: Aviation,Department: Data Science & Analytics,"Employment Type: Full Time, Temporary/Contractual","['Generative AI', 'Natural Language Processing', 'Large Language Models', 'Computer Vision', 'Machine Learning']",2025-06-10 15:19:04
Assoc. Dir. DDIT US&I Sol.Del. MDM,NOVARTIS,6 - 10 years,Not Disclosed,['Hyderabad'],"Summary\n-Senior Specialist for project delivery and/or operations in the given business sub-capability. Partner with Business Stakeholders and TT Strategic Business Partners for demand analysis, solution proposal/evaluation and project delivery.\nAbout the Role\nPosition Title: Associate Director, MDM Solution Delivery\nLocation - Hyd-India# LI Hybrid\nRole Purpose",,,,"['Computer science', 'Publishing', 'Data management', 'Master data management', 'Pharma', 'Scrum', 'Associate Director', 'Project delivery', 'Analytics', 'CRM']",2025-06-10 15:19:06
Senior Talent Sourcer - Global,Reltio,7 - 9 years,Not Disclosed,['Bengaluru'],"Job Title: Senior Talent Sourcer - Global (On Contract)\nLocation: Bangalore, India (Hybrid)\nExperience: 7-9 years\nContract Duration: 12 months\nAt Reltio , we believe data should fuel business success. Reltio s AI-powered data unification and management capabilities encompassing entity resolution, multi-domain master data management (MDM), and data products transform siloed data from disparate sources into unified, trusted, and interoperable data. Reltio Data Cloud delivers interoperable data where and when its needed, empowering data and analytics leaders with unparalleled business responsiveness. Leading enterprise brands across multiple industries around the globe rely on our award-winning data unification and cloud-native MDM capabilities to improve efficiency, manage risk and drive growth.\nAt Reltio, our values guide everything we do. With an unyielding commitment to prioritizing our Customer First , we strive to ensure their success. We embrace our differences and are Better Together as One Reltio. We are always looking to Simplify and Share our knowledge when we collaborate to remove obstacles for each other. We hold ourselves accountable for our actions and outcomes and strive for excellence. We Own It . Every day, we innovate and evolve, so that today is Always Better Than Yesterday . If you share and embody these values, we invite you to join our team at Reltio and contribute to our mission of excellence.\nReltio has earned numerous awards and top rankings for our technology, our culture and our people. Reltio was founded on a distributed workforce and offers flexible work arrangements to help our people manage their personal and professional lives. If you re ready to work on unrivaled technology where your desire to be part of a collaborative team is met with a laser-focused mission to enable digital transformation with connected data, let s talk!\nJob Summary:\nWe are looking for a seasoned and innovative Senior Talent Sourcer to join our Global Talent Acquisition team in Bangalore. This role is critical to building proactive talent pipelines across US, APAC, and Europe, with a strong focus on engineering, product, and strategic business roles.\nYou ll work at the intersection of data-driven sourcing, diversity hiring, and technical acumen playing a key role in identifying and attracting world-class talent through creative sourcing methods, deep technical screening, and strategic engagement\nKey Responsibilities:\nPartner with global recruiters and hiring managers to understand role requirements and create customized sourcing strategies for various geographies.\nLeverage a wide spectrum of sourcing platforms, including:\nLinkedIn Recruiter, GitHub, Stack Overflow, Kaggle, AngelList, Hirect, Google X-ray\nAI-driven sourcing tools like HireEZ, AmazingHiring, and SeekOut\nInternal CRM/ATS tools like Greenhouse, Gem, or Beamery\nDevelop and execute innovative sourcing campaigns, including outreach sequencing, content personalization, employee referral mapping, and passive talent engagement.\nConduct initial screens and assess technical competencies, such as programming fundamentals, system design exposure, cloud platforms, or product thinking, across engineering and product roles.\nBuild robust pipelines for high-impact roles, including Backend Engineers, DevOps, Data Scientists, Product Managers, and cross-functional business teams.\nLead market mapping, competitive intelligence, and org chart building for talent benchmarking and strategic hiring.\nMaintain high-quality pipeline tracking, report sourcing metrics, and contribute insights to optimize recruiter-hiring manager alignment.\nNeeds to work across regions and working hours will have some overlap with global time zones.\nWork closely with employer branding and DEI teams to amplify the global talent brand through inclusive sourcing practices\n\nRequirements:\n7+ years of dedicated sourcing experience with a focus on global markets - US, EMEA, and APAC regions.\nProven expertise in technical sourcing with a strong grasp of:\nProgramming languages (e.g., Java, Python, Golang, JavaScript)\nInfrastructure & cloud (e.g., AWS, Kubernetes, CI/CD)\nData stack and tools (e.g., Snowflake, Kafka, Spark)\nDeep understanding of how to assess candidate profiles, project portfolios, and online contributions to qualify talent beyond the resume.\nStrong track record of driving creative sourcing initiatives and building pipelines for niche and hard-to-fill roles.\nProficiency in Boolean search, X-ray search, social engineering, and sourcing automation.\nExperience working in a high-growth product/tech company, preferably in B2B SaaS.\nStrong interpersonal and storytelling skills to pitch Reltios vision and culture effectively..\nWillingness to work in different timezones for short term and long term assignment as and when required.\nWhy Join Reltio?\nBe part of a high-impact global TA team driving growth at scale\nOpportunity to shape the sourcing strategy across three continents\nAccess to best-in-class tools, analytics, and training\nInclusive, collaborative work environment\nCompetitive compensation, a hybrid work model, and a learning budget",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Backend', 'Automation', 'Talent acquisition', 'Master data management', 'Cloud', 'Javascript', 'System design', 'Competitive intelligence', 'Analytics', 'CRM']",2025-06-10 15:19:09
Senior Risk System Analyst - Murex,New Era Technology,5 - 10 years,Not Disclosed,['Malaysia'],"Job Title: Senior Risk System Analyst\nDomain: Global Market\nSub-Domain: Risk & Data Science\nExperience Level: L3 (5-10 years of experience)\nLocation: ,Kuala Lumpur, Malaysia\nRole Overview:\nWe are seeking an experienced Senior Risk System Analyst with 5-10 years of expertise in market and credit risk management. In this role, you will focus on supporting and enhancing Murex Market Risk and Credit Risk systems, ensuring accurate risk management and reporting capabilities across our global market operations. The candidate must have a deep understanding of VaR (Value at Risk), EWRS, and MLC/Credit Risk systems, and should be proficient in managing large data sets within SQL Server 2012 and Windows 2012 environments.",,,,"['VAR', 'Market Risk', 'Credit Risk', 'Value At Risk', 'SQL Server', 'Murex']",2025-06-10 15:19:11
Lead Technical Architect,Merkle Science,9 - 14 years,Not Disclosed,['Mumbai'],"Job Description:\nBusiness Title\nLead Technical Architect\nYears of Experience\n> 7 Years\nMust have skills\n1. Database ( SQL server / SnowFlake / Teradata / Redshift / Vertica / Oracle / Big query / Azure DW etc)\n2. ETL tool (Talend, Informatica, IICS (Informatica cloud) )\n3. Experience in Cloud computing (one or more of AWS, Azure, GCP)\n4. Python, UNIX shell scripting, Project & resource management\n5. SVN, JIRA, Automation workflow (Apache Airflow, Tidal, Tivoli or similar)\nGood to have skills\n1. PySpark, Big Query, Familiar with NoSQL such as MongoDB etc\n2. Client-facing skills\nJod Descreption\nThe Technical Lead / Technical Consultant is a core role and focal point of the project team responsible for the whole technical solution and managing the day to day delivery. The role will focus on the technical solution architecture, detailed technical design, coaching of the development/implementation team and governance of the technical delivery. Technical ownership of the solution from bid inception through implementation to client delivery, followed by after sales support and best practise advice. Interactions with internal stakeholder s and clients to explain technology solutions and a clear understanding of client s business requirements through which to guide optimal design to meet their needs.\nKey responsibiltes\nAbility to design simple to medium data solutions for clients by using cloud architecture using AWS/GCP\nStrong understanding of DW, data mart, data modelling, data structures, databases, and data ingestion and transformation.\nWorking knowledge of ETL as well as database skills\nWorking knowledge of data modelling, data structures, databases, and ETL processes\nStrong understand of relational and non-relational databases and when to use them\nLeadership and communication skills to collaborate with local leadership as well as our global teams\nTranslating technical requirements into ETL/ SQL application code\nDocument project architecture, explain detailed design to team and create low level to high level design\nCreate technical documents for ETL and SQL developments using Visio, PowerPoint and other MS Office package\nWill need to engage with Project Managers, Business Analysts and Application DBA to implement ETL Solutions\nPerform mid to complex level tasks independently\nSupport Client, Data Scientists and Analytical Consultants working on marketing solution\nWork with cross functional internal team and external clients\nStrong project Management and organization skills. Ability to lead 1 - 2 projects of team size 2 - 3 team members.\nCode management systems which includes Code review, deployment, cod\nWork closely with the QA / Testing team to help identify/implement defect reduction initiatives\nWork closely with the Architecture team to make sure Architecture standards and principles are followed during development\nPerforming Proof of Concepts on new platforms/ validate proposed solutions\nWork with the team to establish and reinforce disciplined software development, processes, standards, and error recovery procedures are deployed\nMust understand software development methodologies including waterfall and agile\nDistribute and manage SQL development Work across the team\nEducation Qulification\n1. Bachelor s or Master Degree in Computer Science\nShift timingGMT (UK Shift) - 2 PM to 11 PM\nLocation:\nMumbai\nBrand:\nMerkle\nTime Type:\nFull time\nContract Type:\nPermanent",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Automation', 'Project management', 'Agile', 'Data structures', 'Workflow', 'Informatica', 'Oracle', 'Apache', 'Teradata', 'Python']",2025-06-10 15:19:13
Senior Consultant - ServiceNow,AstraZeneca India Pvt. Ltd,3 - 8 years,Not Disclosed,['Chennai'],"As a Senior Consultant - ServiceNow, you'll play a pivotal role in transforming our ability to develop life-changing medicines. This position focuses on developing and maintaining comprehensive ServiceNow reports and dashboards across multiple modules, such as ITSM and HRSD. you'll create a variety of reports, leverage performance analytics to build KPIs, and ensure data accuracy and compliance. Your responsibilities will include automating report schedules, transforming ServiceNow data using REST APIs, and collaborating with various teams to refine reporting needs. Additionally, you'll implement standard methodologies for data design in Snowflake, develop ETL/ELT processes, and collaborate with stakeholders to define data strategies. If you have expertise in ServiceNow, SQL, data warehousing, and familiarity with cloud platforms, this is the opportunity for you\nAccountabilities:\nDevelop and maintain ServiceNow reports and dashboards across various modules (ITSM, HRSD, CSM, SecOps, etc).\nCreate and optimize List, Pie, Bar, Heatmaps, Trend Charts, and Pivot Reports for actionable insights.\nUse Performance/platform Analytics to build KPIs, scorecards, and trend reports.\nImplement user experience analytics and Self-Service Analytics.\nEnsure report accuracy, optimization, and compliance with business and security requirements.\nAutomate and schedule reports to ensure timely data delivery for stakeholders.\nExtract and transform ServiceNow data using REST APIs and database queries.\nCollaborate with ServiceNow administrators, developers, and business teams to gather and refine reporting requirements.\nDevelop custom scripts and indicators to enhance analytics capabilities.\nProvide end-user training and documentation for report generation and dashboard usage.\nImplement standard methodologies for database design, performance tuning, and querying in Snowflake.\nDevelop ETL/ELT processes to ensure seamless data integration from various sources into Snowflake (not limited to Power BI).\nCreate and maintain documentation for data pipelines, data modeling, and data integration processes.\nMonitor and optimize the performance of Snowflake instances and fix any issues that arise.\nCollaborate with data architects, data scientists, and business stakeholders to gather/document the requirements and define data strategies.\nCollaborate with the product owner to review and document access permissions for Snowflake, as we'll as the associated reports and dashboards.\nEssential Skills/Experience:\n3+ years of hands-on experience in ServiceNow Reporting & Dashboards\nProficiency in Platform/Performance Analytics and MetricBase\nKnowledge in SQL query to extract data from Snowflake.\nExperience with Filters, Data Sources, and Scheduled Reports\nSolid understanding of ServiceNow Tables, Query Builder, and Data Schema\nExperience with REST APIs for data extraction and integration\nBasic JavaScript & Glide APIs experience for custom reporting (preferred)\nExperience integrating ServiceNow reports with BI tools (Power BI, Tableau, etc)\nSolid understanding of data warehousing concepts and standard methodologies.\nProficient in SQL for data manipulation and querying.\nExperience with ETL/ELT tools and processes.\nFamiliarity with cloud platforms such as AWS, Azure, or Google Cloud is a plus",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Performance tuning', 'Data modeling', 'Database design', 'Machine learning', 'Schema', 'Javascript', 'Data warehousing', 'SQL', 'Recruitment', 'Data extraction']",2025-06-10 15:19:16
Data Engineer,Blend360 India,5 - 10 years,Not Disclosed,['Hyderabad'],"We are looking for an experienced Senior Data Engineer with a strong foundation in Python, SQL, and Spark , and hands-on expertise in AWS, Databricks . In this role, you will build and maintain scalable data pipelines and architecture to support analytics, data science, and business intelligence initiatives. You ll work closely with cross-functional teams to drive data reliability, quality, and performance.\nResponsibilities:\nDesign, develop, and optimize scalable data pipelines using Databricks in AWS such as Glue, S3, Lambda, EMR, Databricks notebooks, workflows and jobs.\nBuilding data lake in WS Databricks.\nBuild and maintain robust ETL/ELT workflows using Python and SQL to handle structured and semi-structured data.\nDevelop distributed data processing solutions using Apache Spark or PySpark .\nPartner with data scientists and analysts to provide high-quality, accessible, and well-structured data.\nEnsure data quality, governance, security, and compliance across pipelines and data stores.\nMonitor, troubleshoot, and improve the performance of data systems and pipelines.\nParticipate in code reviews and help establish engineering best practices.\nMentor junior data engineers and support their technical development.\n\n\nRequirements\nBachelors or masters degree in computer science, Engineering, or a related field.\n5+ years of hands-on experience in data engineering , with at lea",Industry Type: Industrial Automation,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Automation', 'Version control', 'GIT', 'Workflow', 'Data quality', 'Business intelligence', 'Analytics', 'SQL', 'Python']",2025-06-10 15:19:19
Hiring For Gen AI !!,HCLTech,5 - 10 years,Not Disclosed,"['Noida', 'Chennai', 'Bengaluru']","RESPONSIBILITIES:\nDevelop and contribute to end-to-end architecture of highly scalable, distributed machine learning solutions for AI/ML/DL/NLP platforms.\nContribute to the research and development of advanced generative AI models such as LLMs, SLM’s including GANs, VAEs, autoregressive models, and novel architectures.\nDeployment of generative AI models, frameworks, and algorithms into scalable REST API services.",,,,"['Generative Ai', 'Artificial Intelligence', 'Natural Language Processing', 'Machine Learning', 'Deep Learning']",2025-06-10 15:19:21
Analyst,Wipro,0 - 4 years,Not Disclosed,['Gurugram'],"Wipro Limited (NYSEWIT, BSE507685, NSEWIPRO) is a leading technology services and consulting company focused on building innovative solutions that address clients’ most complex digital transformation needs. Leveraging our holistic portfolio of capabilities in consulting, design, engineering, and operations, we help clients realize their boldest ambitions and build future-ready, sustainable businesses. With over 230,000 employees and business partners across 65 countries, we deliver on the promise of helping our customers, colleagues, and communities thrive in an ever-changing world. For additional information, visit us at www.wipro.com.\n About The Role  \n\nTimings 07 PM to 4.30 AM IST.\n\nThe role also demands support to various process related project initiatives, Monthly MIS Reporting, stakeholder management, risk management, handling escalations, and addressing queries of Data Governance team. Working closely with Trade support Team for smooth flow of Transactions.\n\nFund Master Data Management:\n\nMaintain and manage the fund master database, ensuring data integrity and accuracy.\nCollaborate with investment teams to gather and validate fund-related data.\nImplement processes for the onboarding and offboarding of funds.\nData Quality ManagementExpertise in Data Quality processes and Reference Data Maintenance for Term Loans, Bonds, Equities, Derivatives, and Structured Products.\n\nAnalytical ExcellenceConduct detailed analyses to ensure timely, accurate delivery within a compliant, ""no error"" framework.\n\nApplication ProficiencyExperience with Reuters Eikon, Markit, Bloomberg, and familiarity with BlackRock Aladdin preferred.\n\nCorporate Structure KnowledgeUnderstanding of corporate structures and relationship between legal entities and parent issuers, and proficient in researching government registry database.\n\nRatings Agency InsightKnowledge of ratings agencies and their relevance to financial products, especially Term Loans.\n\nClient InteractionCollaborate with clients and onshore teams to gather requirements and identify process improvements.\n\nOperational SupportAssist Middle and Back Office operations, ensuring efficient and accurate task completion.\n\nQualifications:\n\nBachelor’s degree in Finance.\nProven experience in data governance, data management, or fund administration.\nStrong understanding of data quality principles and data governance frameworks.\nFamiliarity with regulatory requirements in the financial services industry.\nExcellent analytical, problem-solving, and communication skills.\nProficiency in data management tools and software (e.g., SQL, data visualization tools,Excel).\nAbility to work collaboratively in a team-oriented environment.\nPreferred\n\nSkills:\n\n\nExperience with fund accounting or investment management systems.\n\n ? \n\n \n\nDeliver\n NoPerformance ParameterMeasure1ProcessNo. of cases resolved per day, compliance to process and quality standards, meeting process level SLAs, Pulse score, Customer feedback2Self- ManagementProductivity, efficiency, absenteeism, Training Hours, No of technical training completed Reinvent your world.We are building a modern Wipro. We are an end-to-end digital transformation partner with the boldest ambitions. To realize them, we need people inspired by reinvention. Of yourself, your career, and your skills. We want to see the constant evolution of our business and our industry. It has always been in our DNA - as the world around us changes, so do we. Join a business powered by purpose and a place that empowers you to design your own reinvention. Come to Wipro. Realize your ambitions. Applications from people with disabilities are explicitly welcome.",Industry Type: IT Services & Consulting,"Department: BFSI, Investments & Trading","Employment Type: Full Time, Permanent","['data management', 'sql', 'data quality', 'data governance', 'data visualization', 'risk management', 'bloomberg', 'accounting', 'financial services', 'master data management', 'derivatives', 'fund administration', 'mis', 'trade support', 'investment management', 'fund accounting']",2025-06-10 15:19:24
AI Engineer,IBM,1 - 5 years,Not Disclosed,['Bengaluru'],"An AI Engineer at IBM is not just a job title - it's a mindset. You'll leverage the watsonx platform to co-create AI value with clients, focusing on technology patterns to enhance repeatability and delight clients.\nSuccess is our passion, and your accomplishments will reflect this, driving your career forward, propelling your team to success, and helping our clients to thrive.\n\n Day-to-Day Duties: \n\n  \n\nProof of Concept (POC) DevelopmentDevelop POCs to validate and showcase the feasibility and effectiveness of the proposed AI solutions. Collaborate with development teams to implement and iterate on POCs, ensuring alignment with customer requirements and expectations.\n\nCollaboration and Project ManagementCollaborate with cross-functional teams, including data scientists, software engineers, and project managers, to ensure smooth execution and successful delivery of AI solutions. Effectively communicate project progress, risks, and dependencies to stakeholders.\n\nCustomer Engagement and SupportAct as a technical point of contact for customers, addressing their questions, concerns, and feedback. Provide technical support during the solution deployment phase and offer guidance on AI-related best practices and use cases.\n\nDocumentation and Knowledge SharingDocument solution architectures, design decisions, implementation details, and lessons learned. Create technical documentation, white papers, and best practice guides. Contribute to internal knowledge sharing initiatives and mentor new team members.\n\nIndustry Trends and InnovationStay up to date with the latest trends and advancements in AI, foundation models, and large language models. Evaluate emerging technologies, tools, and frameworks to assess their potential impact on solution design and implementation.\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nEducationBachelor's, Master's, or Ph.D. degree in Computer Science, Artificial Intelligence, Data Science or a related field.\nTechnical\n\nSkills:\nStrong programming skills, with proficiency in Python and experience with AI frameworks such as TensorFlow, PyTorch, Keras or Hugging Face. Understanding in the usage of libraries such as SciKit Learn, Pandas, Matplotlib, etc. Familiarity with cloud platforms (e.g. Kubernetes, AWS, Azure, GCP) and related services is a plus.\nSoft\n\nSkills:\nExcellent interpersonal and communication skills. Engage with stakeholders for analysis and implementation. Commitment to continuous learning and staying updated with advancements in the field of AI.\n\nGrowth mindsetDemonstrate a growth mindset to understand clients' business processes and challenges.\n\n\nPreferred technical and professional experience\nExperienceProven experience in designing and delivering AI solutions, with a focus on foundation models, large language models, exposure to open source, or similar technologies. Experience in natural language processing (NLP) and text analytics is highly desirable. Understanding of machine learning and deep learning algorithms.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'machine learning', 'tensorflow', 'pytorch', 'keras', 'kubernetes', 'algorithms', 'natural language processing', 'scikit-learn', 'microsoft azure', 'cloud platforms', 'artificial intelligence', 'text analytics', 'pandas', 'deep learning', 'data science', 'gcp', 'matplotlib', 'aws']",2025-06-10 15:19:26
Data Engineer,sanas.ai,2 - 5 years,Not Disclosed,['Bengaluru'],"Sanas is revolutionizing the way we communicate with the world s first real-time algorithm, designed to modulate accents, eliminate background noises, and magnify speech clarity. Pioneered by seasoned startup founders with a proven track record of creating and steering multiple unicorn companies, our groundbreaking GDP-shifting technology sets a gold standard.\n\nSanas is a 200-strong team, established in 2020. In this short span, we ve successfully secured over $100 million in funding. Our innovation have been supported by the industry s leading investors, including Insight Partners, Google Ventures, Quadrille Capital, General Catalyst, Quiet Capital, and other influential investors. Our reputation is further solidified by collaborations with numerous Fortune 100 companies. With Sanas, you re not just adopting a product; you re investing in the future of communication.\n\nWe re looking for a sharp, hands-on Data Engineer to help us build and scale the data infrastructure that powers cutting-edge audio and speech AI products. You ll be responsible for designing robust pipelines, managing high-volume audio data, and enabling machine learning teams to access the right data fast.\n\nAs one of the first dedicated data engineers on the team, youll play a foundational role in shaping how we handle data end-to-end, from ingestion to training-ready features. Youll work closely with ML engineers, research scientists, and product teams to ensure data is clean, accessible, and structured for experimentation and production.\nKey Responsibilities :\nBuild scalable, fault-tolerant pipelines for ingesting, processing, and transforming large volumes of audio and metadata.\nDesign and maintain ETL workflows for training and evaluating ML models, using tools like Airflow or custom pipelines.\nCollaborate with ML research scientists to make raw and derived audio features (e.g., spectrograms, MFCCs) efficiently available for training and inference.\nManage and organize datasets, including labeling workflows, versioning, annotation pipelines, and compliance with privacy policies.\nImplement data quality, observability, and validation checks across critical data pipelines.\nHelp optimize data storage and compute strategies for large-scale training.\nQualifications :\n2-5 years of experience as a Data Engineer, Software Engineer, or similar role with a focus on data infrastructure.\nProficient in Python, SQL, and working with distributed data processing tools (e.g., Spark, Dask, Beam).\nExperience with cloud data infrastructure (AWS/GCP), object storage (e.g.,S3), and data orchestration tools.\nFamiliarity with audio data and its unique challenges (large file sizes, time-series features, metadata handling) is a strong plus.\nComfortable working in a fast-paced, iterative startup environment where systems are constantly evolving.\nStrong communication skills and a collaborative mindset you ll be working cross-functionally with ML, infra, and product teams.\nNice to Have :\nExperience with data for speech models like ASR, TTS, or speaker verification.\nKnowledge of real-time data processing (e.g., Kafka, WebSockets, or low-latency APIs).\nBackground in MLOps, feature engineering, or supporting model lifecycle workflows.\nExperience with labeling tools, audio annotation platforms, or human-in-the-loop systems.\nJoining us means contributing to the world s first real-time speech understanding platform revolutionizing Contact Centers and Enterprises alike.\n\n\nOur technology empowers agents, transforms customer experiences, and drives measurable growth. But this is just the beginning. Youll be part of a team exploring the vast potential of an increasingly sonic future",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['ASR', 'Training', 'metadata', 'orchestration', 'GCP', 'Machine learning', 'Data processing', 'Data quality', 'SQL', 'Python']",2025-06-10 15:19:28
Immediate Hiring R Package Developer - Bengaluru/Pune/Gurugram,Acuity Knowledge Partners,5 - 9 years,Not Disclosed,"['Pune', 'Gurugram', 'Bengaluru']","Desired Skills and experience\nCandidates should have a B.E./B.Tech/MCA/MBA in Finance, Information Systems, Computer Science or a related field\nStrong experience in R programming and package development\nProficiency with GitHub and unit testing frameworks.\nStrong documentation and communication skills.\nA background or work experience in biostatistics or a similar discipline (Preferred).",,,,"['Unit Testing', 'R Package', 'Git', 'Cicd Pipeline', 'Ci/Cd', 'R Program']",2025-06-10 15:19:30
UI R&D,Hindustan Unilever (HUL),3 - 4 years,Not Disclosed,['Mumbai'],"Business Context:\nUnilever International is Unilever s White Space Arm of Unilever and a decacorn in-making. UI has scaled 10x since its launch in 2012 and with a turnover of 1. 4 billion in 2024 it is one of the fastest growing and most profitable sizable business of Unilever. UI caters to the last billion consumers in underserved markets, consumer segments, and channels across 200+ countries and all Unilever categories. As intrapreneurs we deploy an agile innovation model that enables more than 1 launch per day. We introduce Unilever s brands into white space geographies like Korea, Pacific Islands, Mongolia, and spearhead launches of iconic brands like Dove, Vaseline, Knorr, Surf, TRESemme into big markets like U. S, China, Europe, Asia.\nCrafting product innovations for niches like Corporate Partnership serving Hospitality Travel retail, marketing to institutional partners like the United Nations, and seeding Unilever brands to overseas diaspora who crave for a taste of their familiar home brands. As well as premium product segments, on-trend formats and D2C innovations globally. We navigate complex and dynamic global regulatory landscape to ensure product compliance and EHS assurance across Personal Care, Beauty Wellbeing, Homecare, Nutrition Ice Creams.\nSingapore is our global hub leading strategy, marketing thinking, seeding plans, and alignment with the global categories and supply chain. We have a global presence with UI hubs all over the world. RD is primarily spread across India (Mumbai Bangalore) Korea with some resources present globally.\nPC is UI s largest BG representing 42% of UI turnover. Skin cleansing is the largest business for PC ( 275 M), with double-digit growth rates. Deo is the second largest PC business (roughly 240M ), while Oral care, Grooming and others contribute to about 50M to PC turnover. BW is the most profitable BG for UI business growing double digit with an ambition to grow from 400M to 600M . UI Lip care is already number #1 Lip business for Unilever, while Vaseline is number #2 for Unilever. Apart from rapidly rolling BW portfolio globally, UI also has wide mandates for multiple PC BW brands including Pears, Camay, Suave, St Ives, Simple among others, wherein UI leads product design globally as per UI mandates. HC Foods are fast growing BGs for UI representing close to 30% of its turnover.\nJob Title: RD Digital Transformation Lead, Unilever International\nLocation: Mumbai, Unilever House\nFunction: Unilever International - RD\nGeographic Scope: Global\nTravel Requirement :\nPurpose of the Job:\nThis exciting role offers opportunity to make significant impact in embedding Digital Mindset and Digital Initiatives to meet fast track requirements from RD in UI. Data is/will be the lifeline of UI to drive its ambition in FAST paced environment, and there is a need to adopt digital in routine and cut down on nonvalue adding activities.\nCore Responsibilities :\nDigital Tools relevant to RD UI - The Roadmap:\nFirst task is to know what we don t know before solving anything bespoke. You will be responsible to create a typical user journey, or day in a life of a RD professional in UI (E. g. : Conduct user/stakeholder interviews, map day in life of key personals, pain points, what s working and what s not, what is needed in future etc. ). Once done, engage with Digital Intelligence team in UI Digital Partners in BG to understand what s existing that can be adopted and what new needs to be built etc to fill-in the gaps. This roadmap with timings is the crux of how digital ways of working will evolve within UI.\nData Capture and Quality - The Bedrock for everything digital:\nRight data collected in a right way is the bedrock to build any successful digital tool, and is one of the most underrated, least understood aspect of digital transformation. In general, a data scientist spends 70 - 75% of its time on just data cleaning and making it fit for model building! At UI, the fast-track nature of job may lead to loss of critical data. Your key job would be to Identify data capture needs for UI RD processes (the WHAT), engage various digital teams to understand HOW can it be captured automatically, and propose relevant digital tools (WHERE) to record UI data. Propose the ways of working (as needed) to capture minimum / core / good to have in relevant tools, and how that information will be exploited. Ensure data quality checks and balances followed by Digital Data Quality Teams with KPI s and bring them inside UI (improve as needed). Explore how data capture tools can be added as a part of project approval process before final launches happen, so that critical information is not lost.\nDigital Tools Integration outside RD, but relevant to RD - The Efficiency:\nOne of the most exciting and quick wins is to leverage existing tool not well known across Unilever functions and bring information at fingertips for quick decision making. You will be responsible for identifying areas to cut down information availability for RD, especially from other functions such as marketing (sales for eg. ) or supply chain (such as raw material pricing, volume, location used etc) or how relevant info can be extracted from existing data capture system with ease (eg. PLM, AWS, SDC etc. ). This also involves creating simple power BI dashboard, integrating existing disjoint systems for data flow, while protecting data privacy (eg. Access based on role and requirement).\nSuccess Stories, Adoption KPI, Stakeholder and Digital Transformation the Present and Future:\nNothing drives success like success stories! Engaging 100% UI RD team for digital adoption, sharing best practices, solving pain points through digital, cutting time spent of non-value adding activities, engaging last user and senior stakeholder together, and having a sense of success through clear KPI s is critical to keep digital transformation on track. You are expected to share success stories from UI (to UI from RD etc), carrying out regular feedback survey (such as Net Promotor Score survey), connect with digital leader across RD to bring whats-new and solve real pain points. Building futuristic road map on digital revolution in UI on Digital Normal will work in UI, will help you bring futuristic digital tool (such as AI bots, first product design in 24-hours etc. , data driven quick decisions etc) to meet UI ambitions.\nApart from above core responsibilities, you will be engaging various digital leads across RD, be UI representative in relevant RD forums for digital, engage external (non UI) stakeholder as relevant to bring Digital IN for UI.\nPreferred Skills\nDigital first Mindset: MUST be confident and knowledgeable in use of digital tools and challenge the status quo as Why NOT use the digital tool.\nEffective Storyteller: Excellent written and verbal communication skills, enjoys storytelling, with ability to convey complex (even scientific) information in a simple and engaging manner. Adapts communication style to different people and settings.\nTeam Player : Role demands working together with diverse set of teams spread across multi-cultural / geographical / functional etc. You are expected to be able to work in such diverse settings and get moving.\nProblem Solver and Analytical: MUST think on solutions and approaches to overcome any issues faced in day to day working and influence right people to get the job done.\nEngage Stakeholders: Right message to stakeholder and get buy-in is critical to digital transformation, and the skill to influence stakeholders in right direction will be key to get buy-in and embed digital in UI ways of working.\nDesired Qualification Experience:\nBachelors/Masters in any STEM is a must. Formal qualification in Data science is highly desirable.\nExperience in data sciences, big data or use/implementation of digital tools with be a BIG Advantage,\nExperience is digital transformation roles, and/or project leader experience is also a PLUS for this role,\nMUST have a minimum work experience of 3 - 4 years in any role in Unilever\nStandards of Leadership : (Unilever International)\nCARE DEEPLY to nurture our tribe and work and win together\nFOCUS ON WHAT COUNTS - Consumer and customer obsession\nSTAY THREE STEPS AHEAD to drive hunger for growth\nDRIVE WITH EXCELLENCE Passion for execution\nDIGITAL YODA (Digital first mindset)",Industry Type: Advertising & Marketing,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Supply chain', 'PLM', 'EHS', 'Hospitality', 'Assurance', 'Nutrition', 'Analytical', 'Agile', 'Product design', 'Raw material']",2025-06-10 15:19:32
Front End Developer,IBM,2 - 5 years,Not Disclosed,['Bengaluru'],"As a Front End Developer  \nImplement security measures to protect data and applications. Work closely with data scientists, AI researchers, and other engineers to deliver high-quality AI-driven solutions.\nDeploy applications to cloud platforms, monitor their performance, and make necessary improvements.\nEnd-to-End designing, developing, and maintaining full-stack applications, ensuring seamless integration between frontend and back-end components.\nDesign and develop web applications using React.js and Node.js\nEnhance the user experience of web applications by integrating React components\nDebug and troubleshoot React and Node.js applications\nImplement best practices for front-end development\nCollaborate with other developers to ensure application performance\nEnsure the quality of code and maintain code standards\nManage source control and version control for applications\nDevelop and maintain RESTful APIs to connect applications to backend services\nMonitor application performance and take corrective measures\n\n\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nProficiency in  JavaScript, ES6 + – Strong understanding of modern JavaScript concepts and best practices.\n React.js  Expertise – Hands-on experience with React.js, including state management (Redux, Context API), component lifecycle, hooks, and performance optimization.\n Node.js & Express.js  – Strong knowledge of server-side development using Node.js and Express.js to build scalable backend applications.\n RESTful APIs & GraphQL  – Experience in designing, developing, and consuming APIs to enable seamless communication between front-end and back-end services.\n Database Management –  Working knowledge of SQL (PostgreSQL, MySQL) and NoSQL (MongoDB, Firebase) databases.\n Version Control –  Proficiency in Git and GitHub/GitLab for source code management and collaboration.\n Frontend Development  – Strong understanding of HTML5, CSS3, SCSS, Tailwind CSS, and responsive design principles.\nAuthentication & Security – Experience with JWT, OAuth, session-based authentication, and implementing security best practices to protect applications.\nCloud & Deployment – Familiarity with AWS, Azure, or Google Cloud for hosting, CI/CD pipelines, Docker, and Kubernetes.\nTesting & Debugging – Experience with unit testing (Jest, Mocha), integration testing, and debugging tools to ensure code quality.\n\n\n\n\nPreferred technical and professional experience\nBachelor’s/Master’s degree in Computer Science, Engineering, or a related field.\nExperience working in EdTech or similar industries.\nFamiliarity with cloud platforms (AWS, Azure, or GCP) is a bonus.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['css', 'react.js', 'html', 'sass', 'javascript', 'redux', 'continuous integration', 'kubernetes', 'jwt', 'ci/cd', 'mocha', 'docker', 'sql', 'elastic search', 'git', 'mysql', 'graphql', 'mongodb', 'modern javascript', 'rest', 'github', 'microsoft azure', 'jest', 'node.js', 'responsive design', 'gitlab', 'design principles', 'aws']",2025-06-10 15:19:36
Automation Engineer,Capgemini,3 - 6 years,Not Disclosed,['Bengaluru'],"\n\nDesign, develop, and implement MLOps pipelines for the continuous deployment and integration of machine learning models.\n\nCollaborate with data scientists and engineers to understand model requirements and optimize deployment processes.\n\nTake offline models data scientists build and turn them into a real machine learning production system.\n\nAutomate the training, testing and deployment processes for machine learning models.\n\nContinuously monitor and maintain models in production, ensuring optimal performance, accuracy and reliability.\n\nImplement best practices for version control, model reproducibility and governance.\n\nOptimize machine learning pipelines for scalability, efficiency and cost-effectiveness.\n\nTroubleshoot and resolve issues related to model deployment and performance.\n\nEnsure compliance with security and data privacy standards in all MLOps activities.\n\nKeep up-to-date with the latest MLOps tools, technologies and trends.\n\nProvide support and guidance to other team members on MLOps practices.\n\nCommunicate with a team of data scientists, data engineers and architect, document the processe\n\nExperience in designing and implementing pipelines MLOps on AWS, Azure, or GCP.\n\nHands on building CI/CD pipelines orchestration using TeamCity, Jenkins, Airflow or similar tools.\n\nExperience with MLOps Frameworks like Kubeflow, MLFlow, DataRobot, Airflow etc., experience with Docker and Kubernetes, OpenShift.\n\nProgramming languages like Python, Go, Ruby or Bash, good understanding of Linux, knowledge of frameworks such as scikit-learn, Keras, PyTorch, Tensorflow, etc.\n\nAbility to understand tools used by data scientist and experience with software development and test automation.\n\nFluent in English, good communication skills and ability to work in a team.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['continuous integration', 'kubernetes', 'python', 'ci/cd', 'aws', 'software testing', 'scikit-learn', 'automation testing', 'openshift', 'airflow', 'golang', 'microsoft azure', 'docker', 'ruby', 'tensorflow', 'gcp', 'automation engineering', 'linux', 'jenkins', 'pytorch', 'keras', 'bash', 'teamcity']",2025-06-10 15:19:38
Internal Audit - Embedded Data Analytics - Associate,Goldman Sachs,3 - 7 years,Not Disclosed,['Bengaluru'],"Internal Audit s mission is to independently assess the firm s internal control structure, including the firm s governance processes and controls, risk management, capital and anti-financial crime framework. In addition, it is also to raise awareness of control risk and monitor the implementation of management s control measures.\nIn doing so, internal Audit:\nCommunicates and reports on the effectiveness of the firm s governance, risk management and controls that mitigate current and evolving risk\nRaise awareness of control risk\nAssesses the firm s control culture and conduct risks; and\nMonitors management s implementation of control measures\nGoldman Sachs Internal Audit is organized into global teams comprising of business and technology auditors that cover all the firm s businesses and functions - securities, investment banking, consumer and investment management, risk management, finance, cyber-security and technology risk, and engineering\n  Who We Look For\nGoldman Sachs Internal Audit comprises individuals from diverse backgrounds including chartered accountants, developers, risk management professionals, cybersecurity professionals, and data scientists. We are organized into global teams comprising business and technology auditors to cover all the firm s businesses and functions, including securities, investment banking, consumer and investment management, risk management, finance, cyber-security and technology risk, and engineering.\nEmbedded Data Analytics:\nIn Internal Audit, we ensure that Goldman Sachs maintains effective controls by assessing the reliability of financial reports, monitoring the firm s compliance with laws and regulations, and advising management on developing smart control solutions. Embed Data Analytics team leverages its programming and analytical capabilities to build innovative data driven solutions. The team works closely with auditors to understand their pain points and develop data-centric solutions to address the same\nYour Impact :\nAs part of the third line of defense, you will be involved in independently assessing the firm s overall control environment and its effectiveness as it relates to current and emerging risks and communicating the results to local/ global management. In doing so, you will be supporting the provision of independent, objective and timely assurance around the firm s internal control structure, thereby supporting the Audit Committee, Board of Directors and Risk Committee in fulfilling their oversight responsibilities.\nWe are looking for a strong data scientist, passionate about using data to challenge the norm, to join our Embed Data Analytics team. The candidate will work closely with the audit teams to build innovative and reusable analytical tools that will help make audit testing more efficient and provide meaningful insights into firm s control environment\nResponsibilities\nExecute on DA strategy developed by IA management within the context of audit responsibilities, such as risk assessment, audit planning, creation of reusable tools and providing innovative solutions to complex problems\nPartner with audit teams to help identify risks associated with businesses and facilitate strategic data sourcing and develop innovative solutions to increase efficiency and effectiveness of audit testing\nBuild production ready analytical tools to automate repeatable and reusable processes within IA\nBuild and manage relationships and communications with Audit team members\nBasic Qualifications\n3+ years of experience with a minimum of bachelors in Computer Science, Math, or Statistics\nExperience with RDBMS/ SQL\nProficiency in programming languages, such as Python, Java, or C++\nKnowledge of basic statistics, including descriptive statistics, data distribution models, Time Series Analysis, correlation, and regression, and its application to data\nStrong team player with excellent communication skills (written and oral). Ability to communicate what is relevant and important in a clear and concise manner and ability to handle multiple tasks\nStrong contributing member of Data Science team and help build analytical capabilities for Internal Audit Division\nDriven and motivated and constantly taking initiative to improve performance\nPreferred Qualifications\nExperience with advanced data analytics tools and techniques\nFamiliarity with text analytics and NLP using python\nFamiliarity with machine learning algorithms and exposure to supervised and unsupervised learning - Linear/Logistic Regression, SVM, Random Forest and Boosting, Clustering and Patterns Recognition techniques\nExperience with analytical/ statistical programs such as SAS, SPSS, and R\nExperience with visualization tools (Spotfire, Qlikview or Tableau) is a plus\nCreativity/Innovation, ie, ability to create new ways to improve current processes and develop practical solutions that add value to department\nwe're committed to finding reasonable accommodations for candidates with special needs or disabilities during our recruiting process. Learn more: https: / / www.goldmansachs.com / careers / footer / disability-statement.htm",Industry Type: Banking,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Manager Internal Audit', 'C++', 'SAS', 'Risk assessment', 'Analytical', 'Investment banking', 'Investment management', 'Risk management', 'SQL']",2025-06-10 15:19:41
Advisory AI Engineer,IBM,1 - 5 years,Not Disclosed,['Bengaluru'],"An AI Engineer at IBM is not just a job title - it's a mindset. You'll leverage the watsonx platform to co-create AI value with clients, focusing on technology patterns to enhance repeatability and delight clients.\nSuccess is our passion, and your accomplishments will reflect this, driving your career forward, propelling your team to success, and helping our clients to thrive.\n\n Day-to-Day Duties: \n\n  \n\nProof of Concept (POC) DevelopmentDevelop POCs to validate and showcase the feasibility and effectiveness of the proposed AI solutions. Collaborate with development teams to implement and iterate on POCs, ensuring alignment with customer requirements and expectations.\n\nCollaboration and Project ManagementCollaborate with cross-functional teams, including data scientists, software engineers, and project managers, to ensure smooth execution and successful delivery of AI solutions. Effectively communicate project progress, risks, and dependencies to stakeholders.\n\nCustomer Engagement and SupportAct as a technical point of contact for customers, addressing their questions, concerns, and feedback. Provide technical support during the solution deployment phase and offer guidance on AI-related best practices and use cases.\n\nDocumentation and Knowledge SharingDocument solution architectures, design decisions, implementation details, and lessons learned. Create technical documentation, white papers, and best practice guides. Contribute to internal knowledge sharing initiatives and mentor new team members.\n\nIndustry Trends and InnovationStay up to date with the latest trends and advancements in AI, foundation models, and large language models. Evaluate emerging technologies, tools, and frameworks to assess their potential impact on solution design and implementation.\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\n\n Required Technical and Professional Expertise \n\n  \nEducationBachelor's, Master's, or Ph.D. degree in Computer Science, Artificial Intelligence, Data Science or a related field.\nTechnical\n\nSkills:\nStrong programming skills, with proficiency in Python and experience with AI frameworks such as TensorFlow, PyTorch, Keras or Hugging Face. Understanding in the usage of libraries such as SciKit Learn, Pandas, Matplotlib, etc. Familiarity with cloud platforms (e.g. Kubernetes, AWS, Azure, GCP) and related services is a plus.\nSoft\n\nSkills:\nExcellent interpersonal and communication skills. Engage with stakeholders for analysis and implementation. Commitment to continuous learning and staying updated with advancements in the field of AI.\n\nGrowth mindsetDemonstrate a growth mindset to understand clients' business processes and challenges",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'pandas', 'tensorflow', 'pytorch', 'keras', 'kubernetes', 'emerging technologies', 'scikit-learn', 'microsoft azure', 'cloud platforms', 'communication and interpersonal skills', 'artificial intelligence', 'gcp', 'matplotlib', 'software engineering', 'aws', 'technical documentation']",2025-06-10 15:19:43
Data Analytics Lead (Consulting),Capgemini,10 - 15 years,Not Disclosed,['Pune'],"Capgemini Invent \n\nCapgemini Invent is the digital innovation, consulting and transformation brand of the Capgemini Group, a global business line that combines market leading expertise in strategy, technology, data science and creative design, to help CxOs envision and build whats next for their businesses.\n\n Your Role \nUse Design thinking and a consultative approach to conceive cutting edge technology solutions for business problems, mining core Insights as a service model\nEngage with project activities across the Information lifecycle.\nUnderstanding client requirements, develop data analytics strategy and solution that meets client requirements\nApply knowledge and explain the benefits to organizations adopting strategies relating to NextGen/ New age Data Capabilities\nBe proficient in evaluating new technologies and identifying practical business cases to develop enhanced business value and increase operating efficiency\nArchitect large scale AI/ML products/systems impacting large scale clients across industry\nOwn end to end solutioning and delivery of data analytics/transformation programs\nMentor and inspire a team of data scientists and engineers solving AI/ML problems through R&D while pushing the state-of-the-art solution\nLiaise with colleagues and business leaders across Domestic & Global Regions to deliver impactful analytics projects and drive innovation at scale\nAssist sales team in reviewing RFPs, Tender documents, and customer requirements\nDeveloping high-quality and impactful demonstrations, proof of concept pitches, solution documents, presentations, and other pre-sales assets\nHave in-depth business knowledge across a breath of functional areas across sectors such as CPRD/FS/MALS/Utilities/TMT\n\n\n Your Profile \nB.E. / B.Tech. + MBA (Systems / Data / Data Science/ Analytics / Finance) with a good academic background\nMinimum 10 years + on Job experience in data analytics with at least 7 years ofCPRD, FS, MALS, Utilities, TMT or other relevant domain experience required\nSpecialization in data science, data engineering or advance analytics filed is strongly recommended\nExcellent understanding and hand-on experience of data-science and machine learning techniques & algorithms for supervised & unsupervised problems, NLP and computer vision\nGood, applied statistics skills, such as distributions, statistical inference & testing, etc.\nExcellent understanding and hand-on experience on building Deep-learning models for text & image analytics (such as ANNs, CNNs, LSTM, Transfer Learning, Encoder and decoder, etc).\nProficient in coding in common data science language & tools such as R, Python, Go, SAS, Matlab etc.\nAt least 7 years experience deploying digital and data science solutions on large scale project is required\nAt least 7 years experience leading / managing a data Science team is required\n\nExposure or knowledge in cloud (AWS/GCP/Azure) and big data technologies such as Hadoop, Hive\n\n What you will love about working here \nWe recognize the significance of flexible work arrangements to provide support. Be it remote work, or flexible work hours, you will get an environment to maintain healthy work life balance.\nAt the heart of our mission is your career growth. Our array of career growth programs and diverse professions are crafted to support you in exploring a world of opportunities.\nEquip yourself with valuable certifications in the latest technologies such as Generative AI.\n\n\n About Capgemini \n\nCapgemini is a global business and technology transformation partner, helping organizations to accelerate their dual transition to a digital and sustainable world, while creating tangible impact for enterprises and society. It is a responsible and diverse group of 340,000 team members in more than 50 countries. With its strong over 55-year heritage, Capgemini is trusted by its clients to unlock the value of technology to address the entire breadth of their business needs. It delivers end-to-end services and solutions leveraging strengths from strategy and design to engineering, all fueled by its market leading capabilities in AI, cloud and data, combined with its deep industry expertise and partner ecosystem. The Group reported 2023 global revenues of 22.5 billion.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['fs', 'data science', 'tmt', 'python', 'data analytics', 'hive', 'pq', 'golang', 'neural networks', 'artificial intelligence', 'deep learning', 'gcp', 'hadoop', 'ml', 'matlab', 'cnn', 'oq', 'sas', 'natural language processing', 'iq', 'microsoft azure', 'machine learning', 'data engineering', 'r', 'aws', 'statistics']",2025-06-10 15:19:47
Data Engineer-Data Integration,IBM,2 - 5 years,Not Disclosed,['Pune'],"As Data Engineer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\n\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise seach applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modeling results\n\n\n Your primary responsibilities include: \nDevelop & maintain data pipelines for batch & stream processing using informatica power centre or cloud ETL/ELT tools.\nLiaise with business team and technical leads, gather requirements, identify data sources, identify data quality issues, design target data structures, develop pipelines and data processing routines, perform unit testing and support UAT.\nWork with data scientist and business analytics team to assist in data ingestion and data-related technical issues.\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nExpertise in Data warehousing/ information Management/ Data Integration/Business Intelligence using ETL tool Informatica PowerCenter\nKnowledge of Cloud, Power BI, Data migration on cloud skills.\nExperience in Unix shell scripting and python\nExperience with relational SQL, Big Data etc\n\n\nPreferred technical and professional experience\nKnowledge of MS-Azure Cloud\nExperience in Informatica PowerCenter\nExperience in Unix shell scripting and python",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['information management', 'data warehousing', 'business intelligence', 'etl', 'data integration', 'python', 'informatica powercenter', 'power bi', 'relational sql', 'data migration', 'azure cloud', 'sql', 'elastic search', 'unix shell scripting', 'splunk', 'agile', 'big data', 'informatica']",2025-06-10 15:19:50
Snowflake - Senior Technical Lead,Sopra Steria,6 - 10 years,Not Disclosed,['Noida'],"Position: Snowflake - Senior Technical Lead\nExperience: 8-11 years\nLocation: Noida/ Bangalore\nEducation: B.E./ B.Tech./ MCA\nPrimary Skills: Snowflake, Snowpipe, SQL, Data Modelling, DV 2.0, Data Quality, AWS, Snowflake Security\nGood to have Skills: Snowpark, Data Build Tool, Finance Domain  \nPreferred Skills",,,,"['data', 'scala', 'administration', 'data warehousing', 'snowpipe', 'sql', 'star schema', 'cloud', 'scripting', 'security', 'java', 'data modeling', 'gcp', 'data vault', 'etl', 'architecture', 'snowflake', 'python', 'performance tuning', 'talend', 'microsoft azure', 'cloud platforms', 'javascript', 'data quality', 'build', 'aws', 'informatica']",2025-06-10 15:19:53
Data Engineer-Having Stratup-Mid-Size company Exp.@ Bangalore_Urgent,"As a leader in this space, we deliver wo...",8 - 13 years,Not Disclosed,['Bengaluru'],"Data Engineer\n\nLocation: Bangalore - Onsite\nExperience: 8 - 15 years\nType: Full-time\n\nRole Overview\n\nWe are seeking an experienced Data Engineer to build and maintain scalable, high-performance data pipelines and infrastructure for our next-generation data platform. The platform ingests and processes real-time and historical data from diverse industrial sources such as airport systems, sensors, cameras, and APIs. You will work closely with AI/ML engineers, data scientists, and DevOps to enable reliable analytics, forecasting, and anomaly detection use cases.\nKey Responsibilities\nDesign and implement real-time (Kafka, Spark/Flink) and batch (Airflow, Spark) pipelines for high-throughput data ingestion, processing, and transformation.\nDevelop data models and manage data lakes and warehouses (Delta Lake, Iceberg, etc) to support both analytical and ML workloads.\nIntegrate data from diverse sources: IoT sensors, databases (SQL/NoSQL), REST APIs, and flat files.\nEnsure pipeline scalability, observability, and data quality through monitoring, alerting, validation, and lineage tracking.\nCollaborate with AI/ML teams to provision clean and ML-ready datasets for training and inference.\nDeploy, optimize, and manage pipelines and data infrastructure across on-premise and hybrid environments.\nParticipate in architectural decisions to ensure resilient, cost-effective, and secure data flows.\nContribute to infrastructure-as-code and automation for data deployment using Terraform, Ansible, or similar tools.\n\n\nQualifications & Required Skills\n\nBachelors or Master’s in Computer Science, Engineering, or related field.\n6+ years in data engineering roles, with at least 2 years handling real-time or streaming pipelines.\nStrong programming skills in Python/Java and SQL.\nExperience with Apache Kafka, Apache Spark, or Apache Flink for real-time and batch processing.\nHands-on with Airflow, dbt, or other orchestration tools.\nFamiliarity with data modeling (OLAP/OLTP), schema evolution, and format handling (Parquet, Avro, ORC).\nExperience with hybrid/on-prem and cloud platforms (AWS/GCP/Azure) deployments.\nProficient in working with data lakes/warehouses like Snowflake, BigQuery, Redshift, or Delta Lake.\nKnowledge of DevOps practices, Docker/Kubernetes, Terraform or Ansible.\nExposure to data observability, data cataloging, and quality tools (e.g., Great Expectations, OpenMetadata).\nGood-to-Have\nExperience with time-series databases (e.g., InfluxDB, TimescaleDB) and sensor data.\nPrior experience in domains such as aviation, manufacturing, or logistics is a plus.\n\nRole & responsibilities\n\n\nPreferred candidate profile",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['aviation', 'Data Modeling', 'Python', 'OLAP', 'Cloud', 'ORC', 'logistics', 'Avro', 'Terraform', 'Snowflake', 'manufacturing', 'AWS', 'Parquet', 'Java', 'Azure', 'BigQuery', 'Data', 'Redshift', 'SQL', 'TimescaleDB', 'GCP', 'InfluxDB', 'dbt', 'Ansible', 'OLTP', 'Kubernetes']",2025-06-10 15:19:55
Snowflake Architect,Blend360 India,8 - 13 years,Not Disclosed,['Hyderabad'],"Job Description\nWe are looking for an experienced and hands-on Snowflake Architect to lead and expand our data engineering capabilities. This role is ideal for a technically strong leader who can design scalable data architectures, manage a high-performing team, and collaborate cross-functionally to deliver reliable and secure data solutions.\nResponsibilities:\nDesign and implement robust Snowflake data warehouse architectures and ETL pipelines to support business intelligence and advanced analytics use cases.\nLead and mentor a team of data engineers, ensuring high-quality and timely delivery of projects.\nCollaborate closely with data analysts, data scientists, and business stakeholders to understand data needs and design effective data models.\nDevelop, document, and enforce best practices for Snowflake architecture, data modeling, performance optimization, and ETL processes.\nOwn the optimization of Snowflake environments to ensure low-latency and high-availability data access.\nDrive process improvements, evaluate emerging tools, and continuously enhance our data engineering infrastructure.\nEnsure data pipelines are built with high levels of accuracy, completeness, and security, in compliance with data privacy regulations (GDPR, CCPA, etc.).\nPartner with cloud engineering and DevOps teams to integrate data solutions seamlessly within the AWS ecosystem.\nParticipate in capacity planning, budgeting, and resource allocation for the data engineering function.\n\n\nQualifications\nBachelor s degree in Computer Science, Information Technology, or a related field.\n9+ years of overall experience in cloud-based data engineering, with at least 4 years of hands-on",Industry Type: Advertising & Marketing,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Architect', 'Data modeling', 'data security', 'data governance', 'Business intelligence', 'Information technology', 'SQL', 'Python', 'Capacity planning']",2025-06-10 15:19:57
Aws Data Engineer | Gurgaon | Deloitte,Deloitte,4 - 9 years,12-22 Lacs P.A.,"['Gurugram', 'Bengaluru']","To Apply - Submit Details via Google Form - https://forms.gle/8SUxUV2cikzjvKzD9\n\nAs a Senior Consultant in our Consulting team, youll build and nurture positive working relationships with teams and clients with the intention to exceed client expectations\nSeeking experienced AWS Data Engineers to design, implement, and maintain robust data pipelines and analytics solutions using AWS services. The ideal candidate will have a strong background in AWS data services, big data technologies, and programming languages. \n\nRole & responsibilities\n1. Design and implement scalable, high-performance data pipelines using AWS services \n2. Develop and optimize ETL processes using AWS Glue, EMR, and Lambda \n3. Build and maintain data lakes using S3 and Delta Lake \n4. Create and manage analytics solutions using Amazon Athena and Redshift \n5. Design and implement database solutions using Aurora, RDS, and DynamoDB \n6. Develop serverless workflows using AWS Step Functions \n7. Write efficient and maintainable code using Python/PySpark, and SQL/PostgrSQL \n8. Ensure data quality, security, and compliance with industry standards \n9. Collaborate with data scientists and analysts to support their data needs \n10. Optimize data architecture for performance and cost-efficiency \n11. Troubleshoot and resolve data pipeline and infrastructure issues \n\nPreferred candidate profile\n1. Bachelors degree in computer science, Information Technology, or related field \n2. Relevant years of experience as a Data Engineer, with at least 60% of experience focusing on AWS \n3. Strong proficiency in AWS data services: Glue, EMR, Lambda, Athena, Redshift, S3\n4. Experience with data lake technologies, particularly Delta Lake \n5. Expertise in database systems: Aurora, RDS, DynamoDB, PostgreSQL\n6. Proficiency in Python and PySpark programming \n7. Strong SQL skills and experience with PostgreSQL\n8. Experience with AWS Step Functions for workflow orchestration \n\nTechnical Skills: \n- AWS Services: Glue, EMR, Lambda, Athena, Redshift, S3, Aurora, RDS, DynamoDB, Step Functions \n- Big Data: Hadoop, Spark, Delta Lake\n- Programming: Python, PySpark\n- Databases: SQL, PostgreSQL, NoSQL\n- Data Warehousing and Analytics \n- ETL/ELT processes \n- Data Lake architectures \n- Version control: Git \n- Agile methodologies",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Aws Data Lake', 'Etl Process', 'Python', 'Postgresql', 'Hadoop', 'Aws Emr', 'Aws Dms', 'Aurora Db', 'Aws Lambda', 'Data Pipeline', 'Redshift Aws', 'Aws Aurora', 'Hadoop Spark', 'AWS', 'Etl Pipelines', 'Pyspark', 'Aura Framework', 'Aws Glue', 'Amazon Redshift', 'Dynamo Db', 'Delta Lake', 'Nosql Databases', 'Data Lake', 'ETL', 'Athena', 'Amazon Rds']",2025-06-10 15:20:01
DevOps Engineer - Lead,Blend360 India,6 - 8 years,Not Disclosed,['Hyderabad'],"Job Description\nWe are seeking a highly skilled Lead Azure DevOps Engineer to join our team and drive the end-to-end deployment, scalability, and operationalization of machine learning models in production. You will collaborate closely with data scientists, data engineers, and DevOps teams to ensure seamless CI/CD, reproducibility, monitoring, and governance of ML pipelines\nKey Responsibilities\nDesign, implement, and maintain CI/CD pipelines for deploying and monitoring microservices efficiently.\nManage infrastructure as code using Terraform for repeatable and scalable provisioning.\nDeploy and optimize containerized applications using Docker and Azure services (Container Apps, Container Registry, Key Vault, Service Bus, Blob Storage).\nApply best practices for securing Docker images, including vulnerability scanning, reducing image size, and optimizing build efficiency.\nImplement and maintain Azure Monitor for logging, monitoring, and alerting to ensure system reliability.\nEnsure security best practices across cloud environments, including secrets management, access control, and compliance.\n(Nice to have) Design and manage multi-client architectures within shared pipelines and storage accounts in Azure Blob Storage\n\n\nQualifications\n6+ years of experience in DevOps or MLOps with a strong focus on production-grade ML solutions.\nStrong expertise in Azure, particularly with CI/CD, container orchestration, and cloud security. Profi",Industry Type: Advertising & Marketing,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Automation', 'orchestration', 'cloud security', 'devops', 'Machine learning', 'Infrastructure', 'Vulnerability', 'Management', 'Monitoring', 'microservices']",2025-06-10 15:20:04
Senior Data Engineer,Expian Technologies,5 - 10 years,Not Disclosed,['Bengaluru( MG Road )'],"Role & responsibilities\nCollaborate with cross-functional teams to understand data requirements and design scalable and efficient data processing solutions.\nDevelop and maintain data pipelines using PySpark and SQL on the Databricks platform.\nOptimize and tune data processing jobs for performance and reliability.\nImplement automated testing and monitoring processes to ensure data quality and reliability.\nWork closely with data scientists, data analysts, and other stakeholders to understand their data needs and provide effective solutions.\nTroubleshoot and resolve data-related issues, including performance bottlenecks and data quality problems.\nStay up to date with industry trends and best practices in data engineering and Databricks.\n\nPreferred candidate profile\n5+ years of experience as a Data Engineer, with a focus on Databricks and cloud-based data platforms with a minimum of 2 years of experience in writing unit/end-to-end tests for data pipelines and ETL processes on Databricks.\nHands-on experience in PySpark programming for data manipulation, transformation, and analysis.\nStrong experience in SQL and writing complex queries for data retrieval and manipulation.\nExperience in developing and implementing test cases for data processing pipelines using a test-driven development approach.\nExperience in Docker for containerising and deploying data engineering applications is good to have.\nExperience in the scripting language Python is mandatory.\nStrong knowledge of Databricks platform and its components, including Databricks notebooks, clusters, and jobs.\nExperience in designing and implementing data models to support analytical and reporting needs will be an added advantage.\nStrong Knowledge of Azure Data Factory for Data orchestration, ETL workflows, and data integration is good to have.\nGood to have knowledge of cloud-based storage such as Amazon S3 and Azure Blob Storage.\nBachelor's or Master's degree in Computer Science, Engineering, or a related field.\nStrong analytical and problem-solving skills.\nStrong English communication skills, both written and spoken, are crucial.\nCapability to solve complex technical issues and comprehend risks prior to the circumstance.",Industry Type: Management Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Pytest', 'databricks', 'pyspark', 'Data Modeling', 'SQL']",2025-06-10 15:20:06
Data Engineer-Data Platforms,IBM,4 - 7 years,Not Disclosed,['Bengaluru'],"A Data Engineer specializing in enterprise data platforms, experienced in building, managing, and optimizing data pipelines for large-scale environments. Having expertise in big data technologies, distributed computing, data ingestion, and transformation frameworks.\nProficient in Apache Spark, PySpark, Kafka, and Iceberg tables, and understand how to design and implement scalable, high-performance data processing solutions.What you’ll doAs a Data Engineer – Data Platform Services, responsibilities include:\n\nData Ingestion & Processing\nDesigning and developing data pipelines to migrate workloads from IIAS to Cloudera Data Lake.\nImplementing streaming and batch data ingestion frameworks using Kafka, Apache Spark (PySpark).\nWorking with IBM CDC and Universal Data Mover to manage data replication and movement.\nBig Data & Data Lakehouse Management\nImplementing Apache Iceberg tables for efficient data storage and retrieval.\nManaging distributed data processing with Cloudera Data Platform (CDP).\nEnsuring data lineage, cataloging, and governance for compliance with Bank/regulatory policies.\nOptimization & Performance Tuning\nOptimizing Spark and PySpark jobs for performance and scalability.\nImplementing data partitioning, indexing, and caching to enhance query performance.\nMonitoring and troubleshooting pipeline failures and performance bottlenecks.\nSecurity & Compliance\nEnsuring secure data access, encryption, and masking using Thales CipherTrust.\nImplementing role-based access controls (RBAC) and data governance policies.\nSupporting metadata management and data quality initiatives.\nCollaboration & Automation\nWorking closely with Data Scientists, Analysts, and DevOps teams to integrate data solutions.\nAutomating data workflows using Airflow and implementing CI/CD pipelines with GitLab and Sonatype Nexus.\nSupporting Denodo-based data virtualization for seamless data access\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\n4-7 years of experience in big data engineering, data integration, and distributed computing.\nStrong skills in Apache Spark, PySpark, Kafka, SQL, and Cloudera Data Platform (CDP).\nProficiency in Python or Scala for data processing.\nExperience with data pipeline orchestration tools (Apache Airflow, Stonebranch UDM).\nUnderstanding of data security, encryption, and compliance frameworks\n\n\nPreferred technical and professional experience\nExperience in banking or financial services data platforms.\nExposure to Denodo for data virtualization and DGraph for graph-based insights.\nFamiliarity with cloud data platforms (AWS, Azure, GCP).\nCertifications in Cloudera Data Engineering, IBM Data Engineering, or AWS Data Analytics",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'scala', 'pyspark', 'sql', 'spark', 'cloudera', 'continuous integration', 'data analytics', 'data processing', 'airflow', 'big data technologies', 'ci/cd', 'microsoft azure', 'data engineering', 'distributed computing', 'gcp', 'kafka', 'data ingestion', 'gitlab', 'big data', 'aws', 'data integration']",2025-06-10 15:20:09
Azure or AWS Data Engineer,Cignex Datamatics,5 - 10 years,Not Disclosed,['Bengaluru'],"We are looking for a highly skilled Lead Data Engineer with expertise in Azure or AWS and Databricks to join our team. The ideal candidate will lead the design, development, and implementation of data engineering solutions, ensuring scalability, security, and efficiency in our data infrastructure. This role requires strong technical skills, and experience in managing large-scale data processing pipelines.\nKey Responsibilities:\nLead the design and development of scalable and reliable data pipelines using Azure Data Services or AWS Data Services and Databricks.\nArchitect, implement, and optimize ETL/ELT processes to process large volumes of structured and unstructured data.\nDevelop and maintain data models, data lakes, and data warehouses to support analytics and business intelligence needs.\nCollaborate with data scientists, analysts, and business stakeholders to ensure data availability and integrity.\nImplement and enforce data governance, security, and compliance best practices.\nOptimize and monitor performance of data processing frameworks (Spark, Databricks, etc.).\nAutomate and orchestrate data workflows using tools such as Apache Airflow, Azure Data Factory, AWS Step Functions, or Glue.\nGuide and mentor junior data engineers in best practices and modern data engineering techniques.\nMandatory Qualifications:\n5+ years of experience in data engineering\nStrong expertise in Azure Data Services (Azure Data Lake, Azure Synapse, Azure Data Factory) or AWS Data Services (S3, Redshift, Glue, Lambda, Step Functions, EMR).\nProficiency in Databricks and experience with Apache Spark for large-scale data processing.\nStrong programming skills in Python\nExperience working with SQL and NoSQL databases such as PostgreSQL, MySQL, DynamoDB, or CosmosDB.\nSolid understanding of data governance, security, and compliance (GDPR, HIPAA, etc.) is a plus.\nExperience with real-time streaming technologies such as Kafka, Kinesis, or Event Hubs is a plus.\nExcellent problem-solving skills and the ability to work in a fast-paced, agile environment.\nGood to have Qualifications:\nExperience with machine learning pipelines and MLOps.\nFamiliarity with data visualization and BI tools like Power BI, Tableau, or Looker.\nStrong communication and leadership skills to drive best practices across the team.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Postgresql', 'MySQL', 'HIPAA', 'Machine learning', 'Agile', 'Data processing', 'Business intelligence', 'Analytics', 'SQL', 'Python']",2025-06-10 15:20:11
Analyst,Merkle B2b,0 - 2 years,Not Disclosed,"['New Delhi', 'Pune', 'Gurugram', 'Bengaluru']","Marketing Measurement & Optimization Analyst\nJob Description:\nQualifications:\nBachelors degree in Statistics, Mathematics, Computer Science, Engineering, or a related field.\nProven 0-2 years of experience in a similar role.\nStrong problem-solving skills.\nExcellent communication skills.\nSkills:\nProficiency in R (tidyverse, plotly/ggplot2), or Python (pandas, numpy), for data manipulation and visualization, and SQL (joins, aggregation, analytics functions) for data handling.\nAbility to understand marketing data and perform statistical tests.\nKnowledge of data visualization tools such as Tableau or Power BI.\nResponsibilities:\nFamiliar with Media Mix Modelling, Multi-Touch Attribution.\nKnowledge of panel data and its analysis.\nUnderstand of Data Science workflow.\nFamiliarity with marketing channels, performance & effectiveness metrics, and conversion funnel.\nWork with large data and performing data QA & manipulation tasks such as joins/merge, aggregation & segregation, append.\nLocation:\nPune\nBrand:\nMerkle\nTime Type:\nFull time\nContract Type:\nPermanent",Industry Type: Management Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'QA', 'data science', 'data manipulation', 'Workflow', 'power bi', 'data visualization', 'Analytics', 'SQL', 'Python']",2025-06-10 15:20:14
Analyst,Wipro,0 - 4 years,Not Disclosed,['Gurugram'],"Wipro Limited (NYSEWIT, BSE507685, NSEWIPRO) is a leading technology services and consulting company focused on building innovative solutions that address clients’ most complex digital transformation needs. Leveraging our holistic portfolio of capabilities in consulting, design, engineering, and operations, we help clients realize their boldest ambitions and build future-ready, sustainable businesses. With over 230,000 employees and business partners across 65 countries, we deliver on the promise of helping our customers, colleagues, and communities thrive in an ever-changing world. For additional information, visit us at www.wipro.com.\n Trade Support \n\nRole Purpose\n\nThe purpose of the role is to provide effective technical support to the process and actively resolve client issues directly or through timely escalation to meet process SLAs.\n\n\n Trade Support \n\nTrade Support\nTrade Support Team is responsible for Bookings, Confirmation & Settlement of Bonds and Term Loans Products. Team is responsible for multiple activities for Middle office and Back-office operations.\nTeam support settlement Activities for DTC, FED, Euroclear, CEDEL Market, ClearPar for Term Loan.\n\nTrade Support closely work with Data Governance and Asset Servicing Team.\n\nAlso Responsible Post settlements activities such as Failed Trades and Claims Management, Also ensuring Timely matching of unconfirmed Trades from Previous business day.\n\nAbout The Role *\nThis role is primarily responsible for managing end to end trade Support activities e.g. Trade Bookings/Confirmation/Settlements activities for DTC/ Euroclear/Fed and CEDEL Market.\nEnsuring all Trades are booked on time in Order Management System, Confirming all Trades with CPTY\nOn T+0, on VD+2 ensure all trades are pre-matched and settled on good value date.\n\nResolving daily issues and challenges come across in Trade Support Process.\n\nAlso, Responsible for working on Term Loan Settlement process using various tools e.g., ClearPar, Geneva.\n\nMandatory skills*-\no Understanding of Investment Banking and financial products e.g. Bonds, equity, Term Loan etc.\no Sound understanding of Fixed Income Products, Equities. MM and Derivatives Products. FX, OTC trade processing background with risks and controls surrounding this function.\no Good understanding of trade life cycle.\no Excellent verbal and written communication skills and effective interpersonal skills.\no Ability to work under pressure with excellent attention to detail,\no Ability to multitask, prioritize\no liaison with Stakeholders, Counterparties, Custodian, Front Office and trading desk,\no Experience / working knowledge of Trade support Activities for Fixed income market.\no Escalation of critical risks & non-compliance with policies, standards, and limits\n\nWork Timings*\no EMEA/US – Should be flexible\no Process Timings12:30 PM to 4:30 AM\no Note - Candidate should be flexible to work in Night shift, Night shift is core requirement.\n\n ? \n\n \n\nDeliver excellent customer service through effective diagnosis and troubleshooting of client queries  \nProvide product support and resolution to clients by performing a question diagnosis while guiding users through step-by-step solutions\nAssist clients with navigating around product menus and facilitate better understanding of product features\nTroubleshoot all client queries in a user-friendly, courteous and professional manner\nMaintain logs and records of all customer queries as per the standard procedures and guidelines\nAccurately process and record all incoming call and email using the designated tracking software\nOffer alternative solutions to clients (where appropriate) with the objective of retaining customers’ and clients’ business\nOrganize ideas and effectively communicate oral messages appropriate to listeners and situations\nFollow up and make scheduled call backs to customers to record feedback and ensure compliance to contract /SLAs\n\n\n ? \n\n \n\nBuild capability to ensure operational excellence and maintain superior customer service levels of the existing account/client  \nUndertake product trainings to stay current with product features, changes and updates\nEnroll in product specific and any other trainings per client requirements/recommendations\nPartner with team leaders to brainstorm and identify training themes and learning issues to better serve the client\nUpdate job knowledge by participating in self learning opportunities and maintaining personal networks\n\n\n ? \n\n \n\nDeliver\n NoPerformance ParameterMeasure1ProcessNo. of cases resolved per day, compliance to process and quality standards, meeting process level SLAs, Pulse score, Customer feedback2Self- ManagementProductivity, efficiency, absenteeism, Training Hours, No of technical training completed Reinvent your world.We are building a modern Wipro. We are an end-to-end digital transformation partner with the boldest ambitions. To realize them, we need people inspired by reinvention. Of yourself, your career, and your skills. We want to see the constant evolution of our business and our industry. It has always been in our DNA - as the world around us changes, so do we. Join a business powered by purpose and a place that empowers you to design your own reinvention. Come to Wipro. Realize your ambitions. Applications from people with disabilities are explicitly welcome.",Industry Type: IT Services & Consulting,"Department: BFSI, Investments & Trading","Employment Type: Full Time, Permanent","['customer service', 'investment banking', 'trade life cycle', 'derivatives', 'fixed income', 'settlements', 'trading', 'middle office', 'emea', 'asset servicing', 'technical support', 'process compliance', 'trade', 'trade support', 'troubleshooting', 'operational excellence', 'otc']",2025-06-10 15:20:17
HE Analyst,Iqvia Biotech,0 - 3 years,Not Disclosed,['Mumbai'],"HEOR analyst role that contributes to the generation of insights and solutions into ways of improving real-world patient outcomes. Role is focused on projects that will impact decision-making in various health care systems on treatment innovations, care provision and access.\nEssential Functions\nProduce materials and services relating to the Health Economics Outcomes Research business areas (health economic modelling, evidence synthesis, statistical analysis, health technology assessment submissions, report and publication development for value communication) that generate value for our clients\nEngage with large data sets and apply real-world data to health economic decision-support tools\nDevelop your project management skills through client liaison and collaboration with senior project managers\nContribute to business development as we'll as internal initiatives for the advancement of innovative methodological approaches\nWork closely with a diverse range of peers, project managers\nQualifications\nA degree in science, economics, mathematics, statistics, computer science or another relevant discipline.\nStrong numeracy and quantitative skills as we'll as competency using MS Excel and statistical packages (eg, Stata, R, SAS)\nCapable of tackling loosely defined problems analytically and applying pragmatic and logical problem-solving skills.\nCommunicative person that values building strong relationships with colleagues and clients and have the ability to explain complex topics in simple terms and in a structured way, both spoken and written.",Industry Type: Biotechnology,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Economics', 'Analyst', 'Excel', 'Statistical analysis', 'SAS', 'Project management', 'Clinical research', 'Healthcare', 'Life sciences']",2025-06-10 15:20:19
Analyst,Merkle Science,0 - 2 years,Not Disclosed,"['New Delhi', 'Pune', 'Gurugram', 'Bengaluru']","Marketing Measurement & Optimization Analyst\nJob Description:\nQualifications:\nBachelors degree in Statistics, Mathematics, Computer Science, Engineering, or a related field.\nProven 0-2 years of experience in a similar role.\nStrong problem-solving skills.\nExcellent communication skills.\nSkills:\nProficiency in R (tidyverse, plotly/ggplot2), or Python (pandas, numpy), for data manipulation and visualization, and SQL (joins, aggregation, analytics functions) for data handling.\nAbility to understand marketing data and perform statistical tests.\nKnowledge of data visualization tools such as Tableau or Power BI.\nResponsibilities:\nFamiliar with Media Mix Modelling, Multi-Touch Attribution.\nKnowledge of panel data and its analysis.\nUnderstand of Data Science workflow.\nFamiliarity with marketing channels, performance & effectiveness metrics, and conversion funnel.\nWork with large data and performing data QA & manipulation tasks such as joins/merge, aggregation & segregation, append.\nLocation:\nPune\nBrand:\nMerkle\nTime Type:\nFull time\nContract Type:\nPermanent",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'QA', 'data science', 'data manipulation', 'Workflow', 'power bi', 'data visualization', 'Analytics', 'SQL', 'Python']",2025-06-10 15:20:22
Data Bricks,PwC India,7 - 12 years,Not Disclosed,['Bengaluru'],"Job Summary:\n\nWe are seeking a talented Data Engineer with strong expertise in Databricks, specifically in Unity Catalog, PySpark, and SQL, to join our data team. Youll play a key role in building secure, scalable data pipelines and implementing robust data governance strategies using Unity Catalog.\n\nKey Responsibilities:",,,,"['DataBricks', 'Data Bricks', 'Pyspark', 'Delta Lake', 'Databricks Engineer', 'Unity Catalog', 'SQL']",2025-06-10 15:20:24
HE Analyst,Iqvia Biotech,0 - 3 years,Not Disclosed,['Mumbai'],"Job Overview\nHEOR analyst role that contributes to the generation of insights and solutions into ways of improving real-world patient outcomes. Role is focused on projects that will impact decision-making in various health care systems on treatment innovations, care provision and access.\nEssential Functions\nProduce materials and services relating to the Health Economics Outcomes Research business areas (health economic modelling, evidence synthesis, statistical analysis, health technology assessment submissions, report and publication development for value communication) that generate value for our clients\nEngage with large data sets and apply real-world data to health economic decision-support tools\nDevelop your project management skills through client liaison and collaboration with senior project managers\nContribute to business development as well as internal initiatives for the advancement of innovative methodological approaches\nWork closely with a diverse range of peers, project managers\nQualifications\nA degree in science, economics, mathematics, statistics, computer science or another relevant discipline.\nStrong numeracy and quantitative skills as well as competency using MS Excel and statistical packages (e.g., Stata, R, SAS)\nCapable of tackling loosely defined problems analytically and applying pragmatic and logical problem-solving skills.\nCommunicative person that values building strong relationships with colleagues and clients and have the ability to explain complex topics in simple terms and in a structured way, both spoken and written.\n. We create intelligent connections to accelerate the development and commercialization of innovative medical treatments to help improve patient outcomes and population health worldwide . Learn more at https://jobs.iqvia.com",Industry Type: Medical Devices & Equipment,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Economics', 'Excel', 'Statistical analysis', 'SAS', 'Senior Analyst', 'Project management', 'Clinical research', 'Healthcare', 'Life sciences']",2025-06-10 15:20:26
Data Engineer-Data Modeling,IBM,4 - 9 years,Not Disclosed,['Mumbai'],"As an Data Engineer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviours.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modelling results\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\n4+ years of experience in data modelling, data architecture.\nProficiency in data modelling tools ERwin, IBM Infosphere Data Architect and database management systems\nFamiliarity with different data models like relational, dimensional and NoSQl databases.\nUnderstanding of business processes and how data supports business decision making.\nStrong understanding of database design principles, data warehousing concepts, and data governance practices\n\n\nPreferred technical and professional experience\nExcellent analytical and problem-solving skills with a keen attention to detail.\nAbility to work collaboratively in a team environment and manage multiple projects simultaneously.\nKnowledge of programming languages such as SQL",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['database design', 'data modeling', 'data governance', 'data warehousing concepts', 'design principles', 'ibm infosphere', 'data warehousing', 'data architecture', 'erwin', 'machine learning', 'data engineering', 'sql', 'nosql', 'database management', 'elastic search', 'splunk', 'big data']",2025-06-10 15:20:28
Data Engineer-Data Modeling,IBM,4 - 9 years,Not Disclosed,['Pune'],"As an Data Engineer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\nIn this role, your responsibilities may include\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviours.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modelling results\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\n4+ years of experience in data modelling, data architecture.\nProficiency in data modelling tools ERwin, IBM Infosphere Data Architect and database management systems\nFamiliarity with different data models like relational, dimensional and NoSQl databases.\nUnderstanding of business processes and how data supports business decision making.\nStrong understanding of database design principles, data warehousing concepts, and data governance practices\n\n\nPreferred technical and professional experience\nExcellent analytical and problem-solving skills with a keen attention to detail.\nAbility to work collaboratively in a team environment and manage multiple projects simultaneously.\nKnowledge of programming languages such as SQL",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['database design', 'data modeling', 'data governance', 'data warehousing concepts', 'design principles', 'ibm infosphere', 'data warehousing', 'data architecture', 'erwin', 'machine learning', 'data engineering', 'sql', 'nosql', 'database management', 'elastic search', 'splunk', 'big data']",2025-06-10 15:20:31
Principal Machine Learning Engineer,Horizon Therapeutics,2 - 7 years,Not Disclosed,['Hyderabad'],"Career Category Information Systems Job Description Join Amgen s Mission of Serving Patients\nAt Amgen, if you feel like you re part of something bigger, it s because you are. Our shared mission to serve patients living with serious illnesses drives all that we do.\nSince 1980, we ve helped pioneer the world of biotech in our fight against the world s toughest diseases. With our focus on four therapeutic areas -Oncology, Inflammation, General Medicine, and Rare Disease- we reach millions of patients each year. As a member of the Amgen team, you ll help make a lasting impact on the lives of patients as we research, manufacture, and deliver innovative medicines to help people live longer, fuller happier lives.\nOur award-winning culture is collaborative, innovative, and science based. If you have a passion for challenges and the opportunities that lay within them, you ll thrive as part of the Amgen team. Join us and transform the lives of patients while transforming your career.\nWhat you will do\nLet s do this. Let s change the world. We are seeking a highly skilled Machine Learning Engineer with a strong MLOps background to join our team. You will play a pivotal role in building and scaling our machine learning models from development to production. Your expertise in both machine learning and operations will be essential in creating efficient and reliable ML pipelines.\nRoles & Responsibilities:\nCollaborate with data scientists to develop, train, and evaluate machine learning models.\nBuild and maintain MLOps pipelines, including data ingestion, feature engineering, model training, deployment, and monitoring.\nLeverage cloud platforms (AWS, GCP, Azure) for ML model development, training, and deployment.\nImplement DevOps/MLOps best practices to automate ML workflows and improve efficiency.\nDevelop and implement monitoring systems to track model performance and identify issues.\nConduct A/B testing and experimentation to optimize model performance.\nWork closely with data scientists, engineers, and product teams to deliver ML solutions.\nGuide and mentor junior engineers in the team\nStay updated with the latest trends and advancements\nWhat we expect of you\nWe are all different, yet we all use our unique contributions to serve patients.\nBasic Qualifications:\nDoctorate degree and 2 years of Computer Science, Statistics, and Data Science, Machine Learning experience OR\nMaster s degree and 8 to 10 years of Computer Science, Statistics, and Data Science, Machine Learning experience OR\nBachelor s degree and 10 to 14 years of Computer Science, Statistics, and Data Science, Machine Learning experience OR\nDiploma and 14 to 18 years of years of Computer Science, Statistics, and Data Science, Machine Learning experience\nPreferred Qualifications:\nMust-Have Skills:\nStrong foundation in machine learning algorithms and techniques\nExperience in MLOps practices and tools (e.g., MLflow, Kubeflow, Airflow); Experience in DevOps tools (e.g., Docker, Kubernetes, CI/CD)\nProficiency in Python and relevant ML libraries (e.g., TensorFlow, PyTorch, Scikit-learn)\nOutstanding analytical and problem-solving skills; Ability to learn quickly; Excellent communication and interpersonal skills\nGood-to-Have Skills:\nExperience with big data technologies (e.g., Spark), and performance tuning in query and data processing\nExperience with data engineering and pipeline development\nExperience in statistical techniques and hypothesis testing, experience with regression analysis, clustering and classification\nKnowledge of NLP techniques for text analysis and sentiment analysis\nExperience in analyzing time-series data for forecasting and trend analysis\nFamiliar with AWS, Azure, or Google Cloud;\nFamiliar with Databricks platform for data analytics and MLOps\nProfessional Certifications\nCloud Computing and Databricks certificate preferred\nSoft Skills:\nExcellent analytical and fixing skills.\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nTeam-oriented, with a focus on achieving team goals\nStrong presentation and public speaking skills.\nWhat you can expect of us\nAs we work to develop treatments that take care of others, we also work to care for your professional and personal growth and well-being. From our competitive benefits to our collaborative culture, we ll support your journey every step of the way.\nIn addition to the base salary, Amgen offers competitive and comprehensive Total Rewards Plans that are aligned with local industry standards.\nApply now and make a lasting impact with the Amgen team. careers.amgen.com\nAs an organization dedicated to improving the quality of life for people around the world, Amgen fosters an inclusive environment of diverse, ethical, committed and highly accomplished people who respect each other and live the Amgen values to continue advancing science to serve patients. Together, we compete in the fight against serious disease.\nAmgen is an Equal Opportunity employer and will consider all qualified applicants for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, disability status, or any other basis protected by applicable law.\nWe will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation.\n.",Industry Type: Biotechnology,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Cloud computing', 'Performance tuning', 'GCP', 'Analytical', 'Machine learning', 'Oncology', 'Forecasting', 'Monitoring', 'Python']",2025-06-10 15:20:34
Life sciences - Sr.Project Manager,Agilisium,10 - 17 years,Not Disclosed,['Chennai( Perungudi )'],"Job Title: IT Project Manager (Life Sciences)\nLocation: OMR, Chennai\nWork Mode: On-site (Ready to work from office)\nExperience: 12+ Years\nJob Description:\nWe are looking for an IT Project Manager with deep expertise in Life Sciences (Pharma/Biotech/MedTech) and hands-on experience managing Data Engineering projects. The ideal candidate will have 12+ years of experience leading IT initiatives, including data pipelines, cloud-based analytics, and regulatory-compliant data solutions in the Life Sciences domain.",,,,"['Life Sciences', 'Project Management', 'Profit And Loss', 'Project Monitoring', 'Project Documentation', 'Project Planning', 'Project Scheduling', 'Salesforce']",2025-06-10 15:20:37
Senior AI/ML,Voxai,6 - 10 years,Not Disclosed,['Hyderabad'],"Voxai IT Solution Pvt. Ltd.\nhttp://www.voxai.com/\n\nJob Description\nAs a Senior AI/ML Engineer, you will play a key role in the design, development, and continuous improvement of our AI and machine learning solutions. Your primary responsibilities will include ensuring the scalability, accuracy, and reliability of our models, while collaborating with team members and providing technical expertise to drive success.\nJob Title: - Senior AI/ML Engineer Work Location: Hyderabad\n\nKey Responsibilities:\nModel Development: Design, develop, and deploy machine learning models and algorithms to address complex business problems.\nData Analysis: Analyze datasets to uncover trends, patterns, and insights, and ensure data quality and integrity for model training.\nAlgorithm Optimization: Continuously optimize machine learning models for performance, scalability, and efficiency, ensuring that they meet business and technical requirements.\nCollaboration: Work closely with cross-functional teams, including data scientists, engineers, and product managers, to integrate machine learning models into production systems.\nResearch & Innovation: Stay updated with the latest AI/ML trends and research, applying innovative approaches and solutions to improve models and processes.\nModel Evaluation & Tuning: Perform rigorous model validation, hyperparameter tuning, and evaluation to ensure model accuracy and generalization.\nAutomation: Develop and implement tools to automate and streamline ML workflows, including data preprocessing, feature engineering, and model retraining.\nMentorship: Provide guidance and technical leadership to junior team members, helping them with best practices, problem-solving, and professional growth.\nDocumentation: Document model development processes, experiments, and results for internal use and knowledge sharing.\nPerformance Monitoring: Monitor the performance of deployed models in production environments and troubleshoot issues as they arise.\nEthical AI Development: Ensuring AI solutions are ethical, fair, and unbiased while adhering to privacy and security standards.\nQualifications:\nBachelors or Masters degree in Computer Science or a related field.\n8+ years of experience in software development\nMachine Learning (Supervised, Unsupervised, and Reinforcement Learning)\nProgramming: Python, TensorFlow, PyTorch\nCloud Platforms: AWS/Azure/GCP\nModel Deployment (Docker/ Kubernetes)\n\nExperience with SQL / NoSQL databases (e.g.SQL Server, Dynamo DB, MongoDB, Cassandra).\nExperience in Natural language processing (NLP)\nExperience with various Design patterns(eg Pipeline, Ensemble, Transfer Learning) and Metrics (eg Precision, Recall, F1-Score ROC-AUC, MSE, RMSE, Confusion Matrix)\nExperience in evaluating Model Deployment metrics\nKnowledge of ML OPS is a plus\nStrong problem-solving and debugging skills.\nFamiliarity with Agile/Scrum methodologies.\nExcellent communication and interpersonal skills.\nAbility to work effectively in a collaborative team environment.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Artificial Intelligence', 'Python or pytorch or Tensorflow', 'Machine Learning', 'Docker or Kubernetes', 'AWS or Azure or GCP', 'SQL']",2025-06-10 15:20:40
Full Stack Lead,Persistent,7 - 11 years,Not Disclosed,['Bengaluru'],"About Persistent\nWe are a trusted Digital Engineering and Enterprise Modernization partner, combining deep technical expertise and industry experience to help our clients anticipate what’s next. Our offerings and proven solutions create a unique competitive advantage for our clients by giving them the power to see beyond and rise above. We work with many industry-leading organizations across the world including 12 of the 30 most innovative US companies, 80% of the largest banks in the US and India, and numerous innovators across the healthcare ecosystem.\nOur growth trajectory continues, as we reported $1,231M annual revenue (16% Y-o-Y). Along with our growth, we’ve onboarded over 4900 new employees in the past year, bringing our total employee count to over 23,500+ people located in 19 countries across the globe.",,,,"['full stack', 'memcached', 'server', 'css', 'dbms', 'microsoft', 'ajax', 'redis', 'jquery', 'docker', 'cloud', 'react.js', 'java', 'apache', 'ui', 'gcp', 'linux', 'writing', 'json', 'mysql', 'html', 'mvc', 'mongodb', 'communication skills', 'python', 'ux', 'oracle', 'nginx', 'sql server', 'javascript', 'ruby', 'angular', 'spring boot', 'node.js', 'troubleshooting', 'le', 'aws', 'angularjs']",2025-06-10 15:20:43
Senior Machine Learning Engineer,Ortseam Technologies,5 - 10 years,Not Disclosed,[],"Job Title: Senior Machine Learning Engineer\nWork Mode: Remote\nBase Location: Bengaluru\nExperience: 5+ Years\n\nStrong problem-solving skills and ability to work in a fast-paced, collaborative environment.\nStrong programming skills in Python and experience with ML frameworks.\nProficiency in containerization (Docker) and orchestration (Kubernetes) technologies.\nSolid understanding of CI/CD principles and tools (e.g., Jenkins, GitLab CI, GitHub Actions).\nKnowledge of data engineering concepts and experience building data pipelines.\nStrong understandings on Computational, Storage and Orchestration resources on cloud platforms.\nDeploying and managing ML models especially on GCP (cloud platform agnostic though) services such as Cloud Run, Cloud Functions, and Vertex AI.\nImplementing MLOps best practices, including model version tracking, governance, and monitoring for performance degradation and drift.\nCreating and using benchmarks, metrics, and monitoring to measure and improve services\nCollaborating with data scientists and engineers to integrate ML workflows from onboarding to decommissioning.\nExperience with MLOps tools like Kubeflow, MLflow, and Data Version Control (DVC).\nManage ML models on any of the following: AWS (SageMaker), Azure (Machine Learning), and GCP (Vertex AI).\n\nTech Stack:\n\nAws or GCP or Azure Experience. (More GCP Specific)\nmust have done Py spark,\nDatabricks is good.\nML Experience,\nDocker and Kubernetes.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Pyspark', 'Machine Learning', 'Docker', 'GCP', 'Kubernetes', 'Vertex', 'Hadoop', 'Data Bricks', 'Hive', 'Azure Cloud', 'SCALA', 'Cicd Pipeline', 'AWS', 'Python']",2025-06-10 15:20:45
"Databricks Developer- Kolkata, Hyderabad, Bangalore",Immediate Joiner,7 - 12 years,27.5-35 Lacs P.A.,"['Kolkata', 'Hyderabad', 'Bengaluru']","Band 4c & 4D Skill set -Unity Catalog  + Python , Spark , Kafka\nInviting applications for the role of Lead Consultant- Databricks Developer with experience in Unity Catalog + Python , Spark , Kafka for ETL!\nIn this role, the Databricks Developer is responsible for solving the real world cutting edge problem to meet both functional and non-functional requirements.\nResponsibilities\nDevelop and maintain scalable ETL pipelines using Databricks with a focus on Unity Catalog for data asset management.\nImplement data processing frameworks using Apache Spark for large-scale data transformation and aggregation.\nIntegrate real-time data streams using Apache Kafka and Databricks to enable near real-time data processing.\nDevelop data workflows and orchestrate data pipelines using Databricks Workflows or other orchestration tools.\nDesign and enforce data governance policies, access controls, and security protocols within Unity Catalog.\nMonitor data pipeline performance, troubleshoot issues, and implement optimizations for scalability and efficiency.\nWrite efficient Python scripts for data extraction, transformation, and loading.\nCollaborate with data scientists and analysts to deliver data solutions that meet business requirements.\nMaintain data documentation, including data dictionaries, data lineage, and data governance frameworks.\nQualifications we seek in you!\nMinimum qualifications\nBachelors degree in Computer Science, Data Engineering, or a related field.\nexperience in data engineering with a focus on Databricks development.\nProven expertise in Databricks, Unity Catalog, and data lake management.\nStrong programming skills in Python for data processing and automation.\nExperience with Apache Spark for distributed data processing and optimization.\nHands-on experience with Apache Kafka for data streaming and event processing.\nProficiency in SQL for data querying and transformation.\nStrong understanding of data governance, data security, and data quality frameworks.\nExcellent communication skills and the ability to work in a cross-functional environ\nMust have experience in Data Engineering domain .\nMust have implemented at least 2 project end-to-end in Databricks.\nMust have at least experience on databricks which consists of various components as below\nDelta lake\ndbConnect\ndb API 2.0\nDatabricks workflows orchestration\nMust be well versed with Databricks Lakehouse concept and its implementation in enterprise environments.\nMust have good understanding to create complex data pipeline\nMust have good knowledge of Data structure & algorithms.\nMust be strong in SQL and sprak-sql.\nMust have strong performance optimization skills to improve efficiency and reduce cost.\nMust have worked on both Batch and streaming data pipeline.\nMust have extensive knowledge of Spark and Hive data processing framework.\nMust have worked on any cloud (Azure, AWS, GCP) and most common services like ADLS/S3, ADF/Lambda, CosmosDB/DynamoDB, ASB/SQS, Cloud databases.\nMust be strong in writing unit test case and integration test\nMust have strong communication skills and have worked on the team of size 5 plus\nMust have great attitude towards learning new skills and upskilling the existing skills.\nPreferred Qualifications\nGood to have Unity catalog and basic governance knowledge.\nGood to have Databricks SQL Endpoint understanding.\nGood To have CI/CD experience to build the pipeline for Databricks jobs.\nGood to have if worked on migration project to build Unified data platform.\nGood to have knowledge of DBT.\nGood to have knowledge of docker and Kubernetes.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Kafka', 'Spark', 'Unity Catalog', 'Python']",2025-06-10 15:20:47
Azure Data Engineer,Cognitio Analytics,3 - 8 years,Not Disclosed,['Gurugram'],"Sr. Data Scientist\nGurugram (Hybrid)\n3 To 6 years\n+ Job Description\nApply\nIdeal qualifications, skills and experiences we are looking for are:\nWe are actively seeking a talented and results-driven Data Scientist to join our team and take ownership of deliverables through the power of data analytics and insights.\nYour contributions will be instrumental in making data-informed decisions, identifying growth opportunities, and propelling our organization to new levels of success.\nDoctorate/Master s/bachelor s degree in data science, Statistics, Computer Science, Mathematics, Economics, commerce or a related field.\nMinimum of 3 years of experience working as a Data Scientist or in a similar analytical role, with experience leading data science projects and teams.\nExperience in Healthcare domain with exposure to clinical operations, financial, risk rating, fraud, digital, sales and marketing, and wellness, e-commerce or the ed tech industry is a plus.\nExpertise in programming languages such as SQL, Python/PySpark and proficiency with data manipulation, analysis, and visualization libraries (e.g., pandas, NumPy, Matplotlib, seaborn).\nVery strong python and exceptional with pandas, NumPy, advanced python (pytest, class, inheritance, docstrings).\nDeep understanding of machine learning algorithms, model evaluation, and feature engineering. Experience with frameworks like scikit-learn, TensorFlow, or Py torch.",Industry Type: Management Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'data science', 'Analytical', 'Financial risk', 'Machine learning', 'Programming', 'Healthcare', 'Wellness', 'SQL', 'Python']",2025-06-10 15:20:50
Senior Software Engineer Web (React/TypeScript),Endowus,3 - 7 years,Not Disclosed,"['Warangal', 'Hyderabad', 'Nizamabad']","About Us India Tech Hub\nEndowus is Asias leading independent wealth and fund platform, headquartered in Singapore and Hong Kong, managing over US$7 billion in client assets\nOur Hyderabad-based tech hub plays a critical role in building the next generation of global wealthtech products that are redefining how individuals invest across Asia\nOur India team is not a back officeits the engine room\nEngineers, product managers, and data scientists here drive end-to-end ownership of platform innovation, customer experience, and security at scale\nOur team members come from top companies like JPMorgan, Amazon, Google, Oracle, Meta, Grab, Swiggy, Paytm, Razorpay, and Flipkart, bringing together strong fintech and product DNA with the best of global standards and agile startup culture\nWe're growing fast and are on a mission to build a world-class platform that powers the future of digital wealth across Asia\nWe believe that access to smart investing should be universal, and that innovation starts with a world-class team\nThe people you will work with\nThe team has deep domain knowledge in finance and technology, bringing together decades of experience at Morgan Stanley, Amazon, JPMorgan, UBS, Goldman Sachs, Blackstone, Vanguard, ANT, Fidelity, McKinsey, Grab, ByteDance, and many others\nSee our leadership team here\nWe practise inclusion and treasure our diversity in background and experience\nA diverse team is our biggest asset and we look for people who share our belief in Endowusclear mission\n\n\nAbout this role; responsibilities & ownership\nYou will work on consumer-facing products delivered through iterative development, facilitating fast customer feedback loops\nYou will design and build advanced desktop & mobile optimized web applications in TypeScript & Reactjs in a collaborative, agile environment\nYou will need to demonstrate a strong product sense and be empathetic to clients' experiences of using the product\nYou will work closely with Product, Design, Marketing, and other stakeholders to ensure iterative delivery of customer value\nYou will write testable, clean, efficient code that can be confidently released in production\nYou will continuously discover, evaluate, and implement new technologies to maximize development efficiency & customer satisfaction\nYou will mentor other engineers in the team, helping them achieve high levels of productivity, quality, and reliability\nRequirements & Qualifications\nBachelors' or above in Computer Science, a related field, or equivalent professional experience\nAt least 5 years of experience in building web applications in a modern front-end stack like Reactjs/Redux/Nodejs\nDeep experience with Nodejs, TypeScript, JavaScript , HTML/CSS and RESTful APIs\nDeep experience with Reactjs and its lifecycles and hooks\nStrong experience with agile processes, testing, CI/CD and production error/metrics monitoring\nEager and willing to learn new things\nStaying up to date with the latest technologies\nAbility to think through architectural problems, find reasonable solutions, and work with other members to build them\nA keen eye for design and detail to deliver joyous experiences to our customers\nSelf-driven with a strong sense of ownership & bias for action\nBeing a team player is key\nWe are a small but growing team, and we believe in building an environment that fosters a sense of belonging for all of our employees\nPassionate and excited about building innovative commercial services\nYou are serious about building consumer-facing products when writing or reviewing code\nPreferred Skills & Experience\nExperience working in full-stack development including but not limited to server-side programming, RESTful API / GraphQL development, etc-\nExperience working with CI/CD pipelines, infrastructure as code, AWS/GCP, CDNs, etc-\nExperience working in financial services and/or B2C domain\nExposure to CQRS / Event Sourcing architecture patterns\nExperience working with tracking solutions e g- Google Analytics 4, Google Tag Manager\nOur Impact & Vision\nFounded in 2017, Endowus is backed by top-tier investors like Lightspeed, UBS, EDBI, Prosus Ventures, and Citi Ventures\nOur leadership team includes seasoned professionals from BlackRock, Goldman Sachs, Morgan Stanley, Amazon, and Google, united by a shared belief in building ethical and transparent financial solutions\nAs a pioneer in fee-only digital wealth management, Endowus is the first in Asia to unify investing of cash and pension savings (such as CPF and SRS in Singapore) into global public and private markets, through a single, unified platform\nWe've received accolades such as Great Place to Work, Fortunes FinTech Innovators Asia 2024, LinkedIn Top Startups 2024, and recognition by the World Economic Forum as a Tech Pioneer\nEven though we dont serve customers in India today, the work we do here is essential to our global ambitions\nIf youre excited by product-led growth, fintech innovation, and solving real-world investment problems with codecome build with us\nOur Tech Stack\nOur backend services are written in Scala/Akka and the APIs are tested using Cucumber-Scala\nWe are heavy users of DDD, event sourcing, and CQRS\nOur web app is in TypeScript/Reactjs and tested using Playwright\nOur mobile app is in Dart/Flutter and tested using FlutterTest\nWe use Kafka for async communication between services\nCassandra is our transactional database & PostgreSQL is our read-side database\nOur datalake is built using Spark/Athena\nWe are 100% cloud native and rely heavily on CI/CD & IaC for automated deployments & operations\nYou can also read more about our tech culture & development practices on our blog at tech\nendowus com\nShow more Show less",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['rest', 'react.js', 'css', 'html/css', 'html', 'typescript', 'agile', 'javascript']",2025-06-10 15:20:52
Solution Architect,Maddisoft Solutions,10 - 20 years,Not Disclosed,['Chennai'],"Maddisoft has the following immediate opportunity, let us know if you or someone you know would be interested. Send in your resume ASAP. Send in resume along with LinkedIn profile without which applications will not be considered. Call us NOW!\n\nJob Title: Solution Architect\nJob Location: Hyderabad, India\n\nResponsibilities\nInterprets and delivers impactful strategic plans improving data integration, data quality, and data delivery in support of business initiatives and roadmaps\nDesigns the structure and layout of data systems, including databases, warehouses, and lakes\nSelects and implements database management systems that meet the organizations needs by defining data schemas, optimizing data storage, and establishing data access controls and security measures\nDefines and implements the long-term technology strategy and innovations roadmaps across analytics, data engineering, and data platforms\nDesigns and implements processes for the ETL process from various sources into the organizations data systems\nTranslates high-level business requirements into data models and appropriate metadata, test data, and data quality standards\nManages senior business stakeholders to secure strong engagement and ensures that the delivery of the project aligns with longer-term strategic roadmaps\nSimplifies the existing data architecture, delivering reusable services and cost-saving opportunities in line with the policies and standards of the company\nLeads and participates in the peer review and quality assurance of project architectural artifacts across the EA group through governance forums\nDefines and manages standards, guidelines, and processes to ensure data quality\nWorks with IT teams, business analysts, and data analytics teams to understand data consumers needs and develop solutions\nEvaluates and recommends emerging technologies for data management, storage, and analytics\n\nJob Requirements\nBachelor's degree in Computer Science, Information Sciences or related discipline and 5 - 8 years of relevant experience (ex: IT solutions architecture, enterprise architecture, and systems & application design) or 12 -15 years or related experience\nBroad technical expertise in at least one area, such as application development, enterprise applications or IT systems engineering\nExcellent communications skills - Able to effectively communicate highly technical information in non-technical terminology (written and verbal)\nExpert in change management principles associated with new technology implementations\nDeep understanding of project management principles\n\nPreferred Qualifications\nStrong understanding of Azure cloud services\nDevelop and maintain strong relationships with various business areas and IT Teams to understand their needs and challenges.\nProactively identify opportunities for collaboration and engagement across IT Teams.\nAt least five years of relevant experience in design and implementation of data models for enterprise data warehouse initiatives\nExperience leading projects involving data warehousing, data modeling, and data analysis\nDesign experience in Azure Databricks, PySpark, and Power BI/Tableau\nStrong ability in programming languages such as Java, Python, and C/C++\nAbility in data science languages/tools such as SQL, R, SAS, or Excel\nProficiency in the design and implementation of modern data architectures and concepts such as cloud services (AWS, Azure, GCP), real-time data distribution (Kafka, Dataflow), and modern data warehouse tools (Snowflake, Databricks)\nExperience with database technologies such as SQL, NoSQL, Oracle, Hadoop, or Teradata\nUnderstanding of entity-relationship modeling, metadata systems, and data quality tools and techniques\nAbility to think strategically and relate architectural decisions and recommendations to business needs and client culture\nAbility to assess traditional and modern data architecture components based on business needs\nExperience with business intelligence tools and technologies such as ETL, Power BI, and Tableau\nAbility to regularly learn and adopt new technology, especially in the ML/AI realm\nStrong analytical and problem-solving skills\nAbility to synthesize and clearly communicate large volumes of complex information to senior management of various technical understandings\nAbility to collaborate and excel in complex, cross-functional teams involving data scientists, business analysts, and stakeholders\nAbility to guide solution design and architecture to meet business needs.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Java', 'Azure Cloud', 'ETL', 'Python', 'SQL', 'Power Bi', 'AWS', 'Data warehouse']",2025-06-10 15:20:55
Senior PL/SQL Developer,Inspira Enterprise India,7 - 10 years,Not Disclosed,['Kolkata'],"The Senior PL/SQL SAS Mart Developer (BFSI) will be a crucial member of the data warehousing team, responsible for the design, development, and maintenance of data marts specifically tailored for the Banking, Financial Services, and Insurance (BFSI) domain. This role requires a strong combination of deep PL/SQL development skills for data transformation and manipulation within an Oracle environment, coupled with expertise in SAS for data mart creation, reporting, and analytical purposes. The successful candidate will leverage their extensive experience in the BFSI sector to build efficient, robust, and compliant data marts that support critical business intelligence, reporting, and analytical needs.\n\nResponsibilities:\n\n\nData Mart Design and Development: Design and develop dimensional and multi-dimensional data marts based on business requirements within the BFSI context, utilizing both PL/SQL for backend data processing and SAS for data mart structuring and access.\n\nPL/SQL Development for Data Marts: Utilize advanced PL/SQL skills to extract, transform, and load data from the central data warehouse or source systems into the designated data marts, ensuring data quality, performance, and adherence to BFSI data standards. This includes developing complex stored procedures, functions, packages, and triggers.\n\nSAS Data Mart Implementation: Leverage SAS tools (e. g. , SAS Data Integration Studio, SAS Enterprise Guide, SAS OLAP Cube Studio) to structure, populate, and manage data marts for efficient reporting and analysis.\n\nBFSI Data Expertise: Apply a strong understanding of BFSI data models, key performance indicators (KPIs), regulatory reporting requirements (e. g. , BASEL, RBI, Solvency II), and common analytical needs within the financial services and insurance industries to design relevant and effective data marts.\n\nPerformance Optimization: Optimize PL/SQL code and SAS processes related to data mart development and access to ensure high performance and efficient query execution for reporting and analytical tools.\n\nData Quality and Governance: Implement and enforce data quality checks within both PL/SQL and SAS processes to ensure the accuracy, consistency, and integrity of data within the data marts, adhering to the banks data governance policies and BFSI-specific data quality standards.\n\nTroubleshooting and Support: Investigate and resolve issues related to data mart performance, data accuracy, and accessibility, involving both PL/SQL and SAS components. Provide expert-level support for data mart users.\n\nDocumentation: Create and maintain comprehensive technical documentation for data mart designs, PL/SQL code, SAS jobs, data mappings, and user guides, ensuring compliance with BFSI documentation standards.\n\nCollaboration: Work closely with business analysts, report developers, data scientists, and other stakeholders within the BFSI departments to understand their data mart requirements and deliver solutions that meet their analytical and reporting needs.\n\nSecurity and Compliance: Ensure that data marts and the processes used to build and access them comply with the banks security policies and relevant BFSI regulatory requirements regarding data access and privacy.\n\nMentoring: Provide technical guidance and mentorship to junior developers on PL/SQL and SAS skills related to data mart development within the BFSI domain.\n\n\nRequired Skills and Experience:\n\n\nBachelors degree in Computer Science, Information Technology, Statistics, Economics, Finance, or a related field.\n\nProven experience of 7-10 years in developing data warehousing and business intelligence solutions, with a significant focus on data mart development within the Banking, Financial Services, and Insurance (BFSI) sector.\n\nExtensive and demonstrable expertise in PL/SQL development, including advanced querying, stored procedures, functions, packages, and performance tuning within an Oracle environment.\n\nStrong proficiency in SAS programming and experience using SAS tools for data mart creation and management (e. g. , SAS Data Integration Studio, SAS Enterprise Guide, SAS OLAP Cube Studio, SAS Metadata Server).\n\nDeep understanding of BFSI data models, common KPIs, and regulatory reporting requirements (e. g. , BASEL, RBI guidelines, Solvency II, IFRS).\n\nStrong SQL skills and experience working with relational databases, particularly Oracle.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Performance tuning', 'PLSQL', 'OLAP', 'Stored procedures', 'Oracle', 'Business intelligence', 'Troubleshooting', 'Information technology', 'SQL']",2025-06-10 15:20:58
UI/UX Engineer,Yotta Infrastructure,3 - 7 years,Not Disclosed,['Mumbai'],"End-to-End Product Design:\nOwn the design lifecycle for complex featuresfrom research and ideationthrough to prototyping, high-fidelity UI, developer handoff, and qualityassurance.\nUser Research Persona Development:\nConduct stakeholder interviews, contextual inquiry, usability studies, journeymapping, and persona building for diverse enterprise user roles (developers,analysts, business users, etc.).\nWireframing Prototyping:\nTranslate product requirements into user flows, wireframes, interactiveprototypes, and design system components in Figma (or similar tools).\n\nDevelop and evolve a scalable design system covering atomic/componentstructures, documentation, accessibility, and theming (multi-brand, light/darkmodes, etc.).\n\nPartner with Product Managers, Engineers, Data Scientists, and QA to ensureseamless implementation, resolving gaps between UX vision and technicalfeasibility.\n\nDesign advanced interfaces for AI agent orchestration, workflow builders(drag-and drop,node/graph UI), model monitoring, data visualization, and resultexplainability.\n\nChampion accessibility best practices (WCAG 2.1+, keyboard navigation, ARIA),ensuring products are usable by all users, including those with disabilities.\n\nLead rapid design iterations for MVP launches, gather and incorporate userfeedback, and iterate quickly based on analytics and usage patterns.\n\nPlan, conduct, and analyze remote and in-person usability tests; drivecontinuous product and UX improvements using research and data.\n\nMentor junior and mid-level designers, participate in hiring, and help nurturea high-performance design culture.\n\n\nExperience:\no 9+ years in UI/UX/Product Design, with at least 3 years in enterpriseSaaS, workflow, or data/AI product environments.\no Demonstrated leadership in large-scale product launches and/or design systemimplementation.\n\nPortfolio:\no Strong portfolio showcasing complex B2B, SaaS, AI/ML, or workflow automationproducts.\no Evidence of solving highly technical problems through user-centered design.\n\nDesign Tools:\no Mastery of Figma, Adobe Creative Suite, Sketch, prototyping, and usabilitytesting tools (e.g., Maze, UsabilityHub, Lookback.io).\n\nMethodologies:\no Deep understanding of design thinking, user research, informationarchitecture, interaction design, and accessibility.\n\nTechnical Understanding:\no Familiarity with front-end technologies (HTML/CSS, design tokens, componentlibraries).\no Experience designing for complex state management, data visualization, andcollaborative multi-user environments.\n\nSoft Skills:\no Excellent communication, stakeholder management, workshop facilitation, andcross-functional collaboration.\n\n\n\nExperience in AI/ML platforms, agent orchestration tools, or no-code/low-codeproducts.\nBuilding or evolving white-label design systems.\nExperience with internationalization / localization , SOC2/GDPR compliance, andhigh scalemulti-tenant SaaS.\nExposure to agile teams, rapid prototyping, and A/B experimentation. WhatYoull Get\nThe chance to help define and build market-leading AI/ML platforms from theground up.\nA collaborative environment with passionate experts in AI, data science, andproduct engineering.\nOpportunities for professional growth, conference participation, andmentoring.\nCompetitive salary and benefits, plus the freedom to influence productdirection at a high level.",,,,"['css', 'ux', 'aiml', 'facilitation', 'artificial intelligence', 'user research', 'product design', 'b2b', 'ui', 'saas', 'stakeholder management', 'figma', 'enterprise portal', 'html', 'interaction design', 'information architecture', 'design thinking', 'ml', 'communication skills', 'ui/ux']",2025-06-10 15:21:00
Sr Data Architect,HMH,5 - 10 years,Not Disclosed,['Pune'],"The data architect is responsible for designing, creating, and managing an organizations data architecture. This role is critical in establishing a solid foundation for data management within an organization, ensuring that data is organized, accessible, secure, and aligned with business objectives. The data architect designs data models, warehouses, file systems and databases, and defines how data will be collected and organized.\nResponsibilities\nInterprets and delivers impactful strategic plans improving data integration, data quality, and data delivery in support of business initiatives and roadmaps",,,,"['Data Architecture', 'Java', 'Azure', 'data analysis', 'SAS', 'PySpark', 'Hadoop', 'Azure Databricks', 'data warehousing', 'Teradata', 'SQL', 'C/C++', 'Power BI/Tableau', 'R', 'NoSQL', 'data modeling', 'GCP', 'Snowflake', 'metadata systems', 'Databricks', 'Oracle', 'AWS', 'Python']",2025-06-10 15:21:02
Ml Engineer,Ltimindtree,6 - 9 years,Not Disclosed,"['Pune', 'Bengaluru', 'Mumbai (All Areas)']","Job Title: Data Scientist\n\nLooking for someone with 5-8 years of experience manipulating data sets and building statistical models\n\nDesired Skills:\n\nExperience using statistical computer languages (R, Python, etc.) to manage data and draw insights from large data sets.\nKnowledge of a variety of machine learning techniques (clustering, decision tree learning, artificial neural networks, XGBoost, KNN, SVM, ANN, etc.).\nKnowledge of advanced statistical techniques and concepts (regression, properties of distributions, statistical tests and proper usage, etc.) and experience with applications.\nKnowledge and experience in statistical and data mining techniques: GLM/Regression, Random Forest, Boosting, Trees, text mining, social network analysis, etc.\nExperience creating and using advanced machine learning algorithms and statistics: regression, simulation, scenario analysis, modelling, clustering, decision trees, neural networks, etc.\nExperience visualizing/presenting data for stakeholders.\nExperience with Snowflake will be an added advantage.\nExperience in deployment of machine learning models using cloud technologies.\n\nRoles and responsibilities:\n\n1. Work with stakeholders to identify opportunities for leveraging data to drive business solutions.\n2. Mine and analyse data from databases to drive optimization and improvement of product development and business strategies.\n3. Assess the effectiveness and accuracy of new data sources and data gathering techniques.\n4. Develop custom models and algorithms to apply to data sets.\n5. Use predictive modelling to increase and optimize customer experiences.\n6. Coordinate with different functional teams to implement models and monitor outcomes.\n7. Analyse large amounts of information to discover trends and patterns.\n8. Build predictive models and machine-learning algorithms.\n9. Present information using data visualization techniques .\n10. Good to have a knowledge of ML lifecycle and model governance.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Machine Learning', 'R', 'Python']",2025-06-10 15:21:04
Analyst,Merkle B2b,0 - 2 years,Not Disclosed,['Mumbai'],"Data Validation (DV) Specialist (Using SPSS) - Analyst\nJob Description:\nCore Responsibilities:\nPerform data quality checks and validation on market research datasets\nDevelop and execute scripts and automated processes to identify data anomalies.\nCollaborate with the Survey Programming team to review survey questionnaires and make recommendations for efficient programming and an optimal layout that enhances user experience.\nInvestigate and document data discrepancies, working with survey programming team/data collection vendors as needed.\nCreate and maintain detailed data documentation and validation reports.\nCollaborate with Survey Programmers and internal project managers to understand data processing requirements and provide guidance on quality assurance best practices.\nProvide constructive feedback and suggestions for improving the quality of data, aiming to enhance overall survey quality.\nAutomate data validation processes where possible to enhance efficiency and reduce time spent on repetitive data validation tasks.\nMaintain thorough documentation of findings and recommendations to ensure transparency and consistency in quality practices.\nActively participate in team meetings to discuss project developments, quality issues, and improvement strategies, fostering a culture of continuous improvement.\nQualification:\nBachelor s degree in computer science, Information Technology, Statistics, or a related field.\nAt least 2+ years of experience in data validation process.\nFamiliar with data validation using SPSS, Dimension, Quantum platform or similar tools\nA proactive team player who thrives in a fast-paced environment and enjoys repetitive tasks that contribute to project excellence.\nProgramming knowledge in a major programming language such as R, JavaScript, or Python, with an interest in building automation scripts for data validation.\nExcellent problem-solving skills and a willingness to learn innovative quality assurance methodologies.\nA desire for continuous improvement in processes, focusing on creating efficiencies that lead to scalable and high-quality data processing outcomes.\nLocation:\nMumbai\nBrand:\nMerkle\nTime Type:\nFull time\nContract Type:\nPermanent",Industry Type: Management Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Automation', 'Manager Quality Assurance', 'Javascript', 'Data collection', 'Market research', 'Data processing', 'Data quality', 'SPSS', 'Continuous improvement', 'Information technology']",2025-06-10 15:21:07
Internal Audit-Data Strategy- Associate,Goldman Sachs,1 - 3 years,Not Disclosed,['Hyderabad'],"Internal Audit - Data Strategy - Associate - Hyderabad\nWhat We Do\nInternal Audit s mission is to independently assess the firm s internal control structure, including the firm s governance processes and controls, risk management, capital and anti-financial crime framework. In addition, it is also to raise awareness of control risk and monitor the implementation of management s control measures. In doing so, internal Audit:\nCommunicates and reports on the effectiveness of the firm s governance, risk management and controls that mitigate current and evolving risk\nRaise awareness of control risk\nAssesses the firm s control culture and conduct risks; and\nMonitors management s implementation of control measures\nGoldman Sachs Internal Audit comprises individuals from diverse backgrounds including chartered accountants, developers, risk management professionals, cybersecurity professionals, and data scientists. We are organized into global teams comprising business and technology auditors to cover all the firm s businesses and functions, including securities, investment banking, consumer and investment management, risk management, finance, cyber-security and technology risk, and engineering.\nWho We Look For\nGoldman Sachs Internal Auditors demonstrate a strong risk, control and analytical mindset, exercise professional skepticism and challenge status quo on risks and control measures effectively with management. We look for individuals who enjoy learning about audit, businesses, and processes, have innovative and creative mindset in adapting analytical techniques to enhance audit function, develop teamwork and build relationships and are able to evolve and thrive in a fast-paced global environment.\nYour Impact\nAs part of the third line of defense, you will be involved in independently assessing the firm s overall control environment and its effectiveness as it relates to current and emerging risks and communicating the results to local/ global management. In doing so, you will be supporting the provision of independent, objective and timely assurance around the firm s internal control structure, thereby supporting the Audit Committee, Board of Directors and Risk Committee in fulfilling their oversight responsibilities. We are looking for a strong data scientist, passionate about using data to challenge the norm, to join our Embed Data Analytics team. The candidate will work closely with the audit teams to build innovative and reusable analytical tools that will help make audit testing more efficient and provide meaningful insights into firm s control environment.\nThis position description is intended to describe the duties most frequently performed by an individual in this position. It is not intended to be a complete list of assigned duties but to describe a position level. The role shall be performed within a professional office environment. Goldman Sachs has health and safety polices that are available for all workers upon request. There are no specific health risks associate with therole.\nResponsibilities\nExecute on DA strategy developed by IA management within the context of audit responsibilities, such as risk assessment, audit planning, creation of reusable tools and providing innovative solutions to complex problems\nPartner with audit teams to help identify risks associated with businesses and facilitate strategic data sourcing and develop innovative solutions to increase efficiency and effectiveness of audit testing\nBuild production ready analytical tools to automate repeatable and reusable processes within IA\nBuild and manage relationships and communications with Audit team members\nBasic Qualifications\n1-3 years of experience with a minimum of Bachelor s in Computer Science, Math, or Statistics\nExperience with RDBMS/ SQL\nProficiency in programming languages, such as Python, Java, or C++\nKnowledge of basic statistics, including descriptive statistics, data distribution models,\nTime Series Analysis, correlation, and regression, and its application to data\nStrong team player with excellent communication skills (written and oral). Ability to communicate what is relevant and important in a clear and concise manner and ability to handle multiple tasks\nStrong contributing member of Data Science team and help build analytical capabilities for Internal Audit Division\nDriven and motivated and constantly taking initiative to improve performance\nPreferred Qualifications\nExperience with advanced data analytics tools and techniques\nFamiliarity with text analytics and NLP using python\nFamiliarity with machine learning algorithms and exposure to supervised and unsupervised learning - Linear/Logistic Regression, SVM, Random Forest and Boosting,\nClustering and Patterns Recognition techniques\nExperience with analytical/ statistical programs such as SAS, SPSS, and R\nExperience with visualization tools (Spotfire, Qlikview or Tableau) is a plus\nCreativity/Innovation, i.e., ability to create new ways to improve current processes and develop practical solutions that add value to department\n\nABOUT GOLDMAN SACHS",Industry Type: Banking,Department: Risk Management & Compliance,"Employment Type: Full Time, Permanent","['Computer science', 'C++', 'SAS', 'RDBMS', 'Investment banking', 'SPSS', 'QlikView', 'Risk management', 'SQL', 'Python']",2025-06-10 15:21:09
Interview | AI | 10th June'25 | Redserv(Redington Group),Redserv Global Solutions,2 - 4 years,Not Disclosed,['Chennai'],"Dear Candidates\n\nAre you looking for opportunities ?\n\nIf yes, read below to know more about an opportunity in Redserv (Redington Group), one of the leading Supply Chain Management organizations!\n\nRole hiring for: Artificial Intelligence",,,,"['Artificial Intelligence', 'Python', 'openAI', 'Large Language Model', 'Natural Language Processing', 'GPT', 'LLM', 'Machine Learning', 'NLM', 'Deep Learning']",2025-06-10 15:21:12
Internal Audit-Data Strategy - Analyst,Goldman Sachs,1 - 3 years,Not Disclosed,['Hyderabad'],"What We Do\nInternal Audit s mission is to independently assess the firm s internal control structure, including the\nfirm s governance processes and controls, risk management, capital and anti-financial crime\nframework. In addition, it is also to raise awareness of control risk and monitor the implementation of\nmanagement s control measures.\nIn doing so, internal Audit:\nCommunicates and reports on the effectiveness of the firm s governance, risk management and controls that mitigate current and evolving risk\nRaise awareness of control risk\nAssesses the firm s control culture and conduct risks; and\nMonitors management s implementation of control measures\nGoldman Sachs Internal Audit comprises individuals from diverse backgrounds including chartered\naccountants, developers, risk management professionals, cybersecurity professionals, and data\nscientists. We are organized into global teams comprising business and technology auditors to cover\nall the firm s businesses and functions, including securities, investment banking, consumer and\ninvestment management, risk management, finance, cyber-security and technology risk, and\nengineering.\nWho We Look For\nGoldman Sachs Internal Auditors demonstrate a strong risk, control and analytical mindset, exercise\nprofessional skepticism and challenge status quo on risks and control measures effectively with\nmanagement. We look for individuals who enjoy learning about audit, businesses, and processes,\nhave innovative and creative mindset in adapting analytical techniques to enhance audit function,\ndevelop teamwork and build relationships and are able to evolve and thrive in a fast-paced global\nenvironment.\nEmbedded - Data Analytics\nIn Internal Audit, we ensure that Goldman Sachs maintains effective controls by assessing the\nreliability of financial reports, monitoring the firm s compliance with laws and regulations, and\nadvising management on developing smart control solutions. Embed Data Analytics team leverages\nits programming and analytical capabilities to build innovative data driven solutions. The team works\nclosely with auditors to understand their pain points and develop data-centric solutions to address\nthe same.\nYour Impact\nAs part of the third line of defense, you will be involved in independently assessing the firm s overall\ncontrol environment and its effectiveness as it relates to current and emerging risks and\ncommunicating the results to local/ global management. In doing so, you will be supporting the\nprovision of independent, objective and timely assurance around the firm s internal control structure,\nthereby supporting the Audit Committee, Board of Directors and Risk Committee in fulfilling their\noversight responsibilities.\nWe are looking for a strong data scientist, passionate about using data to challenge the norm, to join\nour Embed Data Analytics team. The candidate will work closely with the audit teams to build\ninnovative and reusable analytical tools that will help make audit testing more efficient and provide\nmeaningful insights into firm s control environment.\nResponsibilities\nExecute on DA strategy developed by IA management within the context of audit responsibilities, such as risk assessment, audit planning, creation of reusable tools and providing innovative solutions to complex problems\nPartner with audit teams to help identify risks associated with businesses and facilitate strategic data sourcing and develop innovative solutions to increase efficiency and effectiveness of audit testing\nBuild production ready analytical tools to automate repeatable and reusable processes within IA\nBuild and manage relationships and communications with Audit team members\nBasic Qualifications\n1-3 years of experience with a minimum of Bachelor s in Computer Science, Math, or Statistics\nExperience with RDBMS/ SQL\nProficiency in programming languages, such as Python, Java, or C++\nKnowledge of basic statistics, including descriptive statistics, data distribution models,\nTime Series Analysis, correlation, and regression, and its application to data\nStrong team player with excellent communication skills (written and oral). Ability to communicate what is relevant and important in a clear and concise manner and ability to handle multiple tasks\nStrong contributing member of Data Science team and help build analytical capabilities for Internal Audit Division\nDriven and motivated and constantly taking initiative to improve performance\nPreferred Qualifications\nExperience with advanced data analytics tools and techniques\nStrong experience in RDBMS/ SQL and Data Warehousing.\nExposure to ETL Processes and Data Engineering.\nExperience in implementing Data Quality measures and entitlement models\nFamiliarity in programming languages such as Python.\nStrong team player with excellent communication skills (written and oral). Ability to communicate what is relevant and important in a clear and concise manner and ability to handle multiple tasks\nSelf-driven and motivated to take up initiatives to improve our processes.\nWe re committed to finding reasonable accommodations for candidates with special needs or disabilities during our recruiting process. Learn more: https: / / www.goldmansachs.com / careers / footer / disability-statement.html",Industry Type: Banking,Department: Risk Management & Compliance,"Employment Type: Full Time, Permanent","['C++', 'Manager Internal Audit', 'Assurance', 'Analytical', 'Data analytics', 'HTML', 'Data quality', 'Investment banking', 'Risk management', 'SQL']",2025-06-10 15:21:14
Partial Renewal Analyst,IBM,0 - 3 years,Not Disclosed,['Bengaluru'],"Partial Project Office Analyst-The role is responsible for managing partial requests (partial drops in Subscription and Support (S&S) licenses), conducting deployment analysis, and providing comprehensive support for our customers’ licensing needs.\n\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nBachelor's Degree\n\nRequired technical and professional expertise\n\n\n SAM Tool Data Analysis:  Analyze and summarize data from SAM tools (including IBM and third-party tools) to validate software usage for partial downturn requests, ensuring adherence to software licensing requirements and IBM terms and conditions. This involves examining hardware information for processor-based products, as well as user, server, and installation data for non-processor-based products.\n Licensing Knowledge:  Serve as a Subject Matter Expert for IBM licensing to ensure that customers adhere to the SW licensing requirements. This includes mastering IBM’s licensing models, product relationships, and bundles.\n Deployment Comparison:  Compare customer software deployment with entitlements to provide suggestions for renewals, reinstatements, and new licenses purchases.Ensure deliverables and renewal decisions adheres to process requirements and guidelines, to mitigate risks and reporting errors.\n Entitlement Reconciliation:  Validate entitlement data in the IBM entitlement systems for accurate entitlement quantities and identify potential opportunities for additional license purchases.\n Communication and Engagement : Manage Partial Renewal Outlook accounts and Slack channel to ensure timely processing and support of partial requests.\n Documentation:  Track and document key account information across various request types.\n Support ad hoc requests:  such as answering IBM licensing questions related to a renewal, researching product manuals for licensing guidance, providing S&A licensing support, etc.\n Speed of Execution:  Perform the tasks in line with the expected turnaround time\n\n\nPreferred technical and professional experience\nExperience with software licensing and analyzing data from SAM tools, including but not limited to ILMT, BigFix, SCCM, Flexera, ServiceNow, SNOW, etc.\nAbility to understand knowledge of IBM’s licensing rules and navigating through IBM’s License Information website.\nAbility to analyze system generated data for IBM products for both PVU and non-PVU licensing\nProvide strategic guidance and technical assistance for IBM licensing and remediation efforts (e.g., answer technical ILMT questions, directing customers to support to address needs, provide feedback on licensing issues).\nDesirable experience in a client-facing role, with the ability to manage multiple clients simultaneously and addressing high-visibility requests with executive exposure.\nAbility to collaborate with multiple account teams and organizations within IBM.\nExceptional oral and written communication skills, with the ability to convey technical information to non-technical stakeholders.\nDemonstrated ability to quickly learn and adapt to new tools, processes, and technologies.\nProficiency with Microsoft Office products, particularly Microsoft Excel and PowerPoint.\nDesirable to have the IBM Licensing Expertise badge\nFluent English",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analysis', 'ms office products', 'linux', 'licensing', 'unix', 'tcp', 'financial analysis', 'erp', 'mis reporting', 'investment banking', 'software asset management', 'dbms', 'autocad', 'sql', 'servicenow', 'ms office outlook', 'microsoft windows']",2025-06-10 15:21:17
Assistant Manager - Data Engineer,Ajanta Pharma,5 - 10 years,6-12 Lacs P.A.,['Mumbai (All Areas)'],"Position Overview:\nThe Assistant Manager - Data Engineer will play a pivotal role in the design, development, and maintenance of data pipelines that ensure the efficiency, scalability, and reliability of our data infrastructure. This role will involve optimizing and automating ETL/ELT processes, as well as developing and refining databases, data warehouses, and data lakes. As an Assistant Manager, you will also mentor junior engineers and collaborate closely with cross-functional teams to support business goals and drive data excellence.\n\nKey Responsibilities:\nData Pipeline Development: Design, build, and maintain efficient, scalable, and reliable data pipelines to support data analytics, reporting, and business intelligence initiatives.\nDatabase and Data Warehouse Management: Develop, optimize, and manage databases, data warehouses, and data lakes to enhance data accessibility and business decision-making.\nETL/ELT Optimization: Automate and optimize data extraction, transformation, and loading (ETL/ELT) processes, ensuring efficient data flow and improved system performance.\nData Modeling & Architecture: Develop and maintain data models to enable structured data storage, analysis, and reporting in alignment with business needs.\nWorkflow Management Systems: Implement, optimize, and maintain workflow management tools (e.g., Apache Airflow, Talend) to streamline data engineering tasks and improve operational efficiency.\nTeam Leadership & Mentorship: Guide, mentor, and support junior data engineers to enhance their skills and contribute effectively to projects.\nCollaboration with Cross-Functional Teams: Work closely with data scientists, analysts, business stakeholders, and IT teams to understand requirements and deliver solutions that align with business objectives.\nPerformance Optimization: Continuously monitor and optimize data pipelines and storage solutions to ensure maximum performance and cost efficiency.\nDocumentation & Process Improvement: Create and maintain documentation for data models, workflows, and systems. Contribute to the continuous improvement of data engineering practices.\n\nQualifications:\nEducational Background: B.E., B.Tech., MCA\nProfessional Experience: At least 5 to 7 years of experience in a data engineering or similar role, with hands-on experience in building and optimizing data pipelines, ETL processes, and database management.\nTechnical Skills:\nProficiency in Python and SQL for data processing, transformation, and querying.\nExperience with modern data warehousing solutions (e.g., Amazon Redshift, Snowflake, Google BigQuery, Azure Data Lake).\nStrong background in data modeling (dimensional, relational, star/snowflake schema).\nHands-on experience with ETL tools (e.g., Apache Airflow, Talend, Informatica) and workflow management systems.\nFamiliarity with cloud platforms (AWS, Azure, Google Cloud) and distributed data processing frameworks (e.g., Apache Spark).\nData Visualization & Exploration: Familiarity with data visualization tools (e.g., Tableau, Power BI) for analysis and reporting.\nLeadership Skills: Demonstrated ability to manage and mentor a team of junior data engineers while fostering a collaborative and innovative work environment.\nProblem-Solving & Analytical Skills: Strong analytical and troubleshooting skills with the ability to optimize complex data systems for performance and scalability.\nExperience in Pharma/Healthcare (preferred but not required): Knowledge of the pharmaceutical industry and experience with data in regulated environments.\nDesired Skills:\nFamiliarity with industry-specific data standards and regulations.\nExperience working with machine learning models or data science pipelines is a plus.\nStrong communication skills with the ability to present technical data to non-technical stakeholders.\n\nWhy Join Us:\nImpactful Work: Contribute to the pharmaceutical industry by improving data-driven decisions that impact public health.\nCareer Growth: Opportunities to develop professionally in a fast-growing industry and company.\nCollaborative Environment: Work with a dynamic and talented team of engineers, data scientists, and business stakeholders.\nCompetitive Benefits: Competitive salary, health benefits and more.",Industry Type: Pharmaceutical & Life Sciences,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Data Pipeline Development', 'ELT optimisation', 'Data Visualization', 'Data Warehouse Management', 'Data Modeling', 'Workflow Management Systems', 'Data Warehousing', 'ETL', 'machine learning', 'Google Cloud Platforms', 'Python', 'azure']",2025-06-10 15:21:19
Data Engineer,Truefirms,6 - 9 years,7-14 Lacs P.A.,['Hyderabad'],"Role Overview:\nWe are seeking a talented and forward-thinking Data Engineer for one of the large financial services GCC based in Hyderabad with responsibilities that include designing and constructing data pipelines, integrating data from multiple sources, developing scalable data solutions, optimizing data workflows, collaborating with cross-functional teams, implementing data governance practices, and ensuring data security and compliance.\n\nTechnical Requirements:\n\n1. Proficiency in ETL, Batch, and Streaming Process\n2. Experience with BigQuery, Cloud Storage, and CloudSQL\n3. Strong programming skills in Python, SQL, and Apache Beam for data processing\n4. Understanding of data modeling and schema design for analytics\n5. Knowledge of data governance, security, and compliance in GCP\n6. Familiarity with machine learning workflows and integration with GCP ML tools\n7. Ability to optimize performance within data pipelines\n\nFunctional Requirements:\n\n1. Ability to collaborate with Data Operations, Software Engineers, Data Scientists, and Business SMEs to develop Data Product Features\n2. Experience in leading and mentoring peers within an existing development team\n3. Strong communication skills to craft and communicate robust solutions\n4. Proficient in working with Engineering Leads, Enterprise and Data Architects, and Business Architects to build appropriate data foundations\n5. Willingness to work on contemporary data architecture in Public and Private Cloud environments T\nhis role offers a compelling opportunity for a seasoned Data Engineering to drive transformative cloud initiatives within the financial sector, leveraging unparalleled experience and expertise to deliver innovative cloud solutions that align with business imperatives and regulatory requirements.\nQualification Engineering Grad / Postgraduate\nCRITERIA\n1. Proficient in ETL, Python, and Apache Beam for data processing efficiency.\n2. Demonstrated expertise in BigQuery, Cloud Storage, and CloudSQL utilization.\n3. Strong collaboration skills with cross-functional teams for data product development.\n4. Comprehensive knowledge of data governance, security, and compliance in GCP.\n5. Experienced in optimizing performance within data pipelines for efficiency.\n6. Relevant Experience: 6-9 years\n\nConnect at 9993809253",Industry Type: IT Services & Consulting,Department: Engineering - Hardware & Networks,"Employment Type: Full Time, Permanent","['Cloud Storage', 'GCC', 'ETL Tool', 'Gcp Cloud', 'integration with GCP ML tools', 'Bigquery', 'Apache Beam', 'Data Modeling', 'Python']",2025-06-10 15:21:21
Senior Manager Information Systems,Horizon Therapeutics,8 - 18 years,Not Disclosed,['Hyderabad'],"Career Category Information Systems Job Description\nJoin Amgen s Mission of Serving Patients\nAt Amgen, if you feel like you re part of something bigger, it s because you are. Our shared mission to serve patients living with serious illnesses drives all that we do.\nSince 1980, we ve helped pioneer the world of biotech in our fight against the world s toughest diseases. With our focus on four therapeutic areas -Oncology, Inflammation, General Medicine, and Rare Disease- we reach millions of patients each year. As a member of the Amgen team, you ll help make a lasting impact on the lives of patients as we research, manufacture, and deliver innovative medicines to help people live longer, fuller happier lives.\nOur award-winning culture is collaborative, innovative, and science based. If you have a passion for challenges and the opportunities that lay within them, you ll thrive as part of the Amgen team. Join us and transform the lives of patients while transforming your career.\nSenior Manager Information Systems\nWhat you will do\nLet s do this. Let s change the world. n this vital role you will develop an insight driven sensing capability with a focus on revolutionizing decision making. In this role you will lead the technical delivery for this capability as part of a team data engineers and software engineers. The team will rely on your leadership to own and refine the vision, feature prioritization, partner alignment, and experience leading solution delivery while building this ground-breaking new capability for Amgen. You will drive the software engineering side of the product release and will deliver for the outcomes.\nRoles Responsibilities:\nLead delivery of overall product and product features from concept to end of life management of the product team comprising of technical engineers, product owners and data scientists to ensure that business, quality, and functional goals are met with each product release\nDrives excellence and quality for the respective product releases, collaborating with Partner teams.\nImpacts quality, efficiency and effectiveness of own team. Has significant input into priorities.\nIncorporate and prioritize feature requests into product roadmap; Able to translate roadmap into execution\nDesign and implement usability, quality, and delivery of a product or feature\nPlan releases and upgrades with no impacts to business\nHands on expertise in driving quality and best in class Agile engineering practices\nEncourage and motivate the product team to deliver innovative and exciting solutions with an appropriate sense of urgency\nManages progress of work and addresses production issues during sprints\nCommunication with partners to make sure goals are clear and the vision is aligned with business objectives\nDirect management and staff development of team members\nWhat we expect of you\nWe are all different, yet we all use our unique contributions to serve patients.\nBasic Qualifications:\nMaster s degree and 8 to 10 years of Information Systems experience OR\nBachelor s degree and 10 to 14 years ofInformation Systems experience OR\nDiploma and 14 to 18 years of Information Systems experience\nThorough understanding of modern web application development and delivery, Gen AI applications development, Data integration and enterprise data fabric concepts, methodologies, and technologies e. g. AWS technologies, Databricks\nDemonstrated experience in building strong teams with consistent practices.\nDemonstrated experience in navigating matrix organization and leading change.\nPrior experience writing business case documents and securing funding for product team delivery; Financial/Spend management for small to medium product teams is a plus.\nIn-depth knowledge of Agile process and principles.\nDefine success metrics for developer productivity metrics; on a monthly/quarterly basis analyze how the product team is performing against established KPI s.\nFunctional Skills:\nLeadership:\nInfluences through Collaboration : Builds direct and behind-the-scenes support for ideas by working collaboratively with others.\nStrategic Thinking : Anticipates downstream consequences and tailors influencing strategies to achieve positive outcomes.\nTransparent Decision-Making : Clearly articulates the rationale behind decisions and their potential implications, continuously reflecting on successes and failures to enhance performance and decision-making.\nAdaptive Leadership : Recognizes the need for change and actively participates in technical strategy planning.\nPreferred Qualifications:\nStrong influencing skills, influence stakeholders and be able to balance priorities.\nPrior experience in vendor management.\nPrior hands-on experience leading full stack development using infrastructure cloud services (AWS preferred) and cloud-native tools and design patterns (Containers, Serverless, Docker, etc. )\nExperience with developing solutions on AWS technologies such as S3, EMR, Spark,\nAthena, Redshift and others\nFamiliarity with cloud security (AWS /Azure/ GCP)\nConceptual understanding of DevOps tools (Ansible/ Chef / Puppet / Docker /Jenkins)\nProfessional Certifications\nAWS Certified Solutions Architect (preferred)\nCertified DevOps Engineer (preferred)\nCertified Agile Leader or similar (preferred)\nSoft Skills:\nStrong desire for continuous learning to pick new tools/technologies.\nHigh attention to detail is essential with critical thinking ability.\nShould be an active contributor on technological communities/forums\nProactively engages with cross-functional teams to resolve issues and design solutions using critical thinking and analysis skills and best practices.\nInfluences and energizes others toward the common vision and goal. Maintains excitement for a process and drives to new directions of meeting the goal even when odds and setbacks render one path impassable\nEstablished habit of proactive thinking and behavior and the desire and ability to self-start/learn and apply new technologies\nExcellent organizational and time-management skills\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nTeam-oriented, with a focus on achieving team goals\nStrong presentation and public speaking skills.\nShift Information:\nThis position requires you to work a later shift and may be assigned a second or third shift schedule. Candidates must be willing and able to work during evening or night shifts, as required based on business requirements.\nWhat you can expect of us\nAs we work to develop treatments that take care of others, we also work to care for your professional and personal growth and well-being. From our competitive benefits to our collaborative culture, we ll support your journey every step of the way.\nIn addition to the base salary, Amgen offers competitive and comprehensive Total Rewards Plans that are aligned with local industry standards.\nApply now and make a lasting impact with the Amgen team.\ncareers. amgen. com\nAs an organization dedicated to improving the quality of life for people around the world, Amgen fosters an inclusive environment of diverse, ethical, committed and highly accomplished people who respect each other and live the Amgen values to continue advancing science to serve patients. Together, we compete in the fight against serious disease.\nAmgen is an Equal Opportunity employer and will consider all qualified applicants for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, disability status, or any other basis protected by applicable law.\nWe will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation.\n.",Industry Type: Biotechnology,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Web application development', 'GCP', 'Time management', 'Focus', 'Agile', 'Oncology', 'Public speaking', 'Business case', 'AWS', 'Downstream']",2025-06-10 15:21:24
Data Engineer,Grid Dynamics,4 - 6 years,Not Disclosed,['Bengaluru'],"We are seeking a skilled Data Engineer & Data Analyst with over 4 years of experience to design, build, and maintain scalable data pipelines and perform advanced data analysis to support business intelligence and data-driven decision-making. The ideal candidate will have a strong foundation in computer science principles, extensive experience with SQL and big data tools, and proficiency in cloud platforms and data visualization tools.\nKey Responsibilities:\nDesign, develop, and maintain robust, scalable ETL pipelines using Apache Airflow, DBT, Composer (GCP), Control-M, Cron, Luigi, and similar tools.\nBuild and optimize data architectures including data lakes and data warehouses.\nIntegrate data from multiple sources ensuring data quality and consistency.\nCollaborate with data scientists, analysts, and stakeholders to translate business requirements into technical solutions.\nAnalyze complex datasets to identify trends, generate actionable insights, and support decision-making.\nDevelop and maintain dashboards and reports using Tableau, Power BI, and Jupyter Notebooks for visualization and pipeline validation.\nManage and optimize relational and NoSQL databases such as MySQL, PostgreSQL, Oracle, MongoDB, and DynamoDB.\nWork with big data tools and frameworks including Hadoop, Spark, Hive, Kafka, Informatica, Talend, SSIS, and Dataflow.\nUtilize cloud data services and warehouses like AWS Glue, GCP Dataflow, Azure Data Factory, Snowflake, Redshift, and BigQuery.\nSupport CI/CD pipelines and DevOps workflows using Git, Docker, Terraform, and related tools.\nEnsure data governance, security, and compliance standards are met.\nParticipate in Agile and DevOps processes to enhance data engineering workflows.\n\nRequired Qualifications:\n4+ years of professional experience in data engineering and data analysis roles.\nStrong proficiency in SQL and experience with database management systems such as MySQL, PostgreSQL, Oracle, and MongoDB.\nHands-on experience with big data tools like Hadoop and Apache Spark.\nProficient in Python programming.\nExperience with data visualization tools such as Tableau, Power BI, and Jupyter Notebooks.\nProven ability to design, build, and maintain scalable ETL pipelines using tools like Apache Airflow, DBT, Composer (GCP), Control-M, Cron, and Luigi.\nFamiliarity with data engineering tools including Hive, Kafka, Informatica, Talend, SSIS, and Dataflow.\nExperience working with cloud data warehouses and services (Snowflake, Redshift, BigQuery, AWS Glue, GCP Dataflow, Azure Data Factory).\nUnderstanding of data modeling concepts and data lake/data warehouse architectures.\nExperience supporting CI/CD practices with Git, Docker, Terraform, and DevOps workflows.\nKnowledge of both relational and NoSQL databases, including PostgreSQL, BigQuery, MongoDB, and DynamoDB.\nExposure to Agile and DevOps methodologies.\nExperience with at least one cloud platform:\nGoogle Cloud Platform (BigQuery, Dataflow, Composer, Cloud Storage, Pub/Sub)\nAmazon Web Services (S3, Glue, Redshift, Lambda, Athena)\nMicrosoft Azure (Data Factory, Synapse Analytics, Blob Storage)\n\nPreferred Skills:\nStrong problem-solving and communication skills.\nAbility to work independently and collaboratively in a team environment.\nExperience with service development, REST APIs, and automation testing is a plus.\nFamiliarity with version control systems and workflow automation.",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['SQL', 'Pyspark', 'Azure', 'Data Engineering', 'Amazon Web Services', 'GCP', 'Hadoop', 'Spark', 'Google Cloud Platforms', 'AWS', 'Python']",2025-06-10 15:21:26
Data Engineer-Data Integration,IBM,2 - 5 years,Not Disclosed,['Chennai'],"As a Big Data Engineer, you will develop, maintain, evaluate, and test big data solutions. You will be involved in data engineering activities like creating pipelines/workflows for Source to Target and implementing solutions that tackle the clients needs.\n\n\n\nYour primary responsibilities include:\nDesign, build, optimize and support new and existing data models and ETL processes based on our clients business requirements.\nBuild, deploy and manage data infrastructure that can adequately handle the needs of a rapidly growing data driven organization.\nCoordinate data access and security to enable data scientists and analysts to easily access to data whenever they need too\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nDesign, develop, and maintain Ab Initio graphs for extracting, transforming, and loading (ETL) data from diverse sources to various target systems.\nImplement data quality and validation processes within Ab Initio.\nData Modeling and Analysis:.\nCollaborate with data architects and business analysts to understand data requirements and translate them into effective ETL processes.\nAnalyze and model data to ensure optimal ETL design and performance.\nAb Initio Components:. . Utilize Ab Initio components such as Transform Functions, Rollup, Join, Normalize, and others to build scalable and efficient data integration solutions.\nImplement best practices for reusable Ab Initio components\n\n\nPreferred technical and professional experience\nOptimize Ab Initio graphs for performance, ensuring efficient data processing and minimal resource utilization.\nConduct performance tuning and troubleshooting as needed.\nCollaboration:. .\nWork closely with cross-functional teams, including data analysts, database administrators, and quality assurance, to ensure seamless integration of ETL processes.\nParticipate in design reviews and provide technical expertise to enhance overall solution quality.\nDocumentation:.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['ab initio', 'performance tuning', 'data modeling', 'troubleshooting', 'etl', 'hive', 'python', 'data analysis', 'oracle', 'data warehousing', 'data engineering', 'sql', 'plsql', 'data quality', 'tableau', 'unix shell scripting', 'spark', 'hadoop', 'big data', 'aws', 'data integration', 'informatica', 'unix']",2025-06-10 15:21:28
Specialist Software Engineer,Horizon Therapeutics,4 - 12 years,Not Disclosed,['Hyderabad'],"Career Category Information Systems Job Description\nJoin Amgen s Mission of Serving Patients\nAt Amgen, if you feel like you re part of something bigger, it s because you are. Our shared mission to serve patients living with serious illnesses drives all that we do.\nSince 1980, we ve helped pioneer the world of biotech in our fight against the world s toughest diseases. With our focus on four therapeutic areas -Oncology, Inflammation, General Medicine, and Rare Disease- we reach millions of patients each year. As a member of the Amgen team, you ll help make a lasting impact on the lives of patients as we research, manufacture, and deliver innovative medicines to help people live longer, fuller happier lives.\nOur award-winning culture is collaborative, innovative, and science based. If you have a passion for challenges and the opportunities that lay within them, you ll thrive as part of the Amgen team. Join us and transform the lives of patients while transforming your career.\nSpecialist Software Engineer\nWhat you will do\nLet s do this. Let s change the world. You will play a key role as part of Operations Generative AI (GenAI) Product team to deliver cutting edge innovative GEN AI solutions across various Process Development functions (Drug Substance, Drug Product, Attribute Sciences Combination Products) in Operations functions. The role involves developing, implementing and sustaining GEN AI solutions to help find relevant, actionable information quickly and accurately.\n.\nRole Description:\nThe Specialist Software Engineer is responsible for designing, developing, and maintaining GEN AI solutions software applications and solutions that meet business needs and ensure high availability and performance of critical systems and applications in Process development under Operation. This role involves working closely with Data Scientists, business SME s, and other engineers to create high-quality, scalable GEN AI software solutions to help find relevant, actionable information quickly and accurately, monitoring system health, and responding to incidents to minimize downtime.\nRoles Responsibilities:\nTake ownership of complex software projects from conception to deployment, Manage software delivery scope, risk, and timeline.\nRapidly prototype concepts into working code.\nProvide technical guidance and mentorship to junior developers.\nContribute to front-end and back-end development using cloud technology.\nDevelop innovative solutions using generative AI technologies.\nIntegrate with other systems and platforms to ensure seamless data flow and functionality.\nConduct code reviews to ensure code quality and adherence to best practices.\nCreate and maintain documentation on software architecture, design, deployment, disaster recovery, and operations.\nAnalyze and understand the functional and technical requirements of applications, solutions, and systems and translate them into software architecture and design specifications.\nWork closely with product team, cross-functional teams, enterprise technology teams and QA, to deliver high-quality and compliant software on time.\nEnsure high quality software deliverables free of bugs and performance issues through proper design and comprehensive testing strategies.\nProvide ongoing support and maintenance for applications, ensuring that they operate smoothly and efficiently.\nArchitect and lead the development of scalable, intelligent search systems leveraging NLP, embeddings, LLMs, and vector search\nOwn the end-to-end lifecycle of search solutions, from ingestion and indexing to ranking, relevancy tuning, and UI integration\nIntegrate AI models that improve search precision, query understanding, and result summarization (e. g. , generative answers via LLMs).\nDevelop solutions for handling structured/unstructured data in AI pipelines.\nPartner with platform teams to deploy search solutions on scalable infrastructure (e. g. , Kubernetes, Databricks).\nExperience in integrating Generative AI capabilities and Vision Models to enrich content quality and user engagement.\nWhat we expect of you\nWe are all different, yet we all use our unique contributions to serve patients.\nBasic Qualifications:\nMaster s degree with 4 - 6 years of experience in Computer Science, IT or related field OR\nBachelor s degree with 6 - 8 years of experience in Computer Science, IT or related field OR\nDiploma with 10 - 12 years of experience in Computer Science, IT or related field\nExperience in Python, Java, AI/ML based Python libraries(PyTorch),\nExperienced with Web frameworks like Flask, Django, Fast API\nExperience with design patterns, data structures, data modelling, data algorithms\nFamiliarity with MLOps, CI/CD for ML, and monitoring of AI models in production.\nExperienced with AWS /Azure Platform, building and deploying the code\nExperience in PostgreSQL /Mongo DB SQL database, vector database for large language models, Databricks or RDS, S3 Buckets\nExperience with popular large language models\nExperience with Retrieval-augmented generation (RAG) framework, AI Agents, Vector stores, AI/ML platforms, embedding models ex Open AI, Langchain, Redis, pgvector\nExperience with prompt engineering, model fine tuning\nExperience with generative AI or retrieval-augmented generation (RAG) frameworks\nExperience in Agile software development methodologies\nExperience in End-to-End testing as part of Test-Driven Development\nPreferred Qualifications:\nStrong understanding of cloud platforms (e. g. , AWS, GCP, Azure) and containerization technologies (e. g. , Docker, Kubernetes).\nExperience with monitoring and logging tools (e. g. , Prometheus, Grafana, Splunk).\nExperience with data processing tools like Hadoop, Spark, or similar.\nExperience with Langchain or llamaIndex framework for language models; Experience with prompt engineering, model fine-tuning.\nExperience working on Full stack Applications\nProfessional Certifications:\nAWS, Data Science Certifications(preferred)\nSoft Skills:\nExcellent analytical and troubleshooting skills.\nStrong verbal and written communication skills.\nAbility to work effectively with global, virtual teams.\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nTeam-oriented, with a focus on achieving team goals.\nStrong presentation and public speaking skills.\nWhat you can expect of us\nAs we work to develop treatments that take care of others, we also work to care for your professional and personal growth and well-being. From our competitive benefits to our collaborative culture, we ll support your journey every step of the way.\nIn addition to the base salary, Amgen offers competitive and comprehensive Total Rewards Plans that are aligned with local industry standards.\nApply now and make a lasting impact with the Amgen team.\ncareers. amgen. com\nAs an organization dedicated to improving the quality of life for people around the world, Amgen fosters an inclusive environment of diverse, ethical, committed and highly accomplished people who respect each other and live the Amgen values to continue advancing science to serve patients. Together, we compete in the fight against serious disease.\nAmgen is an Equal Opportunity employer and will consider all qualified applicants for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, disability status, or any other basis protected by applicable law.\nWe will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation.\n.",Industry Type: Biotechnology,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Front end', 'Prototype', 'Postgresql', 'Disaster recovery', 'Agile', 'Data structures', 'Troubleshooting', 'Monitoring', 'Python']",2025-06-10 15:21:31
Data Engineer-Data Integration,IBM,2 - 5 years,Not Disclosed,['Pune'],"As a Big Data Engineer, you will develop, maintain, evaluate, and test big data solutions. You will be involved in data engineering activities like creating pipelines/workflows for Source to Target and implementing solutions that tackle the clients needs.\nYour primary responsibilities include:\nDesign, build, optimize and support new and existing data models and ETL processes based on our client’s business requirements.\nBuild, deploy and manage data infrastructure that can adequately handle the needs of a rapidly growing data driven organization.\nCoordinate data access and security to enable data scientists and analysts to easily access to data whenever they need too\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nDesign, develop, and maintain Ab Initio graphs for extracting, transforming, and loading (ETL) data from diverse sources to various target systems.\nImplement data quality and validation processes within Ab Initio.\nData Modeling and Analysis:.\nCollaborate with data architects and business analysts to understand data requirements and translate them into effective ETL processes.\nAnalyze and model data to ensure optimal ETL design and performance.\nAb Initio Components:. . Utilize Ab Initio components such as Transform Functions, Rollup, Join, Normalize, and others to build scalable and efficient data integration solutions.\nImplement best practices for reusable Ab Initio components\n\n\nPreferred technical and professional experience\nOptimize Ab Initio graphs for performance, ensuring efficient data processing and minimal resource utilization.\nConduct performance tuning and troubleshooting as needed.\nCollaboration:. .\nWork closely with cross-functional teams, including data analysts, database administrators, and quality assurance, to ensure seamless integration of ETL processes.\nParticipate in design reviews and provide technical expertise to enhance overall solution quality.\nDocumentation",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['ab initio', 'performance tuning', 'data modeling', 'troubleshooting', 'etl', 'hive', 'python', 'data analysis', 'oracle', 'data warehousing', 'data engineering', 'sql', 'plsql', 'data quality', 'tableau', 'unix shell scripting', 'spark', 'hadoop', 'big data', 'aws', 'data integration', 'informatica', 'unix']",2025-06-10 15:21:34
Data Engineer - Mixed media modelling,Damco Solutions,5 - 10 years,Not Disclosed,['Pune'],"Role: Data Engineer Mixed Media Model (MMM)\nExp: 4 years +\nLocation: Pune/ Remote\n\nJob Summary:\nThe ideal candidate will have strong experience building scalable ETL pipelines and working with both online and offline marketing data to support MMM, attribution, and ROI analysis.\nThe role requires close collaboration with data scientists and marketing teams to deliver clean, structured datasets for modeling.\n\nMandatory Skills:\nStrong proficiency in SQL and Python or Scala\nHands-on experience with cloud platforms (preferably GCP/BigQuery)\nProven experience with ETL tools like Apache Airflow or DBT\nExperience integrating data from multiple sources: digital platforms (Google Ads, Meta), CRM, POS, TV, Radio, etc.\nUnderstanding of Media Mix Modeling (MMM) and attribution methodologies\n\nGood to have skill:\n\nExperience with data visualization tools (Tableau, Looker, Power BI)\nExposure to statistical modeling techniques\n\n\nPlease share your resume at Neesha1@damcogroup.com",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Mixed media modelling', 'SQL', 'Python', 'Power Bi', 'GCP', 'Bigquery', 'Cloud', 'Apache airflow', 'Data Visualization', 'Tableau', 'ETL']",2025-06-10 15:21:36
Senior Big Data Developer - Youappi,Affle,7 - 10 years,Not Disclosed,"['Gurugram', 'Bengaluru']","We are looking for an experienced Senior Big Data Developer to join our team and help build and optimize high-performance, scalable, and resilient data processing systems. You will work in a fast-paced startup environment, handling highly loaded systems and developing data pipelines that process billions of records in real time.\nAs a key member of the Big Data team, you will be responsible for architecting and optimizing distributed systems, leveraging modern cloud-native technologies, and ensuring high availability and fault tolerance in our data infrastructure.\nPrimary Responsibilities:\nDesign, develop, and maintain real-time and batch processing pipelines using Apache Spark, Kafka, and Kubernetes.\nArchitect high-throughput distributed systems that handle large-scale data ingestion and processing.\nWork extensively with AWS services, including Kinesis, DynamoDB, ECS, S3, and Lambda.\nManage and optimize containerized workloads using Kubernetes (EKS) and ECS.\nImplement Kafka-based event-driven architectures to support scalable, low-latency applications.\nEnsure high availability, fault tolerance, and resilience of data pipelines.\nWork with MySQL, Elasticsearch, Aerospike, Redis, and DynamoDB to store and retrieve massive datasets efficiently.\nAutomate infrastructure provisioning and deployment using Terraform, Helm, or CloudFormation.\nOptimize system performance, monitor production issues, and ensure efficient resource utilization.\nCollaborate with data scientists, backend engineers, and DevOps teams to support advanced analytics and machine learning initiatives.\nContinuously improve and modernize the data architecture to support growing business needs.\nRequired Skills:\n7-10+ years of experience in big data engineering or distributed systems development.\nExpert-level proficiency in Scala, Java, or Python.\nDeep understanding of Kafka, Spark, and Kubernetes in large-scale environments.\nStrong hands-on experience with AWS (Kinesis, DynamoDB, ECS, S3, etc.).\nProven experience working with highly loaded, low-latency distributed systems.\nExperience with Kafka, Kinesis, Flink, or other streaming technologies for event-driven architectures.\nExpertise in SQL and database optimizations for MySQL, Elasticsearch, and NoSQL stores.\nStrong experience in automating infrastructure using Terraform, Helm, or CloudFormation.\nExperience managing production-grade Kubernetes clusters (EKS).\nDeep knowledge of performance tuning, caching strategies, and data consistency models.\nExperience working in a startup environment, adapting to rapid changes and building scalable solutions from scratch.\nNice to Have\nExperience with machine learning pipelines and AI-driven analytics.\nKnowledge of workflow orchestration tools such as Apache Airflow.",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['kubernetes', 'python', 'aws iam', 'performance tuning', 'dynamo db', 'scala', 'data engineering', 'redis', 'helm', 'sql', 'nosql', 'database optimization', 'elastic search', 'java', 'system', 'lambda expressions', 'ecs', 'spark', 'automating', 'kafka', 'caching techniques', 'mysql', 'aws', 'big data']",2025-06-10 15:21:39
Data Engineer-Data Platforms,IBM,5 - 10 years,Not Disclosed,['Pune'],"As a Big Data Engineer, you will develop, maintain, evaluate, and test big data solutions. You will be involved in data engineering activities like creating pipelines/workflows for Source to Target and implementing solutions that tackle the clients needs.\n\nYour primary responsibilities include\nDesign, build, optimize and support new and existing data models and ETL processes based on our clients business requirements.\nBuild, deploy and manage data infrastructure that can adequately handle the needs of a rapidly growing data driven organization.\nCoordinate data access and security to enable data scientists and analysts to easily access to data whenever they need too\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nMust have 5+ years exp in Big Data -Hadoop Spark -Scala ,Python\nHbase, Hive Good to have Aws -S3,\nathena ,Dynomo DB, Lambda, Jenkins GIT\nDeveloped Python and pyspark programs for data analysis.\nGood working experience with python to develop Custom Framework for generating of rules (just like rules engine).\nDeveloped Python code to gather the data from HBase and designs the solution to implement using Pyspark. Apache Spark DataFrames/RDD's were used to apply business transformations and utilized Hive Context objects to perform read/write operations\n\n\nPreferred technical and professional experience\nUnderstanding of Devops.\nExperience in building scalable end-to-end data ingestion and processing solutions\nExperience with object-oriented and/or functional programming languages, such as Python, Java and Scala",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['scala', 'hadoop spark', 'spark', 'big data', 'python', 'hive', 'cloudera', 'pyspark', 'sql', 'java', 'git', 'postgresql', 'devops', 'jenkins', 'data ingestion', 'mysql', 'hadoop', 'etl', 'hbase', 'data analysis', 'dynamo db', 'oozie', 'microsoft azure', 'impala', 'data engineering', 'lambda expressions', 'kafka', 'sqoop', 'aws']",2025-06-10 15:21:43
Data Engineer-Data Platforms,IBM,5 - 10 years,Not Disclosed,['Mumbai'],"As a Big Data Engineer, you will develop, maintain, evaluate, and test big data solutions. You will be involved in data engineering activities like creating pipelines/workflows for Source to Target and implementing solutions that tackle the clients needs.\n\nYour primary responsibilities include:\nDesign, build, optimize and support new and existing data models and ETL processes based on our clients business requirements.\nBuild, deploy and manage data infrastructure that can adequately handle the needs of a rapidly growing data driven organization.\nCoordinate data access and security to enable data scientists and analysts to easily access to data whenever they need too\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nMust have 5+ years exp in Big Data -Hadoop Spark -Scala ,Python\nHbase, Hive Good to have Aws -S3,\nathena ,Dynomo DB, Lambda, Jenkins GIT\nDeveloped Python and pyspark programs for data analysis.\nGood working experience with python to develop Custom Framework for generating of rules (just like rules engine).\nDeveloped Python code to gather the data from HBase and designs the solution to implement using Pyspark. Apache Spark DataFrames/RDD's were used to apply business transformations and utilized Hive Context objects to perform read/write operations\n\n\nPreferred technical and professional experience\nUnderstanding of Devops.\nExperience in building scalable end-to-end data ingestion and processing solutions\nExperience with object-oriented and/or functional programming languages, such as Python, Java and Scala",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['scala', 'hadoop spark', 'spark', 'big data', 'python', 'hive', 'cloudera', 'pyspark', 'sql', 'java', 'git', 'postgresql', 'devops', 'jenkins', 'data ingestion', 'mysql', 'hadoop', 'etl', 'hbase', 'data analysis', 'dynamo db', 'oozie', 'microsoft azure', 'impala', 'data engineering', 'lambda expressions', 'kafka', 'sqoop', 'aws']",2025-06-10 15:21:46
Data Engineer-Data Platforms,IBM,5 - 10 years,Not Disclosed,['Navi Mumbai'],"As a Big Data Engineer, you will develop, maintain, evaluate, and test big data solutions. You will be involved in data engineering activities like creating pipelines/workflows for Source to Target and implementing solutions that tackle the clients needs.\n\nYour primary responsibilities include:\nDesign, build, optimize and support new and existing data models and ETL processes based on our clients business requirements.\nBuild, deploy and manage data infrastructure that can adequately handle the needs of a rapidly growing data driven organization.\nCoordinate data access and security to enable data scientists and analysts to easily access to data whenever they need too\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nMust have 5+ years exp in Big Data -Hadoop Spark -Scala ,Python\nHbase, Hive Good to have Aws -S3,\nathena ,Dynomo DB, Lambda, Jenkins GIT\nDeveloped Python and pyspark programs for data analysis.\nGood working experience with python to develop Custom Framework for generating of rules (just like rules engine).\nDeveloped Python code to gather the data from HBase and designs the solution to implement using Pyspark. Apache Spark DataFrames/RDD's were used to apply business transformations and utilized Hive Context objects to perform read/write operations\n\n\nPreferred technical and professional experience\nUnderstanding of Devops.\nExperience in building scalable end-to-end data ingestion and processing solutions\nExperience with object-oriented and/or functional programming languages, such as Python, Java and Scala",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['scala', 'hadoop spark', 'spark', 'big data', 'python', 'hive', 'cloudera', 'pyspark', 'sql', 'java', 'git', 'postgresql', 'devops', 'jenkins', 'data ingestion', 'mysql', 'hadoop', 'etl', 'hbase', 'data analysis', 'dynamo db', 'oozie', 'microsoft azure', 'impala', 'data engineering', 'lambda expressions', 'kafka', 'sqoop', 'aws']",2025-06-10 15:21:48
Data Engineer-Data Platforms,IBM,4 - 7 years,Not Disclosed,['Gurugram'],"A Data Engineer specializing in enterprise data platforms, experienced in building, managing, and optimizing data pipelines for large-scale environments. Having expertise in big data technologies, distributed computing, data ingestion, and transformation frameworks.\nProficient in Apache Spark, PySpark, Kafka, and Iceberg tables, and understand how to design and implement scalable, high-performance data processing solutions.What you’ll doAs a Data Engineer – Data Platform Services, responsibilities include:\n\nData Ingestion & Processing\nDesigning and developing data pipelines to migrate workloads from IIAS to Cloudera Data Lake.\nImplementing streaming and batch data ingestion frameworks using Kafka, Apache Spark (PySpark).\nWorking with IBM CDC and Universal Data Mover to manage data replication and movement.\nBig Data & Data Lakehouse Management\nImplementing Apache Iceberg tables for efficient data storage and retrieval.\nManaging distributed data processing with Cloudera Data Platform (CDP).\nEnsuring data lineage, cataloging, and governance for compliance with Bank/regulatory policies.\nOptimization & Performance Tuning\nOptimizing Spark and PySpark jobs for performance and scalability.\nImplementing data partitioning, indexing, and caching to enhance query performance.\nMonitoring and troubleshooting pipeline failures and performance bottlenecks.\nSecurity & Compliance\nEnsuring secure data access, encryption, and masking using Thales CipherTrust.\nImplementing role-based access controls (RBAC) and data governance policies.\nSupporting metadata management and data quality initiatives.\nCollaboration & Automation\nWorking closely with Data Scientists, Analysts, and DevOps teams to integrate data solutions.\nAutomating data workflows using Airflow and implementing CI/CD pipelines with GitLab and Sonatype Nexus.\nSupporting Denodo-based data virtualization for seamless data access\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\n4-7 years of experience in big data engineering, data integration, and distributed computing.\nStrong skills in Apache Spark, PySpark, Kafka, SQL, and Cloudera Data Platform (CDP).\nProficiency in Python or Scala for data processing.\nExperience with data pipeline orchestration tools (Apache Airflow, Stonebranch UDM).\nUnderstanding of data security, encryption, and compliance frameworks\n\n\nPreferred technical and professional experience\nExperience in banking or financial services data platforms.\nExposure to Denodo for data virtualization and DGraph for graph-based insights.\nFamiliarity with cloud data platforms (AWS, Azure, GCP).\nCertifications in Cloudera Data Engineering, IBM Data Engineering, or AWS Data Analytics",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'scala', 'pyspark', 'sql', 'spark', 'cloudera', 'continuous integration', 'data analytics', 'data processing', 'airflow', 'big data technologies', 'ci/cd', 'microsoft azure', 'data engineering', 'distributed computing', 'gcp', 'kafka', 'data ingestion', 'gitlab', 'big data', 'aws', 'data integration']",2025-06-10 15:21:50
AI Developer,Electrolux,3 - 8 years,Not Disclosed,['Bengaluru'],"Tech\nPermanent\nJob Description\nBe part of something bigger. Decode the future.\nAt Electrolux, as a leading global appliance company, we strive every day to shape living for the better for our consumers, our people and our planet. We share ideas and collaborate so that together, we can develop solutions that deliver enjoyable and sustainable living.\n\nCome join us as you are. We believe diverse perspectives make us stronger and more innovative. In our global community of people from 100+ countries, we listen to each other, actively contribute, and grow together.\n\nJoin us in our exciting quest to build the future home.\nAll about the role:\nWe are seeking a passionate and skilled AI Developer to join our Contact Center initiatives within the CDI Service Platforms team. This role will focus on designing, building, and deploying AI-powered solutions that enhance customer interaction, automate support workflows, and improve operational efficiency.\nYou will work in a cross-functional team together with product owners, data scientists, and software engineers to integrate conversational AI, natural language processing (NLP), and machine learning into our digital post-purchase landscape. This is a high-impact role with opportunities to shape the technical direction of our AI strategy. In this role, you will be reporting to the Engineering Lead for Contact Center and AI. You will work on a daily basis with colleagues remotely that are based in other countries.\nAbout the CDI Experience Organization:\nThe Consumer Direct Interaction Experience Organization is a Digital Product Organization responsible for delivering tech solutions to our end-users and consumers across both pre-purchase and post-purchase journeys. We are organized in 15+ digital product areas, providing solutions ranging from Contact Center, E-commerce, Marketing, and Identity to AI. You will play a key role in ensuring the right sizing, right skillset, and core competency across these product areas.\nWhat you ll do:\nAs AI developer at Electrolux, your main tasks will include:\nDesign and develop AI-driven solutions for Contact Center platforms, including chatbots, voicebots, and intelligent routing engines.\nContribute to AI architecture decisions and align solutions with enterprise standards and cloud infrastructure.\nTo be constantly improving the functionality of our existing contact center solution.\nDeliver prototypes, proofs of concept (PoCs), and production-grade implementations that scale.\nMonitor and improve AI models through testing, tuning, and feedback loops.\nTo collaborate cross-functionally in your own team and across related teams within the organization.\nTo keep yourself updated with the latest developments within contact center AI and the fast-paced environment of generative AI\nYou should be eager to take on new challenges when exploring these new technologies and to think outside of the box\nWho are you:\nUniversity degree in computer science, NLP, data science, statistics or related field\n3+ years of industry experience with dialog technology, preferably in Google Dialogflow, but similar technologies are also accepted.\nProfound theoretical understanding of machine learning algorithms, including but not limited to the newest versions of generative AI models. Hands-on experience with this technology is a major plus.\nFluent in at least one of the following scripting languages: Python, Node.js, Javascript, Groovy, Perl.\nExperience with REST APIs, microservices, and cloud-native application design.\nHand-on experience with DataScience is a big plus.\nPrevious experience building or integrating AI into Contact Center systems (e.g., Genesys).\nVery good analytical and conceptual skills paired with a pronounced ability to structure and simplify complex relationships\nExcellent communication skills are a must due to the cross-functional nature of the roll.\nThriving in a multi-lingual, multi-cultural environment\nWhere youll be:\nThis is a full-time position, based in Bangalore , India\nBenefits Highlights:\nFlexible work hours and a hybrid work environment\nDiscounts on our award-winning Electrolux products and services\nFamily-friendly benefits\nExtensive learning opportunities and a flexible career path",Industry Type: Consumer Electronics & Appliances,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Application design', 'Analytical', 'Machine learning', 'Cloud', 'Perl', 'Natural language processing', 'Operations', 'Python', 'Customer interaction']",2025-06-10 15:21:52
DS / ML / Time-Series Engineer,Biz-metric India,2 - 5 years,Not Disclosed,"['Mumbai', 'Gurugram', 'Bengaluru']","BIZMETRIC INDIA PRIVATE LIMITED is looking for DS / ML / Time-Series Engineer to join our dynamic team and embark on a rewarding career journey\nData Exploration and Preparation:Explore and analyze large datasets to understand patterns and trends\nPrepare and clean datasets for analysis and model development\nFeature Engineering:Engineer features from raw data to enhance the performance of machine learning models\nCollaborate with data scientists to identify relevant features for model training\nModel Development:Design and implement machine learning models to solve business problems\nWork on both traditional statistical models and modern machine learning algorithms\nScalable Data Pipelines:Develop scalable and efficient data pipelines for processing and transforming data\nUtilize technologies like Apache Spark for large-scale data processing\nModel Deployment:Deploy machine learning models into production environments\nCollaborate with DevOps teams to integrate models into existing systems\nPerformance Optimization:Optimize the performance of data pipelines and machine learning models\nFine-tune models for accuracy, efficiency, and scalability\nCollaboration:Collaborate with cross-functional teams, including data scientists, software engineers, and business stakeholders\nCommunicate technical concepts and findings to non-technical audiences\nContinuous Learning:Stay current with advancements in data science and engineering\nImplement new technologies and methodologies to improve data engineering processes\n\n\nLearning Certification Opportunities: Enhance your professional growth.\nComprehensive Medical Coverage and Life Insurance: For your we'll-being.\nFlexible Work Environment: Enjoy a 5-day work week.\nCollaborative Culture: Be part of a fun, innovative workplace.\nJob Description:\nPython, Data Science, Azure Databricks, Machine Learning",Industry Type: Financial Services,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['algorithms', 'python', 'natural language processing', 'neural networks', 'predictive analytics', 'data pipeline', 'machine learning', 'data engineering', 'artificial intelligence', 'sql', 'deep learning', 'data science', 'spark', 'predictive modeling', 'machine learning algorithms', 'ml']",2025-06-10 15:21:55
Data Science - Analytics - MNC,People Staffing solutions,6 - 11 years,15-30 Lacs P.A.,"['Hyderabad', 'Bengaluru', 'Mumbai (All Areas)']","Key Responsibilities:\nDesign and implement machine learning models to solve real-world business problems.\nWork closely with stakeholders to gather requirements, define KPIs, and deliver data-driven recommendations.\nAnalyze large datasets using SQL, Python (Pandas, NumPy, Scikit-learn, etc.), and derive insights.\nCreate data visualizations and dashboards using tools like Power BI, Tableau, or Matplotlib/Seaborn.\nPerform A/B testing, statistical modeling, and predictive analytics.\nPresent findings clearly to technical and non-technical stakeholders through visual storytelling.\nRequired Skills:\nStrong programming experience in Python for data manipulation and modeling.\nExpertise in SQL for querying relational databases.\nProficiency in machine learning techniques (classification, regression, clustering, etc.).\nExperience with data visualization tools like Tableau, Power BI, or Plotly.\nSolid understanding of statistics and data interpretation.\nExcellent communication and stakeholder management skills.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Machine Learning', 'SQL', 'Data Science', 'Python', 'Predictive Analytics', 'Predictive Modeling', 'Logistic Regression', 'Artificial Intelligence', 'Natural Language Processing', 'Regression', 'Deep Learning', 'Time Series Analysis', 'Linear Regression', 'Predictive', 'Statistical Modeling', 'Time Series']",2025-06-10 15:21:57
Staff Data Engineer - Machine Learning,Netradyne,5 - 8 years,22.5-35 Lacs P.A.,['Bengaluru'],"Role and Responsibilities:\n\nYou will be embedded within a team of machine learning engineers and data scientists; responsible for building and productizing generative AI and deep learning solutions. You will:\nDesign, develop and deploy production ready scalable solutions that utilizes GenAI, Traditional ML models, Data science and ETL pipelines\nCollaborate with cross-functional teams to integrate AI-driven solutions into business operations.\nBuild and enhance frameworks for automation, data processing, and model deployment.\nUtilize Gen-AI tools and workflows to improve the efficiency and effectiveness of AI solutions.\nConduct research and stay updated with the latest advancements in generative AI and related technologies.\nDeliver key product features within cloud analytics.\n\nRequirements:\n\nB. Tech, M. Tech or PhD in Computer Science, Data Science, Electrical Engineering, Statistics, Maths, Operations Research or related domain.\nStrong programming skills in Python, SQL and solid fundamentals in computer science, particularly in algorithms, data structures, and OOP.\nExperience with building end-to-end solutions on AWS cloud infra.\nGood understanding of internals and schema design for various data stores (RDBMS, Vector databases and NoSQL).\nExperience with Gen-AI tools and workflows, and large language models (LLMs).\nExperience with cloud platforms and deploying models at scale.\nStrong analytical and problem-solving skills with a keen attention to detail.\nStrong knowledge of statistics, probability, and estimation theory.\n\nDesired Skills:\n\nFamiliarity with frameworks such as PyTorch, TensorFlow and Hugging Face.\nExperience with data visualization tools like Tableau, Graphana, Plotly-Dash.\nExposure to AWS services like Kinesis, SQS, EKS, ASG, lambda etc.\nExpertise in at least one popular Python web-framework (like FastAPI, Django or Flask).\nExposure to quick prototyping using Streamlit, Gradio, Dash etc.\nExposure to Big Data processing (Snowflake, Redshift, HDFS, EMR)",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Engineering', 'AWS', 'Generative Artificial Intelligence', 'Python', 'Big Data Technologies']",2025-06-10 15:22:00
GenAI Engineer,Xebia It Architects,5 - 10 years,Not Disclosed,"['Pune', 'Chennai', 'Bengaluru']","About Xebia\nXebia is a trusted advisor in the modern era of digital transformation, serving hundreds of leading brands worldwide with end-to-end IT solutions. The company has experts specializing in technology consulting, software engineering, AI, digital products and platforms, data, cloud, intelligent automation, agile transformation, and industry digitization. In addition to providing high-quality digital consulting and state-of-the-art software development, Xebia has a host of standardized solutions that substantially reduce the time-to-market for businesses.\nXebia also offers a diverse portfolio of training courses to help support forward-thinking organizations as they look to upskill and educate their workforce to capitalize on the latest digital capabilities. The company has a strong presence across 16 countries with development centres across the US, Latin America, Western Europe, Poland, the Nordics, the Middle East, and Asia Pacific.\n\nJob Title: Generative AI Engineer\nExp: 5 9 yrs\nLocation: Bengaluru, Chennai, Gurgaon & Pune\nJob Summary:\nWe are seeking a highly skilled Generative AI Engineer with hands-on experience in developing and deploying cutting-edge AI solutions using AWS, Amazon Bedrock, and agentic AI frameworks. The ideal candidate will have a strong background in machine learning and prompt engineering, with a passion for building intelligent, scalable, and secure GenAI applications.\nKey Responsibilities:\n\nDesign, develop, and deploy Generative AI models and pipelines for real-world use cases.\nBuild and optimize solutions using AWS AI/ML services, including Amazon Bedrock, SageMaker, and related cloud-native tools.\nDevelop and orchestrate Agentic AI systems, integrating autonomous agents with structured workflows and dynamic decision-making.\nCollaborate with cross-functional teams including data scientists, cloud engineers, and product managers to translate business needs into GenAI solutions.\nImplement prompt engineering, fine-tuning, and retrieval-augmented generation (RAG) techniques to optimize model performance.\nEnsure robustness, scalability, and compliance in GenAI workloads deployed in production environments.\nRequired Skills & Qualifications:\n\nStrong experience with Generative AI models (e.g., GPT, Claude, Mistral, etc.)\nHands-on experience with Amazon Bedrock and other AWS AI/ML services.\nProficiency in building and managing Agentic AI systems using frameworks like LangChain, AutoGen, or similar.\nSolid understanding of cloud-native architectures and ML Ops on AWS.\nProficiency in Python and relevant GenAI/ML libraries (Transformers, PyTorch, LangChain, etc.)\nFamiliarity with security, cost, and governance best practices for GenAI on cloud.\nPreferred Qualifications:\n\nAWS certifications (e.g., AWS Certified Machine Learning Specialty)\nExperience with LLMOps tools and vector databases (e.g., Pinecone, FAISS, Weaviate)\nBackground in NLP, knowledge graphs, or conversational AI.\nWhy Join Us?\nWork on cutting-edge AI technologies that are transforming industries.\nCollaborative and innovative environment.\nOpportunities for continuous learning and growth.",Industry Type: IT Services & Consulting,"Department: Production, Manufacturing & Engineering","Employment Type: Full Time, Permanent","['GenAI', 'Agentic Ai', 'Ml']",2025-06-10 15:22:02
"Senior Manager, Data Science",Oportun,7 - 12 years,Not Disclosed,[],"We are growing our world-class team of mission-driven, entrepreneurial Data Scientists who are passionate about broadening financial inclusion by untapping insights from non-traditional data. Be part of the team responsible for developing and enhancing Oportuns core intellectual property used in scoring risk for underbanked consumers that lack a traditional credit bureau score. In this role you will be on the cutting edge working with large and diverse (i.e. data from dozens of sources including transactional, mobile, utility, and other financial services) alternative data sets and utilize machine learning and statistical modeling to build scores and strategies for managing risk, collection/loss mitigation, take-up rates and fraud. You will also drive growth and optimize marketing spend across channels by leveraging alternative data to help predict which consumers would likely be interested in Oportuns affordable, credit building loan product.\n\nRESPONSIBILITIES:\nDevelop data products and machine learning models used in Risk, Fraud, Collections, and portfolio management, and provide frictionless customer experience for various products and services Oportun provides.\nBuild accurate and automated monitoring tools which can help us to keep a close eye on the performance of the models and rules.\nBuild model deployment platform which can shorten the time of implementing new models.\nBuild end-to-end reusable pipelines from data acquisition to model output delivery.\nLead initiatives to drive business value from start to finish including project planning,\ncommunication, and stakeholder management.\nLead discussions with Compliance, Bank Partners, and Model Risk Management teams to\nfacilitate the Model Governance Activities such as Model Validations and Monitoring.\nLead, coach and partner with the DS and non-DS team to deliver results\nQUALIFICATIONS:\nA relentless problem solver and out of the box thinker with a proven track record of driving business results in a timely manner\nMasters degree or PhD in Statistics, Mathematics, Computer Science, Engineering or Economics or other quantitative discipline\nHands on experience leveraging machine learning techniques such as Gradient Boosting, Logistic Regression and Neural Network to solve real world problems\n7+ years of hands-on experience with data extraction, cleaning, analysis and building reusable data pipelines; Proficient in SQL, Spark SQL and/or Hive\n7+ years of experience in leveraging modern machine learning toolset and programming languages such as Python\nExcellent written and oral communication skills\nStrong stakeholder management and project management skills\nComfortable in a high-growth, fast-paced, agile environment\nExperience working with AWS EMR, Sage-maker or other cloud-based platforms is a plus\nExperience with HDFS, Hive, Shell script and other big data tools is a plus",Industry Type: FinTech / Payments,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'ML model building', 'Credit Risk', 'sql', 'cloud']",2025-06-10 15:22:05
Artificial Intelligence Architect (AI Architect),Team Computers,10 - 15 years,Not Disclosed,['Noida'],"We are seeking a highly skilled AI Architect to lead our team in developing cutting-edge generative AI solutions. \nThis role requires a deep understanding of AI/ML concepts, strong technical expertise, and exceptional leadership abilities. \nAs the Lead AI Architect, you will be the technical authority responsible for defining the strategy, architecture, and execution of cutting-edge generative AI initiatives.\n You will lead a talented team, driving the development of innovative solutions from concept through production deployment.",,,,"['Generative Ai', 'Natural Language Processing', 'Machine Learning']",2025-06-10 15:22:08
Data/Applied scientist,Bay Area Tek Solutions LLC,2 - 6 years,Not Disclosed,['Bengaluru'],"Strong in Python and experience with Jupyter notebooks, Python packages like polars, pandas, numpy, scikit-learn, matplotlib, etc. Must have: Experience with machine learning lifecycle, including data preparation, training, evaluation, and deployment Must have: Hands-on experience with GCP services for ML & data science Must have: Experience with Vector Search and Hybrid Search techniques Must have: Experience with embeddings generation using models like BERT, Sentence Transformers, or custom models Must have: Experience in embedding indexing and retrieval (e.g., Elastic, FAISS, ScaNN, Annoy) Must have: Experience with LLMs and use cases like RAG (Retrieval-Augmented Generation) Must have: Understanding of semantic vs lexical search paradigms Must have: Experience with Learning to Rank (LTR) techniques and libraries (e.g., XGBoost, LightGBM with LTR support) Should be proficient in SQL and BigQuery for analytics and feature generation Should have experience with Dataproc clusters for distributed data processing using Apache Spark or PySpark Should have experience deploying models and services using Vertex AI, Cloud Run, or Cloud Functions Should be comfortable working with BM25 ranking (via Elasticsearch or OpenSearch) and blending with vector-based approaches Good to have: Familiarity with Vertex AI Matching Engine for scalable vector retrieval Good to have: Familiarity with TensorFlow Hub, Hugging Face, or other model repositories Good to have: Experience with prompt engineering, context windowing, and embedding optimization for LLM-based systems Should understand how to build end-to-end ML pipelines for search and ranking applications Must have: Awareness of evaluation metrics for search relevance (e.g., precision@k, recall, nDCG, MRR) Should have exposure to CI/CD pipelines and model versioning practices",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data science', 'GCP', 'spark', 'Machine learning', 'Data processing', 'Transformers', 'Deployment', 'Analytics', 'SQL', 'Python']",2025-06-10 15:22:11
"Manager, Data & Analytics - Financial Services",RSM US in India,5 - 10 years,Not Disclosed,"['Hyderabad', 'Bengaluru']","RSM is looking for an experienced Hands-On Technical Manager with expertise in big data technologies and multi-cloud platforms to lead our technical team for the financial services industry. The ideal candidate will possess a strong background in big data architecture, cloud computing, and a deep understanding of the financial services industry. As a Technical Manager, you will be responsible for leading technical projects, hands-on development, delivery management, sales and ensuring the successful implementation of data solutions across multiple cloud platforms. This role requires a unique blend of technical proficiency, sales acumen, and presales experience to drive business growth and deliver innovative data solutions to our clients.\nResponsibilities:\nProvide technical expertise and guidance on the selection, and hands-on implementation, and optimization of big data platforms, tools, and technologies across multiple cloud environments (e.g., AWS, Azure, GCP, Snowflake, etc.)\nArchitect and build scalable and secure data pipelines, data lakes, and data warehouses to support the storage, processing, and analysis of large volumes of structured and unstructured data.\nLead and mentor a team of technical professionals in the design, development, and implementation of big data solutions and data analytics projects within the financial services domain.\nStay abreast of emerging trends, technologies, and industry developments in big data, cloud computing, and financial services, and assess their potential impact on the organization.\nDevelop and maintain best practices, standards, and guidelines for data management, data governance, and data security in alignment with regulatory requirements and industry standards.\nCollaborate with the sales and business development teams to identify customer needs, develop solution proposals, and present technical demonstrations and presentations to prospective clients.\nCollaborate with cross-functional teams including data scientists, engineers, business analysts, and stakeholders to define project requirements, objectives, and timelines.\nBasic Qualifications:\nBachelor's degree or higher in Computer Science, Information Technology, Business Administration, Engineering or related field.\nMinimum of ten years of overall technical experience in solution architecture, design, hands-on development with a focus on big data technologies, multi-cloud platforms, and with at-least 5 years of experience specifically in financial services.\nStrong understanding of the financial services industry - capital markets, retail and business banking, asset management, insurance, etc.\nIn-depth knowledge of big data technologies such as Hadoop, Spark, Kafka, and cloud platforms such as AWS, Azure, GCP, Snowflake, Databricks, etc.\nExperience with SQL, Python, Pyspark or other programming languages used for data transformation, analysis, and automation.\nExcellent communication, presentation, and interpersonal skills, with the ability to articulate technical concepts to both technical and non-technical audiences.\nHands-on experience extracting (ETL using CDC, Transaction Logs, Incremental) and processing large data sets for Streaming and Batch data loads.\nAbility to work from our Hyderabad, India office at least twice a week\nPreferred Qualifications:\nProfessional certifications in cloud computing (e.g., AWS Certified Solutions Architect, Microsoft Certified Azure Solutions Architect, Azure Data Engineer, SnowPro Core) and/or big data technologies.\nExperience with Power BI, Tableau or other Reporting and Data Visualization tools\nFamiliarity with DevOps practices, CI/CD pipelines, and infrastructure-as-code tools\nEducation/Experience:\nBachelor s degree in MIS, CS, Engineering or equivalent field.\nMaster s degree is CS or MBA is preferred.\nAdvanced Data and Cloud Certifications are a plus.",Industry Type: Banking,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Financial Services', 'Azure', 'CI/CD pipelines', 'GCP', 'DevOps practices', 'Snowflake', 'Databricks', 'AWS']",2025-06-10 15:22:13
Data Engineer,Bebo Technologies,4 - 9 years,Not Disclosed,['Chandigarh'],"Design, build, and maintain scalable and reliable data pipelines on Databricks, Snowflake, or equivalent cloud platforms.\nIngest and process structured, semi-structured, and unstructured data from a variety of sources including APIs, RDBMS, and file systems.\nPerform data wrangling, cleansing, transformation, and enrichment using PySpark, Pandas, NumPy, or similar libraries.\nOptimize and manage large-scale data workflows for performance, scalability, and cost-efficiency.\nWrite and optimize complex SQL queries for transformation, extraction, and reporting.\nDesign and implement efficient data models and database schemas with appropriate partitioning and indexing strategies for Data Warehouse or Data Mart.\nLeverage cloud services (e.g., AWS S3, Glue, Kinesis, Lambda) for storage, processing, and orchestration.\nUse orchestration tools like Airflow, Temporal, or AWS Step Functions to manage end-to-end workflows.\nBuild containerized solutions using Docker and manage deployment pipelines via CI/CD tools such as Azure DevOps, GitHub Actions, or Jenkins.\nCollaborate closely with data scientists, analysts, and business stakeholders to understand requirements and deliver data solutions.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'python', 'Snowflake', 'Data Bricks', 'sql']",2025-06-10 15:22:15
Full Stack Lead,Persistent,7 - 11 years,Not Disclosed,['Hyderabad'],"About Persistent\nWe are a trusted Digital Engineering and Enterprise Modernization partner, combining deep technical expertise and industry experience to help our clients anticipate what’s next. Our offerings and proven solutions create a unique competitive advantage for our clients by giving them the power to see beyond and rise above. We work with many industry-leading organizations across the world including 12 of the 30 most innovative US companies, 80% of the largest banks in the US and India, and numerous innovators across the healthcare ecosystem.\nOur growth trajectory continues, as we reported $1,231M annual revenue (16% Y-o-Y). Along with our growth, we’ve onboarded over 4900 new employees in the past year, bringing our total employee count to over 23,500+ people located in 19 countries across the globe.",,,,"['full stack', 'memcached', 'server', 'css', 'dbms', 'microsoft', 'ajax', 'jquery', 'redis', 'docker', 'cloud', 'react.js', 'apache', 'java', 'ui', 'gcp', 'linux', 'writing', 'json', 'mysql', 'html', 'mvc', 'mongodb', 'communication skills', 'python', 'oracle', 'ux', 'nginx', 'sql server', 'javascript', 'ruby', 'angular', 'spring boot', 'node.js', 'troubleshooting', 'le', 'aws', 'angularjs']",2025-06-10 15:22:18
Full Stack Lead,Persistent,7 - 11 years,Not Disclosed,['Pune'],"About Persistent\nWe are a trusted Digital Engineering and Enterprise Modernization partner, combining deep technical expertise and industry experience to help our clients anticipate what’s next. Our offerings and proven solutions create a unique competitive advantage for our clients by giving them the power to see beyond and rise above. We work with many industry-leading organizations across the world including 12 of the 30 most innovative US companies, 80% of the largest banks in the US and India, and numerous innovators across the healthcare ecosystem.\nOur growth trajectory continues, as we reported $1,231M annual revenue (16% Y-o-Y). Along with our growth, we’ve onboarded over 4900 new employees in the past year, bringing our total employee count to over 23,500+ people located in 19 countries across the globe.",,,,"['full stack', 'memcached', 'server', 'css', 'dbms', 'microsoft', 'ajax', 'jquery', 'redis', 'docker', 'cloud', 'react.js', 'apache', 'java', 'ui', 'gcp', 'linux', 'writing', 'json', 'mysql', 'html', 'mvc', 'mongodb', 'communication skills', 'python', 'oracle', 'ux', 'nginx', 'sql server', 'javascript', 'ruby', 'angular', 'spring boot', 'node.js', 'troubleshooting', 'le', 'aws', 'angularjs']",2025-06-10 15:22:21
Consultant Data Engineering - Connected Medicine,Eli Lilly And Company,7 - 10 years,Not Disclosed,['Bengaluru'],"The role will be responsible for setting up the data warehouses necessary to handle large volumes of data, create meaningful analyses, and deliver recommendations to leadership.\nCore Responsibilities\nCreate and maintain optimal data pipeline architecture ETL/ ELT into structured data\nAssemble large, complex data sets that meet business requirements and create and maintain multi-dimensional modelling like Star Schema and Snowflake Schema, normalization, de-normalization, joining of datasets.\nExpert level experience in creating a scalable data warehouse including Fact tables, Dimensional tables and ingest datasets into cloud based tools.\nIdentify, design, and implement internal process improvements including automating manual processes, optimizing data delivery and re-designing infrastructure for greater scalability.\nCollaborate with stakeholders to ensure seamless integration of data with internal data marts, enhancing advanced reporting\nSetup and maintain data ingestion, streaming, scheduling, and job monitoring automation using AWS services. Setup Lambda, code pipeline (CI/CD), Glue, S3, Redshift, Power BI needs to be maintained for uninterrupted automation.\nBuild analytics tools that utilize the data pipeline to provide actionable insight into customer acquisition, operational efficiency, and other key business performance metrics.\nWork with stakeholders to assist with data-related technical issues and support their data infrastructure needs.\nUtilize GitHub for version control, code collaboration, and repository management. Implement best practices for code reviews, branching strategies, and continuous integration.\nCreate data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader\nEnsure data privacy and compliance with relevant regulations (eg, GDPR) when handling customer data.\nMaintain data quality and consistency within the application, addressing data-related issues as they arise.\nRequired\n7-10 years of relevant experience\nAdvanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as we'll as working familiarity with a variety of databases and Cloud Data warehouse like AWS Redshift\nExperience in creating scalable, efficient schema designs to support diverse business needs.\nExperience with database normalization, schema evolution, and maintaining data integrity\nProactively share best practices, contributing to team knowledge and improving schema design transitions.\nDevelop data models, create dimensions and facts, and establish views and procedures to enable automation programmability.\nCollaborate effectively with cross-functional teams to gather requirements, incorporate feedback, and align analytical work with business objectives\nPrior Data Modelling, OLAP cube modelling\nData compression into PARQUET to improve processing and finetuning SQL programming skills.\nExperience building and optimizing big data data pipelines, architectures and data sets.\nExperience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.\nExperience with manipulating, processing and extracting value from large disconnected unrelated datasets\nStrong analytic skills related to working with structured and unstructured datasets.\nWorking knowledge of message queuing, stream processing, and highly scalable big data stores.\nExperience supporting and working with cross-functional teams and Global IT.\nFamiliarity of working in an agile based working models.\nPreferred Qualifications/Expertise\nExperience with relational SQL and NoSQL databases, especially AWS Redshift.\nExperience with AWS cloud services Preferable : S3, EC2, Lambda, Glue, EMR, Code pipeline highly preferred. Experience with similar services on another platform would also be considered.\nEducation:\nbachelors or masters degree on Technology and Computer Science background",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Automation', 'Process improvement', 'Analytical', 'Healthcare', 'Scheduling', 'Data quality', 'Operations', 'Analytics', 'Monitoring', 'SQL']",2025-06-10 15:22:23
Lead Technical Architect,Aegis Media,7 - 12 years,Not Disclosed,['Mumbai'],"Job Description:\nBusiness Title\nLead Technical Architect\nYears of Experience\n> 7 Years\nMust have skills\n1. Database ( SQL server / SnowFlake / Teradata / Redshift / Vertica / Oracle / Big query / Azure DW etc)\n2. ETL tool (Talend, Informatica, IICS (Informatica cloud) )\n3. Experience in Cloud computing (one or more of AWS, Azure, GCP)\n4. Python, UNIX shell scripting, Project & resource management\n5. SVN, JIRA, Automation workflow (Apache Airflow, Tidal, Tivoli or similar)\nGood to have skills\n1. PySpark, Big Query, Familiar with NoSQL such as MongoDB etc\n2. Client-facing skills\nJod Descreption\nThe Technical Lead / Technical Consultant is a core role and focal point of the project team responsible for the whole technical solution and managing the day to day delivery. The role will focus on the technical solution architecture, detailed technical design, coaching of the development/implementation team and governance of the technical delivery. Technical ownership of the solution from bid inception through implementation to client delivery, followed by after sales support and best practise advice. Interactions with internal stakeholder s and clients to explain technology solutions and a clear understanding of client s business requirements through which to guide optimal design to meet their needs.\nKey responsibiltes\nAbility to design simple to medium data solutions for clients by using cloud architecture using AWS/GCP\nStrong understanding of DW, data mart, data modelling, data structures, databases, and data ingestion and transformation.\nWorking knowledge of ETL as well as database skills\nWorking knowledge of data modelling, data structures, databases, and ETL processes\nStrong understand of relational and non-relational databases and when to use them\nLeadership and communication skills to collaborate with local leadership as well as our global teams\nTranslating technical requirements into ETL/ SQL application code\nDocument project architecture, explain detailed design to team and create low level to high level design\nCreate technical documents for ETL and SQL developments using Visio, PowerPoint and other MS Office package\nWill need to engage with Project Managers, Business Analysts and Application DBA to implement ETL Solutions\nPerform mid to complex level tasks independently\nSupport Client, Data Scientists and Analytical Consultants working on marketing solution\nWork with cross functional internal team and external clients\nStrong project Management and organization skills. Ability to lead 1 - 2 projects of team size 2 - 3 team members.\nCode management systems which includes Code review, deployment, cod\nWork closely with the QA / Testing team to help identify/implement defect reduction initiatives\nWork closely with the Architecture team to make sure Architecture standards and principles are followed during development\nPerforming Proof of Concepts on new platforms/ validate proposed solutions\nWork with the team to establish and reinforce disciplined software development, processes, standards, and error recovery procedures are deployed\nMust understand software development methodologies including waterfall and agile\nDistribute and manage SQL development Work across the team\nEducation Qulification\n1. Bachelor s or Master Degree in Computer Science\nShift timingGMT (UK Shift) - 2 PM to 11 PM\nLocation:\nMumbai\nBrand:\nMerkle\nTime Type:\nFull time\nContract Type:\nPermanent",Industry Type: Advertising & Marketing,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Automation', 'Project management', 'Agile', 'Data structures', 'Workflow', 'Informatica', 'Oracle', 'Apache', 'Teradata', 'Python']",2025-06-10 15:22:25
Data Engineer,Xyram Software Solution,5 - 9 years,Not Disclosed,['Bengaluru'],"Responsibilities\nDesign, develop, and maintain scalable data pipelines and ETL processes\nOptimize data flow and collection for cross-functional teams\nBuild infrastructure required for optimal extraction, transformation, and loading of data\nEnsure data quality, reliability, and integrity across all data systems\nCollaborate with data scientists and analysts to help implement models and algorithms\nIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, etc.\nCreate and maintain comprehensive technical documentation\nEvaluate and integrate new data management technologies and tools\n\nRequirements\n3-5 years of professional experience in data engineering roles\nBachelors degree in Computer Science, Engineering, or related field; Masters degree preferred Job Description\nExpert knowledge of SQL and experience with relational databases (e.g., PostgreSQL, Redshift, TIDB, MySQL, Oracle, Teradata)\nExtensive experience with big data technologies (e.g., Hadoop, Spark, Hive, Flink)\nProficiency in at least one programming language such as Python, Java, or Scala\nExperience with data modeling, data warehousing, and building ETL pipelines\nStrong knowledge of data pipeline and workflow management tools (e.g., Airflow, Luigi, NiFi)\nExperience with cloud platforms (AWS, Azure, or GCP) and their data services. AWS Preferred\nHands on Experience with building streaming pipelines with flink, Kafka, Kinesis. Flink\nUnderstanding of data governance and data security principles\nExperience with version control systems (e.g., Git) and CI/CD practices\n\nPreferred Skills\nExperience with containerization and orchestration tools (Docker, Kubernetes)\nBasic knowledge of machine learning workflows and MLOps\nExperience with NoSQL databases (MongoDB, Cassandra, etc.)\nFamiliarity with data visualization tools (Tableau, Power BI, etc.)\nExperience with real-time data processing\nKnowledge of data governance frameworks and compliance requirements (GDPR, CCPA, etc.)\nExperience with infrastructure-as-code tools (Terraform, CloudFormation)\n\nPersonal Qualities\nStrong problem-solving skills and attention to detail\nExcellent communication skills, both written and verbal\nAbility to work independently and as part of a team\nProactive approach to identifying and solving problems",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Data management', 'Data modeling', 'Postgresql', 'MySQL', 'Oracle', 'Teradata', 'Technical documentation', 'SQL', 'Python']",2025-06-10 15:22:26
Senior MLOps Engineer,Blend360 India,5 - 9 years,Not Disclosed,['Hyderabad'],"We are seeking a highly skilled AWS MLOps Engineer with a overall experience 5 years with 3 years as ML Engineer particularly in building and managing ML pipelines in AWS. The ideal candidate has successfully built and deployed at least two MLOps projects using Amazon SageMaker or similar services, with a strong foundation in infrastructure as code and a keen understanding of MLOps best practices.\nKey Responsibilities:\nMaintain and enhance existing ML pipelines in AWS with a focus on Infrastructure as Code using CloudFormation.\nImplement minimal but essential pipeline extensions to support ongoing data science workstreams.\nDocument infrastructure usage, architecture, and design using tools like Confluence, GitHub Wikis, and system diagrams.\nAct as the internal infrastructure expert, collaborating with data scientists to guide and support model deployments.\nResearch and implement optimization strategies for ML workflows and infrastructure.\nWork independently and collaboratively with cross-functional teams to support ML product deployment and re-platforming initiatives.\n\n\n5+ years of hands-on DevOps experience with AWS Cloud.\nProven experience with at least two MLOps projects deployed using SageMaker or similar AWS services.\nStrong proficiency in AWS service",Industry Type: Industrial Automation,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Automation', 'Usage', 'github', 'Version control', 'GIT', 'data science', 'Infrastructure', 'Management', 'AWS', 'Python']",2025-06-10 15:22:28
Senior AI/ML Engineer - Blackstraw - Work from Office- Chennai,Blackstraw Technologies,4 - 9 years,Not Disclosed,"['Chennai', 'Mumbai (All Areas)']","Job Summary:\nWe are looking for a passionate and skilled AI/ML Engineer to join our team, focusing on cutting edge solutions in vector search, embeddings, and semantic similarity. In this role, you will design, develop, and optimize intelligent search systems that leverage state-of-the-art machine learning and deep learning techniques. You will collaborate with cross-functional teams to build scalable and efficient solutions that transform how we retrieve and process information.",,,,"['Artificial Intelligence', 'Machine Learning', 'Object Detection', 'Algorithm Development', 'Natural Language Processing', 'Image Recognition', 'Deep Learning', 'Pytorch', 'Huggingface', 'Video Processing', 'Opencv', 'Image Processing', 'Computer Vision', 'Optimize Vector Search Systems', 'Python', 'OCR']",2025-06-10 15:22:31
Senior Software Engineer (Javascript and Ruby),Infraveo Technologies,7 - 12 years,Not Disclosed,[],"We are seeking a Senior Software Engineer (Javascript and Ruby) to join our team. As a Senior Software Engineer with broad expertise, you will be a vital part of our team, developing innovative applications that leverage AI capabilities to enhance user experiences and streamline communication. You will work alongside a talented team of Data Scientists, DevOps, Product Managers, Business Analysts experts and play a key role in designing and implementing specialised AI assistant technology.\n\nResponsibilties:\nExcellent problem-solving and technical skills.\nStrong communication and collaboration skills, with the ability to work in a team.\nInterest and experience in working on early-stage software and a wide range of tasks.\nProven experience using technology and how it helped you build a lasting product.\nRequirements\nPreferred M.Sc or Ph.d degree in Computer Science or a related field.\n7+ years of experience in Software development\nWork experience using both compiled languages (Rust, Ocaml, Golang, Java, C#) or dynamic languages (Javascript, Python, Ruby)\nExperience building web applications or desktop applications technologies such as Electron, tauri, React, Vue.js\nFamiliarity with CI/CD principles and technologies, including experience with GitHub Actions or similar.\nExperience working with Relational and NoSQL databases such as Postgres, Redis, Neo4j, Milviousor MongoDB, and a good understanding of data consistency tradeoffs.\nProven Knowledge of cloud platforms (eg, AWS, Azure, or GCP).\nBenefits\nWork Location: Remote\n5 days working",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Web technologies', 'Business Analyst', 'devops', 'Team development', 'Manager Technology', 'Javascript', 'Ruby', 'Python']",2025-06-10 15:22:34
Senior Data Engineer,Parkar Global Technologies,6 - 10 years,Not Disclosed,['Pune'],"About Position:\nWe are looking for a Senior Data Engineer to play a key role in building, optimizing, and maintaining our Azure-based data platform, which supports IoT data processing, analytics, and AI/ML applications. As part of our Data Platform Team, you will design and develop scalable data pipelines, implement data governance frameworks, and ensure high-performance data processing to drive digital transformation across our business.\n\nResponsibilities:\nData Pipeline Development: Design, build, and maintain high-performance, scalable ETL/ELT pipelines using Azure Data Factory, Databricks, and ADLS.\nData Platform Enhancement: Contribute to the development and optimization of our Azure-based data platform, ensuring efficiency, reliability, and security.\nIoT & High-Volume Data Processing: Work with large-scale IoT and operational datasets, optimizing data ingestion, transformation, and storage.\nData Governance & Quality: Implement data governance best practices, ensuring data integrity, consistency, and compliance.\nPerformance Optimization: Improve query performance and storage efficiency for analytics and reporting use cases.\nCollaboration: Work closely with data scientists, architects, and business teams to ensure data availability and usability.\nInnovation & Automation: Identify opportunities for automation and process improvements, leveraging modern tools and technologies.\n\nRequirement:\n6+ years of experience in data engineering with a focus on Azure cloud technologies.\nStrong expertise in Azure Data Factory, Databricks, ADLS, and Power BI.\nProficiency in SQL, Python, and Spark for data processing and transformation.\nExperience with IoT data ingestion and processing, handling high-volume, real-time data streams.\nStrong understanding of data modeling, lakehouse architectures, and medallion frameworks.\nExperience in building and optimizing scalable ETL/ELT processes.\nKnowledge of data governance, security, and compliance frameworks.\nExperience with monitoring, logging, and performance tuning of data workflows.\nStrong problem-solving and analytical skills with a platform-first mindset.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Data Factory', 'Pyspark', 'Azure Databricks', 'Azure Data Lake', 'Azure Devops', 'SQL']",2025-06-10 15:22:36
Senior Data Engineer,Aventior Digital,7 - 12 years,Not Disclosed,[],"Designation: Senior Data Engineer\nPreferred Experience: 7+ years\n\nResponsibilities:\nDesign, develop, and maintain data pipelines for ingesting, processing, and transforming data from various sources into actionable insights.\nIntegrate data from disparate sources (databases, APIs, and files) into a unified data platform using ETL processes and data integration techniques.\nDesign and implement data models, schemas, and data structures to support analytical queries, reporting, and business intelligence needs.\nOptimize database performance, query execution, and data processing workflows for efficiency, scalability, and reliability.\nEnsure data quality, integrity, and consistency through data validation, cleansing, deduplication, and error-handling mechanisms.\nArchitect and implement data solutions on Azure cloud platforms, leveraging Azure services for data storage, processing, and analytics.\nImplement data security measures, encryption techniques, and access controls to protect sensitive data and ensure compliance with regulations (e.g., GDPR, HIPAA).\nWork closely with cross-functional teams, data scientists, analysts, and stakeholders to understand data requirements, provide data solutions, and communicate insights effectively.\nDocument data processes, workflows, and best practices, and promote data governance standards, data lineage, and metadata management.\nStay updated with emerging technologies, industry trends, and best practices in data engineering, cloud computing, and data analytics to drive innovation and continuous improvement.\n\nRequired Skills:\nProficiency in writing complex SQL queries, stored procedures, and functions for data extraction, transformation, and analysis.\nExperience in database design, optimization, and management using SQL Server/Azure SQL Database.\nKnowledge of data modeling techniques, including entity-relationship diagrams, dimensional modeling, and data normalization, is needed to design efficient data structures.\nFamiliarity with ETL (Extract, Transform, Load) processes and tools such as Azure Data Factory, SSIS (SQL Server Integration Services), or other data integration platforms.\nHands-on experience with Azure cloud services, including Azure SQL Database, Azure Data Factory and Azure Storage.\nExperience handling large-scale data processing and analytics with strong analytical skills and attention to detail.\nStrong statistical knowledge.\nExcellent communication skills.\n\nGood To Have Skills:\nExperience working with C#, .Net Framework\nExperience with other Azure cloud services and sharepoint\nExperience with Python as programming language\nExperience manipulating unstructured data with regular expressions.\nBasic understanding of machine learning concepts and algorithms for data mining, predictive modeling, and statistical analysis.\nKnowledge of data warehousing concepts, methodologies, and tools for building and maintaining data warehouses or data marts.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SQL Queries', 'Azure Data Factory', 'Data Engineering', 'ETL', 'SSIS', 'SQL']",2025-06-10 15:22:38
Analyst,Wipro,2 - 6 years,Not Disclosed,['Gurugram'],"Wipro Limited (NYSE:WIT, BSE:507685, NSE:WIPRO) is a leading technology services and consulting company focused on building innovative solutions that address clients most complex digital transformation needs. Leveraging our holistic portfolio of capabilities in consulting, design, engineering, and operations, we help clients realize their boldest ambitions and build future-ready, sustainable businesses. With over 230,000 employees and business partners across 65 countries, we deliver on the promise of helping our customers, colleagues, and communities thrive in an ever-changing world. For additional information, visit us at www.wipro.com.\n\nAbout The Role :\n\n\n\nRole Purpose\n\nThe purpose of the role is to provide effective technical support to the process and actively resolve client issues directly or through timely escalation to meet process SLAs.\n\n\n\n\n\n\n\n\nDo\n\n\n\n\n\n\nSupport process by managing transactions as per required quality standards\nFielding all incoming help requests from clients via telephone and/or emails in a courteous manner\nDocument all pertinent end user identification information, including name, department, contact information and nature of problem or issue\nUpdate own availability in the RAVE system to ensure productivity of the process\nRecord, track, and document all queries received, problem-solving steps taken and total successful and unsuccessful resolutions\nFollow standard processes and procedures to resolve all client queries\nResolve client queries as per the SLAs defined in the contract\nAccess and maintain internal knowledge bases, resources and frequently asked questions to aid in and provide effective problem resolution to clients\nIdentify and learn appropriate product details to facilitate better client interaction and troubleshooting\nDocument and analyze call logs to spot most occurring trends to prevent future problems\nMaintain and update self-help documents for customers to speed up resolution time\nIdentify red flags and escalate serious client issues to Team leader in cases of untimely resolution\nEnsure all product information and disclosures are given to clients before and after the call/email requests\nAvoids legal challenges by complying with service agreements\n\n\n\n\n\n\n\n\n\nDeliver excellent customer service through effective diagnosis and troubleshooting of client queries\nProvide product support and resolution to clients by performing a question diagnosis while guiding users through step-by-step solutions\nAssist clients with navigating around product menus and facilitate better understanding of product features\nTroubleshoot all client queries in a user-friendly, courteous and professional manner\nMaintain logs and records of all customer queries as per the standard procedures and guidelines\nAccurately process and record all incoming call and email using the designated tracking software\nOffer alternative solutions to clients (where appropriate) with the objective of retaining customers and clients business\nOrganize ideas and effectively communicate oral messages appropriate to listeners and situations\nFollow up and make scheduled call backs to customers to record feedback and ensure compliance to contract /SLAs\n\n\n\n\n\n\n\n\n\nBuild capability to ensure operational excellence and maintain superior customer service levels of the existing account/client\nUndertake product trainings to stay current with product features, changes and updates\nEnroll in product specific and any other trainings per client requirements/recommendations\nPartner with team leaders to brainstorm and identify training themes and learning issues to better serve the client\nUpdate job knowledge by participating in self learning opportunities and maintaining personal networks\n\n\n\n\n\n\n\n\nDeliver\nNoPerformance ParameterMeasure1ProcessNo. of cases resolved per day, compliance to process and quality standards, meeting process level SLAs, Pulse score, Customer feedback2Self- ManagementProductivity, efficiency, absenteeism, Training Hours, No of technical training completed Reinvent your world.We are building a modern Wipro. We are an end-to-end digital transformation partner with the boldest ambitions. To realize them, we need people inspired by reinvention. Of yourself, your career, and your skills. We want to see the constant evolution of our business and our industry. It has always been in our DNA - as the world around us changes, so do we. Join a business powered by purpose and a place that empowers you to design your own reinvention. Come to Wipro. Realize your ambitions. Applications from people with disabilities are explicitly welcome.",Industry Type: IT Services & Consulting,"Department: Customer Success, Service & Operations","Employment Type: Full Time, Permanent","['customer service', 'process compliance', 'technical support', 'client interaction', 'operational excellence', 'operations management', 'project management', 'data analysis', 'team management', 'team leading', 'incident management', 'troubleshooting', 'digital transformation']",2025-06-10 15:22:42
Business Analyst,Flipkart,2 - 5 years,Not Disclosed,['Bengaluru'],About the Role\nWhat You will do \nWill be working on analyzing multiple data trends to arrive at actionable insight\nWill lead directs who would lead multiple reporting and analytics \nYou will be closely working with the business/product teams to enable data-driven decision-making.,,,,"['Advanced Excel', 'SQL', 'Power Bi', 'Business Analytics', 'Data Analysis', 'Business Insights', 'Python']",2025-06-10 15:22:46
Analyst,Wipro,2 - 6 years,Not Disclosed,['Gurugram'],"About The Role :\n\n\n\nRole Purpose\n\nThe purpose of the role is to provide effective technical support to the process and actively resolve client issues directly or through timely escalation to meet process SLAs.\n\n\n\n\n\n\n\n\nDo\n\n\n\n\n\n\nSupport process by managing transactions as per required quality standards\nFielding all incoming help requests from clients via telephone and/or emails in a courteous manner\nDocument all pertinent end user identification information, including name, department, contact information and nature of problem or issue\nUpdate own availability in the RAVE system to ensure productivity of the process\nRecord, track, and document all queries received, problem-solving steps taken and total successful and unsuccessful resolutions\nFollow standard processes and procedures to resolve all client queries\nResolve client queries as per the SLAs defined in the contract\nAccess and maintain internal knowledge bases, resources and frequently asked questions to aid in and provide effective problem resolution to clients\nIdentify and learn appropriate product details to facilitate better client interaction and troubleshooting\nDocument and analyze call logs to spot most occurring trends to prevent future problems\nMaintain and update self-help documents for customers to speed up resolution time\nIdentify red flags and escalate serious client issues to Team leader in cases of untimely resolution\nEnsure all product information and disclosures are given to clients before and after the call/email requests\nAvoids legal challenges by complying with service agreements\n\n\n\n\n\n\n\n\n\nDeliver excellent customer service through effective diagnosis and troubleshooting of client queries\nProvide product support and resolution to clients by performing a question diagnosis while guiding users through step-by-step solutions\nAssist clients with navigating around product menus and facilitate better understanding of product features\nTroubleshoot all client queries in a user-friendly, courteous and professional manner\nMaintain logs and records of all customer queries as per the standard procedures and guidelines\nAccurately process and record all incoming call and email using the designated tracking software\nOffer alternative solutions to clients (where appropriate) with the objective of retaining customers and clients business\nOrganize ideas and effectively communicate oral messages appropriate to listeners and situations\nFollow up and make scheduled call backs to customers to record feedback and ensure compliance to contract /SLAs\n\n\n\n\n\n\n\n\n\nBuild capability to ensure operational excellence and maintain superior customer service levels of the existing account/client\nUndertake product trainings to stay current with product features, changes and updates\nEnroll in product specific and any other trainings per client requirements/recommendations\nPartner with team leaders to brainstorm and identify training themes and learning issues to better serve the client\nUpdate job knowledge by participating in self learning opportunities and maintaining personal networks\n\n\n\n\n\n\n\n\nDeliver\nNoPerformance ParameterMeasure1ProcessNo. of cases resolved per day, compliance to process and quality standards, meeting process level SLAs, Pulse score, Customer feedback2Self- ManagementProductivity, efficiency, absenteeism, Training Hours, No of technical training completed",Industry Type: IT Services & Consulting,"Department: Customer Success, Service & Operations","Employment Type: Full Time, Permanent","['customer service', 'process compliance', 'technical support', 'client interaction', 'operational excellence', 'operations management', 'project management', 'data analysis', 'team management', 'team leading', 'incident management', 'troubleshooting', 'sla management']",2025-06-10 15:22:48
Analyst,Wipro,2 - 6 years,Not Disclosed,['Gurugram'],"Analyst- Investment Management- Structured Finance\n\nBand:B1 - Analyst\n\nLocation:Gurugram / Bengaluru\n\nExperience Range:Minimum 1 year\n\nShift Timings-:2:00 PM - 11:30 PM IST\n\nJob Summary\n\nWe are seeking a highly motivated Investment Management Analyst to join our Asset Based Finance (ABF) team within the Structured Finance Group. As an integral part of our team, you will gain exposure to a diverse portfolio of Asset-Backed Securities (ABS) investments and play a pivotal role in underwriting, monitoring, and managing these deals for our institutional clients.\n\nResponsibilities:\n\nDeal Management:Collaborate closely with ABF Deal Analysts to assist in investment management and monitoring activities for structured finance deals.\n\nIC Memos:Support the deal team in updating Investment Committee approval memos, ensuring a comprehensive understanding of each deal's intricacies.\n\nTerm sheets:Analyze closing documents such as credit agreements, indentures and note purchase agreements and set up processes for analyzing and monitoring the deal post-closing.\n\nDue Diligence:Perform pre-deal due diligence and stratify the collateral pool using Python to assess risk and investment potential.\n\nSurveillance and Reporting:Update surveillance data and create one-pagers for presentations to senior management for both Public and Private deals.\n\nCashflow Modeling:Develop and update cashflow models for structured finance deals. Monitor key metrics to assess risk and expected returns. Conduct stress case scenarios and analyze their impact on repayments.\n\nCollateral Analysis:Create and update asset-level one-pagers for collateral analysis, assisting in underwriting and post-deal monitoring.\n\nAsset Valuation:Track and review revaluation events for underlying assets, ensuring accurate pricing of collateral.\n\nSector-Level Analysis:Update monthly sector-level presentations and pro-actively highlight potential issues to senior management.\n\nCovenant Monitoring:Continuously monitor covenants and key performance indicators (KPIs) at the deal level, thereby assist the Business Analytics team to run portfolio-level analysis.\n\nAd Hoc Projects:Undertake multiple ad hoc projects as requested by senior management to assess the impact of macro events.\n\nClient request/ Compliances:Ensure compliance with investor requirements from an investment management perspective.\n\nData and Tools:Leverage advanced tools such as Intex and Python for in-depth analysis. Utilize Tableau, Street Diligence and Sigma for enhanced data visualization.\n\nSkills Required:\n\nMBA in Finance, CFA, or CA qualification.\n\nExperience in analyzing Asset Backed Finance deal structures and collateral is a plus.\n\nStrong analytical and quantitative skills.\n\n\n\n\n\n\n\n\nDeliver\nNoPerformance ParameterMeasure1ProcessNo. of cases resolved per day, compliance to process and quality standards, meeting process level SLAs, Pulse score, Customer feedback2Self- ManagementProductivity, efficiency, absenteeism, Training Hours, No of technical training completed",Industry Type: IT Services & Consulting,"Department: BFSI, Investments & Trading","Employment Type: Full Time, Permanent","['python', 'collaterals', 'tableau', 'quantitative', 'finance', 'financial analysis', 'analysts', 'due diligence', 'investment banking', 'business analytics', 'process compliance', 'valuation', 'underwriting', 'investment management', 'data visualization', 'asset', 'pricing', 'us mortgage', 'cfa']",2025-06-10 15:22:51
Applied Scientist I at Fintech Platform,Talent 24/7,2 - 4 years,22.5-27.5 Lacs P.A.,[],"Preferred candidate profile\nProficient in Java, C++, Python, or similar languages.\nExperience with SQL and relational databases (e.g., Oracle, Data Warehouse).\nHands-on in building ML models or algorithms for real-world use.\nBackground in ML, deep learning, NLP, computer vision, or data science.\nKnowledge of deep learning architecture, training optimization, and model pruning.",Industry Type: FinTech / Payments,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Machine Learning', 'Computer Vision', 'Python']",2025-06-10 15:22:53
Business Analyst,Radicalstart Infolab,0 - 2 years,Not Disclosed,['Madurai'],"RadicalStart InfoLab is looking for Business Analyst to join our dynamic team and embark on a rewarding career journey.\nEvaluating business processes, anticipating requirements, uncovering areas for improvement, and developing and implementing solutions.\nLeading ongoing reviews of business processes and developing optimization strategies.\nStaying up-to-date on the latest process and IT advancements to automate and modernize systems.\nConducting meetings and presentations to share ideas and findings.\nPerforming requirements analysis.\nDocumenting and communicating the results of your efforts.\nEffectively communicating your insights and plans to cross-functional team members and management.\nGathering critical information from meetings with various stakeholders and producing useful reports.\nWorking closely with clients, technicians, and managerial staff.\nProviding leadership, training, coaching, and guidance to junior staff.\nAllocating resources and maintaining cost efficiency.\nEnsuring solutions meet business needs and requirements.\nPerforming user acceptance testing.\nManaging projects, developing project plans, and monitoring performance.\nUpdating, implementing, and maintaining procedures.\nPrioritizing initiatives based on business needs and requirements.\nServing as a liaison between stakeholders and users.\nManaging competing resources and priorities.\nMonitoring deliverables and ensuring timely completion of projects.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['vendor management', 'project management', 'documentation', 'business analysis', 'process improvement', 'budgeting', 'strategic planning', 'operations', 'people management skills', 'service delivery', 'leadership', 'user acceptance testing', 'cost efficiency', 'reporting']",2025-06-10 15:22:55
Analyst,Wipro,2 - 6 years,Not Disclosed,['Gurugram'],"About The Role :\n\n\n\nRole Purpose\n\nThe purpose of the role is to provide effective technical support to the process and actively resolve client issues directly or through timely escalation to meet process SLAs.\n\n\nReconciliation\n\nJob Location:Gurgaon, Bangalore and Mumbai Shift Time:2:00 PM-11:30 PM\nJOB DESCRIPTION:\nResponsibilities:\nHands on experience with asset servicing, processing of paydown, interest and corporate actions for bonds- both mandatory and voluntary corporate actions.\nHands on experience of using Bloomberg / Refinitiv / DTC for checking trade tickets, accrued interest, corporate actions etc.\nGood to have knowledge on deal documents for private bonds. Good to have knowledge on waterfall payments.\nReconcile cash and positions across all funds on daily basis using DUCO platform\nWork across multiple groups to troubleshoot technical and accounting.\nEnsure all the activities are processed and updated in Geneva. Reach out to Agents / Custody / Internal team for resolving exceptions on daily basis.\nEscalation of unresolved cash and position breaks to Manager and onshore team\nCreate SOP and participate in training program\nLiaise with global teams and support ongoing projects, system integration, perform UAT, as needed\nReview and action incorrect PNL changes in the system (Geneva) vs Thirdparty or Day over day changes. Understanding of Accounting knowledge like PNL, Realized and Unrealized, FX PNL, Accrued Interest / Daily Interest on the various asset class like Bonds, Pvt Deal etc\nReview Deal Document / Credit Agreements / Notices for the asset setup in Geneva\nDesired Candidate Profile:\nKnowledge of Private Deals / Bonds\nProcessing experience of using Geneva as a tool for booking capital activities, bond paydowns and interest (both fixed and variable bond)\nReconciliation experience of Cash and Position using DUCO or any other Recon platform\nHands on experience of processing activities manually in Geneva related to Credit events.\nGood to have knowledge on private placements, Bloomberg, Refinitiv/DTC, Bond Trade lifecycle\nAwareness of Agents Banks, Trustees and Custodian role in supporting Private Credit assets\nStrong Microsoft Office skills (MS Excel, MS PowerPoint and MS Word) with high proficiency in Microsoft Excel\nHighly motivated and the ability to effectively work as an individual contributor and possess strong analytical, problem solving, critical thinking and decision-making skills, multitask and deliver under tight deadlines\nThe profile involves effective communication across Clients' facilities globally and hence possessing excellent interpersonal and communication skills in verbal and written English is a must\n\n\n\n\n\n\n\n\nDeliver excellent customer service through effective diagnosis and troubleshooting of client queries\nProvide product support and resolution to clients by performing a question diagnosis while guiding users through step-by-step solutions\nAssist clients with navigating around product menus and facilitate better understanding of product features\nTroubleshoot all client queries in a user-friendly, courteous and professional manner\nMaintain logs and records of all customer queries as per the standard procedures and guidelines\nAccurately process and record all incoming call and email using the designated tracking software\nOffer alternative solutions to clients (where appropriate) with the objective of retaining customers and clients business\nOrganize ideas and effectively communicate oral messages appropriate to listeners and situations\nFollow up and make scheduled call backs to customers to record feedback and ensure compliance to contract /SLAs\n\n\n\n\n\n\n\n\n\nBuild capability to ensure operational excellence and maintain superior customer service levels of the existing account/client\nUndertake product trainings to stay current with product features, changes and updates\nEnroll in product specific and any other trainings per client requirements/recommendations\nPartner with team leaders to brainstorm and identify training themes and learning issues to better serve the client\nUpdate job knowledge by participating in self learning opportunities and maintaining personal networks\n\n\n\n\n\n\n\n\nDeliver\nNoPerformance ParameterMeasure1ProcessNo. of cases resolved per day, compliance to process and quality standards, meeting process level SLAs, Pulse score, Customer feedback2Self- ManagementProductivity, efficiency, absenteeism, Training Hours, No of technical training completed",Industry Type: IT Services & Consulting,"Department: Customer Success, Service & Operations","Employment Type: Full Time, Permanent","['bloomberg', 'customer service', 'asset servicing', 'process compliance', 'waterfall', 'financial analysis', 'data analysis', 'team management', 'investment banking', 'accounting', 'corporate actions', 'technical support', 'trade', 'troubleshooting', 'operational excellence', 'finance']",2025-06-10 15:22:57
AI Product Leader,Smart Software Testing Solutions,12 - 17 years,Not Disclosed,['Noida'],"Opkey, we are disrupting the space of ERP transformation testing by building an AI-powered No Code Testing platform for Enterprise business applications (like Oracle Fusion Cloud, SAP S4Hana, SAP, Workday, Salesforce, and the likes). Opkey is a fast-growing VC-backed continuous end-to-end test automation software company headquartered in Dublin, California, with additional offices in Pittsburgh (opened in 2022), NYC (opened in 2022), & India (Noida & Bangalore). With the test automation market growing 20% annually, its estimated to reach $50 billion by 2026. Trusted by 250+ enterprise customers, including GAP, Pfizer, and KPMG.\nWe are seeking an experienced AI Product Leader with 12+ years of experience to drive the vision, strategy, and execution of our AI-powered ERP lifecycle optimization solutions. The ideal candidate will have deep expertise in artificial intelligence and enterprise software, with a proven track record of bringing innovative AI products to market. Experience with ERP systems is beneficial but not required.\nKey Responsibilities:\nLead the strategic vision and roadmap for Opkeys AI-powered solutions, including our ERP-specific domain SLM and agentic AI technologies\nTranslate business requirements into product specifications and guide cross-functional teams through the product development lifecycle\nCollaborate with engineering, data science, LLM and design teams to deliver high-quality AI products that address customer needs\nWork with domain experts to develop AI solutions that optimize ERP lifecycles\nEstablish metrics and KPIs to measure product success and drive continuous improvement\nStay current with advances in AI/ML technologies, particularly in NLP, agent-based systems, and enterprise automation\nPartner with sales and marketing teams to articulate the value proposition of our AI solutions to enterprise customers\nLead competitive analysis and market research to ensure product differentiation and market fit\nManage relationships with key technology partners and stakeholders\nRequired Qualifications:\n12+ years of experience in product management or product leadership roles, with at least 5 years focused on AI/ML products\nProven track record of successfully launching enterprise AI solutions\nDeep understanding of modern AI/ML technologies, particularly LLMs, NLP, and agent-based systems\nStrong technical background with the ability to collaborate effectively with data scientists and engineers\nExcellent communication and stakeholder management skills\nExperience in B2B SaaS product management and go-to-market strategy\nDemonstrated ability to drive innovation in complex enterprise environments\nGood to Have:\nExperience with ERP systems (SAP, Oracle, Workday, etc.) and enterprise software\nMBA or advanced degree in Computer Science, AI, or related field\nKnowledge of enterprise software integration challenges\nUnderstanding of service level management (SLM) in enterprise contexts\nSkills & Competencies:\nStrategic thinking and visionary leadership\nDeep technical understanding of AI/ML concepts and applications\nStrong product management fundamentals (market research, roadmapping, prioritization)\nExcellent communication and presentation skills\nAbility to translate complex technical concepts for business audiences\nData-driven decision making\nCross-functional leadership and influence\nCustomer-centric mindset with empathy for enterprise pain points\nAdaptability and comfort with ambiguity in a rapidly evolving field\nWillingness to learn about ERP systems and their specific challenges\nWhat We Offer:\nOpportunity to lead the development of groundbreaking AI solutions in the ERP space\nCompetitive compensation package including equity\nCollaborative and innovative work environment\nProfessional development and growth opportunities\nChance to work with cutting-edge AI technologies and shape the future of ERP management",Industry Type: IT Services & Consulting,Department: Product Management,"Employment Type: Full Time, Permanent","['Product management', 'Computer science', 'ERP', 'Automation', 'SAP', 'Artificial Intelligence', 'Market research', 'Oracle', 'Stakeholder management', 'Salesforce']",2025-06-10 15:23:00
Machine Learning Engineer,Cadfem Engineering Services,2 - 3 years,Not Disclosed,['Hyderabad'],"Design and develop machine learning models tailored to mechanical engineering challenges, including predictive modelling, simulation optimisation, and failure analysis.\nUtilise deep learning and other advanced ML techniques to improve the accuracy and efficiency of CAE simulations.\nPreprocess and analyse large datasets from CAE simulations, experimental tests, and manufacturing processes for modelling.\nTrain, validate, and fine-tune machine learning models using real-world engineering data.\nOptimise models for performance, scalability, and robustness in production environments.\nCollaborate with CAE engineers to integrate ML models into existing simulation workflows (e.g., FEA, CFD, structural analysis).\nAutomate repetitive simulation tasks and enable predictive analytics for design optimisation.\nWork closely with mechanical engineers, data scientists, and software developers to identify business challenges and develop data-driven solutions.\nDeploy machine learning models into production environments and monitor their performance.\nMaintain and update models to ensure reliability and continuous improvement.\nStay abreast of the latest advancements in machine learning, AI, and CAE technologies.\nApply innovative approaches to solve complex engineering problems.\n\n\nRequirements\nBachelor\\u2019s or Master\\u2019s degree in Mechanical Engineering, Computer Science, or a related field\nProven 2-3 years of experience in developing and deploying machine learning models, preferably in mechanical engineering or CAE domain\nHands-on experience with CAE tools such as ANSYS, Abaqus, or similar FEA/CFD software\nStrong programming skills in Python, R, or Java\nProficiency in machine learning frameworks (TensorFlow, PyTorch, scikit-learn)\nExperience with data preprocessing, feature engineering, and statistical analysis\nSolid understanding of mathematics, statistics, and problem-solving skills\nExcellent analytical thinking and ability to tackle complex engineering challenges\nStrong communication and teamwork skills to collaborate across disciplines\nPreferred: Experience with physics-informed machine learning and digital twin technologies\nPreferred: Familiarity with automation of CAE workflows and predictive modelling for product design\n\n\nBenefits\nChallenging job and a chance to team up with a young and dynamic professional group\nChance to build yourself as WE grow.\nRemuneration that stays competitive and attractive to retain the best.\nOpportunity to join an organization experiencing year on year growth",Industry Type: Engineering & Construction,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['CFD', 'Abaqus', 'Automation', 'Simulation', 'Analytical', 'Failure analysis', 'Ansys', 'Product design', 'Structural analysis', 'Python']",2025-06-10 15:23:03
Software Engineer II - Backend System,Seismic Technologies,3 - 6 years,Not Disclosed,['Hyderabad'],"As an Engineer II, you will play a crucial role in developing and optimizing backend systems that power our web application, including content discovery, knowledge management, learning and coaching, meeting intelligence and various AI capabilities. You will collaborate with cross-functional teams to design, build, and maintain scalable, high-performance systems that deliver exceptional value to our customers. This position offers a unique opportunity to make a significant impact on our company s growth and success by contributing to the technical excellence and innovation of our software solutions.\nIf you are a passionate technologist with a strong track record of building AI products, and you thrive in a fast-paced, innovative environment, we want to hear from you!\nWhat you ll be doing:\nDistributed Systems Development: Design, develop, and maintain backend systems and services for AI, information extraction or information retrieval functionality, ensuring high performance, scalability, and reliability.\nIntegration: Collaborate with data scientists, AI engineers, and product teams to integrate AI-driven capabilities across the Seismic platform.\nPerformance Tuning: Monitor and optimize service performance, addressing bottlenecks and ensuring low-latency query responses.\nTechnical Leadership: Provide technical guidance and mentorship to junior engineers, promoting best practices in software backend development.\nCollaboration: Work closely with cross-functional and geographically distributed teams, including product managers, frontend engineers, and UX designers, to deliver seamless and intuitive experiences.\nContinuous Improvement: Stay updated with the latest trends and advancements in software and technologies, conducting research and experimentation to drive innovation.\nWhat you bring to the team:\nExperience : 2+ years of experience in software engineering and a proven track record of building and scaling microservices and working with data retrieval systems.\nTechnical Expertise: Experience with C# and .NET, unit testing, object-oriented programming, and relational databases. Experience with Infrastructure as Code (Terraform, Pulumi, etc.), event driven architectures with tools like Kafka, feature management (Launch Darkly) is good to have. Front-end/full stack experience a plus.\nCloud Expertise : Experience with cloud platforms like AWS, Google Cloud Platform (GCP), or Microsoft Azure. Knowledge of cloud-native services for AI/ML, data storage, and processing. Experience deploying containerized applications into Kubernetes is a plus.\nAI: Proficiency in building and deploying Generative AI use cases is a plus. Experience with Natural Language Processing (NLP). Semantic search with platforms like ElasticSearch is a plus.\nSaaS Knowledge: Extensive experience in SaaS application development and cloud technologies, with a deep understanding of modern distributed systems and cloud operational infrastructure.\nProduct Development: Experience in collaborating with product management and design, with the ability to translate business requirements into technical solutions that drive successful delivery. Proven record of driving feature development from concept to launch.\nEducation: Bachelor s or Master s degree in Computer Science, Engineering, or a related field.\nFast-paced Environment: Experience working in a fast-paced, dynamic environment, preferably in a SaaS or technology-driven company.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Product management', 'Computer science', 'Performance tuning', 'Backend', 'Application development', 'Natural language processing', 'Unit testing', 'Continuous improvement', 'Distribution system', 'Operations']",2025-06-10 15:23:06
Data Engineer,TOP OEM,5 - 10 years,Not Disclosed,['Faridabad'],"Role & responsibilities\nAs a Data Engineer dedicated to projects, you will play a crucial role in designing and maintaining robust data architectures. This position requires 100% dedication to projects and Data Architecture Design: Design and implement scalable, reliable, and maintainable data architectures.\nData Integration: Develop ETL processes to integrate data from various sources into a centralized data warehouse. Ensure data quality and integrity throughout the integration process.\nDatabase Management: Administer and optimize databases for high performance and availability. Implement security measures to safeguard data against unauthorized access.\nData Modelling: Create and maintain data models for efficient storage and retrieval of information. Collaborate with data scientists and analysts to translate data needs into effective structures.\nCoding and Scripting: Utilize programming languages (e.g., Python, SQL) for developing and maintaining data pipelines.\nPerformance Monitoring: Monitor data processing systems to identify and resolve performance bottlenecks.\n\nGood to have Skills:\n1. Capability to create data pipelines for KPI Dashboards.\n2. Optimization of databases (Cloud) for efficient resource utilization.\n3. Expertise in various database technologies with the ability to apply technology to use cases.\n4. Experience with database caching, decoupling the database from reports.\n5. Proficient in using microservices for data ingestion.",Industry Type: Automobile,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'Data Engineering', 'Big Data', 'SQL']",2025-06-10 15:23:08
Senior Software Engineer - Python/AWS,Zenoti,4 - 6 years,Not Disclosed,['Hyderabad'],"As a Senior Software Engineer, you'll be a key player in our engineering lifecycle. you'll:\nDesign and architect robust, scalable, and maintainable software systems, translating business needs into technical designs.\nBuild with Python: Write, test, and deploy high-quality Python code for our production systems, focusing on clean, efficient, and we'll-documented solutions.\nLeverage AWS: Work hands-on with various AWS services (eg, EC2, S3, Lambda, API Gateway, DynamoDB, RDS) to build and manage our cloud infrastructure.\nBoost productivity with AI Tools: Actively utilize AI-powe'red development tools like Cursor IDE, GitHub Copilot, and other intelligent assistants to accelerate development and improve code quality.\nOptimize and troubleshoot: Identify and resolve complex technical challenges, optimize system performance, and ensure the overall health of our applications.\nCollaborate: Work closely with product managers and fellow engineers in an Agile environment, contributing to our collective growth.\nWhat skills do I need\nBachelors degree in Computer Science or IT,\n4 to 6 years of overall experience as an AI Engineer or Data Scientist.\nHaving minimum of 4 years experience in Python and AWS.\nExposure in designing and building scalable solutions using Cloud technologies.\nBenefits\nAttractive Compensation\nComprehensive medical coverage for yourself and your immediate family\nAn environment where we'llbeing is high on priority - access to regular yoga, meditation, breathwork, nutrition counseling, stress management, inclusion of family for most benefit awareness building sessions\nOpportunities to be a part of a community and give back: Social activities are part of our culture; You can look forward to regular engagement, social work, community give-back initiatives",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Software design', 'Nutrition', 'Customer retention', 'Agile', 'Stress management', 'AWS', 'Python', 'CRM']",2025-06-10 15:23:10
Business Analyst,Global Banking Organization,12 - 15 years,Not Disclosed,['Pune'],"Key Skills: Lending, Business Analyst, Lending Operations, Business Analysis, Retail, Retail Banking Operations\nRoles and Responsibilities:\nUnderstand and manage the full project lifecycle within banking and financial institutions.\nApply change adoption techniques to support transformation initiatives.\nExecute projects using Agile methodology with tools like Clarity, JIRA, MS Project, and Confluence.\nAnalyze and drive changes that benefit the bank, its customers, and stakeholders in the retail banking space.\nProvide expertise in lending products and retail banking concepts.\nSupport the design and enhancement of products, propositions, rewards, and partnerships.\nManage and support large, complex, multi-country retail banking projects.\nEngage with senior stakeholders to influence and drive project decisions.\nSolve problems efficiently while adhering to tight deadlines in fast-paced environments.\nTrack and report project status, risks, issues, and milestones.\nMaintain excellent communication and interpersonal relationships with all project stakeholders.\nOversee budget and financial management of projects.\nPromote consistency by sharing best practices across teams for managing complex, multi-market projects.\nSkills Required:\n10+ years of experience in project management in the banking and financial institutions.\nIn depth understanding of the project lifecycle.\nProficient in change adoption techniques.\nIn depth knowledge and experience of project execution in agile framework with extensive hands-on experience in tools such as Clarity/ JIRA/ MS Project/ confluence etc.\nStrong understanding of Retail banking & how change drives benefits for bank, customers and other stakeholders.\nStrong understanding of Retail banking concept specifically lending products.\nGood understating of products & propositions, rewards and partnerships.\nRelevant experience of working in complex retail banking projects across countries or regions.\nAbility to interact and influence senior stakeholders to drive decisions in achieving desired project outcome.\nProblem solving ability with adherence to stringent timelines in fast paced environment.\nExperience in project tracking (setting up project plan, managing risk and issue log, tollgates, reporting and governance)\nOutstanding communication and interpersonal skills.\nProficient in financial and budget management.\nDriving consistency & lead team/s by sharing best practices around how complex & multi market projects are defined, managed, and monitored.\nWhat additional skills will be good to have?\nPMP / Prince 2 / MSP/ PgMP/ Leading SAFe Certifications\nEducation: Bachelor's Degree in related field",Industry Type: Financial Services,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Retail', 'Lending', 'Business Analyst', 'Lending Operations', 'Retail Banking Operations', 'Business Analysis']",2025-06-10 15:23:12
Snowflake Architect,Blend360 India,8 - 13 years,Not Disclosed,['Hyderabad'],"We are looking for an experienced and hands-on Manager - Snowflake Data Engineering to lead and expand our data engineering capabilities. This role is ideal for a technically strong leader who can design scalable data architectures, manage a high-performing team, and collaborate cross-functionally to deliver reliable and secure data solutions.\nResponsibilities:\nDesign and implement robust Snowflake data warehouse architectures and ETL pipelines to support business intelligence and advanced analytics use cases.\nLead and mentor a team of data engineers, ensuring high-quality and timely delivery of projects.\nCollaborate closely with data analysts, data scientists, and business stakeholders to understand data needs and design effective data models.\nDevelop, document, and enforce best practices for Snowflake architecture, data modeling, performance optimization, and ETL processes.\nOwn the optimization of Snowflake environments to ensure low-latency and high-availability data access.\nDrive process improvements, evaluate emerging tools, and continuously enhance our data engineering infrastructure.\nEnsure data pipelines are built with high levels of accuracy, completeness, and security, in compliance with data privacy regulations (GDPR, CCPA, etc.).\nPartner with cloud engineering and DevOps teams to integrate data solutions seamlessly within the AWS ecosystem.\nParticipate in capacity planning, budgeting, and resource allocation for the data engineering function.\n\n\nBachelor s degree in Computer Science, Information Technology, or a related field.\n9+ years of overall experience in cloud-based data engineering, with at least 4 years of hands-on experience in Snowflake.",Industry Type: Industrial Automation,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Data modeling', 'data security', 'data governance', 'Engineering Manager', 'Business intelligence', 'Information technology', 'SQL', 'Python', 'Capacity planning']",2025-06-10 15:23:14
"Big Data Engineer (Spark, Scala) , SQL",Black and white Business Solution,7 - 10 years,Not Disclosed,"['Bhubaneswar', 'Pune', 'Bengaluru']","About Client\n\nHiring for One of the Most Prestigious Multinational Corporations\n\nJob Title: Big Data Engineer (Spark, Scala) , SQL\n\nExperience: 6 to 10 years\n\nKey Responsibilities:\nDesign, develop, and optimize scalable big data pipelines using Apache Spark and Scala.\nBuild batch and real-time data processing workflows to ingest, transform, and aggregate large datasets.\nWrite high-performance SQL queries to support data analysis and reporting.\nCollaborate with data architects, data scientists, and business stakeholders to understand requirements and deliver high-quality data solutions.\nEnsure data quality, integrity, and governance across systems.\nParticipate in code reviews and maintain best practices in data engineering.\nTroubleshoot and optimize performance of Spark jobs and SQL queries.\nMonitor and maintain production data pipelines and perform root cause analysis of data issues.\n\nTechnical Skills:\n\n6 to10 years of overall experience in software/data engineering.\n4+ years of hands-on experience with Apache Spark using Scala.\nStrong proficiency in Scala and functional programming concepts.\nExtensive experience with SQL (preferably in distributed databases like Hive, Presto, Snowflake, or BigQuery).\nExperience working in Hadoop ecosystem (HDFS, Hive, HBase, Oozie, etc.).\nKnowledge of data modeling, data architecture, and ETL frameworks.\nFamiliarity with version control (Git), CI/CD pipelines, and DevOps practices.\nExperience with cloud platforms (AWS, Azure, or GCP) is a plus.\nStrong analytical and problem-solving skills.\nExcellent communication and collaboration abilities.\n\nNotice period : Till 60 days\n\nLocation: BLR//BBSR/PUNE\n\nMode of Work :WFO(Work From Office)\n\n\nThanks & Regards,\nSWETHA\nBlack and White Business Solutions Pvt.Ltd.\nBangalore,Karnataka,INDIA.\nContact Number:8067432433\nrathy@blackwhite.in |www.blackwhite.in",Industry Type: IT Services & Consulting,Department: Other,"Employment Type: Full Time, Permanent","['spark', 'scala', 'Big data', 'SQL']",2025-06-10 15:23:16
Data Engineer - Senior,Iris Solutions,4 - 8 years,Not Disclosed,['Noida'],"Design, implement, and maintain data pipelines for processing large datasets, ensuring data availability, quality, and efficiency for machine learning model training and inference.\n\nCollaborate with data scientists to streamline the deployment of machine learning models, ensuring scalability, performance, and reliability in production environments.\n\nDevelop and optimize ETL (Extract, Transform, Load) processes, ensuring data flow from various sources into structured data storage systems.\n\nAutomate ML workflows using ML Ops tools and frameworks (e. g. , Kubeflow, MLflow, TensorFlow Extended (TFX)).\n\nEnsure effective model monitoring, versioning, and logging to track performance and metrics in a production setting.\n\nCollaborate with cross-functional teams to improve data architectures and facilitate the continuous integration and deployment of ML models.\n\nWork on data storage solutions, including databases, data lakes, and cloud-based storage systems (e. g. , AWS, GCP, Azure).\n\nEnsure data security, integrity, and compliance with data governance policies.\n\nPerform troubleshooting and root cause analysis on production-level machine learning systems.\n\nSkills: Glue, Pyspark, AWS Services, Strong in SQL; Nice to have : Redshift, Knowledge of SAS Dataset\n\nMandatory Competencies\nDevOps - CLOUD AWS\nDevOps - Docker\nETL - AWS Glue\nDevOps - Kubernetes\nDatabase - SQL\nBig Data - PySpark\nDatabase - Redshift\nCloud - Azure\nData Science - Azure ML\nData on Cloud - Azure Data Lake (ADL)\nBeh - Communication and collaboration\n\n\nAt Iris Software, we offer world-class benefits designed to support the financial, health and well-being needs of our associates to help achieve harmony between their professional and personal growth. From comprehensive health insurance and competitive salaries to flexible work arrangements and ongoing learning opportunities, were committed to providing a supportive and rewarding work environment.\n\nJoin us and experience the difference of working at a company that values its employees success and happiness.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Health insurance', 'data science', 'SAS', 'data security', 'GCP', 'Machine learning', 'Cloud', 'data governance', 'Troubleshooting', 'Monitoring']",2025-06-10 15:23:19
Aws Data Engineer,Innova Solutions,4 - 7 years,Not Disclosed,"['Hyderabad', 'Chennai', 'Bengaluru']","Role & responsibilities\nLooking For AWS data Eng-Immediate joiners for Hyderabad,Chennai,Noida,Pune,Bangalore locations.\nMandatory Skill-Python,Pyspark,SQL,Aws Glue\n\nStrong technical skills in services like S3,Athena, Lambda, RGlue and Glue(Pyspark), SQL, Data Warehousing, Informatica, OracleDesign, develop, and implement custom solutions within the Collibra platform to support data governance initiatives.\n\nPreferred candidate profile\nSnowflake, Agile methodology and Tableau. Proficiency in Python/Scala, Spark architecture, complex SQL, and RDBMS. Hands-on experience with ETL tools (e.g., Informatica) and SCD1, SCD2. 2-6 years of DWH, AWS Services and ETL design knowledge.\n\nDevelop ETL processes for data ingestion, transformation, and loading into data lakes and warehouses.\nCollaborate with data scientists and analysts to ensure data availability for analytics and reporting.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'Glue', 'ETL', 'Aws Cloud']",2025-06-10 15:23:21
Senior Data Engineer,Kryon Knowledge Works,6 - 9 years,Not Disclosed,['Hyderabad'],"We are looking for a skilled Data Engineer with hands-on experience in Airflow, Python, AWS, and Big Data technologies like Spark to join our dynamic team.\n\nKey Responsibilities\n\nDesign and implement data pipelines and workflows using Apache Airflow\nDevelop robust and scalable data processing applications using Python\nLeverage AWS services (S3, EMR, Lambda, Glue, Redshift, etc.) for data engineering and ETL pipelines\nWork with Big Data technologies like Apache Spark to process large-scale datasets\nOptimize and monitor data pipelines for performance, reliability, and scalability\nCollaborate with Data Scientists, Analysts, and Business teams to understand data needs and deliver solutions\nEnsure data quality, consistency, and governance across all data pipelines\nDocument processes, pipelines, and best practices\nMandatory Skills\n\nApache Airflow - workflow orchestration and scheduling\nPython - strong programming skills for data engineering\nAWS - hands-on experience with core AWS data services\nBig Data technologies particularly Apache Spark\n\nLocation: Hyderabad (Hybrid)\nPlease share your resume with +91 9361912009",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Airflow', 'python', 'Spark', 'AWS']",2025-06-10 15:23:24
Senior .Net Developer,Latinem,7 - 11 years,Not Disclosed,['Hyderabad'],"Role &\nAbout the Role:\nWe are seeking a skilled .NET Developer with experience on the Internet of Things (IoT) domain to join our dynamic team. The ideal candidate will have a strong background in software development using the .NET framework, along with hands-on experience in IoT solutions. You will be responsible for designing, developing, and maintaining software applications that integrate with IoT devices and platforms, ensuring seamless communication and data exchange.\nKey Responsibilities:\nDesign and Development: Design, develop, and maintain scalable and efficient .NET applications that interact with IoT devices and platforms.\nIoT Integration: Implement and manage IoT solutions, including device connectivity, data collection, and real-time processing.\nAPI Development: Develop and maintain RESTful APIs and other web services to facilitate communication between IoT devices and backend systems.\nData Management: Implement data storage solutions, including databases and cloud services, to manage and analyse IoT-generated data.\nSecurity: Ensure the security and integrity of IoT devices and data by implementing best practices in encryption, authentication, and authorization.\nTesting and Debugging: Conduct thorough testing and debugging of applications to ensure high performance, reliability, and scalability.\nCollaboration: Work closely with cross-functional teams, including hardware engineers, data scientists, and product managers, to deliver integrated IoT solutions.\nDocumentation: Create and maintain comprehensive documentation for software designs, APIs, and IoT integration processes.\nContinuous Improvement: Stay updated with the latest trends and advancements in IoT and .NET technologies and proactively suggest improvements to existing systems.\nQualifications:\nEducation: Bachelors degree in computer science, Information Technology, or a related field.\nExperience:\nMinimum of 7-10 years of experience in .NET development.\nProven experience in IoT domain, including device connectivity, data collection, and real-time processing.\nTechnical Skills:\nProficiency in C# and the .NET framework.\nExperience with IoT protocols such as MQTT, CoAP, and HTTP.\nFamiliarity with IoT platforms like Azure IoT Hub, AWS IoT, or Google Cloud IoT.\nStrong understanding of RESTful APIs and web services.\nExperience with databases (SQL Server, MongoDB, etc.) and cloud services (Azure, AWS, etc.).\nKnowledge of security best practices for IoT and web applications.\nFamiliarity with containerization and orchestration tools (Docker, Kubernetes) is a plus.\nSoft Skills:\nStrong problem-solving and analytical skills.\nExcellent communication and collaboration abilities.\nAbility to work independently and as part of a team.\nStrong attention to detail and a commitment to delivering high-quality software.\nPreferred Qualifications:\nExperience with edge computing and edge analytics.\nKnowledge of machine learning and data analytics in the context of IoT.\nFamiliarity with DevOps practices and CI/CD pipelines. responsibilities\n\n\nPreferred candidate profile",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['.NET', 'C#', 'MQTT', 'AWS IoT', 'Google Cloud IoT.', 'HTTP', 'MongoDB', 'CoAP', 'RESTful APIs', 'IoT', 'Azure IoT Hub', 'sql']",2025-06-10 15:23:26
Data Engineering,Stack Digital,3 - 7 years,Not Disclosed,"['Kolkata', 'Mumbai', 'New Delhi', 'Hyderabad', 'Pune', 'Chennai', 'Bengaluru']","Job Description\nDesign, develop, and optimize large-scale data processing pipelines using PySpark .\nWork with various Apache tools and frameworks (like Hadoop, Hive, HDFS, etc.) to ingest, transform, and manage large datasets.\nEnsure high performance and reliability of ETL jobs in production.\nCollaborate with Data Scientists, Analysts, and other stakeholders to understand data needs and deliver robust data solutions.\nImplement data quality checks and data lineage tracking for transparency and auditability.\nWork on data ingestion, transformation, and integration from multiple structured and unstructured sources.\nLeverage Apache NiFi for automated and repeatable data flow management (if applicable).\nWrite clean, efficient, and maintainable code in Python and Java .\nContribute to architectural decisions, performance tuning, and scalability planning.\nRequired Skills:\n5 7 years of experience.\nStrong hands-on experience with PySpark for distributed data processing.\nDeep understanding of Apache ecosystem (Hadoop, Hive, Spark, HDFS, etc.).\nSolid grasp of data warehousing , ETL principles , and data modeling .\nExperience working with large-scale datasets and performance optimization.\nFamiliarity with SQL and NoSQL databases.\nProficiency in Python and basic to intermediate knowledge of Java .\nExperience in using version control tools like Git and CI/CD pipelines.\nNice-to-Have Skills:\nWorking experience with Apache NiFi for data flow orchestration.\nExperience in building real-time streaming data pipelines .\nKnowledge of cloud platforms like AWS , Azure , or GCP .\nFamiliarity with containerization tools like Docker or orchestration tools like Kubernetes .\nSoft Skills:\nStrong analytical and problem-solving skills.\nExcellent communication and collaboration abilities.\nSelf-driven with the ability to work independently and as part of a team.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Version control', 'GIT', 'orchestration', 'Architecture', 'Data modeling', 'Analytical', 'Data processing', 'Data quality', 'Apache', 'SQL']",2025-06-10 15:23:29
ML Engineer,Prescience Decision Solutions,2 - 5 years,Not Disclosed,['Chennai'],"Prescience Decision Solutions is looking for ML Engineer to join our dynamic team and embark on a rewarding career journey.\n\nWe are seeking a highly skilled and motivated Machine Learning Engineer to join our dynamic team. The Machine Learning Engineer will be responsible for designing, developing, and deploying machine learning models to solve complex problems and enhance our products or services. The ideal candidate will have a strong background in machine learning algorithms, programming, and data analysis. Responsibilities : Problem Definition : Collaborate with cross - functional teams to define and understand business problems suitable for machine learning solutions. Translate business requirements into machine learning objectives. Data Exploration and Preparation : Analyze and preprocess large datasets to extract relevant features for model training. Address data quality issues and ensure data readiness for machine learning tasks. Model Development : Develop and implement machine learning models using state - of - the - art algorithms. Experiment with different models and approaches to achieve optimal performance. Training and Evaluation : Train machine learning models on diverse datasets and fine - tune hyperparameters. Evaluate model performance using appropriate metrics and iterate on improvements. Deployment : Deploy machine learning models into production environments. Collaborate with DevOps and IT teams to ensure smooth integration. Monitoring and Maintenance : Implement monitoring systems to track model performance in real - time. Regularly update and retrain models to adapt to evolving data patterns. Documentation : Document the entire machine learning development pipeline, from data preprocessing to model deployment. Create user guides and documentation for end - users and stakeholders. Collaboration : Collaborate with data scientists, software engineers, and domain experts to achieve project goals. Participate in cross - functional team meetings and knowledge - sharing sessions.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent",['ML Engineer'],2025-06-10 15:23:31
Business Analyst - L4,Wipro,2 - 6 years,Not Disclosed,['Bengaluru'],"About The Role :\n\nRole Purpose\n\nThe purpose of the role is to liaison and bridging the gap between customer and Wipro delivery team to comprehend and analyze customer requirements and articulating aptly to delivery teams thereby, ensuring right solutioning to the customer.\n\nDo\n\n1. Customer requirements gathering and engagement\nInterface and coordinate with client engagement partners to understand the RFP/ RFI requirements\nDetail out scope documents, functional & non-functional requirements, features etc ensuring all stated and unstated customer needs are captured\nConstruct workflow charts and diagrams, studying system capabilities, writing specification after thorough research and analysis of customer requirements\nEngage and interact with internal team - project managers, pre-sales team, tech leads, architects to design and formulate accurate and timely response to RFP/RFIs\nUnderstand and communicate the financial and operational impact of any changes\nPeriodic cadence with customers to seek clarifications and feedback wrt solution proposed for a particular RFP/ RFI and accordingly instructing delivery team to make changes in the design\nEmpower the customers through demonstration and presentation of the proposed solution/ prototype\nMaintain relationships with customers to optimize business integration and lead generation\nEnsure ongoing reviews and feedback from customers to improve and deliver better value (services/ products) to the customers\n2. Engage with delivery team to ensure right solution is proposed to the customer\n\na. Periodic cadence with delivery team to:\nProvide them with customer feedback/ inputs on the proposed solution\nReview the test cases to check 100% coverage of customer requirements\nConduct root cause analysis to understand the proposed solution/ demo/ prototype before sharing it with the customer\nDeploy and facilitate new change requests to cater to customer needs and requirements\nSupport QA team with periodic testing to ensure solutions meet the needs of businesses by giving timely inputs/feedback\nConduct Integration Testing and User Acceptance demo??s testing to validate implemented solutions and ensure 100% success rate\nUse data modelling practices to analyze the findings and design, develop improvements and changes\nEnsure 100% utilization by studying systems capabilities and understanding business specifications\nStitch the entire response/ solution proposed to the RFP/ RFI before its presented to the customer\n\nb. Support Project Manager/ Delivery Team in delivering the solution to the customer\nDefine and plan project milestones, phases and different elements involved in the project along with the principal consultant\nDrive and challenge the presumptions of delivery teams on how will they successfully execute their plans\nEnsure Customer Satisfaction through quality deliverable on time\n3. Build domain expertise and contribute to knowledge repository\nEngage and interact with other BA??s to share expertise and increase domain knowledge across the vertical\nWrite whitepapers/ research papers, point of views and share with the consulting community at large\nIdentify and create used cases for a different project/ account that can be brought at Wipro level for business enhancements\nConduct market research for content and development to provide latest inputs into the projects thereby ensuring customer delight\nDeliver\n\nNo.\n\nPerformance Parameter\n\nMeasure 1. Customer Engagement and Delivery Management PCSAT, utilization % achievement, no. of leads generated from the business interaction, no. of errors/ gaps in documenting customer requirements, feedback from project manager, process flow diagrams (quality and timeliness), % of deal solutioning completed within timeline, velocity generated. 2. Knowledge Management No. of whitepapers/ research papers written, no. of user stories created, % of proposal documentation completed and uploaded into knowledge repository, No of reusable components developed for proposal during quarter",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Business analysis', 'process flow diagram', 'project management', 'charts', 'client engagement', 'documentation', 'test cases', 'user stories', 'rfis', 'market research', 'change request', 'root cause analysis', 'prototype', 'demo', 'diagramming', 'rfi', 'integration testing', 'flow diagrams', 'rfp']",2025-06-10 15:23:33
Business Analyst - L4,Wipro,1 - 3 years,Not Disclosed,['Bengaluru'],"Role Purpose\n\nThe purpose of the role is to liaison and bridging the gap between customer and Wipro delivery team to comprehend and analyze customer requirements and articulating aptly to delivery teams thereby, ensuring right solutioning to the customer.\n\nDo\n\n1. Customer requirements gathering and engagement\nInterface and coordinate with client engagement partners to understand the RFP/ RFI requirements\nDetail out scope documents, functional & non-functional requirements, features etc ensuring all stated and unstated customer needs are captured\nConstruct workflow charts and diagrams, studying system capabilities, writing specification after thorough research and analysis of customer requirements\nEngage and interact with internal team - project managers, pre-sales team, tech leads, architects to design and formulate accurate and timely response to RFP/RFIs\nUnderstand and communicate the financial and operational impact of any changes\nPeriodic cadence with customers to seek clarifications and feedback wrt solution proposed for a particular RFP/ RFI and accordingly instructing delivery team to make changes in the design\nEmpower the customers through demonstration and presentation of the proposed solution/ prototype\nMaintain relationships with customers to optimize business integration and lead generation\nEnsure ongoing reviews and feedback from customers to improve and deliver better value (services/ products) to the customers\n2. Engage with delivery team to ensure right solution is proposed to the customer\n\na. Periodic cadence with delivery team to\nProvide them with customer feedback/ inputs on the proposed solution\nReview the test cases to check 100% coverage of customer requirements\nConduct root cause analysis to understand the proposed solution/ demo/ prototype before sharing it with the customer\nDeploy and facilitate new change requests to cater to customer needs and requirements\nSupport QA team with periodic testing to ensure solutions meet the needs of businesses by giving timely inputs/feedback\nConduct Integration Testing and User Acceptance demos testing to validate implemented solutions and ensure 100% success rate\nUse data modelling practices to analyze the findings and design, develop improvements and changes\nEnsure 100% utilization by studying systems capabilities and understanding business specifications\nStitch the entire response/ solution proposed to the RFP/ RFI before its presented to the customer\n\nb. Support Project Manager/ Delivery Team in delivering the solution to the customer\nDefine and plan project milestones, phases and different elements involved in the project along with the principal consultant\nDrive and challenge the presumptions of delivery teams on how will they successfully execute their plans\nEnsure Customer Satisfaction through quality deliverable on time\n3. Build domain expertise and contribute to knowledge repository\nEngage and interact with other BAs to share expertise and increase domain knowledge across the vertical\nWrite whitepapers/ research papers, point of views and share with the consulting community at large\nIdentify and create used cases for a different project/ account that can be brought at Wipro level for business enhancements\nConduct market research for content and development to provide latest inputs into the projects thereby ensuring customer delight\nDeliver\n\nNo.\n\nPerformance Parameter\n\nMeasure\n1. Customer Engagement and Delivery Management PCSAT, utilization % achievement, no. of leads generated from the business interaction, no. of errors/ gaps in documenting customer requirements, feedback from project manager, process flow diagrams (quality and timeliness), % of deal solutioning completed within timeline, velocity generated.\n2. Knowledge Management No. of whitepapers/ research papers written, no. of user stories created, % of proposal documentation completed and uploaded into knowledge repository, No of reusable components developed for proposal during quarter",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Business Analysis', 'QA', 'Knowledge Management', 'Integration Testing', 'UAT', 'Customer Engagement', 'Delivery Management']",2025-06-10 15:23:35
Business Analyst - L3,Wipro,3 - 5 years,Not Disclosed,['Bengaluru'],"Role Purpose\nThe purpose of the role is to liaison and bridging the gap between customer and Wipro delivery team to comprehend and analyze customer requirements and articulating aptly to delivery teams thereby, ensuring right solutioning to the customer.\nDo\n1. Customer requirements gathering and engagement\nInterface and coordinate with client engagement partners to understand the RFP/ RFI requirements\nDetail out scope documents, functional & non-functional requirements, features etc ensuring all stated and unstated customer needs are captured\nConstruct workflow charts and diagrams, studying system capabilities, writing specification after thorough research and analysis of customer requirements\nEngage and interact with internal team - project managers, pre-sales team, tech leads, architects to design and formulate accurate and timely response to RFP/RFIs\nUnderstand and communicate the financial and operational impact of any changes\nPeriodic cadence with customers to seek clarifications and feedback wrt solution proposed for a particular RFP/ RFI and accordingly instructing delivery team to make changes in the design\nEmpower the customers through demonstration and presentation of the proposed solution/ prototype\nMaintain relationships with customers to optimize business integration and lead generation\nEnsure ongoing reviews and feedback from customers to improve and deliver better value (services/ products) to the customers\n2. Engage with delivery team to ensure right solution is proposed to the customer\na. Periodic cadence with delivery team to:\nProvide them with customer feedback/ inputs on the proposed solution\nReview the test cases to check 100% coverage of customer requirements\nConduct root cause analysis to understand the proposed solution/ demo/ prototype before sharing it with the customer\nDeploy and facilitate new change requests to cater to customer needs and requirements\nSupport QA team with periodic testing to ensure solutions meet the needs of businesses by giving timely inputs/feedback\nConduct Integration Testing and User Acceptance demos testing to validate implemented solutions and ensure 100% success rate\nUse data modelling practices to analyze the findings and design, develop improvements and changes\nEnsure 100% utilization by studying systems capabilities and understanding business specifications\nStitch the entire response/ solution proposed to the RFP/ RFI before its presented to the customer\nb. Support Project Manager/ Delivery Team in delivering the solution to the customer\nDefine and plan project milestones, phases and different elements involved in the project along with the principal consultant\nDrive and challenge the presumptions of delivery teams on how will they successfully execute their plans\nEnsure Customer Satisfaction through quality deliverable on time\n3. Build domain expertise and contribute to knowledge repository\nEngage and interact with other BAs to share expertise and increase domain knowledge across the vertical\nWrite whitepapers/ research papers, point of views and share with the consulting community at large\nIdentify and create used cases for a different project/ account that can be brought at Wipro level for business enhancements\nConduct market research for content and development to provide latest inputs into the projects thereby ensuring customer delight\nDeliver / No . / Performance Parameter / Measure\n1.Customer Engagement and Delivery Management PCSAT, utilization % achievement, no. of leads generated from the business interaction, no. of errors/ gaps in documenting customer requirements, feedback from project manager, process flow diagrams (quality and timeliness), % of deal solutioning completed within timeline, velocity generated.\n2.Knowledge Management No. of whitepapers/ research papers written, no. of user stories created, % of proposal documentation completed and uploaded into knowledge repository, No of reusable components developed for proposal during quarter\nMandatory Skills: ServiceNow - Platform Core.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['ServiceNow', 'data modelling', 'root cause analysis', 'proposal documentation', 'Delivery Management']",2025-06-10 15:23:38
Analyst,Aegis Media,0 - 2 years,Not Disclosed,"['New Delhi', 'Pune', 'Gurugram', 'Bengaluru']","Marketing Measurement & Optimization Analyst\nJob Description:\nQualifications:\nBachelors degree in Statistics, Mathematics, Computer Science, Engineering, or a related field.\nProven 0-2 years of experience in a similar role.\nStrong problem-solving skills.\nExcellent communication skills.\nSkills:\nProficiency in R (tidyverse, plotly/ggplot2), or Python (pandas, numpy), for data manipulation and visualization, and SQL (joins, aggregation, analytics functions) for data handling.\nAbility to understand marketing data and perform statistical tests.\nKnowledge of data visualization tools such as Tableau or Power BI.\nResponsibilities:\nFamiliar with Media Mix Modelling, Multi-Touch Attribution.\nKnowledge of panel data and its analysis.\nUnderstand of Data Science workflow.\nFamiliarity with marketing channels, performance & effectiveness metrics, and conversion funnel.\nWork with large data and performing data QA & manipulation tasks such as joins/merge, aggregation & segregation, append.",Industry Type: Advertising & Marketing,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'QA', 'data science', 'data manipulation', 'Workflow', 'power bi', 'data visualization', 'Analytics', 'SQL', 'Python']",2025-06-10 15:23:40
Analyst,Wipro,2 - 6 years,Not Disclosed,"['Mumbai', 'Gurugram', 'Bengaluru']","About The Role :\n\nJob Location:Gurgaon, Mumbai, Bangalore\nTime:2:00 PM-11:30 PM\nWFO Only\nJOB DESCRIPTION:\nResponsibilities:\nHands on experience with asset servicing, processing principal activities like Drawdown, Paydown, Interest in Geneva\nReconcile cash and positions across all funds on daily basis using DUCO platform\nReviewing upcoming activities exceptions from Geneva WSO accelerator, contract modifications as per the agent notices.\nWork across multiple groups to troubleshoot technical and accounting problems\nEnsure all the activities are processed and updated in Geneva. Reach out to Agents / Trustee / Internal team for resolving exceptions on daily basis.\nEscalation of unresolved cash and position breaks to Manager and onshore team\nCreate SOP and participate in training program\nLiaise with global teams and support ongoing projects, system integration, perform UAT, as needed Desired Candidate Profile:\nKnowledge of Syndicated Bankloan/ Private Credit / CLOs\nProcessing experience of using Geneva as a software tool for Syndicated Bank debt Module\nReconciliation experience of Cash and Position using DUCO or any other Recon platform\nHands on experience of processing activities manually in Geneva related to Credit Facilities in Geneva\nAwareness of Agents Banks, Trustees and Custodian role in supporting Private Credit\nStrong Microsoft Office skills (MS Excel, MS PowerPoint and MS Word) with high proficiency in Microsoft Excel\nHighly motivated and the ability to effectively work as an individual contributor and possess strong analytical, problem solving, critical thinking and decision-making skills, multitask and deliver under tight deadlines\nThe profile involves effective communication across Clients'' facilities globally and hence possessing excellent interpersonal and communication skills in verbal and written English is a must\nA demonstrated ability to write effectively and summarize large amounts of information succinctly and quickly\nA desire to work in an international team environment, often under pressure and with multiple stakeholder\nBuild capability to ensure operational excellence and maintain superior customer service levels of the existing account/client\nUndertake product trainings to stay current with product features, changes and updates\nEnroll in product specific and any other trainings per client requirements/recommendations\nPartner with team leaders to brainstorm and identify training themes and learning issues to better serve the client\nUpdate job knowledge by participating in self learning opportunities and maintaining personal networks\nDeliver\nNoPerformance ParameterMeasure\n1ProcessNo. of cases resolved per day, compliance to process and quality standards, meeting process level SLAs, Pulse score, Customer feedback\n2Self- ManagementProductivity, efficiency, absenteeism, Training Hours, No of technical training completed",Industry Type: IT Services & Consulting,Department: Finance & Accounting,"Employment Type: Full Time, Permanent","['customer service', 'financial analysis', 'operations management', 'project management', 'data analysis', 'team management', 'geneva', 'investment banking', 'accounting', 'corporate actions', 'asset servicing', 'process compliance', 'operational excellence', 'finance']",2025-06-10 15:23:42
Business Analyst - L3,Wipro,3 - 5 years,Not Disclosed,['Bengaluru'],"Wipro Limited (NYSEWIT, BSE507685, NSEWIPRO) is a leading technology services and consulting company focused on building innovative solutions that address clients’ most complex digital transformation needs. Leveraging our holistic portfolio of capabilities in consulting, design, engineering, and operations, we help clients realize their boldest ambitions and build future-ready, sustainable businesses. With over 230,000 employees and business partners across 65 countries, we deliver on the promise of helping our customers, colleagues, and communities thrive in an ever-changing world. For additional information, visit us at www.wipro.com.\n About The Role  _x000D_\n\nRole Purpose\n\nThe purpose of the role is to liaison and bridging the gap between customer and Wipro delivery team to comprehend and analyze customer requirements and articulating aptly to delivery teams thereby, ensuring right solutioning to the customer.\n\n ? _x000D_\n\nDo\n\n1. Customer requirements gathering and engagement\nInterface and coordinate with client engagement partners to understand the RFP/ RFI requirements\nDetail out scope documents, functional & non-functional requirements, features etc ensuring all stated and unstated customer needs are captured\nConstruct workflow charts and diagrams, studying system capabilities, writing specification after thorough research and analysis of customer requirements\nEngage and interact with internal team - project managers, pre-sales team, tech leads, architects to design and formulate accurate and timely response to RFP/RFIs\nUnderstand and communicate the financial and operational impact of any changes\nPeriodic cadence with customers to seek clarifications and feedback wrt solution proposed for a particular RFP/ RFI and accordingly instructing delivery team to make changes in the design\nEmpower the customers through demonstration and presentation of the proposed solution/ prototype\nMaintain relationships with customers to optimize business integration and lead generation\nEnsure ongoing reviews and feedback from customers to improve and deliver better value (services/ products) to the customers\n\n\n ? _x000D_\n\n2. Engage with delivery team to ensure right solution is proposed to the customer\n\na. Periodic cadence with delivery team to\nProvide them with customer feedback/ inputs on the proposed solution\nReview the test cases to check 100% coverage of customer requirements\nConduct root cause analysis to understand the proposed solution/ demo/ prototype before sharing it with the customer\nDeploy and facilitate new change requests to cater to customer needs and requirements\nSupport QA team with periodic testing to ensure solutions meet the needs of businesses by giving timely inputs/feedback\nConduct Integration Testing and User Acceptance demo’s testing to validate implemented solutions and ensure 100% success rate\nUse data modelling practices to analyze the findings and design, develop improvements and changes\nEnsure 100% utilization by studying systems capabilities and understanding business specifications\nStitch the entire response/ solution proposed to the RFP/ RFI before its presented to the customer\n\nb. Support Project Manager/ Delivery Team in delivering the solution to the customer\nDefine and plan project milestones, phases and different elements involved in the project along with the principal consultant\nDrive and challenge the presumptions of delivery teams on how will they successfully execute their plans\nEnsure Customer Satisfaction through quality deliverable on time\n\n\n ? _x000D_\n\n3. Build domain expertise and contribute to knowledge repository\nEngage and interact with other BA’s to share expertise and increase domain knowledge across the vertical\nWrite whitepapers/ research papers, point of views and share with the consulting community at large\nIdentify and create used cases for a different project/ account that can be brought at Wipro level for business enhancements\nConduct market research for content and development to provide latest inputs into the projects thereby ensuring customer delight\n\n\n ? _x000D_\n\nDeliver\n\n\nNo.\n\nPerformance Parameter\n\nMeasure 1. Customer Engagement and Delivery Management PCSAT, utilization % achievement, no. of leads generated from the business interaction, no. of errors/ gaps in documenting customer requirements, feedback from project manager, process flow diagrams (quality and timeliness), % of deal solutioning completed within timeline, velocity generated. 2. Knowledge Management No. of whitepapers/ research papers written, no. of user stories created, % of proposal documentation completed and uploaded into knowledge repository, No of reusable components developed for proposal during quarter\n\nMandatory\n\nSkills:\nServiceNow - Platform Core_x000D_.\n\nExperience3-5 Years_x000D_.\n\nReinvent your world. We are building a modern Wipro. We are an end-to-end digital transformation partner with the boldest ambitions. To realize them, we need people inspired by reinvention. Of yourself, your career, and your skills. We want to see the constant evolution of our business and our industry. It has always been in our DNA - as the world around us changes, so do we. Join a business powered by purpose and a place that empowers you to design your own reinvention. Come to Wipro. Realize your ambitions. Applications from people with disabilities are explicitly welcome.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['project management', 'software testing', 'business analysis', 'root cause analysis', 'servicenow', 'client engagement', 'documentation', 'test cases', 'change request', 'user stories', 'rfis', 'prototype', 'rfi', 'integration testing', 'flow diagrams', 'rfp', 'digital transformation']",2025-06-10 15:23:45
"Manager, Machine Learning",Databricks,3 - 8 years,Not Disclosed,[],"The Machine Learning (ML) Practice team is a highly specialized, collaborative customer-facing ML team at Databricks. we'deliver professional services engagements to help our customers build, scale, and productionize the most cutting-edge ML and GenAI applications. We work cross-functionally to shape long-term strategic priorities and initiatives alongside engineering, product, and developer relations, as we'll as support internal subject matter expert (SME) teams.\nWe are investing heavily in continuing to grow our global team, and looking for a world-class Manager to lead our ML Practice in India. You will report directly to our Global Delivery Center (GDC) Leader and dotted line to our ML Professional Services Global Leader. This role can be remote, with a preference for candidates located in Bangalore.\n  The impact you will have:\nManage hiring, onboarding, and growing the ML Practice team in India, roughly doubling the team size by the end of the year.\nDevelop relationships with key customers and partners, scope customer engagements, and manage escalations\nAlign with Professional Services and Sales leaders both in the Global Delivery Center and globally\nCollaborate cross-functionally with the RD team to define priorities and influence the product roadmap\nLead strategic PS initiatives, practice development, and processes\nOwn OKRs for revenue and utilization, with a focus on driving Databricks consumption\nLead Databricks cultural values by example and champion Databricks brand\nWhat we look for:\n3+ years experience managing, hiring, and building a team of motivated data scientists/ML engineers, including establishing programs and processes\nDeep hands-on technical understanding of data science, ML, GenAI and the latest trends\nWhile managers do not deliver customer engagements directly, we expect that individuals are able to get hands-on in the Databricks product to demo to customers, dogfood product features to provide input to RD, etc\nExperience building production-grade machine learning deployments on AWS, Azure, or GCP\nPassion for collaboration, life-long learning, and driving business value through ML\nCompany first focus and collaborative individuals - we work better when we work together\nGraduate degree in a quantitative discipline (Computer Science, Engineering, Statistics, Operations Research, etc) or equivalent practical experience\n[Preferred] Experience working with Databricks and Apache Spark\n[Preferred] Experience working in a customer-facing role",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['SAN', 'Operations research', 'GCP', 'spark', 'Diversity and Inclusion', 'Machine learning', 'Data analytics', 'Subject Matter Expert']",2025-06-10 15:23:47
Data / Analytics Engineer,Service based Top B2C/B2B MNC in Analyti...,6 - 10 years,Not Disclosed,"['Pune', 'Gurugram', 'Bengaluru']","Job Description:\nWe are looking for a skilled Data / Analytics Engineer with hands-on experience in vector databases and search optimization techniques. You will help build scalable, high-performance infrastructure to support AI-powered applications like semantic search, recommendation systems, and RAG pipelines.\nKey Responsibilities:\nOptimize vector search algorithms for performance and scalability.\nBuild pipelines to process high-dimensional embeddings (e.g., BERT, CLIP, OpenAI).\nImplement ANN indexing techniques like HNSW, IVF, PQ.\nIntegrate vector search with data platforms and APIs.\nCollaborate with cross-functional teams (data scientists, engineers, product).\nMonitor and resolve latency, throughput, and scaling issues.\nMust-Have Skills:\nPython\nAWS\nVector Databases (e.g., Elasticsearch, FAISS, Pinecone)\nVector Search / Similarity Search\nANN Search Algorithms HNSW, IVF, PQ\nSnowflake / Databricks\nEmbedding Models – BERT, CLIP, OpenAI\nKafka / Flink for real-time data pipelines\nREST APIs, GraphQL, or gRPC for integration\nGood to Have:\nKnowledge of semantic caching and hybrid retrieval\nExperience with distributed systems and high-performance computing\nFamiliarity with RAG (Retrieval-Augmented Generation) workflows\nApply Now if You:\nEnjoy solving performance bottlenecks in AI infrastructure\nLove working with cutting-edge ML models and search technologies\nThrive in collaborative, fast-paced environments",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Vector DB', 'AWS', 'Python', 'PQ', 'ANN Search', 'GraphQL', 'Kafka', 'HNSW', 'Flink', 'Vector Search', 'gRPC', 'CLIP', 'Snowflake', 'IVF', 'BERT', 'OpenAI', 'Databricks', 'REST APIs']",2025-06-10 15:23:49
Product Manager,Masters India,5 - 8 years,Not Disclosed,['Noida'],"Job Description\nWe re rethinking the product lifecycle in an era where AI tools like ChatGPT, Manus, and Lovable let us go from problem-definition to a working prototype in hours not weeks. If you believe that intelligent automation should be our co-pilot at every step, this is the role for you.\nKey Responsibilities\nProblem Discovery & Framing\nRapidly surface customer pain points through user interviews, AI-powered data analysis, and heuristic evaluation.\nAI-First Prototyping\nLeverage conversational AI, generative models, and automated design/code assistants (e.g. ChatGPT, Manus, Lovable) to assemble clickable prototypes, MVPs, or live demos in hours.\nRoadmap & Metrics\nDefine and iterate on key outcomes (engagement, retention, ROI), using AI dashboards and analytics to guide prioritization.\nCross-Functional Leadership\nOrchestrate designers, engineers, data scientists, and Growth/Marketing to turn AI-generated insights into production-ready features at breakneck speed.\nContinuous Feedback Loop\nEmbed AI-driven user testing and A/B experimentation directly into development pipelines for real-time learning and iteration.\n\n\nQualifications\nWhat You Bring\nAI Tool Mastery\nHands-on experience with ChatGPT (or equivalents), prompt engineering, and other leading AI assistants.",Industry Type: FinTech / Payments,Department: Product Management,"Employment Type: Full Time, Permanent","['Loop', 'Automation', 'Data analysis', 'Head Business Development', 'Translation', 'Prototype', 'Focus', 'product life cycle', 'QA testing', 'Analytics']",2025-06-10 15:23:52
Java Back End Developer,Elvago Technologies,3 - 5 years,15-18 Lacs P.A.,['Bengaluru'],"Key Responsibilities:\nDevelop and maintain backend services and APIs using Java (Spring Boot preferred).\nIntegrate LLMs and GenAI models (e.g., OpenAI, Hugging Face, LangChain) into applications.\nCollaborate with data scientists to build data pipelines and enable intelligent application features.\nDesign scalable systems to support AI model inference and deployment.\nWork with cloud platforms (AWS, GCP, or Azure) for deploying AI-driven services.\nWrite clean, maintainable, and well-tested code.\nParticipate in code reviews and technical discussions.\nRequired Skills:\n35 years of experience in Java development (preferably with Spring Boot).\nExperience working with RESTful APIs, microservices, and cloud-based deployments.\nExposure to LLMs, NLP, or GenAI tools (OpenAI, Cohere, Hugging Face, LangChain, etc.).\nFamiliarity with Python for data science/ML integration is a plus.\nGood understanding of software engineering best practices (CI/CD, testing, etc.).\nAbility to work collaboratively in cross-functional teams.",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Java', 'Generative Ai', 'Spring Boot', 'Generative Ai Tools', 'Kafka', 'Ci/Cd', 'Microservices', 'SQL', 'Rabbitmq', 'Docker', 'Large Language Model', 'MySQL', 'MongoDB', 'AWS', 'Kubernetes']",2025-06-10 15:23:55
NLP Data Engineer - Risk Insights & Monitoring,MNC IT,10 - 12 years,25-30 Lacs P.A.,"['Pune', 'Mumbai (All Areas)']","Design and implement state-of-the-art NLP models, including but not limited to text classification, semantic search, sentiment analysis, named entity recognition, and summary generation.\nconduct data preprocessing, and feature engineering to improve model accuracy and performance.\nStay updated with the latest developments in NLP and ML, and integrate cutting-edge techniques into our solutions.\ncollaborate with Cross-Functional Teams: Work closely with data scientists, software engineers, and product managers to align NLP projects with business objectives.\ndeploy models into production environments and monitor their performance to ensure robustness and reliability.\nmaintain comprehensive documentation of processes, models, and experiments, and report findings to stakeholders.\nimplement and deliver high quality software solutions / components for the Credit Risk monitoring platform.\nleverage his/her expertise to mentor developers; review code and ensure adherence to standards.\napply a broad range of software engineering practices, from analyzing user needs and developing new features to automated testing and deployment\nensure the quality, security, reliability, and compliance of our solutions by applying our digital principles and implementing both functional and non-functional requirements\nbuild observability into our solutions, monitor production health, help to resolve incidents, and remediate the root cause of risks and issues\nunderstand, represent, and advocate for client needs\nshare knowledge and expertise with colleagues , help with hiring, and contribute regularly to our engineering culture and internal communities.\nExpertise -\nBachelor of Engineering or equivalent.\nIdeally 8-10Yrs years of experience in NLP based applications focused on Banking / Finance sector.\nPreference for experience in financial data extraction and classification.\nInterested in learning new technologies and practices, reuse strategic platforms and standards, evaluate options, and make decisions with long-term sustainability in mind.\nProficiency in programming languages such as Python & Java. Experience with frameworks like TensorFlow, PyTorch, or Keras.\nIn-depth knowledge of NLP techniques and tools, including spaCy, NLTK, and Hugging Face.\nExperience with data handling and processing tools like Pandas, NumPy, and SQL.\nPrior experience in agentic AI, LLMs ,prompt engineering and generative AI is a plus.\nBackend development and microservices using Java Spring Boot, J2EE, REST for implementing projects with high SLA of data availability and data quality.\nExperience of building cloud ready and migrating applications using Azure and understanding of the Azure Native Cloud services, software design and enterprise integration patterns.\nKnowledge of SQL and PL/SQL (Oracle) and UNIX, writing queries, packages, working with joins, partitions, looking at execution plans, and tuning queries.\nA real passion for and experience of Agile working practices, with a strong desire to work with baked in quality subject areas such as TDD, BDD, test automation and DevOps principles\nExperience in Azure development including Databricks , Azure Services , ADLS etc.\nExperience using DevOps toolsets like GitLab, Jenkins",Industry Type: IT Services & Consulting,Department: Project & Program Management,"Employment Type: Full Time, Permanent","['data engineer', 'Natural Language Processing', 'Python', 'Java']",2025-06-10 15:23:56
Sas Administrator,Encora,4 - 9 years,Not Disclosed,"['Hyderabad', 'Bengaluru']","Job Title: SAS Administrator\nExperience: 4-9 Years\nLocation: Bangalore/Hyderabad\n\nJob Summary:\nWe are seeking a skilled SAS Administrator to manage and maintain our SAS Analytics Environment. The ideal candidate will have 4-8 years of experience in SAS platform administration, ensuring optimal performance, security, and reliability of SAS applications.",,,,['SAS Admin'],2025-06-10 15:23:59
Snowflake Architect,Blend360 India,8 - 13 years,Not Disclosed,['Hyderabad'],"We are looking for an experienced and hands-on Snowflake Architect to lead and expand our data engineering capabilities. This role is ideal for a technically strong leader who can design scalable data architectures, manage a high-performing team, and collaborate cross-functionally to deliver reliable and secure data solutions.\nResponsibilities:\nDesign and implement robust Snowflake data warehouse architectures and ETL pipelines to support business intelligence and advanced analytics use cases.\nLead and mentor a team of data engineers, ensuring high-quality and timely delivery of projects.\nCollaborate closely with data analysts, data scientists, and business stakeholders to understand data needs and design effective data models.\nDevelop, document, and enforce best practices for Snowflake architecture, data modeling, performance optimization, and ETL processes.\nOwn the optimization of Snowflake environments to ensure low-latency and high-availability data access.\nDrive process improvements, evaluate emerging tools, and continuously enhance our data engineering infrastructure.\nEnsure data pipelines are built with high levels of accuracy, completeness, and security, in compliance with data privacy regulations (GDPR, CCPA, etc.).\nPartner with cloud engineering and DevOps teams to integrate data solutions seamlessly within the AWS ecosystem.\nParticipate in capacity planning, budgeting, and resource allocation for the data engineering function.\n\n\nBachelor s degree in Computer Science, Information Technology, or a related field.\n9+ years of overall experience in cloud-based data engineering, with at least 4 years of hands-on experience in Snowflake.",Industry Type: Industrial Automation,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Architect', 'Data modeling', 'data security', 'data governance', 'Business intelligence', 'Information technology', 'SQL', 'Python', 'Capacity planning']",2025-06-10 15:24:01
AI Solution Architect,L&T Technology Services (LTTS),6 - 11 years,Not Disclosed,['Mumbai'],"Job Summary:\n\nWe are seeking an experienced AI Solution Architect to lead the design and implementation of scalable AI/ML systems and solutions. The ideal candidate will bridge the gap between business needs and technical execution, translating complex problems into AI-powered solutions that drive strategic value.\n\n\nKey Responsibilities:\nSolution Design & Architecture\nDesign end-to-end AI/ML architectures, integrating with existing enterprise systems and cloud infrastructure.\nLead technical planning, proof-of-concepts (POCs), and solution prototyping for AI use cases.\nAI/ML System Development\nCollaborate with data scientists, engineers, and stakeholders to define solution requirements.\nGuide the selection and use of machine learning frameworks (e.g., TensorFlow, PyTorch, Hugging Face).\nEvaluate and incorporate LLMs, computer vision, NLP, and generative AI models as appropriate.\nTechnical Leadership\nProvide architectural oversight and code reviews to ensure high-quality delivery.\nEstablish best practices for AI model lifecycle management (training, deployment, monitoring).\nAdvocate for ethical and responsible AI practices including bias mitigation and model transparency.\nStakeholder Engagement\nPartner with business units to understand goals and align AI solutions accordingly.\nCommunicate complex AI concepts to non-technical audiences and influence executive stakeholders.\nInnovation & Strategy\nStay updated on emerging AI technologies, trends, and industry standards.\nDrive innovation initiatives to create competitive advantage through AI.\n\n\nRequired Qualifications:\nBachelor s or Master s in Computer Science, Engineering, Data Science, or related field.\n6 years of experience in software development or data engineering roles.\n3+ years of experience designing and delivering AI/ML solutions.\nProficiency in cloud platforms (AWS, Azure, GCP) and container orchestration (Kubernetes, Docker).\nHands-on experience with MLOps tools (e.g., MLflow, Kubeflow, SageMaker, Vertex AI).\nStrong programming skills in Python and familiarity with CI/CD pipelines.\n\n\nPreferred Qualifications:\nExperience with generative AI, foundation models, or large language models (LLMs).\nKnowledge of data privacy regulations (e.g., GDPR, HIPAA) and AI compliance frameworks.\nCertifications in cloud architecture or AI (e.g., AWS Certified Machine Learning, Google Professional ML Engineer).\nStrong analytical, communication, and project management skills.",Industry Type: Electronic Components / Semiconductors,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Architecture', 'Project management', 'Analytical', 'Machine learning', 'Cloud', 'Technical leadership', 'AWS', 'Monitoring', 'Solution Architect', 'Python']",2025-06-10 15:24:04
AI Engineer - Technical Consultant,Saxon Global,8 - 10 years,Not Disclosed,"['Kolkata', 'Hyderabad']","Design, develop, & optimize AI-powered apps\nImplement large language models (LLMs) & generative AI apps\nIntegrate Copilot Studio, Azure AI Search, and other AI/ML platforms into enterprise solutions\nRAG (Retrieval-Augmented Generation) architecture\n\nRequired Candidate profile\n7+ yrs of exp in Python, C#, Java, JavaScript, C++\nGenerative AI capabilities\nStrong .NET Full Stack exp\n3+ yrs of experience as Full Stack Developers & AI exposure can apply\nnot a Data Scientist role",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Generative Ai', 'Large Language Model', 'Full Stack', '.Net', 'C#', 'Java', 'C++', 'Azure', 'Open AI', 'Python']",2025-06-10 15:24:07
Data Engineer,DATA ENGINEER,5 - 10 years,Not Disclosed,['Hyderabad'],"Job Title: Data Engineer\nExperience: 5+ Years\nLocation: Hyderabad (Onsite)\nAvailability: Immediate Joiners Preferred\nJob Description:\nWe are seeking an experienced Data Engineer with a strong background in Java, Spark, and Scala to join our dynamic team in Hyderabad. The ideal candidate will be responsible for building scalable data pipelines, optimizing data processing workflows, and supporting data-driven solutions for enterprise-grade applications. This is a full-time onsite role.\nKey Responsibilities:\nDesign, develop, and maintain robust and scalable data processing pipelines.\nWork with large-scale data using distributed computing technologies like Apache Spark.\nDevelop applications and data integration workflows using Java and Scala.\nCollaborate with cross-functional teams including Data Scientists, Analysts, and Product Managers.\nEnsure data quality, integrity, and security in all data engineering solutions.\nMonitor and troubleshoot performance and data issues in production systems.\nMust-Have Skills:\nStrong hands-on experience with Java, Apache Spark, and Scala.\nProven experience working on large-scale data processing systems.\nSolid understanding of distributed systems and performance tuning.\nGood-to-Have Skills:\nExperience with Hadoop, Hive, and HDFS.\nFamiliarity with data warehousing concepts and ETL processes.\nExposure to cloud data platforms is a plus.\nDesired Candidate Profile:\n5+ years of relevant experience in data engineering or big data technologies.\nStrong problem-solving and analytical skills.\nExcellent communication and collaboration skills.\nAbility to work independently in a fast-paced environment.\nAdditional Details:\nWork Mode: Onsite (Hyderabad)\nEmployment Type: Full-time\nNotice Period: Immediate joiners highly preferred, candidates serving notice period.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Java', 'SCALA', 'Spark', 'Hive', 'Hadoop', 'Kafka']",2025-06-10 15:24:09
Data & AI/ML Architect,"Milestone Technologies, Inc",14 - 19 years,Not Disclosed,['Kochi'],"Job Summary:\nWe are seeking a highly experienced and visionary Databricks Data Architect with over 14 years in data engineering and architecture, including deep hands-on experience in designing and scaling Lakehouse architectures using Databricks . The ideal candidate will possess deep expertise across data modeling, data governance, real-time and batch processing, and cloud-native analytics using the Databricks platform. You will lead the strategy, design, and implementation of modern data architecture to drive enterprise-wide data initiatives and maximize the value from the Databricks platform.\nKey Responsibilities:\nLead the architecture, design, and implementation of scalable and secure Lakehouse solutions using Databricks and Delta Lake .\nDefine and implement data modeling best practices , including medallion architecture (bronze/silver/gold layers).\nChampion data quality and governance frameworks leveraging Databricks Unity Catalog for metadata, lineage, access control, and auditing.\nArchitect real-time and batch data ingestion pipelines using Apache Spark Structured Streaming , Auto Loader , and Delta Live Tables (DLT) .\nDevelop reusable templates, workflows, and libraries for data ingestion, transformation, and consumption across various domains.\nCollaborate with enterprise data governance and security teams to ensure compliance with regulatory and organizational data standards.\nPromote self-service analytics and data democratization by enabling business users through Databricks SQL and Power BI/Tableau integrations .\nPartner with Data Scientists and ML Engineers to enable ML workflows using MLflow , Feature Store , and Databricks Model Serving .\nProvide architectural leadership for enterprise data platforms, including performance optimization , cost governance , and CI/CD automation in Databricks.\nDefine and drive the adoption of DevOps/MLOps best practices on Databricks using Databricks Repos , Git , Jobs , and Terraform .\nMentor and lead engineering teams on modern data platform practices, Spark performance tuning , and efficient Delta Lake optimizations (Z-ordering, OPTIMIZE, VACUUM, etc.) .\nTechnical Skills:\n10+ years in Data Warehousing, Data Architecture, and Enterprise ETL design .\n5+ years hands-on experience with Databricks on Azure/AWS/GCP , including advanced Apache Spark and Delta Lake .\nStrong command of SQL, PySpark, and Spark SQL for large-scale data transformation.\nProficiency with Databricks Unity Catalog , Delta Live Tables , Autoloader , DBFS , Jobs , and Workflows .\nHands-on experience with Databricks SQL and integration with BI tools (Power BI, Tableau, etc.).\nExperience implementing CI/CD on Databricks , using tools like Git , Azure DevOps , Terraform , and Databricks Repos .\nProficient with streaming architecture using Spark Structured Streaming , Kafka , or Event Hubs/Kinesis .\nUnderstanding of ML lifecycle management with MLflow , and experience in deploying MLOps solutions on Databricks.\nFamiliarity with cloud object stores (e.g., AWS S3, Azure Data Lake Gen2) and data lake architectures .\nExposure to data cataloging and metadata management using Unity Catalog or third-party tools.\nKnowledge of orchestration tools like Airflow , Databricks Workflows , or Azure Data Factory .\nExperience with Docker/Kubernetes for containerization (optional, for cross-platform knowledge).\nPreferred Certifications (a plus):\nDatabricks Certified Data Engineer Associate/Professional\nDatabricks Certified Lakehouse Architect\nMicrosoft Certified: Azure Data Engineer / Azure Solutions Architect\nAWS Certified Data Analytics - Specialty\nGoogle Professional Data Engineer",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Performance tuning', 'Automation', 'metadata', 'GIT', 'Architecture', 'Data modeling', 'Data quality', 'microsoft', 'SQL', 'Data architecture']",2025-06-10 15:24:12
DevOps Engineer - Lead,Blend360 India,7 - 12 years,Not Disclosed,['Hyderabad'],"We are seeking a highly skilled Lead Azure DevOps Engineer to join our team and drive the end-to-end deployment, scalability, and operationalization of machine learning models in production. You will collaborate closely with data scientists, data engineers, and DevOps teams to ensure seamless CI/CD, reproducibility, monitoring, and governance of ML pipelines\nKey Responsibilities\nDesign, implement, and maintain CI/CD pipelines for deploying and monitoring microservices efficiently.\nManage infrastructure as code using Terraform for repeatable and scalable provisioning.\nDeploy and optimize containerized applications using Docker and Azure services (Container Apps, Container Registry, Key Vault, Service Bus, Blob Storage).\nApply best practices for securing Docker images, including vulnerability scanning, reducing image size, and optimizing build efficiency.\nImplement and maintain Azure Monitor for logging, monitoring, and alerting to ensure system reliability.\nEnsure security best practices across cloud environments, including secrets management, access control, and compliance.\n(Nice to have) Design and manage multi-client architectures within shared pipelines and storage accounts in Azure Blob Storage\n\n\n6+ years of experience in DevOps or MLOps with a strong focus on production-grade ML solutions.\nStrong expertise in Azure, particularly with CI/CD, container orchestration, and cloud security. Proficiency in Terraform for infrastructu",Industry Type: Industrial Automation,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Automation', 'orchestration', 'cloud security', 'devops', 'Machine learning', 'Infrastructure', 'Vulnerability', 'Management', 'Monitoring', 'microservices']",2025-06-10 15:24:14
Analyst,Wipro,2 - 6 years,Not Disclosed,['Gurugram'],"About The Role :\n\nKey Responsibilities:\n\nAssisting in critical Financial Analysis\nPreparing the financial statements, SEC reporting documents.\nPerform portfolio analytics and performance reporting\nAssist in preparing 10K, 10Q documents.\nPrepare senior management reporting packages\nAssist with ad hoc financial analysis, as requested by senior management\nResponsible for preparing various reconciliations including bank accounts and other Balance sheet accounts\n\n\nKey Responsibilities:\n\nAssisting in critical Financial Analysis\nPreparing the financial statements, SEC reporting documents.\nPerform portfolio analytics and performance reporting\nAssist in preparing 10K, 10Q documents.\nPrepare senior management reporting packages\nAssist with ad hoc financial analysis, as requested by senior management\nResponsible for preparing various reconciliations including bank accounts and other Balance sheet accounts\n\n\n\n\nRequired Skills:\n1-3 years of Credit and/or Finance experience\nBachelors degree in Accounting or Finance\nProficient with Microsoft Office Suite, including Excel, Word, Access and PowerPoint\nMeticulous attention to detail and strong organization skills\nExcellent written and verbal communication skills\nAbility to prioritize multiple tasks in a fast-paced environment\nBuild capability to ensure operational excellence and maintain superior customer service levels of the existing account/client\nUndertake product trainings to stay current with product features, changes and updates\nEnroll in product specific and any other trainings per client requirements/recommendations\nPartner with team leaders to brainstorm and identify training themes and learning issues to better serve the client\nUpdate job knowledge by participating in self learning opportunities and maintaining personal networks\n\n\n\n\n\n\nDeliver\nNoPerformance ParameterMeasure1ProcessNo. of cases resolved per day, compliance to process and quality standards, meeting process level SLAs, Pulse score, Customer feedback2Self- ManagementProductivity, efficiency, absenteeism, Training Hours, No of technical training completed",Industry Type: IT Services & Consulting,Department: Finance & Accounting,"Employment Type: Full Time, Permanent","['financial analysis', 'microsoft office suite', 'process compliance', 'finance', 'financial statements', 'balance sheet', 'operations management', 'project management', 'data analysis', 'team management', 'customer service', 'management reporting', 'accounting', 'operational excellence']",2025-06-10 15:24:16
Analyst,Goldman Sachs,2 - 3 years,Not Disclosed,['Bengaluru'],"ASSET MANAGEMENT\nBringing together traditional and alternative investments, we provide clients around the world with a dedicated partnership and focus on long-term performance. As the firm s primary investment area, we provide investment and advisory services for some of the world s leading pension plans, sovereign wealth funds, insurance companies, endowments, foundations, financial advisors and individuals, for which we oversee more than $2 trillion in assets under supervision. Working in a culture that values integrity and transparency, you will be part of a diverse team that is passionate about our craft, our clients, and building sustainable success. We are:\nInvestors, spanning traditional and alternative markets offering products and services\nAdvisors, understanding our clients priorities and poised to help provide investment advice and strategies that make sense for their portfolios\nThought Leaders, providing timely insights across macro and secular themes to help inform our clients investment decisions\nInnovators, using our suite of digital solutions to help our clients address complex challenges and meet their financial goals\nCritical to the success of GSAM is our ability to leverage a global team of talented professionals to define solutions and lead change across the operational infrastructure.\nYOUR IMPACT\n\nAre you looking for a new challenge in a dynamic, international environment in Asset Management? Do you have excellent communication skills and an interest in a broad range of responsibilities? We re looking for a professional to join the Asset Management Reporting Operations team.\nGoldman Sachs Asset Management (GSAM) is the investment management arm providing global investment opportunities to a substantial and diversified client base including institutions, governments and high net worth individuals. Reporting Operations team is an integral function of the GSAM Funds and Separate Accounts business. The team partners with multiple internal and external teams, vendors, distributors and also clients to understand, execute and manage client deliverables. Automation is a key focus area to increase scale and impact across our reporting offerings.\nBUSINESS UNIT OVERVIEW\nGoldman Sachs Asset Management (GSAM) is the investment management arm providing global investment opportunities to a substantial and diversified client base including institutions, governments and high net worth individuals. Asset Management Operations partners with two main areas of the firm to deliver asset management capabilities to clients around the world, providing essential risk management and control as well as client service and excellence in execution.\nJOB SUMMARY AND RESPONSBILITIES\nThe GSAM Institutional and Fund Reporting team manages the setup and distribution of client reporting deliverables for all Institutional Clients globally and Clients & Prospects invested in GSAM Mutual Funds across multiple regions. In addition, we manage institutional portal client setup, produce Monthly Mutual Fund Updates and provide assistance on a number of other client relationship functions.\nAct as a client reporting specialist within the Bangalore Client Reporting team, partnering closely with Client Relationship Managers, Sales and product team(s) to fulfil regularly scheduled Institutional / Mutual Fund Client Reporting deliverables.\nThe team is responsible for the following functions:\nCheckout / Delivery of Standard reports\nMonthly factsheet Production\nLiaise with upstream teams for ensuring data accuracy\nMetrics reporting to management on a daily basis.\nPreparation & delivery of Custom Reports\nCo-ordination of regulatory reports for GSAM\nManage changes for client statement disclosures and footnotes\nAccess provisioning to GSAM.com\nBASIC QUALIFICATIONS\nRelevant experience of 2 - 3 years\nGood communication skills and ability to clearly articulate issues is crucial\nAttention to details and strong financial risk awareness\nStrong analytical and logical skills. Forward thinking, with an ability to problem solve and think creatively\nAbility to go through large daily volumes and stay focused throughout the day\nPREFFERED QUALIFICATIONS\nCommerce degree from tier-1 or 2 institutes with exposure in Finance.\nSelf-motivated team player, ownership, accountability, organizational/prioritization skills, proactive, ability to multitask, ambitious, independent, positive mind set\nGood PC skills - MS Office\nInclination to learn automation tools or new vendor tools will be helpful in the long term\nBusiness Intelligence Tools like Alteryx, Tableau etc.",Industry Type: Banking,"Department: BFSI, Investments & Trading","Employment Type: Full Time, Permanent","['Wealth management', 'Analytical', 'Financial risk', 'Investment banking', 'Asset management', 'Investment management', 'Risk management', 'MS Office', 'Business intelligence', 'Operations']",2025-06-10 15:24:19
ML Engineer,Wipro,1 - 4 years,Not Disclosed,['Bengaluru'],"Wipro Limited (NYSEWIT, BSE507685, NSEWIPRO) is a leading technology services and consulting company focused on building innovative solutions that address clients’ most complex digital transformation needs. Leveraging our holistic portfolio of capabilities in consulting, design, engineering, and operations, we help clients realize their boldest ambitions and build future-ready, sustainable businesses. With over 230,000 employees and business partners across 65 countries, we deliver on the promise of helping our customers, colleagues, and communities thrive in an ever-changing world. For additional information, visit us at www.wipro.com.\n\nAbout The Role\n\n\n\nRole Purpose\n\nThe purpose of this role is to design, test and maintain software programs for operating systems or applications which needs to be deployed at a client end and ensure its meet 100% quality assurance parameters\n\n?\n\n\n\nDo\n\n1. Instrumental in understanding the requirements and design of the product/ software\nDevelop software solutions by studying information needs, studying systems flow, data usage and work processes\nInvestigating problem areas followed by the software development life cycle\nFacilitate root cause analysis of the system issues and problem statement\nIdentify ideas to improve system performance and impact availability\nAnalyze client requirements and convert requirements to feasible design\nCollaborate with functional teams or systems analysts who carry out the detailed investigation into software requirements\nConferring with project managers to obtain information on software capabilities\n\n\n?\n\n2. Perform coding and ensure optimal software/ module development\nDetermine operational feasibility by evaluating analysis, problem definition, requirements, software development and proposed software\nDevelop and automate processes for software validation by setting up and designing test cases/scenarios/usage cases, and executing these cases\nModifying software to fix errors, adapt it to new hardware, improve its performance, or upgrade interfaces.\nAnalyzing information to recommend and plan the installation of new systems or modifications of an existing system\nEnsuring that code is error free or has no bugs and test failure\nPreparing reports on programming project specifications, activities and status\nEnsure all the codes are raised as per the norm defined for project / program / account with clear description and replication patterns\nCompile timely, comprehensive and accurate documentation and reports as requested\nCoordinating with the team on daily project status and progress and documenting it\nProviding feedback on usability and serviceability, trace the result to quality risk and report it to concerned stakeholders\n\n\n?\n\n3. Status Reporting and Customer Focus on an ongoing basis with respect to project and its execution\nCapturing all the requirements and clarifications from the client for better quality work\nTaking feedback on the regular basis to ensure smooth and on time delivery\nParticipating in continuing education and training to remain current on best practices, learn new programming languages, and better assist other team members.\nConsulting with engineering staff to evaluate software-hardware interfaces and develop specifications and performance requirements\nDocument and demonstrate solutions by developing documentation, flowcharts, layouts, diagrams, charts, code comments and clear code\nDocumenting very necessary details and reports in a formal way for proper understanding of software from client proposal to implementation\nEnsure good quality of interaction with customer w.r.t. e-mail content, fault report tracking, voice calls, business etiquette etc\nTimely Response to customer requests and no instances of complaints either internally or externally\n\n\n?\n\n\n\nDeliver\n\n\n\n\nNo.\n\n\n\nPerformance Parameter\n\n\n\nMeasure 1. Continuous Integration, Deployment & Monitoring of Software 100% error free on boarding & implementation, throughput %, Adherence to the schedule/ release plan 2. Quality & CSAT On-Time Delivery, Manage software, Troubleshoot queries, Customer experience, completion of assigned certifications for skill upgradation 3. MIS & Reporting 100% on time MIS & report generation\nReinvent your world. We are building a modern Wipro. We are an end-to-end digital transformation partner with the boldest ambitions. To realize them, we need people inspired by reinvention. Of yourself, your career, and your skills. We want to see the constant evolution of our business and our industry. It has always been in our DNA - as the world around us changes, so do we. Join a business powered by purpose and a place that empowers you to design your own reinvention. Come to Wipro. Realize your ambitions. Applications from people with disabilities are explicitly welcome.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['continuous integration', 'root cause analysis', 'software development life cycle', 'mis', 'ml', 'python', 'c++', 'data analysis', 'c', 'data analytics', 'natural language processing', 'machine learning', 'artificial intelligence', 'sql', 'deep learning', 'r', 'data science', 'computer vision']",2025-06-10 15:24:21
Business Analyst,Incanus Technologies,0 - 4 years,Not Disclosed,"['Hyderabad', 'Bengaluru', 'Delhi / NCR']","Calling all passionate problem-solvers and data enthusiasts! Are you ready to embark on a thrilling journey as a Business Analyst with one of India's top B2C players? Join us and unlock the power of data to drive impactful decisions and shape the future of our client's business!\n\nRoles & Responsibilities:\n\nUnleash your structured problem-solving skills and wield the right tools to tackle complex challenges head-on.\nLead the charge in KPI reporting and uncover the hidden drivers behind our business success.\nDive deep into the depths of data, unraveling the mysteries of KPI influencers and quantifying their impact.\nTransform raw data into actionable insights that propel our business forward.\nDefine and communicate key metrics and trends to keep our team ahead of the curve.\nMaster the art of data extraction, cleansing, and normalization to ensure the accuracy of our analyses.\n\nDesired Skills and Experience:\n\nAre you a B.Tech graduate from one of the top 50 NIRF colleges? Let your talent shine!\nAdvanced Excel & SQL skills are a must-have in your arsenal.\nProficiency in Python and familiarity with Power BI/Tableau will set you apart from the crowd.\nA thirst for knowledge and the ability to thrive in a dynamic work environment.\nBring your problem-solving prowess to the table and generate invaluable insights.\nExcellent communication skills to engage with both technical and non-technical stakeholders.\n\nHiring Process:\n\nEligible candidates will receive an evaluation link to showcase their skills.\nFurther rounds will be tailored based on your performance in the evaluation test.\n\nNote: Only candidates meeting the specified eligibility criteria will be contacted. Don't miss out on this opportunity to unleash your potential and make a meaningful impact with us! Apply now and let's revolutionize the world of business analytics together.",Industry Type: Education / Training,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Excel', 'SQL', 'Python', 'POWER bi']",2025-06-10 15:24:24
ML Engineer,Randomtrees,3 - 8 years,Not Disclosed,['Chennai'],Role: ML Engineer\nExperience: 3 to 15 Years\nChennai location must or relocation also fine- Hybrid mode\nWe have good budget\n\nJob Description:\nResponsibilities:,,,,"['Generative Ai', 'Aiml', 'RAG', 'LLM', 'Python', 'Data Science', 'Date Scientist', 'Langchain', 'Artificial Intelligence', 'Machine Learning']",2025-06-10 15:24:26
Analytics Software Developer,ZF,2 - 6 years,Not Disclosed,['Coimbatore'],"Key Responsibilities:\nDesign, develop, and deploy scalable analytics and reporting solutions.\n\nBuild and maintain ETL pipelines and data integration processes.\n\nCollaborate with data scientists, analysts, and business stakeholders to understand requirements and deliver insights.\n\nDevelop custom dashboards and visualizations using tools such as Power BI, Tableau, or similar.\n\nOptimize database queries and ensure high performance of analytics systems.\n\nAutomate data workflows and contribute to data quality initiatives.\n\nImplement security best practices in analytics platforms and applications.\n\nStay updated with the latest trends in analytics technologies and tools.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Analytics Software', 'analytics platforms', 'data integration processes', 'Power BI', 'Tableau', 'ETL', 'ETL pipelines']",2025-06-10 15:24:28
Data/ML Ops Engineer,"NTT DATA, Inc.",2 - 5 years,Not Disclosed,['Bengaluru'],"Additional Career Level Description:\n\n\nKnowledge and application:\nSeasoned, experienced professional; has complete knowledge and understanding of area of specialization.\nUses evaluation, judgment, and interpretation to select right course of action.\n\n\n\nProblem solving:\nWorks on problems of diverse scope where analysis of information requires evaluation of identifiable factors.\nResolves and assesses a wide range of issues in creative ways and suggests variations in approach.\n\n\n\nInteraction:\nEnhances relationships and networks with senior internal/external partners who are not familiar with the subject matter often requiring persuasion.\nWorks with others outside of own area of expertise, with the ability to adapt style to differing audiences and often advises others on difficult matters.\n\n\n\nImpact:\nImpacts short to medium term goals through personal effort or influence over team members.\n\n\n\nAccountability:\nAccountable for own targets with work reviewed at critical points.\nWork is done independently and is reviewed at critical points.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['ML Ops', 'python', 'spark', 'big data', 'data engineering', 'artificial intelligence', 'ml', 'sql']",2025-06-10 15:24:30
Analyst,Aegis Media,0 - 3 years,Not Disclosed,['Mumbai'],"Data Validation (DV) Specialist (Using SPSS) - Analyst\nJob Description:\nCore Responsibilities:\nPerform data quality checks and validation on market research datasets\nDevelop and execute scripts and automated processes to identify data anomalies.\nCollaborate with the Survey Programming team to review survey questionnaires and make recommendations for efficient programming and an optimal layout that enhances user experience.\nInvestigate and document data discrepancies, working with survey programming team/data collection vendors as needed.\nCreate and maintain detailed data documentation and validation reports.\nCollaborate with Survey Programmers and internal project managers to understand data processing requirements and provide guidance on quality assurance best practices.\nProvide constructive feedback and suggestions for improving the quality of data, aiming to enhance overall survey quality.\nAutomate data validation processes where possible to enhance efficiency and reduce time spent on repetitive data validation tasks.\nMaintain thorough documentation of findings and recommendations to ensure transparency and consistency in quality practices.\nActively participate in team meetings to discuss project developments, quality issues, and improvement strategies, fostering a culture of continuous improvement.\nQualification:\nBachelor s degree in computer science, Information Technology, Statistics, or a related field.\nAt least 2+ years of experience in data validation process.\nFamiliar with data validation using SPSS, Dimension, Quantum platform or similar tools\nA proactive team player who thrives in a fast-paced environment and enjoys repetitive tasks that contribute to project excellence.\nProgramming knowledge in a major programming language such as R, JavaScript, or Python, with an interest in building automation scripts for data validation.\nExcellent problem-solving skills and a willingness to learn innovative quality assurance methodologies.\nA desire for continuous improvement in processes, focusing on creating efficiencies that lead to scalable and high-quality data processing outcomes.",Industry Type: Advertising & Marketing,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Automation', 'Manager Quality Assurance', 'Javascript', 'Data collection', 'Market research', 'Data processing', 'Data quality', 'SPSS', 'Continuous improvement', 'Information technology']",2025-06-10 15:24:32
Data Engineer,KC International School,8 - 13 years,Not Disclosed,['Chennai'],"KC International School is looking for Data Engineer to join our dynamic team and embark on a rewarding career journey\nLiaising with coworkers and clients to elucidate the requirements for each task.\nConceptualizing and generating infrastructure that allows big data to be accessed and analyzed.\nReformulating existing frameworks to optimize their functioning.\nTesting such structures to ensure that they are fit for use.\nPreparing raw data for manipulation by data scientists.\nDetecting and correcting errors in your work.\nEnsuring that your work remains backed up and readily accessible to relevant coworkers.\nRemaining up-to-date with industry standards and technological advancements that will improve the quality of your outputs.\n\n\nThe DE at KC will design, develop and maintain all school data infrastructure ensuring accurate and efficient data management.",Industry Type: Education / Training,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['hive', 'cloudera', 'python', 'data analysis', 'scala', 'oozie', 'airflow', 'data warehousing', 'pyspark', 'apache pig', 'machine learning', 'data engineering', 'sql', 'mapreduce', 'spark', 'hadoop', 'sqoop', 'big data', 'aws', 'etl', 'hbase']",2025-06-10 15:24:34
Power Bi Developer,IntelERA Inc,7 - 12 years,20-35 Lacs P.A.,[],"Job Title: Power BI Engineer\nLocation: Fully Remote (Preferred candidates in Pune, but open to all)\nWork Hours: 4:30 PM to 1:30 AM IST\nExperience Required: 7+ Years\n\nWe are seeking an experienced Power BI Engineer to join our team. The ideal candidate has a strong background in data analytics, visualization, and business intelligence, with a proven track record of designing and implementing impactful dashboards and reports. This role requires advanced skills in Power BI, data modeling, DAX, and ETL processes, as well as experience with large datasets and complex data transformations.\n\n\nKey Responsibilities:\n\nDesign, develop, and deploy Power BI dashboards and reports to provide insights that drive business decisions.\nWork closely with business stakeholders to gather and translate requirements into scalable, impactful data visualizations.\nImplement and optimize data models, using DAX to create robust calculations and complex measures.\nIntegrate and manage diverse data sources, including SQL databases, cloud services (e.g., Azure), APIs, and flat files.\nConduct data analysis and quality assurance to ensure data accuracy and report integrity.\nDevelop ETL processes using Power Query, SQL, and Azure Data Factory to extract, transform, and load data.\nImplement row-level security (RLS) and ensure adherence to data governance and security policies.\nPerform performance tuning on Power BI dashboards, ensuring efficient load times and smooth user experience.\nCollaborate with data engineers, data scientists, and other stakeholders to integrate advanced analytics and predictive insights.\nStay up-to-date with the latest Power BI features, best practices, and trends to continuously enhance BI solutions.\n\nRequired Skills and Qualifications:\n\n7+ years of experience in business intelligence and data analytics, with a strong focus on Power BI.\nProficient in Power BI Desktop, Power BI Service, and Power BI Report Server.\nExpertise in data modeling, DAX (Data Analysis Expressions), and M language.\nSolid understanding of relational databases and SQL, with experience in database management (SQL Server, Oracle, etc.).\nHands-on experience with ETL tools, such as Power Query, SQL Server Integration Services (SSIS), or Azure Data Factory.\nFamiliarity with cloud platforms, particularly Azure services like Azure SQL Database, Azure Data Lake, and Azure Analysis Services.\nStrong analytical skills, with a keen eye for detail and a commitment to data quality.\nExperience in implementing row-level security (RLS) and maintaining compliance with data governance standards.\nFamiliarity with scripting languages like Python or PowerShell for automation is a plus.\nExcellent communication and interpersonal skills, with the ability to effectively collaborate with both technical and non-technical stakeholders.\n\nSoft Skills:\n\nTakes ownership, is a proactive problem-solver with a positive, can-do attitude.\nPassionate about development.\nExcellent communication and teamwork skills.\nAbility to work effectively in a remote environment.\n\nNice to Have:\n\nKnowledge of Agile methodologies and familiarity with tools like Azure DevOps or JIRA.\nCertifications in Power BI, Microsoft Azure, or data engineering are advantageous.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Power Bi', 'Data Visualization', 'Dax', 'Data Modeling', 'Data Analytics', 'Azure Data Factory', 'Power Bi Dashboards', 'SSIS', 'Power Query', 'SQL']",2025-06-10 15:24:36
Full Stack Developer,Paytring,2 - 5 years,Not Disclosed,['Gurugram'],"We are looking for a Full Stack Developer to produce scalable software solutions. You ll be part of a cross-functional team that s responsible for the full software development life cycle, from conception to deployment.\n\nAs a Full Stack Developer, you should be comfortable around both front-end and back-end coding languages, development frameworks and third-party libraries. You should also be a team player with a knack for visual design and utility.\n\nResponsibilities:-\n\nWork with development teams and product managers to ideate software solutions\nDesign client-side and server-side architecture\nBuild the front-end of applications through appealing visual design\nDevelop and manage well-functioning databases and applications\nWrite effective APIs\nTest software to ensure responsiveness and efficiency\nTroubleshoot, debug and upgrade software\nCreate security and data protection settings\nBuild features and applications with a mobile responsive design\nWrite technical documentation\nWork with data scientists and analysts to improve software\n\nRequirements:-\nMust Haves\n\nProven experience as a Full Stack Developer or similar role\nExperience developing desktop and mobile applications\nFamiliarity with common tech stacks\nKnowledge of multiple front-end languages and libraries (e.g. HTML/ CSS, JavaScript, XML, jQuery)\nKnowledge of back-end languages (e.g. PHP, Python ) and JavaScript frameworks (e.g. Angular, React, Node.js )\nKnowledge of writing/building Rest APIs\nFamiliarity with databases (e.g. MySQL, MongoDB), web servers (e.g. Apache) and UI/UX design\nFamiliarity with using cloud servers, repositories such as GIT, SVN, BitBucket, AWS, Azure\nFamiliarity with security protocols in secure code writing (OWASP)\n\nGood to have\n\nExcellent communication and teamwork skills\nIndependent contributor\nGreat attention to detail\nOrganizational skills\nAn analytical mind\nDegree in Computer Science or relevant field\nPrior experience in payments/fintech domain.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'jQuery', 'Front end', 'Coding', 'XML', 'MySQL', 'PHP', 'HTML', 'Apache', 'Python']",2025-06-10 15:24:38
Product manager (Solar),Renew,5 - 8 years,Not Disclosed,['Haryana'],"About Company\nJob Description\nResponsibilities\n•    Understand Business problems and identify constraints\n•    Design digital and advance analytics solutions to Business problems\n•    Implement the solution with an understanding of end-to-end architecture\n•    Identify opportunities for implementation of new use cases\n•    Keep updates of any policy changes in power markets\n•    Ensure ReD targets are met and delivered on time\n•    Ensure documentation of Use Cases\nOur Ideal Candidate\n•    Education - Engineering (Electrical/Electronics/IT) + MBA \n•    Experience Range – 4 to 7 years\n•    Experience in Renewable energy/ Storage/Hydro/RTC power/Power Trading\n•    Good program management, Project planning & coordination skills\n•    Good Experience of working in cross functional teams\n•    Good IT skills - understanding different solutions and matching solutions to problems\n•    Analytical approach to solving problems with focus on solution delivery\n•    Capable of extrapolating current situation to future scenarios\nFunctional/ Domain expertise\n•    Knowledge of Power markets is a must\n•    Should have experience in evaluating or implementing any of the new technologies (BESS/ Hybrids/ EV, Charging Infra/ Pumped Storage/ Hydrogen/Market procurement of RE - GTAM) \n•    Participated in some Digital transformation/enablement exercise in organisation\n•    Basic understanding of work of Data Scientists & Data engineering roles is a plus\n•    Experience in agile working methodology would be a plus\n•    Experience with tools like PowerBI, Tableau, JIRA would be a plus\nCommunication skills\n•    Ability to communicate with cross functional roles is a must\n•    Excellent written and verbal communication skills\n•    Good presentation skills\nTeamwork\n•    Ability to work as a self-motivated team player\n•    Has worked in large teams with an agile setup in the past\n•    Handle multiple projects across intra and inter-department teams\n",Industry Type: Power,Department: Product Management,"Employment Type: Full Time, Permanent","['power trading', 'analytical', 'program management', 'verbal communication', 'presentation skills', 'power bi', 'coordination', 'analytics', 'tableau', 'product management', 'use cases', 'writing', 'agile', 'rational team concert', 'digital transformation', 'project planning', 'communication skills', 'jira', 'architecture']",2025-06-10 15:24:41
Data Engineer - Hadoop,Wipro,5 - 8 years,Not Disclosed,['Bengaluru'],"Wipro Limited (NYSEWIT, BSE507685, NSEWIPRO) is a leading technology services and consulting company focused on building innovative solutions that address clients’ most complex digital transformation needs. Leveraging our holistic portfolio of capabilities in consulting, design, engineering, and operations, we help clients realize their boldest ambitions and build future-ready, sustainable businesses. With over 230,000 employees and business partners across 65 countries, we deliver on the promise of helping our customers, colleagues, and communities thrive in an ever-changing world. For additional information, visit us at www.wipro.com.\n About The Role  _x000D_\n\n \n\nRole Purpose  \nThe purpose of the role is to support process delivery by ensuring daily performance of the Production Specialists, resolve technical escalations and develop technical capability within the Production Specialists.\n\n\n ? _x000D_\n\n \n\nDo  \n\n\nOversee and support process by reviewing daily transactions on performance parameters\nReview performance dashboard and the scores for the team\nSupport the team in improving performance parameters by providing technical support and process guidance\nRecord, track, and document all queries received, problem-solving steps taken and total successful and unsuccessful resolutions\nEnsure standard processes and procedures are followed to resolve all client queries\nResolve client queries as per the SLA’s defined in the contract\nDevelop understanding of process/ product for the team members to facilitate better client interaction and troubleshooting\nDocument and analyze call logs to spot most occurring trends to prevent future problems\nIdentify red flags and escalate serious client issues to Team leader in cases of untimely resolution\nEnsure all product information and disclosures are given to clients before and after the call/email requests\nAvoids legal challenges by monitoring compliance with service agreements\n\n\n ? _x000D_\n\n\nHandle technical escalations through effective diagnosis and troubleshooting of client queries\nManage and resolve technical roadblocks/ escalations as per SLA and quality requirements\nIf unable to resolve the issues, timely escalate the issues to TA & SES\nProvide product support and resolution to clients by performing a question diagnosis while guiding users through step-by-step solutions\nTroubleshoot all client queries in a user-friendly, courteous and professional manner\nOffer alternative solutions to clients (where appropriate) with the objective of retaining customers’ and clients’ business\nOrganize ideas and effectively communicate oral messages appropriate to listeners and situations\nFollow up and make scheduled call backs to customers to record feedback and ensure compliance to contract SLA’s\n\n\n ? _x000D_\n\n\nBuild people capability to ensure operational excellence and maintain superior customer service levels of the existing account/client\nMentor and guide Production Specialists on improving technical knowledge\nCollate trainings to be conducted as triage to bridge the skill gaps identified through interviews with the Production Specialist\nDevelop and conduct trainings (Triages) within products for production specialist as per target\nInform client about the triages being conducted\nUndertake product trainings to stay current with product features, changes and updates\nEnroll in product specific and any other trainings per client requirements/recommendations\nIdentify and document most common problems and recommend appropriate resolutions to the team\nUpdate job knowledge by participating in self learning opportunities and maintaining personal networks\n\n\n ? _x000D_\n\nDeliver\nNoPerformance ParameterMeasure1ProcessNo. of cases resolved per day, compliance to process and quality standards, meeting process level SLAs, Pulse score, Customer feedback, NSAT/ ESAT2Team ManagementProductivity, efficiency, absenteeism3Capability developmentTriages completed, Technical Test performance\n\nMandatory\n\nSkills:\nHadoop_x000D_.\n\nExperience5-8 Years_x000D_.\n\nReinvent your world. We are building a modern Wipro. We are an end-to-end digital transformation partner with the boldest ambitions. To realize them, we need people inspired by reinvention. Of yourself, your career, and your skills. We want to see the constant evolution of our business and our industry. It has always been in our DNA - as the world around us changes, so do we. Join a business powered by purpose and a place that empowers you to design your own reinvention. Come to Wipro. Realize your ambitions. Applications from people with disabilities are explicitly welcome.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['hive', 'data engineering', 'spark', 'troubleshooting', 'hadoop', 'cloudera', 'python', 'scala', 'big data analytics', 'oozie', 'airflow', 'pyspark', 'data warehousing', 'apache pig', 'machine learning', 'sql', 'mapreduce', 'sqoop', 'big data', 'aws', 'etl', 'hbase']",2025-06-10 15:24:43
Business Analyst MES,Rockwell Automation,3 - 10 years,Not Disclosed,['Pune'],"Rockwell Automation is a global technology leader focused on helping the world s manufacturers be more productive, sustainable, and agile. . Behind our customers - amazing companies that help feed the world, provide life-saving medicine on a global scale, and focus on clean water and green mobility - our people are energized problem solvers that take pride in how the work we do changes the world for the better.\nWe welcome all makers, forward thinkers, and problem solvers who are looking for a place to do their best work. And if that s you we would love to have you join us!\nJob Description",,,,"['Product management', 'Computer science', 'ERP', 'Automation', 'Analytical', 'Shop floor', 'Presales', 'Customer support', 'Automotive', 'Analytics']",2025-06-10 15:24:45
Vertex AI Developer,Swits Digital,3 - 5 years,Not Disclosed,['Chennai'],"Job Title: Vertex AI Developer\nExperience: 3 - 5 Years\nLocation: Chennai / Hyderabad\nNotice Period: Immediate Joiners Preferred\nEmployment Type: Full-Time\nJob Description:\nWe are looking for a passionate and skilled Vertex AI Developer with hands-on experience in Google Cloud s Vertex AI , Python , Machine Learning (ML) , and Generative AI . The ideal candidate will play a key role in designing, developing, and deploying scalable ML/GenAI models and workflows using GCP Vertex AI services.\nKey Responsibilities:\nDevelop, deploy, and manage ML/GenAI models using Vertex AI on Google Cloud Platform (GCP) .\nWork with structured and unstructured data to create and train predictive and generative models.\nIntegrate AI models into scalable applications using Python APIs and GCP components.\nCollaborate with data scientists, ML engineers, and DevOps teams to implement end-to-end ML pipelines.\nMonitor model performance and iterate on improvements as necessary.\nDocument solutions, best practices, and technical decisions.\nMandatory Skills:\n3 to 5 years of experience in Machine Learning/AI Development .\nStrong proficiency in Python and ML libraries such as TensorFlow, PyTorch, Scikit-learn .\nHands-on experience with Vertex AI including AutoML, Pipelines, Model Deployment, and Monitoring.\nExperience in GenAI frameworks (e.g., PaLM, LangChain, LLMOps).\nProficiency in using Google Cloud Platform tools and services.\nStrong understanding of MLOps, CI/CD, and model lifecycle management.\nPreferred Skills:\nExperience with containerization tools like Docker , orchestration tools like Kubernetes .\nExposure to Natural Language Processing (NLP) and LLMs .\nFamiliarity with data engineering concepts (BigQuery, Dataflow).",Industry Type: Recruitment / Staffing,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['orchestration', 'GCP', 'devops', 'Machine learning', 'Cloud', 'Deployment', 'Natural language processing', 'Management', 'Monitoring', 'Python']",2025-06-10 15:24:47
Data Engineer,Accenture,3 - 8 years,Not Disclosed,['Hyderabad'],"Project Role :Data Engineer\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\nMust have skills :Data Engineering\n\n\nGood to have skills :NAMinimum\n\n12 year(s) of experience is required\n\n\nEducational Qualification :15 years full time education\nSummary:seeking a hands-on Senior Engineering Manager of Data Platform to spearhead the development of capabilities that power Vertex products while providing a connected experience for our customers. This role demands a deep engineering background with hands-on experience in building and scaling production-level systems. The ideal candidate will excel in leading teams to deliver high-quality data products and will provide mentorship, guidance, and leadership.In this role, you will work to increase the domain data coverage and adoption of the Data Platform by promoting a connected user experience through data. You will increase data literacy and trust by leading our Data Governance and Master Data Management initiatives. You will contribute to the vision and roadmap of self-serve capabilities through the Data Platform.\nRoles & Responsibilities:Be hands-on in leading the development of features that enhance self-service capabilities of our data platform, ensuring the platform is scalable, reliable, and fully aligned with business objectives, and defining and implementing best practices in data architecture, data modeling, and data governance.Work closely with Product, Engineering, and other departments to ensure the data platform meets business requirements.Influence cross-functional initiatives related to data tools, governance, and cross-domain data sharing. Ensure technical designs are thoroughly evaluated and aligned with business objectives.Determine appropriate recruiting of staff to achieve goals and objectives. Interview, recruit, develop and retain top talent.Manage and mentor a team of engineers, fostering a collaborative and high-performance culture, and encouraging a growth mindset and accountability for outcomes. Interpret how the business strategy links to individual roles and responsibilities.Provide career development opportunities and establish processes and practices for knowledge sharing and communication.Partner with external vendors to address issues, and technical challenges.Stay current with emerging technologies and industry trends in field to ensure the platform remains cutting-edge.\nProfessional & Technical\n\n\nSkills:\n12+ years of hands-on experience in software development (preferably in the data space), with 3+ years of people management experience, demonstrating success in building, growing, and managing multiple teams.Extensive experience in architecting and building complex data platforms and products. In-depth knowledge of cloud-based services and data tools such as Snowflake, AWS, Azure, with expertise in data ingestion, normalization, and modeling.Strong experience in building and scaling production-level cloud-based data systems utilizing data ingestion tools like Fivetran, Data Quality and Observability tools like Monte Carlo, Data Catalog like Atlan and Master Data tools like Reltio or Informatica.Thorough understanding of best practices regarding agile software development and software testing.Experience of deploying cloud-based applications using automated CI/CD processes and container technologies.Understanding of security best practices when architecting SaaS applications on cloud Infrastructure.Ability to understand complex business systems and a willingness to learn and apply new technologies as needed.Proven ability to influence and deliver high-impact initiatives. Forward-thinking mindset with the ability to define and drive the teams mission, vision, and long-term strategies.Excellent leadership skills with a track record of managing teams and collaborating effectively across departments. Strong written and communication skills.Proven ability to work with and lead remote teams to achieve sustainable long-term success.Work together and Get Stuff Done attitude without losing sight of quality, and a sense of responsibility to customers and the team.\nAdditional Information:- The candidate should have a minimum of 12 years of experience in Data Engineering.- This position is based at our Hyderabad office.- A 15 years full-time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['architecting', 'software testing', 'data engineering', 'data ingestion', 'agile software development', 'continuous integration', 'snowflake', 'software development', 'microsoft azure', 'reltio', 'data architecture', 'data quality', 'data modeling', 'data governance', 'aws', 'etl', 'informatica']",2025-06-10 15:24:51
InfoTech_Business Analyst- Veeva QMS,Biocon Biologics Limited,3 - 5 years,Not Disclosed,['Bengaluru( Electronic City Phase 2 )'],"At BBL we leverage cutting-edge science, innovative tech platforms and advanced research & development capabilities to lower treatment costs while improving healthcare outcomes. Intentional curiosity and an innate entrepreneurial scientific spirit fuel our strong research pipeline of biosimilar molecules across diabetes, oncology, immunology, and other non-communicable diseases.\n\nWe have 6500 people across the world innovating, creating, and delivering quality healthcare collaboratively, while employing the highest levels of ethics and integrity, every day.\nWe have been listed in the Top 10 Bio-Pharma Companies to Work For more than once by Science Magazine, including this year.\n\nKEY RESPONSIBILITIES\nThe incumbent will work closely with the Corporate Quality Assurance teams under the supervision of the Digital Business Partner, leading articulation of key technology and digital requirements and ensuring successful rollouts.\nMore specifically s/he will lead the following responsibilities:\n\nBusiness Analysis:\nPoint of contact and between business, IT, end user and other cross functional teams during the project or maintenance phase.\nCollect data, analyse & define the business and user requirements.\nAnalyse business problems and facilitate in IT solution evaluation.\nWork closely with managers and end users to have buy-in for the proposed solution.\nCreating a detailed business analysis, outlining problems, opportunities, and solutions for the business users and stakeholders.\nCategorize the functional and non-functional requirements of the business.\nConduct post project implementation acceptance and requirements reviews.\nPerform additional tasks as assigned by the reporting manager, time-to-time basis in DevOps mode.\nPrepare validation documentation related to release of GxP system. Should have sound knowledge on preparing validation artifacts for GxP IT systems.\nShould have knowledge of GxP system testing and validation.\nSound communication skill with good amount of convincing power (intermediate level).\nAn ideal candidate should be well planned and structured in his/her all deliverables.\nMust have experience in Pharma IT with hands-on expertise for handling GxP systems.\n\nIT Support and Maintenance:\nProvide IT administrative support for Enterprise systems but not limited to the following mostly focusing on L1 and L2 support for Veeva Vault QMS.\nCoordination with OEM partners for L3 support.\nUser Management (Creation, Modification, Password reset etc.).\nUser Access Rights Management.\nActive user list preparation.\nSupport to Biocon users on the enterprise application on service requests, incidents and bug fixes.\nPeriodic review of event logs (as where applicable).\nProvide support to vendor for fixing IT related issues during breakdowns.\nResponsible for adoption, creation and Maintenance of Quality Management Procedures as per IT Operation requirements.\n1st level Point of Contact for all IT related queries from Biocon users for enterprise systems\nCo-ordination with Enterprise functional Team for finding the RCA from IT Perspective in their Incidents and CAPAs.\nResponsible for implement Change controls and log Incidents and deviation related to IT Functions.\nResponsible for adherence of SLA’s defined for various IT administrative support activities.\n\nPROFESSIONAL EXPERIENCE / QUALIFICATIONS\nBachelor’s in Engineering or Master’s degree in a related discipline, including a minimum of 2-4 years of experience gained in technology implementation in the Pharma space\nSpecific experience as a Business Analyst in a GxP application for a pharma company\nVeeva QMS administration experience – preferably with Veeva certification.\nExcellent oral/ written communication skills\nSelf-starter, with the ability to work and execute independently with minimal oversight.\n\nBiocon Biologics is committed to equal opportunity in the terms and conditions of employment for all employees and job applicants without regard to race, colour, religion, sex, sexual orientation, age, gender identity or gender expression, national origin, disability or veteran status. Biocon Biologics also complies with all applicable national, state and local laws governing non-discrimination in employment as well as work authorization and employment eligibility verification requirements of the Immigration and Nationality Act.",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Veeva QMS', 'Quality', 'Quality Management']",2025-06-10 15:24:53
Data Engineer,Forbes Global 2000 IT Services Firm,5 - 10 years,Not Disclosed,['Hyderabad'],"Job Title: Big Data Engineer Java & Spark\nLocation: Hyderabad\nWork Mode: Onsite (5 days a week)\nExperience: 5 to 10 Years\nJob Summary:\nWe are hiring an experienced Big Data Engineer with strong expertise in Java, Apache Spark, and Big Data technologies. You will be responsible for designing and implementing scalable data pipelines that support real-time and batch processing for data-driven applications.\nKey Responsibilities:\nDevelop and maintain scalable batch and streaming data pipelines using Java and Apache Spark\nWork with Hadoop, Hive, Kafka, and HDFS to manage and process large datasets\nCollaborate with data analysts, scientists, and other engineering teams to understand data requirements\nOptimize Spark jobs and ensure performance and reliability in production\nMaintain data quality, governance, and security best practices\nRequired Skills:\n510 years of hands-on experience in data engineering or related roles\nStrong programming skills in both Java\nExpertise in Apache Spark for data processing and transformation\nGood understanding of Big Data frameworks: Hadoop, Hive, Kafka, HDFS\nExperience with distributed systems and large-scale data processing\nFamiliarity with cloud platforms such as AWS, GCP, or Azure\nGood to Have:\nExperience with workflow orchestration tools like Airflow or NiFi\nKnowledge of containerization (Docker, Kubernetes)\nExposure to CI/CD pipelines and version control (e.g., Git)\nEducation:\nBachelors or Masters degree in Computer Science, Engineering, or related field\nWhy Join Us:\nBe part of a high-impact data engineering team\nWork on modern data platforms with the latest open-source tools\nStrong tech culture with career growth opportunities",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Java', 'Spark', 'Hive', 'Hadoop']",2025-06-10 15:24:55
Scala developer (with Spark),Fortune India 500 IT Services Firm,5 - 8 years,Not Disclosed,"['Hyderabad', 'Pune']","We are looking for a highly skilled Scala Developer with solid experience in Apache Spark to join our data engineering team.\n\nExperience- 5 to 8yrs\nLocation- Pune, Hyderabad\nMandatory skills- Scala development, Spark\nKey Responsibilities:\nDesign, develop, and optimize batch and streaming data pipelines using Scala and Apache Spark.\nWrite efficient, reusable, and testable code following functional programming best practices.\nWork with large-scale datasets from a variety of sources (e.g., Kafka, Hive, S3, Parquet).\nCollaborate with data scientists, data analysts, and DevOps to ensure robust and scalable pipelines.\nTune Spark jobs for performance and resource efficiency.\nImplement data quality checks, logging, and error-handling mechanisms.\n\n\n\nInterested candidates share your CV at himani.girnar@alikethoughts.com with below details\n\nCandidate's name-\nEmail and Alternate Email ID-\nContact and Alternate Contact no-\nTotal exp-\nRelevant experience-\nCurrent Org-\nNotice period-\nCCTC-\nECTC-\nCurrent Location-\nPreferred Location-\nPancard No-",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Scala Programming', 'Spark', 'SCALA', 'Big Data Technologies', 'SQL']",2025-06-10 15:24:57
Data Engineer,Diverse Lynx,5 - 10 years,Not Disclosed,['Kolkata'],Diverse Lynx is looking for Data Engineer to join our dynamic team and embark on a rewarding career journey\nLiaising with coworkers and clients to elucidate the requirements for each task.\nConceptualizing and generating infrastructure that allows big data to be accessed and analyzed.\nReformulating existing frameworks to optimize their functioning.\nTesting such structures to ensure that they are fit for use.\nPreparing raw data for manipulation by data scientists.\nDetecting and correcting errors in your work.\nEnsuring that your work remains backed up and readily accessible to relevant coworkers.\nRemaining up-to-date with industry standards and technological advancements that will improve the quality of your outputs.,Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['hive', 'cloudera', 'python', 'data analysis', 'scala', 'oozie', 'airflow', 'data warehousing', 'pyspark', 'apache pig', 'machine learning', 'data engineering', 'sql', 'mapreduce', 'spark', 'hadoop', 'sqoop', 'big data', 'aws', 'etl', 'hbase']",2025-06-10 15:25:00
Business Analyst - L4,Wipro,5 - 8 years,Not Disclosed,['Hyderabad'],"Role Purpose\nThe purpose of the role is to liaison and bridging the gap between customer and Wipro delivery team to comprehend and analyze customer requirements and articulating aptly to delivery teams thereby, ensuring right solutioning to the customer.\n\n\n\nDo\n1. Customer requirements gathering and engagement\nInterface and coordinate with client engagement partners to understand the RFP/ RFI requirements\nDetail out scope documents, functional & non-functional requirements, features etc ensuring all stated and unstated customer needs are captured\nConstruct workflow charts and diagrams, studying system capabilities, writing specification after thorough research and analysis of customer requirements\nEngage and interact with internal team - project managers, pre-sales team, tech leads, architects to design and formulate accurate and timely response to RFP/RFIs\nUnderstand and communicate the financial and operational impact of any changes\nPeriodic cadence with customers to seek clarifications and feedback wrt solution proposed for a particular RFP/ RFI and accordingly instructing delivery team to make changes in the design\nEmpower the customers through demonstration and presentation of the proposed solution/ prototype\nMaintain relationships with customers to optimize business integration and lead generation\nEnsure ongoing reviews and feedback from customers to improve and deliver better value (services/ products) to the customers\n\n\n\n2.Engage with delivery team to ensure right solution is proposed to the customer\na.Periodic cadence with delivery team to:\nProvide them with customer feedback/ inputs on the proposed solution\nReview the test cases to check 100% coverage of customer requirements\nConduct root cause analysis to understand the proposed solution/ demo/ prototype before sharing it with the customer\nDeploy and facilitate new change requests to cater to customer needs and requirements\nSupport QA team with periodic testing to ensure solutions meet the needs of businesses by giving timely inputs/feedback\nConduct Integration Testing and User Acceptance demos testing to validate implemented solutions and ensure 100% success rate\nUse data modelling practices to analyze the findings and design, develop improvements and changes\nEnsure 100% utilization by studying systems capabilities and understanding business specifications\nStitch the entire response/ solution proposed to the RFP/ RFI before its presented to the customer\nb.Support Project Manager/ Delivery Team in delivering the solution to the customer\nDefine and plan project milestones, phases and different elements involved in the project along with the principal consultant\nDrive and challenge the presumptions of delivery teams on how will they successfully execute their plans\nEnsure Customer Satisfaction through quality deliverable on time\n\n\n\n3.Build domain expertise and contribute to knowledge repository\nEngage and interact with other BAs to share expertise and increase domain knowledge across the vertical\nWrite whitepapers/ research papers, point of views and share with the consulting community at large\nIdentify and create used cases for a different project/ account that can be brought at Wipro level for business enhancements\nConduct market research for content and development to provide latest inputs into the projects thereby ensuring customer delight\n\n\n\nDeliver\n\nNo.Performance ParameterMeasure\n1. Customer Engagement and Delivery ManagementPCSAT, utilization % achievement, no. of leads generated from the business interaction, no. of errors/ gaps in documenting customer requirements, feedback from project manager, process flow diagrams (quality and timeliness), % of deal solutioning completed within timeline, velocity generated.\n2. Knowledge ManagementNo. of whitepapers/ research papers written, no. of user stories created, % of proposal documentation completed and uploaded into knowledge repository, No of reusable components developed for proposal during quarter\nMandatory Skills: Telco Processes.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Telco Processes', 'User Acceptance Testing', 'RFP/ RFI', 'Project management', 'Integration Testing', 'Business Analysis']",2025-06-10 15:25:03
Business Analyst - L4,Wipro,5 - 8 years,Not Disclosed,['Hyderabad'],"Wipro Limited (NYSEWIT, BSE507685, NSEWIPRO) is a leading technology services and consulting company focused on building innovative solutions that address clients’ most complex digital transformation needs. Leveraging our holistic portfolio of capabilities in consulting, design, engineering, and operations, we help clients realize their boldest ambitions and build future-ready, sustainable businesses. With over 230,000 employees and business partners across 65 countries, we deliver on the promise of helping our customers, colleagues, and communities thrive in an ever-changing world. For additional information, visit us at www.wipro.com.\n About The Role  _x000D_\n\nRole Purpose\n\nThe purpose of the role is to liaison and bridging the gap between customer and Wipro delivery team to comprehend and analyze customer requirements and articulating aptly to delivery teams thereby, ensuring right solutioning to the customer.\n\n ? _x000D_\n\nDo\n\n1. Customer requirements gathering and engagement\nInterface and coordinate with client engagement partners to understand the RFP/ RFI requirements\nDetail out scope documents, functional & non-functional requirements, features etc ensuring all stated and unstated customer needs are captured\nConstruct workflow charts and diagrams, studying system capabilities, writing specification after thorough research and analysis of customer requirements\nEngage and interact with internal team - project managers, pre-sales team, tech leads, architects to design and formulate accurate and timely response to RFP/RFIs\nUnderstand and communicate the financial and operational impact of any changes\nPeriodic cadence with customers to seek clarifications and feedback wrt solution proposed for a particular RFP/ RFI and accordingly instructing delivery team to make changes in the design\nEmpower the customers through demonstration and presentation of the proposed solution/ prototype\nMaintain relationships with customers to optimize business integration and lead generation\nEnsure ongoing reviews and feedback from customers to improve and deliver better value (services/ products) to the customers\n\n\n ? _x000D_\n\n2. Engage with delivery team to ensure right solution is proposed to the customer\n\na. Periodic cadence with delivery team to\nProvide them with customer feedback/ inputs on the proposed solution\nReview the test cases to check 100% coverage of customer requirements\nConduct root cause analysis to understand the proposed solution/ demo/ prototype before sharing it with the customer\nDeploy and facilitate new change requests to cater to customer needs and requirements\nSupport QA team with periodic testing to ensure solutions meet the needs of businesses by giving timely inputs/feedback\nConduct Integration Testing and User Acceptance demo??s testing to validate implemented solutions and ensure 100% success rate\nUse data modelling practices to analyze the findings and design, develop improvements and changes\nEnsure 100% utilization by studying systems capabilities and understanding business specifications\nStitch the entire response/ solution proposed to the RFP/ RFI before its presented to the customer\n\nb. Support Project Manager/ Delivery Team in delivering the solution to the customer\nDefine and plan project milestones, phases and different elements involved in the project along with the principal consultant\nDrive and challenge the presumptions of delivery teams on how will they successfully execute their plans\nEnsure Customer Satisfaction through quality deliverable on time\n\n\n ? _x000D_\n\n3. Build domain expertise and contribute to knowledge repository\nEngage and interact with other BA??s to share expertise and increase domain knowledge across the vertical\nWrite whitepapers/ research papers, point of views and share with the consulting community at large\nIdentify and create used cases for a different project/ account that can be brought at Wipro level for business enhancements\nConduct market research for content and development to provide latest inputs into the projects thereby ensuring customer delight\n\n\n ? _x000D_\n\nDeliver\n\n\nNo.\n\nPerformance Parameter\n\nMeasure 1. Customer Engagement and Delivery Management PCSAT, utilization % achievement, no. of leads generated from the business interaction, no. of errors/ gaps in documenting customer requirements, feedback from project manager, process flow diagrams (quality and timeliness), % of deal solutioning completed within timeline, velocity generated. 2. Knowledge Management No. of whitepapers/ research papers written, no. of user stories created, % of proposal documentation completed and uploaded into knowledge repository, No of reusable components developed for proposal during quarter\n\nMandatory\n\nSkills:\nHC - Payor_x000D_.\n\nExperience5-8 Years_x000D_.\n\nReinvent your world. We are building a modern Wipro. We are an end-to-end digital transformation partner with the boldest ambitions. To realize them, we need people inspired by reinvention. Of yourself, your career, and your skills. We want to see the constant evolution of our business and our industry. It has always been in our DNA - as the world around us changes, so do we. Join a business powered by purpose and a place that empowers you to design your own reinvention. Come to Wipro. Realize your ambitions. Applications from people with disabilities are explicitly welcome.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['project management', 'business analysis', 'change request', 'root cause analysis', 'rfp', 'charts', 'client engagement', 'documentation', 'test cases', 'user stories', 'rfis', 'market research', 'prototype', 'diagramming', 'rfi', 'integration testing', 'flow diagrams', 'digital transformation']",2025-06-10 15:25:06
Business Analyst - L3,Wipro,3 - 5 years,Not Disclosed,['Pune'],"Role Purpose\nThe purpose of the role is to liaison and bridging the gap between customer and Wipro delivery team to comprehend and analyze customer requirements and articulating aptly to delivery teams thereby, ensuring right solutioning to the customer.\n\n\n\nDo\n1. Customer requirements gathering and engagement\nInterface and coordinate with client engagement partners to understand the RFP/ RFI requirements\nDetail out scope documents, functional & non-functional requirements, features etc ensuring all stated and unstated customer needs are captured\nConstruct workflow charts and diagrams, studying system capabilities, writing specification after thorough research and analysis of customer requirements\nEngage and interact with internal team - project managers, pre-sales team, tech leads, architects to design and formulate accurate and timely response to RFP/RFIs\nUnderstand and communicate the financial and operational impact of any changes\nPeriodic cadence with customers to seek clarifications and feedback wrt solution proposed for a particular RFP/ RFI and accordingly instructing delivery team to make changes in the design\nEmpower the customers through demonstration and presentation of the proposed solution/ prototype\nMaintain relationships with customers to optimize business integration and lead generation\nEnsure ongoing reviews and feedback from customers to improve and deliver better value (services/ products) to the customers\n\n\n\n2.Engage with delivery team to ensure right solution is proposed to the customer\na.Periodic cadence with delivery team to:\nProvide them with customer feedback/ inputs on the proposed solution\nReview the test cases to check 100% coverage of customer requirements\nConduct root cause analysis to understand the proposed solution/ demo/ prototype before sharing it with the customer\nDeploy and facilitate new change requests to cater to customer needs and requirements\nSupport QA team with periodic testing to ensure solutions meet the needs of businesses by giving timely inputs/feedback\nConduct Integration Testing and User Acceptance demos testing to validate implemented solutions and ensure 100% success rate\nUse data modelling practices to analyze the findings and design, develop improvements and changes\nEnsure 100% utilization by studying systems capabilities and understanding business specifications\nStitch the entire response/ solution proposed to the RFP/ RFI before its presented to the customer\nb.Support Project Manager/ Delivery Team in delivering the solution to the customer\nDefine and plan project milestones, phases and different elements involved in the project along with the principal consultant\nDrive and challenge the presumptions of delivery teams on how will they successfully execute their plans\nEnsure Customer Satisfaction through quality deliverable on time\n\n\n\n3.Build domain expertise and contribute to knowledge repository\nEngage and interact with other BAs to share expertise and increase domain knowledge across the vertical\nWrite whitepapers/ research papers, point of views and share with the consulting community at large\nIdentify and create used cases for a different project/ account that can be brought at Wipro level for business enhancements\nConduct market research for content and development to provide latest inputs into the projects thereby ensuring customer delight\n\n\n\nDeliver\n\nNo.Performance ParameterMeasure\n1. Customer Engagement and Delivery ManagementPCSAT, utilization % achievement, no. of leads generated from the business interaction, no. of errors/ gaps in documenting customer requirements, feedback from project manager, process flow diagrams (quality and timeliness), % of deal solutioning completed within timeline, velocity generated.\n2. Knowledge ManagementNo. of whitepapers/ research papers written, no. of user stories created, % of proposal documentation completed and uploaded into knowledge repository, No of reusable components developed for proposal during quarter\nMandatory Skills: eCommerce Consulting.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['eCommerce Consulting', 'User Acceptance Testing', 'RFP/ RFI', 'Project management', 'Integration Testing', 'Business Analysis']",2025-06-10 15:25:08
Business Analyst - L4,Wipro,2 - 6 years,Not Disclosed,['Chennai'],"Wipro Limited (NYSE:WIT, BSE:507685, NSE:WIPRO) is a leading technology services and consulting company focused on building innovative solutions that address clients most complex digital transformation needs. Leveraging our holistic portfolio of capabilities in consulting, design, engineering, and operations, we help clients realize their boldest ambitions and build future-ready, sustainable businesses. With over 230,000 employees and business partners across 65 countries, we deliver on the promise of helping our customers, colleagues, and communities thrive in an ever-changing world. For additional information, visit us at www.wipro.com.\n\nAbout The Role :\n\n\n\nRole Purpose\n\nThe purpose of the role is to liaison and bridging the gap between customer and Wipro delivery team to comprehend and analyze customer requirements and articulating aptly to delivery teams thereby, ensuring right solutioning to the customer.\n\n\n\n\n\nDo\n\n1. Customer requirements gathering and engagement\nInterface and coordinate with client engagement partners to understand the RFP/ RFI requirements\nDetail out scope documents, functional & non-functional requirements, features etc ensuring all stated and unstated customer needs are captured\nConstruct workflow charts and diagrams, studying system capabilities, writing specification after thorough research and analysis of customer requirements\nEngage and interact with internal team - project managers, pre-sales team, tech leads, architects to design and formulate accurate and timely response to RFP/RFIs\nUnderstand and communicate the financial and operational impact of any changes\nPeriodic cadence with customers to seek clarifications and feedback wrt solution proposed for a particular RFP/ RFI and accordingly instructing delivery team to make changes in the design\nEmpower the customers through demonstration and presentation of the proposed solution/ prototype\nMaintain relationships with customers to optimize business integration and lead generation\nEnsure ongoing reviews and feedback from customers to improve and deliver better value (services/ products) to the customers\n\n\n\n\n2. Engage with delivery team to ensure right solution is proposed to the customer\na. Periodic cadence with delivery team to:\nProvide them with customer feedback/ inputs on the proposed solution\nReview the test cases to check 100% coverage of customer requirements\nConduct root cause analysis to understand the proposed solution/ demo/ prototype before sharing it with the customer\nDeploy and facilitate new change requests to cater to customer needs and requirements\nSupport QA team with periodic testing to ensure solutions meet the needs of businesses by giving timely inputs/feedback\nConduct Integration Testing and User Acceptance demos testing to validate implemented solutions and ensure 100% success rate\nUse data modelling practices to analyze the findings and design, develop improvements and changes\nEnsure 100% utilization by studying systems capabilities and understanding business specifications\nStitch the entire response/ solution proposed to the RFP/ RFI before its presented to the customer\nb. Support Project Manager/ Delivery Team in delivering the solution to the customer\nDefine and plan project milestones, phases and different elements involved in the project along with the principal consultant\nDrive and challenge the presumptions of delivery teams on how will they successfully execute their plans\nEnsure Customer Satisfaction through quality deliverable on time\n\n\n\n\n3. Build domain expertise and contribute to knowledge repository\nEngage and interact with other BAs to share expertise and increase domain knowledge across the vertical\nWrite whitepapers/ research papers, point of views and share with the consulting community at large\nIdentify and create used cases for a different project/ account that can be brought at Wipro level for business enhancements\nConduct market research for content and development to provide latest inputs into the projects thereby ensuring customer delight\n\n\n\n\n\n\nDeliver\n\n\n\n\nNo.\n\n\n\nPerformance Parameter\n\n\n\nMeasure 1. Customer Engagement and Delivery Management PCSAT, utilization % achievement, no. of leads generated from the business interaction, no. of errors/ gaps in documenting customer requirements, feedback from project manager, process flow diagrams (quality and timeliness), % of deal solutioning completed within timeline, velocity generated. 2. Knowledge Management No. of whitepapers/ research papers written, no. of user stories created, % of proposal documentation completed and uploaded into knowledge repository, No of reusable components developed for proposal during quarter\nReinvent your world.We are building a modern Wipro. We are an end-to-end digital transformation partner with the boldest ambitions. To realize them, we need people inspired by reinvention. Of yourself, your career, and your skills. We want to see the constant evolution of our business and our industry. It has always been in our DNA - as the world around us changes, so do we. Join a business powered by purpose and a place that empowers you to design your own reinvention. Come to Wipro. Realize your ambitions. Applications from people with disabilities are explicitly welcome.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['business analysis', 'user stories', 'root cause analysis', 'diagramming', 'integration testing', 'process flow diagram', 'project management', 'client engagement', 'documentation', 'test cases', 'rfis', 'market research', 'change request', 'prototype', 'demo', 'rfi', 'rfp', 'digital transformation']",2025-06-10 15:25:11
Business Analyst - L3,Wipro,3 - 5 years,Not Disclosed,['Mumbai'],"Role Purpose\nThe purpose of the role is to liaison and bridging the gap between customer and Wipro delivery team to comprehend and analyze customer requirements and articulating aptly to delivery teams thereby, ensuring right solutioning to the customer.\n\n\n\nDo\n1. Customer requirements gathering and engagement\nInterface and coordinate with client engagement partners to understand the RFP/ RFI requirements\nDetail out scope documents, functional & non-functional requirements, features etc ensuring all stated and unstated customer needs are captured\nConstruct workflow charts and diagrams, studying system capabilities, writing specification after thorough research and analysis of customer requirements\nEngage and interact with internal team - project managers, pre-sales team, tech leads, architects to design and formulate accurate and timely response to RFP/RFIs\nUnderstand and communicate the financial and operational impact of any changes\nPeriodic cadence with customers to seek clarifications and feedback wrt solution proposed for a particular RFP/ RFI and accordingly instructing delivery team to make changes in the design\nEmpower the customers through demonstration and presentation of the proposed solution/ prototype\nMaintain relationships with customers to optimize business integration and lead generation\nEnsure ongoing reviews and feedback from customers to improve and deliver better value (services/ products) to the customers\n\n\n\n2.Engage with delivery team to ensure right solution is proposed to the customer\na.Periodic cadence with delivery team to:\nProvide them with customer feedback/ inputs on the proposed solution\nReview the test cases to check 100% coverage of customer requirements\nConduct root cause analysis to understand the proposed solution/ demo/ prototype before sharing it with the customer\nDeploy and facilitate new change requests to cater to customer needs and requirements\nSupport QA team with periodic testing to ensure solutions meet the needs of businesses by giving timely inputs/feedback\nConduct Integration Testing and User Acceptance demos testing to validate implemented solutions and ensure 100% success rate\nUse data modelling practices to analyze the findings and design, develop improvements and changes\nEnsure 100% utilization by studying systems capabilities and understanding business specifications\nStitch the entire response/ solution proposed to the RFP/ RFI before its presented to the customer\nb.Support Project Manager/ Delivery Team in delivering the solution to the customer\nDefine and plan project milestones, phases and different elements involved in the project along with the principal consultant\nDrive and challenge the presumptions of delivery teams on how will they successfully execute their plans\nEnsure Customer Satisfaction through quality deliverable on time\n\n\n\n3.Build domain expertise and contribute to knowledge repository\nEngage and interact with other BAs to share expertise and increase domain knowledge across the vertical\nWrite whitepapers/ research papers, point of views and share with the consulting community at large\nIdentify and create used cases for a different project/ account that can be brought at Wipro level for business enhancements\nConduct market research for content and development to provide latest inputs into the projects thereby ensuring customer delight\n\n\n\nDeliver\n\nNo.Performance ParameterMeasure\n1. Customer Engagement and Delivery ManagementPCSAT, utilization % achievement, no. of leads generated from the business interaction, no. of errors/ gaps in documenting customer requirements, feedback from project manager, process flow diagrams (quality and timeliness), % of deal solutioning completed within timeline, velocity generated.\n2. Knowledge ManagementNo. of whitepapers/ research papers written, no. of user stories created, % of proposal documentation completed and uploaded into knowledge repository, No of reusable components developed for proposal during quarter\nMandatory Skills: Business Analysis.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Business Analysis', 'User Acceptance Testing', 'RFP/ RFI', 'Project management', 'Integration Testing']",2025-06-10 15:25:13
Business Analyst - L3,Wipro,3 - 5 years,Not Disclosed,['Pune'],"Role Purpose\nThe purpose of the role is to liaison and bridging the gap between customer and Wipro delivery team to comprehend and analyze customer requirements and articulating aptly to delivery teams thereby, ensuring right solutioning to the customer.\n\n\n\nDo\n1. Customer requirements gathering and engagement\nInterface and coordinate with client engagement partners to understand the RFP/ RFI requirements\nDetail out scope documents, functional & non-functional requirements, features etc ensuring all stated and unstated customer needs are captured\nConstruct workflow charts and diagrams, studying system capabilities, writing specification after thorough research and analysis of customer requirements\nEngage and interact with internal team - project managers, pre-sales team, tech leads, architects to design and formulate accurate and timely response to RFP/RFIs\nUnderstand and communicate the financial and operational impact of any changes\nPeriodic cadence with customers to seek clarifications and feedback wrt solution proposed for a particular RFP/ RFI and accordingly instructing delivery team to make changes in the design\nEmpower the customers through demonstration and presentation of the proposed solution/ prototype\nMaintain relationships with customers to optimize business integration and lead generation\nEnsure ongoing reviews and feedback from customers to improve and deliver better value (services/ products) to the customers\n\n\n\n2.Engage with delivery team to ensure right solution is proposed to the customer\na.Periodic cadence with delivery team to:\nProvide them with customer feedback/ inputs on the proposed solution\nReview the test cases to check 100% coverage of customer requirements\nConduct root cause analysis to understand the proposed solution/ demo/ prototype before sharing it with the customer\nDeploy and facilitate new change requests to cater to customer needs and requirements\nSupport QA team with periodic testing to ensure solutions meet the needs of businesses by giving timely inputs/feedback\nConduct Integration Testing and User Acceptance demos testing to validate implemented solutions and ensure 100% success rate\nUse data modelling practices to analyze the findings and design, develop improvements and changes\nEnsure 100% utilization by studying systems capabilities and understanding business specifications\nStitch the entire response/ solution proposed to the RFP/ RFI before its presented to the customer\nb.Support Project Manager/ Delivery Team in delivering the solution to the customer\nDefine and plan project milestones, phases and different elements involved in the project along with the principal consultant\nDrive and challenge the presumptions of delivery teams on how will they successfully execute their plans\nEnsure Customer Satisfaction through quality deliverable on time\n\n\n\n3.Build domain expertise and contribute to knowledge repository\nEngage and interact with other BAs to share expertise and increase domain knowledge across the vertical\nWrite whitepapers/ research papers, point of views and share with the consulting community at large\nIdentify and create used cases for a different project/ account that can be brought at Wipro level for business enhancements\nConduct market research for content and development to provide latest inputs into the projects thereby ensuring customer delight\n\n\n\nDeliver\n\nNo.Performance ParameterMeasure\n1. Customer Engagement and Delivery ManagementPCSAT, utilization % achievement, no. of leads generated from the business interaction, no. of errors/ gaps in documenting customer requirements, feedback from project manager, process flow diagrams (quality and timeliness), % of deal solutioning completed within timeline, velocity generated.\n2. Knowledge ManagementNo. of whitepapers/ research papers written, no. of user stories created, % of proposal documentation completed and uploaded into knowledge repository, No of reusable components developed for proposal during quarter\nMandatory Skills: Agile-Scrum.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Agile', 'User Acceptance Testing', 'RFP/ RFI', 'Project management', 'Integration Testing', 'Scrum', 'Business Analysis']",2025-06-10 15:25:16
Business Analyst - L3,Wipro,3 - 5 years,Not Disclosed,['Pune'],"Role Purpose\nThe purpose of the role is to liaison and bridging the gap between customer and Wipro delivery team to comprehend and analyze customer requirements and articulating aptly to delivery teams thereby, ensuring right solutioning to the customer.\n\nDo\n1. Customer requirements gathering and engagement\nInterface and coordinate with client engagement partners to understand the RFP/ RFI requirements\nDetail out scope documents, functional & non-functional requirements, features etc ensuring all stated and unstated customer needs are captured\nConstruct workflow charts and diagrams, studying system capabilities, writing specification after thorough research and analysis of customer requirements\nEngage and interact with internal team - project managers, pre-sales team, tech leads, architects to design and formulate accurate and timely response to RFP/RFIs\nUnderstand and communicate the financial and operational impact of any changes\nPeriodic cadence with customers to seek clarifications and feedback wrt solution proposed for a particular RFP/ RFI and accordingly instructing delivery team to make changes in the design\nEmpower the customers through demonstration and presentation of the proposed solution/ prototype\nMaintain relationships with customers to optimize business integration and lead generation\nEnsure ongoing reviews and feedback from customers to improve and deliver better value (services/ products) to the customers\n2.Engage with delivery team to ensure right solution is proposed to the customer\na.Periodic cadence with delivery team to:\nProvide them with customer feedback/ inputs on the proposed solution\nReview the test cases to check 100% coverage of customer requirements\nConduct root cause analysis to understand the proposed solution/ demo/ prototype before sharing it with the customer\nDeploy and facilitate new change requests to cater to customer needs and requirements\nSupport QA team with periodic testing to ensure solutions meet the needs of businesses by giving timely inputs/feedback\nConduct Integration Testing and User Acceptance demos testing to validate implemented solutions and ensure 100% success rate\nUse data modelling practices to analyze the findings and design, develop improvements and changes\nEnsure 100% utilization by studying systems capabilities and understanding business specifications\nStitch the entire response/ solution proposed to the RFP/ RFI before its presented to the customer\nb.Support Project Manager/ Delivery Team in delivering the solution to the customer\nDefine and plan project milestones, phases and different elements involved in the project along with the principal consultant\nDrive and challenge the presumptions of delivery teams on how will they successfully execute their plans\nEnsure Customer Satisfaction through quality deliverable on time\n\n\n\n3.Build domain expertise and contribute to knowledge repository\nEngage and interact with other BAs to share expertise and increase domain knowledge across the vertical\nWrite whitepapers/ research papers, point of views and share with the consulting community at large\nIdentify and create used cases for a different project/ account that can be brought at Wipro level for business enhancements\nConduct market research for content and development to provide latest inputs into the projects thereby ensuring customer delight\n\n\n\nDeliver\n\nNo.Performance ParameterMeasure\n1. Customer Engagement and Delivery ManagementPCSAT, utilization % achievement, no. of leads generated from the business interaction, no. of errors/ gaps in documenting customer requirements, feedback from project manager, process flow diagrams (quality and timeliness), % of deal solutioning completed within timeline, velocity generated.\n2. Knowledge ManagementNo. of whitepapers/ research papers written, no. of user stories created, % of proposal documentation completed and uploaded into knowledge repository, No of reusable components developed for proposal during quarter\nMandatory Skills: ServiceNow Core.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['ServiceNow Core', 'User Acceptance Testing', 'RFP/ RFI', 'Project management', 'Integration Testing', 'Business Analysis']",2025-06-10 15:25:19
Business Analyst - L3,Wipro,3 - 5 years,Not Disclosed,['Hyderabad'],"About The Role  \n\nRole Purpose\n\nThe purpose of the role is to liaison and bridging the gap between customer and Wipro delivery team to comprehend and analyze customer requirements and articulating aptly to delivery teams thereby, ensuring right solutioning to the customer.\n\n ? \n\nDo\n\n1. Customer requirements gathering and engagement\nInterface and coordinate with client engagement partners to understand the RFP/ RFI requirements\nDetail out scope documents, functional & non-functional requirements, features etc ensuring all stated and unstated customer needs are captured\nConstruct workflow charts and diagrams, studying system capabilities, writing specification after thorough research and analysis of customer requirements\nEngage and interact with internal team - project managers, pre-sales team, tech leads, architects to design and formulate accurate and timely response to RFP/RFIs\nUnderstand and communicate the financial and operational impact of any changes\nPeriodic cadence with customers to seek clarifications and feedback wrt solution proposed for a particular RFP/ RFI and accordingly instructing delivery team to make changes in the design\nEmpower the customers through demonstration and presentation of the proposed solution/ prototype\nMaintain relationships with customers to optimize business integration and lead generation\nEnsure ongoing reviews and feedback from customers to improve and deliver better value (services/ products) to the customers\n\n\n ? \n\n2. Engage with delivery team to ensure right solution is proposed to the customer\n\na. Periodic cadence with delivery team to\nProvide them with customer feedback/ inputs on the proposed solution\nReview the test cases to check 100% coverage of customer requirements\nConduct root cause analysis to understand the proposed solution/ demo/ prototype before sharing it with the customer\nDeploy and facilitate new change requests to cater to customer needs and requirements\nSupport QA team with periodic testing to ensure solutions meet the needs of businesses by giving timely inputs/feedback\nConduct Integration Testing and User Acceptance demo’s testing to validate implemented solutions and ensure 100% success rate\nUse data modelling practices to analyze the findings and design, develop improvements and changes\nEnsure 100% utilization by studying systems capabilities and understanding business specifications\nStitch the entire response/ solution proposed to the RFP/ RFI before its presented to the customer\n\nb. Support Project Manager/ Delivery Team in delivering the solution to the customer\nDefine and plan project milestones, phases and different elements involved in the project along with the principal consultant\nDrive and challenge the presumptions of delivery teams on how will they successfully execute their plans\nEnsure Customer Satisfaction through quality deliverable on time\n\n\n ? \n\n3. Build domain expertise and contribute to knowledge repository\nEngage and interact with other BA’s to share expertise and increase domain knowledge across the vertical\nWrite whitepapers/ research papers, point of views and share with the consulting community at large\nIdentify and create used cases for a different project/ account that can be brought at Wipro level for business enhancements\nConduct market research for content and development to provide latest inputs into the projects thereby ensuring customer delight\n\n\n ? \n\nDeliver\n\n\nNo.\n\nPerformance Parameter\n\nMeasure 1. Customer Engagement and Delivery Management PCSAT, utilization % achievement, no. of leads generated from the business interaction, no. of errors/ gaps in documenting customer requirements, feedback from project manager, process flow diagrams (quality and timeliness), % of deal solutioning completed within timeline, velocity generated. 2. Knowledge Management No. of whitepapers/ research papers written, no. of user stories created, % of proposal documentation completed and uploaded into knowledge repository, No of reusable components developed for proposal during quarter\n\nMandatory\n\nSkills:\nServiceNow - Platform Core.\n\nExperience3-5 Years.\n\nReinvent your world. We are building a modern Wipro. We are an end-to-end digital transformation partner with the boldest ambitions. To realize them, we need people inspired by reinvention. Of yourself, your career, and your skills. We want to see the constant evolution of our business and our industry. It has always been in our DNA - as the world around us changes, so do we. Join a business powered by purpose and a place that empowers you to design your own reinvention. Come to Wipro. Realize your ambitions. Applications from people with disabilities are explicitly welcome.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['project management', 'software testing', 'business analysis', 'root cause analysis', 'servicenow', 'client engagement', 'documentation', 'test cases', 'change request', 'user stories', 'rfis', 'prototype', 'rfi', 'integration testing', 'flow diagrams', 'rfp', 'digital transformation']",2025-06-10 15:25:21
Business Analyst - L3,Wipro,3 - 5 years,Not Disclosed,['Pune'],"About The Role  \n\nRole Purpose\n\nThe purpose of the role is to liaison and bridging the gap between customer and Wipro delivery team to comprehend and analyze customer requirements and articulating aptly to delivery teams thereby, ensuring right solutioning to the customer.\n\n ? \n\nDo\n\n1. Customer requirements gathering and engagement\nInterface and coordinate with client engagement partners to understand the RFP/ RFI requirements\nDetail out scope documents, functional & non-functional requirements, features etc ensuring all stated and unstated customer needs are captured\nConstruct workflow charts and diagrams, studying system capabilities, writing specification after thorough research and analysis of customer requirements\nEngage and interact with internal team - project managers, pre-sales team, tech leads, architects to design and formulate accurate and timely response to RFP/RFIs\nUnderstand and communicate the financial and operational impact of any changes\nPeriodic cadence with customers to seek clarifications and feedback wrt solution proposed for a particular RFP/ RFI and accordingly instructing delivery team to make changes in the design\nEmpower the customers through demonstration and presentation of the proposed solution/ prototype\nMaintain relationships with customers to optimize business integration and lead generation\nEnsure ongoing reviews and feedback from customers to improve and deliver better value (services/ products) to the customers\n\n\n ? \n\n2. Engage with delivery team to ensure right solution is proposed to the customer\n\na. Periodic cadence with delivery team to\nProvide them with customer feedback/ inputs on the proposed solution\nReview the test cases to check 100% coverage of customer requirements\nConduct root cause analysis to understand the proposed solution/ demo/ prototype before sharing it with the customer\nDeploy and facilitate new change requests to cater to customer needs and requirements\nSupport QA team with periodic testing to ensure solutions meet the needs of businesses by giving timely inputs/feedback\nConduct Integration Testing and User Acceptance demo’s testing to validate implemented solutions and ensure 100% success rate\nUse data modelling practices to analyze the findings and design, develop improvements and changes\nEnsure 100% utilization by studying systems capabilities and understanding business specifications\nStitch the entire response/ solution proposed to the RFP/ RFI before its presented to the customer\n\nb. Support Project Manager/ Delivery Team in delivering the solution to the customer\nDefine and plan project milestones, phases and different elements involved in the project along with the principal consultant\nDrive and challenge the presumptions of delivery teams on how will they successfully execute their plans\nEnsure Customer Satisfaction through quality deliverable on time\n\n\n ? \n\n3. Build domain expertise and contribute to knowledge repository\nEngage and interact with other BA’s to share expertise and increase domain knowledge across the vertical\nWrite whitepapers/ research papers, point of views and share with the consulting community at large\nIdentify and create used cases for a different project/ account that can be brought at Wipro level for business enhancements\nConduct market research for content and development to provide latest inputs into the projects thereby ensuring customer delight\n\n\n ? \n\nDeliver\n\n\nNo.\n\nPerformance Parameter\n\nMeasure 1. Customer Engagement and Delivery Management PCSAT, utilization % achievement, no. of leads generated from the business interaction, no. of errors/ gaps in documenting customer requirements, feedback from project manager, process flow diagrams (quality and timeliness), % of deal solutioning completed within timeline, velocity generated. 2. Knowledge Management No. of whitepapers/ research papers written, no. of user stories created, % of proposal documentation completed and uploaded into knowledge repository, No of reusable components developed for proposal during quarter\n\nMandatory\n\nSkills:\nBusiness Analysis.\n\nExperience3-5 Years.\n\nReinvent your world. We are building a modern Wipro. We are an end-to-end digital transformation partner with the boldest ambitions. To realize them, we need people inspired by reinvention. Of yourself, your career, and your skills. We want to see the constant evolution of our business and our industry. It has always been in our DNA - as the world around us changes, so do we. Join a business powered by purpose and a place that empowers you to design your own reinvention. Come to Wipro. Realize your ambitions. Applications from people with disabilities are explicitly welcome.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['project management', 'software testing', 'business analysis', 'change request', 'root cause analysis', 'client engagement', 'documentation', 'test cases', 'user stories', 'rfis', 'market research', 'prototype', 'rfi', 'integration testing', 'flow diagrams', 'rfp', 'digital transformation']",2025-06-10 15:25:24
Business Analyst - L4,Wipro,5 - 8 years,Not Disclosed,['Pune'],"Role Purpose\nThe purpose of the role is to liaison and bridging the gap between customer and Wipro delivery team to comprehend and analyze customer requirements and articulating aptly to delivery teams thereby, ensuring right solutioning to the customer.\n\n\n\nDo\n1. Customer requirements gathering and engagement\nInterface and coordinate with client engagement partners to understand the RFP/ RFI requirements\nDetail out scope documents, functional & non-functional requirements, features etc ensuring all stated and unstated customer needs are captured\nConstruct workflow charts and diagrams, studying system capabilities, writing specification after thorough research and analysis of customer requirements\nEngage and interact with internal team - project managers, pre-sales team, tech leads, architects to design and formulate accurate and timely response to RFP/RFIs\nUnderstand and communicate the financial and operational impact of any changes\nPeriodic cadence with customers to seek clarifications and feedback wrt solution proposed for a particular RFP/ RFI and accordingly instructing delivery team to make changes in the design\nEmpower the customers through demonstration and presentation of the proposed solution/ prototype\nMaintain relationships with customers to optimize business integration and lead generation\nEnsure ongoing reviews and feedback from customers to improve and deliver better value (services/ products) to the customers\n\n\n\n2.Engage with delivery team to ensure right solution is proposed to the customer\na.Periodic cadence with delivery team to:\nProvide them with customer feedback/ inputs on the proposed solution\nReview the test cases to check 100% coverage of customer requirements\nConduct root cause analysis to understand the proposed solution/ demo/ prototype before sharing it with the customer\nDeploy and facilitate new change requests to cater to customer needs and requirements\nSupport QA team with periodic testing to ensure solutions meet the needs of businesses by giving timely inputs/feedback\nConduct Integration Testing and User Acceptance demos testing to validate implemented solutions and ensure 100% success rate\nUse data modelling practices to analyze the findings and design, develop improvements and changes\nEnsure 100% utilization by studying systems capabilities and understanding business specifications\nStitch the entire response/ solution proposed to the RFP/ RFI before its presented to the customer\nb.Support Project Manager/ Delivery Team in delivering the solution to the customer\nDefine and plan project milestones, phases and different elements involved in the project along with the principal consultant\nDrive and challenge the presumptions of delivery teams on how will they successfully execute their plans\nEnsure Customer Satisfaction through quality deliverable on time\n\n\n\n3.Build domain expertise and contribute to knowledge repository\nEngage and interact with other BAs to share expertise and increase domain knowledge across the vertical\nWrite whitepapers/ research papers, point of views and share with the consulting community at large\nIdentify and create used cases for a different project/ account that can be brought at Wipro level for business enhancements\nConduct market research for content and development to provide latest inputs into the projects thereby ensuring customer delight\n\n\n\nDeliver\n\nNo.Performance ParameterMeasure\n1. Customer Engagement and Delivery ManagementPCSAT, utilization % achievement, no. of leads generated from the business interaction, no. of errors/ gaps in documenting customer requirements, feedback from project manager, process flow diagrams (quality and timeliness), % of deal solutioning completed within timeline, velocity generated.\n2. Knowledge ManagementNo. of whitepapers/ research papers written, no. of user stories created, % of proposal documentation completed and uploaded into knowledge repository, No of reusable components developed for proposal during quarter\nMandatory Skills: Payments and Cards-Awards.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Payments and Cards', 'User Acceptance Testing', 'RFP/ RFI', 'Project management', 'Integration Testing', 'Business Analysis']",2025-06-10 15:25:26
Ai Ml Engineer,Randomtrees,3 - 8 years,Not Disclosed,['Chennai( Chennai Central RS )'],"Requirement\nRole: ML Engineer/Data Scientist\nExperience: 3 to 15 Years\nJob Description:\nResponsibilities:\nStrong understanding of ML algorithms, techniques, and best practices.\nStrong understanding of Databricks, Azure AI services and other ML platforms and cloud computing platforms (e.g., AWS, Azure, GCP) and frameworks (e.g., TensorFlow, PyTorch, scikit-learn).",,,,"['Generative Ai', 'Artificial Intelligence', 'Large Language Model', 'Retrieval Augmented Generation', 'Python', 'Langchain', 'Natural Language Processing', 'Neural Networks', 'Machine Learning', 'Deep Learning', 'Pytorch', 'Algorithms', 'Opencv', 'Image Processing', 'Aiml', 'Keras', 'Computer Vision']",2025-06-10 15:25:28
Software Engineer - Python/AWS,Zenoti,4 - 6 years,Not Disclosed,['Hyderabad'],"We're looking for a skilled Software Engineer (Python/AWS) to join our innovative team! This is a fantastic chance to contribute directly to building scalable and reliable software systems on AWS. you'll be working on core services that power our SaaS platform. If you're passionate about we'll-engineered software, cloud-native solutions, and using modern development tools like Cursor IDE and GitHub Copilot to boost your productivity, we want to hear from you\n  What will I be doing?\nBe a part of the team working on cutting-edge AI products in the we'llness industry.\nDesign, develop, and deploy cloud-based features using Python and relevant frameworks.\nCollaborate with data scientists and product managers to translate business requirements into technical solutions.\nLeverage data analysis techniques to extract insights and improve application functionality.\nWrite clean, maintainable, and we'll-documented code.\nConduct code reviews and contribute to improving code quality across the team.\nStay up-to-date with the latest advancements in Python, cloud technologies, and data analysis practices.\nTroubleshoot and debug complex technical issues.\nProactively identify and implement performance optimizations.\nWork effectively in a cross-functional team environment.\nWhat skills do I need\nBachelors degree in Computer Science or IT.\n4 to 6 years of overall experience as a Python developer.\nExperience in building solutions upon AWS cloud services.\nExperience in using Python data analytics frameworks like numpy, pandas and scipy is a must.\nExposure to ML frameworks like Tensorflow and PyTorch will be an added advantage.\nBenefits\nAttractive Compensation & Benefits\nComprehensive medical coverage for yourself and your immediate family\nAn environment where we'llbeing is high on priority - access to regular yoga, meditation, breathwork, nutrition counseling, stress management, inclusion of family for most benefit awareness building sessions\nOpportunities to be a part of a community and give back: Social activities are part of our culture; You can look forward to regular engagement, social work, community give-back initiatives",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data analysis', 'Nutrition', 'Customer retention', 'Stress management', 'Genetics', 'AWS', 'Python', 'CRM']",2025-06-10 15:25:30
Business_Analyst_DM,Renew,7 - 11 years,Not Disclosed,['Haryana'],"About Company\nJob Description\n  Manage and guide Business Analysts and cross functional teams\nWeekly cadenace with Business for requirement gathering and project updates\nUnderstand Business problems and identify constraints\nDesign digital and advance analytics solutions\nImplement solution with understanding of end-to-end architecture\nIdentify opportunities for implementation of new use cases\nEnsure ReD targets are met and delivered on time\nEnsure documentation of Use Cases\nQualification:\nEducation - Engineering (Electrical/Electronics) + MBA\n6 to 10 yrs relevant exp\nExp in Power Sector/Renewable energy/ Storage/Hydro/RTC power/Power Trading\nAnalytical approach with focus on solution delivery\nHandle multiple projects (intra and inter-department)\nKnowledge of Power markets is a must\nParticipated in some Digital transformation/enablement exercise in organization\nBasic understanding of Data Scientists & Data engineering roles\nExp with PowerBI, Tableau, JIRA would be a plus",Industry Type: Power,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['digital', 'project management', 'python', 'analytical', 'documentation', 'program management', 'power bi', 'business analysis', 'solution delivery', 'data warehousing', 'transformation', 'data engineering', 'sql', 'analytics', 'tableau', 'requirement gathering', 'pmp', 'use cases', 'agile', 'digital transformation', 'jira']",2025-06-10 15:25:33
Full Stack Developer,T7E,5 - 10 years,Not Disclosed,['Mumbai'],"Work with development teams and product managers to ideate software solutions\nDesign client-side and server-side architecture\nBuild the front-end of applications through an appealing visual design\nDevelop and manage well-functioning databases and applications\nWrite effective APIs\nTest software to ensure responsiveness and efficiency\nTroubleshoot, debug, and upgrade software\nCreate security and data protection settings\nBuild features and applications with a mobile-responsive design\nWrite technical documentation\nWork with data scientists and analysts to improve software\nRequirements and Skills:\nProven experience as a Full Stack Developer or similar role\nExperience developing desktop and mobile applications\nFamiliarity with common stacks\nKnowledge of multiple front-end languages and libraries (e.g., HTML/ CSS, JavaScript, XML, jQuery)\nKnowledge of numerous back-end languages (e.g., C#, Java, Python) and JavaScript frameworks (e.g., Angular, React, Node.js)\nFamiliarity with databases (e.g. MySQL, MongoDB), web servers (e.g. Apache), and UI/UX design\nExcellent communication and teamwork skills\nGreat attention to detail\nOrganizational skills\nAn analytical mind\nDegree in Computer Science, Statistics, or a relevant field",Industry Type: Film / Music / Entertainment,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'jQuery', 'Front end', 'XML', 'Analytical', 'MySQL', 'HTML', 'Apache', 'Python', 'Technical documentation']",2025-06-10 15:25:35
Specialist - Software Engineering,Acentra Health,5 - 7 years,Not Disclosed,['Chennai'],"As an AI QASpecialist within Acentra Healths AI & Analytics team, you will ensure theaccuracy, reliability, and validity of our AI, ML, and Generative AI (GenAI)products and services. Leveraging your expertise in quality assurancemethodologies, you will perform Functional, UI, and API testing to effectivelyvalidate products such as AI chatbots, ML/GenAI-generated clinical documents,microservices, and other software solutions. Your role will tackle uniquechallenges in GenAI environments, including output quality assurance andhallucination detection, ensuring stringent standards are consistently met.Aligned with this, you will support and contribute to our maturing AIInter-Rater Reliability (AI-IRR) process and services.",,,,"['algorithms', 'python', 'junit', 'software development', 'pytest', 'test cases', 'computer engineering', 'artificial intelligence', 'software development life cycle', 'selenium', 'qa tools', 'software solutions', 'software engineering', 'sdlc', 'agile methodology', 'communication skills', 'ml']",2025-06-10 15:25:37
Assistant Manager Clinical Data Management - Hyderabad,Hetero,4 - 9 years,Not Disclosed,['Hyderabad'],"Role & responsibilities\n\nDesigning and implementing clinical data collection systems and tools\nCoordinating with investigators and other healthcare professionals to gather and interpret clinical data\nOverseeing the data management process, including data collection, entry, verification, and cleaning",,,,"['clinical data collection', 'Clinical Data Management', 'Clinical Data', 'Clinical Database Design', 'SAS Programming']",2025-06-10 15:25:40
Full Stack Developer (Node JS),T7E,5 - 10 years,Not Disclosed,['Mumbai'],"Work with development teams and product managers to ideate software solutions\nDesign client-side and server-side architecture\nBuild the front-end of applications through an appealing visual design\nDevelop and manage well-functioning databases and applications\nWrite effective APIs\nTest software to ensure responsiveness and efficiency\nTroubleshoot, debug, and upgrade software\nCreate security and data protection settings\nBuild features and applications with a mobile-responsive design\nWrite technical documentation\nWork with data scientists and analysts to improve software\nRequirements and Skills:\nProven experience as a Full Stack Developer or similar role\nExperience developing desktop and mobile applications\nFamiliarity with common stacks\nKnowledge of multiple front-end languages and libraries (e.g., HTML/ CSS, JavaScript, XML, jQuery)\nKnowledge of numerous back-end languages (e.g., C#, Java, Python) and JavaScript frameworks (e.g., Angular, React, Node.js)\nFamiliarity with databases (e.g. MySQL, MongoDB), web servers (e.g. Apache), and UI/UX design\nExcellent communication and teamwork skills\nGreat attention to detail\nOrganizational skills\nAn analytical mind\nDegree in Computer Science, Statistics, or a relevant field",Industry Type: Film / Music / Entertainment,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'jQuery', 'Front end', 'XML', 'Analytical', 'MySQL', 'HTML', 'Apache', 'Python', 'Technical documentation']",2025-06-10 15:25:42
Bigdata Administrator,Kryon Knowledge Works,5 - 8 years,Not Disclosed,['Hyderabad'],"Location: Hyderabad (Hybrid)\nPlease share your resume with +91 9361912009\n\nRoles and Responsibilities\n\nDeep understanding of Linux, networking and security fundamentals.\nExperience working with AWS cloud platform and infrastructure.\nExperience working with infrastructure as code with Terraform or Ansible tools.\nExperience managing large BigData clusters in production (at least one of -- Cloudera, Hortonworks, EMR).\nExcellent knowledge and solid work experience providing observability for BigData platforms using tools like Prometheus, InfluxDB, Dynatrace, Grafana, Splunk etc.\nExpert knowledge on Hadoop Distributed File System (HDFS) and Hadoop YARN.\nDecent knowledge of various Hadoop file formats like ORC, Parquet, Avro etc.\nDeep understanding of Hive (Tez), Hive LLAP, Presto and Spark compute engines.\nAbility to understand query plans and optimize performance for complex SQL queries on Hive and Spark.\nExperience supporting Spark with Python (PySpark) and R (SparklyR, SparkR) languages\nSolid professional coding experience with at least one scripting language - Shell, Python etc.\nExperience working with Data Analysts, Data Scientists and at least one of these related analytical applications like SAS, R-Studio, JupyterHub, H2O etc.\nAble to read and understand code (Java, Python, R, Scala), but expertise in at least one scripting languages like Python or Shell.\n\nNice to have skills:\n\nExperience with workflow management tools like Airflow, Oozie etc.\nKnowledge in analytical libraries like Pandas, Numpy, Scipy, PyTorch etc.\nImplementation history of Packer, Chef, Jenkins or any other similar tooling.\nPrior working knowledge of Active Directory and Windows OS based VDI platforms like Citrix, AWS Workspaces etc.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['ORC', 'Linux', 'Terraform', 'AWS', 'Active Directory', 'Parquet', 'PySpark', 'Hadoop', 'Avro']",2025-06-10 15:25:44
Data Modeller,Inspira Enterprise India,2 - 6 years,Not Disclosed,['Kolkata'],"Data Modeling Design:\n\nDesign and maintain conceptual, logical, and physical data models to support transactional, analytical, and data warehousing systems.\n\nDevelop data models to ensure data consistency, integrity, and quality across multiple banking functions and applications.\n\nDefine and implement the best practices for data modeling, data integration, and metadata management.\n\n\nData Architecture Collaboration:\n\nCollaborate with data architects to align models with enterprise data architecture and ensure optimal performance and scalability.\n\nWork with database administrators to translate logical models into physical database structures and optimize them for performance.\n\n\nData Quality Governance:\n\nEstablish data standards, definitions, and quality rules to ensure data accuracy, consistency, and compliance.\n\nCreate and maintain data dictionaries and metadata repositories to support data governance and facilitate efficient data access.\n\n\nStakeholder Engagement:\n\nEngage with business stakeholders, data scientists, and IT teams to understand data requirements and translate business needs into robust data models.\n\nEnsure data models support key business initiatives, such as regulatory reporting, analytics, and operational efficiency.\n\n\nDocumentation Best Practices:\n\nDevelop and maintain detailed documentation, including data models, entity-relationship diagrams, and mapping specifications.\n\nImplement data modeling standards and mentor team members to promote best practices in data management.\n\n\n\n\n\n\nRequirements:\n\n\nEducational Qualification:\n\nMBA / Engineering degree with relevant industry experience.\n\n\nExperience:\n\nMinimum of 10 years of experience in data modeling or as a data modeler within the banking industry.\n\nProven expertise in designing data models across at least three of the following areas:\n\nData Warehousing\n\nAnalytics BI\n\nData Mining Data Quality\n\nMetadata Management\n\n\n\nTechnical Skills:\n\nProficiency in data modeling tools such as Erwin, IBM InfoSphere, or SAP PowerDesigner.\n\nStrong SQL skills and experience in relational database systems like Oracle, SQL Server, or DB2.\n\nFamiliarity with big data technologies and NoSQL databases is a plus.\n\nKnowledge of ETL processes and tools (e. g. , Informatica, Talend) and experience working with BI tools (e. g. , Tableau, Power BI).\n\nKnowledge of SAS for analytics, data manipulation, and data management is a plus.\n\nStrong understanding of data governance frameworks, data quality management, and regulatory compliance.\n\n\nSoft Skills:\n\nStrong analytical and problem-solving skills, with attention to detail and accuracy.\n\nExcellent communication and interpersonal skills, with the ability to translate technical data concepts for business stakeholders.\n\nProven ability to work collaboratively in a cross-functional environment and manage multiple projects.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['metadata', 'SAP', 'SAS', 'Data management', 'Db2', 'Data modeling', 'Data quality', 'Informatica', 'Data mining', 'Analytics']",2025-06-10 15:25:46
Data Engineer - GCP,Egen Solutions,4 - 6 years,Not Disclosed,['Hyderabad'],"Job Overview:\n\nWe are looking for a skilled and motivated Data Engineer with strong experience in Python programming and Google Cloud Platform (GCP) to join our data engineering team. The ideal candidate will be responsible for designing, developing, and maintaining robust and scalable ETL (Extract, Transform, Load) data pipelines. The role involves working with various GCP services, implementing data ingestion and transformation logic, and ensuring data quality and consistency across systems.\nKey Responsibilities:\nDesign, develop, test, and maintain scalable ETL data pipelines using Python.\nWork extensively on Google Cloud Platform (GCP) services such as:\nDataflow for real-time and batch data processing\nCloud Functions for lightweight serverless compute\nBigQuery for data warehousing and analytics\nCloud Composer for orchestration of data workflows (based on Apache Airflow)\nGoogle Cloud Storage (GCS) for managing data at scale\nIAM for access control and security\nCloud Run for containerized applications\nPerform data ingestion from various sources and apply transformation and cleansing logic to ensure high-quality data delivery.\nImplement and enforce data quality checks, validation rules, and monitoring.\nCollaborate with data scientists, analysts, and other engineering teams to understand data needs and deliver efficient data solutions.\nManage version control using GitHub and participate in CI/CD pipeline deployments for data projects.\nWrite complex SQL queries for data extraction and validation from relational databases such as SQL Server, Oracle, or PostgreSQL.\nDocument pipeline designs, data flow diagrams, and operational support procedures.\nRequired Skills:\n4-6 years of hands-on experience in Python for backend or data engineering projects.\nStrong understanding and working experience with GCP cloud services (especially Dataflow, BigQuery, Cloud Functions, Cloud Composer, etc.).\nSolid understanding of data pipeline architecture, data integration, and transformation techniques.\nExperience in working with version control systems like GitHub and knowledge of CI/CD practices.\nStrong experience in SQL with at least one enterprise database (SQL Server, Oracle, PostgreSQL, etc.).\n\nGood to Have (Optional Skills):\nExperience working with Snowflake cloud data platform.\nHands-on knowledge of Databricks for big data processing and analytics.\nFamiliarity with Azure Data Factory (ADF) and other Azure data engineering tools.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Backend', 'Version control', 'Postgresql', 'Data quality', 'Oracle', 'Apache', 'Analytics', 'Monitoring', 'Python', 'Data extraction']",2025-06-10 15:25:49
Analyst,Wipro,1 - 3 years,Not Disclosed,['Gurugram'],"Wipro Limited (NYSEWIT, BSE507685, NSEWIPRO) is a leading technology services and consulting company focused on building innovative solutions that address clients’ most complex digital transformation needs. Leveraging our holistic portfolio of capabilities in consulting, design, engineering, and operations, we help clients realize their boldest ambitions and build future-ready, sustainable businesses. With over 230,000 employees and business partners across 65 countries, we deliver on the promise of helping our customers, colleagues, and communities thrive in an ever-changing world. For additional information, visit us at www.wipro.com.\n About The Role  _x000D_\n\nrepare and present fund level monthly/quarterly valuations and reports of real estate investments within fund and liquid debt securities\nResponsible for the preparation and reviewing of monthly/quarterly reporting along with various type of analysis including net debt analysis, FX MTM impact analysis and fund benchmarking analysis\nUndertake analysis to identify key trends, investigate unusual items by coordinating across teams and provide variance analysis, with explanations, to the stakeholders\nPrepare and review materials for funds' regulatory reporting\nAssist with finance operations including Limited Liability Partnership administration and execution of centralised finance processes\nOversee co-ordination and submission of quarterly reporting with onshore and offshore teams ensuring the accuracy of the data\nContinuously seek to create operational efficiencies and reporting enhancements\nAd-hoc projects in support of the Firm’s businesses/new initiatives Essential Skills & Experience\nMaster’s degree – Major/Minor in Finance, Banking or Mathematics is a plus\nHighly proficient in Advance Excel & Powerpoint skills\nProficient clarity on basics of Financial statements\nPrevious background within a valuation or reporting role a plus\nCFA designation and knowledge on derivatives is a plus\nHighly organized and able to prioritise deliverables and meet demanding deadlines in a fast paced environment\nExcellent interpersonal and communication skills, both written and verbal\nHighly self motivated with ability to work independently and work effectively in a team\nCollaborative and able to build strong relationships with a broad range of stakeholders\nStrong initiative, energy and confidence completing assignments with limited supervisio\n\n ? _x000D_\n\n repare and present fund level monthly/quarterly valuations and reports of real estate investments within fund and liquid debt securities\nResponsible for the preparation and reviewing of monthly/quarterly reporting along with various type of analysis including net debt analysis, FX MTM impact analysis and fund benchmarking analysis\nUndertake analysis to identify key trends, investigate unusual items by coordinating across teams and provide variance analysis, with explanations, to the stakeholders\nPrepare and review materials for funds' regulatory reporting\nAssist with finance operations including Limited Liability Partnership administration and execution of centralised finance processes\nOversee co-ordination and submission of quarterly reporting with onshore and offshore teams ensuring the accuracy of the data\nContinuously seek to create operational efficiencies and reporting enhancements\nAd-hoc projects in support of the Firm’s businesses/new initiatives Essential Skills & Experience\nMaster’s degree – Major/Minor in Finance, Banking or Mathematics is a plus\nHighly proficient in Advance Excel & Powerpoint skills\nProficient clarity on basics of Financial statements\nPrevious background within a valuation or reporting role a plus\nCFA designation and knowledge on derivatives is a plus\nHighly organized and able to prioritise deliverables and meet demanding deadlines in a fast paced environment\nExcellent interpersonal and communication skills, both written and verbal\nHighly self motivated with ability to work independently and work effectively in a team\nCollaborative and able to build strong relationships with a broad range of stakeholders\nStrong initiative, energy and confidence completing assignments with limited supervisio \n\n ? _x000D_\n\n \n\nDeliver excellent customer service through effective diagnosis and troubleshooting of client queries  \nProvide product support and resolution to clients by performing a question diagnosis while guiding users through step-by-step solutions\nAssist clients with navigating around product menus and facilitate better understanding of product features\nTroubleshoot all client queries in a user-friendly, courteous and professional manner\nMaintain logs and records of all customer queries as per the standard procedures and guidelines\nAccurately process and record all incoming call and email using the designated tracking software\nOffer alternative solutions to clients (where appropriate) with the objective of retaining customers’ and clients’ business\nOrganize ideas and effectively communicate oral messages appropriate to listeners and situations\nFollow up and make scheduled call backs to customers to record feedback and ensure compliance to contract /SLAs\n\n\n ? _x000D_\n\n \n\nBuild capability to ensure operational excellence and maintain superior customer service levels of the existing account/client  \nUndertake product trainings to stay current with product features, changes and updates\nEnroll in product specific and any other trainings per client requirements/recommendations\nPartner with team leaders to brainstorm and identify training themes and learning issues to better serve the client\nUpdate job knowledge by participating in self learning opportunities and maintaining personal networks\n\n\n ? _x000D_\n\n \n\nDeliver\n NoPerformance ParameterMeasure1ProcessNo. of cases resolved per day, compliance to process and quality standards, meeting process level SLAs, Pulse score, Customer feedback2Self- ManagementProductivity, efficiency, absenteeism, Training Hours, No of technical training completedMandatory\n\nSkills:\nInstitutional_Finance_Buy_Side_Others_x000D_.\n\nExperience1-3 Years_x000D_.\n\nReinvent your world. We are building a modern Wipro. We are an end-to-end digital transformation partner with the boldest ambitions. To realize them, we need people inspired by reinvention. Of yourself, your career, and your skills. We want to see the constant evolution of our business and our industry. It has always been in our DNA - as the world around us changes, so do we. Join a business powered by purpose and a place that empowers you to design your own reinvention. Come to Wipro. Realize your ambitions. Applications from people with disabilities are explicitly welcome.",Industry Type: IT Services & Consulting,Department: Finance & Accounting,"Employment Type: Full Time, Permanent","['customer service', 'derivatives', 'process compliance', 'advanced excel', 'financial statements', 'project management', 'mathematics', 'variance analysis', 'regulatory reporting', 'valuation', 'troubleshooting', 'operational excellence', 'digital transformation', 'finance', 'cfa']",2025-06-10 15:25:52
Analyst,Wipro,1 - 3 years,Not Disclosed,['Mumbai'],"Underwrite new business applications against an established set criteria for Consumer finance applications\nAssess all new applications in line with Retail Lending Standards and Policy\nWork in conjunction with our sales teams to identify and resolve any challenges in relation to decisions and appeals\nBe responsible for dealing with the more complex enquiries from both staff and external sources to provide an informed response which results in a TCF outcome for those concerned.\nApply Consumer Duty principles, and aim to deliver fair outcomes in all of our day-to-day activities.\nShare and maintain best practice in lending appropriately and fairly, achieving compliance with all applicable internal and external standards including but not limited to Regulation, Group Internal Audit, Group Credit Risk Policy and Lending Standards, in addition to operating within stated levels of risk appetite/mandate\nMaintain a comprehensive and up to date knowledge and understanding of MotoNovo Finance products and services, providing accurate information and proactively promoting them where appropriate.\nWork collaboratively and assist and support with the Training and Competence of new underwriters ensuring this is appropriately and suitably evidenced, both initially throughout the induction process, and subsequently on an ongoing basis at all mandate levels.\nMaintain an up-to-date knowledge of regulatory and legislative information related to MotoNovo Finance products and services (e.g. ICOB, CONC, FCA Rules, Money Laundering, Data Protection, CDD, KYC etc.)\nMaintain an excellent knowledge and understanding of all relevant policies and procedures relevant to the role, being compliant with these policies and procedures at all times.",Industry Type: IT Services & Consulting,"Department: Customer Success, Service & Operations","Employment Type: Full Time, Permanent","['Retail Lending Standards', 'CDD', 'Group Credit Risk Policy', 'FCA Rules', 'Data Protection', 'KYC', 'Money Laundering', 'Internal Auditing', 'Consumer Duty principles', 'CONC', 'ICOB']",2025-06-10 15:25:54
Analyst,Wipro,0 - 4 years,Not Disclosed,['Gurugram'],"About The Role  \n\nRole Purpose\n\nThe purpose of the role is to provide effective technical support to the process and actively resolve client issues directly or through timely escalation to meet process SLAs.\n\n\n ? \n\n \n\nDo  \n\n \n\nSupport process by managing transactions as per required quality standards  \nFielding all incoming help requests from clients via telephone and/or emails in a courteous manner\nDocument all pertinent end user identification information, including name, department, contact information and nature of problem or issue\nUpdate own availability in the RAVE system to ensure productivity of the process\nRecord, track, and document all queries received, problem-solving steps taken and total successful and unsuccessful resolutions\nFollow standard processes and procedures to resolve all client queries\nResolve client queries as per the SLA’s defined in the contract\nAccess and maintain internal knowledge bases, resources and frequently asked questions to aid in and provide effective problem resolution to clients\nIdentify and learn appropriate product details to facilitate better client interaction and troubleshooting\nDocument and analyze call logs to spot most occurring trends to prevent future problems\nMaintain and update self-help documents for customers to speed up resolution time\nIdentify red flags and escalate serious client issues to Team leader in cases of untimely resolution\nEnsure all product information and disclosures are given to clients before and after the call/email requests\nAvoids legal challenges by complying with service agreements\n\n\n ? \n\n \n\nDeliver excellent customer service through effective diagnosis and troubleshooting of client queries  \nProvide product support and resolution to clients by performing a question diagnosis while guiding users through step-by-step solutions\nAssist clients with navigating around product menus and facilitate better understanding of product features\nTroubleshoot all client queries in a user-friendly, courteous and professional manner\nMaintain logs and records of all customer queries as per the standard procedures and guidelines\nAccurately process and record all incoming call and email using the designated tracking software\nOffer alternative solutions to clients (where appropriate) with the objective of retaining customers’ and clients’ business\nOrganize ideas and effectively communicate oral messages appropriate to listeners and situations\nFollow up and make scheduled call backs to customers to record feedback and ensure compliance to contract /SLAs\n\n\n ? \n\n \n\nBuild capability to ensure operational excellence and maintain superior customer service levels of the existing account/client  \nUndertake product trainings to stay current with product features, changes and updates\nEnroll in product specific and any other trainings per client requirements/recommendations\nPartner with team leaders to brainstorm and identify training themes and learning issues to better serve the client\nUpdate job knowledge by participating in self learning opportunities and maintaining personal networks\n\n\n ? \n\n \n\nDeliver\n NoPerformance ParameterMeasure1ProcessNo. of cases resolved per day, compliance to process and quality standards, meeting process level SLAs, Pulse score, Customer feedback2Self- ManagementProductivity, efficiency, absenteeism, Training Hours, No of technical training completed",Industry Type: IT Services & Consulting,"Department: Customer Success, Service & Operations","Employment Type: Full Time, Permanent","['customer service', 'technical support', 'process compliance', 'troubleshooting', 'operational excellence', 'project management', 'operations management', 'team management', 'workforce management', 'real time analysis', 'process management', 'client interaction', 'wfm', 'sla management']",2025-06-10 15:25:57
Analyst,Wipro,0 - 4 years,Not Disclosed,['Gurugram'],"Wipro Limited (NYSEWIT, BSE507685, NSEWIPRO) is a leading technology services and consulting company focused on building innovative solutions that address clients’ most complex digital transformation needs. Leveraging our holistic portfolio of capabilities in consulting, design, engineering, and operations, we help clients realize their boldest ambitions and build future-ready, sustainable businesses. With over 230,000 employees and business partners across 65 countries, we deliver on the promise of helping our customers, colleagues, and communities thrive in an ever-changing world. For additional information, visit us at www.wipro.com.\n About The Role  \n\nRole Purpose\n\nThe purpose of the role is to provide effective technical support to the process and actively resolve client issues directly or through timely escalation to meet process SLAs.\n\n\n ? \n\n \n\nDo  \n\n \n\nSupport process by managing transactions as per required quality standards  \nFielding all incoming help requests from clients via telephone and/or emails in a courteous manner\nDocument all pertinent end user identification information, including name, department, contact information and nature of problem or issue\nUpdate own availability in the RAVE system to ensure productivity of the process\nRecord, track, and document all queries received, problem-solving steps taken and total successful and unsuccessful resolutions\nFollow standard processes and procedures to resolve all client queries\nResolve client queries as per the SLA’s defined in the contract\nAccess and maintain internal knowledge bases, resources and frequently asked questions to aid in and provide effective problem resolution to clients\nIdentify and learn appropriate product details to facilitate better client interaction and troubleshooting\nDocument and analyze call logs to spot most occurring trends to prevent future problems\nMaintain and update self-help documents for customers to speed up resolution time\nIdentify red flags and escalate serious client issues to Team leader in cases of untimely resolution\nEnsure all product information and disclosures are given to clients before and after the call/email requests\nAvoids legal challenges by complying with service agreements\n\n\n ? \n\n \n\nDeliver excellent customer service through effective diagnosis and troubleshooting of client queries  \nProvide product support and resolution to clients by performing a question diagnosis while guiding users through step-by-step solutions\nAssist clients with navigating around product menus and facilitate better understanding of product features\nTroubleshoot all client queries in a user-friendly, courteous and professional manner\nMaintain logs and records of all customer queries as per the standard procedures and guidelines\nAccurately process and record all incoming call and email using the designated tracking software\nOffer alternative solutions to clients (where appropriate) with the objective of retaining customers’ and clients’ business\nOrganize ideas and effectively communicate oral messages appropriate to listeners and situations\nFollow up and make scheduled call backs to customers to record feedback and ensure compliance to contract /SLAs\n\n\n ? \n\n \n\nBuild capability to ensure operational excellence and maintain superior customer service levels of the existing account/client  \nUndertake product trainings to stay current with product features, changes and updates\nEnroll in product specific and any other trainings per client requirements/recommendations\nPartner with team leaders to brainstorm and identify training themes and learning issues to better serve the client\nUpdate job knowledge by participating in self learning opportunities and maintaining personal networks\n\n\n ? \n\n \n\nDeliver\n NoPerformance ParameterMeasure1ProcessNo. of cases resolved per day, compliance to process and quality standards, meeting process level SLAs, Pulse score, Customer feedback2Self- ManagementProductivity, efficiency, absenteeism, Training Hours, No of technical training completed Reinvent your world.We are building a modern Wipro. We are an end-to-end digital transformation partner with the boldest ambitions. To realize them, we need people inspired by reinvention. Of yourself, your career, and your skills. We want to see the constant evolution of our business and our industry. It has always been in our DNA - as the world around us changes, so do we. Join a business powered by purpose and a place that empowers you to design your own reinvention. Come to Wipro. Realize your ambitions. Applications from people with disabilities are explicitly welcome.",Industry Type: IT Services & Consulting,"Department: Customer Success, Service & Operations","Employment Type: Full Time, Permanent","['customer service', 'technical support', 'process compliance', 'troubleshooting', 'operational excellence', 'project management', 'team management', 'workforce management', 'real time analysis', 'client interaction', 'digital transformation', 'wfm', 'sla management']",2025-06-10 15:25:59
Analyst,Wipro,2 - 6 years,Not Disclosed,['Gurugram'],"About The Role :\n\n\n\nRole Purpose\n\nThe purpose of the role is to provide effective technical support to the process and actively resolve client issues directly or through timely escalation to meet process SLAs.\n\n\nJob Title\n\n\n\n\n\nJob Responsibilities:\nTo work on Private Equity / Real Estate / Hedge Fund Operations such as Fund / Financial Accounting -Bookkeeping, Journal Posting, Preparation of Financial Statements, Management Fee Carried Interest and Expense Calculation & Posting. Capital Calls - Preparation of Capital Call memos, LC Opening, Follow Up with LPs for funding Distribution - Preparation of Distribution working, LP Memo Preparation, Fund Transfer Wire preparation, Repayment of LCs Reconciliation - Cash Reconciliation, Cash Tracking, Posting Entries, Cash reporting. Tracking of Capital transactions. Valuation - Valuation of Portfolio Investments.\nTo prepare and submit Fund and Investor Reports accurately as per SLA\nTo meet TAT and deliver error free services\nTo work on partnership accounting Applications\nTo strive to create a healthy and professional work environment in the team\nDisplay interpersonal skills in handling the day-to-day operations on the floor.\nSuggest and work on process improvements Idea\n\n\n\n\n\n\n\n\n\nDomain Skills:\nVery good understanding of Financial Accounting\nGeneral understanding of Capital Markets\nGeneral understanding of Banking\nGeneral understanding of Private Equity / Real Estate / Hedge Funds\nHands on experience of Private Equity / Real Estate / Hedge Funds systems\nReasonable understanding of Private Equity / Real Estate / Hedge Funds processes such as\nAccounting, Investor Reporting, Capital Calls, Distribution, Financial Statements etc.\n\n\n\n\n\n\n\n\n\nBuild capability to ensure operational excellence and maintain superior customer service levels of the existing account/client\nUndertake product trainings to stay current with product features, changes and updates\nEnroll in product specific and any other trainings per client requirements/recommendations\nPartner with team leaders to brainstorm and identify training themes and learning issues to better serve the client\nUpdate job knowledge by participating in self learning opportunities and maintaining personal networks\n\n\n\n\n\n\n\n\nDeliver\nNoPerformance ParameterMeasure1ProcessNo. of cases resolved per day, compliance to process and quality standards, meeting process level SLAs, Pulse score, Customer feedback2Self- ManagementProductivity, efficiency, absenteeism, Training Hours, No of technical training completed",Industry Type: IT Services & Consulting,Department: Finance & Accounting,"Employment Type: Full Time, Permanent","['private equity', 'capital market', 'process compliance', 'financial accounting', 'financial statements', 'financial analysis', 'data analysis', 'customer service', 'investment banking', 'hedge funds', 'accounting', 'technical support', 'valuation', 'operational excellence', 'finance']",2025-06-10 15:26:02
ML Engineer,HARMAN,3 - 5 years,Not Disclosed,['Bengaluru'],"As a technology leader that is rapidly on the move, HARMAN is filled with people who are focused on making life better. Innovation, inclusivity and teamwork are a part of our DNA. When you add that to the challenges we take on and solve together, you'll discover that at HARMAN you can grow, make a difference and be proud of the work you do everyday.\n  Introduction: Digital Transformation Solutions (DTS)\nwe're a global, multi-disciplinary team that s putting the innovative power of technology to work and transforming tomorrow. As a member of HARMAN Lifestyle, you connect consumers with the power of superior sound.",,,,"['C++', 'Linux', 'Image processing', 'Analytical', 'Machine learning', 'Debugging', 'Automotive', 'Python', 'Android']",2025-06-10 15:26:04
Business Analyst - L2,Wipro,1 - 3 years,Not Disclosed,['Hyderabad'],"Role Purpose\nThe purpose of the role is to liaison and bridging the gap between customer and Wipro delivery team to comprehend and analyze customer requirements and articulating aptly to delivery teams thereby, ensuring right solutioning to the customer.\nDo\n1. Customer requirements gathering and engagement\nInterface and coordinate with client engagement partners to understand the RFP/ RFI requirements\nDetail out scope documents, functional & non-functional requirements, features etc ensuring all stated and unstated customer needs are captured\nConstruct workflow charts and diagrams, studying system capabilities, writing specification after thorough research and analysis of customer requirements\nEngage and interact with internal team - project managers, pre-sales team, tech leads, architects to design and formulate accurate and timely response to RFP/RFIs\nUnderstand and communicate the financial and operational impact of any changes\nPeriodic cadence with customers to seek clarifications and feedback wrt solution proposed for a particular RFP/ RFI and accordingly instructing delivery team to make changes in the design\nEmpower the customers through demonstration and presentation of the proposed solution/ prototype\nMaintain relationships with customers to optimize business integration and lead generation\nEnsure ongoing reviews and feedback from customers to improve and deliver better value (services/ products) to the customers\n2.Engage with delivery team to ensure right solution is proposed to the customer\na.Periodic cadence with delivery team to:\nProvide them with customer feedback/ inputs on the proposed solution\nReview the test cases to check 100% coverage of customer requirements\nConduct root cause analysis to understand the proposed solution/ demo/ prototype before sharing it with the customer\nDeploy and facilitate new change requests to cater to customer needs and requirements\nSupport QA team with periodic testing to ensure solutions meet the needs of businesses by giving timely inputs/feedback\nConduct Integration Testing and User Acceptance demos testing to validate implemented solutions and ensure 100% success rate\nUse data modelling practices to analyze the findings and design, develop improvements and changes\nEnsure 100% utilization by studying systems capabilities and understanding business specifications\nStitch the entire response/ solution proposed to the RFP/ RFI before its presented to the customer\nb.Support Project Manager/ Delivery Team in delivering the solution to the customer\nDefine and plan project milestones, phases and different elements involved in the project along with the principal consultant\nDrive and challenge the presumptions of delivery teams on how will they successfully execute their plans\nEnsure Customer Satisfaction through quality deliverable on time\n\n3.Build domain expertise and contribute to knowledge repository\nEngage and interact with other BAs to share expertise and increase domain knowledge across the vertical\nWrite whitepapers/ research papers, point of views and share with the consulting community at large\nIdentify and create used cases for a different project/ account that can be brought at Wipro level for business enhancements\nConduct market research for content and development to provide latest inputs into the projects thereby ensuring customer delight\n\nDeliver\n\nNo. Performance Parameter Measure\n1.Customer Engagement and Delivery ManagementPCSAT, utilization % achievement, no. of leads generated from the business interaction, no. of errors/ gaps in documenting customer requirements, feedback from project manager, process flow diagrams (quality and timeliness), % of deal solutioning completed within timeline, velocity generated.\n2.Knowledge ManagementNo. of whitepapers/ research papers written, no. of user stories created, % of proposal documentation completed and uploaded into knowledge repository, No of reusable components developed for proposal during quarter\nMandatory Skills: Change Practice SME.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Business Analysis', 'QA', 'client engagement', 'Project Management', 'products services', 'Integration Testing', 'Data Analytics']",2025-06-10 15:26:06
Business Analyst - L2,Wipro,2 - 6 years,Not Disclosed,['Hyderabad'],"About The Role :\n\nRole Purpose\n\nThe purpose of the role is to liaison and bridging the gap between customer and Wipro delivery team to comprehend and analyze customer requirements and articulating aptly to delivery teams thereby, ensuring right solutioning to the customer.\n\nDo\n\n1. Customer requirements gathering and engagement\nInterface and coordinate with client engagement partners to understand the RFP/ RFI requirements\nDetail out scope documents, functional & non-functional requirements, features etc ensuring all stated and unstated customer needs are captured\nConstruct workflow charts and diagrams, studying system capabilities, writing specification after thorough research and analysis of customer requirements\nEngage and interact with internal team - project managers, pre-sales team, tech leads, architects to design and formulate accurate and timely response to RFP/RFIs\nUnderstand and communicate the financial and operational impact of any changes\nPeriodic cadence with customers to seek clarifications and feedback wrt solution proposed for a particular RFP/ RFI and accordingly instructing delivery team to make changes in the design\nEmpower the customers through demonstration and presentation of the proposed solution/ prototype\nMaintain relationships with customers to optimize business integration and lead generation\nEnsure ongoing reviews and feedback from customers to improve and deliver better value (services/ products) to the customers\n2. Engage with delivery team to ensure right solution is proposed to the customer\n\na. Periodic cadence with delivery team to:\nProvide them with customer feedback/ inputs on the proposed solution\nReview the test cases to check 100% coverage of customer requirements\nConduct root cause analysis to understand the proposed solution/ demo/ prototype before sharing it with the customer\nDeploy and facilitate new change requests to cater to customer needs and requirements\nSupport QA team with periodic testing to ensure solutions meet the needs of businesses by giving timely inputs/feedback\nConduct Integration Testing and User Acceptance demos testing to validate implemented solutions and ensure 100% success rate\nUse data modelling practices to analyze the findings and design, develop improvements and changes\nEnsure 100% utilization by studying systems capabilities and understanding business specifications\nStitch the entire response/ solution proposed to the RFP/ RFI before its presented to the customer\n\nb. Support Project Manager/ Delivery Team in delivering the solution to the customer\nDefine and plan project milestones, phases and different elements involved in the project along with the principal consultant\nDrive and challenge the presumptions of delivery teams on how will they successfully execute their plans\nEnsure Customer Satisfaction through quality deliverable on time\n3. Build domain expertise and contribute to knowledge repository\nEngage and interact with other BAs to share expertise and increase domain knowledge across the vertical\nWrite whitepapers/ research papers, point of views and share with the consulting community at large\nIdentify and create used cases for a different project/ account that can be brought at Wipro level for business enhancements\nConduct market research for content and development to provide latest inputs into the projects thereby ensuring customer delight\nDeliver\n\nNo.\n\nPerformance Parameter\n\nMeasure 1. Customer Engagement and Delivery Management PCSAT, utilization % achievement, no. of leads generated from the business interaction, no. of errors/ gaps in documenting customer requirements, feedback from project manager, process flow diagrams (quality and timeliness), % of deal solutioning completed within timeline, velocity generated. 2. Knowledge Management No. of whitepapers/ research papers written, no. of user stories created, % of proposal documentation completed and uploaded into knowledge repository, No of reusable components developed for proposal during quarter",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Business analysis', 'business integration', 'client engagement', 'test cases', 'Integration Testing', 'User Acceptance testing', 'Customer Engagement']",2025-06-10 15:26:09
Data Analyst - L3,Wipro,3 - 5 years,Not Disclosed,['Bengaluru'],"Role Purpose\nThe purpose of this role is to interpret data and turn into information (reports, dashboards, interactive visualizations etc) which can offer ways to improve a business, thus affecting business decisions.\nDo\n1. Managing the technical scope of the project in line with the requirements at all stages\na. Gather information from various sources (data warehouses, database, data integration and modelling) and interpret patterns and trends\nb. Develop record management process and policies\nc. Build and maintain relationships at all levels within the client base and understand their requirements.\nd. Providing sales data, proposals, data insights and account reviews to the client base\ne. Identify areas to increase efficiency and automation of processes\nf. Set up and maintain automated data processes\ng. Identify, evaluate and implement external services and tools to support data validation and cleansing.\nh. Produce and track key performance indicators\n2. Analyze the data sets and provide adequate information\na. Liaise with internal and external clients to fully understand data content\nb. Design and carry out surveys and analyze survey data as per the customer requirement\nc. Analyze and interpret complex data sets relating to customers business and prepare reports for internal and external audiences using business analytics reporting tools\nd. Create data dashboards, graphs and visualization to showcase business performance and also provide sector and competitor benchmarking\ne. Mine and analyze large datasets, draw valid inferences and present them successfully to management using a reporting tool\nf. Develop predictive models and share insights with the clients as per their requirement\n\nDeliver\n\nNo Performance Parameter Measure\n1. Analyses data sets and provide relevant information to the clientNo. Of automation done, On-Time Delivery, CSAT score, Zero customer escalation, data accuracy\n\nMandatory Skills: Tableau.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Analysis', 'Tableau', 'data warehouses', 'data integration', 'Data modelling']",2025-06-10 15:26:11
Data Analyst - L3,Wipro,4 - 8 years,Not Disclosed,['Bengaluru'],"? \n\nDo\n\n1. Managing the technical scope of the project in line with the requirements at all stages\n\na. Gather information from various sources (data warehouses, database, data integration and modelling) and interpret patterns and trends\n\nb. Develop record management process and policies\n\nc. Build and maintain relationships at all levels within the client base and understand their requirements.\n\nd. Providing sales data, proposals, data insights and account reviews to the client base\n\ne. Identify areas to increase efficiency and automation of processes\n\nf. Set up and maintain automated data processes\n\ng. Identify, evaluate and implement external services and tools to support data validation and cleansing.\n\nh. Produce and track key performance indicators\n\n2. Analyze the data sets and provide adequate information\n\na. Liaise with internal and external clients to fully understand data content\n\nb. Design and carry out surveys and analyze survey data as per the customer requirement\n\nc. Analyze and interpret complex data sets relating to customer’s business and prepare reports for internal and external audiences using business analytics reporting tools\n\nd. Create data dashboards, graphs and visualization to showcase business performance and also provide sector and competitor benchmarking\n\ne. Mine and analyze large datasets, draw valid inferences and present them successfully to management using a reporting tool\n\nf. Develop predictive models and share insights with the clients as per their requirement",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analysis', 'data mining', 'data warehousing', 'business analytics', 'data integration', 'python', 'data analytics', 'data validation', 'business analysis', 'dbms', 'dashboards', 'cleansing', 'business intelligence', 'sales', 'sql', 'analytics reporting', 'tableau', 'reporting tools']",2025-06-10 15:26:13
Business Analyst - L3,Wipro,2 - 6 years,Not Disclosed,['Hyderabad'],"Role Purpose\nThe purpose of the role is to liaison and bridging the gap between customer and Wipro delivery team to comprehend and analyze customer requirements and articulating aptly to delivery teams thereby, ensuring right solutioning to the customer.\nDo\n1. Customer requirements gathering and engagement\nInterface and coordinate with client engagement partners to understand the RFP/ RFI requirements\nDetail out scope documents, functional & non-functional requirements, features etc ensuring all stated and unstated customer needs are captured\nConstruct workflow charts and diagrams, studying system capabilities, writing specification after thorough research and analysis of customer requirements\nEngage and interact with internal team - project managers, pre-sales team, tech leads, architects to design and formulate accurate and timely response to RFP/RFIs\nUnderstand and communicate the financial and operational impact of any changes\nPeriodic cadence with customers to seek clarifications and feedback wrt solution proposed for a particular RFP/ RFI and accordingly instructing delivery team to make changes in the design\nEmpower the customers through demonstration and presentation of the proposed solution/ prototype\nMaintain relationships with customers to optimize business integration and lead generation\nEnsure ongoing reviews and feedback from customers to improve and deliver better value (services/ products) to the customers\n2. Engage with delivery team to ensure right solution is proposed to the customer\na. Periodic cadence with delivery team to:\nProvide them with customer feedback/ inputs on the proposed solution\nReview the test cases to check 100% coverage of customer requirements\nConduct root cause analysis to understand the proposed solution/ demo/ prototype before sharing it with the customer\nDeploy and facilitate new change requests to cater to customer needs and requirements\nSupport QA team with periodic testing to ensure solutions meet the needs of businesses by giving timely inputs/feedback\nConduct Integration Testing and User Acceptance demos testing to validate implemented solutions and ensure 100% success rate\nUse data modelling practices to analyze the findings and design, develop improvements and changes\nEnsure 100% utilization by studying systems capabilities and understanding business specifications\nStitch the entire response/ solution proposed to the RFP/ RFI before its presented to the customer\nb. Support Project Manager/ Delivery Team in delivering the solution to the customer\nDefine and plan project milestones, phases and different elements involved in the project along with the principal consultant\nDrive and challenge the presumptions of delivery teams on how will they successfully execute their plans\nEnsure Customer Satisfaction through quality deliverable on time\n3. Build domain expertise and contribute to knowledge repository\nEngage and interact with other BAs to share expertise and increase domain knowledge across the vertical\nWrite whitepapers/ research papers, point of views and share with the consulting community at large\nIdentify and create used cases for a different project/ account that can be brought at Wipro level for business enhancements\nConduct market research for content and development to provide latest inputs into the projects thereby ensuring customer delight",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Business Analysis', 'Root Cause Analysis', 'Change Management', 'Requirements Gathering', 'Stakeholder Management', 'Customer Engagement']",2025-06-10 15:26:16
Business Analyst - L4,Wipro,5 - 8 years,Not Disclosed,['Chennai'],"Role Purpose\nThe purpose of the role is to liaison and bridging the gap between customer and Wipro delivery team to comprehend and analyze customer requirements and articulating aptly to delivery teams thereby, ensuring right solutioning to the customer.\nDo\n1. Customer requirements gathering and engagement\nInterface and coordinate with client engagement partners to understand the RFP/ RFI requirements\nDetail out scope documents, functional & non-functional requirements, features etc ensuring all stated and unstated customer needs are captured\nConstruct workflow charts and diagrams, studying system capabilities, writing specification after thorough research and analysis of customer requirements\nEngage and interact with internal team - project managers, pre-sales team, tech leads, architects to design and formulate accurate and timely response to RFP/RFIs\nUnderstand and communicate the financial and operational impact of any changes\nPeriodic cadence with customers to seek clarifications and feedback wrt solution proposed for a particular RFP/ RFI and accordingly instructing delivery team to make changes in the design\nEmpower the customers through demonstration and presentation of the proposed solution/ prototype\nMaintain relationships with customers to optimize business integration and lead generation\nEnsure ongoing reviews and feedback from customers to improve and deliver better value (services/ products) to the customers\n2.Engage with delivery team to ensure right solution is proposed to the customer\na.Periodic cadence with delivery team to:\nProvide them with customer feedback/ inputs on the proposed solution\nReview the test cases to check 100% coverage of customer requirements\nConduct root cause analysis to understand the proposed solution/ demo/ prototype before sharing it with the customer\nDeploy and facilitate new change requests to cater to customer needs and requirements\nSupport QA team with periodic testing to ensure solutions meet the needs of businesses by giving timely inputs/feedback\nConduct Integration Testing and User Acceptance demos testing to validate implemented solutions and ensure 100% success rate\nUse data modelling practices to analyze the findings and design, develop improvements and changes\nEnsure 100% utilization by studying systems capabilities and understanding business specifications\nStitch the entire response/ solution proposed to the RFP/ RFI before its presented to the customer\nb.Support Project Manager/ Delivery Team in delivering the solution to the customer\nDefine and plan project milestones, phases and different elements involved in the project along with the principal consultant\nDrive and challenge the presumptions of delivery teams on how will they successfully execute their plans\nEnsure Customer Satisfaction through quality deliverable on time\n\n3.Build domain expertise and contribute to knowledge repository\nEngage and interact with other BAs to share expertise and increase domain knowledge across the vertical\nWrite whitepapers/ research papers, point of views and share with the consulting community at large\nIdentify and create used cases for a different project/ account that can be brought at Wipro level for business enhancements\nConduct market research for content and development to provide latest inputs into the projects thereby ensuring customer delight\n\nDeliver\n\nNo. Performance Parameter Measure\n1.Customer Engagement and Delivery ManagementPCSAT, utilization % achievement, no. of leads generated from the business interaction, no. of errors/ gaps in documenting customer requirements, feedback from project manager, process flow diagrams (quality and timeliness), % of deal solutioning completed within timeline, velocity generated.\n2.Knowledge ManagementNo. of whitepapers/ research papers written, no. of user stories created, % of proposal documentation completed and uploaded into knowledge repository, No of reusable components developed for proposal during quarter\nMandatory Skills: Change Practice SME.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Business Analysis', 'QA', 'client engagement', 'Project Management', 'products services', 'Integration Testing', 'Data Analytics']",2025-06-10 15:26:19
Data Analyst - L3,Wipro,4 - 8 years,Not Disclosed,['Bengaluru'],"About The Role  \n\nRole Purpose\n\nThe purpose of this role is to interpret data and turn into information (reports, dashboards, interactive visualizations etc) which can offer ways to improve a business, thus affecting business decisions.\n\n ? \n\nDo\n\n1. Managing the technical scope of the project in line with the requirements at all stages\n\na. Gather information from various sources (data warehouses, database, data integration and modelling) and interpret patterns and trends\n\nb. Develop record management process and policies\n\nc. Build and maintain relationships at all levels within the client base and understand their requirements.\n\nd. Providing sales data, proposals, data insights and account reviews to the client base\n\ne. Identify areas to increase efficiency and automation of processes\n\nf. Set up and maintain automated data processes\n\ng. Identify, evaluate and implement external services and tools to support data validation and cleansing.\n\nh. Produce and track key performance indicators\n\n\n\n2. Analyze the data sets and provide adequate information\n\na. Liaise with internal and external clients to fully understand data content\n\nb. Design and carry out surveys and analyze survey data as per the customer requirement\n\nc. Analyze and interpret complex data sets relating to customer’s business and prepare reports for internal and external audiences using business analytics reporting tools\n\nd. Create data dashboards, graphs and visualization to showcase business performance and also provide sector and competitor benchmarking\n\ne. Mine and analyze large datasets, draw valid inferences and present them successfully to management using a reporting tool\n\nf. Develop predictive models and share insights with the clients as per their requirement\n\n ? \n\nDeliver\nNoPerformance ParameterMeasure1.Analyses data sets and provide relevant information to the clientNo. Of automation done, On-Time Delivery, CSAT score, Zero customer escalation, data accuracy\n\n\n ? \n\n ? \nReinvent your world. We are building a modern Wipro. We are an end-to-end digital transformation partner with the boldest ambitions. To realize them, we need people inspired by reinvention. Of yourself, your career, and your skills. We want to see the constant evolution of our business and our industry. It has always been in our DNA - as the world around us changes, so do we. Join a business powered by purpose and a place that empowers you to design your own reinvention. Come to Wipro. Realize your ambitions. Applications from people with disabilities are explicitly welcome.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analysis', 'data mining', 'data warehousing', 'dashboards', 'data integration', 'python', 'data validation', 'pivot table', 'business analytics', 'dbms', 'vlookup', 'cleansing', 'sales', 'sql', 'analytics reporting', 'tableau', 'vba', 'advanced excel', 'reporting tools', 'digital transformation']",2025-06-10 15:26:21
Business Analyst - L3,Wipro,3 - 5 years,Not Disclosed,['Hyderabad'],"Role Purpose\nThe purpose of the role is to liaison and bridging the gap between customer and Wipro delivery team to comprehend and analyze customer requirements and articulating aptly to delivery teams thereby, ensuring right solutioning to the customer.\nDo\n1. Customer requirements gathering and engagement\nInterface and coordinate with client engagement partners to understand the RFP/ RFI requirements\nDetail out scope documents, functional & non-functional requirements, features etc ensuring all stated and unstated customer needs are captured\nConstruct workflow charts and diagrams, studying system capabilities, writing specification after thorough research and analysis of customer requirements\nEngage and interact with internal team - project managers, pre-sales team, tech leads, architects to design and formulate accurate and timely response to RFP/RFIs\nUnderstand and communicate the financial and operational impact of any changes\nPeriodic cadence with customers to seek clarifications and feedback wrt solution proposed for a particular RFP/ RFI and accordingly instructing delivery team to make changes in the design\nEmpower the customers through demonstration and presentation of the proposed solution/ prototype\nMaintain relationships with customers to optimize business integration and lead generation\nEnsure ongoing reviews and feedback from customers to improve and deliver better value (services/ products) to the customers\n2.Engage with delivery team to ensure right solution is proposed to the customer\na.Periodic cadence with delivery team to:\nProvide them with customer feedback/ inputs on the proposed solution\nReview the test cases to check 100% coverage of customer requirements\nConduct root cause analysis to understand the proposed solution/ demo/ prototype before sharing it with the customer\nDeploy and facilitate new change requests to cater to customer needs and requirements\nSupport QA team with periodic testing to ensure solutions meet the needs of businesses by giving timely inputs/feedback\nConduct Integration Testing and User Acceptance demos testing to validate implemented solutions and ensure 100% success rate\nUse data modelling practices to analyze the findings and design, develop improvements and changes\nEnsure 100% utilization by studying systems capabilities and understanding business specifications\nStitch the entire response/ solution proposed to the RFP/ RFI before its presented to the customer\nb.Support Project Manager/ Delivery Team in delivering the solution to the customer\nDefine and plan project milestones, phases and different elements involved in the project along with the principal consultant\nDrive and challenge the presumptions of delivery teams on how will they successfully execute their plans\nEnsure Customer Satisfaction through quality deliverable on time\n\n3.Build domain expertise and contribute to knowledge repository\nEngage and interact with other BAs to share expertise and increase domain knowledge across the vertical\nWrite whitepapers/ research papers, point of views and share with the consulting community at large\nIdentify and create used cases for a different project/ account that can be brought at Wipro level for business enhancements\nConduct market research for content and development to provide latest inputs into the projects thereby ensuring customer delight\n\nDeliver\n\nNo. Performance Parameter Measure\n1.Customer Engagement and Delivery ManagementPCSAT, utilization % achievement, no. of leads generated from the business interaction, no. of errors/ gaps in documenting customer requirements, feedback from project manager, process flow diagrams (quality and timeliness), % of deal solutioning completed within timeline, velocity generated.\n2.Knowledge ManagementNo. of whitepapers/ research papers written, no. of user stories created, % of proposal documentation completed and uploaded into knowledge repository, No of reusable components developed for proposal during quarter\nMandatory Skills: Change Practice SME.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Business Analysis', 'QA', 'client engagement', 'Project Management', 'products services', 'Integration Testing', 'Data Analytics']",2025-06-10 15:26:24
Business Analyst - L4,Wipro,5 - 8 years,Not Disclosed,['Hyderabad'],"About The Role  \n\nRole Purpose\n\nThe purpose of the role is to liaison and bridging the gap between customer and Wipro delivery team to comprehend and analyze customer requirements and articulating aptly to delivery teams thereby, ensuring right solutioning to the customer.\n\n ? \n\nDo\n\n1. Customer requirements gathering and engagement\nInterface and coordinate with client engagement partners to understand the RFP/ RFI requirements\nDetail out scope documents, functional & non-functional requirements, features etc ensuring all stated and unstated customer needs are captured\nConstruct workflow charts and diagrams, studying system capabilities, writing specification after thorough research and analysis of customer requirements\nEngage and interact with internal team - project managers, pre-sales team, tech leads, architects to design and formulate accurate and timely response to RFP/RFIs\nUnderstand and communicate the financial and operational impact of any changes\nPeriodic cadence with customers to seek clarifications and feedback wrt solution proposed for a particular RFP/ RFI and accordingly instructing delivery team to make changes in the design\nEmpower the customers through demonstration and presentation of the proposed solution/ prototype\nMaintain relationships with customers to optimize business integration and lead generation\nEnsure ongoing reviews and feedback from customers to improve and deliver better value (services/ products) to the customers\n\n\n ? \n\n2. Engage with delivery team to ensure right solution is proposed to the customer\n\na. Periodic cadence with delivery team to\nProvide them with customer feedback/ inputs on the proposed solution\nReview the test cases to check 100% coverage of customer requirements\nConduct root cause analysis to understand the proposed solution/ demo/ prototype before sharing it with the customer\nDeploy and facilitate new change requests to cater to customer needs and requirements\nSupport QA team with periodic testing to ensure solutions meet the needs of businesses by giving timely inputs/feedback\nConduct Integration Testing and User Acceptance demo??s testing to validate implemented solutions and ensure 100% success rate\nUse data modelling practices to analyze the findings and design, develop improvements and changes\nEnsure 100% utilization by studying systems capabilities and understanding business specifications\nStitch the entire response/ solution proposed to the RFP/ RFI before its presented to the customer\n\nb. Support Project Manager/ Delivery Team in delivering the solution to the customer\nDefine and plan project milestones, phases and different elements involved in the project along with the principal consultant\nDrive and challenge the presumptions of delivery teams on how will they successfully execute their plans\nEnsure Customer Satisfaction through quality deliverable on time\n\n\n ? \n\n3. Build domain expertise and contribute to knowledge repository\nEngage and interact with other BA??s to share expertise and increase domain knowledge across the vertical\nWrite whitepapers/ research papers, point of views and share with the consulting community at large\nIdentify and create used cases for a different project/ account that can be brought at Wipro level for business enhancements\nConduct market research for content and development to provide latest inputs into the projects thereby ensuring customer delight\n\n\n ? \n\nDeliver\n\n\nNo.\n\nPerformance Parameter\n\nMeasure 1. Customer Engagement and Delivery Management PCSAT, utilization % achievement, no. of leads generated from the business interaction, no. of errors/ gaps in documenting customer requirements, feedback from project manager, process flow diagrams (quality and timeliness), % of deal solutioning completed within timeline, velocity generated. 2. Knowledge Management No. of whitepapers/ research papers written, no. of user stories created, % of proposal documentation completed and uploaded into knowledge repository, No of reusable components developed for proposal during quarter\n\nMandatory\n\nSkills:\nBusiness Analyst/ Data Analyst(Maps).\n\nExperience5-8 Years.\n\nReinvent your world. We are building a modern Wipro. We are an end-to-end digital transformation partner with the boldest ambitions. To realize them, we need people inspired by reinvention. Of yourself, your career, and your skills. We want to see the constant evolution of our business and our industry. It has always been in our DNA - as the world around us changes, so do we. Join a business powered by purpose and a place that empowers you to design your own reinvention. Come to Wipro. Realize your ambitions. Applications from people with disabilities are explicitly welcome.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['project management', 'business analysis', 'change request', 'root cause analysis', 'rfp', 'charts', 'data analysis', 'client engagement', 'documentation', 'test cases', 'user stories', 'rfis', 'market research', 'prototype', 'rfi', 'integration testing', 'flow diagrams', 'digital transformation']",2025-06-10 15:26:27
Data Engineer,PwC India,4 - 8 years,Not Disclosed,['Bengaluru'],"If Interested please fill the below application link : https://forms.office.com/r/Zc8wDfEGEH\n\n\nResponsibilities:\nDeliver projects integrating data flows within and across technology systems.\nLead data modeling sessions with end user groups, project stakeholders, and technology teams to produce logical and physical data models.",,,,"['Pyspark', 'Aws Cloud', 'Azure Cloud', 'Python']",2025-06-10 15:26:29
Business Analyst - L3,Wipro,3 - 5 years,Not Disclosed,['Chennai'],"Role Purpose\nThe purpose of the role is to liaison and bridging the gap between customer and Wipro delivery team to comprehend and analyze customer requirements and articulating aptly to delivery teams thereby, ensuring right solutioning to the customer.\n\n\n\nDo\n1. Customer requirements gathering and engagement\nInterface and coordinate with client engagement partners to understand the RFP/ RFI requirements\nDetail out scope documents, functional & non-functional requirements, features etc ensuring all stated and unstated customer needs are captured\nConstruct workflow charts and diagrams, studying system capabilities, writing specification after thorough research and analysis of customer requirements\nEngage and interact with internal team - project managers, pre-sales team, tech leads, architects to design and formulate accurate and timely response to RFP/RFIs\nUnderstand and communicate the financial and operational impact of any changes\nPeriodic cadence with customers to seek clarifications and feedback wrt solution proposed for a particular RFP/ RFI and accordingly instructing delivery team to make changes in the design\nEmpower the customers through demonstration and presentation of the proposed solution/ prototype\nMaintain relationships with customers to optimize business integration and lead generation\nEnsure ongoing reviews and feedback from customers to improve and deliver better value (services/ products) to the customers\n\n\n\n2.Engage with delivery team to ensure right solution is proposed to the customer\na.Periodic cadence with delivery team to:\nProvide them with customer feedback/ inputs on the proposed solution\nReview the test cases to check 100% coverage of customer requirements\nConduct root cause analysis to understand the proposed solution/ demo/ prototype before sharing it with the customer\nDeploy and facilitate new change requests to cater to customer needs and requirements\nSupport QA team with periodic testing to ensure solutions meet the needs of businesses by giving timely inputs/feedback\nConduct Integration Testing and User Acceptance demos testing to validate implemented solutions and ensure 100% success rate\nUse data modelling practices to analyze the findings and design, develop improvements and changes\nEnsure 100% utilization by studying systems capabilities and understanding business specifications\nStitch the entire response/ solution proposed to the RFP/ RFI before its presented to the customer\nb.Support Project Manager/ Delivery Team in delivering the solution to the customer\nDefine and plan project milestones, phases and different elements involved in the project along with the principal consultant\nDrive and challenge the presumptions of delivery teams on how will they successfully execute their plans\nEnsure Customer Satisfaction through quality deliverable on time\n\n\n\n3.Build domain expertise and contribute to knowledge repository\nEngage and interact with other BAs to share expertise and increase domain knowledge across the vertical\nWrite whitepapers/ research papers, point of views and share with the consulting community at large\nIdentify and create used cases for a different project/ account that can be brought at Wipro level for business enhancements\nConduct market research for content and development to provide latest inputs into the projects thereby ensuring customer delight\n\n\n\nDeliver\n\nNo.Performance ParameterMeasure\n1.Customer Engagement and Delivery Management\nPCSAT, utilization % achievement, no. of leads generated from the business interaction, no. of errors/ gaps in documenting customer requirements, feedback from project manager, process flow diagrams (quality and timeliness), % of deal solutioning completed within timeline, velocity generated.\n2.Knowledge ManagementNo. of whitepapers/ research papers written, no. of user stories created, % of proposal documentation completed and uploaded into knowledge repository, No of reusable components developed for proposal during quarter\nMandatory Skills: Mainframe Application Rewrite to Cloud.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Mainframe', 'documentation', 'process flow diagrams', 'Business Analysis', 'Delivery Management']",2025-06-10 15:26:32
Business Analyst - L3,Wipro,3 - 5 years,Not Disclosed,['Hyderabad'],"Wipro Limited (NYSEWIT, BSE507685, NSEWIPRO) is a leading technology services and consulting company focused on building innovative solutions that address clients’ most complex digital transformation needs. Leveraging our holistic portfolio of capabilities in consulting, design, engineering, and operations, we help clients realize their boldest ambitions and build future-ready, sustainable businesses. With over 230,000 employees and business partners across 65 countries, we deliver on the promise of helping our customers, colleagues, and communities thrive in an ever-changing world. For additional information, visit us at www.wipro.com.\n About The Role  _x000D_\n\nRole Purpose\n\nThe purpose of the role is to liaison and bridging the gap between customer and Wipro delivery team to comprehend and analyze customer requirements and articulating aptly to delivery teams thereby, ensuring right solutioning to the customer.\n\n ? _x000D_\n\nDo\n\n1. Customer requirements gathering and engagement\nInterface and coordinate with client engagement partners to understand the RFP/ RFI requirements\nDetail out scope documents, functional & non-functional requirements, features etc ensuring all stated and unstated customer needs are captured\nConstruct workflow charts and diagrams, studying system capabilities, writing specification after thorough research and analysis of customer requirements\nEngage and interact with internal team - project managers, pre-sales team, tech leads, architects to design and formulate accurate and timely response to RFP/RFIs\nUnderstand and communicate the financial and operational impact of any changes\nPeriodic cadence with customers to seek clarifications and feedback wrt solution proposed for a particular RFP/ RFI and accordingly instructing delivery team to make changes in the design\nEmpower the customers through demonstration and presentation of the proposed solution/ prototype\nMaintain relationships with customers to optimize business integration and lead generation\nEnsure ongoing reviews and feedback from customers to improve and deliver better value (services/ products) to the customers\n\n\n ? _x000D_\n\n2. Engage with delivery team to ensure right solution is proposed to the customer\n\na. Periodic cadence with delivery team to\nProvide them with customer feedback/ inputs on the proposed solution\nReview the test cases to check 100% coverage of customer requirements\nConduct root cause analysis to understand the proposed solution/ demo/ prototype before sharing it with the customer\nDeploy and facilitate new change requests to cater to customer needs and requirements\nSupport QA team with periodic testing to ensure solutions meet the needs of businesses by giving timely inputs/feedback\nConduct Integration Testing and User Acceptance demo’s testing to validate implemented solutions and ensure 100% success rate\nUse data modelling practices to analyze the findings and design, develop improvements and changes\nEnsure 100% utilization by studying systems capabilities and understanding business specifications\nStitch the entire response/ solution proposed to the RFP/ RFI before its presented to the customer\n\nb. Support Project Manager/ Delivery Team in delivering the solution to the customer\nDefine and plan project milestones, phases and different elements involved in the project along with the principal consultant\nDrive and challenge the presumptions of delivery teams on how will they successfully execute their plans\nEnsure Customer Satisfaction through quality deliverable on time\n\n\n ? _x000D_\n\n3. Build domain expertise and contribute to knowledge repository\nEngage and interact with other BA’s to share expertise and increase domain knowledge across the vertical\nWrite whitepapers/ research papers, point of views and share with the consulting community at large\nIdentify and create used cases for a different project/ account that can be brought at Wipro level for business enhancements\nConduct market research for content and development to provide latest inputs into the projects thereby ensuring customer delight\n\n\n ? _x000D_\n\nDeliver\n\n\nNo.\n\nPerformance Parameter\n\nMeasure 1. Customer Engagement and Delivery Management PCSAT, utilization % achievement, no. of leads generated from the business interaction, no. of errors/ gaps in documenting customer requirements, feedback from project manager, process flow diagrams (quality and timeliness), % of deal solutioning completed within timeline, velocity generated. 2. Knowledge Management No. of whitepapers/ research papers written, no. of user stories created, % of proposal documentation completed and uploaded into knowledge repository, No of reusable components developed for proposal during quarter\n\nMandatory\n\nSkills:\nBusiness Analysis_x000D_.\n\nExperience3-5 Years_x000D_.\n\nReinvent your world. We are building a modern Wipro. We are an end-to-end digital transformation partner with the boldest ambitions. To realize them, we need people inspired by reinvention. Of yourself, your career, and your skills. We want to see the constant evolution of our business and our industry. It has always been in our DNA - as the world around us changes, so do we. Join a business powered by purpose and a place that empowers you to design your own reinvention. Come to Wipro. Realize your ambitions. Applications from people with disabilities are explicitly welcome.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['project management', 'software testing', 'business analysis', 'change request', 'root cause analysis', 'client engagement', 'documentation', 'test cases', 'user stories', 'rfis', 'market research', 'prototype', 'rfi', 'integration testing', 'flow diagrams', 'rfp', 'digital transformation']",2025-06-10 15:26:34
Snowflake Data Engineer,Prudent Globaltech Solutions,7 - 12 years,16-27.5 Lacs P.A.,['Hyderabad( Madhapur )'],"Job Description Data Engineer\nWe are seeking a highly skilled Data Engineer with extensive experience in Snowflake, Data Build Tool (dbt), Snaplogic, SQL Server, PostgreSQL, Azure Data Factory, and other ETL tools. The ideal candidate will have a strong ability to optimize SQL queries and a good working knowledge of Python. A positive attitude and excellent teamwork skills are essential.\n\nRole & responsibilities\nData Pipeline Development: Design, develop, and maintain scalable data pipelines using Snowflake, DBT, Snaplogic, and ETL tools.",,,,"['Snowflake', 'Azure Data Factory', 'ADF', 'Data Engineer', 'ETL', 'SQL']",2025-06-10 15:26:36
Starburst Data Engineer/ Architect,Wipro,5 - 8 years,Not Disclosed,['Bengaluru'],"Starburst Data Engineer/Architect \nExpertise in Starburst and policy management like Ranger or equivalent.\nIn-depth knowledge of data modelling principles and techniques, including relational and dimensional.\nExcellent problem solving skills and the ability to troubleshoot and debug complex data related issues.\nStrong awareness of data tools and platforms like: Starburst, Snowflakes, Databricks and programming languages like SQL.\nIn-depth knowledge of data management principles, methodologies, and best practices with excellent analytical, problem-solving and decision making skills.\nDevelop, implement and maintain database systems using SQL.\nWrite complex SQL queries for integration with applications.\nDevelop and maintain data models (Conceptual, physical and logical) to meet organisational needs.\n\n\n ? \n\nDo\n\n1. Managing the technical scope of the project in line with the requirements at all stages\n\na. Gather information from various sources (data warehouses, database, data integration and modelling) and interpret patterns and trends\n\nb. Develop record management process and policies\n\nc. Build and maintain relationships at all levels within the client base and understand their requirements.\n\nd. Providing sales data, proposals, data insights and account reviews to the client base\n\ne. Identify areas to increase efficiency and automation of processes\n\nf. Set up and maintain automated data processes\n\ng. Identify, evaluate and implement external services and tools to support data validation and cleansing.\n\nh. Produce and track key performance indicators\n\n\n\n2. Analyze the data sets and provide adequate information\n\na. Liaise with internal and external clients to fully understand data content\n\nb. Design and carry out surveys and analyze survey data as per the customer requirement\n\nc. Analyze and interpret complex data sets relating to customer??s business and prepare reports for internal and external audiences using business analytics reporting tools\n\nd. Create data dashboards, graphs and visualization to showcase business performance and also provide sector and competitor benchmarking\n\ne. Mine and analyze large datasets, draw valid inferences and present them successfully to management using a reporting tool\n\nf. Develop predictive models and share insights with the clients as per their requirement\n\n ? \n\nDeliver\nNoPerformance ParameterMeasure1.Analyses data sets and provide relevant information to the clientNo. Of automation done, On-Time Delivery, CSAT score, Zero customer escalation, data accuracy\n\n\n ? \n\n ? \nMandatory\n\nSkills:\nStartburst.\n\nExperience5-8 Years.\n\nReinvent your world. We are building a modern Wipro. We are an end-to-end digital transformation partner with the boldest ambitions. To realize them, we need people inspired by reinvention. Of yourself, your career, and your skills. We want to see the constant evolution of our business and our industry. It has always been in our DNA - as the world around us changes, so do we. Join a business powered by purpose and a place that empowers you to design your own reinvention. Come to Wipro. Realize your ambitions. Applications from people with disabilities are explicitly welcome.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data management', 'sql', 'data bricks', 'data modeling', 'policy management', 'hive', 'snowflake', 'python', 'data mining', 'data warehousing', 'power bi', 'dbms', 'data architecture', 'sql server', 'plsql', 'tableau', 'unix shell scripting', 'spark', 'hadoop', 'etl', 'ssis', 'data integration', 'informatica']",2025-06-10 15:26:38
Analyst,Wipro,2 - 6 years,Not Disclosed,['Gurugram'],"About The Role :\n\n\n\nRole Purpose\n\nThe purpose of the role is to provide effective technical support to the process and actively resolve client issues directly or through timely escalation to meet process SLAs.\n\n\n\n\n\n\n\n\nPrepare and present fund level monthly valuations of real estate investments and debtsecurities\nResponsible for the preparation and reviewing of Quarterly Reporting, Net Debt Analysis, FXand MTM Impact analysis as well as Board Meeting Material\nReconciliation and Variance Analysis:Close coordination with onshore and offshore teams toidentify and reconcile the reporting data and provide regular variance analysis, withexplanations, to global stakeholders\nPrepare and review materials for funds' regulatory reporting\nOversee co-ordination and submission of quarterly US Securities & Exchange Commission(""SEC"") Disclosures for all International entities\nAssist with finance operations including Limited Liability Partnership administration andexecution of centralised finance processes\nContinuously seek to create operational efficiencies and reporting enhancements\nAd-hoc projects in support of the Firms businesses/new initiatives\n\n\n\n\n\n\n\n\nMasters degree- Major/Minor in Finance, Banking or Mathematics is a plus\nProficient clarity on basics of Financial statements\nPrevious background within a valuation or reporting role a plus\nCFA designation and knowledge on derivatives is a plus\nHighly proficient in Advance Excel & Powerpoint skills; experience with Anaplan or Tableau is a plus\nStrong analytical and quantitative skills with a detail orientation\nHighly organized and able to prioritise deliverables and meet demanding deadlines in a fast paced environment\nExcellent interpersonal and communication skills, both written and verbal\nHighly self motivated with ability to work independently and work effectively in a team\nCollaborative and able to build strong relationships with a broad range of stakeholders\nStrong initiative, energy and confidence completing assignments with limited supervision\n\n\n\n\n\n\n\n\n\nBuild capability to ensure operational excellence and maintain superior customer service levels of the existing account/client\nUndertake product trainings to stay current with product features, changes and updates\nEnroll in product specific and any other trainings per client requirements/recommendations\nPartner with team leaders to brainstorm and identify training themes and learning issues to better serve the client\nUpdate job knowledge by participating in self learning opportunities and maintaining personal networks\n\n\n\n\n\n\n\n\nDeliver\nNoPerformance ParameterMeasure1ProcessNo. of cases resolved per day, compliance to process and quality standards, meeting process level SLAs, Pulse score, Customer feedback2Self- ManagementProductivity, efficiency, absenteeism, Training Hours, No of technical training completed",Industry Type: IT Services & Consulting,Department: Finance & Accounting,"Employment Type: Full Time, Permanent","['anaplan', 'tableau', 'advanced excel', 'quantitative', 'financial statements', 'mathematics', 'customer service', 'variance analysis', 'regulatory reporting', 'derivatives', 'process compliance', 'technical support', 'valuation', 'power point presentation', 'operational excellence', 'cfa']",2025-06-10 15:26:40
Analyst,Wipro,3 - 6 years,Not Disclosed,['Mumbai'],"About The Role  \n\nAreaMortgage Application Processing\n\nRoleAnalyst\n\nBandB1\n\nRequired Experience3-6 Years of overall experience, 1 year of Mortgage UW experience\n\nMust haveWorked in a UK Mortgage process\n\nGood to haveUK Underwriting experience/ US mortgage underwriting / loan processor\n\nEssential Hiring\n\nSkills:\n\n\nGood understanding of the UK mortgage market with experience of handling mortgage applications\n\nExperience in the validation of customer data and documentation with history of correctly\n\nidentifying AML, CDD and fraud concerns\n\nExperience of working within a high paced environment, managing high volumes with prioritisation\n\nskills ensuring KPIs and SLAs are adhered to\n\nAbility to work collaboratively with the onshore teams through effective communication skills\n\nStrong attention to detail, utilising your knowledge and support tools to progress mortgage\n\napplications correctly\nMandatory\n\nSkills:\nMortgage( DM).\n\nExperience1-3 Years.\n\nReinvent your world. We are building a modern Wipro. We are an end-to-end digital transformation partner with the boldest ambitions. To realize them, we need people inspired by reinvention. Of yourself, your career, and your skills. We want to see the constant evolution of our business and our industry. It has always been in our DNA - as the world around us changes, so do we. Join a business powered by purpose and a place that empowers you to design your own reinvention. Come to Wipro. Realize your ambitions. Applications from people with disabilities are explicitly welcome.",Industry Type: IT Services & Consulting,"Department: BFSI, Investments & Trading","Employment Type: Full Time, Permanent","['aml', 'mortgage', 'underwriting', 'itsm', 'digital transformation', 'project management', 'operations management', 'team management', 'service catalog', 'problem management', 'change management', 'servicenow', 'process management', 'incident management', 'team leading', 'itil', 'sla management']",2025-06-10 15:26:42
Data Analyst,Wipro,2 - 6 years,Not Disclosed,['Pune'],"About The Role :\n\nRole Purpose\n\nThe purpose of the role is to provide effective technical support to the process and actively resolve client issues directly or through timely escalation to meet process SLAs.\n\n\n\nDo\nSupport process by managing transactions as per required quality standards\nFielding all incoming help requests from clients via telephone and/or emails in a courteous manner\nDocument all pertinent end user identification information, including name, department, contact information and nature of problem or issue\nUpdate own availability in the RAVE system to ensure productivity of the process\nRecord, track, and document all queries received, problem-solving steps taken and total successful and unsuccessful resolutions\nFollow standard processes and procedures to resolve all client queries\nResolve client queries as per the SLAs defined in the contract\nAccess and maintain internal knowledge bases, resources and frequently asked questions to aid in and provide effective problem resolution to clients\nIdentify and learn appropriate product details to facilitate better client interaction and troubleshooting\nDocument and analyze call logs to spot most occurring trends to prevent future problems\nMaintain and update self-help documents for customers to speed up resolution time\nIdentify red flags and escalate serious client issues to Team leader in cases of untimely resolution\nEnsure all product information and disclosures are given to clients before and after the call/email requests\nAvoids legal challenges by complying with service agreements\nDeliver excellent customer service through effective diagnosis and troubleshooting of client queries\nProvide product support and resolution to clients by performing a question diagnosis while guiding users through step-by-step solutions\nAssist clients with navigating around product menus and facilitate better understanding of product features\nTroubleshoot all client queries in a user-friendly, courteous and professional manner\nMaintain logs and records of all customer queries as per the standard procedures and guidelines\nAccurately process and record all incoming call and email using the designated tracking software\nOffer alternative solutions to clients (where appropriate) with the objective of retaining customers and clients business\nOrganize ideas and effectively communicate oral messages appropriate to listeners and situations\nFollow up and make scheduled call backs to customers to record feedback and ensure compliance to contract /SLAs\nBuild capability to ensure operational excellence and maintain superior customer service levels of the existing account/client\nUndertake product trainings to stay current with product features, changes and updates\nEnroll in product specific and any other trainings per client requirements/recommendations\nPartner with team leaders to brainstorm and identify training themes and learning issues to better serve the client\nUpdate job knowledge by participating in self learning opportunities and maintaining personal networks\nDeliver\nNoPerformance ParameterMeasure\n1ProcessNo. of cases resolved per day, compliance to process and quality standards, meeting process level SLAs, Pulse score, Customer feedback\n2Self- ManagementProductivity, efficiency, absenteeism, Training Hours, No of technical training completed",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Analysis', 'product support', 'material management', 'customer service', 'client interaction', 'troubleshooting']",2025-06-10 15:26:45
Analyst,Wipro,2 - 6 years,Not Disclosed,['Gurugram'],"About The Role :\n\nJob Location:Gurgaon ( Work from Office)\nTime:6:00 PM-3:30 AM\nJOB DESCRIPTION:\nBuild customizations using extensions and scripts to meet business needs.\nActively identify opportunities to leverage Coupa functionality to improve efficiency.\nTroubleshoot and debug production issues that users may have which could be data, process, Coupa configuration or integration related.\nSupport the Procure-to-Pay department during the month-end/quarter-end/Year-end close process.\nEnsure that change control procedures are implemented and used for all production deployments.\nUser administration including assignment of roles, & user access levels.\nUser acceptance testing and system integration testing supporting instance upgrades and releases.\nPartner with the vendor, IT and the Reporting team to achieve organizational management information and business intelligence objectives.\n\n\n\n\n\n\n\nDo\n\n\n\n\n\n\nSupport process by managing transactions as per required quality standards\nFielding all incoming help requests from clients via telephone and/or emails in a courteous manner\nDocument all pertinent end user identification information, including name, department, contact information and nature of problem or issue\nUpdate own availability in the RAVE system to ensure productivity of the process\nRecord, track, and document all queries received, problem-solving steps taken and total successful and unsuccessful resolutions\nFollow standard processes and procedures to resolve all client queries\nResolve client queries as per the SLAs defined in the contract\nAccess and maintain internal knowledge bases, resources and frequently asked questions to aid in and provide effective problem resolution to clients\nIdentify and learn appropriate product details to facilitate better client interaction and troubleshooting\nDocument and analyze call logs to spot most occurring trends to prevent future problems\nMaintain and update self-help documents for customers to speed up resolution time\nIdentify red flags and escalate serious client issues to Team leader in cases of untimely resolution\nEnsure all product information and disclosures are given to clients before and after the call/email requests\nAvoids legal challenges by complying with service agreements\n\n\n\n\n\n\n\n\n\nDeliver excellent customer service through effective diagnosis and troubleshooting of client queries\nProvide product support and resolution to clients by performing a question diagnosis while guiding users through step-by-step solutions\nAssist clients with navigating around product menus and facilitate better understanding of product features\nTroubleshoot all client queries in a user-friendly, courteous and professional manner\nMaintain logs and records of all customer queries as per the standard procedures and guidelines\nAccurately process and record all incoming call and email using the designated tracking software\nOffer alternative solutions to clients (where appropriate) with the objective of retaining customers and clients business\nOrganize ideas and effectively communicate oral messages appropriate to listeners and situations\nFollow up and make scheduled call backs to customers to record feedback and ensure compliance to contract /SLAs\n\n\n\n\n\n\n\n\n\nBuild capability to ensure operational excellence and maintain superior customer service levels of the existing account/client\nUndertake product trainings to stay current with product features, changes and updates\nEnroll in product specific and any other trainings per client requirements/recommendations\nPartner with team leaders to brainstorm and identify training themes and learning issues to better serve the client\nUpdate job knowledge by participating in self learning opportunities and maintaining personal networks\n\n\n\n\n\n\n\n\nDeliver\nNoPerformance ParameterMeasure1ProcessNo. of cases resolved per day, compliance to process and quality standards, meeting process level SLAs, Pulse score, Customer feedback2Self- ManagementProductivity, efficiency, absenteeism, Training Hours, No of technical training completed",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['customer service', 'business intelligence', 'process compliance', 'user administration', 'user acceptance testing', 'operations management', 'project management', 'data analysis', 'team management', 'vendor', 'client interaction', 'troubleshooting', 'operational excellence']",2025-06-10 15:26:47
Sleep Company is hiring Data Analyst,the Sleep Company,2 - 7 years,4-8 Lacs P.A.,['Mumbai (All Areas)'],"About Us: As India's fastest-growing D2C brand, we are at the forefront of innovation and transformation in the market. Were a well-funded, rapidly growing (we have recently launched our 100th store), omnichannel D2C brand with a passionate and innovative team.\n\nJob Summary: We are seeking a Senior Data Analyst to join our team to drive data-driven decision-making across the org. You will work closely across functions including growth, product, and ops to analyze data, generate reports and insights, and provide actionable recommendations\n\nKey Responsibilities:\nData Analysis:\nPerform in-depth analysis using statistical techniques like impact, pre-post, funnel analysis to identify insights and optimize to drive growth\nEvaluate product and feature performance, providing insights to inform product development and enhancements\nAnalyze and understand customer behavior and segment customers to tailor marketing and product strategies based on their needs and preferences\nReporting:\nDevelop and maintain comprehensive dashboards and reports that provide visibility into key metrics and self-serve reports to various teams\nInsights Generation:\nTranslate complex data into clear and actionable insights that drive business decisions and strategic initiatives\nCollaboration:\nWork closely with cross-functional teams, including growth, sales, and product, to understand their data needs and provide analytical support\nOwn and build the analytics roadmap from vendor and tool selection to dashboards and long-term projects\nData Quality:\nEnsure data accuracy and integrity by performing regular data validation and cleansing activities\nIdentify opportunities to streamline data processes and improve efficiency through the implementation of automation tools and techniques\nCore Analytics:\nWork end-to-end on projects like buyer attribution, campaign scoring model, optimal pricing strategy, lead funnel optimization, etc., using a data-driven approach\nCollaborate with the growth team to optimize ROAS by designing incrementality tests and campaign optimization and perform RCA\n\nSkills Required:\nMinimum 2-3 years of experience in the analytics domain.\nProficiency in data analysis tools and languages, such as SQL and Excel.\nStrong business acumen with the ability to connect data insights to business goals.\nStrong understanding and experience of statistical methods and advanced analytical techniques.\nSound understanding of data architecture and its implementation.\nExperience with data visualization tools, such as Tableau, Power BI, or Looker.\nSelf-motivated, proactive, and highly accountable.\nStrong communication skills, with the ability to present complex data insights to non-technical stakeholders",Industry Type: Furniture & Furnishing,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['SQL', 'Data Analysis', 'Data Visualization', 'Tableau', 'Data Analytics']",2025-06-10 15:26:50
Business Analyst-ADM,IBM,1 - 4 years,Not Disclosed,['Bengaluru'],The ability to be a team player\nThe ability and skill to train other people in procedural and technical topics\nStrong communication and collaboration skills\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nExperience in EE&U asset management\nLiaison between business and tech team for feature upgrade and implementation\n\n\nPreferred technical and professional experience\nGood communication skills\nMaintain documentation,Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['asset management', 'documentation', 'business analysis', 'sales', 'asset', 'vendor management', 'business loan', 'project management', 'team management', 'sme sales', 'sme', 'working capital', 'term loan', 'business banking', 'mortgage', 'relationship management', 'lap', 'home loans', 'finance']",2025-06-10 15:26:52
Analyst,Charting Now,0 - 4 years,Not Disclosed,['Hyderabad'],"CHARTING NOW VISUAL DATA SOLUTIONS PRIVATE LIMITED is looking for Analyst to join our dynamic team and embark on a rewarding career journey\nManaging master data, including creation, updates, and deletion.\nManaging users and user roles.\nProvide quality assurance of imported data, working with quality assurance analysts if necessary.\nCommissioning and decommissioning of data sets.\nProcessing confidential data and information according to guidelines.\nHelping develop reports and analysis.\nManaging and designing the reporting environment, including data sources, security, and metadata.\nSupporting the data warehouse in identifying and revising reporting requirements.\nSupporting initiatives for data integrity and normalization.\nAssessing tests and implementing new or upgraded software and assisting with strategic decisions on new systems.\nGenerating reports from single or multiple systems.\nTroubleshooting the reporting database environment and reports.\nEvaluating changes and updates to source production systems.\nTraining end-users on new reports and dashboards.\nProviding technical expertise in data storage structures, data mining, and data cleansing.\n\n\nGood proficiency in MS office suits, viz. Excel PowerPoint\nGood command over written and verbal communication\nWilling to work in flexible shifts",Industry Type: Analytics / KPO / Research,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['financial analysis', 'analysts', 'risk management', 'metadata', 'data analysis', 'mortgage advisors', 'report generation', 'data mining', 'administration', 'data integrity', 'dashboards', 'master data management', 'data cleansing', 'excel', 'troubleshooting', 'reporting', 'finance']",2025-06-10 15:26:55
Data Analyst,Abate As,0 - 2 years,Not Disclosed,['Perinthalmanna'],"The Data Analyst will play a key role in transforming raw data into actionable insights to support strategic decision-making and enhance operational efficiency. The ideal candidate is a detail-oriented professional with strong analytical skills, proficiency in data tools, and the ability to communicate findings effectively to diverse stakeholders.\n\nKey Responsibilities\n\nCollect, analyze, and interpret data from multiple sources to provide actionable insights.\nDeliver regular and ad-hoc reports, dashboards, and analytical solutions to stakeholders.\nDevelop high-quality, user-friendly reports and dashboards with self-service capabilities.\nWrite SQL queries to manipulate and extract insights from large datasets.\nCollaborate with teams to ensure data quality and improve data collection and reporting processes.\nPresent data-driven findings and recommendations in a clear and concise manner.\nWork closely with leadership to understand business needs and provide tailored analytical solutions.\nIdentify trends, patterns, and opportunities to support operational and strategic goals.\nExperiece\n\nProven experience in data analysis, reporting, and deriving insights in a fast-paced environment.\nStrong proficiency in manipulating and interpreting large datasets.\nExperience working with business stakeholders to translate data into actionable outcomes.\nFamiliarity with data visualization and reporting tools\nSkills\nDemonstrated ability to deliver high-quality analytical reports and insights.\nCommitment to maintaining data accuracy and improving data processes.\nStrong organizational skills with the ability to manage multiple tasks and deadlines.\nInterpersonal skills to work effectively with cross-functional teams and stakeholders.\nExperience with Zoho Analytics or similar tools is highly desirable but not mandatory",Industry Type: Medical Services / Hospital,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data analysis', 'Analytical', 'Data collection', 'Healthcare', 'Data quality', 'Data Analyst', 'data visualization', 'Operations', 'Analytics', 'Reporting tools']",2025-06-10 15:26:57
Business Analyst - L4,Wipro,1 - 4 years,Not Disclosed,['Hyderabad'],"? \n\nDo\n\n1. Customer requirements gathering and engagement\nInterface and coordinate with client engagement partners to understand the RFP/ RFI requirements\nDetail out scope documents, functional & non-functional requirements, features etc ensuring all stated and unstated customer needs are captured\nConstruct workflow charts and diagrams, studying system capabilities, writing specification after thorough research and analysis of customer requirements\nEngage and interact with internal team - project managers, pre-sales team, tech leads, architects to design and formulate accurate and timely response to RFP/RFIs\nUnderstand and communicate the financial and operational impact of any changes\nPeriodic cadence with customers to seek clarifications and feedback wrt solution proposed for a particular RFP/ RFI and accordingly instructing delivery team to make changes in the design\nEmpower the customers through demonstration and presentation of the proposed solution/ prototype\nMaintain relationships with customers to optimize business integration and lead generation\nEnsure ongoing reviews and feedback from customers to improve and deliver better value (services/ products) to the customers",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['business analysis', 'rfis', 'prototype', 'diagramming', 'rfp', 'project management', 'charts', 'rfps', 'client engagement', 'rfqs', 'software testing', 'proposal writing', 'photoshop', 'presales', 'autocad', 'bid management', 'proposal management', 'rfi', 'brd', 'drawing', 'frd', 'agile']",2025-06-10 15:26:59
Business Analyst,.,4 - 7 years,Not Disclosed,['Pune'],"Role Overview:\n\nThis hybrid role sits within the Distribution Data Stewardship Team and combines operational and technical responsibilities to ensure data accuracy, integrity, and process optimization across sales reporting functions.\n\nKey Responsibilities:\n\nSupport sales reporting inquiries from sales staff at all levels.\nReconcile omnibus activity with sales reporting systems.\nAnalyze data flows to assess impact on commissions and reporting.\nPerform data audits and updates to ensure integrity.\nLead process optimization and automation initiatives.\nManage wholesaler commission processes, including adjustments and manual submissions.\nOversee manual data integration from intermediaries.\nExecute territory alignment changes to meet business objectives.\nContribute to team initiatives and other responsibilities as assigned.\nGrowth Opportunities:\n\nExposure to all facets of sales reporting and commission processes.\nOpportunities to develop project and relationship management skills.\nPotential to explore leadership or technical specialist roles within the firm.\nQualifications:\n\nBachelors degree in Computer Engineering or a related field.\n4–7 years of experience with Python programming and automation.\nStrong background in SQL and data analysis.\nExperience in relationship/customer management and leading teams.\nExperience working with Salesforce is a plus.\nRequired Skills:\n\nTechnical proficiency in Python and SQL.\nStrong communication skills and stakeholder engagement.\nHigh attention to data integrity and detail.\nSelf-directed with excellent time management.\nProject coordination and documentation skills.\nProficiency in MS Office, especially Excel.",Industry Type: Investment Banking / Venture Capital / Private Equity,"Department: BFSI, Investments & Trading","Employment Type: Full Time, Permanent","['Advanced Python', 'Brd', 'FRD', 'Complex Queries', 'Advance Sql', 'Python Scripting', 'Python', 'SQL']",2025-06-10 15:27:02
Data Engineer,Capgemini,3 - 5 years,Not Disclosed,['Pune'],"Capgemini Invent\n\nCapgemini Invent is the digital innovation, consulting and transformation brand of the Capgemini Group, a global business line that combines market leading expertise in strategy, technology, data science and creative design, to help CxOs envision and build whats next for their businesses.\n\nYour Role\nHas data pipeline implementation experience with any of these cloud providers - AWS, Azure, GCP.\nExperience with cloud storage, cloud database, cloud data warehousing and Data Lake solutions like Snowflake, Big query, AWS Redshift, ADLS, S3.\nHas good knowledge of cloud compute services and load balancing.\nHas good knowledge of cloud identity management, authentication and authorization.\nProficiency in using cloud utility functions such as AWS lambda, AWS step functions, Cloud Run, Cloud functions, Azure functions.\nExperience in using cloud data integration services for structured, semi structured and unstructured data such as Azure Databricks, Azure Data Factory, Azure Synapse Analytics, AWS Glue, AWS EMR, Dataflow, Dataproc.\nYour Profile\nGood knowledge of Infra capacity sizing, costing of cloud services to drive optimized solution architecture, leading to optimal infra investment vs performance and scaling.\nAble to contribute to making architectural choices using various cloud services and solution methodologies.\nExpertise in programming using python.\nVery good knowledge of cloud Dev-ops practices such as infrastructure as code, CI/CD components, and automated deployments on cloud.\nMust understand networking, security, design principles and best practices in cloud.\nWhat you will love about working here\nWe recognize the significance of flexible work arrangements to provide support. Be it remote work, or flexible work hours, you will get an environment to maintain healthy work life balance.\nAt the heart of our mission is your career growth. Our array of career growth programs and diverse professions are crafted to support you in exploring a world of opportunities.\nEquip yourself with valuable certifications in the latest technologies such as Generative AI.\nAbout Capgemini\n\nCapgemini is a global business and technology transformation partner, helping organizations to accelerate their dual transition to a digital and sustainable world, while creating tangible impact for enterprises and society. It is a responsible and diverse group of 340,000 team members in more than 50 countries. With its strong over 55-year heritage, Capgemini is trusted by its clients to unlock the value of technology to address the entire breadth of their business needs. It delivers end-to-end services and solutions leveraging strengths from strategy and design to engineering, all fueled by its market leading capabilities in AI, cloud and data, combined with its deep industry expertise and partner ecosystem. The Group reported 2023 global revenues of 22.5 billion.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'networking', 'ops', 'design principles', 'aws', 'azure databricks', 'snowflake', 'azure data lake', 'glue', 'amazon redshift', 'azure synapse', 'microsoft azure', 'data warehousing', 'azure data factory', 'emr', 'aws lambda', 'data engineering', 'dataproc', 'aws glue', 'gcp', 'bigquery', 'data flow']",2025-06-10 15:27:04
FLM-LRR Analyst - BA,Barclays,2 - 7 years,Not Disclosed,['Noida'],"Join Barclays as a FLM-LRR Analyst - BA role, where the role holder will support wider team(Funding and Liquidity Management) with the completion of their reporting(Liquidity Risk Reporting) and control activities, assisting them with the investigation and resolution of more complex issues. At Barclays, we dont just anticipate the future - were creating it.\nTo be successful in this role, you should have:\nMust have good communication skills, both written and verbal, with the ability to work collaboratively with the Reporting teams across the different locations.\nGood presentation skills.\nVery good Microsoft Excel skills.\nUnderstanding of Balance Sheet and Finance processes.\nUnderstanding of Control and Governance frameworks.\nCA / CWA / CS / MBA / Finance / Commerce / Economics background.\nSome other highly valued skills may include:\nExperience supporting the roll out of controls frameworks.\nUnderstanding of Treasury and/or Risk systems in Barclays.\nReporting work experience in a Bank / Financial Institution.\nYou may be assessed on the key critical skills relevant for success in role, such as risk and controls, change and transformation, business acumen strategic thinking and digital and technology, as well as job-specific technical skills.\nThis role is based in our Noida office.\nPurpose of the role\nTo verify that the bank has sufficient funds to meet its short-term and long-term obligations, and the development and implementation of strategies to manage the banks liquidity position.\nAccountabilities\nDevelopment and implementation of funding and liquidity strategies to efficiently manage the bank s liquidity position within regulatory requirements and risk appetite at favourable commercial outcomes with respect to funding costs.\nAnalysis and quantification of the regulatory and behavioural liquidity risk impact of transactions undertaken by business units.\nMaintenance of strong relationships with key business units and working with the business units to manage liquidity to within constrains.\nMonitoring of key liquidity metrics and trends and advising on actions to be taken to maintain funding and liquidity levels within tolerance.\nManaging intra-group funding arrangements to ensure subsidiaries are adequately funded and managed within balance sheet, large exposure and capital constraints.\nDesign and implementation of stress testing methodologies to assess the banks liquidity resilience under various financial shocks, economic downturns, and sector-specific crises, and analysis of stress testing results and development of mitigation strategies to address potential liquidity shortfalls.\nDevelopment of new tools, models and data analysis to support and enhance the bank s funding and liquidity management capabilities.\nAnalyst Expectations\nTo perform prescribed activities in a timely manner and to a high standard consistently driving continuous improvement.\nRequires in-depth technical knowledge and experience in their assigned area of expertise\nThorough understanding of the underlying principles and concepts within the area of expertise\nThey lead and supervise a team, guiding and supporting professional development, allocating work requirements and coordinating team resources.\nIf the position has leadership responsibilities, People Leaders are expected to demonstrate a clear set of leadership behaviours to create an environment for colleagues to thrive and deliver to a consistently excellent standard. The four LEAD behaviours are: L - Listen and be authentic, E - Energise and inspire, A - Align across the enterprise, D - Develop others.\nOR for an individual contributor, they develop technical expertise in work area, acting as an advisor where appropriate.\nWill have an impact on the work of related teams within the area.\nPartner with other functions and business areas.\nTakes responsibility for end results of a team s operational processing and activities.\nEscalate breaches of policies / procedure appropriately.\nTake responsibility for embedding new policies/ procedures adopted due to risk mitigation.\nAdvise and influence decision making within own area of expertise.\nTake ownership for managing risk and strengthening controls in relation to the work you own or contribute to. Deliver your work and areas of responsibility in line with relevant rules, regulation and codes of conduct.\nMaintain and continually build an understanding of how own sub-function integrates with function, alongside knowledge of the organisations products, services and processes within the function.\nDemonstrate understanding of how areas coordinate and contribute to the achievement of the objectives of the organisation sub-function.\nMake evaluative judgements based on the analysis of factual information, paying attention to detail.\nResolve problems by identifying and selecting solutions through the application of acquired technical experience and will be guided by precedents.\nGuide and persuade team members and communicate complex / sensitive information.\nAct as contact point for stakeholders outside of the immediate function, while building a network of contacts outside team and external to the organisation.",Industry Type: Financial Services,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Stress testing', 'Data analysis', 'Excel', 'Senior Analyst', 'liquidity risk', 'Banking', 'Continuous improvement', 'Operations', 'Balance Sheet', 'Monitoring']",2025-06-10 15:27:07
Business Analyst,Global Banking Organization,12 - 15 years,Not Disclosed,['Pune'],"Key Skills: Business Analyst, Financial Accounting, Account Management, Business Analysis, Accounting, Retail Banking Operations\nRoles and Responsibilities:\nGather business requirements and support design thinking for business, operations, and tech projects.\nWork in Agile teams and manage the full requirement lifecycle.\nUnderstand and improve account opening and servicing processes.\nSupport mobile journeys for account opening and management.\nCollaborate with cross-functional teams and follow HSBC's work practices.\nPlan and manage change processes to ensure smooth transitions.\nCommunicate effectively with team members and stakeholders.\nCreate clear documentation and process flow diagrams.\nGuide and support other business analysts.\nSkills Required:\n12+ years of experience in business analysis and most recent experience preferably in the banking and financial domain.\nStrong business analysis, requirements gathering and design thinking skills with a mix of business, operations and technology focused projects.\nAgile expertise, requirement life cycle management and traceability, experience in digital transformation projects in a global banks/consulting firm/ Financial industry\nStrong understanding of Customer Account opening and Servicing journeys.\nStrong domain experience in Mobile STP journeys for Account opening / Account management.\nAdapt and adhere to the HSBCs ways of working and collaborate with array of stakeholders effectively and inclusively and liaison with cross functional team for programme execution.\nExperience in Change management and Change Adoption processes - Plan and implement change intervention to enable smooth transition and embed changes and transition to business as usual, from requirements gathering, communications through to training the final user.\nEffective communication, inter-personal and negotiating skills.\nKnowledge of MS Office and business analysis tools and techniques, knowledge of JIRA and Confluence tools.\nCreate required artefacts and to expected standard (e.g. Behavior Driven Development (BDD) in user stories, end-to-end flow diagrams with touchpoints, Confluence documentation for requirements traceability).\nDemonstrate Leadership, support, coaching and development for Analysts.\nLead planning analysis activity with optimal use of resources to help define and track metrics and KPIs for the product.\nPromote the Scaled delivery approach for multimarket implementation and use of customer, product, and operational procedural insights to optimise experience and propositions.\nEducation: Bachelor's Degree in related field",Industry Type: Financial Services,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Business Analyst', 'Accounting', 'Financial Accounting', 'Account Management', 'Business Analysis', 'Retail Banking Operations']",2025-06-10 15:27:09
Business Analyst / Controller,Nokia,5 - 10 years,Not Disclosed,['Gurugram'],"Professional will oversee financial planning, budgeting, and performance analysis. The ideal candidate will have a strong background in financial management, cost control, and business strategy, with experience in the telecom or technology industry. Develop and manage the financial planning process, including budgeting and forecasting. Analyze financial data to provide insights on business performance and opportunities for improvement. Monitor and manage costs to ensure financial efficiency across business units. Identify cost-saving initiatives and implement strategies to improve profitability. Prepare financial reports, performance dashboards, and variance analyses for senior management.\n\n You have: \nUniversity or college degree in finance / accounting / economics.\n5+ years of experience preferably profile is Services Controller / OPEX Controller / Business Controller preferably in Services / Telcom industry.\nFluent written and spoken English.\nGood knowledge of common finance & accounting concepts is an advantage.\nExperience with SAP, Advanced Excel skill.\n\n It would be nice if you also had: \nCustomer service oriented.\nPrecise and Quality oriented.\nAgility of taking the initiative and proactive attitude.\nGood interpersonal skills.\n\nSupport the reporting unit SPOC to ensure the above task are performed accurately in adherence to Nokia policy and process enabling the entire market / region reporting is correctly / fairly reported.\nEnsure accuracy and timeliness of own activities.\nProvide basic analysis and reports to support decision-making to Cost Owners (Cost Centre owner, Line Manager, MU Delivery Heads, other stakeholders) and reviews calls.\nSupport SPC / SHR (standard production cost / standard hour rates) calculation.\nFixed Production Overheads controlling (gross cost net recharges based on SPC / SHR)\nEnsure actual control of target and cost control for countries / area / unit under own responsibility based on which the SPC/ SHR are calculated.\nEnd to end Cost Center controlling activities. Proactively performs preventative quality controls and takes corrective actions when errors occur.\nAssess if utilization % of resources is at required level, call for action if needed.",Industry Type: IT Services & Consulting,Department: Finance & Accounting,"Employment Type: Full Time, Permanent","['sap', 'business strategy', 'cost control', 'financial management', 'advanced excel', 'opex', 'financial analysis', 'forecasting', 'auditing', 'variance analysis', 'accounting', 'budgeting', 'fpa', 'financial reporting', 'business control', 'financial planning', 'taxation', 'finance']",2025-06-10 15:27:12
Data Engineer,IBM,2 - 4 years,Not Disclosed,['Bengaluru'],"Ingest new data from relational and non-relational source database systems into our warehouse. Connect data from various sources.\n\nIntegrate data from external sources to warehouse by building facts and dimensions based on the EPM data model requirements.\n\n\n\nAutomate data exchange and processing through serverless data pipelines.\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nExperience in data analysis and integration.\nExperience in data building and consuming fact and dimension tables.\nExperience in automating data integration through data pipelines.\nExperience with object-oriented programing languages such as Python.\nExperience with structured data processing languages such as SQL and Spark SQL.\nExperience with REST APIs and JSON\nExperience in IBM Cloud data processing services such as IBM Code Engine, IBM Event Streams (Apache Kafka).\nStrong understanding of Datawarehouse concepts and various data warehouse architectures\n\n\nPreferred technical and professional experience\nExperience with IBM Cloud architecture\nExperience with DevOps.\nKnowledge of Agile development methodologies\nExperience with building containerized applications and running them in serverless environments on the Cloud such as IBM Code Engine, Kubernetes, or Satellite.\nExperience with IBM Cognitive Enterprise Data Platform and CodeHub.\nExperience with data integration tools such as IBM DataStage or Informatica",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'data warehousing', 'sql', 'spark', 'data warehousing concepts', 'hive', 'kubernetes', 'rest', 'data analysis', 'ibm cloud', 'datastage', 'data processing', 'data engineering', 'apache', 'cloud architecture', 'cognitive', 'devops', 'ibm datastage', 'kafka', 'json', 'agile', 'hadoop', 'informatica']",2025-06-10 15:27:14
Scientist - Molecular Biology,Biocon Biologics Limited,4 - 6 years,Not Disclosed,['Bengaluru'],"Molecular Biology activities like plasmid and genomic DNA preparation, PCR amplification, restriction digestion, molecular cloning, sequence confirmation, involved in vector construction for stable and transient gene expression.\nRoutine cell bank preparation of microbial and mammalian cells.\nAnalytical skills related to protein estimation and characterization like, ELISA, SDS PAGE/Western Blot, Octet and HPLC.\nStandard cell culture activity such as vial thaw, subculturing, transfection, single cell cloning, fed batch, monoclonality assurance.\nExperience in bacterial and yeast expression systems preferred.\nRoutine documentation in electronic notebook. Preparation of SOP/GM and development reports.\nIndustrial experience of 4-6 years in cell line development.",Industry Type: Pharmaceutical & Life Sciences,Department: Research & Development,"Employment Type: Full Time, Permanent","['Transfection', 'Molecular Biology', 'Molecular Cloning', 'Cloning', 'Protein Expression', 'SDS-Page', 'Recombinant DNA Technology', 'Molecular Biology Techniques', 'DNA Extraction', 'Electrophoresis', 'PCR', 'ELISA']",2025-06-10 15:27:17
Business Analyst,Global Banking Organization,8 - 12 years,Not Disclosed,['Pune'],"Key Skills: Business Analyst, Digital Channels, Business Analysis, Jira, Digital Transformation, Agile\nRoles and Responsibilities:\nAnalyze and gather business requirements, utilizing design thinking for business, operations, and technology-focused projects.\nWork with Agile methodology and manage the full requirement lifecycle and traceability.\nSupport digital transformation projects, especially within global banks, consulting firms, or the financial industry.\nLeverage asset management, transformation skills, and knowledge of SQL for process mapping and data analysis.\nAdapt and adhere to Company's work practices while collaborating with stakeholders and cross-functional teams for program execution.\nManage change adoption and ensure smooth transitions by planning and implementing change interventions, from requirements gathering to user training.\nDemonstrate excellent communication, interpersonal, and negotiation skills.\nUtilize MS Office and business analysis tools like JIRA and Confluence for project management and documentation.\nCreate artifacts to expected standards (e.g., Behavior Driven Development (BDD) in user stories, end-to-end flow diagrams, and Confluence documentation for requirements traceability).\nLead, coach, and develop analysts in the team.\nPlan and track project analysis activity, ensuring the optimal use of resources and defining metrics and KPIs for the product.\nPromote scaled delivery approaches for multi-market implementations and optimize customer, product, and operational procedures.\nSkills Required:\n8+ years of experience in business analysis and most recent experience preferably in the banking and financial domain.\nStrong business analysis, requirements gathering and design thinking skills with a mix of business, operations and technology focused projects.\nAgile expertise, requirement life cycle management and traceability, experience in digital transformation projects in a global banks/consulting firm/ Financial industry\nAsset management, Transformation skills (BA skills, Process Mapping, Data analysis and knowledge of SQL, Agile- experience with Agile Ways of Working, JIRA, Confluence)\nAdapt and adhere to the COMPANYs ways of working and collaborate with array of stakeholders effectively and inclusively and liaison with cross functional team for programme execution.\nExperience in Change management and Change Adoption processes - Plan and implement change intervention to enable smooth transition and embed changes and transition to business as usual, from requirements gathering, communications through to training the final user.\nEffective communication, inter-personal and negotiating skills.\nKnowledge of MS Office and business analysis tools and techniques, knowledge of JIRA and Confluence tools.\nCreate required artefacts and to expected standard (e.g. Behavior Driven Development (BDD) in user stories, end-to-end flow diagrams with touchpoints, Confluence documentation for requirements traceability).\nDemonstrate Leadership, support, coaching and development for Analysts.\nLead planning analysis activity with optimal use of resources to help define and track metrics and KPIs for the product.\nPromote the Scaled delivery approach for multimarket implementation and use of customer, product, and operational procedural insights to optimise experience and propositions.\nCertified Business Analysis Professional (CBAP) If not held, you would be expected to work towards the qualification\nAgile Certifications\nDesign Thinking\nEducation: Bachelor's Degree in related field",Industry Type: Financial Services,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Business Analyst', 'Digital Transformation', 'Jira', 'Digital Channels', 'Business Analysis', 'Agile']",2025-06-10 15:27:19
Analytics Data science and IOT Engineer,Hexaware Technologies,4 - 6 years,Not Disclosed,"['Pune', 'Chennai', 'Bengaluru']","Job Description:\n\nMinimum 2 years of hands-on experience in Generative AI (GenAI), including working with various LLMs, prompt engineering, fine-tuning, Retrieval-Augmented Generation (RAG), and deploying GenAI solutions in production environments.\nAt least 3 years of experience in Python and SQL, with strong fundamentals in data manipulation and backend logic.",,,,"['Generative Ai', 'Python', 'Aws Sagemaker', 'Retrieval Augmented Generation', 'SQL']",2025-06-10 15:27:22
Big Data Engineer - Python+ PySpark + Spark,Hexaware Technologies,9 - 12 years,Not Disclosed,"['Pune', 'Chennai', 'Bengaluru']","Experience - 9 years - 12 years\nLocation - Mumbai / Chennai / Bangalore / Pune\n\nDevelop and maintain scalable data pipelines using PySpark and Spark SQL for processing large datasets efficiently.\nWrite clean, reusable, and optimized code in Python for data manipulation, analysis, and automation tasks.",,,,"['PySpark', 'Spark', 'Python', 'SQL']",2025-06-10 15:27:24
NetSuite Business Analyst,Finastra,4 - 9 years,Not Disclosed,['Bengaluru'],"What will you contribute?\nReporting to the Finance Transformation IT Product Owner, the NetSuite Business Analyst will help guide the Finastra business to improve business processes and efficiency within our NetSuite platform.\nYou will elicit, document and analyse requirements around business challenges, and then produce data-driven solutions. You will be the go-to person when it comes to communication between IT and business stakeholders to ensure all involved work together to attain the best results.\nThis position is located in Bucharest and will be hybrid working with the expectation of being in the office at least one day per week.\nWhat we re looking for:\nDeep experience of NetSuite and the Order to Cash process.\nAgile development methodology.\nThe ability to learn new things quickly and the understand stakeholders requirements.\nThe ability to translate simple to complex user requirements into functional and actionable solutions within the NetSuite environment.\nResponsibilities & Deliverables:\n\nYour deliverables as a NetSuite Business Analyst will include, but are not limited to, the following:\nDocument and evaluate existing as-is order to cash processes, anticipating requirements, and uncovering areas for improvement.\nParticipates in software design meetings and analyzes user needs to determine technical requirements.\nGather and documents all requirements - translating business owner needs into structured user stories that permit Coupa solution design, accurate build and effective change.\nIdentify and documents opportunities for process optimisation, redesign, or new processes to reduce cost and improve customer satisfaction.\nCreate and manage a backlog of well-formed user stories (requirements) and works closely with technical architects and developers on solution design.\nCreate acceptance criteria and validate that solutions meet business needs through defining and coordinating testing.\nInitiate, plan, execute, monitor, and control Business Analysis activities on projects within agreed parameters of cost, time and quality.\nTrack and ensure user stories are being developed and delivered on time, and within scope.\nServe as a liaison between stakeholders and the development team.\nDeliver effective communication of your insights, user stories and plans to cross-functional team members and management.\nWork closely with Test leads on an appropriate test strategy and plan, and support all testing activity.\nSupport in the updating, implementing, and maintaining of training documentation.\nPrioritizing initiatives based on business needs and requirements.\nFollow agreed Project Management governance and reporting principles.\nOther duties and projects as assigned.\nRequired Skills and Experience\n4+ years NetSuite Business Analyst experience on complex IT transformations .\nExcellent understanding of the Order to Cash and P2P Finance process.\nShould be able to write flow ability to create workflow & generating reports, creating saved search.\nShould have strong experience in NetSuite one world account configuration.\nGood understanding in AR/AP /CoA and inventory\nShould be able perform simple administrative tasks such as (Customer records / Forms / Segments / permissions and roles and CSV Imports.\nExcellent understanding of the Order to Cash Finance process.\nGood understanding and experience of Agile project management delivery and software development lifecycle.\nAbility to take a systematic and analytic approach to problem solving, whilst paying close attention to the detail.\nAbility to provide solutions to a variety of technical problems of moderate scope and complexity independently.\nEffective verbal and written communication skills with the ability to communicate concisely and clearly.\nOutstanding stakeholder management skills.\nStrong interpersonal and influencing skills.\nProficiency with Microsoft Office and Jira applications.\nAbility to work independently or within a team environment and handle multiple projects simultaneously.\nFinancial Services industry experience preferred.\nJob Location: Bangalore\nWe are proud to offer a range of incentives to our employees worldwide. These benefits are available to everyone, regardless of grade, and reflect the values we uphold:\n\nFlexibility: Enjoy unlimited vacation, based on your location and business priorities. Hybrid working arrangements, and inclusive policies such as paid time off for voting, bereavement, and sick leave.\nWell-being: Access confidential one-on-one therapy through our Employee Assistance Program, unlimited personalized coaching via our coaching app, and access to our Gather Groups for emotional and mental support.\nMedical, life & disability insurance, retirement plan, lifestyle and other benefits*\nESG: Benefit from paid time off for volunteering and donation matching.\nDEI: Participate in multiple DE&I groups for . We learn from one another, embrace and celebrate our differences, and create an environment where everyone feels safe to be themselves.\nBe unique, be exceptional, and help us make a difference at Finastra!",Industry Type: Financial Services,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Administration', 'Career development', 'O2C', 'Software design', 'Business Analyst', 'Business analysis', 'agile project management', 'Workflow', 'Stakeholder management', 'Financial services']",2025-06-10 15:27:26
Business Analyst,Capgemini,6 - 11 years,Not Disclosed,"['Chennai', 'Bengaluru', 'Mumbai (All Areas)']","We are hiring experienced Business Analysts / Data Analysts with a strong background in Wholesale Loans, Lending Business, Capital Markets, Finance, or Risk Reporting. If you have expertise in SQL, Data Lineage, and Functional Design, wed love to connect with you!\n\nKey Responsibilities:\n\nAnalyze data requirements and identify disparate data sources for consolidation and distribution\nDocument functional specifications and collaborate with technology teams for implementation\nReview logical and conceptual data models to align with business requirements\nGather business requirements, produce business specifications, and create process flow diagrams\nPerform data tracing, lineage efforts, and validate solution implementations\nProvide production deployment support and investigate data quality issues\nWork with stakeholders to ensure completeness and accuracy of data models\n\nKey Skills & Requirements:\n\nSubject matter expertise in Wholesale Loans / Lending / Capital Markets / Finance / Risk Reporting\nProficiency in SQL, Data Analysis, and Database Management\nStrong documentation, analytical, and business modeling skills\nHands-on experience with MS Office (Excel, Visio, PowerPoint, Word)\nExperience in Data Tracing, Lineage, and Functional Design\nKnowledge of logical and physical data models\nNotice Period: Preferred 0 to 30 days joiners",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Capital Market', 'Business Analytics', 'SQL']",2025-06-10 15:27:29
Analyst,Wipro,2 - 6 years,Not Disclosed,['Gurugram'],"About The Role :\n\nJob Title -Analyst\n\n\n\nJob Location:Gurugram (Work from office)\n\n\n\nTime:12:00 PM to 9:30 pm / 2:00 to 11:30 PM (Should be comfortable for both the slots)\n\n\n\nKey Responsibilities:\nDevelop a good understanding of real estate finance operations, quarterly reporting deliverables, financial systems/applications, and the reporting source data.\nPossess good knowledge about SPVs and its relevance.\nPerform SPV level cash tracking and prepare wire memos for intercompany transfers.\nReconciliation of Gross Debt and bridging the gaps between Gross to Net Debt.\nFund Leverage Limitation Testing to identify the capacity of borrowing.\nAllocation and coding of project level expenses on Coupa.\nPrepare quarterly Distribution tracker for US Real Estate investments.\nPrepare quarterly Disclosure reporting items to BX for the Real Estate business.\nPrepare quarterly SPV financial statements.\nManage system setup, bank account creation and dissolution of SPV entities.\nParticipate in the process documentation efforts and creation of SOPs.\nIdentify process gaps and initiate process improvement projects.\nAd-hoc requests.\n\n\n\n\n\n\nDesired Candidate Profile:\nCandidate must be a Post Graduate or C.A., with knowledge of finance\nGood Understanding of Private Equity business and its Revenue Model.\nCandidates must have 1-4 years of relevant experience in financial reporting, performance reporting.\nStrong Microsoft Office skills (MS Excel, MS PowerPoint and MS Word)\nThe ability to effectively work as an individual contributor and possess strong analytical, problem solving, critical thinking and decision-making skills, multitask and deliver under tight deadlines\nThe profile involves effective communication across Clients facilities globally and hence possessing excellent interpersonal and communication skills in verbal and written English is a must\nA demonstrated ability to write effectively and summarize large amounts of information succinctly and quickly\nA desire to work in an international team environment, often under pressure and with multiple stakeholders",Industry Type: IT Services & Consulting,Department: Finance & Accounting,"Employment Type: Full Time, Permanent","['private equity', 'performance reporting', 'process documentation', 'financial reporting', 'financial statements', 'financial analysis', 'data analysis', 'mis reporting', 'team management', 'investment banking', 'capital market', 'corporate actions', 'portfolio management', 'ca', 'finance']",2025-06-10 15:27:32
Analyst,Lowes Services India Private limited,1 - 6 years,Not Disclosed,['Bengaluru'],"Electronic Data Interchange Analyst is responsible for determining system outages that impact service for electronic commerce transactions\n\n(i\n\ne\n\n, business applications, BizLink, Gentran, LowesLink )\n\nOnce an issue has been determined, the appropriate escalation and resolution of any issues\n\nAnother responsibility is to collaborate with Merchandising and other teams to ensure vendors are set up, tested, and moved to production by the time orders are ready to be placed\n\nOvercoming vendor delays in this process requires a great deal of finesse, encouragement cheerleading\n\nOnce a vendor has moved from test to production, resolving issues with EDI standards and adherence to Lowes EDI requirements requires a coordinator to work with the vendor to resolve issues in the setup of their internal processes and in many cases requires their assistance to work with a Merchandiser or Inventory Fulfillment person to tweak the way their programs are set up to send/receive the appropriate information from the vendor\n\nEnsuring the validity of the data allows the receiving business systems to accurately determine product arrival, schedule resources accordingly, or advise a customer of an upcoming delivery or delay",Industry Type: Retail,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Supply chain', 'Business services', 'Senior Analyst', 'Commerce', 'E-commerce', 'EDI', 'Merchandising', 'Business applications', 'Electronic Data Interchange', 'Inventory']",2025-06-10 15:27:34
Data Analyst - L4,Wipro,5 - 8 years,Not Disclosed,['Hyderabad'],"Role Purpose\nThe purpose of this role is to interpret data and turn into information (reports, dashboards, interactive visualizations etc) which can offer ways to improve a business, thus affecting business decisions.\nDo\n1. Managing the technical scope of the project in line with the requirements at all stages\na. Gather information from various sources (data warehouses, database, data integration and modelling) and interpret patterns and trends\nb. Develop record management process and policies\nc. Build and maintain relationships at all levels within the client base and understand their requirements.\nd. Providing sales data, proposals, data insights and account reviews to the client base\ne. Identify areas to increase efficiency and automation of processes\nf. Set up and maintain automated data processes\ng. Identify, evaluate and implement external services and tools to support data validation and cleansing.\nh. Produce and track key performance indicators\n2. Analyze the data sets and provide adequate information\na. Liaise with internal and external clients to fully understand data content\nb. Design and carry out surveys and analyze survey data as per the customer requirement\nc. Analyze and interpret complex data sets relating to customers business and prepare reports for internal and external audiences using business analytics reporting tools\nd. Create data dashboards, graphs and visualization to showcase business performance and also provide sector and competitor benchmarking\ne. Mine and analyze large datasets, draw valid inferences and present them successfully to management using a reporting tool\nf. Develop predictive models and share insights with the clients as per their requirement\n\nDeliver\n\nNo Performance Parameter Measure\n1. Analyses data sets and provide relevant information to the clientNo. Of automation done, On-Time Delivery, CSAT score, Zero customer escalation, data accuracy\n\nMandatory Skills: Tableau.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Analysis', 'Tableau', 'data warehouses', 'data integration', 'Data modelling']",2025-06-10 15:27:36
Data Analyst,Lenskart,3 - 7 years,Not Disclosed,"['Gurugram', 'Bengaluru']",Job Responsibilities :\nWork closely with teams across the company to understand their data requirements and develop solutions that allow them to make data-backed decisions\nDevelop an in-depth understanding of business performance and generate strategic insights to influence decision making\nCoordinate with key business stakeholders to understand the business problems and solve using strong analytics concepts,,,,"['Power Bi', 'SQL', 'Python', 'Google analytics']",2025-06-10 15:27:38
Analyst,Merkle Science,1 - 2 years,Not Disclosed,['Mumbai'],"Data Validation (DV) Specialist (Using SPSS) - Analyst\nJob Description:\nCore Responsibilities:\nPerform data quality checks and validation on market research datasets\nDevelop and execute scripts and automated processes to identify data anomalies.\nCollaborate with the Survey Programming team to review survey questionnaires and make recommendations for efficient programming and an optimal layout that enhances user experience.\nInvestigate and document data discrepancies, working with survey programming team/data collection vendors as needed.\nCreate and maintain detailed data documentation and validation reports.\nCollaborate with Survey Programmers and internal project managers to understand data processing requirements and provide guidance on quality assurance best practices.\nProvide constructive feedback and suggestions for improving the quality of data, aiming to enhance overall survey quality.\nAutomate data validation processes where possible to enhance efficiency and reduce time spent on repetitive data validation tasks.\nMaintain thorough documentation of findings and recommendations to ensure transparency and consistency in quality practices.\nActively participate in team meetings to discuss project developments, quality issues, and improvement strategies, fostering a culture of continuous improvement.\nQualification:\nBachelor s degree in computer science, Information Technology, Statistics, or a related field.\nAt least 2+ years of experience in data validation process.\nFamiliar with data validation using SPSS, Dimension, Quantum platform or similar tools\nA proactive team player who thrives in a fast-paced environment and enjoys repetitive tasks that contribute to project excellence.\nProgramming knowledge in a major programming language such as R, JavaScript, or Python, with an interest in building automation scripts for data validation.\nExcellent problem-solving skills and a willingness to learn innovative quality assurance methodologies.\nA desire for continuous improvement in processes, focusing on creating efficiencies that lead to scalable and high-quality data processing outcomes.\nLocation:\nMumbai\nBrand:\nMerkle\nTime Type:\nFull time\nContract Type:\nPermanent",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Automation', 'Manager Quality Assurance', 'Javascript', 'Data collection', 'Market research', 'Data processing', 'Data quality', 'SPSS', 'Continuous improvement', 'Information technology']",2025-06-10 15:27:40
Business Analyst - L4,Wipro,5 - 8 years,Not Disclosed,['Hyderabad'],"? \n\nBusiness Analyst\n\nBand:B3\n\nExperience:-\n24 to 36 Months\nBPO experience (preferably in Content Moderation area)\nFluency in communication, presentation & data interpretation skills, Individual contributor, industry knowledge - LLM, Google trends\nTrained on Power-BI / Power Apps (Good to hire skills)\n\n\nResponsibilities:-\n\n1) Generate business insights through -\na) Routine social media trend analytics\nb) Data crunching, and\nc) Industry trends\n\n2) Project studies to highlight program deficiencies, risks, actionable areas\n3) Liaise to conduct Kaizen / VSM workshops\n\n\n\nLocation: Hyderabad\nMandatory\n\nSkills:\nBusiness Analyst/ Data Analyst(Media).\n\nExperience5-8 Years.\n\nReinvent your world. We are building a modern Wipro. We are an end-to-end digital transformation partner with the boldest ambitions. To realize them, we need people inspired by reinvention. Of yourself, your career, and your skills. We want to see the constant evolution of our business and our industry. It has always been in our DNA - as the world around us changes, so do we. Join a business powered by purpose and a place that empowers you to design your own reinvention. Come to Wipro. Realize your ambitions. Applications from people with disabilities are explicitly welcome.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['bi', 'presentation skills', 'business analysis', 'powerapps', 'moderation', 'data analysis', 'data interpretation', 'business analytics', 'market research', 'sql', 'primary research', 'tableau', 'secondary research', 'business insights', 'qualitative research', 'digital transformation']",2025-06-10 15:27:42
Machine Learning Engineer,Adobe,12 - 15 years,Not Disclosed,[],"The engineer will be part of a team working on the development, operations and support of Adobe s AI Platform team. They will be responsible for the design, architecture and development of new features and maintenance of existing features. They will also handle all phases of development, from early specs and definition to release. They are encouraged to be hands-on problem solver and we'll conversant in analyzing, architecting and implementing Golang/python-based world class high-quality software. Prior experience on ML solutions and cloud platform services, workflow orchestrators, data pipeline solutions would be a plus.\n\nWhat you'll Do\nThis is an individual contributor position.\nHands on product/solution development knowledge are a must.\nThe position involves conceptualization of a product, design, development, debugging/triaging, deployment at scale, monitoring, analyzing, etc\nPlanning, effort estimation and risk analysis of a project.\nThe incumbent will plan, evaluate industry alternatives, design and drive new components, solutions, workflow, features, etc\nShould take the initiative to drive frugality through optimizations without compromising stability or resiliency.\n  Requirements\nbachelors / masters degree in engineering.\n12+ years of relevant industry experience.\n3+ years of experience as a lead/architect.\nA proven expertise with building large scale platforms on Kubernetes.\nProven programming skills with languages such as python and go-lang.\nExperience of the latest ML development tools.\nTrack record of delivering cloud-scale, data-driven products, and services that are widely adopted with large customer bases\nExposure to container runtime environments\nExperience in building, deploying, and managing infrastructures in public clouds (specifically AWS)",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Debugging', 'Machine learning', 'Cloud', 'Conceptualization', 'Programming', 'Product design', 'Adobe', 'Monitoring', 'Python']",2025-06-10 15:27:44
Business Analyst,Check Point Software Technologies,1 - 3 years,Not Disclosed,['Bengaluru'],"Check Point Software Technologies Ltd is looking for Business Analyst to join our dynamic team and embark on a rewarding career journey.\nEvaluating business processes, anticipating requirements, uncovering areas for improvement, and developing and implementing solutions.\nLeading ongoing reviews of business processes and developing optimization strategies.\nStaying up-to-date on the latest process and IT advancements to automate and modernize systems.\nConducting meetings and presentations to share ideas and findings.\nPerforming requirements analysis.\nDocumenting and communicating the results of your efforts.\nEffectively communicating your insights and plans to cross-functional team members and management.\nGathering critical information from meetings with various stakeholders and producing useful reports.\nWorking closely with clients, technicians, and managerial staff.\nProviding leadership, training, coaching, and guidance to junior staff.\nAllocating resources and maintaining cost efficiency.\nEnsuring solutions meet business needs and requirements.\nPerforming user acceptance testing.\nManaging projects, developing project plans, and monitoring performance.\nUpdating, implementing, and maintaining procedures.\nPrioritizing initiatives based on business needs and requirements.\nServing as a liaison between stakeholders and users.\nManaging competing resources and priorities.\nMonitoring deliverables and ensuring timely completion of projects.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['vendor management', 'project management', 'documentation', 'business analysis', 'process improvement', 'budgeting', 'strategic planning', 'operations', 'people management skills', 'service delivery', 'leadership', 'user acceptance testing', 'cost efficiency', 'reporting']",2025-06-10 15:27:46
Business Analyst,Paperchase Accountancy,1 - 4 years,3-4.5 Lacs P.A.,['Ahmedabad( Sindhu Bhavan Road )'],"Responsibilities:\nWorkflow & Process Analysis: Collaborate with IT development teams to analyze, design, and implement new workflows; identify opportunities for process improvements and increased operational efficiency.\nTesting & Quality Assurance: Lead User Acceptance Testing (UAT) for new modules, document bugs and issues, and coordinate with developers to resolve them effectively.\nProject Tracking & Reporting: Maintain comprehensive project trackers, monitor progress of ongoing developments, and provide regular updates and reports to senior management.\nStakeholder Coordination: Act as the primary liaison between internal users, external clients, and the IT team to ensure smooth communication and issue resolution.\nSystem Integration Support: Coordinate integration efforts with third-party applications like Xero, STO, and Expensify to enhance platform capabilities and streamline processes.\nPortal & Task Tracker Management: Ensure the client master and task tracker are updated accurately by operations teams; suggest and implement improvements for better usability.\nTraining & User Support: Provide training and continuous support to operations teams on portal modules and new features; address and resolve user queries in a timely manner.\nFeature Rollout & Feedback: Support pre-development analysis, post-deployment validation, and feedback collection to guide developers in enhancing the platform's user experience.\nIssue & Risk Management: Identify and address project bottlenecks, risks, and conflicts; provide timely resolutions and escalate when necessary to ensure project success.\nClient Onboarding & Implementation: Work closely with the Accounts team to ensure successful client onboarding and portal implementation, ensuring alignment with client expectations and internal capabilities.\n\n- Should have excellent written and verbal communication skills.\n- Must be good at coordination across the departments.\n- Should be willing to work from 11 am to 8 pm IST.\n- This is Onsite role only.",Industry Type: Accounting / Auditing,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Bug Tracking', 'Requirement Gathering', 'UAT', 'Stakeholder Management', 'Use Cases', 'Requirement Analysis', 'Issue Resolution']",2025-06-10 15:27:49
Data Analyst - Gurugram,Infosys,5 - 10 years,Not Disclosed,"['Chennai', 'Bengaluru', 'PAN INDIA']","Responsibilities:\nUnderstand architecture requirements and ensure effective design, development, validation, and support activities.\nAnalyze user requirements, envisioning system features and functionality.\nIdentify bottlenecks and bugs, and recommend system solutions by comparing advantages and disadvantages of custom development.\nContribute to team meetings, troubleshooting development and production problems across multiple environments and operating platforms.\nEnsure effective design, development, validation, and support activities for Big Data solutions.\nTechnical and Professional Requirements:\nSkills:\nProficiency in Scala, Spark, Hive, and Kafka.\nIn-depth knowledge of design issues and best practices.\nSolid understanding of object-oriented programming.\nFamiliarity with various design, architectural patterns, and software development processes.\nExperience with both external and embedded databases.\nCreating database schemas that represent and support business processes.\nImplementing automated testing platforms and unit tests.\nPreferred Skills:\nTechnology -> Big Data -> Scala, Spark, Hive, Kafka\nAdditional Responsibilities:\nCompetencies:\nGood verbal and written communication skills.\nAbility to communicate with remote teams effectively.\nHigh flexibility to travel.\nEducational Requirements:Master of Computer Applications, Master of Technology, Master of Engineering, MSc, Bachelor of Technology, Bachelor of Computer Applications, Bachelor of Computer Science, Bachelor of Engineering",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Spark', 'Hive', 'Hadoop', 'Big Data', 'Kafka']",2025-06-10 15:27:52
Data Analyst,Colruyt It Consultancy India,5 - 8 years,10-20 Lacs P.A.,['Coimbatore'],"We are seeking a highly skilled Senior Data Analyst with8-10 years of experience to join our dynamic team. The ideal candidate will utilize their expertise in data analysis, visualization, and business intelligence to support data-driven decision-making across the organization. You will be responsible for transforming data into actionable insights that drive business strategies and improve operational efficiency.\nKey Responsibilities:\nData Collection & Preparation:\n- Collect, clean, integrate, and manage data from multiple sources to build a comprehensive data landscape.\n- Ensure data accuracy, validation, and reliability through rigorous quality checks.\nData Analysis:\n- Conduct exploratory and in-depth data analysis to identify trends, patterns, and anomalies.\n- Provide solutions to specific business challenges by using statistical techniques and advanced data analysis.\nDashboard & Report Development:\n- Design, develop, and maintain interactive dashboards and reports using Power BI, making data accessible and actionable.\n- Customize reports based on business needs and ensure that visualizations are user-friendly and insightful.\nBusiness Insights & Recommendations:\n- Interpret data findings to generate actionable insights and recommendations that align with business objectives.\n- Communicate findings clearly to both technical and non-technical stakeholders.\nCollaboration & Stakeholder Management:\n- Collaborate with cross-functional teams, to meet data needs and drive business growth.\n- Serve as a data expert, guiding teams on best practices for data analysis and reporting.\nData Documentation & Quality Assurance:\n- Document data sources, transformations, and reporting processes.\n- Implement data quality checks and address data integrity issues by collaborating with relevant teams.\nMentoring & Leadership:\n- Mentor junior analysts and provide guidance on analytical tools, techniques, and best practices.\n- Leadcross-functional projects that require advanced analytical expertise.\nRequirements & Qualifications:\nEducation: Bachelors or Masters degree in Data Science, Statistics, Mathematics, Computer Science, or related field.\nExperience:\n8-10 year: Bachelors or Masters degree in Data Science, Statistics, Mathematics, Computer Science, or related field.\nProven experience with data visualization tools (Power BI, Tableau) and data querying languages(SQL, Python).\nTechnical Skills:\n- Strong proficiency in Power BI, including DAX and Power Query for creating interactive dashboards.\n- Experience inbuilding reports in Business Objects (SAP BO)\n- Advanced knowledge of SQL for querying and managing large datasets.\n- Experience with Python for data manipulation and advanced analytics.\n- Knowledge of ETL processes and tools for data integration and transformation.\n- Experience in Tableau reporting is an advantage.\n- Familiarity with cloud platforms like Azure, and experience with data warehouses and data lakes.\nSoft Skills:\n- Excellent analytical and problem-solving skills with attention to detail.\n- Strong communication and presentation skills to convey complex data insights to diverse audiences.\n- Ability to work independently, manage multiple projects, and collaborate across teams.\n\nPreferred Qualifications:\n- Experience in the retail industry.\n- Familiarity with machine learning techniques for predictive analytics is a plus.\nImpact & Contributions:\n- Decision-Making: Enhance decision-making processes by providing data-driven insights and guiding business strategies.\n- Data-Driven Culture: Contribute to fostering a data-driven culture by embedding insights into actions.\n- Stakeholder Collaboration: Effectively manage relationships with stakeholders across varying levels of data maturity, ensuring comprehensive understanding and application of insights.\nThis role offers an opportunity to influence key business decisions through advanced data analysis, reporting and a collaborative approach.\n\nJoin us in driving impactful changes and supporting the growth of our data-driven initiatives.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Report Development', 'Power Query', 'Data Analysis', 'Dax', 'ETL', 'Power Bi', 'Data Extraction', 'Data Collection', 'Tableau', 'SQL', 'Data Visualization', 'Dashboard Development', 'Data Analytics', 'Data Reporting']",2025-06-10 15:27:55
Data Engineer/Consultant Specialist,Hsbc,7 - 8 years,Not Disclosed,['Hyderabad'],"Some careers shine brighter than others.\nIf you re looking for a career that will help you stand out, join HSBC and fulfil your potential. Whether you want a career that could take you to the top, or simply take you in an exciting new direction, HSBC offers opportunities, support and rewards that will take you further.\nHSBC is one of the largest banking and financial services organisations in the world, with operations in 64 countries and territories. We aim to be where the growth is, enabling businesses to thrive and economies to prosper, and, ultimately, helping people to fulfil their hopes and realise their ambitions.\nWe are currently seeking an experienced professional to join our team in the role of Marketing Title.\nIn this role, you will:\nThis role will be responsible for:\nDevelop and support new feeds ingestion / understand the existing framework and do the development as per the business rules and requirements.\nDevelopment and maintenance of new changes / enhancements in Data Ingestion / Juniper and promoting and supporting those in the production environment within the stipulated timelines.\nNeed to get familiar with the Data Ingestion / Data Refinery / Common Data Model / Compdata frameworks quickly and contribute to the application development as soon as possible.\nMethodical and measured approach with a keen eye for attention to detail;\nAbility to work under pressure and remain calm in the face of adversity;\nAbility to collaborate, interact and engage with different business, technical and subject matter experts;\nGood, concise, written and verbal communication\nAbility to manage workload from multiple requests and to balance priorities;\nPro-active, a can do mind-set and attitude;\nGood documentation skills\n\n\n\n\n\n\n\n\n\n\nRequirements\n\n\n\nTo be successful in this role, you should meet the following requirements:\nExperience (1 = essential, 2 = very useful, 3 = nice to have):\n1. Hadoop / Hive / GCP\n2. Agile / Scrum\n3. LINUX\nTechnical skills (1 = essential, 2 = useful, 3 = nice to have):\n1. Any ETL tool\n1. Analytical trouble shooting.\n2. Hive QL\n1. On-Prem / Cloud infra knowledgeYou ll achieve more when you join HSBC.",Industry Type: Consumer Electronics & Appliances,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['hive', 'Linux', 'GCP', 'Analytical', 'business rules', 'Application development', 'Refinery', 'Troubleshooting', 'ETL tool', 'Financial services']",2025-06-10 15:27:58
Data Engineer/ Consultant Specialist,Hsbc,7 - 8 years,Not Disclosed,['Pune'],"Some careers shine brighter than others.\nIf you re looking for a career that will help you stand out, join HSBC and fulfil your potential. Whether you want a career that could take you to the top, or simply take you in an exciting new direction, HSBC offers opportunities, support and rewards that will take you further.\nHSBC is one of the largest banking and financial services organisations in the world, with operations in 64 countries and territories. We aim to be where the growth is, enabling businesses to thrive and economies to prosper, and, ultimately, helping people to fulfil their hopes and realise their ambitions.\nWe are currently seeking an experienced professional to join our team in the role of Marketing Title.\nIn this role, you will:\nThis role will be responsible for:\nDevelop and support new feeds ingestion / understand the existing framework and do the development as per the business rules and requirements.\nDevelopment and maintenance of new changes / enhancements in Data Ingestion / Juniper and promoting and supporting those in the production environment within the stipulated timelines.\nNeed to get familiar with the Data Ingestion / Data Refinery / Common Data Model / Compdata frameworks quickly and contribute to the application development as soon as possible.\nMethodical and measured approach with a keen eye for attention to detail;\nAbility to work under pressure and remain calm in the face of adversity;\nAbility to collaborate, interact and engage with different business, technical and subject matter experts;\nGood, concise, written and verbal communication\nAbility to manage workload from multiple requests and to balance priorities;\nPro-active, a can do mind-set and attitude;\nGood documentation skills\n\n\n\n\n\n\n\n\n\n\nRequirements\n\n\n\nTo be successful in this role, you should meet the following requirements:\nExperience (1 = essential, 2 = very useful, 3 = nice to have):\n1. Hadoop / Hive / GCP\n2. Agile / Scrum\n3. LINUX\nTechnical skills (1 = essential, 2 = useful, 3 = nice to have):\n1. Any ETL tool\n1. Analytical trouble shooting.\n2. Hive QL\n1. On-Prem / Cloud infra knowledge",Industry Type: Consumer Electronics & Appliances,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['hive', 'Linux', 'GCP', 'Analytical', 'business rules', 'Application development', 'Refinery', 'Troubleshooting', 'ETL tool', 'Financial services']",2025-06-10 15:28:01
AI Business Analyst,Tech Mahindra,6 - 11 years,Not Disclosed,['Bengaluru'],"About the Role We are seeking a highly motivated and analytical Business Analyst with an experience of about 6 years as a BA, to join our dynamic team\n\nThe Business Analyst will be responsible for identifying the current business processes, business problems and opportunities, and help in developing solutions\n\nShould have a good knowledge in the space of Cyber Security\n\nKey ResponsibilitiesConducting research and analysis to identify business problems and areas of improvement including but not limited to Cyber Security spaceGathering and documenting business requirements and translating them into functional and non functional requirement specifications\n\nDeveloping and implementing new business processes and procedures to improve organizational efficiency\n\nDeveloping business cases and cost/benefit analyses to support proposed solutions\n\nCollaborating with cross functional teams to develop and implement solutions that meet business requirements\n\nShould serve as a liaison officer between the business stakeholders and the development teamsWriting business test cases, testing and validating solutions to ensure they meet business objectives and requirements\n\nIdentifying and managing risks and issues that may impact the success of the project\n\nCommunicating project status and updates to stakeholders and project teams\n\nContinuously monitoring and analysing business performance to identify areas for improvement\n\nRequirementsBachelors or Masters degree\n\n5 6 years of relevant experience as a BA\n\nProven experience as a BA in large projects\n\nTo be successful in this role you will need a strong understanding of Cyber Security principles, Risk Assessment, threat levels, Regulatory Compliances and best practices\n\nExcellent written, communication and collaboration skills\n\nAbility to work independently and manage multiple projects simultaneously Good to haveOverall knowledge of AWS cloud services\n\nInvolved in AI/ML related projects\n\nUnderstanding of telecom domain\n\nExperience in working on AI/ML, RPA projects",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Telecom', 'Translation', 'cyber security', 'Business Analyst', 'Cloud Services', 'Risk assessment', 'Analytical', 'Test cases', 'Business development', 'Monitoring']",2025-06-10 15:28:03
Business Analyst,Coforge,10 - 15 years,Not Disclosed,['Bengaluru'],"? Breaks down and prioritizes team backlog ? Advanced computer software skills. Minimum 8 years tech experience; Minimum 2 years as TPM / BA;with Airine, GDS domain experience ? Analyses business/customer problems and identify Sabre product gaps ? Excellent written and verbal communication skills; ? Provides input to technical and business aspects of requirements ? Ability to handle multiple projects simultaneously. ? Collaborates with team to provide input on requirements (for sprint planning) ? Provides inputs during sprint execution ? Provides inputs for go/no-go decision making for every release in conjunction with key stakeholders (incl. promotion to CVT, cert, prod environments) ? Authorized to accept/sign-off stories at completion ? Attends scrum ceremonies ? Supports Product Managers, Interfaces with Marketing/Customer for developing an understanding of business problems ? Creates Functional Requirements Document (FRD) as required ? Verifies accuracy of release notes, SAN, User Guide as required in ship with Product Manager ? Owns the sizing process ? Conducts customer demo as needed",,,,"['project management', 'verbal communication', 'business analysis', 'user stories', 'demo', 'tpm', 'business requirement analysis', 'requirement gathering', 'charge entry', 'brd', 'technical analysis', 'sprint planning', 'writing', 'gds', 'reservation', 'frd', 'scrum', 'medical billing', 'agile', 'communication skills']",2025-06-10 15:28:06
Machine Learning Engineer,Catalyst Clinical Research,5 - 10 years,Not Disclosed,"['Kolkata', 'Mumbai', 'New Delhi', 'Hyderabad', 'Pune', 'Chennai', 'Bengaluru']","{""company"":""\nCatalyst Clinical Research provides customizable solutions to the biopharmaceutical and biotechnology industries through , a full-service oncology CRO, and multi-therapeutic global functional and CRO services through . The companys customer-centric flexible service model, innovative technology, expert team members, and global presence advance clinical studies. Visit .\n\nThe Machine Learning Engineer is a pivotal contributor responsible for designing and implementing cutting-edge machine learning solutions with a focus on generative AI technologies. You will drive the development and deployment of advanced models and pipelines that enable the creation of AI-driven applications and enhance organizational decision-making capabilities. Additionally, you will support data engineering initiatives to enable utilization of data across the organization. Collaborating closely with internal and external stakeholders, you will translate complex requirements into innovative solutions that advance Catalysts AI strategies while ensuring alignment with broader enterprise goals.\n"",""role"":""\nPosition Responsibilities/ Accountabilities:\n\nDesign, build, and optimize machine learning workflows, with a focus on generative AI models such as large language models (LLMs) and diffusion-based architectures.\nDevelop and deploy scalable machine learning pipelines using frameworks like TensorFlow, PyTorch, and Databricks MLflow.\nDevelop AI solutions using tools like Azure AI/Copilot Studio and Databricks AI Builder.\nLead the creation of domain-specific generative AI models, ensuring ethical AI practices and bias mitigation throughout the model lifecycle.\nDesign, build, and maintain scalable data pipelines with Delta Live Tables for model integration into enterprise applications.\nEnhance and expand CI/CD strategies for automated testing, model monitoring, and continuous delivery of ML artifacts.\nManage data preprocessing, feature engineering, and synthetic data generation for machine learning use cases.\nCollaborate with cross-functional teams to align AI-driven solutions with business goals and ensure high availability for end-to-end systems.\nProvide technical expertise in the exploration of novel generative AI methods, tools, and frameworks.\nSupport team members in understanding data science and AI best practices, encouraging a culture of innovation and continuous learning. Represent AI as a key member of the Data & Architecture Review Committee.\n\nPosition Qualification Requirements :\nEducation\n: B.S. or M.S. Computer Science, Engineering, Economics, Mathematics, related field, or relevant experience.\n\nExperience:\n5+ years of experience in machine learning engineering, including model development and deployment.\nHands-on experience with generative AI models (e.g., GPT, GANs, VAEs) and frameworks like PyTorch or TensorFlow.\n5+ years of experience with cloud computing technologies (Azure, AWS, GCP), especially AI and ML services.\nProficiency in developing data pipelines and integrating ML models into production environments.\nExpertise in model evaluation and monitoring, including techniques for explainability and fairness in AI.\nExperience collaborating with DevOps and MLOps teams to ensure scalability and reliability of AI solutions.\nFamiliarity with project management tools such as JIRA.\n\nRequired Skills:\nAdvanced proficiency in Python or PySpark for ML applications.\nDeep understanding of generative AI principles, model architecture, and training methodologies.\nExpertise in large-scale data processing and engineering using Spark, Kafka, and Databricks.\nProficiency with big data technologies and data structures like delta, parquet, YAML, JSON, and HTML.\nStrong knowledge of cloud-based AI platforms (e.g. Databricks, Azure ML, etc).\nSolid understanding of machine learning pipelines and MLOps practices.\nExceptional problem-solving and analytical skills.\nAbility to manage priorities and workflow effectively.\nProven ability to handle multiple projects and meet tight deadlines.\nStrong interpersonal skills with an ability to work collaboratively across teams.\nCommitment to excellence and high standards.\nCreative, flexible, and innovative team player.\nAbility to work independently and as part of various committees and teams.\nNice to Have: Data Engineering experience, including Webhooks, API, ELT/ETL, rETL, Data Lakehouse Architecture, and Event-Driven Architectures.\n\nFamiliarity with deep learning frameworks for generative AI (e.g., Hugging Face Transformers).\nKnowledge of synthetic data generation techniques and tools.\nExperience with data visualization tools (e.g., Tableau, Power BI) for AI model interpretability.\nFamiliarity with ethical AI principles, including explainability and bias reduction strategies.\nExperience with containerization and orchestration tools like Docker and Kubernetes.\nBackground or familiarity with clinical trials or pharmaceutical development.\n\nWorking Hours\nEveryday: 1:30 PM - 9:00 PM IST\nOR\nMonday, Wednesday, Friday: 2:30 PM - 10:30 PM IST\nTuesday, Thursday: 9:00 AM - 5:00 PM IST\nNote: Working hours may vary based on individual seniority, business demand, and ability to work independently. This will be evaluated on a case-by-case basis.\n""},""",Industry Type: Miscellaneous,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'CRO', 'Project management', 'Pharma', 'Clinical trials', 'Clinical research', 'Data processing', 'Workflow', 'HTML', 'Monitoring']",2025-06-10 15:28:09
Data Governance Engineer,TELUS Digital,5 - 8 years,8-16 Lacs P.A.,"['Gandhinagar', 'Ahmedabad']","About TELUS Digital\nTELUS Digital (NYSE and TSX: TIXT) designs, builds, and delivers next-generation digital\nsolutions to enhance the customer experience (CX) for global and disruptive brands. The\ncompanys services support the full lifecycle of its clients digital transformation journeys and\nenable them to more quickly embrace next-generation digital technologies to deliver better\nbusiness outcomes. TELUS Digitals integrated solutions and capabilities span digital strategy,\ninnovation, consulting and design, digital transformation and IT lifecycle solutions, data\nannotation, and intelligent automation, and omnichannel CX solutions that include content",,,,"['Mdm Informatica', 'collibra', 'Data Lineage', 'Informatica', 'Data Governance', 'Metadata Management', 'Data Catalog', 'Data Maintenance', 'Governance', 'Data Governance Analyst', 'sql', 'MDM', 'Data Warehousing', 'Data Analytics', 'Python']",2025-06-10 15:28:11
Data Engineer,Global Banking Organization,5 - 10 years,Not Disclosed,['Hyderabad'],"Key Skills: Data Engineer, AI (Artificial intelligence), SQL, Python, Java.\nRoles and Responsibilities:\nArchitect and implement modern, scalable data solutions on cloud platforms, specifically Google Cloud Platform (GCP).\nCollaborate with cross-functional teams to assess, redesign, and modernize legacy data systems.\nDesign and develop efficient ETL pipelines for data extraction, transformation, and loading to support analytics and ML models.\nEnsure robust data governance by maintaining high standards of data security, integrity, and compliance with regulatory requirements.\nMonitor, troubleshoot, and optimize data workflows and pipelines for enhanced system performance and scalability.\nProvide hands-on technical expertise and guidance across data engineering projects, with a focus on cloud adoption and automation.\nWork in an agile environment and contribute to continuous delivery and improvement initiatives.\nExperience Requirements:\n5-10 years experience in designing and implementing data engineering solutions in GCP or other leading cloud platforms.\nSolid understanding of legacy data infrastructure with demonstrated success in modernization and migration projects.\nProficiency in programming languages such as Python and Java for building data solutions and automation scripts.\nStrong SQL skills, with experience in working with both relational (SQL) and non-relational (NoSQL) databases.\nFamiliarity with data warehousing concepts, tools, and practices.\nHands-on experience with data integration tools and frameworks.\nExcellent analytical, problem-solving, and communication skills.\nExperience working in fast-paced agile environments and collaborating with multi-disciplinary teams.\nEducation: B.tech, M.tech, B.com, M.com, MBA, any PG.",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Java', 'Data Engineer', 'Python']",2025-06-10 15:28:13
Data Engineer-Apache Airflow,IBM,8 - 13 years,Not Disclosed,['Mumbai'],"Role Overview :\nSeeking an experienced Apache Airflow specialist to design and manage data orchestration pipelines for batch/streaming workflows in a Cloudera environment.\n\n Key Responsibilities :\nDesign, schedule, and monitor DAGs for ETL/ELT pipelines\nIntegrate Airflow with Cloudera services and external APIs\nImplement retries, alerts, logging, and failure recovery\nCollaborate with data engineers and DevOps teams\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\n\n Skills Required :\n Experience 3–8 years\nExpertise in Airflow 2.x, Python, Bash\nKnowledge of CI/CD for Airflow DAGs\nProven experience with Cloudera CDP, Spark/Hive-based data pipelines\nIntegration with Kafka, REST APIs, databases",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['continuous integration', 'ci/cd', 'spark', 'python', 'bash', 'hive', 'cloudera', 'scala', 'pyspark', 'dbms', 'apache pig', 'sql', 'apache', 'devops', 'linux', 'shell scripting', 'hadoop', 'etl', 'big data', 'hbase', 'rest', 'oozie', 'airflow', 'elt', 'data engineering', 'cdp', 'nosql', 'mapreduce', 'kafka', 'sqoop', 'aws', 'unix']",2025-06-10 15:28:15
Data Analyst,Incanus Technologies,1 - 6 years,5-15 Lacs P.A.,"['Pune', 'Bengaluru', 'Delhi / NCR']","Join the data revolution in one of India's most dynamic consumer companies! Are you ready to dive into the world of data analytics and make a tangible impact on business decisions? If so, we want you on our team!\n\nRoles & Responsibilities:\n\nLead the charge in KPI reporting and unravel the mysteries behind key business drivers.\nDelve deep into the data universe to quantify the impact of various factors on our KPIs.\nTransform raw data into actionable insights that drive strategic decision-making.\nDefine and communicate key metrics and trends to keep the team on track.\nMaster the art of data extraction, cleansing, and normalization to ensure accuracy in our analyses.\n\nDesired Skills and Experience:\n\nGraduates from any discipline, unleash your potential!\nTechnical prowess in Excel & SQL would be a plus.\nUp to 3 years of experience in fields like Data Analytics, Software Development & Support, Operations, Sales, or Customer Service.\nA knack for problem-solving and generating valuable insights.\nStellar communication skills to engage both technical and non-technical stakeholders.\n\nHiring Process:\n\nEligible candidates will receive an evaluation link to showcase their skills.\nFurther rounds will be tailored based on your performance in the evaluation test.\n\n\nNote: We will be reaching out to only candidates meeting the eligibility criteria mentioned above.",Industry Type: Education / Training,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Python', 'SQL', 'Excel']",2025-06-10 15:28:18
Analyst,Wipro,1 - 4 years,Not Disclosed,['Gurugram'],"About The Role  \n\nThe position will be focused on partnership with U.K. and NY stakeholders to primarily perform periodic portfolio and asset level data analysis and reporting to both internal and external stakeholders. The new hire is expected to spend time on iLevel and MDM dependent processes and optimizing existing systems/template setup to create consistent and efficient reporting for Europe region. The key responsibilities are as follows\n\nKey Responsibilities include but are not limited to:\n\n\nProduct and Investment KnowledgeBecome proficient in Client BREDS Business funds and investments\n\n\nPortfolio InsightsSource, summarize, and disseminate real-time information across the global portfolio; work in partnership with other BREDS colleagues and teams (Asset Management, Securities Ops, Revantage, Brio)\n\n\nValuation ReportingWork on ad hoc requests, strategy support, and quarterly reporting of the operating and valuation data of the assets in the alternative fund's portfolio to stakeholders\n\n\nInvestor ReportingPopulate and update investor and insurance SMA reports on periodic basis and assist stakeholders in producing publishing material and supplementals\n\n\nSpecial ProjectsParticipation in and execution of global ad hoc projects relating to portfolio information, insurance client governance and strategy / positioning of the BREDS business",Industry Type: IT Services & Consulting,Department: Finance & Accounting,"Employment Type: Full Time, Permanent","['data analysis', 'asset management', 'valuation', 'mdm', 'finance', 'financial analysis', 'vendor management', 'project management', 'equity research', 'investment banking', 'content editing', 'master data management', 'proof reading', 'editing', 'sql', 'content writing', 'financial modelling']",2025-06-10 15:28:20
Data Analyst - Contract 3 months,RWS Group,5 - 8 years,22.5-30 Lacs P.A.,[],"Role & responsibilities -\nDevelop Sisense Report for Certified HAI (HSBC) to provide key business insights and form part of the QBR\nSupport with business reporting on the Evolve roll-out (LXD)\nImplement automated email triggers with reporting capabilities to track engagement and effectiveness.\nGenerate and maintain HAI reports in Salesforce for sales and customer service teams.\nCreate a HAI customer report in Salesforce for sales enablement and customer insights.\nDevelop and optimize the Marketing dashboard to track campaign performance.\nBuild a PJM management dashboard for project performance monitoring.\nProvide ongoing support for data-driven decision-making through continuous insights.\nAutomate customer surveys to streamline feedback collection.\nImplement a customer feedback review system within dashboards to assess quotes and service quality. \nMonitor conversion rates to improve lead generation and sales performance.\nGenerate customer heatmaps to analyze engagement and behavior trends.\n\nPreferred candidate profile\nProficiency in developing reports and dashboards using tools like Sisense and Salesforce, with a strong ability to deliver business insights that support quarterly business reviews (QBRs) and strategic initiatives.\nExperience in creating, managing, and optimizing reports and dashboards within Salesforce, specifically tailored for sales enablement, customer service, and performance monitoring.\nAbility to design and maintain marketing dashboards to effectively track and analyze campaign performance metrics.\nSkilled in building project management dashboards (e.g., PJM) to support performance analysis and reporting for project teams.\nExperience in implementing automated workflows, such as email triggers with embedded reporting, to track user engagement and process efficiency.\nAbility to automate surveys and develop customer feedback dashboards to assess service quality and support data-driven improvement.\nProven capability in delivering continuous insights and analytical support for strategic decision-making across multiple business units.\nCompetence in monitoring and analyzing conversion rates, customer behavior trends, and engagement heatmaps to inform sales and marketing strategies.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Project management dashboard', 'Sisense', 'Salesforce']",2025-06-10 15:28:22
Analyst,Wipro,0 - 4 years,Not Disclosed,['Gurugram'],"Wipro Limited (NYSEWIT, BSE507685, NSEWIPRO) is a leading technology services and consulting company focused on building innovative solutions that address clients’ most complex digital transformation needs. Leveraging our holistic portfolio of capabilities in consulting, design, engineering, and operations, we help clients realize their boldest ambitions and build future-ready, sustainable businesses. With over 230,000 employees and business partners across 65 countries, we deliver on the promise of helping our customers, colleagues, and communities thrive in an ever-changing world. For additional information, visit us at www.wipro.com.\n About The Role  \n\nAbout The Role :-\n\nTo prepare and submit Fund and Investor Reports accurately as per SLA.\n\nTo meet TAT and deliver error free services.\n\nTo work on partnership accounting Applications.\n\nTo strive to create a healthy and professional work environment in the team.\n\nDisplay interpersonal skills in handling the day-to-day operations on the floor.\n\nSuggest and work on process improvements Idea.\n\nVery good understanding of Financial Accounting.\n\nGeneral understanding of Capital Markets.\n\nGeneral understanding of Banking.\n\nGeneral understanding of Private Equity / Real Estate / Hedge Fund.\n\nHands on experience of Private Equity / Real Estate / Hedge Funds systems.\n\nReasonable understanding of Private Equity / Real Estate / Hedge Funds processes such as Accounting, Investor Reporting, Capital Calls, Distribution, Financial Statements etc.\n\n ? \n\nEssential Details:-\n\nTo work on Private Equity / Real Estate / Hedge Fund Operations such as Fund / Financial Accounting - Book Keeping, Journal Posting, Preparation of Financial Statements.\n\nManagement Fee Carried Interest and Expense Calculation & Posting.\n\nCapital Calls - Preparation of Capital Call memos, LC Opening, Follow Up with LPs for funding Distribution - Preparation of Distribution working,\n\nLP Memo Preparation.\n\nFund Transfer Wire preparation,\n\nRepayment of LCs Reconciliation - Cash Reconciliation,\n\nCash Tracking, Posting Entries,\n\nCash reporting\n\nTracking of Capital transactions.\n\nValuation - Valuation of Portfolio Investments\n\n\n\n ? \n\n \n\nDeliver excellent customer service through effective diagnosis and troubleshooting of client queries  \nProvide product support and resolution to clients by performing a question diagnosis while guiding users through step-by-step solutions\nAssist clients with navigating around product menus and facilitate better understanding of product features\nTroubleshoot all client queries in a user-friendly, courteous and professional manner\nMaintain logs and records of all customer queries as per the standard procedures and guidelines\nAccurately process and record all incoming call and email using the designated tracking software\nOffer alternative solutions to clients (where appropriate) with the objective of retaining customers’ and clients’ business\nOrganize ideas and effectively communicate oral messages appropriate to listeners and situations\nFollow up and make scheduled call backs to customers to record feedback and ensure compliance to contract /SLAs\n\n\n ? \n\n \n\nBuild capability to ensure operational excellence and maintain superior customer service levels of the existing account/client  \nUndertake product trainings to stay current with product features, changes and updates\nEnroll in product specific and any other trainings per client requirements/recommendations\nPartner with team leaders to brainstorm and identify training themes and learning issues to better serve the client\nUpdate job knowledge by participating in self learning opportunities and maintaining personal networks\n\n\n ? \n\n \n\nDeliver\n NoPerformance ParameterMeasure1ProcessNo. of cases resolved per day, compliance to process and quality standards, meeting process level SLAs, Pulse score, Customer feedback2Self- ManagementProductivity, efficiency, absenteeism, Training Hours, No of technical training completed Reinvent your world.We are building a modern Wipro. We are an end-to-end digital transformation partner with the boldest ambitions. To realize them, we need people inspired by reinvention. Of yourself, your career, and your skills. We want to see the constant evolution of our business and our industry. It has always been in our DNA - as the world around us changes, so do we. Join a business powered by purpose and a place that empowers you to design your own reinvention. Come to Wipro. Realize your ambitions. Applications from people with disabilities are explicitly welcome.",Industry Type: IT Services & Consulting,Department: Finance & Accounting,"Employment Type: Full Time, Permanent","['customer service', 'hedge funds', 'accounting', 'capital market', 'financial accounting', 'project management', 'private equity', 'process compliance', 'valuation', 'troubleshooting', 'operational excellence', 'digital transformation', 'finance', 'financial statements', 'sla management']",2025-06-10 15:28:25
Analyst- TLS Business Oversight & Portfolio Analytics,AMERICAN EXPRESS,1 - 3 years,Not Disclosed,['Gurugram'],"Here, your voice and ideas matter, your work makes an impact, and together, you will help us define the future of American Express.\nAmerican Express Travel & Lifestyle Services\nAmerican Express entered the travel agency business in 1915 and is today one of the world s largest luxury travel and lifestyle networks. American Express Travel & Lifestyle Services (TLS) operates around the world with one goal in mind: provide a bespoke and seamless experience for our Card Members and hundreds of travel partners. As a critical component of premium card products value proposition, TLS offers a full range of services from trip planning and booking to enhancing the broader travel experience through proprietary programs.",,,,"['Career development', 'MS Office Powerpoint', 'Data management', 'Finance', 'data visualization', 'Customer engagement', 'SQL', 'Python']",2025-06-10 15:28:28
Analyst,Wipro,0 - 4 years,Not Disclosed,['Gurugram'],"About The Role :\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDaily and Monthly accounting activities (booking journal entries, reconciling accounts, etc.) in oracle; includes legal entity balance sheet, income statement and intercompany reports\nCreate supporting workpapers and reconciliations for all legal entities and accounts\nCreate and upload billing upon request\nProcess ageing upon requests\nRecording and analyzing Management Fee allocation and schedules, Revenue, Expense Allocation\nSaving bank statements and performing cash/bank reconciliations.\nRun project summary daily and project code details upon requests.\nTransfer transactions among project codes upon requests\nCommunicate with Fund accountant for variances break\nOther ad hoc tasks/projects\nProcessing request on Account Reconciliation manager\nAssist in preparation of Regulatory Filings TIC\n\n\nQualifications\nExperience in Oracle is a value advantage\n4+ years of hands-on experience in Advisor or corporate accounting stream.\nStrong knowledge in MS office (MS Excel and MS Word)\nThe Profile involves effective communication across Clients facility globally hence possessing excellent interpersonal and communication skills in verbal and written English is must.\nThe ability to works as individual contributor, multitask and deliver under tight deadlines\n\n\n\n About The Role  \n\nRole Purpose\n\nThe purpose of the role is to provide effective technical support to the process and actively resolve client issues directly or through timely escalation to meet process SLAs.\n\n\n ? \n\n \n\nDo  \n\n \n\nSupport process by managing transactions as per required quality standards  \nFielding all incoming help requests from clients via telephone and/or emails in a courteous manner\nDocument all pertinent end user identification information, including name, department, contact information and nature of problem or issue\nUpdate own availability in the RAVE system to ensure productivity of the process\nRecord, track, and document all queries received, problem-solving steps taken and total successful and unsuccessful resolutions\nFollow standard processes and procedures to resolve all client queries\nResolve client queries as per the SLA’s defined in the contract\nAccess and maintain internal knowledge bases, resources and frequently asked questions to aid in and provide effective problem resolution to clients\nIdentify and learn appropriate product details to facilitate better client interaction and troubleshooting\nDocument and analyze call logs to spot most occurring trends to prevent future problems\nMaintain and update self-help documents for customers to speed up resolution time\nIdentify red flags and escalate serious client issues to Team leader in cases of untimely resolution\nEnsure all product information and disclosures are given to clients before and after the call/email requests\nAvoids legal challenges by complying with service agreements\n\n\n ? \n\n \n\nDeliver excellent customer service through effective diagnosis and troubleshooting of client queries  \nProvide product support and resolution to clients by performing a question diagnosis while guiding users through step-by-step solutions\nAssist clients with navigating around product menus and facilitate better understanding of product features\nTroubleshoot all client queries in a user-friendly, courteous and professional manner\nMaintain logs and records of all customer queries as per the standard procedures and guidelines\nAccurately process and record all incoming call and email using the designated tracking software\nOffer alternative solutions to clients (where appropriate) with the objective of retaining customers’ and clients’ business\nOrganize ideas and effectively communicate oral messages appropriate to listeners and situations\nFollow up and make scheduled call backs to customers to record feedback and ensure compliance to contract /SLAs\n\n\n ? \n\n \n\nBuild capability to ensure operational excellence and maintain superior customer service levels of the existing account/client  \nUndertake product trainings to stay current with product features, changes and updates\nEnroll in product specific and any other trainings per client requirements/recommendations\nPartner with team leaders to brainstorm and identify training themes and learning issues to better serve the client\nUpdate job knowledge by participating in self learning opportunities and maintaining personal networks\n\n\n ? \n\n \n\nDeliver\n NoPerformance ParameterMeasure1ProcessNo. of cases resolved per day, compliance to process and quality standards, meeting process level SLAs, Pulse score, Customer feedback2Self- ManagementProductivity, efficiency, absenteeism, Training Hours, No of technical training completed\n\nAbout The Role :\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDaily and Monthly accounting activities (booking journal entries, reconciling accounts, etc.) in oracle; includes legal entity balance sheet, income statement and intercompany reports\nCreate supporting workpapers and reconciliations for all legal entities and accounts\nCreate and upload billing upon request\nProcess ageing upon requests\nRecording and analyzing Management Fee allocation and schedules, Revenue, Expense Allocation\nSaving bank statements and performing cash/bank reconciliations.\nRun project summary daily and project code details upon requests.\nTransfer transactions among project codes upon requests\nCommunicate with Fund accountant for variances break\nOther ad hoc tasks/projects\nProcessing request on Account Reconciliation manager\nAssist in preparation of Regulatory Filings TIC\n\n\nQualifications\nExperience in Oracle is a value advantage\n4+ years of hands-on experience in Advisor or corporate accounting stream.\nStrong knowledge in MS office (MS Excel and MS Word)\nThe Profile involves effective communication across Clients facility globally hence possessing excellent interpersonal and communication skills in verbal and written English is must.\nThe ability to works as individual contributor, multitask and deliver under tight deadlines",Industry Type: IT Services & Consulting,Department: Finance & Accounting,"Employment Type: Full Time, Permanent","['oracle', 'journal entries', 'customer service', 'accounting', 'corporate accounting', 'accounts reconciliation', 'balance sheet', 'billing', 'technical support', 'process compliance', 'bank reconciliation', 'troubleshooting', 'client interaction', 'operational excellence', 'sla management']",2025-06-10 15:28:31
Business Analyst,Manektech,4 - 8 years,Not Disclosed,['Ahmedabad'],"ManekTech is looking for Business Analyst to join our dynamic team and embark on a rewarding career journey\nEvaluating business processes, anticipating requirements, uncovering areas for improvement, and developing and implementing solutions.\nLeading ongoing reviews of business processes and developing optimization strategies.\nStaying up-to-date on the latest process and IT advancements to automate and modernize systems.\nConducting meetings and presentations to share ideas and findings.\nPerforming requirements analysis.\nDocumenting and communicating the results of your efforts.\nEffectively communicating your insights and plans to cross-functional team members and management.\nGathering critical information from meetings with various stakeholders and producing useful reports.\nWorking closely with clients, technicians, and managerial staff.\nProviding leadership, training, coaching, and guidance to junior staff.\nAllocating resources and maintaining cost efficiency.\nEnsuring solutions meet business needs and requirements.\nPerforming user acceptance testing.\nManaging projects, developing project plans, and monitoring performance.\nUpdating, implementing, and maintaining procedures.\nPrioritizing initiatives based on business needs and requirements.\nServing as a liaison between stakeholders and users.\nManaging competing resources and priorities.\nMonitoring deliverables and ensuring timely completion of projects.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['vendor management', 'project management', 'documentation', 'business analysis', 'process improvement', 'budgeting', 'strategic planning', 'operations', 'people management skills', 'service delivery', 'leadership', 'user acceptance testing', 'cost efficiency', 'reporting']",2025-06-10 15:28:34
Business Analyst,Bhagwan Mahaveer Cancer Hospital & Research Centre,2 - 4 years,Not Disclosed,['Jaipur'],"Bhagwan Mahaveer Cancer Hospital & Research Centre is looking for Business Analyst to join our dynamic team and embark on a rewarding career journey.\nEvaluating business processes, anticipating requirements, uncovering areas for improvement, and developing and implementing solutions.\nLeading ongoing reviews of business processes and developing optimization strategies.\nStaying up-to-date on the latest process and IT advancements to automate and modernize systems.\nConducting meetings and presentations to share ideas and findings.\nPerforming requirements analysis.\nDocumenting and communicating the results of your efforts.\nEffectively communicating your insights and plans to cross-functional team members and management.\nGathering critical information from meetings with various stakeholders and producing useful reports.\nWorking closely with clients, technicians, and managerial staff.\nProviding leadership, training, coaching, and guidance to junior staff.\nAllocating resources and maintaining cost efficiency.\nEnsuring solutions meet business needs and requirements.\nPerforming user acceptance testing.\nManaging projects, developing project plans, and monitoring performance.\nUpdating, implementing, and maintaining procedures.\nPrioritizing initiatives based on business needs and requirements.\nServing as a liaison between stakeholders and users.\nManaging competing resources and priorities.\nMonitoring deliverables and ensuring timely completion of projects.",Industry Type: Medical Services / Hospital,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['vendor management', 'project management', 'documentation', 'business analysis', 'process improvement', 'research', 'budgeting', 'strategic planning', 'operations', 'people management skills', 'service delivery', 'leadership', 'user acceptance testing', 'cost efficiency', 'reporting']",2025-06-10 15:28:36
Business Analyst,Wipro,3 - 5 years,Not Disclosed,['Gurugram'],"About The Role  \n\nSKILLS\n\n'- Strong analytical skills with proficiency in data analysis tools.\n- Excellent communication and presentation skills to convey insights effectively.\n- Ability to manage multiple tasks and prioritize in a fast-paced environment.\n- Strong problem-solving skills and attention to detail.\n- Familiar with various reporting tool such as Google data studio, Google Sheet, Tableau\n\n ? \n\nResponsibility\n\n\n\n'- Analyze the end-to-end processes to identify inefficiencies and recommend enhancements.\n- Generate actionable insights and provide recommendations to improve team performance and meet SLAs.\n- Create detailed reports, dashboards, and presentations for management and stakeholders.\n- Work closely with internal teams and stakeholders to understand requirements and ensure alignment with business objectives.\n\n ? \n\nExperience\n\nBPO experience in data analytics in Tech industry\n\nBachelor’s degree in Business Administration, Operations Management, Data Analytics, or a related field.\n\nIndividual contributor\n\n\n\n ? \n\n ? \nMandatory\n\nSkills:\nWFM (Ops).\n\nExperience3-5 Years.\n\nReinvent your world. We are building a modern Wipro. We are an end-to-end digital transformation partner with the boldest ambitions. To realize them, we need people inspired by reinvention. Of yourself, your career, and your skills. We want to see the constant evolution of our business and our industry. It has always been in our DNA - as the world around us changes, so do we. Join a business powered by purpose and a place that empowers you to design your own reinvention. Come to Wipro. Realize your ambitions. Applications from people with disabilities are explicitly welcome.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analysis', 'data analytics', 'presentation skills', 'analysis tools', 'wfm', 'apqp', 'business analysis', 'power bi', 'dashboards', 'business administration', 'sql', 'tableau', 'fmea', 'google data studio', 'google apps script', 'msa', 'ppap', 'data visualization', 'digital transformation']",2025-06-10 15:28:39
Business Analyst - L4,Wipro,5 - 8 years,Not Disclosed,['Hyderabad'],"Mandatory\n\nSkills:\nBusiness Analyst/ Data Analyst(Media).\n\nExperience5-8 Years.\n\nReinvent your world. We are building a modern Wipro. We are an end-to-end digital transformation partner with the boldest ambitions. To realize them, we need people inspired by reinvention. Of yourself, your career, and your skills. We want to see the constant evolution of our business and our industry. It has always been in our DNA - as the world around us changes, so do we. Join a business powered by purpose and a place that empowers you to design your own reinvention. Come to Wipro. Realize your ambitions. Applications from people with disabilities are explicitly welcome.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['project management', 'data analysis', 'business analysis', 'agile', 'digital transformation', 'digital marketing', 'program management', 'digital strategy', 'change management', 'product management', 'delivery management', 'stakeholder management', 'scrum', 'transition management']",2025-06-10 15:28:41
Business Analyst,Accenture,3 - 8 years,Not Disclosed,['Pune'],"Project Role :Business Analyst\n\n\n\nProject Role Description :Analyze an organization and design its processes and systems, assessing the business model and its integration with technology. Assess current state, identify customer requirements, and define the future state and/or business solution. Research, gather and synthesize information.\n\nMust have skills :Icertis Contract Intelligence ICI Platform Functional\n\n\nGood to have skills :NAMinimum\n\n3 year(s) of experience is required\n\n\nEducational Qualification :15 years full time education\nSummary:As a Business Analyst, you will engage in a variety of tasks that involve analyzing organizational processes and systems. Your typical day will include assessing the current state of business models, identifying customer requirements, and defining future states or business solutions. You will conduct research, gather information, and synthesize data to support decision-making and improve operational efficiency. Collaboration with various stakeholders will be essential as you work to align business needs with technological capabilities, ensuring that solutions are both effective and sustainable.\nRoles & Responsibilities:- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Facilitate workshops and meetings to gather requirements and feedback from stakeholders.- Document business processes and create detailed specifications for system enhancements.\nProfessional & Technical\n\n\nSkills:\n- Must To Have\n\n\nSkills:\nProficiency in Icertis Contract Intelligence ICI Platform Functional.- Strong analytical skills to assess business processes and identify areas for improvement.- Experience with process mapping and documentation techniques.- Ability to communicate effectively with both technical and non-technical stakeholders.- Familiarity with project management methodologies and tools.\nAdditional Information:- The candidate should have minimum 3 years of experience in Icertis Contract Intelligence ICI Platform Functional.- This position is based at our Pune office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['project management', 'business solutions', 'business analysis', 'process mapping', 'information gathering', 'data analysis', 'gap analysis', 'documentation', 'us healthcare', 'presentation skills', 'critical thinking', 'brd', 'business consulting', 'agile', 'requirement analysis', 'jira']",2025-06-10 15:28:44
TVS SCS is hiring Business Analyst - Mumbai,TVS Supply Chain Solutions (TVS SCS),3 - 8 years,Not Disclosed,['Mumbai( Andheri East )'],"Role & responsibilities\nEnsure collation, analysis of data, information, trends on critical performance indicators of the business / vertical\nMIS Planning & Decision Support: Develop templates of management reports that needs to be prepared and finalize the same with the CTOs team\nUtilise various tools, systems, models, reports to extract the required data/information.\nCommunicate final list of reports, data requirements and related frequency, to the concerned stakeholders and ensure its adherence\nCoordinate and work with concerned stakeholders to get the required data, reports, MIS as per the defined formats, accuracy and timelines\nEnsure that the management reports/presentations are prepared in the set format with proper comments, Graphs, KPIs/SLA tables, analyse critical aspects and present key highlights of performance metrics.\nIdentify key trends, deviations, exceptions and share data driven insights to identify areas of improvement and action.\nSupport the team in generating more relevant reports through data collection, analysis and interpretation.\nCustodian of CXO initiatives\nSupport CXO office in review meetings & business presentations\nFocus on special initiatives and projects\n\nPreferred candidate profile\nGraduate with MBA/Engineer with 5-10 years experience, prefer from logistics/Transport industry.\nEffectively communicate and corresponds with all major stakeholders\nAbility to plan & meet specified delivery timelines for projects\nAgility to prioritize task w.r.t. various stakeholder expectations\nPossesses strong analytical/logical thinking skills\nProficient in using Microsoft Office suite applications\nPositive attitude and openness to learning\nExpert in data crunching, analysis, attention to detail, data visualisation and modelling, presentations, report writing, coordination, follow up, excellent written English, communication skills. Advance MS Excel, PowerPoint, Word, Outlook, MS Power BI, Tableau etc.\nComfortable in working with large data, multiple systems, teams located in different geographies.\nAll reports, MIS, Presentations, analysis are accurate and submitted on time in the set formats",Industry Type: Courier / Logistics (Logistics Tech),Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Power Bi', 'Advanced Excel', 'Tableau', 'Logistics']",2025-06-10 15:28:47
Business Analyst,Accenture,15 - 20 years,Not Disclosed,['Hyderabad'],"Project Role :Business Analyst\n\n\n\nProject Role Description :Analyze an organization and design its processes and systems, assessing the business model and its integration with technology. Assess current state, identify customer requirements, and define the future state and/or business solution. Research, gather and synthesize information.\n\nMust have skills :Payments Fundamentals\n\n\nGood to have skills :Business ArchitectureMinimum\n\n7.5 year(s) of experience is required\n\n\nEducational Qualification :15 years full time education\nSummary:As a Business Analyst, you will analyze an organization and design its processes and systems, assessing the business model and its integration with technology. A typical day involves collaborating with various stakeholders to gather and synthesize information, assessing current states, and identifying customer requirements to define future states or business solutions. You will engage in research activities, ensuring that the solutions proposed align with both business needs and technological capabilities, ultimately contributing to the organization's strategic goals and operational efficiency.\nRoles & Responsibilities:- Expected to be an SME.- Collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Facilitate workshops and meetings to gather requirements and drive consensus among stakeholders.- Develop and maintain documentation that outlines business processes, requirements, and solutions.\nProfessional & Technical\n\n\nSkills:\n- Must To Have\n\n\nSkills:\nProficiency in Payments Fundamentals.- Good To Have\n\n\nSkills:\nExperience with Business Architecture.- Strong analytical skills to assess business processes and identify areas for improvement.- Ability to communicate effectively with both technical and non-technical stakeholders.- Experience in process mapping and modeling techniques.- Familiarity with project management methodologies and tools.\nAdditional Information:- The candidate should have minimum 7.5 years of experience in Payments Fundamentals.- This position is based in Hyderabad.- A 15 years full time education is required.\nExperience in Payments Domain is mandatory\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['project management', 'payments', 'business analysis', 'process mapping', 'business architecture', 'gap analysis', 'documentation', 'togaf', 'business solutions', 'enterprise architecture', 'business analytics', 'user stories', 'requirements management', 'scrum', 'agile', 'requirement analysis']",2025-06-10 15:28:49
ML OPS Engineer,Valuebound,3 - 8 years,Not Disclosed,['Chennai'],"What You ll Do\nHandle data: pull, clean, and shape structured & unstructured data.\nManage pipelines: Airflow / Step Functions / ADF your call.\nDeploy models: build, tune, and push to production on SageMaker, Azure ML, or Vertex AI.\nScale: Spark / Databricks for the heavy lifting.\nAutomate processes: Docker, Kubernetes, CI/CD, MLFlow, Seldon, Kubeflow.\nCollaborate effectively: work with engineers, architects, and business professionals to solve real problems promptly.\nWhat You Bring\n3+ years hands-on MLOps (4-5 yrs total software experience).\nProven experience with one hyperscaler (AWS, Azure, or GCP).\nConfidence with Databricks / Spark , Python, SQL, TensorFlow / PyTorch / Scikit-learn.\nExtensive experience handling and troubleshooting Kubernetes and proficiency in Dockerfile management.\nPrototyping with open-source tools, selecting the appropriate solution, and ensuring scalability.\nAnalytical thinker, team player, with a proactive attitude.\nNice-to-Haves\nSagemaker, Azure ML, or Vertex AI in production.\nDedication to clean code, thorough documentation, and precise pull requests.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Scalability', 'GCP', 'spark', 'Analytical', 'Management', 'Troubleshooting', 'Open source', 'AWS', 'SQL', 'Python']",2025-06-10 15:28:51
Engineer-Data Science,Hitachi Energy,5 - 10 years,Not Disclosed,['Vadodara'],". Analysis may be applied to various areas of the business (e.g., Market Economics, Supply Chain, Marketing/Advertising, Scientific Research, etc.). Researching and applying knowledge of existing and emerging data science principles, theories, and techniques to inform business decisions. At higher career levels, may conduct scientific research projects with the goal of breaking new ground in data analytics An Experienced Professional (P2) applies practical knowledge of job area typically obtained through advanced education and work experience. May require the following proficiency: Works independently with general supervision. Problems faced are difficult but typically not complex. May influence others within the job area through explanation of facts, policies and practices.\nHow you ll make an impact\nThe success candidate will be the part of an International Design and Engineering Team heavily specialized in Power Transformers design covering US factory.\nResponsible for building visualizations in PBI based on various sources and datasets of power transformers factories\nResponsible for DAX queries / DAX functions / Power Query Editor\nResponsible for development of transformer dashboard in coordination with global Hitachi Energy factory based on requirement.\nExpertise in using advance level calculations on the data set.\nAble to develop tabular and multidimensional models that are compatible with warehouse standards.\nAble to properly understand the business requirements and develop data models accordingly by taking care of the resources.\nFamiliar with Row Level Security (RLS)\nBasic knowledge and skills for secondary tools such as Microsoft Azure, SQL data warehouse, SSAS, Visual Studio, Power Apps etc.\nLiving Hitachi Energy s core values of safety and integrity, which means taking responsibility for your own actions while caring for your colleagues and the business.\nResponsible to ensure compliance with applicable external and internal regulations, procedures, and guidelines.\nYour Background\nBachelor s degree of Electrical or Mechanical or Data Science Engineering.\n5 - 10 years experience working in Data Analytics from start to end process. Candidates with higher experience also to be considered.\nExperience of manufacturing industry is an additional advantage.\nExtended MS Office knowledge & skills, especially excel but also eg PowerPoint, etc.\nExperienced in building MS Teams Space & SharePoint pages.\nSpecialist on building visualizations in PBI based on various sources and datasets.\nStrong capabilities of DAX queries / DAX functions / Power Query Editor. DA-100 certification preferred.\nExperience with SAP / S4 HANA -Data handling preferred, data sources in Power BI.\nProficiency in both spoken & written English language is required.\n.",Industry Type: Power,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Supply chain', 'Data analysis', 'SAP', 'Data analytics', 'Visual Studio', 'Editor', 'MS Office', 'Data mining', 'Business intelligence', 'SQL']",2025-06-10 15:28:53
ML-GenAI Engg,Tech Mahindra,8 - 13 years,Not Disclosed,['Hyderabad'],"Generative AI (GenAI) EngineerJob Summary:We are seeking a talented and highly motivated Generative AI (GenAI) Engineer\nAs a GenAI Engineer, you will be at the forefront of developing and deploying innovative solutions leveraging cutting-edge generative models\nYou will also be responsible for building, training, and fine-tuning GenAI models for various applications, from text generation, image synthesis, code generation, etc\nThis role requires a strong foundation in machine learning, deep learning, and a passion for exploring the potential of generative AI\nQualifications:Education: Masters or PhD degree\nGood to have any certification in Artificial Intelligence, Machine Learning, or a related field\nExperience: 8+ years of experience in developing and deploying machine learning models\n2+ years of experience with generative models\nExperience with cloud platforms such as AWS, Azure, or GCP\nSkills: Technical Expertise: Strong understanding of generative AI models, such as GANs, VAEs, diffusion models, and large language models\nProficiency in Python and other programming languages commonly used in machine learning\nExperience with model training and fine-tuning techniques\nKnowledge of data preprocessing and feature engineering methods\nFamiliarity with model deployment and monitoring tools\nSoft Skills: Strong analytical and problem-solving skills\nExcellent communication, presentation, and interpersonal skills\nAbility to work independently and as part of a team\nCreativity and a passion for exploring the potential of generative AI",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['deep learning', 'Interpersonal skills', 'Monitoring tools', 'GCP', 'Analytical', 'Artificial Intelligence', 'Machine learning', 'Programming', 'Deployment', 'Python']",2025-06-10 15:28:56
Business Analyst,Software Company,5 - 8 years,Not Disclosed,['Hyderabad'],Business Analyst :\n\nMust have Strong Communication with Good and quick understanding of Product Vision and Domain.\nAWS Cloud knowledge is good to have\n\nLocation : Hyderabad\nExperience - 5 to 8\nNotice Period : Immediate joiners,Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Business Analysis', 'Product Vision', 'Aws Cloud', 'Requirement Gathering', 'User Stories', 'product domain', 'Requirement Analysis', 'Product Ownership']",2025-06-10 15:28:58
Data Engineer-Data Modelling,IBM,2 - 5 years,Not Disclosed,['Pune'],"Translates business needs into a data model, providing expertise on data modeling tools and techniques for designing data models for applications and related systems.\nSkills include logical and physical data modeling, and knowledge of ERWin, MDM, and/or ETL.\nData modeling is a process used to define and analyze data requirements needed to support the business processes within the scope of corresponding information systems in organizations\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nTranslates business needs into a data model, providing expertise on data modeling tools and techniques for designing data models for applications and related systems.\nSkills include logical and physical data modeling, and knowledge of ERWin, MDM, and/or ETL.\nData modeling is a process used to define and analyze data requirements needed to support the business processes within the scope of corresponding information systems in organizations.\nTherefore, the process of data modeling involves professional data modelers working closely with business stakeholders, as well as potential users of the information system.\nThere are three different types of data models produced while progressing from requirements to the actual database to be used for the information system.\n\n\nPreferred technical and professional experience\nTranslates business needs into a data model, providing expertise on data modeling tools and techniques for designing data models for applications and related systems.\nSkills include logical and physical data modeling, and knowledge of ERWin, MDM, and/or ETL.\nData modeling is a process used to define and analyze data requirements needed to support the business processes within the scope of corresponding information systems in organizations.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['erwin', 'data engineering', 'data modeling', 'mdm', 'etl', 'hive', 'python', 'oracle', 'data analysis', 'data warehousing', 'power bi', 'microsoft azure', 'business intelligence', 'sql server', 'sql', 'plsql', 'tableau', 'unix shell scripting', 'spark', 'hadoop', 'big data', 'aws', 'informatica', 'unix']",2025-06-10 15:29:00
Data Engineer-Hive/Impala/Kudo,IBM,15 - 20 years,Not Disclosed,['Mumbai'],"Location Mumbai\n\n Role Overview :\nAs a Big Data Engineer, you'll design and build robust data pipelines on Cloudera using Spark (Scala/PySpark) for ingestion, transformation, and processing of high-volume data from banking systems.\n\n Key Responsibilities :\nBuild scalable batch and real-time ETL pipelines using Spark and Hive\nIntegrate structured and unstructured data sources\nPerform performance tuning and code optimization\nSupport orchestration and job scheduling (NiFi, Airflow)\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nExperience3–15 years\nProficiency in PySpark/Scala with Hive/Impala\nExperience with data partitioning, bucketing, and optimization\nFamiliarity with Kafka, Iceberg, NiFi is a must\nKnowledge of banking or financial datasets is a plus",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['hive', 'scala', 'impala', 'apache nifi', 'kafka', 'cloudera', 'amazon redshift', 'pyspark', 'data warehousing', 'sql', 'etl pipelines', 'apache', 'spark', 'linux', 'hadoop', 'big data', 'etl', 'hbase', 'python', 'performance tuning', 'oozie', 'airflow', 'data engineering', 'nosql', 'mapreduce', 'sqoop', 'aws', 'unix']",2025-06-10 15:29:03
Data Engineer-Talend DQ,IBM,10 - 15 years,Not Disclosed,['Mumbai'],"Role Overview :\n We are hiring aTalend Data Quality Developerto design and implement robust data quality (DQ) frameworks in a Cloudera-based data lakehouse environment. The role focuses on building rule-driven validation and monitoring processes for migrated data pipelines, ensuring high levels of data trust and regulatory compliance across critical banking domains. \n\n Key Responsibilities :\nDesign and implement data quality rules using Talend DQ Studio , tailored to validate customer, account, transaction, and KYC datasets within the Cloudera Lakehouse.\nCreate reusable templates for profiling, validation, standardization, and exception handling.\nIntegrate DQ checks within PySpark-based ingestion and transformation pipelines targeting Apache Iceberg tables .\nEnsure compatibility with Cloudera components (HDFS, Hive, Iceberg, Ranger, Atlas) and job orchestration frameworks (Airflow/Oozie).\n\n\nPerform initial and ongoing data profiling on source and target systems to detect data anomalies and drive rule definitions.\nMonitor and report DQ metrics through dashboards and exception reports.\nWork closely with data governance, architecture, and business teams to align DQ rules with enterprise definitions and regulatory requirements.\nSupport lineage and metadata integration with tools like Apache Atlas or external catalogs.\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\n\n Experience 5–10 years in data management, with 3+ years in  Talend Data Quality  tools.\n\n Platforms Experience in  Cloudera Data Platform (CDP) , with understanding of  Iceberg ,  Hive ,  HDFS , and  Spark ecosystems.\n\n Languages/Tools Talend Studio (DQ module), SQL, Python (preferred), Bash scripting.\n\n Data Concepts Strong grasp of data quality dimensions—completeness, consistency, accuracy, timeliness, uniqueness.\n\n Banking Exposure Experience with financial services data (CIF, AML, KYC, product masters) is highly preferred.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data management', 'talend', 'data quality', 'spark', 'quality tools', 'hive', 'cloudera', 'python', 'metadata', 'data validation', 'oozie', 'airflow', 'financial services', 'data engineering', 'dashboards', 'sql', 'apache', 'data governance', 'apache atlas', 'bash', 'hadoop', 'bash scripting', 'data profiling']",2025-06-10 15:29:05
Insight Analyst,AMS India,5 - 7 years,3-5.5 Lacs P.A.,['Pune'],"Understanding client's requirement and selecting keywords from given job description.\nCreate Boolean Search strings with the help of keywords to source from various social media tools such as Naukri.com, LinkedIn Recruiter, Google, X-Ray search.\nConduct market research, talent landscaping using variety of online public/private portals.\nTap various externally available (e.g. social media) talent market and competitor databases to produce analytic that drive actionable market insights.\nPlay a consultative role in preparing and presenting market analysis, research findings and data insights visually to internal and external audiences.\nLead research initiatives focused on a range of project types across Talent Acquisition including Talent Availability, Location, Gender diversity, Industry, Competitor Mapping, Salary Benchmarks, etc.\nDeveloped insights that are practical and actionable, grounded in contemporary Talent Acquisition workflows.\nUndertaking Research studies and providing insights to clients and helping them understand the markets in a much more efficient and productive manner.",Industry Type: Recruitment / Staffing,Department: Marketing & Communication,"Employment Type: Full Time, Permanent","['Talent Research', 'Internet Research', 'Salary Benchmarking', 'Talent Intelligence', 'Market Research', 'Industry Mapping', 'Research Analysis', 'Industry Research', 'Research', 'Industry Analysis', 'Competitor Analysis', 'Talent insights', 'Company Mapping', 'Market Mapping', 'Market Analysis']",2025-06-10 15:29:07
Ml Engineer,Solix Technologies,5 - 10 years,15-25 Lacs P.A.,['Hyderabad'],"Solix Technologies Inc. is a leading provider of big data applications for enterprise archiving, data privacy, and advanced analytics. We are on a mission to help organizations manage and leverage their data for maximum value, efficiency, and compliance.\nJob Summary:\nWe are seeking a highly skilled and motivated Machine Learning Engineer with hands-on experience in Natural Language Processing (NLP) and Large Language Models (LLMs). You will play a key role in designing, developing, and deploying scalable ML/NLP solutions that drive intelligent automation and data insight across our platforms.\nKey Responsibilities:\nDesign and develop machine learning models, particularly in the domain of NLP and LLMs.\nFine-tune, evaluate, and deploy transformer-based models (e.g., BERT, GPT, T5, LLaMA, etc.).\nApply techniques such as named entity recognition (NER), text classification, semantic search, summarization, and question answering.\nWork with large-scale datasets to extract insights and build data pipelines.\nCollaborate with cross-functional teams including data engineers, product managers, and software developers.\nConduct experiments, model training, and optimization to improve accuracy and performance.\nStay up-to-date with the latest research in NLP, LLMs, and machine learning.\nRequired Skills and Experience:\nBachelor's or Masters degree in Computer Science, Data Science, AI/ML, or a related field.\nMinimum 5+ years of hands-on experience with NLP and LLMs\nProficient in Python and ML frameworks like TensorFlow, PyTorch, Hugging Face Transformers.\nStrong understanding of modern NLP techniques (tokenization, embeddings, attention mechanisms, etc.).\nExperience with ML lifecycle including model development, evaluation, and deployment (MLOps).\nFamiliarity with data handling libraries (Pandas, NumPy) and cloud platforms (AWS, GCP, or Azure).\nGood understanding of data preprocessing, feature engineering, and model validation techniques.\nExperience with open-source LLM fine-tuning and deployment.\nKnowledge of vector databases (e.g., FAISS, Pinecone) and retrieval-augmented generation (RAG).\nPrior experience with large-scale data systems or enterprise data environments.\nPublished papers or open-source contributions in the ML/NLP space.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['LLm', 'Large Language Model', 'Natural Language Processing', 'Ml Algorithms']",2025-06-10 15:29:10
Data Engineer,Grid Dynamics,7 - 12 years,15-25 Lacs P.A.,"['Hyderabad', 'Chennai', 'Bengaluru']","We are seeking a highly motivated and technically proficient Big Data Engineer to join our innovative team and contribute to the development of a next-generation Big Data platform. This is an exciting opportunity to work on cutting-edge solutions handling petabyte-scale datasets for one of the worlds largest technology companies headquartered in Silicon Valley.\nKey Responsibilities\nDesign and develop scalable Big Data analytical applications.\nBuild and optimize complex ETL pipelines and data processing frameworks.\nImplement large-scale, near real-time streaming data processing systems.\nContinuously enhance and support the project’s codebase, CI/CD pipelines, and deployment infrastructure.\nCollaborate with a team of top-tier engineers to build highly performant and resilient data systems using the latest Big Data technologies.\nRequired Qualifications\nStrong programming skills in Scala, Java, or Python (Scala preferred).\nHands-on experience with Apache Spark, Hadoop, and Hive.\nProficiency in stream processing technologies such as Kafka, Spark Streaming, or Akka Streams.\nSolid understanding of data quality, validation, and quality engineering practices.\nExperience with Git and version control best practices.\nAbility to rapidly learn and apply new tools, frameworks, and technologies.\nPreferred Qualifications\nStrong experience with AWS Cloud services (e.g., EMR, S3, Lambda, Glue, Redshift).\nFamiliarity with Unix-based operating systems and shell scripting (bash, ssh, grep, etc.).\nExperience with GitHub-based development workflows and pull request processes.\nKnowledge of JVM-based build tools such as SBT, Maven, or Gradle.\nWhat We Offer\nOpportunity to work on bleeding-edge Big Data projects with global impact.\nA collaborative and intellectually stimulating environment.\nCompetitive compensation and performance-based incentives.\nFlexible work schedule.\nComprehensive benefits package including medical insurance and fitness programs.\nRegular corporate social events and team-building activities.\nOngoing professional growth and career development opportunities.\nAccess to a modern and well-equipped office space.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SCALA', 'Hadoop', 'Big Data', 'Spark', 'AWS', 'Hive']",2025-06-10 15:29:13
Business Analyst,Snow Planet,2 - 6 years,Not Disclosed,['Hyderabad'],"The Sales Operations Analyst will support the forecasting process and advise sales leaders across geographies\nAttend regional forecast calls and perform pipeline analysis to ensure forecast accuracy.\nSupport sales organisation s requirement by addressing their queries related to quota, commissions, pipeline and related metrics\nPrepare daily/weekly/quarterly status report for sales leaders to make strategic decisions\nProvide support by looking into relevant KPIs for Quarterly Business Reviews and Annual Planning\n\n\nTo be successful in this role you have:\n2-6 years of proven experience of Sales Operations in a software sales organisation\nOrganised, with excellent attention to detail and the ability to work in a fast paced environment\nProficient in PPT, Word, and Excel\nExperience with a CRM systems and tools like Anaplan, PowerBI, etc.\nHas been in a role supporting Sales Reps and Sales Territories\nStrong listening, analytic and organizational skills\nAbility to prioritize to meet business needs\nEffective communicator, both written and verbal\nTeam Player with positive attitude ready to work in a fast paced environment\nResults Driven",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Sales', 'Excel', 'Sales operations', 'Business Analyst', 'Software sales', 'Sales Operations Analyst', 'Powerpoint', 'Forecasting', 'Analytics', 'CRM']",2025-06-10 15:29:15
Data analyst,Kbros Aristo,1 - 6 years,Not Disclosed,"['Kolkata', 'Mumbai', 'New Delhi', 'Hyderabad', 'Pune', 'Chennai', 'Bengaluru']","Job_Description"":""\nKey Responsibilities\n1. Database Management & Optimization\nDesign, implement, and maintain robust database systems to store and retrieve data\nefficiently.\nMonitor database performance, identify areas for improvement, and implement\nnecessary optimizations.\nEnsure data accuracy, integrity, and security across all database systems.\nDevelop and maintain documentation related to database configurations, processes, and\nprocedures.\n\n2. Data Collection from Advertising Platforms\nUtilize tools and APIs to extract data from Google Ads and Meta Ads platforms.\nImplement tracking mechanisms, such as UTM parameters, to monitor ad performance.\nIntegrate advertising data into centralized databases for comprehensive analysis.\n\n3. Data Analysis & Reporting\nAnalyze advertising data to assess campaign performance, user engagement, and ROI.\nCreate visualizations and dashboards to communicate insights to stakeholders\nCollaborate with marketing and business teams to align data analysis with strategic\ngoals.\n\n4. AI Integration & Automation\nExplore and implement AI tools to automate routine data analysis tasks.\nStay updated on emerging AI technologies that can enhance data processing and\ninterpretation.\n\nQualifications\nEducation & Experience\nBachelors degree in Computer Science, Information Technology, or a related field.\nMinimum of 1 year of experience in database management and data analysis.\n\nTechnical Skills\nProficiency in Power BI, Tableau, SQL and experience with database systems like\nMySQL, PostgreSQL, or Oracle.\nFamiliarity with data extraction tools and APIs for Google Ads and Meta Ads.\nExperience with data visualization tools such as Tableau, Power BI, or Google Data\nStudio.\nKnowledge of scripting languages like Python or R for data manipulation.\n\nSoft Skills\nStrong analytical and problem-solving abilities.\nExcellent communication skills to convey technical information to non-technical\nstakeholders.\nAbility to work collaboratively in a team-oriented environment.\n\n"",""",Industry Type: Furniture & Furnishing,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Automation', 'Data analysis', 'Postgresql', 'Analytical', 'MySQL', 'Oracle', 'Information technology', 'SQL', 'Python']",2025-06-10 15:29:18
Data Governance - Analyst,RBL FinServe,4 - 6 years,5-11 Lacs P.A.,"['Thane', 'Navi Mumbai', 'Mumbai (All Areas)']","Role & responsibilities\nSupport the implementation and maintenance of data governance policies, procedures, and standards specific to the banking industry.\nHands-on experience in creating and maintaining activities associated with data life cycle management and various data governance activities.\nDevelop, update, and maintain the data dictionary for critical banking data assets, ensuring accurate definitions, attributes, and classifications.\nInterfacing Work with business units and IT teams to standardize terminology across systems for consistency and clarity.",,,,"['Data Lineage', 'Data Quality Management', 'Data Governance Analyst', 'Data Analytics', 'Data Governance', 'Data Integrity', 'ADF', 'data management life cycle', 'Data Management', 'Data Management And Analysis', 'Data Quality', 'Data Analysis', 'analyst']",2025-06-10 15:29:20
Data Analyst-Having Stratup-Mid-Size companies Exp.@ Bangalore_Urgent,"A leader in this space, we deliver world...",8 - 13 years,Not Disclosed,['Bengaluru'],"Data Analyst\n\nLocation: Bangalore\nExperience: 8 - 15 Yrs\nType: Full-time\n\nRole Overview\n\nWe are seeking a skilled Data Analyst to support our platform powering operational intelligence across airports and similar sectors. The ideal candidate will have experience working with time-series datasets and operational information to uncover trends, anomalies, and actionable insights. This role will work closely with data engineers, ML teams, and domain experts to turn raw data into meaningful intelligence for business and operations stakeholders.\n\nKey Responsibilities\n\nAnalyze time-series and sensor data from various sources\nDevelop and maintain dashboards, reports, and visualizations to communicate key metrics and trends.\nCorrelate data from multiple systems (vision, weather, flight schedules, etc) to provide holistic insights.\nCollaborate with AI/ML teams to support model validation and interpret AI-driven alerts (e.g., anomalies, intrusion detection).\nPrepare and clean datasets for analysis and modeling; ensure data quality and consistency.\nWork with stakeholders to understand reporting needs and deliver business-oriented outputs.\n\n\nQualifications & Required Skills\n\nBachelors or Masters degree in Data Science, Statistics, Computer Science, Engineering, or a related field.\n5+ years of experience in a data analyst role, ideally in a technical/industrial domain.\nStrong SQL skills and proficiency with BI/reporting tools (e.g., Power BI, Tableau, Grafana).\nHands-on experience analyzing structured and semi-structured data (JSON, CSV, time-series).\nProficiency in Python or R for data manipulation and exploratory analysis.\nUnderstanding of time-series databases or streaming data (e.g., InfluxDB, Kafka, Kinesis).\nSolid grasp of statistical analysis and anomaly detection methods.\nExperience working with data from industrial systems or large-scale physical infrastructure.\n\n\nGood-to-Have Skills\n\nDomain experience in airports, smart infrastructure, transportation, or logistics.\nFamiliarity with data platforms (Snowflake, BigQuery, Custom-built using open-source).\nExposure to tools like Airflow, Jupyter Notebooks and data quality frameworks.\nBasic understanding of AI/ML workflows and data preparation requirements.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Kafka', 'SQL', 'airports', 'InfluxDB', 'Airflow', 'structured Data', 'time-series', 'JSON', 'Tableau', 'Grafana', 'R', 'AI/ML', 'Kinesis', 'Snowflake', 'time-series databases', 'Data Preparation', 'Python', 'smart infrastructure', 'BigQuery', 'streaming data', 'Power BI', 'CSV', 'transportation', 'logistic', 'reporting tools']",2025-06-10 15:29:22
Data Engineer,Acenet,5 - 7 years,Not Disclosed,"['Pune', 'Gurugram', 'Bengaluru']","About Us:\nJob Summary :\nWe are seeking a highly skilled individual to join our team as a Data Engineering/Operations Specialist. This role will be responsible for maintaining and evolving data pipeline architecture, orchestrating new data sources for further processing, and ensuring the up-to-date documentation of pipelines and data feeds.\nKey Responsibilities:\n*Maintain, upgrade and evolve data pipeline architectures to ensure optimal performance and scalability.\n*Orchestrate the integration of new data sources into existing pipelines for further processing and analysis.\n*Keep documentation up to date for pipelines and data feeds to facilitate smooth operations and collaboration within the team.\n*Collaborate with cross-functional teams to understand data requirements and optimize pipeline performance accordingly.\n*Troubleshoot and resolve any issues related to pipeline architecture and data processing.\nRole Requirements and Qualifications:\n*Experience with Cloud platform for deployment and management of data pipelines.\n*Familiarity with AWS / Azure for efficient data processing workflows.\n*Experience with constructing FAIR data products is highly desirable.\n*Basic understanding of computational clusters to optimize pipeline performance.\n*Prior experience in data engineering or operations roles, preferably in a cloud-based environment.\n*Proven track record of successfully maintaining and evolving data pipeline architectures.\n*Strong problem-solving skills and ability to troubleshoot technical issues independently.\n*Excellent communication skills to collaborate effectively with cross-functional teams.\nWhy Join Us:\n*Opportunities to work on transformative projects, cutting-edge technology and innovative solutions with leading global firms across industry sectors.\n*Continuous investment in employee growth and professional development with a strong focus on up & re-skilling.\n*Competitive compensation & benefits, ESOPs and international assignments.\n*Supportive environment with healthy work-life balance and a focus on employee well-being.\n*Open culture that values diverse perspectives, encourages transparent communication and rewards contributions.\nHow to Apply:\nIf you are interested in joining our team and meet the qualifications listed above, please apply and submit your resume highlighting why you are the ideal candidate for this position.",Industry Type: Management Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Supply chain', 'Architecture', 'Manager Technology', 'Healthcare', 'Data processing', 'Business strategy', 'Fund raising', 'Troubleshooting', 'Financial services', 'Logistics']",2025-06-10 15:29:25
Data Analyst,Talent Hire It Solutions,5 - 10 years,Not Disclosed,"['Kochi', 'Thiruvananthapuram']","Collaborate with business stakeholders to understand data needs and translate them into analytical\nrequirements.\nAnalyze large datasets to uncover trends, patterns, and actionable insights.\nDesign and build dashboards and reports using Power BI.\nPerform ad-hoc analysis and develop data-driven narratives to support decision-making.\nEnsure data accuracy, consistency, and integrity through data validation and quality checks.\nBuild and maintain SQL queries, views, and data models for reporting purposes.\nCommunicate findings clearly through presentations, visualizations, and written summaries.\nPartner with data engineers and architects to improve data pipelines and architecture.\nContribute to the definition of KPIs, metrics, and data governance standards.\n\nJob Specification / Skills and Competencies\nBachelors or Master’s degree in Statistics, Mathematics, Computer Science,\nEconomics, or a related field.\n5+ years of experience in a data analyst or business intelligence role.\nAdvanced proficiency in SQL and experience working with relational databases (e.g.,\nSQL Server, Redshift, Snowflake).\n\nJob Description\n\n2\n\nHands-on experience in Power BI.\nProficiency in Python, Excel and data storytelling.\nUnderstanding of data modelling, ETL concepts, and basic data architecture.\nStrong analytical thinking and problem-solving skills.\nExcellent communication and stakeholder management skills\nTo adhere to the Information Security Management policies and procedures.\n\nSoft Skills Required\nMust be a good team player with good communication skills\nMust have good presentation skills\nMust be a pro-active problem solver and a leader by self\nManage & nurture a team of data engineersRole & responsibilities\n\n\nPreferred candidate profile",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['SQL', 'Power BI', 'Amazon Athena', 'Python']",2025-06-10 15:29:27
Data Engineer,Grid Dynamics,8 - 13 years,Not Disclosed,['Hyderabad'],"Role & responsibilities Details on tech stack\ndatabricks, python, pyspark, Snowflake, SQL\nMin requirements to the candidate\nAdvanced SQL queries, scripts, stored procedures, materialized views, and views\nFocus on ELT to load data into database and perform transformations in database\nAbility to use analytical SQL functions\nSnowflake experience \nCloud Data Warehouse solutions experience (Snowflake, Azure DW, or Redshift); data modeling, analysis, programming\nExperience with DevOps models utilizing a CI/CD tool\nWork in hands-on Cloud environment in Azure Cloud Platform (ADLS, Blob)\nAirflow\nGD Requirements \nGood interpersonal skills; comfort and competence in dealing with different teams within the organization.\nRequires an ability to interface with multiple constituent groups and build sustainable relationships.\nStrong and effective communication skills (verbal and written).\nStrong analytical, problem-solving skills.\nExperience of working in a matrix organization.\nProactive problem solver.\nAbility to prioritize and deliver.\nResults-oriented, flexible, adaptable.\nWork well independently and lead a team.\nVersatile, creative temperament, ability to think out-of-the box while defining sound and practical solutions.\nAbility to master new skills.\nFamiliar with Agile practices and methodologies\nProfessional data engineering experience focused on batch and real-time data pipelines using Spark, Python, SQL\nData warehouse (data modeling, programming)\nExperience working with Snowflake\nExperience working on a cloud environment, preferably, Microsoft Azure Cloud Data Warehouse solutions (Snowflake, Azure DW)\n\nPreferred candidate profile",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Data Factory', 'Pyspark', 'Azure Data Engineering', 'Azure Databricks', 'Python', 'Airflow', 'snowflake', 'airflo', 'SQL']",2025-06-10 15:29:29
Avaloq Business Analyst,Barclays,2 - 7 years,Not Disclosed,['Pune'],"Join us as a Avaloq Business Analyst at Barclays, where youll take part in the evolution of our digital landscape, driving innovation and excellence. Youll harness cutting-edge technology to revolutionize our digital offerings, ensuring unparalleled customer experiences. As a part of the team, you will deliver technology stack, using strong analytical and problem solving skills to understand the business requirements and deliver quality solutions. Youll be working on complex technical problems that will involve detailed analytical skills and analysis. This will be done in conjunction with fellow engineers, business analysts and business stakeholders.\nTo be successful as a Avaloq Business Analyst you should have experience with:\nWorking knowledge on any banking products. Experience with Avaloq or alternatively in other core banking solutions (e.g. Finacle, T24, BaNCS etc.)\nSoftware development lifecycle.\nGood to have working knowledge of JIRA, Confluence, Visio, Service Now and Agile.\nIn depth knowledge of and proven track record in financial services industry working in complex business and architecture environment .\nExcellent overall knowledge of banking application and in depth knowledge of several of the functional modules like security trading, order management, settlement, corporate actions, credit, payments etc.\nStrong analytical and problem solving skills.\nExcellent verbal and written communication\nSome other highly valued skills include:\nMasters or Bachelors Degree (preferably in Computer Science/Engineering) .\nHands on experience in application development experience, of Oracle PL/SQL in banking applications/ products (Avaloq knowledge would be an added advantage).\nExperience in large IT organisation working in a support environment, supporting mission critical complex systems utilizing a large number of technologies and heavy reliance on batch processing.\nOracle PL/SQL / Avaloq script Knowledge of UNIX , Agile Central , Jira and SVN .\nYou may be assessed on key critical skills relevant for success in role, such as risk and controls, change and transformation, business acumen, strategic thinking and digital and technology, as well as job-specific technical skills.\nThis role is based in Pune.\nPurpose of the role\nTo support the organisation, achieve its strategic objectives by the identification of business requirements and solutions that address business problems and opportunities.\nAccountabilities\nIdentification and analysis of business problems and client requirements that require change within the organisation.\nDevelopment of business requirements that will address business problems and opportunities.\nCollaboration with stakeholders to ensure that proposed solutions meet their needs and expectations.\nSupport the creation of business cases that justify investment in proposed solutions.\nConduct feasibility studies to determine the viability of proposed solutions.\nSupport the creation of reports on project progress to ensure proposed solutions are delivered on time and within budget.\nCreation of operational design and process design to ensure that proposed solutions are delivered within the agreed scope.\nSupport to change management activities, including development of a traceability matrix to ensure proposed solutions are successfully implemented and embedded in the organisation.\nAssistant Vice President Expectations\nTo advise and influence decision making, contribute to policy development and take responsibility for operational effectiveness. Collaborate closely with other functions/ business divisions.\nLead a team performing complex tasks, using well developed professional knowledge and skills to deliver on work that impacts the whole business function. Set objectives and coach employees in pursuit of those objectives, appraisal of performance relative to objectives and determination of reward outcomes\nIf the position has leadership responsibilities, People Leaders are expected to demonstrate a clear set of leadership behaviours to create an environment for colleagues to thrive and deliver to a consistently excellent standard. The four LEAD behaviours are: L - Listen and be authentic, E - Energise and inspire, A - Align across the enterprise, D - Develop others.\nOR for an individual contributor, they will lead collaborative assignments and guide team members through structured assignments, identify the need for the inclusion of other areas of specialisation to complete assignments. They will identify new directions for assignments and/ or projects, identifying a combination of cross functional methodologies or practices to meet required outcomes.\nConsult on complex issues; providing advice to People Leaders to support the resolution of escalated issues.\nIdentify ways to mitigate risk and developing new policies/procedures in support of the control and governance agenda.\nTake ownership for managing risk and strengthening controls in relation to the work done.\nPerform work that is closely related to that of other areas, which requires understanding of how areas coordinate and contribute to the achievement of the objectives of the organisation sub-function.\nCollaborate with other areas of work, for business aligned support areas to keep up to speed with business activity and the business strategy.\nEngage in complex analysis of data from multiple sources of information, internal and external sources such as procedures and practises (in other areas, teams, companies, etc).to solve problems creatively and effectively.\nCommunicate complex information. Complex information could include sensitive information or information that is difficult to communicate because of its content or its audience.\nInfluence or convince stakeholders to achieve outcomes.",Industry Type: Financial Services,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Order management', 'Unix', 'Process design', 'Computer science', 'Change management', 'Corporate actions', 'Analytical', 'Business strategy', 'Operations', 'Financial services']",2025-06-10 15:29:31
CXO Analyst,Leading Client,1 - 4 years,Not Disclosed,"['Mumbai', 'Pune', 'Bengaluru']","Location - Bangalore, Mumbai, Pune, Chennai, Hyderabad, Kolkata, Delhi\n\nTechnical Skills:\nData strategist with expertise in Product Growth, Conversion Rate Optimization (experiments) Personalisation using digital analytics skills such as Adobe Analytics, Google Analytics, and others (Quantitative and Qualitative)Proven success in acquisition marketing and retention marketing by leveraging strategic and tactical implementation of Conversion Rate Optimization (A/B Testing) and Product Optimization (Landing Page Optimization, Product-Market Fit) through data insights.Personalisation - It includes customer segmentation, targeted messaging, dynamic content, recommendations, behavioral targeting, and AI-powered personalization.Skilled in leveraging advanced analytics tools for actionable data extraction from extensive datasets such as SQL, Big Query, Excel, Python for data analysis, Power BI, and Data StudioProficient in implementing digital analytics measurement across diverse domains using tools such as Adobe Launch, Google Tag Manager, and TealiumSoft skills:Experience in client-facing projects & stakeholder mgmt, excellent communication skillsCollaborative team player, aligning product vision with business objectives cross-functionallyAvid learner, staying current with industry trends and emerging technologiesCommitted to delivering measurable results through creative thinking, exceeding performance metrics, and fostering growth",Industry Type: Recruitment / Staffing,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['CXO', 'digital analytics', 'python', 'data extraction', 'google tag manager', 'adobe launch', 'adobe analytics', 'data analysis', 'google analytics', 'power bi', 'bigquery', 'SQL']",2025-06-10 15:29:33
Data Engineer Oracle Analytics Cloud & OCI Big Data,Malomatia,7 - 12 years,25-35 Lacs P.A.,['Pune'],"Job Title: Data Engineer Oracle Analytics Cloud & OCI Big Data\nLocation: Remote / Contract\nExperience Level: Mid to Senior (6+ years)\nJob Summary:\nWe are seeking a skilled and experienced Data Engineer with a minimum of 5 years of hands-on experience in building and optimizing data solutions on Oracle Analytics Cloud (OAC), OCI Data Flow, and OCI Big Data platforms. The ideal candidate will play a critical role in designing, developing, and maintaining scalable data pipelines and analytics solutions that support data lakehouse, enterprise reporting and advanced data analytics use cases.\nKey Responsibilities:\nDesign and implement robust data ingestion and transformation pipelines using OCI Data Flow and Big Data services.\nDevelop data models, datasets, and reports within Oracle Analytics Cloud to support business intelligence and analytics needs.\nIntegrate data from multiple sources (structured and unstructured) into cloud-based data platforms using Oracle-native tools and services.\nEnsure data quality and consistency through standardized processes and validation routines.\nOptimize performance of ETL/ELT jobs and troubleshoot data-related issues in Oracle Cloud Infrastructure environments.\nMaintain data platform infrastructure using OCI-native services and automation tools.\nDocument technical processes, data models, and data pipeline architecture.\nRequired Qualifications:\nBachelor’s in Computer Science, Information Systems, Engineering, or a related field.\nMinimum 5 years of hands-on experience in data engineering, with proven expertise in Oracle Analytics Cloud (OAC), OCI Data Flow, and OCI Big Data.\nExperience with Oracle Autonomous Data Warehouse and Data Lakehouse architecture.\nStrong understanding of Oracle Cloud Infrastructure (OCI) and related data services.\nExperience with Spark, PySpark, Hive, and Big Data ecosystems.\nSolid experience in SQL, PL/SQL, and data wrangling using scripting languages (e.g., Python).\nExperience in ETL/ELT development, performance tuning, and data pipeline optimization.\nFamiliarity with data security, privacy, and governance best practices in cloud environments.\nExcellent problem-solving skills and ability to work in a fast-paced, team-oriented environment.\nKnowledge of DevOps practices and CI/CD tools in the context of data platform operations.\nExposure to data cataloging and metadata management tools.\nMust have:\n5+ years hands on data engineering experience using Oracle Analytics Cloud, OCI Data Flow, OCI Big Data, Oracle Autonomous Data Warehouse\nKnowledge of DevOps practices and CI/CD\nCertifications (Nice to Have):\nOracle Cloud Infrastructure (OCI) Data Engineer or Architect Certification\nOracle Analytics Cloud Certification",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Oac', 'OCI Data Flow', 'OCI Big Data']",2025-06-10 15:29:35
Data Engineer,Grid Dynamics,8 - 13 years,Not Disclosed,['Hyderabad'],"Job Description:\nAdvanced SQL queries, scripts, stored procedures, materialized views, and views\nFocus on ELT to load data into database and perform transformations in database\nAbility to use analytical SQL functions\nSnowflake experience \nCloud Data Warehouse solutions experience (Snowflake, Azure DW, or Redshift); data modeling, analysis, programming\nExperience with DevOps models utilizing a CI/CD tool\nWork in hands-on Cloud environment in Azure Cloud Platform (ADLS, Blob)\nAirflow\n\nPreferred candidate profile\nGood interpersonal skills; comfort and competence in dealing with different teams within the organization.\nRequires an ability to interface with multiple constituent groups and build sustainable relationships.\nStrong and effective communication skills (verbal and written).\nStrong analytical, problem-solving skills.\nExperience of working in a matrix organization.\nProactive problem solver.\nAbility to prioritize and deliver.\nResults-oriented, flexible, adaptable.\nWork well independently and lead a team.\nVersatile, creative temperament, ability to think out-of-the box while defining sound and practical solutions.\nAbility to master new skills.\nFamiliar with Agile practices and methodologies\nProfessional data engineering experience focused on batch and real-time data pipelines using Spark, Python, SQL\nData warehouse (data modeling, programming)\nExperience working with Snowflake\nExperience working on a cloud environment, preferably, Microsoft Azure Cloud Data Warehouse solutions (Snowflake, Azure DW)",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Airflow', 'Snowflake', 'Azure Databricks', 'Pyspark', 'Spark', 'Python']",2025-06-10 15:29:37
Business Analyst _Payment Swift _MT/MX/GPP_6-14Yrs,Leading Software Technologies,6 - 11 years,15-30 Lacs P.A.,"['Pune', 'Chennai', 'Bengaluru']","Strong understanding of Global Payments Processing and E2E Payment workflows, SWIFT Cross boarder processing (MT & MX) & ISO 20022 Message Processing, Clearing & Settlement Systems process, Payments Interface data mapping and solution design,E2E imp\n\nRequired Candidate profile\nDomestic clearing like NEFT, ECS, ACH, CHAPS / FEDWIRE / CHIPS / Target2, RTGS, RTP schemes like NPP/FAST/IMPS/InstaPay etc.,SEPA (Direct Debits, Credit Transfers, Mandate Management, Instant Payments",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['GPP', 'MT', 'MX', 'Business Analysis', 'Swift Messaging', 'Visa Installation service', 'VIS', 'NEFT', 'Aquiring', 'SEPA', 'ACH', 'Chips', 'User Stories', 'Payment Domain', 'CHAPS', 'ECS', 'AMEX global specification', 'APACS', 'Requirement Analysis', 'FedWire']",2025-06-10 15:29:39
DCT Data Engineer,Leading Client,1 - 3 years,Not Disclosed,"['Indore', 'Pune', 'Bengaluru']","LocationsPune, Bangalore, Indore\n\nWork modeWork from Office\n\nInformatica data quality - idq\nAzure databricks\nAzure data lake\nAzure Data Factory\nApi integration",Industry Type: Recruitment / Staffing,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'azure databricks', 'hive', 'snowflake', 'python', 'azure data lake', 'api integration', 'informatica data quality', 'informatica powercenter', 'microsoft azure', 'pyspark', 'data warehousing', 'azure data factory', 'sql', 'spark', 'hadoop', 'etl', 'big data', 'aws', 'informatica']",2025-06-10 15:29:43
Data Engineer,wits inovation lab,15 - 20 years,Not Disclosed,"['Hyderabad', 'Pune', 'Bengaluru']","Work closely with the Technology Product Owner and Engineering Leads/SMEs to understand the business requirements and develop technology solutions.\nActively engage in the whole delivery lifecycle from inception, design, development, testing, deployment, operations, monitoring and continuous improvement of systems and services.\nAccountable for the technical excellence of the squad, including technical debt management, technical risk management and ensuring alignment to standards and frameworks. Recognised\nas a technical authority and escalation point for the team, guiding them on the HOW .\nBring a proven record in managing and mentoring teams, with a commitment to building team capabilities and fostering a positive, inclusive culture.\nStay abreast of the latest market developments in technologies and data engineering practices and recommend potential innovative technologies and tools to enhance the capabilities of the engineering team.\nContribute to engineering communities and provide ongoing support of platforms as required.\n\nWhat will you bring\nTo grow and be successful in this role, you will ideally bring the following:\nAt least 5 years of relevant work experience in application delivery with hands on experience with SQL Server Database and Microsoft MSBI Technology (SQL Server Integration Services and SQL Server Reporting Services)\nProven ability to develop and maintain complex SQL queries for data analysis and reporting.\nExtensive hands-on experience in data analysis, profiling and technical solution delivery in line with data warehousing principles in SQL Server.\nGood understanding of networking concepts (protocols, security, virtual network, monitoring and troubleshooting etc.)\nGood to have knowledge around scalable web tier architecture such as J2EE framework, common webserver configuration (JBOSS/IIS), security best practices\nGood experience with DevOps practices, including CI/CD pipelines and observability tools\nGood understanding of SDLC and Agile methodologies. Solid understanding of software engineering principles, patterns, and practices.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'data analysis', 'SQL Server Reporting Services', 'Agile methodologies', 'JBOSS', 'SQL Server Database', 'SDLC', 'software engineering principles', 'IIS', 'DevOps', 'J2EE framework', 'CI/CD', 'SQL Server Integration Services']",2025-06-10 15:29:45
Data Engineer with Machine Learning Specialization,Ortseam Technologies,5 - 10 years,Not Disclosed,[],"Job Requirement for Offshore Data Engineer (with ML expertise)\nWork Mode: Remote\nBase Location: Bengaluru\nExperience: 5+ Years\n\nTechnical Skills & Expertise:\n\nPySpark & Apache Spark:\nExtensive experience with PySpark and Spark for big data processing and transformation.\nStrong understanding of Spark architecture, optimization techniques, and performance tuning.\nAbility to work with Spark jobs in distributed computing environments like Databricks.\nData Mining & Transformation:\nHands-on experience in designing and implementing data mining workflows.\nExpertise in data transformation processes, including ETL (Extract, Transform, Load) pipelines.\nExperience in large-scale data ingestion, aggregation, and cleaning.\nProgramming Languages:\nPython & Scala: Proficient in Python for data engineering tasks, including using libraries like Pandas and NumPy. Scala proficiency is preferred for Spark job development.\nBig Data Concepts: In-depth knowledge of big data frameworks and paradigms, such as distributed file systems, parallel computing, and data partitioning.\nBig Data Technologies:\nCassandra & Hadoop: Experience with NoSQL databases like Cassandra and distributed storage systems like Hadoop.\nData Warehousing Tools: Proficiency with Hive for data warehousing solutions and querying.\nETL Tools: Experience with Beam architecture and other ETL tools for large-scale data workflows.\nCloud Technologies (GCP):\nExpertise in Google Cloud Platform (GCP), including core services like Cloud Storage, BigQuery, and DataFlow.\nExperience with DataFlow jobs for batch and stream processing.\nFamiliarity with managing workflows using Airflow for task scheduling and orchestration in GCP.\nMachine Learning & AI:\nGenAI Experience: Familiarity with Generative AI and its applications in ML pipelines.\nML Model Development: Knowledge of basic ML model building using tools like Pandas, NumPy, and visualization with Matplotlib.\nML Ops Pipeline: Experience in managing end-to-end ML Ops pipelines for deploying models in production, particularly LLM (Large Language Models) deployments.\nRAG Architecture: Understanding and experience in building pipelines using Retrieval-Augmented Generation (RAG) architecture to enhance model performance and output.\n\nTech stack : Spark, Pyspark, Python, Scala, GCP data flow, Data composer (Air flow), ETL, Databricks, Hadoop, Hive, GenAI, ML Modeling basic knowledge, ML Ops experience , LLM deployment, RAG",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'Data Engineering', 'Machine Learning', 'Gcp Cloud', 'Python', 'Airflow', 'Hadoop', 'Data Bricks', 'Hive', 'SCALA', 'Data Flow', 'Spark', 'ETL']",2025-06-10 15:29:47
Informatica Data Engineer,Malomatia,7 - 12 years,20-35 Lacs P.A. (Including Variable: 3%),['Pune'],"Job Title: Data Engineer Informatica IDMC\nLocation: Remote/Contract\nExperience Level: Mid to Senior (7+ years)\nJob Summary:\nWe are seeking a highly skilled Data Engineer with a minimum of 5 years of hands-on experience in Informatica Intelligent Data Management Cloud (IDMC). The successful candidate will design, implement, and maintain scalable data integration solutions using Informatica Cloud services. Experience with CI/CD pipelines is required to ensure efficient development and deployment cycles. Familiarity with Informatica Catalog, Data Governance, and Data Quality or Azure Data Factory is considered a strong advantage.\nKey Responsibilities:\nDesign, build, and optimize end-to-end data pipelines using Informatica IDMC, including Cloud Data Integration and Cloud Application Integration.\nImplement ETL/ELT processes to support data lakehouse, and EDW use cases.\nDevelop and maintain CI/CD pipelines to support automated deployment and version control.\nWork closely with data architects, analysts, and business stakeholders to translate data requirements into scalable solutions.\nMonitor job performance, troubleshoot issues, and ensure compliance with SLAs and data quality standards.\nDocument technical designs, workflows, and integration processes following best practices.\nCollaborate with DevOps and cloud engineering teams to ensure seamless integration within the cloud ecosystem.\nRequired Qualifications:\nBachelor’s degree in Computer Science, Information Systems, Engineering, or a related field.\nMinimum 5 years of hands-on experience with Informatica IDMC.\nExperience in building and deploying CI/CD pipelines using tools such as Git, or Azure DevOps.\nProficient in SQL, data modeling, and transformation logic.\nExperience with cloud platforms (Azure or OCI).\nStrong problem-solving skills in data operations and pipeline performance.\nPreferred / Nice-to-Have Skills:\nExperience with Informatica Data Catalog for metadata and lineage tracking.\nFamiliarity with Informatica Data Governance tools such as Axon and Business Glossary.\nHands-on experience with Informatica Data Quality (IDQ) for data profiling, cleansing.\nExperience developing data pipelines using Azure Data Factory.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Engineering', 'ADF', 'Informatica IDMC', 'Devops', 'Python', 'Azure Data Factory', 'SCALA', 'ETL', 'Azure Storage']",2025-06-10 15:29:49
Data Engineer,Leading Client,4 - 9 years,Not Disclosed,['Bengaluru'],"Roles and Responsibilities:\n4+ years of experience as a data developer using Python\nKnowledge in Spark, PySpark preferable but not mandatory\nAzure Cloud experience (preferred) Alternate Cloud\nexperience is fine preferred experience in Azure platform including\nAzure data Lake, data Bricks, data Factory\nWorking Knowledge on different file formats such as JSON, Parquet, CSV, etc.\nFamiliarity with data encryption, data masking\nDatabase experience in SQL Server is preferable\npreferred experience in NoSQL databases like MongoDB\nTeam player, reliable, self-motivated, and self-disciplined",Industry Type: Recruitment / Staffing,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'hive', 'scala', 'csv', 'datafactory', 'pyspark', 'dbms', 'apache pig', 'parquet', 'sql', 'spark', 'json', 'hadoop', 'big data', 'mongodb', 'hbase', 'python', 'azure data lake', 'oozie', 'microsoft azure', 'sql server', 'azure cloud', 'nosql', 'data bricks', 'mapreduce', 'kafka', 'sqoop', 'aws', 'nosql databases']",2025-06-10 15:29:52
Business Analyst Required in Germany /Sweden under Govt program,Radvision World Consultancy,6 - 11 years,Not Disclosed,"['Sweden', 'Germany']","Chirag 7982170061(Whastapp)\n\nEvaluating business processes, anticipating requirements, uncovering areas for improvement, and developing and implementing solutions.\nLeading ongoing reviews of business processes and developin\nvisa charges applicable\n\nRequired Candidate profile\nStaying up-to-date on the latest process and IT advancements to automate and modernize systems.\nConducting meetings and presentations to share ideas and findings.\nPerforming requirements analysis.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Business analyst', 'Business Strategy', 'Business Research', 'Business Consulting', 'Business And Technical Requirements', 'Business Analytics', 'Business Process Analysis', 'b.tech', 'Market Research', 'Business Analysis']",2025-06-10 15:29:54
Ml Engineer,Sightspectrum,5 - 10 years,1-6 Lacs P.A.,"['Hyderabad', 'Chennai', 'Bengaluru']","Description:\n\n\n\nStrong experience in ETL development, data modeling, and managing data in large-scale environments. - Proficient in AWS services including SageMaker, S3, Glue, Lambda, and CloudFormation/Terraform. - Hands-on expertise with MLOps best practices, including model versioning, monitoring, and CI/CD for ML pipelines.\n\n- Proficiency in Python and SQL; experience with Java is a plus for streaming jobs.\n- Deep understanding of cloud infrastructure automation using Terraform or similar IaC tools. - Excellent problem-solving skills with the ability to troubleshoot data processing and deployment issues.\n- Experience in fast-paced, agile development environments with frequent delivery cycles.\n- Strong communication and collaboration skills to work effectively across cross-functional team Role & responsibilities\n\n\nPreferred candidate profile",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['MLOps', 'Aws Sagemaker', 'Rasa', 'Serverless', 'Ml']",2025-06-10 15:29:56
Data Engineer (Python and SQL),Infraveo Technologies,2 - 5 years,Not Disclosed,[],"AI patterns recognition: you'll be developing multiple AI features, from user categorization to autofilled descriptions, market and conversations sentiment analysis and AI insights to help crypto marketers.\nSDK, User Graph improvement and reliability: we're constantly improving our proprietary user graph by matching wallets to social profiles. Your mission will be supporting new crypto wallets and networks, automating a system to match more users to their identities, and applying data checks to ensure reliability.\nIntegrations: As a customer data platform, we're expanding the number of data sources our customers can import from Web2 and Web3. you'll manage multiple API endpoints and integrate new third-party tools like Mixpanel, Amplitude, Segment, Dune Analytics, and DeFi Llama. This involves not only integration work but also data modeling and architectural design.\nSocial Data analysis: you'll work with social data APIs like Twitter to analyze Key Opinion Leaders performance and trends.\nRequirements\nProven track record as a Data Engineer delivering complex data solutions.\nAdvanced SQL skills and expertise with complex queries.\nMastery in Python development and strong experience with PySpark.\nExtensive proficiency managing cloud services, including AWS Redshift, RDS Postgres, S3, Lambda, Kinesis, SQS, ECS, EC2.\nStrong competency implementing and supporting various data models, such as highly normalized, star schema and Data Vault.\nPractical experience with orchestration tools like Airflow, Dagster or Prefect.\nDemonstrated proficiency consuming and automating interactions with APIs.\nHands-on experience creating data pipelines using dbt for various platforms (ideally AWS Redshift and Postgres).\nExperience in SaaS analytics, marketing, or crypto companies is a plus.\nBenefits\nWork Location: Remote\n5 days working",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data analysis', 'Data modeling', 'Cloud Services', 'Architectural design', 'SDK', 'Management', 'AWS', 'Analytics', 'SQL', 'Python']",2025-06-10 15:29:59
Business Analyst,Coforge,6 - 9 years,Not Disclosed,['Greater Noida'],"The Business Analyst (BA) will act as a key liaison between business stakeholders and technical data teams, focusing on leveraging our data warehouse architecture to deliver analytics, reporting, and talent insights.\n\nThe BA will elicit requirements, validate data solutions, and ensure alignment with business objectives such as candidate matching, client reporting, and workforce analytics.\n\nThe ideal candidate should be an analytical thinker with strong communication skills and experience in data warehouse environments, capable of driving data-driven outcomes in a fast-paced, client-focused setting. Experience on staffing domain.",,,,"['SQL', 'Business Analysis', 'Stakeholder Management']",2025-06-10 15:30:01
Business Analyst,Dslr Technologies,2 - 6 years,Not Disclosed,['Gurugram'],"Job Role: Business Analyst - Retail\n\nAramya is a size-inclusive women's ethnic wear brand focused on delivering comfortable & beautiful daily wear at affordable prices. With $9M in funding from Z47, Accel, and marquee angel investors, were on a path of rapid expansion.\n\nDriven by in-house designing & manufacturing and deep customer insights, we have grown exponentially in the last few quarters, and we are super excited to accelerate this growth further.\n\nWere looking for a data-driven, business-oriented individual to join our team as a Business Analyst. This role is critical to unlocking growth opportunities, optimizing performance, and enabling strategic decision-making across functions. The ideal candidate will possess a strong analytical foundation, business acumen, and a passion for problem-solving.\n\nKey Responsibilities:\nDrive data-led decision-making by analyzing sales, inventory, and customer behavior to improve product, pricing, and store strategies.\nBuild and maintain dashboards and reports that track key retail KPIs, enabling real-time performance visibility.\nCollaborate cross-functionally with merchandising, marketing, and operations to translate insights into business impact.\nPresent strategic recommendations to leadership, influencing growth initiatives and operational efficiency.\n\nQualifications & Skills:\n2-6 years in business analytics, strategy, or data roles in a fast-paced D2C, e-commerce, or consumer tech environment.\nStrong command of SQL, Excel, and at least one BI tool (Power BI, Tableau, Looker, etc.).\nAbility to understand core business drivers and translate data into strategic decisions.\nStrong analytical storytelling and presentation skills to communicate insights effectively.\nBachelor's in Engineering, Economics, Statistics, Business, or a related field. MBA is a plus but not mandatory.\nSelf-starter with high ownership, detail orientation, and a bias for action",Industry Type: Textile & Apparel (Fashion),Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Power Bi Tableau', 'Cross Functional Coordination', 'Retail Analytics', 'E-commerce', 'SQL', 'Business Analytics', 'Data Interpretation', 'Data Visualization', 'Data', 'Dashboarding', 'Looker']",2025-06-10 15:30:03
Machine Learning Engineer,Bay Area Tek Solutions LLC,2 - 5 years,Not Disclosed,['Bengaluru'],"Must have: Strong on programming languages like Python, Java One cloud hands-on experience (GCP preferred) Experience working with Dockers Environments managing (e.g venv, pip, poetry, etc.) Experience with orchestrators like Vertex AI pipelines, Airflow, etc Understanding of full ML Cycle end-to-end Data engineering, Feature Engineering techniques Experience with ML modelling and evaluation metrics Experience with Tensorflow, Pytorch or another framework Experience with Models monitoring Advance SQL knowledge Aware of Streaming concepts like Windowing , Late arrival , Triggers etc",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['GCP', 'Machine learning', 'Cloud', 'Programming', 'Management', 'Monitoring', 'SQL', 'Python']",2025-06-10 15:30:05
Data Analyst,Leading Client,3 - 5 years,Not Disclosed,['Chennai'],"Required Skill, Qualifications and Experience\nBachelor of Computer Science / IT / Software Engineering degree ( or equivalent)from reputed College/University\n3-5 Years of relevant experience with 5-7 Total experience\nPrograming experience with Python, SQL and Shell Scripting\nHands on Development experience w/ Docker/container environment / AWS CDK\nExperience with GIT, Jenkins, Jira\nGood analytical and debugging skills.\nKnowledge of docker/container and VS code IDE environment is preferred.\n\nMandatorySkills:\nData analyst,python coding** Work from home option is available",Industry Type: Recruitment / Staffing,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Analysis', 'container', 'tfs', 'css', 'c++', 'amazon vpc', 'ajax', 'jquery', 'docker', 'ansible', 'sql', 'git', 'java', 'computer science', 'asp.net', 'linux', 'jenkins', 'debugging', 'html', 'mysql', 'shell scripting', 'mvc', 'jira', 'c#', 'github', 'python', 'c', 'ide', 'javascript', 'sql server', 'visual studio', '.net', 'aws', 'vs']",2025-06-10 15:30:08
Data Analyst,Victrix Systems And Labs,8 - 10 years,Not Disclosed,['Delhi / NCR'],"Responsibilities :\n\n- Data Gathering/Data Analysis/Data Modelling/Data Cleansing/Data formatting\n\n- AS IS and TO-BE business process analysis and process modelling including end-to-end data flows\n\n- Authoring data migration plan/cutover plan\n\n- Support change management activities\n\n- Supporting solution development team with data insights as required\n\nMandatory Skills/Experience :\n\n- HCM project experience including upgrade/improvement projects\n\n- People/HR data & process knowledge and experience\n\n- Oracle HCM Cloud skills and experience\n\n- BI Dashboard skills and experience - ability to create/update\n\n- Demonstrated experience developing cutover plans/data migration plans\n\n- Ability to present complex data in easily consumable format to executive level\n\nDesirable/Preferred Skills :\n\n- R-Studio skills and experience to code level\n\n- Knowledge of coding languages - SQL, Python, R\n\nIdeal Candidate :\n\n- Strong customer facing skills - high standard of verbal/written communication skills\n\n- Self-starter with ability to work independently to lead data related work within large HCM project.\n\n- Ability to work across multiple initiatives simultaneously\n\n- Flexibility in work hours with global project team and business\n\n- Ability to work efficiently and effectively via remote work preferably working on US/Canada time zone).",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Gathering', 'Change Management', 'HCM', 'Business Analyst', 'Data Migration', 'Oracle HCM', 'Data Analyst', 'Data Modeling', 'Data Analytics']",2025-06-10 15:30:10
Data Analyst,Victrix Systems And Labs,8 - 10 years,Not Disclosed,['Jaipur'],"Responsibilities :\n\n- Data Gathering/Data Analysis/Data Modelling/Data Cleansing/Data formatting\n\n- AS IS and TO-BE business process analysis and process modelling including end-to-end data flows\n\n- Authoring data migration plan/cutover plan\n\n- Support change management activities\n\n- Supporting solution development team with data insights as required\n\nMandatory Skills/Experience :\n\n- HCM project experience including upgrade/improvement projects\n\n- People/HR data & process knowledge and experience\n\n- Oracle HCM Cloud skills and experience\n\n- BI Dashboard skills and experience - ability to create/update\n\n- Demonstrated experience developing cutover plans/data migration plans\n\n- Ability to present complex data in easily consumable format to executive level\n\nDesirable/Preferred Skills :\n\n- R-Studio skills and experience to code level\n\n- Knowledge of coding languages - SQL, Python, R\n\nIdeal Candidate :\n\n- Strong customer facing skills - high standard of verbal/written communication skills\n\n- Self-starter with ability to work independently to lead data related work within large HCM project.\n\n- Ability to work across multiple initiatives simultaneously\n\n- Flexibility in work hours with global project team and business\n\n- Ability to work efficiently and effectively via remote work preferably working on US/Canada time zone).",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Gathering', 'Change Management', 'HCM', 'Business Analyst', 'Data Migration', 'Oracle HCM', 'Data Analyst', 'Data Modeling', 'Data Analytics']",2025-06-10 15:30:12
Business Analyst,EC Council,5 - 8 years,Not Disclosed,['Mumbai Suburban'],"EC-Council is the world's largest cyber security technical certification body. We operate in 145 countries globally and we are the owner and developer of various world-famous cyber security programs. We are proud to have trained and certified over 220,000 information security professionals globally that have influenced the cyber security mindset of countless organizations worldwide.\n\nWe are seeking a Business Analyst with expertise in Business Process Management (BPM) tools and a strong background in software development projects. The ideal candidate will work closely with stakeholders, product teams, and developers to analyze business processes, define requirements, and optimize workflows using modern BPM platforms.\n\nResponsibilities and Duties:\nCollaborate with stakeholders to gather and document business and functional requirements.\nDefine and analyze business processes, workflows, and system integrations.\nDevelop user stories, process flows, wireframes, and use case diagrams.\nLeverage BPM tools (Appian, Pega, IBM BPM, Camunda, etc.) to design, automate, and optimize business processes.\nAnalyze existing workflows and suggest process improvements to enhance efficiency.\nEnsure BPM solutions align with industry best practices and business goals.\nAct as a bridge between business and technical teams, ensuring alignment on objectives.\nPrepare detailed documentation, including Business Requirement Documents (BRD), Functional Specifications, Process Models and UAT/Production Release Documents.\nConduct impact analysis and feasibility studies for new initiatives.\nWork with QA teams to define test cases and validate business requirements.\nAssist in UAT (User Acceptance Testing) and support issue resolution.\nMonitor key business metrics and process performance using BPM analytics.\nLead limited change management initiatives associated with process redesign and implementation, ensuring stakeholders are informed and prepared for changes.\nStay updated with the latest BPM tools and industry trends.\nRecommend innovative solutions to improve business process automation.\nConduct training and knowledge-sharing sessions for stakeholders.\n\n\nQualifications:\nBachelor's or master's degree in business administration, Education Management, Information Systems, or a related field.\nAt least 5-6 years of proven experience in business analysis, with substantial experience in business process management, preferably within the education sector.\nStrong analytical, problem-solving, and data analysis skills.\nExcellent communication and interpersonal skills, capable of working effectively across diverse teams.\nCertification in Business Process Management (i.e., BPM) or Business Analyst is advantageous\nHands-on experience in Agile (Scrum/Kanban) methodology.\nKnowledge of SQL, API integrations, and system architecture is a plus.\n\nAdditional Information\nWe are an equal opportunity workplace and are an affirmative action employer. We are always committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity, or Veteran status and we do not discriminate based on such characteristics, or any other status protected by the laws or regulations in the locations where we work.\n\nThis job description has been designed to indicate the general nature and level of work performed by employees within this classification. It is not designed to contain or be interpreted as a comprehensive inventory of all duties, responsibilities, and qualifications required of employees assigned to this job.\n\nEC-Council is committed to working with and providing reasonable accommodation to individuals with disabilities. If you have a medical condition or disability which inhibits your ability to complete any part of the application process and need a reasonable accommodation to complete the process, please contact us ecchr@eccouncil.org and let us know how we may assist you.\n\nThis notice together with our Privacy Policy and Terms of Use of this website and any other documents we mention here are meant to inform you on what personal data about you we collect, use, disclose, share, or otherwise process when you are applying for a job at EC-Council or when EC-Council contacts you for recruitment purposes. Please read carefully to understand our views and practices on how we protect your personal data - Privacy Policy | EC-Council (eccouncil.org).",Industry Type: E-Learning / EdTech,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['brd', 'Impact Analysis', 'UAT', 'Business Process Modeling', 'Business Analysis', 'Process Flow', 'User Stories', 'Wireframe', 'software development project', 'process automation', 'BPM tools']",2025-06-10 15:30:15
Data Analyst,Leading Client,4 - 6 years,Not Disclosed,['Chennai'],"3 - 6 years of experience performing quantitative analysis in the financial services industry preferred but not required.\nSkilled ""hands-on"" user of SAS, R, MATLAB, SQL, and/or other statistically programming language required.",Industry Type: Recruitment / Staffing,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analysis', 'matlab', 'python', 'data analytics', 'natural language processing', 'sas', 'predictive analytics', 'machine learning', 'financial services', 'sql', 'tableau', 'r', 'data science', 'quantitative', 'predictive modeling', 'data visualization', 'quantitative analysis', 'statistics']",2025-06-10 15:30:17
Data Engineer,7dxperts,5 - 8 years,15-20 Lacs P.A.,['Bengaluru'],"Role & responsibilities\n3+ years of experience in Spark, Databricks, Hadoop, Data and ML Engineering.\n3+ Years on experience in designing architectures using AWS cloud services & Databricks.\nArchitecture, design and build Big Data Platform (Data Lake / Data Warehouse / Lake house) using Databricks services and integrating with wider AWS cloud services.\nKnowledge & experience in infrastructure as code and CI/CD pipeline to build and deploy data platform tech stack and solution.\nHands-on spark experience in supporting and developing Data Engineering (ETL/ELT) and Machine learning (ML) solutions using Python, Spark, Scala or R languages.\nDistributed system fundamentals and optimising Spark distributed computing.\nExperience in setting up batch and streams data pipeline using Databricks DLT, jobs and streams.\nUnderstand the concepts and principles of data modelling, Database, tables and can produce, maintain, and update relevant data models across multiple subject areas.\nDesign, build and test medium to complex or large-scale data pipelines (ETL/ELT) based on feeds from multiple systems using a range of different storage technologies and/or access methods, implement data quality validation and to create repeatable and reusable pipelines\nExperience in designing metadata repositories, understanding range of metadata tools and technologies to implement metadata repositories and working with metadata.\nUnderstand the concepts of build automation, implementing automation pipelines to build, test and deploy changes to higher environments.\nDefine and execute test cases, scripts and understand the role of testing and how it works.\n\nPreferred candidate profile\nBig Data technologies Databricks, Spark, Hadoop, EMR or Hortonworks.\nSolid hands-on experience in programming languages Python, Spark, SQL, Spark SQL, Spark Streaming, Hive and Presto\nExperience in different Databricks components and API like notebooks, jobs, DLT, interactive and jobs cluster, SQL warehouse, policies, secrets, dbfs, Hive Metastore, Glue Metastore, Unity Catalog and ML Flow.\nKnowledge and experience in AWS Lambda, VPC, S3, EC2, API Gateway, IAM users, roles & policies, Cognito, Application Load Balancer, Glue, Redshift, Spectrum, Athena and Kinesis.\nExperience in using source control tools like git, bit bucket or AWS code commit and automation tools like Jenkins, AWS Code build and Code deploy.\nHands-on experience in terraform and Databricks API to automate infrastructure stack.\nExperience in implementing CI/CD pipeline and ML Ops pipeline using Git, Git actions or Jenkins.\nExperience in delivering project artifacts like design documents, test cases, traceability matrix and low-level design documents.\nBuild references architectures, how-tos, and demo applications for customers.\nReady to complete certifications",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Engineering', 'Data Bricks', 'Python', 'ML', 'ML Engineering', 'Pyspark', 'MLops', 'Ci Cd Pipeline', 'GIT', 'Machine Learning', 'SQL']",2025-06-10 15:30:19
Data Engineer - Bangalore,Cygnus Ad Consulting,5 - 10 years,15-16 Lacs P.A.,"['Bangalore Rural', 'Bengaluru']","Experience in designing, building, and managing data solutions on Azure. Design, develop, and optimize big data pipelines and architectures on Azure. Implement ETL/ELT processes using Azure Data Factory, Databricks, and Spark.\n\nRequired Candidate profile\n5yrs of exp in data engineering and big data technologies.\nHands-on experience with Azure services (Azure Data Factory, Azure Synapse, Azure SQL, ADLS, etc.).\nDatabricks Certification (Mandatory).",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Engineering', 'Databricks Certification', 'Databricks', 'azure', 'Azure Databricks', 'Ci/Cd', 'Azure SQL', 'Devops', 'Azure Data Factory', 'Azure Synapse', 'ETL/ELT', 'ADLS', 'SCALA', 'Big Data Technologies', 'Spark', 'AWS', 'Python']",2025-06-10 15:30:22
Data Engineer,Bay Area Tek Solutions LLC,5 - 8 years,Not Disclosed,['Bengaluru'],"Strong on programming languages like Python, Java Must have one cloud hands-on experience (GCP preferred) Must have: Experience working with Dockers Must have: Environments managing (e.g venv, pip, poetry, etc.) Must have: Experience with orchestrators like Vertex AI pipelines, Airflow, etc Must have: Data engineering, Feature Engineering techniques Proficient in either Apache Spark or Apache Beam or Apache Flink Must have: Advance SQL knowledge Must be aware of Streaming concepts like Windowing , Late arrival , Triggers etc",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Beam', 'GCP', 'spark', 'Cloud', 'Programming', 'Management', 'SQL', 'Python']",2025-06-10 15:30:24
Urgent- Data Governance Analyst (Pharmaceuticals Industry)- Mumbai,-,6 - 11 years,Not Disclosed,['Mumbai (All Areas)'],"Hiring with a leading pharmaceuticals manufacturing industry for their office in Mumbai\n\nJob Title: Data Governance Analyst\n\nLocation: Mumbai, India (On-site)\nExperience: 5-14 Years in Data Governance, including SAP ECC/S4 HANA\n\nKey Responsibilities:\nSAP S4 HANA Implementation & Data Governance:\nWork closely with the SAP S4 HANA implementation team to ensure proper data governance during the brownfield conversion and subsequent functional transformation phases.\nDefine and implement data governance best practices within SAP ECC and SAP S4 HANA.\nSupport data migration strategies, ensuring accuracy, consistency, and completeness of data during the transition from SAP ECC6.0 to SAP S4 HANA.\nManage data cleansing, deduplication, validation, and enrichment processes specific to SAP Master Data (Material, Vendor, Customer, Finance).\nData Governance Policy & Framework Development:\nDevelop and implement data governance policies, standards, and procedures in compliance with industry standards (e.g., DAMA-DMBOK, GDPR, CCPA, Indian Data Protection Bill, GxP compliance).\nEstablish data governance frameworks, including data dictionaries, metadata repositories, and data lineage documentation within SAP and non-SAP ecosystems.\nAlign governance strategies with SAP best practices, ensuring regulatory compliance in healthcare and pharma sectors.\nData Quality Management:\nPerform data profiling, validation, and cleansing within SAP modules such as Material Master, Vendor Master, Customer Master, and Financial Data (FBL1N, FBL3N, FBL5N).\nIdentify and resolve data quality issues related to duplicate records, missing fields, and non-standardized data entries.\nDefine and monitor KPIs for data quality improvement.\nLeverage SAP MDG, Information Steward, and Data Services for data governance and quality enhancement.\nMaster Data Management (MDM):\nLead MDM initiatives within SAP S4 HANA by enforcing data standardization and deduplication rules.\nCollaborate with business stakeholders to maintain consistent master data across business units.\nImplement data lifecycle management strategies for customer, vendor, and product data.\nGovern material classifications, product hierarchies, and financial data structures.\nData Integration & Governance across Enterprise Systems:\nEnsure seamless data integration between SAP S4 HANA, legacy systems, and third-party applications.\nOversee governance of structured and unstructured data from multiple sources (SAP, CRM, external databases, cloud storage, etc.).\nDesign and implement a data governance framework that spans multiple SAP modules, ensuring consistency and compliance.\nCompliance & Risk Mitigation:\nEnsure adherence to data privacy and security standards applicable to the healthcare industry (HIPAA, GDPR, Indian Data Protection laws).\nMitigate risks related to regulatory non-compliance, data breaches, and financial misreporting.\nImplement access control and governance policies in SAP to prevent unauthorized data modifications.\nTraining & Change Management:\nProvide training and guidance to business users and data stewards on SAP data governance best practices.\nDrive user adoption of SAP Fiori apps and other governance tools for data management.\nSupport change management initiatives to promote awareness and compliance with data governance policies.\nMonitoring & Continuous Improvement:\nConduct regular audits to ensure compliance with governance standards.\nUse data governance dashboards (Power BI, SAP Analytics Cloud) to track data quality and performance.\nImplement real-time monitoring and alert mechanisms within SAP for proactive data quality management.\nRequirements:\nExperience in SAP Data Governance: Strong understanding of SAP ECC6.0 and SAP S4 HANA data structures, including material master, vendor master, finance master, and transaction data.\nProficiency in Data Management Tools: Experience with SAP MDG, Data Services, Information Steward, SQL, Power BI, and metadata management tools.\nStrong Analytical Skills: Ability to analyse complex data sets and identify patterns, inconsistencies, and improvement areas.\nKnowledge of Regulatory Frameworks: Familiarity with data privacy regulations (GDPR, CCPA, Indian data protection laws, HIPAA, GxP compliance for pharma).\nExcellent Communication & Collaboration Skills: Ability to engage with cross-functional teams including IT, business units, compliance, and SAP implementation teams.\nHands-on Experience in Data Migration & Quality Management: Proven track record of successful data migration projects within SAP environments.\nAbility to Lead Data Governance Initiatives: Experience in defining governance policies, setting up frameworks, and ensuring cross-functional adoption of best practices.\n\nPreferred Qualifications:\nSAP S4 HANA certification in Data Governance, MDM, or equivalent.\nExposure to healthcare and pharma industry data governance standards.\nExperience working with SAP Fiori-based applications and SAP Analytics Cloud.\n\nIf Interested, Kindly share your updated cv on shweta@topgearconsultants.com\nCurrent company name\nCurrent CTC\nExpected CTC\nNotice Period",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Governance framework', 'Data Governance Analyst', 'automation scripts', 'Data Quality', 'MDM', 'Data Governance', 'Python']",2025-06-10 15:30:26
Data engineer with Gen Ai,Leading Client,4 - 6 years,Not Disclosed,['Chennai'],"We are seeking a skilled Data Engineer who can function as a Data Architect, designing scalable data pipelines, table structures, and ETL workflows. The ideal candidate will be responsible for recommending cost-effective and high-performance data architecture solutions, collaborating with cross-functional teams to enable efficient analytics and data science initiatives.\n\nKey Responsibilities:\n\nDesign and implement ETL workflows, data pipelines, and table structures to support business analytics and data science.\n\nOptimize data storage, retrieval, and processing for cost-efficiency and high performance.\n\nCollaborate with Analytics and Data Science teams for feature engineering and KPI computations.\n\nDevelop and maintain data models for structured and unstructured data.\n\nEnsure data quality, integrity, and security across systems.\n\nWork with cloud platforms (AWS/ Azure/ GCP) to design and manage scalable data architectures.\n\nTechnical Skills Required:\n\nSQL & Python Strong proficiency in writing optimized queries and scripts.\n\nPySpark Hands-on experience with distributed data processing.\n\nCloud Technologies (AWS/ Azure/ GCP) Experience with cloud-based data solutions.\n\nSpark & Airflow Experience with big data frameworks and workflow orchestration.\n\nGen AI (Preferred) Exposure to generative AI applications is a plus.\n\nPreferred Qualifications:\n\nExperience in data modeling, ETL optimization, and performance tuning.\n\nStrong problem-solving skills and ability to work in a fast-paced environment.\n\nPrior experience working with large-scale data processing.",Industry Type: Recruitment / Staffing,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Gen Ai', 'bigdata frameworks', 'python', 'performance tuning', 'data processing', 'airflow', 'microsoft azure', 'pyspark', 'business analytics', 'data architecture', 'data engineering', 'sql', 'gen', 'data quality', 'query optimization', 'data modeling', 'data science', 'spark', 'gcp', 'hadoop', 'aws', 'etl', 'big data']",2025-06-10 15:30:29
Duckcreek Billing Business Analyst,Leading Client,6 - 10 years,Not Disclosed,"['Mumbai', 'Hyderabad', 'Pune']","6-10 years of Duck creek Billing Business Analyst.\nExcellent P&C Commercial Insurance Lines of business knowledge and work experience,\nMin 6 years of Duck Creek Billing platform work experience and should have worked on end-to-end on both Direct and Agency billing implementation projects for US insurance company.\nSound knowledge in billing system workflow and in different integrations (internal and 3rd party)\nGood experience needed in requirements gathering, conducting client workshops and collaborating with multiple stakeholders,\nAbility to understand client requirements, perform business analysis\nDevelop, analyze and document requirements, including business, functional and non-functional\nExcellent written and verbal communications\nLocation:Pune, Mumbai, Chennai, Bangalore, Hyderabad",Industry Type: Recruitment / Staffing,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Duckcreek', 'c#', 'software testing', 'regression testing', 'manual testing', 'business analysis', 'user stories', 'sql server', 'sql', 'change control', 'documents review', 'functional testing', 'brd', 'quality assurance', 'scrum', 'agile', 'duck creek', 'requirement analysis', 'jira', 'agile methodology']",2025-06-10 15:30:32
Analyst,Wipro,1 - 3 years,Not Disclosed,['Gurugram'],"Role Purpose\nThe purpose of the role is to provide effective technical support to the process and actively resolve client issues directly or through timely escalation to meet process SLAs.\nDo\nSupport process by managing transactions as per required quality standards\nFielding all incoming help requests from clients via telephone and/or emails in a courteous manner\nDocument all pertinent end user identification information, including name, department, contact information and nature of problem or issue\nUpdate own availability in the RAVE system to ensure productivity of the process\nRecord, track, and document all queries received, problem-solving steps taken and total successful and unsuccessful resolutions\nFollow standard processes and procedures to resolve all client queries\nResolve client queries as per the SLAs defined in the contract\nAccess and maintain internal knowledge bases, resources and frequently asked questions to aid in and provide effective problem resolution to clients\nIdentify and learn appropriate product details to facilitate better client interaction and troubleshooting\nDocument and analyze call logs to spot most occurring trends to prevent future problems\nMaintain and update self-help documents for customers to speed up resolution time\nIdentify red flags and escalate serious client issues to Team leader in cases of untimely resolution\nEnsure all product information and disclosures are given to clients before and after the call/email requests\nAvoids legal challenges by complying with service agreements\nDeliver excellent customer service through effective diagnosis and troubleshooting of client queries\nProvide product support and resolution to clients by performing a question diagnosis while guiding users through step-by-step solutions\nAssist clients with navigating around product menus and facilitate better understanding of product features\nTroubleshoot all client queries in a user-friendly, courteous and professional manner\nMaintain logs and records of all customer queries as per the standard procedures and guidelines\nAccurately process and record all incoming call and email using the designated tracking software\nOffer alternative solutions to clients (where appropriate) with the objective of retaining customers and clients business\nOrganize ideas and effectively communicate oral messages appropriate to listeners and situations\nFollow up and make scheduled call backs to customers to record feedback and ensure compliance to contract /SLAs\nBuild capability to ensure operational excellence and maintain superior customer service levels of the existing account/client\nUndertake product trainings to stay current with product features, changes and updates\nEnroll in product specific and any other trainings per client requirements/recommendations\nPartner with team leaders to brainstorm and identify training themes and learning issues to better serve the client\nUpdate job knowledge by participating in self learning opportunities and maintaining personal networks.\n\n\nDeliver / No. / Performance Parameter / Measure -\n1. Process - No. of cases resolved per day, compliance to process and quality standards, meeting process level SLAs, Pulse score, Customer feedback.\n2. Self- Management - Productivity, efficiency, absenteeism, Training Hours, No of technical training completed.\nMandatory Skills: Institutional_Finance_Buy_Side_Others.",Industry Type: IT Services & Consulting,"Department: Customer Success, Service & Operations","Employment Type: Full Time, Permanent","['technical support', 'process compliance', 'customer service', 'troubleshooting', 'operational excellence']",2025-06-10 15:30:35
Analyst,Aim Corporate Services,2 - 4 years,3-3.5 Lacs P.A.,['Gurugram'],"Role & responsibilities\nThe job entails investigative research and analysis of cases involving Business Intelligence, forensics and Fraud Investigations.\n\nAnalyze financial records and datasets to identify trends, gaps, or inconsistencies.\nConduct structured research on entities, sectors, and events.\nBased on evidence gathered and research conducted, prepare clear, concise reports and presentations for clients and internal stakeholders.\n\nConduct due diligence on entities.\n\nPerform AML Compliance.\n\nPreferred candidate profile\n\nStrong communication and presentation skills; articulate and professional demeanor.\nDemonstrated excellence in report writing and investigative documentation.\nProficient in financial analysis, Excel-based modeling, and data interpretation.\nSound understanding of company financials and macro/microeconomic trends.\nExcellent research abilities and a sharp eye for detail.\nAnalytical thinker with solid problem-solving capabilities.\nWell-spoken and reliable",Industry Type: Management Consulting,Department: Risk Management & Compliance,"Employment Type: Full Time, Permanent","['Communication Skills', 'Research', 'Report Writing', 'Financial Statement Analysis', 'Aml Compliance', 'Due Diligence', 'Analytics']",2025-06-10 15:30:37
Business Analyst,Kyptronix,5 - 7 years,5-8 Lacs P.A.,['Kolkata'],"Kyptronix LLP is a global tech powerhouse delivering bold, high-performance solutions across Web Development, SaaS, AI, Blockchain, and Growth Marketing.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Requirement Gathering', 'Documentation', 'Document Preparation', 'Requirement Analysis']",2025-06-10 15:30:39
Data Analyst,IntraEdge Technology,1 - 4 years,Not Disclosed,['Chennai'],"The analyst shall define data validation process and perform manual reviews,\ndata source review and alignment with data providers,\nensure data quality standards are met,\nperform gap analyses and design solutions for improving data quality,\nexperience with cost allocation methodology and infrastructure inventory/capacity type data",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Analyst', 'Data validation', 'Quality standards', 'Application development', 'Data quality', 'Data Analyst', 'Cost', 'Inventory']",2025-06-10 15:30:42
Analyst - FP&A - US shift,"Global Capability Center (GCC), part of ...",2 - 5 years,12-18 Lacs P.A.,['Kolkata'],"Role & responsibilities\nFinancial Planning and Analysis: Develop and maintain financial models, forecasts, and analyses to support business decisions.\nFinancial Reporting: Prepare and review financial reports, including income statements, balance sheets, and cash flow statements.\nBudgeting and Forecasting: Assist in the development of annual budgets and forecasts, and provide variance analysis.\nFinancial Data Analysis: Analyze financial data to identify trends, risks, and opportunities for improvement.\nBusiness Partnership: Collaborate with cross-functional teams to understand business needs and provide financial insights to drive decision-making.\nProcess Improvement: Identify opportunities to improve financial processes and implement changes to increase efficiency and accuracy.\nAd-hoc Analysis: Perform ad-hoc financial analyses and provide recommendations to support business initiatives.\n\nPreferred candidate profile\nCA/ ICWA with 2-5 years of work experience. Experience in financial analysis, planning, and budgeting.\nCandidates with prior experience of GCC and US time shift will be given preference.\nProficient in financial modeling, Excel and basic knowledge of SAP.\nStrong analytical and problem-solving skills, with ability to interpret complex financial data.\nExcellent communication and presentation skills, with ability to communicate financial insights to stakeholders.\nUS time shift (CST).",Industry Type: Analytics / KPO / Research,Department: Finance & Accounting,"Employment Type: Full Time, Permanent","['Financial Planning And Analysis', 'FPA', 'Financial Planning']",2025-06-10 15:30:45
Analyst,Eyeota,4 - 9 years,Not Disclosed,['New Delhi'],"Why We Work at Dun & Bradstreet\nDun & Bradstreet unlocks the power of data through analytics, creating a better tomorrow. Each day, we are finding new ways to strengthen our award-winning culture and accelerate creativity, innovation and growth. Our 6,000+ global team members are passionate about what we do. We are dedicated to helping clients turn uncertainty into confidence, risk into opportunity and potential into prosperity. Bold and diverse thinkers are always welcome. Come join us! Learn more at dnb.com/careers .\n\nLocation: Delhi\nBusiness: BAC\nKey Responsibilities:\nAnalyze and understand the requirement of bankers and companies in the perspective Techno-Economic Viability (TEV) mandates involving Greenfield/ Brownfield projects, Debt Restructuring, Detailed Project Reports (DPR), Lenders Independent Engineers (LIE).\nAgency for Specialized Monitoring (ASM) and Project Monitoring mandates\nResponsible for client interaction, site visits, document review, technical and industry analysis, preparation of financial models\nCollect necessary information required for the study from the company\nCo-ordinate with the technical consultants to gather all relevant information required for the study\nUndertake extensive secondary research in the local market to validate cost assumptions\nDraft/ Write necessary reports based on the information collected\nPresents the outcomes of the assessment to clients and bankers at Lenders Consortium Meetings\nRegularly reports all activity and acts within the company s compliance framework\nKey Requirements:\nMBA Finance/CA/CFA (including Under-Graduate as Bachelor of Engineering) with a minimum of 4+ years of relevant experience will be preferred\nPrior experience of building financial models, project management/appraisal and executing Debt Restructuring mandates is a must\nAnalytical capabilities and problem-solving abilities\nSound business knowledge/update on current affairs\nAll Dun & Bradstreet job postings can be found at https: / / www.dnb.com / about-us / careers-and-people / joblistings.html and https://jobs.lever.co/dnb . Official communication from Dun & Bradstreet will come from an email address ending in @dnb.com.\n\n.",Industry Type: IT Services & Consulting,Department: Finance & Accounting,"Employment Type: Full Time, Permanent","['Document review', 'Project management', 'Analytical', 'Debt restructuring', 'DNB', 'HTML', 'Client interaction', 'Financial modelling', 'Analytics', 'Secondary research']",2025-06-10 15:30:47
DBA DATA Engineer || 12 Lakhs CTC,Robotics Technologies,5 - 9 years,12 Lacs P.A.,['Hyderabad( Banjara hills )'],"Dear Candidate,\nWe are seeking a skilled and experienced DBA Data Engineer to join our growing data team. The ideal candidate will play a key role in designing, implementing, and maintaining our databases and data pipeline architecture. You will collaborate with software engineers, data analysts, and DevOps teams to ensure efficient data flow, data integrity, and optimal database performance across all systems. This role requires a strong foundation in database administration, SQL performance tuning, data modeling, and experience with both on-prem and cloud-based environments.\n\nRequirements:\nBachelors degree in Computer Science, Information Systems, or a related field.\n5+ years of experience in database administration and data engineering.\nProven expertise in RDBMS (Oracle, MySQL, and PostgreSQL) and NoSQL systems (MongoDB, Cassandra).\nExperience managing databases in cloud environments (AWS, Azure, or GCP).\nProficiency in ETL processes and tools (e.g., Apache NiFi, Talend, Informatica, AWS Glue).\nStrong experience with scripting languages such as Python, Bash, or PowerShell.\n\nDBA Data Engineer Roles & Responsibilities:\nDesign and maintain scalable and high-performance database architectures.\nMonitor and optimize database performance using tools like CloudWatch, Oracle Enterprise Manager, pgAdmin, Mongo Compass, or Dynatrace.\nDevelop and manage ETL/ELT pipelines to support business intelligence and analytics.\nEnsure data integrity and security through best practices in backup, recovery, and encryption.\nAutomate regular database maintenance tasks using scripting and scheduled jobs.\nImplement high availability, failover, and disaster recovery strategies.\nConduct performance tuning of queries, stored procedures, indexes, and table structures.\nCollaborate with DevOps to automate database deployments using CI/CD and IaC tools (e.g., Terraform, AWS CloudFormation).\nDesign and implement data models, including star/snowflake schemas for data warehousing.\nDocument data flows, data dictionaries, and database configurations.\nManage user access and security policies using IAM roles or database-native permissions.\nAnalyze existing data systems and propose modernization or migration plans (on-prem to cloud, SQL to NoSQL, etc.).\nUse AWS RDS, Amazon Redshift, Azure SQL Database, or Google BigQuery as needed.\nStay up-to-date with emerging database technologies and make recommendations.\n\nMust-Have Skills:\nDeep knowledge of SQL and database performance tuning.\nHands-on experience with database migrations and replication strategies.\nFamiliarity with data governance, data quality, and compliance frameworks (GDPR, HIPAA, etc.).\nStrong problem-solving and troubleshooting skills.\nExperience with data streaming platforms such as Apache Kafka, AWS Kinesis, or Apache Flink is a plus.\nExperience with data lake and data warehouse architectures.\nExcellent communication and documentation skills.\n\nSoft Skills:\nProblem-Solving: Ability to analyze complex problems and develop effective solutions.\nCommunication Skills: Strong verbal and written communication skills to effectively collaborate with cross-functional teams.\nAnalytical Thinking: Ability to think critically and analytically to solve technical challenges.\nTime Management: Capable of managing multiple tasks and deadlines in a fast-paced environment.\nAdaptability: Ability to quickly learn and adapt to new technologies and methodologies.\n\nInterview Mode : F2F for who are residing in Hyderabad / Zoom for other states\nLocation : 43/A, MLA Colony,Road no 12, Banjara Hills, 500034\nTime : 2 - 4pm",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Database Administration', 'Database Migration', 'Dba Skills', 'High Availability', 'Db Administration', 'Hadr', 'Database Security', 'Disaster Recovery', 'Dr Testing', 'DR', 'Luw', 'Db Upgrade', 'Brtools', 'UDDI', 'Os Migration', 'Dbase', 'DBMS', 'IBM DB2', 'Db Migration', 'Disaster Recovery Planning', 'Db Dba', 'Backup And Recovery']",2025-06-10 15:30:50
AI Engineer / Data Analyst,Meru Software,2 - 5 years,Not Disclosed,['Hyderabad'],Meru Software Pvt Ltd is looking for AI Engineer / Data Analyst to join our dynamic team and embark on a rewarding career journey.\n\nDesigning and developing AI algorithms and models to solve specific business problems. Creating and maintaining databases for storing and processing large amounts of data. Developing and deploying machine learning and deep learning models. Implementing and integrating AI solutions with existing systems and software. Analyzing and interpreting complex data sets to extract insights and drive decision - making. Collaborating with cross - functional teams to develop and deploy AI applications. Ensuring the security and privacy of data used in AI applications. Communicating and presenting technical information to non - technical stakeholders. Excellent communication skills & attention to detail.,Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Machine learning', 'Data Analyst', 'SQL', 'Python']",2025-06-10 15:30:53
Ml Engineer/,Orcapod,3 - 5 years,Not Disclosed,['Pune'],"Dear Candidate,\n\n\nWe are currently hiring for our client in Pune location.\nKey Responsibilities\n\n• •Extraction, Transformation, and Loading (ETL) data from source systems and bringing into the desired format for better decision-making.\n• •Identify, analyze, and interpret trends or patterns in complex data sets (Sensor/Machine Data)\n• •Understand Data warehouses and Enterprise Data Lakes  (EDL) and how they work.\n• •Build and refine machine learning models to solve complex problems.\n• •Develop data pipelines for optimal model training and deployment.\n• •Explore new/advanced Data Science techniques/methodologies.\n• •Monitor and optimize model performance post-deployment.\n• •Building and testing the hypothesis using data.\n• •Maintain thorough documentation and communicate technical insights effectively.\n\nRequired Skills\n\n• •Strong knowledge in SQL & PySpark\n• •Hands-on experience in productionizing the ML models.\n• •Experience in working with cloud technologies will be an added advantage (Databricks)\n• •Hands-on experience on any visualization tool (Tableau Preferred)\n• •Strong knowledge of statistical techniques for data analysis\n• •Experience with data mining, machine learning and deep learning for analytical insights\n• •Good communication skills",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['ETL', 'EDL', 'Machine Learning', 'SQL']",2025-06-10 15:30:56
Machine Learning Engineer,SRS Infoway,5 - 7 years,16-17 Lacs P.A.,"['Pune', 'Bengaluru']","ML Engineer with 8+ yrs in Data & Analytics, 5+ in ML. Skilled in Python, FastAPI, Azure, APIs, and deployment. Agile team player, handles multiple projects, values diversity, inclusion, and flexibility.\nMail:kowsalya.k@srsinfoway.com",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Temporary/Contractual","['Power BI', 'Analytics modeling', 'Python', 'ML']",2025-06-10 15:30:58
DBA DATA Engineer || 12 Lakhs CTC,Robotics Technologies,6 - 9 years,12 Lacs P.A.,['Hyderabad( Banjara hills )'],"Dear Candidate,\nWe are seeking a skilled and experienced DBA Data Engineer to join our growing data team. The ideal candidate will play a key role in designing, implementing, and maintaining our databases and data pipeline architecture. You will collaborate with software engineers, data analysts, and DevOps teams to ensure efficient data flow, data integrity, and optimal database performance across all systems. This role requires a strong foundation in database administration, SQL performance tuning, data modeling, and experience with both on-prem and cloud-based environments.\n\nRequirements:\nBachelors degree in Computer Science, Information Systems, or a related field.\n6+ years of experience in database administration and data engineering.\nProven expertise in RDBMS (Oracle, MySQL, and PostgreSQL) and NoSQL systems (MongoDB, Cassandra).\nExperience managing databases in cloud environments (AWS, Azure, or GCP).\nProficiency in ETL processes and tools (e.g., Apache NiFi, Talend, Informatica, AWS Glue).\nStrong experience with scripting languages such as Python, Bash, or PowerShell.\n\nDBA Data Engineer Roles & Responsibilities:\nDesign and maintain scalable and high-performance database architectures.\nMonitor and optimize database performance using tools like CloudWatch, Oracle Enterprise Manager, pgAdmin, Mongo Compass, or Dynatrace.\nDevelop and manage ETL/ELT pipelines to support business intelligence and analytics.\nEnsure data integrity and security through best practices in backup, recovery, and encryption.\nAutomate regular database maintenance tasks using scripting and scheduled jobs.\nImplement high availability, failover, and disaster recovery strategies.\nConduct performance tuning of queries, stored procedures, indexes, and table structures.\nCollaborate with DevOps to automate database deployments using CI/CD and IaC tools (e.g., Terraform, AWS CloudFormation).\nDesign and implement data models, including star/snowflake schemas for data warehousing.\nDocument data flows, data dictionaries, and database configurations.\nManage user access and security policies using IAM roles or database-native permissions.\nAnalyze existing data systems and propose modernization or migration plans (on-prem to cloud, SQL to NoSQL, etc.).\nUse AWS RDS, Amazon Redshift, Azure SQL Database, or Google BigQuery as needed.\nStay up-to-date with emerging database technologies and make recommendations.\n\nMust-Have Skills:\nDeep knowledge of SQL and database performance tuning.\nHands-on experience with database migrations and replication strategies.\nFamiliarity with data governance, data quality, and compliance frameworks (GDPR, HIPAA, etc.).\nStrong problem-solving and troubleshooting skills.\nExperience with data streaming platforms such as Apache Kafka, AWS Kinesis, or Apache Flink is a plus.\nExperience with data lake and data warehouse architectures.\nExcellent communication and documentation skills.\n\nSoft Skills:\nProblem-Solving: Ability to analyze complex problems and develop effective solutions.\nCommunication Skills: Strong verbal and written communication skills to effectively collaborate with cross-functional teams.\nAnalytical Thinking: Ability to think critically and analytically to solve technical challenges.\nTime Management: Capable of managing multiple tasks and deadlines in a fast-paced environment.\nAdaptability: Ability to quickly learn and adapt to new technologies and methodologies.\n\nInterview Mode : F2F for who are residing in Hyderabad / Zoom for other states\nLocation : 43/A, MLA Colony,Road no 12, Banjara Hills, 500034\nTime : 2 - 4pm",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Database Administration', 'Database Migration', 'Dba Skills', 'High Availability', 'Db Administration', 'Hadr', 'Database Security', 'Disaster Recovery', 'Dr Testing', 'DR', 'Luw', 'Db Upgrade', 'Brtools', 'UDDI', 'Os Migration', 'Dbase', 'DBMS', 'IBM DB2', 'Db Migration', 'Disaster Recovery Planning', 'Db Dba', 'Backup And Recovery']",2025-06-10 15:31:01
Data Engineer | Scala - Noida (WFO),CloudKeeper,3 - 6 years,Not Disclosed,['Noida'],"About CloudKeeper\nCloudKeeper is a cloud cost optimization partner that combines the power of\ngroup buying & commitments management, expert cloud consulting & support,\nand an enhanced visibility & analytics platform to reduce cloud cost & help\nbusinesses maximize the value from AWS, Microsoft Azure, & Google Cloud.\nA certified AWS Premier Partner, Azure Technology Consulting Partner,\nGoogle,Cloud Partner, and FinOps Foundation Premier Member, CloudKeeper\nhas helped 400+ global companies save an average of 20% on their cloud bills,\nmodernize their cloud set-up and maximize value all while maintaining\nflexibility and avoiding any long-term commitments or cost.\nCloudKeeper hived off from TO THE NEW, digital technology services company\nwith 2500+ employees and an 8-time GPTW winner.\nPosition Overview:\nWe are looking for an experienced and driven Data Engineer to join our team.\nThe ideal candidate will have a strong foundation in big data technologies,\nparticularly Spark, and a basic understanding of Scala to design and implement\nefficient data pipelines. As a Data Engineer at CloudKeeper, you will be\nresponsible for building and maintaining robust data infrastructure, integrating\nlarge datasets, and ensuring seamless data flow for analytical and operational\npurposes.\n\nKey Responsibilities:\nDesign, develop, and maintain scalable data pipelines and ETL processes to collect, process, and store data from various sources.\nWork with Apache Spark to process large datasets in a distributed environment, ensuring optimal performance and scalability.\nDevelop and optimize Spark jobs and data transformations using Scala for large-scale data processing.\nCollaborate with data analysts and other stakeholders to ensure data pipelines meet business and technical requirements.\nIntegrate data from different sources (databases, APIs, cloud storage, etc.) into a unified data platform.\nEnsure data quality, consistency, and accuracy by building robust data validation and cleansing mechanisms.\nUse cloud platforms (AWS, Azure, or GCP) to deploy and manage data processing and storage solutions.\nAutomate data workflows and tasks using appropriate tools and frameworks.\nMonitor and troubleshoot data pipeline performance, optimizing for efficiency and cost-effectiveness.\nImplement data security best practices, ensuring data privacy and compliance with industry standards.\nRequired Qualifications:\n4- 6 years of experience required as a Data Engineer or an equivalent role\nStrong experience working with Apache Spark with Scala for distributed data processing and big data handling.\nBasic knowledge of Python and its application in Spark for writing efficient data transformations and processing jobs.\nProficiency in SQL for querying and manipulating large datasets.ing technologies.\nExperience with cloud data platforms, preferably AWS (e.g., S3, EC2, EMR, Redshift) or other cloud-based solutions.\nStrong knowledge of data modeling, ETL processes, and data pipeline orchestration.\nFamiliarity with containerization (Docker) and cloud-native tools for deploying data solutions.\nKnowledge of data warehousing concepts and experience with tools like AWS Redshift, Google BigQuery, or Snowflake is a plus.\nExperience with version control systems such as Git.\nStrong problem-solving abilities and a proactive approach to resolving technical challenges.\nExcellent communication skills and the ability to work collaboratively within cross-functional teams.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Temporary/Contractual","['Data Engineering', 'SCALA', 'Pyspark', 'Scala Programming', 'Python Framework', 'SQL Queries', 'Spark', 'Python', 'SQL']",2025-06-10 15:31:03
Data Engineer,Emiza Supply Chain Services,2 - 6 years,Not Disclosed,['Mumbai (All Areas)( Vidya Vihar West )'],"Role & responsibilities\n\nKey Responsibilities\n\nDesign, build, and maintain scalable data pipelines and ETL/ELT processes.\nIntegrate data from various internal and external sources (e.g., ERP, WMS, APIs).\nOptimize and monitor data flows for performance and reliability.\nCollaborate with data analysts, software developers, and business teams to understand data requirements.\nEnsure data quality, consistency, and security across the data lifecycle.\nSupport reporting, dashboarding, and data science initiatives with clean and structured data.\nMaintain data documentation and metadata repositories.\n\nPreferred candidate profile\n\nBachelor's or Masters degree in Computer Science, Engineering, or a related field.\n2+ years of experience in a data engineering or similar role.\nStrong proficiency in SQL and working with relational databases (e.g., PostgreSQL, MySQL).\nExperience with big data technologies like Spark, Hadoop, or similar is a plus.\nHands-on experience with ETL tools (e.g., Apache Airflow, Talend, DBT).\nProficiency in Python or Scala for data processing.\nFamiliarity with cloud platforms (AWS, GCP, or Azure), especially with data services like S3, Redshift, BigQuery, etc.\nKnowledge of APIs and data integration concepts.",Industry Type: Courier / Logistics,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Etl Pipelines', 'Data Engineering', 'Hadoop', 'Cloud Platform', 'Elt', 'Data Pipeline', 'Spark', 'AWS', 'Python', 'SQL']",2025-06-10 15:31:05
Webi Analyst,webid,1 - 6 years,1-4 Lacs P.A.,['Hyderabad'],"Role & responsibilities\n\nsend cvs to shilpa.srivastava@orcapod.work subject Webi Analyst\n\n27k max in hand\nImmediate joiners only\nWebi Contractor\n\nAnalyst\n2-4 years\nHyderabad\n\n2 PM to 11 PM\n\nWeb intelligence development, experience with universe/information design (tool) development, finance background, proficient in SQL,\nexperience with tableau, Power BI, reporting & Data visualization",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Temporary/Contractual","['Excellent Verbal And Written Communication Skills', 'Finance Reporting', 'WebI', 'Web Intelligence', 'universe design tool', 'Power Bi Reports', 'Power Bi', 'Tableau', 'Dashboards', 'Information Design', 'Data Reporting', 'SQL']",2025-06-10 15:31:08
Business Analyst,Omega Healthcare,4 - 8 years,Not Disclosed,['Chennai'],"About The Role\n\nJob Title: Business Analyst US Healthcare (RCM AR & Workflow Tools)\nLocation: Bangalore, Chennai or Hyderabad\nExperience: 4-8 years\n\nJob Summary:",,,,"['us healthcare', 'agile ceremonies', 'stakeholder management', 'agile', 'revenue cycle management', 'project management', 'business strategy', 'business analysis', 'business development', 'market research', 'sprint planning', 'user acceptance testing', 'requirement analysis', 'sdlc']",2025-06-10 15:31:11
Data Engineer,IntraEdge Technology,8 - 13 years,Not Disclosed,['Chennai'],"The Engineer will need to work on backend data model and data architecture, data migration, using python/java to build data ingestion pipelines and data validation processes, develop task schedulers for different data sources",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data migration', 'Backend', 'Data validation', 'Data modeling', 'Application development', 'Python', 'Data architecture']",2025-06-10 15:31:13
Python Data Engineer,GAVS Technologies,4 - 6 years,5.5-15.5 Lacs P.A.,['Pune'],"Skill Expectations\nMust-Have Skills:\nStrong hands-on experience in Python development\nExperience working with Fast API\nData migration and data engineering experience (ETL, pipelines, transformations)\nExperience in web scraping and data extraction techniques\nExperience working with GCP",,,,"['Data Extraction', 'Web Scraping', 'Python', 'GCP', 'Fast Api']",2025-06-10 15:31:16
Data Engineer,Suzva Software Technologies,7 - 8 years,Not Disclosed,"['Mumbai', 'Hyderabad', 'Chennai']","Data Engineer (Contract | 6 Months)\n\nWe are seeking an experienced Data Engineer to join our team for a 6-month contract assignment. The ideal candidate will work on data warehouse development, ETL pipelines, and analytics enablement using Snowflake, Azure Data Factory (ADF), dbt, and other tools.\n\nThis role requires strong hands-on experience with data integration platforms, documentation, and pipeline optimizationespecially in cloud environments such as Azure and AWS.\n\n#KeyResponsibilities\nBuild and maintain ETL pipelines using Fivetran, dbt, and Azure Data Factory\n\nMonitor and support production ETL jobs\n\nDevelop and maintain data lineage documentation for all systems\n\nDesign data mapping and documentation to aid QA/UAT testing\n\nEvaluate and recommend modern data integration tools\n\nOptimize shared data workflows and batch schedules\n\nCollaborate with Data Quality Analysts to ensure accuracy and integrity of data flows\n\nParticipate in performance tuning and improvement recommendations\n\nSupport BI/MDM initiatives including Data Vault and Data Lakes\n\n#RequiredSkills\n7+ years of experience in data engineering roles\n\nStrong command of SQL, with 5+ years of hands-on development\n\nDeep experience with Snowflake, Azure Data Factory, dbt\n\nStrong background with ETL tools (Informatica, Talend, ADF, dbt, etc.)\n\nBachelor's in CS, Engineering, Math, or related field\n\nExperience in healthcare domain (working with PHI/PII data)\n\nFamiliarity with scripting/programming (Python, Perl, Java, Linux-based environments)\n\nExcellent communication and documentation skills\n\nExperience with BI tools like Power BI, Cognos, etc.\n\nOrganized, self-starter with strong time-management and critical thinking abilities\n\n#NiceToHave\nExperience with Data Lakes and Data Vaults\n\nQA & UAT alignment with clear development documentation\nMulti-cloud experience (especially Azure, AWS)",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Azure Data Factory', 'ADF', 'Power BI', 'Cognos', 'Snowflake', 'Informatica', 'ETL', 'UAT testing']",2025-06-10 15:31:18
Data Engineer,MNC Group,5 - 8 years,Not Disclosed,['Pune( Pune Nagar Road )'],"Knowledge and hands-on experience writing effective SQL queries and statements Understanding of AWS services At least 5 years of experience in a similar capacity At least 3 years of proficiency in using Python to develop and modify scripts At least 3 years of proficiency in managing data ingestion and DAG maintenance in airflow Preferred Requirements Knowledge in Hadoop Ecosystem like Spark or PySpark Knowledge in AWS services like S3, Data Lake, Redshift, EMR, EC2, Lambda, Glue, Aurora, RDS, Airflow.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Temporary/Contractual","['Pyspark', 'Airflow', 'Glue', 'AWS', 'Python', 'SQL']",2025-06-10 15:31:20
HRMS Business Analyst,The Veca,7 - 12 years,Not Disclosed,['Mumbai (All Areas)'],"Role & responsibilities -\nPosition Name - HRMS Business Analyst \nLocation - Mumbai \nYears of experience - 7+ years\n\nJob Description - \nSystem Support & Maintenance: Provide ongoing support for HRMS platforms, troubleshoot issues, and ensure data integrity across systems.\nRequirements Gathering & Analysis: Collaborate with HR, IT, and other departments to gather and document business requirements, ensuring alignment with organizational goals.\nSystem Configuration and Implementation: Assist in configuring, testing, and deploying HRMS solutions to ensure they meet business needs and compliance standards.\nData Analysis & Reporting: Analyze HRMS data to identify trends and anomalies, and generate reports to support decision-making processes.\nTraining & Documentation: Develop user manuals, conduct training sessions, and create documentation to ensure effective use of HRMS tools.\nContinuous Improvement: Identify opportunities for process enhancements and system optimizations to improve HR operations.\nPreferred candidate profile\nNeed candidates with have done B.TECH/MCA/BE and immediate to 30 days are preferred. Good experience is needed in HRMS .Please share resume at shraddha.shukla@theveca.com.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['HRMS', 'business analyst', 'HR Implementation', 'Human Resource Management', 'Payroll Software', 'Hr Software', 'Business Analysis']",2025-06-10 15:31:22
Analyst,Wipro,2 - 6 years,Not Disclosed,['Gurugram'],"We are seeking a highly motivated Analyst to join our Capital Formation & Direct Lending team within client. As an integral part of our team, you will gain exposure to a diverse portfolio of Private Credit investments and play a pivotal role in supporting the NY stakeholders in preparing Surveillances and portfolio reviews for our clients.\n\nResponsibilities:\nWork as part of a team-based structure and assist the NY analyst in running Hypos to finalize allocations for each Asset class and Funds.\nUndertake multiple ad hoc projects as requested by senior management. Respond to ad-hoc requests from the Capital formation and Insurance analysts and provide relevant information as needed.\nAssist in various cash management functions including preparation of weekly cash report, cash tracking of capital activities and borrowings / paydowns.\nProvide support to analyst in updating and maintaining seasoning requirements and ratings for issuers / clients and Senior Management.\nMonitoring and creation of Surveillance Reporting for CLNs and SRT deals to track performance and defaults.\nPerform detailed Portfolio Reviews for Affiliates and Clients.\nProvide support for Lending capacity for BDCs and maintaining pipeline transactions for clients.\nSkills Required:\nMBA in Finance, CFA, or CA qualification.\nExperience in Direct Lending or private credit is a plus.\nStrong analytical and quantitative skills. Thorough understanding of basic financial concepts and the ability to critically implement them.\nProficiency in Microsoft Office tools (MS Excel, MS PowerPoint, and MS Word).\nAbility to summarize complex information succinctly and efficiently.\nExcellent written and verbal communication skills & interpersonal skills.\nAbility to manage multiple projects in a fast-paced environment, often under pressure and with multiple stakeholders.\nDetail-oriented with a commitment to accuracy and precision.\nAbility to work independently and collaboratively while demonstrating high sense of ownership and accountability.",Industry Type: IT Services & Consulting,"Department: BFSI, Investments & Trading","Employment Type: Full Time, Permanent","['Financial Modeling', 'Direct Lending', 'Financial Analysis', 'Portfolio Management', 'Investment Banking', 'Private Credit Investments', 'Asset Allocation', 'Portfolio Surveillance', 'Capital Formation Analyst']",2025-06-10 15:31:24
Data Analyst,Chessshiksha Ed-tech,1 - 3 years,Not Disclosed,['Udaipur'],"• Collect and interpret data from various sources to support business needs.\n• Analyze large datasets to identify patterns, trends, and insights.\n• Create reports, dashboards, and visualizations using tools like Excel, Power BI, or Tableau\n\nRequired Candidate profile\n• Work closely with management to prioritize business and information needs.\n• Identify, analyze, and interpret trends or patterns in complex data sets.\n3 Months internship",Industry Type: Education / Training,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Analysis', 'Power BI', 'Data Manipulation', 'Data Profiling', 'Data Visualization', 'Data Cleansing', 'Tableau']",2025-06-10 15:31:27
Data Analyst,Tothr,3 - 5 years,6-12 Lacs P.A.,[],"We are seeking a dynamic candidate with strong data analytical capabilities, advanced Excel proficiency, and excellent English communication skills and bring innovative ideas to the table, and contribute actively toward optimizing internal workflows.\n\nRequired Candidate profile\nExcellent Communication Skill.\nPrior experience in handling vendors or external partners Familiarity with eCommerce workflows and operations",Industry Type: Miscellaneous,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Interactive Reports', 'Vendor Management', 'Advanced Excel', 'excellent Verbal and written communication in English']",2025-06-10 15:31:29
Analyst,Wipro,2 - 6 years,Not Disclosed,['Gurugram'],"About The Role :\n\nTo work on Private Equity / Real Estate / Hedge Fund Operations such as Fund / Financial Accounting -Book Keeping, Journal Posting, Preparation of Financial Statements. Management Fee Carried Interest and Expense Calculation & Posting. Capital Calls - Preparation of Capital Call memos, LC Opening, Follow Up with LPs for funding Distribution - Preparation of Distribution working, LP Memo Preparation, Fund Transfer Wire preparation, Repayment of LCs Reconciliation - Cash Reconciliation, Cash Tracking, Posting Entries, Cash reporting. Tracking of Capital transactions. Valuation - Valuation of Portfolio Investments.\nTo prepare and submit Fund and Investor Reports accurately as per SLA\nTo meet TAT and deliver error free services\nTo work on partnership accounting Applications\nTo strive to create a healthy and professional work environment in the team\nDisplay interpersonal skills in handling the day to day operations on the floor.\nSuggest and work on process improvements Idea Do\nSupport process by managing transactions as per required quality standards\nFielding all incoming help requests from clients via telephone and/or emails in a courteous manner\nDocument all pertinent end user identification information, including name, department, contact information and nature of problem or issue\nUpdate own availability in the RAVE system to ensure productivity of the process\nRecord, track, and document all queries received, problem-solving steps taken and total successful and unsuccessful resolutions\nFollow standard processes and procedures to resolve all client queries\nResolve client queries as per the SLAs defined in the contract\nAccess and maintain internal knowledge bases, resources and frequently asked questions to aid in and provide effective problem resolution to clients\nIdentify and learn appropriate product details to facilitate better client interaction and troubleshooting\nDocument and analyze call logs to spot most occurring trends to prevent future problems\nMaintain and update self-help documents for customers to speed up resolution time\nIdentify red flags and escalate serious client issues to Team leader in cases of untimely resolution\nEnsure all product information and disclosures are given to clients before and after the call/email requests\nAvoids legal challenges by complying with service agreements\nDeliver excellent customer service through effective diagnosis and troubleshooting of client queries\nProvide product support and resolution to clients by performing a question diagnosis while guiding users through step-by-step solutions\nAssist clients with navigating around product menus and facilitate better understanding of product features\nTroubleshoot all client queries in a user-friendly, courteous and professional manner\nMaintain logs and records of all customer queries as per the standard procedures and guidelines\nAccurately process and record all incoming call and email using the designated tracking software\nOffer alternative solutions to clients (where appropriate) with the objective of retaining customers and clients business\nOrganize ideas and effectively communicate oral messages appropriate to listeners and situations\nFollow up and make scheduled call backs to customers to record feedback and ensure compliance to contract /SLAs\nBuild capability to ensure operational excellence and maintain superior customer service levels of the existing account/client\nUndertake product trainings to stay current with product features, changes and updates\nEnroll in product specific and any other trainings per client requirements/recommendations\nPartner with team leaders to brainstorm and identify training themes and learning issues to better serve the client\nUpdate job knowledge by participating in self learning opportunities and maintaining personal networks\n\n\nDeliver\nNoPerformance ParameterMeasure1ProcessNo. of cases resolved per day, compliance to process and quality standards, meeting process level SLAs, Pulse score, Customer feedback2Self- ManagementProductivity, efficiency, absenteeism, Training Hours, No of technical training completed",Industry Type: IT Services & Consulting,Department: Finance & Accounting,"Employment Type: Full Time, Permanent","['Private Equity', 'Journal Posting', 'Financial Statements', 'customer service', 'Hedge Fund Operations', 'Book Keeping', 'RAVE system', 'operational excellence']",2025-06-10 15:31:31
Business Analyst,Lions Workforce Solutions,3 - 5 years,40-55 Lacs P.A.,['Dubai'],We need Business & Systems Analysis assistance for the implementation of a system covering the key things below:\n - Business Requirements Gathering & Documentation\n - Functional Requirements Documentation\n - AS-IS Process assessment and documentation where required\n - To-Be Process design and documentation\n - Develop System Requirements documentation\n - ERP Integration requirements documentation\n - Reviewing Solution Design Document from vendor\n - Working with vendor on solution implementation\n - Developing Test Cases\n - Reviewing system configuration\n - Assisting with system testing where required\n Experience:\n - 3-5 years of experience as a business analyst in systems implementation projects\n - Product Lifecycle Management (PLM) System implementation experience is a plus\n - Experience working in the manufacturing industry is a plus\n - Good facilitation and presentation skills s\n - Very good communication and documentation skills,Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Temporary/Contractual","['Business Analyst', 'Product Lifecycle Management', 'PLM', 'ERP', 'configuration', 'implementation', 'documentation', 'Enterprise resource planning', 'Dubai', 'BA']",2025-06-10 15:31:34
Analyst,Wipro,2 - 6 years,Not Disclosed,['Gurugram'],"Blackstone Credit Treasury Team manages cash and portfolio financing for BDCs, Drawdown Funds and Structured Products Funds investing in privately Originated Debt, Bank Loans, Corporate Bonds, Structured Credit, Distressed Debt, CDS and Equities. Primary responsibilities include: cash management, wire control and funding; portfolio financing and capital optimization; liquidity risk assessment, collateral management and counterparty risk exposure management; relationship management of bank and financing counterparties; regulatory and investor reporting; cash forecasting; systems implementation.\nResponsibilities\nCoordinate with Deal Team/IRBD/Finance on funding mechanics using cash on hand, leverage facility (ABL or subscription line), capital call\nDaily management of cash and liquidity activities for the various entities, including credit facility draws, repayments and interest/waterfall payments\nForecast cash flow, borrowing needs and available funds for investments\nEnsure timely execution of wire payments adhering to strict cut-off times\nReview cash movement activity, including wire instructions and agent notices\nPartner closely with BXC deal team, finance team and capital markets team on Treasury needs globally\nManage resources in our Center of Excellence (CoE) in India and provide hands on support to Treasury team\nProvide middle and back office support for deal funding and trade settlements\nManage and update internal/external standard settlement instructions as required\nCoordinate Know Your Customer refreshes with our banking partners\nAssist on new business initiatives and any special projects that may arise\nRequirements\nExperience leading activities including new Treasury Management System implementations, enhancements, modifications, integrations, and project management\nExperience managing operational aspects of leverage facilities\nIn-depth knowledge of SWIFT message standards\nPrevious experience using Treasury Management Workstation and SWIFT payment processing and reporting\nPrevious experience using Geneva or other accounting systems\nPrevious experience with alternative asset management company\nProficient with Microsoft Office Suite, including Advanced Excel, Word and PowerPoint\nDemonstrated ability to quickly learn new systems and processes\nDemonstrated ability to enhance current process and drive improvements\nAbility to multitask in a fast-paced environment with multiple priorities",Industry Type: IT Services & Consulting,"Department: BFSI, Investments & Trading","Employment Type: Full Time, Permanent","['Treasury Management', 'collateral management', 'risk assessment', 'forecasting', 'liquidity risk', 'trade settlements', 'capital market', 'microsoft office suite', 'process compliance', 'portfolio', 'advanced excel', 'payment processing', 'financing', 'accounting system', 'swift']",2025-06-10 15:31:37
Data Analyst,FCB Kinnect,4 - 8 years,4.5-9.5 Lacs P.A.,['Mumbai( Lower Parel )'],"Key Responsibilities:\nMonitor and analyze website traffic, user engagement, and conversion rates using Piwik PRO and other relevant analytics tools.\nDevelop and maintain dashboards and reports to track key performance indicators (KPIs) for web and digital marketing initiatives.\nDefine, own and drive the tagging and tracking strategy, roadmap and operational plan in line with business needs on website and website funnel performances\nEnsure accurate tagging, tracking, and data collection by working closely with developers to implement Piwik PRO Tag Manager or other tracking solutions.\nBe able to analyze user paths and and optimize desired goals completion.\nProvide insights and recommendations based on data analysis to support business goals.\nIdentify trends, patterns, and areas for improvement to enhance user experience and site performance.\nTrack and report the effectiveness of online campaigns, landing pages, and digital assets.\nCollaborate with marketing, UX/UI designers, developers, and product managers to implement data-driven website enhancements.\nQualifications & Skills:\nExperience in web analytics, digital marketing analytics, or a similar role.\nProficiency in Piwik PRO, Google Tag Manager, or equivalent web analytics tools.\nExperience working with data visualization tools like Excel and Power BI. (Looker studio is a plus.)\nStrong understanding of SEO, digital marketing strategies, and user experience principles.\nKnowledge of HTML, CSS, and web tracking implementation. Knowledge of JavaScript or SQL is a plus.\nStrong analytical and problem-solving skills with attention to detail.",Industry Type: Advertising & Marketing (Digital Marketing),Department: Other,"Employment Type: Full Time, Permanent","['Bigquery', 'GA4', 'Looker Studio', 'Looker', 'GOOGLE ANALYTICS 4', 'Google Data Studio', 'Web Analytics']",2025-06-10 15:31:40
Data Analyst- Kolkata,Madhu Jayanti International,2 - 5 years,Not Disclosed,['Kolkata'],"We are looking for a highly skilled and self-motivated Python Developer with strong expertise in Flask, API integrations (like Amazon and stock market platforms), Selenium automation, and AI/LLM-based applications. The ideal candidate will support and enhance our data-driven platforms, build intelligent automation systems, and contribute to innovation in Business Intelligence and digital workflows.\n\nKey Requirements:\n\nProficiency in Python with 2+ years of hands-on experience.\nStrong experience with Flask or similar Python web frameworks.\nSolid understanding of RESTful API development and consumption.\nHands-on expertise in Selenium for browser automation tasks.\nPractical experience or projects using Large Language Models (LLMs), GPT, or AI toolkits.\nFamiliarity with version control (Git), Docker, and deployment environments.\nStrong analytical and problem-solving skills.\nExcellent communication and collaboration abilities.\n\nKnowledge of Accounts will be added advantage.\n\nSend your resume to mandakranta.mahapatra@jaytea.com with the subject:\n""Application for Python Developer Position"", including:\n- Current CTC\n- Expected CTC\n- Notice Period",Industry Type: FMCG,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['API Integration', 'Python Developer', 'Flask', 'Devops And Deployment', 'SQL', 'Web scraping', 'Django', 'Pandas', 'Numpy', 'Python']",2025-06-10 15:31:42
Analyst-VAPT,Anzen Technologies,0 - 1 years,Not Disclosed,['Mumbai (All Areas)'],"ANZEN Technologies Private Limited. stands as an unparalleled powerhouse, empowering organizations across industries with our visionary services, cutting-edge solutions, and ground-breaking services in the realm of Cyber Security, IT Governance, Risk Management, and Compliance. As your trusted partner, we offer a comprehensive suite of End-to-End security services and consultancy, tailored to safeguard critical infrastructure installations, elevate the standards of BFSI, eCommerce, IT/ITES, Pharmaceuticals, and an array of other sectors.\n\n\nRole & responsibilities :\nVAPT:\n\nQualifications:\n1. Bachelor of engineering in IT, B.S.C. IT, CS, Electronics & Telecommunications in cybersecurity domain.\n2. Candidates should be certified in cybersecurity.\n3. Sound knowledge of OWASP Top 10, SANS top 25, WASC security. Standards and detailed knowledge of common web application attacks. Experience in manual application penetration testing of thick client applications, mobile applications, web services, APIs etc. Manual Mobile application penetration testing on Android and iOS. Good understanding of web application architecture and secure development life cycle\n4. Good communications skill.\n\nNOTE: 3-6 Months of Internship in the related field is must",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Cyber Security', 'Web Application Testing', 'Vapt', 'Mobile Application Testing', 'Owasp Top', 'Penetration Testing', 'API Testing', 'Vulnerability Assessment']",2025-06-10 15:31:45
Hiring Machine Learning Engineer,Motivity Labs,5 - 10 years,14-22.5 Lacs P.A.,['Hyderabad'],"Role - Machine Learning Engineer\nRequired Skills & Experience\n\n5+ years of hands-on experience in building, training, and deploying machine learning models in a professional, production-oriented setting.\nDemonstrable experience with database creation and advanced querying (e.g., SQL, NoSQL), with a strong understanding of data warehousing concepts.\nProven expertise in data blending, transformation, and feature engineering, adept at integrating and harmonizing both structured (e.g., relational databases, CSVs) and unstructured (e.g., text, logs, images) data.\nStrong practical experience with cloud platforms for machine learning development and deployment; significant experience with Google Cloud Platform (GCP) services (e.g., Vertex AI, BigQuery, Dataflow) is highly desirable.\nProficiency in programming languages commonly used in data science (e.g., Python is preferred, R).\nSolid understanding of various machine learning algorithms (e.g., regression, classification, clustering, dimensionality reduction) and experience with advanced techniques like Deep Learning, Natural Language Processing (NLP), or Computer Vision.\nExperience with machine learning libraries and frameworks (e.g., scikit-learn, TensorFlow, PyTorch).\nFamiliarity with MLOps tools and practices, including model versioning, monitoring, A/B testing, and continuous integration/continuous deployment (CI/CD) pipelines.\nExperience with containerization technologies like Docker and orchestration tools like Kubernetes for deploying ML models as REST APIs.\nProficiency with version control systems (e.g., Git, GitHub/GitLab) for collaborative development.\n\nInterested candidates share cv to dikshith.nalapatla@motivitylabs.com",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['GCP', 'Machine Learning', 'Deep Learning', 'Python', 'BigQuery', 'MLOps', 'Git', 'Vertex AI', 'GitHub/GitLab', 'A/B testing', 'Dataflow', 'Kubernetes']",2025-06-10 15:31:48
Data Analyst,Selective Consultants,2 - 7 years,4-5.5 Lacs P.A.,['Faridabad'],"Role & responsibilities\nProject data preparation\nData analysis and interpretation\nProficient in Advanced Excel (formulas, pivot tables, macros)\nData visualization and reporting\nData mining and trend analysis\nCollaborate with stakeholders to understand business requirements\nDevelop and maintain databases\nIdentify areas for process improvement",Industry Type: Automobile,"Department: Customer Success, Service & Operations","Employment Type: Full Time, Permanent","['Project Management', 'MIS', 'Data Management', 'NPD']",2025-06-10 15:31:55
Analyst,VVD & Sons,3 - 5 years,6-8 Lacs P.A.,['Chennai'],"MBA in business development, business management, business analytics\nAge: 30 yrs max\nExp: Min 3 yrs working experience business consultancy, investment banker, financial advisory\nLanguage: Hindi mandatory\nMartial Status: Single",Industry Type: FMCG,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Predictive Analytics', 'Power Bi', 'Data Manipulation', 'SQL Database', 'Data Visualization', 'Python']",2025-06-10 15:31:57
Data Governance - Engineer,Apidel Technologies,1 - 2 years,Not Disclosed,['Mumbai( Vikhroli )'],Strong SQL expertise\nHands-on experience with Advent Geneva and dataset setup\nExcellent communication skills to work across teams and functions.,Industry Type: IT Services & Consulting,Department: Other,"Employment Type: Full Time, Permanent","['SQL Queries', 'Geneva', 'advent geneva', 'Complex Queries', 'SQL']",2025-06-10 15:32:00
Delivery Business Analyst,Acuity Knowledge Partners,5 - 8 years,Not Disclosed,['Bengaluru'],"Job Opportunity :\n\nWe are seeking a skilled IT Business Analyst to join our dynamic team and help us bridge the gap between business needs and technology solutions and the design of business applications that meet our business objectives. This role requires strong technical expertise, business acumen, and project management skills to ensure that our business applications align with company objectives and drive efficiency. The ideal candidate will work cross-functionally with IT, operations, finance, and other departments to enhance application performance and user experience",,,,"['It Business Analysis', 'Agile Methodology', 'project management', 'business requirement', 'Business Acumen', 'technical expertise', 'project timelines', 'Technology Solutions', 'SDLC']",2025-06-10 15:32:03
Abuse Analyst,IBM,2 - 7 years,Not Disclosed,['Bengaluru'],"In this role, you will investigate and resolve reports of malicious, fraudulent, and illegal activity originating from IBM Cloud IP space. By enforcing our Acceptable Use Policy, you will help maintain the reputation and integrity of our network by mitigating abusive activities and eliminating bad actors. Daily efforts include:\n\nInvestigating and validating incoming abuse reports\nOpening incident tickets to notify customers of actionable reports\nEngaging with customers and reporting parties to achieve a timely resolution\nMonitoring open incidents and enforcing resolution timeframes\nMitigating active threats through application of network controls\nAssisting with escalations from other departments\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nBachelor's Degree\n\nRequired technical and professional expertise\n\n\n2+ Years of experience.\n\nUnderstanding of SMTP, DNS, HTTP, Network routing, VPN, and other technologies\nUnderstanding of spam, phishing, fraud, and other behaviors considered to be abusive\nAbility to read and analyze multiple log formats\nDetail-oriented; ability to scrutinize and discern abusive content\nCustomer relations & support\n\n\nPreferred technical and professional experience\n\n\nPrior network security experience\nExisting knowledge of anti-fraud and/or anti-abuse techniques\nBasic server administration skills\nExperience with process documentation\nUnderstanding of Digital Millennium Copyright Act, trademark, intellectual property, Safe Harbor Provisions, GDPR, and other United States federal and international legal precedents",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['smtp', 'dns', 'vpn', 'fraud', 'http', 'switching', 'eigrp', 'rstp', 'networking', 'bgp', 'vrrp', 'cisco asa', 'vlan', 'rip', 'mpls', 'tcp', 'network routing', 'network security', 'ip', 'juniper', 'ospf', 'stp', 'process documentation', 'vtp', 'intellectual property', 'cisco routers', 'hsrp', 'ccna']",2025-06-10 15:32:05
Data Engineer,Meritus Management Service,5 - 10 years,10-20 Lacs P.A.,"['Nagpur', 'Pune']","We are looking for a skilled Data Engineer to design, build, and manage scalable data pipelines and ensure high-quality, secure, and reliable data infrastructure across our cloud and on-prem platforms.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'Synapse Analytics', 'SQL', 'Azure Data Factory', 'Python', 'API Integration', 'Postgresql', 'Data Bricks', 'Scripting', 'SCALA', 'Data Lake', 'MongoDB', 'Data Warehousing', 'Data Modeling', 'ETL', 'Azure Devops']",2025-06-10 15:32:08
Data Engineer,reycruit,7 - 12 years,35-40 Lacs P.A.,['Hyderabad'],"Looking for 8+ years\nPython+Azure/Aws cloud is mandatory\n1st round- virtual\n2nd round- F2F\nMust have 7+ years of relevant experience- should have hands-on experience with ETL/ELT processes, cloud-based data solutions, and big data technologies.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Pipeline', 'Big Data', 'Elt', 'ETL']",2025-06-10 15:32:09
Python Developer/ Python Data Engineer (Python+ ETL+ Pandas),Atyeti,5 - 8 years,Not Disclosed,['Hyderabad'],"Role & responsibilities\nB.Tech or M.Tech in Computer Science, or equivalent experience.\n5+ years of experience working professionally as a Python Software Developer.\nOrganized, self-directed, and resourceful.\nExcellent written and verbal communication skills.\nExpert in python & pandas.\nExperience in building data pipelines, ETL and ELT processes.",,,,"['Pandas', 'ETL', 'Python', 'SQL']",2025-06-10 15:32:11
Cerner Analyst,Tech Mahindra,5 - 10 years,Not Disclosed,"['Hyderabad', 'Pune', 'Bengaluru']","Urgent Requirement .... Tech Mahindra Hiring\n\nEXP: 4 to 12yrs\nShift: 3:30pm to 12:30am / 5.30pm to 2.30am / 6.30pm to 3.30am\nWork Location : Bangalore / Hyderabad / Pune\nWork Model : Hybrid (12 days in a month)\n\nPlease find below JD details.\n\n•        Cerner Application support, Incident resolution, Implementation of Cerner Millennium Projects.\n•        Experience in configuring and troubleshooting CERNER solution functionalities/Components.\n•        Perform complex troubleshooting investigations and documenting notes and knowledge articles.\n•        Gather requirements and determine scope of work and plan for on time delivery.\n•        Ability to work self-sufficiently on assigned time sensitive tasks.\n•        Develop and maintain good relationship with peers and client, provide timely feedback to encourage success.\n•        Strong communication skills with excellent interpersonal skills both in written and verbal correspondence.\n•        Ability to learn and adapt to changing landscape and acquire new skills with technology advancement and to work, coordinate with global teams.\n•        Readiness towards work at odd hours/on-call and weekends as and when needed.\n\n\n\nBuild & Configuration Experience required in following Cerner Applications :\n\nCCL reporting\nMS4\nFSI\nCharges\nPatient accounting\nSecurity\nProvider, Ambulatory\nRegistration\nCerner Rules (Custom Solution Development)\nScheduling\nSoarian Financials\nInvision\nHIM\n\n\nShare profiles to mm00828174@techmahindra.com",Industry Type: IT Services & Consulting,Department: Healthcare & Life Sciences,"Employment Type: Full Time, Permanent","['HIM', 'Patient Accounting', 'Invision', 'Soarian', 'Registration', 'Scheduling', 'MS4']",2025-06-10 15:32:13
Data Engineer,Lance Labs,7 - 12 years,Not Disclosed,"['Noida', 'Chennai']","Deployment, configuration & maintenance of Databricks clusters & workspaces\nSecurity & Access Control\nAutomate administrative task using tools like Python, PowerShell &Terraform\nIntegrations with Azure Data Lake, Key Vault & implement CI/CD pipelines\n\nRequired Candidate profile\nAzure, AWS, or GCP; Azure experience is preferred\nStrong skills in Python, PySpark, PowerShell & SQL\nExperience with Terraform\nETL processes, data pipeline &big data technologies\nSecurity & Compliance",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Pyspark', 'Azure Databricks', 'SQL', 'Terraform', 'Python', 'Powershell', 'Ci/Cd', 'Data Pipeline', 'GCP', 'Azure Cloud', 'Azure Data Lake', 'ETL', 'AWS', 'Data Governance', 'Azure Devops']",2025-06-10 15:32:15
Analyst,Wipro,2 - 6 years,Not Disclosed,['Gurugram'],"Key Responsibilities:\n‚ Cash management (incl. preparation of payments, monthly cash balances reporting)\n‚ Day to day accounting and reporting for various companies of our investments\n‚ Preparation of financial statements and audits coordination (drafting Financial statement,\nticking figures and notes, support in audit requests, confirmations etc.)\n‚ Preparation of financial information / analysis as required\n‚ Assistance & coordination with preparation of Income Tax & VAT returns, and other tax\ncompliance reporting (such as FATCA, CRS, Withholding tax)\n‚ Implement & maintain electronic & hardcopy files and keeping the database up to date\n(supplier acceptance and vendors list)\n‚ Preparation of BCL and CBCr reporting\n‚ Support AML / KYC files and ensure coordination with US ops team\n‚ Support in drafting legal and compliance documentation\n‚ General coordination for transactions occurring at Luxembourg companies level\n‚ Assist the Board of Managers by providing them with the necessary information /\ndocumentation\n‚ Participate in the process documentation efforts and creation of SOPs.\n‚ Identify process gaps and initiate process improvement projects.\nDesired Candidate Profile:\n‚ University Degree in Accounting, Economics or Finance\n‚ Advanced knowledge of MS Office tools.\n‚ Very good written & spoken communication skills, fluent in English. French is considered as\nan asset.\n‚ Good analytical & problem-solving skills. Pro-active & Positive attitude.\n‚ Ability to work efficiently and effectively in a team.\n‚ Excellent customer facing skills and ability to build rapport with clients.\n‚ Strong knowledge of Fundamental Accounting/General Accounting\n‚ General understanding of Investment Banking terms i.e. Capital Markets, Private Equity,\nMutual Funds, Hedge Funds, Real Estate Funds\n‚ General understanding and hands on experience of Private Equity, Real Estate, Hedge Fund",Industry Type: IT Services & Consulting,Department: Finance & Accounting,"Employment Type: Full Time, Permanent","['general accounting', 'accounting', 'capital market', 'ms office tools', 'financial statements', 'aml', 'investment banking', 'private equity', 'cash management', 'hedge funds', 'vat', 'economics', 'process documentation', 'kyc', 'drafting', 'vendor', 'mutual funds', 'tax compliance', 'french', 'finance']",2025-06-10 15:32:17
Statistical Analyst,Simbi Labs India,2 - 7 years,2.5-6 Lacs P.A.,"['Chandigarh', 'Patna', 'Delhi / NCR']","Role & responsibilities\nCollect, clean, and validate data from multiple sources to ensure quality and accuracy.\nDesign and implement statistical models and methodologies to solve real-world problems.\nAnalyze trends, patterns, and relationships in complex data sets.\nDevelop and apply predictive and inferential models.\nCommunicate findings through reports, visualizations, and presentations.\nCollaborate with cross-functional teams, including data scientists, researchers, business analysts, and management.\nStay current with developments in statistical methodologies, software tools, and best practices.\nProvide guidance on experimental design and data collection strategies.\nEnsure compliance with data governance, privacy laws, and industry regulations.\n\n\nPreferred candidate profile\n\nMasters degree in Statistics, Mathematics, Data Science, or a related field (PhD preferred for advanced research roles).\nProven experience in statistical analysis, modeling, and experimental design.\nProficiency in statistical software such as R, SAS, Python, or SPSS.\nStrong knowledge of probability theory, hypothesis testing, regression, and multivariate analysis.\nExperience with data visualization tools (e.g., Tableau, Power BI, ggplot2, matplotlib).\nExcellent problem-solving, communication, and organizational skills.\nAbility to translate complex statistical concepts into actionable business insights.",Industry Type: Management Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Statistical Data Analysis', 'SPSS', 'Spss Statistics', 'Biostatistics', 'Statistical Software', 'Statistical Tools', 'Data Interpretation', 'Hypothesis Testing', 'Statistical Programming', 'Anova', 'Statistics', 'Statistical Techniques', 'Minitab', 'STATA', 'Statistical Analyses']",2025-06-10 15:32:20
Hiring For Business Analyst - US Shift,Digile Technologies,9 - 14 years,Not Disclosed,[],"Role & responsibilities\nPosition Overview\nWe are seeking experienced Business Analysts to help transform Professional Services operations. This team will support strategic documentation and process mapping efforts that will guide future-state road mapping across three key functions: Sales, Consulting Delivery, and Fulfillment. The ideal candidates will bring strong analytical capabilities, proficiency with tools such as Lucid chart or equivalent, and a passion for driving operational excellence.\nKey Responsibilities\nCollaborate with cross-functional stakeholders across Sales, Delivery, and Fulfillment functions to document current processes and identify areas for improvement.\nMap and analyze business processes across Lead-to-Cash, Order-to-Cash, Hire-to-Retire, and Onboarding value streams.\nCreate clear, comprehensive, and visually effective process documentation using Lucid chart or similar tools.\nSupport the development of future-state process flows to enable digital transformation and operational scalability.\nWork with internal and external-facing teams (500+ internal employees, 5,500+ contractors) to understand needs and capture pain points.\nCoordinate across global teams (India, Brazil, and U.S.) to ensure documentation efforts are aligned and time-zone compatible.\nProvide inputs to the broader change management and transformation planning initiatives.\nQualifications\n5+ years of experience as a Business Analyst, preferably in a technology or professional services environment.\nStrong process mapping and documentation skills; experience with Lucid chart, Visio, or similar tools required.\nFamiliarity with enterprise systems such as NetSuite, Salesforce, Hi Bob, or similar platforms.\nExperience documenting and analyzing end-to-end business processes, especially in scaling environments.\nExcellent communication and collaboration skills across distributed global teams.\nAbility to work independently in a fast-paced, ambiguous environment.\nPreferred Qualifications\nExperience supporting large-scale business transformation or ERP/CRM implementation projects.\nPrior exposure to professional services delivery models or managed services organizations.\nKnowledge of SaaS business models and metrics.\n\nPreferred candidate profile",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['enterprise system', 'Lucidchart', 'Business analyst']",2025-06-10 15:32:22
Business Analyst,Opash Software,2 - 4 years,4.8-8.4 Lacs P.A.,['Surat( Adajan )'],Responsibilities:\n* Collaborate with cross-functional teams on JIRA integration\n* Ensure compliance with industry standards and best practices\n* Analyze business needs through requirement gathering,Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Requirement Gathering', 'Use Cases', 'Brd', 'Srs', 'FSD', 'FRS', 'FRD', 'JIRA', 'Jira Integration']",2025-06-10 15:32:24
Business Analyst,Aster Medcity,2 - 5 years,Not Disclosed,['Bengaluru'],"Aster Medcity is looking for Business Analyst to join our dynamic team and embark on a rewarding career journey.\nEvaluating business processes, anticipating requirements, uncovering areas for improvement, and developing and implementing solutions.\nLeading ongoing reviews of business processes and developing optimization strategies.\nStaying up-to-date on the latest process and IT advancements to automate and modernize systems.\nConducting meetings and presentations to share ideas and findings.\nPerforming requirements analysis.\nDocumenting and communicating the results of your efforts.\nEffectively communicating your insights and plans to cross-functional team members and management.\nGathering critical information from meetings with various stakeholders and producing useful reports.\nWorking closely with clients, technicians, and managerial staff.\nProviding leadership, training, coaching, and guidance to junior staff.\nAllocating resources and maintaining cost efficiency.\nEnsuring solutions meet business needs and requirements.\nPerforming user acceptance testing.\nManaging projects, developing project plans, and monitoring performance.\nUpdating, implementing, and maintaining procedures.\nPrioritizing initiatives based on business needs and requirements.\nServing as a liaison between stakeholders and users.\nManaging competing resources and priorities.\nMonitoring deliverables and ensuring timely completion of projects.",Industry Type: Medical Services / Hospital,"Department: BFSI, Investments & Trading","Employment Type: Full Time, Permanent","['vendor management', 'project management', 'documentation', 'business analysis', 'process improvement', 'budgeting', 'strategic planning', 'operations', 'people management skills', 'service delivery', 'leadership', 'user acceptance testing', 'cost efficiency', 'reporting']",2025-06-10 15:32:27
Analyst,Vipsa Talent Solutions,1 - 4 years,9-13 Lacs P.A.,['Bengaluru'],"1-3 yrs PE portfolio management, DCF valuation, LP/GP reporting, real estate valuations, financial modeling.",Industry Type: Financial Services (Asset Management),Department: Finance & Accounting,"Employment Type: Full Time, Permanent","['Private Equity', 'DCF', 'Portfolio Reporting', 'Valuation Reporting', 'Financial Modelling', 'Real Estate']",2025-06-10 15:32:29
Walk interview-Analyst-ARD/AMD-Rabale ( Navi Mumbai),Flamingo Pharmaceuticals,1 - 5 years,Not Disclosed,['Navi Mumbai'],Exp. in ARD/AMD of minimum -1 to 5 years\nExp. in pharmaceutical formulation of Tablet and capsules\nExp. in semi-regulated - ROW,Industry Type: Pharmaceutical & Life Sciences,Department: Other,"Employment Type: Full Time, Permanent","['ARD', 'AMd']",2025-06-10 15:32:31
Business Analyst,TechStar Group,7 - 12 years,10-15 Lacs P.A.,"['Navi Mumbai', 'Bengaluru', 'Mumbai (All Areas)']","Role & responsibilities :\n\nBusiness Analyst - 7 to 10 years experience in Capital markets or global treasury.\nKnowledge of capital markets & financial instruments Equities, Derivatives, Fixed Income, FX.\nUnderstanding of Investment Banking and Asset Management Front, Middle & Back Office functions.\nAsset management experience of Trading Compliance rule coding & testing is strongly preferred, but other experience within Asset Management industry is acceptable Financial industry knowledge within investments and distribution highly desired (i.e., understanding of investment process, capital markets, fixed income, equities Sound knowledge on Database SQL querying.\nShould be a strong team player Excellent communication skills - written & verbal.\nShould be able to create good documentation and correspond on functionalities & issues concisely.\nShould be able to articulate well during discussions.\nShould be able to work with tight deadlines Confident of interacting with business users and various stakeholders.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['MS Excel', 'Word', 'Visio', 'PowerPoint']",2025-06-10 15:32:33
Data Engineer 2 at Fintech Platform,Talent 24/7,3 - 5 years,15-22.5 Lacs P.A.,[],"Role & responsibilities\nDesign real-time data pipelines for structured and unstructured sources.\nCollaborate with analysts and data scientists to create impactful data solutions.\nContinuously improve data infrastructure based on team feedback.\nTake full ownership of complex data problems and iterate quickly.\nPromote strong documentation and engineering best practices.\nMonitor, detect, and fix data quality issues with custom tools.\n\nPreferred candidate profile\nExperience with big data tools like Spark, Hadoop, Hive, and Kafka.\nProficient in SQL and working with relational databases.\nHands-on experience with cloud platforms (AWS, GCP, or Azure).\nFamiliar with workflow tools like Airflow.",Industry Type: FinTech / Payments,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Hadoop', 'Data Streaming', 'Data Bricks']",2025-06-10 15:32:35
Analyst,EVERSANA India Pvt. Ltd,2 - 4 years,Not Disclosed,['Pune'],"Job Description\nWe are seeking a dedicated and experienced Customer Support Executive to join our international healthcare voice support team. The ideal candidate will have prior experience handling US healthcare customers or patients, be familiar with medical terminology, and possess strong communication and problem-solving skills. This role requires working from the office during US business hours.\nKey Responsibilities:\nHandle inbound and outbound calls related to healthcare services\nProvide excellent customer service by addressing queries, concerns, and complaints in a professional manner.\nAccurately document all call interactions in the internal CRM systems.\nEnsure compliance with HIPAA and company policies regarding patient confidentiality.\nCollaborate with internal departments to resolve issues promptly.\nMeet defined metrics such as call quality, average handling time (AHT), and first call resolution (FCR).\nFollow up on unresolved issues and ensure timely closure.\n\n\nQualifications\n2-4 years of experience in international voice process, preferably in the US healthcare domain .\nStrong knowledge of US healthcare processes , inc",Industry Type: Pharmaceutical & Life Sciences,"Department: Customer Success, Service & Operations","Employment Type: Full Time, Permanent","['Outbound', 'Claims', 'Billing', 'HIPAA', 'healthcare management', 'International voice process', 'Spanish', 'Customer service', 'US healthcare', 'CRM']",2025-06-10 15:32:37
Business Analyst MES,Lektronix,2 - 5 years,Not Disclosed,['Pune'],"Business Analyst-MES\nPune-India\nPlex by Rockwell Automation is known for transforming manufacturing organisations through ""top-floor-to-shop-floor"" transformations, by replacing old-school legacy ERP applications. But what about those manufacturers that already have top-floor capabilities and just need state-of-the-art shop floor capabilities? Plex has you covered there, We are now offering our award-winning manufacturing execution suite to those who just need shop floor governance, analytics, and integration to their existing top-floor systems.\nTo support our ongoing growth, we are looking for a Sr. Business Analyst to support the product management team in execution of our MES product vision while also supporting the growth of Plex MES through the adoption and expansion of our products.\nYou will report to the Sales leader Group Product Manager, and have a hybrid schedule working in Pune, India..\nYour Responsibilities:\nSupport Product Managers in execution of multi-year product strategy and roadmap for MES product\nMeet defined outcomes and measures of success for the products and releases\nWork with Technical Team (Product Management and Engineering) to complete MES vision and deliver impactful product\nCollaborate with Product Management, Engineering, UX, Marketing, Sales, and Customer Success teams to ensure successful product adoption creating exceptionally happy and referenceable MES customers through full lifecycle engagement (pre-sales, implementation, and beyond)\nThe Essentials - You Will Have:\nBachelors degree in Computer Science, Engineering, or a related field.\nExperience in manufacturing - discrete, automotive, hybrid, process, food & beverage, or consumer packaged goods\nThe Preferred - You Might Also Have:\nExperience in the software industry, with experience in Product Management, Pre-Sales, Customer Support, or Services.\nExperience in SaaS\nFluidly navigate between business and technical concepts\nAnalytical and oral and written communication skills\nDefine requirements and prioritise requests based upon qualitative and quantitative analysis.\nAbility to work independently, and part of a broader team, in a fast-paced environment.\nWhat We Offer:\nOur benefits package includes\nComprehensive mindfulness programs with a premium membership to Calm\nVolunteer Paid Time off available after 6 months of employment for eligible employees\nCompany volunteer and donation matching program - Your volunteer hours or personal cash donations to an eligible charity can be matched with a charitable donation.\nEmployee Assistance Program\nPersonalized wellbeing programs through our OnTrack program\nOn-demand digital course library for professional development\n... and other local benefits!\nAt Rockwell Automation we are dedicated to building a diverse, inclusive and authentic workplace, so if youre excited about this role but your experience doesnt align perfectly with every qualification in the job description, we encourage you to apply anyway. You may be just the right person for this or other roles.\n#LI-Hybrid\n#LI-NB1\nRockwell Automation s hybrid policy aligns that employees are expected to work at a Rockwell location at least Mondays, Tuesdays, and Thursdays unless they have a business obligation out of the office.",Industry Type: Industrial Automation,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Product management', 'Computer science', 'ERP', 'Automation', 'Analytical', 'Shop floor', 'Presales', 'Customer support', 'Automotive', 'Analytics']",2025-06-10 15:32:39
Business Analyst MES,Kalypso It Solutions,2 - 5 years,Not Disclosed,['Pune'],"Rockwell Automation is a global technology leader focused on helping the world s manufacturers be more productive, sustainable, and agile. With more than 28, 000 employees who make the world better every day, we know we have something special. Behind our customers - amazing companies that help feed the world, provide life-saving medicine on a global scale, and focus on clean water and green mobility - our people are energized problem solvers that take pride in how the work we do changes the world for the better.\nWe welcome all makers, forward thinkers, and problem solvers who are looking for a place to do their best work. And if that s you we would love to have you join us!\nJob Description\nBusiness Analyst-MES\nPune-India\nPlex by Rockwell Automation is known for transforming manufacturing organisations through ""top-floor-to-shop-floor"" transformations, by replacing old-school legacy ERP applications. But what about those manufacturers that already have top-floor capabilities and just need state-of-the-art shop floor capabilitiesPlex has you covered there, We are now offering our award-winning manufacturing execution suite to those who just need shop floor governance, analytics, and integration to their existing top-floor systems.\nTo support our ongoing growth, we are looking for a Sr. Business Analyst to support the product management team in execution of our MES product vision while also supporting the growth of Plex MES through the adoption and expansion of our products.\nYou will report to the Sales leader Group Product Manager, and have a hybrid schedule working in Pune, India. .\nYour Responsibilities:\nSupport Product Managers in execution of multi-year product strategy and roadmap for MES product\nMeet defined outcomes and measures of success for the products and releases\nWork with Technical Team (Product Management and Engineering) to complete MES vision and deliver impactful product\nCollaborate with Product Management, Engineering, UX, Marketing, Sales, and Customer Success teams to ensure successful product adoption creating exceptionally happy and referenceable MES customers through full lifecycle engagement (pre-sales, implementation, and beyond)\nThe Essentials - You Will Have:\nBachelors degree in Computer Science, Engineering, or a related field.\nExperience in manufacturing - discrete, automotive, hybrid, process, food beverage, or consumer packaged goods\nThe Preferred - You Might Also Have:\nExperience in the software industry, with experience in Product Management, Pre-Sales, Customer Support, or Services.\nExperience in SaaS\nFluidly navigate between business and technical concepts\nAnalytical and oral and written communication skills\nDefine requirements and prioritise requests based upon qualitative and quantitative analysis.\nAbility to work independently, and part of a broader team, in a fast-paced environment.\nWhat We Offer:\nOur benefits package includes\nComprehensive mindfulness programs with a premium membership to Calm\nVolunteer Paid Time off available after 6 months of employment for eligible employees\nCompany volunteer and donation matching program - Your volunteer hours or personal cash donations to an eligible charity can be matched with a charitable donation.\nEmployee Assistance Program\nPersonalized wellbeing programs through our OnTrack program\nOn-demand digital course library for professional development\n. . . and other local benefits!\nAt Rockwell Automation we are dedicated to building a diverse, inclusive and authentic workplace, so if youre excited about this role but your experience doesnt align perfectly with every qualification in the job description, we encourage you to apply anyway. You may be just the right person for this or other roles.\n#LI-Hybrid\n#LI-NB1",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Product management', 'Computer science', 'ERP', 'Automation', 'Analytical', 'Shop floor', 'Presales', 'Customer support', 'Automotive', 'Analytics']",2025-06-10 15:32:42
CSV Analyst,Altiushub,2 - 6 years,Not Disclosed,['Hyderabad'],"Computer System Validation Analyst\nExperience: 3-6 years\nLocation: Hyderabad(On-Site)\nRoles and Responsibilities:\nDevelop and execute Validation strategies in line with client needs and regulatory guidelines (GxP, 21 CFR Part 11, GAMP 5, EU Annex 11)\nCreate and maintain validation documentation, including Validation Plans, IQ/OQ/PQ protocols, URS, RTM, and other required documents\nConduct impact assessments, risk assessments, and manage change control processes\nHandle CAPA, incident management, and release management activities related to Validation activities\nDevelop and maintain in-house SOPs related to Validation processes and ensure adherence to QMS\nUtilise JIRA for tracking tasks, issues, and project progress\nCollaborate with cross-functional teams to ensure successful Validation Documents preparation\nStay up-to-date with industry best practices and regulatory changes related to Validation\nQualifications:\n3+ years of proven experience in serialisation implementation within the pharmaceutical industry\nStrong understanding of regulatory requirements (GxP, 21 CFR Part 11, GAMP 5, ICH Q9)\nExperience with validation protocols (IQ, OQ, PQ) and documentation (URS, RTM)\nFamiliarity with quality management systems (QMS) and SOP development\nProficiency in using JIRA or similar project management tools\nExcellent communication and interpersonal skills\nPreferred Skills:\nExperience with specific serialisation software and hardware solutions (mention specific platforms if applicable)\nKnowledge of supply chain management principles\nCertification in relevant areas (e g-, quality management, project management)\nShow more Show less",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['pq', 'oq', 'urs', 'iq', 'project management tools', 'gamp', 'jira', 'communication skills']",2025-06-10 15:32:45
Blackline Analyst,Booking Holdings,3 - 8 years,Not Disclosed,['Bengaluru'],"Key Responsibilities:\n\nImplementation & Configuration:\nGood understanding of Finance processes, especially Intercompany, GL account reconciliation and cross application consolidation.\nAssist with the configuration, customization, and implementation of BlackLine software.\nWork with stakeholders to gather business requirements and translate them into technical specifications for BlackLine deployment.",,,,"['Blackline', 'Intercompany Reconciliation', 'Blackline Implementation', 'Sap Integration']",2025-06-10 15:32:47
CSV Analyst,Altiushub,5 - 10 years,Not Disclosed,['Hyderabad'],"Develop and execute Validation strategies in line with client needs and regulatory guidelines (GxP, 21 CFR Part 11, GAMP 5, ICH Q9).\nCreate and maintain validation documentation, including Validation Plans, IQ/OQ/PQ protocols, URS, RTM, and other required documents.\nConduct impact assessments, risk assessments, and manage change control processes.\nHandle CAPA, incident management, and release management activities related to Validation activities.\nDevelop and maintain in-house SOPs related to Validation processes and ensure adherence to QMS.\nUtilize JIRA for tracking tasks, issues, and project progress.\nCollaborate with cross-functional teams to ensure successful Validation Documents preparation.\nStay up-to-date with industry best practices and regulatory changes related to Validation.\nQualifications\n5+ years of proven experience in serialization implementation within the pharmaceutical industry.\nStrong understanding of regulatory requirements (GxP, 21 CFR Part 11, GAMP 5, ICH Q9).\nExperience with validation protocols (IQ, OQ, PQ) and documentation (URS, RTM).\nFamiliarity with quality management systems (QMS) and SOP development.\nProficiency in using JIRA or similar project management tools.\nExcellent communication and interpersonal skills.\nPreferred Skills\nExperience with specific serialization software and hardware solutions (mention specific platforms if applicable).\nKnowledge of supply chain management principles.\nCertification in relevant areas (e.g., quality management, project management).",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Supply chain management', 'Interpersonal skills', 'Management systems', 'Project management', 'QMS', 'URS', 'Incident management', 'JIRA', 'Release management', 'Quality management']",2025-06-10 15:32:50
