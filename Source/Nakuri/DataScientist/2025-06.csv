title,company,experience,salary,locations,description,industry,department,employment_type,skills,scraped_at
Data Scientist,Visa,5 - 10 years,Not Disclosed,['Bengaluru'],"To ensure that Visa s payment technology is truly available to everyone, everywhere requires the success of our key bank or merchant partners and internal business units. The Global Data Science group supports these partners by using our extraordinarily rich data set that spans more than 3 billion cards globally and captures more than 100 billion transactions in a single year. Our focus lies on building creative solutions that have an immediate impact on the business of our highly analytical partners. We work in complementary teams comprising members from Data Science and various groups at Visa. To support our rapidly growing group we are looking for Data Scientists who are equally passionate about the opportunity to use Visa s rich data to tackle meaningful business problems. You will join one of the Data Science focus areas (eg, banks, merchants & retailers, digital products, marketing) with an opportunity for rotation within Data Science to gain broad exposure to Visa s business.\n  Essential Functions\nBe an out-of-the-box thinker who is passionate about brainstorming innovative ways to use our unique data to answer business problems\nCommunicate with clients to understand the challenges they face and convince them with data\nExtract and understand data to form an opinion on how to best help our clients and derive relevant insights\nDevelop visualizations to make your complex analyses accessible to a broad audience\nFind opportunities to craft products out of analyses that are suitable for multiple clients\nWork with stakeholders throughout the organization to identify opportunities for leveraging Visa data to drive business solutions.\nMine and analyze data from company databases to drive optimization and improvement of product, marketing techniques and business strategies for Visa and its clients\nAssess the effectiveness and accuracy of new data sources and data gathering techniques.\nDevelop custom data models and algorithms to apply to data sets.\nUse predictive modeling to increase and optimize customer experiences, revenue generation, data insights, advertising targeting and other business outcomes.\nDevelop processes and tools to monitor and analyze model performance and data accuracy.\nThis is a hybrid position. Expectation of days in office will be confirmed by your Hiring Manager.\n\n\nBasic Qualifications\nBachelor s or Master s degree in Statistics, Operations Research, Applied Mathematics, Economics, Data Science, Business Analytics, Computer Science, or a related technical field\n5+ years of work experience with a bachelor s degree or 2+ years experience with an advance degree (e.g., Master s or MBA)\nAnalyzing large data sets using programming languages such as Python, R, SQL and/or Spark\nDeveloping and refining machine learning models for predictive analytics, classification and regression tasks.\n\nPreferred Qualifications\n5+ years experience in data-based decision-making or quantitative analysis\nKnowledge of ETL pipelines in Spark, Python, HIVE that process transaction and account level data and standardize data fields across various data sources\nGenerating and visualizing data-based insights in software such as Tableau\nCompetence in Excel, PowerPoint\nPrevious exposure to financial services, credit cards or merchant analytics is a plus",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Operations research', 'Business analytics', 'Analytical', 'Predictive modeling', 'Business solutions', 'Financial services', 'Product marketing', 'SQL', 'Python']",2025-06-12 14:35:25
Data Scientist,Tesco,1 - 3 years,Not Disclosed,['Bengaluru( Whitefield )'],"Job Summary:\n\nEnable data driven decision making across the Tesco business globally by developing analytics solutions using a combination of math, tech and business knowledge\n\nRoles and Responsibilities:\n- Identifying operational improvements and finding solutions by applying CI tools and techniques\n- Responsible for completing tasks and transactions within agreed KPI's",,,,"['Data Science', 'Advanced Excel', 'Data Analytics', 'Python', 'SQL', 'Applied Mathematics', 'Machine Learning', 'Statistics']",2025-06-12 14:35:27
Data Scientist - L3,Wipro,3 - 5 years,Not Disclosed,['Bengaluru'],"Role Purpose\nThe purpose of the role is to define, architect and lead delivery of machine learning and AI solutions.\n\n\n\nDo\n1. Demand generation through support in Solution development\na. Support Go-To-Market strategy\ni. Contribute to development solutions, proof of concepts aligned to key offerings to enable solution led sales\nb. Collaborate with different colleges and institutes for research initiatives and provide data science courses\n2. Revenue generation through Building & operationalizing Machine Learning, Deep Learning solutions\na. Develop Machine Learning / Deep learning models for decision augmentation or for automation solutions\nb. Collaborate with ML Engineers, Data engineers and IT to evaluate ML deployment options\n3. Team Management\na. Talent Management\ni. Support on boarding and training to enhance capability & effectiveness\n\n\n\nDeliver\n\nNo.Performance ParameterMeasure\n1.Demand generation# PoC supported\n2.Revenue generation through deliveryTimeliness, customer success stories, customer use cases\n3.Capability Building & Team Management# Skills acquired\n\n\n\n\n\n\nMandatory Skills: Data Analysis.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Analysis', 'ML deployment', 'Deep learning models', 'Solution development', 'Talent Management', 'Machine Learning']",2025-06-12 14:35:29
Data Scientist - L3,Wipro,3 - 5 years,Not Disclosed,['Ramdurg'],"Role Purpose\nThe purpose of the role is to define, architect and lead delivery of machine learning and AI solutions.\nDo\n1. Demand generation through support in Solution development\na. Support Go-To-Market strategy\ni. Contribute to development solutions, proof of concepts aligned to key offerings to enable solution led sales\nb. Collaborate with different colleges and institutes for research initiatives and provide data science courses\n2. Revenue generation through Building & operationalizing Machine Learning, Deep Learning solutions\na. Develop Machine Learning / Deep learning models for decision augmentation or for automation solutions\nb. Collaborate with ML Engineers, Data engineers and IT to evaluate ML deployment options\n3. Team Management\na. Talent Management\ni. Support on boarding and training to enhance capability & effectiveness\nDeliver\n\nNo.Performance ParameterMeasure\n1.Demand generation# PoC supported\n2.Revenue generation through deliveryTimeliness, customer success stories, customer use cases\n3.Capability Building & Team Management# Skills acquired\n\n\nMandatory Skills: Data Analysis.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Analysis', 'python', 'team management', 'natural language processing', 'scikit-learn', 'ml deployment', 'machine learning', 'data engineering', 'artificial intelligence', 'sql', 'deep learning', 'tensorflow', 'data science', 'predictive modeling', 'statistical modeling', 'ml']",2025-06-12 14:35:31
Data Scientist,Wipro,2 - 7 years,Not Disclosed,['Bengaluru'],"Role Purpose\nThe purpose of this role is to design, test and maintain software programs for operating systems or applications which needs to be deployed at a client end and ensure its meet 100% quality assurance parameters\nDo\nInstrumental in understanding the requirements and design of the product/ software\nDevelop software solutions by studying information needs, studying systems flow, data usage and work processes\nInvestigating problem areas followed by the software development life cycle\nFacilitate root cause analysis of the system issues and problem statement\nIdentify ideas to improve system performance and impact availability\nAnalyze client requirements and convert requirements to feasible design\nCollaborate with functional teams or systems analysts who carry out the detailed investigation into software requirements\nConferring with project managers to obtain information on software capabilities\nPerform coding and ensure optimal software/ module development\nDetermine operational feasibility by evaluating analysis, problem definition, requirements, software development and proposed software\nDevelop and automate processes for software validation by setting up and designing test cases/scenarios/usage cases, and executing these cases\nModifying software to fix errors, adapt it to new hardware, improve its performance, or upgrade interfaces.\nAnalyzing information to recommend and plan the installation of new systems or modifications of an existing system\nEnsuring that code is error free or has no bugs and test failure\nPreparing reports on programming project specifications, activities and status\nEnsure all the codes are raised as per the norm defined for project / program / account with clear description and replication patterns\nCompile timely, comprehensive and accurate documentation and reports as requested\nCoordinating with the team on daily project status and progress and documenting it\nProviding feedback on usability and serviceability, trace the result to quality risk and report it to concerned stakeholders\nStatus Reporting and Customer Focus on an ongoing basis with respect to project and its execution\nCapturing all the requirements and clarifications from the client for better quality work\nTaking feedback on the regular basis to ensure smooth and on time delivery\nParticipating in continuing education and training to remain current on best practices, learn new programming languages, and better assist other team members.\nConsulting with engineering staff to evaluate software-hardware interfaces and develop specifications and performance requirements\nDocument and demonstrate solutions by developing documentation, flowcharts, layouts, diagrams, charts, code comments and clear code\nDocumenting very necessary details and reports in a formal way for proper understanding of software from client proposal to implementation\nEnsure good quality of interaction with customer w.r.t. e-mail content, fault report tracking, voice calls, business etiquette etc\nTimely Response to customer requests and no instances of complaints either internally or externally\nStakeholder Interaction\n\nStakeholder Type\nStakeholder Identification\nPurpose of Interaction\nInternal\nLead Software Developer and Project Manager\nRegular reporting & updates\nSoftware Developers\nFor work coordination and support in providing testing solutions\nExternal\nClients\nProvide apt solutions and support as per the requirement\nDisplay\nLists the competencies required to perform this role effectively:\nFunctional Competencies/ Skill\nLeveraging Technology Knowledge of current and upcoming technology along with expertise in programming (automation, tools and systems) to build efficiencies and effectiveness in own function/ Client organization Competent\nProcess Excellence - Ability to follow the standards and norms to produce consistent results, provide effective control and reduction of risk Expert\nTechnical knowledge knowledge of various programming languages, tools, quality management standards and processes - Expert\n\nCompetency Levels\nFoundation\nKnowledgeable about the competency requirements. Demonstrates (in parts) frequently with minimal support and guidance.\nCompetent\nConsistently demonstrates the full range of the competency without guidance. Extends the competency to difficult and unknown situations as well.\nExpert\nApplies the competency in all situations and is serves as a guide to others as well.\nMaster\nCoaches others and builds organizational capability in the competency area. Serves as a key resource for that competency and is recognised within the entire organization.\nBehavioral Competencies\nFormulation & Prioritization\nInnovation\nManaging Complexity\nExecution Excellence\nPassion for Results\nDeliver / No. / Performance Parameter / Measure -\n1. Continuous Integration, Deployment & Monitoring of Software\n100% error free on boarding & implementation, throughput %, Adherence to the schedule/ release plan\n2. Quality & CSAT\nOn-Time Delivery, Manage software, Troubleshoot queries\nCustomer experience, completion of assigned certifications for skill upgradation\n3. MIS & Reporting\n100% on time MIS & report generation\nMandatory Skills: Python, GenAI, AWS\nPreferred Skills: NLP , AI/ML , LLM\nArchitect and implement Al solutions utilizing cutting-edge technologies like LLM, Langchain, and Machine Learning.\nAIML solution development in Azure using Python\nAbility to build and finetune the model to improve the performance\nCreate own technology if off-the-shelf technology is not solving the problem. E.g changes to traditional RAG approaches, finetune LLM, create architectures.\nUse experience and advise leadership and team of data scientists best approaches, architectures for complex ML use cases.\nLead from the front, responsible for coding, designing, and ensuring best practices & frameworks are adhered by the team.\nCreate end to end AI systems with responsible AI principles\nDevelop data pipelines using SQL to extract and transform data from Snowflake for Al model training and inference.\nPossess expertise in Natural Language Processing (NLP) & GenAI to integrate text-based data sources into the Al architecture.\nCollaborate with data scientists and engineers to ensure seamless integration of Al components into existing systems.\nResponsible for continuous communication about the team progress to keystakeholder.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'GenAI', 'NLP', 'Artificial Intelligence', 'LLM', 'AWS', 'Machine Learning', 'Python']",2025-06-12 14:35:34
ML Engineer/Data Scientist,Altimetrik,6 - 8 years,15-30 Lacs P.A.,"['Hyderabad', 'Chennai', 'Bengaluru']","Role & responsibilities\nData Scientist /ML engineers : ML Engineer with Python, SQL, Machine Learning, Azure skills(Good to have)",Industry Type: IT Services & Consulting,,,"['Machine Learning', 'Python', 'SQL', 'Data Science', 'Ml', 'azure']",2025-06-12 14:35:36
Data Scientist-Artificial Intelligence,IBM,3 - 7 years,Not Disclosed,['Bengaluru'],"As an Data Engineer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\n\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\n\n In this role, your responsibilities may include: \nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviours’.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modelling results\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nProof of Concept (POC) DevelopmentDevelop POCs to validate and showcase the feasibility and effectiveness of the proposed AI solutions.\nHelp in showcasing the ability of Gen AI code assistant to refactor/rewrite and document code from one language to another\nDocument solution architectures, design decisions, implementation details, and lessons learned.\nStay up to date with the latest trends and advancements in AI, foundation models, and large language models.\nEvaluate emerging technologies, tools, and frameworks to assess their potential impact on solution design and implementation\n\n\nPreferred technical and professional experience\nExperience and working knowledge in COBOL & JAVA would be preferred\nHaving experience in Code generation, code matching & code translation leveraging LLM capabilities would be a Big plus\nDemonstrate a growth mindset to understand clients' business processes and challenges",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['elastic search', 'java', 'proof of concept', 'cobol', 'splunk', 'targetlink', 'simulink', 'data management', 'stateflow', 'sil', 'big data', 'can bus', 'matlab', 'python', 'c', 'predictive', 'machine learning', 'presales', 'autosar', 'code generation', 'rtw', 'rfi', 'embedded c', 'model based development', 'rfp']",2025-06-12 14:35:39
Data Scientist - Cybersecurity,Visa,2 - 7 years,Not Disclosed,['Bengaluru'],"As part of Cyber Threat Analytics and Research team (CTAR) , you will leverage cutting-edge technologies to perform statistical profiling, inference, classification, clustering and predictive analysis. As a key member of the technical team, you will create and implement sophisticated machine learning models to help derive new insights to defend against cyber-attacks. You will be working with a large variety of data sets, cutting-edge security technologies, and world-class operation teams to create awesome analytics for security and other business units.\n  Essential Functions:\nAnalyze cyber event logs using Spark and big data technologies and develop deeper insights into products using advanced statistical methods.\nFormulate cyber threat scenarios into technical data problems and develop high fidelity models to capture unseen threats\nDevise and implement deep learning models for building user behavior profiles. This includes data acquisition, feature engineering, model development, and deployment.\nConduct feature engineering on various data sources to build and enrich feature store\nFine tune open source LLM to detect anomalous user behavior\nLeverage Generative AI to perform RAG for helping improve Cyber investigation efficiency\nAs a member of the CTAR team, you will work closely with other data scientists and data engineers to build, design, engineer, and develop analytical software and services that deliver security functionality and improve security efficiency and capabilities through automation.\nAssist in shaping overall direction, life-cycle management, and leadership for Information Security architecture and technology related to Visa.\nCommunicate clean and persuasive data directly to end users, leadership, and other stakeholders, technical and non-technical.\n\n\n\nBasic Qualifications:\n2+ years of relevant work experience and a Bachelors degree, OR 5+ years of relevant work experience\n\nPreferred Qualifications:\n3 or more years of work experience with a Bachelor s Degree or more than 2 years of work experience with an Advanced Degree (e.g. Masters, MBA, JD, MD)\nSolid background and hands on experiences with building Machine learning, deep learning and AI models\nExperience with Generative AI/LLM\nExcellent understanding of algorithms and data structures and proficiency in Python and SQL.\nExperience working with large datasets using tools and Hadoop, Spark, or Hive\nExcellent analytic and problem-solving capability combined with ambition to solve hard problems\nStrong communications skills and ability to collaborate\nHighly driven, resourceful and results oriented\nGood team player and excellent interpersonal skills\nDemonstrated ability to lead and navigate through ambiguity",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Automation', 'Information security', 'Analytical', 'Machine learning', 'Data structures', 'Open source', 'Analytics', 'Monitoring', 'SQL', 'Python']",2025-06-12 14:35:41
Data Scientist - L3,Wipro,3 - 6 years,Not Disclosed,['Bengaluru'],"Wipro Limited (NYSEWIT, BSE507685, NSEWIPRO) is a leading technology services and consulting company focused on building innovative solutions that address clients’ most complex digital transformation needs. Leveraging our holistic portfolio of capabilities in consulting, design, engineering, and operations, we help clients realize their boldest ambitions and build future-ready, sustainable businesses. With over 230,000 employees and business partners across 65 countries, we deliver on the promise of helping our customers, colleagues, and communities thrive in an ever-changing world. For additional information, visit us at www.wipro.com.\n About The Role  \n\nRole Purpose\n\nThe purpose of the role is to define, architect and lead delivery of machine learning and AI solutions.\n\n ? \n\nDo\n\n1. Demand generation through support in Solution development\n\na. Support Go-To-Market strategy\n\ni. Contribute to development solutions, proof of concepts aligned to key offerings to enable solution led sales\n\nb. Collaborate with different colleges and institutes for research initiatives and provide data science courses\n\n2. Revenue generation through Building & operationalizing Machine Learning, Deep Learning solutions\n\na. Develop Machine Learning / Deep learning models for decision augmentation or for automation solutions\n\nb. Collaborate with ML Engineers, Data engineers and IT to evaluate ML deployment options\n\n3. Team Management\n\na. Talent Management\n\ni. Support on boarding and training to enhance capability & effectiveness\n\n ? \n\nDeliver\n\n\nNo.\n\nPerformance Parameter\n\nMeasure 1. Demand generation # PoC supported 2. Revenue generation through delivery Timeliness, customer success stories, customer use cases 3. Capability Building & Team Management # Skills acquired\n\n\n ? \n\n ? \nMandatory\n\nSkills:\nData Analysis.\n\nExperience3-5 Years.\n\nReinvent your world. We are building a modern Wipro. We are an end-to-end digital transformation partner with the boldest ambitions. To realize them, we need people inspired by reinvention. Of yourself, your career, and your skills. We want to see the constant evolution of our business and our industry. It has always been in our DNA - as the world around us changes, so do we. Join a business powered by purpose and a place that empowers you to design your own reinvention. Come to Wipro. Realize your ambitions. Applications from people with disabilities are explicitly welcome.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analysis', 'machine learning', 'deep learning', 'data science', 'ml', 'python', 'natural language processing', 'scikit-learn', 'neural networks', 'ml deployment', 'data engineering', 'artificial intelligence', 'sql', 'tensorflow', 'r', 'predictive modeling', 'statistical modeling', 'statistics']",2025-06-12 14:35:43
Data Scientist-Artificial Intelligence,IBM,3 - 7 years,Not Disclosed,['Bengaluru'],"An AI Data Scientist at IBM is not just a job title - it’s a mindset. You’ll leverage the watsonx,AWS Sagemaker,Azure Open AI platform to co-create AI value with clients, focusing on technology patterns to enhance repeatability and delight clients.\n\nWe are seeking an experienced and innovative AI Data Scientist to be specialized in foundation models and large language models. In this role, you will be responsible for architecting and delivering AI solutions using cutting-edge technologies, with a strong focus on foundation models and large language models. You will work closely with customers, product managers, and development teams to understand business requirements and design custom AI solutions that address complex challenges. Experience with tools like Github Copilot, Amazon Code Whisperer etc. is desirable.\n\nSuccess is our passion, and your accomplishments will reflect this, driving your career forward, propelling your team to success, and helping our clients to thrive.\n\nDay-to-Day Duties:\nProof of Concept (POC) DevelopmentDevelop POCs to validate and showcase the feasibility and effectiveness of the proposed AI solutions. Collaborate with development teams to implement and iterate on POCs, ensuring alignment with customer requirements and expectations.\nHelp in showcasing the ability of Gen AI code assistant to refactor/rewrite and document code from one language to another, particularly COBOL to JAVA through rapid prototypes/ PoC\nDocumentation and Knowledge SharingDocument solution architectures, design decisions, implementation details, and lessons learned. Create technical documentation, white papers, and best practice guides. Contribute to internal knowledge sharing initiatives and mentor new team members.\nIndustry Trends and InnovationStay up to date with the latest trends and advancements in AI, foundation models, and large language models. Evaluate emerging technologies, tools, and frameworks to assess their potential impact on solution design and implementation\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nStrong programming skills, with proficiency in Python and experience with AI frameworks such as TensorFlow, PyTorch, Keras or Hugging Face. Understanding in the usage of libraries such as SciKit Learn, Pandas, Matplotlib, etc. Familiarity with cloud platforms (e.g. Kubernetes, AWS, Azure, GCP) and related services is a plus.\nExperience and working knowledge in COBOL & JAVA would be preferred\no Having experience in Code generation, code matching & code translation leveraging LLM capabilities would be a Big plus (e.g. Amazon Code Whisperer, Github Copilot etc.) Soft\n\nSkills:\nExcellent interpersonal and communication skills. Engage with stakeholders for analysis and implementation. Commitment to continuous learning and staying updated with advancements in the field of AI.\nGrowth mindsetDemonstrate a growth mindset to understand clients' business processes and challenges.\nExperience in python and pyspark will be added advantage\n\n\nPreferred technical and professional experience\nExperienceProven experience in designing and delivering AI solutions, with a focus on foundation models, large language models, exposure to open source, or similar technologies. Experience in natural language processing (NLP) and text analytics is highly desirable. Understanding of machine learning and deep learning algorithms.\nStrong track record in scientific publications or open-source communities\nExperience in full AI project lifecycle, from research and prototyping to deployment in production environments",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'machine learning', 'tensorflow', 'pytorch', 'keras', 'kubernetes', 'github', 'natural language processing', 'scikit-learn', 'pyspark', 'microsoft azure', 'artificial intelligence', 'text analytics', 'pandas', 'deep learning', 'java', 'code generation', 'cobol', 'gcp', 'matplotlib', 'aws']",2025-06-12 14:35:45
Hdfc Bank - Digital Banking - Data Scientist - Generative AI,Hdfc Bank,4 - 9 years,Not Disclosed,['Mumbai (All Areas)'],"Role - Digital Banking-Data Scientist-Digital Experience Analytics\n\nLocation - Mumbai\nGrade - Deputy Manager to Senior Manager\nMinimum 4 years experience\n\nJob Purpose:\nThe Data Scientist will design, develop and deploy advanced AI models to drive digital transformation, enhance customer experience, optimize operations and mitigate risks. This role requires a sound understanding of AI/ML techniques and their application to challenges such as customer engagement and process automation.",,,,"['Data Science', 'Artificial Intelligence', 'Machine Learning', 'Generative AI', 'Risk Analytics', 'Data Analytics']",2025-06-12 14:35:48
Data Scientist,"NTT DATA, Inc.",2 - 5 years,Not Disclosed,['Bengaluru'],"Your day at NTT DATA\nThe Senior Data Scientist is an advanced subject matter expert, tasked with taking accountability in the adoption of data science and analytics within the organization.\n\nThe primary responsibility of this role is to participate in the creation and delivery of data-driven solutions that add business value using statistical models, machine learning algorithms, data mining, and visualization techniques.\n\nWhat youll be doing\n\nKey Responsibilities:\nDesigns, develops, and programs methods, processes, and systems to consolidate and analyze unstructured, diverse big data sources to generate actionable insights and solutions for client services and product enhancement.\nDesigns and enhances data collection procedures to include information that is relevant for building analytic systems.\nResponsible for ensuring that data used for analysis is processed, cleaned and, integrally verified and build algorithms necessary to find meaningful answers.\nDesigns and codes software programs, algorithms, and automated processes to cleanse, integrate and evaluate large datasets from multiple disparate sources\nProvides meaningful insights from large data and metadata sources; interprets and communicates insights and findings from analysis and experiments to product, service, and business managers.\nDirects scalable and highly available applications leveraging the latest tools and technologies.\nAccountable for creatively visualizing and effectively communicating results of data analysis, insights, and ideas in a variety of formats to key decision-makers within the business.\nCreates SQL queries for the analysis of data and visualizes the output of the models.\nResponsible for ensuring that industry standards best practices are applied to development activities.\nKnowledge and Attributes:\nAdvanced understanding of data modelling, statistical methods and machine learning techniques.\nStrong ability to thrive in a dynamic, fast-paced environment.\nStrong quantitative and qualitative analysis skills.\nDesire to acquire more knowledge to keep up to speed with the ever-evolving field of data science.\nCuriosity to sift through data to find answers and more insights.\nAdvanced understanding of the information technology industry within a matrixed organization and the typical business problems such organizations face.\nStrong ability to translate technical findings clearly and fluently to non-technical team business stakeholders to enable informed decision-making.\nStrong ability to create a storyline around the data to make it easy to interpret and understand.\nSelf-driven and able to work independently yet acts as a team player.\nAcademic Qualifications and Certifications:\nBachelors degree or equivalent in Data Science, Business Analytics, Mathematics, Economics, Engineering, Computer Science or a related field.\nRelevant programming certification preferred.\nAgile certification preferred.\nRequired Experience:\nAdvanced demonstrated experience in a data science position in a corporate environment and/or related industry.\nAdvanced demonstrated experience in statistical modelling and data modelling, machine learning, data mining, unstructured data analytics, natural language processing.\nAdvanced demonstrated experience in programming languages (R, Python, etc.).\nAdvanced demonstrated experience working with and creating data architectures.\nAdvanced demonstrated experience with extracting, cleaning, and transforming data and working with data owners to understand the data.\nAdvanced demonstrated experience visualizing and/or presenting data for stakeholder use and reuse across the business.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data science', 'R', 'data modelling', 'data mining', 'statistical modelling', 'machine learning', 'Python', 'SQL']",2025-06-12 14:35:50
Data Scientist,THERMAX,2 - 3 years,Not Disclosed,['Pune'],Job Details:\nWe are seeking a highly motivated and enthusiastic Junior Data Scientist with 2-3 years of experience to join our data science team. This role offers an exciting opportunity to contribute to both traditional Machine Learning projects for our commercial IoT platform and cutting-edge Generative AI initiatives.\n\nExperience,,,,"['Tensorflow', 'Manufacturing', 'Machine Learning', 'IOT', 'Python', 'Pytorch', 'Data Science', 'Aiml', 'Scikit-Learn', 'Ml']",2025-06-12 14:35:53
Specialist Data Scientist,NICE,8 - 11 years,Not Disclosed,['Pune'],"So, what’s the role all about?\nNICE provides state-of-the-art enterprise level AI and analytics for all forms of business communications between speech and digital.   We are a world class research team developing new algorithms and approaches to help companies with solving critical issues such as identifying their best performing agents, preventing fraud, categorizing customer issues, and determining overall customer satisfaction.  If you have interacted with a major contact center in the last decade, it is very likely we have processed your call. \nThe research group partners with all areas of NICE’s business to scale out the delivery of new technology and AI models to customers around the world that are tailored to their company, industry, and language needs.",,,,"['python', 'confluence', 'natural language processing', 'presentation skills', 'big data technologies', 'pyspark', 'microsoft azure', 'bert', 'machine learning', 'sql', 'tensorflow', 'data science', 'gcp', 'pytorch', 'machine learning algorithms', 'aws', 'big data', 'communication skills', 'statistics', 'jira']",2025-06-12 14:35:55
Data Scientist / Machine Learning Professional,Infosys,6 - 8 years,Not Disclosed,['Bengaluru'],"Responsibilities\nJob Title:\nLead Analyst - Data Science (Intermediate/Senior)Role Summary:\nWe are looking for candidates to build and implement analytics solutions to our esteemed clients. The incumbent should have strong aptitude for numbers, experience in any domain and willingness to learn some cutting edge technologies\nTechnical and Professional Requirements:\nOther key to have Skills:\nSQL knowledge and experience working with relational databases.\nUnderstanding of any one of domain (Eg: Retail, Supply chain, Logistics, Manufacturing).\nUnderstanding of the project lifecycles: waterfall and agile.\nSoft Skills:\nStrong verbal and written communication skills with the ability to work well in a team.\nStrong customer focus, ownership, urgency and drive.\nAbility to handle multiple, competing priorities in a fast-paced environment.\nWork well with the team members to maintain high credibility\nWork Experience:\nOverall 6-8 of years of experience in Data Analytics, Data Science and Machine Learning.\nEducational Requirements (any of the following):\nBachelor of Engineering/Bachelor of Technology in any stream with consistent academic track record.\nBachelor's degree in a quantitative discipline (e.g., Statistics, Economics, Mathematics, Marketing Analytics) or significant relevant coursework with consistent academic track record.\nPreferred Skills:\nTechnology->Data Science->Machine Learning\nAdditional Responsibilities:\nAdditional Academic Qualification (good to have):\nMasters in any area related to Science, Mathematics, Statistics, Economy and Finance with consistent academic track record.\nPhD in any stream.Location:\nBangalore, Pune, Hyderabad, Chennai, Trivandrum, Mysore\nEducational Requirements\nMCA,MSc,Bachelor of Engineering,BBA,BCom\nService Line\nData & Analytics Unit\n* Location of posting is subject to business requirements",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Machine Learning', 'Data Science', 'waterfall', 'Data Scientist', 'agile', 'Data Analytics', 'relational databases', 'SQL']",2025-06-12 14:35:58
"Data Scientist with EDA, Python, SQL and Business Analysis",Cognizant,8 - 11 years,Not Disclosed,['Hyderabad'],"Job Summary\nWe are seeking a highly skilled Data Scientist with Business analysis with good hands on in Python , SQL , EDA Data Visualization with 3 to 12+ years of experience\nKey Responsibilities:\nPerform exploratory data analysis (EDA) to uncover trends, patterns, and insights.\nDevelop and maintain dashboards and reports to visualize key business metrics.\nCollaborate with cross-functional teams to gather requirements and deliver data-driven solutions.",,,,"['python', 'eda', 'data analysis', 'data management', 'modeling', 'analytical', 'data manipulation', 'query', 'predictive analytics', 'microsoft azure', 'business analysis', 'cloud platforms', 'business analytics', 'machine learning', 'dashboards', 'sql', 'analytics', 'gcp', 'statistical modeling', 'data visualization', 'aws']",2025-06-12 14:36:00
Data Scientist,Wipro,9 - 14 years,Not Disclosed,"['Pune', 'Bengaluru', 'Delhi / NCR']","We are seeking a candidate with strong working experience in both Data Scientist .The ideal candidate must have utilized Azure services related to data, data engineering, and machine learning. PAN India.\nMachine Learning\n7+ years of relevant experience\nExperience in handling ETRM projects. Good understanding of Power Trading concepts\nMust have good experience in implementing machine learning models such as Prophet, ARIMA, SARIMA, XGBoost, ElasticNet, Ridge, Lasso, Random Forest, and Linear Regression on time-series data.",,,,"['Classification', 'Regression', 'Azure Machine Learning', 'Forecasting', 'Python', 'Pytorch', 'Xgboost', 'Arima', 'Time Series Analysis', 'Clustering', 'Dart', 'Prophet']",2025-06-12 14:36:03
Data Scientist (Offshore),HTC Global Services,2 - 7 years,Not Disclosed,['Chennai'],"We are seeking a Data Scientist (Offshore) with minimum experience of 3 or more years. The ideal candidate should be familiar with relational or NoSQL databases such as Oracle, Teradata, SQL Server, Hadoop and ELK etc.\nRequirements:\nMinimum 3 or more years working with languages such as R, Python or Java\nAt least 3 or more years working with advanced statistical methods such as regressions, classifiers, recommenders, anomaly detection, optimization algorithms, tree methods and neural nets etc.",,,,"['tableau', 'NoSQL', 'Hadoop', 'Agile', 'Teradata SQL', 'data visualization', 'Oracle', 'Powerpoint', 'SDLC', 'Python']",2025-06-12 14:36:05
Data Scientist,"Sourced Group, an Amdocs Company",4 - 9 years,Not Disclosed,['Gurugram'],"0px> Who are we?\nIn one sentence\nThis is a hands-on position for a motivated and talented innovator. The Data Scientist performs data mining and develops algorithms that provide insight from data.\nWhat will your job look like?\nYou will be responsible for and perform end-top-end data-based research.\nYou will craft data mining solutions to be implemented and executed with alignment to the planned scope and design coverage and needs/uses, demonstrating knowledge and a broad understanding of E2E business processes and requirements.\nYou will define the data analytics research plan, scope and resources required to meet the objectives of his/her area of ownership.\nYou will identify and analyze new data analytic directions and their potential business impact to determine the accurate prioritization of data analytics activities based on business needs and analytics value.\nYou will identify data sources, supervises the data collection process and crafts the data structure in collaboration with data experts (BI or big-data) and subject matter and business experts. Ensures that data used in the data analysis activities are of the highest quality.\nYou will construct data models (algorithms and formulas) for required business needs and predictions.\nYou will present results, including the preparation of patents and white papers and facilitating presentations during conferences.\nAll you need is...\nPh.D. in Computer Science, Mathematics or Statistics\n4 years experience in tasks related to data analytics\nKnowledge of telecommunications and of the subject area being investigated - advantage\nKnowledge in the product (ACC or other) application knowledge and configuration knowledge\nKnowledge in BSS, billing, Telco and the business processes\nFamiliarity in the Telco Networking - mobile, landline, cable TV, Internet\nknowledge in Oracle SQL\nWhy you will love this job:\nYou will ensure timely resolution or critical issue within the agreed SLA. This includes creating a positive customer support experience and build strong relationships through problem understanding, presenting promptly on progress, and handling customers with a professional demeanour.\nYou will be able to demonstrates an understanding of key business drivers and ensures strategic directions are followed and the organization succeeds\nWe are a dynamic, multi-cultural organization that constantly innovates and empowers our employees to grow. Our people our passionate, daring, and phenomenal teammates that stand by each other with a dedication to creating a diverse, inclusive workplace!\nWe offer a wide range of stellar benefits including health, dental, vision, and life insurance as well as paid time off, sick time, and parental leave!\n",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Data analysis', 'Bss', 'Networking', 'Billing', 'Data collection', 'Customer handling', 'Customer support', 'Data mining', 'Amdocs']",2025-06-12 14:36:08
"Data Scientist III, ROW AOP",Amazon,5 - 10 years,Not Disclosed,['Hyderabad'],"The AOP (Analytics Operations and Programs) team is responsible for creating core analytics, insight generation and science capabilities for ROW Ops. We develop scalable analytics applications, AI/ML products and research models to optimize operation processes. You will work with Product Managers, Data Engineers, Data Scientists, Research Scientists, Applied Scientists and Business Intelligence Engineers using rigorous quantitative approaches to ensure high quality data/science products for our customers around the world.\n\nWe are looking for a Sr.Data Scientist to join our growing Science Team. As Data Scientist, you are able to use a range of science methodologies to solve challenging business problems when the solution is unclear. You will be responsible for building ML models to solve complex business problems and test them in production environment. The scope of role includes defining the charter for the project and proposing solutions which align with orgs priorities and production constraints but still create impact. You will achieve this by leveraging strong leadership and communication skills, data science skills and by acquiring domain knowledge pertaining to the delivery operations systems. You will provide ML thought leadership to technical and business leaders, and possess ability to think strategically about business, product, and technical challenges. You will also be expected to contribute to the science community by participating in science reviews and publishing in internal or external ML conferences.\n\nOur team solves a broad range of problems that can be scaled across ROW (Rest of the World including countries like India, Australia, Singapore, MENA and LATAM). Here is a glimpse of the problems that this team deals with on a regular basis:\n\nUsing live package and truck signals to adjust truck capacities in real-time\nHOTW models for Last Mile Channel Allocation\nUsing LLMs to automate analytical processes and insight generation\nOps research to optimize middle mile truck routes\nWorking with global partner science teams to affect Reinforcement Learning based pricing models and estimating Shipments Per Route for $MM savings\nDeep Learning models to synthesize attributes of addresses\nAbuse detection models to reduce network losses\n\n\n1. Use machine learning and analytical techniques to create scalable solutions for business problems\nAnalyze and extract relevant information from large amounts of Amazon s historical business data to help automate and optimize key processes\n2. Design, develop, evaluate and deploy, innovative and highly scalable ML/OR models\n3. Work closely with other science and engineering teams to drive real-time model implementations\n4. Work closely with Ops/Product partners to identify problems and propose machine learning solutions\n5. Establish scalable, efficient, automated processes for large scale data analyses, model development, model validation and model maintenance\n6. Work proactively with engineering teams and product managers to evangelize new algorithms and drive the implementation of large-scale complex ML models in production\n7. Leading projects and mentoring other scientists, engineers in the use of ML techniques 5+ years of data scientist experience\nExperience with data scripting languages (e.g. SQL, Python, R etc.) or statistical/mathematical software (e.g. R, SAS, or Matlab)\nExperience with statistical models e.g. multinomial logistic regression\nExperience in data applications using large scale distributed systems (e.g., EMR, Spark, Elasticsearch, Hadoop, Pig, and Hive)\nExperience working with data engineers and business intelligence engineers collaboratively\nDemonstrated expertise in a wide range of ML techniques Experience as a leader and mentor on a data science team\nMasters degree in a quantitative field such as statistics, mathematics, data science, business analytics, economics, finance, engineering, or computer science\nExpertise in Reinforcement Learning and Gen AI is preferred",,,,"['Computer science', 'Publishing', 'SAS', 'Analytical', 'Machine learning', 'Business intelligence', 'MATLAB', 'Distribution system', 'SQL', 'Python']",2025-06-12 14:36:10
Data Scientist,Big Oh Tech,4 - 6 years,Not Disclosed,['Noida'],"Key Responsibilities:\n\nDesign, build, and maintain robust and scalable data pipelines to support analytics and reporting needs.\nManage and optimize data lake architectures, with a focus on Apache Atlas for metadata management, data lineage, and governance.\nIntegrate and curate data from multiple structured and unstructured sources to enable advanced analytics.\nCollaborate with data scientists and business analysts to ensure availability of clean, well-structured data.\nImplement data quality, validation, and monitoring processes across data pipelines.\nDevelop and manage Power BI datasets and data models, supporting dashboard and report creation.\nSupport data cataloging and classification using Apache Atlas for enterprise-wide discoverability and compliance.\nEnsure adherence to data security, privacy, and compliance policies.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['advanced analytics', 'metadata', 'Compliance', 'Business Analyst', 'data security', 'power bi', 'Data quality', 'Management', 'Apache', 'Monitoring']",2025-06-12 14:36:13
Data Scientist,Ltimindtree,7 - 12 years,Not Disclosed,['Hyderabad'],Data Scientist\n\nJob Description\n\nResponsibilities\n\nWork with team members across multiple disciplines to understand the data behind product features user behaviors the security landscape and our goals\nAnalyze data from several large sources then automate solutions using scheduled processes models and alerts\nWork with partners to design and improve metrics that guide our decisions for the product\nDetect patterns associated with fraudulent accounts and anomalous behavior\nSolve scientific problems and create new methods independently\nTranslate requirements and security questions into data insights\nSet up alerting mechanisms so our leadership is always aware of the security posture\n\nQualifications\n\nPostgraduate degree with specialization in machine learning artificial intelligence statistics or related fields or 2 years of equivalent work experience in applied machine learning and analytics\nExperience with SQL Snowflake and NoSQL databases\nProficiency in Python programming\nFamiliarity with statistics modeling and data visualization\n\nExperience\n\nExperience building statistical and machine learning models applying techniques such as regression classification clustering and anomaly detection Time series and Classical ML modeling\nFamiliarity with Snowflake SQL\nFamiliarity with cloud platforms such as AWS\nSome experience to software development or data engineering\nAnalyze business problems or research questions identify relevant data points and extract meaningful insights,Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Machine Learning', 'Snowflake Sql', 'AWS']",2025-06-12 14:36:15
Data Scientist,Celebal Technologies,4 - 9 years,20-35 Lacs P.A.,"['Mumbai', 'Navi Mumbai', 'Pune']","Job Summary: We are looking for a highly skilled Data Scientist with deep expertise in time series forecasting, particularly in demand forecasting and customer lifecycle analytics (CLV). The ideal candidate will be proficient in Python or PySpark, have hands-on experience with tools like Prophet and ARIMA, and be comfortable working in Databricks environments. Familiarity with classic ML models and optimization techniques is a plus. Key Responsibilities • Develop, deploy, and maintain time series forecasting models (Prophet, ARIMA, etc.) for demand forecasting and customer behavior modeling. • Design and implement Customer Lifetime Value (CLV) models to drive customer retention and engagement strategies. • Process and analyze large datasets using PySpark or Python (Pandas). • Partner with cross-functional teams to identify business needs and translate them into data science solutions. • Leverage classic ML techniques (classification, regression) and boosting algorithms (e.g., XGBoost, LightGBM) to support broader analytics use cases. • Use Databricks for collaborative development, data pipelines, and model orchestration. • Apply optimization techniques where relevant to improve forecast accuracy and business decision-making. • Present actionable insights and communicate model results effectively to technical and non-technical stakeholders. Required Qualifications • Strong experience in Time Series Forecasting, with hands-on knowledge of Prophet, ARIMA, or equivalent Mandatory. • Proven track record in Demand Forecasting Highly Preferred. • Experience in modeling Customer Lifecycle Value (CLV) or similar customer analytics use cases – Highly Preferred. • Proficiency in Python (Pandas) or PySpark – Mandatory. • Experience with Databricks – Mandatory. • Solid foundation in statistics, predictive modeling, and machine learning",,,,"['Machine Learning', 'Data Bricks', 'Optimization', 'Pricing', 'Time Series', 'Pyspark', 'Arima', 'Classic ML', 'Artificial Intelligence', 'Regression', 'Customer Lifecycle', 'Manufacturing Industry', 'Regression Modeling', 'Forecasting', 'Data Science', 'Xgboost', 'Time Series Analysis', 'Pandas', 'Classical', 'Python', 'Prophet']",2025-06-12 14:36:18
Data Scientist,Mindpro Technologies,4 - 9 years,5-12 Lacs P.A.,"['Karur', 'Dharwad']","Greetings From Mind Pro Technologies Pvt ltd (www.mindprotech.com)\n\nJob Title : Data Scientist\nWork Location : Karur (Tamil Nadu) or Dharwad (Karnataka )\nNp : 15days or Less\n\n\nJOB DESCRIPTION:\n Must have At least 4+ Years of experience in Python with Data Science.\n Must have worked on at least one Live project.\nExperience in relevant field such as Statistics, Computer Science or Applied Math or Operational Research.\nMust have Masters in (Maths/Statistics or Applied Mathematics/Machine Learning etc.)\nHistory of successfully performing customer implementations\nStrong customer facing skills, and previous consulting experience.\nExperience of handling high frequency streaming data for real time analysis and reporting.\nFamiliarity with - Natural Language Processing, Statistical Analysis (distribution analysis, correlation, variance, deep learning.\nExperience in tools like AWS, IBM Watson is a plus.\nExperience with open source technologies is a must.\nExcellent communication\nAbility to lead & build strong teams\nAbility to work in an ambiguous environment\n\nDesired Skills and Experience\nLanguages/Tools: Python/R.\nApproaches: Machine Learning\nConcepts: Supervised ANN, Bayesian, Gaussian, Vector Quantization, Logistic Model, Statistical, Predictive Modeling, Minimum Message Length, SVM, Random Forest, Ensembles, ANOVA, Decision Trees, Hidden Markov Models\nUnsupervised ANN, ARL, Clustering Hierarchical, Cluster Analysis\nReinforcement\nGen AI, LLM, LSTM, RNN, CNN, KNN\nBig Data (Good to have): Hadoop /Kafka / Storm / Spark streaming\nOS: Linux, Windows 32/64 bits.\n\nNote:  should know supervised and unsupervised learning,   semi-supervised learning, neural networks concepts, and how ML algorithms works with training and testing data. Experience on particular data set to train, test and roll-out for production use\n\nTool sets : Python, R, MATLAB or  any AI frame work, Neural network, Gen AI, LLM\nContact Details:\n\nRecruitment Team\nMindpro Technologies Pvt Ltd (www.mindprotech.com)\n+91-04324-240904 / +91-9600672304",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Gen AI', 'Statistical Modeling', 'LLM', 'Predictive Modeling', 'Artificial Intelligence', 'Natural Language Processing', 'Neural Networks', 'Machine Learning', 'Deep Learning', 'Python']",2025-06-12 14:36:20
Data Scientist IV - Python / LLM,Sadup Soft,6 - 8 years,Not Disclosed,['Hyderabad'],"Must have skills :\n\n- 6+ Years of Experience.\n\n- Statics, SQL, Big query, LLM, AI, Python\n\n- Work experience in the payments, ecommerce, or financial services industry is a plus .\n\nResponsibilities :\n\n- At least 6 years of experience analyzing large, multi-dimensional data sets and synthesizing insights into actionable solutions\n\n- Bachelor's/Master's degree in a quantitative field (such as Analytics, Statistics, Mathematics, Economics or Engineering) or equivalent field experience\n\n- Advanced SQL experience, preferable with Big Query analytics (Google Cloud) on Jupyter Notebooks and experience analyzing very large, complex, multi-dimensional data sets.\n\n- Understanding of statistics (e.g hypothesis testing, statistical inference, regression) and experience designing and evaluating complex experiments-\n\n- Ability to solve problems analytically and create actionable recommendations\n\n- Advanced ability to use reporting tools like Tableau and/or Excel to share analysis\n\n- Strong written and verbal communication skills with the ability to translate complex problems into simpler terms, expertise in stitching together findings to convey coherent insights and effectively influence both peers and senior leadership\n\n- Prior work experience in a product analytics space would be highly valued\n\n- A passion for problem-solving and comfort with ambiguity\n\n- Work experience in the payments, ecommerce, or financial services industry is a plus.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Python', 'Data Science', 'BigQuery', 'Data Management', 'Jupyter', 'LLM', 'Statistics']",2025-06-12 14:36:22
Data Scientist For DMAI,Prodapt Solutions,2 - 5 years,Not Disclosed,['Chennai'],"Overview\n\nThe Senior Data Science Engineer will leverage advanced data science techniques to solve complex business problems, guide decision-making processes, and mentor junior team members. This role requires a combination of technical expertise in data analysis, machine learning, and project management skills.\n\nResponsibilities\n\n Data Analysis and Modeling Analyze large-scale telecom datasets to extract actionable insights and build predictive models for network optimization and customer retention.\n Conduct statistical analyses  to validate models and ensure their effectiveness.\n Machine Learning Development Design and implement machine learning algorithms for fraud detection, churn prediction, and network failure analysis.\n Telecom-Specific Analytics Apply domain knowledge to improve customer experience by analyzing usage patterns, optimizing services, and predicting customer lifetime value.\n ETL Processes Develop robust pipelines for extracting, transforming, and loading telecom data from diverse sources.\n Collaboration Work closely with data scientists, software engineers, and telecom experts to deploy solutions that enhance operational efficiency.\n Data Governance :  Ensure data integrity, privacy, security and compliance with industry standards\n\n\nAdvanced degree in Data Science, Statistics, Computer Science, or a related field.\nExtensive experience in data science roles with a strong focus on machine learning and statistical modeling.\nProficiency in programming languages such as Python or R and strong SQL skills.\nFamiliarity with big data technologies (e.g., Hadoop, Spark) is advantageous.\nExpertise in cloud platforms such as AWS or Azure.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['project management', 'data analysis', 'machine learning', 'sql', 'statistical modeling', 'algorithms', 'python', 'big data technologies', 'microsoft azure', 'cloud platforms', 'r', 'data science', 'spark', 'data governance', 'hadoop', 'aws', 'etl', 'machine learning algorithms', 'statistics']",2025-06-12 14:36:25
Data Scientist,Swits Digital,5 - 12 years,Not Disclosed,['Chennai'],"Job Title: Data Scientist\nLocation: Chennai\nExperience: 5-12 Years\nJob Summary:\nWe are seeking a highly analytical and results-driven Data Scientist with a strong background in statistics , machine learning , and data science , combined with domain knowledge in mechanical engineering and cost analysis . The ideal candidate will have experience working with Google Cloud Platform (GCP) and will play a key role in transforming engineering and operational data into actionable insights to drive business decisions.\nRequired Skills & Experience:\nStrong knowledge of statistics , machine learning , and data science principles\nHands-on experience with Google Cloud Platform (GCP) , especially BigQuery , Vertex AI , and Cloud Functions\nProficiency in Python or R for data analysis and modeling\nSolid understanding of mechanical engineering concepts and their application in data analysis\nExperience with cost modeling , cost-benefit analysis , or operational performance analytics\nExcellent problem-solving , analytical thinking , and communication skills\nAbility to work with large datasets and create clear, actionable insights",Industry Type: Recruitment / Staffing,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data analysis', 'data science', 'GCP', 'Analytical', 'Machine learning', 'Cost benefit analysis', 'Operations', 'Mechanical engineering', 'Analytics', 'Python']",2025-06-12 14:36:27
Data Scientist,H3 Technologies,3 - 8 years,Not Disclosed,['Thiruvananthapuram'],"Position: Data Scientist\nLocation: Trivandrum\nJob Description :\nWe are urgently looking for a motivated Data Scientist with a focus on Computer Vision and Machine Learning. The candidate will have a passion for solving complex problems using deep learning, image processing, and AI-driven techniques. He shall work closely with a team of data scientists, engineers, etc and to build, optimize, and deploy machine learning models for real-world applications\nKey Responsibilities :\nDevelop, train, and optimize deep learning models for image classification, object detection, segmentation, and other computer vision tasks.\nImplement and fine-tune machine learning algorithms for structured and unstructured data analysis.\nPreprocess and augment image/video datasets to improve model accuracy and robustness.\nWork with frameworks such as YOLO, TensorFlow, PyTorch, and OpenCV to build scalable models.\nAssist in deploying models to production environments, including cloud and edge computing platforms.\nCollaborate with cross-functional teams to integrate AI solutions into existing workflows and products.\nStay up-to-date with the latest research and trends in AI, computer vision, and machine learning.\nQualifications :\nBachelors or masters degree in computer science, Data Science, AI/ML, or a related field.\nMinimum of 3 year of professional experience in Python programming and AI/ML integrations\nSolid understanding of machine learning concepts, neural networks, and deep learning architectures.\nHands-on experience in training and optimizing computer vision models.\nFamiliarity with data preprocessing techniques, image annotation tools, and model evaluation metrics.\nStrong problem-solving skills and the ability to work in a fast-paced environment.\nJoining: Immediate to less than 30 days\nBudget: 13 - 14 LPA\n"",",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Computer vision', 'deep learning', 'Data analysis', 'Image processing', 'data science', 'Neural networks', 'Machine learning', 'Budgeting', 'Python']",2025-06-12 14:36:30
Specialist Data Scientist,Atlasrtx,3 - 7 years,Not Disclosed,['Pune'],"So, what s the role all about\n\nNICE provides state-of-the-art enterprise level AI and analytics for all forms of business communications between speech and digital. We are a world class research team developing new algorithms and approaches to help companies with solving critical issues such as identifying their best performing agents, preventing fraud, categorizing customer issues, and determining overall customer satisfaction. If you have interacted with a major contact center in the last decade, it is very likely we have processed your call.\n\nThe research group partners with all areas of NICE s business to scale out the delivery of new technology and AI models to customers around the world that are tailored to their company, industry, and language needs.\n\n\nHow will you make an impact\n\nConduct cutting-edge research and develop advanced NLP algorithms and models.\n\nBuild and fine-tune deep learning and machine learning models, with a focus on large language models.\n\nWork closely with internal stakeholders to define model requirements and ensure alignment with business objectives.\n\nDevelop AI predictive models and perform data and model accuracy analyses.\n\nProduce and present findings, technical concepts, and model recommendations to both technical and non-technical stakeholders.\n\nDevelop and maintain scripts/tools to automate both new model production and updates to existing model packages.\n\nStay abreast of the latest advancements in data science research and contribute to the development of our knowledge base.\n\nCollaborate with developers to design automation and tool improvements for model building.\n\nMaintain documentation of processes and projects across all supported languages and environments.\n\n\nHave you got what it takes\n\nMasters degree in the field of Computer Science, Technology, Engineering, Math, or equivalent practical experience\n\nMinimum of 8 years of data science work experience, including implementing machine learning and NLP models using real-life data.\n\nExperience with Retrieval-Augmented Generation (RAG) pipelines or LLMOps.\n\nAdvanced knowledge of statistics and machine learning algorithms.\n\nProficiency in Python programming and familiarity with R.\n\nExperience with deep learning models and libraries such as PyTorch, TensorFlow, and JAX.\n\nFamiliarity with relational databases and query languages (e. g. , MSSQL) and basic SQL knowledge.\n\nHands-on experience with transformer models (BERT, FlanT5, Llama, etc. ) and GenAI frameworks (HuggingFace, LangChain, Ollama, etc. ).\n\nExperience deploying NLP models in production environments, ensuring scalability and performance using AWS/GCP/Azure\n\nStrong verbal and written communication skills, including effective presentation abilities.\n\nAbility to work independently and as part of a team, demonstrating analytical thinking and problem-solving skills.\n\n\n\nYou will have an advantage if you also have:\n\nExpertise with Big Data technologies (e. g. , PySpark).\n\nBackground in knowledge graphs, graph databases, or GraphRAG architectures.\n\nUnderstanding of multimodal models (text, audio, vision).\n\nExperience in Customer Experience domains.\n\nExperience with package development and technical writing.\n\nFamiliarity with tools like Jira, Confluence, and source control packages and methodology.\n\nKnowledge and interest in foreign languages and linguistics.\n\nExperience working on international, globe-spanning teams and with AWS.\n\nPast participation in a formal research setting.\n\nExperience as part of a software organization.\n\n\n\nWhat s in it for you\n\n\n\nEnjoy NICE-FLEX!\n\n\n\nRequisition ID : 7481\nReporting into : Tech Manager\nRole Type : Individual Contributor\n\nAbout NICE",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Automation', 'Technical writing', 'GCP', 'Analytical', 'Machine learning', 'Flex', 'Analytics', 'SQL', 'Python']",2025-06-12 14:36:32
Data Scientist,Celebal Technologies,3 - 6 years,Not Disclosed,"['Navi Mumbai', 'Mumbai (All Areas)']","Job Title: Data Scientists\nLocation: Navi Mumbai\nDuration: Fulltime\nPositions: Multiple\n\nWe are looking for a highly skilled Data Scientists with deep expertise in time series forecasting, particularly in demand forecasting and customer lifecycle analytics (CLV). The ideal candidate will be proficient in Python or PySpark, have hands-on experience with tools like Prophet and ARIMA, and be comfortable working in Databricks environments. Familiarity with classic ML models and optimization techniques is a plus.",,,,"['Demand Forecasting', 'Data Bricks', 'Time Series', 'Pyspark', 'Arima', 'Customer Lifecycle', 'Forecasting', 'Machine Learning', 'Optimization', 'Data Science', 'Xgboost', 'Time Series Analysis', 'Prophet', 'Python']",2025-06-12 14:36:35
Data Scientist,Celebal Technologies,3 - 6 years,Not Disclosed,"['Mumbai', 'Navi Mumbai']","About Us: Celebal Technologies is a leading Solution Service company that provide Services the field of Data Science, Big Data, Enterprise Cloud & Automation. We are at the forefront of leveraging cuttingedge technologies to drive innovation and enhance our business processes. As part of our commitment to staying ahead in the industry, we are seeking a talented and experienced Data & AI Engineer with strong Azure cloud competencies to join our dynamic team.\n\nJob Summary: We are looking for a highly skilled Data Scientist with deep expertise in time series forecasting, particularly in demand forecasting and customer lifecycle analytics (CLV). The ideal candidate will be proficient in Python or PySpark, have hands-on experience with tools like Prophet and ARIMA, and be comfortable working in Databricks environments. Familiarity with classic ML models and optimization techniques is a plus.\n\nKey Responsibilities\n• Develop, deploy, and maintain time series forecasting models (Prophet, ARIMA, etc.) for demand forecasting and customer behavior modeling.\n• Design and implement Customer Lifetime Value (CLV) models to drive customer retention and engagement strategies.\n• Process and analyze large datasets using PySpark or Python (Pandas).\n• Partner with cross-functional teams to identify business needs and translate them into data science solutions.\n• Leverage classic ML techniques (classification, regression) and boosting algorithms (e.g., XGBoost, LightGBM) to support broader analytics use cases.\n• Use Databricks for collaborative development, data pipelines, and model orchestration.\n• Apply optimization techniques where relevant to improve forecast accuracy and business decision-making.\n• Present actionable insights and communicate model results effectively to technical and non-technical stakeholders.\n\nRequired Qualifications\n• Strong experience in Time Series Forecasting, with hands-on knowledge of Prophet, ARIMA, or equivalent Mandatory.\n• Proven track record in Demand Forecasting Highly Preferred.\n• Experience in modeling Customer Lifecycle Value (CLV) or similar customer analytics use cases Highly Preferred.\n• Proficiency in Python (Pandas) or PySpark Mandatory.\n• Experience with Databricks Mandatory.\n• Solid foundation in statistics, predictive modeling, and machine learning",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Machine Learning Operations', 'Demand Forecasting', 'Data Bricks', 'Pyspark', 'Large Language Model', 'Time Series', 'Spark', 'Machine Learning', 'Python']",2025-06-12 14:36:37
Data Scientist,Apcfss,2 - 6 years,Not Disclosed,"['Vijayawada', 'Guntur', 'Mangalagiri']","Location: Vijayawada, Andhra Pradesh\nExperience: 2 to 6 years\nEmployment Type: Full-Time\n\nJob Opening: Data Scientist\nWe are seeking a data-driven problem solver to join our team as a Data Scientist. You will play a key role in transforming data into actionable insights and building models that support strategic decisions across the organization. Collaborating with cross-functional teams, youll help turn complex data into clear value.\nKey Responsibilities\nAnalyze large and complex datasets to uncover trends, patterns, and insights\nBuild, validate, and deploy predictive and statistical models\nWork closely with engineering and product teams to integrate models into production systems\nCommunicate analytical findings and insights clearly to both technical and non-technical stakeholders\nRequirements\nProficiency in Python or R, and strong command of SQL\nHands-on experience with machine learning and statistical modeling\nStrong analytical and problem-solving skills\nExperience with cloud platforms such as AWS, GCP, or Azure\nNice to Have\nExperience in Natural Language Processing (NLP), deep learning, or time-series forecasting\nPrior work in [industry-specific domain, e.g., fintech, healthcare, e-commerce]",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Python', 'GCP', 'Machine Learning', 'AWS', 'Deep Learning', 'SQL']",2025-06-12 14:36:39
Data Scientist,Callaway Digital Technologies,6 - 9 years,Not Disclosed,['Hyderabad'],"JOB OVERVIEW\nThe ideal candidate will be responsible for analyzing and interpreting large data sets related to finance, sales and supply chain operations to optimize business processes, identify opportunities for improvement, and provide strategic insights to support decision-making. The Data Scientist will work closely with cross-functional teams to identify key business questions, design and implement statistical models, and develop innovative data-driven solutions.\nKey Responsibilities:",,,,"['Statistical Modeling', 'Machine Learning', 'Python', 'Data Visualization', 'Azzure', 'R Program', 'SQL']",2025-06-12 14:36:41
Data Scientist,An Indian NBFC,3 - 8 years,Not Disclosed,['Chennai'],"Responsibilities:\nCollect, clean, and analyze large sets of structured and unstructured data to extract meaningful insights and trends\nDevelop and implement advanced machine learning algorithms to solve complex business problems\nSupport moving models to production, by creating high quality code modules that can be seamlessly integrated into existing systems (both on-prem and cloud)\nCommunicate complex findings to both technical and non-technical audiences through effective data visualization and storytelling.\nCollaborate with cross-functional teams to identify data-driven opportunities and translate business requirements into actionable data solutions.\nSupport the development and maintenance of data pipelines and infrastructure\nStay up-to-date with industry trends and advancements in Data Science and Machine Learning technologies.\n\nSkills Required:\nStrong foundation in statistics, and machine learning algorithms\nStrong proficiency in programming languages like Python and SQL.\nExcellent problem-solving and analytical skills.\nAbility to work independently and as part of a team.\nShould have built production models using at least 2 of the ML techniques: Clustering, Regression, Classification\nExperience in Banking & Financial Services is preferred.\nExperience working on cloud platforms (e.g., AWS, GCP) is preferred.\nA passion for data and a curiosity to explore new trends and technologies",Industry Type: NBFC,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Pipeline', 'Data Extraction', 'Model Building', 'Artificial Intelligence', 'Cloud', 'Machine Learning']",2025-06-12 14:36:43
Business Analyst/ Data Scientist - SAS & SQL,Khushboo,3 - 8 years,10-20 Lacs P.A.,['Hyderabad'],"hands on SQL/ SAS programming experience & handling complex/large data\nMust have experience inTableau/Power BI\nExperience in campaign performance measurement, customer targeting framework\nProven ability to design and lead strategic projects\n\nRequired Candidate profile\nMust - SAS , SQL, Python\nGood in Statistical model , Predictive model, Logistic regression, Linear regression\nBFSI Mandatory - Credit risk, Credit Card, Retail Banking",Industry Type: Financial Services,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Decision Tree', 'sas', 'sql', 'Advanced Analytics', 'Strategy Building', 'Predictive Modeling', 'python', 'Logistic Regression', 'Segmentation', 'Random Forest', 'Linear Regression', 'Classification', 'Statistical Modeling', 'Credit Risk']",2025-06-12 14:36:46
Data Scientist,Jsg. Consulting. Pvt.Ltd.,3 - 5 years,9.6-10.8 Lacs P.A.,['Jaipur'],"Familiarity with MDM (Meter Data Management), HES, and utility billing systems.\nExposure to AMI events analysis, load curves, and customer behavior analytics.\nKnowledge of regulatory requirements, data retention, and data .",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['billing exceptions', 'load profiling', 'Machine Learning', 'Meter Data Management', 'Smart Metering', 'Hes']",2025-06-12 14:36:48
"Senior Data Scientist (AI/ML, Data Analysis, Cloud (AWS), and Model",Synechron,8 - 13 years,Not Disclosed,['Pune'],"job requisition idJR1027352\n\nJob Summary\nSynechron is seeking an analytical and innovative Senior Data Scientist to support and advance our data-driven initiatives. The ideal candidate will have a solid understanding of data science principles, hands-on experience with AI/ML tools and techniques, and the ability to interpret complex data sets to deliver actionable insights. This role contributes to the organizations strategic decision-making and technology innovation by applying advanced analytics and machine learning models in a collaborative environment.\n\nSoftware\n\nRequired\n\nSkills:\nPython (including libraries such as pandas, scikit-learn, TensorFlow, PyTorch) proficiency in developing and deploying models\nR (optional, but preferred)\nData management tools (SQL, NoSQL databases)\nCloud platforms (preferably AWS or Azure) for data storage and ML deployment\nJupyter Notebooks or similar interactive development environments\nVersion control tools such as Git\nPreferred\n\nSkills:\nBig data technologies (Spark, Hadoop)\nModel deployment tools (MLflow, Docker, Kubernetes)\nData visualization tools (Tableau, Power BI)\nOverall Responsibilities\nAnalyze and interpret large and complex data sets to generate insights for business and technology initiatives.\nAssist in designing, developing, and implementing AI/ML models and algorithms to solve real-world problems.\nCollaborate with cross-functional teams including data engineers, software developers, and business analysts to integrate models into production systems.\nStay current with emerging trends, research, and best practices in AI/ML/Data Science and apply them to ongoing projects.\nDocument methodologies, modeling approaches, and insights clearly for technical and non-technical stakeholders.\nSupport model validation, testing, and performance monitoring to ensure accuracy and reliability.\nContribute to the development of data science workflows and standards within the organization.\nPerformance Outcomes:\nAccurate and reliable data models that support strategic decision-making.\nClear documentation and communication of findings and recommendations.\nEffective collaboration with technical teams to deploy scalable models.\nContinuous adoption of best practices in AI/ML and data management.\nTechnical Skills (By Category)\n\nProgramming Languages:\nEssential: Python (best practices in ML development), SQL\nPreferred: R, Java (for integration purposes)\nDatabases/Data Management:\nSQL databases, NoSQL (MongoDB, Cassandra)\nCloud data storage solutions (AWS S3, Azure Blob Storage)\nCloud Technologies:\nAWS (S3, EC2, SageMaker, Lambda)\nAzure Machine Learning (preferred)\nFrameworks & Libraries:\nTensorFlow, PyTorch, scikit-learn, Keras, XGBoost\nDevelopment Tools & Methodologies:\nJupyter Notebooks, Git, CI/CD pipelines\nAgile and Scrum processes\nSecurity Protocols:\nBest practices in data security and privacy, GDPR compliance\nExperience\n8+ years of professional experience in AI, ML, or Data Science roles.\nProven hands-on experience designing and deploying ML models in real-world scenarios.\nDemonstrated ability to analyze complex data sets and translate findings into business insights.\nPrevious experience working with cloud-based data science solutions is preferred.\nStrong portfolio showcasing data science projects, models developed, and practical impact.\nAlternative Pathways:\nCandidates with extensive research or academic experience in AI/ML can be considered, provided they demonstrate practical application of skills.\n\nDay-to-Day Activities\nConduct data exploration, cleaning, feature engineering, and model development.\nCollaborate with data engineers to prepare data pipelines for model training.\nBuild, validate, and refine machine learning models.\nPresent insights, models, and recommendations to technical and business stakeholders.\nSupport deployment of models into production environments.\nMonitor model performance and iterate to improve effectiveness.\nParticipate in team meetings, project planning, and reviewing progress.\nDocument methodologies and maintain version control of codebase.\nQualifications\nBachelors degree in Computer Science, Mathematics, Statistics, Data Science, or a related field; Masters or PhD highly desirable.\nEvidence of relevant coursework, certifications, or professional training in AI/ML.\nProfessional certifications (e.g., AWS Certified Machine Learning Specialty, Microsoft Certified Data Scientist) are a plus.\nCommitment to ongoing professional development in AI/ML methodologies.\nProfessional Competencies\nStrong analytical and critical thinking to solve complex problems.\nEffective communication skills for technical and non-technical audiences.\nDemonstrated ability to work collaboratively in diverse teams.\nAptitude for learning new tools, techniques, and technologies rapidly.\nInnovation mindset with a focus on applying emerging research.\nStrong organizational skills to manage multiple projects and priorities.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['java', 'data science', 'python', 'deploying models', 'aws', 'continuous integration', 'kubernetes', 'scikit-learn', 'ci/cd', 'artificial intelligence', 'sql', 'docker', 'tensorflow', 'spark', 'pytorch', 'keras', 'hadoop', 'big data', 'mongodb', 'microsoft azure', 'nosql', 'pandas', 'amazon ec2', 'r', 'cassandra', 'agile']",2025-06-12 14:36:50
Assistant Data Scientist,Rocket Software,0 - 1 years,Not Disclosed,['Pune'],"Face to Face interview in Pune . Please apply only if you are available for a Face to Face interview .\n\nJob highlights\n\nRequired Qualifications . 0 -2 years of relevant industry experience or fresh graduates are welcome to apply.\nBasic experience or understanding in applying Data Science methodologies to extract, process, and transform data from multiple sources.\nPreferred Qualifications . Bachelors degree in Data Science , AI, Statistics ,Computer Science, Economics, or a directly related field.\n\nEssential Duties and Responsibilities\n\nAssist in developing, fine-tuning, and deploying machine learning models.\nAid in consulting with key internal and external stakeholders to understand and frame model requirements and potential applications.\nParticipate in the development of sound analytic plans based on available data sources, business partner needs, and required timelines.\nWork with software engineers in integrating trained models into end-user applications.\nHelp manage deliverables across multiple projects in a deadline-driven environment.\nPresent results, insights, and recommendations to both technical and non-technical stakeholders.\n\nRequired Qualifications\n\n0 -2 years of relevant industry experience or fresh graduates are welcome to apply.\nGood knowledge of Python and Linux, familiarity with ML frameworks, and a willingness to learn.\nDemonstrated problem-solving abilities and creative thinking.\nBasic experience or understanding in applying Data Science methodologies to extract, process, and transform data from multiple sources.\nExcellent communication and interpersonal skills.\nMust be comfortable working in a team-oriented environment.\n\nPreferred Qualifications\n\nBachelor's degree in Statistics, Computer Science, Economics, or a directly related field.\nMasters degree or current enrollment in a Masters program in Statistics, Computer Science, Mathematics, Economics, or directly related fields is a plus.\nDemonstrated passion for continued learning and innovation.\nAs a Data Science Assistant, we expect not just skills and qualifications, but also an enthusiasm for learning and growing within our team. We value those who are adaptable, innovative, and ready to take on challenges in a fast-paced work environment.\n\nDiversity, Inclusion & Equity\n\nAt Rocket we are committed to an inclusive workplace environment, where every Rocketeer can thrive by bringing their full selves to work. Being a Rocketeer means you are part of our movement to continually drive inclusivity, diversity and equity in our workforce.",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'NLP', 'Natural Language Processing', 'Artificial Intelligence', 'Machine Learning', 'Python']",2025-06-12 14:36:52
"Data Scientist II, Regulatory Intelligence, Safety, and Compliance",Amazon,2 - 7 years,Not Disclosed,['Bengaluru'],"About Amazon Regulatory Intelligence, Safety, and Compliance (RISC).\n\nAmazon RISC s vision is to make Amazon the Earth s most trusted shopping destination for safe and compliant products. Towards this mission, we take a science-first approach to building technology, products and services, that protect customers from unsafe, illegal, controversial, or policy-violating products while offering the optimal selling partner experience.\n\nJob Summary\n\nWe are seeking an exceptional Data Scientist to join a team of experts in the field of AI/ML, and work together to tackle challenging business problems across diverse compliance domains. We leverage and train state-of-the-art multi-modal, large-language-models (LLMs), and vision language models (VLMs) to detect illegal and unsafe products across the Amazon catalog. We work on machine learning problems for generative AI, agentic system, multi-modal classification, intent detection, information retrieval, anomaly and fraud detection.\n\nThis is an exciting and challenging position to deliver scientific innovations into production systems at Amazon-scale to make immediate, meaningful customer impacts while also pursuing ambitious, long-term research. You will work in a highly collaborative environment where you can analyze and process large amounts of images, texts, documents, and tabular data. You will work on hard science problems that have not been solved before, conduct rapid prototyping to validate your hypothesis, and deploy your algorithmic ideas at scale. There will be something new to learn every day as we work in an environment with rapidly evolving regulations and adversarial actors looking to outwit your best ideas.\n\n\nDesign and evaluate state-of-the-art algorithms and approaches in generative AI, agentic system, multi-modal classification, intent detection, information retrieval, anomaly and fraud detection.\nTranslate product and CX requirements into measurable science problems and metrics.\nCollaborate with product and tech partners and customers to validate hypothesis, drive adoption, and increase business impact\nKey author in writing high quality scientific papers in internal and external peer-reviewed conferences.\n\nA day in the life\nUnderstanding customer problems, project timelines, and team/project mechanisms\nProposing science formulations and brainstorming ideas with team to solve business problems\nWriting code, and running experiments with re-usable science libraries\nReviewing labels and audit results with investigators and operations associates\nSharing science results with science, product and tech partners and customers\nWriting science papers for submission to peer-review venues, and reviewing science papers from other scientists in the team.\nContributing to team retrospectives for continuous improvements\nDriving science research collaborations and attending study groups with scientists across Amazon\n\nAbout the team\nWe are a team of scientists and engineers building AI/ML solutions to make Amazon the Earth s most trusted shopping destination for safe and compliant products. PhD, or Masters degree with 2+ years of machine learning experience, or bachelor degree with 3+ years of machine learning experience\nExperience programming in Python, Java, C++, or related language\nExperience with neural deep learning methods, LLM, and natural language processing\nExperience with conducting research in a corporate setting Experience with large scale machine learning systems such as profiling and debugging and understanding of system performance and scalability",,,,"['deep learning', 'C++', 'Debugging', 'Machine learning', 'Information retrieval', 'Natural language processing', 'Scientist II', 'Fraud detection', 'Auditing', 'Python']",2025-06-12 14:36:55
"Data Scientist II, Regulatory Intelligence, Safety, and Compliance",Amazon,2 - 7 years,Not Disclosed,['Bengaluru'],"We are seeking an exceptional Data Scientist to join a team of experts in the field of machine learning, and work together to tackle challenging problems across diverse compliance domains. We leverage multi-modal and large-language-models (LLMs) to detect illegal and unsafe products across the Amazon catalog. We work on machine learning problems for multi-modal classification, intent detection, information retrieval, anomaly and fraud detection, and generative AI.\n\nThis is an exciting and challenging position to deliver scientific innovations into production systems at Amazon-scale to make immediate, meaningful customer impacts while also pursuing ambitious, long-term research. You will work in a highly collaborative environment where you can analyze and process large amounts of image, text and tabular data. You will work on hard science problems that have not been solved before, conduct rapid prototyping to validate your hypothesis, and deploy your algorithmic ideas at scale. There will be something new to learn every day as we work in an environment with rapidly evolving regulations and adversarial actors looking to outwit your best ideas.\n\n\nExplore and evaluate state-of-the-art algorithms and approaches in multi-modal classification, large language models (LLMs), intent detection, information retrieval, anomaly and fraud detection, and generative AI\nTranslate product and CX requirements into measurable science problems and metrics.\nCollaborate with product and tech partners and customers to validate hypothesis, drive adoption, and increase business impact\n\nA day in the life\nUnderstanding customer problems, project timelines, and team/project mechanisms\nProposing science formulations and brainstorming ideas with team to solve business problems\nWriting code, and running experiments with re-usable science libraries\nReviewing labels and audit results with investigators and operations associates\nSharing science results with science, product and tech partners and customers\nContributing to team retrospectives for continuous improvements\nParticipating in science research collaborations and attending study groups with scientists across Amazon\n\nAbout the team\nWe are a team of scientists building AI/ML solutions to make Amazon Earth s most trusted shopping destination for safe and compliant products. 2+ years of data scientist experience\n2+ years of data querying languages (e.g. SQL), scripting languages (e.g. Python) or statistical/mathematical software (e.g. R, SAS, Matlab, etc.) experience\n2+ years of machine learning/statistical modeling data analysis tools and techniques, and parameters that affect their performance experience\nExperience applying theoretical models in an applied environment\nKnowledge of relevant statistical measures such as confidence intervals, significance of error measurements, development and evaluation data sets, etc. Experience in Python, Perl, or another scripting language\nExperience in a ML or data scientist role with a large technology company",,,,"['Data analysis', 'Statistical modeling', 'SAS', 'Machine learning', 'Information retrieval', 'Perl', 'MATLAB', 'Fraud detection', 'Auditing', 'Python']",2025-06-12 14:36:57
"Associate Scientist, Data Sourcing & Solutions",XL India Business Services Pvt. Ltd,1 - 5 years,Not Disclosed,"['Hyderabad', 'Ahmedabad', 'Bengaluru']","Associate Scientist - Data Sourcing & Solutions Gurgaon/Bangalore, India AXA XL recognises data and information as critical business assets, both in terms of managing risk and enabling new business opportunities\n\nThis data should not only be high quality, but also actionable - enabling AXA XLs executive leadership team to maximise benefits and facilitate sustained enterprise advantage\n\nOur Innovation, Data, and Analytics Office (IDA) is focused on driving innovation by optimizing how we leverage data to drive strategy and create a new business model - disrupting the insurance market\n\nAs we develop an enterprise-wide data and digital strategy that moves us toward a greater focus on the use of data and data-driven insights, we are seeking an Associate Scientist for our Data Sourcing & Solutions team\n\nThe role sits across the IDA Department to make sure customer requirements are properly captured and transformed into actionable data specifications\n\nSuccess in the role will require a focus on proactive management of the sourcing and management of data from source through usage\n\nWhat you ll be DOING What will your essential responsibilities include? Accountable for documenting data requirements (Business and Function Requirements) and assessing the reusability of Axiom assets\n\nBuild processes to simplify and expedite data sourcing to focus on delivering data to AXA XL business stakeholders frequently\n\nDevelops and operationalizes strategic data products, and answers and proactively manages the sourcing and management of data from source through usage (reusable Policy and Claim Domain data assets)\n\nData Validation Testing of the data products in partnership with the AXA XL business to ensure the accuracy of the data and validation of the requirements\n\nAssesses all data required as part of the Data Ecosystem to make sure data has a single version of the truth\n\nRespond to ad-hoc data requests to support AXA XLs business\n\nInstill a customer-first attitude, prioritizing service for our business stakeholders above all else\n\nInternalize and execute IDA and company-wide goals to become a data-driven organization\n\nContribute to best practices and standards to make sure there is a consistent and efficient approach to capturing business requirements and translating them into functional, non-functional, and semantic specifications\n\nDevelop a comprehensive understanding of the data and our customers\n\nDrive root cause analysis for identified data deficiencies within reusable data assets delivered via IDA\n\nIdentify solution options to improve the consistency, accuracy, and quality of data when captured at its source\n\nYou will report to the Team Lead - Data Sourcing & Solutions\n\nWhat you will BRING We re looking for someone who has these abilities and skills: Required Skills and Abilities: Experience in a data role (business analyst, data analyst, analytics) preferably in the Insurance industry and within a data division\n\nA minimum of a bachelor s or masters degree (preferred) in a relevant discipline\n\nRobust SQL knowledge and technical ability to query AXA XL data sources to understand our data\n\nExcellent presentation, communication (oral & written), and relationship-building skills, across all levels of management and customer interaction\n\nInsurance experience in data, underwriting, claims, and/or operations, including influencing, collaborating, and leading efforts in complex, disparate, and interrelated teams with competing priorities\n\nPassion for data and experience working within a data-driven organization\n\nWork together internal data with external industry data to deliver holistic answers\n\nWork with unstructured data to unlock information needed by the business to create unique products for the insurance industry\n\nPossesses robust exploratory analysis skills and high intellectual curiosity\n\nDisplays exceptional organizational skills and is detail-oriented\n\nThe robust conceptual thinker who connects dots, and has critical thinking, and analytical skills\n\nDesired Skills and Abilities: Ability to work with team members across the globe and departments\n\nAbility to take ownership, work under pressure, and meet deadlines\n\nBuilds trust and rapport within and across groups\n\nApplies in-depth knowledge of business and specialized areas to solve business problems and understand integration challenges and long-term impact creatively and strategically\n\nAbility to manage data needs of an individual project(s) while being able to understand the broader enterprise data perspective\n\nExpected to recommend innovation and improvement to policies, and procedures, deploying resources, and performing core activities\n\nExperience with SQL Server, Azure Databricks Notebook, Qlikview, PowerBI, and Jira/Confluence a plus",Industry Type: Insurance,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data validation', 'Claims', 'Underwriting', 'Agile', 'QlikView', 'Business strategy', 'JIRA', 'Analytics', 'SQL', 'Customer interaction']",2025-06-12 14:37:00
Senior Associate Data Scientist,Amgen Inc,1 - 3 years,Not Disclosed,['Hyderabad'],"What you will do\nIn this vital role you will identify trends, root causes, and potential improvements in our products and processes, ensuring that patient voices are heard and addressed with utmost precision.\nAs the Sr Associate Data Scientist at Amgen, you will be responsible for developing and deploying basic machine learning, operational research, semantic analysis, and statistical methods to uncover structure in large data sets. This role involves creating analytics solutions to address customer needs and opportunities.\nCollect, clean, and manage large datasets related to product performance and patient complaints.\nEnsure data integrity, accuracy, and accessibility for further analysis.\nDevelop and maintain databases and data systems for storing patient complaints and product feedback.\nAnalyze data to identify patterns, trends, and correlations in patient complaints and product issues.\nUse advanced statistical methods and machine learning techniques to uncover insights and root causes.\nDevelop analytics or predictive models to foresee potential product issues and patient concerns to address customer needs and opportunities.\nPrepare comprehensive reports and visualizations to communicate findings to key collaborators.\nPresent insights and recommendations to cross-functional teams, including product development, quality assurance, and customer service.\nCollaborate with regulatory and compliance teams to ensure adherence to healthcare standards and regulations.\nFind opportunities for product enhancements and process improvements based on data analysis.\nWork with product complaint teams to implement changes and monitor their impact.\nStay abreast of industry trends, emerging technologies, and standard methodologies in data science and healthcare analytics.\nEvaluate data to support product complaints.\nWork alongside software developers and software engineers to translate algorithms into commercially viable products and services.\nWork in technical teams in development, deployment, and application of applied analytics, predictive analytics, and prescriptive analytics.\nPerform exploratory and targeted data analyses using descriptive statistics and other methods.\nWork with data engineers on data quality assessment, data cleansing and data analytics\nGenerate reports, annotated code, and other projects artifacts to document, archive, and communicate your work and outcomes.\n\nBasic Qualifications:\nMasters degree and 1 to 3 years of Data Science and with one or more analytic software tools or languages, and data visualization tools experience OR\nBachelors degree and 3 to 5 years of Data Science and with one or more analytic software tools or languages, and data visualization tools experience OR\nDiploma and 7 to 9 years of Data Science and with one or more analytic software tools or languages, and data visualization tools experience\nPreferred Qualifications:\nDemonstrated skill in the use of applied analytics, descriptive statistics, feature extraction and predictive analytics on industrial datasets.\nExperience in statistical techniques and hypothesis testing, experience with regression analysis, clustering and classification.\nExperience in analyzing time-series data for forecasting and trend analysis.\nExperience with Data Bricks platform for data analytics.\nExperience working with healthcare data, including patient complaints, product feedback, and regulatory requirements.",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'data bricks', 'hypothesis testing', 'predictive analytics', 'data visualization', 'machine learning', 'statistics']",2025-06-12 14:37:02
"Data Scientist II, Regulatory Intelligence, Safety, and Compliance",Amazon,2 - 7 years,Not Disclosed,['Bengaluru'],"We are seeking an exceptional Data Scientist to join a team of experts in the field of machine learning, and work together to tackle challenging problems across diverse compliance domains. We leverage risk models (including boosted trees and graph neural networks) as well as vision and large-language-models (LLMs) to detect illegal and unsafe products across the Amazon catalog. We work on machine learning problems for multi-modal classification, intent detection, information retrieval, anomaly and fraud detection, and generative AI.\n\nThis is an exciting and challenging position to deliver scientific innovations into production systems at Amazon-scale to make immediate, meaningful customer impacts while also pursuing ambitious, long-term research. You will work in a highly collaborative environment where you can analyze and process large amounts of image, text and tabular data. You will work on hard science problems that have not been solved before, conduct rapid prototyping to validate your hypothesis, and deploy your algorithmic ideas at scale. There will be something new to learn every day as we work in an environment with rapidly evolving regulations and adversarial actors looking to outwit your best ideas.\n\n\nExplore and evaluate state-of-the-art algorithms and approaches in risk modeling and vision/language models\nTranslate product and CX requirements into measurable science problems and metrics.\nCollaborate with product and tech partners and customers to validate hypothesis, drive adoption, and increase business impact\nEvaluate model performance in production and refresh/implement necessary updates to maintain optimal system performance.\n\nA day in the life\nUnderstanding customer problems, project timelines, and team/project mechanisms\nProposing science formulations and brainstorming ideas with team to solve business problems\nWriting code, and running experiments with re-usable science libraries\nReviewing labels and audit results with investigators and operations associates\nSharing science results with science, product and tech partners and customers\nContributing to team retrospectives for continuous improvements\nParticipating in science research collaborations and attending study groups with scientists across Amazon\n\nAbout the team\nWe are a team of scientists building AI/ML solutions to make Amazon Earth s most trusted shopping destination for safe and compliant products. 2+ years of data scientist experience\n3+ years of data querying languages (e.g. SQL), scripting languages (e.g. Python) or statistical/mathematical software (e.g. R, SAS, Matlab, etc.) experience\n3+ years of machine learning/statistical modeling data analysis tools and techniques, and parameters that affect their performance experience\nExperience applying theoretical models in an applied environment\nKnowledge of relevant statistical measures such as confidence intervals, significance of error measurements, development and evaluation data sets, etc. Experience in Python, Perl, or another scripting language\nExperience in a ML or data scientist role with a large technology company",,,,"['Data analysis', 'SAS', 'Neural networks', 'risk modeling', 'Machine learning', 'Information retrieval', 'Perl', 'MATLAB', 'Auditing', 'Python']",2025-06-12 14:37:04
Lead Data Scientist,Bizopp Management Consultants,11 - 18 years,25-35 Lacs P.A.,['Chennai'],"• Proficiency in Python and SQL for data extraction, manipulation, and analysis\n\n• Exploratory data analysis (EDA), developing, and deploying machine learning models\n\n• Expertise in deploying ML models on cloud platforms such as AWS or Azure",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['EDA', 'Data Scientist', 'Python', 'SQL', 'Azure', 'Exploratory data analysis', 'Machine Learning', 'AWS', 'ML']",2025-06-12 14:37:06
Lead Data Scientist,Trion Consultancy Services,10 - 18 years,20-35 Lacs P.A.,['Chennai'],"LD Scientist with 12 yrs of industry exp, including at least 5 yrs of hands-on exp in data science & a proven track record of delivering impactful data science solutions.\nData Analysis &Exploration\nTime Series Analysis\nModel Deployment & Integration\n\nRequired Candidate profile\n12+ yrs/including 5+ yrs in data science\nExp in Python and SQL for data extraction, manipulation & analysis\nDS & Model Development: Demonstrated exp in performing exploratory data analysis (EDA)",Industry Type: Emerging Technologies (AI/ML),Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Lead Data science', 'Machine Learning', 'Python']",2025-06-12 14:37:08
Senior Data Scientist,Epsilon,6 - 9 years,Not Disclosed,['Bengaluru'],"Responsibilities: -\nContribute and build an internal product library that is focused on solving business problems related to prediction & recommendation.\nResearch unfamiliar methodologies, techniques to fine tune existing models in the product suite and, recommend better solutions and/or technologies.\nImprove features of the product to include newer machine learning algorithms in the likes of product recommendation, real time predictions, fraud detection, offer personalization etc\nCollaborate with client teams to on-board data, build models and score predictions.\nParticipate in building automations and standalone applications around machine learning algorithms to enable a One Click solution to getting predictions and recommendations.\nAnalyze large datasets, perform data wrangling operations, apply statistical treatments to filter and fine tune input data, engineer new features and eventually aid the process of building machine learning models.\nRun test cases to tune existing models for performance, check criteria and define thresholds for success by scaling the input data to multifold.\nDemonstrate a basic understanding of different machine learning concepts such as Regression, Classification, Matrix Factorization, K-fold Validations and different algorithms such as Decision Trees, Random Forrest, K-means clustering.\nDemonstrate working knowledge and contribute to building models using deep learning techniques, ensuring robust, scalable and high-performance solutions\nMinimum Qualifications:\nEducation: Master's or PhD in a quantitative discipline (Statistics, Economics, Mathematics, Computer Science) is highly preferred.\nDeep Learning Mastery: Extensive experience with deep learning frameworks (TensorFlow, PyTorch, or Keras) and advanced deep learning projects across various domains, with a focus on multimodal data applications.\nGenerative AI Expertise: Proven experience with generative AI models and techniques, such as RAG, VAEs, Transformers, and applications at scale in content creation or data augmentation.\nProgramming and Big Data: Expert-level proficiency in Python and big data/cloud technologies (Databricks and Spark) with a minimum of 4-5 years of experience.\nRecommender Systems and Real-time Predictions: Expertise in developing sophisticated recommender systems, including the application of real-time prediction frameworks.\nMachine Learning Algorithms: In-depth experience with complex algorithms such as logistic regression, random forest, XGBoost, advanced neural networks, and ensemble methods.\nExperienced with machine learning algorithms such as logistic regression, random forest, XG boost, KNN, SVM, neural network, linear regression, lasso regression and k-means.\nDesirable Qualifications:\nGenerative AI Tools Knowledge: Proficiency with tools and platforms for generative AI (such as OpenAI, Hugging Face Transformers).\nDatabricks and Unity Catalog: Experience leveraging Databricks and Unity Catalog for robust data management, model deployment, and tracking.\nWorking experience in CI/CD tools such as GIT & BitBucket",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Data Engineering', 'Pyspark', 'Azure Aws', 'Generative AI', 'Big Data', 'AWS', 'Data Bricks', 'Deep Learning', 'Python', 'SQL']",2025-06-12 14:37:11
Sr. Data Scientist-Stratup-Mid-Size companies Exp.@ Bangalore_Urgent,"A leader in this space, we deliver world...",8 - 13 years,Not Disclosed,['Bengaluru'],"Senior Data Scientist\n\nLocation: Onsite Bangalore\nExperience: 8+ years\n\nRole Overview\n\nWe are seeking a Senior Data Scientist with a strong foundation in machine learning, deep learning, and statistical modeling, with the ability to translate complex operational problems into scalable AI/ML solutions. In addition to core data science responsibilities, the role involves building production-ready backends in Python and contributing to end-to-end model lifecycle management. Exposure to computer vision is a plus, especially for industrial use cases like identification, intrusion detection, and anomaly detection.\n\nKey Responsibilities\n\nDevelop, validate, and deploy machine learning and deep learning models for forecasting, classification, anomaly detection, and operational optimization\nBuild backend APIs using Python (FastAPI, Flask) to serve ML/DL models in production environments\nApply advanced computer vision models (e.g., YOLO, Faster R-CNN) to object detection, intrusion detection, and visual monitoring tasks\nTranslate business problems into analytical frameworks and data science solutions\nWork with data engineering and DevOps teams to operationalize and monitor models at scale\nCollaborate with product, domain experts, and engineering teams to iterate on solution design\nContribute to technical documentation, model explainability, and reproducibility practices\n\n\nRequired Skills\n\nStrong proficiency in Python for data science and backend development\nExperience with ML/DL libraries such as scikit-learn, TensorFlow, or PyTorch\nSolid knowledge of time-series modeling, forecasting techniques, and anomaly detection\nExperience building and deploying APIs for model serving (FastAPI, Flask)\nFamiliarity with real-time data pipelines using Kafka, Spark, or similar tools\nStrong understanding of model validation, feature engineering, and performance tuning\nAbility to work with SQL and NoSQL databases, and large-scale datasets\nGood communication skills and stakeholder engagement experience\n\n\nGood to Have\n\nExperience with ML model deployment tools (MLflow, Docker, Airflow)\nUnderstanding of MLOps and continuous model delivery practices\nBackground in aviation, logistics, manufacturing, or other industrial domains\nFamiliarity with edge deployment and optimization of vision models\n\n\nQualifications\n\nMasters or PhD in Data Science, Computer Science, Applied Mathematics, or related field\n7+ years of experience in machine learning and data science, including end-to-end deployment of models in production",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['scikit-learn', 'time-series modeling', 'ML/DL libraries', 'data science', 'Python', 'Airflow', 'Kafka', 'MLflow', 'logistics', 'anomaly detection', 'aviation', 'SQL', 'PyTorch', 'NoSQL', 'MLOps', 'forecasting techniques', 'Docker', 'manufacturing', 'FastAPI', 'Spark', 'TensorFlow', 'Flask']",2025-06-12 14:37:13
IN Senior Associate Cloud Data Engineer-- Data and Analytics,PwC Service Delivery Center,4 - 7 years,Not Disclosed,['Bengaluru'],"Not Applicable\nSpecialism\nData, Analytics & AI\nManagement Level\nSenior Associate\n& Summary\nAt PwC, our people in data and analytics engineering focus on leveraging advanced technologies and techniques to design and develop robust data solutions for clients. They play a crucial role in transforming raw data into actionable insights, enabling informed decisionmaking and driving business growth.\n\nIn data engineering at PwC, you will focus on designing and building data infrastructure and systems to enable efficient data processing and analysis. You will be responsible for developing and implementing data pipelines, data integration, and data transformation solutions.\nWhy PWC\n& Summary\ns\nJob Title Cloud Data Engineer (AWS/Azure/Databricks/GCP)\nExperience 47 years in Data Engineering\nWe are seeking skilled and dynamic Cloud Data Engineers specializing in AWS, Azure, Databricks, and GCP. The ideal candidate will have a strong background in data engineering, with a focus on data ingestion, transformation, and warehousing. They should also possess excellent knowledge of PySpark or Spark, and a proven ability to optimize performance in Spark job executions.\nKey Responsibilities\nDesign, build, and maintain scalable data pipelines for a variety of cloud platforms including AWS, Azure, Databricks, and GCP.\nImplement data ingestion and transformation processes to facilitate efficient data warehousing.\nUtilize cloud services to enhance data processing capabilities\nAWS Glue, Athena, Lambda, Redshift, Step Functions, DynamoDB, SNS.\nAzure Data Factory, Synapse Analytics, Functions, Cosmos DB, Event Grid, Logic Apps, Service Bus.\nGCP Dataflow, BigQuery, DataProc, Cloud Functions, Bigtable, Pub/Sub, Data Fusion.\nOptimize Spark job performance to ensure high efficiency and reliability.\nStay proactive in learning and implementing new technologies to improve data processing frameworks.\nCollaborate with crossfunctional teams to deliver robust data solutions.\nWork on Spark Streaming for realtime data processing as necessary.\nQualifications\n47 years of experience in data engineering with a strong focus on cloud environments.\nProficiency in PySpark or Spark is mandatory.\nProven experience with data ingestion, transformation, and data warehousing.\nIndepth knowledge and handson experience with cloud services(AWS/Azure/GCP)\nDemonstrated ability in performance optimization of Spark jobs.\nStrong problemsolving skills and the ability to work independently as well as in a team.\nCloud Certification (AWS, Azure, or GCP) is a plus.\nFamiliarity with Spark Streaming is a bonus.\nMandatory skill sets\nPython, Pyspark, SQL with (AWS or Azure or GCP)\nPreferred skill sets\nPython, Pyspark, SQL with (AWS or Azure or GCP)\nYears of experience required\n710\nEducation qualification\nBE/BTECH, ME/MTECH, MBA, MCA\nEducation\nDegrees/Field of Study required Bachelor of Engineering, Master of Business Administration, Bachelor of Technology, Master of Engineering\nDegrees/Field of Study preferred\nRequired Skills\nPySpark, Python (Programming Language), Structured Query Language (SQL)\nAccepting Feedback, Accepting Feedback, Active Listening, Agile Scalability, Amazon Web Services (AWS), Analytical Thinking, Apache Airflow, Apache Hadoop, Azure Data Factory, Communication, Creativity, Data Anonymization, Data Architecture, Database Administration, Database Management System (DBMS), Database Optimization, Database Security Best Practices, Databricks Unified Data Analytics Platform, Data Engineering, Data Engineering Platforms, Data Infrastructure, Data Integration, Data Lake, Data Modeling, Data Pipeline {+ 28 more}\nTravel Requirements\nGovernment Clearance Required?",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['GCP', 'Data modeling', 'Analytical', 'Agile', 'Data processing', 'DBMS', 'Apache', 'AWS', 'SQL', 'Python']",2025-06-12 14:37:15
Sr Analyst I Data Engineering,DXC Technology,9 - 12 years,Not Disclosed,['Hyderabad'],"Job Description:\nEssential Job Functions:\nParticipate in data engineering tasks, including data processing and integration activities.\nAssist in the development and maintenance of data pipelines.\nCollaborate with team members to collect, process, and store data.\nContribute to data quality assurance efforts and adherence to data standards.\nUse data engineering tools and techniques to analyze and generate insights from data.\nCollaborate with data engineers and other analysts on data-related projects.\nSeek out opportunities to enhance data engineering skills and domain knowledge.\nStay informed about data engineering trends and best practices.\n\nBasic Qualifications:\nBachelors degree in a relevant field or equivalent combination of education and experience\nTypically, 5+ years of relevant work experience in industry, with a minimum of 2 years in a similar role\nProven experience in data engineering\nProficiencies in data engineering tools and technologies\nA continuous learner that stays abreast with industry knowledge and technology\n\nOther Qualifications:\nAdvanced degree in a relevant field a plus\nRelevant certifications, such as Oracle Certified Professional, MySQL Database Administrator a plus\nRecruitment fraud is a scheme in which fictitious job opportunities are offered to job seekers typically through online services, such as false websites, or through unsolicited emails claiming to be from the company. These emails may request recipients to provide personal information or to make payments as part of their illegitimate recruiting process. DXC does not make offers of employment via social media networks and DXC never asks for any money or payments from applicants at any point in the recruitment process, nor ask a job seeker to purchase IT or other equipment on our behalf. More information on employment scams is available here .",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Usage', 'Manager Quality Assurance', 'Senior Analyst', 'Social media', 'Manager Technology', 'Data processing', 'Data quality', 'Oracle', 'mysql database administrator', 'Recruitment']",2025-06-12 14:37:18
Senior Data Scientist,Toast,6 - 11 years,Not Disclosed,['Bengaluru'],"Senior Data Scientist - S&A\nNow, more than ever, the Toast team is committed to our customers. We re taking steps to help restaurants navigate these unprecedented times with technology, resources, and community. We focus on building the restaurant platform that helps restaurants adapt, take control, and get back to what they do best: building the businesses they love. And because our technology is purpose-built for restaurants, by restaurant people, restaurants can trust that we ll deliver on their needs for today while investing in experiences that will power their restaurant of the future.\nBready*\nto make a change?\n\nAs the Senior Data Scientist in our Bangalore Data Science team, you will contribute to building machine learning algorithms using our huge reservoir of point of sale transaction data. You will work with architects, engineers and product managers to solve business and customer problems and turn machine learning models into business impact across product lines, including financial processing and fraud.\nAbout this\nRoll*\n:\nDesign, build, train and evaluate machine learning models to drive business value for Toast and our restaurant customers\nCollaborate closely with internal and external product stakeholders, both technical and non-technical and help translate deep machine learning knowledge to product applications\nBreak down larger ML initiatives into smaller problems that enables data science to deliver incremental business value and lead the team to execute on them\nWork closely with Production Engineering and Data Platform teams to deploy models to production and regularly monitor for efficiency, key KPIs and enhance them as needed\nWork with incident response and problem resolution teams to check and resolve any problems/challenges as and when identified in the model deployed in the production\nEffectively document all steps and manage code repositories for easy scalability and knowledge sharing with the the team\nIncorporate up-to-date ML technology and DS approach as best practice for the team\nHelp in continuing to build out and expand the Data Science and ML Engineering teams\nWork effectively in a dynamic, changing environment while focusing on key goals and objectives\nDo you have the right\ningredients*\n?\nAdvanced degree in Data Science, Statistics, Applied Math, Computer Science, Engineering or other equivalent quantitative disciplines\n6 + years of industry experience in the field of Data Science and Machine Learning\nExperience in time series modelling. Familiarity with ARIMA, SARIMA, ETS, VAR models. Familiarity with forecasting tools like Facebook Prophet, GluonTS, or NeuralProphet.\nStrong proficiency in Python and SQL; experience with some of the following languages, tools, and frameworks: R, Spark, Scala, scikit-learn, Tensorflow, PyTorch, etc.\nFamiliarity with standard software engineering practices and tools including object-oriented programming, test-driven development, CI/CD, git, shell scripting, task orchestration (Airflow, Luigi, etc.) and preferably AWS tooling (Sagemaker, DynamoDB, ECS, etc.)\nStrong knowledge of underlying mathematical foundations of statistics and machine learning\nPrior success deploying machine learning solutions in large-scale production environments\nExperience collaborating with cross-functional teams and stakeholders to evaluate new Machine Learning opportunities\nProblem solver who loves to dig into different kinds of data and can communicate their findings to cross-functional stakeholders\nBonus\ningredients*\n:\nPassion for research and curiosity that calls you to go beyond good enough to create something innovative and exciting\nDiversity, Equity, and Inclusion is Baked into our Recipe for Success\nAt Toast, our employees are our secret ingredient when they thrive, we thrive. The restaurant industry is one of the most diverse, and we embrace that diversity with authenticity, inclusivity, respect, and humility. By embedding these principles into our culture and design, we create equitable opportunities for all and raise the bar in delivering exceptional experiences.\nWe Thrive Together\nWe embrace a hybrid work model that fosters in-person collaboration while valuing individual needs. Our goal is to build a strong culture of connection as we work together to empower the restaurant community. To learn more about how we work globally and regionally, check out: https: / / careers.toasttab.com / locations-toast .\nApply today!\nToast is committed to creating an accessible and inclusive hiring process. As part of this commitment, we strive to provide reasonable accommodations for persons with disabilities to enable them to access the hiring process. If you need an accommodation to access the job application or interview process, please contact .\n------\nFor roles in the United States, It is unlawful in Massachusetts to require or administer a lie detector test as a condition of employment or continued employment. An employer who violates this law shall be subject to criminal penalties and civil liability.",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'orchestration', 'data science', 'Production engineering', 'Machine learning', 'Shell scripting', 'test driven development', 'Forecasting', 'SQL', 'Python']",2025-06-12 14:37:20
IN Senior Associate Cloud Data Engineer,PwC Service Delivery Center,5 - 8 years,Not Disclosed,['Bengaluru'],"Not Applicable\nSpecialism\nSAP\nManagement Level\nSenior Associate\n& Summary\nAt PwC, our people in data and analytics engineering focus on leveraging advanced technologies and techniques to design and develop robust data solutions for clients. They play a crucial role in transforming raw data into actionable insights, enabling informed decisionmaking and driving business growth.\n\nIn data engineering at PwC, you will focus on designing and building data infrastructure and systems to enable efficient data processing and analysis. You will be responsible for developing and implementing data pipelines, data integration, and data transformation solutions.\nWhy PWC\nAt PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purposeled and valuesdriven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us .\nResponsibilities\nStrong handson experience with multi cloud (AWS, Azure, GCP) services such as GCP BigQuery, Dataform AWS Redshift, Proficient in PySpark and SQL for building scalable data processing pipelines Knowledge of utilizing serverless technologies like AWS Lambda, and Google Cloud Functions Experience in orchestration frameworks like Apache Airflow, Kubernetes, and Jenkins to manage and orchestrate data pipelines Experience in developing and optimizing ETL/ELT pipelines and working on cloud data warehouse migration projects. Exposure to clientfacing roles, with strong problemsolving and communication skills. Prior experience in consulting or working in a consulting environment is preferred.\nMandatory skill sets\n(AWS, Azure, GCP) services such as GCP BigQuery, Dataform AWS Redshift, Python\nPreferred skill sets\nDevops\nYears of experience required\n5 8 Years\nEducation qualification\nBE/B.Tech/MBA/MCA/M.Tech\nEducation\nDegrees/Field of Study required Bachelor of Engineering, Bachelor of Technology, Master of Business Administration\nDegrees/Field of Study preferred\nRequired Skills\nAWS Devops\nAccepting Feedback, Accepting Feedback, Active Listening, Agile Scalability, Amazon Web Services (AWS), Analytical Thinking, Apache Airflow, Apache Hadoop, Azure Data Factory, Communication, Creativity, Data Anonymization, Data Architecture, Database Administration, Database Management System (DBMS), Database Optimization, Database Security Best Practices, Databricks Unified Data Analytics Platform, Data Engineering, Data Engineering Platforms, Data Infrastructure, Data Integration, Data Lake, Data Modeling, Data Pipeline {+ 27 more}\nTravel Requirements\nGovernment Clearance Required?",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SAP', 'Data modeling', 'Analytical', 'Consulting', 'Database administration', 'Data processing', 'Corporate advisory', 'DBMS', 'Database management system', 'SQL']",2025-06-12 14:37:23
Hadoop Data Engineer,Info Origin,0 - 2 years,Not Disclosed,['Gurugram'],"Job Overview:\nWe are looking for experienced Data Engineers proficient in Hadoop, Hive, Python, SQL, and Pyspark/Spark to join our dynamic team. Candidates will be responsible for designing, developing, and maintaining scalable big data solutions.\nKey Responsibilities:\nDevelop and optimize data pipelines for large-scale data processing.\nWork with structured and unstructured datasets to derive actionable insights.\nCollaborate with cross-functional teams to enhance data-driven decision-making.\nEnsure the performance, scalability, and reliability of data architectures.\nImplement best practices for data security and governance.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['hive', 'Scalability', 'data security', 'spark', 'Hadoop', 'Data processing', 'big data', 'SQL', 'Python']",2025-06-12 14:37:25
Senior Data Scientist - AI/ML,Inumellas Consultancy Services,9 - 14 years,Not Disclosed,"['Hyderabad', 'Pune', 'Bengaluru']","Role - Senior Data Scientist / Senior Gen AI Engineer\nExp Range - 8 to 18 yrs\nPosition - Permanent Fulltime\nCompany - Data Analytics & AIML MNC\nLocation - Hyderabad, Pune, Bangalore (Relocation accepted)\nAbout the Role:\n\nWe are seeking a Software Engineer with expertise in Generative AI and Microsoft technologies to design, develop, and deploy AI-powered solutions using the Microsoft ecosystem. You will work with cross-functional teams to build scalable applications leveraging generative AI models and Azure services.\n\nSkills Required:\n\nExperience with Large Language Models (LLMs) like GPT, LLaMA, Claude, etc.\nProficiency in Python for building and fine-tuning AI/ML models\nFamiliarity with LangChain, LLMOps, or RAG (Retrieval-Augmented Generation) pipelines\nExperience with Vector Databases (e.g. FAISS, Pinecone, Weaviate)\nKnowledge of Prompt Engineering and model evaluation techniques\nExposure to cloud platforms (Azure, AWS or GCP) for deploying GenAI solutions\n\nPreferred Skills:\n\nExperience with Azure OpenAI, Databricks or Microsoft Fabric\nHands-on with Hugging Face Transformers, OpenAI APIs or custom model training",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Deep Learning', 'Prompt Engineering', 'Large Language Model', 'Vector Database', 'Retrieval Augmented Generation', 'GenAI', 'Langchain', 'Artificial Intelligence', 'LLMOps', 'LLaMa', 'GPT', 'Azure OpenAI', 'Machine Learning', 'ML Models', 'Model Evaluation', 'Huggingface', 'Aiml', 'OpenAI', 'Azure Machine Learning', 'Python']",2025-06-12 14:37:27
IN Senior Associate Cloud Data Engineer,PwC Service Delivery Center,5 - 8 years,Not Disclosed,['Bengaluru'],"Not Applicable\nSpecialism\nSAP\nManagement Level\nSenior Associate\n& Summary\nAt PwC, our people in data and analytics engineering focus on leveraging advanced technologies and techniques to design and develop robust data solutions for clients. They play a crucial role in transforming raw data into actionable insights, enabling informed decisionmaking and driving business growth.\n\nIn data engineering at PwC, you will focus on designing and building data infrastructure and systems to enable efficient data processing and analysis. You will be responsible for developing and implementing data pipelines, data integration, and data transformation solutions.\nWhy PWC\nAt PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purposeled and valuesdriven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us .\nResponsibilities\nStrong handson experience with multi cloud (AWS, Azure, GCP) services such as GCP BigQuery, Dataform AWS Redshift, Proficient in PySpark and SQL for building scalable data processing pipelines Knowledge of utilizing serverless technologies like AWS Lambda, and Google Cloud Functions Experience in orchestration frameworks like Apache Airflow, Kubernetes, and Jenkins to manage and orchestrate data pipelines Experience in developing and optimizing ETL/ELT pipelines and working on cloud data warehouse migration projects. Exposure to clientfacing roles, with strong problemsolving and communication skills. Prior experience in consulting or working in a consulting environment is preferred.\nMandatory skill sets\n(AWS, Azure, GCP) services such as GCP BigQuery, Dataform AWS Redshift, Python\nPreferred skill sets\nDevops\nYears of experience required\n5 8 Years\nEducation qualification\nBE/B.Tech/MBA/MCA/M.Tech\nEducation\nDegrees/Field of Study required Bachelor of Engineering, Master of Business Administration, Bachelor of Technology\nDegrees/Field of Study preferred\nRequired Skills\nAWS Devops\nAccepting Feedback, Accepting Feedback, Active Listening, Agile Scalability, Amazon Web Services (AWS), Analytical Thinking, Apache Airflow, Apache Hadoop, Azure Data Factory, Communication, Creativity, Data Anonymization, Data Architecture, Database Administration, Database Management System (DBMS), Database Optimization, Database Security Best Practices, Databricks Unified Data Analytics Platform, Data Engineering, Data Engineering Platforms, Data Infrastructure, Data Integration, Data Lake, Data Modeling, Data Pipeline {+ 27 more}\nTravel Requirements\nGovernment Clearance Required?",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SAP', 'Data modeling', 'Analytical', 'Consulting', 'Database administration', 'Data processing', 'Corporate advisory', 'DBMS', 'Database management system', 'SQL']",2025-06-12 14:37:29
IN Senior Associate Cloud Data Engineer,PwC Service Delivery Center,5 - 8 years,Not Disclosed,['Bengaluru'],"Not Applicable\nSpecialism\nSAP\nManagement Level\nSenior Associate\n& Summary\nAt PwC, our people in data and analytics engineering focus on leveraging advanced technologies and techniques to design and develop robust data solutions for clients. They play a crucial role in transforming raw data into actionable insights, enabling informed decisionmaking and driving business growth.\n\nIn data engineering at PwC, you will focus on designing and building data infrastructure and systems to enable efficient data processing and analysis. You will be responsible for developing and implementing data pipelines, data integration, and data transformation solutions.\nWhy PWC\nAt PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purposeled and valuesdriven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us .\nResponsibilities\nStrong handson experience with multi cloud (AWS, Azure, GCP) services such as GCP BigQuery, Dataform AWS Redshift, Proficient in PySpark and SQL for building scalable data processing pipelines Knowledge of utilizing serverless technologies like AWS Lambda, and Google Cloud Functions Experience in orchestration frameworks like Apache Airflow, Kubernetes, and Jenkins to manage and orchestrate data pipelines Experience in developing and optimizing ETL/ELT pipelines and working on cloud data warehouse migration projects. Exposure to clientfacing roles, with strong problemsolving and communication skills. Prior experience in consulting or working in a consulting environment is preferred.\nMandatory skill sets\n(AWS, Azure, GCP) services such as GCP BigQuery, Dataform AWS Redshift, Python\nPreferred skill sets\nDevops\nYears of experience required\n5 8 Years\nEducation qualification\nBE/B.Tech/MBA/MCA/M.Tech\nEducation\nDegrees/Field of Study required Master of Business Administration, Bachelor of Technology, Bachelor of Engineering\nDegrees/Field of Study preferred\nRequired Skills\nAWS Devops, Microsoft Azure, Python (Programming Language)\nDevOps\nTravel Requirements\nGovernment Clearance Required?",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Business administration', 'SAP', 'Consulting', 'Manager Technology', 'Data processing', 'microsoft azure', 'Corporate advisory', 'AWS', 'Analytics', 'SQL']",2025-06-12 14:37:32
IN Senior Associate Cloud Data Engineer,PwC Service Delivery Center,5 - 8 years,Not Disclosed,['Bengaluru'],"Not Applicable\nSpecialism\nSAP\nManagement Level\nSenior Associate\n& Summary\nAt PwC, our people in data and analytics engineering focus on leveraging advanced technologies and techniques to design and develop robust data solutions for clients. They play a crucial role in transforming raw data into actionable insights, enabling informed decisionmaking and driving business growth.\n\nIn data engineering at PwC, you will focus on designing and building data infrastructure and systems to enable efficient data processing and analysis. You will be responsible for developing and implementing data pipelines, data integration, and data transformation solutions.\nWhy PWC\nAt PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purposeled and valuesdriven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us .\n& Summary We are looking for a seasoned Cloud Data Engineer\nResponsibilities\nStrong handson experience with multi cloud (AWS, Azure, GCP) services such as GCP BigQuery, Dataform AWS Redshift, Proficient in PySpark and SQL for building scalable data processing pipelines Knowledge of utilizing serverless technologies like AWS Lambda, and Google Cloud Functions Experience in orchestration frameworks like Apache Airflow, Kubernetes, and Jenkins to manage and orchestrate data pipelines Experience in developing and optimizing ETL/ELT pipelines and working on cloud data warehouse migration projects. Exposure to clientfacing roles, with strong problemsolving and communication skills. Prior experience in consulting or working in a consulting environment is preferred.\nMandatory skill sets\n(AWS, Azure, GCP) services such as GCP BigQuery, Dataform AWS Redshift, Python\nPreferred skill sets\nDevops\nYears of experience required\n5 8 Years\nEducation qualification\nBE/B.Tech/MBA/MCA/M.Tech\nEducation\nDegrees/Field of Study required Master of Business Administration, Master of Engineering, Bachelor of Engineering\nDegrees/Field of Study preferred\nRequired Skills\nCloud Data Catalog\nAccepting Feedback, Accepting Feedback, Active Listening, Agile Scalability, Amazon Web Services (AWS), Analytical Thinking, Apache Airflow, Apache Hadoop, Azure Data Factory, Communication, Creativity, Data Anonymization, Data Architecture, Database Administration, Database Management System (DBMS), Database Optimization, Database Security Best Practices, Databricks Unified Data Analytics Platform, Data Engineering, Data Engineering Platforms, Data Infrastructure, Data Integration, Data Lake, Data Modeling, Data Pipeline {+ 27 more}\nTravel Requirements\nGovernment Clearance Required?",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SAP', 'GCP', 'Data modeling', 'Analytical', 'Consulting', 'Agile', 'DBMS', 'Apache', 'SQL', 'Python']",2025-06-12 14:37:34
Senior Data Engineering Analyst,Optum,4 - 7 years,Not Disclosed,['Bengaluru'],"Job Description\n\nExperience 4 to 7 years.\nExperience in any ETL tools [e.g. DataStage] with implementation experience in large Data Warehouse\nProficiency in programming languages such as Python etc.\nExperience with data warehousing solutions (e.g., Snowflake, Redshift) and big data technologies (e.g., Hadoop, Spark).\nStrong knowledge of SQL and database management systems.\nFamiliarity with cloud platforms (e.g., AWS, Azure, GCP) and data pipeline orchestration tools (e.g. Airflow).\nProven ability to lead and develop high-performing teams, with excellent communication and interpersonal skills.\nStrong analytical and problem-solving abilities, with a focus on delivering actionable insights.\nResponsibilities\nDesign, develop, and maintain advanced data pipelines and ETL processes using niche technologies.\nCollaborate with cross-functional teams to understand complex data requirements and deliver tailored solutions.\nEnsure data quality and integrity by implementing robust data validation and monitoring processes.\nOptimize data systems for performance, scalability, and reliability.\nDevelop comprehensive documentation for data engineering processes and systems.",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['ETL', 'SQL', 'Python', 'Azure', 'Datastage', 'Snowflake', 'Ab Initio', 'Informatica', 'Teradata', 'AWS']",2025-06-12 14:37:36
Senior Data Scientist with GCP,TVS Next,5 - 7 years,Not Disclosed,['Bengaluru'],"What you’ll do:\nUtilize advanced mathematical, statistical, and analytical expertise to research, collect, analyze, and interpret large datasets from internal and external sources to provide insight and develop data driven solutions across the company\nBuild and test predictive models including but not limited to credit risk, fraud, response, and offer acceptance propensity\nResponsible for the development, testing, validation, tracking, and performance enhancement of statistical models and other BI reporting tools leading to new innovative origination strategies within marketing, sales, finance, and underwriting",,,,"['analytical', 'scikit-learn', 'searching', 'bi', 'pyspark', 'numpy', 'sql', 'analytics', 'apache', 'automation', 'data science', 'spark', 'gcp', 'bigquery', 'data visualization', 'xgboost', 'programming', 'reporting', 'ml', 'advanced analytics', 'python', 'data processing', 'predictive', 'jupyter notebook', 'bert', 'pandas', 'matplotlib', 'statistics']",2025-06-12 14:37:39
Senior Data Scientist,Straive,5 - 10 years,Not Disclosed,"['Hyderabad', 'Gurugram', 'Bengaluru']","Role & responsibilities\nRequires 5-8 years of proven experience in banking/payments/other domains\nStrong experience in developing Machine Learning models, Python & SQL\nExperience working with pre-trained models, awareness of state-of-art in embeddings and applicability for use cases\nDetailed oriented with a proactive mindset towards problem-solving\nExcellent communication and presentation skills with the ability to convey complex information clearly and concisely",,,,"['Machine Learning', 'Python', 'SQL', 'Xgboost', 'Neural Networks', 'Random Forest']",2025-06-12 14:37:41
"Senior Data Engineer ( T-SQL & SSIS,Data Warehousing & ETL Specialist)",Synechron,5 - 10 years,Not Disclosed,"['Hyderabad', 'Pune', 'Bengaluru']","Job Summary\nSynechron is seeking a highly skilled Senior Data Engineer specializing in T-SQL and SSIS to lead and advance our data integration and warehousing initiatives. In this role, you will design, develop, and optimize complex ETL processes and database solutions to support enterprise data needs. Your expertise will enable efficient data flow, ensure data integrity, and facilitate actionable insights, contributing to our organizations commitment to data-driven decision-making and operational excellence.\nSoftware Requirements\nRequired Skills:\nProficiency in SQL Server, advanced T-SQL querying, stored procedures, functions, and scripting\nExpertise in SQL Server Integration Services (SSIS), including design, deployment, and troubleshooting\nDeep understanding of data warehousing concepts, schema design, and ETL best practices\nExperience in performance tuning, query optimization, and troubleshooting SQL and SSIS packages\nHands-on experience with database security, compliance, and data masking standards\nFamiliarity with source system analysis and complex data migration\nPreferred Skills:\nExperience with cloud platforms (Azure Data Factory, AWS Glue)\nKnowledge of Azure SQL, Amazon RDS or other cloud-based data services\nExperience with scripting languages like PowerShell or Python for automation\nOverall Responsibilities\nDesign, develop, and maintain robust ETL workflows using SSIS to meet diverse data integration requirements\nWrite optimized, scalable T-SQL queries, stored procedures, and functions aligned with data quality standards\nDevelop and manage data warehouse schemas, tables, and relationships to support reporting and analytics\nEnsure data accuracy, security, and compliance across all data processes\nCollaborate closely with business analysts and other stakeholders to understand data requirements and translate them into technical solutions\nTroubleshoot and resolve performance issues in SQL Server and SSIS packages\nDocument data workflows, schemas, and data mappings to support ongoing maintenance and audits\nParticipate in code reviews, performance tuning, and implementing best practices for ETL and database management\nSupport data migration projects and facilitate seamless data transfers between systems\nTechnical Skills (By Category)\nProgramming Languages:\nEssential: T-SQL, SQL\nPreferred: PowerShell, Python, or similar scripting languages for automation and scripting\nDatabases & Data Management:\nEssential: SQL Server (2016 or higher), relational data modeling, ETL processes\nPreferred: Azure SQL, Amazon RDS, or other cloud-based databases\nFrameworks & Libraries:\nEssential: SSIS (SQL Server Integration Services) packages and components\nPreferred: Data analysis libraries (e.g., Pandas, Power BI integrations)\nDevelopment Tools & Methodologies:\nEssential: SQL Server Management Studio (SSMS), Visual Studio, SQL Server Data Tools (SSDT), version control (Git)\nPreferred: Azure Data Factory, DevOps pipelines, automated deployment tools\nDesign & Architecture:\nData warehouse schema design (star schema, snowflake)\nData flow and process automation best practices\nSecurity & Compliance:\nBasic understanding of database security, access control, and data masking standards\nExperience Requirements\nMinimum of 5+ years working in database development, data warehousing, or ETL processing\nProven experience designing and optimizing large-scale ETL workflows in enterprise environments\nDemonstrated proficiency in writing complex T-SQL queries, stored procedures, and functions\nExperience with SSIS, including package development, deployment, and troubleshooting\nBackground in data migration and data governance is preferred\nIndustry experience in financial services, banking, or large enterprise environments is advantageous\nAlternative pathways include extensive hands-on experience or relevant professional training in ETL and database management\nDay-to-Day Activities\nDevelop, test, and deploy robust ETL workflows using SSIS to meet business needs\nBuild and optimize T-SQL queries, stored procedures, and functions for performance and reliability\nAnalyze source system data structures, identify best-fit schemas, and design data warehouse models\nMonitor and troubleshoot ETL processes, resolving performance bottlenecks and errors\nCollaborate with stakeholders to gather requirements and translate them into technical designs\nReview and optimize database performance and security configurations\nDocument data models, mappings, and processes for operational clarity\nEngage in regular team meetings, code reviews, and process improvement initiatives\nQualifications\nBachelors degree or higher in Computer Science, Information Technology, or related field\nRelevant certifications such as Microsoft Certified: Data Engineer or SQL Server certifications (preferred)\nProven track record of designing and maintaining enterprise data warehouses and ETL processes\nProfessional Competencies\nCritical thinking and analytical skills to troubleshoot complex data issues\nStrong communication skills for effective stakeholder engagement and documentation\nAbility to work independently and collaboratively in a fast-paced environment\nAdaptability to evolving data technologies and requirements\nResults-oriented with excellent time and priority management\nCommitment to continuous improvement and learning new data management techniques.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'T-SQL', 'Azure Data Factory', 'query optimization', 'performance tuning', 'database security', 'AWS Glue', 'Data Warehousing', 'SSIS', 'ETL']",2025-06-12 14:37:43
Senior Cloud Data Engineer,PwC India,10 - 15 years,12-22 Lacs P.A.,['Bengaluru'],"Role & responsibilities\n\nExperienced Senior Data Engineer utilizing Big Data & Google Cloud technologies to develop large scale, on-cloud data processing pipelines and data warehouses with Overall 12 to 15 years of experience\nHave 3 to 4 years of experience of leading Data Engineer teams developing enterprise grade data processing pipelines on multi Clouds like GCP and AWS",,,,"['Pyspark', 'Python', 'SQL', 'Datafactory', 'GCP', 'Microsoft Azure', 'ETL', 'AWS', 'Data Bricks']",2025-06-12 14:37:45
Senior Data Analyst-Azure Data Factory,Lumen Technologies,8 - 12 years,Not Disclosed,['Bengaluru'],"Were looking for a Senior Data Analyst with a strong foundation in Azure-based data engineering and Machine Learning to design, develop, and optimize robust data pipelines, applications, and analytics infrastructure. This role demands deep technical expertise, cross-functional collaboration, and the ability to align data solutions with dynamic business needs.\nKey Responsibilities:\nData Pipeline Development:\nDesign and implement efficient data pipelines using Azure Databricks with PySpark to transform and process large datasets.\nOptimize data workflows for scalability, reliability, and performance.\nApplication Integration:\nCollaborate with cross-functional teams to develop APIs using the .NET Framework for Azure Web Application integration.\nEnsure smooth data exchange between applications and downstream systems.\nData Warehousing and Analytics:\nBuild and manage data warehousing solutions using Synapse Analytics and Azure Data Factory (ADF).\nDevelop and maintain reusable and scalable data models to support business intelligence needs.\nAutomation and Orchestration:\nUtilize Azure Logic Apps, Function Apps, and Azure DevOps to automate workflows and streamline deployments.\nImplement CI/CD pipelines for efficient code deployment and testing.\nInfrastructure Management:\nOversee Azure infrastructure management and maintenance, ensuring a secure and optimized environment.\nProvide support for performance tuning and capacity planning.\nBusiness Alignment:\nGain a deep understanding of AMO data sources and their business implications.\nWork closely with stakeholders to provide customized solutions aligning with business needs.\nBAU Support:\nMonitor and support data engineering workflows and application functionality in BAU mode.\nTroubleshoot and resolve production issues promptly to ensure business continuity.\nTechnical Expertise:\nProficiency in Microsoft SQL for complex data queries and database management.\nAdvanced knowledge of Azure Databricks and PySpark for data engineering and ETL processes.\nExperience with Azure Data Factory (ADF) for orchestrating data workflows.\nExpertise in Azure Synapse Analytics for data integration and analytics.\nProficiency in .NET Framework for API development and integration.\nCloud and DevOps Skills:\nStrong experience in Azure Infrastructure Management and optimization.\nHands-on knowledge of Azure Logic Apps, Function Apps, and Azure DevOps for CI/CD automation.\n""We are an equal opportunity employer committed to fair and ethical hiring practices. We do not charge any fees or accept any form of payment from candidates at any stage of the recruitment process. If anyone claims to offer employment opportunities in our company in exchange for money or any other benefit, please treat it as fraudulent and report it immediately.""\n#LI-BS1",Industry Type: Telecom / ISP,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Performance tuning', 'Automation', 'orchestration', 'Infrastructure management', 'Machine learning', 'Business intelligence', 'Business continuity', 'Analytics', 'Downstream', 'Capacity planning']",2025-06-12 14:37:48
Data Analyst,Sara Business Solution,0 - 2 years,1.44-1.8 Lacs P.A.,['Karur'],"Responsibilities:\n* Analyze data using advanced tools and techniques.\n* Support agri-data projects with insights from the field.\nPassion for agriculture, rural development, or farming\nBasic computer skills (Excel, Google Sheets, etc.)",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent",['any degree with Agriculture background'],2025-06-12 14:37:50
Data Engineer _Technology Lead,Broadridge,6 - 10 years,Not Disclosed,['Bengaluru'],"Key Responsibilities:\nAnalyzes and solve problems using technical experience, judgment and precedents\nProvides informal guidance to new team members\nExplains complex information to others in straightforward situations\n1. Data Engineering and Modelling:\nDesign & Develop Scalable Data Pipelines: Leverage AWS technologies to design, develop, and manage end-to-end data pipelines with services like .",,,,"['Star Schema', 'Snowflake', 'AWS', 'Apache Airflow']",2025-06-12 14:37:52
"Senior Python Developer (Machine Learning,Data Analysis,Visualization)",Synechron,3 - 5 years,Not Disclosed,"['Pune', 'Hinjewadi']","Software Requirements\nRequired Skills:\nProficiency in Python (version 3.6+) with experience in data analysis, manipulation, and scripting\nKnowledge of SQL for data extraction, transformation, and database querying\nExperience with data visualization tools such as PowerBI, Tableau, or QlikView\nFamiliarity with AI and Machine Learning frameworks such as TensorFlow, Keras, PyTorch, or equivalent\nHands-on experience in developing, deploying, and optimizing machine learning models\nPreferred Skills:\nExperience with R for data analysis\nFamiliarity with cloud platforms like AWS, Azure, or GCP for deploying AI solutions\nKnowledge of version control systems such as Git\nOverall Responsibilities\nAnalyze, interpret, and visualize large and complex datasets to extract actionable insights\nDesign, develop, and implement machine learning and AI models for predictive and prescriptive analytics\nCollaborate with cross-functional teams to understand business requirements and translate them into data-driven solutions\nCommunicate findings, insights, and recommendations via reports, dashboards, and presentations to stakeholders\nEvaluate and refine models and algorithms to maximize accuracy, efficiency, and impact\nStay informed on emerging AI, Data Science, and analytics trends and incorporate best practices into projects\nSupport automation efforts, optimize data pipelines, and enhance existing analytical workflows\nContribute to organizational learning by sharing knowledge and mentoring team members\nStrategic objectives:\nDrive innovation through the application of AI and machine learning\nEnable data-driven decision-making across business units\nImprove operational efficiencies and business outcomes\nPerformance outcomes:\nAccurate, robust, and scalable AI models\nHigh-quality insights delivered on time and aligned with business needs\nWell-documented solutions and knowledge-sharing artifacts\nTechnical Skills (By Category)\nProgramming Languages (Essential):\nPython (required); experience with R is a plus\nSQL (required); experience with data manipulation and querying\nData Analysis & Visualization Tools (Essential):\nPowerBI, Tableau, or QlikView\nFrameworks & Libraries (Essential):\nTensorFlow, Keras, PyTorch, or similar frameworks for AI/ML development\nData Management & Databases (Essential):\nRelational databases (e.g., MySQL, PostgreSQL, Oracle)\nData extraction and transformation (ETL processes)\nCloud & Deployment (Preferred):\nExperience deploying models on cloud platforms such as AWS, Azure, GCP\nDevelopment & Version Control (Preferred):\nGit for code versioning\nOther Skills:\nStrong statistical knowledge and experience with data preprocessing, feature engineering\nFamiliarity with agile development methodologies\nExperience Requirements\n3 to 5 years of relevant experience in AI, Data Science, or Data Analytics roles\nProven track record applying machine learning techniques to real-world problems\nExperience working with large datasets and scalable data pipelines\nExperience collaborating with cross-functional teams to deliver analytics-driven solutions\nIndustry experience in finance, healthcare, retail, or similar data-rich sectors is preferred\nAlternative pathways:\nCandidates with extensive AI & ML project experience, strong programming skills, and relevant certifications can be considered with slightly varied years of experience\nDay-to-Day Activities\nCollect, clean, and explore large datasets to identify patterns and insights\nDevelop and tune machine learning models to address business problems\nCollaborate with business analysts, data engineers, and product owners to align technical solutions with organizational goals\nDocument methodologies, code, and analytical findings to ensure reproducibility and knowledge sharing\nCreate dashboards, visualizations, and reports to communicate insights effectively\nEvaluate model performance regularly and optimize models for accuracy and efficiency\nParticipate in team meetings, project planning, and review sessions\nKeep abreast of advancements in AI/ML technologies, tools, and best practices\nQualifications\nBachelors degree in Computer Science, Data Science, Statistics, or related field\nMasters degree or higher in AI, Data Science, or related disciplines is a plus\nProfessional certifications in AI/ML (e.g., TensorFlow Developer, AWS Machine Learning Specialty) are advantageous\nWilling to learn new tools and stay updated with emerging AI trends\nAbility to work independently and collaborate effectively in a dynamic environment\nProfessional Competencies\nAnalytical and problem-solving mindset with a focus on actionable insights\nExcellent verbal and written communication skills for diverse audiences\nStrong interpersonal skills and stakeholder management\nAdaptability to fast-changing technology landscapes\nGrowth mindset with continuous learning enthusiasm\nOrganizational skills to handle multiple projects and priorities simultaneously\nInnovation-driven approach and proactive problem resolution",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Python', 'PostgreSQL', 'MySQL', 'Data Analysis', 'Data Visualization', 'Oracle', 'ETL', 'Machine Learning']",2025-06-12 14:37:54
Data Analyst - Gurugram,Infosys,5 - 10 years,Not Disclosed,"['Chennai', 'Bengaluru', 'PAN INDIA']","Responsibilities:\nUnderstand architecture requirements and ensure effective design, development, validation, and support activities.\nAnalyze user requirements, envisioning system features and functionality.\nIdentify bottlenecks and bugs, and recommend system solutions by comparing advantages and disadvantages of custom development.\nContribute to team meetings, troubleshooting development and production problems across multiple environments and operating platforms.\nEnsure effective design, development, validation, and support activities for Big Data solutions.\nTechnical and Professional Requirements:\nSkills:\nProficiency in Scala, Spark, Hive, and Kafka.\nIn-depth knowledge of design issues and best practices.\nSolid understanding of object-oriented programming.\nFamiliarity with various design, architectural patterns, and software development processes.\nExperience with both external and embedded databases.\nCreating database schemas that represent and support business processes.\nImplementing automated testing platforms and unit tests.\nPreferred Skills:\nTechnology -> Big Data -> Scala, Spark, Hive, Kafka\nAdditional Responsibilities:\nCompetencies:\nGood verbal and written communication skills.\nAbility to communicate with remote teams effectively.\nHigh flexibility to travel.\nEducational Requirements:Master of Computer Applications, Master of Technology, Master of Engineering, MSc, Bachelor of Technology, Bachelor of Computer Applications, Bachelor of Computer Science, Bachelor of Engineering",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Spark', 'Hive', 'Hadoop', 'Big Data', 'Kafka']",2025-06-12 14:37:56
Associate Data Engineer,Amgen Inc,0 - 3 years,Not Disclosed,['Hyderabad'],"What you will do\nIn this vital role We are seeking a Associate Data Engineer to design, build, and maintain scalable data solutions that drive business insights. You will work with large datasets, cloud platforms (AWS preferred), and big data technologies to develop ETL pipelines, ensure data quality, and support data governance initiatives.\nDevelop and maintain data pipelines, ETL/ELT processes, and data integration solutions.\nDesign and implement data models, data dictionaries, and documentation for accuracy and consistency.\nEnsure data security, privacy, and governance standard processes.\nUse Databricks, Apache Spark (PySpark, SparkSQL), AWS, Redshift, for scalable data processing.\nCollaborate with cross-functional teams to understand data needs and deliver actionable insights.\nOptimize data pipeline performance and explore new tools for efficiency.\nFollow best practices in coding, testing, and infrastructure-as-code (CI/CD, version control, automated testing).\n\n\nWhat we expect of you\nWe are all different, yet we all use our unique contributions to serve patients. Strong problem-solving, critical thinking, and communication skills.\nAbility to collaborate effectively in a team setting.\nProficiency in SQL, data analysis tools, and data visualization.\nHands-on experience with big data technologies (Databricks, Apache Spark, AWS, Redshift ).\nExperience with ETL tools, workflow orchestration, and performance tuning for big data.\nBasic Qualifications:\nBachelors degree and 0 to 3 years of experience OR Diploma and 4 to 7 years of experience in Computer science, IT or related field.\nPreferred Qualifications:\nKnowledge of data modeling, warehousing, and graph databases\nExperience with Python, SageMaker, and cloud data platforms.\nAWS Certified Data Engineer or Databricks certification preferred.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'data analysis', 'data modeling', 'data warehousing', 'data visualization', 'Databricks', 'ETL', 'AWS', 'SQL', 'Apache Spark', 'Python']",2025-06-12 14:37:58
Associate Data Engineer,Amgen Inc,0 - 2 years,Not Disclosed,['Hyderabad'],"Role Description:\nWe are looking for an Associate Data Engineer with deep expertise in writing data pipelines to build scalable, high-performance data solutions. The ideal candidate will be responsible for developing, optimizing and maintaining complex data pipelines, integration frameworks, and metadata-driven architectures that enable seamless access and analytics. This role prefers deep understanding of the big data processing, distributed computing, data modeling, and governance frameworks to support self-service analytics, AI-driven insights, and enterprise-wide data management.\nRoles & Responsibilities:\nData Engineer who owns development of complex ETL/ELT data pipelines to process large-scale datasets\nContribute to the design, development, and implementation of data pipelines, ETL/ELT processes, and data integration solutions\nEnsuring data integrity, accuracy, and consistency through rigorous quality checks and monitoring\nExploring and implementing new tools and technologies to enhance ETL platform and performance of the pipelines\nProactively identify and implement opportunities to automate tasks and develop reusable frameworks\nEager to understand the biotech/pharma domains & build highly efficient data pipelines to migrate and deploy complex data across systems\nWork in an Agile and Scaled Agile (SAFe) environment, collaborating with cross-functional teams, product owners, and Scrum Masters to deliver incremental value\nUse JIRA, Confluence, and Agile DevOps tools to manage sprints, backlogs, and user stories.\nSupport continuous improvement, test automation, and DevOps practices in the data engineering lifecycle\nCollaborate and communicate effectively with the product teams, with cross-functional teams to understand business requirements and translate them into technical solutions\nMust-Have Skills:\nExperience in Data Engineering with a focus on Databricks, AWS, Python, SQL, and Scaled Agile methodologies\nProficiency & Strong understanding of data processing and transformation of big data frameworks (Databricks, Apache Spark, Delta Lake, and distributed computing concepts)\nStrong understanding of AWS services and can demonstrate the same\nAbility to quickly learn, adapt and apply new technologies\nStrong problem-solving and analytical skills\nExcellent communication and teamwork skills\nExperience with Scaled Agile Framework (SAFe), Agile delivery, and DevOps practices\nGood-to-Have Skills:\nData Engineering experience in Biotechnology or pharma industry\nExposure to APIs, full stack development\nExperienced with SQL/NOSQL database, vector database for large language models\nExperienced with data modeling and performance tuning for both OLAP and OLTP databases\nExperienced with software engineering best-practices, including but not limited to version control (Git, Subversion, etc.), CI/CD (Jenkins, Maven etc.), automated unit testing, and Dev Ops\nEducation and Professional Certifications\nBachelors degree and 2 to 5 + years of Computer Science, IT or related field experience\nOR\nMasters degree and 1 to 4 + years of Computer Science, IT or related field experience\nAWS Certified Data Engineer preferred\nDatabricks Certificate preferred\nScaled Agile SAFe certification preferred\nSoft Skills:\nExcellent analytical and troubleshooting skills.\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nTeam-oriented, with a focus on achieving team goals.\nAbility to learn quickly, be organized and detail oriented.\nStrong presentation and public speaking skills.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Maven', 'test automation', 'data engineering lifecycle', 'Scaled Agile methodologies', 'JIRA', 'SQL', 'Apache Spark', 'Jenkins', 'Agile DevOps tools', 'ETL platform', 'Confluence', 'Scaled Agile', 'Delta Lake', 'Agile', 'Databricks', 'AWS', 'Python']",2025-06-12 14:38:01
Graph Engineer- Data Science,HARMAN,4 - 9 years,Not Disclosed,['Bengaluru'],"Job Description\nIntroduction: Digital Transformation Solutions (DTS)\n.\nExtensive experience in defining, developing, and implementing security software, ideally with a strong embedded firmware development background\nAbout the Role\nThis position offers an opportunity to work in a globally distributed team where you will get a unique opportunity of personal development in a multi-cultural environment. You will also get a challenging environment to develop expertise in the technologies useful in the industry.",,,,"['Computer science', 'Product quality', 'UML', 'XML', 'Relationship', 'Javascript', 'HTML', 'Oracle', 'Automotive', 'Python']",2025-06-12 14:38:03
Data Engineer,HARMAN,5 - 10 years,Not Disclosed,['Bengaluru'],"-Strong analytical thinking and problem-solving skills, with the ability to translate complex data into actionable insights\n-Excellent communication skills, with the ability to effectively convey complex findings to both technical and non-technical stakeholders.\nCandidate to work form SRIB Bangalore with 3 days working from office is mandatory\n  What You Will Do",,,,"['Digital media', 'CTV', 'Analytical', 'Machine learning', 'Agile', 'Data processing', 'Automotive', 'Python']",2025-06-12 14:38:06
"Data Analyst, Staff",Qualcomm,4 - 7 years,Not Disclosed,['Bengaluru'],"Job Area: Miscellaneous Group, Miscellaneous Group > Data Analyst\n \n\nQualcomm Overview: \nQualcomm is a company of inventors that unlocked 5G ushering in an age of rapid acceleration in connectivity and new possibilities that will transform industries, create jobs, and enrich lives. But this is just the beginning. It takes inventive minds with diverse skills, backgrounds, and cultures to transform 5Gs potential into world-changing technologies and products. This is the Invention Age - and this is where you come in.\n\nGeneral Summary:\n\nAbout the Team\n\nQualcomm's People Analytics team plays a crucial role in transforming data into strategic workforce insights that drive HR and business decisions. As part of this lean but high-impact team, you will have the opportunity to analyze workforce trends, ensure data accuracy, and collaborate with key stakeholders to enhance our data ecosystem. This role is ideal for a generalist who thrives in a fast-paced, evolving environment""”someone who can independently conduct data analyses, communicate insights effectively, and work cross-functionally to enhance our People Analytics infrastructure.\n\nWhy Join Us\n\n\nEnd-to-End ImpactWork on the full analytics cycle""”from data extraction to insight generation""”driving meaningful HR and business decisions.\n\n\nCollaboration at ScalePartner with HR leaders, IT, and other analysts to ensure seamless data integration and analytics excellence.\n\n\nData-Driven CultureBe a key player in refining our data lake, ensuring data integrity, and influencing data governance efforts.\n\n\nProfessional GrowthGain exposure to multiple areas of people analytics, including analytics, storytelling, and stakeholder engagement.\n\n\nKey Responsibilities\n\n\nPeople Analytics & Insights\nAnalyze HR and workforce data to identify trends, generate insights, and provide recommendations to business and HR leaders.\nDevelop thoughtful insights to support ongoing HR and business decision-making.\nPresent findings in a clear and compelling way to stakeholders at various levels, including senior leadership.\n\n\nData Quality & Governance\nEnsure accuracy, consistency, and completeness of data when pulling from the data lake and other sources.\nIdentify and troubleshoot data inconsistencies, collaborating with IT and other teams to resolve issues.\nDocument and maintain data definitions, sources, and reporting standards to drive consistency across analytics initiatives.\n\n\nCollaboration & Stakeholder Management\nWork closely with other analysts on the team to align methodologies, share best practices, and enhance analytical capabilities.\nAct as a bridge between People Analytics, HR, and IT teams to define and communicate data requirements.\nPartner with IT and data engineering teams to improve data infrastructure and expand available datasets.\n\n\nQualifications\n\nRequired4-7 years experience in a People Analytics focused role\n\n\nAnalytical & Technical Skills\nStrong ability to analyze, interpret, and visualize HR and workforce data to drive insights.\nExperience working with large datasets and ensuring data integrity.\nProficiency in Excel and at least one data visualization tool (e.g., Tableau, Power BI).\n\n\nCommunication & Stakeholder Management\nAbility to communicate data insights effectively to both technical and non-technical audiences.\nStrong documentation skills to define and communicate data requirements clearly.\nExperience collaborating with cross-functional teams, including HR, IT, and business stakeholders.\n\n\nPreferred:\n\n\nTechnical Proficiency\nExperience with SQL, Python, or R for data manipulation and analysis.\nFamiliarity with HR systems (e.g., Workday) and cloud-based data platforms.\n\n\nPeople Analytics Expertise\nPrior experience in HR analytics, workforce planning, or related fields.\nUnderstanding of key HR metrics and workforce trends (e.g., turnover, engagement, diversity analytics).\n\n\nAdditional Information\nThis is an office-based position (4 days a week onsite) with possible locations that may include India and Mexico",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['sql', 'people analytics', 'documentation', 'tableau', 'data integration tools', 'hiring', 'data warehousing', 'data architecture', 'sourcing', 'jquery', 'staffing', 'plsql', 'oracle 10g', 'java', 'etl tool', 'html', 'etl', 'mongodb', 'python', 'oracle', 'power bi', 'hrsd', 'r', 'node.js', 'hr analytics', 'angularjs']",2025-06-12 14:38:08
Lead Engineer - Data Science,Sasken Technologies,5 - 8 years,Not Disclosed,['Bengaluru'],"Job Summary\nPerson at this position takes ownership of a module and associated quality and delivery. Person at this position provides instructions, guidance and advice to team members to ensure quality and on time delivery.\nPerson at this position is expected to be able to instruct and review the quality of work done by technical staff.\nPerson at this position should be able to identify key issues and challenges by themselves, prioritize the tasks and deliver results with minimal direction and supervision.\nPerson at this position has the ability to investigate the root cause of the problem and come up alternatives/ solutions based on sound technical foundation gained through in-depth knowledge of technology, standards, tools and processes.\nPerson has the ability to organize and draw connections among ideas and distinguish between those which are implementable.\nPerson demonstrates a degree of flexibility in resolving problems/ issues that atleast to in-depth command of all techniques, processes, tools and standards within the relevant field of specialisation.\n\n\nRoles & Responsibilities\nResponsible for requirement analysis and feasibility study including system level work estimation while considering risk identification and mitigation.\nResponsible for design, coding, testing, bug fixing, documentation and technical support in the assigned area. Responsible for on time delivery while adhering to quality and productivity goals.\nResponsible for traceability of the requirements from design to delivery Code optimization and coverage.\nResponsible for conducting reviews, identifying risks and ownership of quality of deliverables.\nResponsible for identifying training needs of the team.\nExpected to enhance technical capabilities by attending trainings, self-study and periodic technical assessments.\nExpected to participate in technical initiatives related to project and organization and deliver training as per plan and quality.\nExpected to be a technical mentor for junior members.\nPerson may be given additional responsibility of managing people based on discretion of Project Manager.\n\nEducation and Experience Required\nEngineering graduate, MCA, etc Experience: 5-8 years\n\n\nCompetencies Description\nData Science TCB is applicable to one who\n1) Analyses data to arrive at patterns/Insights/models\n2) Come up with models based on the data to provide recommendations, predictive analytics etc\n3) Provides implementation of the models in R, Matlab etc\n4) Can understand and apply machine learning/AI techniques\nPlatforms-\nUnix\nTools-\nR, Matlab, Spark Machine Learning, Python-ML, SPSS, SAS\nLanguages-\nR, Perl, Python, Scala\nSpecialization-\nCOGNITIVE ANALYTICS INCLUDING COMPUTER VISION, AI and ML, STATISTICS",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Unix', 'R', 'SAS', 'Scala', 'Perl', 'SPSS', 'Machine Learning', 'Python']",2025-06-12 14:38:10
Senior Data Engineer - AWS,Blend360 India,5 - 10 years,Not Disclosed,['Hyderabad'],"Job Description\nWe are looking for an experienced Senior Data Engineer with a strong foundation in Python, SQL, and Spark , and hands-on expertise in AWS, Databricks . In this role, you will build and maintain scalable data pipelines and architecture to support analytics, data science, and business intelligence initiatives. You ll work closely with cross-functional teams to drive data reliability, quality, and performance.\nResponsibilities:\nDesign, develop, and optimize scalable data pipelines using Databricks in AWS such as Glue, S3, Lambda, EMR, Databricks notebooks, workflows and jobs.\nBuilding data lake in WS Databricks.\nBuild and maintain robust ETL/ELT workflows using Python and SQL to handle structured and semi-structured data.\nDevelop distributed data processing solutions using Apache Spark or PySpark .\nPartner with data scientists and analysts to provide high-quality, accessible, and well-structured data.\nEnsure data quality, governance, security, and compliance across pipelines and data stores.\nMonitor, troubleshoot, and improve the performance of data systems and pipelines.\nParticipate in code reviews and help establish engineering best practices.\nMentor junior data engineers and support their technical development.\n\n\nQualifications\nRequirements\nBachelors or masters degree in computer science, Engineering, or a related field.\n5+ years of hands-on experience in data",Industry Type: Advertising & Marketing,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Automation', 'Version control', 'GIT', 'Workflow', 'Data quality', 'Business intelligence', 'Analytics', 'SQL', 'Python']",2025-06-12 14:38:13
"Senior Staff Engineer, Big Data Engineer",Nagarro,10 - 13 years,Not Disclosed,['India'],"We're Nagarro.\nWe are a Digital Product Engineering company that is scaling in a big way! We build products, services, and experiences that inspire, excite, and delight. We work at scale across all devices and digital mediums, and our people exist everywhere in the world (18000+ experts across 38 countries, to be exact). Our work culture is dynamic and non-hierarchical. We're looking for great new colleagues. That's where you come in!\n\nREQUIREMENTS:\nTotal experience 10+ years.\nExcellent knowledge and experience in Big data engineer.\nStrong working experience with architecture and development in Apache Spark, Spark, Scala, AWS/Azure/GCP, Data Pipelines, Kafka, SQL Server/NoSQL.\nStrong expertise in Django Rest Framework, Databricks and PostgreSQL.\nHands on experience in building data pipelines and building data frameworks for unit testing, data lineage tracking, and automation.\nFamiliarity with streaming technologies (e.g., Kafka, Kinesis, Flink).\nExperience with Machine Learning and Looker.\nKnowledge of additional server-side programming languages (e.g. Golang, C#, Ruby).\nExperience with building and maintaining a cloud system.\nFamiliarity with data modeling, data warehousing, and building distributed systems.\nExpertise in Spanner for high-availability, scalable database solutions.\nKnowledge of data governance and security practices in cloud-based environments.\nProblem-solving mindset with the ability to tackle complex data engineering challenges.\nStrong communication and teamwork skills, with the ability to mentor and collaborate effectively.\n\nRESPONSIBILITIES:\nWriting and reviewing great quality code\nUnderstanding the clients business use cases and technical requirements and be able to convert them in to technical design which elegantly meets the requirements\nMapping decisions with requirements and be able to translate the same to developers\nIdentifying different solutions and being able to narrow down the best option that meets the client’s requirements\nDefining guidelines and benchmarks for NFR considerations during project implementation\nWriting and reviewing design document explaining overall architecture, framework, and high-level design of the application for the developers\nReviewing architecture and design on various aspects like extensibility, scalability, security, design patterns, user experience, NFRs, etc., and ensure that all relevant best practices are followed\nDeveloping and designing the overall solution for defined functional and non-functional requirements; and defining technologies, patterns, and frameworks to materialize it\nUnderstanding and relating technology integration scenarios and applying these learnings in projects\nResolving issues that are raised during code/review, through exhaustive systematic analysis of the root cause, and being able to justify the decision taken\nCarrying out POCs to make sure that suggested design/technologies meet the requirements",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Big Data', 'Django Framework', 'Spark', 'Data Bricks', 'Apache Spark']",2025-06-12 14:38:16
Senior Data Engineer,Qualcomm,5 - 10 years,Not Disclosed,['Hyderabad'],"Job Area: Information Technology Group, Information Technology Group > IT Data Engineer\n\nGeneral Summary:\n\nDeveloper will play an integral role in the PTEIT Machine Learning Data Engineering team. Design, develop and support data pipelines in a hybrid cloud environment to enable advanced analytics. Design, develop and support CI/CD of data pipelines and services. - 5+ years of experience with Python or equivalent programming using OOPS, Data Structures and Algorithms - Develop new services in AWS using server-less and container-based services. - 3+ years of hands-on experience with AWS Suite of services (EC2, IAM, S3, CDK, Glue, Athena, Lambda, RedShift, Snowflake, RDS) - 3+ years of expertise in scheduling data flows using Apache Airflow - 3+ years of strong data modelling (Functional, Logical and Physical) and data architecture experience in Data Lake and/or Data Warehouse - 3+ years of experience with SQL databases - 3+ years of experience with CI/CD and DevOps using Jenkins - 3+ years of experience with Event driven architecture specially on Change Data Capture - 3+ years of Experience in Apache Spark, SQL, Redshift (or) Big Query (or) Snowflake, Databricks - Deep understanding building the efficient data pipelines with data observability, data quality, schema drift, alerting and monitoring. - Good understanding of the Data Catalogs, Data Governance, Compliance, Security, Data sharing - Experience in building the reusable services across the data processing systems. - Should have the ability to work and contribute beyond defined responsibilities - Excellent communication and inter-personal skills with deep problem-solving skills.\n\nMinimum Qualifications:\n3+ years of IT-related work experience with a Bachelor's degree in Computer Engineering, Computer Science, Information Systems or a related field.\nOR\n5+ years of IT-related work experience without a Bachelors degree.\n\n2+ years of any combination of academic or work experience with programming (e.g., Java, Python).\n1+ year of any combination of academic or work experience with SQL or NoSQL Databases.\n1+ year of any combination of academic or work experience with Data Structures and algorithms.\n5 years of Industry experience and minimum 3 years experience in Data Engineering development with highly reputed organizations- Proficiency in Python and AWS- Excellent problem-solving skills- Deep understanding of data structures and algorithms- Proven experience in building cloud native software preferably with AWS suit of services- Proven experience in design and develop data models using RDBMS (Oracle, MySQL, etc.)\n\nDesirable - Exposure or experience in other cloud platforms (Azure and GCP) - Experience working on internals of large-scale distributed systems and databases such as Hadoop, Spark - Working experience on Data Lakehouse platforms (One House, Databricks Lakehouse) - Working experience on Data Lakehouse File Formats (Delta Lake, Iceberg, Hudi)\n\nBachelor's or Master's degree in Computer Science, Software Engineering, or a related field.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['algorithms', 'python', 'data quality', 'data structures', 'aws', 'schema', 'continuous integration', 'glue', 'amazon redshift', 'event driven architecture', 'ci/cd', 'data engineering', 'sql', 'alerts', 'java', 'data modeling', 'spark', 'devops', 'data flow', 'nosql databases', 'sql database']",2025-06-12 14:38:18
Senior Big Data Engineer,Qualcomm,2 - 7 years,Not Disclosed,['Hyderabad'],"Job Area: Engineering Group, Engineering Group > Software Engineering\n\nGeneral Summary:\n\nAs a leading technology innovator, Qualcomm pushes the boundaries of what's possible to enable next-generation experiences and drives digital transformation to help create a smarter, connected future for all. As a Qualcomm Software Engineer, you will design, develop, create, modify, and validate embedded and cloud edge software, applications, and/or specialized utility programs that launch cutting-edge, world class products that meet and exceed customer needs. Qualcomm Software Engineers collaborate with systems, hardware, architecture, test engineers, and other teams to design system-level software solutions and obtain information on performance requirements and interfaces.\n\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 2+ years of Software Engineering or related work experience.\nOR\nMaster's degree in Engineering, Information Systems, Computer Science, or related field and 1+ year of Software Engineering or related work experience.\nOR\nPhD in Engineering, Information Systems, Computer Science, or related field.\n\n2+ years of academic or work experience with Programming Language such as C, C++, Java, Python, etc.\n\nGeneral Summary\n\nPreferred Qualifications\n\n3+ years of experience as a Data Engineer or in a similar role\n\nExperience with\n\ndata modeling, data warehousing, and building ETL pipelines\n\nSolid working experience with\n\nPython, AWS analytical technologies and related resources (Glue, Athena, QuickSight, SageMaker, etc.,)\n\nExperience with\n\nBig Data tools, platforms and architecture with solid working experience with SQL\n\nExperience working in a very large data warehousing environment,\n\nDistributed System.\n\nSolid understanding on various data exchange formats and complexities\n\nIndustry experience in software development, data engineering, business intelligence, data science, or related field with a track record of manipulating, processing, and extracting value from large datasets\n\nStrong data visualization skills\n\nBasic understanding of Machine Learning; Prior experience in ML Engineering a plus\n\nAbility to manage on-premises data and make it inter-operate with AWS based pipelines\n\nAbility to interface with Wireless Systems/SW engineers and understand the Wireless ML domain; Prior experience in Wireless (5G) domain a plus\n\n\nEducation\n\nBachelor's degree in computer science, engineering, mathematics, or a related technical discipline\n\nPreferred QualificationsMasters in CS/ECE with a Data Science / ML Specialization\n\n\nMinimum Qualifications:\n\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 3+ years of Software Engineering or related work experience.\n\nOR\n\nMaster's degree in Engineering, Information Systems, Computer Science, or related field\n\nOR\n\nPhD in Engineering, Information Systems, Computer Science, or related field.\n\n3+ years of experience with Programming Language such as C, C++, Java, Python, etc.\n\n\nDevelops, creates, and modifies general computer applications software or specialized utility programs. Analyzes user needs and develops software solutions. Designs software or customizes software for client use with the aim of optimizing operational efficiency. May analyze and design databases within an application area, working individually or coordinating database development as part of a team. Modifies existing software to correct errors, allow it to adapt to new hardware, or to improve its performance. Analyzes user needs and software requirements to determine feasibility of design within time and cost constraints. Confers with systems analysts, engineers, programmers and others to design system and to obtain information on project limitations and capabilities, performance requirements and interfaces. Stores, retrieves, and manipulates data for analysis of system capabilities and requirements. Designs, develops, and modifies software systems, using scientific analysis and mathematical models to predict and measure outcome and consequences of design.\n\nPrincipal Duties and Responsibilities:\n\nCompletes assigned coding tasks to specifications on time without significant errors or bugs.\n\nAdapts to changes and setbacks in order to manage pressure and meet deadlines.\n\nCollaborates with others inside project team to accomplish project objectives.\n\nCommunicates with project lead to provide status and information about impending obstacles.\n\nQuickly resolves complex software issues and bugs.\n\nGathers, integrates, and interprets information specific to a module or sub-block of code from a variety of sources in order to troubleshoot issues and find solutions.\n\nSeeks others' opinions and shares own opinions with others about ways in which a problem can be addressed differently.\n\nParticipates in technical conversations with tech leads/managers.\n\nAnticipates and communicates issues with project team to maintain open communication.\n\nMakes decisions based on incomplete or changing specifications and obtains adequate resources needed to complete assigned tasks.\n\nPrioritizes project deadlines and deliverables with minimal supervision.\n\nResolves straightforward technical issues and escalates more complex technical issues to an appropriate party (e.g., project lead, colleagues).\n\nWrites readable code for large features or significant bug fixes to support collaboration with other engineers.\n\nDetermines which work tasks are most important for self and junior engineers, stays focused, and deals with setbacks in a timely manner.\n\nUnit tests own code to verify the stability and functionality of a feature.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'sql', 'software engineering', 'data visualization', 'aws', 'quicksight', 'c', 'software development', 'glue', 'aws sagemaker', 'data warehousing', 'machine learning', 'business intelligence', 'data engineering', 'java', 'data science', 'data modeling', 'athena', 'wireless', 'big data', 'etl', 'ml']",2025-06-12 14:38:21
Senior Data Engineer - Azure,Blend360 India,3 - 6 years,Not Disclosed,['Hyderabad'],"Job Description\nAs a Data Engineer, your role is to spearhead the data engineering teams and elevate the team to the next level! You will be responsible for laying out the architecture of the new project as well as selecting the tech stack associated with it. You will plan out the development cycles deploying AGILE if possible and create the foundations for good data stewardship with our new data products!\nYou will also set up a solid code framework that needs to be built to purpose yet have enough flexibility to adapt to new business use cases tough but rewarding challenge!\n\nResponsibilities\nCollaborate with several stakeholders to deeply understand the needs of data practitioners to deliver at scale\nLead Data Engineers to define, build and maintain Data Platform\nWork on building Data Lake in Azure Fabric processing data from multiple sources\nMigrating existing data store from Azure Synapse to Azure Fabric\nImplement data governance and access control\nDrive development effort End-to-End for on-time delivery of high-quality solutions that conform to requirements, conform to the architectural vision, and comply with all applicable standards.\nPresent technical solutions, capabilities, considerations, and features in business terms. Effectively communicate status, issues, and risks in a precise and timely manner.\nFurther develop critical initiatives, such as Data Discovery, Data Lineage and Data Quality\nLeading team and Mentor junior resources\nHelp your team members grow in their role and achieve their career aspirations\nBuild data systems, pipelines, analytical tools and programs\nConduct complex data analysis and report on results\n\n\nQualifications\n3+ Years of Experience as a data engineer or similar role in Azure Synapses, ADF or\nrelevant exp in Azure Fabric\nDegree in Computer Science, Data Science, Mathematics, IT, or similar fiel",Industry Type: Advertising & Marketing,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Data analysis', 'Data modeling', 'Analytical', 'Agile', 'data governance', 'Data quality', 'Data mining', 'SQL', 'Python']",2025-06-12 14:38:23
Senior Data Engineer,Fractal Analytics,8 - 10 years,Not Disclosed,['Mumbai'],"Job Description:\nAs a Backend (Java) Engineer, you would be part of the team consisting of Scrum Master, Cloud Engineers, AI/ML Engineers, and UI/UX Engineers to build end-to-end Data to Decision Systems.\nMandatory:\n8+ years of demonstrable experience designing, building, and working as a Java Developer for enterprise web applications\nIdeally, this would include the following:\no Expert-level proficiency with Java\no Expert-level proficiency with SpringBoot\nFamiliarity with common databases (RDBMS such as MySQL & NoSQL such as MongoDB) and data warehousing concepts (OLAP, OLTP)\nUnderstanding of REST concepts and building/interacting with REST APIs\nDeep understanding of core backend concepts:\no Develop and design RESTful services and APIs\no Develop functional databases, applications, and servers to support websites on the back end\no Performance optimization and multithreading concepts\no Experience with deploying and maintaining high traffic infrastructure (performance testing is a plus)\nIn addition, the ideal candidate would have great problem-solving skills, and familiarity with code versioning tools such as GitHub",,,,"['Backend', 'Multithreading', 'RDBMS', 'MySQL', 'Performance testing', 'OLAP', 'Scrum', 'MongoDB', 'Apache', 'OLTP']",2025-06-12 14:38:26
Big Data Lead,IQVIA,8 - 13 years,25-40 Lacs P.A.,['Bengaluru'],"Job Title / Primary Skill: Big Data Developer (Lead/Associate Manager)\nManagement Level: G150\nYears of Experience: 8 to 13 years\nJob Location: Bangalore (Hybrid)\nMust Have Skills: Big data, Spark, Scala, SQL, Hadoop Ecosystem.\nEducational Qualification: BE/BTech/ MTech/ MCA, Bachelor or masters degree in Computer Science,\n\nJob Overview\nOverall Experience 8+ years in IT, Software Engineering or relevant discipline.\nDesigns, develops, implements, and updates software systems in accordance with the needs of the organization.\nEvaluates, schedules, and resources development projects; investigates user needs; and documents, tests, and maintains computer programs.\nJob Description:\nWe look for developers to have good knowledge of Scala programming skills and Knowledge of SQL\nTechnical Skills:\nScala, Python -> Scala is often used for Hadoop-based projects, while Python and Scala are choices for Apache Spark-based projects.\nSQL -> Knowledge of SQL (Structured Query Language) is important for querying and manipulating data\nShell Script -> Shell scripts are used for batch processing of data, it can be used for scheduling the jobs and shell scripts are often used for deploying applications\nSpark Scala -> Spark Scala allows you to write Spark applications using the Spark API in Scala\nSpark SQL -> It allows to work with structured data using SQL-like queries and Data Frame APIs.\nWe can execute SQL queries against Data Frames, enabling easy data exploration, transformation, and analysis.\n\nThe typical tasks and responsibilities of a Big Data Developer include:\n1. Data Ingestion: Collecting and importing data from various sources, such as databases, logs, APIs into the Big Data infrastructure.\n2. Data Processing: Designing data pipelines to clean, transform, and prepare raw data for analysis. This often involves using technologies like Apache Hadoop, Apache Spark.\n3. Data Storage: Selecting appropriate data storage technologies like Hadoop Distributed File System (HDFS), HIVE, IMPALA, or cloud-based storage solutions (Snowflake, Databricks).",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SCALA', 'Big Data', 'Spark', 'Hive', 'Apache Pig', 'Hadoop', 'Hadoop Development', 'Mapreduce', 'Hdfs', 'Impala', 'YARN']",2025-06-12 14:38:28
Big Data Lead,Hexaware Technologies,8 - 13 years,18-25 Lacs P.A.,"['Chennai', 'Bengaluru', 'Mumbai (All Areas)']","As an Azure Data Engineer, we are looking for candidates who possess expertise in the following:\nDatabricks\nData Factory\nSQL\nPyspark/Spark\n\nRoles and Responsibilities:",,,,"['Databricks', 'Sql', 'Python']",2025-06-12 14:38:30
Data Analyst - L3,Wipro,3 - 5 years,Not Disclosed,['Hyderabad'],"Role Purpose\nThe purpose of this role is to interpret data and turn into information (reports, dashboards, interactive visualizations etc) which can offer ways to improve a business, thus affecting business decisions.\nDo\n1. Managing the technical scope of the project in line with the requirements at all stages\na. Gather information from various sources (data warehouses, database, data integration and modelling) and interpret patterns and trends\nb. Develop record management process and policies\nc. Build and maintain relationships at all levels within the client base and understand their requirements.\nd. Providing sales data, proposals, data insights and account reviews to the client base\ne. Identify areas to increase efficiency and automation of processes\nf. Set up and maintain automated data processes\ng. Identify, evaluate and implement external services and tools to support data validation and cleansing.\nh. Produce and track key performance indicators\n2. Analyze the data sets and provide adequate information\na. Liaise with internal and external clients to fully understand data content\nb. Design and carry out surveys and analyze survey data as per the customer requirement\nc. Analyze and interpret complex data sets relating to customers business and prepare reports for internal and external audiences using business analytics reporting tools\nd. Create data dashboards, graphs and visualization to showcase business performance and also provide sector and competitor benchmarking\ne. Mine and analyze large datasets, draw valid inferences and present them successfully to management using a reporting tool\nf. Develop predictive models and share insights with the clients as per their requirement\n\nDeliver\n\nNoPerformance ParameterMeasure1.Analyses data sets and provide relevant information to the clientNo. Of automation done, On-Time Delivery, CSAT score, Zero customer escalation, data accuracy\n\nMandatory Skills: Business Analyst/ Data Analyst(Media). Experience: 3-5 Years.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data analysis', 'data validation', 'data mining', 'business analysis', 'data warehousing', 'business analytics', 'dbms', 'dashboards', 'sales', 'analytics reporting', 'reporting tools', 'data integration', 'digital transformation']",2025-06-12 14:38:33
Cloud Data Engineer,PwC India,5 - 8 years,10-20 Lacs P.A.,['Bengaluru'],"Role & responsibilities\nStrong hands-on experience with multi cloud (AWS, Azure, GCP)  services such as GCP BigQuery, Dataform AWS Redshift, \nProficient in PySpark and SQL for building scalable data processing pipelines\nKnowledge of utilizing serverless technologies like AWS Lambda, and Google Cloud Functions \nExperience in orchestration frameworks like Apache Airflow, Kubernetes, and Jenkins to manage and orchestrate data pipelines",,,,"['Pyspark', 'Python', 'SQL', 'Datafactory', 'GCP', 'Microsoft Azure', 'AWS', 'Data Bricks']",2025-06-12 14:38:36
Cloud Data Engineer,PwC India,3 - 8 years,Not Disclosed,"['Gurugram', 'Bengaluru']","Job Description:\n\nWe are seeking skilled and dynamic Cloud Data Engineers specializing in AWS, Azure, Databricks. The ideal candidate will have a strong background in data engineering, with a focus on data ingestion, transformation, and warehousing. They should also possess excellent knowledge of PySpark or Spark, and a proven ability to optimize performance in Spark job executions.\n\nKey Responsibilities:",,,,"['AWS OR Azure', 'Azure Data Engineer OR AWS Data Engineer', 'Azure', 'AWS']",2025-06-12 14:38:39
Data Analyst,Teleperformance (TP),3 - 7 years,3-8 Lacs P.A.,['Hyderabad'],"Key Responsibilities:\nAnalyze large volumes of labeled and unlabeled data to identify trends, anomalies, and labeling patterns that can improve model training or operational efficiency.\nDesign and maintain automated dashboards and reporting frameworks to track labeling quality, throughput, and issue trends.\nPartner with Client leadership to understand data requirements and provide actionable insights for model optimization.\nDevelop scalable data pipelines for data validation, aggregation, and visualization.\nApply data mining techniques to evaluate annotation consistency, inter-rater reliability, and data quality.\nContribute to AI data evaluation strategies through analytical experimentation and feedback integration.\nCollaborate with cross-functional teams to enhance data annotation workflows and ensure metrics alignment.\nRequirements:\nBachelors degree in Statistics, Mathematics, Computer Science, Data Science, or a related field.\n3–8 years of hands-on experience in data analysis roles, preferably in AI/ML or data labeling environments.\nProficient in SQL and Python for data manipulation, analysis, and automation.\nUnderstanding of data labeling workflows and familiarity with metrics like accuracy, precision, recall, and inter-rater agreement.\nStrong analytical thinking with the ability to interpret large datasets and provide actionable insights.\nExcellent communication skills with the ability to present findings to both technical and non-technical audiences.\nSelf-starter with a keen eye for detail and a passion for working in AI-driven data environments.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data anayst', 'SQL', 'Google Suite', 'Data Analysis', 'Advanced Excel', 'MI']",2025-06-12 14:38:41
Data Engineer II,Amazon,3 - 8 years,Not Disclosed,['Bengaluru'],"Amazon strives to be the worlds most customer-centric company, where customers can research and purchase anything they might want online\nWe set big goals and are looking for people who can help us reach and exceed them\nThe CPT Data Engineering & Analytics (DEA) team builds and maintains critical data infrastructure that enhances seller experience and protects the privacy of Amazon business partners throughout their lifecycle\nWe are looking for a strong Data Engineer to join our team\n\nThe Data Engineer I will work with well-defined requirements to develop and maintain data pipelines that help internal teams gather required insights for business decisions timely and accurately\nYou will collaborate with a team of Data Scientists, Business Analysts and other Engineers to build solutions that reduce investigation defects and assess the health of our Operations business while ensuring data quality and regulatory compliance\n\nThe ideal candidate must be passionate about building reliable data infrastructure, detail-oriented, and driven to help protect Amazons customers and business partners\nThey will be an individual contributor who works effectively with guidance from senior team members to successfully implement data solutions\nThe candidate must be proficient in SQL and at least one scripting language (e\ng\nPython, Perl, Scala), with strong understanding of data management fundamentals and distributed systems concepts\n\n\nBuild and optimize physical data models and data pipelines for simple datasets\nWrite secure, stable, testable, maintainable code with minimal defects\nTroubleshoot existing datasets and maintain data quality\nParticipate in team design, scoping, and prioritization discussions\nDocument solutions to ensure ease of use and maintainability\nHandle data in accordance with Amazon policies and security requirements Masters degree in computer science, engineering, analytics, mathematics, statistics, IT or equivalent\n3+ years of data engineering experience\nExperience with SQL\nExperience with data modeling, warehousing and building ETL pipelines\nKnowledge of distributed systems concepts from data storage and compute perspective\nAbility to work effectively in a team environment Experience with AWS technologies like Redshift, S3, AWS Glue, EMR, Kinesis, FireHose, Lambda, and IAM roles and permissions\nFamiliarity with big data technologies (Hadoop, Spark, etc\n)\nKnowledge of data security and privacy best practices\nStrong problem-solving and analytical skills\nExcellent written and verbal communication skills",Industry Type: Internet,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Data management', 'Data modeling', 'data security', 'Perl', 'Data quality', 'Distribution system', 'Analytics', 'SQL', 'Python']",2025-06-12 14:38:44
Specialist Data/AI Engineering,ATT Communication Services,4 - 9 years,Not Disclosed,['Bengaluru'],"Key Roles and Responsibilities\nDevelop, enhance, and support assigned applications, ensuring seamless functionality and high performance.\nProvide subject matter expertise, acting as a go-to resource for application-specific knowledge.\nCollaborate and communicate effectively with team members, stakeholders, and end-users.\nTroubleshoot issues, monitor performance, and optimize processes for continuous improvement.\nStay current with industry trends and share best practices with the team.\nAdhere to company policies, procedures, and security requirements while maintaining compliance standards.\nFlexible with shifts and occasional weekend support.\nKey Competencies\nFull life-cycle experience on enterprise software development projects.\nExperience in relational databases/ data marts/data warehouses and complex SQL programming.\nExtensive experience in ETL, shell or python scripting, data modelling, analysis, and preparation\nExperience in Unix/Linux system, files systems, shell scripting.\nGood to have knowledge on any cloud platforms like Azure, Databricks etc.\nGood to have experience in BI Reporting tools Power BI\nGood problem-solving and analytical skills used to resolve technical problems.\nMust possess a good understanding of Compliance and Security Standards (preferably US standards).\nAbility to work independently but must be a team player. Should be able to drive business decisions and take ownership of their work.\nExperience in presentation design, development, delivery, and good communication skills to present analytical results and recommendations for action-oriented data driven decisions and associated operational and financial impacts.\nRequired/Desired Skills\nApplication: Databricks\nLanguages: Python, Shell Scripting, PLSQL, SQL\nCloud Technologies: Azure.\nTools: Jupyter Notebooks, SQL Developer, Postman, IDE.\nDatabase: Oracle, Snowflake and SQL Server.\nDevops: Jenkins, Kubernetes and Docker.\nBachelor s degree in computer science, Information Systems or related field.\n4+ years of experience in working in Engineering or Development roles with Compliance Standards\n4+ years of experience building high transaction defensive web applications and Python applications\n2+ years of experience in cloud technologies: AWS, Azure, OpenStack, Ansible, Chef or Terraform\n2+ years of experience in creating application for container services Docker, Kubernetes,\n2+ years of experience in build and CICD technologies: GitHub, Maven, Jenkins, Nexus or Sonar\nProficiency in Unix/Linux command line\n1+ years of experience in building applications using Large Language Models.\n1+ years of experience in building intelligent automation using Power Automate\n1+ years of experience in using Agentic framework\n1+ years of experience in building accurate prompts for AI tools.\nExpert knowledge and experience working with asynchronous message processing, stream processing and event driven computing.\nExperience working within Agile/Scrum/Kanban development team\nFamiliarity with HTML5, JavaScript frameworks, and CSS3\nCertified in Java, Spring or Azure technologies\nExcellent written and verbal communication skills with demonstrated ability to present complex technical information in a clear manner to peers, developers, and senior leaders\nEducation & Qualifications\nUniversity Degree in Computer Science and/or Analytics\nMinimum Experience required: 6-9 years in relational database design & development, ETL development, GenAI\nAdditional Details\nShift timing (if any): 12.30 to 9.30 IST(Bangalore)\nWork mode: Hybrid (3 days mandatory in office)\nLocation: Bangalore\n#DataPlatform\n#DataScience\nJob ID R-61633 Date posted 06/03/2025",Industry Type: Telecom / ISP,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Unix', 'Computer science', 'Automation', 'Linux', 'Database design', 'Shell scripting', 'Javascript', 'Design development', 'SQL', 'Python']",2025-06-12 14:38:47
Data Techology Senior Associate,MSCI Services,4 - 7 years,Not Disclosed,['Pune'],"Overview\nThe Data Technology team at MSCI is responsible for meeting the data requirements across various business areas, including Index, Analytics, and Sustainability. Our team collates data from multiple sources such as vendors (e.g., Bloomberg, Reuters), website acquisitions, and web scraping (e.g., financial news sites, company websites, exchange websites, filings). This data can be in structured or semi-structured formats. We normalize the data, perform quality checks, assign internal identifiers, and release it to downstream applications.\nResponsibilities\nAs data engineers, we build scalable systems to process data in various formats and volumes, ranging from megabytes to terabytes. Our systems perform quality checks, match data across various sources, and release it in multiple formats. We leverage the latest technologies, sources, and tools to process the data. Some of the exciting technologies we work with include Snowflake, Databricks, and Apache Spark.\nQualifications\nCore Java, Spring Boot, Apache Spark, Spring Batch, Python. Exposure to sql databases like Oracle, Mysql, Microsoft Sql is a must. Any experience/knowledge/certification on Cloud technology preferrably Microsoft Azure or Google cloud platform is good to have. Exposures to non sql databases like Neo4j or Document database is again good to have.\n What we offer you\nTransparent compensation schemes and comprehensive employee benefits, tailored to your location, ensuring your financial security, health, and overall wellbeing.\nFlexible working arrangements, advanced technology, and collaborative workspaces.\nA culture of high performance and innovation where we experiment with new ideas and take responsibility for achieving results.\nA global network of talented colleagues, who inspire, support, and share their expertise to innovate and deliver for our clients.\nGlobal Orientation program to kickstart your journey, followed by access to our Learning@MSCI platform, LinkedIn Learning Pro and tailored learning opportunities for ongoing skills development.\nMulti-directional career paths that offer professional growth and development through new challenges, internal mobility and expanded roles.\nWe actively nurture an environment that builds a sense of inclusion belonging and connection, including eight Employee Resource Groups. All Abilities, Asian Support Network, Black Leadership Network, Climate Action Network, Hola! MSCI, Pride & Allies, Women in Tech, and Women’s Leadership Forum.\nAt MSCI we are passionate about what we do, and we are inspired by our purpose – to power better investment decisions. You’ll be part of an industry-leading network of creative, curious, and entrepreneurial pioneers. This is a space where you can challenge yourself, set new standards and perform beyond expectations for yourself, our clients, and our industry.\nMSCI is a leading provider of critical decision support tools and services for the global investment community. With over 50 years of expertise in research, data, and technology, we power better investment decisions by enabling clients to understand and analyze key drivers of risk and return and confidently build more effective portfolios. We create industry-leading research-enhanced solutions that clients use to gain insight into and improve transparency across the investment process.\nMSCI Inc. is an equal opportunity employer. It is the policy of the firm to ensure equal employment opportunity without discrimination or harassment on the basis of race, color, religion, creed, age, sex, gender, gender identity, sexual orientation, national origin, citizenship, disability, marital and civil partnership/union status, pregnancy (including unlawful discrimination on the basis of a legally protected parental leave), veteran status, or any other characteristic protected by law. MSCI is also committed to working with and providing reasonable accommodations to individuals with disabilities. If you are an individual with a disability and would like to request a reasonable accommodation for any part of the application process, please email Disability.Assistance@msci.com and indicate the specifics of the assistance needed. Please note, this e-mail is intended only for individuals who are requesting a reasonable workplace accommodation; it is not intended for other inquiries.\n To all recruitment agencies\nMSCI does not accept unsolicited CVs/Resumes. Please do not forward CVs/Resumes to any MSCI employee, location, or website. MSCI is not responsible for any fees related to unsolicited CVs/Resumes.\n Note on recruitment scams\nWe are aware of recruitment scams where fraudsters impersonating MSCI personnel may try and elicit personal information from job seekers. Read our full note on careers.msci.com",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['hive', 'access', 'scala', 'pyspark', 'data warehousing', 'hibernate', 'research', 'sql', 'analytics', 'spring', 'java', 'spring batch', 'spark', 'gcp', 'mysql', 'html', 'hadoop', 'big data', 'etl', 'snowflake', 'python', 'oracle', 'data analysis', 'microsoft azure', 'power bi', 'sql server', 'javascript', 'data bricks', 'spring boot', 'tableau', 'neo4j', 'aws', 'sql database']",2025-06-12 14:38:49
AWS Data Engineer,Infosys,5 - 9 years,Not Disclosed,"['Hyderabad', 'Pune', 'Bengaluru']","Significant 5 to 9 years of experience in designing and implementing scalable data engineering solutions on AWS.\nStrong proficiency in Python programming language.\nExpertise in serverless architecture and AWS services such as Lambda, Glue, Redshift, Kinesis, SNS, SQS, and CloudFormation.\nExperience with Infrastructure as Code (IaC) using AWS CDK for defining and provisioning AWS resources.\nProven leadership skills with the ability to mentor and guide junior team members.\nExcellent understanding of data modeling concepts and experience with tools like ERStudio.\nStrong communication and collaboration skills, with the ability to work effectively in a cross-functional team environment.\nExperience with Apache Airflow for orchestrating data pipelines is a plus.\nKnowledge of Data Lakehouse, dbt, or Apache Hudi data format is a plus.\nRoles and Responsibilities\nDesign, develop, test, deploy, and maintain large-scale data pipelines using AWS services such as S3, Glue, Lambda, Redshift.\nCollaborate with cross-functional teams to gather requirements and design solutions that meet business needs.\nDesired Candidate Profile\n5-9 years of experience in an IT industry setting with expertise in Python programming language (Pyspark).\nStrong understanding of AWS ecosystem including S3, Glue, Lambda, Redshift.\nBachelor's degree in Any Specialization (B.Tech/B.E.).",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Aws Glue', 'Pyspark', 'Aws Lambda', 'Amazon Redshift', 'Athena', 'Python']",2025-06-12 14:38:52
Senior Data Engineer,Grid Dynamics,8 - 13 years,15-25 Lacs P.A.,['Bengaluru'],"We are looking for an enthusiastic and technology-proficient Big Data Engineer, who is eager to participate in the design and implementation of a top-notch Big Data solution to be deployed at massive scale.\nOur customer is one of the world's largest technology companies based in Silicon Valley with operations all over the world. On this project we are working on the bleeding-edge of Big Data technology to develop high performance data analytics platform, which handles petabytes datasets.\nEssential functions\nParticipate in design and development of Big Data analytical applications.\nDesign, support and continuously enhance the project code base, continuous integration pipeline, etc.\nWrite complex ETL processes and frameworks for analytics and data management.\nImplement large-scale near real-time streaming data processing pipelines.\nWork inside the team of industry experts on the cutting edge Big Data technologies to develop solutions for deployment at massive scale.\nQualifications\nStrong coding experience with Scala, Spark,Hive, Hadoop.\nIn-depth knowledge of Hadoop and Spark, experience with data mining and stream processing technologies (Kafka, Spark Streaming, Akka Streams).\nUnderstanding of the best practices in data quality and quality engineering.\nExperience with version control systems, Git in particular.\nDesire and ability for quick learning of new tools and technologies.\nWould be a plus\nKnowledge of Unix-based operating systems (bash/ssh/ps/grep etc.).\nExperience with Github-based development processes.\nExperience with JVM build systems (SBT, Maven, Gradle).\nWe offer\nOpportunity to work on bleeding-edge projects\nWork with a highly motivated and dedicated team\nCompetitive salary\nFlexible schedule\nBenefits package - medical insurance, sports\nCorporate social events\nProfessional development opportunities\nWell-equipped office\nAbout us\nGrid Dynamics (NASDAQ: GDYN) is a leading provider of technology consulting, platform and product engineering, AI, and advanced analytics services. Fusing technical vision with business acumen, we solve the most pressing technical challenges and enable positive business outcomes for enterprise companies undergoing business transformation. A key differentiator for Grid Dynamics is our 8 years of experience and leadership in enterprise AI, supported by profound expertise and ongoing investment in data, analytics, cloud & DevOps, application modernization and customer experience. Founded in 2006, Grid Dynamics is headquartered in Silicon Valley with offices across the Americas, Europe, and India.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Hive', 'SCALA', 'Hadoop', 'Big Data', 'Spark']",2025-06-12 14:38:54
Azure Data Engineer,Infosys,5 - 9 years,Not Disclosed,"['Hyderabad', 'Pune', 'Bengaluru']","Job description\nWe are looking for Azure Data Engineer's resources having minimum 5 to 9 years of Experience.\n\nRole & responsibilities\nBlend of technical expertise with 5 to 9 year of experience, analytical problem-solving, and collaboration with cross-functional teams. Design and implement Azure data engineering solutions (Ingestion & Curation)\nCreate and maintain Azure data solutions including Azure SQL Database, Azure Data Lake, and Azure Blob Storage.\nDesign, implement, and maintain data pipelines for data ingestion, processing, and transformation in Azure.\nUtilizing Azure Data Factory or comparable technologies, create and maintain ETL (Extract, Transform, Load) operations\nUse Azure Data Factory and Databricks to assemble large, complex data sets\nImplementing data validation and cleansing procedures will ensure the quality, integrity, and dependability of the data.\nEnsure data quality / security and compliance.\nOptimize Azure SQL databases for efficient query performance.\nCollaborate with data engineers, and other stakeholders to understand requirements and translate them into scalable and reliable data platform architectures.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Databricks', 'Azure Data Factory', 'Azure Synapse', 'Pyspark', 'Azure Data Lake']",2025-06-12 14:38:56
Senior Engineer - Data Science,Sasken Technologies,2 - 5 years,Not Disclosed,['Bengaluru'],"Job Summary\nPerson at this position has gained significant work experience to be able to apply their knowledge effectively and deliver results. Person at this position is also able to demonstrate the ability to analyse and interpret complex problems and improve change or adapt existing methods to solve the problem.\nPerson at this position regularly interacts with interfacing groups / customer on technical issue clarification and resolves the issues. Also participates actively in important project/ work related activities and contributes towards identifying important issues and risks. Reaches out for guidance and advice to ensure high quality of deliverables.\nPerson at this position consistently seek opportunities to enhance their existing skills, acquire more complex skills and work towards enhancing their proficiency level in their field of specialisation.\nWorks under limited supervision of Team Lead/ Project Manager.\n\n\nRoles & Responsibilities\nResponsible for design, coding, testing, bug fixing, documentation and technical support in the assigned area. Responsible for on time delivery while adhering to quality and productivity goals. Responsible for adhering to guidelines and checklists for all deliverable reviews, sending status report to team lead and following relevant organizational processes. Responsible for customer collaboration and interactions and support to customer queries. Expected to enhance technical capabilities by attending trainings, self-study and periodic technical assessments. Expected to participate in technical initiatives related to project and organization and deliver training as per plan and quality.\n\n\nEducation and Experience Required\nEngineering graduate, MCA, etc Experience: 2-5 years\n\nCompetencies Description\nData Science TCB is applicable to one who\n1) Analyses data to arrive at patterns/Insights/models\n2) Come up with models based on the data to provide recommendations, predictive analytics etc\n3) Provides implementation of the models in R, Matlab etc\n4) Can understand and apply machine learning/AI techniques\nPlatforms-\nUnix\nTechnology Standard-\nNA\nTools-\nR, Matlab, Spark Machine Learning, Python-ML, SPSS, SAS\nLanguages-\nR, Perl, Python, Scala\nSpecialization-\nCOGNITIVE ANALYTICS INCLUDING COMPUTER VISION, AI and ML, STATISTICS.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'R', 'SAS', 'Scala', 'Perl', 'SPSS', 'Spark', 'machine learning', 'Python']",2025-06-12 14:38:59
"4 To 8 years of exp. as a Data Analyst @ Banglore, Hyderabad , Chennai",A Client of Career Focus Consultancy,4 - 8 years,5-10 Lacs P.A.,"['Hyderabad', 'Chennai', 'Bengaluru']","Strong proficiency in Advanced SQL with experience in writing optimized queries for large datasets.\nMandatory skill : Data Analyst, Python ,SQL, Power BI\n\n\nExposure in, including predictive modeling and machine learning techniques.\n\nRequired Candidate profile\nHands-on experience with Python, R, or similar analytical tools is a plus.\nFamiliarity with cloud platforms such as AWS, Azure, or GCP for data processing and analytics.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['R', 'Power BI', 'Data Analyst', 'Python', 'SQL', 'Azure', 'GCP', 'AWS']",2025-06-12 14:39:01
Azure Data Engineer,Wipro,5 - 8 years,Not Disclosed,['Bengaluru'],"Role Purpose\nThe purpose of the role is to support process delivery by ensuring daily performance of the Production Specialists, resolve technical escalations and develop technical capability within the Production Specialists.\n\nDo\nOversee and support process by reviewing daily transactions on performance parameters\nReview performance dashboard and the scores for the team\nSupport the team in improving performance parameters by providing technical support and process guidance\nRecord, track, and document all queries received, problem-solving steps taken and total successful and unsuccessful resolutions\nEnsure standard processes and procedures are followed to resolve all client queries\nResolve client queries as per the SLAs defined in the contract\nDevelop understanding of process/ product for the team members to facilitate better client interaction and troubleshooting\nDocument and analyze call logs to spot most occurring trends to prevent future problems\nIdentify red flags and escalate serious client issues to Team leader in cases of untimely resolution\nEnsure all product information and disclosures are given to clients before and after the call/email requests\nAvoids legal challenges by monitoring compliance with service agreements\n\nHandle technical escalations through effective diagnosis and troubleshooting of client queries\nManage and resolve technical roadblocks/ escalations as per SLA and quality requirements\nIf unable to resolve the issues, timely escalate the issues to TA & SES\nProvide product support and resolution to clients by performing a question diagnosis while guiding users through step-by-step solutions\nTroubleshoot all client queries in a user-friendly, courteous and professional manner\nOffer alternative solutions to clients (where appropriate) with the objective of retaining customers and clients business\nOrganize ideas and effectively communicate oral messages appropriate to listeners and situations\nFollow up and make scheduled call backs to customers to record feedback and ensure compliance to contract SLAs\nBuild people capability to ensure operational excellence and maintain superior customer service levels of the existing account/client\nMentor and guide Production Specialists on improving technical knowledge\nCollate trainings to be conducted as triage to bridge the skill gaps identified through interviews with the Production Specialist\nDevelop and conduct trainings (Triages) within products for production specialist as per target\nInform client about the triages being conducted\nUndertake product trainings to stay current with product features, changes and updates\nEnroll in product specific and any other trainings per client requirements/recommendations\nIdentify and document most common problems and recommend appropriate resolutions to the team\nUpdate job knowledge by participating in self learning opportunities and maintaining personal networks\n\nDeliver\n\nNoPerformance ParameterMeasure1ProcessNo. of cases resolved per day, compliance to process and quality standards, meeting process level SLAs, Pulse score, Customer feedback, NSAT/ ESAT2Team ManagementProductivity, efficiency, absenteeism3Capability developmentTriages completed, Technical Test performance\nMandatory Skills: Azure Data Factory. Experience: 5-8 Years.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure', 'azure databricks', 'azure data lake', 'ssas', 'ssrs', 'microsoft azure', 'azure data factory', 'ssis', 'msbi', 'sql server', 'sql']",2025-06-12 14:39:03
Data Engineer III,Expedia Group,5 - 10 years,Not Disclosed,['Bengaluru'],"Why Join Us?\nTo shape the future of travel, people must come first. Guided by our Values and Leadership Agreements, we foster an open culture where everyone belongs, differences are celebrated and know that when one of us wins, we all win.\nWe provide a full benefits package, including exciting travel perks, generous time-off, parental leave, a flexible work model (with some pretty cool offices), and career development resources, all to fuel our employees passion for travel and ensure a rewarding career journey. We re building a more open world. Join us.\nData Engineer III\nIntroduction to the Team\nExpedia Technology teams partner with our Product teams to create innovative products, services, and tools to deliver high-quality experiences for travelers, partners, and our employees. A singular technology platform powered by data and machine learning provides secure, differentiated, and personalized experiences that drive loyalty and traveler satisfaction.\nExpedia Group is seeking a skilled and motivated Data Engineer III to join our Finance Business Intelligence team supporting the Product & Technology Finance organization. In this role, you will help drive data infrastructure and analytics solutions that support strategic financial planning, reporting, and operational decision-making across the Global Finance community. You ll work closely with Finance and Technology partners to ensure data accuracy, accessibility, and usability in support of Expedia s business objectives.\nAs a Data Engineer III, you have strong experience working with a variety of datasets, data environments, tools, and analytical techniques. You enjoy a fun, collaborative and stimulating team environment. Successful candidates should be able to own projects end-to-end, including identifying problems and solutions, building and maintain data pipelines and dashboards, distilling key insights and communicate to stakeholders.\nIn this role, you will:\nDevelop new and improve existing end to end Business Intelligence products (data pipelines, Tableau dashboards, and Machine Learning predictive forecasting models).\nDrive internal efficiencies through streamline code/documentation/Tableau development to maintain high data integrity.\nTroubleshoot and resolve production issues with the team products (automation opportunities, optimizations, back-end data issues, data reconciliations).\nProactively reach out to subject matter experts /stakeholders and collaborate to solve problems.\nRespond to ad hoc data requests and conduct analysis to provide valuable insights to stakeholders.\nCollaborate and coordinate with team members/stakeholders to translate complex data into meaningful insights, that improve the analytical capabilities of the business.\nApply knowledge of database design to support migration of data pipelines from on prem to cloud environment (including data extraction, ingestion, processing of large data sets)\nSupport dashboard development on cloud environment to enable self-service reporting.\nCommunicate clearly on current work status and design considerations\nThink broadly and comprehend the how, why, and what behind data architecture designs\nExperience & Qualifications:\nBachelor s in Computer Science, Mathematics, Statistics, Information Systems, or related field\n5+ years experience in a Data Analyst, Data Engineer or Business Analyst role\nProven expertise in SQL, with practical experience utilizing query engines including SQL Server, Starburst, Trino, Querybook and data science tools such as Python/R, SparkSQL.\nProficient visualization skills (Tableau, Looker, or similar) and excel modeling/report automation.\nExceptional understanding of relational and dimensional datasets, data warehouse and data mining and applies database design principles to solve data requirements\nExperience building robust data extract, load and transform (ELT) processes, that source data from multiple databases.\nDemonstrated record of defining and executing key analysis and solving problems with minimal supervision.\nDynamic individual contributor who consistently enhances operational playbooks to address business problems.\n3+ year working in a hybrid environment that uses both on-premise and cloud technologies is preferred.\nExperience working in an environment that manipulates large datasets on the cloud platform preferred.\nBackground in analytics, finance or a comparable reporting and analytics role preferred.\nAccommodation requests\nIf you need assistance with any part of the application or recruiting process due to a disability, or other physical or mental health conditions, please reach out to our Recruiting Accommodations Team through the Accommodation Request .\nWe are proud to be named as a Best Place to Work on Glassdoor in 2024 and be recognized for award-winning culture by organizations like Forbes, TIME, Disability:IN, and others.\nExpedia Groups family of brands includes: Brand Expedia , Hotels.com , Expedia Partner Solutions, Vrbo , trivago , Orbitz , Travelocity , Hotwire , Wotif , ebookers , CheapTickets , Expedia Group Media Solutions, Expedia Local Expert , CarRentals.com , and Expedia Cruises . 2024 Expedia, Inc. All rights reserved. Trademarks and logos are the property of their respective owners. . Never provide sensitive, personal information to someone unless you re confident who the recipient is. Expedia Group does not extend job offers via email or any other messaging tools to individuals with whom we have not made prior contact. Our email domain is @expediagroup.com. The official website to find and apply for job openings at Expedia Group is careers.expediagroup.com/jobs .\nExpedia is committed to creating an inclusive work environment with a diverse workforce. All qualified applicants will receive consideration for employment without regard to race, religion, gender, sexual orientation, national origin, disability or age.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Automation', 'Database design', 'Machine learning', 'Business intelligence', 'Data mining', 'Analytics', 'SQL', 'Python', 'Data architecture']",2025-06-12 14:39:05
AWS Data Engineer,Infosys,5 - 9 years,Not Disclosed,"['Hyderabad', 'Pune', 'Bengaluru']","AWS Data Engineer\n\nTo Apply, use the below link:\nhttps://career.infosys.com/jobdesc?jobReferenceCode=INFSYS-EXTERNAL-210775&rc=0\n\nJOB Profile:\nSignificant 5 to 9 years of experience in designing and implementing scalable data engineering solutions on AWS.\nStrong proficiency in Python programming language.\nExpertise in serverless architecture and AWS services such as Lambda, Glue, Redshift, Kinesis, SNS, SQS, and CloudFormation.\nExperience with Infrastructure as Code (IaC) using AWS CDK for defining and provisioning AWS resources.\nProven leadership skills with the ability to mentor and guide junior team members.\nExcellent understanding of data modeling concepts and experience with tools like ERStudio.\nStrong communication and collaboration skills, with the ability to work effectively in a cross-functional team environment.\nExperience with Apache Airflow for orchestrating data pipelines is a plus.\nKnowledge of Data Lakehouse, dbt, or Apache Hudi data format is a plus.\n\n\nRoles and Responsibilities\nDesign, develop, test, deploy, and maintain large-scale data pipelines using AWS services such as S3, Glue, Lambda, Redshift.\nCollaborate with cross-functional teams to gather requirements and design solutions that meet business needs.\nDesired Candidate Profile\n5-9 years of experience in an IT industry setting with expertise in Python programming language (Pyspark).\nStrong understanding of AWS ecosystem including S3, Glue, Lambda, Redshift.\nBachelor's degree in Any Specialization (B.Tech/B.E.).",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Aws Glue', 'AWS Data Engineer', 'Pyspark', 'Aws Lambda', 'Redshift Aws', 'Python']",2025-06-12 14:39:08
Azure Data Engineer,Hexaware Technologies,6 - 10 years,Not Disclosed,"['Chennai', 'Bengaluru']","Hi\n\nWork Location : Chennai AND Bangalore\nWork location : Imm - 30 days\n\nPrimary: Azure Databricks,ADF, Pyspark SQL",,,,"['Pyspark', 'Azure Databricks', 'SQL', 'Azure Data Factory']",2025-06-12 14:39:11
MDM Data Analyst / Steward Lead,Gallagher Service Center (GSC),3 - 7 years,Not Disclosed,['Bengaluru'],"Role & responsibilities\n\nThe MDM Analyst / Data Steward works closely with business stakeholders to understand and gather data requirements, develop data models and database designs, and define and implement data standards, policies, and procedures. This role also implements any rules inside of the MDM tool to improve the data, performs deduplication projects to develop golden records, and overall works towards improving the quality of data in the domain assigned.\n\nRequired skills :\nTechnical Skills: Proficiency in MDM tools and technologies such as Informatica MDM, CluedIn, or similar platforms is essential. Familiarity with data modeling, data integration, and data quality control techniques is also important. Experience with data governance platforms like Collibra and Alation can be beneficial1.\nAnalytical Skills: Strong analytical and problem-solving skills are crucial for interpreting and working with large volumes of data. The ability to translate complex business requirements into practical MDM solutions is also necessary.\nData Management: Experience in designing, implementing, and maintaining master data management systems and solutions. This includes conducting data cleansing, data auditing, and data validation activities.\nCommunication and Collaboration: Excellent communication and interpersonal skills to effectively collaborate with business stakeholders, IT teams, and other departments.\nData Governance: In-depth knowledge of data governance, data quality, and data integration principles. The ability to develop and implement data management processes and policies is essential.\nEducational Background: A Bachelor's or Master's degree in Computer Science, Information Systems, Data Science, or a related field is typically required1.\nCertifications: Certification in the MDM domain (e.g., Certified MDM Professional) can be a plus\n\nKey Skills:\nBecome the expert at the assigned domain of data\nUnderstand all source systems feeding into the MDM\nWrite documentation of stewardship for the domain\nDevelop rules and standards for the domain of data\nGenerate measures of improvement to demonstrate to the business the quality of the data\n\nWe are seeking candidates who can join immediately or within a maximum of 30 days' notice.\nMinimum of 3+ years of relevant experience is required.\nCandidates who are willing to relocate to Bangalore or are already based in Bangalore.\nCandidates should be flexible with working UK/US shifts.",Industry Type: Analytics / KPO / Research,Department: Other,"Employment Type: Full Time, Permanent","['Informatica Mdm', 'Data Modeling', 'Data Integration']",2025-06-12 14:39:14
"Data Engineer - Snowflake, Azure Data Factory (ADF)",Suzva Software Technologies,0 - 1 years,Not Disclosed,['Mumbai'],"We are seeking an experienced Data Engineer to join our team for a 6-month contract assignment. The ideal candidate will work on data warehouse development, ETL pipelines, and analytics enablement using Snowflake, Azure Data Factory (ADF), dbt, and other tools.\n\nThis role requires strong hands-on experience with data integration platforms, documentation, and pipeline optimizationespecially in cloud environments such as Azure and AWS.\n\n#KeyResponsibilities\nBuild and maintain ETL pipelines using Fivetran, dbt, and Azure Data Factory\n\nMonitor and support production ETL jobs\n\nDevelop and maintain data lineage documentation for all systems\n\nDesign data mapping and documentation to aid QA/UAT testing\n\nEvaluate and recommend modern data integration tools\n\nOptimize shared data workflows and batch schedules\n\nCollaborate with Data Quality Analysts to ensure accuracy and integrity of data flows\n\nParticipate in performance tuning and improvement recommendations\n\nSupport BI/MDM initiatives including Data Vault and Data Lakes\n\n#RequiredSkills\n7+ years of experience in data engineering roles\n\nStrong command of SQL, with 5+ years of hands-on development\n\nDeep experience with Snowflake, Azure Data Factory, dbt\n\nStrong background with ETL tools (Informatica, Talend, ADF, dbt, etc.)\n\nBachelor's in CS, Engineering, Math, or related field\n\nExperience in healthcare domain (working with PHI/PII data)\n\nFamiliarity with scripting/programming (Python, Perl, Java, Linux-based environments)\n\nExcellent communication and documentation skills\n\nExperience with BI tools like Power BI, Cognos, etc.\n\nOrganized, self-starter with strong time-management and critical thinking abilities\n\n#NiceToHave\nExperience with Data Lakes and Data Vaults\n\nQA & UAT alignment with clear development documentation\nMulti-cloud experience (especially Azure, AWS)",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Java', 'Azure', 'Power BI', 'UAT', 'Perl', 'QA', 'Azure Data Factory', 'Linux', 'Cognos', 'Snowflake', 'ETL', 'AWS', 'Python']",2025-06-12 14:39:16
Lead Data Engineer,Conduent,8 - 13 years,Not Disclosed,['Noida'],"Job Overview \n\nWe are looking for a Data Engineer who will be part of our Analytics Practice and will be expected to actively work in a multi-disciplinary fast paced environment. This role requires a broad range of skills and the ability to step into different roles depending on the size and scope of the project; its primary responsibility is the acquisition, transformation, loading and processing of data from a multitude of disparate data sources, including structured and unstructured data for advanced analytics and machine learning in a big data environment.\n\n\n Responsibilities: \nEngineer a modern data pipeline to collect, organize, and process data from disparate sources.\nPerforms data management tasks, such as conduct data profiling, assess data quality, and write SQL queries to extract and integrate data\nDevelop efficient data collection systems and sound strategies for getting quality data from different sources\nConsume and analyze data from the data pool to support inference, prediction and recommendation of actionable insights to support business growth.\nDesign and develop ETL processes using tools and scripting. Troubleshoot and debug ETL processes. Performance tuning and opitimization of the ETL processes.\nProvide support to new of existing applications while recommending best practices and leading projects to implement new functionality.\nCollaborate in design reviews and code reviews to ensure standards are met. Recommend new standards for visualizations.\nLearn and develop new ETL techniques as required to keep up with the contemporary technologies.\nReviews the solution requirements and architecture to ensure selection of appropriate technology, efficient use of resources and integration of multiple systems and technology.\nSupport presentations to Customers and Partners\nAdvising on new technology trends and possible adoption to maintain competitive advantage\n\n\n Experience Needed: \n8+ years of related experience is required.\nA BS or Masters degree in Computer Science or related technical discipline is required\nETL experience with data integration to support data marts, extracts and reporting\nExperience connecting to varied data sources\nExcellent SQL coding experience with performance optimization for data queries.\nUnderstands different data models like normalized, de-normalied, stars, and snowflake models. Worked with transactional, temporarl, time series, and structured and unstructured data.\nExperience on Azure Data Factory and Azure Synapse Analytics\nWorked in big data environments, cloud data stores, different RDBMS and OLAP solutions.\nExperience in cloud-based ETL development processes.\nExperience in deployment and maintenance of ETL Jobs.\nIs familiar with the principles and practices involved in development and maintenance of software solutions and architectures and in service delivery.\nHas strong technical background and remains evergreen with technology and industry developments.\nAt least 3 years of demonstrated success in software engineering, release engineering, and/or configuration management.\nHighly skilled in scripting languages like PowerShell.\nSubstantial experience in the implementation and exectuion fo CI/CD processes.\n\n\n Additional  \nDemonstrated ability to have successfully completed multiple, complex technical projects\nPrior experience with application delivery using an Onshore/Offshore model\nExperience with business processes across multiple Master data domains in a services based company\nDemonstrates a rational and organized approach to the tasks undertaken and an awareness of the need to achieve quality.\nDemonstrates high standards of professional behavior in dealings with clients, colleagues and staff.\nIs able to make sound and far reaching decisions alone on major issues and to take full responsibility for them on a technical basis.\nStrong written communication skills. Is effective and persuasive in both written and oral communication.\nExperience with gathering end user requirements and writing technical documentation\nTime management and multitasking skills to effectively meet deadlines under time-to-market pressure",Industry Type: BPM / BPO,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sql coding', 'sql', 'configuration management', 'software engineering', 'release engineering', 'continuous integration', 'rdbms', 'sql queries', 'performance tuning', 'azure synapse', 'ci/cd', 'azure data factory', 'machine learning', 'data engineering', 'powershell', 'olap', 'etl', 'big data']",2025-06-12 14:39:19
Lead Data Engineer - Azure,Blend360 India,7 - 12 years,Not Disclosed,['Hyderabad'],"Job Description\nAs a Sr Data Engineer, your role is to spearhead the data engineering teams and elevate the team to the next level! You will be responsible for laying out the architecture of the new project as well as selecting the tech stack associated with it. You will plan out the development cycles deploying AGILE if possible and create the foundations for good data stewardship with our new data products!\nYou will also set up a solid code framework that needs to be built to purpose yet have enough flexibility to adapt to new business use cases tough but rewarding challenge!\n\nResponsibilities\nCollaborate with several stakeholders to deeply understand the needs of data practitioners to deliver at scale\nLead Data Engineers to define, build and maintain Data Platform\nWork on building Data Lake in Azure Fabric processing data from multiple sources\nMigrating existing data store from Azure Synapse to Azure Fabric\nImplement data governance and access control\nDrive development effort End-to-End for on-time delivery of high-quality solutions that conform to requirements, conform to the architectural vision, and comply with all applicable standards.\nPresent technical solutions, capabilities, considerations, and features in business terms. Effectively communicate status, issues, and risks in a precise and timely manner.\nFurther develop critical initiatives, such as Data Discovery, Data Lineage and Data Quality\nLeading team and Mentor junior resources\nHelp your team members grow in their role and achieve their career aspirations\nBuild data systems, pipelines, analytical tools and programs\nConduct complex data analysis and report on results\n\n\nQualifications\n7+ Years of Experience as a data engineer or similar role in Azure Synapses, ADF or\nrelevant exp in Azure Fabric\nDegree in Computer Science, Data Science, Mathematics, IT, or similar fiel",Industry Type: Advertising & Marketing,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Data analysis', 'Data modeling', 'Analytical', 'Agile', 'data governance', 'Data quality', 'Data mining', 'SQL', 'Python']",2025-06-12 14:39:21
Data Engineer,AMERICAN EXPRESS,2 - 4 years,13-17 Lacs P.A.,"['Gurugram', 'Delhi / NCR']","Role & responsibilities\nUnderstanding business use cases and be able to convert to technical design\nPart of a cross-disciplinary team, working closely with other data engineers, software engineers, data scientists, data managers and business partners.\nYou will be designing scalable, testable and maintainable data pipelines\nIdentify areas for data governance improvements and help to resolve data quality problems through the appropriate choice of error detection and correction, process control and improvement, or process design changes",,,,"['Spark', 'SQL', 'Python', 'Hadoop', 'Big Data']",2025-06-12 14:39:24
Data Engineer,AMERICAN EXPRESS,3 - 8 years,Not Disclosed,['Chennai'],"You Lead the Way. We've Got Your Back.\n\nWith the right backing, people and businesses have the power to progress in incredible ways. When you join Team Amex, you become part of a global and diverse community of colleagues with an unwavering commitment to back our customers, communities and each other. Here, youll learn and grow as we help you create a career journey thats unique and meaningful to you with benefits, programs, and flexibility that support you personally and professionally.\nAt American Express, you’ll be recognized for your contributions, leadership, and impact—every colleague has the opportunity to share in the company’s success. Together, we’ll win as a team, striving to uphold our company values and powerful backing promise to provide the world’s best customer experience every day. And we’ll do it with the utmost integrity, and in an environment where everyone is seen, heard and feels like they belong. As part of our diverse tech team, you can architect, code and ship software that makes us an essential part of our customers’ digital lives. Here, you can work alongside talented engineers in an open, supportive, inclusive environment where your voice is valued, and you make your own decisions on what tech to use to solve challenging problems. Amex offers a range of opportunities to work with the latest technologies and encourages you to back the broader engineering community through open source. And because we understand the importance of keeping your skills fresh and relevant, we give you dedicated time to invest in your professional development. Find your place in technology on #TeamAmex.",,,,"['Data Engineering', 'GCP', 'Airflow', 'Pyspark', 'Bigquery', 'Hadoop', 'Big Data', 'SQL', 'Python', 'Backend Development']",2025-06-12 14:39:26
Data Engineer,Bajaj Financial Securities,2 - 5 years,Not Disclosed,['Pune'],"We're Hiring: Data Engineer | 25 Years Experience | AWS + Real-time Focus\nJoin our fast-moving team as a Data Engineer where you'll build scalable, real-time data pipelines, own cloud infrastructure, and collaborate across teams to drive data-first decisions.\nIf you're strong in Python, experienced with streaming platforms like Kafka/Kinesis, and have shipped cloud-native data pipelines (preferably AWS) — we want to hear from you.\nMust-Haves:\n2–5 years of experience in Data Engineering\nPython (Pandas, PySpark, async), SQL, ETL/ELT\nStreaming experience (Kafka/Kinesis)\nAWS cloud stack (Glue, Lambda, S3, Athena)\nExperience in APIs, data warehousing, and data modelling\nBonus if you know: Docker, Kubernetes, Airflow/dbt, or have a background in MLOps.Role & responsibilities\n\n\nPreferred candidate profile",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Aws Lambda', 'Docker', 'Cloud Platform', 'Data Warehousing', 'Python', 'Pyspark', 'Api Gateway', 'Kinesis', 'Kafka', 'ETL', 'SQL', 'Kubernetes']",2025-06-12 14:39:29
Data loss prevention Analyst,Forvis Mazars,2 - 5 years,Not Disclosed,['Mumbai (All Areas)'],"Role Overview:\nWe are looking for dedicated professionals to join our DLP Operations Desk in Mumbai. The candidate should have practical experience in managing Data Loss Prevention (DLP) technologies and operational workflows.\nProduct Expertise:\nZscalar\nKey Responsibilities:\nMonitor and analyze DLP alerts and incidents as per established processes.\nInvestigate security incidents, coordinate with stakeholders, and drive closure.\nPrepare and share executive reports of DLP incidents/alerts on a defined schedule.\nContinuously fine-tune and optimize DLP policies based on operational insights, emerging threats, and best practices.\nStay updated with the latest trends in DLP and recommend enhancements to current policies.\n\nRequirements:\n2-5 years experience in security operations with a focus on DLP.\nPrior exposure to DLP tools and incident management processes.\nAnalytical mindset with strong documentation and reporting skills.\nAbility to research and apply best practices to policy management.\nExcellent communication and collaboration skills.",Industry Type: Miscellaneous,Department: Other,"Employment Type: Full Time, Permanent","['Dlp', 'Data Loss Prevention', 'Zscaler', 'Forcepoint']",2025-06-12 14:39:31
"Senior Manager- Middle and Back Office Data Analyst- ISS,",Fidelity International,10 - 15 years,Not Disclosed,"['Gurugram', 'Bengaluru']","Title: Middle and Back Office Data Analyst - ISS Data (Senior Manager)\nDepartment: Technology\nLocation: Bangalore & Gurgaon (hybrid / flexible working permitted)\nReports To: Middle and Back Office Data Product Owner\nLevel: Senior Manager\nWe re proud to have been helping our clients build better financial futures for over 50 years. How have we achieved this? By working together - and supporting each other - all over the world. So, join our [insert name of team/ business area] team and feel like you re part of something bigger.\nAbout your team\nThe Technology function provides IT services that are integral to running an efficient run-the business operating model and providing change-driven solutions to meet outcomes that deliver on our business strategy. These include the development and support of business applications that underpin our revenue, operational, compliance, finance, legal, marketing and customer service functions. The broader organisation incorporates Infrastructure services that the firm relies on to operate on a day-to-day basis including data centre, networks, proximity services, security, voice, incident management and remediation.\nThe ISS Technology group is responsible for providing Technology solutions to the Investment Solutions & Services (ISS) business (which covers Investment Management, Asset Management Operations & Distribution business units globally)\n\nThe ISS Technology team supports and enhances existing applications as well as designs, builds and procures new solutions to meet requirements and enable the evolving business strategy.\nAs part of this group, a dedicated ISS Data Programme team has been mobilised as a key foundational programme to support the execution of the overarching ISS strategy.\nAbout your role\nThe Middle and Back Office Data Analyst role is instrumental in the creation and execution of a future state design for Fund Servicing & Oversight data across Fidelity s key business areas. The successful candidate will have an in- depth knowledge of data domains that represent Middle and Back-office operations and technology.\nThe role will sit within the ISS Delivery Data Analysis chapter and fully aligned to deliver Fidelity s cross functional ISS Data Programme in Technology, and the candidate will leverage their extensive industry knowledge to build a future state platform in collaboration with Business Architecture, Data Architecture, and business stakeholders.\nThe role is to maintain strong relationships with the various business contacts to ensure a superior service to our clients.\nData Product - Requirements Definition and Delivery of Data Outcomes\nAnalysis of data product requirements to enable business outcomes, contributing to the data product roadmap\nCapture both functional and non-functional data requirements considering the data product and consumers perspectives.\nConduct workshops with both the business and tech stakeholders for requirements gathering, elicitation and walk throughs.\nResponsible for the definition of data requirements, epics and stories within the product backlog and providing analysis support throughout the SDLC.\nResponsible for supporting the UAT cycles, attaining business sign off on outcomes being delivered\nData Quality and Integrity:\nDefine data quality use cases for all the required data sets and contribute to the technical frameworks of data quality.\nAlign the functional solution with the best practice data architecture & engineering principles.\nCoordination and Communication:\nExcellent communication skills to influence technology and business stakeholders globally, attaining alignment and sign off on the requirements.\nCoordinate with internal and external stakeholders to communicate data product deliveries and the change impact to the operating model.\nAn advocate for the ISS Data Programme.\nCollaborate closely with Data Governance, Business Architecture, and Data owners etc.\nConduct workshops within the scrum teams and across business teams, effectively document the minutes and drive the actions.\nAbout you\nAt least 10 years of proven experience as a business/technical/data analyst within technology and/or business changes within the financial services /asset management industry.\nMinimum 5 years as a senior business/technical/data analyst adhering to agile methodology, delivering data solutions using industry leading data platforms such as Snowflake, State Street Alpha Data, Refinitiv Eikon, SimCorp Dimension, BlackRock Aladdin, FactSet etc.\nProven experience. of delivering data driven business outcomes using industry leading data platforms such as Snowflake.\nExcellent knowledge of data life cycle that drives Middle and Back Office capabilities such as trade execution, matching, confirmation, trade settlement, record keeping, accounting, fund & cash positions, custody, collaterals/margin movements, corporate actions , derivations and calculations such as holiday handling, portfolio turnover rates, funds of funds look through .\nIn Depth expertise in data and calculations across the investment industry covering the below.\nAsset-specific data: This includes data related to financial instruments reference data like asset specifications, maintenance records, usage history, and depreciation schedules.\nMarket data: This includes data like security prices, exchange rates, index constituents and licensing restrictions on them.\nABOR & IBOR data: This includes calculation engines covering input data sets, calculations and treatment of various instruments for ABOR and IBOR data leveraging platforms such as Simcorp, Neoxam, Invest1, Charles River, Aladdin etc. Knowledge of TPAs, how data can be structured in a unified way from heterogenous structures.\nShould possess Problem Solving, Attention to detail, Critical thinking.\nTechnical Skills: Excellent hands-on SQL, Advanced Excel, Python, ML (optional) and proven experience and knowledge of data solutions.\nKnowledge of data management, data governance, and data engineering practices\nHands on experience on data modelling techniques such as dimensional, data vault etc.\nWillingness to own and drive things, collaboration across business and tech stakeholders.\nFeel rewarded",Industry Type: Financial Services,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['IT services', 'Data analysis', 'Data management', 'Incident management', 'Scrum', 'Customer service', 'Asset management', 'SDLC', 'SQL', 'Python']",2025-06-12 14:39:33
HIH - Data Science Lead Analyst - Evernorth,ManipalCigna Health Insurance,5 - 8 years,Not Disclosed,['Hyderabad'],"Internal Title: Data Science Lead Analyst\nExternal Title: Data Science Lead Analyst\nRole Summary\nAs a member of the Data Science Center of Expertise (DSCOE), the DS Lead Analyst is responsible for leading and enabling Data Science within Cigna Group with demonstrable aptitude in Data Science (i) Technical Skills (ii) Leadership (iii) Scope & Impact (iv) Influence. Please see Qualifications section below for more details.\n\nThe role will support the development and maintenance of machine learning models, with a focus on ensuring that models meet Cigna s requirements for governance and legal compliance. The role will require collaboration with other data scientists and involve work across many lines of business.\nKey Responsibilities:\nAnalyze model performance of new models with specific regards to requirements for legal compliance and governance standards around accuracy and bias;\nPerform periodic analyses of performance of existing models to ensure continued compliance with internal and external standards for accuracy and bias;\nConduct research (i.e. literature review) to understand when bias may be biologically or medically justifiable, and to what degree, for example: finding evidence from literature that heart disease is more prevalent among older populations\nUsing machine learning development tools to mitigate model bias when this is determined to be necessary\nCollaborating with data scientists, business stakeholders, and governance/compliance teams to ensure models meet compliance and governance standards\nQualifications:\nBachelors or Masters/PhD (preferred) in statistics or computer science or equivalent field with 5-8 years of relevant experience\nStrong proficiency in ML, statistics, python or R, SQL, version control (e.g., Git), health care data (e.g., claims, EHR)\nAbility to promote best coding practices, championing a culture of documentation/logging\nThorough understanding of ML lifecycle, including necessary tradeoffs and associated risks\nLeadership in Data Science\nCan own a project end-to-end e.g., scoping, business value estimation, ideation, dev, prod, timeline\nCollaborates and guides junior team members in completion of projects and career development\nWorks cross functionally with technical (e.g., Data Science, Data Engineering) and business (e.g., clinical, marketing, pricing, business analysts) to implement solutions with measurable value\nScope and Impact\nIndependently delivers clear and well-developed presentations for both technical and business audiences\nCreates data science specific project goals associated with project deliverables\nArticulates timeline changes, rationale, and goals to meet deadlines moving forward\nValues diversity, growth mindset, and improving health outcomes of our customers\n\nLevel of Influence\nCommunicate with stakeholders to identify opportunities and possible solutions based on business need\nDraft project charter, timeline, and features/stories\nInfluence matrix-partner leadership\nAbout Evernorth Health Services",Industry Type: Insurance,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Version control', 'Claims', 'data science', 'Legal compliance', 'Coding', 'Pharmacy', 'Machine learning', 'SQL', 'Python']",2025-06-12 14:39:35
MDM Associate Data Steward,Amgen Inc,0 - 3 years,Not Disclosed,['Hyderabad'],"ABOUT THE ROLE\n\nRole Description\n\nWe are seeking an MDM Associate Data Steward who will be responsible for ensuring the accuracy, completeness, and reliability of master data across critical business domains such as Customer, Product, Affiliations, and Payer. This role involves actively managing and curating master data through robust data stewardship processes, comprehensive data cataloging, and data governance frameworks utilizing Informatica or Reltio MDM platforms. Additionally, the incumbent will perform advanced data analysis, data validation, and data transformation tasks through SQL queries and Python scripts to enable informed, data-driven business decisions. The role emphasizes cross-functional collaboration with various teams, including Data Engineering, Commercial, Medical, Compliance, and IT, to align data management activities with organizational goals and compliance standards.\n\nRoles & Responsibilities\nResponsible for master data stewardship, ensuring data accuracy and integrity across key master data domains (e.g., Customer, Product, Affiliations).\nConduct advanced data profiling, cataloging, and reconciliation activities using Informatica or Reltio MDM platforms.\nManage the reconciliation of potential matches, ensuring accurate resolution of data discrepancies and preventing duplicate data entries.\nEffectively manage Data Change Request (DCR) processes, including reviewing, approving, and documenting data updates in compliance with established procedures and SLAs.\nExecute and optimize SQL queries for validation and analysis of master data.\nPerform basic Python for data transformation, quality checks, and automation.\nCollaborate effectively with cross-functional teams including Data Engineering, Commercial, Medical, Compliance, and IT to fulfill data requirements.\nSupport user acceptance testing (UAT) and system integration tests for MDM related system updates.\nImplement data governance processes ensuring compliance with enterprise standards, policies, and frameworks.\nDocument and maintain accurate SOPs, Data Catalogs, Playbooks, and SLAs.\nIdentify and implement process improvements to enhance data stewardship and analytic capabilities.\nPerform regular audits and monitoring to maintain high data quality and integrity.\nBasic Qualifications and Experience\nMasters degree with 1 - 3 years of experience in Business, Engineering, IT or related fieldOR\nBachelors degree with 2 - 5 years of experience in Business, Engineering, IT or related fieldOR\nDiploma with 6 - 8 years of experience in Business, Engineering, IT or related field\nFunctional\n\nSkills:\nMust-Have Skills:\nDirect experience in data stewardship, data profiling, and master data management.\nHands-on experience with Informatica or Reltio MDM platforms.\nProficiency in SQL for data analysis and querying.\nKnowledge of data cataloging techniques and tools.\nBasic proficiency in Python scripting for data processing.\nGood-to-Have\n\nSkills:\nExperience with PySpark and Databricks for large-scale data processing.\nBackground in the pharmaceutical, healthcare, or life sciences industries.\nFamiliarity with AWS or other cloud-based data solutions.\nStrong project management and agile workflow familiarity (e.g., using Jira, Confluence).\nUnderstanding of regulatory compliance related to data protection (GDPR, CCPA).\nProfessional Certifications\nAny ETL certification ( e.g. Informatica)\nAny Data Analysis certification (SQL)\nAny cloud certification (AWS or AZURE)\nSoft\n\nSkills:\nStrong analytical abilities to assess and improve master data processes and solutions.\nExcellent verbal and written communication skills, with the ability to convey complex data concepts clearly to technical and non-technical stakeholders.\nEffective problem-solving skills to address data-related issues and implement scalable solutions.\nAbility to work effectively with global, virtual teams",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data management', 'python', 'project management', 'data analysis', 'data stewardship', 'agile database', 'data processing', 'sql', 'data profiling']",2025-06-12 14:39:37
Big Data Engineer - Hadoop,Info Origin Technologies Pvt Ltd,3 - 7 years,Not Disclosed,"['Hyderabad', 'Gurugram']","Role: Hadoop Data Engineer\nLocation: Gurgaon / Hyderabad\nWork Mode: Hybrid\nEmployment Type: Full-Time\nInterview Mode: First Video then In Person\nJob Description\nJob Overview:\nWe are looking for experienced Data Engineers proficient in Hadoop, Hive, Python, SQL, and Pyspark/Spark to join our dynamic team. Candidates will be responsible for designing, developing, and maintaining scalable big data solutions.\nKey Responsibilities:\nDevelop and optimize data pipelines for large-scale data processing.\nWork with structured and unstructured datasets to derive actionable insights.\nCollaborate with cross-functional teams to enhance data-driven decision-making.\nEnsure the performance, scalability, and reliability of data architectures.\nImplement best practices for data security and governance.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Hive', 'Hadoop', 'Pyspark', 'Big Data', 'Python', 'SQL']",2025-06-12 14:39:39
Senior Associate - Data Science,Axtria,3 - 8 years,Not Disclosed,['Noida'],"Job Summary-\nData Scientist with good hands-on experience of 3+ years in developing state of the art and scalable Machine Learning models and their operationalization, leveraging off-the-shelf workbench production.\n\nJob Responsibilities-\n\n1. Hands on experience in Python data-science and math packages such as NumPy, Pandas, Sklearn, Seaborn, PyCaret, Matplotlib\n2. Proficiency in Python and common Machine Learning frameworks (TensorFlow, NLTK, Stanford NLP, PyTorch, Ling Pipe, Caffe, Keras, SparkML and OpenAI etc.)\n3. Experience of working in large teams and using collaboration tools like GIT, Jira and Confluence\n4. Good understanding of any of the cloud platform - AWS, Azure or GCP\n5. Understanding of Commercial Pharma landscape and Patient Data / Analytics would be a huge plus\n6. Should have an attitude of willingness to learn, accepting the challenging environment and confidence in delivering the results within timelines. Should be inclined towards self motivation and self-driven to find solutions for problems.\n7. Should be able to mentor and guide mid to large sized teams under him/her\n\nJob -\n1. Strong experience on Spark with Scala/Python/Java\n2. Strong proficiency in building/training/evaluating state of the art machine learning models and its deployment\n3. Proficiency in Statistical and Probabilistic methods such as SVM, Decision-Trees, Bagging and Boosting Techniques, Clustering\n4. Proficiency in Core NLP techniques like Text Classification, Named Entity Recognition (NER), Topic Modeling, Sentiment Analysis, etc. Understanding of Generative AI / Large Language Models / Transformers would be a plus",Industry Type: Analytics / KPO / Research,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['scala', 'java', 'spark', 'machine learning algorithms', 'python', 'confluence', 'scikit-learn', 'nltk', 'training', 'numpy', 'tensorflow', 'git', 'seaborn', 'gcp', 'pytorch', 'keras', 'spark mllib', 'jira', 'sentiment analysis', 'lingpipe', 'caffe', 'microsoft azure', 'pandas', 'matplotlib', 'aws', 'statistics']",2025-06-12 14:39:42
Data Analyst -Python,Sopra Steria,4 - 8 years,Not Disclosed,['Chennai'],"Experience working in large Software Development Teams\nKnowledge and experience in Agile Delivery mechanisms\nWork with business stakeholders, SCRUM masters, Designers and testers in SCRUM team.\nProficient in English language with ability to lead stakeholder conversations.\nExperience in generating insights through data and articulating stories addressing business problems.\nTotal Experience Expected: 6-8 years\nMandatory",,,,"['Data analysis', 'tableau', 'Agile', 'Scrum', 'Data Analyst', 'Windows', 'data visualization', 'SQL', 'Python']",2025-06-12 14:39:44
Senior Associate - Data Science,Axtria,2 - 5 years,Not Disclosed,['Noida'],"Be an Individual Contributor in the Analytics and Development team and solve real-world problems using cutting-edge capabilities and emerging technologies based on LLM/GenAI/GPT\nSoftware development experience in python is needed as backend for UI based applications\nBe a part of large delivery teams working on advanced projects when expert assistance is required.\nDeliver advanced Data Science capabilities to businesses in a meaningful manner through successful proof-of-concept solutions, and later smoothly transition the proof-of-concept into production.\nCreate Technical documents, develop, test, and deploy data analytics processes using Python, SQL on Azure/AWS platforms\nCan interact with client on GenAI related capabilities and use cases\n\n\nMust have\n\nSkills:\n\n\nMinimum of 3-5years develop, test, and deploy Python based applications on Azure/AWS platforms\nMust have basic knowledge on concepts of Generative AI / LLMs / GPT\nDeep understanding of architecture and work experience on Web Technologies\nPython, SQL hands-on experience\nExpertise in any popular python web frameworks e.g. flask, Django etc.\nFamiliarity with frontend technologies like HTML, JavaScript, REACT",Industry Type: Analytics / KPO / Research,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'software testing', 'gpm', 'microsoft azure', 'python web framework', 'data analytics', 'neural networks', 'aws stack', 'machine learning', 'javascript', 'artificial intelligence', 'sql', 'react.js', 'deep learning', 'django', 'data science', 'html', 'flask', 'aws']",2025-06-12 14:39:46
"Sr. Data Analyst – Tableau, SQL, Snowflake",Int9 Solutions,5 - 7 years,Not Disclosed,['Bengaluru'],"We are looking for a skilled Data Analyst with excellent communication skills and deep expertise in SQL, Tableau, and modern data warehousing technologies. This role involves designing data models, building insightful dashboards, ensuring data quality, and extracting meaningful insights from large datasets to support strategic business decisions.\n\nKey Responsibilities:\nWrite advanced SQL queries to retrieve and manipulate data from cloud data warehouses such as Snowflake, Redshift, or BigQuery.\nDesign and develop data models that support analytics and reporting needs.\nBuild dynamic, interactive dashboards and reports using tools like Tableau, Looker, or Domo.\nPerform advanced analytics techniques including cohort analysis, time series analysis, scenario analysis, and predictive analytics.\nValidate data accuracy and perform thorough data QA to ensure high-quality output.\nInvestigate and troubleshoot data issues; perform root cause analysis in collaboration with BI or data engineering teams.\nCommunicate analytical insights clearly and effectively to stakeholders.\n\nRequired Skills & Qualifications:\nExcellent communication skills are mandatory for this role.\n5+ years of experience in data analytics, BI analytics, or BI engineering roles.\nExpert-level skills in SQL, with experience writing complex queries and building views.\nProven experience using data visualization tools like Tableau, Looker, or Domo.\nStrong understanding of data modeling principles and best practices.\nHands-on experience working with cloud data warehouses such as Snowflake, Redshift, BigQuery, SQL Server, or Oracle.\nIntermediate-level proficiency with spreadsheet tools like Excel, Google Sheets, or Power BI, including functions, pivots, and lookups.\nBachelor's or advanced degree in a relevant field such as Data Science, Computer Science, Statistics, Mathematics, or Information Systems.\nAbility to collaborate with cross-functional teams, including BI engineers, to optimize reporting solutions.\nExperience in handling large-scale enterprise data environments.\nFamiliarity with data governance, data cataloging, and metadata management tools (a plus but not required).",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Snowflake', 'Tableau', 'Data Warehousing', 'Data Analytics', 'SQL', 'Scenario Analysis', 'Cohort Analysis', 'Data Modeling', 'Predictive Analysis', 'Redshift']",2025-06-12 14:39:49
Lead AWS Glue Data Engineer,Allegis Group,8 - 13 years,Not Disclosed,[],"Lead AWS Glue Data Engineer\nJob Location : Hyderabad / Bangalore / Chennai / Noida/ Gurgaon / Pune / Indore / Mumbai/ Kolkata\n\nWe are seeking a skilled Lead AWS Data Engineer with 8+ years of strong programming and SQL skills to join our team. The ideal candidate will have hands-on experience with AWS Data Analytics services and a basic understanding of general AWS services. Additionally, prior experience with Oracle and Postgres databases and secondary skills in Python and Azure DevOps will be an advantage.\n\nKey Responsibilities:\nDesign, develop, and optimize data pipelines using AWS Data Analytics services such as RDS, DMS, Glue, Lambda, Redshift, and Athena.\nImplement data migration and transformation processes using AWS DMS and Glue.\nWork with SQL (Oracle & Postgres) to query, manipulate, and analyse large datasets.\nDevelop and maintain ETL/ELT workflows for data ingestion and transformation.\nUtilize AWS services like S3, IAM, CloudWatch, and VPC to ensure secure and efficient data operations.\nWrite clean and efficient Python scripts for automation and data processing.\nCollaborate with DevOps teams using Azure DevOps for CI/CD pipelines and infrastructure management.\nMonitor and troubleshoot data workflows to ensure high availability and performance.\n\nPreferred Qualifications:\nAWS certifications in Data Analytics, Solutions Architect, or DevOps.\nExperience with data warehousing concepts and data lake implementations.\nHands-on experience with Infrastructure as Code (IaC) tools like Terraform or CloudFormation.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['RDS', 'Glue', 'DMS', 'Lambda', 'Redshift', 'Athena']",2025-06-12 14:39:51
"Sr. Data Analyst – Tableau, SQL, Snowflake",Int9 Solutions,5 - 10 years,Not Disclosed,"['Mumbai', 'Delhi / NCR', 'Bengaluru']","We are looking for a skilled Data Analyst with excellent communication skills and deep expertise in SQL, Tableau, and modern data warehousing technologies. This role involves designing data models, building insightful dashboards, ensuring data quality, and extracting meaningful insights from large datasets to support strategic business decisions.\n\nKey Responsibilities:\nWrite advanced SQL queries to retrieve and manipulate data from cloud data warehouses such as Snowflake, Redshift, or BigQuery.\nDesign and develop data models that support analytics and reporting needs.\nBuild dynamic, interactive dashboards and reports using tools like Tableau, Looker, or Domo.\nPerform advanced analytics techniques including cohort analysis, time series analysis, scenario analysis, and predictive analytics.\nValidate data accuracy and perform thorough data QA to ensure high-quality output.\nInvestigate and troubleshoot data issues; perform root cause analysis in collaboration with BI or data engineering teams.\nCommunicate analytical insights clearly and effectively to stakeholders.\n\nRequired Skills & Qualifications:\nExcellent communication skills are mandatory for this role.\n5+ years of experience in data analytics, BI analytics, or BI engineering roles.\nExpert-level skills in SQL, with experience writing complex queries and building views.\nProven experience using data visualization tools like Tableau, Looker, or Domo.\nStrong understanding of data modeling principles and best practices.\nHands-on experience working with cloud data warehouses such as Snowflake, Redshift, BigQuery, SQL Server, or Oracle.\nIntermediate-level proficiency with spreadsheet tools like Excel, Google Sheets, or Power BI, including functions, pivots, and lookups.\nBachelor's or advanced degree in a relevant field such as Data Science, Computer Science, Statistics, Mathematics, or Information Systems.\nAbility to collaborate with cross-functional teams, including BI engineers, to optimize reporting solutions.\nExperience in handling large-scale enterprise data environments.\nFamiliarity with data governance, data cataloging, and metadata management tools (a plus but not required).\nLocation : - Mumbai, Delhi / NCR, Bengaluru , Kolkata, Chennai, Hyderabad, Ahmedabad, Pune, Remote",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Snowflake', 'Tableau', 'SQL', 'BI Tools', 'Scenario Analysis', 'Cohort Analysis', 'Data Warehousing', 'SQL Server', 'Data Modeling', 'Data Analytics', 'Predictive Analysis', 'Redshift']",2025-06-12 14:39:54
"Lead Engineer, Data Engineering (J2EE/Angular/React/React Full Stack)",S&P Global Market Intelligence,10 - 15 years,Not Disclosed,"['Mumbai', 'Maharastra']","About the Role:\nGrade Level (for internal use): 11\nThe Team\nYou will be an expert contributor and part of the Rating Organizations Data Services Product Engineering Team. This team, who has a broad and expert knowledge on Ratings organizations critical data domains, technology stacks and architectural patterns, fosters knowledge sharing and collaboration that results in a unified strategy. All Data Services team members provide leadership, innovation, timely delivery, and the ability to articulate business value. Be a part of a unique opportunity to build and evolve S&P Ratings next gen analytics platform.\nResponsibilities:\nArchitect, design, and implement innovative software solutions to enhance S&P Ratings' cloud-based analytics platform.\nMentor a team of engineers (as required), fostering a culture of trust, continuous growth, and collaborative problem-solving.\nCollaborate with business partners to understand requirements, ensuring technical solutions align with business goals.\nManage and improve existing software solutions, ensuring high performance and scalability.\nParticipate actively in all Agile scrum ceremonies, contributing to the continuous improvement of team processes.\nProduce comprehensive technical design documents and conduct technical walkthroughs.\nExperience & Qualifications:\nBachelors degree in computer science, Information Systems, Engineering, equivalent or more is required\nProficient with software development lifecycle (SDLC) methodologies like Agile, Test-driven development\n10+ years of experience with 4+ years designing/developing enterprise products, modern tech stacks and data platforms\n4+ years of hands-on experience contributing to application architecture & designs, proven software/enterprise integration design patterns and full-stack knowledge including modern distributed front end and back-end technology stacks\n5+ years full stack development experience in modern web development technologies, Java/J2EE, UI frameworks like Angular, React, SQL, Oracle, NoSQL Databases like MongoDB\nExperience designing transactional/data warehouse/data lake and data integrations with Big data eco system leveraging AWS cloud technologies\nThorough understanding of distributed computing\nPassionate, smart, and articulate developer\nQuality first mindset with a strong background and experience with developing products for a global audience at scale\nExcellent analytical thinking, interpersonal, oral and written communication skills with strong ability to influence both IT and business partners\nSuperior knowledge of system architecture, object-oriented design, and design patterns.\nGood work ethic, self-starter, and results-oriented\nExcellent communication skills are essential, with strong verbal and writing proficiencies\nExp. with Delta Lake systems like Databricks using AWS cloud technologies and PySpark is a plus\nAdditional Preferred Qualifications:\nExperience working AWS\nExperience with SAFe Agile Framework\nBachelor's/PG degree in Computer Science, Information Systems or equivalent.\nHands-on experience contributing to application architecture & designs, proven software/enterprise integration design principles\nAbility to prioritize and manage work to critical project timelines in a fast-paced environment\nExcellent Analytical and communication skills are essential, with strong verbal and writing proficiencies\nAbility to train and mentor",Industry Type: Banking,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'AWS cloud technologies', 'PySpark', 'J2EE', 'React Full Stack', 'Databricks', 'React', 'Angular']",2025-06-12 14:39:56
Data Engineer,Grid Dynamics,4 - 9 years,Not Disclosed,['Bengaluru'],"Required Qualifications:\n4+ years of professional experience in data engineering and data analysis roles.\nStrong proficiency in SQL and experience with database management systems such as MySQL, PostgreSQL, Oracle, and MongoDB.\nHands-on experience with big data tools like Hadoop and Apache Spark.\nProficient in Python programming.\nExperience with data visualization tools such as Tableau, Power BI, and Jupyter Notebooks.\nProven ability to design, build, and maintain scalable ETL pipelines using tools like Apache Airflow, DBT, Composer (GCP), Control-M, Cron, and Luigi.\nFamiliarity with data engineering tools including Hive, Kafka, Informatica, Talend, SSIS, and Dataflow.\nExperience working with cloud data warehouses and services (Snowflake, Redshift, BigQuery, AWS Glue, GCP Dataflow, Azure Data Factory).\nUnderstanding of data modeling concepts and data lake/data warehouse architectures.\nExperience supporting CI/CD practices with Git, Docker, Terraform, and DevOps workflows.\nKnowledge of both relational and NoSQL databases, including PostgreSQL, BigQuery, MongoDB, and DynamoDB.\nExposure to Agile and DevOps methodologies.\nExperience with at least one cloud platform:\nGoogle Cloud Platform (BigQuery, Dataflow, Composer, Cloud Storage, Pub/Sub)\nAmazon Web Services (S3, Glue, Redshift, Lambda, Athena)\nMicrosoft Azure (Data Factory, Synapse Analytics, Blob Storage)\nEssential functions\nKey Responsibilities:\nDesign, develop, and maintain robust, scalable ETL pipelines using Apache Airflow, DBT, Composer (GCP), Control-M, Cron, Luigi, and similar tools.\nBuild and optimize data architectures including data lakes and data warehouses.\nIntegrate data from multiple sources ensuring data quality and consistency.\nCollaborate with data scientists, analysts, and stakeholders to translate business requirements into technical solutions.\nAnalyze complex datasets to identify trends, generate actionable insights, and support decision-making.\nDevelop and maintain dashboards and reports using Tableau, Power BI, and Jupyter Notebooks for visualization and pipeline validation.\nManage and optimize relational and NoSQL databases such as MySQL, PostgreSQL, Oracle, MongoDB, and DynamoDB.\nWork with big data tools and frameworks including Hadoop, Spark, Hive, Kafka, Informatica, Talend, SSIS, and Dataflow.\nUtilize cloud data services and warehouses like AWS Glue, GCP Dataflow, Azure Data Factory, Snowflake, Redshift, and BigQuery.\nSupport CI/CD pipelines and DevOps workflows using Git, Docker, Terraform, and related tools.\nEnsure data governance, security, and compliance standards are met.\nParticipate in Agile and DevOps processes to enhance data engineering workflows.\nQualifications\nData Engineer with experience in MySQL or SQL or PL/SQL and any cloud experience like GCP or AWS or Azure\nWould be a plus\nPreferred Skills:\nStrong problem-solving and communication skills.\nAbility to work independently and collaboratively in a team environment.\nExperience with service development, REST APIs, and automation testing is a plus.\nFamiliarity with version control systems and workflow automation.\nWe offer\nOpportunity to work on bleeding-edge projects\nWork with a highly motivated and dedicated team\nCompetitive salary\nFlexible schedule\nBenefits package - medical insurance, sports\nCorporate social events\nProfessional development opportunities\nWell-equipped office",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data analysis', 'Automation', 'Data modeling', 'MySQL', 'Workflow', 'Informatica', 'Oracle', 'Apache', 'SSIS', 'Analytics']",2025-06-12 14:39:59
"Senior Data Engineer (Snowflake, DBT)",Allegis Global Solutions (AGS),5 - 10 years,Not Disclosed,[],"Senior Data Engineer (Snowflake, DBT, Azure)\nJob Location: Hyderabad / Bangalore / Chennai / Kolkata / Noida/ Gurgaon / Pune / Indore / Mumbai\n\nJob Details\nTechnical Expertise:\nStrong proficiency in Snowflake architecture, including data sharing, partitioning, clustering, and materialized views.\nAdvanced experience with DBT for data transformations and workflow management.\nExpertise in Azure services, including Azure Data Factory, Azure Data Lake, Azure Synapse, and Azure Functions.\nData Engineering:\nProficiency in SQL, Python, or other relevant programming languages.\nStrong understanding of data modeling concepts, including star schema and normalization.\nHands-on experience with ETL/ELT pipelines and data integration tools.\n\nPreferred Qualifications:\nCertifications in Snowflake, DBT, or Azure Data Engineering.\nFamiliar with data visualization tools like Power BI or Tableau.\nKnowledge of CI/CD pipelines and DevOps practices for data workflows.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Snowflake', 'Data Build Tool', 'Azure']",2025-06-12 14:40:01
Senior Flexera Data Analyst,Luxoft,6 - 11 years,Not Disclosed,['Bengaluru'],"Internal Data Structures & Modeling\nDesign, maintain, and optimize internal data models and structures within the Flexera environment.\nMap business asset data to Flexeras normalized software models with precision and accuracy.\nEnsure accurate data classification, enrichment, and normalization to support software lifecycle tracking.\nPartner with infrastructure, operations, and IT teams to ingest and reconcile data from various internal systems.\nReporting & Analytics\nDesign and maintain reports and dashboards in Flexera or via external BI tools such as Power BI or Tableau.\nProvide analytical insights on software usage, compliance, licensing, optimization, and risk exposure.\nAutomate recurring reporting processes and ensure timely delivery to business stakeholders.\nWork closely with business users to gather requirements and translate them into meaningful reports and visualizations.\nAutomated Data Feeds & API Integrations\nDevelop and support automated data feeds using Flexera REST/SOAP APIs.\nIntegrate Flexera with enterprise tools (e.g., CMDB, SCCM, ServiceNow, ERP) to ensure reliable and consistent data flow.\nMonitor, troubleshoot, and resolve issues related to data extracts and API communication.\nImplement robust logging, alerting, and exception handling for integration pipelines.\nSkills\nMust have\nMinimum 6+ years of working with Flexera or similar software.\nFlexera Expertise: Strong hands-on experience with Flexera One, FlexNet Manager Suite, or similar tools.\nTechnical Skills:\nProficient in REST/SOAP API development and integration.\nStrong SQL skills and familiarity with data transformation/normalization concepts.\nExperience using reporting tools like Power BI, Tableau, or Excel for data visualization.\nFamiliarity with enterprise systems such as SCCM, ServiceNow, ERP, CMDBs, etc.\nProcess & Problem Solving:\nStrong analytical and troubleshooting skills for data inconsistencies and API failures.\nUnderstanding of license models, software contracts, and compliance requirements.\nNice to have\nSoft Skills: Excellent communication skills to translate technical data into business insights.\nOther\nLanguages\nEnglish: C1 Advanced\nSeniority\nSenior\nRefer a Friend\nPositive work environments and stellar reputations attract and retain top talent. Find out why Luxoft stands apart from the rest.\nRecommend a friend\nRelated jobs View all vacancies\nData Engineer with Neo4j\nData Science\nIndia\nChennai\nData Engineer with Neo4j\nData Science\nIndia\nGurugram\nBusiness Analyst\nData Science\nPoland\nRemote Poland\nBengaluru, India\nReq. VR-114544\nData Science\nBCM Industry\n23/05/2025\nReq. VR-114544\nApply for Senior Flexera Data Analyst in Bengaluru\n*",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['ERP', 'neo4j', 'Analytical', 'Data structures', 'data visualization', 'SCCM', 'Licensing', 'Analytics', 'Reporting tools', 'SQL']",2025-06-12 14:40:04
Senior Flexera Data Analyst,Luxoft,6 - 11 years,Not Disclosed,['Chennai'],"Internal Data Structures & Modeling\nDesign, maintain, and optimize internal data models and structures within the Flexera environment.\nMap business asset data to Flexeras normalized software models with precision and accuracy.\nEnsure accurate data classification, enrichment, and normalization to support software lifecycle tracking.\nPartner with infrastructure, operations, and IT teams to ingest and reconcile data from various internal systems.\nReporting & Analytics\nDesign and maintain reports and dashboards in Flexera or via external BI tools such as Power BI or Tableau.\nProvide analytical insights on software usage, compliance, licensing, optimization, and risk exposure.\nAutomate recurring reporting processes and ensure timely delivery to business stakeholders.\nWork closely with business users to gather requirements and translate them into meaningful reports and visualizations.\nAutomated Data Feeds & API Integrations\nDevelop and support automated data feeds using Flexera REST/SOAP APIs.\nIntegrate Flexera with enterprise tools (e.g., CMDB, SCCM, ServiceNow, ERP) to ensure reliable and consistent data flow.\nMonitor, troubleshoot, and resolve issues related to data extracts and API communication.\nImplement robust logging, alerting, and exception handling for integration pipelines.\nSkills\nMust have\nMinimum 6+ years of working with Flexera or similar software.\nFlexera Expertise: Strong hands-on experience with Flexera One, FlexNet Manager Suite, or similar tools.\nTechnical Skills:\nProficient in REST/SOAP API development and integration.\nStrong SQL skills and familiarity with data transformation/normalization concepts.\nExperience using reporting tools like Power BI, Tableau, or Excel for data visualization.\nFamiliarity with enterprise systems such as SCCM, ServiceNow, ERP, CMDBs, etc.\nProcess & Problem Solving:\nStrong analytical and troubleshooting skills for data inconsistencies and API failures.\nUnderstanding of license models, software contracts, and compliance requirements.\nNice to have\nSoft Skills: Excellent communication skills to translate technical data into business insights.\nOther\nLanguages\nEnglish: C1 Advanced\nSeniority\nSenior\nRefer a Friend\nPositive work environments and stellar reputations attract and retain top talent. Find out why Luxoft stands apart from the rest.\nRecommend a friend\nRelated jobs View all vacancies\nData Engineer with Neo4j\nData Science\nIndia\nGurugram\nData Engineer with Neo4j\nData Science\nIndia\nBengaluru\nData Scientist\nData Science\nIndia\nBengaluru\nChennai, India\nReq. VR-114544\nData Science\nBCM Industry\n23/05/2025\nReq. VR-114544\nApply for Senior Flexera Data Analyst in Chennai\n*",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['ERP', 'neo4j', 'Analytical', 'Data structures', 'data visualization', 'SCCM', 'Licensing', 'Analytics', 'Reporting tools', 'SQL']",2025-06-12 14:40:06
Senior Flexera Data Analyst,Luxoft,6 - 11 years,Not Disclosed,['Gurugram'],"Internal Data Structures & Modeling\nDesign, maintain, and optimize internal data models and structures within the Flexera environment.\nMap business asset data to Flexeras normalized software models with precision and accuracy.\nEnsure accurate data classification, enrichment, and normalization to support software lifecycle tracking.\nPartner with infrastructure, operations, and IT teams to ingest and reconcile data from various internal systems.\nReporting & Analytics\nDesign and maintain reports and dashboards in Flexera or via external BI tools such as Power BI or Tableau.\nProvide analytical insights on software usage, compliance, licensing, optimization, and risk exposure.\nAutomate recurring reporting processes and ensure timely delivery to business stakeholders.\nWork closely with business users to gather requirements and translate them into meaningful reports and visualizations.\nAutomated Data Feeds & API Integrations\nDevelop and support automated data feeds using Flexera REST/SOAP APIs.\nIntegrate Flexera with enterprise tools (e.g., CMDB, SCCM, ServiceNow, ERP) to ensure reliable and consistent data flow.\nMonitor, troubleshoot, and resolve issues related to data extracts and API communication.\nImplement robust logging, alerting, and exception handling for integration pipelines.\nSkills\nMust have\nMinimum 6+ years of working with Flexera or similar software.\nFlexera Expertise: Strong hands-on experience with Flexera One, FlexNet Manager Suite, or similar tools.\nTechnical Skills:\nProficient in REST/SOAP API development and integration.\nStrong SQL skills and familiarity with data transformation/normalization concepts.\nExperience using reporting tools like Power BI, Tableau, or Excel for data visualization.\nFamiliarity with enterprise systems such as SCCM, ServiceNow, ERP, CMDBs, etc.\nProcess & Problem Solving:\nStrong analytical and troubleshooting skills for data inconsistencies and API failures.\nUnderstanding of license models, software contracts, and compliance requirements.\nNice to have\nSoft Skills: Excellent communication skills to translate technical data into business insights.\nOther\nLanguages\nEnglish: C1 Advanced\nSeniority\nSenior\nRefer a Friend\nPositive work environments and stellar reputations attract and retain top talent. Find out why Luxoft stands apart from the rest.\nRecommend a friend\nRelated jobs View all vacancies\nData Engineer with Neo4j\nData Science\nIndia\nChennai\nData Engineer with Neo4j\nData Science\nIndia\nBengaluru\nData Scientist\nData Science\nIndia\nBengaluru\nGurugram, India\nReq. VR-114544\nData Science\nBCM Industry\n23/05/2025\nReq. VR-114544\nApply for Senior Flexera Data Analyst in Gurugram\n*",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['ERP', 'neo4j', 'Analytical', 'Data structures', 'data visualization', 'SCCM', 'Licensing', 'Analytics', 'Reporting tools', 'SQL']",2025-06-12 14:40:08
Big Data Developer/Data Engineer,Grid Dynamics,5 - 10 years,Not Disclosed,['Bengaluru'],"Role & responsibilities\nExperience: 5 - 8 years\nEmployment Type: Full-Time\n\nJob Summary:\nWe are looking for a highly skilled Scala and Spark Developer to join our data engineering team. The ideal candidate will have strong experience in building scalable data processing solutions using Apache Spark and writing robust, high-performance applications in Scala. You will work closely with data scientists, data analysts, and product teams to design, develop, and optimize large-scale data pipelines and ETL workflows.\n\nKey Responsibilities:\nDevelop and maintain scalable data processing pipelines using Apache Spark and Scala.\nWork on batch and real-time data processing using Spark (RDD/DataFrame/Dataset).\nWrite efficient and maintainable code following best practices and coding standards.\nCollaborate with cross-functional teams to understand data requirements and implement solutions.\nOptimize performance of Spark jobs and troubleshoot data-related issues.\nIntegrate data from multiple sources and ensure data quality and consistency.\nParticipate in design reviews, code reviews, and provide technical leadership when needed.\nContribute to data modeling, schema design, and architecture discussions.\nRequired Skills:\nStrong programming skills in Scala.\nExpertise in Apache Spark (Core, SQL, Streaming).\nHands-on experience with distributed computing and large-scale data processing.\nExperience with data formats like Parquet, Avro, ORC, and JSON.\nGood understanding of functional programming concepts.\nFamiliarity with data ingestion tools (Kafka, Flume, Sqoop, etc.).\nExperience working with Hadoop ecosystem (HDFS, Hive, YARN, etc.) is a plus.\nStrong SQL skills and experience working with relational and NoSQL databases.\nExperience with version control tools like Git.\nPreferred Qualifications:\nBachelor's or Masters degree in Computer Science, Engineering, or related field.\nExperience with cloud platforms like AWS, Azure, or GCP (especially EMR, Databricks, etc.).\nKnowledge of containerization (Docker, Kubernetes) is a plus.\nFamiliarity with CI/CD tools and DevOps practices.ndidate profile",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Scala', 'Pyspark', 'Spark']",2025-06-12 14:40:11
HIH - Data Science Lead Analyst - Evernorth,ManipalCigna Health Insurance,5 - 8 years,Not Disclosed,['Hyderabad'],"Internal Title: Data Science Lead Analyst\nExternal Title: Data Science Lead Analyst\nRole Summary\nAs a member of the Data Science Center of Expertise (DSCOE), the DS Lead Analyst is responsible for leading and enabling Data Science within Cigna Group with demonstrable aptitude in Data Science (i) Technical Skills (ii) Leadership (iii) Scope & Impact (iv) Influence. Please see Qualifications section below for more details.\nThe role will support the development and maintenance of proprietary advanced neural network ( AI ) foundation models in support of Cigna s business operations.\nKey Responsibilities:\nWrite code using PyTorch and/or Tensorflow to implement, test, and operationalize deep learning models\nCollaborate with data scientists and engineers to improve deep learning models and implement business-facing solutions built on top of those models\nTake responsibility for improving code performance and quality\nFollow developments in deep learning technology to identify opportunities to improve models\nQualifications:\nBachelors or Masters(preferred) in computer science or statistics or any other equivalent discipline with 5-8 years of relevant experience\nStrong proficiency in ML, statistics, python or R, SQL, version control (e.g., Git), health care data (e.g., claims, EHR), with emphasis on Tensorflow and Pytorch\nKnows and follows best coding and software engineering practices\nSuccessfully completes technical project components with limited guidance\nFamiliarity with deploying machine learning and predictive models to production and cloud environments\nLeadership in Data Science\nUnderstands how assigned work is related to purpose of the overall project\nIndependently identifies project roadblocks, and solutions\nSeeks to understand the health insurance domain\nScope and Impact\nDocuments the business considerations, methodology, process, code, and results associated with their work\nCollaborates to deliver clear and well developed presentations for both technical and business audiences\nConsistently communicates decisions, considerations, and needs for support\nReceives and responds to feedback in a professional and appropriate manner\nLevel of Influence\nPresent technical topics and results to non-technical stakeholders\nCommunicate and gather domain knowledge from non-technical stakeholders\nAbout Evernorth Health Services",Industry Type: Insurance,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Version control', 'GIT', 'Claims', 'Coding', 'Machine learning', 'Lead Analyst', 'SQL', 'Python', 'Business operations']",2025-06-12 14:40:13
Azure Data Engineer,HTC Global Services,4 - 8 years,Not Disclosed,['Bengaluru( Murugeshpalya )'],"Job Summary:\nWe are looking for a highly skilled Azure Data Engineer with experience in building and managing scalable data pipelines using Azure Data Factory, Synapse, and Databricks. The ideal candidate should be proficient in big data tools and Azure services, with strong programming knowledge and a solid understanding of data architecture and cloud platforms.\n\nKey Responsibilities:",,,,"['Power Bi', 'Azure Databricks', 'Azure Data Factory', 'Synapse', 'Python', 'Java', 'Scala', 'Kafka', 'big data tools', 'SQL', 'EventHub', 'Azure cloud services', 'Spark']",2025-06-12 14:40:15
Data Engineer,Infiniti Research,3 - 7 years,22.5-25 Lacs P.A.,['Bengaluru'],"Role & responsibilities\n3-6 years of experience in Data Engineering Pipeline Ownership and Quality Assurance, with hands-on expertise in building, testing, and maintaining data pipelines.\nProficiency with Azure Data Factory (ADF), Azure Databricks (ADB), and PySpark for data pipeline orchestration and processing large-scale datasets.\nStrong experience in writing SQL queries and performing data validation, data profiling, and schema checks.\nExperience with big data validation, including schema enforcement, data integrity checks, and automated anomaly detection.\nAbility to design, develop, and implement automated test cases to monitor and improve data pipeline efficiency.\nDeep understanding of Medallion Architecture (Raw, Bronze, Silver, Gold) for structured data flow management.\nHands-on experience with Apache Airflow for scheduling, monitoring, and managing workflows.\nStrong knowledge of Python for developing data quality scripts, test automation, and ETL validations.\nFamiliarity with CI/CD pipelines for deploying and automating data engineering workflows.\nSolid data governance and data security practices within the Azure ecosystem.\n\nAdditional Requirements:\nOwnership of data pipelines ensuring end-to-end execution, monitoring, and troubleshooting failures proactively.\nStrong stakeholder management skills, including follow-ups with business teams across multiple regions to gather requirements, address issues, and optimize processes.\nTime flexibility to align with global teams for efficient communication and collaboration.\nExcellent problem-solving skills with the ability to simulate and test edge cases in data processing environments.\nStrong communication skills to document and articulate pipeline issues, troubleshooting steps, and solutions effectively.\nExperience with Unity Catalog or willingness to learn.\n\nPreferred candidate profile\nImmediate Joiner's",Industry Type: Analytics / KPO / Research,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['ADF', 'pyspark', 'Unity Catalog', 'ADB', 'SQL', 'Medallion Architecture']",2025-06-12 14:40:18
Collibra Data Governance Engineer,Allegis Group,6 - 11 years,Not Disclosed,[],"Collibra Data Governance Engineer\nJob Location : Hyderabad / Bangalore / Chennai / Noida/ Gurgaon / Pune / Indore / Mumbai/ Kolkata\nRequired Skills\n5+ years of experience in data governance and/or metadata management.\nHands-on experience with Collibra Data Governance Center (Collibra DGC), including workflow configuration, cataloging, and operating model customization.\nStrong knowledge of metadata management, data lineage, and data quality principles.\nHands-on experience with Snowflake\nFamiliarity with data integration tools and AWS cloud platform\nExperience with SQL and working knowledge of relational databases.\nUnderstanding of data privacy regulations (e.g., GDPR, CCPA) and compliance frameworks.\nPreferred Skills\nCertifications such as Collibra Certified Solution Architect.\nExperience integrating Collibra with tools like Snowflake, Tableau or other BI/analytics platforms.\nExposure to DataOps, MDM (Master Data Management), and data governance frameworks like DAMA-DMBOK.\nStrong communication and stakeholder management skills.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Collibra', 'Metadata', 'Data Governance']",2025-06-12 14:40:20
Big Data Developer/ Senior Big Data Developer,Grid Dynamics,5 - 10 years,Not Disclosed,"['Hyderabad', 'Chennai', 'Bengaluru']","About us\nGrid Dynamics (NASDAQ: GDYN) is a leading provider of technology consulting, platform and product engineering, AI, and advanced analytics services. Fusing technical vision with business acumen, we solve the most pressing technical challenges and enable positive business outcomes for enterprise companies undergoing business transformation. A key differentiator for Grid Dynamics is our 8 years of experience and leadership in enterprise AI, supported by profound expertise and ongoing investment in data, analytics, cloud & DevOps, application modernization and customer experience. Founded in 2006, Grid Dynamics is headquartered in Silicon Valley with offices across the Americas, Europe, and India.\n\nRole & responsibilities\nWe are looking for an enthusiastic and technology-proficient Big Data Engineer, who is eager to participate in the design and implementation of a top-notch Big Data solution to be deployed at massive scale.\nOur customer is one of the world's largest technology companies based in Silicon Valley with operations all over the world. On this project we are working on the bleeding-edge of Big Data technology to develop high performance data analytics platform, which handles petabytes datasets.\nEssential functions\nParticipate in design and development of Big Data analytical applications.\nDesign, support and continuously enhance the project code base, continuous integration pipeline, etc.\nWrite complex ETL processes and frameworks for analytics and data management.\nImplement large-scale near real-time streaming data processing pipelines.\nWork inside the team of industry experts on the cutting edge Big Data technologies to develop solutions for deployment at massive scale.\nQualifications\nStrong coding experience with Scala, Spark,Hive, Hadoop.\nIn-depth knowledge of Hadoop and Spark, experience with data mining and stream processing technologies (Kafka, Spark Streaming, Akka Streams).\nUnderstanding of the best practices in data quality and quality engineering.\nExperience with version control systems, Git in particular.\nDesire and ability for quick learning of new tools and technologies.\nWould be a plus\nKnowledge of Unix-based operating systems (bash/ssh/ps/grep etc.).\nExperience with Github-based development processes.\nExperience with JVM build systems (SBT, Maven, Gradle).\nWe offer\nOpportunity to work on bleeding-edge projects\nWork with a highly motivated and dedicated team\nCompetitive salary\nFlexible schedule\nBenefits package - medical insurance, sports\nCorporate social events\nProfessional development opportunities\nWell-equipped office",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SCALA', 'Big Data', 'Spark']",2025-06-12 14:40:23
Data Bricks,PwC India,7 - 12 years,Not Disclosed,['Bengaluru'],"Job Summary:\n\nWe are seeking a talented Data Engineer with strong expertise in Databricks, specifically in Unity Catalog, PySpark, and SQL, to join our data team. Youll play a key role in building secure, scalable data pipelines and implementing robust data governance strategies using Unity Catalog.\n\nKey Responsibilities:",,,,"['DataBricks', 'Data Bricks', 'Pyspark', 'Delta Lake', 'Databricks Engineer', 'Unity Catalog', 'SQL']",2025-06-12 14:40:25
Data Science,Global Banking Organization,5 - 10 years,Not Disclosed,['Bengaluru'],"Key Skills: Machine Learning, Data Science, Azure, Python, Hadoop.\nRoles and Responsibilities:\nStrong understanding of Math, Statistics, and the theoretical foundations of Statistical & Machine Learning, including Parametric and Non-parametric models.\nApply advanced data mining techniques to curate, process, and transform raw data into reliable datasets.\nUse various statistical techniques and ML methods to perform predictive modeling/classification for problems related to clients, distribution, sales, client profiles, and segmentation, and provide actionable insights for business decision-making.\nDemonstrate expertise in the full Machine Learning lifecycle--feature engineering, training, validation, scaling, deployment, scoring, monitoring, and feedback loops.\nProficiency in Python visualization libraries such as matplotlib and seaborn.\nExperience with cloud computing infrastructure like Azure, including Machine Learning Studio, Azure Data Factory, Synapse, Python, and PySpark.\nAbility to develop, test, and deploy models on cloud/web platforms.\nExcellent knowledge of Deep Learning Architectures, including Convolutional Neural Networks and Transformer/LLM Foundation Models.\nStrong expertise in supervised and adversarial learning techniques.\nRobust working knowledge of deep learning frameworks such as TensorFlow, Keras, and PyTorch.\nExcellent Python coding skills.\nExperience with version control tools (Git, GitHub/GitLab) and data version control.\nExperience in end-to-end model deployment and productionization.\nDemonstrated proficiency in deploying, scaling, and optimizing ML models in production environments with low latency, high availability, and cost efficiency.\nSkilled in model interpretability and CI/CD for ML using tools like MLflow and Kubeflow, with the ability to implement automated monitoring, logging, and retraining strategies.\nExperience Requirement:\n5-12 years of experience in designing and deploying deep learning and machine learning solutions.\nProven track record of delivering AI/ML solutions in real-world business applications at scale.\nHands-on experience working in cross-functional teams including data engineers, product managers, and business stakeholders.\nExperience mentoring junior data scientists and providing technical leadership within a data science team.\nExperience working with big data tools and environments such as Hadoop, Spark, or Databricks is a plus.\nPrior experience in managing model lifecycle in enterprise production environments including drift detection and retraining pipelines.\nEducation: B.Tech.",Industry Type: Financial Services,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Azure', 'Hadoop.', 'Machine Learning', 'Python']",2025-06-12 14:40:28
IN_Manager_Azure Data Engineer_Data Analytics_Advisory,PwC Service Delivery Center,5 - 10 years,Not Disclosed,['Bengaluru'],"Not Applicable\nSpecialism\nData, Analytics & AI\n& Summary\nAt PwC, our people in data and analytics engineering focus on leveraging advanced technologies and techniques to design and develop robust data solutions for clients. They play a crucial role in transforming raw data into actionable insights, enabling informed decisionmaking and driving business growth.\n\nIn data engineering at PwC, you will focus on designing and building data infrastructure and systems to enable efficient data processing and analysis. You will be responsible for developing and implementing data pipelines, data integration, and data transformation solutions.\n& Summary A career within Data and Analytics services will provide you with the opportunity to help organisations uncover enterprise insights and drive business results using smarter data analytics. We focus on a collection of organisational technology capabilities, including business intelligence, data management, and data assurance that help our clients drive innovation, growth, and change within their organisations in order to keep up with the changing nature of customers and technology. We make impactful decisions by mixing mind and machine to leverage data, understand and navigate risk, and help our clients gain a competitive edge.\nResponsibilities\nMust have\nCandidates with minimum 5 years of relevant experience for 1012 years of total experience (Architect / Managerial level).\nDeep expertise with technologies such as Data factory, Data Bricks (Advanced), SQLDB (writing complex Stored Procedures), Synapse, Python scripting (mandatory), Pyspark scripting, Azure Analysis Services.\nMust be certified with DP 203 (Azure Data Engineer Associate), Databricks Certified Data Engineer Professional (Architect / Managerial level)\nStrong troubleshooting and debugging skills. Proven experience in working source control technologies (such as GITHUB, Azure DevOps), build and release pipelines.\nExperience in writing complex PySpark queries to perform data analysis.\nMandatory skill sets\nAzure Databricks, Pyspark, Datafactory\nPreferred skill sets\nAzure Databricks, Pyspark, Datafactory, Python, Azure Devops\nYears of experience required\n712yrs\nEducation qualification\nB.Tech / M.Tech / MBA / MCA\nEducation\nDegrees/Field of Study required Bachelor of Technology, Master of Business Administration\nDegrees/Field of Study preferred\nRequired Skills\nMicrosoft Azure\nAccepting Feedback, Accepting Feedback, Active Listening, Agile Scalability, Amazon Web Services (AWS), Analytical Thinking, Apache Airflow, Apache Hadoop, Azure Data Factory, Coaching and Feedback, Communication, Creativity, Data Anonymization, Data Architecture, Database Administration, Database Management System (DBMS), Database Optimization, Database Security Best Practices, Databricks Unified Data Analytics Platform, Data Engineering, Data Engineering Platforms, Data Infrastructure, Data Integration, Data Lake, Data Modeling {+ 32 more}\nTravel Requirements\nGovernment Clearance Required?",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data analysis', 'Data modeling', 'Debugging', 'Database administration', 'Agile', 'Stored procedures', 'Apache', 'Business intelligence', 'Troubleshooting', 'Python']",2025-06-12 14:40:30
"Data Engineering : Sr Software Engineer, Tech Lead & Sr Tech Lead",Reflion Tech,7 - 12 years,22.5-37.5 Lacs P.A.,"['Mumbai( Ghansoli )', 'Navi Mumbai', 'Mumbai (All Areas)']","Hiring: Data Engineering Senior Software Engineer / Tech Lead / Senior Tech Lead\n\n- Hybrid (3 Days from office) | Shift: 2 PM 11 PM IST\n- Experience: 5 to 12+ years (based on role & grade)\n\nOpen Grades/Roles:\nSenior Software Engineer: 58 Years\nTech Lead: 7–10 Years\nSenior Tech Lead: 10–12+ Years\n\nJob Description – Data Engineering Team\n\nCore Responsibilities (Common to All Levels):\n\nDesign, build and optimize ETL/ELT pipelines using tools like Pentaho, Talend, or similar\nWork on traditional databases (PostgreSQL, MSSQL, Oracle) and MPP/modern systems (Vertica, Redshift, BigQuery, MongoDB)\nCollaborate cross-functionally with BI, Finance, Sales, and Marketing teams to define data needs\nParticipate in data modeling (ER/DW/Star schema), data quality checks, and data integration\nImplement solutions involving messaging systems (Kafka), REST APIs, and scheduler tools (Airflow, Autosys, Control-M)\nEnsure code versioning and documentation standards are followed (Git/Bitbucket)\n\nAdditional Responsibilities by Grade\n\nSenior Software Engineer (5–8 Yrs):\nFocus on hands-on development of ETL pipelines, data models, and data inventory\nAssist in architecture discussions and POCs\nGood to have: Tableau/Cognos, Python/Perl scripting, GCP exposure\n\nTech Lead (7–10 Yrs):\nLead mid-sized data projects and small teams\nDecide on ETL strategy (Push Down/Push Up) and performance tuning\nStrong working knowledge of orchestration tools, resource management, and agile delivery\n\nSenior Tech Lead (10–12+ Yrs):\nDrive data architecture, infrastructure decisions, and internal framework enhancements\nOversee large-scale data ingestion, profiling, and reconciliation across systems\nMentoring junior leads and owning stakeholder delivery end-to-end\nAdvantageous: Experience with AdTech/Marketing data, Hadoop ecosystem (Hive, Spark, Sqoop)\n\n- Must-Have Skills (All Levels):\n\nETL Tools: Pentaho / Talend / SSIS / Informatica\nDatabases: PostgreSQL, Oracle, MSSQL, Vertica / Redshift / BigQuery\nOrchestration: Airflow / Autosys / Control-M / JAMS\nModeling: Dimensional Modeling, ER Diagrams\nScripting: Python or Perl (Preferred)\nAgile Environment, Git-based Version Control\nStrong Communication and Documentation",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'SQL', 'ETL', 'Orchestration', 'Postgresql', 'Peri', 'Informatica', 'ETL Tool', 'SSIS', 'Elt', 'Modeling', 'MongoDB', 'Data Architecture', 'Talend', 'Pentaho', 'Python']",2025-06-12 14:40:33
Big Data Developer - N,Infosys,5 - 10 years,Not Disclosed,"['Hyderabad', 'Pune', 'Bengaluru']","Job description\n\nHiring for Bigdata Developer with experience range 5 to 15 years.\n\nMandatory Skills: Bigdata, Scala, Spark, Hive, Kafka\n\nEducation: BE/B.Tech/MCA/M.Tech/MSc./MSts",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Hive', 'SCALA', 'Big Data', 'Kafka', 'Spark', 'Bigdata Technologies']",2025-06-12 14:40:34
Data Annotation hiring For Fresher || Excellent communication skills,Multinational Company,0 - 4 years,1-3 Lacs P.A.,"['Gurugram', 'Delhi / NCR']","Lead data annotation and collection projects.\nDevelop and implement data annotation guidelines and processes.\nTrain and manage data annotation teams.\nCollaborate with data scientists and engineers to understand data requirements.\n\nHR - 63980 09438\n\nRequired Candidate profile\nQualification - Graduate\nSalary :-\nCTC\n25,000 / experience\n20,000 / fresher\nExperience - Data Annotation only\nTransport:- Both Side\n\n5 Day working / Rotation shift / 2 day Rotation week off",Industry Type: BPM / BPO,"Department: Customer Success, Service & Operations","Employment Type: Full Time, Permanent","['Object Detection', 'Data Annotation', 'Business Intelligence', 'Digital Image Processing', 'Data Management', 'Image Recognition', 'Image Analysis', 'Annotation', 'Deep Learning', 'Pattern Recognition', 'Image Processing', 'Imaging', 'Content Moderation', 'Data Warehousing', 'Data Analytics']",2025-06-12 14:40:37
Senior Data Engineer -Bangalore,Happiest Minds Technologies,6 - 10 years,Not Disclosed,['Bengaluru'],"Job Overview:\nThe primary purpose of this role is to translate business requirements and functional specifications into logical program designs and to deliver dashboards, schema, data pipelines, and software solutions. This includes developing, configuring, or modifying data components within various complex business and/or enterprise application solutions in various computing environments. You will partner closely with multiple Business partners, Product Owners, Data Strategy, Data Platform, Data Science and Machine Learning (MLOps) teams to drive innovative data products for end users. Additionally, you will help shape overall solution & data products, develop scalable solutions through best-in-class engineering practices.",,,,"['NoSQL', 'big data systems', 'Data Pipeline', 'MongoDB', 'SQL', 'Hive', 'GIT', 'Hadoop', 'Kafka', 'Agile', 'MQL', 'Ci/Cd']",2025-06-12 14:40:39
Sr Data Engineer,Lowes Services India Private limited,5 - 10 years,Not Disclosed,['Bengaluru'],"We are seeking a seasoned Senior Data Engineer to join our Marketing Data Platform team. This role is pivotal in designing, building, and optimizing scalable data pipelines and infrastructure that support our marketing analytics and customer engagement strategies. The ideal candidate will have extensive experience with big data technologies, cloud platforms, and a strong understanding of marketing data dynamics.\n\nData Pipeline Development & Optimization\nDesign, develop, and maintain robust ETL/ELT pipelines using Apache PySpark on GCP services like Dataproc and Cloud Composer.\nEnsure data pipelines are scalable, efficient, and reliable to handle large volumes of marketing data.\nData Warehousing & Modeling\nImplement and manage data warehousing solutions using BigQuery, ensuring optimal performance and cost-efficiency.\nDevelop and maintain data models that support marketing analytics and reporting needs.\nCollaboration & Stakeholder Engagement\nWork closely with marketing analysts, data scientists, and cross-functional teams to understand data requirements and deliver solutions that drive business insights.\nTranslate complex business requirements into technical specifications and data architecture.\nData Quality & Governance\nImplement data quality checks and monitoring to ensure the accuracy and integrity of marketing data.\nAdhere to data governance policies and ensure compliance with data privacy regulations.\nContinuous Improvement & Innovation\nStay abreast of emerging technologies and industry trends in data engineering and marketing analytics.\nPropose and implement improvements to existing data processes and infrastructure\n  Years of Experience\n5 Years in Data Engineer space\n  Education Qualification & Certifications\nB.Tech or MCA\n  Experience\nProven experience with Apache PySpark, GCP (including Dataproc, BigQuery, Cloud Composer), and data pipeline orchestration.\nTechnical Skills\nProficiency in SQL and Python.\nExperience with data modeling, ETL/ELT processes, and data warehousing concepts.",Industry Type: Retail,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['orchestration', 'Data modeling', 'data governance', 'Data quality', 'Apache', 'Continuous improvement', 'Monitoring', 'SQL', 'Python', 'Data architecture']",2025-06-12 14:40:41
Senior Data Engineer,Talentien Global Solutions,4 - 8 years,12-18 Lacs P.A.,"['Hyderabad', 'Chennai', 'Coimbatore']","We are seeking a skilled and motivated Data Engineer to join our dynamic team. The ideal candidate will have experience in designing, developing, and maintaining scalable data pipelines and architectures using Hadoop, PySpark, ETL processes, and Cloud technologies.\n\nResponsibilities:\nDesign, develop, and maintain data pipelines for processing large-scale datasets.\nBuild efficient ETL workflows to transform and integrate data from multiple sources.\nDevelop and optimize Hadoop and PySpark applications for data processing.\nEnsure data quality, governance, and security standards are met across systems.\nImplement and manage Cloud-based data solutions (AWS, Azure, or GCP).\nCollaborate with data scientists and analysts to support business intelligence initiatives.\nTroubleshoot performance issues and optimize query executions in big data environments.\nStay updated with industry trends and advancements in big data and cloud technologies.\nRequired Skills:\nStrong programming skills in Python, Scala, or Java.\nHands-on experience with Hadoop ecosystem (HDFS, Hive, Spark, etc.).\nExpertise in PySpark for distributed data processing.\nProficiency in ETL tools and workflows (SSIS, Apache Nifi, or custom pipelines).\nExperience with Cloud platforms (AWS, Azure, GCP) and their data-related services.\nKnowledge of SQL and NoSQL databases.\nFamiliarity with data warehousing concepts and data modeling techniques.\nStrong analytical and problem-solving skills.\n\nInterested can reach us at +91 7305206696/ saranyadevib@talentien.com",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'Data Engineering', 'Hadoop', 'Spark', 'ETL', 'Airflow', 'Etl Pipelines', 'Big Data', 'EMR', 'Gcp Cloud', 'Data Bricks', 'Azure Cloud', 'Data Pipeline', 'SCALA', 'Snowflake', 'Data Lake', 'Data Warehousing', 'Data Modeling', 'AWS', 'Python']",2025-06-12 14:40:44
"Sr. Staff Engineer, Data Frameworks",NetSkope Software,10 - 15 years,Not Disclosed,"['Kolkata', 'Mumbai', 'New Delhi', 'Hyderabad', 'Pune', 'Chennai', 'Bengaluru']","As a Sr. Staff Engineer on the Data Engineering Team you'll be working on some of the hardest problems in the field of Data, Cloud and Security with a mission to achieve the highest standards of customer success. You will be building blocks of technology that will define Netskope s future. You will leverage open source Technologies around OLAP, OLTP, Streaming, Big Data and ML models. You will help design, and build an end-to-end system to manage the data and infrastructure used to improve security insights for our global customer base.\nYou will be part of a growing team of renowned industry experts in the exciting space of Data and Cloud Analytics\nYour contributions will have a major impact on our global customer-base and across the industry through our market-leading products\nYou will solve complex, interesting challenges, and improve the depth and breadth of your technical and business skills.\nWhat you will be doing\nConceiving and building services used by Netskope products to validate, transform, load and perform analytics of large amounts of data using distributed systems with cloud scale and reliability.\nHelping other teams architect their applications using services from the Data team wile using best practices and sound designs.\nEvaluating many open source technologies to find the best fit for our needs, and contributing to some of them.\nWorking with the Application Development and Product Management teams to scale their underlying services\nProviding easy-to-use analytics of usage patterns, anticipating capacity issues and helping with long term planning\nLearning about and designing large-scale, reliable enterprise services.\nWorking with great people in a fun, collaborative environment.\nCreating scalable data mining and data analytics frameworks using cutting edge tools and techniques\nRequired skills and experience\n10+ years of industry experience building highly scalable distributed Data systems\nProgramming experience in Python, Java or Golang\nExcellent data structure and algorithm skills\nProven good development practices like automated testing, measuring code coverage.\nProven experience developing complex Data Platforms and Solutions using Technologies like Kafka, Kubernetes, MySql, Hadoop, Big Query and other open source databases\nExperience designing and implementing large, fault-tolerant and distributed systems around columnar data stores.\nExcellent written and verbal communication skills\nBonus points for contributions to the open source community\nEducation\nBSCS or equivalent required, MSCS or equivalent strongly preferred",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Product management', 'data security', 'MySQL', 'OLAP', 'Application development', 'Open source', 'Data mining', 'OLTP', 'Distribution system', 'Python']",2025-06-12 14:40:47
Senior Data Engineer,SPAN.IO,5 - 10 years,Not Disclosed,['Bengaluru( Indira Nagar )'],"Senior Data Engineer\n\nOur Mission\n\nSPAN is enabling electrification for all\nWe are a mission-driven company designing, building, and deploying products that electrify the built environment, reduce carbon emissions, and slow the effects of climate change.\nDecarbonization is the process to reduce or remove greenhouse gas emissions, especially carbon dioxide, from entering our atmosphere.\nElectrification is the process of replacing fossil fuel appliances that run on gas or oil with all-electric upgrades for a cleaner way to power our lives.\n\nAt SPAN, we believe in:\nEnabling homes and vehicles powered by clean energy\nMaking electrification upgrades possible\nBuilding more resilient homes with reliable backup\nDesigning a flexible and distributed electrical grid\n\nThe Role\nAs a Data Engineer you would be working to design, build, test and create infrastructure necessary for real time analytics and batch analytics pipelines. You will work with multiple teams within the org to provide analysis, insights on the data. You will also be involved in writing ETL processes that support data ingestion. You will also guide and enforce best practices for data management, governance and security. You will build infrastructure to monitor these data pipelines / ETL jobs / tasks and create tooling/infrastructure for providing visibility into these.\n\nResponsibilities\nWe are looking for a Data Engineer with passion for building data pipelines, working with product, data science and business intelligence teams and delivering great solutions. As a part of the team you:-\nAcquire deep business understanding on how SPAN data flows from IoT device to cloud through the system and build scalable and optimized data solutions that impact many stakeholders.\nBe an advocate for data quality and excellence of our platform.\nBuild tools that help streamline the management and operation of our data ecosystem.\nEnsure best practices and standards in our data ecosystem are shared across teams.\nWork with teams within the company to build close relationships with our partners to understand the value our platform can bring and how we can make it better.\nImprove data discovery by creating data exploration processes and promoting adoption of data sources across the company.\nHave a desire to write tools and applications to automate work rather than do everything by hand.\nAssist internal teams in building out data logging, alerting and monitoring for their applications\nAre passionate about CI/CD process.\nDesign, develop and establish KPIs to monitor analysis and provide strategic insights to drive growth and performance.\n\nAbout You\n\nRequired Qualifications\nBachelor's Degree in a quantitative discipline: computer science, statistics, operations research, informatics, engineering, applied mathematics, economics, etc.\n5+ years of relevant work experience in data engineering, business intelligence, research or related fields.\nExpert level production-grade, programming experience in at least one of these languages (Python, Kotlin, or other JVM based languages)\nExperience in writing clean, concise and well structured code in one of the above languages.\nExperience working with Infrastructure-as-code tools: Pulumi, Terraform, etc.\nExperience working with CI/CD systems: Circle-CI, Github Actions, Argo-CD, etc.\nExperience managing data engineering infrastructure through Docker and Kubernetes\nExperience working with latency data processing solutions like Flink, Prefect, AWS Kinesis, Kafka, Spark Stream processing etc.\nExperience with SQL/Relational databases, OLAP databases like Snowflake.\nExperience working in AWS: S3, Glue, Athena, MSK, EMR, ECR etc.\n\nBonus Qualifications\nExperience with the Energy industry\nExperience with building IoT and/or hardware products\nUnderstanding of electrical systems and residential loads\nExperience with data visualization using Tableau.\nExperience in Data loading tools like FiveTran as well as data debugging tools such as DataDog\n\nLife at SPAN\nOur Bengaluru team plays a pivotal role in SPANs continued growth and expansion. Together, were driving engineering, product development, and operational excellence to shape the future of home energy solutions.\nAs part of our team in India, youll have the opportunity to collaborate closely with our teams in the US and across the globe. This international collaboration fosters innovation, learning, and growth, while helping us achieve our bold mission of electrifying homes and advancing clean energy solutions worldwide.\nOur in-office culture offers the chance for dynamic interactions and hands-on teamwork, making SPAN a truly collaborative environment where every team members contribution matters.\nOur climate-focused culture is driven by a team of forward-thinkers, engineers, and problem-solvers who push boundaries every day.\nDo mission-driven work: Every role at SPAN directly advances clean energy adoption.\nBring powerful ideas to life: We encourage diverse ideas and perspectives to drive stronger products.\nNurture an innovation-first mindset: We encourage big thinking and bold action.\nDeliver exceptional customer value: We value hard work, and the ability to deliver exceptional customer value.\n\nBenefits at SPAN India\nGenerous paid leave\nComprehensive Insurance & Health Benefits\nCentrally located office in Bengaluru with easy access to public transit, dining, and city amenities\n\nInterested in joining our team? Apply today and well be in touch with the next steps!",Industry Type: Electronics Manufacturing,"Department: Production, Manufacturing & Engineering","Employment Type: Full Time, Permanent","['Terraform', 'Snowflake', 'AWS', 'Python', 'SQL', 'Java', 'Apache Flink', 'Kotlin']",2025-06-12 14:40:49
Senior Data Engineer,Epsilon,5 - 9 years,Not Disclosed,['Bengaluru'],"This position in the Engineering team under the Digital Experience organization. We drive the first mile of the customer experience through personalization of offers and content. We are currently on the lookout for a smart, highly driven engineer.\nYou will be part of a team that is focused on building & managing solutions, pipelines using marketing technology stacks. You will also be expected to Identify and implement improvements including for optimizing data delivery and automate processes/pipelines.\nThe incumbent is also expected to partner with various stakeholders, bring scientific rigor to design and develop high quality solutions.\nCandidate must have excellent verbal and written communication skills and be comfortable working in an entrepreneurial, startup environment within a larger company.\nClick here to view how Epsilon transforms marketing with 1 View, 1 Vision and 1 Voice.\n\nBrief Description of Role:\nExperience with both structured and unstructured data\nExperience working on AdTech or MarTech technologies.\nExperience in relational and non-relational databases and SQL (NoSQL is a plus).\nUnderstanding of Data Modeling, Data Catalog concepts and tools\nAbility to deal with data imperfections such as missing values, outliers, inconsistent formatting, etc.\nCollaborate with other members of the team to ensure high quality deliverables\nLearning and implementing the latest design patterns in data engineering\n\nData Management\nExperience with both structured and unstructured data\nExperience building Data and CI/CD pipelines\nExperience working on AdTech or MarTech technologies is added advantage\nExperience in relational and non-relational databases and SQL (NoSQL is a plus).\nHands on experience building ETL workflows/pipelines on large volumes of data\nGood understanding of Data Modeling, Data Warehouse, Data Catalog concepts and tools\nAble to identify, join, explore, and examine data from multiple disparate sources and formats\nAbility to reduce large quantities of unstructured or formless data and get it into a form in which it can be analyzed\nAbility to deal with data imperfections such as missing values, outliers, inconsistent formatting, etc.\nDevelopment\nAbility to write code in programming languages such as Python and shell script on Linux\nFamiliarity with development methodology such as Agile/Scrum\nLove to learn new technologies, keep abreast of the latest technologies within the cloud architecture, and drive your organization to adapt to emerging best practices\nGood knowledge of working in UNIX/LINUX systems\nQualifications\nBachelors degree in computer science with 5+ years of similar experience\nTech Stack: Python, SQL, Scripting language (preferably JavaScript)\nExperience or knowledge on Adobe Experience Platform (RT-CDP/AEP)\nExperience working in Cloud Platforms (GCP or AWS)\nFamiliarity with automated unit/integration test frameworks\nGood written and spoken communication skills, team player.\nStrong analytic thought process and ability to interpret findings",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'Data Engineering', 'Data Bricks', 'Python', 'SQL', 'Azure Aws', 'AWS']",2025-06-12 14:40:51
"Senior Data Engineer Databricks, ADF, PySpark",Suzva Software Technologies,6 - 11 years,Not Disclosed,"['Mumbai', 'Delhi / NCR', 'Bengaluru']","Senior Data Engineer (Remote, Contract 6 Months) Databricks, ADF, and PySpark.\nWe are hiring a Senior Data Engineer for a 6-month remote contract position. The ideal candidate is highly skilled in building scalable data pipelines and working within the Azure cloud ecosystem, especially Databricks, ADF, and PySpark. You'll work closely with cross-functional teams to deliver enterprise-level data engineering solutions.\n\nKeyResponsibilities\nBuild scalable ETL pipelines and implement robust data solutions in Azure.\n\nManage and orchestrate workflows using ADF, Databricks, ADLS Gen2, and Key Vaults.\n\nDesign and maintain secure and efficient data lake architecture.\n\nWork with stakeholders to gather data requirements and translate them into technical specs.\n\nImplement CI/CD pipelines for seamless data deployment using Azure DevOps.\n\nMonitor data quality, performance bottlenecks, and scalability issues.\n\nWrite clean, organized, reusable PySpark code in an Agile environment.\n\nDocument pipelines, architectures, and best practices for reuse.\n\nMustHaveSkills\nExperience: 6+ years in Data Engineering\n\nTech Stack: SQL, Python, PySpark, Spark, Azure Databricks, ADF, ADLS Gen2, Azure DevOps, Key Vaults\n\nCore Expertise: Data Warehousing, ETL, Data Pipelines, Data Modelling, Data Governance\n\nAgile, SDLC, Containerization (Docker), Clean coding practices\n\nGoodToHaveSkills\nEvent Hubs, Logic Apps\n\nPower BI\n\nStrong logic building and competitive programming background\n\nLocation : - Remote,Mumbai, Delhi / NCR, Bengaluru , Kolkata, Chennai, Hyderabad, Ahmedabad, Pune",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Databricks', 'ADF', 'PySpark', 'ADLS Gen2', 'Azure Databricks', 'Key Vaults', 'Spark', 'Azure DevOps', 'SQL', 'Python']",2025-06-12 14:40:53
"Senior Data Engineer - Airflow, PLSQL",Relanto Global,5 - 10 years,Not Disclosed,['Bengaluru'],"PositionSenior Data Engineer - Airflow, PLSQL \n\n Experience5+ Years \n\n LocationBangalore/Hyderabad/Pune \n\n\n\nSeeking a Senior Data Engineer with strong expertise in Apache Airflow and Oracle PL/SQL, along with working experience in Snowflake and Agile methodologies. The ideal candidate will also take up Scrum Master responsibilities and lead a data engineering scrum team to deliver robust, scalable data solutions.\n\n\n Key Responsibilities: \nDesign, develop, and maintain scalable data pipelines using Apache Airflow.\nWrite and optimize complex PL/SQL queries, procedures, and packages on Oracle databases.\nCollaborate with cross-functional teams to design efficient data models and integration workflows.\nWork with Snowflake for data warehousing and analytics use cases.\nOwn the delivery of sprint goals, backlog grooming, and facilitation of agile ceremonies as the Scrum Master.\nMonitor pipeline health and troubleshoot production data issues proactively.\nEnsure code quality, documentation, and best practices across the team.\nMentor junior data engineers and promote a culture of continuous improvement.\n\n\n Required Skills and Qualifications: \n5+ years of experience as a Data Engineer in enterprise environments.\nStrong expertise in  Apache Airflow  for orchestrating workflows.\nExpert in  Oracle PL/SQL  - stored procedures, performance tuning, debugging.\nHands-on experience with  Snowflake  - data modeling, SQL, optimization.\nWorking knowledge of version control (Git) and CI/CD practices.\nPrior experience or certification as a  Scrum Master  is highly desirable.\nStrong analytical and problem-solving skills with attention to detail.\nExcellent communication and leadership skills.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sql', 'plsql', 'stored procedures', 'oracle pl', 'performance tuning', 'hive', 'continuous integration', 'ci/cd', 'data warehousing', 'pyspark', 'git', 'apache', 'data modeling', 'spark', 'debugging', 'hadoop', 'big data', 'snowflake', 'python', 'oracle', 'sql queries', 'airflow', 'data engineering', 'agile', 'sqoop']",2025-06-12 14:40:56
Data Analyst,FedEx,2 - 4 years,Not Disclosed,"['Gurugram', 'Bengaluru']","Role & responsibilities :\n\nCollect, analyze, and interpret complex data sets using Python and SQL to support business objectives.\nCollaborate with stakeholders to understand business needs, formulate analytic solutions, and provide actionable insights.\nDevelop and maintain data models and reports to track key performance indicators (KPIs) and business metrics.\nCreate meaningful data visualizations to communicate findings, trends, and actionable insights to non-technical stakeholders.\nConduct exploratory data analysis and identify patterns, trends, and opportunities for business improvement.\nSupport data quality initiatives, ensuring accuracy and consistency across data sources.\nUtilize statistical and quantitative techniques to support problem-solving and business optimization efforts.\n\n\n\n\nPreferred candidate profile\n\nPython: Proficiency in data manipulation, data analysis libraries (Pandas, NumPy),and data visualization libraries (Matplotlib, Seaborn).\nSQL: Strong command of SQL for data extraction, transformation, and complex queries.\nBusiness Acumen: Ability to understand business context and objectives, aligning analytics with organizational goals.\nQuantitative Aptitude: Strong analytical and problem-solving skills, with a keen attention to detail.\nData Visualization: Basic skills in data visualization to effectively communicate insights.\nStatistical Analysis: Foundational understanding of statistical methods (e.g., regression, hypothesis testing).\nCommunication Skills: Ability to distill complex data insights into clear,actionable recommendations for stakeholders.",Industry Type: Courier / Logistics (Logistics Tech),Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Analysis', 'SQL', 'Python', 'Power Bi', 'Business Insights', 'Tableau', 'Data Analytics']",2025-06-12 14:40:58
Senior ML Compiler Engineer,Qualcomm,0 - 5 years,Not Disclosed,['Hyderabad'],"Job Area: Engineering Group, Engineering Group > Software Engineering\n\nGeneral Summary:\n\nInterested in accelerating machine learning and artificial intelligence on mobile devices for millions of usersCome join our team. We are building software platforms that enable users of Qualcomms silicon to construct optimized neural networks and machine learning algorithms. We are looking for software engineers with a machine learning or compiler background who will help us build these software platforms. In this role, you will construct and tune machine learning frameworks, build compilers and tools, and collaborate with Qualcomm hardware and software engineers to enable efficient usage of Qualcomms silicon for machine learning applications.\n\nMinimum qualifications:\nBachelors degree in Engineering, Information Systems, Computer Science, or related field.\nProgramming in C/C++\n0 to 10 years of software engineering or related work experience\n\n\nPreferred qualifications:\nExperience in machine learning frameworks such as MxNet/NNVM/TVM, Pytorch, Tensorflow, Caffe\n\nOR experience in compilers with an interest in machine learning\nDeep knowledge of software engineering\n\n\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 2+ years of Software Engineering or related work experience.\nOR\nMaster's degree in Engineering, Information Systems, Computer Science, or related field and 1+ year of Software Engineering or related work experience.\nOR\nPhD in Engineering, Information Systems, Computer Science, or related field.\n\n2+ years of academic or work experience with Programming Language such as C, C++, Java, Python, etc.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'machine learning', 'tensorflow', 'pytorch', 'software engineering', 'algorithms', 'c++', 'natural language processing', 'caffe', 'neural networks', 'mxnet', 'artificial intelligence', 'sql', 'deep learning', 'r', 'java', 'data science', 'computer vision', 'machine learning algorithms', 'ml']",2025-06-12 14:41:00
Data Analyst,Primary Healthtech,1 - 5 years,2.5-4.5 Lacs P.A.,['Noida'],"Roles and Responsibilities\nCollect data from various sources, clean it, and analyze it using statistical tools.\nCreate reports based on analysis findings to present insights to stakeholders.\nDevelop dashboards and visualizations to effectively communicate results.\nManage databases by designing schema, writing queries, and optimizing performance.\nEnsure accuracy of data through quality control measures.\nDesired Candidate Profile\n1-5 years of experience in Data Analysis or related field (Data Analytics).\nB.Tech/B.E. degree in Any Specialization.\nProficiency in SQL programming language with knowledge of database management systems like MySQL or PostgreSQL.\nStrong understanding of statistics, data interpretation, and data visualization techniques.",Industry Type: Medical Devices & Equipment,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Interpretation', 'Data Analysis', 'Data Analytics', 'Data Management', 'Data Visualization', 'Data Reporting']",2025-06-12 14:41:02
Data Engineer,ZS,1 - 6 years,Not Disclosed,['Pune'],"Create and maintain optimal data pipeline architecture.\nIdentify, design, and implement internal process improvements, automating manual processes, optimizing data delivery, re-designing infrastructure for scalability.\nDesign, develop and deploy high volume ETL pipelines to manage complex and near-real time data collection.\nDevelop and optimize SQL queries and stored procedures to meet business requirements.\nDesign, implement, and maintain REST APIs for data interaction between systems.\nEnsure performance, security, and availability of databases.",,,,"['Root cause analysis', 'SQL database', 'Management consulting', 'Financial planning', 'Data collection', 'Manager Technology', 'Engineering Manager', 'Information technology', 'Analytics']",2025-06-12 14:41:05
"AI/ML Engineer (Specializing in NLP/ML, Large Data Processing,",Synechron,8 - 10 years,Not Disclosed,"['Pune', 'Hinjewadi']","job requisition idJR1027361\n\nJob Summary\nSynechron seeks a highly skilled AI/ML Engineer specializing in Natural Language Processing (NLP), Large Language Models (LLMs), Foundation Models (FMs), and Generative AI (GenAI). The successful candidate will design, develop, and deploy advanced AI solutions, contributing to innovative projects that transform monolithic systems into scalable microservices integrated with leading cloud platforms such as Azure, Amazon Bedrock, and Google Gemini. This role plays a critical part in advancing Synechrons capabilities in cutting-edge AI technologies, enabling impactful business insights and product innovations.\n\nSoftware\n\nRequired Proficiency:\nPython (core librariesTensorFlow, PyTorch, Hugging Face transformers, etc.)\nCloud platformsAzure, AWS, Google Cloud (familiarity with AI/ML services)\nContainerizationDocker, Kubernetes\nVersion controlGit\nData management toolsSQL, NoSQL databases (e.g., MongoDB)\nModel deployment and MLOps toolsMLflow, CI/CD pipelines, monitoring tools\nPreferred\n\nSkills:\nExperience with cloud-native AI frameworks and SDKs\nFamiliarity with AutoML tools\nAdditional programming languages (e.g., Java, Scala)\nOverall Responsibilities\nDesign, develop, and optimize NLP models, including advanced LLMs and Foundation Models, for diverse business use cases.\nLead the development of large data pipelines for training, fine-tuning, and deploying models on big data platforms.\nArchitect, implement, and maintain scalable AI solutions in line with MLOps best practices.\nTransition legacy monolithic AI systems into modular, microservices-based architectures for scalability and maintainability.\nBuild end-to-end AI applications from scratch, including data ingestion, model training, deployment, and integration.\nImplement retrieval-augmented generation techniques for enhanced context understanding and response accuracy.\nConduct thorough testing, validation, and debugging of AI/ML models and pipelines.\nCollaborate with cross-functional teams to embed AI capabilities into customer-facing and enterprise products.\nSupport ongoing maintenance, monitoring, and scaling of deployed AI systems.\nDocument system designs, workflows, and deployment procedures for compliance and knowledge sharing.\nPerformance Outcomes:\nProduction-ready AI solutions delivering high accuracy and efficiency.\nRobust data pipelines supporting training and inference at scale.\nSeamless integration of AI models with cloud infrastructure.\nEffective collaboration leading to innovative AI product deployment.\nTechnical Skills (By Category)\n\nProgramming Languages:\nEssential: Python (TensorFlow, PyTorch, Hugging Face, etc.)\nPreferred: Java, Scala\nDatabases/Data Management:\nSQL (PostgreSQL, MySQL), NoSQL (MongoDB, DynamoDB)\nCloud Technologies:\nAzure AI, AWS SageMaker, Bedrock, Google Cloud Vertex AI, Gemini\nFrameworks and Libraries:\nTransformers, Keras, scikit-learn, XGBoost, Hugging Face engines\nDevelopment Tools & Methodologies:\nDocker, Kubernetes, Git, CI/CD pipelines (Jenkins, Azure DevOps)\nSecurity & Compliance:\nKnowledge of data security standards and privacy policies (GDPR, HIPAA as applicable)\nExperience\n8 to 10 years of hands-on experience in AI/ML development, especially NLP and Generative AI.\nDemonstrated expertise in designing, fine-tuning, and deploying LLMs, FMs, and GenAI solutions.\nProven ability to develop end-to-end AI applications within cloud environments.\nExperience transforming monolithic architectures into scalable microservices.\nStrong background with big data processing pipelines.\nPrior experience working with cloud-native AI tools and frameworks.\nIndustry experience in finance, healthcare, or technology sectors is advantageous.\nAlternative Experience:\nCandidates with extensive research or academic experience in AI/ML, especially in NLP and large-scale data processing, are eligible if they have practical deployment experience.\n\nDay-to-Day Activities\nDevelop and optimize sophisticated NLP/GenAI models fulfilling business requirements.\nLead data pipeline construction for training and inference workflows.\nCollaborate with data engineers, architects, and product teams to ensure scalable deployment.\nConduct model testing, validation, and performance tuning.\nImplement and monitor model deployment pipelines, troubleshoot issues, and improve system robustness.\nDocument models, pipelines, and deployment procedures for audit and knowledge sharing.\nStay updated with emerging AI/ML trends, integrating best practices into projects.\nPresent findings, progress updates, and technical guidance to stakeholders.\nQualifications\nBachelors degree in Computer Science, Data Science, or related field; Masters or PhD preferred.\nCertifications in AI/ML, Cloud (e.g., AWS, Azure, Google Cloud), or Data Engineering are a plus.\nProven professional experience with advanced NLP and Generative AI solutions.\nCommitment to continuous learning to keep pace with rapidly evolving AI technologies.\nProfessional Competencies\nStrong analytical and problem-solving capabilities.\nExcellent communication skills, capable of translating complex technical concepts.\nCollaborative team player with experience working across global teams.\nAdaptability to rapidly changing project scopes and emerging AI trends.\nInnovation-driven mindset with a focus on delivering impactful solutions.\nTime management skills to prioritize and manage multiple projects effectively.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'data management', 'data processing', 'pipeline', 'big data', 'continuous integration', 'kubernetes', 'deploying models', 'natural language processing', 'ci/cd', 'fms', 'artificial intelligence', 'docker', 'sql', 'microservices', 'tensorflow', 'java', 'pytorch', 'jenkins', 'keras', 'aws']",2025-06-12 14:41:07
Procurement Data Analyst,Response Informatics,1 - 5 years,Not Disclosed,"['Kolkata', 'Mumbai', 'New Delhi', 'Hyderabad', 'Pune', 'Chennai', 'Bengaluru']","Key Responsibilities:\nClean, validate, and standardize extracted datasets in accordance with best practices and any specifications provided by the Client.\nDesign and develop dashboards, visualizations, and reporting tools using Microsoft Excel, Power BI, or other equivalent analytics platforms as may be approved by the Client.\nConduct in-depth analysis of procurement data to identify tail spend, category-level insights, and potential cost-saving opportunities.\nSegment and categorize procurement data by business unit, supplier, spend category, and geographic region to enable targeted sourcing strategies.\nProduce periodic reports and executive summaries for review by the Client s Procurement leadership team.\nTranslate data-driven trends and findings into clear, actionable sourcing recommendations.\nCollaborate closely with the Client s internal buying teams to ensure alignment of data outputs with sourcing priorities and project selection criteria.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Procurement', 'Excel', 'Senior Executive', 'power bi', 'Data Analyst', 'Saving', 'Cost', 'Analytics', 'Reporting tools']",2025-06-12 14:41:09
Business Data Analyst,CGI,5 - 8 years,Not Disclosed,['Hyderabad'],"Business Data Analyst - HealthCare\n\nJob Summary\nWe are seeking an experienced and results-driven Business Data Analyst with 5+ years of hands-on experience in data analytics, visualization, and business insight generation. This role is ideal for someone who thrives at the intersection of business and datatranslating complex data sets into compelling insights, dashboards, and strategies that support decision-making across the organization.\nYou will collaborate closely with stakeholders across departments to identify business needs, design and build analytical solutions, and tell compelling data stories using advanced visualization tools.\nKey Responsibilities\nData Analytics & Insights Analyze large and complex data sets to identify trends, anomalies, and opportunities that help drive business strategy and operational efficiency.\n• Dashboard Development & Data Visualization Design, develop, and maintain interactive dashboards and visual reports using tools like Power BI, Tableau, or Looker to enable data-driven decisions.\n• Business Stakeholder Engagement Collaborate with cross-functional teams to understand business goals, define metrics, and convert ambiguous requirements into concrete analytical deliverables.\n• KPI Definition & Performance Monitoring Define, track, and report key performance indicators (KPIs), ensuring alignment with business objectives and consistent measurement across teams.\n• Data Modeling & Reporting Automation Work with data engineering and BI teams to create scalable, reusable data models and automate recurring reports and analysis processes.\n• Storytelling with Data Communicate findings through clear narratives supported by data visualizations and actionable recommendations to both technical and non-technical audiences.\n• Data Quality & Governance Ensure accuracy, consistency, and integrity of data through validation, testing, and documentation practices.\nRequired Qualifications\nBachelor’s or Master’s degree in Business, Economics, Statistics, Computer Science, Information Systems, or a related field.\n• 5+ years of professional experience in a data analyst or business analyst role with a focus on data visualization and analytics.\n• Proficiency in data visualization tools: Power BI, Tableau, Looker (at least one).\n• Strong experience in SQL and working with relational databases to extract, manipulate, and analyze data.\n• Deep understanding of business processes, KPIs, and analytical methods.\n• Excellent problem-solving skills with attention to detail and accuracy.\n• Strong communication and stakeholder management skills with the ability to explain technical concepts in a clear and business-friendly manner.\n• Experience working in Agile or fast-paced environments.\nPreferred Qualifications\nExperience working with cloud data platforms (e.g., Snowflake, BigQuery, Redshift).\n• Exposure to Python or R for data manipulation and statistical analysis.\n• Knowledge of data warehousing, dimensional modeling, or ELT/ETL processes.\n• Domain experience in Healthcare is a plus.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Bigquery', 'Snowflake', 'Data Warehousing', 'Redshift', 'Python', 'ETL']",2025-06-12 14:41:11
Data Analyst-Having Stratup-Mid-Size companies Exp.@ Bangalore_Urgent,"A leader in this space, we deliver world...",8 - 13 years,Not Disclosed,['Bengaluru'],"Data Analyst\n\nLocation: Bangalore\nExperience: 8 - 15 Yrs\nType: Full-time\n\nRole Overview\n\nWe are seeking a skilled Data Analyst to support our platform powering operational intelligence across airports and similar sectors. The ideal candidate will have experience working with time-series datasets and operational information to uncover trends, anomalies, and actionable insights. This role will work closely with data engineers, ML teams, and domain experts to turn raw data into meaningful intelligence for business and operations stakeholders.\n\nKey Responsibilities\n\nAnalyze time-series and sensor data from various sources\nDevelop and maintain dashboards, reports, and visualizations to communicate key metrics and trends.\nCorrelate data from multiple systems (vision, weather, flight schedules, etc) to provide holistic insights.\nCollaborate with AI/ML teams to support model validation and interpret AI-driven alerts (e.g., anomalies, intrusion detection).\nPrepare and clean datasets for analysis and modeling; ensure data quality and consistency.\nWork with stakeholders to understand reporting needs and deliver business-oriented outputs.\n\n\nQualifications & Required Skills\n\nBachelors or Masters degree in Data Science, Statistics, Computer Science, Engineering, or a related field.\n5+ years of experience in a data analyst role, ideally in a technical/industrial domain.\nStrong SQL skills and proficiency with BI/reporting tools (e.g., Power BI, Tableau, Grafana).\nHands-on experience analyzing structured and semi-structured data (JSON, CSV, time-series).\nProficiency in Python or R for data manipulation and exploratory analysis.\nUnderstanding of time-series databases or streaming data (e.g., InfluxDB, Kafka, Kinesis).\nSolid grasp of statistical analysis and anomaly detection methods.\nExperience working with data from industrial systems or large-scale physical infrastructure.\n\n\nGood-to-Have Skills\n\nDomain experience in airports, smart infrastructure, transportation, or logistics.\nFamiliarity with data platforms (Snowflake, BigQuery, Custom-built using open-source).\nExposure to tools like Airflow, Jupyter Notebooks and data quality frameworks.\nBasic understanding of AI/ML workflows and data preparation requirements.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Kafka', 'SQL', 'airports', 'InfluxDB', 'Airflow', 'structured Data', 'time-series', 'JSON', 'Tableau', 'Grafana', 'R', 'AI/ML', 'Kinesis', 'Snowflake', 'time-series databases', 'Data Preparation', 'Python', 'smart infrastructure', 'BigQuery', 'streaming data', 'Power BI', 'CSV', 'transportation', 'logistic', 'reporting tools']",2025-06-12 14:41:14
Data Engineer III,Expedia Group,6 - 11 years,Not Disclosed,['Gurugram'],"Why Join Us?\nTo shape the future of travel, people must come first. Guided by our Values and Leadership Agreements, we foster an open culture where everyone belongs, differences are celebrated and know that when one of us wins, we all win.\nWe provide a full benefits package, including exciting travel perks, generous time-off, parental leave, a flexible work model (with some pretty cool offices), and career development resources, all to fuel our employees passion for travel and ensure a rewarding career journey. We re building a more open world. Join us.\nData Engineer III\nExpedia Group s CTO Enablement team is looking for a highly motivated Data Engineer III to lead the design, delivery, and stewardship of business-critical data infrastructure that powers our Capitalization program and Business Operations functions . This role is at the intersection of finance, strategy, and engineering , where data precision and operational rigor directly support the company s financial integrity and execution effectiveness.\nYou will collaborate with stakeholders across Finance, BizOps, and Technology to build scalable data solutions that ensure capitalization accuracy, enable deep operational analytics, and streamline financial and business reporting at scale.\nWhat you will do:\nDesign, build, and maintain high-scale data pipelines and transformation logic to support CapEx/OpEx classification, capitalization tracking, and operational data modeling.\nDeliver clean, well-documented, governed datasets that drive finance reporting, strategic planning, and key operational dashboards.\nPartner with cross-functional teams (Finance, Engineering, Strategy) to translate business and compliance requirements into technical solutions.\nLead the development of data models and ETL processes to support performance monitoring, workforce utilization, project financials, and business KPIs.\nEstablish and enforce data quality, lineage, and access control standards to ensure trust in business-critical data.\nProactively identify and resolve data reliability issues related to financial close processes, budget tracking, and capitalization rules.\nServe as a technical advisor to BizOps and Finance stakeholders, recommending improvements in tooling, architecture, and process automation.\nMentor other engineers and contribute to the growth of a high-performance data team culture.\nWho you are:\n6+ years of experience in data engineering , analytics engineering , or data infrastructure roles with a focus on operational and financial data.\nExpertise in SQL and Python , and experience with data pipeline orchestration tools such as Airflow , dbt , or equivalent.\nStrong understanding of cloud-based data platforms (e.g., Snowflake, BigQuery, Redshift, or Databricks).\nDeep familiarity with capitalization standards , CapEx/OpEx distinction, and operational reporting in a tech-driven environment.\nDemonstrated ability to build scalable, reliable ETL/ELT workflows that serve diverse analytical and reporting needs.\nExperience working cross-functionally in complex organizations with multiple stakeholder groups.\nPassion for operational excellence, data governance, and driving actionable business insights from data.\nPreferred qualifications:\n- Experience supporting BizOps , FP&A , or Product Finance teams with data tooling and reporting.\n- Familiarity with BI platforms like Looker , Power BI , or Tableau .\n- Exposure to agile delivery frameworks and enterprise-level operational rhythms.\nAccommodation requests\nIf you need assistance with any part of the application or recruiting process due to a disability, or other physical or mental health conditions, please reach out to our Recruiting Accommodations Team through the Accommodation Request .\nWe are proud to be named as a Best Place to Work on Glassdoor in 2024 and be recognized for award-winning culture by organizations like Forbes, TIME, Disability:IN, and others.\nExpedia Groups family of brands includes: Brand Expedia , Hotels.com , Expedia Partner Solutions, Vrbo , trivago , Orbitz , Travelocity , Hotwire , Wotif , ebookers , CheapTickets , Expedia Group Media Solutions, Expedia Local Expert , CarRentals.com , and Expedia Cruises . 2024 Expedia, Inc. All rights reserved. Trademarks and logos are the property of their respective owners. . Never provide sensitive, personal information to someone unless you re confident who the recipient is. Expedia Group does not extend job offers via email or any other messaging tools to individuals with whom we have not made prior contact. Our email domain is @expediagroup.com. The official website to find and apply for job openings at Expedia Group is careers.expediagroup.com/jobs .\nExpedia is committed to creating an inclusive work environment with a diverse workforce. All qualified applicants will receive consideration for employment without regard to race, religion, gender, sexual orientation, national origin, disability or age.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Operational excellence', 'Data modeling', 'Talent acquisition', 'Analytical', 'Strategic planning', 'Data quality', 'Operations', 'Analytics', 'SQL', 'Business operations']",2025-06-12 14:41:16
DataBricks - Data Engineering Professional,Wipro,3 - 5 years,Not Disclosed,['Pune'],"Role Purpose\nThe purpose of this role is to design, test and maintain software programs for operating systems or applications which needs to be deployed at a client end and ensure its meet 100% quality assurance parameters\n\nDo\n1. Instrumental in understanding the requirements and design of the product/ software\nDevelop software solutions by studying information needs, studying systems flow, data usage and work processes\nInvestigating problem areas followed by the software development life cycle\nFacilitate root cause analysis of the system issues and problem statement\nIdentify ideas to improve system performance and impact availability\nAnalyze client requirements and convert requirements to feasible design\nCollaborate with functional teams or systems analysts who carry out the detailed investigation into software requirements\nConferring with project managers to obtain information on software capabilities\n\n\n\n2. Perform coding and ensure optimal software/ module development\nDetermine operational feasibility by evaluating analysis, problem definition, requirements, software development and proposed software\nDevelop and automate processes for software validation by setting up and designing test cases/scenarios/usage cases, and executing these cases\nModifying software to fix errors, adapt it to new hardware, improve its performance, or upgrade interfaces.\nAnalyzing information to recommend and plan the installation of new systems or modifications of an existing system\nEnsuring that code is error free or has no bugs and test failure\nPreparing reports on programming project specifications, activities and status\nEnsure all the codes are raised as per the norm defined for project / program / account with clear description and replication patterns\nCompile timely, comprehensive and accurate documentation and reports as requested\nCoordinating with the team on daily project status and progress and documenting it\nProviding feedback on usability and serviceability, trace the result to quality risk and report it to concerned stakeholders\n\n\n\n3. Status Reporting and Customer Focus on an ongoing basis with respect to project and its execution\nCapturing all the requirements and clarifications from the client for better quality work\nTaking feedback on the regular basis to ensure smooth and on time delivery\nParticipating in continuing education and training to remain current on best practices, learn new programming languages, and better assist other team members.\nConsulting with engineering staff to evaluate software-hardware interfaces and develop specifications and performance requirements\nDocument and demonstrate solutions by developing documentation, flowcharts, layouts, diagrams, charts, code comments and clear code\nDocumenting very necessary details and reports in a formal way for proper understanding of software from client proposal to implementation\nEnsure good quality of interaction with customer w.r.t. e-mail content, fault report tracking, voice calls, business etiquette etc\nTimely Response to customer requests and no instances of complaints either internally or externally\n\n\n\nDeliver\n\nNo.Performance ParameterMeasure1.Continuous Integration, Deployment & Monitoring of Software100% error free on boarding & implementation, throughput %, Adherence to the schedule/ release plan2.Quality & CSATOn-Time Delivery, Manage software, Troubleshoot queries,Customer experience, completion of assigned certifications for skill upgradation3.MIS & Reporting100% on time MIS & report generation\nMandatory Skills: DataBricks - Data Engineering. Experience: 3-5 Years.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'data bricks', 'software development life cycle', 'continuous integration', 'software development', 'mis', 'software management', 'root cause analysis']",2025-06-12 14:41:18
Data Science & AI Engineer,Blue Altair,5 - 8 years,Not Disclosed,['Pune'],"Greetings from Blue Altair!\nJob Overview:\nWe are seeking an experienced and highly skilled Data Science and AI Engineer to join our dynamic team. The ideal candidate will have 5+ years of experience working on cutting-edge data science and AI technologies across various cloud platforms with a strong focus to work on LLMs and SLMs. The role demands a professional capable of performing in a client-facing environment, as well as mentoring and guiding junior team members.\n\nTitle: Consultant/Sr. Consultant - Data Science Engineer\nExperience: 5-8 years\nLocation: Pune/Bangalore (Hybrid)\n\nRoles and responsibilities:\nDevelop, implement, and optimize machine learning models and AI algorithms to solve complex business problems.\nDesign, build, and fine-tune AI models, particularly focusing on LLMs and SLMs, using state-of-the-art techniques and architectures.\nApply advanced techniques in prompt engineering, model fine-tuning, and optimization to tailor models for specific business needs.\nDeploy and manage machine learning models and pipelines on cloud platforms (AWS, GCP, Azure, etc.).\nWork closely with clients to understand their data and AI needs and provide tailored solutions.\nCollaborate with cross-functional teams to integrate AI solutions into broader software architectures.\nMentor junior team members and provide guidance in implementing best practices in data science and AI development.\nStay up-to-date with the latest trends and advancements in data science, AI, and cloud technologies.\nPrepare technical documentation and present insights to both technical and non-technical stakeholders.\n\nRequirement:\n5+ years of experience in data science, machine learning, and AI technologies.\nProven experience working with cloud platforms such as Google Cloud, Microsoft Azure, or AWS.\nExpertise in programming languages such as Python, R, Julia, and AI frameworks like TensorFlow, PyTorch, Scikit-learn, Hugging face Transformers.\nKnowledge of data visualization tools (e.g., Matplotlib, Seaborn, Tableau)\nSolid understanding of data engineering concepts including ETL, data pipelines, and databases (SQL, NoSQL).\nExperience with MLOps practices and deployment of models in production environments.\nFamiliarity with NLP (Natural Language Processing) tasks and working with large-scale datasets.\nHands-on experience with generative AI models like GPT, Gemini, Claude, Mistral etc.\nClient-facing experience with strong communication skills to manage and engage stakeholders.\nStrong problem-solving skills and analytical mindset.\nAbility to work independently and as part of a team and mentor and provide technical leadership to junior team members.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['LLMs', 'Artificial Intelligence', 'MLOps', 'RAG', 'Natural Language Processing', 'Neural Networks', 'LLM', 'Machine Learning', 'AI Models', 'Data Science', 'PyTorch', 'SLM', 'AI Automation']",2025-06-12 14:41:21
Senior Data Engineer,Wavicle Data Solutions,6 - 11 years,15-25 Lacs P.A.,"['Chennai', 'Coimbatore', 'Bengaluru']","Hi Professionals,\n\nWe are looking for Senior Data Engineer for Permanent Role\n\nWork Location: Hybrid Chennai, Coimbatore or Bangalore\n\nExperience: 6 to 12 Years\n\nNotice Period: 0 TO 15 Days or Immediate Joiner.\n\nSkills:\n1. Python\n2. Pyspark\n3. SQL\n4. AWS\n5. GCP\n6. MLOps\n\nInterested can send your resume to gowtham.veerasamy@wavicledata.com.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'GCP', 'AWS', 'Ml']",2025-06-12 14:41:23
Data Engineer,Dun & Bradstreet,5 - 9 years,Not Disclosed,['Hyderabad'],"Key Responsibilities:\n1. Design, build, and deploy new data pipelines within our Big Data Eco-Systems using Streamsets/Talend/Informatica BDM etc. Document new/existing pipelines, Datasets.\n2. Design ETL/ELT data pipelines using StreamSets, Informatica or any other ETL processing engine. Familiarity with Data Pipelines, Data Lakes and modern Data Warehousing practices (virtual data warehouse, push down analytics etc.)\n3. Expert level programming skills on Python\n4. Expert level programming skills on Spark\n5. Cloud Based Infrastructure: GCP\n6. Experience with one of the ETL Informatica, StreamSets in creation of complex parallel loads, Cluster Batch Execution and dependency creation using Jobs/Topologies/Workflows etc.,\n7. Experience in SQL and conversion of SQL stored procedures into Informatica/StreamSets, Strong exposure working with web service origins/targets/processors/executors, XML/JSON Sources and Restful APIs.\n8. Strong exposure working with relation databases DB2, Oracle & SQL Server including complex SQL constructs and DDL generation.\n9. Exposure to Apache Airflow for scheduling jobs\n10. Strong knowledge of Big data Architecture (HDFS), Cluster installation, configuration, monitoring, cluster security, cluster resources management, maintenance, and performance tuning\n11. Create POCs to enable new workloads and technical capabilities on the Platform.\n12. Work with the platform and infrastructure engineers to implement these capabilities in production.\n13. Manage workloads and enable workload optimization including managing resource allocation and scheduling across multiple tenants to fulfill SLAs.\n14. Participate in planning activities, Data Science and perform activities to increase platform skills\n\nKey Requirements:\n1. Minimum 6 years of experience in ETL/ELT Technologies, preferably StreamSets/Informatica/Talend etc.,\n2. Minimum of 6 years hands-on experience with Big Data technologies e.g. Hadoop, Spark, Hive.\n3. Minimum 3+ years of experience on Spark\n4. Minimum 3 years of experience in Cloud environments, preferably GCP\n5. Minimum of 2 years working in a Big Data service delivery (or equivalent) roles focusing on the following disciplines:\n6. Any experience with NoSQL and Graph databases\n7. Informatica or StreamSets Data integration (ETL/ELT)\n8. Exposure to role and attribute based access controls\n9. Hands on experience with managing solutions deployed in the Cloud, preferably on GCP\n10. Experience working in a Global company, working in a DevOps model is a plus",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Airflow', 'GCP', 'Data engineer', 'Spark', 'ETL']",2025-06-12 14:41:25
Azure Cloud Data Engineering Consultant,Optum,7 - 10 years,17-27.5 Lacs P.A.,['Gurugram'],"Primary Responsibilities:\nDesign and develop applications and services running on Azure, with a strong emphasis on Azure Databricks, ensuring optimal performance, scalability, and security.\nBuild and maintain data pipelines using Azure Databricks and other Azure data integration tools.\nWrite, read, and debug Spark, Scala, and Python code to process and analyze large datasets.\nWrite extensive query in SQL and Snowflake\nImplement security and access control measures and regularly audit Azure platform and infrastructure to ensure compliance.\nCreate, understand, and validate design and estimated effort for given module/task, and be able to justify it.\nPossess solid troubleshooting skills and perform troubleshooting of issues in different technologies and environments.\nImplement and adhere to best engineering practices like design, unit testing, functional testing automation, continuous integration, and delivery.\nMaintain code quality by writing clean, maintainable, and testable code.\nMonitor performance and optimize resources to ensure cost-effectiveness and high availability.\nDefine and document best practices and strategies regarding application deployment and infrastructure maintenance.\nProvide technical support and consultation for infrastructure questions.\nHelp develop, manage, and monitor continuous integration and delivery systems.\nTake accountability and ownership of features and teamwork.\nComply with the terms and conditions of the employment contract, company policies and procedures, and any directives.\nRequired Qualifications:\nB.Tech/MCA (Minimum 16 years of formal education)\nOverall 7+ years of experience.\nMinimum of 3 years of experience in Azure (ADF), Databricks and DevOps.\n5 years of experience in writing advanced level SQL.\n2-3 years of experience in writing, reading, and debugging Spark, Scala, and Python code.\n3 or more years of experience in architecting, designing, developing, and implementing cloud solutions on Azure.\nProficiency in programming languages and scripting tools.\nUnderstanding of cloud data storage and database technologies such as SQL and NoSQL.\nProven ability to collaborate with multidisciplinary teams of business analysts, developers, data scientists, and subject-matter experts.\nFamiliarity with DevOps practices and tools, such as continuous integration and continuous deployment (CI/CD) and Teraform.\nProven proactive approach to spotting problems, areas for improvement, and performance bottlenecks.\nProven excellent communication, writing, and presentation skills.\nExperience in interacting with international customers to gather requirements and convert them into solutions using relevant skills.\nPreferred Qualifications:\nKnowledge of AI/ML or LLM (GenAI).\nKnowledge of US Healthcare domain and experience with healthcare data.\nExperience and skills with Snowflake.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Data Factory', 'Azure Databricks', 'ETL', 'SQL', 'Python', 'Airflow', 'Pyspark', 'Snowflake', 'SCALA', 'Spark', 'Data Bricks']",2025-06-12 14:41:28
Cloud Data Engineer - GCP,Synechron,2 - 3 years,Not Disclosed,"['Hyderabad', 'Gachibowli']","Job Summary\nSynechron is seeking a highly motivated and skilled Senior Cloud Data Engineer GCP to join our cloud solutions team. In this role, you will collaborate closely with clients and internal stakeholders to design, implement, and manage scalable, secure, and high-performance cloud-based data solutions on Google Cloud Platform (GCP). You will leverage your technical expertise to ensure the integrity, security, and efficiency of cloud data architectures, enabling the organization to derive maximum value from cloud data assets. This role contributes directly to our mission of delivering innovative digital transformation solutions and supports the organizations strategic objectives of scalable and sustainable cloud infrastructure.\nSoftware Requirements\nRequired Skills:\nProficiency with Google Cloud Platform (GCP) services (Compute Engine, Cloud Storage, BigQuery, Cloud Pub/Sub, Dataflow, etc.)\nBasic scripting skills with Python, Bash, or similar languages\nFamiliarity with virtualization and cloud networking concepts\nUnderstanding of cloud security best practices and compliance standards\nExperience with infrastructure as code tools (e.g., Terraform, Deployment Manager)\nStrong knowledge of data management, data pipelines, and ETL processes\nPreferred Skills:\nExperience with other cloud platforms (AWS, Azure)\nKnowledge of SQL and NoSQL databases\nFamiliarity with containerization (Docker, GKE)\nExperience with data visualization tools\nOverall Responsibilities\nDesign, implement, and operate cloud data solutions that are secure, scalable, and optimized for performance\nCollaborate with clients and internal teams to identify infrastructure and data architecture requirements\nManage and monitor cloud infrastructure and ensure operational reliability\nResolve technical issues related to cloud data workflows and storage solutions\nParticipate in project planning, timelines, and technical documentation\nContribute to best practices and continuous improvement initiatives within the organization\nEducate and support clients in adopting cloud data services and best practices\nTechnical Skills (By Category)\nProgramming Languages:\nEssential: Python, Bash scripts\nPreferred: SQL, Java, or other data processing languages\nDatabases & Data Management:\nEssential: BigQuery, Cloud SQL, Cloud Spanner, Cloud Storage\nPreferred: NoSQL databases like Firestore, MongoDB\nCloud Technologies:\nEssential: Google Cloud Platform core services (Compute, Storage, BigQuery, Dataflow, Pub/Sub)\nPreferred: Cloud monitoring, logging, and security tools\nFrameworks & Libraries:\nEssential: Data pipeline frameworks, Cloud SDKs, APIs\nPreferred: Apache Beam, Data Studio\nDevelopment Tools & Methodologies:\nEssential: Infrastructure as Code (Terraform, Deployment Manager)\nPreferred: CI/CD tools (Jenkins, Cloud Build)\nSecurity Protocols:\nEssential: IAM policies, data encryption, network security best practices\nPreferred: Compliance frameworks such as GDPR, HIPAA\nExperience Requirements\n2-3 years of experience in cloud data engineering, cloud infrastructure, or related roles\nHands-on experience with GCP is preferred; experience with AWS or Azure is a plus\nBackground in designing and managing cloud data pipelines, storage, and security solutions\nProven ability to deliver scalable data solutions in cloud environments\nExperience working with cross-functional teams on cloud deployments\nAlternative experience pathways: academic projects, certifications, or relevant internships demonstrating cloud data skills\nDay-to-Day Activities\nDevelop and deploy cloud data pipelines, databases, and analytics solutions\nCollaborate with clients and team members to plan and implement infrastructure architecture\nPerform routine monitoring, maintenance, and performance tuning of cloud data systems\nTroubleshoot technical issues affecting data workflows and resolve performance bottlenecks\nDocument system configurations, processes, and best practices\nEngage in continuous learning on new cloud features and data management tools\nParticipate in project meetings, code reviews, and knowledge sharing sessions\nQualifications\nBachelors or Masters degree in computer science, engineering, information technology, or a related field\nRelevant certifications (e.g., Google Cloud Professional Data Engineer, Cloud Architect) are preferred\nTraining in cloud security, data management, or infrastructure design is advantageous\nCommitment to professional development and staying updated with emerging cloud technologies\nProfessional Competencies\nCritical thinking and problem-solving skills to resolve complex cloud architecture challenges\nAbility to work collaboratively with multidisciplinary teams and clients\nStrong communication skills for technical documentation and stakeholder engagement\nAdaptability to evolving cloud technologies and project priorities\nOrganized with a focus on quality and detail-oriented delivery\nProactive learner with a passion for innovation in cloud data solutions\nAbility to manage multiple tasks effectively and prioritize in a fast-paced environment",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['GCP', 'Jenkins', 'Java', 'NoSQL', 'Bash scripts', 'Data Studio', 'Data Management', 'CI/CD', 'Apache Beam', 'MongoDB', 'Cloud Build']",2025-06-12 14:41:30
Data Analyst - Python/Hadoop,Sadup Soft,3 - 6 years,Not Disclosed,['Bengaluru'],"- Minimum of 3 years of hands-on experience.\n\n- Python/ML, Hadoop, Spark : Minimum of 2 years of experience.\n\n- At least 3 years of prior experience as a Data Analyst.\n\n- Detail-oriented with a structured thinking and analytical mindset.\n\n- Proven analytic skills, including data analysis, data validation, and technical writing.\n\n- Strong proficiency in SQL and Excel.\n\n- Experience with Big Query is mandatory.\n\n- Knowledge of Python and machine learning algorithms is a plus.\n\n- Excellent communication skills with the ability to be precise and clear.\n\n- Learning Ability : Ability to quickly learn and adapt to new analytic tools and technologies.\n\nKey Responsibilities :\n\nData Analysis :\n\n- Perform comprehensive data analysis using SQL, Excel, and Big Query.\n\n- Validate data integrity and ensure accuracy across datasets.\n\n- Develop detailed reports and dashboards that provide actionable insights.\n\n- Create and deliver presentations to stakeholders with clear and concise findings.\n\n- Document queries, reports, and analytical processes clearly and accurately.\n\n- Leverage Python/ML for advanced data analysis and model development.\n\n- Utilize Hadoop and Spark for handling and processing large datasets.\n\n- Work closely with cross-functional teams to understand data requirements and provide analytical support.\n\n- Communicate findings effectively and offer recommendations based on data analysis.\n\nEducation : Bachelor's degree in Computer Science, Data Science, Statistics, or a related field.\n\nExperience : Minimum of 3 years of experience as a Data Analyst with a strong focus on SQL, Excel, and Big Query.\n\nTechnical Skills : Proficiency in SQL, Excel, and Big Query; experience with Python, ML, Hadoop, and Spark is preferred.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Analysis', 'Data Validation', 'Big Query', 'Data Integrity', 'Hadoop', 'Spark', 'Python', 'SQL']",2025-06-12 14:41:33
Data Analyst III,Sadup Soft,5 - 8 years,Not Disclosed,['Bengaluru'],"- Minimum of 5 years of experience in data analysis with a strong SQL background.\n\n- Solid experience in creating and extracting metrics, and writing complex SQL scripts.\n\n- Hands-on experience with Tableau, Looker, or any equivalent data visualization tools.\n\n- Strong skills in SQL and Excel, with the ability to quickly learn other analytic tools.\n\n- Knowledge of Python and ML algorithms is a plus.\n\nResponsibilities :\n\n- Perform detailed data analysis and validation to ensure data integrity and accuracy.\n\n- Extract and create meaningful metrics to support business decisions\n\n- Design, develop, and maintain interactive dashboards using Tableau, Looker, or equivalent tools.\n\n- Translate complex data into visually appealing and actionable insights.\n\n- Document queries, reports, and analytical processes clearly and accurately.\n\n- Create detailed reports and presentations to communicate findings to stakeholders.\n\n- Work closely with cross-functional teams to understand data requirements and provide analytical support.\n\n- Communicate findings effectively and provide actionable recommendations.\n\n- Utilize SQL and Excel extensively for data analysis and reporting.\n\n- Apply Python and ML algorithms as needed for advanced analytics (preferred but not mandatory)",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Analysis', 'Data Integrity', 'Data Analyst', 'Data Visualization Tools', 'Tableau', 'Data Analytics', 'Looker']",2025-06-12 14:41:35
Senior Data Engineer,Eurofins Digital Testing,7 - 10 years,Not Disclosed,['Bengaluru'],"Job Description\nSenior Data Engineer - Quality Engineering\nExperience Range: 7-10 Years\nLocation: Bangalore (Hybrid Mode)\nResillion, a leading quality engineering company with offices around the world, is seeking a talented Data Engineer to join our growing India team. In this role, you will play a critical part in building and maintaining the data infrastructure that supports our AI-powered testing tools and analytics solutions. You will have technical responsibility for the entire data lifecycle, from data acquisition and ingestion to storage, processing, and analysis.\nResponsibilities:\nCollaborate with GenAI engineers, Software Engineers, Test automation engineers and other stakeholders within Resillion to understand data needs and translate them into technical solutions, including designing data pipelines for training & deploying models and data pre-processing for AI, including generative AI applications.\nDesign, implement, and advise on data migration testing strategies and data quality assurance strategies for Resillion customers, ensuring a smooth transition of data to new customer systems.\nDesign, develop, and implement scalable data pipelines using cloud-based data engineering tools and technologies, with a focus on both Microsoft Azure solutions (e.g., Azure Data Factory, Azure Databricks) and Google Cloud Platform (GCP) solutions (e.g., Google Cloud Dataflow, Google Cloud Dataproc).\nWrite efficient and maintainable code to extract, transform, and load data from various sources, leveraging your expertise in Azure Data Lake Storage and other relevant Azure services, as well as Google Cloud Storage and other relevant GCP services.\nBuild and manage data warehouses and data lakes for quality engineering data, utilizing your knowledge of Azure Synapse Analytics or similar technologies, and Google BigQuery or similar technologies.\nDevelop and implement data quality checks and monitoring procedures to ensure data integrity using Azure Data Catalog or other appropriate tools, as well as Google Cloud Data Catalog or other appropriate tools.\nAutomate data engineering tasks and workflows using Azure automation tools and GCP automation tools (e.g., Cloud Functions, Cloud Composer).\nSet up quality intelligence dashboards for quality assurance data using Microsoft Power BI to provide stakeholders with clear and actionable insights.\nStay up-to-date with the latest data engineering tools and technologies, including advancements in AI, Machine Learning (MLOps), and generative AI for data processing in both the Azure and GCP environments.\nAdvise on and implement test data generation strategies and solutions for various testing needs.\n\n\nQualifications\nMinimum 7+ years of experience in data engineering or a related field\nProven experience in designing, developing, and deploying data pipelines",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Engineering services', 'Manager Quality Assurance', 'GCP', 'Quality engineering', 'Machine learning', 'Quality Engineer', 'Monitoring', 'Analytics', 'SQL', 'Python']",2025-06-12 14:41:37
Senior Software Engineer - Data Engineering,Zendesk,2 - 6 years,Not Disclosed,['Bengaluru'],"Design, build, and maintain data quality systems and pipelines.\nWork with tools such as Snowflake, Docker/Kubernetes, and Kafka to enable scalable, observable data movement.\nCollaborate cross-functionally to close skill gaps in DQ and data platform tooling.\nContribute to building internal tooling that supports schema validation, data experimentation, and automated checks.\nCollaborate cross-functionally with data producers, analytics engineers, platform teams, and business stakeholders.\nOwn the reliability, scalability, and performance of ingestion systems deployed on AWS\nArchitect and build core components of our real-time ingestion platform using Kafka, Snowpipe Streaming.\nChampion software engineering excellence including testing, observability, CI/CD, and automation\nDrive the development of platform tools that ensure data quality, observability, and lineage through Protobuf-based schema management..\nParticipate in the implementation of ingestion best practices and reusable frameworks across data and software engineering teams.\nCore Skills:\nSolid programming experience (preferably in Java )\nExperience with distributed data systems ( Kafka, Snowflake )\nFamiliarity with Data Quality tooling and concepts\nGood working knowledge of SQL (especially for diagnostics and DQ workflows)\nExperience with containerization (Docker, Kubernetes )\nStrong debugging, observability, and pipeline reliability practices\n\nWhat You Bring:\nA systems mindset with strong software engineering fundamentals.\nPassion for building resilient, high-throughput, real-time platforms.\nAbility to influence technical direction across teams and drive alignment.\nStrong communication and mentoring skills.\nA bias toward automation, continuous improvement, and platform thinking.\n\nNice to Haves:\nExperience with GenAI tools or supporting ML/AI data workflows\nFamiliarity with cloud-native data platforms (e.g., AWS, GCP)\nExposure to dbt or ELT frameworks",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Automation', 'GCP', 'Quality systems', 'Debugging', 'Data quality', 'Customer service', 'Continuous improvement', 'Monitoring', 'Analytics', 'SQL']",2025-06-12 14:41:39
Senior GCP Data Engineer,Swits Digital,6 - 9 years,Not Disclosed,['Bengaluru'],"Job Title: Senior GCP Data Engineer\nLocation: Chennai, Bangalore, Hyderabad\nExperience: 6-9 Years\nJob Summary:\nWe are seeking a GCP Data & Cloud Engineer with strong expertise in Google Cloud Platform services, including BigQuery, Cloud Run, Cloud Storage , and Pub/Sub . The ideal candidate will have deep experience in SQL coding , data pipeline development, and deploying cloud-native solutions.\nKey Responsibilities:\nDesign, implement, and optimize scalable data pipelines and services using GCP\nBuild and manage cloud-native applications deployed via Cloud Run\nDevelop complex and performance-optimized SQL queries for analytics and data transformation\nManage and automate data storage, retrieval, and archival using Cloud Storage\nImplement event-driven architectures using Google Pub/Sub\nWork with large datasets in BigQuery , including ETL/ELT design and query optimization\nEnsure security, monitoring, and compliance of cloud-based systems\nCollaborate with data analysts, engineers, and product teams to deliver end-to-end cloud solutions\nRequired Skills & Experience:\n3+ years of experience working with Google Cloud Platform (GCP)\nStrong proficiency in SQL coding , query tuning, and handling complex data transformations\nHands-on experience with:\nBigQuery\nCloud Run\nCloud Storage\nPub/Sub\nUnderstanding of data pipeline and ETL/ELT workflows in cloud environments\nFamiliarity with containerized services and CI/CD pipelines\nExperience in scripting languages (e.g., Python, Shell) is a plus\nStrong analytical and problem-solving skills",Industry Type: Recruitment / Staffing,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['SUB', 'query optimization', 'GCP', 'Analytical', 'Cloud', 'query', 'cloud storage', 'Security monitoring', 'SQL coding', 'Python']",2025-06-12 14:41:41
Data & Gen AI Specialist,Altimetrik,1 - 4 years,Not Disclosed,['Bengaluru'],"Job Title: Data & GenAI AWS Specialist\nExperience: 1-4 Years\nLocation: Bangalore\nMandatory Qualification: B.E./ B.Tech/ M.Tech/ MS from IIT or IISc ONLY\nJob Overview:\nWe are seeking a seasoned Data & GenAI Specialist with deep expertise in AWS Managed Services (PaaS) to join our innovative team. The ideal candidate will have extensive experience in designing sophisticated, scalable architectures for data pipelines and Generative AI (GenAI) solutions leveraging cloud services.",,,,"['Generative Ai', 'Cloud', 'Data Science', 'Open Source', 'Data Pipeline', 'GCP', 'Azure Cloud', 'Snowflake', 'Machine Learning', 'AWS']",2025-06-12 14:41:43
Sr. Data Analyst,Icims,4 - 9 years,Not Disclosed,['Hyderabad'],"Overview\nThe Senior Data Analyst is responsible for serving as a subject matter expert who can lead efforts to analyze data with the goal of delivering insights that will influence our products and customers. This position will report into the Data Analytics Manager, and will work closely with members of our product and marketing teams, data engineers, and members of our Customer Success organization supporting client outreach efforts. The chief functions of this role will be finding and sharing data-driven insights to deliver value to less technical audiences, and instilling best practices for analytics in the rest of the team.",,,,"['server', 'data', 'vlookup', 'market data', 'data mapping', 'dashboards', 'research', 'sql', 'analytics', 'tables', 'prep', 'pivot', 'data visualization', 'communication skills', 'python', 'data analytics', 'data analysis', 'insights', 'pivot table', 'data engineering', 'graph', 'excel', 'data quality', 'tableau', 'data governance', 'root cause']",2025-06-12 14:41:46
Data Science Manager,ZS,10 - 15 years,Not Disclosed,"['Pune', 'Bengaluru']","A key enabler of our services is leveraging data in delivering client solutions. The data available about customers is getting richer and the problems that our customers are trying to answer continue to evolve. In our endeavor to stay ahead in providing solutions to these evolving complex problems, ZS has set up an Advanced Data Science which has three major focus areas:\nResearch the evolving datasets and advanced analytical techniques to develop new offerings/solutions\nDeliver client impact by collaboratively implementing these solutions",,,,"['Team management', 'data science', 'Pharma', 'Analytical', 'Management consulting', 'Financial planning', 'Healthcare', 'Project planning', 'Predictive modeling', 'Financial services']",2025-06-12 14:41:48
Data Analyst,B.M. House India limited,2 - 4 years,3.5-4.5 Lacs P.A.,['Bengaluru( HSR Layout )'],"Microsoft Excel (including PivotTables, VLOOKUP/XLOOKUP, Power Query, Macros, and Charts) to analyze and present data. CANVA and Photoshop experience and digital marketing ensure data accuracy and integrity at all times.",Industry Type: Import & Export,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Canva', 'Advanced Excel', 'Power Bi', 'VLOOKUP', 'Data Management', 'Data Analysis', 'Pivot', 'Digital Marketing', 'Data Reporting']",2025-06-12 14:41:51
Senior Data Manager/ Lead,Codeforce 360,6 - 8 years,Not Disclosed,['Hyderabad'],"Job Description:\nWe are looking for a highly experienced and dynamic Senior Data Manager / Lead to oversee a team of Data Engineers and Data Scientists. This role demands a strong background in data platforms such as Snowflake and proficiency in Python, combined with excellent people management and project leadership skills. While hands-on experience in the technologies is beneficial, the primary focus of this role is on team leadership, strategic planning, and project delivery .\n\nJob Title : Senior Data Manager / Lead\nLocation: Hyderabad (Work From Office)\nShift Timing: 10AM-7PM\nKey Responsibilities:\nLead, mentor, and manage a team of Data Engineers and Data Scientists.\nOversee the design and implementation of data pipelines and analytics solutions using Snowflake and Python.\nCollaborate with cross-functional teams (product, business, engineering) to align data solutions with business goals.\nEnsure timely delivery of projects, with high quality and performance.\nConduct performance reviews, training plans, and support career development for the team.\nSet priorities, allocate resources, and manage workloads within the data team.\nDrive adoption of best practices in data management, governance, and documentation.\nEvaluate new tools and technologies relevant to data engineering and data science.\n\nRequired Skills & Qualifications:\n6+ years of experience in data-related roles, with at least 23 years in a leadership or management position.\nStrong understanding of Snowflake architecture, performance tuning, data sharing, security, etc.\nSolid knowledge of Python for data engineering or data science tasks.\nExperience in leading data migration, ETL/ELT, and analytics projects.\nAbility to translate business requirements into technical solutions.\nExcellent leadership, communication, and stakeholder management skills.\nExposure to tools like Databricks, Dataiku, Airflow, or similar platforms is a plus.\nBachelors or Master’s degree in Computer Science, Engineering, Mathematics, or a related field.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Snowflake', 'Data Bricks', 'Python', 'Airflow', 'Data Migration', 'Dataiku', 'Data Warehousing', 'ETL', 'ELT', 'SQL']",2025-06-12 14:41:53
Associate Specialist Data Science,Merck Sharp & Dohme (MSD),2 - 7 years,Not Disclosed,['Pune'],"Primary Responsibilities\nSupport in establishing frameworks to standardize, productize and scale existing and new capabilities / analytical solutions\nImplement the vision, roadmap, and best practices for the Data Science Center of Excellence ( CoE ) to align with business goals\nSupport establishing governance frameworks to measure the value of products, standardize data science methodologies, coding practices, and project workflows\nWork with senior CoE members in development and maintenance of best practices for model and algorithm development and design, deployment, and monitoring across the enterprise functions\nCollaborate with product team on product development incorporating Agile framework and latest industry best practices and norms\nSupport in development of MLOps and ModelOps frameworks to streamline the development-to-deployment product pipeline\nDrive innovation by identifying, evaluating, and implementing cutting-edge data science methodologies based on latest published literature\n\nQualifications\nEducation & Work Experience Requiremen ts:\nMaster s degree (relevant field like Economics, Statistics, Mathematics, Operational Research) with 2+ years work experience.\nBachelor s degree (in Engineering or related field, such as Computer Science, Data Science, Statistics, Business, etc.) with at least 3 + years relevant experience\nPrior experience in research publications in reputed journal is a plus\nSkillset:\nCandidates must have -\nStrong programming skills in languages such as Python or R, and SQL with experience in data manipulation and analysis libraries (e.g., pandas, NumPy, scikit-learn, stats models)\nExperience with data science principles, machine learning (supervised and unsupervised) and GenAI algorithms, test-control analysis, propensity score matching etc.\nExposure to product roadmaps, Agile methodologies and backlog management, ensuring iterative and incremental product improvements\nStrong problem solving, business analysis and quantitative skills\nAbility to effectively communicate proposals to key stakeholders\nCandidates are desired but not mandatory to have -\nExperience and familiarity with underlying concepts such as Patient analytics, MMx etc.\nUnderstanding of Pharma commercial landscape will be a plus\nExperience working with healthcare, financial, or enterprise SaaS products\n  Search Firm Representatives Please Read Carefully\nEmployee Status:\nRegular\nRelocation:\nVISA Sponsorship:\nTravel Requirements:\nFlexible Work Arrangements:\nNot Applicable\nShift:\nValid Driving License:\nHazardous Material(s):\n\nRequired Skills:\nBusiness Intelligence (BI), Database Design, Data Engineering, Data Modeling, Data Science, Data Visualization, Machine Learning, Software Development, Stakeholder Relationship Management, Waterfall Model",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Relationship management', 'Business analysis', 'Coding', 'Pharma', 'Analytical', 'Healthcare', 'Business intelligence', 'Analytics', 'Monitoring', 'SQL']",2025-06-12 14:41:55
Data Engineer-Having Stratup-Mid-Size company Exp.@ Bangalore_Urgent,"As a leader in this space, we deliver wo...",8 - 13 years,Not Disclosed,['Bengaluru'],"Data Engineer\n\nLocation: Bangalore - Onsite\nExperience: 8 - 15 years\nType: Full-time\n\nRole Overview\n\nWe are seeking an experienced Data Engineer to build and maintain scalable, high-performance data pipelines and infrastructure for our next-generation data platform. The platform ingests and processes real-time and historical data from diverse industrial sources such as airport systems, sensors, cameras, and APIs. You will work closely with AI/ML engineers, data scientists, and DevOps to enable reliable analytics, forecasting, and anomaly detection use cases.\nKey Responsibilities\nDesign and implement real-time (Kafka, Spark/Flink) and batch (Airflow, Spark) pipelines for high-throughput data ingestion, processing, and transformation.\nDevelop data models and manage data lakes and warehouses (Delta Lake, Iceberg, etc) to support both analytical and ML workloads.\nIntegrate data from diverse sources: IoT sensors, databases (SQL/NoSQL), REST APIs, and flat files.\nEnsure pipeline scalability, observability, and data quality through monitoring, alerting, validation, and lineage tracking.\nCollaborate with AI/ML teams to provision clean and ML-ready datasets for training and inference.\nDeploy, optimize, and manage pipelines and data infrastructure across on-premise and hybrid environments.\nParticipate in architectural decisions to ensure resilient, cost-effective, and secure data flows.\nContribute to infrastructure-as-code and automation for data deployment using Terraform, Ansible, or similar tools.\n\n\nQualifications & Required Skills\n\nBachelors or Master’s in Computer Science, Engineering, or related field.\n6+ years in data engineering roles, with at least 2 years handling real-time or streaming pipelines.\nStrong programming skills in Python/Java and SQL.\nExperience with Apache Kafka, Apache Spark, or Apache Flink for real-time and batch processing.\nHands-on with Airflow, dbt, or other orchestration tools.\nFamiliarity with data modeling (OLAP/OLTP), schema evolution, and format handling (Parquet, Avro, ORC).\nExperience with hybrid/on-prem and cloud platforms (AWS/GCP/Azure) deployments.\nProficient in working with data lakes/warehouses like Snowflake, BigQuery, Redshift, or Delta Lake.\nKnowledge of DevOps practices, Docker/Kubernetes, Terraform or Ansible.\nExposure to data observability, data cataloging, and quality tools (e.g., Great Expectations, OpenMetadata).\nGood-to-Have\nExperience with time-series databases (e.g., InfluxDB, TimescaleDB) and sensor data.\nPrior experience in domains such as aviation, manufacturing, or logistics is a plus.\n\nRole & responsibilities\n\n\nPreferred candidate profile",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['aviation', 'Data Modeling', 'Python', 'OLAP', 'Cloud', 'ORC', 'logistics', 'Avro', 'Terraform', 'Snowflake', 'manufacturing', 'AWS', 'Parquet', 'Java', 'Azure', 'BigQuery', 'Data', 'Redshift', 'SQL', 'TimescaleDB', 'GCP', 'InfluxDB', 'dbt', 'Ansible', 'OLTP', 'Kubernetes']",2025-06-12 14:41:57
Data Engineer - Databricks,KPI Partners,3 - 6 years,Not Disclosed,"['Hyderabad', 'Pune', 'Bengaluru']","About KPI Partners.\nKPI Partners is a leading provider of data analytics solutions, dedicated to helping organizations transform data into actionable insights. Our innovative approach combines advanced technology with expert consulting, allowing businesses to leverage their data for improved performance and decision-making.\n\nJob Description.\nWe are seeking a skilled and motivated Data Engineer with experience in Databricks to join our dynamic team. The ideal candidate will be responsible for designing, building, and maintaining scalable data pipelines and data processing solutions that support our analytics initiatives. You will collaborate closely with data scientists, analysts, and other engineers to ensure the consistent flow of high-quality data across our platforms.",,,,"['python', 'data analytics', 'analytical', 'scala', 'pyspark', 'microsoft azure', 'data warehousing', 'data pipeline', 'data architecture', 'data engineering', 'sql', 'data bricks', 'cloud', 'analytics', 'data quality', 'data modeling', 'gcp', 'teamwork', 'integration', 'aws', 'etl', 'programming', 'communication skills', 'etl scripts']",2025-06-12 14:42:00
Data Engineer - Databricks,Inorg,2 - 5 years,Not Disclosed,"['Kolkata', 'Mumbai', 'New Delhi', 'Hyderabad', 'Pune', 'Chennai', 'Bengaluru']",InOrg Global is looking for Data Engineer - Databricks to join our dynamic team and embark on a rewarding career journey.\n\nLiaising with coworkers and clients to elucidate the requirements for each task.\nConceptualizing and generating infrastructure that allows big data to be accessed and analyzed.\nReformulating existing frameworks to optimize their functioning.\nTesting such structures to ensure that they are fit for use.\nPreparing raw data for manipulation by data scientists.\nDetecting and correcting errors in your work.\nEnsuring that your work remains backed up and readily accessible to relevant coworkers.\nRemaining up - to - date with industry standards and technological advancements that will improve the quality of your outputs.,Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent",['Data Engineer - Databricks'],2025-06-12 14:42:02
Data Engineer,Talent Aspire,2 - 7 years,Not Disclosed,"['Chandigarh', 'Bengaluru']","As the Data Engineer, you will play a pivotal role in shaping our data infrastructure and\nexecuting against our strategy. You will ideate alongside engineering, data and our clients to\ndeploy data products with an innovative and meaningful impact to clients. You will design, build, and maintain scalable data pipelines and workflows on AWS. Additionally, your expertise in AI and machine learning will enhance our ability to deliver smarter, more predictive solutions.\n\nKey Responsibilities\nCollaborate with other engineers, customers to brainstorm and develop impactful data\nproducts tailored to our clients.\nLeverage AI and machine learning techniques to integrate intelligent features into our\nofferings.\nDevelop, and optimize end-to-end data pipelines on AWS\nFollow best practices in software architecture and development.\nImplement effective cost management and performance optimization strategies.\nDevelop and maintain systems using Python, SQL, PySpark, and Django for front-end\ndevelopment.\nWork directly with clients and end-users and address their data needs\nUtilize databases and tools including and not limited to, Postgres, Redshift, Airflow, and\nMongoDB to support our data ecosystem.\nLeverage AI frameworks and libraries to integrate advanced analytics into our solutions.\nQualifications\n\nExperience:\nMinimum of 3 years of experience in data engineering, software development, or\nrelated roles.\nProven track record in designing and deploying AWS cloud infrastructure\nsolutions\nAt least 2 years in data analysis and mining techniques to aid in descriptive and\ndiagnostic insights\nExtensive hands-on experience with Postgres, Redshift, Airflow, MongoDB, and\nreal-time data workflows.\n\nTechnical Skills:\nExpertise in Python, SQL, and PySpark\nStrong background in software architecture and scalable development practices.\nTableau, Metabase or similar viz tools experience\nWorking knowledge of AI frameworks and libraries is a plus.\nLeadership & Communication:\nDemonstrates ownership and accountability for delivery with a strong\ncommitment to quality.\nExcellent communication skills with a history of effective client and end-user\nengagement.\nStartup & Fintech Mindset:\nAdaptability and agility to thrive in a fast-paced, early-stage startup environment.\nPassion for fintech innovation and a strong desire to make a meaningful impact\non the future of finance.",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Engineering', 'performance optimization strategies', 'PySpark', 'Django', 'cost management', 'AWS', 'AI frameworks', 'Python', 'SQL']",2025-06-12 14:42:04
Lead Big Data Engineer - Python & Spark,Hubnex,7 - 12 years,Not Disclosed,['Bengaluru'],"Job Title: Lead Big Data Engineer - Python & Spark\nLocation: Gurgaon, India\nExperience: 7+ Years\nEmployment Type: Full-Time | Onsite\nDepartment: Data Engineering\nAbout Hubnex Labs:\nHubnex Labs is a forward-looking IT consulting and software services company, building next-generation data platforms, AI systems, and enterprise-grade applications. We re looking for a Lead Big Data Engineer to drive the development and deployment of high-performance data processing solutions.\nRole Overview:\nAs a Lead Big Data Engineer , you will be responsible for architecting and implementing scalable big data solutions using Spark, Python, Hive, and related technologies. You will mentor a team of developers and work closely with cross-functional stakeholders to ensure on-time, error-free software delivery.\nKey Responsibilities:\nLead and mentor a team of Python and big data developers to deliver robust data-driven applications\nDesign, develop, and maintain scalable data processing pipelines using Spark (Scala/PySpark), Hive, and Hadoop ecosystems\nWrite efficient, reusable, and well-documented code in Python, SQL, and shell scripting\nOptimize Spark applications for performance and scalability; tune existing Hadoop-based systems\nCollaborate with QA, DevOps, and business teams to ensure high-quality software delivery\nPerform code reviews , enforce coding standards, and contribute to overall architectural decisions\nActively participate in daily Scrum meetings , sprint planning, retrospectives, and release cycles\nTroubleshoot complex issues across the full data pipeline\nRequired Qualifications:\n7+ years of hands-on experience in software development, with a strong background in big data frameworks\nDeep expertise in Hadoop ecosystem including HDFS, Hive, HQL, Spark (Scala/PySpark), Sqoop\n4+ years of programming experience using Python , SQL , and Unix shell scripting\nProven experience in leading development teams and delivering enterprise-level solutions\nStrong grasp of Spark architecture , performance tuning, and data frame APIs\nExcellent understanding of database concepts , data structures , and distributed systems\nBachelors degree in Computer Science , Engineering , or a related field\nExceptional communication, problem-solving, and leadership skills\nPreferred Skills:\nExperience with CI/CD pipelines for data projects\nFamiliarity with cloud-based data platforms (AWS EMR, GCP DataProc, or Azure HDInsight)\nWorking knowledge of Kafka , Airflow , or other data orchestration tools\nWhy Join Hubnex Labs?\nWork on mission-critical big data solutions that impact businesses globally\nBe part of an innovative, agile, and supportive team\nLeadership opportunities and exposure to latest technologies in AI and cloud computing\nCompetitive salary, performance incentives, and professional growth support",Industry Type: Internet,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Performance tuning', 'Cloud computing', 'Coding', 'Agile', 'Data structures', 'Scrum', 'Unix shell scripting', 'SQL', 'Python']",2025-06-12 14:42:06
Data Engineering - Senior Developer with Salesforce,Job Delights,5 - 10 years,25-27.5 Lacs P.A.,"['Hyderabad', 'Chennai', 'Bengaluru']","Data Engineering with SQL, Python, ETL & SalesForce Marketing Cloud (Must)",Industry Type: Software Product,Department: Other,"Employment Type: Full Time, Permanent","['Salesforce Marketing Cloud', 'SQL', 'Marketing Cloud', 'Salesforce', 'Python']",2025-06-12 14:42:08
HIH - Data Science Lead Analyst - Evernorth,Cigna Medical Group,5 - 8 years,Not Disclosed,['Hyderabad'],"Role Summary\nAs a member of the Data Science Center of Expertise (DSCOE), the DS Lead Analyst is responsible for leading and enabling Data Science within Cigna Group with demonstrable aptitude in Data Science (i) Technical Skills (ii) Leadership (iii) Scope & Impact (iv) Influence. Please see Qualifications section below for more details.\n\nThe role will support the development and maintenance of machine learning models, with a focus on ensuring that models meet Cigna requirements for governance and legal compliance. The role will require collaboration with other data scientists and involve work across many lines of business.\nKey Responsibilities:\nAnalyze model performance of new models with specific regards to requirements for legal compliance and governance standards around accuracy and bias;\nPerform periodic analyses of performance of existing models to ensure continued compliance with internal and external standards for accuracy and bias;\nConduct research (i.e. literature review) to understand when bias may be biologically or medically justifiable, and to what degree, for example: finding evidence from literature that heart disease is more prevalent among older populations\nUsing machine learning development tools to mitigate model bias when this is determined to be necessary\nCollaborating with data scientists, business stakeholders, and governance/compliance teams to ensure models meet compliance and governance standards\nQualifications:\nBachelors or Masters/PhD (preferred) in statistics or computer science or equivalent field with 5-8 years of relevant experience\nStrong proficiency in ML, statistics, python or R, SQL, version control (e.g., Git), health care data (e.g., claims, EHR)\nAbility to promote best coding practices, championing a culture of documentation/logging\nThorough understanding of ML lifecycle, including necessary tradeoffs and associated risks\nLeadership in Data Science\nCan own a project end-to-end e.g., scoping, business value estimation, ideation, dev, prod, timeline\nCollaborates and guides junior team members in completion of projects and career development\nWorks cross functionally with technical (e.g., Data Science, Data Engineering) and business (e.g., clinical, marketing, pricing, business analysts) to implement solutions with measurable value\nScope and Impact\nIndependently delivers clear and well-developed presentations for both technical and business audiences\nCreates data science specific project goals associated with project deliverables\nArticulates timeline changes, rationale, and goals to meet deadlines moving forward\nValues diversity, growth mindset, and improving health outcomes of our customers\n\nLevel of Influence\nCommunicate with stakeholders to identify opportunities and possible solutions based on business need\nDraft project charter, timeline, and features/stories\nInfluence matrix-partner leadership",Industry Type: Medical Services / Hospital,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Version control', 'GIT', 'Claims', 'data science', 'Legal compliance', 'Coding', 'Machine learning', 'SQL', 'Python']",2025-06-12 14:42:10
Senior Data Engineer,The Main Stage Productions,4 - 6 years,Not Disclosed,['Bengaluru'],"Design and implement cloud-native data architectures on AWS, including data lakes, data warehouses, and streaming pipelines using services like S3, Glue, Redshift, Athena, EMR, Lake Formation, and Kinesis.\nDevelop and orchestrate ETL/ELT pipelines\n\nRequired Candidate profile\nParticipate in pre-sales and consulting activities such as:\nEngaging with clients to gather requirements and propose AWS-based data engineering solutions.\nSupporting RFPs/RFIs, technical proposals",Industry Type: Advertising & Marketing,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['AWS Glue', 'GitHub Actions', 'PySpark', 'Scala', 'CodePipeline', 'Step Functions', 'data engineering']",2025-06-12 14:42:13
Azure Data Engineer/Lead/Architect (5 - 20 Years) (Pan India Location),Allegis Group,5 - 10 years,Not Disclosed,[],"Azure Data Engineer/Lead/Architect (5 - 20 Years) (Pan India Location)\nJob Location : Hyderabad / Bangalore / Chennai / Kolkata / Noida/ Gurgaon / Pune / Indore / Mumbai\n\n\n5 -20 years of relevant hands on development experience. And 4+ years as Azure Data Engineering role\nProficient in Azure technologies like ADB, ADF, SQL(capability of writing complex SQL queries), ADB, PySpark, Python, Synapse, Delta Tables, Unity Catalog\nHands on in Python, PySpark or Spark SQL\nHands on in Azure Analytics and DevOps\nTaking part in Proof of Concepts (POCs) and pilot solutions preparation\nAbility to conduct data profiling, cataloguing, and mapping for technical design and construction of technical data flows\nExperience in business processing mapping of data and analytics solutions",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Analytics', 'Azure Data Engineering', 'Azure Databricks', 'Devops', 'Python', 'Azure Data Factory', 'Pyspark', 'Azure', 'Adb']",2025-06-12 14:42:16
Data Engineer with Neo4j,Luxoft,3 - 5 years,Not Disclosed,['Gurugram'],"Graph Data Modeling & Implementation.\nDesign and implement complex graph data models using Cypher and Neo4j best practices.\nLeverage APOC procedures, custom plugins, and advanced graph algorithms to solve domain-specific problems.\nOversee integration of Neo4j with other enterprise systems, microservices, and data platforms.\nDevelop and maintain APIs and services in Java, Python, or JavaScript to interact with the graph database.\nMentor junior developers and review code to maintain high-quality standards.\nEstablish guidelines for performance tuning, scalability, security, and disaster recovery in Neo4j environments.\nWork with data scientists, analysts, and business stakeholders to translate complex requirements into graph-based solutions.\nSkills\nMust have\n12+ years in software/data engineering, with at least 3-5 years hands-on experience with Neo4j.\nLead the technical strategy, architecture, and delivery of Neo4j-based solutions.\nDesign, model, and implement complex graph data structures using Cypher and Neo4j best practices.\nGuide the integration of Neo4j with other data platforms and microservices.\nCollaborate with cross-functional teams to understand business needs and translate them into graph-based models.\nMentor junior developers and ensure code quality through reviews and best practices.\nDefine and enforce performance tuning, security standards, and disaster recovery strategies for Neo4j.\nStay up-to-date with emerging technologies in the graph database and data engineering space.\nStrong proficiency in Cypher query language, graph modeling, and data visualization tools (e.g., Bloom, Neo4j Browser).\nSolid background in Java, Python, or JavaScript and experience integrating Neo4j with these languages.\nExperience with APOC procedures, Neo4j plugins, and query optimization.\nFamiliarity with cloud platforms (AWS) and containerization tools (Docker, Kubernetes).\nProven experience leading engineering teams or projects.\nExcellent problem-solving and communication skills.\nNice to have\nN/A\nOther\nLanguages\nEnglish: C1 Advanced\nSeniority\nSenior\nRefer a Friend\nPositive work environments and stellar reputations attract and retain top talent. Find out why Luxoft stands apart from the rest.\nRecommend a friend\nRelated jobs View all vacancies\nSenior Flexera Data Analyst\nData Science\nIndia\nBengaluru\nSenior Flexera Data Analyst\nData Science\nIndia\nChennai\nData Scientist\nData Science\nIndia\nBengaluru\nGurugram, India\nReq. VR-114556\nData Science\nBCM Industry\n23/05/2025\nReq. VR-114556\nApply for Data Engineer with Neo4j in Gurugram\n*",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Performance tuning', 'query optimization', 'neo4j', 'data science', 'Data modeling', 'Disaster recovery', 'Javascript', 'Data structures', 'data visualization', 'Python']",2025-06-12 14:42:18
Consultant - Lead Data Engineer,Affine Analytics,5 - 8 years,Not Disclosed,['Bengaluru'],"Strong experience with Python, SQL, pySpark, AWS Glue. Good to have - Shell Scripting, Kafka\nGood knowledge of DevOps pipeline usage (Jenkins, Bitbucket, EKS, Lightspeed)\nExperience of AWS tools (AWS S3, EC2, Athena, Redshift, Glue, EMR, Lambda, RDS, Kinesis, DynamoDB, QuickSight etc.).\nOrchestration using Airflow\nGood to have - Streaming technologies and processing engines, Kinesis, Kafka, Pub/Sub and Spark Streaming\nGood debugging skills",,,,"['Python', 'RDS', 'Shell Scripting', 'Kafka', 'AWS Glue', 'DynamoDB', 'Lightspeed', 'EMR', 'EKS', 'pySpark', 'Redshift', 'SQL', 'Jenkins', 'QuickSight', 'Glue', 'EC2', 'Kinesis', 'AWS S3', 'Bitbucket', 'Athena', 'Lambda']",2025-06-12 14:42:21
Data Engineer with Neo4j,Luxoft,3 - 5 years,Not Disclosed,['Bengaluru'],"Graph Data Modeling & Implementation.\nDesign and implement complex graph data models using Cypher and Neo4j best practices.\nLeverage APOC procedures, custom plugins, and advanced graph algorithms to solve domain-specific problems.\nOversee integration of Neo4j with other enterprise systems, microservices, and data platforms.\nDevelop and maintain APIs and services in Java, Python, or JavaScript to interact with the graph database.\nMentor junior developers and review code to maintain high-quality standards.\nEstablish guidelines for performance tuning, scalability, security, and disaster recovery in Neo4j environments.\nWork with data scientists, analysts, and business stakeholders to translate complex requirements into graph-based solutions.\nSkills\nMust have\n12+ years in software/data engineering, with at least 3-5 years hands-on experience with Neo4j.\nLead the technical strategy, architecture, and delivery of Neo4j-based solutions.\nDesign, model, and implement complex graph data structures using Cypher and Neo4j best practices.\nGuide the integration of Neo4j with other data platforms and microservices.\nCollaborate with cross-functional teams to understand business needs and translate them into graph-based models.\nMentor junior developers and ensure code quality through reviews and best practices.\nDefine and enforce performance tuning, security standards, and disaster recovery strategies for Neo4j.\nStay up-to-date with emerging technologies in the graph database and data engineering space.\nStrong proficiency in Cypher query language, graph modeling, and data visualization tools (e.g., Bloom, Neo4j Browser).\nSolid background in Java, Python, or JavaScript and experience integrating Neo4j with these languages.\nExperience with APOC procedures, Neo4j plugins, and query optimization.\nFamiliarity with cloud platforms (AWS) and containerization tools (Docker, Kubernetes).\nProven experience leading engineering teams or projects.\nExcellent problem-solving and communication skills.\nNice to have\nN/A\nOther\nLanguages\nEnglish: C1 Advanced\nSeniority\nSenior\nRefer a Friend\nPositive work environments and stellar reputations attract and retain top talent. Find out why Luxoft stands apart from the rest.\nRecommend a friend\nRelated jobs View all vacancies\nSenior Flexera Data Analyst\nData Science\nIndia\nGurugram\nSenior Flexera Data Analyst\nData Science\nIndia\nChennai\nBusiness Analyst\nData Science\nPoland\nRemote Poland\nBengaluru, India\nReq. VR-114556\nData Science\nBCM Industry\n23/05/2025\nReq. VR-114556\nApply for Data Engineer with Neo4j in Bengaluru\n*",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Performance tuning', 'query optimization', 'neo4j', 'data science', 'Data modeling', 'Disaster recovery', 'Javascript', 'Data structures', 'data visualization', 'Python']",2025-06-12 14:42:23
Data Engineer with Neo4j,Luxoft,3 - 5 years,Not Disclosed,['Chennai'],"Graph Data Modeling & Implementation.\nDesign and implement complex graph data models using Cypher and Neo4j best practices.\nLeverage APOC procedures, custom plugins, and advanced graph algorithms to solve domain-specific problems.\nOversee integration of Neo4j with other enterprise systems, microservices, and data platforms.\nDevelop and maintain APIs and services in Java, Python, or JavaScript to interact with the graph database.\nMentor junior developers and review code to maintain high-quality standards.\nEstablish guidelines for performance tuning, scalability, security, and disaster recovery in Neo4j environments.\nWork with data scientists, analysts, and business stakeholders to translate complex requirements into graph-based solutions.\nSkills\nMust have\n12+ years in software/data engineering, with at least 3-5 years hands-on experience with Neo4j.\nLead the technical strategy, architecture, and delivery of Neo4j-based solutions.\nDesign, model, and implement complex graph data structures using Cypher and Neo4j best practices.\nGuide the integration of Neo4j with other data platforms and microservices.\nCollaborate with cross-functional teams to understand business needs and translate them into graph-based models.\nMentor junior developers and ensure code quality through reviews and best practices.\nDefine and enforce performance tuning, security standards, and disaster recovery strategies for Neo4j.\nStay up-to-date with emerging technologies in the graph database and data engineering space.\nStrong proficiency in Cypher query language, graph modeling, and data visualization tools (e.g., Bloom, Neo4j Browser).\nSolid background in Java, Python, or JavaScript and experience integrating Neo4j with these languages.\nExperience with APOC procedures, Neo4j plugins, and query optimization.\nFamiliarity with cloud platforms (AWS) and containerization tools (Docker, Kubernetes).\nProven experience leading engineering teams or projects.\nExcellent problem-solving and communication skills.\nNice to have\nN/A\nOther\nLanguages\nEnglish: C1 Advanced\nSeniority\nSenior\nRefer a Friend\nPositive work environments and stellar reputations attract and retain top talent. Find out why Luxoft stands apart from the rest.\nRecommend a friend\nRelated jobs View all vacancies\nSenior Flexera Data Analyst\nData Science\nIndia\nGurugram\nSenior Flexera Data Analyst\nData Science\nIndia\nBengaluru\nData Scientist\nData Science\nIndia\nBengaluru\nChennai, India\nReq. VR-114556\nData Science\nBCM Industry\n23/05/2025\nReq. VR-114556\nApply for Data Engineer with Neo4j in Chennai\n*",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Performance tuning', 'query optimization', 'neo4j', 'data science', 'Data modeling', 'Disaster recovery', 'Javascript', 'Data structures', 'data visualization', 'Python']",2025-06-12 14:42:25
Data Engineer,Supersourcing,4 - 8 years,Not Disclosed,"['Noida', 'Pune', 'Bengaluru']","Job Description-\nWe are hiring a Data Engineer with strong expertise in JAVA, Apache Spark and AWS Cloud. You will design and develop high-performance, scalable applications and data pipelines for cloud-based environments.\n\nKey Skills:\n3+ years in Java (Java 8+), Spring Boot, and REST APIs\n3+ years in Apache Spark (Core, SQL, Streaming)\nStrong hands-on with AWS services: S3, EC2, Lambda, Glue, EMR\nExperience with microservices, CI/CD, and Git\nGood understanding of distributed systems and performance tuning\n\nNice to Have:\nExperience with Kafka, Airflow, Docker, or Kubernetes\nAWS Certification (Developer/Architect)",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Java', 'Spark', 'Pyspark', 'SQL']",2025-06-12 14:42:28
Data Engineer,Lenskart,1 - 4 years,Not Disclosed,['Bengaluru'],"Key Responsibilities\nBuild and maintain scalable ETL/ELT data pipelines using Python and cloud-native tools.\nDesign and optimize data models and queries on Google BigQuery for analytical workloads.\nDevelop, schedule, and monitor workflows using orchestration tools like Apache Airflow or Cloud Composer.\nIngest and integrate data from multiple structured and semi-structured sources, including MySQL, MongoDB, APIs, and cloud storage.",,,,"['GCP', 'Bigquery', 'MySQL', 'MongoDB', 'Python']",2025-06-12 14:42:30
IT Manager - Data Engineering & Analytics,ZS,12 - 15 years,Not Disclosed,['Pune'],"IT MANAGER, DATA ENGINEERING AND ANALYTICS will lead a team of data engineers and analysts responsible for designing, developing, and maintaining robust data systems and integrations. This role is critical for ensuring the smooth collection, transformation, integration and visualization of data, making it easily accessible for analytics and decision-making across the organization. The Manager will collaborate closely with analysts, developers, business leaders and other stakeholders to ensure that the data infrastructure meets business needs and is scalable, reliable, and efficient.\n",,,,"['Data modeling', 'Project management', 'Analytical', 'Financial planning', 'Management consulting', 'Data quality', 'Troubleshooting', 'Stakeholder management', 'Analytics', 'SQL']",2025-06-12 14:42:33
Data Engineer,Luxoft,5 - 8 years,Not Disclosed,['Pune'],"Help Group Enterprise Architecture team to develop our suite of EA tools and workbenches\nWork in the development team to support the development of portfolio health insights\nBuild data applications from cloud infrastructure to visualization layer\nProduce clear and commented code\nProduce clear and comprehensive documentation\nPlay an active role with technology support teams and ensure deliverables are completed or escalated on time\nProvide support on any related presentations, communications, and trainings\nBe a team player, working across the organization with skills to indirectly manage and influence\nBe a self-starter willing to inform and educate others\nSkills\nMust have\nB.Sc./M.Sc. degree in computing or similar\n5-8+ years experience as a Data Engineer, ideally in a large corporate environment\nIn-depth knowledge of SQL and data modelling/data processing\nStrong experience working with Microsoft Azure\nExperience with visualisation tools like PowerBI (or Tableau, QlikView or similar)\nExperience working with Git, JIRA, GitLab\nStrong flair for data analytics\nStrong flair for IT architecture and IT architecture metrics\nExcellent stakeholder interaction and communication skills\nUnderstanding of performance implications when making design decisions to deliver performant and maintainable software.\nExcellent end-to-end SDLC process understanding.\nProven track record of delivering complex data apps on tight timelines\nFluent in English both written and spoken.\nPassionate about development with focus on data and cloud\nAnalytical and logical, with strong problem solving skills\nA team player, comfortable with taking the lead on complex tasks\nAn excellent communicator who is adept in, handling ambiguity and communicating with both technical and non-technical audiences\nComfortable with working in cross-functional global teams to effect change\nPassionate about learning and developing your hard and soft professional skills\nNice to have\nExperience working in the financial industry\nExperience in complex metrics design and reporting\nExperience in using artificial intelligence for data analytics\nOther\nLanguages\nEnglish: C1 Advanced\nSeniority\nSenior\nRefer a Friend\nPositive work environments and stellar reputations attract and retain top talent. Find out why Luxoft stands apart from the rest.\nRecommend a friend\nRelated jobs View all vacancies\nSenior Power BI Developer\nBI Engineering\nIndia\nBengaluru\nSenior Power BI Developer\nBI Engineering\nIndia\nChennai\nSenior Power BI Developer\nBI Engineering\nIndia\nGurugram\nPune, India\nReq. VR-114797\nBI Engineering\nBCM Industry\n02/06/2025\nReq. VR-114797\nApply for Data Engineer in Pune\n*",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['GIT', 'Enterprise architecture', 'Analytical', 'Artificial Intelligence', 'Data processing', 'Data analytics', 'QlikView', 'JIRA', 'SDLC', 'SQL']",2025-06-12 14:42:35
Data Engineer KL-BL,Puresoftware,5 - 12 years,Not Disclosed,['Bengaluru'],"Core Competences Required and Desired Attributes:\nBachelors degree in computer science, Information Technology, or a related field.\nProficiency in Azure Data Factory, Azure Databricks and Unity Catalog, Azure SQL Database, and other Azure data services.\nStrong programming skills in SQL, Python and PySpark languages.\nExperience in the Asset Management domain would be preferable.\nStrong proficiency in data analysis and data modelling, with the ability to extract insights from complex data sets.\nHands-on experience in Power BI, including creating custom visuals, DAX expressions, and data modelling.\nFamiliarity with Azure Analysis Services, data modelling techniques, and optimization.\nExperience with data quality and data governance frameworks with an ability to debug, fine tune and optimise large scale data processing jobs.\nStrong analytical and problem-solving skills, with a keen eye for detail.\nExcellent communication and interpersonal skills, with the ability to work collaboratively in a team environment.\nProactive and self-motivated, with the ability to manage multiple tasks and deliver high-quality results within deadlines.",Industry Type: Management Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data analysis', 'Interpersonal skills', 'Data modeling', 'Analytical', 'data governance', 'Data quality', 'Asset management', 'Information technology', 'SQL', 'Python']",2025-06-12 14:42:37
Data Engineer,Axis Finance (AFL),7 - 11 years,Not Disclosed,"['Mumbai', 'Mumbai (All Areas)']","Key Responsibilities:\nShould have experience in below\nDesign, develop, and implement a Data Lake House architecture on AWS, ensuring scalability, flexibility, and performance.\nBuild ETL/ELT pipelines for ingesting, transforming, and processing structured and unstructured data.\nCollaborate with cross-functional teams to gather data requirements and deliver data solutions aligned with business needs.\nDevelop and manage data models, schemas, and data lakes for analytics, reporting, and BI purposes.\nImplement data governance practices, ensuring data quality, security, and compliance.\nPerform data integration between on-premise and cloud systems using AWS services.\nMonitor and troubleshoot data pipelines and infrastructure for reliability and scalability.\nSkills and Qualifications:\n7 + years of experience in data engineering, with a focus on cloud data platforms.\nStrong experience with AWS services: S3, Glue, Redshift, Athena, Lambda, IAM, RDS, and EC2.\nHands-on experience in building data lakes, data warehouses, and lake house architectures.\nShould have experience in ETL/ELT pipelines using tools like AWS Glue, Apache Spark, or similar.\nExpertise in SQL and Python or Java for data processing and transformations.\nFamiliarity with data modeling and schema design in cloud environments.\nUnderstanding of data security and governance practices, including IAM policies and data encryption.\nExperience with big data technologies (e.g., Hadoop, Spark) and data streaming services (e.g., Kinesis, Kafka).\nHave lending domain knowledge will be added advantage\nPreferred Skills:\nExperience with Databricks or similar platforms for data engineering.\nFamiliarity with DevOps practices for deploying data solutions on AWS (CI/CD pipelines).\nKnowledge of API integration and cloud data migration strategies.",Industry Type: NBFC,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['pipeline tools', 'lending domain', 'AWS', 'data models', 'spark', 'devops', 'databricks', 'date engineering platforms', 'hadoop', 'data lake house', 'API integration']",2025-06-12 14:42:40
Data Engineer,Aqilea Softech,5 - 9 years,13-20 Lacs P.A.,"['Bangalore Rural', 'Bengaluru']","Job Title: Data Engineer\nCompany : Aqilea India(Client : H&M India)\nEmployment Type: Full Time\nLocation: Bangalore(Hybrid)\nExperience: 4.5 to 9 years\nClient : H&M India\n\nAt H&M, we welcome you to be yourself and feel like you truly belong. Help us reimagine the future of an entire industry by making everyone look, feel, and do good. We take pride in our history of making fashion accessible to everyone and led by our values we strive to build a more welcoming, inclusive, and sustainable industry. We are privileged to have more than 120,000 colleagues, in over 75 countries across the world. Thats 120 000 individuals with unique experiences, skills, and passions. At H&M, we believe everyone can make an impact, we believe in giving people responsibility and a strong sense of ownership. Our business is your business, and when you grow, we grow.\nWebsite : https://career.hm.com/\n\nWe are seeking a skilled and forward-thinking Data Engineer to join our Emerging Tech team. This role is designed for someone passionate about working with cutting-edge technologies such as AI, machine learning, IoT, and big data to turn complex data sets into actionable insights.\nAs the Data Engineer in Emerging Tech, you will be responsible for designing, implementing, and optimizing data architectures and processes that support the integration of next-generation technologies. Your role will involve working with large-scale datasets, building predictive models, and utilizing emerging tools to enable data-driven decision-making across the business. You ll collaborate with technical and business teams to uncover insights, streamline data pipelines, and ensure the best use of advanced analytics technologies.\n\nKey Responsibilities:\nDesign and build scalable data architectures and pipelines that support machine learning, analytics, and IoT initiatives.\nDevelop and optimize data models and algorithms to process and analyse large-scale, complex data sets.\nImplement data governance, security, and compliance measures to ensure high-quality\nCollaborate with cross-functional teams (engineering, product, and business) to translate business requirements into data-driven solutions.\nEvaluate, integrate, and optimize new data technologies to enhance analytics capabilities and drive business outcomes.\nApply statistical methods, machine learning models, and data visualization techniques to deliver actionable insights.\nEstablish best practices for data management, including data quality, consistency, and scalability.\nConduct analysis to identify trends, patterns, and correlations within data to support strategic business initiatives.\nStay updated on the latest trends and innovations in data technologies and emerging data management practices.\n\nSkills Required :\nBachelors or masters degree in data science, Computer Science, Engineering, Statistics, or a related field.\n4.5-9 years of experience in data engineering, data science, or a similar analytical role, with a focus on emerging technologies.\nProficiency with big data frameworks (e.g., Hadoop, Spark, Kafka) and experience with modern cloud platforms (AWS, Azure, or GCP).\nSolid skills in Python, SQL, and optionally R, along with experience using machine learning libraries such as Scikit-learn, TensorFlow, or PyTorch.\nExperience with data visualization tools (e.g., Tableau or Power BI or D3.js) to communicate insights effectively.\nFamiliarity with IoT and edge computing data architectures is a plus.\nUnderstanding of data governance, compliance, and privacy standards.\nAbility to work with both structured and unstructured data.\nExcellent problem-solving, communication, and collaboration skills, with the ability to work in a fast-paced, cross-functional team environment.\nA passion for emerging technologies and a continuous desire to learn and innovate.\nInterested Candidates can share your Resumes to mail id karthik.prakadish@aqilea.com",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Powerbi', 'Hadoop', 'Kafka', 'Tableau', 'Azure', 'GCP', 'Data Engineer', 'Spark', 'AWS', 'Python', 'SQL']",2025-06-12 14:42:42
Big Data Engineer,Rarr Technologies,6 - 8 years,Not Disclosed,['Bengaluru'],Job description\n\nProven experience working with data pipelines ETL BI regardless of the technology\nProven experience working with AWS including at least 3 of RedShift S3 EMR Cloud Formation DynamoDB RDS lambda\nBig Data technologies and distributed systems one of Spark Presto or Hive\nPython language scripting and object oriented\nFluency in SQL for data warehousing RedShift in particular is a plus\nGood understanding on data warehousing and Data modelling concepts\nFamiliar with GIT Linux CICD pipelines is a plus\nStrong systems process orientation with demonstrated analytical thinking organization skills and problem solving skills\nAbility to self manage prioritize and execute tasks in a demanding environment\nStrong consultancy orientation and experience with the ability to form collaborative\nproductive working relationships across diverse teams and cultures is a must\nWillingness and ability to train and teach others\nAbility to facilitate meetings and follow up with resulting action items,Industry Type: IT Services & Consulting,Department: Other,"Employment Type: Full Time, Permanent","['python scripting', 'Big Data Technologies', 'ETL', 'AWS']",2025-06-12 14:42:45
Consultant - Data Engineer,Affine Analytics,5 - 8 years,Not Disclosed,['Bengaluru'],"We are looking for a highly skilled API & Pixel Tracking Integration Engineer to lead the development and deployment of server-side tracking and attribution solutions across multiple platforms. The ideal candidate brings deep expertise in CAPI integrations (Meta, Google, and other platforms), secure data handling using cryptographic techniques, and experience working within privacy-first environments like Azure Clean Rooms.\nThis role requires strong hands-on experience in Azure cloud services, OCI (Oracle Cloud Infrastructure), and marketing technology stacks including Adobe Tag Management and Pixel Management. You will work closely with engineering, analytics, and marketing teams to deliver scalable, compliant, and secure data tracking solutions that drive business insights and performance.",,,,"['Python', 'Azure Cloud technologies', 'Azure Data Factory', 'Adobe Tag Management', 'Azure security', 'ADF', 'ADLS', 'Azure Functions', 'Key Vault']",2025-06-12 14:42:47
HIH - Data Science Lead Analyst - Evernorth,Cigna Medical Group,5 - 8 years,Not Disclosed,['Hyderabad'],"Role Summary\nAs a member of the Data Science Center of Expertise (DSCOE), the DS Lead Analyst is responsible for leading and enabling Data Science within Cigna Group with demonstrable aptitude in Data Science (i) Technical Skills (ii) Leadership (iii) Scope & Impact (iv) Influence. Please see Qualifications section below for more details.\nThe role will support the development and maintenance of proprietary advanced neural network (AI) foundation models in support of Cigna business operations.\nKey Responsibilities:\nWrite code using PyTorch and/or Tensorflow to implement, test, and operationalize deep learning models\nCollaborate with data scientists and engineers to improve deep learning models and implement business-facing solutions built on top of those models\nTake responsibility for improving code performance and quality\nFollow developments in deep learning technology to identify opportunities to improve models\nQualifications:\nBachelors or Masters(preferred) in computer science or statistics or any other equivalent discipline with 5-8 years of relevant experience\nStrong proficiency in ML, statistics, python or R, SQL, version control (e.g., Git), health care data (e.g., claims, EHR), with emphasis on Tensorflow and Pytorch\nKnows and follows best coding and software engineering practices\nSuccessfully completes technical project components with limited guidance\nFamiliarity with deploying machine learning and predictive models to production and cloud environments\nLeadership in Data Science\nUnderstands how assigned work is related to purpose of the overall project\nIndependently identifies project roadblocks, and solutions\nSeeks to understand the health insurance domain\nScope and Impact\nDocuments the business considerations, methodology, process, code, and results associated with their work\nCollaborates to deliver clear and well developed presentations for both technical and business audiences\nConsistently communicates decisions, considerations, and needs for support\nReceives and responds to feedback in a professional and appropriate manner\nLevel of Influence\nPresent technical topics and results to non-technical stakeholders\nCommunicate and gather domain knowledge from non-technical stakeholders",Industry Type: Medical Services / Hospital,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Health insurance', 'Version control', 'GIT', 'Claims', 'Coding', 'Machine learning', 'SQL', 'Python', 'Business operations']",2025-06-12 14:42:49
Data Lineage Engineers,Altimetrik,5 - 10 years,15-30 Lacs P.A.,"['Pune', 'Chennai', 'Bengaluru']","Role & responsibilities\nSkill need :  Data Lineage with Ab-initio - Metadata hub\n\nSkills & Experience:\nExpertise in mHub or similar tools, data pipelines, and cloud platforms.\nProficiency in Python, Oracle, SQL, Java, and ETL tools.\n5-10 years of experience in data engineering and governance.",,,,"['Ab-initio', 'Metadata hub', 'Python', 'mhub', 'ETL', 'Oracle', 'SQL']",2025-06-12 14:42:51
Data Engineer,LTIMindtree,5 - 8 years,Not Disclosed,"['Noida', 'Chennai', 'Bengaluru']","1.Big Data Engineer:\n\nCompany: LTIMINDTREE\nMandotory skills - Python,Pyspark & AWS\nLocation : Noida & Pan India\nExperience : 5-8Years\nSalary: 19LPA\n\nShare your cv at Muktai.S@alphacom.in",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'AWS', 'Python']",2025-06-12 14:42:53
Big Data Engineer,Client of Hiresquad Resources,5 - 8 years,22.5-30 Lacs P.A.,"['Noida', 'Hyderabad', 'Bengaluru']","Role: Data Engineer\nExp: 5 to 8 Years\nLocation: Bangalore, Noida, and Hyderabad (Hybrid, weekly 2 Days office must)\nNP: Immediate to 15 Days (Try to find only immediate joiners)\n\n\nNote:\nCandidate must have experience in Python, Kafka Streams, Pyspark, and Azure Databricks.\nNot looking for candidates who have only Exp in Pyspark and not in Python.\n\n\nJob Title: SSE Kafka, Python, and Azure Databricks (Healthcare Data Project)\nExperience:  5 to 8 years\n\nRole Overview:\nWe are looking for a highly skilled with expertise in Kafka, Python, and Azure Databricks (preferred) to drive our healthcare data engineering projects. The ideal candidate will have deep experience in real-time data streaming, cloud-based data platforms, and large-scale data processing. This role requires strong technical leadership, problem-solving abilities, and the ability to collaborate with cross-functional teams.\n\nKey Responsibilities:\nLead the design, development, and implementation of real-time data pipelines using Kafka, Python, and Azure Databricks.\nArchitect scalable data streaming and processing solutions to support healthcare data workflows.\nDevelop, optimize, and maintain ETL/ELT pipelines for structured and unstructured healthcare data.\nEnsure data integrity, security, and compliance with healthcare regulations (HIPAA, HITRUST, etc.).\nCollaborate with data engineers, analysts, and business stakeholders to understand requirements and translate them into technical solutions.\nTroubleshoot and optimize Kafka streaming applications, Python scripts, and Databricks workflows.\nMentor junior engineers, conduct code reviews, and ensure best practices in data engineering.\nStay updated with the latest cloud technologies, big data frameworks, and industry trends.\n\n\nRequired Skills & Qualifications:\n4+ years of experience in data engineering, with strong proficiency in Kafka and Python.\nExpertise in Kafka Streams, Kafka Connect, and Schema Registry for real-time data processing.\nExperience with Azure Databricks (or willingness to learn and adopt it quickly).\nHands-on experience with cloud platforms (Azure preferred, AWS or GCP is a plus).\nProficiency in SQL, NoSQL databases, and data modeling for big data processing.\nKnowledge of containerization (Docker, Kubernetes) and CI/CD pipelines for data applications.\nExperience working with healthcare data (EHR, claims, HL7, FHIR, etc.) is a plus.\nStrong analytical skills, problem-solving mindset, and ability to lead complex data projects.\nExcellent communication and stakeholder management skills.\n\n\n\nEmail: Sam@hiresquad.in",Industry Type: Medical Services / Hospital,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Kafka', 'Azure Databricks', 'Python', 'Etl Pipelines', 'Pyspark', 'spark architecture', 'Data Engineering', 'opps concepts', 'Data Streaming', 'Medallion Architecture', 'python scripts', 'schema registry', 'SQL Database', 'Nosql Databases', 'spark tuning', 'Kafka Streams', 'kafka connect']",2025-06-12 14:42:56
Data Engineer - SAS Migration,Crisil,2 - 4 years,Not Disclosed,['Mumbai'],"The SAS to Databricks Migration Developer will be responsible for migrating existing SAS code, data processes, and workflows to the Databricks platform\n\nThis role requires expertise in both SAS and Databricks, with a focus on converting SAS logic into scalable PySpark and Python code\n\nThe developer will design, implement, and optimize data pipelines, ensuring seamless integration and functionality within the Databricks environment\n\nCollaboration with various teams is essential to understand data requirements and deliver solutions that meet business needs",Industry Type: Financial Services,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['hive', 'scala', 'pyspark', 'data warehousing', 'data migration', 'azure data factory', 'sql', 'sql azure', 'java', 'spark', 'mysql', 'hadoop', 'big data', 'etl', 'python', 'sas', 'microsoft azure', 'power bi', 'machine learning', 'sql server', 'data bricks', 'migration', 'sqoop', 'aws', 'ssis']",2025-06-12 14:42:58
Big Data Engineer,Apexon,11 - 16 years,Not Disclosed,['Bengaluru'],"We enable #HumanFirstDigital\n\nJob Summary:\nWe are looking for a highly experienced and strategic Data Engineer to drive the design, development, and optimization of our enterprise data platform. This role requires deep technical expertise in AWS, StreamSets, and Snowflake, along with solid experience in Kubernetes, Apache Airflow, and unit testing. The ideal candidate will lead a team of data engineers and play a key role in delivering scalable, secure, and high-performance data solutions for both historical and incremental data loads.\nKey Responsibilities:\nLead the architecture, design, and implementation of end-to-end data pipelines using StreamSets and Snowflake.\nOversee the development of scalable ETL/ELT processes for historical data migration and incremental data ingestion.\nGuide the team in leveraging AWS services (S3, Lambda, Glue, IAM, etc.) to build cloud-native data solutions.\nProvide technical leadership in deploying and managing containerized applications using Kubernetes.\nDefine and implement workflow orchestration strategies using Apache Airflow.\nEstablish best practices for unit testing, code quality, and data validation.\nCollaborate with data architects, analysts, and business stakeholders to align data solutions with business goals.\nMentor junior engineers and foster a culture of continuous improvement and innovation.\nMonitor and optimize data workflows for performance, scalability, and cost-efficiency.\nRequired Skills & Qualifications:\nHigh proficiency in AWS, including hands-on experience with core services (S3, Lambda, Glue, IAM, CloudWatch).\nExpert-level experience with StreamSets, including Data Collector, Transformer, and Control Hub.\nStrong Snowflake expertise, including data modeling, SnowSQL, and performance tuning.\nMedium-level experience with Kubernetes, including container orchestration and deployment.\nWorking knowledge of Apache Airflow for workflow scheduling and monitoring.\nExperience with unit testing frameworks and practices in data engineering.\nProven experience in building and managing ETL pipelines for both batch and real-time data.\nStrong command of SQL and scripting languages such as Python or Shell.\nExperience with CI/CD pipelines and version control tools (e.g., Git, Jenkins).\nPreferred Qualifications:\nAWS certification (e.g., AWS Certified Data Analytics, Solutions Architect).\nExperience with data governance, security, and compliance frameworks.\nFamiliarity with Agile methodologies and tools like Jira and Confluence.\nPrior experience in a leadership or mentoring role within a data engineering team.\nOur Commitment to Diversity & Inclusion:\nOur Perks and Benefits:\nOur benefits and rewards program has been thoughtfully designed to recognize your skills and contributions, elevate your learning/upskilling experience and provide care and support for you and your loved ones. As an Apexon Associate, you get continuous skill-based development, opportunities for career advancement, and access to comprehensive health and well-being benefits and assistance.\nWe also offer:\no Group Health Insurance covering family of 4\no Term Insurance and Accident Insurance\no Paid Holidays & Earned Leaves\no Paid Parental LeaveoLearning & Career Development\no Employee Wellness\nJob Location : Bengaluru, India",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Performance tuning', 'Data modeling', 'Agile', 'Wellness', 'Workflow', 'Healthcare', 'Unit testing', 'Apache', 'SQL', 'Python']",2025-06-12 14:43:00
Microsoft Fabrics Data Engineer,Swits Digital,5 - 10 years,Not Disclosed,['Bengaluru'],"Job TItle: Microsoft Fabric Data Engineer\nLocation: Bangalore\nJob Type: Conract (24 Months)\nJob Description:\nWe are seeking a highly skilled and experienced Microsoft Fabric Data Engineer/Architect to design, develop, and maintain robust, scalable, and secure data solutions within the Microsoft Fabric ecosystem. This role will leverage the full suite of Microsoft Azure data services, including Azure Data Bricks, Azure Data Factory, and Azure Data Lake, to build end-to-end data pipelines, data warehouses, and data lakehouses that enable advanced analytics and business intelligence.\nRequired Skills & Qualifications:\nBachelors degree in Computer Science, Engineering, or a related field.\n5+ years of experience in data architecture and engineering, with a strong focus on Microsoft Azure data platforms.\nProven hands-on expertise with Microsoft Fabric and its components, including:\nOneLake\nData Factory (Pipelines, Dataflows Gen2)\nSynapse Analytics (Data Warehousing, SQL analytics endpoint)\nLakehouses and Warehouses\nNotebooks (PySpark)\nExtensive experience with Azure Data Bricks, including Spark development (PySpark, Scala, SQL).\nStrong proficiency in Azure Data Factory for building and orchestrating ETL/ELT pipelines.\nDeep understanding and experience with Azure Data Lake Storage Gen2.\nProficiency in SQL (T-SQL, Spark SQL), Python, and/or other relevant scripting languages.\nSolid understanding of data warehousing concepts, dimensional modeling, and data lakehouse architectures.\nExperience with data governance principles and tools (e.g., Microsoft Purview).\nFamiliarity with CI/CD practices, version control (Git), and DevOps for data pipelines.\nExcellent problem-solving, analytical, and communication skills.\nAbility to work independently and collaboratively in a fast-paced, agile environment.\nPreferred Qualifications:\nMicrosoft certifications in Azure Data Engineering (e.g., DP-203, DP-600: Microsoft Fabric Analytics Engineer Associate).\nExperience with Power BI for data visualization and reporting.\nFamiliarity with real-time analytics and streaming data processing.\nExposure to machine learning workflows and integrating ML models with data solutions",Industry Type: Recruitment / Staffing,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['GIT', 'Analytical', 'microsoft azure', 'data visualization', 'microsoft', 'Business intelligence', 'Data warehousing', 'Analytics', 'Data architecture', 'Python']",2025-06-12 14:43:03
Data Engineer,Nemetschek,5 - 10 years,Not Disclosed,"['Hyderabad', 'Bengaluru', 'Mumbai (All Areas)']","Role & responsibilities\n5+ years in software development, with a focus on data-intensive applications, cloud solutions, and scalable data architectures.\nDevelopment experience in GoLang for building scalable and efficient data applications.\nExperience with Snowflake, Redshift, or similar data platforms including architecture, data modeling, performance optimization, and integrations.\nExperience designing and building data lakes and data warehouses, ensuring data integrity, scalability, and performance.\nProficient in developing and managing ETL pipelines, using modern tools and techniques to transform, load, and integrate data efficiently.\nExperience with high-volume event streams (such as Kafka, Kinesis) and near real-time data processing solutions for fast and accurate reporting.\nHands-on experience with Terraform for automating infrastructure deployment and configuration management in cloud environments.\nExperience with containerization technologies (Docker, Kubernetes) and orchestration.\nSolid grasp of database fundamentals (SQL, NoSQL, data modeling, performance tuning)\nExperience with CI/CD pipelines and automation tools for testing, deployment, and continuous improvement.\nExperience working in AWS cloud environments, specifically with big data solutions and serverless architectures\nAbility to mentor and guide junior engineers, fostering a culture of learning and innovation\nStrong communication skills to articulate technical concepts clearly to non-technical stakeholders.\nWHAT WE OFFER\nA young, dynamic, and innovation-oriented environment\nA wide variety of projects within different industries\nA very open and informal culture where knowledge sharing, and employee development are key.\nRoom for personal initiative, development, and growth\nRealistic career opportunities\nCompetitive package and fringe benefits.\n\nPreferred candidate profile",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Golang', 'Snowflake', 'Javascript', 'ETL', 'AWS']",2025-06-12 14:43:05
Data Streaming Engineer,Data Streaming Engineer,5 - 10 years,Not Disclosed,"['Pune', 'Chennai', 'Bengaluru']","Hello Candidates,\n\nWe are Hiring !!\n\nJob Position - Data Streaming Engineer\nExperience - 5+ years\nLocation - Mumbai, Pune , Chennai , Bangalore\nWork mode - Hybrid ( 3 days WFO)\n\nJOB DESCRIPTION\n\nRequest for Data Streaming Engineer Data Streaming @ offshore :\n• Flink , Python Language.\n• Data Lake Systems. (OLAP Systems).\n• SQL (should be able to write complex SQL Queries)\n• Orchestration (Apache Airflow is preferred).\n• Hadoop (Spark and Hive: Optimization of Spark and Hive apps).\n• Snowflake (good to have).\n• Data Quality (good to have).\n• File Storage (S3 is good to have)\n\nNOTE - Candidates can share their resume on - shrutia.talentsketchers@gmail.com",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Flink', 'Apache Airflow', 'Data Quality', 'Hadoop', 'Snowflake', 'Data Lake', 'orchastration', 'Python', 'SQL']",2025-06-12 14:43:07
Data Engineer,Amakshar Technology,4 - 7 years,Not Disclosed,"['Mumbai', 'Pune', 'Chennai', 'Bengaluru']","Job Category: IT\nJob Type: Full Time\nJob Location: Bangalore Chennai Mumbai Pune\nLocation- Mumbai, Pune, Bangalore, Chennai\nExperience- 5+\nData Engineer: Expertise in Python Language is MUST. SQL (should be able to write complex SQL Queries) is MUST Data Lake Development experience. Orchestration (Apache Airflow is preferred). Spark and Hive: Optimization of Spark/PySpark and Hive apps is MUST Trino/(AWS Athena) (Good to have) Snowflake (good to have). Data Quality (good to have). File Storage (S3 is good to have)\nKind Note: Please apply or share your resume only if it matches the above criteria",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['hive', 'SQL queries', 'orchestration', 'spark', 'Data quality', 'Apache', 'AWS', 'Python']",2025-06-12 14:43:09
Data Engineer (5-10 Years) | @ Banking | Bangalore & Mumbai,Net Connect,5 - 10 years,Not Disclosed,"['Bengaluru', 'Mumbai (All Areas)']","Job Summary\nWe are seeking a skilled Data Engineer to design, develop, and optimize scalable data pipelines and infrastructure. The ideal candidate will have expertise in relational databases, data modeling, cloud migration, and automation, working closely with cross-functional teams to drive data-driven decision-making.\n\nKey Responsibilities",,,,"['Data Modeling', 'ETL', 'Scripting', 'SQL', 'Azure Data Factory', 'Informatica']",2025-06-12 14:43:11
Consultant - Data Engineer (with Fabric),Affine Analytics,5 - 8 years,Not Disclosed,['Bengaluru'],"We are looking for a highly skilled API & Pixel Tracking Integration Engineer to lead the development and deployment of server-side tracking and attribution solutions across multiple platforms. The ideal candidate brings deep expertise in CAPI integrations (Meta, Google, and other platforms), secure data handling using cryptographic techniques, and experience working within privacy-first environments like Azure Clean Rooms.\nThis role requires strong hands-on experience in C# development, Azure cloud services, OCI (Oracle Cloud Infrastructure), and marketing technology stacks including Adobe Tag Management and Pixel Management. You will work closely with engineering, analytics, and marketing teams to deliver scalable, compliant, and secure data tracking solutions that drive business insights and performance.",,,,"['Adobe Tag Management', 'Data Engineering', 'Azure Data Factory', 'Azure security', 'ADF', 'ADLS', 'Azure Functions', 'Meta CAPI', 'Google Enhanced Conversions', 'Key Vault', 'Cosmos DB']",2025-06-12 14:43:14
Data & Analytics Specialist,Roche Diagnostics,5 - 10 years,Not Disclosed,['Pune'],"At Roche you can show up as yourself, embraced for the unique qualities you bring. Our culture encourages personal expression, open dialogue, and genuine connections, where you are valued, accepted and respected for who you are, allowing you to thrive both personally and professionally. This is how we aim to prevent, stop and cure diseases and ensure everyone has access to healthcare today and for generations to come. Join Roche, where every voice matters.\nThe Position The Position\nWe are looking for a Data & Analytics Specialist/ Business Analyst who will join us in the newly setup Integrated Informatics for a journey to drive transformation with data and foster automated and efficient decision making throughout the organisation\nThe Data and Analytics Specialist must be the big-picture thinker who understands the value of data to the organisation, has a strong focus on delivering high value, connecting the dots, investing in right initiatives with reusability at the heart of it.\nIn this position you will be acting as squad lead, have end to end ownership of Product delivery with setting up teams from multiple teams/areas with focus on Lifecycle management of the product\nResponsibilities\nYou will work on various aspects of Analytics Solution Development, Data Management, Governance and Information Architecture including but not limited to:\nCollaborate with business stakeholders to understand their data and analytics needs and develop a product roadmap that aligns with business goals.\nDefine and prioritise product requirements, user stories, and acceptance criteria for data and analytics products and ensure what was agreed gets delivered.\nWork with data engineers and data scientists to develop data pipelines, analytical models, and visualisations that meet business requirements.\nCollaborate with Infrastructure Teams and software developers to ensure that data and analytics products are integrated into existing systems and platforms in a sustainable way that still meets the needs of business to generate the insights necessary to drive their decisions.\nMonitor data and analytics product performance and identify opportunities for improvement.\nStay up-to-date with industry trends and emerging technologies related to data and analytics in the pharmaceutical industry.\nAct as a subject matter expert for data and analytics products and provide guidance to business stakeholders on how to effectively use these products.\nAccountable to Develop and maintain documentation, training materials, and user guides for data and analytics products.\nThe ideal candidate\nBachelors or Masters degree in computer science, information systems, or a related field.\n5+ years of experience in roles such as Senior Data & Analytics Specialist, Data Solutions Lead, Data Architect, or Data Consultant, with a focus on solution design and implementation. Alternatively, 3-4 years of experience in data streams (e.g., Data Science, Data Engineering, Data Governance) combined with a couple of years in Strategic Data Consultancy / Data Product Ownership. Experience in the pharmaceutical or healthcare industry is highly desirable.\nHigh Level understanding of data engineering, data science, Data governance and analytics concepts and technologies.\nExperience working with cross-functional teams, including data engineers, data scientists, and software developers.\nExcellent communication and interpersonal skills.\nStrong analytical and problem-solving skills.\nExperience with agile development methodologies.\nKnowledge of regulatory requirements related to data and analytics in the pharmaceutical industry.\nKnowledge of working with vendor and customer master data for different divisions - Pharmaceuticals, Diagnostic, & Diabetes care.\nUnderstanding of the transparency reporting landscape.\nHands-on experience of working on applications such as Jira, SQL, Postman, SAP GUI, Monday.com, Trello\nProficient in the knowledge of different CRM/Master Data Management systems such as SFDC, Reltio MDM\nUnderstanding data protection laws and consent processes applicable to healthcare professionals and organizations before transparency disclosure.\nWho we are\nAt Roche, more than 100,000 people across 100 countries are pushing back the frontiers of healthcare. Working together, we ve become one of the world s leading research-focused healthcare groups. Our success is built on innovation, curiosity and diversity.\nBasel is the headquarters of the Roche Group and one of its most important centres of pharmaceutical research. Over 10,700 employees from over 100 countries come together at our Basel/Kaiseraugst site, which is one of Roche`s largest sites. Read more.\nBesides extensive development and training opportunities, we offer flexible working options, 18 weeks of maternity leave and 10 weeks of gender independent partnership leave. Our employees also benefit from multiple services on site such as child-care facilities, medical services, restaurants and cafeterias, as well as various employee events.\nWe believe in the power of diversity and inclusion, and strive to identify and create opportunities that enable all people to bring their unique selves to Roche.\nRoche is an Equal Opportunity Employer.\nWho we are\n.\n\nLet s build a healthier future, together.\nRoche is an Equal Opportunity Employer.\n""",Industry Type: Biotechnology,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'SAP', 'Diagnostics', 'HP data protector', 'Analytical', 'Healthcare', 'JIRA', 'Analytics', 'SQL', 'CRM']",2025-06-12 14:43:16
Data & Analytics Specialist,Hoffmann La Roche,5 - 10 years,Not Disclosed,['Pune'],"At Roche you can show up as yourself, embraced for the unique qualities you bring. Our culture encourages personal expression, open dialogue, and genuine connections, where you are valued, accepted and respected for who you are, allowing you to thrive both personally and professionally. This is how we aim to prevent, stop and cure diseases and ensure everyone has access to healthcare today and for generations to come. Join Roche, where every voice matters.\nThe Position The Position\nWe are looking for a Data & Analytics Specialist/ Business Analyst who will join us in the newly setup Integrated Informatics for a journey to drive transformation with data and foster automated and efficient decision making throughout the organisation\nThe Data and Analytics Specialist must be the big-picture thinker who understands the value of data to the organisation, has a strong focus on delivering high value, connecting the dots, investing in right initiatives with reusability at the heart of it.\nIn this position you will be acting as squad lead, have end to end ownership of Product delivery with setting up teams from multiple teams/areas with focus on Lifecycle management of the product\nResponsibilities\nYou will work on various aspects of Analytics Solution Development, Data Management, Governance and Information Architecture including but not limited to:\nCollaborate with business stakeholders to understand their data and analytics needs and develop a product roadmap that aligns with business goals.\nDefine and prioritise product requirements, user stories, and acceptance criteria for data and analytics products and ensure what was agreed gets delivered.\nWork with data engineers and data scientists to develop data pipelines, analytical models, and visualisations that meet business requirements.\nCollaborate with Infrastructure Teams and software developers to ensure that data and analytics products are integrated into existing systems and platforms in a sustainable way that still meets the needs of business to generate the insights necessary to drive their decisions.\nMonitor data and analytics product performance and identify opportunities for improvement.\nStay up-to-date with industry trends and emerging technologies related to data and analytics in the pharmaceutical industry.\nAct as a subject matter expert for data and analytics products and provide guidance to business stakeholders on how to effectively use these products.\nAccountable to Develop and maintain documentation, training materials, and user guides for data and analytics products.\nThe ideal candidate\nBachelors or Masters degree in computer science, information systems, or a related field.\n5+ years of experience in roles such as Senior Data & Analytics Specialist, Data Solutions Lead, Data Architect, or Data Consultant, with a focus on solution design and implementation. Alternatively, 3-4 years of experience in data streams (e.g., Data Science, Data Engineering, Data Governance) combined with a couple of years in Strategic Data Consultancy / Data Product Ownership. Experience in the pharmaceutical or healthcare industry is highly desirable.\nHigh Level understanding of data engineering, data science, Data governance and analytics concepts and technologies.\nExperience working with cross-functional teams, including data engineers, data scientists, and software developers.\nExcellent communication and interpersonal skills.\nStrong analytical and problem-solving skills.\nExperience with agile development methodologies.\nKnowledge of regulatory requirements related to data and analytics in the pharmaceutical industry.\nKnowledge of working with vendor and customer master data for different divisions - Pharmaceuticals, Diagnostic, & Diabetes care.\nUnderstanding of the transparency reporting landscape.\nHands-on experience of working on applications such as Jira, SQL, Postman, SAP GUI, Monday.com, Trello\nProficient in the knowledge of different CRM/Master Data Management systems such as SFDC, Reltio MDM\nUnderstanding data protection laws and consent processes applicable to healthcare professionals and organizations before transparency disclosure.\nWho we are\n.\nBasel is the headquarters of the Roche Group and one of its most important centres of pharmaceutical research. Over 10,700 employees from over 100 countries come together at our Basel/Kaiseraugst site, which is one of Roche`s largest sites. Read more.\nBesides extensive development and training opportunities, we offer flexible working options, 18 weeks of maternity leave and 10 weeks of gender independent partnership leave. Our employees also benefit from multiple services on site such as child-care facilities, medical services, restaurants and cafeterias, as well as various employee events.\nWe believe in the power of diversity and inclusion, and strive to identify and create opportunities that enable all people to bring their unique selves to Roche.\nRoche is an Equal Opportunity Employer.\nWho we are\nA healthier future drives us to innovate. Together, more than 100 000 employees across the globe are dedicated to advance science, ensuring everyone has access to healthcare today and for generations to come. Our efforts result in more than 26 million people treated with our medicines and over 30 billion tests conducted using our Diagnostics products. We empower each other to explore new possibilities, foster creativity, and keep our ambitions high, so we can deliver life-changing healthcare solutions that make a global impact.\n\nLet s build a healthier future, together.\nRoche is an Equal Opportunity Employer.\n""",Industry Type: Biotechnology,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'SAP', 'Diagnostics', 'HP data protector', 'Analytical', 'Healthcare', 'JIRA', 'Analytics', 'SQL', 'CRM']",2025-06-12 14:43:18
Data Architect,Neo Aid,12 - 20 years,48-60 Lacs P.A.,['Bengaluru'],"Data Architect\nBangalore (Pune option).\nHybrid, 2-3 days WFO,\nUp to 60 LPA. Needs GCP, Data Engineering, Analytics, Visualization, Modeling. 1\n5-20 yrs exp, end-to-end data pipeline.\nNotice: 60 days.\nArchitecture - recent 1-2 years\n\n\nProvident fund",Industry Type: Recruitment / Staffing,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Visualizing', 'Data Architecture', 'Data Modeling', 'Data Analytics', 'Gcp Cloud', 'ETL']",2025-06-12 14:43:20
"Senior Data Engineer II, Business Intelligence & Reporting",XL India Business Services Pvt. Ltd,3 - 7 years,Not Disclosed,['Gurugram'],"Senior Data Engineer II, Business Intelligence & Reporting Gurgaon, Haryana, India AXA XL recognizes digital, data, and information assets are critical for the business, both in terms of managing risk and enabling new business opportunities\n\nThis data should not only be high quality, but also actionable - enabling AXA XL s executive leadership team to maximize benefits and facilitate sustained dynamic advantage\n\nOur Data, Intelligence & Analytics function is focused on driving innovation by optimizing how we leverage digital, data, and AI to drive strategy and differentiate ourselves from the competition\n\nAs we develop an enterprise-wide data and digital strategy that moves us toward a greater focus on the use of data and strengthening our digital, AI capabilities, we are seeking a Deputy Manager, BI and Reporting\n\nIn this role, you will support/manage BI & reporting\n\nWhat you ll be DOING Your essential responsibilities include: BI & Reporting Management: Oversee and support Business Intelligence (BI) and Reporting products, ensuring their effectiveness and alignment with organizational goals\n\nStakeholder Engagement: Manage Business as Usual (BAU) activities for BI and Reporting, fostering effective communication and relationships with stakeholders to understand their needs and expectations\n\nModel Integration: Energize and synergize various Business Intelligence models and reporting systems to enhance data insights and reporting capabilities\n\nStrategic Initiative Support: Collaborate with the Data Intelligence and Analytics (DIA) team on various strategic initiatives, enabling the development of BI and Reporting functions and related capabilities\n\nTalent Development: Foster the growth of BI and Reporting talent across AXA XL by promoting an inclusive and diverse environment that encourages the utilization and value creation of our strategic digital, data, and analytics assets\n\nCustomer-Centric Culture: Instill a customer-first mindset within the team, prioritizing exceptional service for business stakeholders and ensuring their needs are met\n\nTeam Development: Contribute to the enhancement of the Business Intelligence teams tools, skills, and culture, driving positive impacts on team performance and outcomes\n\nYou will report to Senior Manager, Business Intelligence & Reporting\n\nWhat you will BRING At AXA XL, we view individuals holistically through their People, Business, and Technical Skills\n\nWe re interested in what you bring, how you think, and your potential for growth\n\nWe value diverse backgrounds and perspectives, recognizing that each person contributes uniquely to our teams success\n\nWe value relevant education and experience in a related field\n\nAdditionally, we encourage candidates with diverse educational backgrounds or equivalent experience to apply\n\nHere are some of the key skills important for the role: People Skills Customer Centricity: Brings a collaborative spirit, a can-do attitude, and a Customer First mindset, ensuring that stakeholder needs are prioritized\n\nCross-Functional Collaboration: Ability to communicate effectively with teams, peers, and stakeholders across the globe, fostering collaboration and understanding\n\nAble to help and guide team members on technical issues, fostering their development and promoting self-directed problem-solving\n\nGrowth Mindset: Passion for digital, data, and AI, along with a commitment to personal and team development in a digital and data-driven organization\n\nResilience: Ability to lead a project or team, demonstrating adaptability and leadership under various circumstances\n\nAnalytical & Strategic Mindset: Ability to analyze data effectively and develop strategic insights that drive decision-making and improve business outcomes\n\nPerformance Excellence: Commitment to delivering high-quality results and continuously improving processes and performance metrics within the team\n\nBUSINESS Skills Business & Insurance Acumen: Ability to showcase relevant industry knowledge supporting multiple specialty areas of Data and Analytics\n\nStakeholder Management: Ability to manage stakeholders effectively, understanding their needs and ensuring clear communication and support\n\nSimplifies Complexity: Ability to distill complex data concepts and analyses into clear, actionable insights for stakeholders\n\nEnsuring that technical information is accessible, enabling informed decision-making and fostering collaboration between technical and non-technical teams\n\nTECHNICAL Skills Data Visualization: Experience with end-user BI tools like Power BI, enabling effective presentation and visualization of data insights\n\nReporting Tools: Proficincy in SQL, Advanced Excel, MS Access, and VBA, allowing for effective data manipulation and reporting\n\nData Analytics: Ability to help and guide team members on technical issues, fostering skill development within the team to self-directedly manage data analytics tasks",Industry Type: Insurance,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Stakeholder Engagement', 'MS Access', 'Analytical', 'Agile', 'digital strategy', 'Business strategy', 'data visualization', 'Stakeholder management', 'Reporting tools', 'SQL']",2025-06-12 14:43:23
ETL Developer - Data & Analytics,Canpack India,3 - 5 years,Not Disclosed,['Pune'],"Giorgi Global Holdings, Inc. ( GGH ) is a privately held, diversified consumer products/packaging company with approximately 11,000 employees and operations in 20 countries. GGH consists of four US based companies ( The Giorgi Companies ) and one global packaging company ( CANPACK ).\nGGH has embarked on a transformation journey to become a digital, technology enabled, customer-centric, data and insights-driven organization. This transformation is evolving our business, strategy, core operations and IT solutions.\nAs an ETL Developer, you will be an integral part of our Data and Analytics team, working closely with the ETL Architect and other developers to design, develop, and maintain efficient data integration and transformation solutions. We are looking for a highly skilled ETL Developer with a deep understanding of ETL processes and data warehousing. The ideal candidate is passionate about optimizing data extraction, transformation, and loading workflows, ensuring high performance, accuracy, and scalability to support business intelligence initiatives.\nWhat you will do:\n1. Design, develop, test and maintain ETL processes and data pipelines to support data integration and transformation needs.\n2. Continuously improve ETL performance and reliability through best practices and optimization techniques.\n3. Develop and implement data validation and quality checks to ensure the integrity and consistency of data.\n4. Collaborate with ETL Architect, Data Engineers, and Business Intelligence teams to understand business requirements and translate them into technical solutions.\n5. Monitor, troubleshoot, and resolve ETL job failures, performance bottlenecks, and data discrepancies.\n6. Proactively identify and resolve ETL-related issues, minimizing impact on business operations.\n7. Contribute to documentation, training, and knowledge sharing to enhance team capabilities.\n8. Communicate progress and challenges clearly to both technical and non-technical teams\nEssential Requirements:\nBachelor s or master s degree in information technology, Computer Science, or a related field.\n3-5 years of relevant experience.\nPower-BI, Tabular Editor/Dax Studio, ALM/Github/Azure Devops skills\nExposure to SAP Systems/Modules like SD, MD, etc. to understand functional data.,\nExposure to MS Fbric, MS Azure Synapse Analytics\nCompetencies needed:\n- Hands-on experience with ETL development and data integration for large-scale systems\n- Experience with platforms such as Synapse Analytics, Azure Data Factory, Fabric, Redshift or Databricks\n- A solid understanding of data warehousing and ETL processes\n- Advanced SQL and PL/SQL skills such as query optimization, complex joins, window functions\n- Expertise in Python (pySpark) programming with a focus on data manipulation and analysis\n- Experience with Azure DevOps and CI/CD process\n- Excellent problem-solving and analytical skills\n- Experience in creating post-implementation documentation\n- Strong team collaboration skills\n- Attention to detail and a commitment to quality\nStrong interpersonal skills including analytical thinking, creativity, organizational abilities, high commitment, initiative in task execution, and a fast-learning capability for understanding IT concepts\n\nIf you are a current CANPACK employee, please apply through your Workday account .\nCANPACK Group is an Equal Opportunity Employer and all qualified applicants will receive consideration for employment without regard to race, colour, religion, age, sex, sexual orientation, gender identity, national origin, disability, or any other characteristic protected by law or not related to job requirements, unless such distinction is required by law.",Industry Type: Packaging & Containers,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'SAP', 'Analytical', 'Packaging', 'PLSQL', 'Business intelligence', 'Information technology', 'Analytics', 'Python', 'Data extraction']",2025-06-12 14:43:25
Enterprise Data Operations Manager,Pepsico,12 - 17 years,Not Disclosed,['Hyderabad'],"Overview\n\nDeputy Director - Data Engineering\n\nPepsiCo operates in an environment undergoing immense and rapid change. Big-data and digital technologies are driving business transformation that is unlocking new capabilities and business innovations in areas like eCommerce, mobile experiences and IoT. The key to winning in these areas is being able to leverage enterprise data foundations built on PepsiCos global business scale to enable business insights, advanced analytics, and new product development. PepsiCos Data Management and Operations team is tasked with the responsibility of developing quality data collection processes, maintaining the integrity of our data foundations, and enabling business leaders and data scientists across the company to have rapid access to the data they need for decision-making and innovation.\nIncrease awareness about available data and democratize access to it across the company.\nAs a data engineering lead, you will be the key technical expert overseeing PepsiCo's data product build & operations and drive a strong vision for how data engineering can proactively create a positive impact on the business. You'll be empowered to create & lead a strong team of data engineers who build data pipelines into various source systems, rest data on the PepsiCo Data Lake, and enable exploration and access for analytics, visualization, machine learning, and product development efforts across the company. As a member of the data engineering team, you will help lead the development of very large and complex data applications into public cloud environments directly impacting the design, architecture, and implementation of PepsiCo's flagship data products around topics like revenue management, supply chain, manufacturing, and logistics. You will work closely with process owners, product owners and business users. You'll be working in a hybrid environment with in-house, on-premises data sources as well as cloud and remote systems.\nResponsibilities\n\nData engineering lead role for D&Ai data modernization (MDIP)\n\nIdeally Candidate must be flexible to work an alternative schedule either on tradition work week from Monday to Friday; or Tuesday to Saturday or Sunday to Thursday depending upon coverage requirements of the job. The candidate can work with immediate supervisor to change the work schedule on rotational basis depending on the product and project requirements.\nResponsibilities\nManage a team of data engineers and data analysts by delegating project responsibilities and managing their flow of work as well as empowering them to realize their full potential.\nDesign, structure and store data into unified data models and link them together to make the data reusable for downstream products.\nManage and scale data pipelines from internal and external data sources to support new product launches and drive data quality across data products.\nCreate reusable accelerators and solutions to migrate data from legacy data warehouse platforms such as Teradata to Azure Databricks and Azure SQL.\nEnable and accelerate standards-based development prioritizing reuse of code, adopt test-driven development, unit testing and test automation with end-to-end observability of data\nBuild and own the automation and monitoring frameworks that captures metrics and operational KPIs for data pipeline quality, performance and cost.\nCollaborate with internal clients (product teams, sector leads, data science teams) and external partners (SI partners/data providers) to drive solutioning and clarify solution requirements.\nEvolve the architectural capabilities and maturity of the data platform by engaging with enterprise architects to build and support the right domain architecture for each application following well-architected design standards.\nDefine and manage SLAs for data products and processes running in production.\nCreate documentation for learnings and knowledge transfer to internal associates.\nQualifications\n\n12+ years of engineering and data management experience\n\nQualifications\n12+ years of overall technology experience that includes at least 5+ years of hands-on software development, data engineering, and systems architecture.\n8+ years of experience with Data Lakehouse, Data Warehousing, and Data Analytics tools.\n6+ years of experience in SQL optimization and performance tuning on MS SQL Server, Azure SQL or any other popular RDBMS\n6+ years of experience in Python/Pyspark/Scala programming on big data platforms like Databricks\n4+ years in cloud data engineering experience in Azure or AWS.\nFluent with Azure cloud services. Azure Data Engineering certification is a plus.\nExperience with integration of multi cloud services with on-premises technologies.\nExperience with data modelling, data warehousing, and building high-volume ETL/ELT pipelines.\nExperience with data profiling and data quality tools like Great Expectations.\nExperience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets.\nExperience with at least one business intelligence tool such as Power BI or Tableau\nExperience with running and scaling applications on the cloud infrastructure and containerized services like Kubernetes.\nExperience with version control systems like ADO, Github and CI/CD tools for DevOps automation and deployments.\nExperience with Azure Data Factory, Azure Databricks and Azure Machine learning tools.\nExperience with Statistical/ML techniques is a plus.\nExperience with building solutions in the retail or in the supply chain space is a plus.\nUnderstanding of metadata management, data lineage, and data glossaries is a plus.\nBA/BS in Computer Science, Math, Physics, or other technical fields.\nCandidate must be flexible to work an alternative work schedule either on tradition work week from Monday to Friday; or Tuesday to Saturday or Sunday to Thursday depending upon product and project coverage requirements of the job.\nCandidates are expected to be in the office at the assigned location at least 3 days a week and the days at work needs to be coordinated with immediate supervisor\nSkills, Abilities, Knowledge:\nExcellent communication skills, both verbal and written, along with the ability to influence and demonstrate confidence in communications with senior level management.\nProven track record of leading, mentoring data teams.\nStrong change manager. Comfortable with change, especially that which arises through company growth.\nAbility to understand and translate business requirements into data and technical requirements.\nHigh degree of organization and ability to manage multiple, competing projects and priorities simultaneously.\nPositive and flexible attitude to enable adjusting to different needs in an ever-changing environment.\nStrong leadership, organizational and interpersonal skills; comfortable managing trade-offs.\nFoster a team culture of accountability, communication, and self-management.\nProactively drives impact and engagement while bringing others along.\nConsistently attain/exceed individual and team goals.\nAbility to lead others without direct authority in a matrixed environment.\nComfortable working in a hybrid environment with teams consisting of contractors as well as FTEs spread across multiple PepsiCo locations.\nDomain Knowledge in CPG industry with Supply chain/GTM background is preferred.",Industry Type: Beverage,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Pyspark', 'Azure', 'Power BI', 'Github', 'Azure Databricks', 'Tableau', 'ADO', 'Scala programming', 'SQL', 'Azure Data Factory', 'Azure Machine learning', 'Data Lakehouse', 'Azure Data Engineering', 'CI/CD', 'Data Warehousing', 'Data Analytics', 'AWS', 'Python']",2025-06-12 14:43:27
Associate- Referral - Decision Science / Data Science,Axtria,3 - 5 years,Not Disclosed,['Gurugram'],"Position Summary \n\nThis Requisition is for the Employee Referral Campaign.\n\nWe are seeking high-energy, driven, and innovative Data Scientists to join our Data Science Practice to develop new, specialized capabilities for Axtria, and to accelerate the company’s growth by supporting our clients’ commercial & clinical strategies.\n\n Job Responsibilities \n\nBe an Individual Contributor tothe Data Science team and solve real-world problems using cutting-edge capabilities and emerging technologies.\n\nHelp clients translate the business use cases they are trying to crack into data science solutions. Provide genuine assistance to users by advising them on how to leverage Dataiku DSS to implement data science projects, from design to production.\n\nData Source Configuration, Maintenance, Document and maintain work-instructions.\n\nDeep working onmachine learning frameworks such as TensorFlow, Caffe, Keras, SparkML\n\nExpert knowledge in Statistical and Probabilistic methods such as SVM, Decision-Trees, Clustering\n\nExpert knowledge of python data-science and math packages such as NumPy , Pandas, Sklearn\n\nProficiency in object-oriented languages (Java and/or Kotlin),Python and common machine learning frameworks(TensorFlow, NLTK, Stanford NLP, Ling Pipe etc\n\n\n Education \n\nBachelor Equivalent - Engineering\nMaster's Equivalent - Engineering\n\n Work Experience \n\nData Scientist 3-5 years of relevant experience in advanced statistical and mathematical models and predictive modeling using Python. Experience in the data science space prior relevant experience in Artificial intelligence and machine Learning algorithms for developing scalable models supervised and unsupervised techniques likeNLP and deep Learning Algorithms. Ability to build scalable models using Python, R-Studio, R Shiny, PySpark, Keras, and TensorFlow. Experience in delivering data science projects leveraging cloud infrastructure. Familiarity with cloud technology such as AWS / Azure and knowledge of AWS tools such as S3, EMR, EC2, Redshift, and Glue; viz tools like Tableau and Power BI. Relevant experience in Feature Engineering, Feature Selection, and Model Validation on Big Data. Knowledge of self-service analytics platforms such as Dataiku/ KNIME/ Alteryx will be an added advantage.\n\nML Ops Engineering 3-5 years of experience with MLOps Frameworks like Kubeflow, MLFlow, Data Robot, Airflow, etc., experience with Docker and Kubernetes, OpenShift. Prior experience in end-to-end automated ecosystems including, but not limited to, building data pipelines, developing & deploying scalable models, orchestration, scheduling, automation, and ML operations. Ability to design and implement cloud solutions and ability to build MLOps pipelines on cloud solutions (AWS, MS Azure, or GCP). Programming languages like Python, Go, Ruby, or Bash, a good understanding of Linux, knowledge of frameworks such as Keras, PyTorch, TensorFlow, etc. Ability to understand tools used by data scientists and experience with software development and test automation. Good understanding of advanced AI/ML algorithms & their applications.\n\nGen AI :Minimum of 4-6 years develop, test, and deploy Python based applications on Azure/AWS platforms.Must have basic knowledge on concepts of Generative AI / LLMs / GPT.Deep understanding of architecture and work experience on Web Technologies.Python, SQL hands-on experience.Expertise in any popular python web frameworks e.g. flask, Django etc. Familiarity with frontend technologies like HTML, JavaScript, REACT.Be an Individual Contributor in the Analytics and Development team and solve real-world problems using cutting-edge capabilities and emerging technologies based on LLM/GenAI/GPT.Can interact with client on GenAI related capabilities and use cases.",Industry Type: Analytics / KPO / Research,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'gpm', 'machine learning', 'python data', 'statistics', 'kubernetes', 'microsoft azure', 'numpy', 'javascript', 'sql', 'docker', 'pandas', 'tensorflow', 'java', 'django', 'predictive modeling', 'python web framework', 'mathematical modeling', 'pytorch', 'keras', 'aws', 'flask', 'advanced statistical']",2025-06-12 14:43:30
"Sr. Data Engineer, R&D Data Catalyst Team",Amgen Inc,7 - 9 years,Not Disclosed,['Hyderabad'],"The R&D Data Catalyst Team is responsible for buildingData Searching, Cohort Building, and Knowledge Management tools that provide the Amgen scientific community with visibility to Amgens wealth of human datasets, projects and study histories, and knowledge over various scientific findings. These solutions are pivotal tools in Amgens goal to accelerate the speed of discovery, and speed to market of advanced precision medications.\nThe Data Engineer will be responsible for the end-to-end development of an enterprise analytics and data mastering solution leveraging Databricks and Power BI. This role requiresexpertise in both data architecture and analytics, with the ability to create scalable, reliable, and high-performing enterprise solutions that research cohort-building and advanced research pipeline.The ideal candidate will have experience creating and surfacing large unifiedrepositories of human data, based on integrations from multiple repositories and solutions, and be exceptionally skilled with data analysis and profiling.\nYou will collaborate closely with stakeholders, product team members, and related IT teams, to design and implement data models, integrate data from various sources, and ensure best practices for data governance and security. The ideal candidate will have a strong background in data warehousing, ETL, Databricks, Power BI, and enterprise data mastering.\nRoles & Responsibilities:\nDesign and build scalable enterprise analytics solutions using Databricks, Power BI, and other modern data tools.\nLeverage data virtualization, ETL, and semantic layers to balance need for unification, performance, and data transformation with goal to reduce data proliferation\nBreak down features into work that aligns with the architectural direction runway\nParticipate hands-on in pilots and proofs-of-concept for new patterns\nCreate robust documentation from data analysis and profiling, and proposed designs and data logic\nDevelop advanced sql queries to profile, and unify data\nDevelop data processing code in sql, along with semantic views to prepare data for reporting\nDevelop PowerBI Models and reporting packages\nDesign robust data models, and processing layers, that support both analytical processing and operational reporting needs.\nDesign and develop solutions based on best practices for data governance, security, and compliance within Databricks and Power BI environments.\nEnsure the integration of data systems with other enterprise applications, creating seamless data flows across platforms.\nDevelop and maintain Power BI solutions, ensuring data models and reports are optimized for performance and scalability.\nCollaborate with stakeholders to define data requirements, functional specifications, and project goals.\nContinuously evaluate and adopt new technologies and methodologies to enhance the architecture and performance of data solutions.\nBasic Qualifications and Experience:\nMasters degree with 1 to 3years of experience in Data Engineering OR\nBachelors degree with 4 to 5 years of experience in Data Engineering\nDiploma and 7 to 9 years of experience in Data Engineering.\nFunctional Skills:\nMust-Have Skills:\nMinimum of 3 years of hands-on experience with BI solutions (Preferrable Power BI or Business Objects) including report development, dashboard creation, and optimization.\nMinimum of 3years of hands-on experience building Change-data-capture (CDC) ETL pipelines, data warehouse design and build, and enterprise-level data management.\nHands-on experience with Databricks, including data engineering, optimization, and analytics workloads.\nDeep understanding of Power BI, including model design, DAX, and Power Query.\nProven experience designing and implementing data mastering solutions and data governance frameworks.\nExpertise in cloud platforms (AWS), data lakes, and data warehouses.\nStrong knowledge of ETL processes, data pipelines, and integration technologies.\nStrong communication and collaboration skills to work with cross-functional teams and senior leadership.\nAbility to assess business needs and design solutions that align with organizational goals.\nExceptional hands-on capabilities with data profiling, data transformation, data mastering\nSuccess in mentoring and training team members\nGood-to-Have Skills:\nExperience in developing differentiated and deliverable solutions\nExperience with human data, ideally human healthcare data\nFamiliarity with laboratory testing, patient data from clinical care, HL7, FHIR, and/or clinical trial data management\nProfessional Certifications (please mention if the certification is preferred or mandatory for the role):\nITIL Foundation or other relevant certifications (preferred)\nSAFe Agile Practitioner (6.0)\nMicrosoft Certified: Data Analyst Associate (Power BI) or related certification.\nDatabricks Certified Professional or similar certification.\nSoft Skills:\nExcellent analytical and troubleshooting skills\nDeep intellectual curiosity\nHighest degree of initiative and self-motivation\nStrong verbal and written communication skills, including presentation to varied audiences of complex technical/business topics\nConfidence technical leader\nAbility to work effectively with global, virtual teams, specifically including leveraging of tools and artifacts to assure clear and efficient collaboration across time zones\nAbility to manage multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals\nStrong problem solving, analytical skills;\nAbility to learn quickly and retain and synthesize complex information from diverse sources",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Engineering', 'data analysis', 'ETL processes', 'DAX', 'Business Objects', 'data warehouse design', 'ETL', 'PowerBI Models', 'AWS', 'Power Query']",2025-06-12 14:43:33
Sr Data Engineering Manager,Amgen Inc,12 - 15 years,Not Disclosed,['Hyderabad'],"ABOUT THE ROLE\nRole Description:\nWe are seeking a Senior Data Engineering Manager with a strong background in Regulatory or Integrated Product Teams within the Biotech or Pharmaceutical domain. This role will lead the end-to-end data strategy and execution for regulatory product submissions, lifecycle management, and compliance reporting, ensuring timely and accurate delivery of regulatory data assets across global markets.You will be embedded in a cross-functional Regulatory Integrated Product Team (IPT) and serve as the data and technology lead, driving integration between scientific, regulatory, and engineering functions to support submission-ready data and regulatory intelligence solutions.\nRoles & Responsibilities:\nFunctional Skills:\nLead the engineering strategy and implementation for end-to-end regulatory operations, including data ingestion, transformation, integration, and delivery across regulatory systems.\nServe as the data engineering SME in the Integrated Product Team (IPT) to support regulatory submissions, agency interactions, and lifecycle updates.\nCollaborate with global regulatory affairs, clinical, CMC, quality, safety, and IT teams to gather submission data requirements and translate them into data engineering solutions.\nManage and oversee the development of data pipelines, data models, and metadata frameworks that support submission data standards (e.g., eCTD, IDMP, SPL, xEVMPD).\nEnable integration and reporting across regulatory information management systems (RIMS), EDMS, clinical trial systems, and lab data platforms.\nImplement data governance, lineage, validation, and audit trails for regulatory data workflows, ensuring GxP and regulatory compliance.\nGuide the development of automation solutions, dashboards, and analytics that improve visibility into submission timelines, data quality, and regulatory KPIs.\nEnsure interoperability between regulatory data platforms and enterprise data lakes or lakehouses for cross-functional reporting and insights.\nCollaborate with IT, data governance, and enterprise architecture teams to ensure alignment with overall data strategy and compliance frameworks.\nDrive innovation by evaluating emerging technologies in data engineering, graph data, knowledge management, and AI for regulatory intelligence.\nLead, mentor, and coach a small team of data engineers and analysts, fostering a culture of excellence, innovation, and delivery.\nDrive Agile and Scaled Agile (SAFe) methodologies, managing sprint backlogs, prioritization, and iterative improvements to enhance team velocity and project delivery.\nStay up-to-date with emerging data technologies, industry trends, and best practices, ensuring the organization leverages the latest innovations in data engineering and architecture.\nMust-Have Skills:\n812 years of experience in data engineering or data architecture, with 3+ years in a senior or managerial capacity, preferably within the biotech or pharmaceutical industry.\nProven experience supporting regulatory functions, including submissions, tracking, and reporting for FDA, EMA, and other global authorities.\nExperience with ETL/ELT tools, data pipelines, and cloud-based data platforms (e.g., Databricks, AWS, Azure, or GCP).\nFamiliarity with regulatory standards and data models such as eCTD, IDMP, HL7, CDISC, and xEVMPD.\nDeep understanding of GxP data compliance, audit requirements, and regulatory submission processes.\nExperience with tools like Power BI, Tableau, or Qlik for regulatory dashboarding and visualization is a plus.\nStrong project management, stakeholder communication, and leadership skills, especially in matrixed, cross-functional environments.\nAbility to translate technical capabilities into regulatory and business outcomes.Prepare team members for stakeholder discussions by helping assess data costs, access requirements, dependencies, and availability for business scenarios.\nGood-to-Have Skills:\nPrior experience working on integrated product teams or regulatory transformation programs.\nKnowledge of Regulatory Information Management Systems (RIMS), Veeva Vault RIM, or Master Data Management (MDM) in regulated environments.\nFamiliarity with Agile/SAFe methodologies and DevOps/DataOps best practices.\nEducation and Professional Certifications\n12 to 15 years of experience in Computer Science, IT or related field\nScaled Agile SAFe certification preferred\nProject Management certifications preferred\nSoft Skills:\nExcellent analytical and troubleshooting skills\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation\nAbility to manage multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals\nStrong presentation and public speaking skills",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'engineering strategy', 'DevOps', 'Project Management', 'DataOps', 'Agile', 'data strategy']",2025-06-12 14:43:35
"Senior data engineer - Python, Pyspark, AWS - 5+ years Gurgaon",One of the largest insurance providers.,5 - 10 years,Not Disclosed,['Gurugram'],"Senior data engineer - Python, Pyspark, AWS - 5+ years Gurgaon\n\nSummary: An excellent opportunity for someone having a minimum of five years of experience with expertise in building data pipelines. A person must have experience in Python, Pyspark and AWS.\n\nLocation- Gurgaon (Hybrid)\n\nYour Future Employer- One of the largest insurance providers.\n\nResponsibilities-\nTo design, develop, and maintain large-scale data pipelines that can handle large datasets from multiple sources.\nReal-time data replication and batch processing of data using distributed computing platforms like Spark, Kafka, etc.\nTo optimize the performance of data processing jobs and ensure system scalability and reliability.\nTo collaborate with DevOps teams to manage infrastructure, including cloud environments like AWS.\nTo collaborate with data scientists, analysts, and business stakeholders to develop tools and platforms that enable advanced analytics and reporting.\n\nRequirements-\nHands-on experience with AWS services such as S3, DMS, Lambda, EMR, Glue, Redshift, RDS (Postgres) Athena, Kinesics, etc.\nExpertise in data modeling and knowledge of modern file and table formats.\nProficiency in programming languages such as Python, PySpark, and SQL/PLSQL for implementing data pipelines and ETL processes.\nExperience data architecting or deploying Cloud/Virtualization solutions (Like Data Lake, EDW, Mart ) in the enterprise.\nCloud/hybrid cloud (preferably AWS) solution for data strategy for Data lake, BI and Analytics.\nWhat is in for you-\nA stimulating working environment with equal employment opportunities.\nGrowing of skills while working with industry leaders and top brands.\nA meritocratic culture with great career progression.\n\nReach us- If you feel that you are the right fit for the role please share your updated CV at randhawa.harmeen@crescendogroup.in\n\nDisclaimer- Crescendo Global specializes in Senior to C-level niche recruitment. We are passionate about empowering job seekers and employers with an engaging memorable job search and leadership hiring experience. Crescendo Global does not discriminate on the basis of race, religion, color, origin, gender, sexual orientation, age, marital status, veteran status, or disability status.",Industry Type: Insurance,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Pyspark', 'Data Pipeline', 'AWS', 'Data Ingestion', 'Data Engineering', 'Data Processing']",2025-06-12 14:43:38
Senior Engineering Manager - Data Operations,Amgen Inc,12 - 15 years,Not Disclosed,['Hyderabad'],"ABOUT THE ROLE\nRole Description:\nWe are seeking a seasoned Senior Engineering Manager(Data Engineering) to lead the end-to-end management of enterprise data assets and operational data workflows. This role is critical in ensuring the availability, quality, consistency, and timeliness of data across platforms and functions, supporting analytics, reporting, compliance, and digital transformation initiatives.As a senior leader in the data organization, you will oversee the day-to-day data operations, manage a team of data professionals, and drive process excellence in data intake, transformation, validation, and delivery. You will work closely with cross-functional teams including data engineering, analytics, IT, governance, and business stakeholders to align operational data capabilities with enterprise needs.\nRoles & Responsibilities:\nLead and manage the enterprise data operations team, responsible for data ingestion, processing, validation, quality control, and publishing to various downstream systems.\nDefine and implement standard operating procedures for data lifecycle management, ensuring availability, accuracy, completeness, and integrity of critical data assets.\nOversee and continuously improve daily operational workflows, including scheduling, monitoring, and troubleshooting data jobs across cloud and on-premise environments.\nEstablish and track key data operations metrics (SLAs, throughput, latency, data quality, incident resolution) and drive continuous improvements.\nPartner with data engineering and platform teams to optimize pipelines, support new data integrations, and ensure scalability and resilience of operational data flows.\nCollaborate with data governance, compliance, and security teams to maintain regulatory compliance, data privacy, and access controls.\nServe as the primary escalation point for data incidents and outages, ensuring rapid response and root cause analysis.\nBuild strong relationships with business and analytics teams to understand data consumption patterns, prioritize operational needs, and align with business objectives.\nDrive adoption of best practices for documentation, metadata, lineage, and change management across data operations processes.\nMentor and develop a high-performing team of data operations analysts and leads.\nFunctional Skills:\nMust-Have Skills:\nExperience managing a team of data engineers in biotech/pharmadomain companies.\nExperience in designing and maintainingdata pipelines and analytics solutions that extract, transform, and load data from multiple source systems.\nDemonstrated hands-on experience with cloud platforms (AWS) and the ability to architect cost-effective and scalable data solutions.\nExperience managing data workflows on Databricks in cloud environments such as AWS, Azure, or GCP.\nStrong problem-solving skills with the ability to analyze complex data flow issues and implement sustainable solutions.\nWorking knowledge of SQL, Python, PySparkor scripting languages for process monitoring and automation.\nExperience collaborating with data engineering, analytics, IT operations, and business teams in a matrixed organization.\nFamiliarity with data governance, metadata management, access control, and regulatory requirements (e.g., GDPR, HIPAA, SOX).\nExcellent leadership, communication, and stakeholder engagement skills.\nWell versed with full stack development& DataOps automation, logging & observability frameworks, and pipeline orchestration tools.\nStrong analytical and problem-solving skills to address complex data challenges.\nEffective communication and interpersonal skills to collaborate with cross-functional teams.\nGood-to-Have Skills:\nData Engineering Management experience in Biotech/Life Sciences/Pharma\nExperience using graph databases such as Stardog or Marklogic or Neo4J or Allegrograph, etc.\nEducation and Professional Certifications\n12 to 15 years of experience in Computer Science, IT or related field\nDatabricks Certificate preferred\nScaled Agile SAFe certification preferred\nExperience in life sciences, healthcare, or other regulated industries with large-scale operational data environments.\nFamiliarity with incident and change management processes (e.g., ITIL).\nSoft Skills:\nExcellent analytical and troubleshooting skills\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation\nAbility to manage multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals\nStrong presentation and public speaking skills.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Operations', 'Azure', 'Data Engineering', 'Neo4J', 'GCP', 'Engineering Management', 'troubleshooting', 'Stardog', 'Marklogic', 'AWS']",2025-06-12 14:43:40
"Walk-In Senior Data Engineer - DataStage, Azure & Power BI",Net Connect,6 - 10 years,5-11 Lacs P.A.,['Hyderabad( Madhapur )'],"Greetings from NCG!\n\nWe have a opening for Snowflake Developer role in Hyderabad office!\nBelow JD for your reference\n\nJob Description:\n\nWe are hiring an experienced Senior Data Engineer with strong expertise in IBM DataStage, , and . The ideal candidate will be responsible for end-to-end data integration, transformation, and reporting solutions that drive business decisions.",,,,"['Azure Data Factory', 'Datastage', 'Etl Datastage']",2025-06-12 14:43:43
Senior Data Engineer : 7+ Years,Jayam Solutions Pvt Ltd - CMMI Level III Company,5 - 9 years,Not Disclosed,['Hyderabad( Madhapur )'],"Job Description:\nPosition: Sr.Data Engineer\nExperience: Minimum 7 years\nLocation: Hyderabad\nJob Summary:\n\nWhat Youll Do\n\nDesign and build efficient, reusable, and reliable data architecture leveraging technologies like Apache Flink, Spark, Beam and Redis to support large-scale, real-time, and batch data processing.\nParticipate in architecture and system design discussions, ensuring alignment with business objectives and technology strategy, and advocating for best practices in distributed data systems.\nIndependently perform hands-on development and coding of data applications and pipelines using Java, Scala, and Python, including unit testing and code reviews.\nMonitor key product and data pipeline metrics, identify root causes of anomalies, and provide actionable insights to senior management on data and business health.\nMaintain and optimize existing datalake infrastructure, lead migrations to lakehouse architectures, and automate deployment of data pipelines and machine learning feature engineering requests.\nAcquire and integrate data from primary and secondary sources, maintaining robust databases and data systems to support operational and exploratory analytics.\nEngage with internal stakeholders (business teams, product owners, data scientists) to define priorities, refine processes, and act as a point of contact for resolving stakeholder issues.\nDrive continuous improvement by establishing and promoting technical standards, enhancing productivity, monitoring, tooling, and adopting industry best practices.\n\nWhat Youll Bring\n\nBachelors degree or higher in Computer Science, Engineering, or a quantitative discipline, or equivalent professional experience demonstrating exceptional ability.\n7+ years of work experience in data engineering and platform engineering, with a proven track record in designing and building scalable data architectures.\nExtensive hands-on experience with modern data stacks, including datalake, lakehouse, streaming data (Flink, Spark), and AWS or equivalent cloud platforms.\nCloud - AWS\nApache Flink/Spark , Redis\nDatabase platform- Databricks.\nProficiency in programming languages such as Java, Scala, and Python(Good to have) for data engineering and pipeline development.\nExpertise in distributed data processing and caching technologies, including Apache Flink, Spark, and Redis.\nExperience with workflow orchestration, automation, and DevOps tools (Kubernetes,git,Terraform, CI/CD).\nAbility to perform under pressure, managing competing demands and tight deadlines while maintaining high-quality deliverables.\nStrong passion and curiosity for data, with a commitment to data-driven decision making and continuous learning.\nExceptional attention to detail and professionalism in report and dashboard creation.\nExcellent team player, able to collaborate across diverse functional groups and communicate complex technical concepts clearly.\nOutstanding verbal and written communication skills to effectively manage and articulate the health and integrity of data and systems to stakeholders.\n\nPlease feel free to contact us: 9440806850\nEmail ID : careers@jayamsolutions.com",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Apache Flink', 'Redis', 'Spark', 'Python', 'SCALA', 'Ci/Cd', 'Devops', 'AWS']",2025-06-12 14:43:45
Data Quality Analyst,Yallas Technology Solutions Opc,5 - 10 years,Not Disclosed,[],"Title: Data Quality Analyst/Developer\nDuration: 6 months to 1 year contract\nLocation:  Remote\nNotice period - Immediate to 7 days\nUAN /EPFO Report Required\n\nWork Experience:\n5 + years of this experience - Experience doing Data Emendation\nDesign/Develop Rules, monitoring mechanisms, notification\nDesign/Develop UI, Workflows, security\nDesign/Develop analytics (overall DQ reporting, usage statistics, etc).\nDesign/Develop migration activities to migrate existing DQ assets between our existing DQ platform and new DQ platform.\nDesign integration with MDM & Catalog (as needed)\nMonitor system performance and suggest optimization strategies (as needed).\nWork with DT to maintain system - patches, backups, etc.\nWork with LYB's Data Stewards to support their governance activities.\nTesting\n\nThe DQ Analyst/Developer should have experience with IMDC (for the sake of our example) cloud DQ and observability, JSON (depending on tool) Deep SQL skills, Integration tools/methodologies - API as well as ETL, Data Analysis, Snowflake or Databricks knowledge (for lineage), Power BI (nice to have), SAP ECC knowledge (nice to have), experience with cloud platforms (Azure, AWS, Google).\nIf you are interested please share required details along with resume\nFull Name:\nCurrent or Previous organization:\nCurrent Location:\nTotal Experience:\nRelevant experience as Python Developer:\nhow many years of experience In Azure, AWS, Google\nHow many years of experience in UI, Workflows, security\nWorking as full time or contract:\nReason for job change:\nAny other offers inhand:\nCurrent CTC:\nexpected CTC:\nNotice Period:\nemail id:\ncontact Number :\nDomain name:\nare you ok to work Cotractual role?:\nshare your aadhar or pan card for the verification",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Temporary/Contractual","['Data quality analyst', 'cloud data quality', 'Azure', 'data quality developer', 'JSON', 'google', 'Informatica', 'AWS']",2025-06-12 14:43:48
Senior Azure Data Engineer,Cloud Angles Digital Transformation,8 - 12 years,Not Disclosed,['Hyderabad'],"Job Summary:\nWe are seeking a highly skilled Data Engineer with expertise in leveraging Data Lake architecture and the Azure cloud platform to develop, deploy, and optimise data-driven solutions. . You will play a pivotal role in transforming raw data into actionable insights, supporting strategic decision-making across the organisation.\nResponsibilities\nDesign and implement scalable data science solutions using Azure Data Lake, Azure Data Bricks, Azure Data Factory and related Azure services.\nDevelop, train, and deploy machine learning models to address business challenges.\nCollaborate with data engineering teams to optimise data pipelines and ensure seamless data integration within Azure cloud infrastructure.\nConduct exploratory data analysis (EDA) to identify trends, patterns, and insights.\nBuild predictive and prescriptive models to support decision-making processes.\nExpertise in developing end-to-end Machine learning lifecycle utilizing crisp-DM which includes of data collection, cleansing, visualization, preprocessing, model development, model validation and model retraining\nProficient in building and implementing RAG systems that enhance the accuracy and relevance of model outputs by integrating retrieval mechanisms with generative models.\nEnsure data security, compliance, and governance within the Azure cloud ecosystem.\nMonitor and optimise model performance and scalability in production environments.\nPrepare clear and concise documentation for developed models and workflows.\nSkills Required:\nGood experience in using Pyspark, Python, MLops (Optional), ML flow (Optional), Azure Data Lake Storage. Unity Catalog\nWorked and utilized data from various RDBMS like MYSQL, SQL Server, Postgres and NoSQL databases like MongoDB, Cassandra, Redis and graph DB like Neo4j, Grakn.\nProven experience as a Data Engineer with a strong focus on Azure cloud platform and Data Lake architecture.\nProficiency in Python, Pyspark,\nHands-on experience with Azure services such as Azure Data Lake, Azure Synapse Analytics, Azure Machine Learning, Azure Databricks, and Azure Functions.\nStrong knowledge of SQL and experience in querying large datasets from Data Lakes.\nFamiliarity with data engineering tools and frameworks for data ingestion and transformation in Azure.\nExperience with version control systems (e.g., Git) and CI/CD pipelines for machine learning projects.\nExcellent problem-solving skills and the ability to work collaboratively in a team environment.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Data Factory', 'Azure Data Engineering', 'Azure Databricks', 'Pyspark', 'Azure Data Lake', 'Python']",2025-06-12 14:43:50
Senior Data Engineer,Amgen Inc,3 - 7 years,Not Disclosed,['Hyderabad'],"Role Description:\nWe are looking for highly motivated expert Senior Data Engineer who can own the design & development of complex data pipelines, solutions and frameworks. The ideal candidate will be responsible to design, develop, and optimize data pipelines, data integration frameworks, and metadata-driven architectures that enable seamless data access and analytics. This role prefers deep expertise in big data processing, distributed computing, data modeling, and governance frameworks to support self-service analytics, AI-driven insights, and enterprise-wide data management.\nRoles & Responsibilities:\nDesign, develop, and maintain scalable ETL/ELT pipelines to support structured, semi-structured, and unstructured data processing across the Enterprise Data Fabric.\nImplement real-time and batch data processing solutions, integrating data from multiple sources into a unified, governed data fabric architecture.\nOptimize big data processing frameworks using Apache Spark, Hadoop, or similar distributed computing technologies to ensure high availability and cost efficiency.\nWork with metadata management and data lineage tracking tools to enable enterprise-wide data discovery and governance.\nEnsure data security, compliance, and role-based access control (RBAC) across data environments.\nOptimize query performance, indexing strategies, partitioning, and caching for large-scale data sets.\nDevelop CI/CD pipelines for automated data pipeline deployments, version control, and monitoring.\nImplement data virtualization techniques to provide seamless access to data across multiple storage systems.\nCollaborate with cross-functional teams, including data architects, business analysts, and DevOps teams, to align data engineering strategies with enterprise goals.\nStay up to date with emerging data technologies and best practices, ensuring continuous improvement of Enterprise Data Fabric architectures.\nMust-Have Skills:\nHands-on experience in data engineering technologies such as Databricks, PySpark, SparkSQL Apache Spark, AWS, Python, SQL, and Scaled Agile methodologies.\nProficiency in workflow orchestration, performance tuning on big data processing.\nStrong understanding of AWS services\nExperience with Data Fabric, Data Mesh, or similar enterprise-wide data architectures.\nAbility to quickly learn, adapt and apply new technologies\nStrong problem-solving and analytical skills\nExcellent communication and teamwork skills\nExperience with Scaled Agile Framework (SAFe), Agile delivery practices, and DevOps practices.\nGood-to-Have Skills:\nGood to have deep expertise in Biotech & Pharma industries\nExperience in writing APIs to make the data available to the consumers\nExperienced with SQL/NOSQL database, vector database for large language models\nExperienced with data modeling and performance tuning for both OLAP and OLTP databases\nExperienced with software engineering best-practices, including but not limited to version control (Git, Subversion, etc.), CI/CD (Jenkins, Maven etc.), automated unit testing, and Dev Ops\nEducation and Professional Certifications\n9 to 12 years of Computer Science, IT or related field experience\nAWS Certified Data Engineer preferred\nDatabricks Certificate preferred\nScaled Agile SAFe certification preferred\nSoft Skills:\nExcellent analytical and troubleshooting skills.\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nTeam-oriented, with a focus on achieving team goals.\nAbility to learn quickly, be organized and detail oriented.\nStrong presentation and public speaking skills.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Maven', 'SparkSQL Apache Spark', 'PySpark', 'Subversion', 'OLAP', 'Scaled Agile methodologies', 'SQL', 'Scaled Agile Framework', 'Jenkins', 'NOSQL database', 'Git', 'Databricks', 'Data Fabric', 'Data Mesh', 'AWS', 'Python']",2025-06-12 14:43:53
Senior Data Engineer,Amgen Inc,3 - 8 years,Not Disclosed,['Hyderabad'],"Role Description:\nWe are looking for highly motivated expert Senior Data Engineer who can own the design & development of complex data pipelines, solutions and frameworks. The ideal candidate will be responsible to design, develop, and optimize data pipelines, data integration frameworks, and metadata-driven architectures that enable seamless data access and analytics. This role prefers deep expertise in big data processing, distributed computing, data modeling, and governance frameworks to support self-service analytics, AI-driven insights, and enterprise-wide data management.\nRoles & Responsibilities:\nDesign, develop, and maintain scalable ETL/ELT pipelines to support structured, semi-structured, and unstructured data processing across the Enterprise Data Fabric.\nImplement real-time and batch data processing solutions, integrating data from multiple sources into a unified, governed data fabric architecture.\nOptimize big data processing frameworks using Apache Spark, Hadoop, or similar distributed computing technologies to ensure high availability and cost efficiency.\nWork with metadata management and data lineage tracking tools to enable enterprise-wide data discovery and governance.\nEnsure data security, compliance, and role-based access control (RBAC) across data environments.\nOptimize query performance, indexing strategies, partitioning, and caching for large-scale data sets.\nDevelop CI/CD pipelines for automated data pipeline deployments, version control, and monitoring.\nImplement data virtualization techniques to provide seamless access to data across multiple storage systems.\nCollaborate with cross-functional teams, including data architects, business analysts, and DevOps teams, to align data engineering strategies with enterprise goals.\nStay up to date with emerging data technologies and best practices, ensuring continuous improvement of Enterprise Data Fabric architectures.\nMust-Have Skills:\nHands-on experience in data engineering technologies such as Databricks, PySpark, SparkSQL Apache Spark, AWS, Python, SQL, and Scaled Agile methodologies.\nProficiency in workflow orchestration, performance tuning on big data processing.\nStrong understanding of AWS services\nExperience with Data Fabric, Data Mesh, or similar enterprise-wide data architectures.\nAbility to quickly learn, adapt and apply new technologies\nStrong problem-solving and analytical skills\nExcellent communication and teamwork skills\nExperience with Scaled Agile Framework (SAFe), Agile delivery practices, and DevOps practices.\nGood-to-Have Skills:\nGood to have deep expertise in Biotech & Pharma industries\nExperience in writing APIs to make the data available to the consumers\nExperienced with SQL/NOSQL database, vector database for large language models\nExperienced with data modeling and performance tuning for both OLAP and OLTP databases\nExperienced with software engineering best-practices, including but not limited to version control (Git, Subversion, etc.), CI/CD (Jenkins, Maven etc.), automated unit testing, and DevOps\nEducation and Professional Certifications\nMasters degree and 3 to 4 + years of Computer Science, IT or related field experience\nOR\nBachelors degree and 5 to 8 + years of Computer Science, IT or related field experience\nAWS Certified Data Engineer preferred\nDatabricks Certificate preferred\nScaled Agile SAFe certification preferred\nSoft Skills:\nExcellent analytical and troubleshooting skills.\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nTeam-oriented, with a focus on achieving team goals.\nAbility to learn quickly, be organized and detail oriented.\nStrong presentation and public speaking skills.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Git', 'PySpark', 'CI/CD', 'Databricks', 'ETL', 'NOSQL', 'AWS', 'data integration', 'SQL', 'Apache Spark', 'Python']",2025-06-12 14:43:55
Senior Data Engineer,Amgen Inc,9 - 12 years,Not Disclosed,['Hyderabad'],"Role Description:\nWe are looking for highly motivated expert Senior Data Engineer who can own the design & development of complex data pipelines, solutions and frameworks. The ideal candidate will be responsible to design, develop, and optimize data pipelines, data integration frameworks, and metadata-driven architectures that enable seamless data access and analytics. This role prefers deep expertise in big data processing, distributed computing, data modeling, and governance frameworks to support self-service analytics, AI-driven insights, and enterprise-wide data management.\nRoles & Responsibilities:\nDesign, develop, and maintain scalable ETL/ELT pipelines to support structured, semi-structured, and unstructured data processing across the Enterprise Data Fabric.\nImplement real-time and batch data processing solutions, integrating data from multiple sources into a unified, governed data fabric architecture.\nOptimize big data processing frameworks using Apache Spark, Hadoop, or similar distributed computing technologies to ensure high availability and cost efficiency.\nWork with metadata management and data lineage tracking tools to enable enterprise-wide data discovery and governance.\nEnsure data security, compliance, and role-based access control (RBAC) across data environments.\nOptimize query performance, indexing strategies, partitioning, and caching for large-scale data sets.\nDevelop CI/CD pipelines for automated data pipeline deployments, version control, and monitoring.\nImplement data virtualization techniques to provide seamless access to data across multiple storage systems.\nCollaborate with cross-functional teams, including data architects, business analysts, and DevOps teams, to align data engineering strategies with enterprise goals.\nStay up to date with emerging data technologies and best practices, ensuring continuous improvement of Enterprise Data Fabric architectures.\nMust-Have Skills:\nHands-on experience in data engineering technologies such as Databricks, PySpark, SparkSQL Apache Spark, AWS, Python, SQL, and Scaled Agile methodologies.\nProficiency in workflow orchestration, performance tuning on big data processing.\nStrong understanding of AWS services\nExperience with Data Fabric, Data Mesh, or similar enterprise-wide data architectures.\nAbility to quickly learn, adapt and apply new technologies\nStrong problem-solving and analytical skills\nExcellent communication and teamwork skills\nExperience with Scaled Agile Framework (SAFe), Agile delivery practices, and DevOps practices.\nGood-to-Have Skills:\nGood to have deep expertise in Biotech & Pharma industries\nExperience in writing APIs to make the data available to the consumers\nExperienced with SQL/NOSQL database, vector database for large language models\nExperienced with data modeling and performance tuning for both OLAP and OLTP databases\nExperienced with software engineering best-practices, including but not limited to version control (Git, Subversion, etc.), CI/CD (Jenkins, Maven etc.), automated unit testing, and Dev Ops\nEducation and Professional Certifications\n9 to 12 years of Computer Science, IT or related field experience\nAWS Certified Data Engineer preferred\nDatabricks Certificate preferred\nScaled Agile SAFe certification preferred\nSoft Skills:\nExcellent analytical and troubleshooting skills.\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nTeam-oriented, with a focus on achieving team goals.\nAbility to learn quickly, be organized and detail oriented.\nStrong presentation and public speaking skills.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data engineering', 'performance tuning', 'data security', 'data processing', 'Hadoop', 'Apache Spark', 'SQL', 'CI/CD', 'troubleshooting', 'big data', 'aws', 'ETL', 'Python']",2025-06-12 14:43:57
Senior Data Engineer - AWS,Blend360 India,6 - 10 years,Not Disclosed,['Hyderabad'],"We are looking for an experienced Senior Data Engineer with a strong foundation in Python, SQL, and Spark , and hands-on expertise in AWS, Databricks . In this role, you will build and maintain scalable data pipelines and architecture to support analytics, data science, and business intelligence initiatives. You ll work closely with cross-functional teams to drive data reliability, quality, and performance.\nResponsibilities:\nDesign, develop, and optimize scalable data pipelines using Databricks in AWS such as Glue, S3, Lambda, EMR, Databricks notebooks, workflows and jobs.\nBuilding data lake in WS Databricks.\nBuild and maintain robust ETL/ELT workflows using Python and SQL to handle structured and semi-structured data.\nDevelop distributed data processing solutions using Apache Spark or PySpark .\nPartner with data scientists and analysts to provide high-quality, accessible, and well-structured data.\nEnsure data quality, governance, security, and compliance across pipelines and data stores.\nMonitor, troubleshoot, and improve the performance of data systems and pipelines.\nParticipate in code reviews and help establish engineering best practices.\nMentor junior data engineers and support their technical development.\n\n\nRequirements\nBachelors or masters degree in computer science, Engineering, or a related field.\n5+ years of hands-on experience in data engineering , with at lea",Industry Type: Industrial Automation,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Automation', 'Version control', 'GIT', 'Workflow', 'Data quality', 'Business intelligence', 'Analytics', 'SQL', 'Python']",2025-06-12 14:44:00
Senior Data Engineer,Amgen Inc,3 - 7 years,Not Disclosed,['Hyderabad'],"What you will do\nRole Description:\nWe are seeking a Senior Data Engineer with expertise in Graph Data technologies to join our data engineering team and contribute to the development of scalable, high-performance data pipelines and advanced data models that power next-generation applications and analytics. This role combines core data engineering skills with specialized knowledge in graph data structures, graph databases, and relationship-centric data modeling, enabling the organization to leverage connected data for deep insights, pattern detection, and advanced analytics use cases. The ideal candidate will have a strong background in data architecture, big data processing, and Graph technologies and will work closely with data scientists, analysts, architects, and business stakeholders to design and deliver graph-based data engineering solutions.\nRoles & Responsibilities:\nDesign, build, and maintain robust data pipelines using Databricks (Spark, Delta Lake, PySpark) for complex graph data processing workflows.\nOwn the implementation of graph-based data models, capturing complex relationships and hierarchies across domains.\nBuild and optimize Graph Databases such as Stardog, Neo4j, Marklogic or similar to support query performance, scalability, and reliability.\nImplement graph query logic using SPARQL, Cypher, Gremlin, or GSQL, depending on platform requirements.\nCollaborate with data architects to integrate graph data with existing data lakes, warehouses, and lakehouse architectures.\nWork closely with data scientists and analysts to enable graph analytics, link analysis, recommendation systems, and fraud detection use cases.\nDevelop metadata-driven pipelines and lineage tracking for graph and relational data processing.\nEnsure data quality, governance, and security standards are met across all graph data initiatives.\nMentor junior engineers and contribute to data engineering best practices, especially around graph-centric patterns and technologies.\nStay up to date with the latest developments in graph technology, graph ML, and network analytics.\nWhat we expect of you\nMust-Have Skills:\nHands-on experience in Databricks, including PySpark, Delta Lake, and notebook-based development.\nHands-on experience with graph database platforms such as Stardog, Neo4j, Marklogic etc.\nStrong understanding of graph theory, graph modeling, and traversal algorithms\nProficiency in workflow orchestration, performance tuning on big data processing\nStrong understanding of AWS services\nAbility to quickly learn, adapt and apply new technologies with strong problem-solving and analytical skills\nExcellent collaboration and communication skills, with experience working with Scaled Agile Framework (SAFe), Agile delivery practices, and DevOps practices.\nGood-to-Have Skills:\nGood to have deep expertise in Biotech & Pharma industries\nExperience in writing APIs to make the data available to the consumers\nExperienced with SQL/NOSQL database, vector database for large language models\nExperienced with data modeling and performance tuning for both OLAP and OLTP databases\nExperienced with software engineering best-practices, including but not limited to version control (Git, Subversion, etc.), CI/CD (Jenkins, Maven etc.), automated unit testing, and Dev Ops\nEducation and Professional Certifications\nMasters degree and 3 to 4 + years of Computer Science, IT or related field experience\nBachelors degree and 5 to 8 + years of Computer Science, IT or related field experience\nAWS Certified Data Engineer preferred\nDatabricks Certificate preferred\nScaled Agile SAFe certification preferred\nSoft Skills:\nExcellent analytical and troubleshooting skills.\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nTeam-oriented, with a focus on achieving team goals.\nAbility to learn quickly, be organized and detail oriented.\nStrong presentation and public speaking skills.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'SPARQL', 'Maven', 'PySpark', 'GSQL', 'Subversion', 'AWS services', 'Stardog', 'Cypher', 'SAFe', 'Jenkins', 'DevOps', 'Git', 'Neo4j', 'Delta Lake', 'Graph Databases', 'Spark', 'Marklogic', 'Gremlin']",2025-06-12 14:44:02
Analyst,Merkle B2b,0 - 2 years,Not Disclosed,['Mumbai'],"The purpose of this role is to deliver analysis inline with client business objectives, goals, and to maintain, develop and exceed client performance targets.\nJob Description:\nKey responsibilities:Understands the client needs in specific.\nEnsures crisp communication with clients and work as an interface between team members and client counterpart.\nDiscusses issues related to questionnaires with clients and suggest solutions for the sameUses specialised knowledge of market research tools / programming languages to understand the client requirements and build surveys/ deliver data tables as per the requirement with required quality and productivity levelsReviews project requirements and executes projects, under the direction of senior team members, per requirements by following the guidelines and deploying the tools/systems as applicableCreates and follows work allocation schedule and project plan\nLocation:\nDGS India - Mumbai - Goregaon Prism Tower\nBrand:\nMerkle\nTime Type:\nFull time\nContract Type:\nPermanent",Industry Type: Management Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Senior Analyst', 'Programming', 'Market research', 'Deployment', 'Project planning']",2025-06-12 14:44:05
Data Engineering Lead,Yotta Techports,10 - 15 years,30-35 Lacs P.A.,['Hyderabad'],"Responsibilities:\nLead and manage an offshore team of data engineers, providing strategic guidance, mentorship, and support to ensure the successful delivery of projects and the development of team members.\nCollaborate closely with onshore stakeholders to understand project requirements, allocate resources efficiently, and ensure alignment with client expectations and project timelines.\nDrive the technical design, implementation, and optimization of data pipelines, ETL processes, and data warehouses, ensuring scalability, performance, and reliability.\nDefine and enforce engineering best practices, coding standards, and data quality standards to maintain high-quality deliverables and mitigate project risks.\nStay abreast of emerging technologies and industry trends in data engineering, and provide recommendations for tooling, process improvements, and skill development.\nAssume a data architect role as needed, leading the design and implementation of data architecture solutions, data modeling, and optimization strategies.\nDemonstrate proficiency in AWS services such as:\nExpertise in cloud data services, including AWS services like Amazon Redshift, Amazon EMR, and AWS Glue, to design and implement scalable data solutions.\nExperience with cloud infrastructure services such as AWS EC2, AWS S3, to optimize data processing and storage.\nKnowledge of cloud security best practices, IAM roles, and encryption mechanisms to ensure data privacy and compliance.\nProficiency in managing or implementing cloud data warehouse solutions, including data modeling, schema design, performance tuning, and optimization techniques.\nDemonstrate proficiency in modern data platforms such as Snowflake and Databricks, including:\nDeep understanding of Snowflake's architecture, capabilities, and best practices for designing and implementing data warehouse solutions.\nHands-on experience with Databricks for data engineering, data processing, and machine learning tasks, leveraging Spark clusters for scalable data processing.\nAbility to optimize Snowflake and Databricks configurations for performance, scalability, and cost-effectiveness.\nManage the offshore team's performance, including resource allocation, performance evaluations, and professional development, to maximize team productivity and morale.\n\nQualifications:\nBachelor's degree in Computer Science, Engineering, or a related field; advanced degree preferred.\n10+ years of experience in data engineering, with a proven track record of leadership and technical expertise in managing complex data projects.\nProficiency in programming languages such as Python, Java, or Scala, as well as expertise in SQL and relational databases (e.g., PostgreSQL, MySQL).\nStrong understanding of distributed computing, cloud technologies (e.g., AWS), and big data frameworks (e.g., Hadoop, Spark).\nExperience with data architecture design, data modeling, and optimization techniques.\nExcellent communication, collaboration, and leadership skills, with the ability to effectively manage remote teams and engage with onshore stakeholders.\nProven ability to adapt to evolving project requirements and effectively prioritize tasks in a fast-paced environment.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Team Handling', 'Snowflake', 'Data Services', 'Cloud Infrastructure', 'Data Bricks']",2025-06-12 14:44:07
Senior Data Engineer,Conversehr Business Solutions,7 - 12 years,30-45 Lacs P.A.,['Hyderabad'],"What is the Data team responsible for?\nAs a Senior Data Engineer, youll be a key member of the Data & AI team. This team is responsible for designing and delivering data engineering, analytics, and generative AI solutions that drive meaningful business impact. Were looking for a pragmatic, results-driven problem solver who thrives in a fast-paced environment and is passionate about building solutions on a scale #MID_SENIOR_LEVEL\nThe ideal candidate has a strong technical foundation, a collaborative mindset, and the ability to navigate complex challenges. You should be comfortable working in a fast-moving, startup-like environment within an established enterprise, and should bring strong skill sets to adapt to new solutions fast. You will play a pivotal role in optimizing data infrastructure, enabling data-driven decision-making and integrating AI across the organization.\nWhat is the Lead Software Engineer (Senior Data Engineer) responsible for?\nServe as a hands-on technical lead, driving project execution and delivery in our growing data team based in the Hyderabad office.\nCollaborate closely with the U.S.-based team and cross-functional stakeholders to understand business needs and deliver scalable solutions.\nLead the initiative to build firmwide data models and master data management solutions for structured data (in Snowflake) and manage unstructured data using vector embeddings.\nBuild, maintain, and optimize robust data pipelines and frameworks to support business intelligence and operational workflows.\nDevelop dashboards and data visualizations that support strategic business decisions.\nStay current with emerging trends in data engineering and help implement best practices within the team.\nMentor and support junior engineers, fostering a culture of learning and technical excellence.\nWhat ideal qualifications, skills & experience would help someone to be successful?\nBachelors or master’s degree in computer science, data science, engineering, or a related field.\n7+ years of experience in data engineering including 3+ years in a technical leadership role.\nStrong SQL skills and hands-on experience with modern data pipeline technologies (e.g., Spark, Flink).\nDeep expertise in the Snowflake ecosystem, including data modeling, data warehousing, and master data management.\nProficiency in at least one programming language - Python preferred.\nExperience with Tableau and Alteryx is a plus.\nSelf-starter with a passion for learning new tools and technologies.\nStrong communication skills and a collaborative, ownership-driven mindset.\nWork Shift Timings - 2:00 PM - 11:00 PM IST",Industry Type: Investment Banking / Venture Capital / Private Equity,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Engineering', 'Snowflake', 'Python', 'flink', 'Data Pipeline', 'Spark', 'SQL']",2025-06-12 14:44:09
Data Science Lead,Protiviti India,9 - 14 years,25-40 Lacs P.A.,['Mumbai (All Areas)'],"Role & responsibilities\n8+ year bachelors or master’s degree from reputed University with concentration on finance, economics or other quantitative field such as statistics or engineering.\nManage multiple client engagements in Financial Services locally in India\nActively drive pre-sales, sales activities primarily for FS clients locally in Data Science Domain\nUnderstand client requirements in detail and create technical & commercial proposal\nDrive client conversations specifically for business development activities",,,,"['Data Science', 'Natural Language Processing', 'Presales', 'Machine Learning', 'AWS', 'GCP', 'Cloud Platform', 'Python']",2025-06-12 14:44:11
Data Engineer (C2H),First Mile Consulting,4 - 8 years,Not Disclosed,"['Hyderabad', 'Pune', 'Chennai', 'Bengaluru']","Very strong in python, pyspark and SQL. Good experience in any cloud . They use AWS but any cloud experience is ok. They will train on other things but if candidates have experience with ETL (like AWS Airflow), datalakes like Snowflake",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Temporary/Contractual","['Pyspark', 'Cloud', 'Data engineer', 'Python', 'Sql', 'Airflow']",2025-06-12 14:44:13
Lead Data Engineer (Immediate joiner),Decision Point,4 - 9 years,15-30 Lacs P.A.,"['Gurugram', 'Chennai']","Role & responsibilities\n• Assume ownership of Data Engineering projects from inception to completion.\nImplement fully operational Unified Data Platform solutions in production environments using technologies like Databricks, Snowflake, Azure Synapse etc.\nShowcase proficiency in Data Modelling and Data Architecture\nUtilize modern data transformation tools such as DBT (Data Build Tool) to streamline and automate data pipelines (nice to have).",,,,"['Pyspark', 'Azure Databricks', 'SQL', 'Azure Synapse', 'Python', 'Etl Pipelines', 'Airflow', 'Bigquery', 'Advance Sql', 'Azure Cloud', 'GCP', 'Data Modeling', 'Data Architecture', 'AWS']",2025-06-12 14:44:16
Data Analyst,Corelogic,5 - 10 years,Not Disclosed,['Noida'],"At Cotality, we are driven by a single mission to make the property industry faster, smarter, and more people-centric. Cotality is the trusted source for property intelligence, with unmatched precision, depth, breadth, and insights across the entire ecosystem. Our talented team of 5,000 employees globally uses our network, scale, connectivity and technology to drive the largest asset class in the world. Join us as we work toward our vision of fueling a thriving global property ecosystem and a more resilient society.\nCotality is committed to cultivating a diverse and inclusive work culture that inspires innovation and bold thinking; its a place where you can collaborate, feel valued, develop skills and directly impact the real estate economy. We know our people are our greatest asset. At Cotality, you can be yourself, lift people up and make an impact. By putting clients first and continuously innovating, were working together to set the pace for unlocking new possibilities that better serve the property industry.\nJob Description:\nIn India, we operate as Next Gear India Private Limited, a fully-owned subsidiary of Cotality with offices in Kolkata, West Bengal, and Noida, Uttar Pradesh. Next Gear India Private Limited plays a vital role in Cotalitys Product Development capabilities, focusing on creating and delivering innovative solutions for the Property & Casualty (P&C) Insurance and Property Restoration industries.\nWhile Next Gear India Private Limited operates under its own registered name in India, we are seamlessly integrated into the Cotality family, sharing the same commitment to innovation, quality, and client success.\nWhen you join Next Gear India Private Limited, you become part of the global Cotality team. Together, we shape the future of property insights and analytics, contributing to a smarter and more resilient property ecosystem through cutting-edge technology and insights.\nCompany Description\nAt Cotality, we are driven by a single mission to make the property industry faster, smarter, and more people-centric. CoreLogic is the trusted source for property intelligence, with unmatched precision, depth, breadth, and insights across the entire ecosystem. Our talented team of 5,000 employees globally uses our network, scale, connectivity, and technology to drive the largest asset class in the world. Join us as we work toward our vision of fueling a thriving global property ecosystem and a more resilient society.\n\nCotality is committed to cultivating a diverse and inclusive work culture that inspires innovation and bold thinking; its a place where you can collaborate, feel valued, develop skills, and directly impact the insurance marketplace. We know our people are our greatest asset. At Cotality, you can be yourself, lift people up and make an impact. By putting clients first and continuously innovating, were working together to set the pace for unlocking new possibilities that better serve the property insurance and restoration industry.\nRegular working hours will be from 12noon to 9pm IST\nIt will be Hybrid work where the team will need to be in the office for the first half of the day for 3 days a week and they can leave in the afternoon around 3pm or 4pm and log back at home for the remaining hours (same as other local teams).\nTraining will be provided about the product.\nDescription\nWe are seeking a highly skilled Data Analyst to join our Analytics Support team to serve customers across the property insurance and restoration industries. As a data analyst you will play a crucial role in developing methods and models to inform data-driven decision processes resulting in improved business performance for both internal and external stakeholder groups. You will be responsible for interpreting complex data sets and providing valuable insights to enhance the value of data assets. The successful candidate will have a strong understanding of data mining techniques, methods of statistical analysis, and data visualization tools. This position offers an exciting opportunity to work in a dynamic environment, collaborating with cross-functional teams to support decision processes that will guide the respective industries into the future.\nResponsibilities\nCollaborate with cross-functional teams to understand and document requirements for analytics products.\nServe as the primary point of contact for new data/analytics requests and support for customers.\nAct as the domain expert and voice of the customer to internal stakeholders during the analytics development process.\nDevelop and maintain an inventory of data, reporting, and analytic product deliverables for assigned customers.\nWork with customer success teams to establish and maintain appropriate customer expectations for analytics deliverables.\nCreate and manage change order tickets on behalf of customers within internal frameworks.\nEnsure timely delivery of assets to customers and aid in the development of internal processes for the delivery of analytics deliverables.\nWork with IT/Infrastructure teams to provide customer access to assets and support internal audit processes to ensure data security.\nCreate and optimize complex SQL queries for data extraction, transformation, and aggregation.\nDevelop and maintain data models, dashboards, and reports to visualize data and track key performance metrics.\nConduct validation checks and implement error handling mechanisms to ensure data reliability.\nCollaborate closely with stakeholders to align project goals with business needs and perform ad-hoc analysis to provide actionable recommendations.\nAnalyze large and complex datasets to identify trends, patterns, and insights, and present findings and recommendations to stakeholders in a clear and concise manner.\nJob Qualifications:\n5+ years experience of building PowerBI dashboards, data modeling and analysis\nBachelor s degree in computer science, data science, statistics, or a related field is preferred.\nAdvanced knowledge of data analysis tools such as Power Query, Excel, and Power BI.\nDemonstrated expertise in Power BI creating reports and dashboards, including the ability to connect to various data sources, prepare and model data, and create visualizations.\nExcellent visual and storytelling skills with data. Experience with Power Query for importing, transforming, and shaping data.\nExpert knowledge of DAX for creating calculated columns and measures to meet report-specific requirements.\nProficiency in SQL with the ability to write complex queries and optimize performance.\nExperience with ETL processes, data pipeline and automation a plus.\nStrong analytical and problem-solving skills\nExcellent attention to detail and the ability to work with large datasets.\nEffective communication skills, both written and verbal.\nAbility to work independently and collaborate in a team environment.\nKnowledge of property insurance industry will be a plus.\nCotalitys Diversity Commitment:\nCotality is fully committed to employing a diverse workforce and creating an inclusive work environment that embraces everyone s unique contributions, experiences and values. We offer an empowered work environment that encourages creativity, initiative and professional growth and provides a competitive salary and benefits package. We are better together when we support and recognize our differences.\nEqual Opportunity Employer Statement:\nCotality is an Equal Opportunity employer committed to attracting and retaining the best-qualified people available, without regard to race, ancestry, place of origin, colour, ethnic origin, citizenship, creed, sex, sexual orientation, record of offences, age, marital status, family status or disability. Cotality maintains a Drug-Free Workplace.\nPlease apply on our website for consideration.\nPrivacy Policy\nGlobal Applicant Privacy Policy\nBy providing your telephone number, you agree to receive automated (SMS) text messages at that number from Cotality regarding all matters related to your application and, if you are hired, your employment and company business. Message & data rates may apply. You can opt out at any time by responding STOP or UNSUBSCRIBING and will automatically be opted out company-wide.\nConnect with us on social media! Click on the quicklinks below to find out more about our company and associates",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data analysis', 'Manager Internal Audit', 'Automation', 'Data modeling', 'Analytical', 'Social media', 'SMS', 'Data mining', 'SQL', 'Data extraction']",2025-06-12 14:44:18
Data Science_ Lead,Rishabh Software,8 - 13 years,Not Disclosed,"['Ahmedabad', 'Bengaluru', 'Vadodara']","Job Description\n\nWith excellent analytical and problem-solving skills, you should understand business problems of the customers, translate them into scope of work and technical specifications for developing into Data Science projects. Efficiently utilize cutting edge technologies in AI, Generative AI areas and implement solutions for business problems. Good exposure technology platforms for Data Science, AI, Gen AI, cloud with implementation experience. Ability to provide end to end technical solutions leveraging latest AI, Gen AI tools, frameworks for the business problems. This Job requires the following:",,,,"['Data Science', 'gen ai', 'Computer Vision', 'Machine Learning', 'Deep Learning', 'Tensorflow', 'NLP', 'Artificial Intelligence', 'Dl', 'Python']",2025-06-12 14:44:20
Data Engineer,Prohr Strategies,9 - 11 years,Not Disclosed,['Bengaluru'],"Hands-on Data Engineer with strong Databricks expertise in Git/DevOps integration, Unity Catalog governance, and performance tuning of data transformation workloads. Skilled in optimizing pipelines and ensuring secure, efficient data operations.",Industry Type: Emerging Technologies (AI/ML),Department: Data Science & Analytics,"Employment Type: Full Time, Temporary/Contractual","['Data Transformation', 'GIT', 'Azure Databricks', 'Databricks', 'Devops', 'Data Engineering', 'Governance', 'Catalog', 'Code Versioning Tools']",2025-06-12 14:44:22
Senior Data Engineer (Identity),Kargo,6 - 11 years,Not Disclosed,[],"Success takes all kinds. Diversity describes our workforce. Inclusion defines our culture. We do not discriminate on the basis of race, color, religion, sex, sexual orientation, gender identity, marital status, age, national origin, protected veteran status, disability or other legally protected status. Individuals with disabilities are provided reasonable accommodation to participate in the job application process, perform essential job functions, and receive other benefits and privileges of employment.\nTitle: Senior Data Engineer\nJob Type: Permanent\n\nJob Location: Remote\nThe Opportunity\nAt Kargo, we are rapidly evolving our data infrastructure and capabilities to address challenges of data scale, new methodologies for onboarding and targeting, and rigorous privacy standards. Were looking for an experienced Senior Data Engineer to join our team, focusing on hands-on implementation, creative problem-solving, and exploring new technical approaches. Youll work collaboratively with our technical leads and peers, actively enhancing and scaling the data processes that drive powerful targeting systems.\nThe Daily To-Do\nIndependently implement, optimize, and maintain robust ETL/ELT pipelines using Python, Airflow, Spark, Iceberg, Snowflake, Aerospike, Docker, Kubernetes (EKS), AWS, and real-time streaming technologies like Kafka and Flink.\nEngage proactively in collaborative design and brainstorming sessions, contributing technical insights and innovative ideas for solving complex data engineering challenges.\nSupport the definition and implementation of robust testing strategies, and guide the team in adopting disciplined CI/CD practices using ArgoCD to enable efficient and reliable deployments.\nMonitor and optimize data systems and infrastructure to ensure operational reliability, performance efficiency, and cost-effectiveness.\nActively contribute to onboarding new datasets, enhancing targeting capabilities, and exploring modern privacy-compliant methodologies.\nMaintain thorough documentation of technical implementations, operational procedures, and best practices for effective knowledge sharing and onboarding.\nQualifications:\nStrong expertise in implementing, maintaining, and optimizing large-scale data systems with minimal oversight.\nDeep proficiency in Python, Spark, and Iceberg, with a clear understanding of data structuring for efficiency and performance.\nExperience with Airflow for building robust data workflows is strongly preferred.\nExtensive DevOps experience, particularly with AWS (including EKS), Docker, Kubernetes, CI/CD automation using ArgoCD, and monitoring via Prometheus.\nFamiliarity with Snowflake, including writing and optimizing SQL queries and understanding Snowflakes performance and cost dynamics.\nComfort with Agile methodologies, including regular use of Jira and Confluence for task management and documentation.\nProven ability to independently drive implementation and problem-solving, turning ambiguity into clearly defined actions.\nExcellent communication skills to effectively engage in discussions with technical teams and stakeholders.\nFamiliarity with identity, privacy, and targeting methodologies in AdTech is required.\nFollow Our Lead\nBig Picture: kargo.com\nThe Latest: Instagram ( @kargomobile ) and LinkedIn ( Kargo )",Industry Type: Advertising & Marketing,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SQL queries', 'Automation', 'CTV', 'spark', 'Agile', 'JIRA', 'Operations', 'Cost', 'AWS', 'Python']",2025-06-12 14:44:24
Data Analyst,Bhavani Shipping services,1 - 3 years,1-2.75 Lacs P.A.,"['Thane', 'Navi Mumbai', 'Mumbai (All Areas)']","Principal Duties and Responsibilities\nInterpreting data, analyzing results using statistical techniques.\nDeveloping and implementing data analyses, data collection systems and other strategies that optimize statistical efficiency and quality.\nAcquiring data from primary or secondary data sources and maintaining databases.\n\nKey Responsibilities:\n\nData Collection and Processing:\nGather data from various sources, ensuring accuracy and completeness.\nCleanse and preprocess data to remove errors and inconsistencies.\nStatistical Analysis and Interpretation:\nUtilize statistical methods to analyze data and identify trends, patterns, and correlations.\nPresent findings through reports, visualizations, and presentations to stakeholders.\nData Visualization and Reporting:\nCreate visualizations and dashboards to effectively communicate insights.\nPrepare regular reports and ad-hoc analyses to support strategic decision-making.\nProblem-Solving and Recommendations:\nCollaborate with cross-functional teams to address business challenges using data-driven insights.",Industry Type: Courier / Logistics,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Reporting', 'Data Analysis', 'Excel Powerpoint', 'Data Extraction', 'Charts', 'Data Collection', 'Advanced Excel', 'Power Point Presentation', 'Powerpoint', 'Excel Report Preparation', 'Presentation Skills', 'Report Generation', 'MS Office Tools', 'Dashboards', 'MS Office Word']",2025-06-12 14:44:27
Data Engineering Lead Microsoft Power BI,Client of Techs To Suit,8 - 12 years,25-32.5 Lacs P.A.,"['Indore', 'Hyderabad', 'Ahmedabad']","Poistion - Data Engineering Lead\nExp - 8 to 12 Years\nJob Location: Hyderabad, Ahmedabad, Indore, India.\n\nMust be Able to join in 30 days\nJob Summary:\nAs a Data Engineering Lead, your role will involve designing, developing, and implementing\ninteractive dashboards and reports using data engineering tools. You will work closely with\nstakeholders to gather requirements and translate them into effective data visualizations that\nprovide valuable insights. Additionally, you will be responsible for extracting, transforming, and\nloading data from multiple sources into Power BI, ensuring its accuracy and integrity. Your\nexpertise in Power BI and data analytics will contribute to informed decision-making and\nsupport the organization in driving data-centric strategies and initiatives.\nWe are looking for you!\nAs an ideal candidate for the Data Engineering Lead position, you embody the qualities of a\nteam player with a relentless get-it-done attitude. Your intellectual curiosity and customer\nfocus drive you to continuously seek new ways to add value to your job accomplishments. You\nthrive under pressure, maintaining a positive attitude and understanding that your career is a\njourney. You are willing to make the right choices to support your growth. In addition to your\nexcellent communication skills, both written and verbal, you have a proven ability to create\nvisually compelling designs using tools like Power BI and Tableau that effectively\ncommunicate our core values.\nYou build high-performing, scalable, enterprise-grade applications and teams. Your creativity\nand proactive nature enable you to think differently, find innovative solutions, deliver high-\nquality outputs, and ensure customers remain referenceable. With over eight years of\nexperience in data engineering, you possess a strong sense of self-motivation and take\nownership of your responsibilities. You prefer to work independently with little to no\nsupervision.\nYou are process-oriented, adopt a methodical approach, and demonstrate a quality-first\nmindset. You have led mid to large-size teams and accounts, consistently using constructive\nfeedback mechanisms to improve productivity, accountability, and performance within the\nteam. Your track record showcases your results-driven approach, as you have consistently\ndelivered successful projects with customer case studies published on public platforms.\nOverall, you possess a unique combination of skills, qualities, and experiences that make you\nan ideal fit to lead our data engineering team(s). You value inclusivity and want to join a culture\nthat empowers you to show up as your authentic self.\nYou know that success hinges on commitment, our differences make us stronger, and the\nfinish line is always sweeter when the whole team crosses together. In your role, you shouldbe driving the team using data, data, and more data. You will manage multiple teams, oversee\nagile stories and their statuses, handle escalations and mitigations, plan ahead, identify hiring\nneeds, collaborate with recruitment teams for hiring, enable sales with pre-sales teams, and\nwork closely with development managers/leads for solutioning and delivery statuses, as well\nas architects for technology research and solutions.\nWhat You Will Do:\nAnalyze Business Requirements.\nAnalyze the Data Model and do GAP analysis with Business Requirements and Power\nBI. Design and Model Power BI schema.\nTransformation of Data in Power BI/SQL/ETL Tool.\nCreate DAX Formula, Reports, and Dashboards. Able to write DAX formulas.\nExperience writing SQL Queries and stored procedures.\nDesign effective Power BI solutions based on business requirements.\nManage a team of Power BI developers and guide their work.\nIntegrate data from various sources into Power BI for analysis.\nOptimize performance of reports and dashboards for smooth usage.\nCollaborate with stakeholders to align Power BI projects with goals.\nKnowledge of Data Warehousing(must), Data Engineering is a plus",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Power Bi Dashboards', 'Microsoft Power Bi', 'SQL Queries', 'Azure Databricks', 'Dax', 'Azure Data Factory']",2025-06-12 14:44:29
Senior Data Engineer,Binary Infoways,5 - 9 years,12-19.2 Lacs P.A.,['Hyderabad'],"Responsibilities:\n* Design, develop & maintain data pipelines using Airflow, Python & SQL.\n* Optimize performance through Spark & Splunk analytics.\n* Collaborate with cross-functional teams on big data initiatives.\n* AWS",Industry Type: BPM / BPO,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Airflow', 'Big Data Technologies', 'ETL', 'AWS', 'Python', 'Glue', 'Snowflake', 'Spark', 'Splunk', 'SQL']",2025-06-12 14:44:31
Senior Data Engineer (Data Architect),Adastra Corp,8 - 13 years,Not Disclosed,[],"Join our innovative team and architect the future of data solutions on Azure, Synapse, and Databricks!\nSenior Data Engineer (Data Architect)\nAdditional Details:\nNotice Period: 30 days (maximum)\nLocation: Remote\nAbout the Role\nDesign and implement scalable data pipelines, data warehouses, and data lakes that drive business growth. Collaborate with stakeholders to deliver data-driven insights and shape the data landscape.\nRequirements\n8+ years of experience in data engineering and data architecture\nStrong expertise in Azure services (Synapse Analytics, Databricks, Storage, Active Directory)\nProven experience in designing and implementing data pipelines, data warehouses, and data lakes\nStrong understanding of data governance, data quality, and data security\nExperience with infrastructure design and implementation, including DevOps practices and tools",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Azure', 'DESIGN', 'Architecture', 'Synapse Analytics']",2025-06-12 14:44:33
Senior Data Engineer,Jeavio,5 - 10 years,Not Disclosed,[],"We are seeking an experienced Senior Data Engineer to join our team. The ideal candidate will have a strong background in data engineering and AWS infrastructure, with hands-on experience in building and maintaining data pipelines and the necessary infrastructure components. The role will involve using a mix of data engineering tools and AWS services to design, build, and optimize data architecture.\n\nKey Responsibilities:\nDesign, develop, and maintain data pipelines using Airflow and AWS services.\nImplement and manage data warehousing solutions with Databricks and PostgreSQL.\nAutomate tasks using GIT / Jenkins.\nDevelop and optimize ETL processes, leveraging AWS services like S3, Lambda, AppFlow, and DMS.\nCreate and maintain visual dashboards and reports using Looker.\nCollaborate with cross-functional teams to ensure smooth integration of infrastructure components.\nEnsure the scalability, reliability, and performance of data platforms.\nWork with Jenkins for infrastructure automation.\n\nTechnical and functional areas of expertise:\nWorking as a senior individual contributor on a data intensive project\nStrong experience in building high performance, resilient & secure data processing pipelines preferably using Python based stack.\nExtensive experience in building data intensive applications with a deep understanding of querying and modeling with relational databases preferably on time-series data.\nIntermediate proficiency in AWS services (S3, Airflow)\nProficiency in Python and PySpark\nProficiency with ThoughtSpot or Databricks.\nIntermediate proficiency in database scripting (SQL)\nBasic experience with Jenkins for task automation\n\nNice to Have :\nIntermediate proficiency in data analytics tools (Power BI / Tableau / Looker / ThoughSpot)\nExperience working with AWS Lambda, Glue, AppFlow, and other AWS transfer services.\nExposure to PySpark and data automation tools like Jenkins or CircleCI.\nFamiliarity with Terraform for infrastructure-as-code.\nExperience in data quality testing to ensure the accuracy and reliability of data pipelines.\nProven experience working directly with U.S. client stakeholders.\nAbility to work independently and take the lead on tasks.\n\nEducation and experience:\nBachelors or masters in computer science or related fields.\n5+ years of experience\n\nStack/Skills needed:\nDatabricks\nPostgreSQL\nPython & Pyspark\nAWS Stack\nPower BI / Tableau / Looker / ThoughSpot\nFamiliarity with GIT and/or CI/CD tools",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Pyspark', 'Data Engineering', 'AWS', 'Data Bricks', 'Python', 'Etl Pipelines', 'Airflow', 'Database Scripting', 'Postgresql', 'Looker', 'SQL']",2025-06-12 14:44:36
Lead Data Engineer,Prolegion,8 - 12 years,Not Disclosed,['Hyderabad'],"Job Summary:\nWe are seeking a highly skilled Lead Data Engineer/Associate Architect to lead the design, implementation, and optimization of scalable data architectures. The ideal candidate will have a deep understanding of data modeling, ETL processes, cloud data solutions, and big data technologies. You will work closely with cross-functional teams to build robust, high-performance data pipelines and infrastructure to enable data-driven decision-making.\n\nExperience: 8 - 12+ years\nWork Location: Hyderabad (Hybrid)\nMandatory skills: Python, SQL, Snowflake\n\nResponsibilities:\nDesign and Develop scalable and resilient data architectures that support business needs, analytics, and AI/ML workloads.\nData Pipeline Development: Design and implement robust ETL/ELT processes to ensure efficient data ingestion, transformation, and storage.\nBig Data & Cloud Solutions: Architect data solutions using cloud platforms like AWS, Azure, or GCP, leveraging services such as Snowflake, Redshift, BigQuery, and Databricks.\nDatabase Optimization: Ensure performance tuning, indexing strategies, and query optimization for relational and NoSQL databases.\nData Governance & Security: Implement best practices for data quality, metadata management, compliance (GDPR, CCPA), and security.\nCollaboration & Leadership: Work closely with data engineers, analysts, and business stakeholders to translate business requirements into scalable solutions.\nTechnology Evaluation: Stay updated with emerging trends, assess new tools and frameworks, and drive innovation in data engineering.\n\nRequired Skills:\nEducation: Bachelors or Masters degree in Computer Science, Data Engineering, or a related field.\nExperience: 8 - 12+ years of experience in data engineering\nCloud Platforms: Strong expertise in AWS data services.\nBig Data Technologies: Experience with Hadoop, Spark, Kafka, and related frameworks.\nDatabases: Hands-on experience with SQL, NoSQL, and columnar databases such as PostgreSQL, MongoDB, Cassandra, and Snowflake.\nProgramming: Proficiency in Python, Scala, or Java for data processing and automation.\nETL Tools: Experience with tools like Apache Airflow, Talend, DBT, or Informatica.\nMachine Learning & AI Integration (Preferred): Understanding of how to architect data solutions for AI/ML applications\n\n,",Industry Type: Defence & Aerospace,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Performance tuning', 'Automation', 'Data modeling', 'Postgresql', 'Informatica', 'Apache', 'Analytics', 'SQL', 'Python']",2025-06-12 14:44:38
Senior Data Engineer,Keeptruckin,6 - 11 years,Not Disclosed,[],"Who we are:\nMotive empowers the people who run physical operations with tools to make their work safer, more productive, and more profitable. For the first time ever, safety, operations and finance teams can manage their drivers, vehicles, equipment, and fleet related spend in a single system. Combined with industry leading AI, the Motive platform gives you complete visibility and control, and significantly reduces manual workloads by automating and simplifying tasks.\nMotive serves more than 120,000 customers - from Fortune 500 enterprises to small businesses - across a wide range of industries, including transportation and logistics, construction, energy, field service, manufacturing, agriculture, food and beverage, retail, and the public sector.\nVisit gomotive.com to learn more.\nAbout the Role:\nAs a Senior Data Engineer you will be part of the core data team building out world class data models and pipelines that will feed into our products. You will be working on the intersection of all data streams in the company from our IOT data consuming 100s of thousands of data points per minute to our user data. You will partner closely with both the Product, Engineering, as well as the Strategic Analytics teams. We are seeking strong team players who thrive on innovation and continuous improvement. We pride ourselves on our culture, and ability to work effectively across a highly diversified team.\nWhat Youll Do:\nBuild data pipelines based on product data which will go into our product for Enterprise customers\nArchitect and design data models in collaboration with data and product teams\nCommunicate effectively across multiple teams and projects.\nActively work on deploying Data Ops into Motive, driving the most robust data models using the latest tools in the market\nYou will be working with airflow, aws, great expectations and table creation frameworks similar to dbt.\nWhat Were Looking For:\nBachelors degree or higher in a quantitative field, e.g. Computer Science, Math, Economics, or Statistics\n6+ years experience in Data Engineering, including experience building modeled tables\nExpertise with data engineering stack, dBt, Snowflake, airflow, data observability tools, AWS.\nExpertise in SQL and Python\nWillingness to learn new technologies\nSolid communication, collaboration, and people skills\nCreating a diverse and inclusive workplace is one of Motives core values. We are an equal opportunity employer and welcome people of different backgrounds, experiences, abilities and perspectives.\nPlease review our Candidate Privacy Notice here .\nUK Candidate Privacy Notice here .\nThe applicant must be authorized to receive and access those commodities and technologies controlled under U.S. Export Administration Regulations. It is Motives policy to require that employees be authorized to receive access to Motive products and technology.\n#LI-Remote",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Product engineering', 'Manager Technology', 'Continuous improvement', 'AWS', 'Analytics', 'Team building', 'SQL', 'Python', 'Logistics']",2025-06-12 14:44:40
Lead Data Engineer - Azure,Blend360 India,7 - 12 years,Not Disclosed,['Hyderabad'],"As a Sr Data Engineer, your role is to spearhead the data engineering teams and elevate the team to the next level! You will be responsible for laying out the architecture of the new project as well as selecting the tech stack associated with it. You will plan out the development cycles deploying AGILE if possible and create the foundations for good data stewardship with our new data products!\nYou will also set up a solid code framework that needs to be built to purpose yet have enough flexibility to adapt to new business use cases tough but rewarding challenge!\n\nResponsibilities\nCollaborate with several stakeholders to deeply understand the needs of data practitioners to deliver at scale\nLead Data Engineers to define, build and maintain Data Platform\nWork on building Data Lake in Azure Fabric processing data from multiple sources\nMigrating existing data store from Azure Synapse to Azure Fabric\nImplement data governance and access control\nDrive development effort End-to-End for on-time delivery of high-quality solutions that conform to requirements, conform to the architectural vision, and comply with all applicable standards.\nPresent technical solutions, capabilities, considerations, and features in business terms. Effectively communicate status, issues, and risks in a precise and timely manner.\nFurther develop critical initiatives, such as Data Discovery, Data Lineage and Data Quality\nLeading team and Mentor junior resources\nHelp your team members grow in their role and achieve their career aspirations\nBuild data systems, pipelines, analytical tools and programs\nConduct complex data analysis and report on results\n\n\n7+ Years of Experience as a data engineer or similar role in Azure Synapses, ADF or\nrelevant exp in Azure Fabric\nDegree in Computer Science, Data Science, Mathematics, IT, or similar field\nMust have experience e",Industry Type: Industrial Automation,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Data analysis', 'Data modeling', 'Analytical', 'Agile', 'data governance', 'Data quality', 'Data mining', 'SQL', 'Python']",2025-06-12 14:44:42
Senior Data Engineer - Azure,Blend360 India,6 - 11 years,Not Disclosed,['Hyderabad'],"As a Data Engineer, your role is to spearhead the data engineering teams and elevate the team to the next level! You will be responsible for laying out the architecture of the new project as well as selecting the tech stack associated with it. You will plan out the development cycles deploying AGILE if possible and create the foundations for good data stewardship with our new data products!\nYou will also set up a solid code framework that needs to be built to purpose yet have enough flexibility to adapt to new business use cases tough but rewarding challenge!\n\nResponsibilities\nCollaborate with several stakeholders to deeply understand the needs of data practitioners to deliver at scale\nLead Data Engineers to define, build and maintain Data Platform\nWork on building Data Lake in Azure Fabric processing data from multiple sources\nMigrating existing data store from Azure Synapse to Azure Fabric\nImplement data governance and access control\nDrive development effort End-to-End for on-time delivery of high-quality solutions that conform to requirements, conform to the architectural vision, and comply with all applicable standards.\nPresent technical solutions, capabilities, considerations, and features in business terms. Effectively communicate status, issues, and risks in a precise and timely manner.\nFurther develop critical initiatives, such as Data Discovery, Data Lineage and Data Quality\nLeading team and Mentor junior resources\nHelp your team members grow in their role and achieve their career aspirations\nBuild data systems, pipelines, analytical tools and programs\nConduct complex data analysis and report on results\n\n\n3+ Years of Experience as a data engineer or similar role in Azure Synapses, ADF or\nrelevant exp in Azure Fabric\nDegree in Computer Science, Data Science, Mathematics, IT, or similar field\nMust have experience e",Industry Type: Industrial Automation,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Data analysis', 'Data modeling', 'Analytical', 'Agile', 'data governance', 'Data quality', 'Data mining', 'SQL', 'Python']",2025-06-12 14:44:44
Senior Data Engineer,Neal Analytics,10 - 15 years,Not Disclosed,['Mumbai'],"Its fun to work in a company where people truly BELIEVE in what they are doing!\nWere committed to bringing passion and customer focus to the business.\nJob Description:\nAs a Backend (Java) Engineer, you would be part of the team consisting of Scrum Master, Cloud Engineers, AI/ML Engineers, and UI/UX Engineers to build end-to-end Data to Decision Systems.\nMandatory:\n8+ years of demonstrable experience designing, building, and working as a Java Developer for enterprise web applications\nIdeally, this would include the following:\no Expert-level proficiency with Java\no Expert-level proficiency with SpringBoot\nFamiliarity with common databases (RDBMS such as MySQL & NoSQL such as MongoDB) and data warehousing concepts (OLAP, OLTP)\nUnderstanding of REST concepts and building/interacting with REST APIs\nDeep understanding of core backend concepts:\no Develop and design RESTful services and APIs\no Develop functional databases, applications, and servers to support websites on the back end\no Performance optimization and multithreading concepts\no Experience with deploying and maintaining high traffic infrastructure (performance testing is a plus)\nIn addition, the ideal candidate would have great problem-solving skills, and familiarity with code versioning tools such as GitHub\nGood to have:\nFamiliarity with Microsoft Azure Cloud Services (particularly Azure Web App, Storage and VM), or familiarity with AWS (EC2 containers) or GCP Services.\nExperience with Microservices, Messaging Brokers (e.g., RabbitMQ)\nExperience with fine-tuning reverse proxy engines such as Nginx, Apache HTTPD\nIf you like wild growth and working with happy, enthusiastic over-achievers, youll enjoy your career with us!\nNot the right fit? Let us know youre interested in a future opportunity by clicking Introduce Yourself in the top-right corner of the page or create an account to set up email alerts as new job postings become available that meet your interest!",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Backend', 'Multithreading', 'RDBMS', 'MySQL', 'Performance testing', 'OLAP', 'Scrum', 'MongoDB', 'Apache', 'OLTP']",2025-06-12 14:44:46
Remote Data Visualization Engineer 36Lakhs CTC|| Kandi Srinivasa Reddy,Integra Technologies,8 - 11 years,35-37.5 Lacs P.A.,"['Kolkata', 'Ahmedabad', 'Bengaluru']","Dear Candidate,\nWe are seeking a Data Visualization Engineer to turn complex data into clear, engaging visual insights that empower business decisions. This role involves working closely with analysts and stakeholders to build interactive dashboards and reports.\n\nKey Responsibilities:\nDesign and develop visualizations using tools like Power BI, Tableau, or Looker.\nTranslate data into compelling stories and business insights.\nOptimize dashboard performance and usability for end users.\nCollaborate with data engineering and analytics teams.\nImplement visualization standards and data governance practices.\nRequired Skills & Qualifications:\nProficiency in data visualization tools (Power BI, Tableau, D3.js).\nStrong knowledge of SQL and data modeling.\nUnderstanding of UX principles in data presentation.\nExperience working with large datasets and cloud-based data platforms.\nKnowledge of scripting languages (Python, R) is a plus.\nSoft Skills:\nStrong troubleshooting and problem-solving skills.\nAbility to work independently and in a team.\nExcellent communication and documentation skills.\n\nNote: If interested, please share your updated resume and preferred time for a discussion. If shortlisted, our HR team will contact you.\n\nKandi Srinivasa Reddy\nDelivery Manager\nIntegra Technologies",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Tableau', 'Quicksight', 'Data Visualization', 'Dashboard Development', 'Business Intelligence', 'Power Bi', 'Bi Tools', 'Dashboarding', 'Business Objects', 'Reporting Tools', 'SQL', 'Microstrategy', 'Dashboards', 'QlikView']",2025-06-12 14:44:50
Urgent hiring For Cloud Data Engineer,Wowjobs,7 - 10 years,30-45 Lacs P.A.,"['Hyderabad', 'Chennai', 'Bengaluru']","Role & responsibilities\n*  Design, Build, and Maintain ETL Pipelines: Develop robust, scalable, and efficient ETL workflows to ingest, transform, and load data into distributed data products within the Data Mesh architecture.\n*   Data Transformation with dbt: Use dbt to build modular, reusable transformation workflows that align with the principles of Data Products.\n*   Cloud Expertise: Leverage Google Cloud Platform (GCP) services such as BigQuery, Cloud Storage, Pub/Sub, and Dataflow to implement highly scalable data solutions.\n*   Data Quality & Governance: Enforce strict data quality standards by implementing validation checks, anomaly detection mechanisms, and monitoring frameworks.\n*   Performance Optimization: Continuously optimize ETL pipelines for speed, scalability, and cost efficiency.\n*   Collaboration & Ownership: Work closely with data product owners, BI developers, and stakeholders to understand requirements and deliver on expectations. Take full ownership of your deliverables.\n*   Documentation & Standards: Maintain detailed documentation of ETL workflows, enforce coding standards, and adhere to best practices in data engineering.\n*   Troubleshooting & Issue Resolution: Proactively identify bottlenecks or issues in pipelines and resolve them quickly with minimal disruption.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Python coding', 'Cloud data engineer', 'ETL Workflow']",2025-06-12 14:44:52
Snowflake Data Engineer,Tredence,3 - 8 years,Not Disclosed,['Bengaluru'],"Role & responsibilities\n\nDesign, build, and maintain scalable data pipelines using DBT and Airflow.\nDevelop and optimize SQL queries and data models in Snowflake.\nImplement ETL/ELT workflows, ensuring data quality, performance, and reliability.\nWork with Python for data processing, automation, and integration tasks.\nHandle JSON data structures for data ingestion, transformation, and APIs.\nLeverage AWS services (e.g., S3, Lambda, Glue, Redshift) for cloud-based data solutions. Collaborate with data analysts, engineers, and business teams to deliver high-quality data products.",,,,"['Snowflake', 'DBT', 'SQL']",2025-06-12 14:44:54
Director - Data Engineering,Blend360 India,10 - 15 years,Not Disclosed,['Bengaluru'],"We are seeking a strategic Director of Data & AI Engineering to lead the growth and evolution of our data engineering function. This role will play a pivotal part in designing scalable platforms, enabling advanced AI and ML applications, and building a world-class engineering team. The ideal candidate is both a seasoned technical leader and a visionary strategist who thrives at the intersection of innovation, execution, and impact. You will collaborate with internal and client teams to develop systems and infrastructure that power intelligent products and data-driven decision-making. You will also build and mentor a high-performing team of Data and AI Engineers, foster a modern data culture, and champion best practices for scalable architecture, AI integration, and engineering excellence.\nResponsibilities:\nSolution Design & Engineering Leadership\nArchitect and build scalable, high-performance data and AI pipelines using tools such as Spark, PySpark, SQL, Python, DBT, Airflow, and cloud-native platforms (AWS, GCP, or Azure).\nLead the design of hybrid/on-prem data platforms, incorporating security, governance, and performance optimization.\nStrategic Client & Stakeholder Engagement\nServe as a trusted technical advisor to internal and external stakeholders.\nTranslate complex business needs into practical engineering solutions and oversee the end-to-end delivery lifecycle.\nAI-Driven Productivity & Innovation\nIntroduce and scale AI-driven tools and practices to accelerate development and enhance data quality, resilience, and maintainability.\nChampion the adoption of generative AI and foundation models to enable intelligent automation and insight generation.\nGrowth & Team Leadership\nBuild, lead, and inspire a diverse team of Data Engineers, ML Engineers, and AI Specialists.\nSet a clear vision and goals, provide mentorship, and cultivate a strong engineering culture and standards.\nPlatform and Data Strategy\nLead initiatives to modernize data infrastructure, improve data discoverability, and support real-time analytics and experimentation.\nCollaborate cross-functionally to shape product data strategies and influence the overall AI roadmap.\nPresales & Business Development Support\nPartner with sales and solution teams to craft compelling proposals, technical solutions, and client presentations.\nRepresent the engineering function in client discussions, workshops, and RFP responses to articulate value and differentiation.\nSupport opportunity scoping, estimation, and roadmap planning for prospective engagements.\n\n\n10+ years of experience in data engineering, AI/ML engineering, or product/platform engineering.\nProven track record of leading high-performing teams and managing senior engineers and managers.\nBachelor s or masters degre",Industry Type: Industrial Automation,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Automation', 'GCP', 'Business Development Manager', 'Social media', 'Data quality', 'RFP', 'Analytics', 'SQL', 'Python']",2025-06-12 14:44:56
Tech. PM - Data Engineering-Data Analytics@ Gurgaon/Blore_Urgent,A global leader in delivering innovative...,5 - 10 years,Not Disclosed,"['Gurugram', 'Bengaluru']","Job Title - Technical Project Manager\n\nLocation - Gurgaon/ Bangalore\n\nNature of Job - Permanent\n\nDepartment - data analytics\n\nWhat you will be doing\n\n\nDemonstrated client servicing and business analytics skills with at least 5 - 9 years of experience as data engineer, BI developer, data analyst, technical project manager, program manager etc.\nTechnical project management- drive BRD, project scope, resource allocation, team\ncoordination, stakeholder communication, UAT, Prod fix, change requests, project governance\nSound knowledge of banking industry (payments, retail operations, fraud etc.)\nStrong ETL experience or experienced Teradata developer\nManaging team of business analysts, BI developers, ETL developers to ensure that projects are completed on time\nResponsible for providing thought leadership and technical advice on business issues\nDesign methodological frameworks and solutions.\n\n\nWhat were looking for\n\n\nBachelors/masters degree in computer science/data science/AI/statistics, Certification in Gen AI. Masters degree Preferred.\nManage multiple projects, at a time, from inception to delivery\nSuperior problem-solving, analytical, and quantitative skills\nEntrepreneurial mindset, coupled with a “can do” attitude\nDemonstrated ability to collaborate with cross-functional, cross-border teams and coach / mentor colleagues.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Technical Project Manager', 'Data Engineering', 'multiple projects', 'Technical project management', 'Data Analytics', 'project scope', 'ETL Pipeline', 'team coordination', 'resource allocation', 'Prod fix', 'drive BRD', 'program manager', 'Big data']",2025-06-12 14:44:59
Sr Data Engineer - Fully Remote & Immediate Opportunity,Zealogics.com,10 - 15 years,Not Disclosed,[],"10 yrs of exp working in cloud-native data (Azure Preferred),Databricks, SQL,PySpark, migrating from Hive Metastore to Unity Catalog, Unity Catalog, implementing Row-Level Security (RLS), metadata-driven ETL design patterns,Databricks certifications",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'Azure', 'Metadata', 'Data Bricks', 'Unity Catalog', 'ETL design', 'SQL']",2025-06-12 14:45:01
Data Engineer II - Marketplace (Experimentation Track),Booking Holdings,5 - 10 years,Not Disclosed,['Bengaluru'],"We are looking for a Data Engineer to join our team and help us to improve the platform that supports one of the best experimentation tools in the world.\nYou will work side by side with other data engineers and site reliability engineers to improve the reliability, scalability, maintenance and operations of all the data products that are part of the experimentation tool at Booking.com.\nYour day to day work includes but is not limited to: maintenance and operations of data pipelines and products that handles data at big scale; the development of capabilities for monitoring, alerting, testing and troubleshooting of the data ecosystem of the experiment platform; and the delivery of data products that produce metrics for experimentation at scale. You will collaborate with colleagues in Amsterdam to achieve results the right way. This will include engineering managers, product managers, engineers and data scientists.\nKey Responsibilities and Duties\nTake ownership of multiple data pipelines and products and provide innovative solutions to reduce the operational workload required to maintain them\nRapidly developing next-generation scalable, flexible, and high-performance data pipelines.\nContribute to the development of data platform capabilities such as testing, monitoring, debugging and alerting to improve the development environment of data products\nSolve issues with data and data pipelines, prioritizing based on customer impact.\nEnd-to-end ownership of data quality in complex datasets and data pipelines.\nExperiment with new tools and technologies, driving innovative engineering solutions to meet business requirements regarding performance, scaling, and data quality.\nProvide self-organizing tools that help the analytics community discover data, assess quality, explore usage, and find peers with relevant expertise.\nServe as the main point of contact for technical and business stakeholders regarding data engineering issues, such as pipeline failures and data quality concerns\nRole requirements\nMinimum 5 years of hands-on experience in data engineering as a Data Engineer or as a Software Engineer developing data pipelines and products.\nBachelors degree in Computer Science, Computer or Electrical Engineering, Mathematics, or a related field or 5 years of progressively responsible experience in the specialty as equivalent\nSolid experience in at least one programming language. We use Java and Python\nExperience building production data pipelines in the cloud, setting up data-lakes and server-less solutions\nHands-on experience with schema design and data modeling\nExperience designing systems E2E and knowledge of basic concepts (lb, db, caching, NoSQL, etc)\nKnowledge of Flink, CDC, Kafka, Airflow, Snowflake, DBT or equivalent tools\nPractical experience building data platform capabilities like testing, alerting, monitoring, debugging, security\nExperience working with big data.\nExperience working with teams located in different timezones is a plus\nExperience with experimentation, statistics and A/B testing is a plus",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Airflow', 'Java', 'CDC', 'NoSQL', 'Snowflake', 'DBT', 'Kafka', 'Python']",2025-06-12 14:45:03
Data Engineering Specialist,Overture Rede,10 - 15 years,Not Disclosed,"['Mumbai', 'Hyderabad', 'Gurugram', 'Bengaluru']","Job Title: Sales Excellence COE Data Engineering Specialist\nLocations: Mumbai / Bangalore / Gurgaon / Hyderabad\nExperience: 1012 Years Level: Team Lead / Specialist (Level 9)\n\nJob Role\nLead data engineering efforts to support sales insights through scalable pipelines, statistical modeling, and ML workflows across cloud platforms.\n\nRequired Skills\nProficiency in Python\nAdvanced SQL (Views, Functions, Procedures)\nExperience with Google Cloud Platform (GCP) ML workflow setup\nStrong in Data Modeling and ETL Development\nExcel skills including VBA, Power Pivot, Cube Functions\nSolid understanding of Sales Processes\n\n\n",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Statistical modeling', 'Sales', 'Excel', 'VBA', 'Data modeling', 'GCP', 'Cloud', 'Workflow', 'SQL', 'Python']",2025-06-12 14:45:06
Data Engineering Manager,NOVARTIS,6 - 8 years,Not Disclosed,['Hyderabad'],"Summary\nWe are seeking a highly skilled and motivated GCP Data Engineering Manager to join our dynamic team. As a Data Engineering manager specializing in Google Cloud Platform (GCP), you will play a crucial role in designing, implementing, and maintaining scalable data pipelines and\nsystems. You will leverage your expertise in Google Big Query, SQL, Python, and analytical skills to drive data-driven decision-making processes and support various business functions.\nAbout the Role\nKey Responsibilities:\nData Pipeline Development: Design, develop, and maintain robust data pipelines using GCP services like Dataflow, Dataproc, ensuring high performance and scalability.\nGoogle Big Query Expertise: Utilize your hands-on experience with Google Big Query to manage and optimize data storage, retrieval, and processing.\nSQL Proficiency: Write and optimize complex SQL queries to transform and analyze large datasets, ensuring data accuracy and integrity.\nPython Programming: Develop and maintain Python scripts for data processing, automation, and integration with other systems and tools.\nData Integration: Collaborate with data analysts, and other stakeholders to integrate data from various sources, ensuring seamless data flow and consistency.\nData Quality and Governance: Implement data quality checks, validation processes, and governance frameworks to maintain high data standards.\nPerformance Tuning: Monitor and optimize the performance of data pipelines, queries, and storage solutions to ensure efficient data processing.\nDocumentation: Create comprehensive documentation for data pipelines, processes, and best practices to facilitate knowledge sharing and team collaboration.\nMinimum Qualifications:\nProven experience (minimum 6 - 8 yrs) in Data Engineer, with significant hands-on experience in Google Cloud Platform (GCP) and Google Big Query.\nProficiency in SQL for data transformation, analysis and performance optimization.\nStrong programming skills in Python, with experience in developing data processing scripts and automation.\nProven analytical skills with the ability to interpret complex data and provide actionable insights.\nExcellent problem-solving abilities and attention to detail.\nStrong communication and collaboration skills, with the ability to work effectively in a team enviro\nDesired Skills :\nExperience with Google Analytics data and understanding of digital marketing data.\nFamiliarity with other GCP services such as Cloud Storage, Dataflow, Pub/Sub, and Dataproc.\nKnowledge of data visualization tools such as Looker, Tableau, or Data Studio.\nExperience with machine learning frameworks and libraries.\nWhy Novartis: Helping people with disease and their families takes more than innovative science. It takes a community of smart, passionate people like you. Collaborating, supporting and inspiring each other. Combining to achieve breakthroughs that change patients lives. Ready to create a brighter future together? https://www. novartis. com / about / strategy / people-and-culture\nJoin our Novartis Network: Not the right Novartis role for you? Sign up to our talent community to stay connected and learn about suitable career opportunities as soon as they come up: https://talentnetwork. novartis. com/network\nBenefits and Rewards: Read our handbook to learn about all the ways we ll help you thrive personally and professionally:",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Performance tuning', 'Automation', 'Google Analytics', 'Machine learning', 'Data processing', 'Data quality', 'data visualization', 'Digital marketing', 'SQL', 'Python']",2025-06-12 14:45:08
Senior Applied AI Scientist,ZS,4 - 9 years,Not Disclosed,['Bengaluru'],"Write complex SQL queries for data extraction, perform exploratory data analysis (EDA) to uncover insights.\nStrong proficiency in Python and Py Spark for scalable data processing and analytics.\nCreate, transform, and optimize features to enhance model performance.\nTrain, evaluate, and maintain machine learning models in production.\nWrite efficient, maintainable, and version-controlled code that handles large datasets.\nRegularly update internal teams and clients on project progress, results, and insights.\nConduct hypothesis testing and experiment analysis to drive data-driven decisions using AB testing.",,,,"['Data analysis', 'data security', 'Financial planning', 'Management consulting', 'Machine learning', 'Data processing', 'Analytics', 'Data extraction', 'Python']",2025-06-12 14:45:10
GCP Data Engineer,TVS Next,3 - 5 years,Not Disclosed,['Bengaluru'],"What you’ll be doing:\nAssist in developing machine learning models based on project requirements\nWork with datasets by preprocessing, selecting appropriate data representations, and ensuring data quality.\nPerforming statistical analysis and fine-tuning using test results.\nSupport training and retraining of ML systems as needed.\nHelp build data pipelines for collecting and processing data efficiently.",,,,"['kubernetes', 'pyspark', 'data pipeline', 'sql', 'docker', 'cloud', 'tensorflow', 'java', 'spark', 'gcp', 'pytorch', 'bigquery', 'programming', 'ml', 'cloud sql', 'cd', 'python', 'airflow', 'cloud spanner', 'cloud pubsub', 'application engine', 'machine learning', 'apache flink', 'data engineering', 'dataproc', 'kafka', 'cloud storage', 'terraform', 'bigtable']",2025-06-12 14:45:13
Senior Data Engineer,Conviction HR,8 - 10 years,Not Disclosed,"['Kolkata', 'Hyderabad', 'Pune( Malad )']","Must have -Azure Data Factory (Mandatory). Azure Databricks, Pyspark and Python and advance SQL Azure eco-system. 1) Advanced SQL Skills. 2)Data Analysis. 3) Data Models. 4) Python (Desired). 5) Automation - Experience required : 8 to 10 years.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Data Factory', 'Data Engineering', 'Python', 'Azure Databricks', 'Data Modeling', 'Data Bricks', 'SQL']",2025-06-12 14:45:15
Data Engineer (AWS),Neoware Technology Solutions,4 - 10 years,Not Disclosed,"['Chennai', 'Bengaluru']","Data Engineer (AWS) - Neoware Technology Solutions Private Limited Data Engineer (AWS)\nRequirements\n4 - 10 years of hands-on experience in designing, developing and implementing data engineering solutions.\nStrong SQL development skills, including performance tuning and query optimization.\nGood understanding of data concepts.\nProficiency in Python and a solid understanding of programming concepts.\nHands-on experience with PySpark or Spark Scala for building data pipelines.\nUnderstanding of streaming data pipelines for near real-time analytics.\nExperience with Azure services including Data Factory, Functions, Databricks, Synapse Analytics, Event Hub, Stream Analytics and Data Lake Storage.\nFamiliarity with at least one NoSQL database.\nKnowledge of modern data architecture patterns and industry trends in data engineering.\nUnderstanding of data governance concepts for data platforms and analytical solutions.\nExperience with Git for managing version control for source code.\nExperience with DevOps processes, including experience implementing CI/CD pipelines for data engineering solutions.\nStrong analytical and problem-solving skills.\nExcellent communication and teamwork skills.\nResponsibilities\nAzure Certifications related to Data Engineering are highly preferred.\nExperience with Amazon AppFlow, EKS, API Gateway, NoSQL database services.\nStrong understanding and experience with BI/visualization tools like Power BI.\nChennai, Bangalore\nFull time\n4+ years\nOther positions Principal Architect (Data and Cloud) Development",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Performance tuning', 'Version control', 'GIT', 'NoSQL', 'query optimization', 'Analytical', 'data governance', 'Analytics', 'Python', 'Data architecture']",2025-06-12 14:45:17
GCP Data Engineer,Swits Digital,4 - 6 years,Not Disclosed,['Bengaluru'],"Job Title: GCP Data Engineer\nLocation: Chennai, Bangalore, Hyderabad\nExperience: 4-6 Years\nJob Summary:\nWe are seeking a GCP Data & Cloud Engineer with strong expertise in Google Cloud Platform services, including BigQuery, Cloud Run, Cloud Storage , and Pub/Sub . The ideal candidate will have deep experience in SQL coding , data pipeline development, and deploying cloud-native solutions.\nKey Responsibilities:\nDesign, implement, and optimize scalable data pipelines and services using GCP\nBuild and manage cloud-native applications deployed via Cloud Run\nDevelop complex and performance-optimized SQL queries for analytics and data transformation\nManage and automate data storage, retrieval, and archival using Cloud Storage\nImplement event-driven architectures using Google Pub/Sub\nWork with large datasets in BigQuery , including ETL/ELT design and query optimization\nEnsure security, monitoring, and compliance of cloud-based systems\nCollaborate with data analysts, engineers, and product teams to deliver end-to-end cloud solutions\nRequired Skills & Experience:\n4 years of experience working with Google Cloud Platform (GCP)\nStrong proficiency in SQL coding , query tuning, and handling complex data transformations\nHands-on experience with:\nBigQuery\nCloud Run\nCloud Storage\nPub/Sub\nUnderstanding of data pipeline and ETL/ELT workflows in cloud environments\nFamiliarity with containerized services and CI/CD pipelines\nExperience in scripting languages (e.g., Python, Shell) is a plus\nStrong analytical and problem-solving skills",Industry Type: Recruitment / Staffing,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['SUB', 'query optimization', 'GCP', 'Analytical', 'Cloud', 'query', 'cloud storage', 'Analytics', 'SQL coding', 'Python']",2025-06-12 14:45:19
Python Data Engineer with AI/ML,Expleo,5 - 6 years,Not Disclosed,['Pune'],"Overview\nWe are looking for a Python Data Engineer with expertise in real-time data monitoring, extraction, transformation, and visualization. The ideal candidate will have experience working with Oracle SQL databases, multithreading, and AI/ML techniques and should be proficient in deploying Python applications on IIS servers . The role involves developing a system to monitor live files and folders, extract data, transform it using various techniques, and display insights on a Plotly Dash-based dashboard .\nResponsibilities",,,,"['Computer science', 'IIS', 'Data analysis', 'Backend', 'Multithreading', 'Debugging', 'Data processing', 'Troubleshooting', 'Python', 'Data extraction']",2025-06-12 14:45:21
Gcp Data Engineer,Saama Technologies,3 - 8 years,Not Disclosed,"['Pune', 'Chennai', 'Coimbatore']","We are looking for immediate joiners only.\nPosition: GCP Data Engineer\nWe are seeking a skilled and experienced GCP Data Engineer to join our dynamic team. The ideal candidate will have a strong background in Google Cloud Platform (GCP), BigQuery, Dataform, and data warehouse concepts. Experience with Airflow/Cloud Composer and cloud computing knowledge will be a significant advantage.\nResponsibilities:\n- Designing, developing, and maintaining data pipelines and workflows on the Google Cloud Platform.",,,,"['Pyspark', 'GCP', 'Python', 'SQL', 'Google Cloud Platforms']",2025-06-12 14:45:24
Data Engineer (Azure),Neoware Technology Solutions,4 - 10 years,Not Disclosed,"['Chennai', 'Bengaluru']","Data Engineer (Azure) - Neoware Technology Solutions Private Limited Data Engineer (Azure)\nRequirements\n4 - 10 years of hands-on experience in designing, developing and implementing data engineering solutions.\nStrong SQL development skills, including performance tuning and query optimization.\nGood understanding of data concepts.\nProficiency in Python and a solid understanding of programming concepts.\nHands-on experience with PySpark or Spark Scala for building data pipelines.\nEnsure data consistency and address ambiguities or inconsistencies across datasets.\nUnderstanding of streaming data pipelines for near real-time analytics.\nExperience with Azure services including Data Factory, Functions, Databricks, Synapse Analytics, Event Hub, Stream Analytics and Data Lake Storage.\nFamiliarity with at least one NoSQL database.\nKnowledge of modern data architecture patterns and industry trends in data engineering.\nUnderstanding of data governance concepts for data platforms and analytical solutions.\nExperience with Git for managing version control for source code.\nExperience with DevOps processes, including experience implementing CI/CD pipelines for data engineering solutions.\nStrong analytical and problem-solving skills.\nExcellent communication and teamwork skills.\nResponsibilities\nAzure Certifications related to Data Engineering are highly preferred.\nExperience with Azure Kubernetes Service (AKS), Container Apps and API Management.\nStrong understanding and experience with BI/visualization tools like Power BI.\nChennai, Bangalore\nFull time\n4+ years\nOther positions\nChennai / Bangalore / Mumbai\n3+ years\nPrincipal Architect (Data and Cloud) Development",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Performance tuning', 'Version control', 'GIT', 'query optimization', 'NoSQL', 'Analytical', 'SCALA', 'Analytics', 'Python', 'Data architecture']",2025-06-12 14:45:26
Azure Data Engineer,JRD Systems,7 - 12 years,Not Disclosed,"['Hyderabad', 'Bengaluru']","Cloud Data Engineer\n\nThe Cloud Data Engineer will be responsible for developing the data lake platform and all applications on Azure cloud. Proficiency in data engineering, data modeling, SQL, and Python programming is essential. The Data Engineer will provide design and development solutions for applications in the cloud.\nEssential Job Functions:\nUnderstand requirements and collaborate with the team to design and deliver projects.\nDesign and implement data lake house projects within Azure.\nDevelop application lifecycle utilizing Microsoft Azure technologies.\nParticipate in design, planning, and necessary documentation.\nEngage in Agile ceremonies including daily standups, scrum, retrospectives, demos, and code reviews.\nHands-on experience with Python/SQL development and Azure data pipelines.\nCollaborate with the team to develop and deliver cross-functional products.\nKey Skills:\na. Data Engineering and SQL\nb. Python\nc. PySpark\nd. Azure Data Lake and ADF\ne. Databricks\nf. CI/CD\ng. Strong communication\nOther Responsibilities:\nDocument and maintain project artifacts.\nMaintain comprehensive knowledge of industry standards, methodologies, processes, and best practices.\nComplete training as required for Privacy, Code of Conduct, etc.\nPromptly report any known or suspected loss, theft, or unauthorized disclosure or use of PI to the General Counsel/Chief Compliance Officer or Chief Information Officer.\nAdhere to the company's compliance program.\nSafeguard the company's intellectual property, information, and assets.\nOther duties as assigned.\nMinimum Qualifications and Job Requirements:\nBachelor's degree in Computer Science.\n7 years of hands-on experience in designing and developing distributed data pipelines.\n5 years of hands-on experience in Azure data service technologies.\n5 years of hands-on experience in Python, SQL, Object-oriented programming, ETL, and unit testing.\nExperience with data integration with APIs, Web services, Queues.\nExperience with Azure DevOps and CI/CD as well as agile tools and processes including JIRA, Confluence.\n*Required: Azure data engineering associate and databricks data engineering certification",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'Delta Table', 'Azure Databricks', 'SQL', 'Python', 'SCALA', 'Big Data', 'Kafka', 'Azure Data Lake', 'Spark', 'ETL', 'Data Bricks']",2025-06-12 14:45:28
Azure Data Engineer ( Azure Databricks),Apex One,4 - 8 years,Not Disclosed,"['Hyderabad', 'Bengaluru']","Job Summary\nWe are seeking a skilled Azure Data Engineer with 4 years of overall experience, including at least 2 years of hands-on experience with Azure Databricks (Must). The ideal candidate will have strong expertise in building and maintaining scalable data pipelines and working across cloud-based data platforms.\nKey Responsibilities\nDesign, develop, and optimize large-scale data pipelines using Azure Data Factory, Azure Databricks, and Azure Synapse.\nImplement data lake solutions and work with structured and unstructured datasets in Azure Data Lake Storage (ADLS).\nCollaborate with data scientists, analysts, and engineering teams to design and deliver end-to-end data solutions.\nDevelop ETL/ELT processes and integrate data from multiple sources.\nMonitor, debug, and optimize workflows for performance and cost-efficiency.\nEnsure data governance, quality, and security best practices are maintained.\nMust-Have Skills\n4+ years of total experience in data engineering.\n2+ years of experience with Azure Databricks (PySpark, Notebooks, Delta Lake).\nStrong experience with Azure Data Factory, Azure SQL, and ADLS.\nProficient in writing SQL queries and Python/Scala scripting.\nUnderstanding of CI/CD pipelines and version control systems (e.g., Git).\nSolid grasp of data modeling and warehousing concepts.",Industry Type: Management Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure', 'Azure Data Factory', 'SQL queries', 'PySpark', 'Delta Lake', 'Azure Databricks', 'Notebooks', 'Azure SQL']",2025-06-12 14:45:30
"Data Engineer( Python, AWS, Databricks, EKS, Airflow)",Banking,5 - 9 years,Not Disclosed,['Bengaluru'],"Exprence 5-8 Years\nLocation - Bangalore\nMode C2H\n\nHands on data engineering experience.\nHands on experience with Python programming\nHands-on Experience with AWS & EKS\nWorking knowledge of Unix, Databases, SQL\nWorking Knowledge on Databricks\nWorking Knowledge on Airflow and DBT",Industry Type: Investment Banking / Venture Capital / Private Equity,Department: Engineering - Software & QA,"Employment Type: Full Time, Temporary/Contractual","['Airflow', 'Data Engineering', 'AWS', 'Python', 'SQL', 'Databricks', 'Eks']",2025-06-12 14:45:33
Big Data Engineer_ Info Edge _Noida,Info Edge,1 - 4 years,14-19 Lacs P.A.,['Noida'],"About Info Edge India Ltd.\nInfo Edge: Info Edge (India) Limited (NSE: NAUKRI) is among the leading internet companies in India. Info Edge, Indias premier online classifieds company is fundamentally in the matching business. With a network of 62 offices located in 43 cities throughout India, Info Edge has 5000 plus employees engaged in innovation, product development, integration with mobile and social media, technology and technology updation, research and development, quality assurance, sales, marketing and payment collection.\nThe umbrella brand has an online recruitment classifieds, www.naukri.com– India’s No. 1 Jobsite with over 75% traffic share, a matrimony classifieds, www.jeevansathi.com, a real estate classifieds, www.99acres.com– India’s largest property marketplace and an education classifieds, www.shiksha.com. Find out more about the Company at",,,,"['Data Modeling', 'Python', 'SCALA']",2025-06-12 14:45:35
"AI/ML TESTING-AI, ML, DEEP LEARNING, DATA MINING",Zensar,2 - 7 years,Not Disclosed,['Pune'],"Zensar Technologies is looking for AI/ML TESTING-AI, ML, DEEP LEARNING, DATA MINING, ANALYTICS AI/ML TESTING-AI, ML, DEEP LEARNING, DATA MINING, ANALYTICS to join our dynamic team and embark on a rewarding career journey\n\nDevelops and executes test plans for AI and machine learning models\n\nValidates model accuracy, fairness, performance, and edge-case behavior\n\nImplements automation tools and creates synthetic test datasets\n\nEnsures compliance with model validation protocols and documentation",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Design engineering', 'deep learning', 'Technology consulting', 'Focus', 'Agile', 'Conceptualization', 'Management', 'Data mining', 'Analytics', 'Testing']",2025-06-12 14:45:37
Data Engineer,Infoobjects Inc.,3 - 6 years,Not Disclosed,['Jaipur'],"Role & responsibilities:\nDesign, develop, and maintain robust ETL/ELT pipelines to ingest and process data from multiple sources.\nBuild and maintain scalable and reliable data warehouses, data lakes, and data marts.\nCollaborate with data scientists, analysts, and business stakeholders to understand data needs and deliver solutions.\nEnsure data quality, integrity, and security across all data systems.\nOptimize data pipeline performance and troubleshoot issues in a timely manner.\nImplement data governance and best practices in data management.\nAutomate data validation, monitoring, and reporting processes.\n\n\n\nPreferred candidate profile:\nBachelor's or Masters degree in Computer Science, Engineering, Information Systems, or related field.\nProven experience (X+ years) as a Data Engineer or similar role.\nStrong programming skills in Python, Java, or Scala.\nProficiency with SQL and working knowledge of relational databases (e.g., PostgreSQL, MySQL).\nHands-on experience with big data technologies (e.g., Spark, Hadoop).\nFamiliarity with cloud platforms such as AWS, GCP, or Azure (e.g., S3, Redshift, BigQuery, Data Factory).\nExperience with orchestration tools like Airflow or Prefect.\nKnowledge of data modeling, warehousing, and architecture design principles.\nStrong problem-solving skills and attention to detail.\n\nPerks and benefits\nFree Meals\nPF and Gratuity\nMedical and Term Insurance",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SCALA', 'Kafka', 'AWS', 'Python', 'Pyspark', 'Java', 'Postgresql', 'Hadoop', 'Spark', 'ETL', 'SQL']",2025-06-12 14:45:40
Immediate Joiner- Data Engineer,Healthedge,1 - 4 years,Not Disclosed,['Bengaluru'],"Data Engineer\nYou will be working with agile cross functional software development teams developing cutting age software to solve a significant problem in the Provider Data Management space. This hire will have experience building large scale complex data systems involving multiple cross functional data sets and teams. The ideal candidate will be excited about working on new product development, is comfortable pushing the envelope and challenging the status quo, sets high standards for him/herself and the team, and works well with ambiguity.\nWhat you will do:\nBuild data pipelines to assemble large, complex sets of data that meet non-functional and functional business requirements.\nWork closely with data architect, SMEs and other technology partners to develop & execute data architecture and product roadmap.\nBuild analytical tools to utilize the data pipeline, providing actionable insight into key business performance including operational efficiency and business metrics.\nWork with stakeholders including the leadership, product, customer teams to support their data infrastructure needs while assisting with data-related technical issues.\nAct as a subject matter expert to other team members for technical guidance, solution design and best practices within the customer organization.\nKeep current on big data and data visualization technology trends, evaluate, work on proof-of-concept and make recommendations on cloud technologies.\nWhat you bring:\n2+ years of data engineering experience working in partnership with large data sets (preferably terabyte scale)\nExperience in building data pipelines using any of the ETL tools such as Glue, ADF, Notebooks, Stored Procedures, SQL/Python constructs or similar.\nDeep experience working with industry standard RDBMS such Postgres, SQL Server, Oracle, MySQL etc. and any of the analytical cloud databases such as Big Query, Redshift, Snowflake or similar\nAdvanced SQL expertise and solid programming experience with Python and/or Spark\nExperience working with orchestration tools such as Airflow and building complex dependency workflows.\nExperience, developing and implementing Data Warehouse or Data Lake Architectures, OLAP technologies, data modeling with star/snowflake-schemas to enable analytics & reporting.\nGreat problem-solving capabilities, troubleshooting data issues and experience in stabilizing big data systems.\nExcellent communication and presentation skills as youll be regularly interacting with stakeholders and engineering leadership.\nBachelors or master's in quantitative disciplines such as Computer Science, Computer Engineering, Analytics, Mathematics, Statistics, Information Systems, or other scientific fields.\nBonus points:\nHands-on deep experience with cloud data migration, and experience working with analytic platforms like Fabric, Databricks on the cloud.\nCertification in one of the cloud platforms (AWS/GCP/Azure)\nExperience or demonstrated understanding with real-time data streaming tools like Kafka, Kinesis or any similar tools.",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['ETL', 'SQL', 'Pyspark', 'Cloud', 'Python']",2025-06-12 14:45:42
Data Engineer,Amgen Inc,1 - 6 years,Not Disclosed,['Hyderabad'],"Role Description:\nAs part of the cybersecurity organization, In this vital role you will be responsible for designing, building, and maintaining data infrastructure to support data-driven decision-making. This role involves working with large datasets, developing reports, executing data governance initiatives, and ensuring data is accessible, reliable, and efficiently managed. The role sits at the intersection of data infrastructure and business insight delivery, requiring the Data Engineer to design and build robust data pipelines while also translating data into meaningful visualizations for stakeholders across the organization. The ideal candidate has strong technical skills, experience with big data technologies, and a deep understanding of data architecture, ETL processes, and cybersecurity data frameworks.\nRoles & Responsibilities:\nDesign, develop, and maintain data solutions for data generation, collection, and processing.\nBe a key team member that assists in design and development of the data pipeline.\nBuild data pipelines and ensure data quality by implementing ETL processes to migrate and deploy data across systems.\nDevelop and maintain interactive dashboards and reports using tools like Tableau, ensuring data accuracy and usability\nSchedule and manage workflows the ensure pipelines run on schedule and are monitored for failures.\nCollaborate with multi-functional teams to understand data requirements and design solutions that meet business needs.\nDevelop and maintain data models, data dictionaries, and other documentation to ensure data accuracy and consistency.\nImplement data security and privacy measures to protect sensitive data.\nLeverage cloud platforms (AWS preferred) to build scalable and efficient data solutions.\nCollaborate and communicate effectively with product teams.\nCollaborate with data scientists to develop pipelines that meet dynamic business needs.\nShare and discuss findings with team members practicing SAFe Agile delivery model.\n\n\nBasic Qualifications:\nMasters degree and 1 to 3 years of experience of Computer Science, IT or related field experience OR\nBachelors degree and 3 to 5 years of Computer Science, IT or related field experience OR\nDiploma and 7 to 9 years of Computer Science, IT or related field experience\nPreferred Qualifications:\nHands on experience with data practices, technologies, and platforms, such as Databricks, Python, GitLab, LucidChart, etc.\nHands-on experience with data visualization and dashboarding toolsTableau, Power BI, or similar is a plus\nProficiency in data analysis tools (e.g. SQL) and experience with data sourcing tools\nExcellent problem-solving skills and the ability to work with large, complex datasets\nUnderstanding of data governance frameworks, tools, and best practices\nKnowledge of and experience with data standards (FAIR) and protection regulations and compliance requirements (e.g., GDPR, CCPA)\n\nGood-to-Have Skills:\nExperience with ETL tools and various Python packages related to data processing, machine learning model development\nStrong understanding of data modeling, data warehousing, and data integration concepts\nKnowledge of Python/R, Databricks, cloud data platforms\nExperience working in Product team's environment\nExperience working in an Agile environment\n\nProfessional Certifications:\nAWS Certified Data Engineer preferred\nDatabricks Certificate preferred\n\nSoft Skills:\nInitiative to explore alternate technology and approaches to solving problems\nSkilled in breaking down problems, documenting problem statements, and estimating efforts\nExcellent analytical and troubleshooting skills\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation\nAbility to handle multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data engineering', 'data analysis', 'data modeling', 'analysis tools', 'data warehousing', 'troubleshooting', 'data architecture', 'data integration', 'etl process']",2025-06-12 14:45:44
Data Engineer,Xenonstack,2 - 5 years,Not Disclosed,['Mohali( Phase 8B Mohali )'],"At XenonStack, We committed to become the Most Value Driven Cloud Native, Platform Engineering and Decision Driven Analytics Company. Our Consulting Services and Solutions towards the Neural Company and its Key Drivers.\nXenonStacks DataOps team is looking for a Data Engineer who will be responsible for employing techniques to create and sustain structures that allow for the analysis of data while remaining familiar with dominant programming and deployment strategies in the field.\nYou should demonstrate flexibility, creativity, and the capacity to receive and utilize constructive criticism. The ideal candidate should be highly skilled in all aspects of Python, Java/Scala, SQL and analytical skills.\nJob Responsibilities:\nDevelop, construct, test and maintain Data Platform Architectures\nAlign Data Architecture with business requirements\nLiaising with co-workers and clients to elucidate the requirements for each task.\nScalable and High Performant Data Platform Infrastructure that allows big data to be accessed and analysed quickly by BI & AI Teams.\nReformulating existing frameworks to optimize their functioning.\nTransforming Raw Data into InSights for manipulation by Data Scientists.\nEnsuring that your work remains backed up and readily accessible to relevant co-workers.\nRemaining up-to-date with industry standards and technological advancements that will improve the quality of your outputs.\nRequirements:\nTechnical Requirements\nExperience of Python, Java/Scala\nGreat Statistical / SQL based Analytical Skills\nExperience of Data Analytics Architectural Design Patterns for Batch, Event Driven and Real-Time Analytics Use Cases\nUnderstanding of Data warehousing, ETL tools, machine learning, Data EPIs\nExcellent in Algorithms and Data Systems\nUnderstanding of Distributed System for Data Processing and Analytics\nFamiliarity with Popular Data Analytics Framework like Hadoop , Spark , Delta Lake , Time Series / Analytical Stores Stores.\nProfessional Attributes:\nExcellent communication skills & Attention to detail.\nAnalytical mind and problem-solving Aptitude with Strong Organizational skills & Visual Thinking.\nBenefits:\nDiscover the benefits of joining our team:\nDynamic and purposeful work culture in a people-oriented organization contributing to multi-million-dollar projects with guaranteed job security.\nOpen, authentic, and transparent communication fostering a warm work environment.\nRegular constructive feedback and exposure to diverse technologies.\nRecognition and rewards for exceptional performance achievements.\nAccess to certification courses & Skill Sessions to develop continually and refine your skills.\nAdditional allowances for team members assigned to specific projects.\nSpecial skill allowances to acknowledge and compensate for unique expertise.\nComprehensive medical insurance policy for your health and well-being.\nTo Learn more about the company -\nWebsite - http://www.xenonstack.com/",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Hadoop', 'Spark', 'ETL', 'Python', 'SQL', 'Java', 'Data Processing', 'Machine Learning']",2025-06-12 14:45:46
Data Analyst,Skywings Advisors,4 - 9 years,10-17 Lacs P.A.,['Mumbai'],"Actively work with latest technologies&leading practices specific to analytics,data visualization,AI/ML&RPA to drive strategic benefits in the area of audit quality,efficiency&value creation leading audit related data extractions,enablement,analytics\n\nRequired Candidate profile\nMin 4 yrs relevant data analytics experience,EDW concepts,good understanding of data mining and programming languages such as python, oracle sql\nworking exp with visualization tools power BI,SAP BO",Industry Type: Banking,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Analysis', 'Data Anlalytics', 'Power Bi', 'Oracle SQL', 'Data Visualization', 'Data Extraction', 'SQL']",2025-06-12 14:45:48
Data Analyst,Flywings Hr Services,10 - 17 years,20-22.5 Lacs P.A.,['Pune'],"we are looking Data analyst who has experienced in Data transformation, Etl, Data modeling at least 5 years experienced & No Sql & Complex database at least 5 Years Experienced",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Transformation', 'NoSQL', 'Data Modeling', 'ETL']",2025-06-12 14:45:50
Data Engineer,Amgen Inc,1 - 6 years,Not Disclosed,['Hyderabad'],"ABOUT THE ROLE\nRole Description:\nAs part of the cybersecurity organization, the Data Engineer is responsible for designing, building, and maintaining data infrastructure to support data-driven decision-making. This role involves working with large datasets, developing reports, executing data governance initiatives, and ensuring data is accessible, reliable, and efficiently managed. The ideal candidate has strong technical skills, experience with big data technologies, and a deep understanding of data architecture, ETL processes, and cybersecurity data frameworks.\nRoles & Responsibilities:\nDesign, develop, and maintain data solutions for data generation, collection, and processing.\nBe a key team member that assists in design and development of the data pipeline.\nCreate data pipelines and ensure data quality by implementing ETL processes to migrate and deploy data across systems.\nSchedule and manage workflows the ensure pipelines run on schedule and are monitored for failures.\nCollaborate with cross-functional teams to understand data requirements and design solutions that meet business needs.\nDevelop and maintain data models, data dictionaries, and other documentation to ensure data accuracy and consistency.\nImplement data security and privacy measures to protect sensitive data.\nLeverage cloud platforms (AWS preferred) to build scalable and efficient data solutions.\nCollaborate and communicate effectively with product teams.\nCollaborate with data scientists to develop pipelines that meet dynamic business needs.\nShare and discuss findings with team members practicing SAFe Agile delivery model.\nFunctional Skills:\nBasic Qualifications:\nMasters degree and 1 to 3 years of Computer Science, IT or related field experience OR\nBachelors degree and 3 to 5 years of Computer Science, IT or related field experience OR\nDiploma and 7 to 9 years of Computer Science, IT or related field experience\nPreferred Qualifications:\nHands on experience with data practices, technologies, and platforms, such as Databricks, Python, Gitlab, LucidChart,etc.\nProficiency in data analysis tools (e.g. SQL) and experience with data sourcing tools\nExcellent problem-solving skills and the ability to work with large, complex datasets\nUnderstanding of data governance frameworks, tools, and best practices\nKnowledge of and experience with data standards (FAIR) and protection regulations and compliance requirements (e.g., GDPR, CCPA)\nGood-to-Have Skills:\nExperience with ETL tools and various Python packages related to data processing, machine learning model development\nStrong understanding of data modeling, data warehousing, and data integration concepts\nKnowledge of Python/R, Databricks, cloud data platforms\nExperience working in Product team's environment\nExperience working in an Agile environment\nProfessional Certifications:\nAWS Certified Data Engineer preferred\nDatabricks Certificate preferred\nSoft Skills:\nInitiative to explore alternate technology and approaches to solving problems\nSkilled in breaking down problems, documenting problem statements, and estimating efforts\nExcellent analytical and troubleshooting skills\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation\nAbility to manage multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data engineering', 'data security', 'Agile', 'cloud data platforms', 'Databricks', 'data governance frameworks', 'ETL', 'AWS', 'SQL', 'Python']",2025-06-12 14:45:52
Data Engineer,Atyeti,2 - 4 years,Not Disclosed,['Pune'],"Role & responsibilities\n\nDevelop and Maintain Data Pipelines: Design, develop, and manage scalable ETL pipelines to process large datasets using PySpark, Databricks, and other big data technologies.\nData Integration and Transformation: Work with various structured and unstructured data sources to build efficient data workflows and integrate them into a central data warehouse.\nCollaborate with Data Scientists & Analysts: Work closely with the data science and business intelligence teams to ensure the right data is available for advanced analytics, machine learning, and reporting.",,,,"['Azure Synapse', 'Pyspark', 'ETL', 'Python']",2025-06-12 14:45:54
Data Engineer - SSIS - 5+ Years - Gurugram (Hybrid),Crescendo Global,5 - 10 years,Not Disclosed,['Gurugram'],"Data Engineer - SSIS - 5+ Years - Gurugram (Hybrid)\n\nAre you a skilled Data Engineer with expertise in SSIS and 5+ years of experience? Do you have a passion for analytics and want to work in a hybrid setup in Gurugram? Our client is seeking a talented individual to join their team and contribute to their data engineering projects.\n\nLocation : Gurugram (Hybrid)\n\nYour Future EmployerOur client is a leading organization in the analytics domain, known for fostering an inclusive and diverse work environment. They are committed to providing their employees with opportunities for growth and development.\n\nResponsibilities\nDesign, develop, and maintain data pipelines using SSIS for efficient data processing\nCollaborate with cross-functional teams to understand data requirements and provide effective data solutions\nOptimize data pipelines for performance and scalability\nEnsure data quality and integrity throughout the data engineering process\n\nRequirements\n5+ years of experience in data engineering with a strong focus on SSIS\nProficiency in data warehousing concepts and ETL processes\nHands-on experience with SQL databases and data modeling4.Strong analytical and problem-solving skills\nBachelor's degree in Computer Science, Engineering, or related field\n\nWhat's in it for you : In this role, you will have the opportunity to work on challenging projects and enhance your expertise in data engineering. The organization offers a competitive compensation package and a supportive work environment where your contributions are valued.\n\nReach us : If you feel this opportunity is well aligned with your career progression plans, please feel free to reach me with your updated profile at rohit.kumar@crescendogroup.in\n\nDisclaimer : Crescendo Global specializes in Senior to C-level niche recruitment. We are passionate about empowering job seekers and employers with an engaging memorable job search and leadership hiring experience. Crescendo Global does not discriminate on the basis of race, religion, color, origin, gender, sexual orientation, age, marital status, veteran status or disability status.\n\nNote : We receive a lot of applications on a daily basis so it becomes a bit difficult for us to get back to each candidate. Please assume that your profile has not been shortlisted in case you don't hear back from us in 1 week. Your patience is highly appreciated.\n\nScammers can misuse Crescendo Globals name for fake job offers. We never ask for money, purchases, or system upgrades. Verify all opportunities at www.crescendo-global.com and report fraud immediately. Stay alert!\n\nProfile keywords : Data Engineer, SSIS, Data Warehousing, ETL, SQL, Analytics",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Engineering', 'SSIS', 'SQL', 'Analytics']",2025-06-12 14:45:56
Director - Data Science,Axtria,12 - 17 years,Not Disclosed,['Noida'],"Minimum 12+ years of relevant experience in building software applications in data and analytics field\nEnhance the go-to-market strategy by designing new and relevant solution frameworks to accelerate our clients’ journeys for impacting patient outcomes. Pitch for these opportunities and craft winning proposals to grow the Data Science Practice.\nBuild and lead a team of data scientists and analysts, fostering a collaborative and innovative environment.\nOversee the design and delivery of the models, ensuring projects are completed on time and meet business objectives.\nEngaging in consultative selling with clients to grow/deliver business.\nDevelop and operationalize scalable processes to deliver on large & complex client engagements.\nExtensive hands-on experience with Python, R, or Julia, focusing on data science and generative AI frameworks.\nExpertise in working with generative models such as GPT, DALL-E, Stable Diffusion, Codex, and MidJourney for various applications.\nProficiency in fine-tuning and deploying generative models using libraries like Hugging Face Transformers, Diffusers, or PyTorch Lightning.\nStrong understanding of generative techniques, including GANs, VAEs, diffusion models, and autoregressive models.\nExperience in prompt engineering, zero-shot, and few-shot learning for optimizing generative AI outputs across different use cases.\nExpertise in managing generative AI data pipelines, including preprocessing large-scale multimodal datasets for text, image, or code generation.",Industry Type: Analytics / KPO / Research,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['application software', 'python', 'artificial intelligence', 'r', 'julia', 'hive', 'natural language processing', 'neural networks', 'predictive analytics', 'machine learning', 'sql', 'deep learning', 'java', 'data science', 'spark', 'predictive modeling', 'pytorch', 'hadoop', 'statistics']",2025-06-12 14:45:58
Manager - Data Science,Axtria,6 - 11 years,Not Disclosed,['Noida'],"Job Summary-\n\nData Scientist with good hands-on experience of 6+ years in developing state of the art and scalable Machine Learning models and their operationalization, leveraging off-the-shelf workbench production.\n\nJob Responsibilities-\n\nHands on experience in Python data-science and math packages such as NumPy, Pandas, Sklearn, Seaborn, PyCaret, Matplotlib\nProficiency in Python and common Machine Learning frameworks (TensorFlow, NLTK, Stanford NLP, PyTorch, Ling Pipe, Caffe, Keras, SparkML and OpenAI etc.)\nExperience of working in large teams and using collaboration tools like GIT, Jira and Confluence\nGood understanding of any of the cloud platform – AWS, Azure or GCP\nUnderstanding of Commercial Pharma landscape and Patient Data / Analytics would be a huge plus\nShould have an attitude of willingness to learn, accepting the challenging environment and confidence in delivering the results within timelines. Should be inclined towards self motivation and self-driven to find solutions for problems.\nShould be able to mentor and guide mid to large sized teams under him/her\n\n\nJob -\nStrong experience on Spark with Scala/Python/Java\nStrong proficiency in building/training/evaluating state of the art machine learning models and its deployment\nProficiency in Statistical and Probabilistic methods such as SVM, Decision-Trees, Bagging and Boosting Techniques, Clustering\nProficiency in Core NLP techniques like Text Classification, Named Entity Recognition (NER), Topic Modeling, Sentiment Analysis, etc. Understanding of Generative AI / Large Language Models / Transformers would be a plus",Industry Type: Analytics / KPO / Research,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['scala', 'java', 'spark', 'machine learning algorithms', 'python', 'confluence', 'scikit-learn', 'nltk', 'training', 'numpy', 'tensorflow', 'git', 'seaborn', 'gcp', 'pytorch', 'keras', 'spark mllib', 'jira', 'sentiment analysis', 'lingpipe', 'caffe', 'microsoft azure', 'pandas', 'matplotlib', 'aws', 'statistics']",2025-06-12 14:46:01
Data Engineer,Konrad Group,3 - 7 years,15-30 Lacs P.A.,['Gurugram( Sector 42 Gurgaon )'],"Who We Are\n\nKonrad is a next generation digital consultancy. We are dedicated to solving complex business problems for our global clients with creative and forward-thinking solutions. Our employees enjoy a culture built on innovation and a commitment to creating best-in-class digital products in use by hundreds of millions of consumers around the world. We hire exceptionally smart, analytical, and hard working people who are lifelong learners.\nAbout The Role\nAs a Data Engineer youll be tasked with designing, building, and maintaining scalable data platforms and pipelines. Your deep knowledge of data platforms such as Azure Fabric, Databricks, and Snowflake will be essential as you collaborate closely with data analysts, scientists, and other engineers to ensure reliable, secure, and efficient data solutions.\n\nWhat Youll Do\n\nDesign, build, and manage robust data pipelines and data architectures.\nImplement solutions leveraging platforms such as Azure Fabric, Databricks, and Snowflake.\nOptimize data workflows, ensuring reliability, scalability, and performance.\nCollaborate with internal stakeholders to understand data needs and deliver tailored solutions.\nEnsure data security and compliance with industry standards and best practices.\nPerform data modelling, data extraction, transformation, and loading (ETL/ELT).\nIdentify and recommend innovative solutions to enhance data quality and analytics capabilities.\n\nQualifications\n\nBachelors degree or higher in Computer Science, Data Engineering, Information Technology, or a related field.\nAt least 3 years of professional experience as a Data Engineer or similar role.\nProficiency in data platforms such as Azure Fabric, Databricks, and Snowflake.\nHands-on experience with data pipeline tools, cloud services, and storage solutions.\nStrong programming skills in SQL, Python, or related languages.\nExperience with big data technologies and concepts (Spark, Hadoop, Kafka).\nExcellent analytical, troubleshooting, and problem-solving skills.\nAbility to effectively communicate technical concepts clearly to non-technical stakeholders.\nAdvanced English\n\nNice to have\n\nCertifications related to Azure Data Engineering, Databricks, or Snowflake.\nFamiliarity with DevOps practices and CI/CD pipelines.\n\nPerks and Benefits\n\nComprehensive Health & Wellness Benefits Package \nSocials, Outings & Retreats\nCulture of Learning & Development\nFlexible Working Hours\nWork from Home Flexibility\nService Recognition Programs\n\nKonrad is committed to maintaining a diverse work environment and is proud to be an equal opportunity employer. All qualified applicants, regardless of race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status will receive consideration for employment. If you have any accessibility requirements or concerns regarding the hiring process or employment with us, please notify us so we can provide suitable accommodation.\nWhile we sincerely appreciate all applications, only those candidates selected for an interview will be contacted.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Hadoop', 'Azure Data Factory', 'Azure Databricks', 'Spark', 'Fabric', 'Python']",2025-06-12 14:46:03
Azure Data Engineer,Big4,7 - 12 years,18-30 Lacs P.A.,['Bengaluru'],"Urgently Hiring for Senior Azure Data Engineer\n\nJob Location- Bangalore\nMinimum exp - Total 7+yrs with min 4 years relevant exp\n\nKeywords Databricks, Pyspark, SCALA, SQL, Live / Streaming data, batch processing data\n\nShare CV siddhi.pandey@adecco.com\nOR Call 6366783349\n\nRoles and Responsibilities:\nThe Data Engineer will work on data engineering projects for various business units, focusing on delivery of complex data management solutions by leveraging industry best practices. They work with the project team to build the most efficient data pipelines and data management solutions that make data easily available for consuming applications and analytical solutions. A Data engineer is expected to possess strong technical skills\n\nKey Characteristics\nTechnology champion who constantly pursues skill enhancement and has inherent curiosity to understand work from multiple dimensions\nInterest and passion in Big Data technologies and appreciates the value that can be brought in with an effective data management solution\nHas worked on real data challenges and handled high volume, velocity, and variety of data.\nExcellent analytical & problem-solving skills, willingness to take ownership and resolve technical challenges.\nContributes to community building initiatives like CoE, CoP.\nMandatory skills:\nAzure - Master\nELT - Skill\nData Modeling - Skill\nData Integration & Ingestion - Skill\nData Manipulation and Processing - Skill\nGITHUB, Action, Azure DevOps - Skill\nData factory, Databricks, SQL DB, Synapse, Stream Analytics, Glue, Airflow, Kinesis, Redshift, SonarQube, PyTest - Skill\nOptional skills:\nExperience in project management, running a scrum team.\nExperience working with BPC, Planning.\nExposure to working with external technical ecosystem.\nMKDocs documentation\n\nShare CV siddhi.pandey@adecco.com\nOR Call 6366783349",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['databricks', 'Azure Synapse', 'Pyspark', 'Stream Analytics', 'SCALA', 'SQL Azure', 'Data Bricks', 'SQL']",2025-06-12 14:46:05
Gcp Data Engineer,Royal Cyber,9 - 14 years,Not Disclosed,[],"Minimum 7+ years in data engineering with 5+ years of hands-on experience on GCP.\nProven track record with tools and services like BigQuery, Cloud Composer (Apache Airflow), Cloud Functions, Pub/Sub, Cloud Storage, Dataflow, and IAM/VPC.\nDemonstrated expertise in Apache Spark (batch and streaming), PySpark, and building scalable API integrations.\nAdvanced Airflow skills including custom operators, dynamic DAGs, and workflow performance tuning.\nCertifications\nGoogle Cloud Professional Data Engineer certification preferred.\nKey Skills\nMandatory Technical Skills\nAdvanced Python (PySpark, Pandas, pytest) for automation and data pipelines.\nStrong SQL with experience in window functions, CTEs, partitioning, and optimization.\nProficiency in GCP services including BigQuery, Dataflow, Cloud Composer, Cloud Functions, and Cloud Storage.\nHands-on with Apache Airflow, including dynamic DAGs, retries, and SLA enforcement.\nExpertise in API data ingestion, Postman collections, and REST/GraphQL integration workflows.\nFamiliarity with CI/CD workflows using Git, Jenkins, or Bitbucket.\nExperience with infrastructure security and governance using IAM and VPC.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Part Time, Temporary/Contractual","['GCP', 'Bigquery', 'Google Cloud Platforms', 'Cloud Storage', 'Data Flow']",2025-06-12 14:46:07
Data Engineer,Databeat,3 - 7 years,Not Disclosed,['Hyderabad( Rai Durg )'],"Experience Required: 3+ years\n\nTechnical knowledge: AWS, Python, SQL, S3, EC2, Glue, Athena, Lambda, DynamoDB, RedShift, Step Functions, Cloud Formation, CI/CD Pipelines, Github, EMR, RDS,AWS Lake Formation, GitLab, Jenkins and AWS CodePipeline.\n\n\n\nRole Summary: As a Senior Data Engineer,with over 3 years of expertise in Python, PySpark, SQL to design, develop and optimize complex data pipelines, support data modeling, and contribute to the architecture that supports big data processing and analytics to cutting-edge cloud solutions that drive business growth. You will lead the design and implementation of scalable, high-performance data solutions on AWS and mentor junior team members.This role demands a deep understanding of AWS services, big data tools, and complex architectures to support large-scale data processing and advanced analytics.\nKey Responsibilities:\nDesign and develop robust, scalable data pipelines using AWS services, Python, PySpark, and SQL that integrate seamlessly with the broader data and product ecosystem.\nLead the migration of legacy data warehouses and data marts to AWS cloud-based data lake and data warehouse solutions.\nOptimize data processing and storage for performance and cost.\nImplement data security and compliance best practices, in collaboration with the IT security team.\nBuild flexible and scalable systems to handle the growing demands of real-time analytics and big data processing.\nWork closely with data scientists and analysts to support their data needs and assist in building complex queries and data analysis pipelines.\nCollaborate with cross-functional teams to understand their data needs and translate them into technical requirements.\nContinuously evaluate new technologies and AWS services to enhance data capabilities and performance.\nCreate and maintain comprehensive documentation of data pipelines, architectures, and workflows.\nParticipate in code reviews and ensure that all solutions are aligned to pre-defined architectural specifications.\nPresent findings to executive leadership and recommend data-driven strategies for business growth.\nCommunicate effectively with different levels of management to gather use cases/requirements and provide designs that cater to those stakeholders.\nHandle clients in multiple industries at the same time, balancing their unique needs.\nProvide mentoring and guidance to junior data engineers and team members.\n\n\n\nRequirements:\n3+ years of experience in a data engineering role with a strong focus on AWS, Python, PySpark, Hive, and SQL.\nProven experience in designing and delivering large-scale data warehousing and data processing solutions.\nLead the design and implementation of complex, scalable data pipelines using AWS services such as S3, EC2, EMR, RDS, Redshift, Glue, Lambda, Athena, and AWS Lake Formation.\nBachelor's or Masters degree in Computer Science, Engineering, or a related technical field.\nDeep knowledge of big data technologies and ETL tools, such as Apache Spark, PySpark, Hadoop, Kafka, and Spark Streaming.\nImplement data architecture patterns, including event-driven pipelines, Lambda architectures, and data lakes.\nIncorporate modern tools like Databricks, Airflow, and Terraform for orchestration and infrastructure as code.\nImplement CI/CD using GitLab, Jenkins, and AWS CodePipeline.\nEnsure data security, governance, and compliance by leveraging tools such as IAM, KMS, and AWS CloudTrail.\nMentor junior engineers, fostering a culture of continuous learning and improvement.\nExcellent problem-solving and analytical skills, with a strategic mindset.\nStrong communication and leadership skills, with the ability to influence stakeholders at all levels.\nAbility to work independently as well as part of a team in a fast-paced environment.\nAdvanced data visualization skills and the ability to present complex data in a clear and concise manner.\nExcellent communication skills, both written and verbal, to collaborate effectively across teams and levels.\n\nPreferred Skills:\nExperience with Databricks, Snowflake, and machine learning pipelines.\nExposure to real-time data streaming technologies and architectures.\nFamiliarity with containerization and serverless computing (Docker, Kubernetes, AWS Lambda).",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'Aws Glue', 'SQL', 'Data Pipeline', 'Python', 'Amazon Ec2', 'Data Engineering', 'Data Bricks', 'Aws Lambda', 'Amazon Redshift', 'Azure Cloud', 'Data Lake', 'Data Modeling', 'Athena']",2025-06-12 14:46:10
Data Engineer IV - Big Data / Spark,Sadup Soft,5 - 7 years,Not Disclosed,['Chennai'],"Must have skills :\n\n- Minimum of 5-7 years of experience in software development, with a focus on Java and infrastructure tools.\n\n- Min 6+ years of experience as a Data Engineer.\n\n- Good Experience in handling Big Data Spark, Hive SQL, BigQuery, SQL.\n\n- Candidate worked on cloud platforms and GCP would be an added advantage.\n\n- Good understanding of Hadoop based ecosystem including hard sequel, HDFS would be very essential.\n\n- Very good professional knowledge of PySpark or using Scala\n\nResponsibilities :\n\n- Collaborate with cross-functional teams such as Data Scientists, Product Partners and Partner Team Developers to identify opportunities for Big Data, Query ( Spark, Hive SQL, BigQuery, SQL ) tuning opportunities that can be solved using machine learning and generative AI.\n\n- Write clean, high-performance, high-quality, maintainable code.\n\n- Design and develop Big Data Engineering Solutions Applications for above ensuring scalability, efficiency, and maintainability of such solutions.\n\nRequirements :\n\n- A Bachelor or Master's degree in Computer Science or a related field.\n\n- Proven experience working as a Big Data & MLOps Engineer, with a focus on Spark, Scala Spark or PySpark, Spark SQL, BigQuery, Python, Google Cloud,.\n\n- Deep understanding and experience in tuning Dataproc, BigQuery, Spark Applications.\n\n- Solid knowledge of software engineering best practices, including version control systems (e.g Git), code reviews, and testing methodologies.\n\n- Strong communication skills to effectively collaborate and present findings to both technical and non-technical stakeholders.\n\n- Proven ability to adapt and learn new technologies and frameworks quickly.\n\n- A proactive mindset with a passion for continuous learning and research.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Big Data', 'Data Engineering', 'BigQuery', 'GCP', 'Spark', 'Machine Learning', 'Python', 'SQL']",2025-06-12 14:46:12
Data Engineer - AWS,Happiest Minds Technologies,6 - 10 years,Not Disclosed,"['Pune', 'Bengaluru']","Role & responsibilities\nEssential Skills: Experience: 6 to 10 yrs\n- Technical Expertise: Proficiency in AWS services such as Amazon S3, Redshift, EMR, Glue, Lambda, and Kinesis. Strong skills in SQL and experience with scripting languages like Python or Java.\n- Data Engineering Experience: Hands on experience in building and maintaining data pipelines, data modeling, and working with big data technologies.\n- Problem-Solving Skills: Ability to analyze complex data issues and develop effective solutions to optimize data processing and storage.",,,,"['Data Engineering', 'Pyspark', 'Aws Lambda', 'Amazon Redshift', 'Aws Glue', 'Athena', 'AWS', 'Python', 'SQL']",2025-06-12 14:46:14
Aws Data Engineer,Hiring for Leading MNC Company!!,8 - 13 years,Not Disclosed,['Bengaluru'],"Warm Greetings from SP Staffing!!\n\nRole:AWS Data Engineer\nExperience Required :8 to 15 yrs\nWork Location :Bangalore\n\nRequired Skills,\n\nTechnical knowledge of data engineering solutions and practices. Implementation of data pipelines using tools like EMR, AWS Glue, AWS Lambda, AWS Step Functions, API Gateway, Athena\nProficient in Python and Spark, with a focus on ETL data processing and data engineering practices.\n\nInterested candidates can send resumes to nandhini.spstaffing@gmail.com",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['ETL', 'AWS', 'Pyspark', 'python', 'EMR', 'Aws Glue', 'Aws Emr', 'AWS Data Engineer', 'Aws Lambda', 'Lakehouse', 'spark', 'Data Engineer', 'Athena', 'gateway']",2025-06-12 14:46:16
Data Engineer,XL India Business Services Pvt. Ltd,1 - 7 years,Not Disclosed,['Gurugram'],"Senior Engineer, Data Modeling Gurgaon/Bangalore, India AXA XL recognizes data and information as critical business assets, both in terms of managing risk and enabling new business opportunities\n\nThis data should not only be high quality, but also actionable - enabling AXA XL s executive leadership team to maximize benefits and facilitate sustained industrious advantage\n\nOur Chief Data Office also known as our Innovation, Data Intelligence & Analytics team (IDA) is focused on driving innovation through optimizing how we leverage data to drive strategy and create a new business model - disrupting the insurance market\n\nAs we develop an enterprise-wide data and digital strategy that moves us toward greater focus on the use of data and data-driven insights, we are seeking a Data Engineer\n\nThe role will support the team s efforts towards creating, enhancing, and stabilizing the Enterprise data lake through the development of the data pipelines\n\nThis role requires a person who is a team player and can work well with team members from other disciplines to deliver data in an efficient and strategic manner\n\nWhat you ll be doing What will your essential responsibilities include? Act as a data engineering expert and partner to Global Technology and data consumers in controlling complexity and cost of the data platform, whilst enabling performance, governance, and maintainability of the estate\n\nUnderstand current and future data consumption patterns, architecture (granular level), partner with Architects to make sure optimal design of data layers\n\nApply best practices in Data architecture\n\nFor example, balance between materialization and virtualization, optimal level of de-normalization, caching and partitioning strategies, choice of storage and querying technology, performance tuning\n\nLeading and hands-on execution of research into new technologies\n\nFormulating frameworks for assessment of new technology vs business benefit, implications for data consumers\n\nAct as a best practice expert, blueprint creator of ways of working such as testing, logging, CI/CD, observability, release, enabling rapid growth in data inventory and utilization of Data Science Platform\n\nDesign prototypes and work in a fast-paced iterative solution delivery model\n\nDesign, Develop and maintain ETL pipelines using Py spark in Azure Databricks using delta tables\n\nUse Harness for deployment pipeline\n\nMonitor Performance of ETL Jobs, resolve any issue that arose and improve the performance metrics as needed\n\nDiagnose system performance issue related to data processing and implement solution to address them\n\nCollaborate with other teams to make sure successful integration of data pipelines into larger system architecture requirement\n\nMaintain integrity and quality across all pipelines and environments\n\nUnderstand and follow secure coding practice to make sure code is not vulnerable\n\nYou will report to the Application Manager\n\nWhat you will BRING We re looking for someone who has these abilities and skills: Required Skills and Abilities: Effective Communication skills\n\nBachelor s degree in computer science, Mathematics, Statistics, Finance, related technical field, or equivalent work experience\n\nRelevant years of extensive work experience in various data engineering & modeling techniques (relational, data warehouse, semi-structured, etc), application development, advanced data querying skills\n\nRelevant years of programming experience using Databricks\n\nRelevant years of experience using Microsoft Azure suite of products (ADF, synapse and ADLS)\n\nSolid knowledge on network and firewall concepts\n\nSolid experience writing, optimizing and analyzing SQL\n\nRelevant years of experience with Python\n\nAbility to break complex data requirements and architect solutions into achievable targets\n\nRobust familiarity with Software Development Life Cycle (SDLC) processes and workflow, especially Agile\n\nExperience using Harness\n\nTechnical lead responsible for both individual and team deliveries\n\nDesired Skills and Abilities: Worked in big data migration projects\n\nWorked on performance tuning both at database and big data platforms\n\nAbility to interpret complex data requirements and architect solutions\n\nDistinctive problem-solving and analytical skills combined with robust business acumen\n\nExcellent basics on parquet files and delta files\n\nEffective Knowledge of Azure cloud computing platform\n\nFamiliarity with Reporting software - Power BI is a plus\n\nFamiliarity with DBT is a plus\n\nPassion for data and experience working within a data-driven organization\n\nYou care about what you do, and what we do\n\nWho WE are AXA XL, the P&C and specialty risk division of AXA, is known for solving complex risks\n\nFor mid-sized companies, multinationals and even some inspirational individuals we don t just provide re/insurance, we reinvent it\n\nHow? By combining a comprehensive and efficient capital platform, data-driven insights, leading technology, and the best talent in an agile and inclusive workspace, empowered to deliver top client service across all our lines of business property, casualty, professional, financial lines and specialty\n\nWith an innovative and flexible approach to risk solutions, we partner with those who move the world forward\n\nLearn more at axaxl\n\ncom What we OFFER Inclusion AXA XL is committed to equal employment opportunity and will consider applicants regardless of gender, sexual orientation, age, ethnicity and origins, marital status, religion, disability, or any other protected characteristic\n\nAt AXA XL, we know that an inclusive culture and a diverse workforce enable business growth and are critical to our success\n\nThat s why we have made a strategic commitment to attract, develop, advance and retain the most diverse workforce possible, and create an inclusive culture where everyone can bring their full selves to work and can reach their highest potential\n\nIt s about helping one another and our business to move forward and succeed\n\nFive Business Resource Groups focused on gender, LGBTQ+, ethnicity and origins, disability and inclusion with 20 Chapters around the globe Robust support for Flexible Working Arrangements Enhanced family friendly leave benefits Named to the Diversity Best Practices Index Signatory to the UK Women in Finance Charter Learn more at axaxl\n\ncom / about-us / inclusion-and-diversity\n\nAXA XL is an Equal Opportunity Employer\n\nTotal Rewards AXA XL s Reward program is designed to take care of what matters most to you, covering the full picture of your health, wellbeing, lifestyle and financial security\n\nIt provides dynamic compensation and personalized, inclusive benefits that evolve as you do\n\nWe re committed to rewarding your contribution for the long term, so you can be your best self today and look forward to the future with confidence\n\nSustainability At AXA XL, Sustainability is integral to our business strategy\n\nIn an ever-changing world, AXA XL protects what matters most for our clients and communities\n\nWe know that sustainability is at the root of a more resilient future\n\nOur 2023-26 Sustainability strategy, called Roots of resilience , focuses on protecting natural ecosystems, addressing climate change, and embedding sustainable practices across our operations\n\nOur Pillars: Valuing nature: How we impact nature affects how nature impacts us\n\nResilient ecosystems - the foundation of a sustainable planet and society - are essential to our future\n\nWe re committed to protecting and restoring nature - from mangrove forests to the bees in our backyard - by increasing biodiversity awareness and inspiring clients and colleagues to put nature at the heart of their plans\n\nAddressing climate change: The effects of a changing climate are far reaching and significant\n\nUnpredictable weather, increasing temperatures, and rising sea levels cause both social inequalities and environmental disruption\n\nWere building a net zero strategy, developing insurance products and services, and mobilizing to advance thought leadership and investment in societal-led solutions\n\nIntegrating ESG: All companies have a role to play in building a more resilient future\n\nIncorporating ESG considerations into our internal processes and practices builds resilience from the roots of our business\n\nWe re training our colleagues, engaging our external partners, and evolving our sustainability governance and reporting\n\nAXA Hearts in Action: We have established volunteering and charitable giving programs to help colleagues support causes that matter most to them, known as AXA XL s Hearts in Action programs\n\nThese include our Matching Gifts program, Volunteering Leave, and our annual volunteering day - the Global Day of Giving\n\nFor more information, please see axaxl\n\ncom/sustainability",Industry Type: Insurance,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Performance tuning', 'Data modeling', 'Coding', 'Agile', 'Workflow', 'Application development', 'SDLC', 'SQL', 'Python', 'Firewall']",2025-06-12 14:46:19
Data Engineer,Trantor,5 - 10 years,Not Disclosed,[],"We are looking for a skilled and motivated Data Engineer with deep expertise in GCP,\nBigQuery, Apache Airflow to join our data platform team. The ideal candidate should have hands-on experience building scalable data pipelines, automating workflows, migrating large-scale datasets, and optimizing distributed systems. The candidate should have experience with building Web APIs using Python. This role will play a key part in designing and maintaining robust data engineering solutions across cloud and on-prem environments.\nKey Responsibilities\nBigQuery & Cloud Data Pipelines:\nDesign and implement scalable ETL pipelines for ingesting large-scale datasets.\nBuild solutions for efficient querying of tables in BigQuery.\nAutomated scheduled data ingestion using Google Cloud services and scheduled\nApache Airflow DAGs",,,,"['Airflow', 'Etl Pipelines', 'GCP', 'Bigquery', 'Python', 'SFTP', 'ETL', 'SQL']",2025-06-12 14:46:21
Data Engineer,Cloud Angles Digital Transformation,3 - 5 years,Not Disclosed,['Noida'],"Essential Functions/Responsibilities/Duties\n•       Work closely with Senior Business Intelligence engineer and BI architect to understand the schema objects and build BI reports and Dashboards\n•       Participation in sprint refinement, planning, and kick-off to understand the Agile process and Sprint priorities\n•       Develop necessary transformations and aggregate tables required for the reporting\\Dashboard needs\n•       Understand the Schema layer in MicroStrategy and business requirements\n•       Develop complex reports and Dashboards in MicroStrategy\n•       Investigate and troubleshoot issues with Dashboard and reports\n•       Proactively researching new technologies and proposing improvements to processes and tech stack\n•       Create test cases and scenarios to validate the dashboards and maintain data accuracy\nEducation and Experience\n•       3 years of experience in Business Intelligence and Data warehousing\n•       3+ years of experience in MicroStrategy Reports and Dashboard development\n•       2 years of experience in SQL\n•       Bachelors or masters degree in IT or Computer Science or ECE.\n•       Nice to have – Any MicroStrategy certifications\nRequired Knowledge, Skills, and Abilities\n•       Good in writing complex SQL, including aggregate functions, subqueries and complex date calculations and able to teach these concepts to others.\n•       Detail oriented and able to examine data and code for quality and accuracy.\n•       Self-Starter – taking initiative when inefficiencies or opportunities are seen.\n•       Good understanding of modern relational and non-relational models and differences between them\n•       Good understanding of Datawarehouse concepts, snowflake & star schema architecture and SCD concepts\n•       Good understanding of MicroStrategy Schema objects\n•       Develop Public objects such as metrics, filters, prompts, derived objects, custom groups and consolidations in MicroStrategy\n•       Develop complex reports and dashboards using OLAP and MTDI cubes\n•       Create complex dashboards with data blending\n•       Understand VLDB settings and report optimization\n•       Understand security filters and connection mappings in MSTR\nWork Environment\nAt Personify Health, we value and celebrate diversity and are committed to creating an inclusive environment for all employees. We believe in creating teams made up of individuals with various backgrounds, experiences, and perspectives. Diversity inspires innovation and collaboration and challenges us to produce better solutions. But more than this, diversity is our strength and a catalyst in our ability to change lives for the good. \nPhysical Requirements\n•       Constantly operates a computer and other office productivity machinery, such as copy machine, computer printer, calculator, etc.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Microstrategy', 'SQL', 'Dashboards']",2025-06-12 14:46:23
Azure Data Engineer,Jurist Associates,5 - 10 years,7-12 Lacs P.A.,"['Kochi', 'Hyderabad', 'Bengaluru']","Design, build, and maintain scalable and efficient data pipelines using Azure services such as Azure Data Factory (ADF), Azure Databricks, and Azure Synapse Analytics. Develop and optimize ETL/ELT workflows for ingestion, cleansing, transformation,\n\nRequired Candidate profile\nStrong understanding of data warehouse architecture, data lakes, and big data frameworks. Candidates who have atleast 5 years of experience should only apply.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Data Engineering', 'Azure Data Factory', 'GenAI Tools', 'Azure Data Warehouse', 'Azure data lake Gen2', 'Azure Synapse Analytics', 'Azure Databricks', 'Azure Data Lake', 'Nosql Databases', 'Data Modeling', 'Semantic Analytics', 'ML/DL Models']",2025-06-12 14:46:25
Data Engineer,Fortune India 500 Chemicals Firm,12 - 18 years,Not Disclosed,['Mumbai (All Areas)'],"Skills:\nData Management: Expertise in data warehousing, SQL/NoSQL, cloud platforms (AWS, Azure, GCP)\nETL Tools: Proficient in Informatica, Talend, Azure Data Factory\nModelling: Strong in dimensional modelling, star/snowflake schema\nGovernance & Compliance: Knowledge of GDPR, HIPAA, data governance frameworks\nLanguages: T-SQL, PL/SQL\nSoft Skills: Effective communicator, strong analytical and problem-solving skills\nKey Responsibilities:\nArchitecture: Designed scalable, high-performance data warehouse architectures and data models\nETL & Integration: Led ETL design/development for structured/unstructured data across platforms\nGovernance: Defined data quality standards and collaborated on data governance policy implementation\nCollaboration: Interfaced with BI, data science, and business teams to align data strategies\nPerformance & Security: Optimized queries/ETL jobs and ensured data security and compliance\nDocumentation: Maintained standards and documentation for architecture, ETL, and workflows",Industry Type: Chemicals,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Warehousing', 'GCP', 'Snowflake', 'Microsoft Azure', 'Dimensional Modeling', 'Data Modeling', 'ETL', 'AWS']",2025-06-12 14:46:27
DBA DATA Engineer || 12 Lakhs CTC,Robotics Technologies,6 - 10 years,12 Lacs P.A.,['Hyderabad( Banjara hills )'],"Dear Candidate,\nWe are seeking a skilled and experienced DBA Data Engineer to join our growing data team. The ideal candidate will play a key role in designing, implementing, and maintaining our databases and data pipeline architecture. You will collaborate with software engineers, data analysts, and DevOps teams to ensure efficient data flow, data integrity, and optimal database performance across all systems. This role requires a strong foundation in database administration, SQL performance tuning, data modeling, and experience with both on-prem and cloud-based environments.\n\nRequirements:\nBachelors degree in Computer Science, Information Systems, or a related field.\n6+ years of experience in database administration and data engineering.\nProven expertise in RDBMS (Oracle, MySQL, and PostgreSQL) and NoSQL systems (MongoDB, Cassandra).\nExperience managing databases in cloud environments (AWS, Azure, or GCP).\nProficiency in ETL processes and tools (e.g., Apache NiFi, Talend, Informatica, AWS Glue).\nStrong experience with scripting languages such as Python, Bash, or PowerShell.\n\nDBA Data Engineer Roles & Responsibilities:\nDesign and maintain scalable and high-performance database architectures.\nMonitor and optimize database performance using tools like CloudWatch, Oracle Enterprise Manager, pgAdmin, Mongo Compass, or Dynatrace.\nDevelop and manage ETL/ELT pipelines to support business intelligence and analytics.\nEnsure data integrity and security through best practices in backup, recovery, and encryption.\nAutomate regular database maintenance tasks using scripting and scheduled jobs.\nImplement high availability, failover, and disaster recovery strategies.\nConduct performance tuning of queries, stored procedures, indexes, and table structures.\nCollaborate with DevOps to automate database deployments using CI/CD and IaC tools (e.g., Terraform, AWS CloudFormation).\nDesign and implement data models, including star/snowflake schemas for data warehousing.\nDocument data flows, data dictionaries, and database configurations.\nManage user access and security policies using IAM roles or database-native permissions.\nAnalyze existing data systems and propose modernization or migration plans (on-prem to cloud, SQL to NoSQL, etc.).\nUse AWS RDS, Amazon Redshift, Azure SQL Database, or Google BigQuery as needed.\nStay up-to-date with emerging database technologies and make recommendations.\n\nMust-Have Skills:\nDeep knowledge of SQL and database performance tuning.\nHands-on experience with database migrations and replication strategies.\nFamiliarity with data governance, data quality, and compliance frameworks (GDPR, HIPAA, etc.).\nStrong problem-solving and troubleshooting skills.\nExperience with data streaming platforms such as Apache Kafka, AWS Kinesis, or Apache Flink is a plus.\nExperience with data lake and data warehouse architectures.\nExcellent communication and documentation skills.\n\nSoft Skills:\nProblem-Solving: Ability to analyze complex problems and develop effective solutions.\nCommunication Skills: Strong verbal and written communication skills to effectively collaborate with cross-functional teams.\nAnalytical Thinking: Ability to think critically and analytically to solve technical challenges.\nTime Management: Capable of managing multiple tasks and deadlines in a fast-paced environment.\nAdaptability: Ability to quickly learn and adapt to new technologies and methodologies.\n\nInterview Mode : F2F for who are residing in Hyderabad / Zoom for other states\nLocation : 43/A, MLA Colony,Road no 12, Banjara Hills, 500034\nTime : 2 - 4pm",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Database Administration', 'Database Migration', 'Dba Skills', 'High Availability', 'Db Administration', 'Hadr', 'Database Security', 'Disaster Recovery', 'Dr Testing', 'DR', 'Luw', 'Db Upgrade', 'Brtools', 'UDDI', 'Os Migration', 'Dbase', 'DBMS', 'IBM DB2', 'Db Migration', 'Disaster Recovery Planning', 'Db Dba', 'Backup And Recovery']",2025-06-12 14:46:29
Data Engineer,Amgen Inc,1 - 3 years,Not Disclosed,['Hyderabad'],"What you will do\nIn this vital role you will be responsible to design, develop, and optimize data pipelines, data integration frameworks, and metadata-driven architectures that enable seamless data access and analytics. This role prefers deep expertise in big data processing, distributed computing, data modeling, and governance frameworks to support self-service analytics, AI-driven insights, and enterprise-wide data management.\nDesign, develop, and maintain complex ETL/ELT data pipelines in Databricks using PySpark, Scala, and SQL to process large-scale datasets\nUnderstand the biotech/pharma or related domains & build highly efficient data pipelines to migrate and deploy complex data across systems\nDesign and Implement solutions to enable unified data access, governance, and interoperability across hybrid cloud environments\nIngest and transform structured and unstructured data from databases (PostgreSQL, MySQL, SQL Server, MongoDB etc.), APIs, logs, event streams, images, pdf, and third-party platforms\nEnsuring data integrity, accuracy, and consistency through rigorous quality checks and monitoring\nExpert in data quality, data validation and verification frameworks\nInnovate, explore and implement new tools and technologies to enhance efficient data processing\nProactively identify and implement opportunities to automate tasks and develop reusable frameworks\nWork in an Agile and Scaled Agile (SAFe) environment, collaborating with cross-functional teams, product owners, and Scrum Masters to deliver incremental value\nUse JIRA, Confluence, and Agile DevOps tools to manage sprints, backlogs, and user stories\nSupport continuous improvement, test automation, and DevOps practices in the data engineering lifecycle\nCollaborate and communicate effectively with the product teams, with cross-functional teams to understand business requirements and translate them into technical solutions\nWhat we expect of you\nWe are all different, yet we all use our unique contributions to serve patients. We are looking for highly motivated expert Data Engineer who can own the design & development of complex data pipelines, solutions and frameworks.\nBasic Qualifications:\nMasters degree and 1 to 3 years of Computer Science, IT or related field experience OR\nBachelors degree and 3 to 5 years of Computer Science, IT or related field experience OR\nDiploma and 7 to 9 years of Computer Science, IT or related field experience\nHands-on experience in data engineering technologies such as Databricks, PySpark, SparkSQL Apache Spark, AWS, Python, SQL, and Scaled Agile methodologies\nProficiency in workflow orchestration, performance tuning on big data processing\nStrong understanding of AWS services\nAbility to quickly learn, adapt and apply new technologies\nStrong problem-solving and analytical skills\nExcellent communication and teamwork skills\nExperience with Scaled Agile Framework (SAFe), Agile delivery practices, and DevOps practices\nPreferred Qualifications:\nAWS Certified Data Engineer preferred\nDatabricks Certificate preferred\nScaled Agile SAFe certification preferred\nData Engineering experience in Biotechnology or pharma industry\nExperience in writing APIs to make the data available to the consumers\nExperienced with SQL/NOSQL database, vector database for large language models\nExperienced with data modeling and performance tuning for both OLAP and OLTP databases\nExperienced with software engineering best-practices, including but not limited to version control (Git, Subversion, etc.), CI/CD (Jenkins, Maven etc.), automated unit testing, and DevOps\nSoft Skills:\nExcellent analytical and troubleshooting skills.\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nTeam-oriented, with a focus on achieving team goals.\nAbility to learn quickly, be organized and detail oriented.\nStrong presentation and public speaking skills.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data engineering', 'Maven', 'data validation', 'PySpark', 'Scala', 'APIs', 'SQL Server', 'SQL', 'Jenkins', 'Git', 'MySQL', 'troubleshooting', 'MongoDB', 'ETL']",2025-06-12 14:46:32
Data Engineer,Centrilogic,15 - 20 years,Not Disclosed,['Hyderabad'],"Data Engineer\n\nPurpose:\n\nOver 15 years, we have become a premier global provider of multi-cloud management, cloud-native application development solutions, and strategic end-to-end digital transformation services.\nHeadquartered in Canada and with regional headquarters in the U.S. and the United Kingdom, Centrilogic delivers smart, streamlined solutions to clients worldwide.\n\nWe are looking for a passionate and experienced Data Engineer to work with our other 70 Software, Data and DevOps engineers to guide and assist our clients data modernization journey.\n\nOur team works with companies with ambitious missions - clients who are creating new, innovative products, often in uncharted markets. We work as embedded members and leaders of our clients development and data teams. We bring experienced senior engineers, leading-edge technologies and mindsets, and creative thinking. We show our clients how to move to the modern frameworks of data infrastructures and processing, and we help them reach their full potential with the power of data.\n\nIn this role, youll be the day-to-day primary point of contact with our clients to modernize their data infrastructures, architecture, and pipelines.\n\nPrincipal Responsibilities:\n\nConsulting clients on cloud-first strategies for core bet-the-company data initiatives\nProviding thought leadership on both process and technical matters\nBecoming a real champion and trusted advisor to our clients on all facets of Data Engineering\nDesigning, developing, deploying, and supporting the modernization and transformation of our client s end-to-end data strategy, including infrastructure, collection, transmission, processing, and analytics\nMentoring and educating clients teams to keep them up to speed with the latest approaches, tools and skills, and setting them up for continued success post-delivery\n\nRequired Experience and Skills:\n\nMust have either Microsoft Certified Azure Data Engineer Associate or Fabric Data Engineer Associate certification.\nMust have experience working in a consulting or contracting capacity on large data management and modernization programs.\nExperience with SQL Servers, data engineering, on platforms such as Azure Data Factory, Databricks, Data Lake, and Synapse.\nStrong knowledge and demonstrated experience with Delta Lake and Lakehouse Architecture.\nStrong knowledge of securing Azure environment, such as RBAC, Key Vault, and Azure Security Center.\nStrong knowledge of Kafka and Spark and extensive experience using them in a production environment.\nStrong and demonstrable experience as DBA in large-scale MS SQL environments deployed in Azure.\nStrong problem-solving skills, with the ability to get to the route of an issue quickly.\nStrong knowledge of Scala or Python.\nStrong knowledge of Linux administration and networking.\nScripting skills and Infrastructure as Code (IaC) experience using PowerShell, Bash, and ARM templates.\nUnderstanding of security and corporate governance issues related with cloud-first data architecture, as well as accepted industry solutions.\nExperience in enabling continuous delivery for development teams using scripted cloud provisioning and automated tooling.\nExperience working with Agile development methodology that is fit for purpose.\nSound business judgment and demonstrated leadership",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['MS SQL', 'Networking', 'Data management', 'Powershell', 'Consulting', 'Application development', 'microsoft', 'Analytics', 'Python', 'Data architecture']",2025-06-12 14:46:34
Data Engineer,Luxoft,5 - 10 years,Not Disclosed,['Pune'],"Are you passionate about data and analytics? Are you keen to be part of the journey to modernize a data warehouse/ analytics suite of application(s). Do you take pride in the quality of software delivered for each development iteration?\nWere looking for someone like that to join us and\nbe a part of a high-performing team on a high-profile project.\nsolve challenging problems in an elegant way\nmaster state-of-the-art technologies\nbuild a highly responsive and fast updating application in an Agile & Lean environment\napply best development practices and effectively utilize technologies\nwork across the full delivery cycle to ensure high-quality delivery\nwrite high-quality code and adhere to coding standards\nwork collaboratively with diverse team(s) of technologists\nYou are:\nCurious and collaborative, comfortable working independently, as well as in a team\nFocused on delivery to the business\nStrong in analytical skills. For example, the candidate must understand the key dependencies among existing systems in terms of the flow of data among them. It is essential that the candidate learns to understand the big picture of how IB industry/business functions.\nAble to quickly absorb new terminology and business requirements\nAlready strong in analytical tools, technologies, platforms, etc. The candidate must also demonstrate a strong desire for learning and self-improvement.\nOpen to learning home-grown technologies, support current state infrastructure and help drive future state migrations. imaginative and creative with newer technologies\nAble to accurately and pragmatically estimate the development effort required for specific objectives\nYou will have the opportunity to work under minimal supervision to understand local and global system requirements, design and implement the required functionality/bug fixes/enhancements. You will be responsible for components that are developed across the whole team and deployed globally.\nYou will also have the opportunity to provide third-line support to the applications global user community, which will include assisting dedicated support staff and liaising with the members of other development teams directly, some of which will be local and some remote.\nSkills\nMust have\nA bachelors or masters degree, preferably in Information Technology or a related field (computer science, mathematics, etc.), focusing on data engineering.\n5+ years of relevant experience as a data engineer in Big Data is required.\nStrong Knowledge of programming languages (Python / Scala) and Big Data technologies (Spark, Databricks or equivalent) is required.\nStrong experience in executing complex data analysis and running complex SQL/Spark queries.\nStrong experience in building complex data transformations in SQL/Spark.\nStrong knowledge of Database technologies is required.\nStrong knowledge of Azure Cloud is advantageous.\nGood understanding and experience with Agile methodologies and delivery.\nStrong communication skills with the ability to build partnerships with stakeholders.\nStrong analytical, data management and problem-solving skills.\nNice to have\nExperience working on the QlikView tool\nUnderstanding of QlikView scripting and data model\nOther\nLanguages\nEnglish: C1 Advanced\nSeniority\nSenior\nRefer a Friend\nPositive work environments and stellar reputations attract and retain top talent. Find out why Luxoft stands apart from the rest.\nRecommend a friend\nRelated jobs View all vacancies\nBig Data Engineer (Scala/Java/Python)\nBigData Development\nUnited States of America\nStamford, US\nBig Data Engineer (Scala/Java/Python)\nBigData Development\nUnited States of America\nWeehawken\nData Engineer - PostgreSQL\nBigData Development\nPoland\nRemote Poland\nPune, India\nReq. VR-114879\nBigData Development\nBCM Industry\n05/06/2025\nReq. VR-114879\nApply for Data Engineer in Pune\n*",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Data analysis', 'Data management', 'Coding', 'Postgresql', 'Agile', 'Information technology', 'Analytics', 'SQL', 'Python']",2025-06-12 14:46:36
Data Engineer,zocket,4 - 5 years,Not Disclosed,['Chennai'],"About Zocket: AI Assistant for Marketers\nZocket empowers businesses to scale social media advertising seamlessly with advanced AI . From search to conversions, Zocket automates the entire workflow effortlessly.\n\nHeadquartered in Chennai and San Francisco , with 500+ customers across 25+ countries.\n\nOur GenAI Product Suite:\nCreative Studio: Make ad creatives 10x faster, 20x better\n\nAudience Assistant: Precision targeting, platform-wide\n\nSnoop AI: Competitive & industry insights in seconds\n\nInsights AI: 360 performance dashboard across channels\n\nGrowth Infra: Whitelisted infra for unbounded scale\n\nTrusted and Recognized:\nAWS GenAI Accelerator 2024 (Top 80 global startups)\n\nGoogle for Startups (GFSA Class VIII Ai-first)\n\nNASSCOM Gen AI Foundry\n\n4.9 Trustpilot | #1 Product of the Day & Week on Product Hunt\n\n\nKey Responsibilities\n\nDesign, build, and maintain robust ETL pipelines to process large-scale structured and unstructured data.\n\nWork with GraphQL databases and Vector DBs (e.g., Pinecone, Chroma) to power intelligent, AI-driven features.\n\nBuild and optimize scalable ML pipelines and contribute to model lifecycle management .\n\nImplement real-time and batch data streaming solutions using Kafka and Spark .\n\nWork with Airflow to schedule and orchestrate data workflows.\n\nDevelop efficient data models in PostgreSQL and ensure high-performance data access.\n\nIntegrate and optimize Elasticsearch for fast search and analytics.\n\nEnsure high data quality and reliability through monitoring and automation.\n\nCollaborate with Product, Data Science, and Engineering teams to deliver scalable data solutions.\n\nOwn the data infrastructure on AWS - from data lake to warehouse and analytics stack.\n\nDrive internal data tooling improvements and support analytics for business decision-making.\n\n\nKey Requirements\n\n4-5 years of experience as a Data Engineer or in a similar backend/data-intensive role.\n\nStrong programming experience with Python and familiarity with ML model development workflows .\n\nHands-on experience with GraphQL and Vector DBs (Pinecone, Chroma).\n\nProficiency in PostgreSQL and working knowledge of NoSQL and search systems like Elasticsearch .\n\nSolid understanding of data engineering on AWS (EC2, S3, RDS, Redshift, etc.).\n\nExperience with Apache Kafka , Apache Spark , and Airflow is essential.\n\nFamiliarity with modern ML Ops and data science integration is a plus.\n\nStrong analytical thinking and problem-solving skills.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Automation', 'Backend', 'data science', 'Analytical', 'Social media', 'Workflow', 'Data quality', 'AWS', 'Analytics', 'Monitoring']",2025-06-12 14:46:39
AI Python Data Science Engineer,Probeseven,4 - 5 years,Not Disclosed,['Coimbatore'],"AI Python Data Science Engineer\nHot Openings\nAs a data science and analytics engineer, you will be involved in developing computer visions and data algorithms. Artificial intelligence development and deep machine learning implementations will be part of your development and deployment to the cloud.\n\nExperience for senior positions: 4 - 5+ years\nExperience for junior positions: 2 - 3 years\n\nRequired Tech Skills\nExperience in Python (must have) and/or R language.\nExperience with computer vision algorithms and data science.\nExperience in deep machine learning models.\nWell-versed in data visualization techniques.\nTroubleshoot and resolve code issues.\nCollaborate with data engineers to design and integrate the data sources.\nExperience in handling multiple priorities with Agile development.\nExperience with Git and working in a collaborative and distributive team environment.\nRequired Soft Skills\nExcellent listening, verbal, and written communication skills.\nStrong interpersonal & customer relationship skills.\nStrong analytical, problem solving, and decision-making skills.\nDocumentation skills.\nApply now\nHot Openings\nPHP + Node.js Developers\nFull Time\nTech Development\nExperience 4 - 6+ years",Industry Type: Film / Music / Entertainment,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer vision', 'GIT', 'data science', 'Analytical', 'Artificial Intelligence', 'Machine learning', 'PHP', 'Customer relationship', 'Analytics', 'Python']",2025-06-12 14:46:41
Data Engineer 4,Comcast,5 - 11 years,Not Disclosed,['Chennai'],".\nResponsible for designing, building and overseeing the deployment and operation of technology architecture, solutions and software to capture, manage, store and utilize structured and unstructured data from internal and external sources. Establishes and builds processes and structures based on business and technical requirements to channel data from multiple inputs, route appropriately and store using any combination of distributed (cloud) structures, local databases, and other applicable storage forms as required. Develops technical tools and programming that leverage artificial intelligence, machine learning and big-data techniques to cleanse, organize and transform data and to maintain, defend and update data structures and integrity on an automated basis. Creates and establishes design standards and assurance processes for software, systems and applications development to ensure compatibility and operability of data connections, flows and storage requirements. Reviews internal and external business and product requirements for data operations and activity and suggests changes and upgrades to systems and storage to accommodate ongoing needs. Work with data modelers/analysts to understand the business problems they are trying to solve then create or augment data assets to feed their analysis. Integrates knowledge of business and functional priorities. Acts as a key contributor in a complex and crucial environment. May lead teams or projects and shares expertise.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBachelors Degree\nWhile possessing the stated degree is preferred, Comcast also may consider applicants who hold some combination of coursework and experience, or who have extensive related professional experience.\n7-10 Years\nComcast is proud to be an equal opportunity workplace. We will consider all qualified applicants for employment without regard to race, color, religion, age, sex, sexual orientation, gender identity, national origin, disability, veteran status, genetic information, or any other basis protected by applicable law.",,,,"['Engineering services', 'Assurance', 'Process optimization', 'MySQL', 'Machine learning', 'Data structures', 'Data quality', 'Troubleshooting', 'Downstream', 'Python']",2025-06-12 14:46:43
Data Engineer,IT Services Company,2 - 3 years,6-7 Lacs P.A.,['Pune'],"Data Engineer\nJob Description :\nJash Data Sciences: Letting Data Speak!\nDo you love solving real-world data problems with the latest and best techniques? And having fun while solving them in a team! Then come and join our high-energy team of passionate data people. Jash Data Sciences is the right place for you.\nWe are a cutting-edge Data Sciences and Data Engineering startup based in Pune, India. We believe in continuous learning and evolving together. And we let the data speak!\nWhat will you be doing?\nYou will be discovering trends in the data sets and developing algorithms to transform\nraw data for further analytics\nCreate Data Pipelines to bring in data from various sources, with different formats,\ntransform it, and finally load it to the target database.\nImplement ETL/ ELT processes in the cloud using tools like AirFlow, Glue, Stitch, Cloud\nData Fusion, and DataFlow.\nDesign and implement Data Lake, Data Warehouse, and Data Marts in AWS, GCP, or\nAzure using Redshift, BigQuery, PostgreSQL, etc.\nCreating efficient SQL queries and understanding query execution plans for tuning\nqueries on engines like PostgreSQL.\nPerformance tuning of OLAP/ OLTP databases by creating indices, tables, and views.\nWrite Python scripts for the orchestration of data pipelines\nHave thoughtful discussions with customers to understand their data engineering\nrequirements. Break complex requirements into smaller tasks for execution.\nWhat do we need from you?\nStrong Python coding skills with basic knowledge of algorithms/data structures and\ntheir application.\nStrong understanding of Data Engineering concepts including ETL, ELT, Data Lake, Data\nWarehousing, and Data Pipelines.\nExperience designing and implementing Data Lakes, Data Warehouses, and Data Marts\nthat support terabytes of scale data.\nA track record of implementing Data Pipelines on public cloud environments\n(AWS/GCP/Azure) is highly desirable\nA clear understanding of Database concepts like indexing, query performance\noptimization, views, and various types of schemas.\nHands-on SQL programming experience with knowledge of windowing functions,\nsubqueries, and various types of joins.\nExperience working with Big Data technologies like PySpark/ Hadoop\nA good team player with the ability to communicate with clarity\nShow us your git repo/ blog!\nQualification\n1-2 years of experience working on Data Engineering projects for Data Engineer I\n2-5 years of experience working on Data Engineering projects for Data Engineer II\n1-5 years of Hands-on Python programming experience\nBachelors/Masters' degree in Computer Science is good to have\nCourses or Certifications in the area of Data Engineering will be given a higher preference.\nCandidates who have demonstrated a drive for learning and keeping up to date with technology by continuing to do various courses/self-learning will be given high preference.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Airflow', 'Elt', 'Data Mart', 'Data Pipeline', 'ETL', 'Pyspark', 'Hadoop', 'Data Bricks', 'SQL', 'Data Fusion', 'Glue', 'GCP', 'Data Flow', 'Data Warehousing', 'Azzure', 'AWS']",2025-06-12 14:46:45
Data Engineer- MS Fabric,InfoCepts,5 - 9 years,Not Disclosed,['India'],"Position: Data Engineer – MS Fabric\n  Purpose of the Position: As an MS Fabric Data engineer you will be responsible for designing, implementing, and managing scalable data pipelines. Strong experience in implementation and management of lake House using MS Fabric Azure Tech stack (ADLS Gen2, ADF, Azure SQL) .\nProficiency in data integration techniques, ETL processes and data pipeline architectures. Well versed in Data Quality rules, principles and implementation.\n",,,,"['components', 'data', 'scala', 'delta', 'pyspark', 'data warehousing', 'rules', 'azure data factory', 'sql', 'parquet', 'analytics', 'sql azure', 'spark', 'oracle adf', 'data pipeline architecture', 'etl', 'python', 'azure synapse', 'microsoft azure', 'power bi', 'data bricks', 'data quality', 'system', 't', 'fabric', 'data integration', 'etl process']",2025-06-12 14:46:48
Data Engineer,Society Managers,3 - 5 years,Not Disclosed,['Mumbai (All Areas)'],"We are seeking a skilled and driven SDE-II (Data Engineering) to join our dynamic team. In this role, you will design, develop, and maintain scalable data pipelines, working with large, complex datasets. Youll collaborate closely with cross-functional teams to gather data requirements and contribute to the architecture of our data systems, leveraging your expertise in tools like Databricks, Spark, and SQL.\n\nRoles and responsibilities\nData Pipeline Development: Design,build, and maintain scalable data pipelines using Databricks, Python,and Spark.\nData Processing & Transformation: Handle large, complex datasets to ensure efficient data processing and transformations.\nCollaboration: Work with cross-functional teams to gather, understand, and implement data requirements.\nSQL & ETL: Write and optimize SQL queries for data extraction, transformation, and loading (ETL) processes.\nData Quality & Security: Ensure data accuracy, integrity, and security across all stages of the data lifecycle.\nSystem Design & Architecture: Contribute to the design and architecture of scalable data systems and solutions.\nRequired Skills and Qualification\nExperience: 3+ years of experience in data engineering or a related field.\nDatabricks & Spark: Strong expertise in Databricks and distributed data processing with Spark.\nProgramming: Proficiency in Python for data engineering tasks.\nSQL Optimization: Solid experience in writing and optimizing complex SQL queries.\nData Systems Knowledge: Hands-on experience with large-scale data systems and tools.\nDomain Knowledge: Familiarity with Capital Market/Private Equity is a plus (relaxation may apply).\nData Visualization: Experience with Tableau for creating insightful data visualizations and reports.\nPreferred skills\nCloud Platforms: Familiarity with cloud services like AWS, Azure, or GCP.\nData Warehousing & ETL:Experience with data warehousing concepts and ETL processes.\nAnalytical Skills: Strong problem-solving and analytical capabilities Analytics Tools: Hands-on experience with tools like Amplitude, PostHog, Google Analytics, or Mixpanel.\nAdditional Tools: Knowledge of Python for web scraping and frameworks like Django (good to have).",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'Microsoft Azure', 'Data Bricks', 'Spark', 'ETL', 'Python', 'SQL']",2025-06-12 14:46:50
"Data Engineer Openings at Advantum Health, Hyderabad",Advantum Health,3 - 5 years,Not Disclosed,['Hyderabad'],"Data Engineer openings at Advantum Health Pvt Ltd, Hyderabad.\nOverview:\nWe are looking for a Data Engineer to build and optimize robust data pipelines that support AI and RCM analytics. This role involves integrating structured and unstructured data from diverse healthcare systems into scalable, AI-ready datasets.\nKey Responsibilities:\nDesign, implement, and optimize data pipelines for ingesting and transforming healthcare and RCM data.\nBuild data marts and warehouses to support analytics and machine learning.\nEnsure data quality, lineage, and governance across AI use cases.\nIntegrate data from EMRs, billing platforms, claims databases, and third-party APIs.\nSupport data infrastructure in a HIPAA-compliant cloud environment.\nQualifications:\nBachelors in Computer Science, Data Engineering, or related field.\n3+ years of experience with ETL/ELT pipelines using tools like Apache Airflow, dbt, or Azure Data Factory.\nStrong SQL and Python skills.\nExperience with healthcare data standards (HL7, FHIR, X12) preferred.\nFamiliarity with data lake house architectures and AI integration best practices\nPh: 9177078628\nEmail id: jobs@advantumhealth.com\nAddress: Advantum Health Private Limited, Cyber gateway, Block C, 4th floor Hitech City, Hyderabad.\nDo follow us on LinkedIn, Facebook, Instagram, YouTube and Threads\nAdvantum Health LinkedIn Page:\nhttps://lnkd.in/gVcQAXK3\n\nAdvantum Health Facebook Page:\nhttps://lnkd.in/g7ARQ378\n\nAdvantum Health Instagram Page:\nhttps://lnkd.in/gtQnB_Gc\n\nAdvantum Health India YouTube link:\nhttps://lnkd.in/g_AxPaPp\n\nAdvantum Health Threads link:\nhttps://lnkd.in/gyq73iQ6",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Data Factory', 'SQL', 'Python', 'Airflow', 'ETL', 'Elt']",2025-06-12 14:46:52
DBA DATA Engineer || 12 Lakhs CTC,Robotics Technologies,8 - 9 years,12 Lacs P.A.,['Hyderabad( Banjara hills )'],"Dear Candidate,\nWe are seeking a skilled and experienced DBA Data Engineer to join our growing data team. The ideal candidate will play a key role in designing, implementing, and maintaining our databases and data pipeline architecture. You will collaborate with software engineers, data analysts, and DevOps teams to ensure efficient data flow, data integrity, and optimal database performance across all systems. This role requires a strong foundation in database administration, SQL performance tuning, data modeling, and experience with both on-prem and cloud-based environments.\n\nRequirements:\nBachelors degree in Computer Science, Information Systems, or a related field.\n8+ years of experience in database administration and data engineering.\nProven expertise in RDBMS (Oracle, MySQL, and PostgreSQL) and NoSQL systems (MongoDB, Cassandra).\nExperience managing databases in cloud environments (AWS, Azure, or GCP).\nProficiency in ETL processes and tools (e.g., Apache NiFi, Talend, Informatica, AWS Glue).\nStrong experience with scripting languages such as Python, Bash, or PowerShell.\n\nDBA Data Engineer Roles & Responsibilities:\nDesign and maintain scalable and high-performance database architectures.\nMonitor and optimize database performance using tools like CloudWatch, Oracle Enterprise Manager, pgAdmin, Mongo Compass, or Dynatrace.\nDevelop and manage ETL/ELT pipelines to support business intelligence and analytics.\nEnsure data integrity and security through best practices in backup, recovery, and encryption.\nAutomate regular database maintenance tasks using scripting and scheduled jobs.\nImplement high availability, failover, and disaster recovery strategies.\nConduct performance tuning of queries, stored procedures, indexes, and table structures.\nCollaborate with DevOps to automate database deployments using CI/CD and IaC tools (e.g., Terraform, AWS CloudFormation).\nDesign and implement data models, including star/snowflake schemas for data warehousing.\nDocument data flows, data dictionaries, and database configurations.\nManage user access and security policies using IAM roles or database-native permissions.\nAnalyze existing data systems and propose modernization or migration plans (on-prem to cloud, SQL to NoSQL, etc.).\nUse AWS RDS, Amazon Redshift, Azure SQL Database, or Google BigQuery as needed.\nStay up-to-date with emerging database technologies and make recommendations.\n\nMust-Have Skills:\nDeep knowledge of SQL and database performance tuning.\nHands-on experience with database migrations and replication strategies.\nFamiliarity with data governance, data quality, and compliance frameworks (GDPR, HIPAA, etc.).\nStrong problem-solving and troubleshooting skills.\nExperience with data streaming platforms such as Apache Kafka, AWS Kinesis, or Apache Flink is a plus.\nExperience with data lake and data warehouse architectures.\nExcellent communication and documentation skills.\n\nSoft Skills:\nProblem-Solving: Ability to analyze complex problems and develop effective solutions.\nCommunication Skills: Strong verbal and written communication skills to effectively collaborate with cross-functional teams.\nAnalytical Thinking: Ability to think critically and analytically to solve technical challenges.\nTime Management: Capable of managing multiple tasks and deadlines in a fast-paced environment.\nAdaptability: Ability to quickly learn and adapt to new technologies and methodologies.\n\nInterview Mode : F2F for who are residing in Hyderabad / Zoom for other states\nLocation : 43/A, MLA Colony,Road no 12, Banjara Hills, 500034\nTime : 2 - 4pm",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Database Administration', 'Database Migration', 'Dba Skills', 'High Availability', 'Db Administration', 'Hadr', 'Database Security', 'Disaster Recovery', 'Dr Testing', 'DR', 'Luw', 'Db Upgrade', 'Brtools', 'UDDI', 'Os Migration', 'Dbase', 'DBMS', 'IBM DB2', 'Db Migration', 'Disaster Recovery Planning', 'Db Dba', 'Backup And Recovery']",2025-06-12 14:46:55
Data Engineer,Reputed Client,5 - 10 years,18-25 Lacs P.A.,[],"Data Engineer\n(Python, PySpark, SQL and Spark SQL)\n\nExperience - 5-10 Years\nMandate Skills: Python, PySpark, SQL and SparkSQL\nWorking Hours: 11:00 am to 8 pm\n\n(Candidate has to be flexible. 4-hour overlap with US business hours)\n\nSalary : 1.50 LPM to 2 LPM + Tax (Rate is not fixed, negotiable depending upon the candidate feedback)\n\nRemote / Hybrid (3 Days in a week WFO) (Pune, Bangalore, Noida, Mumbai, Hyderabad)\n\nNOTE: Need candidates within these cities, they have to collect assets from the office / need to be available for meetings - if they are working remotely)\n\nIt's a 6 months (C2H role).",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Temporary/Contractual","['Python', 'PySpark', 'Spark', 'SQL']",2025-06-12 14:46:57
Data Engineer,Conversehr Business Solutions,4 - 7 years,15-30 Lacs P.A.,['Hyderabad'],"What are the ongoing responsibilities of Data Engineer responsible for?\nWe are building a growing Data and AI team. You will play a critical role in the efforts to centralize structured and unstructured data for the firm. We seek a candidate with skills in data modeling, data management and data governance, and can contribute first-hand towards firms data strategy. The ideal candidate is a self-starter with a strong technical foundation, a collaborative mindset, and the ability to navigate complex data challenges #ASSOCIATE\nWhat ideal qualifications, skills & experience would help someone to be successful?\nBachelors degree in computer science or computer applications; or equivalent experience in lieu of degree with 3 years of industry experience.\nStrong expertise in data modeling and data management concepts. Experience in implementing master data management is preferred.\nSound knowledge on Snowflake and data warehousing techniques.\nExperience in building, optimizing, and maintaining data pipelines and data management frameworks to support business needs.\nProficiency in at least one programming language, preferably python.\nCollaborate with cross-functional teams to translate business needs into scalable data and AI-driven solutions.\nTake ownership of projects from ideation to production, operating in a startup-like culture within an enterprise environment. Excellent communication, collaboration, and ownership mindset.\nFoundational Knowledge of API development and integration.\nKnowledge of Tableau, Alteryx is good-to-have.\nWork Shift Timings - 2:00 PM - 11:00 PM IST",Industry Type: Financial Services (Asset Management),Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Snowflake', 'Master Data Management', 'Python', 'Etl Pipelines', 'Alteryx', 'ai', 'Data Modeling', 'Tableau', 'ETL']",2025-06-12 14:46:59
"Associate Engineer, Digital Data Development",XL India Business Services Pvt. Ltd,1 - 4 years,Not Disclosed,['Gurugram'],"Engineer, Digital Data Development Gurgaon/ Bangalore, India AXA XL offers risk transfer and risk management solutions to clients globally\n\nWe offer worldwide capacity, flexible underwriting solutions, a wide variety of client-focused loss prevention services, and a team-based account management approach\n\nAXA XL recognizes data and information as critical business assets, both in terms of managing risk and enabling new business opportunities\n\nThis data should not only be high quality, but also actionable - enabling AXA XL s executive leadership team to maximize benefits and facilitate sustained dynamic advantage\n\nOur Innovation, Data, and Analytics (IDA) organization is focused on driving innovation by optimizing how we leverage data to drive strategy and create a new business model - disrupting the insurance market\n\nThis role is part of the Digital Data Dev Division within the Digital Transformation vertical of IDA\n\nIt will be responsible for different aspects of Data Product development lifecycle activities, including but not limited to Data Production Support, business stakeholders engagement for usage & problem resolutions, Product migrations, and platform/data product rollouts, performance stability & reliability\n\nWhat you ll be DOING What will your essential responsibilities include? Hands-on experience with CI/CD tools: Harness, Azure DevOps\n\nImplement and manage DevSecOps tools and CI/CD pipelines with security controls\n\nAutomate security scanning and compliance checks (SAST, DAST, container scanning, etc)\n\nCollaborate with development, operations, and security teams to embed security best practices\n\nConduct threat modeling, vulnerability assessments, and risk\n\nBuild, Release Management & DevSecOps support for various data solutions owned and managed by IDA organization\n\nExperience with cloud platforms like Azure is preferred\n\nProficiency in scripting languages: Python, Bash, PowerShell\n\nFamiliarity with containerization and orchestration: Docker, Kubernetes, OpenShift\n\nExperience using Tools like Git, JIRA, Confluence etc Knowledge of Artifactory like JFrog / X-Ray\n\nExperience of working with Agile methodologies\n\nGood knowledge of OOP concepts & Microservice-based architecture\n\nAnalyze and mitigate risks (technical or otherwise) about Data Solution build & release delivery timelines\n\nProvide top-class DevSecOps functionalities and support\n\nPartner with the Product & Production Support team(s) as a Data/DevSecOps/Technical SME for migration of re-architected Product/Product functionalities to the new Cloud Platform\n\nDemonstrate proactive communication with Business users, Development, Technology, Production Support, and Delivery Teams, and Senior Management\n\nProvide day-to-day management of the DevSecOps services and ensure smooth operation of the Release pipelines to various Environments\n\nWork in the Follow the Sun support model providing cross-team support coverage across Digital Data Dev division responsibilities\n\nBuild/Setup/Maintain various critical monitoring processes, alerts, and overall health reports (performance and functional) of production, and pre-production environments to be used by the Production Support Teams\n\nWork with Product Teams to build deployment pipelines for various Data Science Products used within IDA/Pricing & Analytics Teams\n\nOversee the development and maintenance of Build & Release Management processes and their documentation\n\nEnsure that all policies, standards, and best practices are followed and kept up to date\n\nTimely and accurate completion of emergency Release pipelines/processes in a manner that is auditable, testable, and maintainable\n\nEnsure any builds are consistent with Solution design, Security recommendations and business specifications\n\nAchieve & maintain the highest business customer confidence and net promoter score (NPS)\n\nGood grasp of Azure fundamentals (Microsoft AZ-900)\n\nRobust understanding of Designing and Implementing DevOps/DevSecOps Solutions (Microsoft AZ-400)\n\nKnowledge of Python or R Programming Language is a plus\n\nYou will report to Senior Delivery Lead\n\nWhat you will BRING We re looking for someone who has these abilities and skills: Required Skills and Abilities: Excellent understanding of DevOps principles with integrated security practices\n\nA minimum of an Undergraduate University Degree in Computer Science or related fields\n\nExtensive experience in data-focused roles (analytics, specialist, or engineer) and one or more areas of Build, Release & Data Management\n\nDistinctive problem-solving and analytical skills combined with robust business acumen\n\nExperience/knowledge of Microservices, Dot Net, R Programming Language, Python, Azure, and Kibana\n\nExperience with SQL, HIVE, ADLS, and Document Databases like Cosmos, SQL Databases & SQL DW Analytics\n\nExperience/Understanding of systems integration, and developer support tools Azure DevOps/DevSecOps, CI/CD pipelines, Release Management, Configuration Management, and Automation\n\nData Engineering background or working experience with ETL and big data platforms (HDInsight / ADLS / Data Bricks) a plus\n\nDesired Skills and Abilities: Demonstrates a level of experience/ability to influence and understand business problems in technical terminology and able to liaise with staff at all levels in the organization\n\nExcellent writing skills, with the ability to create clear requirements, specifications, and documentation for data systems\n\nExperience with multiple software delivery models (Waterfall, Agile, etc) is a plus\n\nPrevious experience leading small teams with a mix of onsite/offshore developers",Industry Type: Insurance,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Automation', 'Production support', 'Configuration management', 'Agile', 'microsoft', 'Risk management', 'Release management', 'Analytics', 'SQL', 'Python']",2025-06-12 14:47:01
Data Engineer - ETL/ Python,Meritus Management Service,5 - 7 years,10-14 Lacs P.A.,['Indore'],"Focus on Python, you'll play a crucial role in designing, developing, and maintaining data pipelines and ETL processes. Python to manage large datasets, automate data workflows, and ensure data accuracy and efficiency across our organization.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pandas', 'MySQL', 'Sqlalchemy', 'Numpy', 'Python', 'Azure Synapse', 'Postgresql', 'Etl Process', 'SQL']",2025-06-12 14:47:03
"Data Engineer, AVP",NatWest Markets,16 - 18 years,Not Disclosed,['Gurugram'],"Join us as a Data Engineer\nWe re looking for someone to build effortless, digital first customer experiences to help simplify our organisation and keep our data safe and secure\nDay-to-day, you ll develop innovative, data-driven solutions through data pipelines, modelling and ETL design while inspiring to be commercially successful through insights\nIf you re ready for a new challenge, and want to bring a competitive edge to your career profile by delivering streaming data ingestions, this could be the role for you\nWere offering this role at assistant vice president level\nWhat you ll do\nYour daily responsibilities will include you developing a comprehensive knowledge of our data structures and metrics, advocating for change when needed for product development. You ll also provide transformation solutions and carry out complex data extractions.\nWe ll expect you to develop a clear understanding of data platform cost levels to build cost-effective and strategic solutions. You ll also source new data by using the most appropriate tooling before integrating it into the overall solution to deliver it to our customers.\nYou ll also be responsible for:\nDriving customer value by understanding complex business problems and requirements to correctly apply the most appropriate and reusable tools to build data solutions\nParticipating in the data engineering community to deliver opportunities to support our strategic direction\nCarrying out complex data engineering tasks to build a scalable data architecture and the transformation of data to make it usable to analysts and data scientists\nBuilding advanced automation of data engineering pipelines through the removal of manual stages\nLeading on the planning and design of complex products and providing guidance to colleagues and the wider team when required\nThe skills you ll need\nTo be successful in this role, you ll have an understanding of data usage and dependencies with wider teams and the end customer. You ll also have experience of extracting value and features from large scale data.\nWe ll expect you to have experience of ETL technical design, data quality testing, cleansing and monitoring, data sourcing, exploration and analysis, and data warehousing and data modelling capabilities.\nYou ll also need:\nExperience of using programming languages alongside knowledge of data and software engineering fundamentals\nGood knowledge of modern code development practices\nGreat communication skills with the ability to proactively engage with a range of stakeholders\nHours\n45\nJob Posting Closing Date:\n16/06/2025",Industry Type: Banking,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Automation', 'Usage', 'Technical design', 'Programming', 'Data structures', 'Data quality', 'Assistant Vice President', 'Data warehousing', 'Monitoring', 'Data architecture']",2025-06-12 14:47:05
Aws Data Engineer,Astrosoft Technologies,2 - 4 years,6.5-10 Lacs P.A.,['Hyderabad( Gachibowli )'],"Company: AstroSoft Technologies (https://www.astrosofttech.com/)\nAstrosoft is an award-winning company that specializes in the areas of Data, Analytics, Cloud, AI/ML, Innovation, Digital. We have a customer first mindset and take extreme ownership in delivering solutions and projects for our customers and have consistently been recognized by our clients as the premium partner to work with. We bring to bear top tier talent, a robust and structured project execution framework, our significant experience over the years and have an impeccable record in delivering solutions and projects for our clients.\nFounded in 2004, Headquarters in FL,USA, Corporate Office - India, Hyderabad\nBenefits from Astrosoft Technologies\nH1B Sponsorship (Depends on Project & Performance)\nLunch & Dinner (Every day)\nHealth Insurance Coverage- Group\nIndustry Standards Leave Policy\nSkill Enhancement Certification\nHybrid Mode\nRole & responsibilities\nJob Title: AWS Engineer\nRequired Skills:\nMinimum of 2+ years direct experience with AWS Data Engineer\nStrong Experience in AWS Services like Redshift & ETL Glue, Spark, Python, Lambda,Kafka,S3, EMR Etc experience is a must.\nMonitoring tools - Cloudwatch\nExperience in Development & Support Projects as well.\nStrong verbal and written communication skills\nStrong experience and understanding of streaming architecture and development practices using kafka, spark, flink etc,\nStrong AWS development experience using S3, SNS, SQS, MWAA (Airflow) Glue, DMS and EMR.\nStrong knowledge of one or more programing languages Python/Java/Scala (ideally Python)\nExperience using Terraform to build IAC components in AWS.\nStrong experience with ETL Tools in AWS; ODI experience is as plus.\nStrong experience with Database Platforms: Oracle, AWS Redshift\nStrong experience in SQL tuning, tuning ETL solutions, physical optimization of databases.\nVery familiar with SRE concepts which includes evaluating and implementing monitoring and observability tools like Splunk, Data Dog, CloudWatch and other job, log or dashboard concepts for customer support and application health checks.\nAbility to collaborate with our business partners to understand and implement their requirements.\nExcellent interpersonal skills and be able to build consensus across teams.\nStrong critical thinking and ability to think out-of-the box.\nSelf-motivated and able to perform under pressure.\nAWS certified (preferred)\nQualifications:\nEducation: Bachelor's degree in Computer Science, Information Technology, or a related field (or equivalent work experience).\nExperience:\nProven experience as an AWS Support Engineer or in a similar role.\nHands-on experience with a wide range of AWS services (e.g., EC2, S3, RDS, Lambda, CloudFormation, IAM).\nSoft Skills:\nExcellent communication and interpersonal skills.\nStrong problem-solving and analytical skills.\nAbility to work independently and as part of a team.\nCustomer-focused mindset with a commitment to delivering high-quality support.\nWhat We Offer:\nCompetitive salary and benefits package.\nOpportunities for professional growth and development.\nA collaborative and supportive work environment.\nAccess to the latest AWS technologies and training resources.\nIf you are passionate about cloud technology and enjoy helping customers solve complex technical challenges, we would love to hear from you!\nAcknowledge the mail with your updated cv,\nJulekha.Gousiya@astrosofttech.com\n\n\n\nDetails As we discussed, Please revert with your acknowledgment.\nTotal Experience-\nAws\nRedshift\nGlue-\nCurrent Location-\nCurrent Company-\nC-CTC-\nEx-CTC –\nOffer –\nNP –\nReady to Relocate Hyderabad (Y/N) – Yes\n(Hybrid) –(Y/N) - Yes",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['AWS', 'Glue', 'redshift', 'Spark']",2025-06-12 14:47:07
Data Governance Engineers,Meritus Management Service,4 - 9 years,14-17 Lacs P.A.,"['Pune', 'Gurugram']","Define, implement, & enforce data governance policies & standards to ensure data quality, consistency, & compliance across the organization\nCollaborate with data stewards, business users, & IT teams to maintain metadata, lineage, & data catalog tools",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Stewardship', 'Metadata', 'Data Governance', 'Metadata Management', 'Data Lineage', 'Data Modeling', 'SQL']",2025-06-12 14:47:09
Data Science Lead,Algoleap Technologies,10 - 15 years,Not Disclosed,['Hyderabad'],"As we build our Data Science Center of Excellence (CoE), we are looking for an entrepreneurial and technically strong Data Science Lead who can lay the foundation for a high-performing team. You will work directly with stakeholders across multiple business units to define use cases, lead model development, and ensure successful deployment and value realization.\n\nKey Responsibilities:\nServe as the technical and strategic lead for the Data Science CoE.",,,,"['customer analytics', 'Usage', 'data science', 'GCP', 'Machine learning', 'model development', 'Deployment', 'Stakeholder management', 'SQL', 'Python']",2025-06-12 14:47:12
Immediate Opening For Data Science,Happiest Minds Technologies,8 - 13 years,Not Disclosed,['Bengaluru( Madiwala )'],"Machine Learning, Deep Learning models, Data Science. (Important);-R / python programming (mandatory) ;- Fast API development ;- deployment of models experience ; - any cloud Azure (good to have - for this requirement); - basics of Generative AI , NLP (optional - Good to have)\n\nGIS data, Geospatial data, Google Maps, ArcGIS, Demand pattern analysis\n\n5 to 15 Yrs",,,,"['Data Science', 'Machine Learning', 'Deep Learning', 'Python', 'GenAi', 'Natural Language Processing']",2025-06-12 14:47:14
Data Engineer,Meritus Management Service,4 - 9 years,9-18 Lacs P.A.,"['Pune', 'Gurugram']","The first Data Engineer specializes in traditional ETL with SAS DI and Big Data (Hadoop, Hive). The second is more versatile, skilled in modern data engineering with Python, MongoDB, and real-time processing.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Etl Pipelines', 'Big Data', 'Informatica', 'SAS DI', 'SQL', 'Hive', 'Hadoop', 'Talend', 'ETL Tool', 'Python']",2025-06-12 14:47:16
Data Solution Architect,Maveric,13 - 20 years,Not Disclosed,"['Chennai', 'Bengaluru']","Position Overview\nWe are looking for a highly experienced and versatile Solution Architect Data to lead the solution design and delivery of next-generation data solutions for our BFS clients. The ideal candidate will have a strong background in data architecture and engineering, deep domain expertise in financial services, and hands-on experience with cloud-native data platforms and modern data analytics tools. The role will require architecting solutions across Retail, Corporate, Wealth, and Capital Markets, as well as Payments, Lending, and Onboarding journeys. Possession of Data Analytics and Exposure to Data regulatory domain will be of distinct advantage. Hands on experience of AI & Gen AI enabling data related solution will be a distinct advantage for the position.",,,,"['Data Quality', 'Data Engineering', 'Data Governance', 'GenAI']",2025-06-12 14:47:18
Principal Architect (Data and Cloud),Neoware Technology Solutions,10 - 15 years,Not Disclosed,"['Chennai', 'Bengaluru']","Principal Architect (Data and Cloud) - Neoware Technology Solutions Private Limited Principal Architect (Data and Cloud)\nRequirements\nMore than 10 years of experience in Technical, Solutioning, and Analytical roles.\n5+ years of experience in building and managing Data Lakes, Data Warehouse, Data Integration, Data Migration and Business Intelligence/Artificial Intelligence solutions on Cloud (GCP/AWS/Azure).\nAbility to understand business requirements, translate them into functional and non-functional areas, define non-functional boundaries in terms of Availability, Scalability, Performance, Security, Resilience etc.\nExperience in architecting, designing, and implementing end to end data pipelines and data integration solutions for varied structured and unstructured data sources and targets.\nExperience of having worked in distributed computing and enterprise environments like Hadoop, GCP/AWS/Azure Cloud.\nWell versed with various Data Integration, and ETL technologies on Cloud like Spark, Pyspark/Scala, Dataflow, DataProc, EMR, etc. on various Cloud.\nExperience of having worked with traditional ETL tools like Informatica / DataStage / OWB / Talend , etc.\nDeep knowledge of one or more Cloud and On-Premise Databases like Cloud SQL, Cloud Spanner, Big Table, RDS, Aurora, DynamoDB, Oracle, Teradata, MySQL, DB2, SQL Server, etc.\nExposure to any of the No-SQL databases like Mongo dB, CouchDB, Cassandra, Graph dB, etc.\nExperience in architecting and designing scalable data warehouse solutions on cloud on Big Query or Redshift.\nExperience in having worked on one or more data integration, storage, and data pipeline tool sets like S3, Cloud Storage, Athena, Glue, Sqoop, Flume, Hive, Kafka, Pub-Sub, Kinesis, Dataflow, DataProc, Airflow, Composer, Spark SQL, Presto, EMRFS, etc.\nPreferred experience of having worked on Machine Learning Frameworks like TensorFlow, Pytorch, etc.\nGood understanding of Cloud solutions for Iaas, PaaS, SaaS, Containers and Microservices Architecture and Design.\nAbility to compare products and tools across technology stacks on Google, AWS, and Azure Cloud.\nGood understanding of BI Reporting and Dashboarding and one or more tool sets associated with it like Looker, Tableau, Power BI, SAP BO, Cognos, Superset, etc.\nUnderstanding of Security features and Policies in one or more Cloud environments like GCP/AWS/Azure.\nExperience of having worked in business transformation projects for movement of On-Premise data solutions to Clouds like GCP/AWS/Azure.\nBe a trusted technical advisor to customers and solutions for complex Cloud & Data related technical challenges.\nBe a thought leader in architecture design and development of cloud data analytics solutions.\nLiaison with internal and external stakeholders to design optimized data analytics solutions.\nPartner with SMEs and Solutions Architects from leading cloud providers to present solutions to customers.\nSupport Sales and GTM teams from a technical perspective in building proposals and SOWs.\nLead discovery and design workshops with potential customers across the globe.\nDesign and deliver thought leadership webinars and tech talks alongside customers and partners.\nResponsibilities\nLead multiple data engagements on GCP Cloud for data lakes, data engineering, data migration, data warehouse, and business intelligence.\nInterface with multiple stakeholders within IT and business to understand the data requirements.\nTake complete responsibility for the successful delivery of all allocated projects on the parameters of Schedule, Quality, and Customer Satisfaction.\nResponsible for design and development of distributed, high volume multi-thread batch, real-time, and event processing systems.\nImplement processes and systems to validate data, monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it.\nWork with the Pre-Sales team on RFP, RFIs and help them by creating solutions for data.\nMentor young Talent within the Team, Define and track their growth parameters.\nContribute to building Assets and Accelerators.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Db2', 'Cognos', 'MySQL', 'Datastage', 'Presales', 'Informatica', 'Oracle', 'Teradata', 'Business intelligence', 'SQL']",2025-06-12 14:47:20
Manager Data Engineer – Research Data and Analytics,Amgen Inc,4 - 6 years,Not Disclosed,['Hyderabad'],"What you will do\nIn this vital role you will create and develop data lake solutions for scientific data that drive business decisions for Research. You will build scalable and high-performance data engineering solutions for large scientific datasets and collaborate with Research collaborators. You will also provide technical leadership to junior team members. The ideal candidate possesses experience in the pharmaceutical or biotech industry, demonstrates deep technical skills, is proficient with big data technologies, and has a deep understanding of data architecture and ETL processes.\nRoles & Responsibilities:\nLead, manage, and mentor a high-performing team of data engineers\nDesign, develop, and implement data pipelines, ETL processes, and data integration solutions\nTake ownership of data pipeline projects from inception to deployment, manage scope, timelines, and risks\nDevelop and maintain data models for biopharma scientific data, data dictionaries, and other documentation to ensure data accuracy and consistency\nOptimize large datasets for query performance\nCollaborate with global multi-functional teams including research scientists to understand data requirements and design solutions that meet business needs\nImplement data security and privacy measures to protect sensitive data\nLeverage cloud platforms (AWS preferred) to build scalable and efficient data solutions\nCollaborate with Data Architects, Business SMEs, Software Engineers and Data Scientists to design and develop end-to-end data pipelines to meet fast paced business needs across geographic regions\nIdentify and resolve data-related challenges\nAdhere to best practices for coding, testing, and designing reusable code/component\nExplore new tools and technologies that will help to improve ETL platform performance\nParticipate in sprint planning meetings and provide estimations on technical implementation\n\n\nWhat we expect of you\nWe are all different, yet we all use our unique contributions to serve patients. The [vital attribute] professional we seek is a [type of person] with these qualifications.\nBasic Qualifications:\nDoctorate Degree OR\nMasters degree with 4 - 6 years of experience in Computer Science, IT, Computational Chemistry, Computational Biology/Bioinformatics or related field OR\nBachelors degree with 6 - 8 years of experience in Computer Science, IT, Computational Chemistry, Computational Biology/Bioinformatics or related field OR\nDiploma with 10 - 12 years of experience in Computer Science, IT, Computational Chemistry, Computational Biology/Bioinformatics or related field\nPreferred Qualifications:\n3+ years of experience in implementing and supporting biopharma scientific research data analytics (software platforms)\n\n\nFunctional Skills:\nMust-Have Skills:\nProficiency in SQL and Python for data engineering, test automation frameworks (pytest), and scripting tasks\nHands on experience with big data technologies and platforms, such as Databricks, Apache Spark (PySpark, SparkSQL), workflow orchestration, performance tuning on big data processing\nExcellent problem-solving skills and the ability to work with large, complex datasets\nAble to engage with business collaborators and mentor team to develop data pipelines and data models\n\n\nGood-to-Have Skills:\nA passion for tackling complex challenges in drug discovery with technology and data\nGood understanding of data modeling, data warehousing, and data integration concepts\nGood experience using RDBMS (e.g. Oracle, MySQL, SQL server, PostgreSQL)\nKnowledge of cloud data platforms (AWS preferred)\nExperience with data visualization tools (e.g. Dash, Plotly, Spotfire)\nExperience with diagramming and collaboration tools such as Miro, Lucidchart or similar tools for process mapping and brainstorming\nExperience writing and maintaining technical documentation in Confluence\nUnderstanding of data governance frameworks, tools, and best practices\n\n\nProfessional Certifications:\nDatabricks Certified Data Engineer Professional preferred\n\n\nSoft Skills:\nExcellent critical-thinking and problem-solving skills\nGood communication and collaboration skills\nDemonstrated awareness of how to function in a team setting\nDemonstrated presentation skills",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Spotfire', 'PySpark', 'PostgreSQL', 'Plotly', 'SparkSQL', 'SQL server', 'SQL', 'process mapping', 'Dash', 'MySQL', 'ETL', 'Oracle', 'data governance frameworks', 'Python']",2025-06-12 14:47:22
Hadoop Data Engineer,Envision Technology Solutions,3 - 8 years,5-15 Lacs P.A.,"['New Delhi', 'Hyderabad', 'Gurugram']","Primary Skill – Hadoop, Hive, Python, SQL, Pyspark/Spark.\nLocation –Hyderabad / Gurgaon;",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Hadoop', 'Hive', 'Spark', 'Python', 'SQL']",2025-06-12 14:47:25
Devops AWS DATA Engineeer|| Technical Analyst || 12Lakhs CTC,Robotics Technologies,8 - 9 years,11-12 Lacs P.A.,['Hyderabad( Banjara hills )'],"We are seeking a highly skilled Devops Engineer to join our dynamic development team. In this role, you will be responsible for designing, developing, and maintaining both frontend and backend components of our applications using Devops and associated technologies.\nYou will collaborate with cross-functional teams to deliver robust, scalable, and high-performing software solutions that meet our business needs. The ideal candidate will have a strong background in devops, experience with modern frontend frameworks, and a passion for full-stack development.\n\nRequirements:\nBachelor's degree in Computer Science Engineering, or a related field.\n8 to 9+ years of experience in full-stack development, with a strong focus on DevOps.\n\nDevOps with AWS Data Engineer - Roles & Responsibilities:\nUse AWS services like EC2, VPC, S3, IAM, RDS, and Route 53.\nAutomate infrastructure using Infrastructure as Code (IaC) tools like Terraform or AWS CloudFormation.\nBuild and maintain CI/CD pipelines using tools AWS CodePipeline, Jenkins,GitLab CI/CD.\nCross-Functional Collaboration\nAutomate build, test, and deployment processes for Java applications.\nUse Ansible, Chef, or AWS Systems Manager for managing configurations across environments.\nContainerize Java apps using Docker.\nDeploy and manage containers using Amazon ECS, EKS (Kubernetes), or Fargate.\nMonitoring & Logging using Amazon CloudWatch,Prometheus + Grafana,E\nStack (Elasticsearch, Logstash, Kibana),AWS X-Ray for distributed tracing manage access with IAM roles/policies.\nUse AWS Secrets Manager / Parameter Store for managing credentials.\nEnforce security best practices, encryption, and audits.\nAutomate backups for databases and services using AWS Backup, RDS Snapshots, and S3 lifecycle rules.\nImplement Disaster Recovery (DR) strategies.\nWork closely with development teams to integrate DevOps practices.\nDocument pipelines, architecture, and troubleshooting runbooks.\nMonitor and optimize AWS resource usage.\nUse AWS Cost Explorer, Budgets, and Savings Plans.\n\nMust-Have Skills:\nExperience working on Linux-based infrastructure.\nExcellent understanding of Ruby, Python, Perl, and Java.\nConfiguration and managing databases such as MySQL, Mongo.\nExcellent troubleshooting.\nSelecting and deploying appropriate CI/CD tools\nWorking knowledge of various tools, open-source technologies, and cloud services.\nAwareness of critical concepts in DevOps and Agile principles.\nManaging stakeholders and external interfaces.\nSetting up tools and required infrastructure.\nDefining and setting development, testing, release, update, and support processes for DevOps operation.\nHave the technical skills to review, verify, and validate the software code developed in the project.\nInterview Mode : F2F for who are residing in Hyderabad / Zoom for other states\nLocation : 43/A, MLA Colony,Road no 12, Banjara Hills, 500034\nTime : 2 - 4pm",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Iac', 'Devops', 'Jenkins', 'AWS', 'Kubernetes', 'RDS', 'Aws Cloudformation', 'Amazon Cloudwatch', 'Prometheus', 'Ci/Cd', 'Grafana', 'DR', 'Cloud Trail', 'Docker', 'IAM', 'Ansible / Chef', 'fargate', 'Gitlab', 'Monitoring', 'Python']",2025-06-12 14:47:27
Devops AWS DATA Engineeer|| Technical Analyst || 12Lakhs CTC,Robotics Technologies,7 - 9 years,11-12 Lacs P.A.,['Hyderabad( Banjara hills )'],"We are seeking a highly skilled Devops Engineer to join our dynamic development team. In this role, you will be responsible for designing, developing, and maintaining both frontend and backend components of our applications using Devops and associated technologies.\nYou will collaborate with cross-functional teams to deliver robust, scalable, and high-performing software solutions that meet our business needs. The ideal candidate will have a strong background in devops, experience with modern frontend frameworks, and a passion for full-stack development.\n\nRequirements:\nBachelor's degree in Computer Science Engineering, or a related field.\n7 to 9+ years of experience in full-stack development, with a strong focus on DevOps.\n\nDevOps with AWS Data Engineer - Roles & Responsibilities:\nUse AWS services like EC2, VPC, S3, IAM, RDS, and Route 53.\nAutomate infrastructure using Infrastructure as Code (IaC) tools like Terraform or AWS CloudFormation.\nBuild and maintain CI/CD pipelines using tools AWS CodePipeline, Jenkins,GitLab CI/CD.\nCross-Functional Collaboration\nAutomate build, test, and deployment processes for Java applications.\nUse Ansible, Chef, or AWS Systems Manager for managing configurations across environments.\nContainerize Java apps using Docker.\nDeploy and manage containers using Amazon ECS, EKS (Kubernetes), or Fargate.\nMonitoring & Logging using Amazon CloudWatch,Prometheus + Grafana,E\nStack (Elasticsearch, Logstash, Kibana),AWS X-Ray for distributed tracing manage access with IAM roles/policies.\nUse AWS Secrets Manager / Parameter Store for managing credentials.\nEnforce security best practices, encryption, and audits.\nAutomate backups for databases and services using AWS Backup, RDS Snapshots, and S3 lifecycle rules.\nImplement Disaster Recovery (DR) strategies.\nWork closely with development teams to integrate DevOps practices.\nDocument pipelines, architecture, and troubleshooting runbooks.\nMonitor and optimize AWS resource usage.\nUse AWS Cost Explorer, Budgets, and Savings Plans.\n\nMust-Have Skills:\nExperience working on Linux-based infrastructure.\nExcellent understanding of Ruby, Python, Perl, and Java.\nConfiguration and managing databases such as MySQL, Mongo.\nExcellent troubleshooting.\nSelecting and deploying appropriate CI/CD tools\nWorking knowledge of various tools, open-source technologies, and cloud services.\nAwareness of critical concepts in DevOps and Agile principles.\nManaging stakeholders and external interfaces.\nSetting up tools and required infrastructure.\nDefining and setting development, testing, release, update, and support processes for DevOps operation.\nHave the technical skills to review, verify, and validate the software code developed in the project.\nInterview Mode : F2F for who are residing in Hyderabad / Zoom for other states\nLocation : 43/A, MLA Colony,Road no 12, Banjara Hills, 500034\nTime : 2 - 4pm",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Iac', 'Devops', 'Jenkins', 'AWS', 'Kubernetes', 'RDS', 'Aws Cloudformation', 'Amazon Cloudwatch', 'Prometheus', 'Ci/Cd', 'Grafana', 'DR', 'Cloud Trail', 'Docker', 'IAM', 'Ansible / Chef', 'fargate', 'Gitlab', 'Monitoring', 'Python']",2025-06-12 14:47:29
Azure Data Engineer,Arges Global,2 - 5 years,8-18 Lacs P.A.,['Pune( Baner )'],"Scope of Work:\nCollaborate with the lead Business / Data Analyst to gather and analyse business requirements for data processing and reporting solutions.\nMaintain and run existing Python code, ensuring smooth execution and troubleshooting any issues that arise.\nDevelop new features and enhancements for data processing, ingestion, transformation, and report building.\nImplement best coding practices to improve code quality, maintainability, and efficiency.\nWork within Microsoft Fabric to manage data integration, warehousing, and analytics, ensuring optimal performance and reliability.\nSupport and maintain CI/CD workflows using Git-based deployments or other automated deployment tools, preferably in Fabric.\nDevelop complex business rules and logic in Python to meet functional specifications and reporting needs.\nParticipate in an agile development environment, providing feedback, iterating on improvements, and supporting continuous integration and delivery processes.\nRequirements:\nThis person will be an individual contributor responsible for programming, maintenance support, and troubleshooting tasks related to data movement, processing, ingestion, transformation, and report building.\nAdvanced-level Python developer.\nModerate-level experience in working in Microsoft Fabric environment (at least one and preferably two or more client projects in Fabric).\nWell-versed with understanding of modelling, databases, data warehousing, data integration, and technical elements of business intelligence technologies.\nAbility to understand business requirements and translate them into functional specifications for reporting applications.\nExperience in GIT-based deployments or other CI/CD workflow options, preferably in Fabric.\nStrong verbal and written communication skills.\nAbility to perform in an agile environment where continual development is prioritized.\nWorking experience in the financial industry domain and familiarity with financial accounting terms and statements like general ledger, balance sheet, and profit & loss statements would be a plus.\nAbility to create Power BI dashboards, KPI scorecards, and visual reports would be a plus.\nDegree in Computer Science or Information Systems, along with a good understanding of financial terms or working experience in banking/financial institutions, is preferred.",Industry Type: Financial Services (Asset Management),Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Power Bi', 'Microsoft Azure', 'Python', 'Azure Data Factory', 'Microsoft Fabric', 'Azure Databricks', 'Azure Data Lake']",2025-06-12 14:47:31
Data & Analytics Subject Matter Expert,Trianz,10 - 15 years,Not Disclosed,"['Hyderabad', 'Chennai', 'Bengaluru']","Role Overview\nWe are looking for a Data & Analytics Subject Matter Expert with deep expertise in Data Engineering, Business Intelligence (BI), and AWS cloud ecosystems . This role demands strategic thinking, hands-on execution, and collaboration across technical and business teams to deliver impactful data-driven solutions.\nKey Responsibilities\n1. Data Architecture & Engineering\nDesign and implement scalable, high-performance data solutions on AWS.\nBuild robust data pipelines, ETL/ELT workflows, and data lake architectures.\nEnforce data quality, security, and governance practices.\n2. Business Intelligence & Insights\nDevelop interactive dashboards and visualizations using Power BI, Tableau, or QuickSight.\nDefine data models and KPIs to support data-driven decision-making.\nCollaborate with business teams to extract insights that drive action.\n3. Cloud & Advanced Analytics\nDeploy data warehousing solutions using Redshift, Glue, S3, Athena, and other AWS services.\nOptimize storage and processing strategies for performance and cost-efficiency.\nExplore AI/ML integrations for predictive and advanced analytics (preferred).\n4. Collaboration & Best Practices\nPartner with cross-functional teams (engineering, data science, business) to align on data needs.\nChampion best practices in data governance, compliance, and architecture.\nTranslate business requirements into scalable technical solutions.\nRequired Qualifications\nEducation\nBachelor s or Master s in Computer Science, Information Technology, Data Science, or related discipline.\nExperience\n10+ years of experience in data engineering, BI, and analytics domains.\nProven experience with AWS data tools and modern data architectures.\nTechnical Skills\nStrong command of AWS services: Redshift, Glue, S3, Athena, Lambda, Kinesis.\nProficient in SQL, Python, or Scala.\nExperience building and maintaining ETL/ELT workflows and data models.\nExpertise in BI tools like Power BI, Tableau, QuickSight, or Looker.\nFamiliarity with AI/ML models and frameworks is a plus.\nCertifications\nPreferred: AWS Certified Data Analytics - Specialty.\nAdditional certifications in AWS, data engineering, or analytics are a plus.\nWhy Join Trianz\nJoin a high-growth, innovation-led firm delivering transformation at scale.\nCollaborate with global teams on cutting-edge cloud and analytics projects.\nEnjoy a competitive compensation structure and clear career progression pathways.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['power bi', 'Data analytics', 'Subject Matter Expert', 'Business intelligence', 'AWS', 'Information technology', 'Analytics', 'SQL', 'Data architecture', 'Python']",2025-06-12 14:47:34
MDM Associate Data Engineer,Amgen Inc,1 - 4 years,Not Disclosed,['Hyderabad'],"We are seeking an MDM Associate Data Engineerwith 25 years of experience to support and enhance our enterprise MDM (Master Data Management) platforms using Informatica/Reltio. This role is critical in delivering high-quality master data solutions across the organization, utilizing modern tools like Databricks and AWS to drive insights and ensure data reliability. The ideal candidate will have strong SQL, data profiling, and experience working with cross-functional teams in a pharma environment.To succeed in this role, the candidate must have strong data engineering experience along with MDM knowledge, hence the candidates having only MDM experience are not eligible for this role. Candidate must have data engineering experience on technologies like (SQL, Python, PySpark, Databricks, AWS etc), along with knowledge of MDM (Master Data Management)\nRoles & Responsibilities:\nAnalyze and manage customer master data using Reltio or Informatica MDM solutions.\nPerform advanced SQL queries and data analysis to validate and ensure master data integrity.\nLeverage Python, PySpark, and Databricks for scalable data processing and automation.\nCollaborate with business and data engineering teams for continuous improvement in MDM solutions.\nImplement data stewardship processes and workflows, including approval and DCR mechanisms.\nUtilize AWS cloud services for data storage and compute processes related to MDM.\nContribute to metadata and data modeling activities.\nTrack and manage data issues using tools such as JIRA and document processes in Confluence.\nApply Life Sciences/Pharma industry context to ensure data standards and compliance.\nBasic Qualifications and Experience:\nMasters degree with 1 - 3 years of experience in Business, Engineering, IT or related field OR\nBachelors degree with 2 - 5 years of experience in Business, Engineering, IT or related field OR\nDiploma with 6 - 8 years of experience in Business, Engineering, IT or related field\nFunctional Skills:\nMust-Have Skills:\nAdvanced SQL expertise and data wrangling.\nStrong experience in Python and PySpark for data transformation workflows.\nStrong experience with Databricks and AWS architecture.\nMust have knowledge of MDM, data governance, stewardship, and profiling practices.\nIn addition to above, candidates having experience with Informatica or Reltio MDM platforms will be preferred.\nGood-to-Have Skills:\nExperience with IDQ, data modeling and approval workflow/DCR.\nBackground in Life Sciences/Pharma industries.\nFamiliarity with project tools like JIRA and Confluence.\nStrong grip on data engineering concepts.\nProfessional Certifications:\nAny ETL certification (e.g. Informatica)\nAny Data Analysis certification (SQL, Python, Databricks)\nAny cloud certification (AWS or AZURE)\nSoft Skills:\nStrong analytical abilities to assess and improve master data processes and solutions.\nExcellent verbal and written communication skills, with the ability to convey complex data concepts clearly to technical and non-technical stakeholders.\nEffective problem-solving skills to address data-related issues and implement scalable solutions.\nAbility to work effectively with global, virtual teams",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['MDM', 'PySpark', 'AWS architecture', 'Jira', 'Reltio', 'SQL', 'Informatica MDM', 'data modeling', 'Confluence', 'IDQ', 'Databricks', 'data stewardship processes', 'Python']",2025-06-12 14:47:36
Data Architect - AWS,Happiest Minds Technologies,10 - 15 years,Not Disclosed,"['Noida', 'Pune', 'Bengaluru']","Roles and responsibilities\nWork closely with the Product Owners and stake holders to design the Technical Architecture for data platform to meet the requirements of the proposed solution.\nWork with the leadership to set the standards for software engineering practices within the machine learning engineering team and support across other disciplines\nPlay an active role in leading team meetings and workshops with clients.\nChoose and use the right analytical libraries, programming languages, and frameworks for each task.",,,,"['SQL', 'data architect', 'Python', 'Pyspark', 'Apache Airflow', 'GLUE', 'Kinesis', 'Amazon Redshift', 'Data Architecture Principles', 'Data Modeling', 'Data Warehousing', 'Athena', 'Lambda', 'AWS']",2025-06-12 14:47:39
Data Architecture,Top B2B MNC in Management Consulting Dom...,5 - 8 years,Not Disclosed,['Bengaluru'],"About the Company\nGreetings from Teamware Solutions a division of Quantum Leap Consulting Pvt. Ltd\n\nAbout the Role\nWe are hiring a Data Architecture\n\nLocation: Bangalore\nWork Model: Hybrid\nExperience: 5-9 Years\nNotice Period: Immediate to 15 Days\n\nJob Description:\nData Architecture, Data Governance, Data Modeling\n\nAdditional Information:\nMandatory Skills: Data Architecture, Data Governance, Data Modeling\nNice to have skills Certification in Data Engineering\nInterview Mode Virtual Interview\nminimum 5 yrs relevant experience and maximum 9 yrs for this requirement. Someone with more experience in building PySpark data streaming jobs on Azure Databricks\nwho have done real projects, have expertise, and hands-on experience also\nAlso, Data governance and data modeling experience with a minimum of 4 years is mandatory\nCommunication should be excellent\n\n\nPlease let me know if you are interested in this position and send me your resumes to netra.s@twsol.com",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Data Architecture', 'Data Modeling', 'Data Governance', 'Data Engineering']",2025-06-12 14:47:40
Azure Data Bricks (4-15 Yrs) - Bangalore,Happiest Minds Technologies,4 - 9 years,Not Disclosed,['Bengaluru'],"Hi,\n\nGreetings from Happiest Minds Technologies\n\nCurrently we are hiring for below positions and looking for immediate joiners.\n1. Azure Databricks Bangalore 5 to 10 Yrs - Bangalore\nAs a Senior Azure Data Engineer, you will leverage Azure technologies to drive data transformation, analytics, and machine learning. You will design scalable Databricks data pipelines using PySpark, transforming raw data into actionable insights. Your role includes building, deploying, and maintaining machine learning models using MLlib or TensorFlow while optimizing cloud data integration from Azure Blob Storage, Data Lake, and SQL/NoSQL sources. You will execute large-scale data processing using Spark Pools, fine-tuning configurations for efficiency. The ideal candidate holds a Bachelors or Masters in Computer Science, Data Science, or a related field, with 7+ years in data engineering and 3+ years specializing in Azure Databricks, PySpark, and Spark Pools. Proficiency in Python PySpark, Pandas, NumPy, SciPy, Spark SQL, DataFrames, RDDs, Delta Lake, Databricks Notebooks, and MLflow is required, along with hands-on experience in Azure Data Lake, Blob Storage, and Synapse Analytics.",,,,"['Pyspark', 'Azure', 'Data Bricks', 'sql', 'ETL']",2025-06-12 14:47:43
Master Data Management Architect,K-logix Partnering Solutions,8 - 13 years,Not Disclosed,[],"Bachelor'sMaster's\nOverview:\n\nWe are seeking a highly skilled and experienced Celonis MDM Data Architect to lead the design, implementation, and optimization of our Master Data Management (MDM) solutions in alignment with Celonis Process Mining and Execution Management System (EMS) capabilities.\nThe ideal candidate will play a key role in bridging data architecture and business process insights, ensuring data quality, consistency, and governance across the enterprise.\n\nKey Responsibilities:\nDesign and implement MDM architecture and data models aligned with enterprise standards and best practices.\n• Lead the integration of Celonis with MDM platforms to drive intelligent process automation, data governance, and operational efficiencies.\n• Collaborate with business stakeholders and data stewards to define MDM policies, rules, and processes.\n• Support data profiling, data cleansing, and data harmonization efforts to improve master data quality.\n• Work closely with Celonis analysts, data engineers, and process owners to deliver actionable insights based on MDM-aligned process data.\n• Develop and maintain scalable, secure, and high-performance data pipelines and integration architectures.\n• Translate business requirements into technical solutions, ensuring alignment with both MDM and Celonis data models.\n• Create and maintain data architecture documentation, data dictionaries, and metadata repositories.\n• Monitor and optimize the performance of MDM systems and Celonis EMS integrations.\nQualifications:\nBachelors or Masters degree in Computer Science, Information Systems, Data Engineering, or a related field.\n• 7+ years of experience in data architecture, MDM, or enterprise data management.\n• 2+ years of hands-on experience with Celonis and process mining tools.\n• Proficient in MDM platforms (e.g., Informatica MDM, SAP MDG, Oracle MDM, etc.).\n• Strong knowledge of data modeling, data governance, and metadata management.\n• Proficiency in SQL, data integration tools (e.g., ETL/ELT platforms), and APIs.\n• Deep understanding of business process management and data-driven transformation initiatives.",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['Celonis', 'MDM', 'Master Data Management', 'ETL', 'Elt']",2025-06-12 14:47:45
"Applied Scientist, Amazon Autos",Amazon,3 - 8 years,Not Disclosed,['Gurugram'],"Interested in building something new? Join the Amazon Autos team on an exhilarating journey to redefine the vehicle shopping experience.\nThis is an opportunity to be part of the Amazons new business ventures. Our goal is to create innovative automotive discovery and shopping experiences on Amazon, providing customers with greater convenience and a wider selection.\nYoull work in a creative, fast-paced, and entrepreneurial environment at the center of Amazons innovation. As a key member, youll play a pivotal role in helping us achieve our mission. We are looking for a highly accomplished Applied Science professional drive our science strategy, foster a culture of data-driven decision-making, and drive impactful business outcomes through advanced state-of-the-art science methodologies.\nIf youre enthusiastic about innovating and delivering exceptional shopping experiences to customers, thrive on new challenges, and excel at solving complex problems using top-notch ML models, LLM and GenAI techniques, then youre the perfect candidate for this role. Strong business acumen and interpersonal skills are a must, as youll work closely with business owners to understand customer needs and design scalable solutions.\nJoin us on this exhilarating journey and be part of redefining the vehicle shopping experience.\n\n\nAs an Applied Scientist in Amazon Autos, you will:\n\nShape the roadmap and strategy for applying science to solve customer problems in the Amazon AutoStore domain.\nDrive big picture innovations with clear roadmaps for intermediate delivery.\nApply your skills in areas such as deep learning and reinforcement learning while building scalable solutions for business problems.\nProduce and deliver models that help build best-in-class customer experiences and build systems that allow us to deploy these models to production with low latency and high throughput.\nUtilize your Generative AI, time series and predictive modeling skills, and creative problem-solving skills to drive new projects from ideation to implementation.\nInterface with business customers, gathering requirements and delivering science solutions.\nCollaborate with cross-functional teams, including software engineers, data scientists, and product managers, to define project requirements, establish success metrics, and deliver high-quality solutions.\nEffectively communicate complicated machine learning concepts to multiple partners.\nResearch new and innovative machine learning approaches.\n\nA day in the life\nIn this role, you will be part of a multidisciplinary team working on one of Amazons newest business ventures. As a key member, you will collaborate closely with engineering, product, design, operations, and business development to bring innovative solutions to our customers.\nYour science expertise will be leveraged to research and deliver novel solutions to existing problems, explore emerging problem spaces, and create new knowledge. You will invent and apply state-of-the-art technologies, such as large language models, machine learning, natural language processing, and computer vision, to build next-generation solutions for Amazon.\nYoull publish papers, file patents, and work closely with engineers to bring your ideas to production.\n\nAbout the team\nThis is a critical role for Amazon Autos team with a vision to create innovative automotive discovery and shopping experiences on Amazon, providing customers better convenience and more selection. We re collaborating with other experienced teams at Amazon to define the future of how customers research and shop for cars online. 3+ years of building models for business application experience\nPhD, or Masters degree and 4+ years of CS, CE, ML or related field experience\nExperience in patents or publications at top-tier peer-reviewed conferences or journals\nExperience programming in Java, C++, Python or related language\nExperience in any of the following areas: algorithms and data structures, parsing, numerical optimization, data mining, parallel and distributed computing, high-performance computing Experience using Unix/Linux\nExperience in professional software development\nExperience building complex software systems, especially involving deep learning, machine learning and computer vision, that have been successfully delivered to customers",,,,"['Unix', 'Computer vision', 'C++', 'Linux', 'Machine learning', 'Data structures', 'Product design', 'Data mining', 'Automotive', 'Python']",2025-06-12 14:47:47
It Recruiter,IonIdea,0 - 3 years,Not Disclosed,['Bengaluru'],"Key Responsibilities:\nTalent Sourcing: Utilize various channels such as job boards, social media, LinkedIn, networking events, and internal databases to source and attract high-quality candidates for a variety of technical positions (software developers, systems engineers, data scientists, etc.).\nCandidate Screening: Review resumes, conduct initial phone screenings, and assess candidates technical skills, experience, and cultural fit.\nInterview Coordination: Schedule and facilitate interviews with hiring managers, ensuring a smooth and efficient process for all parties involved.\nCandidate Engagement: Build relationships with both active and passive candidates to maintain a strong pipeline of qualified talent. Keep candidates informed throughout the hiring process.\nOffer Management: Work with HR and hiring managers to present offers, negotiate terms, and ensure a positive candidate experience during the offer process.\n\nQualifications:\nExperience: Fresher-3years\n\nTechnical Knowledge: A solid understanding of IT roles, including knowledge of programming languages, software development frameworks, network infrastructure, cloud technologies, and emerging IT trends.\nRecruitment Tools: Proficient in using Applicant Tracking Systems (ATS), job boards (e.g., LinkedIn, Indeed), and social media platforms for sourcing candidates.\nCommunication Skills: Excellent written and verbal communication skills with the ability to engage with both technical and non-technical stakeholders.",Industry Type: IT Services & Consulting,Department: Human Resources,"Employment Type: Full Time, Permanent","['IT Recruitment', 'C2H', 'Contract Hiring']",2025-06-12 14:47:49
Data Engineer - R&D Data Catalyst Team,Amgen Inc,1 - 3 years,Not Disclosed,['Hyderabad'],"What you will do\nIn this vital role, you will be responsible for the end-to-end development of an enterprise analytics and data mastering solution using Databricks and Power BI. This role requires expertise in both data architecture and analytics, with the ability to create scalable, reliable, and impactful enterprise solutions that research cohort-building and advanced research pipeline. The ideal candidate will have experience creating and surfacing large unified repositories of human data, based on integrations from multiple repositories and solutions, and be extraordinarily skilled with data analysis and profiling.\nYou will collaborate closely with key customers, product team members, and related IT teams, to design and implement data models, integrate data from various sources, and ensure best practices for data governance and security. The ideal candidate will have a good background in data warehousing, ETL, Databricks, Power BI, and enterprise data mastering.\nDesign and build scalable enterprise analytics solutions using Databricks, Power BI, and other modern data tools.\nLeverage data virtualization, ETL, and semantic layers to balance need for unification, performance, and data transformation with goal to reduce data proliferation\nBreak down features into work that aligns with the architectural direction runway\nParticipate hands-on in pilots and proofs-of-concept for new patterns\nCreate robust documentation from data analysis and profiling, and proposed designs and data logic\nDevelop advanced sql queries to profile, and unify data\nDevelop data processing code in sql, along with semantic views to prepare data for reporting\nDevelop PowerBI Models and reporting packages\nDesign robust data models, and processing layers, that support both analytical processing and operational reporting needs.\nDesign and develop solutions based on best practices for data governance, security, and compliance within Databricks and Power BI environments.\nEnsure the integration of data systems with other enterprise applications, creating seamless data flows across platforms.\nDevelop and maintain Power BI solutions, ensuring data models and reports are optimized for performance and scalability.\nCollaborate with key customers to define data requirements, functional specifications, and project goals.\nContinuously evaluate and adopt new technologies and methodologies to enhance the architecture and performance of data solutions.\n\n\nWhat we expect of you\nWe are all different, yet we all use our unique contributions to serve patients. The R&D Data Catalyst Team is responsible for building Data Searching, Cohort Building, and Knowledge Management tools that provide the Amgen scientific community with visibility to Amgens wealth of human datasets, projects and study histories, and knowledge over various scientific findings. These solutions are pivotal tools in Amgens goal to accelerate the speed of discovery, and speed to market of advanced precision medications.\nBasic Qualifications:\nMasters degree and 1 to 3 years of Data Engineering experience OR\nBachelors degree and 3 to 5 years of Data Engineering experience OR\nDiploma and 7 to 9 years of Data Engineering experience\nMust Have Skills:\nMinimum of 3 years of hands-on experience with BI solutions (Preferable Power BI or Business Objects) including report development, dashboard creation, and optimization.\nMinimum of 3 years of hands-on experience building Change-data-capture (CDC) ETL pipelines, data warehouse design and build, and enterprise-level data management.\nHands-on experience with Databricks, including data engineering, optimization, and analytics workloads.\nDeep understanding of Power BI, including model design, DAX, and Power Query.\nProven experience designing and implementing data mastering solutions and data governance frameworks.\nExpertise in cloud platforms (AWS), data lakes, and data warehouses.\nStrong knowledge of ETL processes, data pipelines, and integration technologies.\nGood communication and collaboration skills to work with cross-functional teams and senior leadership.\nAbility to assess business needs and design solutions that align with organizational goals.\nExceptional hands-on capabilities with data profiling, data transformation, data mastering\nSuccess in mentoring and training team members\nGood to Have Skills:\nITIL Foundation or other relevant certifications (preferred)\nSAFe Agile Practitioner (6.0)\nMicrosoft Certified: Data Analyst Associate (Power BI) or related certification.\nDatabricks Certified Professional or similar certification.\nSoft Skills:\nExcellent analytical and troubleshooting skills\nDeep intellectual curiosity\nThe highest degree of initiative and self-motivation\nStrong verbal and written communication skills, including presentation to varied audiences of complex technical/business topics\nConfidence technical leader\nAbility to work effectively with global, remote teams, specifically including using of tools and artifacts to assure clear and efficient collaboration across time zones\nAbility to handle multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals\nStrong problem solving, analytical skills;\nAbility to learn quickly and retain and synthesize complex information from diverse sources.",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Engineering', 'data management', 'Power BI', 'data governance', 'data warehousing', 'Databricks', 'ETL', 'AWS']",2025-06-12 14:47:51
ETL Developer - Data & Analytics,CANPACK,3 - 5 years,Not Disclosed,['Pune'],"Giorgi Global Holdings, Inc. ( GGH ) is a privately held, diversified consumer products/packaging company with approximately 11,000 employees and operations in 20 countries. GGH consists of four US based companies ( The Giorgi Companies ) and one global packaging company ( CANPACK ).\nGGH has embarked on a transformation journey to become a digital, technology enabled, customer-centric, data and insights-driven organization. This transformation is evolving our business, strategy, core operations and IT solutions.\nAs an ETL Developer, you will be an integral part of our Data and Analytics team, working closely with the ETL Architect and other developers to design, develop, and maintain efficient data integration and transformation solutions. We are looking for a highly skilled ETL Developer with a deep understanding of ETL processes and data warehousing. The ideal candidate is passionate about optimizing data extraction, transformation, and loading workflows, ensuring high performance, accuracy, and scalability to support business intelligence initiatives.\nWhat you will do:\n1. Design, develop, test and maintain ETL processes and data pipelines to support data integration and transformation needs.\n2. Continuously improve ETL performance and reliability through best practices and optimization techniques.\n3. Develop and implement data validation and quality checks to ensure the integrity and consistency of data.\n4. Collaborate with ETL Architect, Data Engineers, and Business Intelligence teams to understand business requirements and translate them into technical solutions.\n5. Monitor, troubleshoot, and resolve ETL job failures, performance bottlenecks, and data discrepancies.\n6. Proactively identify and resolve ETL-related issues, minimizing impact on business operations.\n7. Contribute to documentation, training, and knowledge sharing to enhance team capabilities.\n8. Communicate progress and challenges clearly to both technical and non-technical teams\nEssential Requirements:\nBachelor s or master s degree in information technology, Computer Science, or a related field.\n3-5 years of relevant experience.\nPower-BI, Tabular Editor/Dax Studio, ALM/Github/Azure Devops skills\nExposure to SAP Systems/Modules like SD, MD, etc. to understand functional data.,\nExposure to MS Fbric, MS Azure Synapse Analytics\nCompetencies needed:\n- Hands-on experience with ETL development and data integration for large-scale systems\n- Experience with platforms such as Synapse Analytics, Azure Data Factory, Fabric, Redshift or Databricks\n- A solid understanding of data warehousing and ETL processes\n- Advanced SQL and PL/SQL skills such as query optimization, complex joins, window functions\n- Expertise in Python (pySpark) programming with a focus on data manipulation and analysis\n- Experience with Azure DevOps and CI/CD process\n- Excellent problem-solving and analytical skills\n- Experience in creating post-implementation documentation\n- Strong team collaboration skills\n- Attention to detail and a commitment to quality\nStrong interpersonal skills including analytical thinking, creativity, organizational abilities, high commitment, initiative in task execution, and a fast-learning capability for understanding IT concepts\n\nIf you are a current CANPACK employee, please apply through your Workday account .",Industry Type: Beverage,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'SAP', 'Analytical', 'Packaging', 'PLSQL', 'Business intelligence', 'Information technology', 'Analytics', 'Python', 'Data extraction']",2025-06-12 14:47:54
Data Analytics Mgr,Amgen Inc,4 - 6 years,Not Disclosed,['Hyderabad'],"What you will do\n\n\nIn this vital role you will report to the Organizational Planning Analytics & Insights Procurement & Sourcing Lead, you will support Amgens Tech & Workforce Strategy by applying business analytics and change leadership skills to drive insights that impact resource allocation and sourcing strategy.\n\nYour responsibilities include dashboard development, ad-hoc reporting, business partnering & engagement, and financial baselining. This role supports organizational change and enables the development of an integrated approach to global sourcing and financial planning.\n\nReporting to the Organizational Planning Analytics & Insights Procurement & Sourcing Lead, you will support Amgens Tech & Workforce Strategy by applying business analytics and change management skills to drive insights that impact resource allocation and sourcing strategy.\n\nYour responsibilities include dashboard development, ad-hoc reporting, business partnering & engagement, and financial baselining. This role supports change management and enables the development of an integrated approach to global sourcing and financial planning.\n\nRoles & Responsibilities:\nAddressing business challenges through process evaluation and insight generation.\nDevelop insights with a strong focus on Tableau and Power BI dashboard creation, as well as PowerPoint presentations.\nGuide data analysts and data engineers on standard methodologies for building data pipelines to support dashboards and other business objectives.\nConduct ad hoc analyses of FP&A and sourcing/procurement data.\nAddressing business challenges through process evaluation and insight generation.\nDevelop insights with a strong focus on Tableau and Power BI dashboard creation, as well as PowerPoint presentations.\nGuide data analysts and data engineers on standard methodologies for building data pipelines to support dashboards and other business objectives.\nConduct ad hoc analyses of FP&A and sourcing/procurement data.\nWhat we expect of you\n\nWe are all different, yet we all use our unique contributions to serve patients.\n\nBasic Qualifications:\nMasters degree and 4 to 6 years of applicable experience in business analysis (finance analysis, data analysis, sourcing analysis, or similar experience OR\nBachelors degree and 6 to 8 years of applicable experience in business analysis (finance analysis, data analysis, sourcing analysis, or similar experience OR\nDiploma and 10 to 12 years of applicable experience in business analysis (finance analysis, data analysis, sourcing analysis, or similar experience\nPreferred Qualifications:\nMasters degree in data science, business, statistics, data mining, applied mathematics, business analytics, engineering, computer science, or a related field\n4 years of relevant experience in data science, data analytics, consulting, and/or financial planning & analysis.\nA keen eye for design, with the ability to craft engaging PowerPoint decks and develop compelling Power BI and Tableau dashboards.\nProven expertise in statistical/mathematical modeling and working with structured/unstructured data.\nExperience with procurement, sourcing, and/or financial planning data.\nSkilled in automating data workflows using tools like Tableau, Python, R, Alteryx, and PowerApps.\nKnowledge of global finance systems, Procurement, and sourcing operations.\nExperience with data analysis, budgeting, forecasting, and strategic planning in the Bio-Pharmaceutical or biotech industry.\nGrowing in a start-up environment, building a data-driven transformation capability.\nUnderstanding of the Bio-Pharmaceutical and biotech industry trends and operations.\nProven ability to engage with cross-functional business leaders to align data strategies with corporate objectives, redefining complex data insights into actionable strategies.\nFlexible work models, including remote work arrangements, where possible\n\nAs we work to develop treatments that deal with others, we also work to care for your professional and personal growth and well-being. From our competitive benefits to our collaborative culture, well support your journey every step of the way.",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Analysis', 'data analytics', 'data science', 'mathematical modeling', 'financial planning', 'financial planning and analysis', 'statistics']",2025-06-12 14:47:56
Assoc. Data Engineer - R&D Precision Medicine Team,Amgen Inc,1 - 3 years,Not Disclosed,['Hyderabad'],"What you will do\n\n\nThe R&D Precision Medicine team is responsible for Data Standardization, Data Searching, Cohort Building, and Knowledge Management tools that provide the Amgen scientific community with access to Amgens wealth of human datasets, projects and study histories, and knowledge over various scientific findings. These data include clinical data, omics, and images. These solutions are pivotal tools in Amgens goal to accelerate the speed of discovery, and speed to market of advanced precision medications.\n\nThe Data Engineer will be responsible for full stack development of enterprise analytics and data mastering solutions leveraging Databricks and Power BI. This role requires expertise in both data architecture and analytics, with the ability to create scalable, reliable, and high-performing enterprise solutions that support research cohort-building and advanced AI pipelines. The ideal candidate will have experience creating and surfacing large unified repositories of human data, based on integrations from multiple repositories and solutions, and be exceptionally skilled with data analysis and profiling.\n\nYou will collaborate closely with partners, product team members, and related IT teams, to design and implement data models, integrate data from various sources, and ensure best practices for data governance and security. The ideal candidate will have a solid background in data warehousing, ETL, Databricks, Power BI, and enterprise data mastering.\n\nRoles & Responsibilities\nDesign and build scalable enterprise analytics solutions using Databricks, Power BI, and other modern data management tools.\nLeverage data virtualization, ETL, and semantic layers to balance need for unification, performance, and data transformation with goal to reduce data proliferation\nBreak down features into work that aligns with the architectural direction runway\nParticipate hands-on in pilots and proofs-of-concept for new patterns\nCreate robust documentation from data analysis and profiling, and proposed designs and data logic\nDevelop advanced sql queries to profile, and unify data\nDevelop data processing code in sql, along with semantic views to prepare data for reporting\nDevelop PowerBI Models and reporting packages\nDesign robust data models, and processing layers, that support both analytical processing and operational reporting needs.\nDesign and develop solutions based on best practices for data governance, security, and compliance within Databricks and Power BI environments.\nEnsure the integration of data systems with other enterprise applications, creating seamless data flows across platforms.\nDevelop and maintain Power BI solutions, ensuring data models and reports are optimized for performance and scalability.\nCollaborate with partners to define data requirements, functional specifications, and project goals.\nContinuously evaluate and adopt new technologies and methodologies to enhance the architecture and performance of data solutions.\n\n\n\nWhat we expect of you\nWe are all different, yet we all use our unique contributions to serve patients. The professional we seek is someone with these qualifications.\n\nBasic Qualifications:\nMasters degree with 1 to 3 years of experience in Data Engineering OR\nBachelors degree with 1 to 3 years of experience in Data Engineering\nMust-Have\n\nSkills:\nMinimum of 1 year of hands-on experience with BI solutions (Preferrable Power BI or Business Objects) including report development, dashboard creation, and optimization.\nMinimum of 1 year of hands-on experience building Change-data-capture (CDC) ETL pipelines, data warehouse design and build, and enterprise-level data management.\nHands-on experience with Databricks, including data engineering, optimization, and analytics workloads.\nExperience using cloud platforms (AWS), data lakes, and data warehouses.\nWorking knowledge of ETL processes, data pipelines, and integration technologies.\nGood communication and collaboration skills to work with cross-functional teams and senior leadership.\nAbility to assess business needs and design solutions that align with organizational goals.\nExceptional hands-on capabilities with data profiling and data anlysis\nGood-to-Have\n\nSkills:\nExperience with human data, ideally human healthcare data\nFamiliarity with laboratory testing, patient data from clinical care, HL7, FHIR, and/or clinical trial data management\nProfessional Certifications:\nITIL Foundation or other relevant certifications (preferred)\nSAFe Agile Practitioner (6.0)\nMicrosoft CertifiedData Analyst Associate (Power BI) or related certification.\nDatabricks Certified Professional or similar certification.\nSoft\n\nSkills:\nExcellent analytical and troubleshooting skills\nDeep intellectual curiosity\nHighest degree of initiative and self-motivation\nStrong verbal and written communication skills, including presentation to varied audiences of complex technical/business topics\nConfidence technical leader\nAbility to work effectively with global, virtual teams, specifically including using of tools and artifacts to assure clear and efficient collaboration across time zones\nAbility to manage multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals\nStrong problem solving, analytical skills;\nAbility to learn quickly and retain and synthesize complex information from diverse sources",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Engineering', 'data lakes', 'data pipelines', 'ETL processes', 'AWS', 'data warehouses', 'BI solutions']",2025-06-12 14:47:58
"Director, Enterprise Data Architecture",Horizon Therapeutics,10 - 12 years,Not Disclosed,['Hyderabad'],"Career Category Engineering Job Description\nABOUT AMGEN\nAmgen harnesses the best of biology and technology to fight the world s toughest diseases, and make people s lives easier, fuller and longer. We discover, develop, manufacture and deliver innovative medicines to help millions of patients. Amgen helped establish the biotechnology industry more than 40 years ago and remains on the cutting-edge of innovation, using technology and human genetic data to push beyond what s known today.\nABOUT THE ROLE\nRole Description:\nThe Director for Data Architecture and Solutions will lead Amgen s enterprise data architecture and solutions strategy, overseeing the design, integration, and deployment of scalable, secure, and future-ready data systems. This leader will define the architectural vision and guide a high-performing team of architects and technical experts to implement data and analytics solutions that drive business value and innovation.\nThis role demands a strong blend of business acumen, deep technical expertise, and strategic thinking to align data capabilities with the companys mission and growth. The Director will also serve as a key liaison with executive leadership, influencing technology investment and enterprise data direction\n.\nRoles & Responsibilities:\nDevelop and own the enterprise data architecture and solutions roadmap, aligned with Amgen s business strategy and digital transformation goals.\nProvide executive leadership and oversight of data architecture initiatives across business domains (R&D, Commercial, Manufacturing, etc.).\nLead and grow a high-impact team of data and solution architects. Coach, mentor, and foster innovation and continuous improvement in the team.\nDesign and promote modern data architectures (data mesh, data fabric, lakehouse etc.) across hybrid cloud environments and enable for AI readiness.\nCollaborate with stakeholders to define solution blueprints, integrating business requirements with technical strategy to drive value.\nDrive enterprise-wide adoption of data modeling, metadata management, and data lineage standards.\nEnsure solutions meet enterprise-grade requirements for security, performance, scalability, compliance, and data governance.\nPartner closely with Data Engineering, Analytics, AI/ML, and IT Security teams to operationalize data solutions that enable advanced analytics and decision-making.\nChampion innovation and continuous evolution of Amgen s data and analytics landscape through new technologies and industry best practices.\nCommunicate architectural strategy and project outcomes to executive leadership and other non-technical stakeholders.\nFunctional Skills:\nMust-Have Skills:\n10+ years of experience in data architecture or solution architecture leadership roles, including experience at the enterprise level.\nProven experience leading architecture strategy and delivery in the life sciences or pharmaceutical industry.\nExpertise in cloud platforms (AWS, Azure, or GCP) and modern data technologies (data lakes, APIs, ETL/ELT frameworks).\nStrong understanding of data governance, compliance (e.g., HIPAA, GxP), and data privacy best practices.\nDemonstrated success managing cross-functional, global teams and large-scale data programs.\nExperience with enterprise architecture frameworks (TOGAF, Zachman, etc.).\nProven leadership skills with a track record of managing and mentoring high-performing data architecture teams.\nGood-to-Have Skills:\nMaster s or doctorate in Computer Science, Engineering, or related field.\nCertifications in cloud architecture (AWS, GCP, Azure).\nExperience integrating AI/ML solutions into enterprise Data Achitecture.\nFamiliarity with DevOps, CI/CD pipelines, and Infrastructure as Code (Terraform, CloudFormation).\nScaled Agile or similar methodology experience.\nLeadership and Communication Skills:\nStrategic thinker with the ability to influence at the executive level.\nStrong executive presence with excellent communication and storytelling skills.\nAbility to lead in a matrixed, global environment with multiple stakeholders.\nHighly collaborative, proactive, and business-oriented mindset.\nStrong organizational and prioritization skills to manage complex initiatives.\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nBasic Qualifications:\nDoctorate degree and 2 years of Information Systems experience, or\nMaster s degree and 6 years of Information Systems experience, or\nBachelor s degree and 8 years of Information Systems experience, or\nAssociates degree and 10 years of Information Systems experience, or\n4 years of managerial experience directly managing people and leadership experience leading teams, projects, or programs.\n.",Industry Type: Biotechnology,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Solution architecture', 'metadata', 'Data modeling', 'Enterprise architecture', 'TOGAF', 'HIPAA', 'Agile', 'Analytics', 'Data architecture']",2025-06-12 14:48:00
IN Senior Associate AWS DataOps Engineer,PwC Service Delivery Center,4 - 8 years,Not Disclosed,['Bengaluru'],"Not Applicable\nSpecialism\nSAP\nManagement Level\nSenior Associate\n& Summary\nAt PwC, our people in data and analytics engineering focus on leveraging advanced technologies and techniques to design and develop robust data solutions for clients. They play a crucial role in transforming raw data into actionable insights, enabling informed decisionmaking and driving business growth.\n\nIn data engineering at PwC, you will focus on designing and building data infrastructure and systems to enable efficient data processing and analysis. You will be responsible for developing and implementing data pipelines, data integration, and data transformation solutions.\nWhy PWC\nAt PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purposeled and valuesdriven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us .\n& Summary We are looking for a seasoned AWS DataOps Engineer\nResponsibilities\nDesign, implement, and manage scalable data pipelines and ETL processes on AWS. Collaborate with data teams to understand requirements and translate them into robust data solutions. Proven experience with AWS data services such as S3, Redshift, RDS, Glue, and Lambda. Strong understanding of data architecture, data modeling, and data warehousing concepts. Strong programming and scripting skills in languages like Python, SQL, or Shell scripting. Experience with data pipeline and ETL tools, such as Apache Airflow or AWS Data Pipeline. Ensure data quality, integrity, and security through automated testing, monitoring, and alerting systems. Optimize data storage and retrieval using AWS services such as S3, Redshift, RDS, and DynamoDB. Implement data governance and compliance standards to ensure data privacy and security. Automate data integration and deployment processes using tools like AWS Data Pipeline, Glue, and Step Functions. Monitor and troubleshoot data workflows to ensure reliability and performance. Provide technical support and guidance to data teams on best practices for data management and operations.\nMandatory skill sets\n(AWS, Azure, GCP) services such as GCP BigQuery, Dataform AWS Redshift, Python\nPreferred skill sets\nDevops\nYears of experience\nrequired\n48 Years\nEducation qualification BE/B.Tech/MBA/MCA/M.Tech\nEducation\nDegrees/Field of Study required Master of Business Administration, Master of Engineering, Bachelor of Engineering\nDegrees/Field of Study preferred\nRequired Skills\nDevOps\nAccepting Feedback, Accepting Feedback, Active Listening, Agile Scalability, Amazon Web Services (AWS), Analytical Thinking, Apache Airflow, Apache Hadoop, Azure Data Factory, Communication, Creativity, Data Anonymization, Data Architecture, Database Administration, Database Management System (DBMS), Database Optimization, Database Security Best Practices, Databricks Unified Data Analytics Platform, Data Engineering, Data Engineering Platforms, Data Infrastructure, Data Integration, Data Lake, Data Modeling, Data Pipeline {+ 27 more}\nTravel Requirements\nGovernment Clearance Required?",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SAP', 'Data management', 'Data modeling', 'Shell scripting', 'Database administration', 'Agile', 'Apache', 'Technical support', 'SQL', 'Python']",2025-06-12 14:48:03
IN Senior Associate AWS DataOps Engineer,PwC Service Delivery Center,4 - 8 years,Not Disclosed,['Bengaluru'],"Not Applicable\nSpecialism\nSAP\nManagement Level\nSenior Associate\n& Summary\nAt PwC, our people in data and analytics engineering focus on leveraging advanced technologies and techniques to design and develop robust data solutions for clients. They play a crucial role in transforming raw data into actionable insights, enabling informed decisionmaking and driving business growth.\n\nIn data engineering at PwC, you will focus on designing and building data infrastructure and systems to enable efficient data processing and analysis. You will be responsible for developing and implementing data pipelines, data integration, and data transformation solutions.\nWhy PWC\nAt PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purposeled and valuesdriven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us .\n& Summary We are looking for a seasoned AWS DataOps Engineer\nResponsibilities\nDesign, implement, and manage scalable data pipelines and ETL processes on AWS. Collaborate with data teams to understand requirements and translate them into robust data solutions. Proven experience with AWS data services such as S3, Redshift, RDS, Glue, and Lambda. Strong understanding of data architecture, data modeling, and data warehousing concepts. Strong programming and scripting skills in languages like Python, SQL, or Shell scripting. Experience with data pipeline and ETL tools, such as Apache Airflow or AWS Data Pipeline. Ensure data quality, integrity, and security through automated testing, monitoring, and alerting systems. Optimize data storage and retrieval using AWS services such as S3, Redshift, RDS, and DynamoDB. Implement data governance and compliance standards to ensure data privacy and security. Automate data integration and deployment processes using tools like AWS Data Pipeline, Glue, and Step Functions. Monitor and troubleshoot data workflows to ensure reliability and performance. Provide technical support and guidance to data teams on best practices for data management and operations.\nMandatory skill sets\n(AWS, Azure, GCP) services such as GCP BigQuery, Dataform AWS Redshift, Python\nPreferred skill sets\nDevops\nYears of experience\nrequired\n48 Years\nEducation qualification BE/B.Tech/MBA/MCA/M.Tech\nEducation\nDegrees/Field of Study required Master of Business Administration, Master of Engineering, Bachelor of Engineering\nDegrees/Field of Study preferred\nRequired Skills\nDevOps\nAccepting Feedback, Accepting Feedback, Active Listening, Agile Scalability, Amazon Web Services (AWS), Analytical Thinking, Apache Airflow, Apache Hadoop, Azure Data Factory, Communication, Creativity, Data Anonymization, Data Architecture, Database Administration, Database Management System (DBMS), Database Optimization, Database Security Best Practices, Databricks Unified Data Analytics Platform, Data Engineering, Data Engineering Platforms, Data Infrastructure, Data Integration, Data Lake, Data Modeling, Data Pipeline {+ 27 more}\nTravel Requirements\nGovernment Clearance Required?",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SAP', 'Data management', 'Data modeling', 'Shell scripting', 'Database administration', 'Agile', 'Apache', 'Technical support', 'SQL', 'Python']",2025-06-12 14:48:06
Architect (Data Engineering),Amgen Inc,9 - 12 years,Not Disclosed,['Hyderabad'],"ABOUT THE ROLE\n\nRole Description:\n\nWe are seeking a Data Solutions Architect with deep expertise in Biotech/Pharma to design, implement, and optimize scalable and high-performance data solutions that support enterprise analytics, AI-driven insights, and digital transformation initiatives. This role will focus on data strategy, architecture, governance, security, and operational efficiency, ensuring seamless data integration across modern cloud platforms. The ideal candidate will work closely with engineering teams, business stakeholders, and leadership to establish a future-ready data ecosystem, balancing performance, cost-efficiency, security, and usability. This position requires expertise in modern cloud-based data architectures, data engineering best practices, and Scaled Agile methodologies.\n\nRoles & Responsibilities:\nDesign and implement scalable, modular, and future-proof data architectures that initiatives in enterprise.\nDevelop enterprise-wide data frameworks that enable governed, secure, and accessible data across various business domains.\nDefine data modeling strategies to support structured and unstructured data, ensuring efficiency, consistency, and usability across analytical platforms.\nLead the development of high-performance data pipelines for batch and real-time data processing, integrating APIs, streaming sources, transactional systems, and external data platforms.\nOptimize query performance, indexing, caching, and storage strategies to enhance scalability, cost efficiency, and analytical capabilities.\nEstablish data interoperability frameworks that enable seamless integration across multiple data sources and platforms.\nDrive data governance strategies, ensuring security, compliance, access controls, and lineage tracking are embedded into enterprise data solutions.\nImplement DataOps best practices, including CI/CD for data pipelines, automated monitoring, and proactive issue resolution, to improve operational efficiency.\nLead Scaled Agile (SAFe) practices, facilitating Program Increment (PI) Planning, Sprint Planning, and Agile ceremonies, ensuring iterative delivery of enterprise data capabilities.\nCollaborate with business stakeholders, product teams, and technology leaders to align data architecture strategies with organizational goals.\nAct as a trusted advisor on emerging data technologies and trends, ensuring that the enterprise adopts cutting-edge data solutions that provide competitive advantage and long-term scalability.\nMust-Have\n\nSkills:\nExperience in data architecture, enterprise data management, and cloud-based analytics solutions.\nWell versed in domain of Biotech/Pharma industry and has been instrumental in solving complex problems for them using data strategy.\nExpertise in Databricks, cloud-native data platforms, and distributed computing frameworks.\nStrong proficiency in modern data modeling techniques, including dimensional modeling, NoSQL, and data virtualization.\nExperience designing high-performance ETL/ELT pipelines and real-time data processing solutions.\nDeep understanding of data governance, security, metadata management, and access control frameworks.\nHands-on experience with CI/CD for data solutions, DataOps automation, and infrastructure as code (IaC).\nProven ability to collaborate with cross-functional teams, including business executives, data engineers, and analytics teams, to drive successful data initiatives.\nStrong problem-solving, strategic thinking, and technical leadership skills.\nExperienced with SQL/NOSQL database, vector database for large language models\nExperienced with data modeling and performance tuning for both OLAP and OLTP databases\nExperienced with Apache Spark, Apache Airflow\nExperienced with software engineering best-practices, including but not limited to version control (Git, Subversion, etc.), CI/CD (Jenkins, Maven etc.), automated unit testing, and Dev Ops\nGood-to-Have\n\nSkills:\nExperience with Data Mesh architectures and federated data governance models.\nCertification in cloud data platforms or enterprise architecture frameworks.\nKnowledge of AI/ML pipeline integration within enterprise data architectures.\nFamiliarity with BI & analytics platforms for enabling self-service analytics and enterprise reporting.\nEducation and Professional Certifications\n9 to 12 years of experience in Computer Science, IT or related field\nAWS Certified Data Engineer preferred\nDatabricks Certificate preferred\nSoft\n\nSkills:\nExcellent analytical and troubleshooting skills.\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nTeam-oriented, with a focus on achieving team goals.\nAbility to learn quickly, be organized and detail oriented.\nStrong presentation and public speaking skills.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'continuous integration', 'technical leadership', 'metadata management', 'presentation skills', 'ci/cd', 'distributed computing', 'sql', 'data bricks', 'git', 'data modeling', 'spark', 'devops', 'data governance', 'jenkins', 'troubleshooting', 'access control', 'etl']",2025-06-12 14:48:08
Data Architect,Calibo,12 - 16 years,Not Disclosed,[],"About the Role:\n\nWe are looking for a highly skilled Data Engineering Architect with strong Data Engineering pipeline implementation experience to serve as the lead Solution/Technical Architect and Subject Matter Expert for customer experience data solutions across multiple data sources. The ideal candidate will collaborate with the Enterprise Architect and the client IT team to establish and implement strategic initiatives.\n\nResponsibilities and Technical Skills:\n12+ years of relevant experience in designing and Architecting ETL, ELT, Reverse ETL, Data Management or Data Integration, Data Warehouse, Data Lake, and Data Migration.\nMust have expertise in building complex ETL pipelines and large Data Processing, Data Quality and Data security\nExperience in delivering quality work on time with multiple, competing priorities.\nExcellent troubleshooting and problem-solving skills must be able to consistently identify critical elements, variables and alternatives to develop solutions.\nExperience in identifying, analyzing and translating business requirements into conceptual, logical and physical data models in complex, multi-application environments.\nExperience with Agile and Scaled Agile Frameworks.\nExperience in identifying and documenting data integration issues, and challenges such as duplicate data, non-conformed data, and unclean data. Multiple platform development experience.\nStrong experience in performance tuning of ETL processes using Data Platforms\nMust have experience in handling Data formats like Delta Tables, Parquet files, Iceberg etc.\nExperience in Cloud technologies such as AWS/Azure or Google Cloud.\nApache Spark design and development experience using Scala, Java, Python or Data Frames with Resilient Distributed Datasets (RDDs).\nDevelopment experience in databases like Oracle, AWS Redshift, AWS RDS, Postgres Databricks and/or Snowflake.\nHands-on professional work experience with Python is highly desired.\nExperience in Hadoop ecosystem tools for real-time or batch data ingestion.\nStrong communication and teamwork skills to interface with development team members, business analysts, and project management. Excellent analytical skills.\nIdentification of data sources, internal and external, and defining a plan for data management as per business data strategy.\nCollaborating with cross-functional teams for the smooth functioning of the enterprise data system.\nManaging end-to-end data architecture, from selecting the platform, designing the technical architecture, and developing the application to finally testing and implementing the proposed solution.\nPlanning and execution of big data solutions using Databricks, Big Data, Hadoop, Big Query, Snowflake, MongoDB, DynamoDB, PostgreSQL and SQL Server\nHands-on experience in defining and implementing various Machine Learning models for different business needs.\nIntegrating technical functionality, ensuring data accessibility, accuracy, and security.\nProgramming / Scripting Languages like Python / Java / Go, Microservices\nMachine Learning / AI tools like Scikit-learn / TensorFlow / PyTorch",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['cloud', 'ETL', 'AWS', 'Data Handling', 'Spark']",2025-06-12 14:48:10
Data Engineering Manager,Amgen Inc,8 - 12 years,Not Disclosed,['Hyderabad'],"ABOUT THE ROLE\n\n\nRole Description:\n\nWe are seeking a seasoned Engineering Manager (Data Engineering) to lead the end-to-end management of enterprise data assets and operational data workflows. This role is critical in ensuring the availability, quality, consistency, and timeliness of data across platforms and functions, supporting analytics, reporting, compliance, and digital transformation initiatives. You will be responsible for the day-to-day data operations, manage a team of data professionals, and drive process excellence in data intake, transformation, validation, and delivery. You will work closely with cross-functional teams including data engineering, analytics, IT, governance, and business stakeholders to align operational data capabilities with enterprise needs.\n\nRoles & Responsibilities:\nLead and manage the enterprise data operations team, responsible for data ingestion, processing, validation, quality control, and publishing to various downstream systems.\nDefine and implement standard operating procedures for data lifecycle management, ensuring accuracy, completeness, and integrity of critical data assets.\nOversee and continuously improve daily operational workflows, including scheduling, monitoring, and troubleshooting data jobs across cloud and on-premise environments.\nEstablish and track key data operations metrics (SLAs, throughput, latency, data quality, incident resolution) and drive continuous improvements.\nPartner with data engineering and platform teams to optimize pipelines, support new data integrations, and ensure scalability and resilience of operational data flows.\nCollaborate with data governance, compliance, and security teams to maintain regulatory compliance, data privacy, and access controls.\nServe as the primary escalation point for data incidents and outages, ensuring rapid response and root cause analysis.\nBuild strong relationships with business and analytics teams to understand data consumption patterns, prioritize operational needs, and align with business objectives.\nDrive adoption of best practices for documentation, metadata, lineage, and change management across data operations processes.\nMentor and develop a high-performing team of data operations analysts and leads.\nFunctional\n\nSkills:\nMust-Have Skills:\nExperience managing a team of data engineers in biotech/pharma domain companies.\nExperience in designing and maintaining data pipelines and analytics solutions that extract, transform, and load data from multiple source systems.\nDemonstrated hands-on experience with cloud platforms (AWS) and the ability to architect cost-effective and scalable data solutions.\nExperience managing data workflows in cloud environments such as AWS, Azure, or GCP.\nStrong problem-solving skills with the ability to analyze complex data flow issues and implement sustainable solutions.\nWorking knowledge of SQL, Python, or scripting languages for process monitoring and automation.\nExperience collaborating with data engineering, analytics, IT operations, and business teams in a matrixed organization.\nFamiliarity with data governance, metadata management, access control, and regulatory requirements (e.g., GDPR, HIPAA, SOX).\nExcellent leadership, communication, and stakeholder engagement skills.\nWell versed with full stack development & DataOps automation, logging frameworks, and pipeline orchestration tools.\nStrong analytical and problem-solving skills to address complex data challenges.\nEffective communication and interpersonal skills to collaborate with cross-functional teams.\nGood-to-Have\n\nSkills:\nData Engineering Management experience in Biotech/Life Sciences/Pharma\nExperience using graph databases such as Stardog or Marklogic or Neo4J or Allegrograph, etc.\nEducation and Professional Certifications\nDoctorate Degree with 3-5 + years of experience in Computer Science, IT or related field\nOR\nMasters degree with 6 - 8 + years of experience in Computer Science, IT or related field\nOR\nBachelors degree with 10 - 12 + years of experience in Computer Science, IT or related field\nAWS Certified Data Engineer preferred\nDatabricks Certificate preferred\nScaled Agile SAFe certification preferred\nSoft\n\nSkills:\nExcellent analytical and troubleshooting skills\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation\nAbility to manage multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals\nStrong presentation and public speaking skills",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'fullstack development', 'logging framework', 'stakeholder engagement', 'troubleshooting', 'cloud platforms']",2025-06-12 14:48:12
Azure Data Engineer,CODERZON Technologies Pvt Ltd,3 - 8 years,6-18 Lacs P.A.,['Kochi'],"Looking for a Data Engineer with 3+ yrs exp in Azure Data Factory, Synapse, Data Lake, Databricks, SQL, Python, Spark, CI/CD. Preferred: DP-203 cert, real-time data tools (Kafka, Stream Analytics), data governance (Purview), Power BI.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Data Factory', 'Azure Synapse', 'Pyspark', 'Azure Databricks', 'Azure Data Lake', 'SQL Azure', 'Python']",2025-06-12 14:48:14
Data Science Professional,Algoleap Technologies,6 - 11 years,Not Disclosed,['Hyderabad'],"Job_Description"":""\nJob Title: Data Science CoE\nLocation: Hyderabad, India (Hybrid)\nExperience: 6+ years\nRole Type: Full-time\nStart Date : Immediate\nAbout the Role:\nAs we build our Data Science Center of Excellence (CoE), we are looking for an entrepreneurial and technically strong Data Science Lead who can lay the foundation for a high-performing team. You will work directly with stakeholders across multiple business units to define use cases, lead model development, and ensure successful deployment and value realization.",,,,"['customer analytics', 'Usage', 'data science', 'GCP', 'Machine learning', 'model development', 'Deployment', 'Stakeholder management', 'SQL', 'Python']",2025-06-12 14:48:17
Data Architect Telecom Domain databrick BSS OSS,fast growing Data Driven IT solutions an...,10 - 20 years,45-55 Lacs P.A.,"['Noida', 'Hyderabad', 'Gurugram']","Data Architect Telecom Domain\nTo design comprehensive data architecture and technical solutions specifically for telecommunications industry challenges, leveraging TMforum frameworks and modern data platforms. To work closely with customers, and technology partners to deliver data solutions that address complex telecommunications business requirements including customer experience management, network optimization, revenue assurance, and digital transformation initiatives.\nResponsibilities:\nDesign and articulate enterprise-scale telecom data architectures incorporating TMforum standards and frameworks, including SID (Shared Information/Data Model), TAM (Telecom Application Map), and eTOM (enhanced Telecom Operations Map)\nDevelop comprehensive data models aligned with TMforum guidelines for telecommunications domains such as Customer, Product, Service, Resource, and Partner management\nCreate data architectures that support telecom-specific use cases including customer journey analytics, network performance optimization, fraud detection, and revenue assurance\nDesign solutions leveraging Microsoft Azure and Databricks for telecom data processing and analytics\nConduct technical discovery sessions with telecom clients to understand their OSS/BSS architecture, network analytics needs, customer experience requirements, and digital transformation objectives\nDesign and deliver proof of concepts (POCs) and technical demonstrations showcasing modern data platforms solving real-world telecommunications challenges\nCreate comprehensive architectural diagrams and implementation roadmaps for telecom data ecosystems spanning cloud, on-premises, and hybrid environments\nEvaluate and recommend appropriate big data technologies, cloud platforms, and processing frameworks based on telecom-specific requirements and regulatory compliance needs.\nDesign data governance frameworks compliant with telecom industry standards and regulatory requirements (GDPR, data localization, etc.)\nStay current with the latest advancements in data technologies including cloud services, data processing frameworks, and AI/ML capabilities\nContribute to the development of best practices, reference architectures, and reusable solution components for accelerating proposal development\nQualifications:\nBachelor's or Master's degree in Computer Science, Telecommunications Engineering, Data Science, or a related technical field\n10+ years of experience in data architecture, data engineering, or solution architecture roles with at least 5 years in telecommunications industry\nDeep knowledge of TMforum frameworks including SID (Shared Information/Data Model), eTOM, TAM, and their practical implementation in telecom data architectures\nDemonstrated ability to estimate project efforts, resource requirements, and implementation timelines for complex telecom data initiatives\nHands-on experience building data models and platforms aligned with TMforum standards and telecommunications business processes\nStrong understanding of telecom OSS/BSS systems, network management, customer experience management, and revenue management domains\nHands-on experience with data platforms including Databricks, and Microsoft Azure in telecommunications contexts\nExperience with modern data processing frameworks such as Apache Kafka, Spark and Airflow for real-time telecom data streaming\nProficiency in Azure cloud platform and its respective data services with an understanding of telecom-specific deployment requirements\nKnowledge of system monitoring and observability tools for telecommunications data infrastructure\nExperience implementing automated testing frameworks for telecom data platforms and pipelines\nFamiliarity with telecom data integration patterns, ETL/ELT processes, and data governance practices specific to telecommunications\nExperience designing and implementing data lakes, data warehouses, and machine learning pipelines for telecom use cases\nProficiency in programming languages commonly used in data processing (Python, Scala, SQL) with telecom domain applications\nUnderstanding of telecommunications regulatory requirements and data privacy compliance (GDPR, local data protection laws)\nExcellent communication and presentation skills with ability to explain complex technical concepts to telecom stakeholders\nStrong problem-solving skills and ability to think creatively to address telecommunications industry challenges\nGood to have TMforum certifications or telecommunications industry certifications\nRelevant data platform certifications such as Databricks, Azure Data Engineer are a plus\nWillingness to travel as required\nif you will all or most of the criteria contact bdm@intellisearchonline.net M 9341626895",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Telecom Bss', 'Data Architect', 'Telecom OSS', 'ETOM', 'Data Bricks']",2025-06-12 14:48:19
Azure Data Factory,Swift Staffing,8 - 12 years,6.5-14 Lacs P.A.,"['Mumbai', 'Hyderabad', 'Pune']","Job Description:\n5+ years in data engineering with at least 2 years on Azure Synapse.\nStrong SQL, Spark, and Data Lake integration experience.\nFamiliarity with Azure Data Factory, Power BI, and DevOps pipelines.\nExperience in AMS or managed services environments is a plus.\nDetailed JD\nDesign, develop, and maintain data pipelines using Azure Synapse Analytics.\nCollaborate with customer to ensure SLA adherence and incident resolution.\nOptimize Synapse SQL pools for performance and cost.\nImplement data security, access control, and compliance measures.\nParticipate in calibration and transition phases with client stakeholders",Industry Type: Recruitment / Staffing,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Azure Data Factory', 'Azure Synapse', 'Data Engineering', 'Azure Databricks', 'SQL Azure', 'Power Bi', 'Devops']",2025-06-12 14:48:22
Big Data Developer,Techstar Group,7 - 10 years,Not Disclosed,['Hyderabad'],"Responsibilities of the Candidate :\n\n- Be responsible for the design and development of big data solutions. Partner with domain experts, product managers, analysts, and data scientists to develop Big Data pipelines in Hadoop\n\n- Be responsible for moving all legacy workloads to a cloud platform\n\n- Work with data scientists to build Client pipelines using heterogeneous sources and provide engineering services for data PySpark science applications\n\n- Ensure automation through CI/CD across platforms both in cloud and on-premises\n\n- Define needs around maintainability, testability, performance, security, quality, and usability for the data platform\n\n- Drive implementation, consistent patterns, reusable components, and coding standards for data engineering processes\n\n- Convert SAS-based pipelines into languages like PySpark, and Scala to execute on Hadoop and non-Hadoop ecosystems\n\n- Tune Big data applications on Hadoop and non-Hadoop platforms for optimal performance\n\n- Apply an in-depth understanding of how data analytics collectively integrate within the sub-function as well as coordinate and contribute to the objectives of the entire function.\n\n- Produce a detailed analysis of issues where the best course of action is not evident from the information available, but actions must be recommended/taken.\n\n- Assess risk when business decisions are made, demonstrating particular consideration for the firm's reputation and safeguarding Citigroup, its clients, and assets, by driving compliance with applicable laws, rules, and regulations, adhering to Policy, applying sound ethical judgment regarding personal behavior, conduct, and business practices, and escalating, managing and reporting control issues with transparency\n\nRequirements :\n\n- 6+ years of total IT experience\n\n- 3+ years of experience with Hadoop (Cloudera)/big data technologies\n\n- Knowledge of the Hadoop ecosystem and Big Data technologies Hands-on experience with the Hadoop eco-system (HDFS, MapReduce, Hive, Pig, Impala, Spark, Kafka, Kudu, Solr)\n\n- Experience in designing and developing Data Pipelines for Data Ingestion or Transformation using Java Scala or Python.\n\n- Experience with Spark programming (Pyspark, Scala, or Java)\n\n- Hands-on experience with Python/Pyspark/Scala and basic libraries for machine learning is required.\n\n- Proficient in programming in Java or Python with prior Apache Beam/Spark experience a plus.\n\n- Hand on experience in CI/CD, Scheduling and Scripting\n\n- Ensure automation through CI/CD across platforms both in cloud and on-premises\n\n- System level understanding - Data structures, algorithms, distributed storage & compute\n\n- Can-do attitude on solving complex business problems, good interpersonal and teamwork skills",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Big Data', 'Hive', 'Data Engineering', 'Data Pipeline', 'PySpark', 'Hadoop', 'Kafka', 'HDFS', 'Spark', 'Python']",2025-06-12 14:48:24
Snowflake Data Engineer,Epam Systems,5 - 10 years,Not Disclosed,['Chennai'],"Key Skills:\nSnowflake (Snow SQL, Snow PLSQL and Snowpark)\nStrong Python\nAirflow/DBT\nAny DevOps tools\nAWS/Azure Cloud Skills\n\nRequirements:\nLooking for engineer for information warehouse\nWarehouse is based on AWS/Azure, DBT, Snowflake.\nStrong programming experience with Python.\nExperience with workflow management tools like Argo/Oozie/Airflow.\nExperience in Snowflake modelling - roles, schema, databases\nExperience in data Modeling (Data Vault).\nExperience in design and development of data transformation pipelines using the DBT framework.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Snowflake', 'Python', 'Azure Cloud', 'AWS', 'SQL']",2025-06-12 14:48:26
Job opening For Data Warehouse + ADF + ETL,bct,3 - 6 years,Not Disclosed,['Pune'],"Greetings of the Day !!!\n\nWe have job opening for Data Warehouse + ADF + ETL with one of our Client .If you are interested for this role , kindly share update resume along with below details in this email id : shaswati.m@bct-consulting.com\n\nJob Description:\nSenior Data Engineer\nAs a Senior Data Engineer, you will support the European World Area using the Windows & Azure suite of Analytics & Data platforms. The focus of the role is on the technical aspects and implementation of data gathering, integration and database design.\nWe look forward to seeing your application!\nIn This Role, Your Responsibilities Will Be:\nData Ingestion and Integration: Collaborate with Product Owners and analysts to understand data requirements & design, develop, and maintain data pipelines for ingesting, transforming, and integrating data from various sources into Azure Data Services.\nMigration of existing ETL packages: Migrate existing SSIS packages to Synapse pipelines\nData Modelling: Assist in designing and implementing data models, data warehouses, and databases in Azure Synapse Analytics, Azure Data Lake Storage, and other Azure services.\nData Transformation: Develop ETL (Extract, Transform, Load) processes using SQL Server Integration Services (SSIS), Azure Synapse Pipelines, or other relevant tools to prepare data for analysis and reporting.\nData Quality and Governance: Implement data quality checks and data governance practices to ensure the accuracy, consistency, and security of data assets.\nMonitoring and Optimization: Monitor and optimize data pipelines and workflows for performance, scalability, and cost efficiency.\nDocumentation: Maintain comprehensive documentation of processes, including data lineage, data dictionaries, and pipeline schedules.\nCollaboration: Work closely with cross-functional teams, including data analysts, data scientists, and business stakeholders, to understand their data needs and deliver solutions accordingly.\nAzure Services: Stay updated on Azure data services and best practices to recommend and implement improvements in our data architecture and processes\nFor This Role, You Will Need:\n3-5 years of experience in Data Warehousing with On-Premises or Cloud technologies\nStrong practical experience of Synapse pipelines / ADF.\nStrong practical experience of developing ETL packages using SSIS.\nStrong practical experience with T-SQL or any variant from other RDBMS.\nGraduate degree educated in computer science or a relevant subject.\nStrong analytical and problem-solving skills.\nStrong communication skills in dealing with internal customers from a range of functional areas.\nWillingness to work flexible working hours according to project requirements.\nTechnical documentation skills.\nFluent in English.\nPreferred Qualifications that Set You Apart:\nOracle PL/SQL.\nExperience in working on Azure Services like Azure Synapse Analytics, Azure Data Lake.\nWorking experience with Azure DevOps paired with knowledge of Agile and/or Scrum methods of delivery.\nLanguages: French, Italian, or Spanish would be an advantage.\nAgile certification.\nThanks,\nShaswati",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['ADF', 'ETL', 'SSIS', 'Data ware house']",2025-06-12 14:48:28
Azure Data Engineer (Azure Databricks must),Fortune India 500 IT Services Firm,5 - 8 years,Not Disclosed,['Hyderabad'],"We are looking for an experienced Azure Data Engineer with strong expertise in Azure Databricks to join our data engineering team.\n\nMandatory skill- Azure Databricks\nExperience- 5 to 8 years\nLocation- Hyderabad\nKey Responsibilities:\nDesign and build data pipelines and ETL/ELT workflows using Azure Databricks and Azure Data Factory\nIngest, clean, transform, and process large datasets from diverse sources (structured and unstructured)\nImplement Delta Lake solutions and optimize Spark jobs for performance and reliability\nIntegrate Azure Databricks with other Azure services including Data Lake Storage, Synapse Analytics, and Event Hubs\n\n\n\nInterested candidates share your CV at himani.girnar@alikethoughts.com with below details\n\nCandidate's name-\nEmail and Alternate Email ID-\nContact and Alternate Contact no-\nTotal exp-\nRelevant experience-\nCurrent Org-\nNotice period-\nCCTC-\nECTC-\nCurrent Location-\nPreferred Location-\nPancard No-",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Azure Databricks', 'Azure Data Factory', 'Pyspark', 'Azure Data Lake', 'SQL']",2025-06-12 14:48:30
Fresher - Analyst,NLB Services,0 - 1 years,Not Disclosed,['Noida'],"Location: Noida\nShift Timings: Flexible (Rotational Shifts)\nAbout the Role:\nWe are seeking motivated and adaptable freshers to join our team in the Banking and Financial Services sector. This is an excellent opportunity for individuals looking to start their careers in a dynamic industry, with multiple process roles available, including research, remediation, keying, and data entry.\nKey Responsibilities:\nConduct research and analysis to support various banking and financial processes.\nPerform tasks such as remediation, keying, data entry, or other activities depending on the process requirements, which may involve check adjustments, returns, exceptions, or other related areas.\nEnsure data confidentiality and integrity throughout the workflow.\nWork collaboratively with team members and other departments to meet project deadlines.\nAdapt to assigned tasks within the process as per business needs.\nConsistently meet performance metrics and quality standards.\n\nTraining and Support:\nComprehensive process-specific training will be provided to help you succeed in your role.\nPerks and Benefits:\n2-way cab pick-up and drop-off facility for a stress-free commute.\nOne complimentary meal provided during the shift.\nExposure to multiple functional areas, enhancing your professional growth.\nSupportive work environment focused on learning and development.",Industry Type: Banking,"Department: BFSI, Investments & Trading","Employment Type: Full Time, Permanent","['Banking Process', 'Banking Operations']",2025-06-12 14:48:32
Business Analyst | Amazon Now,Amazon,2 - 7 years,Not Disclosed,['Bengaluru'],"Are you ready to embark on a thrilling journey in the realm of grocery e-commerce? Were on the lookout for a team member to work on our latest initiative, operating at the forefront of innovation in a dynamic, fast-paced environment. This role demands the agility to navigate analytics landscape across multiple functions seamlessly, the resilience to thrive in a fast paced environment, excitement to handle challenges head-on and excellence in analytical abilities.\n\nAs a Business Analyst, youll be deciphering our customers ever-evolving needs and shaping solutions that elevate their experience with Amazon.\n\nWere seeking someone who thrives on ambiguity, harnessing their first-principle problem-solving skills to drive impactful outcomes. Your ability to cultivate a customer-centric mindset, coupled with a penchant for out-of-the-box thinking, will be instrumental in navigating the complex landscape of our initiative.\n\nA successful candidate will possess:\nGood analytical and quantitative skills, leveraging data and metrics to inform strategic decisions.\nImpeccable attention to detail, adept at juggling multiple projects and priorities with finesse.\nA knack for thriving in a fast-paced, innovation-driven environment, where adaptability is key.\nClear and compelling communication skills, capable of articulating data insights to diverse stakeholders.\n\nIf youre ready to challenge the status quo, lead with innovation, and leave an indelible mark on the future of e-commerce, then we want to hear from you!\n\n\nResponsibilities:\nUnderstand the various operations across Amazon Now\nDesign and develop highly available dashboards and metrics using SQL, Quicksight, and Python\nUnderstand the requirements of stakeholders and map them with the data sources/data warehouse\nOwn the delivery and backup of periodic metrics, dashboards to the leadership team\nDraw inferences and conclusions, and create dashboards and visualizations of processed data, identify trends, anomalies\nExecute high priority (i.e. cross functional, high impact) projects to improve business performance across different verticals\nPerform business analysis and data queries using appropriate tools\nWork closely with internal stakeholders such as business teams, engineering teams, and partner teams and align them with respect to your focus area\nExecute analytical projects and understanding of analytical methods (forecasting, Machine Learning Techniques, etc.)\n\nAbout the team\nWe are building and scaling the 10 minute delivery service of Amazon Bachelors degree or equivalent\nExperience defining requirements and using data and metrics to draw business insights\nExperience with SQL or ETL\n2+ years of Excel or Tableau (data manipulation, macros, charts and pivot tables) experience\nKnowledge of Microsoft Excel at an advanced level, including: pivot tables, macros, index/match, vlookup, VBA, data links, etc.\nExperience with reporting and Data Visualization tools such as Quick Sight / Tableau / Power BI or other BI packages Experience using very large datasets",,,,"['Excel', 'Business analysis', 'VLOOKUP', 'Analytical', 'Machine learning', 'Forecasting', 'Macros', 'Analytics', 'SQL', 'Python']",2025-06-12 14:48:35
Analyst,Aegis Media,0 - 3 years,Not Disclosed,['Mumbai'],"The purpose of this role is to deliver analysis inline with client business objectives, goals, and to maintain, develop and exceed client performance targets.\nJob Description:\nKey responsibilities:Understands the client needs in specific. Ensures crisp communication with clients and work as an interface between team members and client counterpart. Discusses issues related to questionnaires with clients and suggest solutions for the sameUses specialised knowledge of market research tools / programming languages to understand the client requirements and build surveys/ deliver data tables as per the requirement with required quality and productivity levelsReviews project requirements and executes projects, under the direction of senior team members, per requirements by following the guidelines and deploying the tools/systems as applicableCreates and follows work allocation schedule and project plan\nLocation:\nDGS India - Mumbai - Goregaon Prism Tower\nBrand:\nMerkle\nTime Type:\nFull time\nContract Type:\nPermanent",Industry Type: Advertising & Marketing,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Senior Analyst', 'Programming', 'Market research', 'Deployment', 'Project planning']",2025-06-12 14:48:37
Business Analyst,Sapiens,4 - 9 years,Not Disclosed,['Bengaluru'],"Sapiens is on the lookout for a Business Analyst to become a key player in our Bangalore team. If you're a seasoned BA pro and ready to take your career to new heights with an established, globally successful company, this role could be the perfect fit.\nWorking Model: Our flexible work arrangement combines both remote and in-office work, optimizing flexibility and productivity.\nWhat you ll do:\nWork closely with customer to identify, analyse, validate and document business processes and functional requirements. Oversee proper implementation by providing functional specifications and acceptance criteria. Act as a liaison between the customer business users and the project development and testing team.\nUnderstand and document customer s functional and technical requirements, user stories, and acceptance scenarios.\nSpecialize in Sapiens ALIS application; Understand limitations and possibilities of the system and their implications on the business processes and functionality.\nInitiate and oversee project solution design\nProvide presentations and demonstrations on Product Features\nFunctional support to development teams in design processes\nFunctional support to the testing teams by preparing test scenarios and participate in system testing before releases to the customers\nWrite new requirements / User Stories documents, for new functionalities (CRs, new features, etc.).\nAssist Sapiens RI 2nd and 3rd line support representatives in analysing and reproducing incidents reported by the customer.\nParticipate in training activities of employees and customers.\nSupport the TW with updating product documentation.\nWhat to Have for this position.\nMust have Skills.\nEducation : BE and MBA - MUST\nExperience required is minimum 4+ years.\nExcellent analytical skills\nExperience with information systems (such as ERP) at a super user level\nExperience with Financial services / Systems. Excellent command of the insurance business (experience in the actuarial or insurance fields)- must, Reinsurance is advantage\nAbility to match between customer functional requirements and application system options/functionalities in an efficient way.\nInsurance business knowledge - an advantage\nExcellent communications skills English (mother tongue level) - must\nRepresentative\nWilling to travel extensively\nExperience with overseas customers.\nExcellent analytical skills\nExperience with core organizational product implementations\nAbility to match between customer functional requirements and application system options/functionalities in an efficient way.\nHaving experience in Insurance background and worked on insurance products are added advantage.",Industry Type: BPM / BPO,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Business Analysis', 'product documentation', 'ERP', 'product implementation', 'business process analysis']",2025-06-12 14:48:39
Business Analyst,Diageo,3 - 5 years,Not Disclosed,['Bengaluru'],"Job Description :\nJob Title: Business Analyst\nAbout us:\nWith over 200 brands sold in nearly 180 countries, we re the world s leading premium drinks company. Bring your passion and use your curiosity as you explore, collaborate, and innovate to build brands consumers love. Together with passionate people from all over the world, you ll test new insights, learn and grow, and unlock a brighter, more exciting future. Join us to create a career worth celebrating.\nAbout the Function:\n\nOur Digital and Technology (D&T) team are innovators, delivering ground-breaking solutions that will help shape the future of our iconic brands. Technology touches every part of our business, from the sourcing of sustainable ingredients to marketing and development of our online platforms. We utilise data insights to build competitive advantage, supporting our people to deliver value faster.\nOur D&T team includes some of the most dedicated digital professionals in the industry. Every day, we come together to push boundaries and innovate, shaping the digital solutions of tomorrow. Whatever your passion, we ll help you become the best you can be, creating career-defining work and delivering breakthrough thinking.\n\nAbout the team: TE\n\nAbout the role:\nIT Business Analyst role is responsible for delivering business value within customer value stream by partnering with commercial and project delivery team. You will work closely with the commercial team / market leads to identify key priority area of investments, drive Process excellence Initiatives and Projects across the markets and bring efficiencies and effectiveness in processes. The role requires excellent knowledge on sales force automation (SFA) tools, ecommerce solutions, data analytics and practicable working skills in business environment.\nCore element to this role is to support SFA product to ensure new and existing business users realise business value and ensure investment is aligned to overall organisational strategy. This role is based in India but with responsibilities spanning a cross multiple location where Diageo operates.\nRole Responsibilities:\nResponsible for overall end-to-end SFA support, customer satisfaction, and improvement projects aimed at improving customer experience and efficiency.\nWork with other teams to develop product roadmap plans, process improvement and innovation strategy for SFA, ecommerce and analytic tools.\nBe interface between the technical team and business and aid in requirement collection, refinement and prepare user story into backlog, development cycle and implement user acceptance testing.\nClosely work with global vendors and market specific vendors in the execution of service level agreement and service management of the application and infrastructure as per the contracts and carry out regular performance reviews.\nSupport in shaping up business case, conducting discovery activities and take up relevant process mapping/ value stream mapping techniques to identify gaps, inefficiencies and create lean and agile processes.\nDevelop measurement capability to supervise and repcritical metric KPI, SLAs, OKR for the implemented investment Vs benefit realized.\nCo-ordinate with external partners / vendors on development activities that require multi-functional undertaking.\nProvide regular updates on quality initiatives and process improvement projects to key project customers across the organization as required.\nExperience / skills required:\nBachelor s Degree or equivalent experience in IT, computer science, business or related field.\n3-5 years of professional experience in digital and technology space with working exposure in a reputable software development environment.\nLeadership capability that ensures you stay focused on priorities by adopting Diageo leadership behaviours namely collaborate efficiently, act critically, experiment and learn and be externally curious.\nDemonstrable understanding of quality and process improvement methodologies (Six Sigma, LEAN, ITIL)\nWorking experience with collaborators located over a geographical spread (remote working), from multiple cultures, and based internally and externally\nDemonstrable experience in agile project management domain and user story mapping, integration technologies is A MUST.\nGood understanding of the commercial / business knowledge in sales, marketing and finance.\nGood technical skills sets on salesforce automation, ecommerce, digital marketing, Gen AI , web development, data analytics is required.\nExcellent remote working knowledge, communication and customer management skills\nProficiency with agile development tools (ADO, JIRA etc), process mapping / documentation tools.\nFlexible Working Statement:\nFlexibility is key to our success. Talk to us about what flexibility means to you so that you re supported to own your wellbeing and balance your priorities from day one.\nRewards & Benefits Statement: TE\nDiversity statement:\nOur purpose is to celebrate life, every day, everywhere. And creating an inclusive culture, where everyone feels valued and that they can belong, is a crucial part of this. We adopt diversity in the broadest possible sense. This means that you ll be welcomed and celebrated for who you are just by being you. You ll be part of and help build and champion an inclusive culture that celebrates people of different gender, ethnicity, ability, age, sexual orientation, social class, educational backgrounds, experiences, mindsets, and more. Our ambition is to create the best performing, most trusted and respected consumer products companies in the world. Join us and help transform our business as we take our brands to the next level and build new ones as part of shaping the next generation of celebrations for consumers around the world.\nFeel inspired? Then this may be the opportunity for you.\nIf you require a reasonable adjustment, please ensure that you gather this information when you submit your application.\nWorker Type :\nRegular\nPrimary Location:\nBangalore Karle Town SEZ\nAdditional Locations :\n2025-05-28",Industry Type: Agriculture / Forestry / Fishing,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Service management', 'Automation', 'Process improvement', 'Web development', 'JIRA', 'Project delivery', 'Digital marketing', 'User acceptance testing', 'Six sigma']",2025-06-12 14:48:42
VKYC Analyst,Wikilabs India,0 - 5 years,Not Disclosed,['Mumbai (All Areas)( Kalina )'],"VKYC Analyst - Operations and Support\n\nKey Responsibilities:\nKYC applications timely review, with a feedback in line with laid down Bank Policy and Procedures\nEnsure that onboardings are originated in compliance with established policies and procedures\nAssist to enhance the quality of vKYC applications\nEnsure regulatory compliance\nMaintain working knowledge of relevant legislation, statutory instruments, codes of practice, and organization policies, and ensure adherence\n\nQualification Requirements:\n1+ year relevant experience preferably at a high-growth tech startup in the financial services space\nAbility to analyze documentation, as long as outstanding attention to details\nAbility to read large volumes of documents effectively and extract necessary information\nIn-depth understanding of the risk mitigation strategies (KYC)\nStrong in oral communication, analytical and report writing skills\nSense of teamwork, cooperative, and adaptable to change.\nAbility to work under pressure and problem-solving abilities\nBachelors Degree\nWilling to work Nights and Weekends on a rotating shift basis\n\nWe consider as a plus:\nprevious startup experience\nknowledge of the credit card or consumer finance industry in fintech",Industry Type: FinTech / Payments,"Department: Customer Success, Service & Operations","Employment Type: Full Time, Permanent","['Good Communication In English', 'Video Kyc', 'Document Verification', 'VKYC', 'Video Know Your Customer', 'Credit Cards']",2025-06-12 14:48:44
Hiring Business Analyst with MSTR or Dataiku - Bangalore/ Chennai !!!!,Tech Mahindra,8 - 13 years,Not Disclosed,"['Chennai', 'Bengaluru']","A Business Analyst in the Financial Crime Surveillance Operations (FCSO) Data & Reporting PO Team understands the core concepts, principles, processes or procedures of Data & MI.\nExperienced in using MSTR reports & Dataiku. The FCSO Business Analyst gathers requirements from various stakeholders and creates user stories for the squad to understand and take it for delivery. They must have strong analytical skills, understand the strategic framework & make sense of data.\n\n1.Core Business Analysis Skills\nRequirement Gathering\nDocumentation\nGap analysis\n\n2. Data & MI Expertise\nData Analysis\nData mapping & Metrics understanding\nFCSO Process knowledge (Good to have)\n\n3. Technical Skills\nQuery Databases\nFamiliarity with BI Tools like Dataiku, MSTR\n\n4. Agile & Delivery management\n\nUnderstanding of Scrum for collaborating with Squads\nUser Story Creation\nBacklog Management\nStakeholder Management",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Microstrategy', 'Business Analytics', 'Dataiku', 'Business Analysis']",2025-06-12 14:48:46
Senior/Lead MLops Engineer,Tiger Analytics,7 - 10 years,Not Disclosed,"['Hyderabad', 'Chennai', 'Bengaluru']","JOB DESCRIPTION\n\nSenior MLE / Architect MLE (ML Ops) Chennai / Bangalore / Hyderabad (Hybrid)\n\nWho we are Tiger Analytics is a global leader in AI and analytics, helping Fortune 1000 companies solve their toughest challenges. We offer fullstack AI and analytics services & solutions to empower businesses to achieve real outcomes and value at scale. We are on a mission to push the boundaries of what AI and analytics can do to help enterprises navigate uncertainty and move forward decisively. Our purpose is to provide certainty to shape a better tomorrow. Our team of 4000+ technologists and consultants are based in the US, Canada, the UK, India, Singapore and Australia, working closely with clients across CPG, Retail, Insurance, BFS, Manufacturing, Life Sciences, and Healthcare. Many of our team leaders rank in Top 10 and 40 Under 40 lists, exemplifying our dedication to innovation and excellence. We are a Great Place to Work-Certified (2022-24), recognized by analyst firms such as Forrester, Gartner, HFS, Everest, ISG and others. We have been ranked among the Best and Fastest Growing analytics firms lists by Inc., Financial Times, Economic Times and Analytics India Magazine.",,,,"['MLops', 'Azure', 'Snowflake', 'Deployment', 'Ci/Cd', 'Machine Learning']",2025-06-12 14:48:49
Business Analyst,Global Banking Organization,3 - 8 years,Not Disclosed,['Bengaluru'],"Key Skills: Marketing Analytics, Analytics, SQL, Python, Business Analysis, Predictive Analysis, Statistical Analysis.\nRoles and Responsibilities:\nGathers operational data from various cross-functional stakeholders to examine past business performance.\nIdentifies data patterns and trends, and provides insights to enhance business decision-making capability in business planning, process improvement, solution assessment, etc.\nRecommends actions for future developments and strategic business opportunities, as well as enhancements to operational policies.\nMay be involved in exploratory data analysis, confirmatory data analysis, and/or qualitative analysis.\nTranslates data into consumer or customer behavioral insights to drive targeting and segmentation strategies, and communicates clearly and effectively to business partners and senior leaders all findings.\nContinuously improves processes and strategies by exploring and evaluating new data sources, tools, and capabilities.\nWorks closely with internal and external business partners in building, implementing, tracking, and improving decision strategies.\nAppropriately assesses risk when business decisions are made, demonstrating particular consideration for the firm's reputation and safeguarding its clients and assets, by driving compliance with applicable laws, rules and regulations, adhering to policy, applying sound ethical judgment regarding personal behavior, conduct and business practices, and escalating, managing, and reporting control issues with transparency.\nExperience Requirement:\n3-8 years of relevant experience in business analytics, data analysis, or business intelligence roles.\nProven experience in using analytical tools such as SQL, Excel, Python, or R to extract and analyze data.\nHands-on experience with data visualization tools such as Tableau, Power BI, or similar platforms.\nExperience working in cross-functional teams and supporting decision-making through data-driven insights.\nStrong track record of identifying business trends and providing actionable recommendations based on data analysis.\nDemonstrated ability to handle multiple projects simultaneously with a strong attention to detail.\nEducation: B.Tech M.Tech (Dual), MCA, B.Tech.",Industry Type: Financial Services,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Marketing Analytics', 'Analytics', 'SQL', 'Python', 'Business Analysis', 'Statistical Analysis.', 'Predictive Analysis']",2025-06-12 14:48:51
Senior Engineer II,AMERICAN EXPRESS,8 - 13 years,Not Disclosed,['Bengaluru'],"Join Team Amex and lets lead the way together.\nAmerican Express is looking for Senior Engineers to contribute to the company s focus on building products, like @Work, to support our large and global corporate clients. @Work helps our clients manage their Corporate Card and Corporate Purchasing Card programs more efficiently online. From performing everyday administrative tasks and account maintenance, to accessing reports and utilizing reconciliation solutions, @Work enables fast, efficient and effective program management resulting in time and cost savings for our clients.",,,,"['Computer science', 'Administration', 'Career development', 'Maven', 'Finance', 'Reconciliation', 'MySQL', 'Workflow', 'Monitoring', 'SQL']",2025-06-12 14:48:53
"Post-Close Analyst, Granite",Altisource,1 - 7 years,Not Disclosed,['Bengaluru'],"Review and update construction documents in the system to ensure completeness and accuracy.\nMaintain and update construction budget details while placing inspection orders with field inspectors.\nAssign inspection orders to nearby inspectors and track their acceptance and scheduling within the stipulated timeframe.\nCommunicate effectively with lenders, contractors, and field inspectors via calls and emails to obtain missing or required documentation.\nCoordinate with stakeholders through email and phone communication to ensure order fulfillment.\nSchedule and manage site inspections with field inspectors across the United States, following up to expedite order completion.\nNotify clients about order status updates via emails and chat messages to ensure transparency and efficiency.\n\n\nGraduate\nExcellent attention to detail in reviewing documentation and identifying gaps.\nExcellent communication skills to interact with clients, contractors, and i",Industry Type: Real Estate,"Department: Customer Success, Service & Operations","Employment Type: Full Time, Permanent","['US mortgage', 'Construction', 'Senior Analyst', 'Inspection', 'Scheduling', 'Management', 'Budgeting']",2025-06-12 14:48:56
IN Senior Associate AWS Full Stack Developer,PwC Service Delivery Center,4 - 8 years,Not Disclosed,['Bengaluru'],"Not Applicable\nSpecialism\nSAP\nManagement Level\nSenior Associate\n& Summary\nAt PwC, our people in data and analytics engineering focus on leveraging advanced technologies and techniques to design and develop robust data solutions for clients. They play a crucial role in transforming raw data into actionable insights, enabling informed decisionmaking and driving business growth.\n\nIn data engineering at PwC, you will focus on designing and building data infrastructure and systems to enable efficient data processing and analysis. You will be responsible for developing and implementing data pipelines, data integration, and data transformation solutions.\n& Summary We are looking for a seasoned AWS Full Stack Developer\nResponsibilities\nDesign, develop, and deploy fullstack applications using AWS services such as Lambda, EC2, S3, and RDS.\nCollaborate with product managers, UX designers, and other stakeholders to gather requirements and translate them into technical solutions.\nImplement frontend interfaces using frameworks like React, Angular, or Vue.js, ensuring responsive and intuitive user experiences.\nDevelop backend services using Node.js, Python, Java, or similar languages, and integrate them with AWS services.\nImplement CI/CD pipelines and automated testing frameworks to enhance development efficiency and application reliability.\nExperience with database management, particularly with SQL databases like MySQL or PostgreSQL and NoSQL databases like DynamoDB.\nStrong understanding of DevOps practices and tools such as Jenkins, Docker, and Kubernetes.\nExcellent problemsolving skills and ability to work in a fastpaced, collaborative environment.\nMandatory skill sets\n(AWS, Azure, GCP) services such as GCP BigQuery, Dataform AWS Redshift, Python\nPreferred skill sets\nDevops\nYears of experience\nrequired\n48 Years\nEducation qualification\nBE/B.Tech/MBA/MCA/M.Tech\nEducation\nDegrees/Field of Study required Bachelor of Engineering, Master of Business Administration, Bachelor of Technology\nDegrees/Field of Study preferred\nRequired Skills\nAWS Devops\nAccepting Feedback, Accepting Feedback, Active Listening, Agile Scalability, Amazon Web Services (AWS), Analytical Thinking, Apache Airflow, Apache Hadoop, Azure Data Factory, Communication, Creativity, Data Anonymization, Data Architecture, Database Administration, Database Management System (DBMS), Database Optimization, Database Security Best Practices, Databricks Unified Data Analytics Platform, Data Engineering, Data Engineering Platforms, Data Infrastructure, Data Integration, Data Lake, Data Modeling, Data Pipeline {+ 27 more}\nTravel Requirements\nGovernment Clearance Required?",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SAP', 'Data modeling', 'Postgresql', 'MySQL', 'Database administration', 'Agile', 'DBMS', 'Apache', 'SQL', 'Python']",2025-06-12 14:48:58
IN Senior Associate AWS AI/ML/GenAI Developer,PwC Service Delivery Center,4 - 8 years,Not Disclosed,['Bengaluru'],"Not Applicable\nSpecialism\nSAP\nManagement Level\nSenior Associate\n& Summary\nAt PwC, our people in data and analytics engineering focus on leveraging advanced technologies and techniques to design and develop robust data solutions for clients. They play a crucial role in transforming raw data into actionable insights, enabling informed decisionmaking and driving business growth.\n\nIn data engineering at PwC, you will focus on designing and building data infrastructure and systems to enable efficient data processing and analysis. You will be responsible for developing and implementing data pipelines, data integration, and data transformation solutions.\n\n& Summary We are looking for a seasoned AWS\nAI/ML/GenAI Developer\nResponsibilities\nDesign and implement AI/ML/GenAI models using AWS services such as AWS Bedrock, SageMaker, Comprehend, Rekognition, and others.\nStrong programming skills in Python, R etc\nExperience with machine learning frameworks such as TensorFlow, PyTorch, or Scikitlearn.\nKnowledge of data preprocessing, feature engineering, and model evaluation techniques.\nDevelop and deploy generative AI solutions to solve complex business problems and improve operational efficiency.\nCollaborate with data scientists, engineers, and product teams to understand requirements and translate them into technical solutions.\nOptimize and finetune machine learning models for performance and scalability. Ensure the security, reliability, and scalability of AI/ML solutions by adhering to best practices.\nMaintain and update existing AI/ML models to ensure they meet evolving business needs.\nStay uptodate with the latest advancements in AI/ML and GenAI technologies and integrate relevant innovations into our solutions.\nProvide technical guidance and mentorship to junior developers and team members.\nExcellent problemsolving skills and ability to work in a fastpaced, collaborative environment.\nGood to have AWS Certified Machine Learning Specialty or other relevant AWS certifications.\nMandatory skill sets\n(AWS, Azure, GCP) services such as GCP BigQuery, Dataform AWS Redshift, Python\nPreferred skill sets\nDevops\nYears of experience\nrequired 48 Years\nEducation qualification\nBE/B.Tech/MBA/MCA/M.Tech\nEducation\nDegrees/Field of Study required Master of Business Administration, Bachelor of Engineering, Bachelor of Technology\nDegrees/Field of Study preferred\nRequired Skills\nAWS Devops\nAccepting Feedback, Accepting Feedback, Active Listening, Agile Scalability, Amazon Web Services (AWS), Analytical Thinking, Apache Airflow, Apache Hadoop, Azure Data Factory, Communication, Creativity, Data Anonymization, Data Architecture, Database Administration, Database Management System (DBMS), Database Optimization, Database Security Best Practices, Databricks Unified Data Analytics Platform, Data Engineering, Data Engineering Platforms, Data Infrastructure, Data Integration, Data Lake, Data Modeling, Data Pipeline {+ 27 more}\nTravel Requirements\nGovernment Clearance Required?",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SAP', 'GCP', 'Data modeling', 'Analytical', 'Machine learning', 'Agile', 'Data processing', 'Corporate advisory', 'Operations', 'AWS']",2025-06-12 14:49:01
IN Senior Associate AWS AI/ML/GenAI Developer,PwC Service Delivery Center,4 - 8 years,Not Disclosed,['Bengaluru'],"Not Applicable\nSpecialism\nSAP\nManagement Level\nSenior Associate\n& Summary\nAt PwC, our people in data and analytics engineering focus on leveraging advanced technologies and techniques to design and develop robust data solutions for clients. They play a crucial role in transforming raw data into actionable insights, enabling informed decisionmaking and driving business growth.\n\nIn data engineering at PwC, you will focus on designing and building data infrastructure and systems to enable efficient data processing and analysis. You will be responsible for developing and implementing data pipelines, data integration, and data transformation solutions.\n\n& Summary We are looking for a seasoned AWS\nAI/ML/GenAI Developer\nResponsibilities\nDesign and implement AI/ML/GenAI models using AWS services such as AWS Bedrock, SageMaker, Comprehend, Rekognition, and others.\nStrong programming skills in Python, R etc\nExperience with machine learning frameworks such as TensorFlow, PyTorch, or Scikitlearn.\nKnowledge of data preprocessing, feature engineering, and model evaluation techniques.\nDevelop and deploy generative AI solutions to solve complex business problems and improve operational efficiency.\nCollaborate with data scientists, engineers, and product teams to understand requirements and translate them into technical solutions.\nOptimize and finetune machine learning models for performance and scalability. Ensure the security, reliability, and scalability of AI/ML solutions by adhering to best practices.\nMaintain and update existing AI/ML models to ensure they meet evolving business needs.\nStay uptodate with the latest advancements in AI/ML and GenAI technologies and integrate relevant innovations into our solutions.\nProvide technical guidance and mentorship to junior developers and team members.\nExcellent problemsolving skills and ability to work in a fastpaced, collaborative environment.\nGood to have AWS Certified Machine Learning Specialty or other relevant AWS certifications.\nMandatory skill sets\n(AWS, Azure, GCP) services such as GCP BigQuery, Dataform AWS Redshift, Python\nPreferred skill sets\nDevops\nYears of experience\nrequired 48 Years\nEducation qualification\nBE/B.Tech/MBA/MCA/M.Tech\nEducation\nDegrees/Field of Study required Bachelor of Engineering, Bachelor of Technology, Master of Business Administration\nDegrees/Field of Study preferred\nRequired Skills\nAWS Devops\nAccepting Feedback, Accepting Feedback, Active Listening, Agile Scalability, Amazon Web Services (AWS), Analytical Thinking, Apache Airflow, Apache Hadoop, Azure Data Factory, Communication, Creativity, Data Anonymization, Data Architecture, Database Administration, Database Management System (DBMS), Database Optimization, Database Security Best Practices, Databricks Unified Data Analytics Platform, Data Engineering, Data Engineering Platforms, Data Infrastructure, Data Integration, Data Lake, Data Modeling, Data Pipeline {+ 27 more}\nTravel Requirements\nGovernment Clearance Required?",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SAP', 'GCP', 'Data modeling', 'Analytical', 'Machine learning', 'Agile', 'Data processing', 'Corporate advisory', 'Operations', 'AWS']",2025-06-12 14:49:03
Big Data Developer,Binary Infoways,6 - 10 years,12-20 Lacs P.A.,['Hyderabad'],"AWS (EMR, S3, Glue, Airflow, RDS, Dynamodb, similar)\nCICD (Jenkins or another)\nRelational Databases experience (any)\nNo SQL databases experience (any)\nMicroservices or Domain services or API gateways or similar\nContainers (Docker, K8s, similar)",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'AWS', 'Python', 'Airflow', 'Java', 'Big Data', 'EMR', 'SQL', 'Jenkins', 'Glue', 'SCALA', 'Big Data Technologies', 'Spark']",2025-06-12 14:49:05
Senior Lead business execution consultant,Wells Fargo,7 - 12 years,Not Disclosed,['Bengaluru'],"About this role:\nWells Fargo is seeking a Senior Lead business execution consultant\n\nIn this role, you will:\nAct as a Business Execution advisor to leadership to drive performance and initiatives, and develop and implement information delivery or presentations to key stakeholders and senior management",,,,"['Business execution', 'Business Implementation', 'Data Engineering', 'NLP', 'generative AI', 'Data Mining', 'machine learning', 'Strategic Planning', 'agentic AI']",2025-06-12 14:49:08
Azure Data Architect,Syren Technologies,10 - 18 years,Not Disclosed,[],"About Syren Cloud\n\nSyren Cloud Technologies is a cutting-edge company specializing in supply chain solutions and data engineering. Their intelligent insights, powered by technologies like AI and NLP, empower organizations with real-time visibility and proactive decision-making. From control towers to agile inventory management, Syren unlocks unparalleled success in supply chain management.\n\nRole Summary",,,,"['Pyspark', 'Azure', 'Architecture', 'Data Bricks']",2025-06-12 14:49:10
Principal - Data Architect,Affine Analytics,8 - 12 years,Not Disclosed,['Chennai'],"We are seeking a highly skilled Data Architect to design and implement robust, scalable, and secure data solutions on AWS Cloud. The ideal candidate should have expertise in AWS services, data modeling, ETL processes, and big data technologies, with hands-on experience in Glue, DMS, Python, PySpark, and MPP databases like Snowflake, Redshift, or Databricks.\nKey Responsibilities:\nArchitect and implement data solutions leveraging AWS services such as EC2, S3, IAM, Glue (Mandatory), and DMS for efficient data processing and storage.",,,,"['Python', 'S3', 'AWS Glue', 'DMS', 'SQL Server', 'Redshift', 'Glue', 'IAM', 'EC2', 'Snowflake', 'Databricks', 'Oracle', 'Lambda']",2025-06-12 14:49:12
Snowflake Developer with Azure Data Factory,Net Connect,6 - 10 years,6-11 Lacs P.A.,['Hyderabad'],Greetings from NCG!\n\nWe have a opening for Snowflake Developer role in Hyderabad office!\n\nBelow JD for your reference\n\nJob Description:,,,,"['Azure Data Factory', 'Snowflake', 'SQL']",2025-06-12 14:49:14
Data Architect (Data Bricks),Diacto Technologies Pvt Ltd,5 - 9 years,Not Disclosed,['Pune( Baner )'],"Job Overview:\nDiacto is seeking an experienced and highly skilled Data Architect to lead the design and development of scalable and efficient data solutions. The ideal candidate will have strong expertise in Azure Databricks, Snowflake (with DBT, GitHub, Airflow), and Google BigQuery. This is a full-time, on-site role based out of our Baner, Pune office.\n\nQualifications:\nB.E./B.Tech in Computer Science, IT, or related discipline\nMCS/MCA or equivalent preferred\n\nKey Responsibilities:\nDesign, build, and optimize robust data architecture frameworks for large-scale enterprise solutions\nArchitect and manage cloud-based data platforms using Azure Databricks, Snowflake, and BigQuery\nDefine and implement best practices for data modeling, integration, governance, and security\nCollaborate with engineering and analytics teams to ensure data solutions meet business needs\nLead development using tools such as DBT, Airflow, and GitHub for orchestration and version control\nTroubleshoot data issues and ensure system performance, reliability, and scalability\nGuide and mentor junior data engineers and developers\n\nExperience and Skills Required:\n5 to12 years of experience in data architecture, engineering, or analytics roles\nHands-on expertise in Databricks, especially Azure Databricks\nProficient in Snowflake, with working knowledge of DBT, Airflow, and GitHub\nExperience with Google BigQuery and cloud-native data processing workflows\nStrong knowledge of modern data architecture, data lakes, warehousing, and ETL pipelines\nExcellent problem-solving, communication, and analytical skills\n\nNice to Have:\nCertifications in Azure, Snowflake, or GCP\nExperience with containerization (Docker/Kubernetes)\nExposure to real-time data streaming and event-driven architecture\n\nWhy Join Diacto Technologies?\nCollaborate with experienced data professionals and work on high-impact projects\nExposure to a variety of industries and enterprise data ecosystems\nCompetitive compensation, learning opportunities, and an innovation-driven culture\nWork from our collaborative office space in Baner, Pune\nHow to Apply:\nOption 1 (Preferred)\n\nCopy and paste the following link on your browser and submit your application for the automated interview process: -\n\nhttps://app.candidhr.ai/app/candidate/gAAAAABoRrTQoMsfqaoNwTxsE_qwWYcpcRyYJk7NzSUmO3LKb6rM-8FcU58CUPYQKc65n66feHor-TGdCEfyouj0NmKdgYcNbA==/\n\nOption 2\n\n1. Please visit our website's career section at https://www.diacto.com/careers/\n2. Scroll down to the ""Who are we looking for?"" section\n3. Find the listing for "" Data Architect (Data Bricks)"" and\n4. Proceed with the virtual interview by clicking on ""Apply Now.""",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Snowflake', 'Azure Databricks', 'Airflow', 'Etl Pipelines', 'Github', 'google BigQuery', 'DBT', 'Data Security', 'Data Modeling', 'Elt', 'Data Governance']",2025-06-12 14:49:16
Business Analyst,Amdocs,2 - 4 years,Not Disclosed,['Pune'],"Amdocs helps those who build the future to make it amazing. With our market-leading portfolio of software products and services, we unlock our customers innovative potential, empowering them to provide next-generation communication and media experiences for both the individual end user and enterprise customers. Our employees around the globe are here to accelerate service providers migration to the cloud, enable them to differentiate in the 5G era, and digitalize and automate their operations. Listed on the NASDAQ Global Select Market, Amdocs had revenue of $5.00 billion in fiscal 2024. For more information, visit www.amdocs.com\n\n\nIn one sentence\n\nUnderstand the business needs of the customer and assess the impact of those needs in order to communicate and implement the recommended efficient solutions.\n\n\nWhat will your job look like\n""Lead domain-specific solutioning activities across solution and delivery engagements.\nAct as a trusted advisor to customers, providing deep expertise in your domain (e.g., Charging & Billing, CRM, Ordering, Catalog, Network Provisioning).\nDefine end-to-end domain solution and ensure alignment with customer business goals and operational strategies.\nCollaborate with Solution Architects, Business Analysts, and Product Managers for requirement feasibility and solution scope.\nProvide functional and technical support during design, integration, migration, and testing phases.\nIdentify domain risks, dependencies, and business impacts; recommend best practices and innovative approaches.""\n\n\nAll you need is...\nDegree in Computer Science or Industrial Engineering & Management - Information System.\nCustomer-facing experience - ability to communicate the Amdocs solution using various methods (presentations, demos, and so on).\nWide knowledge of relevant products and E2E Business process.\nKnowledge of the telecom industry and Amdocs business processes (ETOM, ASOM).\nExperience in managing a team in cross-Amdocs domain solutions.\n\n\nWhy you will love this job:\nUse your outstanding business analysis skills to make a significant impact on leading solutions that produce the most efficient product solutions.\nBe a key member of a global, dynamic and highly collaborative team with various possibilities for personal and professional development!\nYou will have the opportunity to work in multinational environment for the global market leader in its field.\nWe offer a wide range of stellar benefits including health, dental, vision, and life insurance as well as paid time off, sick time, and parental leave!\n\n",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['business analysis', 'demo', 'end user', 'operations strategy', 'acquisition', 'project management', 'program management', 'presentation skills', 'business analytics', 'business development', 'business planning', 'corporate planning', 'business consulting', 'telecom', 'crm']",2025-06-12 14:49:19
Analyst,Indegene,1 - 2 years,Not Disclosed,['Bengaluru'],"Title: Analyst\nDate: 10 Jun 2025\nLocation:\nBangalore, KA, IN\nJob Description\nWe are a technology-led healthcare solutions provider. We are driven by our purpose to enable healthcare organizations to be future-ready. We offer accelerated, global growth opportunities for talent that s bold, industrious, and nimble. With Indegene, you gain a unique career experience that celebrates entrepreneurship and is guided by passion, innovation, collaboration, and empathy. To explore exciting opportunities at the convergence of healthcare and technology, check out www.careers.indegene.com\n\nLooking to jump-start your career\nWe understand how important the first few years of your career are, which create the foundation of your entire professional journey. At Indegene, we promise you a differentiated career experience. You will not only work at the exciting intersection of healthcare and technology but also will be mentored by some of the most brilliant minds in the industry. We are offering a global fast-track career where you can grow along with Indegene s high-speed growth.\n\nWe are purpose-driven. We enable healthcare organizations to be future ready and our customer obsession is our driving force. We ensure that our customers achieve what they truly want. We are bold in our actions, nimble in our decision-making, and industrious in the way we work.\n\nIf this excites you, then apply below.\n\nRole: Analyst\n\nDescription: You will be responsible for:\nIdentify and report data defect for tag auditing.\ne2e test and release of any deliverable website/mobile app with the best quality possible.\nUnderstanding the client requirements related to tag validation in-depth and also from end-user point of view.\nUnderstanding the process and the underlying implementation.\nDesign and execute test scenarios on tag debugging tools to verify the quality of new curated data sets.\nMaintaining and enhancing existing data verifications techniques.\nWorking with implementation engineers to make automation testing an integral, efficient and scalable part of the data curation pipeline.\nMonitoring and documenting post-implementation problems and revision requests to ensure it meets end user needs.\nCompleting tasks / deliverables within schedule constraints (for self).\nMust Have\nKnowledge of Tealium iQ, Adobe Analytics & Google Analytics, tags and tag debugging tools like Charles Proxy.\nExperience in data-verification focused QA role.\nUnderstanding of HTML DOM Structure, CSS & Java Script.\nKnowledge of automated testing for web & mobile based testing.\nMinimum of 1-2 years of successful experience in an alike role.\nShould possess high analytical and problem-solving skills. Quite often the role will require an individual to impact assess critical and complex test scenarios related to data validation.\nShould be able to independently design and executing complex test cases, test scenarios etc.\nDefect articulation/reporting skills should be excellent.\nComplete understanding of web domain testing.\nTeam player who endorses collaborative work style with good communication skills.\nGood to have\nDeep expertise Manual Testing, Writing Test Plans, Test Cases & Strategies & Bug Identification.\nHands-on experience in testing Android, iOS Mobile apps and APIs preferably.\nKnowledge and experience of working in agile environment.\nExperience of tools like JIRA or likewise tool.\nEQUAL OPPORTUNITY",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Manual testing', 'DOM', 'Debugging', 'Agile', 'Healthcare', 'HTML', 'Test cases', 'JIRA', 'Monitoring', 'Android']",2025-06-12 14:49:21
Business Analyst,Barbeque Nation,6 - 11 years,8-12 Lacs P.A.,['Bengaluru( Sarjapur Road )'],"Corporate Finance and Investor relations role:\n\nKey Responsibilities:\n\nFinancial Analysis and Strategy:\nProvide strategic financial analysis to senior management to support decision-making, with a focus on profitability, growth, and market expansion.\nFinancial Modeling: Develop and maintain financial models to support business planning and decision-making\nReporting & Dashboards: Prepare monthly/quarterly financial reports, dashboards, and presentations for senior leadership\nCost Management: Analyze costs, profitability, and business performance to optimize financial outcomes\nScenario Analysis: Evaluate various business scenarios and their financial impact, providing data-driven recommendations\nWork closely with the finance team to track key performance indicators (KPIs), develop financial models, and assist in budget forecasting and financial planning\nAnalyze market trends and competitive positioning to inform strategic decisions and communicate findings to both internal stakeholders and investors\nInvestor Relations:\nLead the creation of investor presentations, quarterly earnings releases, investor briefings, annual report and ESG/BRSR reporting\nManage the flow of financial and strategic information to investors and analysts, ensuring transparency, accuracy, and timeliness\nRespond to investor inquiries and provide updates on corporate performance, strategy, and market developments\nBuild and maintain relationships with institutional investors, analysts, and stakeholders to effectively communicate the company's vision, strategy, and financial performance\nCorporate development and M&A Support:\nLead financial due diligence for potential mergers, acquisitions, and partnerships, collaborating with legal, finance, and business development teams\nAssist in assessing the financial health of potential targets, analyzing synergies, and preparing financial reports and recommendations\nConduct valuations and financial modeling to support the evaluation of potential investment opportunities",Industry Type: Hotels & Restaurants,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Business Analysis', 'Financial Analytics', 'Business Analytics', 'Business Modeling', 'Business Insights', 'Financial Modelling']",2025-06-12 14:49:24
Machine Learning Engineer,Panacorp Software Solutions,0 - 5 years,Not Disclosed,"['Nagercoil', 'Kanyakumari']","Research Programmer (Python/ MATLAB) Fresher & Experienced\nAbout Panacorp Software Solutions\nPanacorp Software Solutions is a research-driven organization specializing in providing technical assistance for PhD research projects. Our focus is on supporting research scholars with programming, simulations, and computational analysis in various domains, including AI, Machine Learning, and numerical computing.\n\nJob Role & Responsibilities\nAssist in research-based projects related to PhD studies.\nPerform simulations, numerical computing, and data analysis using Python, MATLAB, and Simulink.\nSupport research scholars in implementing Machine Learning (ML) and Deep Learning (DL) models.\nAutomate processes and optimize research workflows through scripting.\nDocument research methodologies, findings, and technical reports.\nWork closely with scholars to analyze and interpret computational results.\nEligibility Criteria\nQualification: BE/B.Tech/MCA\nExperience: 0 5+ years (Freshers with strong academic knowledge can apply).\nStrong understanding of research methodologies and computational tools.\nPreferred Skills\nProficiency in Python, MATLAB, and Simulink.\nKnowledge of data analysis, AI/ML techniques, and numerical simulations.\nAbility to interpret and validate research outcomes.\nStrong analytical and problem-solving skills.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Machine Learning', 'matlab', 'simulink', 'python', 'data analysis', 'research methodology', 'artificial intelligence']",2025-06-12 14:49:26
Python Senior Developer,Infosys,3 - 5 years,Not Disclosed,['Bengaluru'],"Job Title\nPython Senior Developer\n\nResponsibilities\nSolid development experience in Data Science Arch.\nExperience in Application Architecture & Design of Java Based Applications\nGood Knowledge of Architecture and related technologies\nExperience in Integration Technologies and Architecture\nWorking knowledge of frontend and database technologies\nExcellent Analytical and Debugging Skills\nFamiliarity with Agile & DevSecOps, Log Analytics, APM\nExperience in leading the teams technically\nExperience in requirements gathering, analysis & design and estimation\nGood communication and articulation skills Technical and Professional :\nWe are seeking a skilled Python and SQL Developer to join our dynamic team. The ideal candidate will have a strong background in Python programming and SQL database management.\nDevelop and maintain Python-based applications and scripts.\nWrite efficient SQL queries for data extraction and manipulation.\nCollaborate with cross-functional teams to gather requirements and deliver solutions.\nFamiliarity with Linux operating systems.\nBasic understanding of cloud platforms (e.g., AWS, Azure, Google Cloud).\nKnowledge of Model Quantization and Pruning\nExperience playing a Data Scientist role Preferred Skills:\nPython Technology-Open System-Open System- ALL-Python Technology-Full stack-Java Full stack-Frontend(Vue.js)+Enterprise layer(Python)+DB Additional Responsibilities:\nIn-depth knowledge of design issues and best practices\nSolid understanding of object-oriented programming\nFamiliar with various design, architectural patterns and software development process.\nExperience with both external and embedded databases\nCreating database schemas that represent and support business processes\nImplementing automated testing platforms and unit tests\nGood verbal and written communication skills\nAbility to communicate with remote teams in effective manner\nHigh flexibility to travelSoft Skills\nGood verbal & written communication skills articulate value of AI to business, project managers & other team members\nAbility to break complex problem into smaller problems and create hypothesis\nInnovation and experimentation Educational Master of Computer Science,Master Of Science,Master Of Technology,MCA,Bachelor Of Comp. Applications,Bachelor Of Computer Science,Bachelor of Engineering,Bachelor Of Technology Service LineApplication Development and Maintenance* Location of posting is subject to business requirements",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Python', 'Enterprise layer', 'software development', 'report generation', 'MIS', 'CI/CD', 'Java Full stack-Frontend', 'SDLC']",2025-06-12 14:49:28
Senior ML Compiler Engineer,Qualcomm,2 - 4 years,Not Disclosed,['Hyderabad'],"Job Area: Engineering Group, Engineering Group > Software Engineering\n\nGeneral Summary:\n\nInterested in accelerating machine learning and artificial intelligence on mobile devices for millions of usersCome join our team. We are building software platforms that enable users of Qualcomms silicon to construct optimized neural networks and machine learning algorithms. We are looking for software engineers with a machine learning or compiler background who will help us build these software platforms. In this role, you will construct and tune machine learning frameworks, build compilers and tools, and collaborate with Qualcomm hardware and software engineers to enable efficient usage of Qualcomms silicon for machine learning applications.\n\nMinimum qualifications:\nBachelors degree in Engineering, Information Systems, Computer Science, or related field.\nProgramming in C/C++\n2 to 4 years of software engineering or related work experience\n\n\nPreferred qualifications:\nExperience in machine learning frameworks such as MxNet/NNVM/TVM, Pytorch, Tensorflow, Caffe\n\nOR experience in compilers with an interest in machine learning\nDeep knowledge of software engineering\n\n\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 2+ years of Software Engineering or related work experience.\nOR\nMaster's degree in Engineering, Information Systems, Computer Science, or related field and 1+ year of Software Engineering or related work experience.\nOR\nPhD in Engineering, Information Systems, Computer Science, or related field.\n\n2+ years of academic or work experience with Programming Language such as C, C++, Java, Python, etc.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'machine learning', 'tensorflow', 'pytorch', 'software engineering', 'algorithms', 'c++', 'natural language processing', 'caffe', 'neural networks', 'mxnet', 'artificial intelligence', 'sql', 'deep learning', 'r', 'java', 'data science', 'computer vision', 'machine learning algorithms', 'ml']",2025-06-12 14:49:31
Data Science Consultant,Techf Solutions,8 - 13 years,22.5-30 Lacs P.A.,['Indore'],"As a Senior AI Developer/ AI Architect in the AI team, you will work and mentor a team of developers, working on the Fusion AI Team and its AI engine AI Talos alongside research in the space, such as large language models, simulations, & agentic AI.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['RAG architecture', 'Python', 'full-stack developer', 'GitHub', 'Hugging Face', 'Jira', 'Pytorch', 'Gen AI', 'Llama2', 'Docker', 'Pandas', 'Pydantic', 'Pyarrow', 'Scikit', 'Mistral AI']",2025-06-12 14:49:34
"AIML - Machine Learning Engineer, Search & AI",Apple,7 - 12 years,Not Disclosed,['Bengaluru'],"Our team is responsible for delivering next-generation Search and Question Answering systems across Apple products including Siri, Safari, Spotlight, and more. This is your chance to shape how people get information by leveraging your Search and applied machine learning expertise along with robust software engineering skills.You will collaborate with outstanding Search and AI engineers on large scale machine learning to improve Query Understanding, Retrieval, and Ranking, developing fundamental building blocks needed for AI powered experiences such as fine-tuning and reinforcement learning. This involves pushing the boundaries on document retrieval and ranking, developing sophisticated machine learning models, using embeddings and deep learning to understand the quality of matches. It also includes online learning to react quickly to change and natural language processing to understand queries. You will work with petabytes of data and combine information from multiple structured and unstructured sources to provide best results and accurate answers to satisfy users information-seeking needs.\n7+ years experience in shipping Search and Q&A technologies and ML systems\nExcellent programming skills in mainstream programming languages such as C++, Python, Scala, and Go\nExperience delivering tooling and frameworks to evaluate individual components and end-to-end quality\nStrong analytical skills to systematically identify opportunities to improve search relevance and answer accuracy\nExcellent communication skills and ability to work in a collaborative research environment\nPassion for building phenomenal products and curiosity to learn\nPreferred Qualifications\nBackground in Search Relevance and Ranking, Question Answering systems, Query Understanding, Personalization or Recommendation systems, or data-informed decision-making\nHands-on experience in Retrieval Augmented Generation, including developing, evaluating and enhancing for both retrievers and generative LLMs\nMS in AI, Machine Learning, Information Retrieval, Computer Science, Statistics, or a related field",Industry Type: Consumer Electronics & Appliances,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'QA', 'Analytical skills', 'C++', 'Machine learning', 'SCALA', 'query', 'Information retrieval', 'Natural language processing', 'Python']",2025-06-12 14:49:36
Senior Copy Writer- WFH,Aegis Softtech,6 - 7 years,6.5-9 Lacs P.A.,"['Hyderabad', 'Chennai', 'Bengaluru']","Immediate Openings for Senior Copy Writer (Permanent WFH)\n\nCopywriter - Job Description\n\nAbout Aegis :\nAegis Softtech is a global technology services firm delivering customized software solutions in AI, ML, Cloud, Data Engineering, CRM Consulting, and more. We work with tech leaders, enterprises, and fast-scaling startups to help them solve business problems with scalable, future-ready software.\n\nWe are building a lean, quality-first content team.\nIf you're a copywriter who knows how to write for humans, doesn't hide behind jargon, and loves shaping complex tech stories into simple, compelling narratives, lets work together.\n\nWhat Youll Do:\n1.Craft compelling, conversion-driven content across formats: website pages, landing pages, email copy, social posts, and ad creatives\n2. Own copy development for key service areas like AI/ML, Data & Cloud, and CRM solutions (Microsoft Dynamics, Salesforce, etc.)\n3. Translate technical inputs into benefit-focused, client-first messaging that aligns with our authoritative yet approachable voice\n4.Collaborate with the Content Lead, designers, and developers to align messaging across touchpoints.\n5. Edit and refine content for clarity, brevity, tone, and SEO, without diluting meaning.\n6.Bring consistency to brand tone across different regions and verticals.\n\nWhat Were Looking For\n> 5–8 years of proven experience as a copywriter, ideally in the B2B tech or SaaS industry.\n> Comfort working across multiple formats and content lengths—from short CTAs to full-service pages.\n> A storytelling mindset with a keen understanding of buyer psychology and content structure.\n> Strong grasp of SEO principles and how to write for both humans and search engines.\n> Ability to simplify complex tech ideas without dumbing them down.\n> Self-driven, collaborative, and comfortable with remote async work.\n\nWhy Join Us\n> Flexible remote work with a global team of tech thinkers and builders\n> A chance to influence messaging at a strategic level, not just execute briefs\n> Open and transparent communication culture\n> Opportunity to work closely with a Lead who values content quality as much as delivery speed.\n\nTo Apply:\nSend your resume, a short note about why this role speaks to you, and 2–3 samples that show your ability to:\nTranslate tech to value.\nBuild momentum with words.\nWrite with clarity and character.\nEmail your application to hr@aegissofttech.com with the subject line: Copywriter Application – [Your Name].",Industry Type: IT Services & Consulting,"Department: Content, Editorial & Journalism","Employment Type: Full Time, Permanent","['SEO Writing', 'technical content', 'copy writing', 'Content Writing', 'Content Strategy']",2025-06-12 14:49:38
Analyst - Rebates and Discount,UPL Limited,3 - 6 years,Not Disclosed,['Pune'],"Comply with organisation's finance and accounting policies for respective process.\n\nResponsible for the timely completion of activities as part of the Rebate & Discount function for the following activities.1. SAP Knowledge\n2. Microsoft Excel - Advnace\n3. Rebate Creation / Rebate Correction\n4. Business Knowledge about Sales\n5. Rebate Provision / Pricing\n6. English Communication\n7. Releasing of Schemes - Rebates\n8. Scheme Working\n9. Reporting MIS / Open Rebates\nAccountable for evaluating, reconciling and resolving complex accounting transactions and ensuring reconciliations of accounts.",Industry Type: Fertilizers / Pesticides / Agro chemicals,Department: Finance & Accounting,"Employment Type: Full Time, Permanent","['hr generalist activities', 'sap', 'employee relations', 'mis reporting', 'policies', 'hrsd', 'accounting', 'microsoft', 'reconciliation', 'sales', 'hr operations', 'excel', 'talent acquisition', 'employee engagement', 'induction', 'recruitment', 'mis', 'english', 'compensation', 'payroll', 'performance management', 'reporting', 'pricing']",2025-06-12 14:49:41
"Engineer, Principal/Manager - Machine Learning, AI",Qualcomm,8 - 13 years,Not Disclosed,['Bengaluru'],"General Summary:\nAs a leading technology innovator, Qualcomm pushes the boundaries of what's possible to enable next-generation experiences and drives digital transformation to help create a smarter, connected future for all. As a Qualcomm Systems Engineer, you will research, design, develop, simulate, and/or validate systems-level software, hardware, architecture, algorithms, and solutions that enables the development of cutting-edge technology. Qualcomm Systems Engineers collaborate across functional teams to meet and exceed system-level requirements and standards.\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 8+ years of Systems Engineering or related work experience.\nOR\nMaster's degree in Engineering, Information Systems, Computer Science, or related field and 7+ years of Systems Engineering or related work experience.\nOR\nPhD in Engineering, Information Systems, Computer Science, or related field and 6+ years of Systems Engineering or related work experience.\nPrincipal Engineer Machine Learning\nWe are looking for a Principal AI/ML Engineer with expertise in model inference, optimization, debugging, and hardware acceleration. This role will focus on building efficient AI inference systems, debugging deep learning models, optimizing AI workloads for low latency, and accelerating deployment across diverse hardware platforms.\nIn addition to hands-on engineering, this role involves cutting-edge research in efficient deep learning, model compression, quantization, and AI hardware-aware optimization techniques. You will explore and implement state-of-the-art AI acceleration methods while collaborating with researchers, industry experts, and open-source communities to push the boundaries of AI performance.\nThis is an exciting opportunity for someone passionate about both applied AI development and AI research, with a strong focus on real-world deployment, model interpretability, and high-performance inference.\nEducation & Experience:\n20+ years of experience in AI/ML development, with at least 5 years in model inference, optimization, debugging, and Python-based AI deployment.\nMasters or Ph.D. in Computer Science, Machine Learning, AI\nLeadership & Collaboration\nLead a team of AI engineers in Python-based AI inference development.\nCollaborate with ML researchers, software engineers, and DevOps teams to deploy optimized AI solutions.\nDefine and enforce best practices for debugging and optimizing AI models\nKey Responsibilities\nModel Optimization & Quantization\nOptimize deep learning models using quantization (INT8, INT4, mixed precision etc), pruning, and knowledge distillation.\nImplement Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT) for deployment.\nFamiliarity with TensorRT, ONNX Runtime, OpenVINO, TVM\nAI Hardware Acceleration & Deployment\nOptimize AI workloads for Qualcomm Hexagon DSP, GPUs (CUDA, Tensor Cores), TPUs, NPUs, FPGAs, Habana Gaudi, Apple Neural Engine.\nLeverage Python APIs for hardware-specific acceleration, including cuDNN, XLA, MLIR.\nBenchmark models on AI hardware architectures and debug performance issues\nAI Research & Innovation\nConduct state-of-the-art research on AI inference efficiency, model compression, low-bit precision, sparse computing, and algorithmic acceleration.\nExplore new deep learning architectures (Sparse Transformers, Mixture of Experts, Flash Attention) for better inference performance.\nContribute to open-source AI projects and publish findings in top-tier ML conferences (NeurIPS, ICML, CVPR).\nCollaborate with hardware vendors and AI research teams to optimize deep learning models for next-gen AI accelerators.\nDetails of Expertise:\nExperience optimizing LLMs, LVMs, LMMs for inference\nExperience with deep learning frameworks: TensorFlow, PyTorch, JAX, ONNX.\nAdvanced skills in model quantization, pruning, and compression.\nProficiency in CUDA programming and Python GPU acceleration using cuPy, Numba, and TensorRT.\nHands-on experience with ML inference runtimes (TensorRT, TVM, ONNX Runtime, OpenVINO)\nExperience working with RunTimes Delegates (TFLite, ONNX, Qualcomm)\nStrong expertise in Python programming, writing optimized and scalable AI code.\nExperience with debugging AI models, including examining computation graphs using Netron Viewer, TensorBoard, and ONNX Runtime Debugger.\nStrong debugging skills using profiling tools (PyTorch Profiler, TensorFlow Profiler, cProfile, Nsight Systems, perf, Py-Spy).\nExpertise in cloud-based AI inference (AWS Inferentia, Azure ML, GCP AI Platform, Habana Gaudi).\nKnowledge of hardware-aware optimizations (oneDNN, XLA, cuDNN, ROCm, MLIR, SparseML).\nContributions to open-source community\nPublications in International forums conferences journals",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Machine Learning', 'AWS Inferentia', 'Azure ML', 'AI/ML', 'ONNX Runtime', 'OpenVINO', 'GCP AI', 'TVM', 'XLA', 'MLIR', 'TensorRT', 'Python']",2025-06-12 14:49:43
Cloud Machine Learning LLM Serving Engineer,Qualcomm,2 - 7 years,Not Disclosed,['Bengaluru'],"Job Area: Engineering Group, Engineering Group > Software Engineering\n\nGeneral Summary:\n\nJD for Cloud Machine Learning LLM Serving engineer\n\nJob Overview:\n\nThe Qualcomm Cloud Computing team is developing hardware and software for Machine Learning solutions spanning the data center, edge, infrastructure, automotive market. We are seeking ambitious, bright, and innovative engineers with experience in machine learning framework development. Job activities span the whole product life cycle from early design to commercial deployment. The environment is fast-paced and requires cross-functional interaction daily so good communication, planning and execution skills are a must.\n\nKey Responsibilities\nImprove and optimize key Deep Learning models on Qualcomm AI 100.\nBuild deep learning framework extensions for Qualcomm AI 100 in upstream open-source repositories.\nImplement Kernels for AI workloads\nCollaborate and interact with internal teams to analyze and optimize training and inference for deep learning.\nBuild software tools and ecosystem around AI SW Stack.\nWork on vLLM, Triton, ExecuTorch, Inductor, TorchDynamo to build abstraction layers for inference accelerator.\nOptimize workloads for both scale-up (multi-SoC) and scale-out (multi-card) systems.\nOptimize the entire deep learning pipeline including graph compiler integration.\nApply knowledge of software engineering best practices.\n\n\nDesirable Skills and Aptitudes\nDeep Learning experience or knowledge- LLMs, Natural Language Processing, Vision, Audio, Recommendation systems.\nKnowledge of the structure and function of different components of Pytorch, TensorFlow software stacks.\nExcellent C/C++/Python programming and software design skills, including debugging, performance analysis, and test design.\nAbility to work independently, define requirements and scope, and lead your own development effort.\nWell versed with open-source development practices.\nStrong developer with a research mindset- strives to innovate.\nAvid problem solver- should be able to find solutions to key engineering and domain problems.\n\n\nKnowledge of tiling and scheduling a Machine learning operator is a plus.\nExperience in using C++ 14 (advanced features)\nExperience of profiling software and optimization techniques\nHands on experience writing SIMD and/or multi-threaded high-performance code is a plus.\nExperience of ML compiler, Auto-code generation (using MLIR) is a plus.\nExperiences to run workloads on large scale heterogeneous clusters is a plus.\nHands-on experience with CUDA, CUDNN is a plus.\n\n\nQualifications:\nBachelor's / Masters/ PHD degree in Engineering, Machine learning/ AI, Information Systems, Computer Science, or related field.\n2+ years Software Engineering or related work experience.\n2+ years experience with Programming Language such as C++, Python.\n\n\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'c++', 'c', 'software design', 'software engineering', 'cuda', 'natural language processing', 'scale', 'machine learning', 'artificial intelligence', 'deep learning', 'tensorflow', 'code generation', 'computer science', 'pytorch', 'debugging', 'machine learning algorithms', 'ml']",2025-06-12 14:49:46
"Senior Staff Engineer, Frontend React",Nagarro,10 - 13 years,Not Disclosed,['India'],"We're Nagarro.\n\nWe are a Digital Product Engineering company that is scaling in a big way! We build products, services, and experiences that inspire, excite, and delight. We work at scale across all devices and digital mediums, and our people exist everywhere in the world (18000+ experts across 38 countries, to be exact). Our work culture is dynamic and non-hierarchical. We're looking for great new colleagues. That's where you come in!\n\nREQUIREMENTS:\nTotal Experience 10+ years.\nHands on working experience in front-end or full-stack development experience, with building production apps in React.js and Next.js.\nHands-on expertise writing unit/integration tests for React components (Jest, React Testing Library, etc.)\nSolid grasp of state-management patterns and libraries (Redux, React Context, Zustand, etc.).\nStrong understanding of RESTful APIs, asynchronous programming (Promises, async/await), and modern build tools (Webpack, Vite, or Turbopack).\nPractical experience with Git, pull-request workflows, and collaborative development tools (GitHub, GitLab, Bitbucket).\nAdvanced proficiency in JavaScript (ES6+) and TypeScript.\nProblem-solving mindset with the ability to tackle complex data engineering challenges. \nStrong communication and teamwork skills, with the ability to mentor and collaborate effectively.\n\nRESPONSIBILITIES:\nWriting and reviewing great quality code\nUnderstanding the clients business use cases and technical requirements and be able to convert them in to technical design which elegantly meets the requirements\nMapping decisions with requirements and be able to translate the same to developers.\nIdentifying different solutions and being able to narrow down the best option that meets the clients requirements.\nDefining guidelines and benchmarks for NFR considerations during project implementation\nWriting and reviewing design document explaining overall architecture, framework, and high-level design of the application for the developers.\nReviewing architecture and design on various aspects like extensibility, scalability, security, design patterns, user experience, NFRs, etc., and ensure that all relevant best practices are followed.\nDeveloping and designing the overall solution for defined functional and non-functional requirements; and defining technologies, patterns, and frameworks to materialize it\nUnderstanding and relating technology integration scenarios and applying these learnings in projects\nResolving issues that are raised during code/review, through exhaustive systematic analysis of the root cause, and being able to justify the decision taken.\nCarrying out POCs to make sure that suggested design/technologies meet the requirements",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Javascript', 'React.Js', 'Nextjs', 'Typescript']",2025-06-12 14:49:48
"Senior Staff Engineer, QA Automation",Nagarro,10 - 13 years,Not Disclosed,['India'],"We're Nagarro.\nWe are a Digital Product Engineering company that is scaling in a big way! We build products, services, and experiences that inspire, excite, and delight. We work at scale across all devices and digital mediums, and our people exist everywhere in the world (18000+ experts across 38 countries, to be exact). Our work culture is dynamic and non-hierarchical. We're looking for great new colleagues. That's where you come in!\n\nREQUIREMENTS:\nTotal Experience 10+ years.\nStrong working experience in QA with a strong background in both manual and automation testing.\nHands-on experience with Selenium WebDriver, Appium, and Postman.\nSolid understanding of REST API testing and automation using tools like RestAssured.\nProficient in testing frameworks such as TestNG, JUnit, or Cypress.\nStrong experience in automation of mobile and web applications.\nFamiliarity with CI/CD tools like Jenkins, GitLab CI, or equivalent.\nWorking knowledge of bug tracking and test management tools (e.g., JIRA, TestRail).\nExperience with BDD frameworks like Cucumber.\nGood command of scripting or programming in Java, Python, or similar languages is a plus.\nProblem-solving mindset with the ability to tackle complex data engineering challenges.\nStrong communication and teamwork skills, with the ability to mentor and collaborate effectively.\nRESPONSIBILITIES:\nUnderstanding the clients business use cases and technical requirements and be able to convert them in to technical design which elegantly meets the requirements\nMapping decisions with requirements and be able to translate the same to developers.\nIdentifying different solutions and being able to narrow down the best option that meets the clients requirements.\nDefining guidelines and benchmarks for NFR considerations during project implementation\nWriting and reviewing design document explaining overall architecture, framework, and high-level design of the application for the developers.\nReviewing architecture and design on various aspects like extensibility, scalability, security, design patterns, user experience, NFRs, etc., and ensure that all relevant best practices are followed.\nDeveloping and designing the overall solution for defined functional and non-functional requirements; and defining technologies, patterns, and frameworks to materialize it\nUnderstanding and relating technology integration scenarios and applying these learnings in projects\nResolving issues that are raised during code/review, through exhaustive systematic analysis of the root cause, and being able to justify the decision taken.\nCarrying out POCs to make sure that suggested design/technologies meet the requirements",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['API Testing', 'Appium', 'QA Automation', 'Selenium', 'Postman']",2025-06-12 14:49:50
"Sr Software Eng: Generative AI, Go/Python, AWS, Kubernetes 7-12 Yrs",Cisco,7 - 12 years,Not Disclosed,['Bengaluru'],"Meet The Team\nThe Cisco AI Software & Platform Group drives the development of groundbreaking generative AI applications. We empower Cisco's diverse product portfolio, spanning networking and security, with intelligent assistants and agents. We work on pioneering technologies that proactively defend against threats, safeguard critical business assets, and simplify security operations. Fueled by a passion for AI/ML, we strive to create a secure future for businesses. Our collaborative and passionate team thrives with tackling sophisticated challenges and delivering innovative solutions.",,,,"['Golang', 'Generative Ai', 'AWS', 'Python', 'Kubernetes', 'Java']",2025-06-12 14:49:52
"Associate Staff Engineer, Frontend React",Nagarro,5 - 7 years,Not Disclosed,['Bengaluru'],"We're Nagarro.\n\nWe are a Digital Product Engineering company that is scaling in a big way! We build products, services, and experiences that inspire, excite, and delight. We work at scale across all devices and digital mediums, and our people exist everywhere in the world (18000+ experts across 38 countries, to be exact). Our work culture is dynamic and non-hierarchical. We're looking for great new colleagues. That's where you come in!\n\nREQUIREMENTS:\nTotal Experience 5+ years.\nHands on working experience in front-end or full-stack development experience, with building production apps in React.js and Next.js.\nHands-on expertise writing unit/integration tests for React components (Jest, React Testing Library, etc.)\nSolid grasp of state-management patterns and libraries (Redux, React Context, Zustand, etc.).\nStrong understanding of RESTful APIs, asynchronous programming (Promises, async/await), and modern build tools (Webpack, Vite, or Turbopack).\nPractical experience with Git, pull-request workflows, and collaborative development tools (GitHub, GitLab, Bitbucket).\nAdvanced proficiency in JavaScript (ES6+) and TypeScript.\nProblem-solving mindset with the ability to tackle complex data engineering challenges. \nStrong communication and teamwork skills, with the ability to mentor and collaborate effectively.\n\nRESPONSIBILITIES:\nWriting and reviewing great quality code\nUnderstanding the clients business use cases and technical requirements and be able to convert them in to technical design which elegantly meets the requirements\nMapping decisions with requirements and be able to translate the same to developers.\nIdentifying different solutions and being able to narrow down the best option that meets the clients requirements.\nDefining guidelines and benchmarks for NFR considerations during project implementation\nWriting and reviewing design document explaining overall architecture, framework, and high-level design of the application for the developers.\nReviewing architecture and design on various aspects like extensibility, scalability, security, design patterns, user experience, NFRs, etc., and ensure that all relevant best practices are followed.\nDeveloping and designing the overall solution for defined functional and non-functional requirements; and defining technologies, patterns, and frameworks to materialize it\nUnderstanding and relating technology integration scenarios and applying these learnings in projects\nResolving issues that are raised during code/review, through exhaustive systematic analysis of the root cause, and being able to justify the decision taken.\nCarrying out POCs to make sure that suggested design/technologies meet the requirements.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Typescript', 'Javascript', 'React.Js']",2025-06-12 14:49:54
Senior Consultanr - AI Cloud Engineer,AstraZeneca India Pvt. Ltd,5 - 10 years,Not Disclosed,['Chennai'],"Job Title: Senior Consultant - AI Cloud Engineer Career Level: D2 Introduction to role:\nAre you ready to tackle some of the most exciting machine-learning challenges in drug discovery? We are seeking a Senior AI Platform Engineer to join our innovative AI platform team, IGNITE. With your expertise in AWS cloud environments, youll design and deploy large-scale production infrastructure that will redefine healthcare and improve the lives of millions worldwide. As part of a close-knit team of technical specialists, youll create tools that support major AI initiatives, from clinical trial data analysis to imaging and Omics. Your role will be pivotal in providing frameworks for data scientists to develop scalable machine learning models safely and robustly. Are you prepared to bridge the gap between science and engineering with your deep expertise?\nAccountabilities:\nDesign, implement, and manage cloud infrastructure on AWS using Infrastructure as Code (IaC) tools such as Terraform or AWS CloudFormation.\nMaintain and enhance CI/CD pipelines using tools like GitHub Actions, AWS CodePipeline, Jenkins, or ArgoCD.\nEnsure platform reliability, scalability, and high availability across development, staging, and production environments.\nAutomate operational tasks, environment provisioning, and deployments using scripting languages such as Python, Bash, or PowerShell.\nEnable and maintain Amazon SageMaker environments for scalable ML model training, hosting, and pipelines.\nIntegrate AWS Bedrock to provide foundation model access for generative AI applications, ensuring security and cost control.\nLead and publish curated infrastructure templates through AWS Service Catalogue to enable consistent and compliant provisioning.\nCollaborate with security and compliance teams to implement best practices around IAM, encryption, logging, monitoring, and cost optimization.\nImplement and manage observability tools like Amazon CloudWatch, Prometheus/Grafana, or ELK for monitoring and alerting.\nSupport container orchestration environments using EKS (Kubernetes), ECS, or Fargate.\nContribute to incident response, post-mortems, and continuous improvement of the platform s operational excellence.\nEssential Skills/Experience:\nBachelor s degree in Computer Science, Engineering, or related field (or equivalent experience).\n5+ years of hands-on experience with AWS cloud services.\nStrong experience with Terraform, AWS CDK, or CloudFormation.\nProficiency in Linux system administration and networking fundamentals.\nSolid understanding of IAM policies, VPC design, security groups, and encryption.\nExperience with Docker and container orchestration using Kubernetes (EKS preferred).\nHands-on experience with CI/CD tools and version control (Git).\nExperience with monitoring, logging, and alerting systems.\nStrong solving skills and ability to work independently or in a team.\nDesirable Skills/Experience:\nAWS Certification (e.g., AWS Certified DevOps Engineer, Solutions Architect - Associate/Professional).\nExperience with serverless technologies like AWS Lambda, Step Functions, and EventBridge.\nExperience supporting machine learning or big data workloads on AWS.\nExperience with SAFe agile principles and practices.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Data analysis', 'Version control', 'Networking', 'Machine learning', 'Agile', 'Healthcare', 'Monitoring', 'Python', 'Recruitment']",2025-06-12 14:49:56
"Senior Staff Engineer, Mobile -Flutter",Nagarro,10 - 13 years,Not Disclosed,['India'],"We're Nagarro.\n\nWe are a Digital Product Engineering company that is scaling in a big way! We build products, services, and experiences that inspire, excite, and delight. We work at scale across all devices and digital mediums, and our people exist everywhere in the world (18000+ experts across 38 countries, to be exact). Our work culture is dynamic and non-hierarchical. We're looking for great new colleagues. That's where you come in!\n\nREQUIREMENTS:\nTotal Experience 10+ years.\nStrong working experience in Dart and Flutter.\nSolid understanding of Mobile App Architecture (MVVM, BLoC, Provider, GetX).\nExperience with local databases like Sqflite or alternatives.\nHands-on experience in unit testing and test automation for Flutter apps.\nProven experience in building and deploying apps to the App Store and Google Play Store.\nFamiliarity with Git, GitHub/GitLab, CI/CD tools (e.g., Jenkins, Bitrise, GitHub Actions).\nDeep knowledge of mobile app lifecycle, design principles, and clean architecture patterns (MVVM, MVC, etc.)\nExpertise in mobile app performance optimization and security best practices.\nExperience in API integration, RESTful services, and handling JSON data.\nProficient in Version Control Systems like Git.\nProblem-solving mindset with the ability to tackle complex data engineering challenges.\nStrong communication and teamwork skills, with the ability to mentor and collaborate effectively.\nRESPONSIBILITIES:\nWriting and reviewing great quality code\nUnderstanding the clients business use cases and technical requirements and be able to convert them in to technical design which elegantly meets the requirements\nMapping decisions with requirements and be able to translate the same to developers.\nIdentifying different solutions and being able to narrow down the best option that meets the clients requirements.\nDefining guidelines and benchmarks for NFR considerations during project implementation\nWriting and reviewing design document explaining overall architecture, framework, and high-level design of the application for the developers.\nReviewing architecture and design on various aspects like extensibility, scalability, security, design patterns, user experience, NFRs, etc., and ensure that all relevant best practices are followed.\nDeveloping and designing the overall solution for defined functional and non-functional requirements; and defining technologies, patterns, and frameworks to materialize it\nUnderstanding and relating technology integration scenarios and applying these learnings in projects\nResolving issues that are raised during code/review, through exhaustive systematic analysis of the root cause, and being able to justify the decision taken.\nCarrying out POCs to make sure that suggested design/technologies meet the requirements",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['GIT', 'Flutter', 'Dart', 'Swift', 'IOS', 'Android']",2025-06-12 14:49:58
Lead Pyspark Developer,Synechron,5 - 10 years,Not Disclosed,"['Chennai', 'Bengaluru']","job requisition idJR1027452\n\n\n\nOverall Responsibilities:\nData Pipeline Development:Design, develop, and maintain highly scalable and optimized ETL pipelines using PySpark on the Cloudera Data Platform, ensuring data integrity and accuracy.\nData Ingestion:Implement and manage data ingestion processes from a variety of sources (e.g., relational databases, APIs, file systems) to the data lake or data warehouse on CDP.\nData Transformation and Processing:Use PySpark to process, cleanse, and transform large datasets into meaningful formats that support analytical needs and business requirements.\nPerformance Optimization:Conduct performance tuning of PySpark code and Cloudera components, optimizing resource utilization and reducing runtime of ETL processes.\nData Quality and Validation:Implement data quality checks, monitoring, and validation routines to ensure data accuracy and reliability throughout the pipeline.\nAutomation and Orchestration:Automate data workflows using tools like Apache Oozie, Airflow, or similar orchestration tools within the Cloudera ecosystem.\nMonitoring and Maintenance:Monitor pipeline performance, troubleshoot issues, and perform routine maintenance on the Cloudera Data Platform and associated data processes.\nCollaboration:Work closely with other data engineers, analysts, product managers, and other stakeholders to understand data requirements and support various data-driven initiatives.\nDocumentation:Maintain thorough documentation of data engineering processes, code, and pipeline configurations.\n\n\n\nSoftware :\nAdvanced proficiency in PySpark, including working with RDDs, DataFrames, and optimization techniques.\nStrong experience with Cloudera Data Platform (CDP) components, including Cloudera Manager, Hive, Impala, HDFS, and HBase.\nKnowledge of data warehousing concepts, ETL best practices, and experience with SQL-based tools (e.g., Hive, Impala).\nFamiliarity with Hadoop, Kafka, and other distributed computing tools.\nExperience with Apache Oozie, Airflow, or similar orchestration frameworks.\nStrong scripting skills in Linux.\n\n\n\nCategory-wise Technical\n\nSkills:\nPySpark:Advanced proficiency in PySpark, including working with RDDs, DataFrames, and optimization techniques.\nCloudera Data Platform:Strong experience with Cloudera Data Platform (CDP) components, including Cloudera Manager, Hive, Impala, HDFS, and HBase.\nData Warehousing:Knowledge of data warehousing concepts, ETL best practices, and experience with SQL-based tools (e.g., Hive, Impala).\nBig Data Technologies:Familiarity with Hadoop, Kafka, and other distributed computing tools.\nOrchestration and Scheduling:Experience with Apache Oozie, Airflow, or similar orchestration frameworks.\nScripting and Automation:Strong scripting skills in Linux.\n\n\n\nExperience:\n5-12 years of experience as a Data Engineer, with a strong focus on PySpark and the Cloudera Data Platform.\nProven track record of implementing data engineering best practices.\nExperience in data ingestion, transformation, and optimization on the Cloudera Data Platform.\n\n\n\nDay-to-Day Activities:\nDesign, develop, and maintain ETL pipelines using PySpark on CDP.\nImplement and manage data ingestion processes from various sources.\nProcess, cleanse, and transform large datasets using PySpark.\nConduct performance tuning and optimization of ETL processes.\nImplement data quality checks and validation routines.\nAutomate data workflows using orchestration tools.\nMonitor pipeline performance and troubleshoot issues.\nCollaborate with team members to understand data requirements.\nMaintain documentation of data engineering processes and configurations.\n\n\n\nQualifications:\nBachelors or Masters degree in Computer Science, Data Engineering, Information Systems, or a related field.\nRelevant certifications in PySpark and Cloudera technologies are a plus.\n\n\n\nSoft\n\nSkills:\nStrong analytical and problem-solving skills.\nExcellent verbal and written communication abilities.\nAbility to work independently and collaboratively in a team environment.\nAttention to detail and commitment to data quality.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['cloudera', 'hive', 'pyspark', 'linux', 'hadoop', 'scala', 'amazon redshift', 'data warehousing', 'emr', 'sql', 'docker', 'apache', 'java', 'spark', 'gcp', 'etl', 'big data', 'hbase', 'data lake', 'python', 'oozie', 'airflow', 'microsoft azure', 'impala', 'data engineering', 'nosql', 'amazon ec2', 'mapreduce', 'kafka', 'sqoop', 'aws']",2025-06-12 14:50:01
CPU Full Stack Python Developer (Staff/Sr. Staff),Qualcomm,4 - 9 years,Not Disclosed,['Bengaluru'],"Job Area: Engineering Group, Engineering Group > Hardware Engineering\n\nGeneral Summary:\n\nWe are seeking a highly skilled Full Stack Python Developer to join our dynamic team. The ideal candidate should have a strong background in tool development, data science, and automation of complex tasks. You will be responsible for developing high volume regression dashboard, parametric and power tools and contributing to both front-end and back-end development.\n\nMinimum Qualifications:\nBachelor's degree in Computer Science, Electrical/Electronics Engineering, Engineering, or related field and 4+ years of Hardware Engineering or related work experience.\nOR\nMaster's degree in Computer Science, Electrical/Electronics Engineering, Engineering, or related field and 3+ years of Hardware Engineering or related work experience.\nOR\nPhD in Computer Science, Electrical/Electronics Engineering, Engineering, or related field and 2+ years of Hardware Engineering or related work experience.\n\nTechnical\n\nSkills:\n\n\n\nPythonProficiency in Python programming, including libraries like Pandas, NumPy, and SciPy for data science.\n\n\nFull Stack DevelopmentExperience with both front-end (HTML, CSS, JavaScript, React, Vue.js) and back-end (Django, Flask) technologies.\n\n\nTool DevelopmentAbility to develop parametric and power tools, possibly using frameworks like Vue.js , PyQt or Tkinter for GUI development.\n\n\nData ScienceStrong understanding of data analysis, machine learning (using libraries like scikit-learn, TensorFlow), and data visualization (using Matplotlib, Seaborn).\n\n\nAutomationExperience in automating complex tasks using scripting and tools like Selenium, Airflow, or custom automation scripts.\n\n\nSoft\n\nSkills:\n\n\n\nProblem-SolvingAbility to tackle complex problems and develop innovative solutions.\n\n\nCommunicationStrong communication skills to effectively collaborate with team members and stakeholders.\n\n\nAdaptabilityFlexibility to adapt to new technologies and methodologies.\n\n\nExperience:\n\n\nProjectsPrevious experience in developing tools and automation solutions.\n\n\nIndustry KnowledgeFamiliarity with the specific industry or domain you're working in can be a plus.\n\n\nKey Responsibilities:\n\nDevelop and maintain parametric and power tools using Python.\n\nDesign and implement automation solutions for complex tasks.\n\nCollaborate with data scientists to analyze and visualize data.\n\nBuild and maintain web applications using Django or Flask.\n\nDevelop front-end components using HTML, CSS, JavaScript, and React.\n\nIntegrate third-party APIs and services.\n\nOptimize applications for maximum speed and scalability.\n\nWrite clean, maintainable, and efficient code.\n\nTroubleshoot and debug applications.\n\nStay updated with the latest industry trends and technologies.\n\n\nPreferred Qualifications:\n\nBachelor's degree in Computer Science, Engineering, or related field.\n\nPrevious experience in tool development and automation.\n\nFamiliarity with industry-specific tools and technologies.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['tool development', 'data science', 'python', 'data analysis', 'machine learning', 'css', 'hiring', 'scikit-learn', 'vue.js', 'numpy', 'staffing', 'react.js', 'tensorflow', 'seaborn', 'selenium', 'pyqt', 'html', 'data visualization', 'scipy', 'hardware engineering', 'javascript', 'pandas', 'django', 'matplotlib', 'flask']",2025-06-12 14:50:04
Analyst,Merkle Science,1 - 2 years,Not Disclosed,['Mumbai'],"The purpose of this role is to deliver analysis inline with client business objectives, goals, and to maintain, develop and exceed client performance targets.\nJob Description:\nKey responsibilities:Understands the client needs in specific. Ensures crisp communication with clients and work as an interface between team members and client counterpart. Discusses issues related to questionnaires with clients and suggest solutions for the sameUses specialised knowledge of market research tools / programming languages to understand the client requirements and build surveys/ deliver data tables as per the requirement with required quality and productivity levelsReviews project requirements and executes projects, under the direction of senior team members, per requirements by following the guidelines and deploying the tools/systems as applicableCreates and follows work allocation schedule and project plan\nLocation:\nDGS India - Mumbai - Goregaon Prism Tower\nBrand:\nMerkle\nTime Type:\nFull time\nContract Type:\nPermanent",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Senior Analyst', 'Programming', 'Market research', 'Deployment', 'Project planning']",2025-06-12 14:50:06
Analyst,Wipro,1 - 3 years,Not Disclosed,['Kolkata'],"Role Purpose\nThe purpose of the role is to resolve, maintain and manage clients software/ hardware/ network based on the service requests raised from the end-user as per the defined SLAs ensuring client satisfaction\n\n\n\nDo\nEnsure timely response of all the tickets raised by the client end user\nService requests solutioning by maintaining quality parameters\nAct as a custodian of clients network/ server/ system/ storage/ platform/ infrastructure and other equipments to keep track of each of their proper functioning and upkeep\nKeep a check on the number of tickets raised (dial home/ email/ chat/ IMS), ensuring right solutioning as per the defined resolution timeframe\nPerform root cause analysis of the tickets raised and create an action plan to resolve the problem to ensure right client satisfaction\nProvide an acceptance and immediate resolution to the high priority tickets/ service\nInstalling and configuring software/ hardware requirements based on service requests\n100% adherence to timeliness as per the priority of each issue, to manage client expectations and ensure zero escalations\nProvide application/ user access as per client requirements and requests to ensure timely solutioning\nTrack all the tickets from acceptance to resolution stage as per the resolution time defined by the customer\nMaintain timely backup of important data/ logs and management resources to ensure the solution is of acceptable quality to maintain client satisfaction\nCoordinate with on-site team for complex problem resolution and ensure timely client servicing\nReview the log which Chat BOTS gather and ensure all the service requests/ issues are resolved in a timely manner\n\n\n\nDeliver\n\nNoPerformance ParameterMeasure1.100% adherence to SLA/ timelines\nMultiple cases of red time\nZero customer escalation\nClient appreciation emails\n\n\nMandatory Skills: L&P Policy Acquisition & Servicing. Experience: 1-3 Years.",Industry Type: IT Services & Consulting,"Department: Customer Success, Service & Operations","Employment Type: Full Time, Permanent","['root cause analysis', 'itsm', 'incident management', 'acquisition', 'problem management', 'digital transformation', 'itil']",2025-06-12 14:50:09
Business Analyst - L4,Wipro,5 - 8 years,Not Disclosed,['Hyderabad'],"Role Purpose\nThe purpose of the role is to liaison and bridging the gap between customer and Wipro delivery team to comprehend and analyze customer requirements and articulating aptly to delivery teams thereby, ensuring right solutioning to the customer.\n\nDo\n1. Customer requirements gathering and engagement\nInterface and coordinate with client engagement partners to understand the RFP/ RFI requirements\nDetail out scope documents, functional & non-functional requirements, features etc ensuring all stated and unstated customer needs are captured\nConstruct workflow charts and diagrams, studying system capabilities, writing specification after thorough research and analysis of customer requirements\nEngage and interact with internal team - project managers, pre-sales team, tech leads, architects to design and formulate accurate and timely response to RFP/RFIs\nUnderstand and communicate the financial and operational impact of any changes\nPeriodic cadence with customers to seek clarifications and feedback wrt solution proposed for a particular RFP/ RFI and accordingly instructing delivery team to make changes in the design\nEmpower the customers through demonstration and presentation of the proposed solution/ prototype\nMaintain relationships with customers to optimize business integration and lead generation\nEnsure ongoing reviews and feedback from customers to improve and deliver better value (services/ products) to the customers\n\n2.Engage with delivery team to ensure right solution is proposed to the customer\na.Periodic cadence with delivery team to:\nProvide them with customer feedback/ inputs on the proposed solution\nReview the test cases to check 100% coverage of customer requirements\nConduct root cause analysis to understand the proposed solution/ demo/ prototype before sharing it with the customer\nDeploy and facilitate new change requests to cater to customer needs and requirements\nSupport QA team with periodic testing to ensure solutions meet the needs of businesses by giving timely inputs/feedback\nConduct Integration Testing and User Acceptance demos testing to validate implemented solutions and ensure 100% success rate\nUse data modelling practices to analyze the findings and design, develop improvements and changes\nEnsure 100% utilization by studying systems capabilities and understanding business specifications\nStitch the entire response/ solution proposed to the RFP/ RFI before its presented to the customer\nb.Support Project Manager/ Delivery Team in delivering the solution to the customer\nDefine and plan project milestones, phases and different elements involved in the project along with the principal consultant\nDrive and challenge the presumptions of delivery teams on how will they successfully execute their plans\nEnsure Customer Satisfaction through quality deliverable on time\n\n\n\n3.Build domain expertise and contribute to knowledge repository\nEngage and interact with other BAs to share expertise and increase domain knowledge across the vertical\nWrite whitepapers/ research papers, point of views and share with the consulting community at large\nIdentify and create used cases for a different project/ account that can be brought at Wipro level for business enhancements\nConduct market research for content and development to provide latest inputs into the projects thereby ensuring customer delight\n\nDeliver\n\nNo.Performance ParameterMeasure1.Customer Engagement and Delivery ManagementPCSAT, utilization % achievement, no. of leads generated from the business interaction, no. of errors/ gaps in documenting customer requirements, feedback from project manager, process flow diagrams (quality and timeliness), % of deal solutioning completed within timeline, velocity generated.2.Knowledge ManagementNo. of whitepapers/ research papers written, no. of user stories created, % of proposal documentation completed and uploaded into knowledge repository, No of reusable components developed for proposal during quarter\nMandatory Skills: ServiceNow - Platform Core. Experience: 5-8 Years.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Business analysis', 'client engagement', 'Business Analyst', 'test cases', 'Integration Testing']",2025-06-12 14:50:11
Engineer II,AMERICAN EXPRESS,3 - 8 years,Not Disclosed,['Bengaluru'],"you're a talented, creative, and motivated engineer who loves developing powerful, stable, and intuitive apps and you're excited to work with a team of individuals with that same passion. you've accumulated years of experience, and you're excited about taking your mastery of Cloud, Big Data and Java to a new level. You enjoy challenging projects involving big data sets and are cool under pressure. you're no stranger to fast-paced environments and agile development methodologies in fact, you embrace them. With your strong analytical skills, your unwavering commitment to quality, your excellent technical skills, and your collaborative work ethic, you'll do great things here at American Express.",,,,"['GIT', 'RDBMS', 'Coding', 'Finance', 'MySQL', 'Agile', 'JIRA', 'SQL', 'Python']",2025-06-12 14:50:14
Software Engineer Staff,Juniper Networks,10 - 15 years,Not Disclosed,['Bengaluru'],"The Infrastructure team under Juniper Apstra owns and evolves the backend infrastructure for Juniper Apstra.\nJoin a team where systems and data engineering converge\nyou'll work at the core of a high-performance, custom-built infrastructure platform while also helping shape the data pipelines that power our next-gen cloud analytics stack. This role offers rare dual exposure: from low-level optimization in performance-critical systems, to hands-on feature development in in-house time-series and graph databases . you'll contribute to building and refining connectivity for these databases to the cloud, driving real-world data flows across hybrid environments. If you're excited by deep infrastructure, data-intensive workloads, and full-stack thinking, this is where your impact multiplies.\n  What the Team Does\n  1. Core Infrastructure (60% of Role)\nMaintains and enhances the backend platform that powers Apstra s configuration, telemetry, and analytics pipeline.\nOwns:\nProduct packaging, upgrades, and deployment orchestration\nHomegrown file-based telemetry database (custom-built, optimized for performance)\nDistributed querying, data merging, system calls, IPC\nPython + C++ hybrid execution, including C-extensions for performance\n2. Data Engineering (40% of Role)\nEnables data synchronization from on-prem Apstra to on JCloud.\nConstructs data pipelines and ETL flows to prepare, transform, and deliver telemetry for downstream cloud analytics consumers.\nOwns cloud enablement and sync logic (eg examples datasets, format normalization, usage signals).\nWhy Candidates Should Join This Team\n  you'll Build Real Infrastructure Not Just Configure It\nThis isn t infra as code it s infrastructure as a product . you'll design, scale, and optimize distributed systems that support real-time, multi-version enterprise network workloads across campus, data center, and WAN fabrics. It s infrastructure that directly powers next-gen cloud analytics.\n  Hybrid Challenge: On-Prem Meets Cloud\nFew roles offer this blend: deep low-level systems work coupled with forward-looking data engineering . you'll help bridge Apstra s on-prem legacy with its evolving cloud-native future , working across environments to modernize and integrate.\n  Deep Ownership + Technical Core\nThis team doesn t just connect APIs. It owns the architecture behind Apstra s analytics and telemetry engines critical infrastructure that underpins Juniper s most advanced networking insights.\n  Custom Database Engineering + Analytics Edge\nyou'll work directly on our in-house telemetry database , focusing on:\nCompression, distributed query execution , and performance tuning\nBuilding feature enhancements for in-house time-series and graph databases\nEnabling cloud connectivity and insights generation for real-world data use cases\nCloud\nyou'll get early exposure to JCloud , Juniper s internal cloud platform. Think AWS-like building blocks serverless functions, ETL pipelines, distributed stores but tuned for networking workloads, security, and high observability.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Performance tuning', 'C++', 'Backend', 'Networking', 'WAN', 'Packaging', 'Distribution system', 'Analytics', 'Downstream', 'Python']",2025-06-12 14:50:16
Business Analyst,"Sourced Group, an Amdocs Company",1 - 3 years,Not Disclosed,['Pune'],"0px> Who are we?\nIn one sentence\nUnderstand the business needs of the customer and assess the impact of those needs in order to communicate and implement the recommended efficient solutions.\nWhat will your job look like?\n""Lead domain-specific solutioning activities across solution and delivery engagements.\nAct as a trusted advisor to customers, providing deep expertise in your domain (e.g., Charging & Billing, CRM, Ordering, Catalog, Network Provisioning).\nDefine end-to-end domain solution and ensure alignment with customer business goals and operational strategies.\nCollaborate with Solution Architects, Business Analysts, and Product Managers for requirement feasibility and solution scope.\nProvide functional and technical support during design, integration, migration, and testing phases.\nIdentify domain risks, dependencies, and business impacts; recommend best practices and innovative approaches.""\nAll you need is...\nDegree in Computer Science or Industrial Engineering & Management - Information System.\nCustomer-facing experience - ability to communicate the Amdocs solution using various methods (presentations, demos, and so on).\nWide knowledge of relevant products and E2E Business process.\nKnowledge of the telecom industry and Amdocs business processes (ETOM, ASOM).\nExperience in managing a team in cross-Amdocs domain solutions.\nWhy you will love this job:\nUse your outstanding business analysis skills to make a significant impact on leading solutions that produce the most efficient product solutions.\nBe a key member of a global, dynamic and highly collaborative team with various possibilities for personal and professional development!\nYou will have the opportunity to work in multinational environment for the global market leader in its field.\nWe offer a wide range of stellar benefits including health, dental, vision, and life insurance as well as paid time off, sick time, and parental leave!",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Business process', 'Telecom', 'eTOM', 'Business analysis', 'Billing', 'Amdocs', 'Operations', 'Technical support', 'CRM']",2025-06-12 14:50:19
Software Engineer Gen AI,Wells Fargo,2 - 7 years,Not Disclosed,['Bengaluru'],"locationsBengaluru, India\nposted onPosted 4 Days Ago\njob requisition idR-462134\nAbout this role:\nWells Fargo is seeking a Software Engineer.\n\nIn this role, you will:\nParticipate in low to moderately complex initiatives and projects associated with the technology domain, including installation, upgrades, and deployment efforts\nIdentify opportunities for service quality and availability improvements within the technology domain environment\nDesign, code, test, debug, and document for low to moderately complex projects and programs associated with technology domain, including upgrades and deployments\nReview and analyze technical assignments or challenges that are related to low to medium risk deliverables and that require research, evaluation, and selection of alternative technology domains\nPresent recommendations for resolving issues or may escalate issues as needed to meet established service level agreements\nExercise some independent judgment while also developing understanding of given technology domain in reference to security and compliance requirements\nProvide information to technology colleagues, internal partners, and stakeholders\n\nRequired Qualifications:\n2+ years of software engineering experience, or equivalent demonstrated through one or a combination of the following: work experience, training, military experience, education\n\nDesired Qualifications:\nWork as a Generative AI engineer developing enterprise-scale AI applications\nDesign, implement, and optimize LLM-based solutions using state-of-the-art frameworks\nLead Gen AI initiatives focused on developing intelligent agents and conversational systems\nDesign and build robust LLM interfaces and orchestration pipelines\nDevelop evaluation frameworks to measure and improve model performance\nImplement prompt engineering techniques to optimize model outputs\nIntegrate Gen AI capabilities with existing enterprise applications\nBuild and maintain frontend interfaces for AI applications\nStrong proficiency in Python/Java and LLM orchestration frameworks (LangChain, LangGraph)\nBasic Knowledge of model context protocols, RAG architectures, and embedding techniques\nExperience with model evaluation frameworks and metrics for LLM performance\nProficiency in frontend development with React.js for AI applications\nExperience with UI/UX design patterns specific to AI interfaces\nExperience with vector databases and efficient retrieval methods\nKnowledge of prompt engineering techniques and best practices\nExperience with containerization and microservices architecture\nStrong understanding of semantic search and document retrieval systems\nWorking knowledge of both structured and unstructured data processing\nExperience with version control using GitHub and CI/CD pipelines\nExperience working with globally distributed teams in Agile scrums\n\nJob Expectations:\nUnderstanding of enterprise use cases for Generative AI\nKnowledge of responsible AI practices and ethical considerations\nAbility to optimize AI solutions for performance and cost\nWell versed in MLOps concepts for LLM applications\nStaying current with rapidly evolving Gen AI technologies and best practices\nExperience implementing security best practices for AI applications",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Gen AI', 'Java', 'UI design', 'UX design', 'LLM orchestration', 'React.js', 'Python', 'MLOps concepts']",2025-06-12 14:50:21
Business Analyst - L5,Wipro,8 - 10 years,Not Disclosed,['Pune'],"Role Purpose\nThe purpose of the role is to liaison and bridging the gap between customer and Wipro delivery team to comprehend and analyze customer requirements and articulating aptly to delivery teams thereby, ensuring right solutioning to the customer.\n\n\n\nDo\n1. Customer requirements gathering and engagement\nInterface and coordinate with client engagement partners to understand the RFP/ RFI requirements\nDetail out scope documents, functional & non-functional requirements, features etc ensuring all stated and unstated customer needs are captured\nConstruct workflow charts and diagrams, studying system capabilities, writing specification after thorough research and analysis of customer requirements\nEngage and interact with internal team - project managers, pre-sales team, tech leads, architects to design and formulate accurate and timely response to RFP/RFIs\nUnderstand and communicate the financial and operational impact of any changes\nPeriodic cadence with customers to seek clarifications and feedback wrt solution proposed for a particular RFP/ RFI and accordingly instructing delivery team to make changes in the design\nEmpower the customers through demonstration and presentation of the proposed solution/ prototype\nMaintain relationships with customers to optimize business integration and lead generation\nEnsure ongoing reviews and feedback from customers to improve and deliver better value (services/ products) to the customers\n\n\n\n2.Engage with delivery team to ensure right solution is proposed to the customer\na.Periodic cadence with delivery team to:\nProvide them with customer feedback/ inputs on the proposed solution\nReview the test cases to check 100% coverage of customer requirements\nConduct root cause analysis to understand the proposed solution/ demo/ prototype before sharing it with the customer\nDeploy and facilitate new change requests to cater to customer needs and requirements\nSupport QA team with periodic testing to ensure solutions meet the needs of businesses by giving timely inputs/feedback\nConduct Integration Testing and User Acceptance demos testing to validate implemented solutions and ensure 100% success rate\nUse data modelling practices to analyze the findings and design, develop improvements and changes\nEnsure 100% utilization by studying systems capabilities and understanding business specifications\nStitch the entire response/ solution proposed to the RFP/ RFI before its presented to the customer\nb.Support Project Manager/ Delivery Team in delivering the solution to the customer\nDefine and plan project milestones, phases and different elements involved in the project along with the principal consultant\nDrive and challenge the presumptions of delivery teams on how will they successfully execute their plans\nEnsure Customer Satisfaction through quality deliverable on time\n\n\n\n3.Build domain expertise and contribute to knowledge repository\nEngage and interact with other BAs to share expertise and increase domain knowledge across the vertical\nWrite whitepapers/ research papers, point of views and share with the consulting community at large\nIdentify and create used cases for a different project/ account that can be brought at Wipro level for business enhancements\nConduct market research for content and development to provide latest inputs into the projects thereby ensuring customer delight\n\n\n\nDeliver\n\nNo.Performance ParameterMeasure1.Customer Engagement and Delivery ManagementPCSAT, utilization % achievement, no. of leads generated from the business interaction, no. of errors/ gaps in documenting customer requirements, feedback from project manager, process flow diagrams (quality and timeliness), % of deal solutioning completed within timeline, velocity generated.2.Knowledge ManagementNo. of whitepapers/ research papers written, no. of user stories created, % of proposal documentation completed and uploaded into knowledge repository, No of reusable components developed for proposal during quarter\nMandatory Skills: Business Analysis. Experience: 8-10 Years.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Business analysis', 'business integration', 'client engagement', 'data modelling', 'Customer Engagement']",2025-06-12 14:50:23
Teamcenter Analyst,Tata Technologies,2 - 5 years,Not Disclosed,['Bengaluru'],"Sustainability Tool Implementation (Siemens TcPCM), Carbon footprint, Recycled content, Know-How on Scope 1, 2 and 3 emissions, Water and Waste ManagementConsulting awarenessCost and Carbon balanceCompliance management exposureSustainability Tool Implementatio n (Siemens TcPCM), Carbon footprint, Recycled content, Know-How on Scope 1, 2 and 3 emissions, Water and Waste ManagementConsulting awarenessCost and Carbon balanceCompliance management exposureSustainability Tool Implementation (Siemens TcPCM), Carbon footpri nt, Recycled content, Know-How on Scope 1, 2 and 3 emissions, Water and Waste ManagementConsulting awarenessCost and Carbon balanc eCompliance management exposure",Industry Type: Building Material (Cement),Department: Research & Development,"Employment Type: Full Time, Permanent","['Siemens TcPCM', 'Waste Management', 'Compliance management', 'Recycled content', 'Sustainability Tool Implementation', 'Carbon footprint']",2025-06-12 14:50:25
Business Intel Engineer,Amazon,2 - 7 years,Not Disclosed,['Bengaluru'],"Amazon Business Payments and Lending organization is seeking a highly quantitative Business Intelligence Engineer to drive the development of analytics and insights. You will succeed in this role if you are an organized self-starter who can learn new technologies quickly and excel in a fast-paced environment. In this position, you will be a key contributor and sparring partner, developing analytical solutions that global executive management teams and business leaders will use to deep dive into the businesses and define strategies.\n\nOur team offers a unique opportunity to build a new set of analytical experiences from the ground up. You will be part of the team that is focused on payments around the world. The position is based out of India but will interact with global leaders and teams across Europe, Japan, and US. You should be highly analytical, resourceful, customer focused, team oriented, and have an ability to work independently under time constraints to meet deadlines. You will be comfortable thinking big and diving deep. A proven track record in taking on end-to-end ownership and successfully delivering results in a fast-paced, dynamic business environment is strongly preferred.\n\nNote: This role is part of the rekindle program. For more details on rekindle program, please visit\n\n\n\nOwn the design, development, and maintenance of ongoing metrics, reports, analyses, dashboards, etc. to drive key business decisions. Ensure data accuracy by validating data for new and existing tools.\nRetrieve and analyze data using SQL, Excel, and other data management systems and develop reporting and data visualization solutions -using tools like AWS QuickSight and Looker.\nRecognize and adopt best practices in reporting and analysis: data integrity, test design, analysis, validation, and documentation.\nModel data and metadata to support the reporting pipelines and automate the manual reporting solutions.\nUnderstand trends related to business metrics and recommend strategies to stakeholders to help drive business growth. Reporting of key insight trends, using statistical rigor (Hypothesis testing, measuring experiment success defining statistical significance, developing basic regression and forecasting models) to simplify and inform the larger team of noteworthy story lines.\n\nA day in the life\nAnalyze data and find insights to either drive strategic business decisions or to drive incremental signups or revenue.\nDefine and develop business critical metrics and reports across all international business levers, key performance indicators, and financials.\nOwn alignment and standardization of analytical initiatives across the global business teams\nDrive efforts across international business leaders, BI leaders and executive management across Europe, Asia and North America.\nOwn key executive reports and metrics that are consumed by our VPs and Directors\nProvide thought leadership in global business deep dives across a variety of key performance indicators 2+ years of analyzing and interpreting data with Redshift, Oracle, NoSQL etc. experience\nExperience with data visualization using Tableau, Quicksight, or similar tools\nExperience with one or more industry analytics visualization tools (e.g. Excel, Tableau, QuickSight, MicroStrategy, PowerBI) and statistical methods (e.g. t-test, Chi-squared)\nExperience with scripting language (e.g., Python, Java, or R) Masters degree, or Advanced technical degree\nKnowledge of data modeling and data pipeline design\nExperience with statistical analysis, co-relation analysis",,,,"['Microstrategy', 'metadata', 'Data management', 'Data modeling', 'Oracle', 'Business intelligence', 'Forecasting', 'Analytics', 'SQL', 'Python']",2025-06-12 14:50:27
FinOps Analyst,Amazon,1 - 2 years,Not Disclosed,['Pune'],"Amazon Finance Operations / payroll team is looking for individuals with Expertise in the Indian Payroll, Employee Tax and Benefits process experience.\nAlong with Payroll, the individual having auditing skills, problem solving skills, payroll system knowledge is plus.\n\nAmazon seeks a Payroll Analyst with at least 1.5 to 2 years relevant experience in a large customer-oriented corporate environment where Payroll is processed on a monthly basis.\n\nCore deliverables,\n\n-Pay Amazon employees accurately, on time , every time.\n-Manage Off-Cycle Payrun and payments.\n-Process payroll i.e. Salary, employee / employer tax s and other statutory deductions using in house and 3rd party payroll platform.\n-Produce timely responses to queries received from employees, support team and HRBP s.\n-View and manage human resource data on people portal.\n-Process manual check calculations, work with the vendor to process stop payments/reversals, enter paycheck card entries and assist with check distribution and backup other payroll analysts as needed.\n-Support scheduled and adhoc payroll task such as reporting, reconciliations, tax filings i.e. Start of year, end or year, annual employee earning records.\n-5 days working from office in a week is required.\n1.5 2 years of relevant work experience.\nGood communication skills (verbal and written)\nGood knowledge on MS Office is a must Graduate in commerce, accounting or finance.\nUnderstanding of Lean, Six Sigma and other process improvement methodology.",,,,"['Accounting', 'Process improvement', 'Payroll Analyst', 'Corporate', 'Financial operations', 'HR', 'MS Office', 'Lean six sigma', 'Recruitment', 'Auditing']",2025-06-12 14:50:30
Transistion Analyst,Capco,1 - 8 years,Not Disclosed,['Pune'],"About Us\nCapco, a Wipro company, is a global technology and management consulting firm. Awarded with Consultancy of the year in the British Bank Award and has been ranked Top 100 Best Companies for Women in India 2022 by\nAvtar & Seramount\n. With our presence across 32 cities across globe, we support 100+ clients across banking, financial and Energy sectors. We are recognized for our deep transformation execution and delivery.\nWHY JOIN CAPCO?\nYou will work on engaging projects with the largest international and local banks, insurance companies, payment service providers and other key players in the industry. The projects that will transform the financial services industry.\nMAKE AN IMPACT\nInnovative thinking, delivery excellence and thought leadership to help our clients transform their business. Together with our clients and industry partners, we deliver disruptive work that is changing energy and financial services.\n#BEYOURSELFATWORK\nCapco has a tolerant, open culture that values diversity, inclusivity, and creativity.\nCAREER ADVANCEMENT\nWith no forced hierarchy at Capco, everyone has the opportunity to grow as we grow, taking their career into their own hands.\nDIVERSITY & INCLUSION\nWe believe that diversity of people and perspective gives us a competitive advantage.\nMAKE AN IMPACT\nJob Title: Transition Analyst\nLocation: Pune\nExperience Required:\nBachelor s degree is required.\nMinimum 6 to 8 years of total work experience.\nMinimum 1 to 2 years of relevant experience in Project/Program Management or Support roles.\nTechnical & Functional Expertise:\nTechnical:\nProficiency in MS Office products including Office 365, Project Online, SharePoint, Power BI, and other analytics tools.\nStrong understanding of process workflow design, data architecture, and related tools.\nFunctional:\nStrong business acumen and functional understanding.\nExperience in planning and monitoring for program workstreams, project deliverables, and reporting.\nAbility to handle transition-related documentation, administrative tasks, risk management, due diligence, and stakeholder coordination.\nExperience in knowledge transfer, SOP documentation, and hyper care support.\nKey Responsibilities:\nSupport planning and execution of program and transition projects.\nTrack deliverables, manage risks, and ensure timely reporting.\nEnsure compliance with GBS methodologies and toolkits.\nManage travel and logistics for transition-related requirements.\nCoordinate with operational teams and business functions for successful transitions.\nLead the documentation of SOPs and manage sign-off processes.\nCollaborate with various business units including Procurement, Finance, and IT.\nSupport project reporting, dashboard preparation, and Power BI-based analytics.\nHandle highly confidential material with discretion and professionalism.\nParticipate in customer-facing meetings and internal stakeholder communications.\nFacilitate workshops, team meetings, and process improvement initiatives.\nKey Challenges:\nNavigating fragmented systems and tools.\nEngaging a wide range of stakeholders across global functions.\nManaging services at a large scale with geographical and cultural diversity.\nAdapting to evolving digital technologies and technical tools.\nEnsuring alignment with global process design standards.\nSkills & Competencies:\nCore Skills:\nProject planning and reporting skills\nWorkflow and process documentation\nRisk identification and mitigation\nData visualization and reporting tools (especially Power BI)\nSoft Skills:\nExcellent multitasking and prioritization skills\nStrong interpersonal, presentation, and written communication skills\nFluency in English (spoken and written)\nKnowledge of local regulations and compliance standards\nFamiliarity with Pune s local business environment\nAbility to work effectively in a regional service center ecosystem",Industry Type: Banking,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Procurement', 'Administration', 'Management consulting', 'Project planning', 'MS Office', 'Risk management', 'Financial services', 'Analytics', 'Monitoring', 'Logistics']",2025-06-12 14:50:32
AI/ML framework Staff Engineer,Qualcomm,4 - 9 years,Not Disclosed,['Bengaluru'],"Job Area: Engineering Group, Engineering Group > Systems Engineering\n\nGeneral Summary:\n\nLooking for ""ML framework and AI compiler Engineer"" responsible for\nDesigning, implementing, and deploying machine learning models using PyTorch\nFocusing on backend infrastructure and system architecture.\nResponsibilities often include developing framework, integrating with other AI tools, and ensuring scalability and reliability.\n\nHere's a more detailed breakdown of what you might see in such a job description:\n\nKey Responsibilities:\n\n\nModel Development and DeploymentDesigning, building, and deploying AI models, particularly those leveraging PyTorch for deep learning.\n\n\nBackend InfrastructureDeveloping and maintaining the backend systems that power AI applications, including data ingestion, processing, and storage.\n\n\nSystem ArchitectureDesigning scalable and high-performance backend architectures to handle AI workloads.\n\n\nModel OptimizationOptimizing model performance for speed, accuracy, and resource efficiency.\n\n\nIntegrationIntegrating AI models with other systems and applications.\n\n\nAPI DevelopmentCreating and maintaining APIs for communication between frontend and backend components.\n\n\nData HandlingManaging data ingestion, preprocessing, and storage for AI training and inference.\n\n\nCollaborationWorking with data scientists, product managers, and other engineers to bring AI solutions to life.\n\nTools, Technologies, Skills and Programming:\n\n\nC, C++: Strong programming capability using advanced techniques to design and develop AI compilers and backends.\n\n\nScripting: Strong expertise in Python with design, develop, release and maintain projects.\n\n\nAI Frameworks: Familiarity with other AI frameworks like PyTorch, TensorFlow, Hugging Face, etc.\n\n\nMachine Learning Knowledge: Understanding of machine learning principles and algorithms starting Computer vision to large language models and continuously update to new trends.\nExpertise to deep learning accelerator programming (GPU, NPU). Any parallel programming experience (Like CUDA, OpenCL, MKLDNN ..etc) is a plus.\nExperience with deep leaning compilers like Glow, TVM ""etc is a plus.\n\n\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 4+ years of Systems Engineering or related work experience.\nOR\nMaster's degree in Engineering, Information Systems, Computer Science, or related field and 3+ years of Systems Engineering or related work experience.\nOR\nPhD in Engineering, Information Systems, Computer Science, or related field and 2+ years of Systems Engineering or related work experience.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'machine learning', 'tensorflow', 'pytorch', 'system engineering', 'c#', 'cuda', 'algorithms', 'c++', 'parallel programming', 'artificial intelligence', 'opencl', 'deep learning', 'java', 'product management', 'computer vision', 'asp.net', 'multithreading', 'mvc', 'ml']",2025-06-12 14:50:34
Software Engineer 4,Juniper Networks,7 - 13 years,Not Disclosed,['Bengaluru'],"you'll work at the core of a high-performance, custom-built infrastructure platform while also helping shape the data pipelines that power our next-gen cloud analytics stack. This role offers rare dual exposure: from low-level optimization in performance-critical systems, to hands-on feature development in in-house time-series and graph databases . you'll contribute to building and refining connectivity for these databases to the cloud, driving real-world data flows across hybrid environments. If you're excited by deep infrastructure, data-intensive workloads, and full-stack thinking, this is where your impact multiplies.\nWhat the Team Does\n1. Core Infrastructure (60% of Role)\nMaintains and enhances the backend platform that powers Apstra s configuration, telemetry, and analytics pipeline.\nOwns:\nProduct packaging, upgrades, and deployment orchestration\nHomegrown file-based telemetry database (custom-built, optimized for performance)\nDistributed querying, data merging, system calls, IPC\nPython + C++ hybrid execution, including C-extensions for performance\n2. Data Engineering (40% of Role)\nEnables data synchronization from on-prem Apstra to on JCloud.\nConstructs data pipelines and ETL flows to prepare, transform, and deliver telemetry for downstream cloud analytics consumers.\nOwns cloud enablement and sync logic (eg examples datasets, format normalization, usage signals).\nWhy Candidates Should Join This Team\nyou'll Build Real Infrastructure Not Just Configure It\nThis isn t infra as code it s infrastructure as a product . you'll design, scale, and optimize distributed systems that support real-time, multi-version enterprise network workloads across campus, data center, and WAN fabrics. It s infrastructure that directly powers next-gen cloud analytics.\nHybrid Challenge: On-Prem Meets Cloud\nFew roles offer this blend: deep low-level systems work coupled with forward-looking data engineering . you'll help bridge Apstra s on-prem legacy with its evolving cloud-native future , working across environments to modernize and integrate.\nDeep Ownership + Technical Core\nThis team doesn t just connect APIs. It owns the architecture behind Apstra s analytics and telemetry engines critical infrastructure that underpins Juniper s most advanced networking insights.\nCustom Database Engineering + Analytics Edge\nyou'll work directly on our in-house telemetry database , focusing on:\nCompression, distributed query execution , and performance tuning\nBuilding feature enhancements for in-house time-series and graph databases\nEnabling cloud connectivity and insights generation for real-world data use cases\nCloud\nyou'll get early exposure to JCloud , Juniper s internal cloud platform. Think AWS-like building blocks serverless functions, ETL pipelines, distributed stores but tuned for networking workloads, security, and high observability.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Performance tuning', 'C++', 'Backend', 'Networking', 'WAN', 'Packaging', 'Distribution system', 'Analytics', 'Downstream', 'Python']",2025-06-12 14:50:37
"Software Development Engineer II, International Emerging Stores",Amazon,3 - 8 years,Not Disclosed,['Bengaluru'],"Shaping the Future of Global E-commerce!!\n\nAt Amazons International Emerging Stores (IES), were reinventing how millions of customers discover and shop online. Our team is architecting foundational platforms and mechanisms that will power Amazons next generation of shopping experiences globally. Were tackling intrinsically hard problems at the intersection of AI, personalization, and scalable systems challenges that require innovative solutions while maintaining our commitment to operational excellence. Our charter extends beyond traditional e-commerce boundaries, focusing on creating competitive advantages through technical innovation. Were building solutions that not only serve immediate business needs but establish new patterns and practices that can be adopted across Amazon. Our work demands deep technical judgment, cross-organizational collaboration, and the ability to influence at the highest levels of the organization.\n\nThe Opportunity: Software Development Engineer\n\nAt IES, were building the future of retail, and were looking for a talented Software Development Engineer to join our innovative team. As an SDE, youll be instrumental in revolutionizing how customers make purchase decisions across our retail platform by developing next-generation shopping experiences powered by artificial intelligence and adaptive technologies.\n\nIn this role, youll design and implement intelligent systems that deliver personalized shopping experiences, working with cutting-edge generative AI and ML models to create innovative visual experiences. Youll develop sophisticated algorithms for adaptive layout optimization, product visualization features, theme-based recommendation systems, and customer behavior analysis. Your work will span multiple customer touchpoints, requiring you to write high-quality, scalable code while collaborating with product managers, designers, and data scientists to drive technical solutions.\n\nWere seeking someone with strong programming skills and software design expertise, particularly in distributed systems and scalable architectures. Knowledge of AI/ML technologies and their practical applications is essential, as is the ability to translate complex business requirements into technical solutions. Youll participate in architecture discussions, technical design reviews, and contribute to the continuous improvement of customer experience metrics while debugging complex production issues and optimizing system performance.\n\nThis position offers an exciting opportunity to work on cutting-edge technologies while solving complex engineering challenges that impact millions of customers globally. Youll be part of a team that values technical excellence and innovation, with the chance to shape the future of e-commerce through technological advancement.\n\n3+ years of non-internship professional software development experience\n2+ years of non-internship design or architecture (design patterns, reliability and scaling) of new and existing systems experience\nExperience programming with at least one software programming language 3+ years of full software development life cycle, including coding standards, code reviews, source control management, build processes, testing, and operations experience\nBachelors degree in computer science or equivalent",,,,"['Computer science', 'Software design', 'Operational excellence', 'Software Development Engineer II', 'Coding', 'Artificial Intelligence', 'Debugging', 'Continuous improvement', 'Internship', 'Distribution system']",2025-06-12 14:50:40
Snowflake - Senior Technical Lead,Sopra Steria,2 - 11 years,Not Disclosed,['Noida'],"Position: Snowflake - Senior Technical Lead\nExperience: 8-11 years\nLocation: Noida/ Bangalore\nEducation: B.E./ B.Tech./ MCA\nPrimary Skills: Snowflake, Snowpipe, SQL, Data Modelling, DV 2.0, Data Quality, AWS, Snowflake Security\nGood to have Skills: Snowpark, Data Build Tool, Finance Domain\nPreferred Skills",,,,"['Performance tuning', 'Schema', 'HIPAA', 'Javascript', 'Data quality', 'Informatica', 'Analytics', 'SQL', 'Python', 'Auditing']",2025-06-12 14:50:42
Analyst,WPP,3 - 5 years,Not Disclosed,['Mumbai'],"Preparation of payment runs for vendors\nSending remittance advice to vendors.\nLiaising with the Treasury team & agency with regards to amendments for BACS payment files, ensuring any anomaly on the payment selections are reported to Treasury team & agency to resolve within the agreed timescale.\nMaintain accurate and up-to-date filing.\nAssist with Internal and External Client Audits.\nMaintain the level of accuracy and meet the deadlines for manual payments - Daily.\nChecking payment runs in line with the agreed signature Matrix.\nProvide holiday and sick cover as necessary.\nAs a holiday & sick cover able to take handle below tasks:\nEnsure all expense paperwork is date stamped on day of receipt.\nTo process and check expense claims for the Agencies, ensuring appropriate documentation and approvals are available to support the expense claims.\nEnsure expenses are processed within the weekly cut off timetable set by the Treasury team.\nPerform the final approval for all expenses (system generated approval).\nReview hard copy claims and corresponding receipts to ensure compliance with WPP Policies.\nLiaise with claimant and agency to give details of amendments, rejected claims and general queries.\nWhat youll need:\nJob Location - Vikhroli, Mumbai\nShould be open for Night shift (US) - 6.30PM Onwards\nBCom degree in Finance, Accounting or MBA in Finance\nMinimum 3-5years experience into P2P process\nKnowledge of the methods, principles, and practices of P2P.\nAdherence to laws and best practices in regard to dealing with customers and data\nExcellent knowledge of MS Office (particularly Excel)\nProficiency in English\nOrganizational and time-management skills",Industry Type: Advertising & Marketing,Department: Finance & Accounting,"Employment Type: Full Time, Permanent","['BACS', 'Claims', 'Excel', 'Compliance', 'Time management', 'Accounting', 'Manager Technology', 'MS Office', 'Recruitment', 'Marketing']",2025-06-12 14:50:45
"Quality Assurance Engineer I, Ads QA",Amazon,2 - 7 years,Not Disclosed,['Bengaluru'],"Are you looking to join a team that is among the fastest growing organizations at Amazon? Does wearing multiple hats and working in a fast-paced, entrepreneurial environment sound like a good fit? Then consider joining Amazon Ads.\nAmazon Advertising operates at the intersection of e-commerce and advertising, offering a range of digital advertising solutions with the goal of helping customers discover and shop for anything they want to buy. The Amazon Advertising business is growing at a fast pace and this team s mission is to apply technology to accelerate that growth through best-in-class software engineering, data engineering, and business intelligence.\n\nWithin Amazons advertising ecosystem, the QA team serves as the cornerstone of quality assurance focusing on supporting testing for all initiatives that materially change the advertiser experience or involve significant re-architecture of the ad stack. This includes comprehensive testing of all Tier 1 launches, our most critical deployments that directly affect revenue and customer experience. We drive engineering excellence through automation, developing and maintaining tools that enhance developer productivity and solve testing challenges. We evangelize quality related best practices, building mechanisms to track and report them.\n\nWe are hiring experienced Quality Assurance Engineer (QAE) to drive quality excellence in our advertising systems. In this role, you will architect, design and build test suites and frameworks to push our advertising systems to their limits and beyond. You will work with program management, development teams and our QA organization to understand the customer requirements, scope out features and then work side-by-side with the team to ensure our high quality bar is met and raised. Youll be responsible for raising our quality standards through innovative automation solutions and technical leadership, while consistently exceeding delivery expectations.\n\n\nParticipate in the full development life cycle, working within broadly defined parameters, including test plan execution and software quality needs.\nWriting and executing test plans, designing and developing test tools, automation, debugging and reporting code bugs and pushing quality upstream.\nOwn the delivery of an entire software development test suites and frameworks.\nWork closely with the technical leaders to develop the best approach for testing our functionality at scale. You are capable of understanding the interaction between the components in a distributed system in order to ensure they are functioning properly.\nCreate and execute appropriate test strategies and processes that align with business objectives and project timelines. 2+ years of quality assurance engineering experience\nExperience in manual testing\nExperience in automation testing\nExperience designing and planning test conditions, test scripts, and test data sets to ensure appropriate and adequate coverage and control Experience in API & Mobile testing\nExperience with technologies (like Selenium, Junit, TestNG, and other open source tools)\nExperience with at least one modern language such as Java, Python, C++, or C# including object-oriented design",,,,"['C++', 'Manual testing', 'Test scripts', 'Debugging', 'Test planning', 'Selenium', 'software quality', 'Business intelligence', 'Open source', 'Python']",2025-06-12 14:50:47
AutoIT Solutioning Engineer-Staff,Qualcomm,3 - 7 years,Not Disclosed,['Bengaluru'],"Job Area: Engineering Group, Engineering Group > Software Engineering\n \n\nQualcomm Overview: \nQualcomm is a company of inventors that unlocked 5G ushering in an age of rapid acceleration in connectivity and new possibilities that will transform industries, create jobs, and enrich lives. But this is just the beginning. It takes inventive minds with diverse skills, backgrounds, and cultures to transform 5Gs potential into world-changing technologies and products. This is the Invention Age - and this is where you come in.\n\nGeneral Summary:\n\nAs a Site Reliability Engineer (SRE), youll be part of a highly collaborative team focused on provisioning and maintaining infrastructure and services with stability, sustainability, and security always on your mind. You will work in a self-guided, cross-functional team responsible for everything from modernizing traditional services and applications to deploying new technology. You'll collaborate closely with software engineers, data scientists, and product managers to maintain and optimize our systems. If you're passionate about automotive technology, software reliability, and continuous improvement, this role is perfect for you.\n\nYour Guiding Principles:\n\n\nAutomationYou understand the power of automation and ""infrastructure as code"" concepts. Automation is your primary consideration in problem-solving.\n\n\nCollaboration: You share a common language with fellow engineers, understand their needs, and thrive working in a high trust collaborate culture in which people are rewarded for taking risks.\n\n\nData-drivenYou understand why decisions are supported by facts and not opinions. You have experience applying logical approach to decision making. Skilled at metric collection and using that data to drive change.\n\n\nDebuggingYou understand debugging principles and are adept at applying them routinely and successfully.\n\n\nDevSecOps: You understand that DevSecOps is a culture which needs to be cultivated and you can help nurture those philosophies.\n\n\nSecurityYou know how to layer appropriate security within solutions across the lifecycle. You understand the security implications and consequences of any deployment.\n\n\nSelf-Driven: You understand how to prioritize work and time allocation at a personal and team level.\n\n\nStability: You know what it means to deliver a service with a high degree of reliability and are intimately familiar with how disruptions impact consumers.\n\n\nSustainability: You avoid one off solutions which are challenging to support. Instead, your solutions are aligned with team goals and strategic vision. You routinely dedicate cycles to reducing technical debt.\n\n\nWhat you have:\nExtensive Linux experience with servers and workstations. You can easily navigate the CLI, knowledgeable with typical Linux troubleshooting tools, and have a broad understanding of Ubuntu and RedHat.\nThe ability to automate through scripting languages such as Python, Bash, Go, etc.\nThe skill to provide sufficient automated test coverage of various implementations.\nYou have familiarity with Jenkins, Puppet, Splunk, JIRA, Vault, Docker, AWS, Cloud services, etc.\nAbility to respond rapidly to changing landscapes while providing stable, reliable, and secure services to customers.\nYou have a passion for continuous learning and leverage the scientific method to ensure nothing is taken for granted.\n\n\nResponsibilities:\nSystem Monitoring and Incident Response:\nMonitor system health, detect anomalies, and respond promptly to incidents.\nInvestigate and troubleshoot issues related to services.\nImplement proactive measures to prevent service disruptions.\nInfrastructure Automation:\nDevelop and maintain infrastructure-as-code (IaC) scripts for deployment and scaling.\nAutomate routine tasks to improve efficiency and reduce manual intervention.\nPerformance Optimization:\nCollaborate with development teams to optimize software performance.\nIdentify bottlenecks and implement solutions to enhance system speed and reliability.\nCapacity Planning:\nForecast resource requirements based on traffic patterns and business growth.\nScale infrastructure to accommodate increasing demand.\nSecurity and Compliance:\nEnsure compliance with industry standards and best practices.\nImplement security controls and participate in security audits.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['docker', 'linux', 'python', 'puppet', 'aws', 'kubernetes', 'owasp', 'golang', 'redhat linux', 'vulnerability assessment', 'ansible', 'microservices', 'java', 'devops', 'jenkins', 'debugging', 'penetration testing', 'vault', 'jira', 'cloud services', 'ubuntu', 'microsoft azure', 'splunk', 'bash', 'devsecops', 'terraform']",2025-06-12 14:50:49
Staff System Test Engineer,Qualcomm,8 - 13 years,Not Disclosed,['Bengaluru'],"Job Area: Engineering Group, Engineering Group > Systems Test Engineering\n\nGeneral Summary:\n\n\n\nWe are seeking a Senior Staff AI System-Level Test Engineer to lead end-to-end testing of Retrieval-Augmented Generation (RAG) AI systems for Hybrid, Edge-AI Inference solutions. This role will focus on designing, developing, and executing comprehensive test strategies for evaluating the reliability, accuracy, usability and scalability of large-scale AI models integrated with external knowledge retrieval systems.\n\nThe ideal candidate needs to have deep expertise in AI testing methodologies, experience with large language models (LLMs), expertise in building test solutions for AI Inference stacks, RAG, search/retrieval architecture, and a strong background in automation frameworks, performance validation, and building E2E automation architecture.\n\nExperience testing large-scale generative AI applications, familiarity with LangChain, LlamaIndex, or other RAG-specific frameworks, and knowledge of adversarial testing techniques for AI robustness are preferred qualifications\n\nKey Responsibilities:\n\nTest Strategy & Planning\nDefine end-to-end test strategies for RAG, retrieval, generation, response coherence, and knowledge correctness\nDevelop test plans & automation frameworks to validate system performance across real-world scenarios.\nHands-on experience in benchmarking and optimizing Deep Learning Models on AI Accelerators/GPUs\nImplement E2E solutions to integrate Inference systems with customer software workflows\nIdentify and implement metrics to measure retrieval accuracy, LLM response quality\n\n\nTest Automation\nBuild automated pipelines for regression, integration, and adversarial testing of RAG workflows.\nValidate search relevance, document ranking, and context injection into LLMs using rigorous test cases.\nCollaborate with ML engineers and data scientists to debug model failures and identify areas for improvement.\nConduct scalability and latency tests for retrieval-heavy applications. Analyze failure patterns, drift detection, and robustness against hallucinations and misinformation.\n\n\nCollaboration\nWork closely with AI research, engineering teams & customer teams to align testing with business requirements.\nGenerate test reports, dashboards, and insights to drive model improvements.\nStay up to date with the latest AI testing frameworks, LLM evaluation benchmarks, and retrieval models.\n\n\nRequired Qualifications:\n8+ years of experience in AI/ML system testing, software quality engineering, or related fields.\nBachelors or masters degree in computer science engineering/ data science / AI/ML\nHands-on experience with test automation frameworks (e.g., PyTest, Robot Framework, JMeter).\nProficiency in Python, SQL, API testing, vector databases (e.g., FAISS, Weaviate, Pinecone) and retrieval pipelines.\nExperience with ML model validation metrics (e.g., BLEU, ROUGE, MRR, NDCG).\nExpertise in CI/CD pipelines, cloud platforms (AWS/GCP/Azure), and containerization (Docker, Kubernetes).\n\n\nWhy Join Us\nWork on cutting-edge AI retrieval-augmented generation technologies\nCollaborate with world-class AI researchers and engineers.\n\nIf you are passionate about AI system testing and ensuring the reliability of next-generation generative models, apply now!\n\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 6+ years of Systems Test Engineering or related work experience.\nOR\nMaster's degree in Engineering, Information Systems, Computer Science, or related field and 5+ years of Systems Test Engineering or related work experience.\nOR\nPhD in Engineering, Information Systems, Computer Science, or related field and 4+ years of Systems Test Engineering or related work experience.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['automation framework', 'continuous integration', 'python', 'sql', 'ci cd pipeline', 'kubernetes', 'ci/cd', 'cloud platforms', 'software quality', 'artificial intelligence', 'docker', 'test engineering', 'quality engineering', 'e2e', 'testing methodologies', 'vector', 'aws', 'api testing']",2025-06-12 14:50:52
Artificial Intelligence Intern,Kumaran Systems,0 - 1 years,4.5-5 Lacs P.A.,['Chennai( Siruseri Sipcot IT Park )'],"We are looking for a passionate and motivated AI Developer Fresher to join our growing AI team. This role will focus on Generative AI (GenAI) technologies such as large language models (LLMs), diffusion models, and other cutting-edge machine learning techniques.\n\nAs a fresher, youll work closely with senior AI engineers and data scientists to build and fine-tune generative models, contribute to prompt engineering, and support model integration into real-world applications.",,,,"['Data Science', 'Mechine Learning', 'Artificial Intelligence', 'GEN AI', 'Python']",2025-06-12 14:50:54
"Associate Analyst, R Programmer-1",Mastercard,1 - 4 years,Not Disclosed,['Gurugram'],"We are looking for an R programmer to join Mastercard s Economics Institute, reporting to the team lead for Economics Technology.\nAn individual who will:\ncreate clear, compelling data visualisations that communicate economic insights to diverse audiences\ndevelop reusable R functions and packages to support analysis and automation\ncreate and format analytical content using R Markdown and/or Quarto\ndesign and build scalable Shiny apps\ndevelop interactive visualisations using JavaScript charting libraries (eg Plotly, Highcharts, D3.js) or front-end frameworks (eg React, Angular, Vue.js)work with databases and data platforms (eg. SQL, Hadoop)\nwrite clear, we'll-documented code that others can understand and maintain\ncollaborate using Git for version control\nAll About You\n\nproficient in R and the RStudio IDE\nproficient in R packages like dplyr for data cleaning, transformation, and aggregation\nfamiliarity with dependency management and documentation in R (eg roxygen2)\nfamiliar with version control concepts and tools (eg Git, GitHub, Bitbucket) for collaborative development\nexperience writing SQL and working with relational databases\ncreative and passionate about data, coding, and technology\nstrong collaborator who can also work independently organized and able to prioritise work across multiple projects comfortable working with engineers, product owners, data scientists, economists",Industry Type: Financial Services,Department: Other,"Employment Type: Full Time, Permanent","['Automation', 'Front end', 'Associate Analyst', 'Version control', 'Coding', 'Information security', 'Analytical', 'Manager Technology', 'Corporate security', 'SQL']",2025-06-12 14:50:56
"Associate Analyst, R Programmer-2",Mastercard,3 - 6 years,Not Disclosed,['Gurugram'],"The Mastercard Economics Institute (MEI) is an economics lab powering scale at Mastercard by owning economic thought leadership in support of Mastercard s efforts to build a more inclusive and sustainable digital economy\nThe Economics Institute was launched in 2020 to analyze economic trends through the lens of the consumer to deliver tailored and actionable insights on economic issues for customers, partners and policymakers\nThe Institute is composed of a team of economists and data scientists that utilize & synthesize the anonymized and aggregated data from the Mastercard network together with public data to bring powerful insights to life, in the form of 1:1 presentation, global thought leadership, media participation, and commercial work through the company s product suites\nAbout the Role\nWe are looking for an R programmer to join Mastercard s Economics Institute, reporting to the team lead for Economics Technology. An individual who will:\ncreate clear, compelling data visualisations that communicate economic insights to diverse audiences\ndevelop reusable R functions and packages to support analysis and automation\ncreate and format analytical content using R Markdown and/or Quarto\ndesign and build scalable Shiny apps\ndevelop interactive visualisations using JavaScript charting libraries (eg Plotly, Highcharts, D3.js) or front-end frameworks (eg React, Angular, Vue.js)work with databases and data platforms (eg. SQL, Hadoop)\nwrite clear, we'll-documented code that others can understand and maintain\ncollaborate using Git for version control\nAll About You\n\nproficient in R and the RStudio IDE\nproficient in R packages like dplyr for data cleaning, transformation, and aggregation\nfamiliarity with dependency management and documentation in R (eg roxygen2)\nfamiliar with version control concepts and tools (eg Git, GitHub, Bitbucket) for collaborative development\nexperience writing SQL and working with relational databases\ncreative and passionate about data, coding, and technology\nstrong collaborator who can also work independently organized and able to prioritise work across multiple projects comfortable working with engineers, product owners, data scientists, economists",Industry Type: Financial Services,Department: Other,"Employment Type: Full Time, Permanent","['Automation', 'Front end', 'Associate Analyst', 'Version control', 'Coding', 'Information security', 'Analytical', 'Manager Technology', 'Corporate security', 'SQL']",2025-06-12 14:50:58
Machine Learning Engineer 2,Adobe,4 - 7 years,Not Disclosed,['Noida'],"Develop algorithms that apply deep learning and innovative methods in NLP & computer vision, combined with traditional large sophisticated solutions/codebases!\nDeveloping innovative solutions using Generative AI, Python, Machine Learning and Data Science!\nBuild experiments, algorithms and ship solution that not only yield high accuracy but are also crafted and engineered to scale.\nCollaborate across multiple research and engineering teams, making the tradeoffs required to rapidly deliver AI/ML software solutions.\nDriven, Energetic and a team player are must haves.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer vision', 'deep learning', 'data science', 'Machine learning', 'Research', 'Adobe', 'Software solutions', 'Python']",2025-06-12 14:51:01
"Associate Analyst, R Programmer-3",Mastercard,4 - 7 years,Not Disclosed,['Gurugram'],"The Mastercard Economics Institute (MEI) is an economics lab powering scale at Mastercard by owning economic thought leadership in support of Mastercard s efforts to build a more inclusive and sustainable digital economy\nThe Economics Institute was launched in 2020 to analyze economic trends through the lens of the consumer to deliver tailored and actionable insights on economic issues for customers, partners and policymakers\nThe Institute is composed of a team of economists and data scientists that utilize & synthesize the anonymized and aggregated data from the Mastercard network together with public data to bring powerful insights to life, in the form of 1:1 presentation, global thought leadership, media participation, and commercial work through the company s product suites\nAbout the Role\nWe are looking for an R programmer to join Mastercard s Economics Institute, reporting to the team lead for Economics Technology.\n  An individual who will:\ncreate clear, compelling data visualisations that communicate economic insights to diverse audiences\ndevelop reusable R functions and packages to support analysis and automation\ncreate and format analytical content using R Markdown and/or Quarto\ndesign and build scalable Shiny apps\ndevelop interactive visualisations using JavaScript charting libraries (eg Plotly, Highcharts, D3.js) or front-end frameworks (eg React, Angular, Vue.js)work with databases and data platforms (eg. SQL, Hadoop)\nwrite clear, we'll-documented code that others can understand and maintain\ncollaborate using Git for version control\nAll About You\n\nproficient in R and the RStudio IDE\nproficient in R packages like dplyr for data cleaning, transformation, and aggregation\nfamiliarity with dependency management and documentation in R (eg roxygen2)\nfamiliar with version control concepts and tools (eg Git, GitHub, Bitbucket) for collaborative development\nexperience writing SQL and working with relational databases\ncreative and passionate about data, coding, and technology\nstrong collaborator who can also work independently organized and able to prioritise work across multiple projects comfortable working with engineers, product owners, data scientists, economists",Industry Type: Financial Services,Department: Other,"Employment Type: Full Time, Permanent","['Automation', 'Front end', 'Associate Analyst', 'Version control', 'Coding', 'Information security', 'Analytical', 'Manager Technology', 'Corporate security', 'SQL']",2025-06-12 14:51:03
"Associate Analyst, R Programmer-1",Dynamic Yield,1 - 4 years,Not Disclosed,['Gurugram'],"Our Purpose\nTitle and Summary\nAssociate Analyst, R Programmer-1\nOverview\nThe Mastercard Economics Institute (MEI) is an economics lab powering scale at Mastercard by owning economic thought leadership in support of Mastercard s efforts to build a more inclusive and sustainable digital economy\nThe Economics Institute was launched in 2020 to analyze economic trends through the lens of the consumer to deliver tailored and actionable insights on economic issues for customers, partners and policymakers\nThe Institute is composed of a team of economists and data scientists that utilize & synthesize the anonymized and aggregated data from the Mastercard network together with public data to bring powerful insights to life, in the form of 1:1 presentation, global thought leadership, media participation, and commercial work through the company s product suites\nAbout the Role\nWe are looking for an R programmer to join Mastercard s Economics Institute, reporting to the team lead for Economics Technology. An individual who will:\ncreate clear, compelling data visualisations that communicate economic insights to diverse audiences\ndevelop reusable R functions and packages to support analysis and automation\ncreate and format analytical content using R Markdown and/or Quarto\ndesign and build scalable Shiny apps\ndevelop interactive visualisations using JavaScript charting libraries (e.g. Plotly, Highcharts, D3.js) or front-end frameworks (e.g. React, Angular, Vue.js)work with databases and data platforms (eg. SQL, Hadoop)\nwrite clear, well-documented code that others can understand and maintain\ncollaborate using Git for version control\nAll About You\n\nproficient in R and the RStudio IDE\nproficient in R packages like dplyr for data cleaning, transformation, and aggregation\nfamiliarity with dependency management and documentation in R (e.g. roxygen2)\nfamiliar with version control concepts and tools (e.g. Git, GitHub, Bitbucket) for collaborative development\nexperience writing SQL and working with relational databases\ncreative and passionate about data, coding, and technology\nstrong collaborator who can also work independently organized and able to prioritise work across multiple projects comfortable working with engineers, product owners, data scientists, economists",Industry Type: Software Product,Department: Other,"Employment Type: Full Time, Permanent","['Automation', 'Front end', 'Associate Analyst', 'Version control', 'Coding', 'Information security', 'Analytical', 'Manager Technology', 'Corporate security', 'SQL']",2025-06-12 14:51:06
Asset & Wealth Management - AM FI Macro Strats - Associate,Goldman Sachs,2 - 7 years,Not Disclosed,['Bengaluru'],"Who We Are\nAt Goldman Sachs, we connect people, capital and ideas to help solve problems for our clients. We are a leading global financial services firm providing investment banking, securities and investment management services to a substantial and diversified client base that includes corporations, financial institutions, governments and individuals.\nAt Goldman Sachs, our Engineers don t just make things - we make things possible. We change the world by connecting people and capital with ideas and solve the most challenging and pressing engineering problems for our clients. Our engineering teams build scalable software and systems, architect low latency infrastructure solutions, proactively guard against cyber threats, and leverage machine learning alongside financial engineering to continuously turn data into action.\nEngineering, which is comprised of our Technology Division and global strategist groups, is at the critical center of our business. Our dynamic environment requires innovative strategic thinking. Want to push the limit of digital possibilities? Start here.\nGoldman Sachs Asset & Wealth Management\nAs one of the worlds leading asset managers, our mission is to help our clients achieve their investment goals. To best serve our clients diverse and evolving needs, we have built our business to be global, broad and deep across asset classes, geographies and solutions.\nGoldman Sachs Asset & Wealth Management is one of the worlds leading asset management institutions. AWM delivers innovative investment solutions managing close to Two Trillion US Dollars on a global, multi-product platform. In addition to traditional products (e.g. Equities, Fixed Income) our product offering also includes Hedge Funds, Private Equity, Fund of Funds, Quantitative Strategies, Fundamental Equity and a Multi-Asset Pension Solutions Business. Software is engineered in a fast-paced, dynamic environment, adapting to market and customer needs to deliver robust solutions in an ever-changing business environment. AM Data Engineering builds on top of cutting edge in-house and cloud platforms complimented with a strong focus on leveraging open source solutions.\nBusiness Overview\nThe External Investing Group ( XIG ) provides investors with investment and advisory solutions across leading private equity funds, hedge fund managers, real estate managers, public equity strategies, and fixed income strategies. XIG manages globally diversified programs, targeted sector-specific strategies, customized portfolios, and a range of advisory services. Our investors access opportunities through new fund commitments, fund-of-fund investments, strategic partnerships, secondary-market investments, co-investments, and seed-capital investments. With over 350 professionals across 11 offices around the world, XIG provides manager diligence, portfolio construction, risk management, and liquidity solutions to investors, drawing on Goldman Sachs market insights and risk management expertise. We extend these global capabilities to the world s leading sovereign wealth funds, pension plans, governments, financial institutions, endowments, foundations, and family offices, for which we invest or advise on over $300 billion of alternative investments, public equity strategies, and fixed income strategies.\nWhat We Do\nWithin Asset Management, Strategists (also known as Strats ) play important roles in research, valuation, portfolio construction, and risk management analytics. A Strategist will apply quantitative and analytical methods to come up with solutions that are accurate, robust, and scalable. Strats are innovators and problem-solvers, building novel and creative solutions for manager selection, portfolio construction, and risk management. You will develop advanced computational models, architectures, and applications to meet the challenges of a rapidly growing and evolving business.\nStrats collaborate across the business to develop solutions. These daily interactions with other team members across geographies demand an ability to communicate clearly about complex financial, business, and mathematical concepts. We look for creative collaborators who evolve, adapt to change, and thrive in a fast-paced global environment.\nBasic Qualifications\nOutstanding background in a quantitative discipline, with excellent analytical, quantitative, and problem-solving skills, and demonstrated abilities in research and data visualization\nProgramming expertise in a scripting language (e.g. Python, R, Matlab)\nStrong general and technical communication skills, with an ability to effectively articulate complex financial and mathematical concepts\nCreativity and problem-solving skills\nAbility to work independently and in a team environment\n2+ years of applicable experience\nGoldman Sachs Engineering Culture",Industry Type: Banking,"Department: BFSI, Investments & Trading","Employment Type: Full Time, Permanent","['Wealth management', 'Analytical', 'Fixed income', 'Investment banking', 'Asset management', 'Investment management', 'Risk management', 'Private equity', 'Analytics', 'Financial services']",2025-06-12 14:51:08
"Associate Analyst, R Programmer-2",Dynamic Yield,3 - 6 years,Not Disclosed,['Gurugram'],"Our Purpose\nTitle and Summary\nAssociate Analyst, R Programmer-2\nOverview\nThe Mastercard Economics Institute (MEI) is an economics lab powering scale at Mastercard by owning economic thought leadership in support of Mastercard s efforts to build a more inclusive and sustainable digital economy\nThe Economics Institute was launched in 2020 to analyze economic trends through the lens of the consumer to deliver tailored and actionable insights on economic issues for customers, partners and policymakers\nThe Institute is composed of a team of economists and data scientists that utilize & synthesize the anonymized and aggregated data from the Mastercard network together with public data to bring powerful insights to life, in the form of 1:1 presentation, global thought leadership, media participation, and commercial work through the company s product suites\nAbout the Role\nWe are looking for an R programmer to join Mastercard s Economics Institute, reporting to the team lead for Economics Technology. An individual who will:\ncreate clear, compelling data visualisations that communicate economic insights to diverse audiences\ndevelop reusable R functions and packages to support analysis and automation\ncreate and format analytical content using R Markdown and/or Quarto\ndesign and build scalable Shiny apps\ndevelop interactive visualisations using JavaScript charting libraries (e.g. Plotly, Highcharts, D3.js) or front-end frameworks (e.g. React, Angular, Vue.js)work with databases and data platforms (eg. SQL, Hadoop)\nwrite clear, well-documented code that others can understand and maintain\ncollaborate using Git for version control\nAll About You\n\nproficient in R and the RStudio IDE\nproficient in R packages like dplyr for data cleaning, transformation, and aggregation\nfamiliarity with dependency management and documentation in R (e.g. roxygen2)\nfamiliar with version control concepts and tools (e.g. Git, GitHub, Bitbucket) for collaborative development\nexperience writing SQL and working with relational databases\ncreative and passionate about data, coding, and technology\nstrong collaborator who can also work independently organized and able to prioritise work across multiple projects comfortable working with engineers, product owners, data scientists, economists",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Automation', 'Front end', 'Associate Analyst', 'Version control', 'Coding', 'Information security', 'Analytical', 'Manager Technology', 'Corporate security', 'SQL']",2025-06-12 14:51:10
Senior Analytics Consultant,Wells Fargo,4 - 9 years,Not Disclosed,['Hyderabad'],"About this role:\nWells Fargo is seeking a Senior Analytics Consultant with a proven track record of success preferably in the banking industry.\n\nIn this role, you will:\nConsult, review and research moderately complex business, operational, and technical challenges that require an in-depth evaluation of variable data factors",,,,"['data manipulation', 'Data Engineering', 'data analysis', 'data management', 'SQL']",2025-06-12 14:51:13
Business Analyst,CGI,6 - 11 years,Not Disclosed,['Hyderabad'],"Business Data Analyst - HealthCare\nPosition Description\nJob Summary\nWe are seeking an experienced and results-driven Business Data Analyst with 5+ years of hands-on experience in data analytics, visualization, and business insight generation. This role is ideal for someone who thrives at the intersection of business and datatranslating complex data sets into compelling insights, dashboards, and strategies that support decision-making across the organization.\nYou will collaborate closely with stakeholders across departments to identify business needs, design and build analytical solutions, and tell compelling data stories using advanced visualization tools.\nKey Responsibilities\nData Analytics & Insights Analyze large and complex data sets to identify trends, anomalies, and opportunities that help drive business strategy and operational efficiency.\n• Dashboard Development & Data Visualization Design, develop, and maintain interactive dashboards and visual reports using tools like Power BI, Tableau, or Looker to enable data-driven decisions.\n• Business Stakeholder Engagement Collaborate with cross-functional teams to understand business goals, define metrics, and convert ambiguous requirements into concrete analytical deliverables.\n• KPI Definition & Performance Monitoring Define, track, and report key performance indicators (KPIs), ensuring alignment with business objectives and consistent measurement across teams.\n• Data Modeling & Reporting Automation Work with data engineering and BI teams to create scalable, reusable data models and automate recurring reports and analysis processes.\n• Storytelling with Data Communicate findings through clear narratives supported by data visualizations and actionable recommendations to both technical and non-technical audiences.\n• Data Quality & Governance Ensure accuracy, consistency, and integrity of data through validation, testing, and documentation practices.\nRequired Qualifications\nBachelor’s or Master’s degree in Business, Economics, Statistics, Computer Science, Information Systems, or a related field.\n• 5+ years of professional experience in a data analyst or business analyst role with a focus on data visualization and analytics.\n• Proficiency in data visualization tools: Power BI, Tableau, Looker (at least one).\n• Strong experience in SQL and working with relational databases to extract, manipulate, and analyze data.\n• Deep understanding of business processes, KPIs, and analytical methods.\n• Excellent problem-solving skills with attention to detail and accuracy.\n• Strong communication and stakeholder management skills with the ability to explain technical concepts in a clear and business-friendly manner.\n• Experience working in Agile or fast-paced environments.\nPreferred Qualifications\nExperience working with cloud data platforms (e.g., Snowflake, BigQuery, Redshift).\n• Exposure to Python or R for data manipulation and statistical analysis.\n• Knowledge of data warehousing, dimensional modeling, or ELT/ETL processes.\n• Domain experience in Healthcare is a plus.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Healthcare Domain', 'Bigquery', 'Redshift Aws', 'Snowflake', 'Data Analytics', 'Data Visualization', 'Python']",2025-06-12 14:51:15
"Associate Analyst, R Programmer-3",Dynamic Yield,4 - 7 years,Not Disclosed,['Gurugram'],"Our Purpose\nTitle and Summary\nAssociate Analyst, R Programmer-3\nOverview\nThe Mastercard Economics Institute (MEI) is an economics lab powering scale at Mastercard by owning economic thought leadership in support of Mastercard s efforts to build a more inclusive and sustainable digital economy\nThe Economics Institute was launched in 2020 to analyze economic trends through the lens of the consumer to deliver tailored and actionable insights on economic issues for customers, partners and policymakers\nThe Institute is composed of a team of economists and data scientists that utilize & synthesize the anonymized and aggregated data from the Mastercard network together with public data to bring powerful insights to life, in the form of 1:1 presentation, global thought leadership, media participation, and commercial work through the company s product suites\nAbout the Role\nWe are looking for an R programmer to join Mastercard s Economics Institute, reporting to the team lead for Economics Technology. An individual who will:\ncreate clear, compelling data visualisations that communicate economic insights to diverse audiences\ndevelop reusable R functions and packages to support analysis and automation\ncreate and format analytical content using R Markdown and/or Quarto\ndesign and build scalable Shiny apps\ndevelop interactive visualisations using JavaScript charting libraries (e.g. Plotly, Highcharts, D3.js) or front-end frameworks (e.g. React, Angular, Vue.js)work with databases and data platforms (eg. SQL, Hadoop)\nwrite clear, well-documented code that others can understand and maintain\ncollaborate using Git for version control\nAll About You\n\nproficient in R and the RStudio IDE\nproficient in R packages like dplyr for data cleaning, transformation, and aggregation\nfamiliarity with dependency management and documentation in R (e.g. roxygen2)\nfamiliar with version control concepts and tools (e.g. Git, GitHub, Bitbucket) for collaborative development\nexperience writing SQL and working with relational databases\ncreative and passionate about data, coding, and technology\nstrong collaborator who can also work independently organized and able to prioritise work across multiple projects comfortable working with engineers, product owners, data scientists, economists",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Automation', 'Front end', 'Associate Analyst', 'Version control', 'Coding', 'Information security', 'Analytical', 'Manager Technology', 'Corporate security', 'SQL']",2025-06-12 14:51:17
Python Engineer - ML/Big Query - Hyd/Chennai/Bangalore,People staffing Solutions,5 - 10 years,12-20 Lacs P.A.,"['Hyderabad', 'Chennai', 'Bengaluru']","Key Responsibilities:\nDesign, develop, and maintain scalable and optimized ETL pipelines using Python and SQL.\nWork with Google BigQuery and other cloud-based platforms to build data warehousing solutions.\nDevelop and deploy ML models; collaborate with Data Scientists for productionizing models.\nWrite efficient and optimized SQL queries for large-scale data processing.\nBuild APIs using Flask/Django for machine learning and data applications.\nWork with both SQL and NoSQL databases including Elasticsearch.\nImplement data ingestion using batch and streaming technologies.\nEnsure data quality, integrity, and governance across the data lifecycle.\nAutomate and optimize CI/CD pipelines for data solutions.\nCollaborate with cross-functional teams to gather data requirements and deliver solutions.\nTroubleshoot and monitor data pipelines for seamless operations.\nRequired Skills & Qualifications:\nBachelor's or Master's degree in Computer Science, Engineering, or related field.\n5+ years of experience with Python in a data engineering and/or ML context.\nStrong hands-on experience with SQL, BigQuery, and cloud data platforms (preferably GCP).\nPractical knowledge of ML concepts and experience developing ML models.\nProficiency in frameworks such as Flask and Django.\nExperience with NoSQL databases and data streaming technologies.\nSolid understanding of data modeling, warehousing, and ETL frameworks.\nFamiliarity with CI/CD tools and automation best practices.\nExcellent communication, problem-solving, and collaboration skills.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Django', 'Machine Learning', 'Python', 'SQL', 'Pandas', 'Numpy', 'Ml', 'Flask']",2025-06-12 14:51:19
AI Test Lead,Naukri,8 - 13 years,20-32.5 Lacs P.A.,['Bengaluru'],"Role & responsibilities\nKey Responsibilities:\nAI Testing Strategy and Planning\nCollaborate with cross-functional teams to develop comprehensive AI testing strategies and plans for AI-powered applications.\nWork closely with product managers, data scientists, and developers to understand AI model requirements, use cases, and project goals.\nDefine the scope and objectives of AI testing efforts, including performance, accuracy, bias detection, and robustness of AI models. Test Execution for AI Models and Algorithms\nDesign, develop, and execute test cases for AI systems and models (including machine learning and deep learning algorithms).\nTest and validate AI solutions across various stages of the development lifecycle, including model training, testing, and deployment.\nEnsure that AI models meet business requirements and perform accurately under various real-world conditions.\nEvaluate the performance of AI models by assessing speed, efficiency, scalability, and resource utilization.\nPerform manual and automated testing on AI-based applications, platforms, and solutions.\nAI Model Accuracy and Validation\nTest AI models for accuracy, precision, recall, F1 score, and other performance metrics.\nEnsure AI models' fairness by conducting tests for potential bias in decisionmaking processes, especially in clinical or medical applications.\nValidate AI model predictions against real-world data, ensuring that results are consistent, reliable, and actionable. Also, need to look the test results from a business perspective and help evaluate the balance between risks and benefits.\nCollaboration and Knowledge Sharing\nWork with data scientists, AI engineers and Test Manager to improve testing methodologies and continuously optimize AI model testing processes.\nProvide feedback on AI models, pointing out any potential improvements in testing coverage or areas for model retraining.\nCommunicate findings, bugs, and issues related to AI models to technical teams, ensuring prompt resolution.\nHelp the team set up AI Testing standards, make informed decisions, and build knowledge across projects\nHelp the team in decision-making processes, such as whether to continue or stop investments based on testing results. Test Automation for AI Projects\nDevelop and implement automated testing scripts and frameworks specifically designed for AI applications.\nUtilize AI testing tools and frameworks (RAGAS etc.) to automate the validation of AI models and algorithms.\nIntegrate automated AI testing within continuous integration and continuous deployment (CI/CD) pipelines.\nCompliance and Regulatory Testing\nEnsure that AI applications comply with industry-specific regulations, especially in the pharma and healthcare sectors (e.g., FDA regulations, HIPAA compliance).\nVerify that all AI-driven processes adhere to ethical standards and data privacy laws.\nContinuous Improvement and Research\nStay up-to-date with the latest trends, tools, and techniques in AI testing and apply these advancements to optimize the testing process.\nParticipate in AI testing forums and workshops, contributing insights to improve best practices within the team. Reporting and Documentation\nDocument test results, methodologies, and issues clearly, providing insights into test coverage, risk analysis, and performance benchmarks.\nPrepare detailed reports for both technical and non-technical stakeholders, summarizing testing outcomes and potential risks associated with AI implementations.\nAssist in the creation and maintenance of knowledge-sharing platforms related to AI testing best practices.\nKey Skills and Qualifications:\nTechnical Expertise\nStrong knowledge of AI/ML testing methodologies and best practices.\nExperience with any AI development frameworks and libraries such as TensorFlow, Keras, PyTorch, scikit-learn, RAGAS and MLlib.\nExperience in testing tools and environments for AI-based systems (e.g., Jupyter Notebooks, Apache Spark, and DataRobot).\nExperience with performance testing tools like Grafana K6 and JMeter for AI solutions.\nKnowledge of Python (Must to have), R, JavaScript or other programming languages frequently used in AI/ML.\nKnowledge of cloud technologies like Microsoft Azure / AWS.\nUnderstanding of test automation frameworks and experience in tools like Cypress, Playwright and Pytest for automating AI tests. AI Model Evaluation\nSolid understanding of machine learning and deep learning models, including supervised and unsupervised learning techniques.\nFamiliarity with evaluating AI models on metrics such as accuracy, precision, recall, F1 score, confusion matrices, and AUC.\nAbility to identify and test for model biases, fairness, and ethical implications, especially in sensitive applications like healthcare and pharma. Analytical and Problem-Solving Skills\nStrong problem-solving abilities and keen attention to detail, with a systematic approach to diagnosing and resolving AI-related issues.\nAbility to perform root cause analysis of issues in AI algorithms and suggest actionable fixes.\nCollaboration and Communication\nExcellent teamwork and communication skills, with the ability to collaborate with cross-functional teams, including data scientists, engineers, and product managers.\nStrong verbal and written communication skills to convey technical information clearly and concisely to both technical and non-technical stakeholders.\nExperience\nMinimum of 8+ years experience in software testing, with at least 2 years focused on testing AI/ML models or AI-based applications.\nProven experience in testing AI/ML algorithms in production or staging environments.\nExperience in testing Visual AI Assistant Applications is good to have.\nExperience working in a regulated industry (such as pharmaceuticals or healthcare) is a plus.\nPreferred Qualifications:\nExperience with cloud platforms (e.g., AWS, Azure) for deploying AI applications and models. Certification in AWS/Azure will be good to have.\nFamiliarity with DevOps practices and integrating AI testing into CI/CD pipelines.\nCertification in AI/ML or related testing frameworks (e.g. ISTQB AI Tester)\nThis AI Tester role is a unique opportunity to shape the future of AI in the pharmaceutical industry. If youre passionate about AI, testing, and making a difference in healthcare, we encourage you to apply.\n\nPreferred candidate profile",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Automation Testing', 'AI/ ML', 'Python', 'Performance Testing', 'Automation Strategy', 'AI Framework', 'AI testing']",2025-06-12 14:51:22
Senior Software Engineer - Python Developer,FactSet,5 - 10 years,Not Disclosed,['Hyderabad'],"FactSet creates flexible, open data and software solutions for over 200,000 investment professionals worldwide, providing instant access to financial data and analytics that investors use to make crucial decisions.\nAt FactSet, our values are the foundation of everything we do. They express how we act and operate , serve as a compass in our decision-making, and play a big role in how we treat each other, our clients, and our communities. We believe that the best ideas can come from anyone, anywhere, at any time, and that curiosity is the key to anticipating our clients needs and exceeding their expectations.",,,,"['Computer science', 'C++', 'Data analysis', 'GCP', 'Analytical', 'Machine learning', 'Technical leadership', 'Monitoring', 'SQL', 'Python']",2025-06-12 14:51:24
"Senior Engineer, Application Development",S&P Global Market Intelligence,5 - 8 years,Not Disclosed,['Hyderabad'],"Grade Level (for internal use):\n10\nMarket Intelligence\nThe Role: Senior Full Stack Developer\nGrade level :10\nThe Team: You will work with a team of intelligent, ambitious, and hard-working software professionals. The team is responsible for the architecture, design, development, quality, and maintenance of the next-generation financial data web platform. Other responsibilities include transforming product requirements into technical design and implementation. You will be expected to participate in the design review process, write high-quality code, and work with a dedicated team of QA Analysts, and Infrastructure Teams\nThe Impact: Market Intelligence is seeking a Software Developer to create software design, development, and maintenance for data processing applications. This person would be part of a development team that manages and supports the internal & external applications that is supporting the business portfolio. This role expects a candidate to handle any data processing, big data application development. We have teams made up of people that learn how to work effectively together while working with the larger group of developers on our\nplatform.\nWhats in it for you:\nOpportunity to contribute to the development of a world-class Platform Engineering team .\nEngage in a highly technical, hands-on role designed to elevate team capabilities and foster continuous skill enhancement.\nBe part of a fast-paced, agile environment that processes massive volumes of dataideal for advancing your software development and data engineering expertise while working with a modern tech stack.\nContribute to the development and support of Tier-1, business-critical applications that are central to operations.\nGain exposure to and work with cutting-edge technologies including AWS Cloud , EMR and Apache NiFi .\nGrow your career within a globally distributed team , with clear opportunities for advancement and skill development.\nResponsibilities:\nDesign and develop applications, components, and common services based on development models, languages and tools, including unit testing, performance testing and monitoring and implementation\nSupport business and technology teams as necessary during design, development and delivery to ensure scalable and robust solutions\nBuild data-intensive applications and services to support and enhance fundamental financials in appropriate technologies.( C#, .Net Core, Databricsk, Spark ,Python, Scala, NIFI , SQL)\nBuild data modeling, achieve performance tuning and apply data architecture concepts\nDevelop applications adhering to secure coding practices and industry-standard coding guidelines, ensuring compliance with security best practices (e.g., OWASP) and internal governance policies.\nImplement and maintain CI/CD pipelines to streamline build, test, and deployment processes; develop comprehensive unit test cases and ensure code quality\nProvide operations support to resolve issues proactively and with utmost urgency\nEffectively manage time and multiple tasks\nCommunicate effectively, especially written with the business and other technical groups\nWhat Were Looking For:\nBasic Qualifications:\nBachelorsMasters Degree in Computer Science, Information Systems or equivalent.\nMinimum 5 to 8 years of strong hand-development experience in C#, .Net Core, Cloud Native, MS SQL Server backend development. Proficiency with Object Oriented Programming.\nAdvance SQL programming skills\nPreferred experience or familiarity with tools and technologies such as Odata, Grafana, Kibana, Big Data platforms, Apache Kafka, GitHub, AWS EMR, Terraform, and emerging areas like AI/ML and GitHub Copilot.\nHighly recommended skillset in Databricks, SPARK, Scalatechnologies.\nUnderstanding of database performance tuning in large datasets\nAbility to manage multiple priorities efficiently and effectively within specific timeframes\nExcellent logical, analytical and communication skills are essential, with strong verbal and writing proficiencies\nKnowledge of Fundamentals, or financial industry highly preferred.\nExperience in conducting application design and code reviews\nProficiency with following technologies:\nObject-oriented programming\nPrograming Languages (C#, .Net Core)\nCloud Computing\nDatabase systems (SQL, MS SQL)\nNice to have: No-SQL (Databricks, Spark, Scala, python), Scripting (Bash, Scala, Perl, Powershell)\nPreferred Qualifications:\nHands-on experience with cloud computing platforms including AWS , Azure , or Google Cloud Platform (GCP) .\nProficient in working with Snowflake and Databricks for cloud-based data analytics and processing.\nBenefits:\nHealth & Wellness: Health care coverage designed for the mind and body.\nContinuous Learning: Access a wealth of resources to grow your career and learn valuable new skills.\nInvest in Your Future: Secure your financial future through competitive pay, retirement planning, a continuing education program with a company-matched student loan contribution, and financial wellness programs.\nFamily Friendly Perks: Its not just about you. S&P Global has perks for your partners and little ones, too, with some best-in class benefits for families.\nBeyond the Basics: From retail discounts to referral incentive awardssmall perks can make a big difference.",Industry Type: Banking,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['GitHub Copilot', 'AI/ML', 'Kibana', 'python', 'GitHub', 'Scala', 'AWS EMR', 'Grafana', 'Odata', 'Big Data platforms', 'Terraform', 'Apache Kafka', 'Databricks', 'Spark']",2025-06-12 14:51:26
Sr. Developer,Cognizant,7 - 10 years,Not Disclosed,['Chennai'],"Data Engineer Skills and Qualifications\nSQL - Mandatory\nStrong knowledge of AWS services (e.g., S3, Glue, Redshift, Lambda). - Mandatory\nExperience working with DBT – Nice to have\nProficiency in PySpark or Python for big data processing. - Mandatory\nExperience with orchestration tools like Apache Airflow and AWS CodePipeline. - Mandatory\nFamiliarity with CI/CD tools and DevOps practices.",,,,"['continuous integration', 'kubernetes', 'orchestration', 'aws iam', 'modeling', 'amazon redshift', 'data warehousing', 'pyspark', 'ci/cd', 'aws codedeploy', 'tools', 'sql', 'docker', 'apache', 'java', 'data modeling', 'devops', 'linux', 'etl', 'big data', 'cd', 'python', 'airflow', 'data processing', 'javascript', 'lambda expressions', 'aws', 'etl process']",2025-06-12 14:51:29
Manager-Business Analyst,Jubilant FoodWorks (JFL),8 - 12 years,Not Disclosed,['Noida'],"The IT Business Analyst is responsible for bridging business needs and IT capabilities, ensuring that technology solutions align with strategic objectives. This role involves analysing business processes including SAP, gathering requirements, and collaborating with IT teams to develop effective solutions. The ideal candidate has strong analytical skills, deep understanding of both business processes preferably Finance background along with IT systems including SAP, and experience in project management.\n\nKey Responsibilities:\n\nBusiness Requirements Gathering\nWork with business stakeholders to identify needs, pain points, and opportunities for process improvement.\nDocument business requirements in clear, detailed formats for technical teams.\n\nSolution Design and Analysis\nAnalyze and evaluate technology solutions that best align with business requirements.\nCreate functional specifications, use cases, and workflow diagrams to communicate solutions effectively.\n\nProject Support and Coordination\nCollaborate with project managers to ensure that projects meet business goals and timelines.\nTrack progress and provide updates on requirements, ensuring adherence to project scope and budget.\n\nTesting and Quality Assurance\nDevelop test cases and participate in system testing to validate that requirements are met.\nAssist in User Acceptance Testing (UAT) and ensure successful project delivery.\n\nContinuous Improvement and Documentation\nRecommend improvements to business processes based on data analysis.\nMaintain and update documentation for requirements, processes, and system changes.\n\nPreferred qualification & skills\n\nBachelors degree preferably in Finance Business, IT, or a related field.\n8+ years of experience in business analysis or a similar role. Experience of Finance & HR domain is preferred under QSR Industry.\nStrong analytical and problem-solving skills, with experience in requirements gathering and process improvement.\nExcellent communication skills, with the ability to work cross-functionally.\nFamiliarity with project management methodologies (Agile, Waterfall).\nKnowledge of data analysis and ERP preferably SAP, CRM, or other business applications.",Industry Type: Hotels & Restaurants,Department: Project & Program Management,"Employment Type: Full Time, Permanent","['User Acceptance Testing', 'SAP', 'Digital Transformation', 'Business Transformation', 'Digitization', 'Process Improvement']",2025-06-12 14:51:31
Lead ML Ops Engineer with GCP,TVS Next,8 - 10 years,Not Disclosed,['Bengaluru'],"What you’ll be doing:\nAssist in developing machine learning models based on project requirements\nWork with datasets by preprocessing, selecting appropriate data representations, and ensuring data quality.\nPerforming statistical analysis and fine-tuning using test results.\nSupport training and retraining of ML systems as needed.\nHelp build data pipelines for collecting and processing data efficiently.",,,,"['hive', 'kubernetes', 'data pipeline', 'sql', 'docker', 'tensorflow', 'java', 'product management', 'gcp', 'spark', 'pytorch', 'bigquery', 'hadoop', 'big data', 'programming', 'hbase', 'ml', 'cloud sql', 'python', 'airflow', 'cloud spanner', 'cloud pubsub', 'machine learning', 'data engineering', 'ops', 'mapreduce', 'kafka', 'cloud storage', 'hdfs', 'bigtable', 'aws']",2025-06-12 14:51:33
Senior Software Quality Engineer,Mastercard,4 - 9 years,Not Disclosed,['Pune'],"Senior Software Quality Engineer\n?\n\nMastercard is a global technology company in the payments industry. We work to connect and power an inclusive, digital economy that benefits everyone, everywhere by making transactions safe, simple, smart and accessible. Using secure data and networks, partnerships and passion, our innovations and solutions help individuals, financial institutions, governments, and businesses realize their greatest potential.\n\n\n\nOverview:\n\nTransfer Solutions is responsible for driving Mastercard s expansion into new payment flows such as Disbursements & Remittances. The team is working on creating a market-leading money transfer proposition, Mastercard Move, to power the next generation of payments between people and businesses, whether money is moving domestically or across borders, by delivering the ability to pay and get paid with choice, transparency, and flexibility.\n\nThe Product & Engineering teams within Transfer Solutions are responsible for designing, developing, launching, and maintaining products and services designed to capture these flows from a wide range of customer segments. By addressing customer pain points for domestic and cross-border transfers, the goal is to scale Mastercard s Disbursements & Remittances business, trebling volume over the next 4 years.\n\nThe Role:\narticipate in requirements discussion, test planning, test data creation and execution of testing Plan in adherence with MasterCard standards, processes and best practices.\nWork with project teams to meet scheduled due dates, while identifying emerging issues and recommending solutions for problems and independently perform assigned tasks.\nDesign and develop test automation frameworks to validate system to system interfaces and complete software solutions (for Database/ETL, API and UI tests)\nInteract with business and development stakeholders to define test plans and schedules\nTranslate complex system requirements into test requirements and testing methods\nIdentify and implement complex automation efforts, including refactoring of automation code where needed\nDevelop test scripts and perform automated and manual exploratory testing to ensure software meets business and security requirements and established practices.\nDesign and develop test data management for defined test cases, recognize test environment preparation needs, and execute existing test plans and report results\nOwn responsibility for defect management and oversight and escalation of issues discovered during the testing phase\nDocument as per Software Development Best Practices and follow MasterCard Quality Assurance and Quality Control processes.\nDocument performance test strategies and test plans, and execute performance validation\nCollect quality metric data and communicate test status/risks to stakeholders\nAct as first-review for project-level reviews, walkthroughs and inspections\nProvide technical support and mentoring to junior team members\nPerform demos of new product functionality to stakeholders\nDevelop business and product knowledge over time.\nIdentify opportunities to improve effectiveness and time-to-market\nProvide training and guidance to team members on quality best practices and principles\nFacilitate knowledge sharing sessions to promote a culture of quality awareness\nBe a strong individual contributor to the implementation efforts of product solutions\n\nAll About You:\n\nBachelors degree in Information Technology, Computer Science or Management Information Systems or equivalent work experience\n8+ years of experience in the Software Engineering with a focus on Quality Engineering methodologies\nTechnical skills in Java, Selenium, Cucumber, Soap UI, Spring framework, REST, JSON, Eclipse, GIT, Jmeter/Blazemeter\nExcellent SQL skills to work on large and complex data sources and capability of comprehending and writing complex queries\nExperience testing APIs (REST and SOAP), web user interface, and/or reports\nExperience in implementing CI/CD build pipelines with tools like Git/Bit Bucket, Jenkins and Maven\nSuccessfully validated one or more application codebases via automation, for new feature functionality and regression testing\nExperience working in Agile teams and conversant with Agile/SAFe tenets and ceremonies. Strong analytical and problem-solving abilities, with quick adaptation to new technologies, methodologies, and systems\nExcellent English communication skills (both written and verbal) to effectively interact with multiple technical teams and other stakeholders\nHigh-energy, detail-oriented and proactive, with ability to function under pressure in an independent environment along with a high degree of initiative and self-motivation to drive results\nEager to experiment with new team processes and innovate on testing approach\nPrior experience with Data Analysis and Data Engineering is a plus\nStrong collaboration skills and ability to work effectively in a cross-functional, interdependent team environment",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Maven', 'Manager Quality Assurance', 'Eclipse', 'Information security', 'Agile', 'JSON', 'Selenium', 'Information technology', 'Technical support', 'SQL']",2025-06-12 14:51:36
Senior MSBI Developer (SQL & SSRS/SSIS Expert),Synechron,8 - 10 years,Not Disclosed,"['Pune', 'Hinjewadi']","job requisition idJR1027513\n\nJob Summary\nSynechron is seeking an experienced and detail-oriented Senior MSBI Developerexpertise in MSBI (Microsoft Business Intelligence) to join our data and analytics team. In this role, you will contribute to designing, developing, and maintaining robust reporting and data integration solutions that support our business objectives. Your expertise will help deliver actionable insights, improve decision-making processes, and enhance overall data management efficiency within the organization.\n\nSoftware\n\nRequired\n\nSkills:\nMSBI Suite (including SSIS, SSRS, SSAS)\nSQL Server (including SQL Server Management Studio and Query Performance Tuning)\nVersionsRecent versions of SQL Server (2016 or later preferred)\nProven experience in creating complex reports, data transformation, and integration workflows\nPreferred\n\nSkills:\nPower BI or other visualization tools\nExperience with cloud-based data solutions (e.g., Azure SQL, Synapse Analytics)\nOverall Responsibilities\nDevelop, implement, and maintain MSBI solutions such as SSIS packages, SSRS reports, and data models to meet business requirements\nCollaborate with business stakeholders and data teams to gather reporting needs and translate them into scalable solutions\nOptimize and troubleshoot existing reports and data pipelines to improve performance and reliability\nEnsure data accuracy, security, and compliance within reporting processes\nDocument solution architectures, workflows, and processes for ongoing support and knowledge sharing\nParticipate in team initiatives to enhance data governance and best practices\nContribute to strategic planning for data platform evolution and modernization\nTechnical Skills (By Category)\n\nProgramming Languages:\nRequiredSQL (Advanced proficiency in query writing, stored procedures, and performance tuning)\nPreferredT-SQL scripting for data transformations and automation\nDatabases / Data Management:\nRequiredDeep knowledge of relational database concepts with extensive experience in SQL Server databases\nPreferredFamiliarity with data warehouse concepts, OLAP cubes, and data mart design\nCloud Technologies:\nDesiredBasic understanding of cloud-based data platforms like Azure Data Factory, Azure Synapse\nFrameworks and Libraries:\nNot directly applicable, focus on MSBI tools\nDevelopment Tools and Methodologies:\nExperience working within Agile development environments\nData pipeline development and testing best practices\nSecurity Protocols:\nImplement data security measures, role-based access controls, and ensure compliance with data privacy policies\nExperience\n8 to 10 years of professional experience in software development with substantial hands-on MSBI expertise\nDemonstrated experience in designing and deploying enterprise-level BI solutions\nDomain experience in finance, healthcare, retail, or similar industries is preferred\nAlternative candidacyExtensive prior experience with BI tools and proven success in similar roles may be considered in lieu of exact industry background\nDay-to-Day Activities\nDesign and develop SSIS data integration workflows to automate data loading processes\nCreate and optimize SSRS reports and dashboards for various organizational units\nEngage in troubleshooting and resolving technical issues in existing BI solutions\nCollaborate with data architects, developers, and business analysts to align data solutions with business needs\nConduct code reviews, testing, and validation of reports and data pipelines\nParticipate in scrum meetings, planning sessions, and stakeholder discussions\nEnsure documentation of solutions, processes, and workflows for ease of maintenance and scalability\nQualifications\nBachelors degree or equivalent in Computer Science, Information Technology, or related field\nRelevant certifications in Microsoft BI or SQL Server (e.g., Microsoft Certified Data Engineer Associate) preferred\nOngoing engagement in professional development related to BI, data management, and analytics tools\nProfessional Competencies\nAnalytical mindset with strong problem-solving abilities in data solution development\nCapable of working collaboratively across diverse teams and communicating technical concepts effectively\nStakeholder management skills to interpret and prioritize reporting needs\nAdaptability to evolving technologies and continuous learning mindset\nFocus on delivering high-quality, sustainable data solutions with attention to detail\nEffective time management, prioritizing tasks to meet project deadlines",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['performance tuning', 'stored procedures', 'sql scripting', 'query writing', 'msbi', 'sql server database', 'software development', 'ssas', 'power bi', 'microsoft azure', 'data mart design', 'olap cubes', 'sql server', 'sql azure', 'ssrs', 'data warehousing concepts', 'data transformation', 'ssis']",2025-06-12 14:51:39
Lead Analytics Consultant,Wells Fargo,5 - 10 years,Not Disclosed,['Hyderabad'],"locationsHyderabad, India\nposted onPosted Yesterday\njob requisition idR-446112\nAbout this role:\nWells Fargo is seeking a Lead Analytics Consultant People Analytics. As a consultant, you will work as analytics professional in HR People Analytics and Business Insights delivery team and will be responsible for effective delivery of projects as per the business priority. The incumbent is expected to be an expert into executive summary, people strategy, HR consulting, HR advisory, advanced analytics & data science and value addition to the projects.",,,,"['Data Analytics', 'Data science tools', 'product lifecycle', 'SAS programming', 'ETL development', 'Alteryx', 'Data Management', 'Tableau Prep', 'SQL']",2025-06-12 14:51:41
Architect (AI and Cloud),Rakuten Symphony,10 - 18 years,Not Disclosed,['Bengaluru( Kadubeesanahalli )'],"Why should you choose us? Rakuten Symphony is reimagining telecom, changing supply chain norms and disrupting outmoded thinking that threatens the industrys pursuit of rapid innovation and growth. Based on proven modern infrastructure practices, its open interface platforms make it possible to launch and operate advanced mobile services in a fraction of the time and cost of conventional approaches, with no compromise to network quality or security. Rakuten Symphony has operations in Japan, the United States, Singapore, India, South Korea, Europe, and the Middle East Africa region. For more information, visit: https://symphony.rakuten.com\n\nBuilding on the technology Rakuten used to launch Japans newest mobile network, we are taking our mobile offering global. To support our ambitions to provide an innovative cloud-native telco platform for our customers, Rakuten Symphony is looking to recruit and develop top talent from around the globe. We are looking for individuals to join our team across all functional areas of our business from sales to engineering, support functions to product development. Lets build the future of mobile telecommunications together!\n\nAbout Rakuten Rakuten Group, Inc. (TSE: 4755) is a global leader in internet services that empower individuals, communities, businesses and society. Founded in Tokyo in 1997 as an online marketplace, Rakuten has expanded to offer services in ecommerce, fintech, digital content and communications to approximately 1.5 billion members around the world. The Rakuten Group has over 27,000 employees, and operations in 30 countries and regions. For more information visit https://global.rakuten.com/corp/\n\nJob Summary:\nThe AI Architect is a senior technical leader responsible for designing and implementing the overall AI infrastructure and architecture for the organization. This role will define the technical vision for AI initiatives, select appropriate technologies and platforms, and ensure that AI systems are scalable, reliable, secure, and aligned with business requirements. The AI Architect will work closely with CTO Office, product manager, engineering manager, data scientists, machine learning engineers, and other stakeholders to build a robust and efficient AI ecosystem.\n\nMandatory Skills:\nCloud Computing Platforms (AWS, Azure, GCP).\nAI/ML Frameworks (TensorFlow, PyTorch, scikit-learn) .\nData Engineering Tools (Spark, Hadoop, Kafka).\nMicroservices Architecture.\nAI/ML as a service Deployment.\nDevOps Principles (CI/CD/CT).\nStrong understanding of AI/ML algorithms and techniques\n\nRoles & Responsibilities:\nDefine the overall AI architecture and infrastructure strategy for the organization.\nSelect appropriate technologies and platforms for AI development and deployment. • Design scalable, reliable, and secure AI systems.\nDevelop and maintain architectural blueprints and documentation.\nProvide technical leadership and guidance to tech lead, engineering manager, data scientists, machine learning engineers, and other stakeholders.\nEnsure that AI systems are aligned with business requirements and industry best practices. Evaluate new AI technologies and trends.\nCollaborate with security and compliance teams to ensure that AI systems meet regulatory requirements.\nCollaborate with CTO Office to ensure the AI strategy implemented aligned with overall business unit strategy.",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Architect', 'Artificial Intelligence', 'Cloud', 'Microservice Based Architecture', 'Tensorflow', 'Pytorch', 'Ai', 'Machine Learning', 'Solution Architect', 'Scikit-Learn', 'Technical Architecture']",2025-06-12 14:51:43
AI Technical Architect,Care Allianz,7 - 11 years,Not Disclosed,"['Kolkata', 'Mumbai', 'New Delhi', 'Hyderabad', 'Pune', 'Chennai', 'Bengaluru']","Care Allianz is looking for AI Technical Architect_ to join our dynamic team and embark on a rewarding career journey\n\nDesigns AI-based system architectures for scalable solutions\n\nCollaborates with data scientists and engineers for model integration\n\nEnsures performance, scalability, and security of AI platforms\n\nGuides development teams in implementing AI strategies",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Technical Architect', 'Manager Technology']",2025-06-12 14:51:46
RAG Architect,Qualcomm,13 - 18 years,Not Disclosed,['Bengaluru'],"Job Area: Engineering Group, Engineering Group > Software Test Engineering\n\nGeneral Summary:\n\nJob description\n\nWe are seeking an experienced AI Architect to design, develop, and deploy Retrieval-Augmented Generation (RAG) solutions for Qualcomm Cloud AI Platforms.\n\nRoles and Responsibilities\nLead the design and development of applications for RAG AI models and provide APIs for frontend consumption. Manage the interaction between retrieval-augmented techniques and generative models.\nBuild services that connect AI models (e.g., transformers, embeddings, and vector search) to handle tasks such as query retrieval, model inference, and generating responses. Leverage frameworks like Flask, FastAPI, or Django for API development.\nDesign pipelines to preprocess, clean, and prepare data for AI model training, as well as for serving the models in production environments. Optimize these pipelines to support both batch and real-time data processing. Implement RESTful APIs or GraphQL endpoints for seamless frontend-backend interaction.\nImplement cloud solutions to host Python-based services, ensuring that AI models are scalable and that the infrastructure can handle high traffic. Leverage containerization (Docker) and orchestration (Kubernetes) for model deployment and management.\nSet up monitoring, logging, and alerting for Python backend services, ensuring smooth operation of AI features. Use tools like Prometheus, Grafana, and ELK stack for real-time performance tracking.\nContinuously optimize model performance by fine-tuning and adapting Python-based AI models for real-time use cases. Manage trade-offs between computation load, response time, and quality of generated content.\nPartner with data scientists, machine learning engineers, and mobile/web developers to ensure tight integration between AI models, mobile/web front-end, and backend infrastructure.\n\n- Experience:\n13+ years of overall SW development experience\n10+ years Strong experience in working with technologies (e.g., React, React Native, Flutter, Django, Flask, FastAPI).\n5+ years of experience in building AI applications with a focus on NLP, machine learning, generative models, and retrieval-augmented systems.\nProven experience in designing and deploying AI systems that integrate retrieval-based techniques (e.g., FAISS, Weaviate) and generative models (e.g., GPT, BERT). - Expertise in cloud platforms (e.g., AWS, GCP, Azure) and deployment of Python-based microservices.\nBuilding RESTful APIs or GraphQL services (using frameworks like Flask, FastAPI, or Django).\nHandling AI model inference and data processing (using libraries like NumPy, Pandas, TensorFlow, PyTorch, and Hugging Face Transformers).\nIntegrating vector search solutions (e.g., FAISS, Pinecone, Weaviate) with the AI models for efficient retrieval-augmented generation. - Experience with containerization (Docker) and Kubernetes for deploying scalable Python-based services.\nProficient in cloud infrastructure management, with a focus on managing Python services in the cloud.\nExperience in End-to-End product development and Software Lifecycle\n\n\nKey\n\nSkills:\n\nAdvanced proficiency in Python for building backend services and data processing pipelines. Familiarity with frameworks like Flask, Django, and FastAPI. Experience with AI libraries and frameworks (TensorFlow, PyTorch, Hugging Face Transformers).\nFamiliarity with vector databases (e.g., Pinecone, FAISS, Weaviate) and integration with retrieval-augmented systems.\nStrong knowledge of RESTful API design, GraphQL, and API security best practices (e.g., OAuth, JWT).\nExcellent problem-solving abilities and a strong focus on creating highly scalable and performant solutions.\nStrong communication skills, with the ability to collaborate across different teams and geography\nAbility to mentor junior team members and lead technical discussions.\n\n\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 6+ years of Software Test Engineering or related work experience.\nOR\nMaster's degree in Engineering, Information Systems, Computer Science, or related field and 5+ years of Software Test Engineering or related work experience.\nOR\nPhD in Engineering, Information Systems, Computer Science, or related field and 4+ years of Software Test Engineering or related work experience.\n\n2+ year of work experience with Software Test or System Test, developing and automating test plans, and/or tools (e.g., Source Code Control Systems, Continuous Integration Tools, and Bug Tracking Tools).",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'data processing', 'cloud platforms', 'api', 'graphql', 'natural language processing', 's w development', 'rest api design', 'system testing', 'react native', 'machine learning', 'pipeline', 'react.js', 'flutter', 'test engineering', 'django', 'cloud infrastructure management', 'flask']",2025-06-12 14:51:48
AI/ML,Larsen & Toubro (L&T),2 - 4 years,Not Disclosed,"['Chennai', 'Bengaluru']","Experience Required\n\n2 to 4 years of experience in AI/ML model development, deployment, and optimization. Hands-on experience in building machine learning pipelines and working with large datasets\n\nDomain Experience (Functional)\nExperience in domains such as natural language processing (NLP), computer vision, predictive analytics, or recommendation systems. Exposure to industry-specific AI applications (e.g., healthcare, finance, retail, manufacturing) is a plus.\n\nQualification\nBachelors or Masters degree in Computer Science, Artificial Intelligence, Data Science, Mathematics, or a related field\n\nRoles & Responsibilities\nDesign, develop, and deploy machine learning and deep learning models.\nCollaborate with data engineers and domain experts to collect, clean, and preprocess data.\nConduct experiments, evaluate model performance, and iterate for improvement.\nIntegrate AI models into production systems and monitor their performance.\nStay updated with the latest research and advancements in AI/ML.\nDocument model development processes and contribute to knowledge sharing.\n\nTechnical Skills\n\nProficient in Python and core ML libraries: TensorFlow, PyTorch, Scikit-learn.\nStrong with Pandas, NumPy for data handling.\nSolid grasp of ML algorithms, statistics, and model evaluation.\nFamiliar with cloud platforms (AWS/Azure/GCP).\nExperience with Git and basic CI/CD for model deployment",Industry Type: Engineering & Construction,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Npl', 'Aiml', 'Tensorflow', 'Ci/Cd', 'Machine Learning', 'Deep Learning', 'Scikit-Learn', 'Numpy', 'Pytorch', 'GCP', 'Pandas', 'Microsoft Azure', 'AWS', 'Python']",2025-06-12 14:51:50
ETL Developer,Wipro,5 - 8 years,Not Disclosed,['Bengaluru'],"Role Purpose\nThe purpose of this role is to design, test and maintain software programs for operating systems or applications which needs to be deployed at a client end and ensure its meet 100% quality assurance parameters\n\nResponsibilities:\nDesign and implement the data modeling, data ingestion and data processing for various datasets\nDesign, develop and maintain ETL Framework for various new data source\nDevelop data ingestion using AWS Glue/ EMR, data pipeline using PySpark, Python and Databricks.\nBuild orchestration workflow using Airflow & databricks Job workflow\nDevelop and execute adhoc data ingestion to support business analytics.\nProactively interact with vendors for any questions and report the status accordingly\nExplore and evaluate the tools/service to support business requirement\nAbility to learn to create a data-driven culture and impactful data strategies.\nAptitude towards learning new technologies and solving complex problem.\nQualifications:\nMinimum of bachelors degree. Preferably in Computer Science, Information system, Information technology.\nMinimum 5 years of experience on cloud platforms such as AWS, Azure, GCP.\nMinimum 5 year of experience in Amazon Web Services like VPC, S3, EC2, Redshift, RDS, EMR, Athena, IAM, Glue, DMS, Data pipeline & API, Lambda, etc.\nMinimum of 5 years of experience in ETL and data engineering using Python, AWS Glue, AWS EMR /PySpark and Airflow for orchestration.\nMinimum 2 years of experience in Databricks including unity catalog, data engineering Job workflow orchestration and dashboard generation based on business requirements\nMinimum 5 years of experience in SQL, Python, and source control such as Bitbucket, CICD for code deployment.\nExperience in PostgreSQL, SQL Server, MySQL & Oracle databases.\nExperience in MPP such as AWS Redshift, AWS EMR, Databricks SQL warehouse & compute cluster.\nExperience in distributed programming with Python, Unix Scripting, MPP, RDBMS databases for data integration\nExperience building distributed high-performance systems using Spark/PySpark, AWS Glue and developing applications for loading/streaming data into Databricks SQL warehouse & Redshift.\nExperience in Agile methodology\nProven skills to write technical specifications for data extraction and good quality code.\nExperience with big data processing techniques using Sqoop, Spark, hive is additional plus\nExperience in data visualization tools including PowerBI, Tableau.\nNice to have experience in UI using Python Flask framework anglular\n\n\nMandatory Skills: Python for Insights. Experience: 5-8 Years.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['ETL', 'data bricks', 'aws glue', 'amazon ec2', 'python', 'spark', 'glue', 'amazon redshift', 'cloud platforms', 'aws', 'data engineering', 'sql']",2025-06-12 14:51:53
Senior Generative AI Engineer - Python Programming,Zettamine Labs,7 - 8 years,Not Disclosed,['Bengaluru'],"We are looking for a Senior Generative AI Engineer who is passionate about cutting-edge AI innovation and has significant hands-on experience in building and deploying Generative AI models. In this role, you will be responsible for designing, fine-tuning, and optimizing large language models (LLMs), implementing innovative GenAI solutions, and contributing to the architecture of AI-driven platforms that deliver real business value.\n\nYou will collaborate with cross-functional teams including data scientists, machine learning engineers, product managers, and cloud infrastructure teams to build scalable, reliable, and secure AI systems. This is a high-impact position where you will directly influence the AI roadmap and innovation strategy.\n\nKey Responsibilities :\n\n- Design, develop, and fine-tune state-of-the-art Generative AI and LLM models tailored for various business use cases.\n\n- Build, integrate, and optimize solutions using transformer-based architectures (e.g., GPT, BERT, T5, LLaMA, Mistral).\n\n- Apply techniques such as fine-tuning, prompt engineering, RLHF (Reinforcement Learning from Human Feedback), and knowledge distillation to improve model performance.\n\n- Work with vector databases (e.g., FAISS, Pinecone, Weaviate) for implementing retrieval-augmented generation (RAG) pipelines.\n\n- Develop and deploy embedding models and integrate them into LLM pipelines.\n\n- Collaborate with engineering and product teams to deploy scalable AI systems using MLOps practices and CI/CD pipelines.\n\n- Leverage LangChain, Hugging Face Transformers, OpenAI APIs, and similar frameworks/tools to accelerate development.\n\n- Optimize model performance across different environments (cloud/on-premise).\n\n- Develop end-to-end pipelines, from data preprocessing to real-time inference and monitoring.\n\n- Ensure high standards of software quality, including testing, version control, code reviews, and documentation.\n\n- Stay up to date with the latest research in Generative AI and translate breakthroughs into production-ready solutions.\n\nRequired Skills & Qualifications :\n\n- Experience : 7+ years in AI/ML, data science, or software engineering; at least 3 - 4 years in Generative AI/LLMs.\n\n- Advanced Python programming skills, including familiarity with object-oriented design and software engineering best practices.\n\n- Deep expertise in PyTorch, TensorFlow, Transformers (Hugging Face), LangChain, and OpenAI or Anthropic APIs.\n\n- Experience in LLM fine-tuning, parameter-efficient tuning methods (LoRA, PEFT), RLHF, and model evaluation.\n\n- Experience with embeddings, vector stores (FAISS, Pinecone), semantic search, and RAG systems.\n\n- Hands-on experience with AWS, GCP, or Azure; knowledge of MLOps tools (SageMaker, Vertex AI, MLflow, Kubeflow) for training, deploying, and monitoring models.\n\n- Familiarity with structured/unstructured data handling and integrating AI systems with SQL/NoSQL databases.\n\n- Strong analytical thinking, problem-solving ability, and a keen interest in research and innovation.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Python Programming', 'Tensorflow', 'PyTorch', 'Generative AI', 'MLOps', 'NoSQL', 'ChatGPT', 'Artificial Intelligence', 'Data Modeling', 'LLM', 'Python', 'SQL']",2025-06-12 14:51:55
Senior Software Engineer,Xoom,3 - 5 years,Not Disclosed,['Bengaluru'],"Job Summary\nWhat you need to know about the role\nAs a Software Engineer in our Risk department, you will play a critical role in developing and maintaining cutting-edge risk detection and prevention systems that protect PayPals users and merchants from financial loss. You will work closely with cross-functional teams to design, build, and deploy scalable and efficient solutions that leverage machine learning, data analytics, and automation to identify and mitigate potential risks, ensuring the integrity of our platform and driving business growth.\n\nMeet our team\nAs an engineer in Global Fraud Risk - Automation team, You will work closely with data scientists, engineering, and analytical teams, understand the requirements and drive full development lifecycle of the teams products, transforming research work to real products. We are looking for strong technologists who are passionate about technology and able to continuously deliver state of the art software solutions in scalable way.\nJob Description\nYour way to impact\nAt PayPal, Backend Software Engineers are the architects of our global payment platform. Youll design, develop, and optimize core systems that power millions of transactions daily, directly impacting our customers experiences and our companys success.\nYour day-to-day\nAs a Senior Software Engineer - Backend, youll design and implement backend solutions. Youll collaborate with cross-functional teams to deliver high-quality products.\nDesign and develop scalable backend systems.\nOptimize system performance and reliability.\nMentor junior engineers.\nWhat do you need to bring\nBachelors degree in Computer Science or related field.\n3-5 years of backend development experience.\nProficiency in at least one backend language (Python, Java, Ruby on Rails)\nAdvanced proficiency in backend development with either Java EE frameworks, including experience with Spring MVC, or Hibernate.\nExperience designing and implementing RESTful services, focusing on scalability and reliability, using Java.\nProven ability to mentor junior engineers and contribute to code reviews and design discussions.\nExperience with cloud platforms (AWS, GCP, Azure)\nExperience with databases (SQL, NoSQL)\nStrong understanding of database design, including SQL and NoSQL databases, and experience with ORM tools.\nPreferred Qualifications\nExperience with large-scale, high-performance systems.\nKnowledge of the payment processing industry and relevant regulations.\nExperience with cloud platforms (AWS, GCP, Azure).\nContributions to open-source projects .\n**We know the confidence gap and imposter syndrome can get in the way of meeting spectacular candidates. Please dont hesitate to apply.\nPreferred Qualification\nSubsidiary\nPayPal\nTravel Percent\n0\nFor the majority of employees, PayPals balanced hybrid work model offers 3 days in the office for effective in-person collaboration and 2 days at your choice of either the PayPal office or your home workspace, ensuring that you equally have the benefits and conveniences of both locations.\nOur Benefits\nWe have great benefits including a flexible work environment, employee shares options, health and life insurance and more. To learn more about our benefits please visit https//www.paypalbenefits.com .\nWho We Are\nClick Here to learn more about our culture and community.\nCommitment to Diversity and Inclusion\nPayPal provides equal employment opportunity (EEO) to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, pregnancy, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state, or local law. In addition, PayPal will provide reasonable accommodations for qualified individuals with disabilities. .\nBelonging at PayPal\nOur employees are central to advancing our mission, and we strive to create an environment where everyone can do their best work with a sense of purpose and belonging. Belonging at PayPal means creating a workplace with a sense of acceptance and security where all employees feel included and valued. We are proud to have a diverse workforce reflective of the merchants, consumers, and communities that we serve, and we continue to take tangible actions to cultivate inclusivity and belonging at PayPal.\nAny general requests for consideration of your skills, please Join our Talent Community .\nWe know the confidence gap and imposter syndrome can get in the way of meeting spectacular candidates. Please don t hesitate to apply.",Industry Type: Internet,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Hibernate', 'Automation', 'Backend', 'Database design', 'Analytical', 'Machine learning', 'Open source', 'SQL', 'Python']",2025-06-12 14:51:57
Senior Research Engineer - LLM,Trask,10 - 15 years,Not Disclosed,['Bengaluru'],"Develop, execute, implement and check methods, plans, toolsets and approaches that are appropriate and compliant to achieve digital solutions for the process intended\nIdentify technical problems and apply/integrate solutions as needed with a set based design approach\nAnalyse customer requirements and define technical solutions as input for proposals to develop proof of concepts\nWork with cross functional teams and multiple sites during the development process\nGenerate assigned project deliverables, documents, and reports according to the project milestones\nSupport and participate in technical reviews, including the creation and preparation of technical data and presentations as needed and support other engineers with peer to peer reviews\nSupport Lean culture and improvement initiatives in the organisation\nTake part in regular sprint planning meetings to plan, review and deliver outputs based on agile philosophy\nSupport in creation of training material and knowledge sharing in the relevant area of work\nSupport idea generation and CI activities\nPerform all activities independently and help other engineers within the program as required.\nBachelors in Engineering or higher, with minimum of 10 years of relevant experience Automation & Software development.\nDesign data pipelines to handle large-scale data for training, ensuring data security and compliance with aerospace and defence standards.\nExcellent experience in shop floor automation and I4.0/IOT Integration\nExcellent Understanding of Industrial Communication protocols and establishing communication between different Industrial systems.\nGood knowledge of data structure, data modelling and database architecture\nGood Knowledge of implementing business process into functional codes\nExcellent knowledge of software coding , integrated development platforms\nProficiency in programming with python, C++,C, C#, Java, .NET, VB, SQL and working knowledge in GIT\nAbility to conduct POCs and guide team members to extract valuable insights and drive data-driven decision-making.\nEvaluate and select appropriate tools and applications for tasks.\nStrong software development skills, including version control (e.g., Git), debugging, testing, and documentation. Familiarity with containerization (e.g., Docker) and orchestration (e.g., Kubernetes) is beneficial.\nStay updated with latest developments in Automation, Software Development, NLP, ML, AI, LLM technology around the globe relevant to aerospace and defence sector and work along with team to quickly leverage, test and validate new solutions applicable to the working projects by applying cutting edge technologies.\nShould have strong problem-solving skills and ability to collaborate with cross-functional teams, including domain experts, data scientists, and engineering teams, to gather requirements and translate them into scalable applications.\nBachelors in Engineering or higher, with minimum of 10 years of relevant experience Automation & Software development.\nDesign data pipelines to handle large-scale data for training, ensuring data security and compliance with aerospace and defence standards.\nExcellent experience in shop floor automation and I4.0/IOT Integration\nExcellent Understanding of Industrial Communication protocols and establishing communication between different Industrial systems.\nGood knowledge of data structure, data modelling and database architecture\nGood Knowledge of implementing business process into functional codes\nExcellent knowledge of software coding , integrated development platforms\nProficiency in programming with python, C++,C, C#, Java, .NET, VB, SQL and working knowledge in GIT\nAbility to conduct POCs and guide team members to extract valuable insights and drive data-driven decision-making.\nEvaluate and select appropriate tools and applications for tasks.\nStrong software development skills, including version control (e.g., Git), debugging, testing, and documentation. Familiarity with containerization (e.g., Docker) and orchestration (e.g., Kubernetes) is beneficial.\nStay updated with latest developments in Automation, Software Development, NLP, ML, AI, LLM technology around the globe relevant to aerospace and defence sector and work along with team to quickly leverage, test and validate new solutions applicable to the working projects by applying cutting edge technologies.\nShould have strong problem-solving skills and ability to collaborate with cross-functional teams, including domain experts, data scientists, and engineering teams, to gather requirements and translate them into scalable applications.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Business process', 'C++', 'Automation', 'GIT', 'Coding', 'data security', 'Aerospace', 'VB', 'SQL', 'Python']",2025-06-12 14:52:00
TRQ26-0057-02-Senior Engineer L3,IBS Software Services,15 - 20 years,Not Disclosed,"['Kolkata', 'Mumbai', 'New Delhi', 'Hyderabad', 'Pune', 'Chennai', 'Bengaluru']","Senior Azure Data Engineer:\na. 15+ years of experience in IT Industry\nb. 5+ years of experience with Azure Data Engineering Stack (Data Factory, Databricks, Synapse, Event Hub, Cosmos DB, ADLS Gen2, Function app)\nc. 3+ years of experience with Python / Pyspark\nd. Good understanding of other Azure services\ne. Excellent knowledge of SQL\nf. Good understanding of Data Warehouse Architecture, Data Modelling and design concepts\ng. Experience in Power BI, SFTP, Messaging, APIs\nh. Lead and guide the data engineering team technically and share best practices with the team\ni. Excellent analytical and organization skills.\nj. Effective working in a team as well as working independently.\nk. Experience of working in Agile delivery\nl. Knowledge of software development best practices.\nm. Strong written and verbal communication skills.\nn. DP200/DP201 certification is added advantage",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['C', 'Architecture', 'Data modeling', 'Analytical', 'Agile', 'power bi', 'Cosmos', 'Data warehousing', 'SQL', 'Python']",2025-06-12 14:52:02
"Senior Manager, Engineering",Reltio,10 - 15 years,Not Disclosed,['Bengaluru'],"Job Summary:\nAre you passionate about data management and unifying complex datasets? Do you have a track record of leading successful data unification projects? Reltio is seeking a dynamic and experienced Senior Manager to join our Data Unification Team in India. As a Senior Manager, you will play a pivotal role in driving the development and execution of data unification strategies and initiatives, ensuring high-quality and accurate data for our clients.\nJob Duties and Responsibilities:\nLeadership: Provide strong leadership and guidance to the Data Unification Team, driving a culture of excellence, innovation, and collaboration.\nData Unification Strategy: Develop and implement data unification strategies, frameworks, and best practices to deliver effective data management solutions.\nTeam Management: Lead, mentor, and inspire a team of data engineers and analysts, fostering their professional growth and ensuring the teams success in meeting project goals.\nData Governance: Define and enforce data governance policies, standards, and procedures to ensure data quality, integrity, and security across all data unification activities.\nProject Management: Oversee end-to-end project management, including scoping, planning, resource allocation, and execution of data unification projects, ensuring timely delivery within budget and scope.\nStakeholder Collaboration: Collaborate with cross-functional teams, including Product Management, Engineering, and Customer Success, to align data unification initiatives with overall business objectives and customer requirements.\nContinuous Improvement: Identify areas for process improvement, automation, and optimization, driving efficiency and scalability in data unification operations.\nIndustry Trends: Stay updated with industry trends, emerging technologies, and best practices in data management and unification, leveraging this knowledge to drive innovation and enhance our offerings.\nSkills You Must Have:\nMinimum of 9+ years of experience in data management, data unification, or related fields, with a focus on managing large-scale data projects.\nStrong leadership and managerial skills, with a proven track record of successfully leading and motivating high-performing teams.\nIn-depth knowledge of data unification methodologies, tools, and technologies, including Master Data Management (MDM) and data integration techniques.\nSolid understanding of data governance principles, data quality frameworks, and data security best practices.\nExcellent project management skills, with the ability to manage multiple projects simultaneously, prioritize tasks, and meet deadlines.\nStrong analytical and problem-solving abilities, with the capacity to analyze complex data sets, identify patterns, and propose innovative solutions.\nEffective communication and stakeholder management skills, with the ability to collaborate and influence cross-functional teams and senior leadership.\nBachelors or Masters degree in Computer Science, Information Systems, or a related field.",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Product management', 'Computer science', 'Automation', 'Team management', 'data security', 'Analytical', 'Process improvement', 'Data quality', 'Stakeholder management', 'Analytics']",2025-06-12 14:52:05
"Senior Manager, Software Engineering",Diligent Corporation,10 - 15 years,Not Disclosed,['Bengaluru'],"Position Overview\nYou will work closely with technology and business teams to understand requirements, design robust architectures, and influence technology choices to deliver innovative solutions. In addition to leading SaaS software development, you will drive initiatives in Data Engineering, Data Warehousing, and Artificial Intelligence (AI). You will collaborate with principal engineers and leadership, and have opportunities to cross-collaborate with inter-disciplinary teams to solve unique challenges in the GRC & ESG domain.\nKey Responsibilities\nShape the product and technical vision for the team, collaborating with product, business, and engineering leadership across the company.\nManage and mentor a team of engineers developing highly scalable, performant, maintainable, and well-tested SaaS features.\nLead the design and implementation of modern data warehouse solutions, ensuring data quality, scalability, and security.\nOversee the integration of advanced AI and machine learning models into SaaS products to deliver intelligent features and insights.\nHire, mentor, and lead a world-class group of engineers with expertise in SaaS, data engineering, and AI.\nFoster a culture of innovation, experimentation, and continuous improvement.\nEvaluate engineering requirements and design proposals, especially in the context of data-driven and AI-powered applications.\nAssess and develop the technical performance of individual contributors within the team.\nStay current with the latest frameworks and technologies in SaaS, Data Engineering, and AI, influencing technology choices for the application stack.\nFacilitate daily stand-ups, risk identification and mitigation, dependency resolution, and follow-ups for gap closure.\nPartner with Product Management and Business teams to drive the agenda, set priorities, create project plans, and deliver outstanding products.\nRequired Experience/Skills\n10 to 15 years of relevant experience in developing enterprise SaaS applications using MERN/.NET, MySQL, MS SQL, and caching technologies.\n2+ years of experience leading engineering teams building scalable platforms and architectures, including data engineering and AI initiatives.\nProven experience designing, building, and maintaining data warehouse solutions (such as Snowflake, Redshift, or BigQuery) and data pipelines (ETL/ELT).\nHands-on experience with AI/ML frameworks (such as TensorFlow, PyTorch, or Scikit-learn) and integrating AI models into production SaaS environments.\nStrong background in data modeling, data governance, and data quality best practices.\nExperience with cloud platforms (AWS/Azure), CI/CD, DevOps, scripting, and SQL/NoSQL databases.\nDemonstrated success in migrating monolithic applications to microservices and on-premises solutions to cloud environments.\nPassion for building a data-driven culture, growing talent, and making a significant impact through technology.\nStrong communication skills for engaging with end users, technical, and business teams to gather requirements and describe product features and technical designs.\nAbility to seek clarity in ambiguous situations and drive projects to completion.\nExperience in Agile development and knowledge of Scrum and Kanban methodologies.\nSelf-motivated learner and builder with a strong customer focus and a commitment to delivering high-quality solutions.",Industry Type: Design,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Product management', 'MS SQL', 'NoSQL', 'Data modeling', 'MySQL', 'Machine learning', 'Scrum', 'Data quality', 'SQL', 'Recruitment']",2025-06-12 14:52:08
"Senior Manager, Software Engineering",Diligent Corporation,10 - 15 years,Not Disclosed,['Bengaluru'],"About Us\nDiligent is the AI leader in governance, risk and compliance (GRC) SaaS solutions, helping more than 1 million users and 700,000 board members to clarify risk and elevate governance. The Diligent One Platform gives practitioners, the C-Suite and the board a consolidated view of their entire GRC practice so they can more effectively manage risk, build greater resilience and make better decisions, faster.\nAt Diligent, were building the future with people who think boldly and move fast. Whether youre designing systems that leverage large language models or part of a team reimaging workflows with AI, youll help us unlock entirely new ways of working and thinking. Curiosity is in our DNA, we look for individuals willing to ask the big questions and experiment fearlessly - those who embrace change not as a challenge, but as an opportunity. The future belongs to those who keep learning, and we are building it together. At Diligent, you re not just building the future - you re an agent of positive change, joining a global community on a mission to make an impact.\nLearn more at diligent.com or follow us on LinkedIn and Facebook\nPosition Overview\nYou will work closely with technology and business teams to understand requirements, design robust architectures, and influence technology choices to deliver innovative solutions. In addition to leading SaaS software development, you will drive initiatives in Data Engineering, Data Warehousing, and Artificial Intelligence (AI). You will collaborate with principal engineers and leadership, and have opportunities to cross-collaborate with inter-disciplinary teams to solve unique challenges in the GRC & ESG domain.\nKey Responsibilities\nShape the product and technical vision for the team, collaborating with product, business, and engineering leadership across the company.\nManage and mentor a team of engineers developing highly scalable, performant, maintainable, and well-tested SaaS features.\nLead the design and implementation of modern data warehouse solutions, ensuring data quality, scalability, and security.\nOversee the integration of advanced AI and machine learning models into SaaS products to deliver intelligent features and insights.\nHire, mentor, and lead a world-class group of engineers with expertise in SaaS, data engineering, and AI.\nFoster a culture of innovation, experimentation, and continuous improvement.\nEvaluate engineering requirements and design proposals, especially in the context of data-driven and AI-powered applications.\nAssess and develop the technical performance of individual contributors within the team.\nStay current with the latest frameworks and technologies in SaaS, Data Engineering, and AI, influencing technology choices for the application stack.\nFacilitate daily stand-ups, risk identification and mitigation, dependency resolution, and follow-ups for gap closure.\nPartner with Product Management and Business teams to drive the agenda, set priorities, create project plans, and deliver outstanding products.\nRequired Experience/Skills\n10 to 15 years of relevant experience in developing enterprise SaaS applications using MERN/.NET, MySQL, MS SQL, and caching technologies.\n2+ years of experience leading engineering teams building scalable platforms and architectures, including data engineering and AI initiatives.\nProven experience designing, building, and maintaining data warehouse solutions (such as Snowflake, Redshift, or BigQuery) and data pipelines (ETL/ELT).\nHands-on experience with AI/ML frameworks (such as TensorFlow, PyTorch, or Scikit-learn) and integrating AI models into production SaaS environments.\nStrong background in data modeling, data governance, and data quality best practices.\nExperience with cloud platforms (AWS/Azure), CI/CD, DevOps, scripting, and SQL/NoSQL databases.\nDemonstrated success in migrating monolithic applications to microservices and on-premises solutions to cloud environments.\nPassion for building a data-driven culture, growing talent, and making a significant impact through technology.\nStrong communication skills for engaging with end users, technical, and business teams to gather requirements and describe product features and technical designs.\nAbility to seek clarity in ambiguous situations and drive projects to completion.\nExperience in Agile development and knowledge of Scrum and Kanban methodologies.\nSelf-motivated learner and builder with a strong customer focus and a commitment to delivering high-quality solutions.\nWhat Diligent Offers You\nCreativity is ingrained in our culture. We are innovative collaborators by nature. We thrive in exploring how things can be differently both in our internal processes and to help our clients\nWe care about our people. Diligent offers a flexible work environment, global days of service, comprehensive health benefits, meeting free days, generous time off policy and wellness programs to name a few\nWe have teams all over the world . We may be headquartered in New York City, but we have office hubs in Washington D.C., Vancouver, London, Galway, Budapest, Munich, Bengaluru, Singapore, and Sydney.\nDiversity is important to us. Growing, maintaining and promoting a diverse team is a top priority for us. We foster and encourage diversity through our Employee Resource Groups and provide access to resources and education to support the education of our team, facilitate dialogue, and foster understanding.\nDiligent created the modern governance movement. Our world-changing idea is to empower leaders with the technology, insights and connections they need to drive greater impact and accountability - to lead with purpose. Our employees are passionate, smart, and creative people who not only want to help build the software company of the future, but who want to make the world a more sustainable, equitable and better place.\nHeadquartered in New York, Diligent has offices in Washington D.C., London, Galway, Budapest, Vancouver, Bengaluru, Munich, Singapore and Sydney. To foster strong collaboration and connection, this role will follow a hybrid work model. If you are within a commuting distance to one of our Diligent office locations, you will be expected to work onsite at least 50% of the time. We believe that in-person engagement helps drive innovation, teamwork, and a strong sense of community.\nTo all recruitment agencies: Diligent does not accept unsolicited agency resumes. Please do not forward resumes to our jobs alias, Diligent employees or any other organization location. Diligent is not responsible for any fees related to unsolicited resumes.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Product management', 'MS SQL', 'NoSQL', 'Data modeling', 'MySQL', 'Machine learning', 'Scrum', 'Data quality', 'SQL', 'Recruitment']",2025-06-12 14:52:10
Sr. Database Engineer For US shift (Eastern Time),TEOCO,5 - 10 years,Not Disclosed,"['Kolkata', 'Bengaluru']","Position: Sr. Database Engineer US shift (Eastern Time)\nLocation: Kolkata or Bangalore\nFull time permanent position\n\nUS shift (Eastern Time) : 4.30PM to 12.30AM IST (complete work from home)\n\nExperience required: 6-8+ years\n\n\nMajor skills required: SQL, C# or Python, any ETL tool\n\n\nProduct Development:\n\nWork with the business analysts to understand the high level business need and requirements;\nWork with operation team to understand issues and provide step-by-step solutions;\nImplement business logic using SQL;\nMonitor the system for any issues and quickly respond to emergencies;\nDeep detailed understanding of internal ETL tool;\nPrepare product for the release and drive the process;\nProficiency in query optimization (understanding query plans, improving execution time etc.);\nWrite scripts using internal code language (C# based) in order to optimize the process;\nUnderstand the overall process and data flows;\nPerform detailed analysis of the code and do research to help analysts understand current situation and make a decision.\n\nExperience and required skills:\n\nStrong understanding of relational databases;\nAdvanced SQL knowledge is required;\nWorking experience with MPP (MPP Massively Parallel Processing) Databases, understanding database design (data distribution, partitioning etc.);\nMedium Linux knowledge is required;\nC# or Python mid-level;\nExperience with analytic reporting tools such as SAP Business Object is preferred;\nAbility to work in a multi-cultured team environment;\nStrong oral and written communication skills.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SQL', 'C#', 'python', 'ETL']",2025-06-12 14:52:13
Ai Ml Engineer,Optum,5 - 10 years,Not Disclosed,['Noida'],"Optum is a global organization that delivers care, aided by technology to help millions of people live healthier lives. The work you do with our team will directly improve health outcomes by connecting people with the care, pharmacy benefits, data and resources they need to feel their best. Here, you will find a culture guided by inclusion, talented peers, comprehensive benefits and career development opportunities. Come make an impact on the communities we serve as you help us advance health optimization on a global scale. Join us to start Caring. Connecting. Growing together.  \nAI Engineer is tasked with the design, development, and deployment of advanced generative AI models and systems. This position requires close collaboration with data scientists, product managers, and other stakeholders to integrate generative AI solutions into existing products and develop new innovative features. Proficiency in the Agentic AI framework is vital for coordinating multiple autonomous AI agents to accomplish complex tasks.\n\nPrimary Responsibilities:\nImplement Generative AI Models: Develop sophisticated generative AI algorithms and models to create new data samples, patterns, or content based on existing data or inputs\nData Processing: Collaborate with stakeholders to preprocess, analyze, and interpret extensive datasets\nModel Deployment: Deploy generative AI models into production environments, ensuring scalability and robustness\nOptimization: Conduct model testing, validation, and optimization to enhance performance\nIntegration: Work with cross-functional teams to seamlessly integrate generative AI solutions into products\nResearch: Stay current with the latest advancements in generative AI technologies and practices\nAgentic AI Framework: Utilize the Agentic AI framework to coordinate multiple AI agents for the completion of complex tasks\nMentorship: Provide mentorship to junior team members and offer technical guidance\nComply with the terms and conditions of the employment contract, company policies and procedures, and any and all directives (such as, but not limited to, transfer and/or re-assignment to different work locations, change in teams and/or work shifts, policies in regards to flexibility of work benefits and/or work environment, alternative work arrangements, and other decisions that may arise due to the changing business environment). The Company may adopt, vary or rescind these policies and directives in its absolute discretion and without any limitation (implied or otherwise) on its ability to do so\n\nRequired Qualifications:\nBachelor's or Master's degree in Computer Science, Engineering, or a related field\n5+ years of experience in software engineering with a focus on AI/ML\nExperience with data preprocessing and analysis\nKnowledge of the Agentic AI framework and its application in AI systems\nProficiency in machine learning frameworks such as TensorFlow and PyTorch\nSolid programming skills in Python, Java, or C++\nFamiliarity with cloud platforms (e.g., AWS, Google Cloud, Azure)\nProven excellent problem-solving abilities and algorithmic thinking\nProven solid communication and teamwork skills\n\nPreferred Qualifications:\nExperience with data processing\nKnowledge of version control systems like Git\nUnderstanding of Generative AI, associated technologies and frameworks like RAG, agents etc.",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Agentic Ai', 'Gen AI', 'Cloud', 'RAG', 'LLM']",2025-06-12 14:52:15
Software Engineer II,Chegg,3 - 8 years,Not Disclosed,['New Delhi'],"About the Team\nChegg's engineering team is a group of passionate engineers who, in close collaboration with data scientists, product managers, designers, and other backend developers, build the future of the online education industry. We develop our products to scale and to last, we dont take shortcuts (hello unit tests and documentation), and we take pride in delivering high-quality solutions on time. We are cloud native.\nRole\nWe are looking for software engineers passionate about solving real-world problems for students in online education using technology. The ideal candidate can think outside the box, is passionate about technology, is adaptable, thinks big, and is passionate about making an impact. Chegg is evolving very fast, and we are constantly redefining our offerings to match the requirements of our student community; the candidate should have the appetite to pivot fast and be interested in continuous improvement and learning. Chegg has a very open and vibrant engineering culture where the candidate will get the opportunity to work with the best in the industry; the role demands ideating and sharing creative ideas as you never know the next big thing Chegg works on can come from you !! If you have dreamt of leveraging your skills and knowledge to impact something big enough to matter, Chegg provides those opportunities, and the candidate should make the best use of them.\nResponsibilities\nDetermine operational feasibility by evaluating analysis, problem definition, requirements, solution development, and proposed solutions;\nCross-team collaboration in driving the end-to-end delivery of SDN on Edge;\nParticipating in the code reviews and design discussions of other engineers;\nHave a strong sense of end-to-end ownership;\nAdhere to key principles: Code and design for best performance, scalability, and resiliency;\nParticipate in daily SCRUM meetings;\nParticipates in the testing process through test review and analysis, test witnessing, and certification of software;\nBe a self-starter, capable of solving ambiguous and challenging technical problems with wide scope;\nFull stack development of new features/tools, including design, documentation, implementation, and testing;\nWork alongside other engineers on the team to elevate technology and consistently apply best practices.\nSkills and Qualifications [Must Have]\nB.E., B.Tech, . degree in Computer Science or a related technical field\n3+ years of product lifecycle experience (from customer requirements -> functional spec -> design -> development/testing -> deployment and monitoring);\nStrong interpersonal and communication skills;\nStrong hands-on development/scripting experience with Python and shell.\nUse tools and methodologies to create representations of workflows, user interfaces, data schemas, etc;\nSolid understanding of software design and development;\nExperience with third-party libraries and APIs;\nExcellent design and problem-solving skills.\nStrong experience with Cloud technologies such as AWS\nExperience with Unit testing frameworks for TDD (Test Driven Development) methodology\nSkills and Qualifications [Good To Have]\nSolid understanding of Agile methodologies and experience working in Agile teams.\nHands-on experience with CI/CD pipelines, preferably using GitLab.\nDevelopment knowledge of mobile apps (android/iOS)",Industry Type: E-Learning / EdTech,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Python', 'schema', 'continuous integration', 'software testing', 'software design', 'unit testing', 'android', 'ci/cd', 'solution development', 'ios', 'cloud technologies', 'tdd', 'full stack', 'scrum', 'gitlab', 'shell scripting', 'software engineering', 'code review', 'agile', 'api', 'agile methodology']",2025-06-12 14:52:17
AI Engineer,HCLTech,10 - 14 years,Not Disclosed,['Noida'],"Seniority: Senior\nDescription & Requirements\nPosition Summary\nThe Senior AI Engineer with GenAI expertise is responsible for developing advanced technical solutions, integrating cutting-edge generative AI technologies. This role requires a deep understanding of modern technical and cloud-native practices, AI, DevOps, and machine learning technologies, particularly in generative models. You will support a wide range of customers through the Ideation to MVP journey, showcasing leadership and decision-making abilities while tackling complex challenges.",,,,"['AI engineering', 'VMware', 'Java', 'Azure', 'Data engineering', 'AI models', 'Node.js', 'NLP', 'Azure AKS', 'Machine Learning Operations', 'AWS', 'Kubernetes', 'Python']",2025-06-12 14:52:20
"Business Intelligence Engineer, RBS ARTS",Amazon,5 - 10 years,Not Disclosed,['Chennai'],"An candidate will be a self-starter who is passionate about discovering and solving complicated problems, learning complex systems, working with numbers, and organizing and communicating data and reports. You will be detail-oriented and organized, capable of handling multiple projects at once, and capable of dealing with ambiguity and rapidly changing priorities. You will have expertise in process optimizations and systems thinking and will be required to engage directly with multiple internal teams to drive business projects/automation for the RBS team. Candidates must be successful both as individual contributors and in a team environment, and must be customer-centric. Our environment is fast-paced and requires someone who is flexible, detail-oriented, and comfortable working in a deadline-driven work environment. Responsibilities Include Works across team(s) and Ops organization at country, regional and/or cross regional level to drive improvements and enables to implement solutions for customer, cost savings in process workflow, systems configuration and performance metrics.\n\nBasic Qualifications\nBachelors degree in Computer Science, Information Technology, or a related field\nProficiency in automation using Python\nExcellent oral and written communication skills\nExperience with SQL, ETL processes, or data transformation\n\nPreferred Qualifications\nExperience with scripting and automation tools\nFamiliarity with Infrastructure as Code (IaC) tools such as AWS CDK\nKnowledge of AWS services such as SQS, SNS, CloudWatch and DynamoDB\nUnderstanding of DevOps practices, including CI/CD pipelines and monitoring solutions\nUnderstanding of cloud services, serverless architecture, and systems integration\n\n\nAs a Business Intelligence Engineer in the team, you will collaborate closely with business partners, architect, design, implement, and BI projects & Automations.\n\nResponsibilities:\n\nDesign, development and ongoing operations of scalable, performant data warehouse (Redshift) tables, data pipelines, reports and dashboards.\nDevelopment of moderately to highly complex data processing jobs using appropriate technologies (eg SQL, Python, Spark, AWS Lambda, etc)\nDevelopment of dashboards and reports.\nCollaborating with stakeholders to understand business domains, requirements, and expectations. Additionally, working with owners of data source systems to understand capabilities and limitations.\nDeliver minimally to moderately complex data analysis; collaborating as needed with Data Science as complexity increases.\nActively manage the timeline and deliverables of projects, anticipate risks and resolve issues.\nAdopt best practices in reporting and analysis: data integrity, test design, analysis, validation, and documentation.\nInternal job description\n\nRetail Business Service, ARTS is a growing team that supports the Retail Efficiency and Paid Services business and tech teams. There is ample growth opportunity in this role for someone who exhibits Ownership and Insist on the Highest Standards, and has strong engineering and operational best practices experience.\n\nBasic qualifications:\n\n5+ years of relevant professional experience in business intelligence, analytics, statistics, data engineering, data science or related field.\nExperience with Data modeling, SQL, ETL, Data Warehousing and Data Lakes.\nStrong experience with engineering and operations best practices (version control, data quality/testing, monitoring, etc)\nExpert-level SQL.\nProficiency with one or more general purpose programming languages (eg Python, Java, Scala, etc)\nKnowledge of AWS products such as Redshift, Quicksight, and Lambda.\nExcellent verbal/written communication & data presentation skills, including ability to succinctly summarize key findings and effectively communicate with both business and technical teams.\n\nPreferred qualifications:\n\nExperience with data-specific programming languages/packages such as R or Python Pandas.\nExperience with AWS solutions such as EC2, DynamoDB, S3, and EMR.\nKnowledge of machine learning techniques and concepts. 3+ years of analyzing and interpreting data with Redshift, Oracle, NoSQL etc experience\nExperience with data visualization using Tableau, Quicksight, or similar tools\nExperience with data modeling, warehousing and building ETL pipelines\nExperience in Statistical Analysis packages such as R, SAS and Matlab\nExperience using SQL to pull data from a database or data warehouse and scripting experience (Python) to process data for modeling Experience with AWS solutions such as EC2, DynamoDB, S3, and Redshift\nExperience in data mining, ETL, etc and using databases in a business environment with large-scale, complex datasets",,,,"['SAS', 'Data modeling', 'Oracle', 'Business intelligence', 'MATLAB', 'Information technology', 'Analytics', 'SQL', 'Python']",2025-06-12 14:52:22
"Staff Engineer, Nodejs",Nagarro,7 - 10 years,Not Disclosed,['India'],"We're Nagarro.\n\nWe are a Digital Product Engineering company that is scaling in a big way! We build products, services, and experiences that inspire, excite, and delight. We work at scale across all devices and digital mediums, and our people exist everywhere in the world (18000+ experts across 38 countries, to be exact). Our work culture is dynamic and non-hierarchical. We're looking for great new colleagues. That's where you come in!\n\nREQUIREMENTS:\nTotal Experience 7+ years.\nExcellent knowledge developing scalable and highly available Restful APIs using NodeJS technologies.\nThorough understanding of React.js and its core principles and experience with popular React.js workflows (such as Flux or Redux or Context API or Data Structures).\nFamiliarity with common programming tools such as RESTful APIs, TypeScript, version control software, and remote deployment tools, CI/CD tools.\nUnderstanding of linter libraries (TSLINT, Prettier etc) and Unit testing using Jest, Enzyme, Jasmine or equivalent framework.\nStrong proficiency in JavaScript, including DOM manipulation and the JavaScript object model. Proficient with the latest versions of ECMAScript (JavaScript or TypeScript).\nUnderstanding of containerization, experienced in Dockers, Kubernetes.\nExposed to API gateway integrations like 3Scale.\nUnderstanding of Single-Sign-on or token-based authentication (Rest, JWT, OAuth).\nPossess expert knowledge of task/message queues include but not limited to: AWS, Microsoft Azure, Pushpin. and Kafka.\nPractical experience with GraphQL is good to have.\nWriting tested, idiomatic, and documented JavaScript, HTML and CSS.\nExperiencing in Developing responsive web-based UI.\nHave experience on Styled Components, Tailwind CSS, Material UI and other CSS-in-JS techniques.\nProblem-solving mindset with the ability to tackle complex data engineering challenges.\nStrong communication and teamwork skills, with the ability to mentor and collaborate effectively.\nRESPONSIBILITIES:\nWriting and reviewing great quality code\nUnderstanding the clients business use cases and technical requirements and be able to convert them into technical design which elegantly meets the requirements\nMapping decisions with requirements and be able to translate the same to developers.\nIdentifying different solutions and being able to narrow down the best option that meets the clients requirements.\nDefining guidelines and benchmarks for NFR considerations during project implementation\nWriting and reviewing design document explaining overall architecture, framework, and high-level design of the application for the developers.\nReviewing architecture and design on various aspects like extensibility, scalability, security, design patterns, user experience, NFRs, etc., and ensure that all relevant best practices are followed.\nDeveloping and designing the overall solution for defined functional and non-functional requirements; and defining technologies, patterns, and frameworks to materialize it\nUnderstanding and relating technology integration scenarios and applying these learnings in projects\nResolving issues that are raised during code/review, through exhaustive systematic analysis of the root cause, and being able to justify the decision taken.\nCarrying out POCs to make sure that suggested design/technologies meet the requirements",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure', 'Typescript', 'Node.Js', 'Docker', 'Microservices', 'Kubernetes']",2025-06-12 14:52:24
PCI Analyst,Yum! India,6 - 8 years,Not Disclosed,[],"PCI Analyst\n\nLocation-Remote\nShift Timings- 11:00 AM IST to 8:00 PM IST\n\nThe PCI Analyst plays a critical role in ensuring PCI DSS compliance across assigned divisions. This role partners with BISO teams and markets to gather, review, and submit evidence required for PCI assessments. The PCI Analyst maintains ongoing compliance readiness and contributes to the overall compliance strategy by collaborating with assessors and internal teams.\n\nRole & responsibilities\nAssist in planning, coordinating, and executing PCI DSS assessments and audits, including internal assessments and third-party audits.\nPartner with Brand/Market & BISO teams to gather, validate, and submit evidence for PCI assessments.\nWork to complete PCI DSS Service Provider and/or Merchant assessments in global markets, including but not limited to the UK, US, and Israel.\nEnsure ongoing PCI DSS compliance readiness across assigned divisions.\nCollaborate with internal teams to respond promptly and effectively to assessor requests.\nProvide guidance on PCI DSS requirements and evidence collection processes.\nStay informed of changes in PCI standards and industry best practices and communicate relevant updates to internal stakeholders.\nAssist with training and awareness initiatives related to payment card security and compliance.\n\nPreferred candidate profile\n\nCertification as a PCI DSS QSA, AQSA, or ISA.\nBachelors degree in Cyber security, Compliance, or a related field.\n6-8 years of hands-on experience in PCI compliance or related roles.\nIn-depth knowledge of PCI DSS requirements and compliance frameworks.\nStrong organizational and collaboration skills, with a focus on accuracy and efficiency.\nExperience supporting PCI DSS assessments (SAQ or ROC).\nKnowledge of security controls and technologies (e.g., firewalls, encryption, vulnerability scanning).\nIndustry certifications such as CISA, CISSP, PCI ISA, or CompTIA Security+",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['PCI DSS', 'Cyber Security', 'Compliance', 'ISA', 'Governance', 'audit', 'QSA']",2025-06-12 14:52:27
Business Analyst,XL India Business Services Pvt. Ltd,1 - 4 years,Not Disclosed,"['Hyderabad', 'Ahmedabad', 'Bengaluru']","Business Analyst Bangalore, Karnataka, India We seek a highly skilled Business Analyst with an effective focus on data pattern analysis and managing stakeholder expectations to join our team\n\nThe ideal candidate will be able to bridge the gap between business needs and technical solutions, ensuring seamless communication, data quality and integrity\n\nThe Senior Business Analayst should be able to communicate effectively with all stakeholders and drive quality assurance testing and UAT, and triage incoming issues, defects, and enhancement requests\n\nIndividuals with a balance in decision making, empathy towards business needs, and an understanding of delivery (Waterfall and AGILE) related constraints will be preferred\n\nWhat you ll be DOING What will your essential responsibilities include? Elicit, analyze, manage stakeholder expectations and deliver business artifacts (Requirement Documents or User Stories) and functional specifications\n\nCollaborate with stakeholders to gather, challenge, facilitate and document business requirements (up to securing sign off) in a business requirements specification, or a set of Features, Epics & User stories\n\nConduct business process analysis and identify opportunities for improvement\n\nIncludes creation of process flows, swim lanes, hosting focused group, brainstorming or shadowing/ reverse-shadowing sessions\n\nDesign, develop and execute test scenarios to validate data models and data flows\n\nEnsure adverse impact to existing systems or integrated platforms to be minimal and advise the respective teams well in advance\n\nPerform data quality checks and identify data inconsistencies or anomalies\n\nAssist with daily issue related tickets where business needs additional support or design confirmations\n\nManage, train and constantly improve existing processes that may or may not involve vendor partners with a goal to attain optimal efficiency\n\nYou will report to the Application Manager\n\nWhat you will BRING We re looking for someone who has these abilities and skills: Required Skills and Abilities: Experience with the end-to-end insurance value chain process (especially the claims process flow)\n\nAbility to query (basic or advanced) will be an additional advantage\n\nCBAP, INS (or other BA or Insurance certifications) will be preferred\n\nMust possess excellent interpersonal skills, effective communication skills (written and verbal), and the ability to present information in an influencing manner at the leadership level\n\nExcellent stakeholder management skills, with a proven ability to build trusting relationships with the business\n\nExcellent analytical skills to identify root causes, troubleshoot issues and propose solutions\n\nAble to test canonical data models\n\nDesired Skills and Abilities: Significant experience in supporting all phases of the software development life cycle\n\nAbilities to multi-task, prioritize and being a self-driven team player are required for this role\n\nWorking with tools like JIRA, Confluence\n\nBasic or advanced querying capabilities will be preferred\n\nPrevious domain exposure in the capacity of a business analyst with changes on Property & Casualty (Americas) platforms or General Insurance (Europe) will be an added advantage\n\nProven planning and organization skills, with an ability to work resiliently, planning their analysis work, prioritizing workload, preparing in advance, and setting realistic timescales\n\nAdaptable to new/different strategies, programs, technologies, practices, cultures, etc Comfortable with change, able to easily make transitions\n\nWho WE are AXA XL, the P&C and specialty risk division of AXA, is known for solving complex risks\n\nFor mid-sized companies, multinationals and even some inspirational individuals we don t just provide re/insurance, we reinvent it\n\nHow? By combining a comprehensive and efficient capital platform, data-driven insights, leading technology, and the best talent in an agile and inclusive workspace, empowered to deliver top client service across all our lines of business property, casualty, professional, financial lines and specialty\n\nWith an innovative and flexible approach to risk solutions, we partner with those who move the world forward\n\nLearn more at axaxl\n\ncom What we OFFER Inclusion AXA XL is committed to equal employment opportunity and will consider applicants regardless of gender, sexual orientation, age, ethnicity and origins, marital status, religion, disability, or any other protected characteristic\n\nAt AXA XL, we know that an inclusive culture and a diverse workforce enable business growth and are critical to our success\n\nThat s why we have made a strategic commitment to attract, develop, advance and retain the most diverse workforce possible, and create an inclusive culture where everyone can bring their full selves to work and can reach their highest potential\n\nIt s about helping one another and our business to move forward and succeed\n\nFive Business Resource Groups focused on gender, LGBTQ+, ethnicity and origins, disability and inclusion with 20 Chapters around the globe Robust support for Flexible Working Arrangements Enhanced family friendly leave benefits Named to the Diversity Best Practices Index Signatory to the UK Women in Finance Charter Learn more at axaxl\n\ncom / about-us / inclusion-and-diversity\n\nAXA XL is an Equal Opportunity Employer\n\nTotal Rewards AXA XL s Reward program is designed to take care of what matters most to you, covering the full picture of your health, wellbeing, lifestyle and financial security\n\nIt provides competitive compensation and personalized, inclusive benefits that evolve as you do\n\nWe re committed to rewarding your contribution for the long term, so you can be your best self today and look forward to the future with confidence\n\nSustainability At AXA XL, Sustainability is integral to our business strategy\n\nIn an ever-changing world, AXA XL protects what matters most for our clients and communities\n\nWe know that sustainability is at the root of a more resilient future\n\nOur 2023-26 Sustainability strategy, called Roots of resilience , focuses on protecting natural ecosystems, addressing climate change, and embedding sustainable practices across our operations\n\nOur Pillars: Valuing nature: How we impact nature affects how nature impacts us\n\nResilient ecosystems - the foundation of a sustainable planet and society - are essential to our future\n\nWe re committed to protecting and restoring nature - from mangrove forests to the bees in our backyard - by increasing biodiversity awareness and inspiring clients and colleagues to put nature at the heart of their plans\n\nAddressing climate change: The effects of a changing climate are far reaching and significant\n\nUnpredictable weather, increasing temperatures, and rising sea levels cause both social inequalities and environmental disruption\n\nWere building a net zero strategy, developing insurance products and services, and mobilizing to advance thought leadership and investment in societal-led solutions\n\nIntegrating ESG: All companies have a role to play in building a more resilient future\n\nIncorporating ESG considerations into our internal processes and practices builds resilience from the roots of our business\n\nWe re training our colleagues, engaging our external partners, and evolving our sustainability governance and reporting\n\nAXA Hearts in Action: We have established volunteering and charitable giving programs to help colleagues support causes that matter most to them, known as AXA XL s Hearts in Action programs\n\nThese include our Matching Gifts program, Volunteering Leave, and our annual volunteering day - the Global Day of Giving\n\nFor more information, please see axaxl\n\ncom/sustainability",Industry Type: Insurance,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Manager Quality Assurance', 'Claims', 'Agile', 'Test scenarios', 'Software development life cycle', 'Data quality', 'Business process analysis', 'Business strategy', 'JIRA', 'Stakeholder management']",2025-06-12 14:52:29
Business Analyst,XL India Business Services Pvt. Ltd,1 - 4 years,Not Disclosed,"['Hyderabad', 'Ahmedabad', 'Bengaluru']","Business Analyst Bengaluru/Gurgaon, India The Analyst will bridge the gap between business and IT by being able to communicate effectively with all stakeholders\n\nAnalyst will be responsible for supporting an application or multiple applications within a Delivery Team (or at times a answer Delivery Center) and her/his core responsibilities include eliciting and documenting both business, functional, non-functional and technical requirements, supporting quality assurance testing and UAT, and triaging incoming issues, defects, and enhancement requests\n\nAnalysts will also support projects that impact the application(s) s/he supports\n\nWhat you ll be DOING What will your essential responsibilities include? Collect, challenge, facilitate and document business requirements as well as define business specifications and requirements\n\nMake configuration changes to core products in support of meeting business requirements\n\nValidate the overall IT solution to make sure alignment to business requirements, review requirements and/or specifications with the development and test teams to make sure understanding\n\nPerform Root Cause Analysis in support of Problem Management and support operational tasks\n\nPerform deliverable reviews and manage measurement of deliverable quality\n\nCollaborate with Transformation & Change Delivery process resources to requirements and answer fit into the process flow and do not create gaps and/or breakages\n\nHelp identify how requirements can help make the process more efficient\n\nYou will report to the Analyst Lead\n\nWhat you will BRING We re looking for someone who has these abilities and skills: Ability to understand general business requirements and implement corresponding technical solutions\n\nFull technical knowledge of all phases of applications systems analysis\n\nAbility to work with data and complex systems\n\nFamiliarity with the Genius insurance application preferred\n\nExperience in an insurance or technology field\n\nExperience with multiple SDLC methodologies, particularly Agile principles\n\nMust have familiarity with Analyst methodologies e g Use Cases; Business Rule Development; User Interface Specs; Functional Specs\n\nMust possess excellent interpersonal skills, robust communication skills (written and verbal), and be team oriented\n\nMust be detail conscious, technically motivated, creative, and user oriented\n\nAble to determine and communicate impacts of system functionality and technical approach on performance, scalability and maintainability\n\nAbility to manage ambiguity and create accurate project estimates\n\nAccurately quantifies project specific risks through the project lifecycle\n\nAbility to present information in an influencing manner to leadership and all business stakeholders\n\nStakeholder management\n\nProven planning and organization skills, creating own work schedules, prioritizing workload, preparing in advance and setting realistic timescales\n\nAdaptable to new/different strategies, programs, technologies, practices, cultures, etc Comfortable with change, able to easily make transitions\n\nWho WE are AXA XL, the P&C and specialty risk division of AXA, is known for solving complex risks\n\nFor mid-sized companies, multinationals and even some inspirational individuals we don t just provide re/insurance, we reinvent it\n\nHow? By combining a robust and efficient capital platform, data-driven insights, leading technology, and the best talent in an agile and inclusive workspace, empowered to deliver top client service across all our lines of business property, casualty, professional, financial lines and specialty\n\nWith an innovative and flexible approach to risk solutions, we partner with those who move the world forward\n\nAt AXA XL we are happy to talk flexible working\n\nWe are committed to building a diverse and inclusive workforce and consider flexible ways of working for every role\n\nTalk to us about how we can make flexibility work for you\n\nLearn more at axaxl\n\ncom\n\nCorporate Responsibility At AXA XL our approach to corporate responsibility (CR) is the same as our approach to business; constantly seeking to provide innovative solutions to the world s most complex problems\n\nFrom offering our expertise, products and services to help build more resilient communities, to advancing understanding and response to climate change, our strategy - Our Impact\n\nOur Future\n\n- aligns key issues that are pertinent to our business - climate, water and financial resilience - and contributes to AXA Group s purpose to Act for human progress by protecting what matters\n\nClimate: We re reducing our carbon footprint, protecting ecosystems and exploring how our business can help build a better world\n\nWater: We re developing water resilience where it is and will be needed most\n\nFinancial resilience: We re helping create opportunities for the unemployed and underemployed, so they can be better prepared for unexpected changes\n\nHearts in Action: We have established volunteering and charitable giving programs to help colleagues support causes that matter most to them, known as our Hearts in Action programs\n\nFor more information, please see the Corporate Responsibility section on our website\n\nWhat we OFFER Inclusion At AXA XL, we know that an inclusive culture and a diverse workforce enable business growth and are critical to our success\n\nThat s why we have made a strategic commitment to attract, develop, advance and retain the most diverse workforce possible, while creating an inclusive culture where everyone can bring their full selves to work and can reach their highest potential\n\nIt s about helping one another and our business to move forward and succeed\n\nFive Business Resource Groups focused on gender, LGBTQ+, race/ethnicity, disability and inclusion with 20 Chapters around the globe Robust support for Flexible Working Arrangements Enhanced family friendly leave benefits Named to the Diversity Best Practices Index Signatory to the UK Women in Finance Charter Learn more at axaxl\n\ncom / about-us / inclusion-and-diversity\n\nAXA XL is an Equal Opportunity Employer\n\nTotal Rewards AXA XL s Reward program is designed to take care of what matters most to you, covering the full picture of your health, wellbeing, lifestyle and financial security\n\nIt provides competitive compensation and personalized, inclusive benefits that evolve as you do\n\nWe re committed to rewarding your contribution for the long term, so you can be your best self today and look forward to the future with confidence\n\nSustainability At AXA XL, Sustainability is integral to our business strategy\n\nIn an ever-changing world, AXA XL protects what matters most for our clients and communities\n\nWe know that sustainability is at the root of a more resilient future\n\nOur 2023-26 Sustainability strategy, called Roots of resilience , focuses on protecting natural ecosystems, addressing climate change, and embedding sustainable practices across our operations\n\nOur Pillars: Valuing nature: How we impact nature affects how nature impacts us\n\nResilient ecosystems - the foundation of a sustainable planet and society - are essential to our future\n\nWe re committed to protecting and restoring nature - from mangrove forests to the bees in our backyard - by increasing biodiversity awareness and inspiring clients and colleagues to put nature at the heart of their plans\n\nAddressing climate change: The effects of a changing climate are far-reaching and significant\n\nUnpredictable weather, increasing temperatures, and rising sea levels cause both social inequalities and environmental disruption\n\nWere building a net zero strategy, developing insurance products and services, and mobilizing to advance thought leadership and investment in societal-led solutions\n\nIntegrating ESG: All companies have a role to play in building a more resilient future\n\nIncorporating ESG considerations into our internal processes and practices builds resilience from the roots of our business\n\nWe re training our colleagues, engaging our external partners, and evolving our sustainability governance and reporting\n\nAXA Hearts in Action: We have established volunteering and charitable giving programs to help colleagues support causes that matter most to them, known as AXA XL s Hearts in Action programs\n\nThese include our Matching Gifts program, Volunteering Leave, and our annual volunteering day - the Global Day of Giving\n\nFor more information, please see axaxl\n\ncom/sustainability",Industry Type: Insurance,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Root cause analysis', 'Interpersonal skills', 'Manager Quality Assurance', 'Project estimation', 'Agile', 'Problem management', 'Business strategy', 'Stakeholder management', 'Operations', 'SDLC']",2025-06-12 14:52:31
Analyst,Barclays,1 - 10 years,Not Disclosed,['Chennai'],"Join Barclays as a Analyst role where youll spearhead the evolution of our digital landscape, driving innovation and excellence. Youll harness cutting-edge technology to revolutionise our digital offerings, ensuring unapparelled customer experiences. At Barclays, we dont just anticipate the future - were creating it.\nTo be successful in this role, you should have below skills:\nExperienced with MS office toolkit (Word, PPT, Excel, Access Database etc).\nExcellent communication and presentation skills in both formal and informal settings ability to interact with the region and UK / US as part of a global team (written and verbal) Strong control awareness - in particular, the ability to identify and escalate potential control breakdowns and to streamline processes in an effective and efficient manner.\nHas the initiative and ability to break down problems into components parts and resolving them.\nAttention to detail and analytical.\nConfident and assertive manner.\nSome other highly valued skills may include below:\nCFA/master s in finance / financial engineering would be an advantage\nArticle ship / Industrial trainee in mid to large sized firms\nAudit exposure in large financial institutions / banks preferred.\nYou may be assessed on the key critical skills relevant for success in role, such as risk and controls, change and transformation, business acumen strategic thinking and digital and technology, as well as job-specific technical skills.\nThis role is based in our Chennai office.\nPurpose of the role\nTo support business areas with day-to-day processing, reviewing, reporting, trading and issue resolution.\nAccountabilities\nSupport various business areas with day-to-day initiatives including processing, reviewing, reporting, trading, and issue resolution.\nCollaboration with teams across the bank to align and integrate operational processes.\nIdentification of areas for improvement and providing recommendations in operational processes.\nDevelopment and implementation of operational procedures and controls to mitigate risks and maintain operational efficiency.\nDevelopment of reports and presentations on operational performance and communicate findings to internal senior stakeholders.\nIdentification of industry trends and developments to implement best practice in banking operations.\nParticipation in projects and initiatives to improve operational efficiency and effectiveness.\nAnalyst Expectations\nWill have an impact on the work of related teams within the area.\nPartner with other functions and business areas.\nTakes responsibility for end results of a team s operational processing and activities.\nEscalate breaches of policies / procedure appropriately.\nTake responsibility for embedding new policies/ procedures adopted due to risk mitigation.\nAdvise and influence decision making within own area of expertise.\nTake ownership for managing risk and strengthening controls in relation to the work you own or contribute to. Deliver your work and areas of responsibility in line with relevant rules, regulation and codes of conduct.\nMaintain and continually build an understanding of how own sub-function integrates with function, alongside knowledge of the organisations products, services and processes within the function.\nDemonstrate understanding of how areas coordinate and contribute to the achievement of the objectives of the organisation sub-function.\nMake evaluative judgements based on the analysis of factual information, paying attention to detail.\nResolve problems by identifying and selecting solutions through the application of acquired technical experience and will be guided by precedents.\nGuide and persuade team members and communicate complex / sensitive information.\nAct as contact point for stakeholders outside of the immediate function, while building a network of contacts outside team and external to the organisation.",Industry Type: Financial Services,"Department: BFSI, Investments & Trading","Employment Type: Full Time, Permanent","['SUB', 'Senior Analyst', 'Analytical', 'Banking operations', 'Service excellence', 'Issue resolution', 'MS Office', 'Operations', 'Auditing', 'Risk mitigation']",2025-06-12 14:52:34
Business Analyst,Karur Vysya Bank,2 - 5 years,4-7 Lacs P.A.,['Karur'],"Role & responsibilities\nDrive awareness of requirements across business units and identify substandard systems/ processes through evaluation of real-time data.\nServe as thought leader for technical business processes, developing systems prototypes that promote increased efficiency and productivity on multiple levels.\nCreate and implement precise management plans for every project, with attention to transparent communication at all levels",,,,"['Power Bi', 'Tableau', 'Business Analysis', 'SQL']",2025-06-12 14:52:36
Business Analyst,Netcracker,10 - 16 years,Not Disclosed,"['Hyderabad', 'Pune', 'Gurugram']","Job Description\nGeneral purpose of position:\nEssentially, the work will be around conducting requirement analysis, preparing and tracking delivery of high level and detailed solution designs. It will require candidate to understand and analyze client business operations and work on impact analysis on to-be or already deployed NC product stack. The candidate is expected to provide solution options that deliver value to the client and will be responsible for high quality assurance of his work products.\nSkills and abilities: The incumbent must have the ability and orientation to work on simple to complex solutions with high degree of quality and accuracy.\n       Strong expertise in requirements management processes, including standard tools and techniques such as facilitated meetings, use cases, mapping documents, models and other requirements management tools\n       Applied knowledge of different data interchange formats and Telecom APIs is a must, especially covering the following:\n       Should be able to read JSON, XML structure/documentation.\n       Hands on experience on UML Diagrams like (Sequence, Component Architecture, and ER etc.)\n       Experience in File based, Web-Service based, and Messaging Based Integration.\n       Basic Knowledge of FTP, REST/SOAP, Queue Concepts.\n       Excellent written and verbal communication skills.\n       Process oriented. Ability to learn new processes and willingness to apply the learning.\n       Ability to meet occasionally tight deadlines, proactive and committed to excellence in providing results\n       Highly Self-motivated and able to work independently with limited supervision\n       Basic knowledge and understanding of SQL, object data model\nMinimum Education and Experience:\n       People with experience in Business Analysis in Telecom BSS processes and in IT and not just limited to documentation of functional aspects\n       Graduates and Post graduates with excellent academics.\n       Prior telecom experience is mandatory.\nSkills: E2E Solutioning, eTOM, TMF Open Api, Digital Transformation\n \nRoles and Responsibilities\nMajor duties and responsibilities:\n       Understand end to end telecom processes around Customer acquisition, CIM, POC, Ordering etc (End to End Telecom BSS experience/knowledge required)\n       Understand catalog configuration and Integration processes involved in telecom around Customer Data, Billing and Payments Data, Resource Inventory and provisioning systems.\n       Analyze the integration data sources including the Interface agreements involved in the integration with third-party/external systems.\n       Conduct detailed business analysis and prepare designs for various stages for development and implementation projects as telecom subject matter expert;\n       Consolidate various inputs and data from multiple sources to create and modify solution options.\n       Perform through quality assurance on documents to ensure consistency and quality.\n       Keep in touch with the newest technologies and trends in the telecommunications industry\n       Manage client engagement through written and verbal communication throughout the scoping, analysis and design delivery stages.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['bss', 'api integration', 'data', 'tmf', 'sql', 'requirement gathering', 'uml', 'xml', 'open api', 'writing', 'json', 'e2e', 'telecom', 'api', 'requirement analysis', 'communication skills', 'rest', 'process', 'etom', 'ftp', 'solutioning', 'verbal communication', 'business analysis', 'telecom bss', 'requirements management', 'integration', 'soap', 'object']",2025-06-12 14:52:39
Business Analyst - Demo Engineering,Etech Global Services,2 - 6 years,Not Disclosed,[],"About Etech\nEtech is a leading provider of customer engagement solutions and services utilising inbound and outbound voice as well as web chat. For over a decade, we have been helping companies cost-effectively acquire new customers and maximise profits by servicing and growing existing customers. We are a Tier One preferred provider for Fortune 500 companies and employ roughly 3000+ team members operating in 9 Global Centres (6 in the US, 2 in India, and 1 in Jamaica)\nWe are hiring for one of the top Saas-based reputed clients based in the US which is associated with Etech. What is the client software?\n\nThe client product is a SaaS (Software as a Service) platform to help companies learn about their customers feedback about their products and services. This domain is called CEM (Customer Experience Management).\n\nJob Description:\nAs an analyst, you will first learn a lot about the client platform and how to configure/create a solution for its customers through extensive product training. After that, you will work with client project managers and other team members to build and support our product implementations for large companies. You will utilise your knowledge of technology to think of creative solutions on the client platform.\n\nThe Role:\nAs an Analyst, you will be assigned to multiple projects depending on our needs, your interests, and expertise. The projects can vary from assisting your team in implementing, maintaining, and testing our software for new customers to managing customer accounts post-launch.\n\nRESPONSIBILITIES:\n\nProduct Implementation:\nWork with senior team members to carry out customer implementations and program enhancements.\nParticipate in the implementation design, setup, and review processes\nIdentify improvements to our feedback products and processes\nUtilize Client software knowledge for testing customized software solutions\nClient Management:\nBuild long-standing customer relationships by improving customer feedback programs\nProvide support to client meetings by leveraging in-depth Client system capabilities\nWork with client teams in resolving technical/system-related inquiries\nProvide quality assurance support when providing features to clients\nProvide client support when analysing large sets of data\nQualification Required:\nExcellent analytical skills (including Microsoft Excel) and attention to detail\nStrong written, oral communication and presentation skills\nBachelor's / Master's degree in Computer Application (B.Sc. IT, M.Sc. IT, BCA, MCA, B.Tech Computer Engineering, B.Tech Computer Science etc.)\nExperience working on JavaScript, HTML, CSS, and XML.\nGUI would be an added advantage\nETL Experience / MEC Pre-processor building\nExperience working with customer IT teams for setting up data transfer API calls / SFTP\ntransfers\nAny experience in working with Audio / Speech / Speech to text (in or outside Medallia)\nImplementation experience (not just servicing), if you have Medallia experience\nCustomer facing Experience\nPreferred Candidates:\nExperience in management consulting, IT consulting, market research, and/or enterprise software client management either in college or at work to facilitate teamwork in remote settings will be preferred.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['CSS', 'Javascript', 'HTML', 'API', 'ETL', 'XML']",2025-06-12 14:52:41
Business Analyst,Speridian Technologies,4 - 9 years,Not Disclosed,"['Kochi', 'Mumbai', 'Bengaluru']","Excellent communication skills as it needs interaction with business team\nCollaborative\nRequirement Gathering experience.\nAnalytical skills Analysing business req, data.\nUnderstanding of Requirement gathering techniques like brain storming, setting up focus group sessions etc.\nShould know how to write BRD (Business Req Document)\nUnderstanding of Agile techniques.\nCan create user stories.\nSalesforce Knowledge (can be addon) but can we be optional",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Analytical skills', 'Business Analyst', 'Agile', 'Salesforce']",2025-06-12 14:52:43
Senior Research Analyst,Demandbase,8 - 10 years,Not Disclosed,['Hyderabad'],"Introduction to Demandbase:\nDemandbase is the Smarter GTM company for B2B brands. We help marketing and sales teams overcome the disruptive data and technology fragmentation that inhibits insight and forces them to spam their prospects. We do this by injecting Account Intelligence into every step of the buyer journey, wherever our clients interact with customers, and by helping them orchestrate every action across systems and channels - through advertising, account-based experience, and sales motions. The result? You spot opportunities earlier, engage with them more intelligently, and close deals faster.\nAs a company, we re as committed to growing careers as we are to building world-class technology. We invest heavily in people, our culture, and the community around us. We have offices in the San Francisco Bay Area, New York, Seattle, and teams in the UK and India . We have also been continuously recognized as one of the best places to work in the San Francisco Bay Area.\nWere committed to attracting, developing, retaining, and promoting a diverse workforce. By ensuring that every Demandbase employee is able to bring a diversity of talents to work, were increasingly capable of living out our mission to transform how B2B goes to market. We encourage people from historically underrepresented backgrounds and all walks of life to apply. Come grow with us at Demandbase!\nAbout the Role:\nWe are seeking a highly motivated and detail-oriented Senior Research Analyst to join our dynamic team. This role is crucial for driving informed business decisions through data gathering, analysis, and insightful reporting. The ideal candidate will possess a strong understanding of business research methodologies, data analysis techniques, and a passion for data accuracy and problem-solving.\nKey Responsibilities:\nLead Comprehensive Data Research and Analysis: Source, collect, research, and analyze data from a variety of business information sources and specialized databases to generate actionable insights.\nDrive Data-Driven Decision-Making: Conduct in-depth strategic analysis to identify trends, anomalies, and root causes, translating complex findings into clear, impactful recommendations for product and business growth.\nEnsure Data Quality and Integrity: Apply strong problem-solving skills to resolve data queries, perform rigorous quality checks, and proactively identify and address data coverage gaps.\nProvide Training and Knowledge Transfer: Mentor and train new team members on industry best practices and advanced data analysis techniques.\nLeverage Domain and Product Expertise: Work closely with data engineers, product teams, and business stakeholders to define and deliver technical roadmaps, ensuring sound solutions and maximizing customer value.\nRequired Skills & Experience:\nBachelor s or Master s degree in Business or Commerce\n8-10 years of relevant work experience\nExpertise in sourcing and extracting data from diverse business information sources\nAdvanced proficiency in Microsoft Excel (e.g., pivot tables, VLOOKUP, complex formulas, data validation, charting) for data manipulation, analysis, and reporting\nSkill in translating complex data into visually compelling narratives for various audiences\nAbility to design and create clear, insightful, and actionable dashboards and reports\nExcellent communication and interpersonal skills\nSelf-organized and self-driven, with strong personal integrity\nStrong understanding and application of data quality principles and best practices\nAbility to perform root cause analysis on large datasets and identify underlying business drivers\nProven ability to train and mentor new team members, sharing best practices and advanced techniques and strong knowledge transfer skills.\nA strong passion for data, continuous learning, and staying updated with industry best practices and emerging analytical techniques.\nStrong organizational and time management skills\nAbility to work independently, manage multiple priorities, and meet deadlines in a fast-paced environment.\nOur Commitment to Diversity, Equity, and Inclusion at Demandbase\nAt Demandbase, we believe in creating a workplace culture that values and celebrates diversity in all its forms. We recognize that everyone brings unique experiences, perspectives, and identities to the table, and we are committed to building a community where everyone feels valued, respected, and supported. Discrimination of any kind is not tolerated, and we strive to ensure that every individual has an equal opportunity to succeed and grow, regardless of their gender identity, sexual orientation, disability, race, ethnicity, background, marital status, genetic information, education level, veteran status, national origin, or any other protected status. We do not automatically disqualify applicants with criminal records and will consider each applicant on a case-by-case basis.\nWe recognize that not all candidates will have every skill or qualification listed in this job description. If you feel you have the level of experience to be successful in the role, we encourage you to apply!\nWe acknowledge that true diversity and inclusion require ongoing effort, and we are committed to doing the work required to make our workplace a safe and equitable space for all. Join us in building a community where we can learn from each other, celebrate our differences, and work together.\nPersonal information that you submit will be used by Demandbase for recruiting and other business purposes. Our Privacy Policy explains how we collect and use personal information.\nPersonal information that you submit will be used by Demandbase for recruiting and other business purposes. Our Privacy Policy explains how we collect and use personal information.",Industry Type: Advertising & Marketing,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['SAN', 'Data analysis', 'Data validation', 'Excel', 'Data research', 'Business research', 'VLOOKUP', 'Analytical', 'Data quality', 'Senior Research Analyst']",2025-06-12 14:52:45
Business Analyst (LTS),Relanto Global,5 - 10 years,Not Disclosed,['Bengaluru'],"Client - Cisco \n\n Experience - 5+Years \n\n Primary Job Responsibilities: \nManage customer involvement for tool and legal platform design, testing, and program roll-out.\nConduct business process analysis, scope assessments, and preliminary cost/benefits/gap analyses for business initiatives.\nHighly experienced in identifying policy violations, analyzing, and proposing risk mitigating design solutions.\nAbility to translate work processes into precise business requirements, epics, and user stories.\nSuccessfully plan, implement, test, and enable new features or enhancements for new or existing applications.\nServe as a liaison between the business, cross-functional teams, and IT to provide technical and business solutions that meet user needs.\nConduct training and knowledge transfer sessions to worldwide business teams.\nAbility to efficiently handle work that crosses across inter-related business teams and tools.\nAbility to access data from repositories using SQL queries to analyze large datasets and determine trends, opportunities, alarming issues, and hidden patterns.\nManage customer involvement for tool and legal platform design, testing, and program roll-out.\nConduct business process analysis, scope assessments, and preliminary cost/benefits/gap analyses for business initiatives.\nHighly experienced in identifying policy violations, analyzing, and proposing risk mitigating design solutions.\nAbility to translate work processes into precise business requirements, epics, and user stories.\nSuccessfully plan, implement, test, and enable new features or enhancements for new or existing applications.\nServe as a liaison between the business, cross-functional teams, and IT to provide technical and business solutions that meet user needs.\nConduct training and knowledge transfer sessions to worldwide business teams.\nAbility to efficiently handle work that crosses across inter-related business teams and tools.\nAbility to access data from repositories using SQL queries to analyze large datasets and determine trends, opportunities, alarming issues, and hidden patterns.\n\n General expectations for this role: \nUnderstand contracting tools and contract repositories\nHave awareness of business needs and practical application for business needs\nBuild, depict, and optimize contract process flows for different levels of complexities\nRespond to requests; gathers necessary information to resolve cases\nDelivers quality work product on assigned tasks; aids in troubleshooting and drives creative solutions; implements solutions and/or fixes\n\n Desired Skills and Characteristics: \nOutstanding ability to discuss complex issues in a clear and simple manner both in writing and orally;\nFocused attention to detail and ability to work independently;\nStrong organizational and social skills and desire to make legal processes more efficient;\nExcellent business judgment, advocating for acceptable legal risk to enable business outcomes;\nCustomer and client-centric approach, demonstrating respect and a positive attitude under all circumstances;\nStrong leadership skills, wanting to chip in to a team culture focused on shared success\nCritical skills include problem solving, ability to envision the implications of decisions\nProcess and systems background\nKnowledge and experience in agile project methodology\n\n Minimum Qualifications: \n\nBachelor's degree in computer science or related field\n\n5+ years proven experience in related field\n\nExperience with configuring and updating custom objects, fields, conditional logic, user access permission, validation rules, workflows, and approvals\n\nKnowledge of end-to-end legal contracting processes including contract generation, contract negotiations, and contract repositories\n\nExperience in contract lifecycle management tools such as Ariba, Conga, Ironclad, Apttus, Icertis, or others",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['sql queries', 'business process analysis', 'agile', 'conga', 'ariba', 'visualforce', 'soql', 'software testing', 'sfdc', 'business solutions', 'business analysis', 'user stories', 'triggers', 'process flow', 'sql', 'apex', 'salesforce', 'sales force development', 'workflow analysis']",2025-06-12 14:52:47
Senior ML Engineer,Corpay India,5 - 10 years,Not Disclosed,['Nagpur'],"Your role As a Senior ML Engineer, you will play a crucial role in analyzing, interpreting and building ML models to drive informed business decisions. You will be responsible for developing and implementing OCR solutions, analyzing complex datasets, and providing valuable insights to support our business objectives. Your expertise in statistical techniques, data mining, and reporting will contribute to optimizing efficiency and quality within our organization. What youll be doing\nRoles & Responsibilities\nInterpret and analyse data using statistical techniques to identify trends, patterns, and insights.\nDevelop and implement databases, data collection systems, and data analytics strategies to optimize statistical efficiency and quality.\nOwn and lead the project and the team under you.\nAcquire data from primary and secondary sources to build models.\nBuild, train and deploy models into Production systems.\nClean and filter data by reviewing computer reports, printouts, and performance indicators to identify and correct code problems.\nCollaborate with management to prioritize business and information needs.\nIdentify and define new process improvement opportunities based on data analysis findings.\nAct as primary and sole contact for the project.\nDevelop and present ongoing reports and dashboards to stakeholders, highlighting key insights and recommendations.\nAbility to take ad hoc meetings to support offshore customer queries.\nUtilize reporting packages, databases, and programming languages (such as SQL, Python, or R) for data analysis and visualization.\nStay updated with the latest trends and advancements in data analysis techniques and tools.\nSkills Required\nBachelors degree in computer science, Statistics, Mathematics, or a related field. A masters degree is a plus.\nMinimum of 5 years of proven working experience as ML Engineer or Data Science Engineer, preferably in a technology or finance-related industry.\nStrong technical expertise in data models, database design development, data mining, and segmentation techniques.\nProficiency in reporting packages (e.g., Business Objects), databases (e.g., SQL), and programming languages (e.g., Python, R frameworks).\nKnowledge of statistics and experience using statistical packages for analyzing datasets (e.g., Excel, SPSS, SAS).\nExceptional analytical skills with the ability to collect, organize, Analyse, and disseminate significant amounts of information with attention to detail and accuracy.\nProficient in querying databases, report writing, and presenting findings effectively to both technical and non-technical stakeholders.\nStrong problem-solving abilities and a proactive mindset to identify opportunities for process improvements.\nExcellent communication and collaboration skills, with the ability to work effectively in a team-oriented environment.\nExperience in building OCR models is an advantage.\nAbout Corpay\nCorpay is a global technology organisation that is leading the future of commercial payments with a culture of innovation that drives us to constantly create new and better ways to pay. Our specialized payment solutions help businesses control, simplify, and secure payment for fuel, general payables, toll and lodging expenses. Millions of people in over 80 countries around the world use our solutions for their payments.\nAll offers of employment made by Corpay (and its subsidiary companies) are subject to the successful completion of satisfactory pre-employment vetting by an independent supplier (Experian). This is in accordance with Corpays Resourcing Policy and include employment referencing, identity, adverse financial, criminal and sanctions list checks. We do this to meet our legal and regulatory requirements.\nCorpay is dedicated to encouraging a supportive and inclusive culture among our employees. It is within our best interest to promote diversity and eliminate discrimination in the workplace. We seek to ensure that all employees and job applicants are given equal opportunities.\nNotice to Agency and Search Firm Representatives: Corpay will not accept unsolicited CVs from agencies and/or search firms for this job posting. Resumes submitted to any Corpay employee by a third party agency and/or search firm without a valid written & signed search agreement, will become the sole property of Corpay. No fee will be paid if a candidate is hired for this position as a result of an unsolicited agency or search firm referral. Thank you.",Industry Type: Financial Services,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Data analysis', 'Business objects', 'SAS', 'Process improvement', 'Resourcing', 'Data collection', 'SPSS', 'Data mining', 'SQL']",2025-06-12 14:52:50
Analyst,Essen Vision Software,4 - 5 years,Not Disclosed,['Bengaluru'],"Key Responsibilities:\nMonitor Checkpoint firewall logs and alerts for anomalies or security threats.\nPerform initial triage and basic troubleshooting of firewall-related issues.\nHandle incidents and service requests related to firewall policies, NAT rules, and VPN access.\nEscalate complex issues to L2/L3 teams with detailed documentation.\nMaintain and update tickets in accordance with SLA requirements.\nAssist with periodic firewall health checks and performance monitoring.\nWork on predefined Standard Operating Procedures (SOPs).\nCoordinate with internal teams and vendors for issue resolution.\nEnsure compliance with security policies and procedures.\nRequired Skills:\nBasic understanding of firewall concepts, IP routing, and network protocols (TCP/IP, DNS, DHCP, etc.).\nExposure to Checkpoint firewall (R80.x preferred) or equivalent security solutions.\nFamiliarity with security incident management tools and ticketing systems.\nGood communication and documentation skills.\nWillingness to work in 24x7 environments or rotational shifts.\nPreferred Qualifications:\nBachelors degree in Engineering , Information Technology, or related field.\nCheckpoint CCSA (or in progress) is a plus.",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['TCP', 'Ccsa', 'Checkpoint Firewall', 'Configuration', 'VPN', 'IP', 'Troubleshooting']",2025-06-12 14:52:52
Remote Cybersecurity Analyst 57LakhsCTC|| Srinivasa Reddy Kandi,Integra Technologies,12 - 15 years,55-60 Lacs P.A.,"['Ahmedabad', 'Chennai', 'Bengaluru']","Dear Candidate,\nWe are seeking a Cybersecurity Analyst to detect, investigate, and prevent security threats across digital assets and systems.\n\nKey Responsibilities:\nMonitor and analyze security alerts, logs, and events.\nPerform threat intelligence, malware analysis, and incident response.\nConduct vulnerability assessments and patch management.\nSupport compliance and audit activities (ISO, NIST, GDPR).\nEducate staff on cybersecurity best practices and awareness.\nRequired Skills & Qualifications:\nExperience with SIEM tools (Splunk, AlienVault, QRadar).\nKnowledge of firewalls, IDS/IPS, endpoint protection, and antivirus.\nFamiliarity with scripting for automation and reporting.\nStrong analytical, investigative, and communication skills.\nSecurity certifications preferred (e.g., CompTIA Security+, SOC Analyst, CISSP).\nSoft Skills:\nStrong troubleshooting and problem-solving skills.\nAbility to work independently and in a team.\nExcellent communication and documentation skills.\nNote: If interested, please share your updated resume and preferred time for a discussion. If shortlisted, our HR team will contact you.\n\nSrinivasa Reddy Kandi\nDelivery Manager\nIntegra Technologies",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Cyber Security', 'Threat Analysis', 'Incident Response', 'Malware Analysis', 'Encase', 'Chfi', 'Forensic', 'Forensic Investigation', 'Cyber Forensics', 'Nuix', 'Forensic Investigations', 'Brainspace', 'Digital Forensics', 'Computer Forensics', 'E-discovery', 'Incident Handling', 'Ftk']",2025-06-12 14:52:54
Business Analyst,TechStar Group,7 - 12 years,15-25 Lacs P.A.,"['Bengaluru', 'Mumbai (All Areas)']","Business Analyst - 7 to 10years' experience in Capital markets or global treasury.\nKnowledge of capital markets & financial instruments Equities, Derivatives, Fixed Income, FX.\nUnderstanding of Investment Banking and Asset Management  Front, Middle & Back Office functions.\nAsset management experience of Trading Compliance rule coding & testing is strongly preferred, but other experience within Asset Management industry is acceptable Financial industry knowledge within investments and distribution highly desired (i.e., understanding of investment process, capital markets, fixed income, equities Sound knowledge on Database SQL querying.\nShould be a strong team player Excellent communication skills - written & verbal.\nShould be able to create good documentation and correspond on functionalities & issues concisely.\nShould be able to articulate well during discussions.\nShould be able to work with tight deadlines Confident of interacting with business users and various stakeholders.\nResponsibility: Should be able to work with tight deadlines • Confident of interacting with business users and various stakeholders. • Skilled at using MS Excel, Word, PowerPoint & Visio.",Industry Type: IT Services & Consulting,"Department: BFSI, Investments & Trading","Employment Type: Full Time, Permanent","['Capital Market', 'Investment Banking', 'Derivatives', 'Global Treasury', 'FX', 'Equity', 'Derivative Market', 'Fixed Income']",2025-06-12 14:52:56
MDM Business Analyst,Gallagher Service Center (GSC),3 - 5 years,Not Disclosed,['Bengaluru'],"Role & responsibilities :\nThis role involves understanding and analyzing business requirements, designing and implementing MDM solutions, and ensuring the successful integration and management of master data across various systems and platforms. The MDM Business Analyst works closely with business stakeholders, IT teams, and data governance teams to align MDM initiatives with overall business objectives.\n\nKey responsibilities include conducting thorough analysis of existing data management processes and systems to identify areas for improvement and optimization. The MDM Business Analyst is also responsible for developing and documenting business requirements, creating data models, and defining data standards and policies. Additionally, they collaborate with various teams to ensure data integrity, accuracy, and consistency, and provide support for data governance and data quality initiatives\n\nRequired Skillsets:\nA Bachelor's or Master's degree in Computer Science, Information Systems, or a related field.\nProven experience, usually around 5 years, working as an MDM Domain Expert or in a similar role.\nIn-depth knowledge of MDM concepts, methodologies, and best practices.\nStrong understanding of data governance, data quality, and data integration principles.\nProficiency in MDM tools and technologies, such as Informatica MDM, CluedIn, or similar platforms.\nExcellent analytical and problem-solving skills to translate complex business requirements into practical MDM solutions.\nStrong communication and interpersonal skills to effectively collaborate with clients and internal teams.\nAbility to work independently and manage multiple projects simultaneously.\nCertification in the MDM domain (e.g., Certified MDM Professional) can be a plus.\n\nKey Responsibilities:\nDocument all existing rules in the MDM system\nDevelop the framework for stewards to create golden records\nWork directly with the business to identify the measures and areas of improvement for the data\n\n\nWe are seeking candidates who can join immediately or within a maximum of 30 days' notice\n3+ years of relevant experience is required\nCandidates who are willing to relocate to Bangalore or are already based in Bangalore\nCandidates should be flexible with working UK/US shifts.\n\nInterested candidates can share the profiles to the below mentioned email id's.\nmohammedjaveed_shaikbabu@ajg.com\nAbhishekKumar_Priyadarshi@ajg.com",Industry Type: Analytics / KPO / Research,Department: Other,"Employment Type: Full Time, Permanent","['Informatica Mdm', 'methodologies', 'MDM concepts', 'CluedIn', 'Informatica Master Data Management', 'MDM', 'Master Data Management']",2025-06-12 14:52:58
Business Analyst,TechStar Group,7 - 10 years,8-14 Lacs P.A.,"['Navi Mumbai', 'Bengaluru', 'Mumbai (All Areas)']","Job Description:\nBusiness Analyst - 7 to 10years' experience in Capital markets or global treasury.\nKnowledge of capital markets & financial instruments Equities, Derivatives, Fixed Income, FX.\nUnderstanding of Investment Banking and Asset Management Front, Middle & Back Office functions.\nAsset management experience of Trading Compliance rule coding & testing is strongly preferred, but other experience within Asset Management industry is acceptable Financial industry knowledge within investments and distribution highly desired (i.e., understanding of investment process, capital markets, fixed income, equities Sound knowledge on Database SQL querying.\nShould be a strong team player Excellent communication skills - written & verbal.\nShould be able to create good documentation and correspond on functionalities & issues concisely.\nShould be able to articulate well during discussions.\nShould be able to work with tight deadlines Confident of interacting with business users and various stakeholders.\n\nRole & responsibilities\n\nShould be able to work with tight deadlines • Confident of interacting with business users and various stakeholders.\nSkilled at using MS Excel, Word, PowerPoint & Visio.\n\n\n\nPreferred candidate profile",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Capital Market', 'Investment Banking', 'Asset Management', 'Derivatives', 'Fixed Income', 'Treasury', 'SQL']",2025-06-12 14:53:00
Sr. Associate Full Stack Software Engineer,Amgen Inc,1 - 3 years,Not Disclosed,['Hyderabad'],"What you will do\n\n\nIn this vital role you will be responsible for designing, developing, and maintaining software applications and solutions that meet business needs and ensuring the availability and performance of critical systems and applications. This role involves working closely with product managers, designers, data engineers, and other engineers to create high-quality, scalable software solutions and automating operations, monitoring system health, and responding to incidents to minimize downtime.\n\nYou will play a key role in a regulatory submission content automation initiative which will modernize and digitize the regulatory submission process, positioning Amgen as a leader in regulatory innovation. The initiative demonstrates innovative technologies, including Generative AI, Structured Content Management, and integrated data to automate the creation, and management of regulatory content.\n\n\n\nRoles & Responsibilities:\nPossesses strong rapid prototyping skills and can quickly translate concepts into working code\nContribute to both front-end and back-end development using cloud technology\nDevelop innovative solution using generative AI technologies\nEnsure code quality and consistency to standard methodologies\nCreate and maintain documentation on software architecture, design, deployment, disaster recovery, and operations\nIdentify and resolve technical challenges effectively\nStay updated with the latest trends and advancements\nWork closely with product team, business team, and other collaborators\nDesign, develop, and implement applications and modules, including custom reports, interfaces, and enhancements\nAnalyze and understand the functional and technical requirements of applications, solutions and systems and translate them into software architecture and design specifications\nDevelop and implement unit tests, integration tests, and other testing strategies to ensure the quality of the software\nIdentify and resolve software bugs and performance issues\nWork closely with multi-functional teams, including product management, design, and QA, to deliver high-quality software on time\nCustomize modules to meet specific business requirements\nWork on integrating with other systems and platforms to ensure seamless data flow and functionality\nProvide ongoing support and maintenance for applications, ensuring that they operate smoothly and efficiently\n\n\n\nWhat we expect of you\nWe are all different, yet we all use our unique contributions to serve patients.\nMasters degree and 1 to 3 years of experience in Computer Science, IT or related field OR\nBachelors degree and 3 to 5 years of experience in Computer Science, IT or related field OR\nDiploma and 7 to 9 years of experience in Computer Science, IT or related field\nPreferred Qualifications:\n\n\n\nFunctional\n\nSkills:\nMust-Have Skills:\nProficiency in Python/PySpark development, Fast API, PostgreSQL, Databricks, DevOps Tools, CI/CD, Data Ingestion.\nCandidates should be able to write clean, efficient, and maintainable code.\nKnowledge of HTML, CSS, and JavaScript, along with popular front-end frameworks like React or Angular, is required to build interactive and responsive web applications\nIn-depth knowledge of data engineering concepts, ETL processes, and data architecture principles. Solid understanding of cloud computing principles, particularly within the AWS ecosystem\nSolid understanding of software development methodologies, including Agile and Scrum\nExperience with version control systems like Git\nHands on experience with various cloud services, understand pros and cons of various cloud service in well architected cloud design principles\nStrong problem solving, analytical skills; Ability to learn quickly; Good communication and interpersonal skills\nExperienced with API integration, serverless, microservices architecture.\nExperience in SQL/NOSQL database, vector database for large language models\n\n\n\nGood-to-Have\n\nSkills:\nSolid understanding of cloud platforms (e.g., AWS, GCP, Azure) and containerization technologies (e.g., Docker, Kubernetes)\nExperience with monitoring and logging tools (e.g., Prometheus, Grafana, Splunk)\nExperience with data processing tools like Hadoop, Spark, or similar\n\n\n\nSoft\n\nSkills:\nExcellent analytical and solving skills\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation\nAbility to manage multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals\nStrong presentation and public speaking skills",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Python', 'PySpark development', 'Data Ingestion', 'PostgreSQL', 'Fast API', 'CI/CD', 'DevOps Tools', 'Databricks']",2025-06-12 14:53:03
Business Analyst with Custody and Settlements,Luxoft,8 - 13 years,Not Disclosed,['Pune'],"Creation of a Test pack, which would be relevant to Custody and Settlements\nExecution of the test cases\nDefect management\nResult of the executed Test cases [ Pass, Fail ]\nAny new requirements to be suggested, as identified GAP\nSkills\nMust have\nOverall, 8+ years of experience as a Business Analyst in the Settlement and Custody domains.\nExperience as a Business Analyst in the IT industry in the Finance domain.\nKnowledge of Capital Market activities, Financial Products, and Financial Terminologies is a must.\nUnderstanding of the Trade Life Cycle.\nThorough understanding of the complete Software Development Lifecycle.\nSelf-motivated, good interpersonal skills, and inclination to constantly upgrade on new technologies and frameworks.\nGood communication, good interpersonal skills and coordination activities, self-motivation, and inclination to constantly upgrade on new developments in the industry.\nAbility to understand business requirements easily and translate them into functional requirements.\nCapability to understand and analyze complex IT applications and financial product structures.\nExcellent documentation ability.\nHave experience in preparing requirement specifications and performing UAT.\nNice to have\nBasic technical understanding of Database, and development environment like Bitbucket, Git\nOther\nLanguages\nEnglish: C2 Proficient\nSeniority\nSenior\nRefer a Friend\nPositive work environments and stellar reputations attract and retain top talent. Find out why Luxoft stands apart from the rest.\nRecommend a friend\nRelated jobs View all vacancies\nSenior Business Analyst - Payments\nBusiness Analysis\nIndia\nBengaluru\nSenior Business Analyst - Payments\nBusiness Analysis\nIndia\nChennai\nBusiness analyst\nBusiness Analysis\nRomania\nBucharest\nPune, India\nReq. VR-114441\nBusiness Analysis\nBCM Industry\n21/05/2025\nReq. VR-114441\nApply for Business Analyst with Custody and Settlements in Pune\n*",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Interpersonal skills', 'Business Analyst', 'Business analysis', 'financial products', 'Finance', 'Software development life cycle', 'Senior Business Analyst', 'Capital market', 'Test cases', 'Defect management']",2025-06-12 14:53:05
Business Analyst with Custody and Settlements,Luxoft,8 - 13 years,Not Disclosed,['Bengaluru'],"Creation of a Test pack, which would be relevant to Custody and Settlements\nExecution of the test cases\nDefect management\nResult of the executed Test cases [ Pass, Fail ]\nAny new requirements to be suggested, as identified GAP\nSkills\nMust have\nOverall, 8+ years of experience as a Business Analyst in the Settlement and Custody domains.\nExperience as a Business Analyst in the IT industry in the Finance domain.\nKnowledge of Capital Market activities, Financial Products, and Financial Terminologies is a must.\nUnderstanding of the Trade Life Cycle.\nThorough understanding of the complete Software Development Lifecycle.\nSelf-motivated, good interpersonal skills, and inclination to constantly upgrade on new technologies and frameworks.\nGood communication, good interpersonal skills and coordination activities, self-motivation, and inclination to constantly upgrade on new developments in the industry.\nAbility to understand business requirements easily and translate them into functional requirements.\nCapability to understand and analyze complex IT applications and financial product structures.\nExcellent documentation ability.\nHave experience in preparing requirement specifications and performing UAT.\nNice to have\nBasic technical understanding of Database, and development environment like Bitbucket, Git\nOther\nLanguages\nEnglish: C2 Proficient\nSeniority\nSenior\nRefer a Friend\nPositive work environments and stellar reputations attract and retain top talent. Find out why Luxoft stands apart from the rest.\nRecommend a friend\nRelated jobs View all vacancies\nSenior Business Analyst - Payments\nBusiness Analysis\nIndia\nChennai\nBusiness analyst\nBusiness Analysis\nRomania\nBucharest\nBusiness Analyst_TT\nBusiness Analysis\nUnited States of America\nNew York City\nBengaluru, India\nReq. VR-114441\nBusiness Analysis\nBCM Industry\n21/05/2025\nReq. VR-114441\nApply for Business Analyst with Custody and Settlements in Bengaluru\n*",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Interpersonal skills', 'Business Analyst', 'Business analysis', 'financial products', 'Finance', 'Software development life cycle', 'Senior Business Analyst', 'Capital market', 'Test cases', 'Defect management']",2025-06-12 14:53:08
Quality Assurance - ETL QA Engineer - Lead,Kearney-Cervello,6 - 10 years,Not Disclosed,['Bengaluru'],"About the Role:\nAs a Lead ETL QA Engineer, you will drive the QA strategy and execution for a major data pipeline modernization initiative on Azure and Snowflake. This role requires a deep understanding of data quality frameworks, test planning, and stakeholder engagement. The candidate should possess leadership capabilities and be hands-on with SQL and automation.\n\nThe job responsibilities are as follows:",,,,"['Azure Data Factory', 'Quality Assurance', 'ETL Testing', 'SQL', 'Database Testing', 'Sql Database Testing']",2025-06-12 14:53:10
Business Analyst,Genisys Information Systems,10 - 14 years,Not Disclosed,['Bengaluru'],"Role & responsibilities :\nCollaborate with Business, finance, technical & Security team /stakeholders to capture and document business requirements.\nAnalyze existing Business & financial processes and propose improvements or automation opportunities with respect to ESOP Plan Management\nAct as the liaison between Business, finance and technical teams for NetSuite configuration and customization.",,,,"['NetSuite', 'Client Management', 'ESOP Plan Management', 'Business Analysis', 'Stakeholder Management', 'Requirement Gathering', 'Process Improvement', 'System Integration']",2025-06-12 14:53:12
Python/Pyspark developer,Zensar,4 - 5 years,Not Disclosed,"['Pune', 'Bengaluru']","Job Description:\nWe are seeking a highly skilled and motivated Python/PySpark Developer to join our growing team. In this role, you will be responsible for designing, developing, and maintaining high-performance data processing pipelines using Python and the PySpark framework. You will work closely with data engineers, data scientists, and other stakeholders to deliver impactful data-driven solutions.\nResponsibilities:\n- Design, develop, and implement scalable and efficient data pipelines using PySpark.\n- Write clean, well-documented, and maintainable Python code.\n- Optimize data processing performance and resource utilization.\n- Implement ETL (Extract, Transform, Load) processes to migrate and transform data across various systems.\n- Collaborate with data scientists and analysts to understand data requirements and translate them into technical solutions.\n- Troubleshoot and debug data processing issues.\n- Stay up-to-date with the latest advancements in big data technologies and best practices.\nQualifications:\n- Bachelor's degree in Computer Science, Engineering, or a related field.\n- 3+ years of experience in Python development.\n- 2+ years of experience with PySpark and Spark ecosystem.\n- Strong understanding of data structures, algorithms, and object-oriented programming.\n- Experience with SQL and relational databases.\n- Familiarity with cloud platforms such as AWS, Azure, or GCP (preferred).\n- Excellent problem-solving and analytical skills.\n- Strong communication and teamwork skills.\nBonus Points:\n- Experience with data visualization tools (e.g., Tableau, Power BI).\n- Knowledge of machine learning and data science concepts.\n- Experience with containerization technologies (e.g., Docker, Kubernetes).\n- Contributions to open-source projects.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'Cloud Technologies', 'SQL', 'Python']",2025-06-12 14:53:14
Learning Consultant India Learning Consultant,Zensar,5 - 10 years,Not Disclosed,"['Kolkata', 'Mumbai', 'New Delhi', 'Hyderabad', 'Pune', 'Chennai', 'Bengaluru']","Consultants are responsible for advising, guiding, scoping, developing strategic learning plans that align to channel partner business teams and their outcomes. Although this is a learning consulting role, having a strong ability to get buy-ins from senior business stakeholders and being able to showcase L&D is crucial to success. They establish and embed processes for learner journeys into partner organizations. Motivated, analytical, and strategic thinkers who also have excellent stakeholder management skills and the ability to develop and execute plans will succeed in this consultative role.\n  High-level skills and experience\nStrong consultative mindset and consulting skill set with a strong point-of-view on solutions.\nStrategic, big picture thinking and connecting the dots.\nAbility to understand the why along with the what and how of a partner need (ie, we'll-rounded learning plans that drive business outcomes rather than just chasing headcount/numbers)\nStrong commercial awareness/business acumen of the client s and partner s business.\nAbility to align learning with client and partner ROI and business drivers.\nThought leadership with new ideas, strategies, framework, processes.\nExecutive presence with senior stakeholders\nData analytics (ability to dig into trends and forecast) and data story telling.\nTechnical prowess to pick up data/AI/cloud/technical knowledge.\nExcellent oral/written communication and presentation skills for business reviews with senior stakeholders.\nHighly collaborative across various business functions/senior stakeholder management.\nProactive and independent, ability to excel in a fast-paced environment while working from vision through execution.\nExpected tasks and responsibilities.\nExecute frameworks to assess capability and capacity skill gaps.\nConduct partner needs analysis (discovery and scoping at business, technical and learning levels)\nEnabling consulting conversations with business leaders in partner teams to understand their business drivers.\nPropose strategic learning plans to all internal and external stakeholders.\nPrescribe relevant learning based on scope and workload (such as certifications, solutions, products, etc)\nBuild custom partner learning plans, where required, based on the results of scoping/needs analysis and business priority.\nMonitor learner progress and detect any issues.\nCreate and present Quarterly/Monthly Business Reviews with senior L&D/Business/Technology stakeholders\nMonitor and analyze learning stats for data-driven decisions.\nExplore business problems and create different solution models.\nMake recommendations for improvement and present to partners and internal stakeholders.\nDevelop and implement new procedures and/or training to support proposed changes.\nQualifications\n5+ years relevant experience being an L&D consultant or in any business facing consultant role driving outcomes.\nbachelors degree in Engineering and Computer Science, or a related field (required)\nMasters candidates (preferred)\nCertifications in Data Engineering, AI/ML, Cloud platforms (preferred).\nChannel partner/pre-sales/data & AI background encouraged.\nAbout the role\nChannel consultants are part of the Channel and Partner Success Consulting team and are passionate about helping our client strengthen their channel partner readiness program through technical and pre-sales capability and capacity assessment. They work closely with our client s channel partners to build partner teams and skills that deliver to business objectives and outcomes. The client is a global technology company.",Industry Type: IT Services & Consulting,Department: Strategic & Top Management,"Employment Type: Full Time, Permanent","['Training', 'Analytical', 'Consulting', 'Cloud', 'Manager Technology', 'Presales', 'Data analytics', 'Stakeholder management', 'Monitoring']",2025-06-12 14:53:16
Python Fullstack Developer,CGI,2 - 5 years,Not Disclosed,['Bengaluru'],"Role & responsibilities\nExcellent Knowledge & Understanding of Python, Pandas and Oracle, SQL.\nGood Knowledge & Understanding of data modelling.\nFlair for learning new tools & technology.\n2-5 years of experience in Banking IT, with a good understanding of the Corporate and Institutional\nBanking activity. Knowledge of Capital Markets an asset.\nGood knowledge and understanding of Windows Batch or PowerShell scripting.\nGood knowledge in systems, application frameworks, database optimization, and experience being\nresponsible for the success of software development projects.\nProven record interpreting and fulfilling requirements by developing high performing, scalable and\nmaintainable solutions with multiple technologies.\nHands-on experience with SDLC methodologies and best practices including Waterfall Process, Agile\nmethodologies, deployment automation, code reviews, and test-driven development.\nStrong coordination and organizational skills\nExcellent communication skills and multi-tasking capabilities.\nBeing aware of new technologies and frameworks.\nExperience and knowledge of Python language features like basic data types, functions with keywords\nargs, default args, variable length args.\nExperience in using Python database API with any relational database like Oracle/SQL Server using pure\nPython or native database driver.\nExperience in using built in collection types [for example: sequence types, dict, sets].\nExperience in using text manipulation facilities like regex, string methods from the standard library.\nExperience in using file and directory manipulation modules like paths, globs from the standard library.\nKnowledge of widely used libraries in data science like NumPy, Pandas is a must.\nVisualization is a good to have but not on the mandatory path.\nKnowledge and Experience in using collections from the collections module will be good to have though\nnot mandatory.\nFamiliarity with date manipulation using modules like datetime.\n•\nKnowledge of Generators will be good to have though not mandatory\n\nPreferred candidate profile",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Django', 'Pandas', 'Oracle', 'Python', 'SQL']",2025-06-12 14:53:19
Manager - BIM,Axtria,10 - 15 years,Not Disclosed,['Bengaluru'],"Position Summary \n\nLooking for a Salesforce Data Cloud Engineer to design, implement, and manage data integrations and solutions using Salesforce Data Cloud (formerly Salesforce CDP). This role is essential for building a unified, 360-degree view of the customer by integrating and harmonizing data across platforms.\n\n Job Responsibilities \n\nConsolidate the Customer data to create a Unified Customer profile\nDesign and implement data ingestion pipelines into Salesforce Data Cloud from internal and third-party systems .\nWork with stakeholders to define Customer 360 data model requirements, identity resolution rules, and calculated insights.\nConfigure and manage the Data Cloud environment, including data streams, data bundles, and harmonization.\nImplement identity resolution, micro segmentation, and activation strategies.\nCollaborate with Salesforce Marketing Cloud, to enable real-time personalization and journey orchestration.\nEnsure data governance, and platform security.\nMonitor data quality, ingestion jobs, and overall platform performance.\n\n\n Education \n\nBE/B.Tech\nMaster of Computer Application\n\n Work Experience \nOverall experience of minimum 10 years in Data Management and Data Engineering role, with a minimum experience of 3 years as Salesforce Data Cloud Data Engineer\nHands-on experience with Salesforce Data Cloud (CDP), including data ingestion, harmonization, and segmentation.\nProficient in working with large datasets, data modeling, and ETL/ELT processes.\nUnderstanding of Salesforce core clouds (Sales, Service, Marketing) and how they integrate with Data Cloud.\nExperience with Salesforce tools such as Marketing Cloud.\nStrong knowledge of SQL, JSON, Apache Iceberg and data transformation logic.\nFamiliarity with identity resolution and customer 360 data unification concepts.\nSalesforce certifications (e.g., Salesforce Data Cloud Accredited Professional, Salesforce Administrator, Platform App Builder).\nExperience with CDP platforms other than Salesforce (e.g., Segment, Adobe Experience Platform (Good to have)).\nExperience with cloud data storage and processing tools (Azure, Snowflake, etc.).\n\n\n Behavioural Competencies \n\nTeamwork & Leadership\nMotivation to Learn and Grow\nOwnership\nCultural Fit\nTalent Management\n\n Technical Competencies \n\nProblem Solving\nAzure Data Factory\nAzure DevOps\nAzure SQL",Industry Type: Analytics / KPO / Research,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sql', 'apache', 'data modeling', 'data transformation', 'etl', 'snowflake', 'navisworks', 'data management', 'bim', 'revit architecture', 'microsoft azure', 'revit mep', 'azure data factory', 'autocad', 'azure devops', 'salesforce', 'talent management', 'sql azure', 'revit', 'json', 'salesforce core']",2025-06-12 14:53:21
Technical Illustrator(Automotive),Cyient,4 - 9 years,Not Disclosed,"['Pune', 'Bengaluru']","Create and revise Parts Lists in the Parts Catalogs for automotive, agriculture and construction equipment.\nCreating exploded view artworks associated with parts catalogs as per given standards.\nProduce complete, clear and accurate parts catalogs content as per customer standards and guidelines.\nProcessing of information and data (engineering documents, changes, etc.) from\nEngineering, Manufacturing, Parts Marketing, Customer Service, Parts Warehouse and Suppliers.",,,,"['Iso Draw', 'Technical Illustration']",2025-06-12 14:53:23
Machine Learning Engineer - Python/Tensorflow,Vayuz Technologies,4 - 5 years,Not Disclosed,['Bengaluru'],"Key Responsibilities :\n- Conduct feature engineering, data analysis, and data exploration to extract valuable insights.\n- Develop and optimize Machine Learning models to achieve high accuracy and performance.\n- Design and implement Deep Learning models, including Artificial Neural Networks (ANN), Convolutional Neural Networks (CNN), and Reinforcement Learning techniques.\n- Handle real-time imbalanced datasets and apply appropriate techniques to improve model fairness and robustness.\n- Deploy models in production environments and ensure continuous monitoring, improvement, and updates based on feedback.\n- Collaborate with cross-functional teams to align ML solutions with business goals.\n- Utilize fundamental statistical knowledge and mathematical principles to ensure the reliability of models.\n- Bring in the latest advancements in ML and AI to drive innovation.\n\nRequirements :\n- 4-5 years of hands-on experience in Machine Learning and Deep Learning.\n- Strong expertise in feature engineering, data exploration, and data preprocessing.\n- Experience with imbalanced datasets and techniques to improve model generalization.\n- Proficiency in Python, TensorFlow, Scikit-learn, and other ML frameworks.\n- Strong mathematical and statistical knowledge with problem-solving skills.\n- Ability to optimize models for high accuracy and performance in real-world scenarios.\n\nPreferred Qualifications :\n- Experience with Big Data technologies (Hadoop, Spark, etc.)\n- Familiarity with containerization and orchestration tools (Docker, Kubernetes).\n- Experience in automating ML pipelines with MLOps practices.\n- Experience in model deployment using cloud platforms (AWS, GCP, Azure) or MLOps tools.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Machine Learning', 'Tensorflow', 'Azure', 'MLOps', 'GCP', 'Big Data', 'Neural Networks', 'AWS', 'Scikit-Learn', 'Deep Learning', 'Python']",2025-06-12 14:53:25
Sr ETL/SSIS developer,Sagility India,6 - 11 years,Not Disclosed,['Bengaluru'],"Job Summary\nWe are seeking a highly skilled and self-driven SSIS with strong communication and client-facing skills to join our healthcare analytics team. This role requires a combination of deep technical expertise in SSIS and data integration along with the ability to consult and collaborate directly with clients to understand and address their data needs.\nThe ideal candidate will be experienced in building and maintaining scalable data pipelines, working with diverse healthcare data sources, and ensuring data quality and availability for downstream analytics. You will play a key role in delivering clean, trusted, and timely data for insights and reporting.\nKey Responsibilities\nDesign, develop, and maintain robust and scalable SSIS to support healthcare analytics and reporting platforms.\nEngage directly with clients to gather requirements, provide consultation, and translate business needs into technical solutions.\nIntegrate and normalize data from diverse healthcare data sources, including claims, EMR, lab, pharmacy, and eligibility systems.\nEnsure data accuracy, completeness, and consistency throughout ingestion and transformation processes.\nOptimize and tune data workflows for performance and scalability in a cloud or on-premise data platform.\nTroubleshoot and resolve data issues in a timely and proactive manner to support high data availability.\nCollaborate with analysts, data scientists, and business stakeholders to ensure data pipelines meet analytical needs.\nCreate and maintain comprehensive technical documentation for data pipelines, data dictionaries, and workflows.\nStay informed on healthcare compliance requirements (e.g., HIPAA), and ensure data handling practices follow regulatory standards.\nRequired Skills and Qualifications\n6+ years of experience in SSIS development and data engineering\nProven ability to interact directly with clients and translate business problems into data solutions\nStrong experience with SQL, SSIS, or PySpark for data processing\nDeep understanding of data warehousing concepts and dimensional modeling\nExperience working with healthcare datasets (e.g., claims, eligibility, clinical data)\nFamiliarity with cloud platforms (Azure, AWS, or GCP) and data lakes\nStrong troubleshooting, problem-solving, and performance tuning skills\nExcellent verbal and written communication skills\nBachelor's or Masters degree in Computer Science, Engineering, Information Systems, or a related field\nPreferred Qualifications\nProficiency in building data pipelines using tools such as Azure Data Factory, Informatica, Databricks, or equivalent\nExperience with FHIR, HL7, or other healthcare data standards\nFamiliarity with HIPAA and healthcare compliance requirements\nKnowledge of reporting tools like Power BI or Tableau\nExposure to CI/CD and data pipeline automation\nWhy Join Us?\nWork on high-impact healthcare projects with meaningful outcomes\nEngage directly with clients and make a tangible difference in their data strategy\nCollaborative team culture and continuous learning opportunities\nFlexible work arrangements and competitive compensation\n\nLocation - Bangalore\nShit Timing - 2 Pm to 11 PM\nWork - Hybrid\n\nRegards,\nnaveen.vediyappan@sagility.com",Industry Type: BPM / BPO,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SSIS', 'SQL', 'ETL']",2025-06-12 14:53:28
Senior Etl Informatica Developer,VBeyond,6 - 8 years,19-25 Lacs P.A.,"['Noida', 'Chennai', 'Bengaluru']","We are seeking a highly skilled and experienced Senior ETL & Reporting QA Analyst to join our dynamic team. The ideal candidate will bring strong expertise in ETL and Report Testing, with a solid command of SQL, and hands-on experience in Informatica, as well as BI Reporting tools. A strong understanding of the Insurance domain is crucial to this role. This position will be instrumental in ensuring the accuracy, reliability, and performance of our data pipelines and reporting solutions.\n\nKey Responsibilities:\nDesign, develop, and execute detailed test plans and test cases for ETL processes, data migration, and data warehousing solutions.\nPerform data validation and data reconciliation using complex SQL queries across various source and target systems.\nValidate Informatica ETL workflows and mappings to ensure accurate data transformation and loading.\nConduct end-to-end report testing and dashboard validations using Cognos (preferred), or comparable BI tools such as Tableau or Power BI.\nCollaborate with cross-functional teams including Business Analysts, Developers, and Data Engineers to understand business requirements and transform them into comprehensive test strategies.\nIdentify, log, and track defects to closure using test management tools and actively participate in defect triage meetings.\nMaintain and enhance test automation scripts and frameworks where applicable.\nEnsure data integrity, consistency, and compliance across reporting environments, particularly in the insurance domain context.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Informatica', 'ETL', 'Power Bi', 'Insurance Domain', 'Tableau', 'SQL']",2025-06-12 14:53:30
Sr Product Manager,Ennoventure Technologies,5 - 10 years,Not Disclosed,['Bengaluru'],"Job_Description"":""\nAt Ennoventure, we are redefining the fight against counterfeit goods with our groundbreaking technology. Backed by key investors like Fenice Investment Group and Tanglin Venture Partners, we are ready to embark on the next phase of our journey.\n\nOur aim? To build a world where authenticity reigns, ensuring every product and experience is genuine. Here, innovation moves fast, collaboration fuels success, and your growth isn\\u2019t just encouraged\\u2014it\\u2019s inevitable.\n\nAs a Lead Product Manager, you will drive exponential business growth by shaping product strategy, delivering exceptional user\nexperiences, and leading cross-functional collaboration. You will work closely with data science, engineering, design, and commercial teams to build scalable, cutting-edge products that deliver significant value to a global customer base.\n\n\nProduct Strategy & Vision\n- Define and evolve product strategy aligned with company goals and AI capabilities.\n- Translate AI advancements into differentiated product features with clear value for end-users.\n- Work with senior leadership to articulate product vision, business value, and a scalable roadmap.\n\nCustomer Discovery & Market Analysis\n- Conduct in-depth customer research to understand user pain points, workflows, and unmet needs.\n- Analyze competitive landscape, market trends, and regulatory considerations in AI SaaS.\n\nRoadmap Ownership\n- Define product requirements and maintain a clear, prioritized roadmap.\n- Lead the product lifecycle from ideation to delivery and post-launch iterations.\n\nCross-functional Leadership\n- Collaborate with engineering, design, and data science teams to build scalable, ethical, and user-centric AI products.\n- Partner with GTM teams (Sales, Marketing, Customer Success) to ensure successful product launches and feedback\nloops.\n\nExecution Excellence\n- Write detailed product specs, define OKRs, and drive sprint planning with agile teams.\n- Ensure timely delivery without compromising on quality or customer impact.\n\nAI-Product Interface\n- Work closely with machine learning engineers and data scientists to understand model capabilities and constraints.\n- Translate AI research and experiments into real-world applications and intuitive user experiences.\n\nBe the P&L Owner\n- Demonstrate strong business judgment and data obsession.\n- Own long-term growth strategies and drive measurable impact on the product P&L.\n\nBe the Product Evangelist\n- Engage in customer discovery to unlock more value and read market evolution that ensure the product evolves to\nmeet new customer needs and market trends.\n- Be fearless and drive Product thought process across the organization.\n\n\nRequirements\n- 5+ years of product management experience, with at least 2 years in B2B SaaS.\n- Proven experience in delivering AI/ML-powered products (preferably in Computer Vision, predictive analytics, or\nintelligent automation).\n- Strong technical foundation \\u2013 able to collaborate effectively with engineering and data science teams.\n- Demonstrated ability to drive product vision, strategy, and roadmap in a fast-growing environment.\n- Has owned and delivered successful product outcomes from opportunity identification to launch\n- Strong product sense \\u2013 highly analytical, with the ability to identify the right problems, think big and long term, and\nmake data-informed decisions.\n- Well-rounded solutioning skills \\u2013 human-centered, business-focused, and technology-driven.\n- Builds lasting peer relationships and has the ability to motivate and inspire teams to perform at their best.\n- Excellent written and verbal communication skills, with the ability to influence stakeholders at all levels.\n- Strong user empathy and a passion for creating delightful user experiences with complex technology.\n- Familiarity with tools like JIRA, Figma, Product board, or similar.\n\nNice to Have\n- Experience with AI model lifecycle (training, evaluation, deployment, retraining).\n- Understanding of data privacy, security, and ethical AI frameworks.\n- Prior startup experience or having scaled AI products in early-stage environments.\n\n\n\nBenefits\nWe believe that our people are the driving force behind our success, fueling big ambitions with bigger impact. We\\u2019re building more than just a workplace, we\\u2019re crafting a space where everyone feels seen, heard, and unstoppable. Here, you don\\u2019t just thrive, you grow, innovate, and leave a mark that matters.\n\nThat\\u2019s why we\\u2019re committed to equipping you with the best: a Total Rewards Policy that integrates-\n\n- Pay: A Competitive Salary that reflects your talent and drive!\n- Financial Reward: Performance-based Rewards that recognize your impact.\n- Well-being: Comprehensive Health Insurance & Mental Health Programs to keep you at your best!\n- Learning: An ongoing investment in you and your skills.\n- Personalized Development: Self-growth plans crafted to match your performance and career aspirations.\n- Compensation Reviews: Regular reviews to ensure your value aligns with market trends.\n\n"",""",Industry Type: IT Services & Consulting,Department: Product Management,"Employment Type: Full Time, Permanent","['Product management', 'Computer vision', 'Market analysis', 'Automation', 'data science', 'Analytical', 'Machine learning', 'Agile', 'Engineering Design', 'Product strategy']",2025-06-12 14:53:33
"Principal Analyst, Insights",Zeta Global,3 - 4 years,Not Disclosed,['Bengaluru'],"Description: Principal Data Analyst will be responsible for analyzing complex datasets, identifying opportunities for process improvements, and implementing automation solutions to streamline workflows. This role requires a deep understanding of data analytics, process automation tools, and excellent problem-solving skills. The ideal candidate will be proactive, detail-oriented, and able to work collaboratively with cross-functional teams to drive data-driven initiatives.\nWhat you ll do:\nAnalyze large and complex datasets to identify trends, patterns, and insights that drive business decisions.\nDevelop, implement, and maintain automated processes to improve data accuracy, efficiency,and reporting capabilities.\nCollaborate with stakeholders to understand business requirements and translate them into technical solutions.\nDesign and build automated dashboards and reports to provide real-time insights to various departments.\nUtilize data visualization tools to present findings in a clear and actionable manner.\nContinuously monitor and refine automated processes to ensure optimal performance and scalability.\nStay updated with industry trends and best practices in data analytics and process automation.\nMentor and provide guidance to junior data analysts on best practices and technical skills.\nWho you are:\nA great communicator who can convey complex technical features in simple terms.\nAble to multitask and prioritize among several high-profile clients.\nHave a high degree of creativity, self-motivation, and drive.\nEagerness to work in a startup team environment that will be rapidly changing.\nEnthusiastic team player with a penchant for collaboration and knowledge sharing.\nWillingness to do whatever it takes to get the job done.\nNerdy but loveable.\nData driven, technical, self-starting and curious.\nWhat you need:\nBachelor s or Master s degree in data science, Computer Science, Statistics, or a related field.\nMinimum of 3-4 years of experience in data analysis, with a focus on process automation.\nA minimum of 2 years of work experience in analytics (minimum of 1 year with a Ph.D.)\nExperience with data querying languages (e.g. SQL), scripting languages (e.g. Python), and/or statistical/mathematical software (e.g. R) Experience with combining and consolidating disparate datasets in apps such as Big Query, Data Bricks\nProficiency in programming languages such as Python, R, or SQL.\nExtensive experience with data visualization tools such as Tableau, Power BI or similar.\nStrong knowledge of process automation tools and platforms (e.g., Alteryx, UiPath, Microsoft Power Automate).\nExperience with database management systems (e.g., SQL Server, MySQL, PostgreSQL).\nExcellent analytical and problem-solving skills.\nAbility to work effectively in a fast-paced, collaborative environment.\nStrong communication skills, with the ability to convey complex data insights to non-technical stakeholders.\nExperience with machine learning and predictive analytics is a plus.\nBonus if you have:\nMaster s or Ph.D. Degree in a quantitative field.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['SQL', 'UiPath', 'R', 'Power BI', 'PostgreSQL', 'MySQL', 'Alteryx', 'Tableau', 'Python']",2025-06-12 14:53:35
IT Analyst(Finance)-5+yrs-Bangalore/Pune/Hyderabad-Hybrid,Databuzz ltd,5 - 10 years,Not Disclosed,"['Hyderabad', 'Pune', 'Bengaluru']","Databuzz is Hiring for IT Analyst(Finance)-5+yrs-Bangalore/Pune/Hyderabad-Hybrid\n\nPlease mail your profile to haritha.jaddu@databuzzltd.com with the below details, If\nyou are Interested.\n\nAbout DatabuzzLTD:\nDatabuzz is One stop shop for data analytics specialized in Data Science, Big Data, Data Engineering, AI & ML, Cloud Infrastructure and Devops. We are an MNC based in both UK and INDIA. We are a ISO 27001 & GDPR complaint company.\n\nCTC -\nECTC -\nNotice Period/LWD - (Candidate serving notice period will be preferred)\n\nPosition: IT Analyst(Finance)\nLocation: Bangalore/Pune/Hyderabad\nExp -5+ yrs\n\nMandatory skills :\nCandidate should have Strong knowledge on Functionalities in AR,GL,CE,AP (R12)\nGood understanding on Order to Cash Flow\nExperienced in implementation/ Configuration/ Customization / Support\nExperienced in localizations\nExperience in implementing integrations with external systems\nGood documentation skills and presentation skills to prepare MD050 / PPT and do the solution reviews with various teams\nAble to write SQLs on tables and pull the required data\nUnderstanding of Fusion finance.\n\n\nRegards,\nHaritha\nTalent Acquisition specialist\nharitha.jaddu@databuzzltd.com",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['AR', 'Ce', 'R12', 'GL', 'Ap']",2025-06-12 14:53:37
Senior DevSecOps (Bangalore) - 8+ Years - Hybrid,Databuzz Ltd,8 - 10 years,5-15 Lacs P.A.,['Bengaluru'],"Databuzz is Hiring for Senior DevSecOps Engineer Dynamics 365 (Bangalore) - 8+ Years - Hybrid\n\nPlease mail your profile to jagadish.raju@databuzzltd.com with the below details, If you are Interested.\n\nAbout DatabuzzLTD: Databuzz is One stop shop for data analytics specialized in Data Science, Big Data, Data Engineering, AI & ML, Cloud Infrastructure and Devops. We are an MNC based in both UK and INDIA. We are a ISO 27001 & GDPR complaint company.\n\nCTC -\nECTC -\nNotice Period/LWD - (Candidate serving notice period will be preferred)\nDOB-\n\nPosition: Senior DevSecOps Engineer Dynamics 365 (Bangalore) - 8+ Years - Hybrid\n\nMandatory Skills:\n\nShould have experience in Azure DevOps\nShould have experience Dynamics - 365\nStrong experience with Python and PowerShell for scripting and automation tasks\nExperienced in working with Kubernetes, Terraform,\nGood to have Service Now, YAML\n\nRegards,\nJagadish Raju - Talent Acquisition Specialist\njagadish.raju@databuzzltd.com",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Dynamics 365', 'Powershell', 'Azure Devops', 'Python', 'Terraform', 'Docker', 'Servicenow', 'Yaml', 'Kubernetes']",2025-06-12 14:53:40
SR. Databricks Developer,Labcorp,7 - 12 years,Not Disclosed,['Bengaluru'],"Labcorp is hiring a Senior Data engineer.  This person will be an integrated member of Labcorp Data and Analytics team and work within the IT team.   Play a crucial role in designing, developing and maintaining data solutions using Databricks, Fabric, Spark, PySpark and Python.  Responsible to review business requests and translate them into technical solution and technical specification.  In addition, work with team members to mentor fellow developers to grow their knowledge and expertise.  Work in a fast paced and high-volume processing environment, where quality and attention to detail are vital.\n\nRESPONSIBILITIES:\nDesign and implement end-to-end data engineering solutions by leveraging the full suite of Databricks, Fabric tools, including data ingestion, transformation, and modeling.\nDesign, develop and maintain end-to-end data pipelines by using spark, ensuring scalability, reliability, and cost optimized solutions.\nConduct performance tuning and troubleshooting to identify and resolve any issues.\nImplement data governance and security best practices, including role-based access control, encryption, and auditing.\nWork in fast-paced environment and perform effectively in an agile development environment.\n\nREQUIREMENTS:\n8+ years of experience in designing and implementing data solutions with at least 4+ years of experience in data engineering.\nExtensive experience with Databricks, Fabric, including a deep understanding of its architecture, data modeling, and real-time analytics.\nMinimum 6+ years of experience in Spark, PySpark and Python.\nMust have strong experience in SQL, Spark SQL, data modeling & RDBMS concepts.\nStrong knowledge of Data Fabric services, particularly Data engineering, Data warehouse, Data factory, and Real- time intelligence.\nStrong problem-solving skills, with ability to perform multi-tasking.\nFamiliarity with security best practices in cloud environments, Active Directory, encryption, and data privacy compliance.\nCommunicate effectively in both oral and written.\nExperience in AGILE development, SCRUM and Application Lifecycle Management (ALM).\nPreference given to current or former Labcorp employees.\n\nEDUCATION:\nBachelors in engineering, MCA.",Industry Type: Medical Services / Hospital (Diagnostics),Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Pyspark', 'Data Bricks', 'Python', 'Parquet', 'UDP', 'Shell Scripting', 'Microsoft SQL Server', 'DW BI project', 'Kafka', 'Mapreduce', 'EMR', 'Redshift', 'Hive', 'MySQL', 'Spark', 'Aws Databricks', 'Oracle', 'Redshift spectrum', 'Fabric', 'Lambda', 'Athena']",2025-06-12 14:53:42
Business Analyst - Alternative Investments,Vichara Technologies,6 - 10 years,25-35 Lacs P.A.,"['Bengaluru', 'Delhi / NCR', 'Mumbai (All Areas)']","Create documentation and user stories.\nWork with engineering teams to review upcoming and backlog Jira tickets.\nProvide guidance on design decisions in areas including Credit and tech including Snowflake and Streamlit\nDevelop reporting in powerBI\n\nRequired Candidate profile\n5+ years of experience as a Business analyst especially in Alternative assets, Credit, CLO, Real Estate etc.\nExperience creating complex dashboards in powerBI\nExposure to Snowflake and Streamlit",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Investment Banking', 'Power Bi', 'User Stories', 'Private Equity', 'Data Warehousing', 'streamlit', 'Alternative Investments', 'JIRA', 'Investment Management', 'Analytics', 'Business Analysis', 'Loan Syndication', 'SQL', 'private credit', 'Real Estate', 'Snowflake', 'Private Debt', 'Dashboards', 'Argus', 'Python']",2025-06-12 14:53:45
Artificial Intelligence Architect,Emerson,10 - 20 years,Not Disclosed,['Pune'],"Role & responsibilities\nDesign robust and scalable AI/ML architectures that support the development and deployment of machine learning models and AI solutions.\nDevelop and guide the implementation of end-to-end AI/ML solutions, including model development, data processing, and system integration.\nEvaluate and recommend the latest AI/ML technologies, frameworks, and tools to enhance system capabilities and performance.\nCollaborate with software engineers and other development teams to integrate AI/ML solutions into existing systems and applications. Ensure seamless operation and performance.\nWork with cross-functional teams, including developers, data scientists, machine learning engineers, and business stakeholders, to understand requirements and design solutions that align with business objectives.\n\nPreferred candidate profile\nBachelors degree in computer science, Data Science, Statistics, or a related field or a master's degree or higher is preferred.\nMore than 3 years of experience in designing and implementing AI/ML architectures, with a proven track record of successful projects.\nExtensive experience with machine learning frameworks (e.g., Go, TensorFlow, PyTorch), programming languages C#, .Net, NodeJS and data processing tools.\nStrong understanding of system architecture principles, including distributed systems, microservices, and cloud computing.\nExperience with Microsoft Azure cloud services and their AI/ML offerings\nExperience with event-handling systems such as Kafka\nExperience with big data technologies and data engineering practices.\nExcellent verbal and written communication skills, with the ability to convey complex technical concepts to non-technical stakeholders.",Industry Type: Industrial Automation,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Aiml', 'Ml', 'Python', 'Tensorflow', 'Pytorch', 'Architecture', 'Artificial Intelligence', '.Net', 'Machine Learning', 'Scikit-Learn']",2025-06-12 14:53:47
Media AdTech Specialist,Capgemini,5 - 10 years,Not Disclosed,['Kolkata'],"Provide ad operations and/or AdTech operations expertise\nExecute and help implement an AdTech compliance program\nStrong understanding of Programmatic Ad Eco system and Retail Media bidding advertisement.\nYouTube, Connected TV and Video Ads advertisement and hands on ad set up experience.\nDesign and implement advertising solutions tailored for retail media needs.\nBuild operational systems that enhances the productivity of our Ad Operations team\nCollaborate with other departments and stakeholders to identify and solve complex problems\nContinuously testing and improving software solutions to ensure optimal performance and user experience\nCreate and manage strong relationships with DSPs and other relevant players in the ad tech space that can help our clients achieve their objectives.\nSpearhead initiatives to refine and expand digital media services.\nCollaborate with a diverse team of experts to drive innovation. (data scientists, developers, engineers, clients and stakeholders).\nEnsure seamless integration and service delivery.\nApply the latest industry trends and best practices to achieve our clients outcomes\nStay abreast of media regulations and trends affecting digital advertising.\nBuild highly performant AdTech platforms that will support our future growth in the Ads Space\n\nQualifications:\n5 years experience in advertising operations (AdTech ops) and/or revenue operations (Revops).\nStrong understanding of DSPs, digital advertising ecosystems, ad networks, and/or advertising exchanges.\nDemonstrated excellence in client relationship management.\nDemonstrated ability to build and work across teams.\nExperience in Technical Solutions Architecture and design leadership.\nManage multiple projects and prioritize tasks effectively\nBroad knowledge across multiple technology areas Marketing Operation, Ecommerce Domain and Retail Media.\nStrong organizational skills and attention to detail.\nAbility to work independently and as part of a team.\n\nTools:\nFacebook Ads Manager, Pinterest Ad Manager, Instagram ads, DV360, Programmatic, Campaign Manager 360, Google Ad Manager, TTD\nPower-Bi, Excel, PowerPoint will be a plus point.\nYouTube, Videos, CTV related ad platforms.",Industry Type: IT Services & Consulting,Department: Marketing & Communication,"Employment Type: Full Time, Permanent","['DSP', 'Programmatic Buying', 'DV360', 'Facebook Ads Manager', 'Bidding', 'Display Video', 'Google Ads', 'Media Buying', 'Atl', 'Media Planning', 'Pinterest', 'Campaign Management', 'Btl']",2025-06-12 14:53:50
Analytics & Visualization Developer,Qualcomm,7 - 10 years,Not Disclosed,['Hyderabad'],"Job Area: Information Technology Group, Information Technology Group > IT Programmer Analyst\n\nGeneral Summary:\n\nQualcomms Engineering IT EDAAP team is looking for an independent contributor experienced in development and sustaining enterprise level software applications. Experience:7-10 years of experience developing dashboard with reporting tools- Tableau (Tableau API), Power BI, OBIEE. SkillsMust:\nExpert in developing visualizations/dashboards with Tableau\nStrong knowledge with SQL\nFundamentals in object-oriented design, data structures, algorithms and problem solving.\nTest, debug and performance tuning of dashboards/reports\nExperience working in an agile development environment.\nTranslate ad hoc report requests into common dashboards and application requirements\nKnowledge of different types of enterprise systems, their interaction, boundaries within an enterprise.\nUnderstanding of complex data models.\nExperience working with Oracle/MySQl/Postgres\nWILLINGNESS to multi task and work in a fast paced environment.\nMust be willing to take ownership and drive tasks to completion. Desirable:\nPython programming experience.\nExperience developing dashboards with Power BI.\nExposure to Qlikview, OBIEE and ThoughtSpot\nExperience with semiconductor industry.\nExperience working with NoSQL Databases (MongoDB) as well as relational DBs (MySQL/Oracle) Education\nBachelor's degree in technical discipline or equivalent experience required.\n\nQualifications\n5 or more years of experience in applying AI and machine learning techniques to practical and comprehensive technology solutions.\nA strong background in machine learning, deep learning, and natural language processing.\nExpertise in ML, deep learning, Py Torch, Python, NLP and Transformer architecture.\nExperience in deploying LLMs, embedding model/sentence transformers in production use cases.\nThorough knowledge in basic algorithms, object-oriented and functional design principles, and best-practice patterns\nStrong expertise in programming (Rust/Python)\nExperience in fine-tuning a large language model using custom content (documents, data, code).\nExperience in developing Generative AI applications, Agentic Systems and Retrieval Augmented Generation.\nExperience working with large-scale datasets, preprocess them, and create appropriate data representations.\nSolid understanding of statistics, linear algebra, and probability theory.\n\nPreferred Qualifications\nBachelors/masters degree in computer science, Artificial Intelligence, Data Science, or a related field.\nExperience in implementing projects involving end to end ML/NLP systems from development to deployment.\nExperience with transformer-based models (e.g., BERT, GPT, T5, Llama).\nExperience working in a distributed team.\nExperience with cloud environments (GCP/AWS).\nWorking knowledge of Rust is a plus.\n\nMinimum Qualifications:\n4+ years of work experience in programming, scripting, and/or automation or IT-relevant work experience with a Bachelor's degree.\nOR\n6+ years of work experience in programming, scripting, and/or automation or IT-relevant work experience without a Bachelors degree.\n\n2+ years experience with Database Design structures such as Mongo DB, MySQL.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'natural language processing', 'machine learning', 'deep learning', 'pytorch', 'algorithms', 'functional design', 'dashboards', 'artificial intelligence', 'sql', 'database design', 'tableau', 'data visualization', 'design principles', 'linear algebra', 'ml', 'statistics']",2025-06-12 14:53:53
AI-Enabler Custom Developer,Roche Diagnostics,2 - 4 years,Not Disclosed,['Pune'],"At Roche you can show up as yourself, embraced for the unique qualities you bring. Our culture encourages personal expression, open dialogue, and genuine connections, where you are valued, accepted and respected for who you are, allowing you to thrive both personally and professionally. This is how we aim to prevent, stop and cure diseases and ensure everyone has access to healthcare today and for generations to come. Join Roche, where every voice matters.\nThe Position\nThroughout our 125-year history, Roche has grown into one of the world s largest biotech companies and a global supplier of transformative, innovative solutions across major disease areas.\n\nWe are now entering an exciting new chapter of our digital transformation journey by embracing the power of Artificial Intelligence. In line with our Roche Group AI Strategy and our 10-Year Ambition to Transform our business with data & digital solutions, we are developing AI capabilities across all levels of the organization from EverydayAI, which enhances individual productivity, to Reshape initiatives, which reimagine business processes, to Big Ideas, which push the boundaries of what s possible in healthcare.\nWe are looking for forward-thinking professionals to join Roche Informatics and help us bring this strategy to life.\n\nPune continues to play the role of a Technology Acceleration Hub, building capabilities that drive digital innovation, including cutting-edge AI solutions that support Roche s mission to prevent, stop, or cure diseases with the highest societal burden.\n\nOur Expectations\nWe are looking for a Software Engineer eager to develop and implement AI-powered solutions within Roches\ntechnology ecosystem. The ideal candidate should have a strong foundation in software development, a willingness to\nupskill in AI and Generative AI technologies, and the ability to integrate large language models (LLMs) into applications\nand software development processes (testing, refactoring, requirements management, deployment).\n\nKey Competencies & Skills\nAI Expertise\nUnderstanding of how LLMs work, their strengths, limitations, and practical applications.\nExperience in basic prompt engineering.\nFamiliarity with direct LLM API usage (e.g., OpenAI API, SDKs).\nConceptual understanding of RAG architecture.\nHands-on experience with libraries to create basic LLM workflows (e.g., LangChain, LlamaIndex) and\nvector databases (e.g., Qdrant) is a plus.\nBasic understanding of NLP concepts such as tokens and embeddings.\nSoftware Development & Cloud Engineering:\nStrong programming skills in at least one language (e.g., Python, Go, TypeScript, Java, Kotlin) with a\ngood understanding of tooling, ecosystem, and software development best practices.\nExperience with API usage and basic understanding of cloud-based AI deployments (AWS/Azure/GCP).\nFamiliarity with AI software development tools (e.g., Github Copilot).\n\nDevOps Practices:\nUnderstanding of CI/CD pipelines and automation testing.\nFamiliarity with GitHub and GitLab.\n\nCollaboration & Knowledge Sharing:\nGood communication skills in English (B2/C1 level) to work within cross-functional teams.\nAbility to collaborate with Engineers and potentially Data Scientists on AI-driven enhancements.\nKey Responsibilities\nDesign, develop, and optimize custom applications, potentially incorporating AI-powered features within Roche s\necosystem.\nIntegrate Large Language Models (LLMs) into custom applications, leveraging APIs and prompt engineering\ntechniques, under guidance.\nUtilize cloud-based AI services (AWS, Azure, or GCP) to build basic AI-driven functionalities.\nCollaborate with internal teams to apply AI for the enhancement of custom applications.\nContribute to identifying opportunities for AI to improve application functionality.\nParticipate in knowledge sharing activities within the team.\n\nExample Projects You May Work On\nIntegrating AI features into existing custom applications to improve user experience.\nDeveloping basic AI-powered tools to aid in application development.\nAssisting in the integration of AI into application workflows.\n\nWhat We Value\nGood analytical and problem-solving skills.\nAdaptability to learn about AI and work in Agile environments.\nCuriosity and a willingness to take ownership of tasks.\nWho we are\n.\n\nLet s build a healthier future, together.\nRoche is an Equal Opportunity Employer.\n""",Industry Type: Biotechnology,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Basic', 'github', 'Usage', 'GCP', 'Analytical', 'Artificial Intelligence', 'Agile', 'Healthcare', 'Application development', 'Python']",2025-06-12 14:53:55
AI-Enabler Custom Developer,Hoffmann La Roche,2 - 4 years,Not Disclosed,['Pune'],"At Roche you can show up as yourself, embraced for the unique qualities you bring. Our culture encourages personal expression, open dialogue, and genuine connections, where you are valued, accepted and respected for who you are, allowing you to thrive both personally and professionally. This is how we aim to prevent, stop and cure diseases and ensure everyone has access to healthcare today and for generations to come. Join Roche, where every voice matters.\nThe Position\nThroughout our 125-year history, Roche has grown into one of the world s largest biotech companies and a global supplier of transformative, innovative solutions across major disease areas.\n\nWe are now entering an exciting new chapter of our digital transformation journey by embracing the power of Artificial Intelligence. In line with our Roche Group AI Strategy and our 10-Year Ambition to Transform our business with data & digital solutions, we are developing AI capabilities across all levels of the organization from EverydayAI, which enhances individual productivity, to Reshape initiatives, which reimagine business processes, to Big Ideas, which push the boundaries of what s possible in healthcare.\nWe are looking for forward-thinking professionals to join Roche Informatics and help us bring this strategy to life.\n\nPune continues to play the role of a Technology Acceleration Hub, building capabilities that drive digital innovation, including cutting-edge AI solutions that support Roche s mission to prevent, stop, or cure diseases with the highest societal burden.\n\nOur Expectations\nWe are looking for a Software Engineer eager to develop and implement AI-powered solutions within Roches\ntechnology ecosystem. The ideal candidate should have a strong foundation in software development, a willingness to\nupskill in AI and Generative AI technologies, and the ability to integrate large language models (LLMs) into applications\nand software development processes (testing, refactoring, requirements management, deployment).\n\nKey Competencies & Skills\nAI Expertise\nUnderstanding of how LLMs work, their strengths, limitations, and practical applications.\nExperience in basic prompt engineering.\nFamiliarity with direct LLM API usage (e.g., OpenAI API, SDKs).\nConceptual understanding of RAG architecture.\nHands-on experience with libraries to create basic LLM workflows (e.g., LangChain, LlamaIndex) and\nvector databases (e.g., Qdrant) is a plus.\nBasic understanding of NLP concepts such as tokens and embeddings.\nSoftware Development & Cloud Engineering:\nStrong programming skills in at least one language (e.g., Python, Go, TypeScript, Java, Kotlin) with a\ngood understanding of tooling, ecosystem, and software development best practices.\nExperience with API usage and basic understanding of cloud-based AI deployments (AWS/Azure/GCP).\nFamiliarity with AI software development tools (e.g., Github Copilot).\n\nDevOps Practices:\nUnderstanding of CI/CD pipelines and automation testing.\nFamiliarity with GitHub and GitLab.\n\nCollaboration & Knowledge Sharing:\nGood communication skills in English (B2/C1 level) to work within cross-functional teams.\nAbility to collaborate with Engineers and potentially Data Scientists on AI-driven enhancements.\nKey Responsibilities\nDesign, develop, and optimize custom applications, potentially incorporating AI-powered features within Roche s\necosystem.\nIntegrate Large Language Models (LLMs) into custom applications, leveraging APIs and prompt engineering\ntechniques, under guidance.\nUtilize cloud-based AI services (AWS, Azure, or GCP) to build basic AI-driven functionalities.\nCollaborate with internal teams to apply AI for the enhancement of custom applications.\nContribute to identifying opportunities for AI to improve application functionality.\nParticipate in knowledge sharing activities within the team.\n\nExample Projects You May Work On\nIntegrating AI features into existing custom applications to improve user experience.\nDeveloping basic AI-powered tools to aid in application development.\nAssisting in the integration of AI into application workflows.\n\nWhat We Value\nGood analytical and problem-solving skills.\nAdaptability to learn about AI and work in Agile environments.\nCuriosity and a willingness to take ownership of tasks.\nWho we are\nA healthier future drives us to innovate. Together, more than 100 000 employees across the globe are dedicated to advance science, ensuring everyone has access to healthcare today and for generations to come. Our efforts result in more than 26 million people treated with our medicines and over 30 billion tests conducted using our Diagnostics products. We empower each other to explore new possibilities, foster creativity, and keep our ambitions high, so we can deliver life-changing healthcare solutions that make a global impact.\n\nLet s build a healthier future, together.\nRoche is an Equal Opportunity Employer.\n""",Industry Type: Biotechnology,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Basic', 'github', 'Usage', 'GCP', 'Analytical', 'Artificial Intelligence', 'Agile', 'Healthcare', 'Application development', 'Python']",2025-06-12 14:53:58
Consultant,Amdocs,4 - 9 years,Not Disclosed,['Pune'],"Amdocs helps those who build the future to make it amazing. With our market-leading portfolio of software products and services, we unlock our customers innovative potential, empowering them to provide next-generation communication and media experiences for both the individual end user and enterprise customers. Our employees around the globe are here to accelerate service providers migration to the cloud, enable them to differentiate in the 5G era, and digitalize and automate their operations. Listed on the NASDAQ Global Select Market, Amdocs had revenue of $5.00 billion in fiscal 2024. For more information, visit www.amdocs.com\n\n\nIn one sentence\n\nWe are seeking a Data Engineer with advanced expertise in Databricks SQL, PySpark, Spark SQL, and workflow orchestration using Airflow. The successful candidate will lead critical projects, including migrating SQL Server Stored Procedures to Databricks Notebooks, designing incremental data pipelines, and orchestrating workflows in Azure Databricks\n\n\nWhat will your job look like\n\nMigrate SQL Server Stored Procedures to Databricks Notebooks, leveraging PySpark and Spark SQL for complex transformations.\nDesign, build, and maintain incremental data load pipelines to handle dynamic updates from various sources, ensuring scalability and efficiency.\nDevelop robust data ingestion pipelines to load data into the Databricks Bronze layer from relational databases, APIs, and file systems.\nImplement incremental data transformation workflows to update silver and gold layer datasets in near real-time, adhering to Delta Lake best practices.\nIntegrate Airflow with Databricks to orchestrate end-to-end workflows, including dependency management, error handling, and scheduling.\nUnderstand business and technical requirements, translating them into scalable Databricks solutions.\nOptimize Spark jobs and queries for performance, scalability, and cost-efficiency in a distributed environment.\nImplement robust data quality checks, monitoring solutions, and governance frameworks within Databricks.\nCollaborate with team members on Databricks best practices, reusable solutions, and incremental loading strategies\n\n\nAll you need is...\n\nBachelor s degree in computer science, Information Systems, or a related discipline.\n4+ years of hands-on experience with Databricks, including expertise in Databricks SQL, PySpark, and Spark SQL.\nProven experience in incremental data loading techniques into Databricks, leveraging Delta Lake's features (e.g., time travel, MERGE INTO).\nStrong understanding of data warehousing concepts, including data partitioning, and indexing for efficient querying.\nProficiency in T-SQL and experience in migrating SQL Server Stored Procedures to Databricks.\nSolid knowledge of Azure Cloud Services, particularly Azure Databricks and Azure Data Lake Storage.\nExpertise in Airflow integration for workflow orchestration, including designing and managing DAGs.\nFamiliarity with version control systems (e.g., Git) and CI/CD pipelines for data engineering workflows.\nExcellent analytical and problem-solving skills with a focus on detail-oriented development.\n  Preferred Qualifications  \nAdvanced knowledge of Delta Lake optimizations, such as compaction, Z-ordering, and vacuuming.\nExperience with real-time streaming data pipelines using tools like Kafka or Azure Event Hubs.\nFamiliarity with advanced Airflow features, such as SLA monitoring and external task dependencies.\nCertifications such as Databricks Certified Associate Developer for Apache Spark or equivalent.\nExperience in Agile development methodologie\n\n\nWhy you will love this job:\nYou will be able to use your specific insights to lead business change on a large scale and drive transformation within our organization.\nYou will be a key member of a global, dynamic and highly collaborative team with various possibilities for personal and professional development.\nYou will have the opportunity to work in multinational environment for the global market leader in its field!\nWe offer a wide range of stellar benefits including health, dental, vision, and life insurance as well as paid time off, sick time, and parental leave!",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['azure databricks', 'airflow', 'pyspark', 'sql', 'spark', 'azure cloud services', 'continuous integration', 'azure data lake', 'workflow orchestration', 'ci/cd', 'warehouse', 't-sql', 'sql server', 'stored procedures', 'data bricks', 'git', 'kafka', 'data warehousing concepts', 'agile']",2025-06-12 14:54:01
Analyst,National Australia Bank (NAB),3 - 10 years,Not Disclosed,['Gurugram'],"NAB is looking for Analyst to join our dynamic team and embark on a rewarding career journey.\nCollect, interpret, and analyze data to help the organization make informed business decisions\nCreate reports, dashboards, and visual presentations to communicate insights clearly to stakeholders\nIdentify trends, patterns, and discrepancies in large datasets using statistical tools and software",,,,"['database maintenance', 'data analysis', 'performance tuning', 'database mirroring', 'data warehousing', 'business analysis', 'sql server dba', 'dashboards', 'database administration', 'research', 'sql server', 'sql', 'excel', 'data quality', 'compliance', 'data governance', 'log shipping']",2025-06-12 14:54:03
Machine Learning Engineer,Avani Infosoft,1 - 2 years,2.4-6.6 Lacs P.A.,['Bengaluru( Kamakshipalya )'],"Responsibilities:\n* Develop machine learning models using TensorFlow, NumPy & OpenCV.\n* Implement computer vision solutions with CNNs & object detection techniques.\n\n\nProvident fund",Industry Type: BPM / BPO,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Handling', 'Object Detection', 'Opencv', 'Deployment', 'Model Development', 'Tensorflow', 'Cnn', 'Computer Vision', 'Machine Learning', 'Numpy', 'Deep Learning']",2025-06-12 14:54:06
AI Associate Consultant - Platform Delivery,ZS,3 - 5 years,Not Disclosed,['Gurugram'],"Customer Success Associate Consultant design, develop and execute high-impact analytics solutions for large, complex, structured, and unstructured data sets (including big data) to drive impact on client business (topline). This person will lead the engagement for AI based SaaS product deployment to clients across industries. Leverage their strong Data Science, analytics and engineering skills to build Advanced analytics processes, build scalable and operational process pipelines and find data-driven insights that help our clients solve their most important business problems and bring optimizations. Associate Consultants also engage with Project Leadership team and clients to help them understand the insights, summaries, implications and make plans to act on them.",,,,"['Hospitality', 'Team management', 'Analytical', 'Management consulting', 'Financial planning', 'Healthcare', 'Predictive modeling', 'Stakeholder management', 'Analytics', 'SQL']",2025-06-12 14:54:09
Business Analyst,Amgen Inc,1 - 3 years,Not Disclosed,['Hyderabad'],"What you will do\nIn this vital role we are seeking a Business Systems Analyst with a good background in data and analytics to define and manage product requirements for AI-driven applications.\nPartner with Data Scientists, ML Engineers, and Product Managers to define business processes, product needs, and AI solution requirements.\nCapture and document epics, user stories, acceptance criteria, and data process flows for AI-powered analytics applications.\nWork closely with partners to define scope, priorities, and impact of new AI and data initiatives.\nEnsure non-functional requirements, such as data security, model interpretability, and system performance, are included in product backlogs.\nFacilitate the breakdown of Epics into Features and Sprint-Sized User Stories and lead backlog grooming sessions.\nEnsure alignment of technical requirements and UX for AI-based applications and interactive dashboards.\nCollaborate with engineers to define data ingestion, transformation, and model deployment processes.\nDevelop and implement product demonstrations showcasing AI-driven insights and analytics.\nMaintain detailed documentation of data pipelines, model lifecycle management, and system integrations.\nStay engaged throughout software development, providing proactive feedback to ensure business needs are met\nWhat we expect of you\nWe are all different, yet we all use our unique contributions to serve patients. This role bridges the gap between business needs and technical execution, ensuring the development of high-quality, scalable AI solutions. You will collaborate with data scientists, engineers, and product managers to shape product roadmaps, refine requirements, and drive alignment between business objectives and technical capabilities.\nBasic Qualifications:\nMasters degree and 1 to 3 years expereince in Computer Science, Data Science, Information Systems, or related field OR\nBachelors degree and 3 to 5 years of in Computer Science, Data Science, Information Systems, or related field OR\nDiploma and 7 to 9 years of in Computer Science, Data Science, Information Systems, or related field\nPreferred Qualifications:\nExperience defining requirements for AI/ML models, data pipelines, or analytics dashboards.\nFamiliarity with cloud platforms (AWS, Azure, GCP) for AI and data applications.\nUnderstanding of data security, governance, and compliance in AI solutions.\nAbility to communicate complex AI concepts and technical constraints to non-technical partners.\nKnowledge of MLOps, model monitoring, and CI/CD for AI applications.",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Business analysis', 'continuous integration', 'data science', 'gcp', 'ci/cd', 'microsoft azure', 'information systems', 'aws', 'artificial intelligence']",2025-06-12 14:54:13
AI Delivery Lead,Tata Technologies,10 - 15 years,Not Disclosed,['Pune'],"10+ years of experience in machine learning, AI, or data science, with at least 3 years in a leadership role.Proven track record of delivering ML/AI projects at scale in an enterprise environment.Deep functional expertise in AI/ML, coupled with a solid understan ding of data science solution developmentExperience in managing teams and stakeholder expectations.Strong communication skills and demonstrated experience in stakeholder managementTechnical SkillsStrong expertise in ML frameworks and tools (e.g., TensorFlow, P yTorch, Scikit-learn).Good experience in Gen AI (various LLMs and its framework)Proficiency in programming languages like Python, R, or Java.Familiarity with cloud platforms (AWS, Azure, GCP) for ML workflows.Solid understanding of MLOps principles, including CI/CD pipelines, model deployment, and monitoring.Knowledge of big data technologies (e.g., Spark, Hadoop) and data engineering bes t practices.Preferred Certifications AWS/Azure/GCP Certified ML",Industry Type: Building Material (Cement),Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['CI/CD pipelines', 'Java', 'Azure', 'R', 'GCP', 'Hadoop', 'Spark', 'AWS', 'Python']",2025-06-12 14:54:16
Specialist Business Analyst,Amgen Inc,4 - 6 years,Not Disclosed,['Hyderabad'],"What you will do\nIn this vital role you will be responsible for business process expertise to detail product requirements as epics and user stories, along with supporting artifacts like business process maps, use cases, and test plans for the software development teams.\nThis role involves working closely with Veeva - Site Collaboration and Veeva Vault Study Training business partners, Veeva engineers, data engineers, AI/ML engineers to ensure that the technical requirements for upcoming development are thoroughly elaborated. This enables the delivery team to estimate, plan, and commit to delivery with high confidence and identify test cases and scenarios to ensure the quality and performance of IT Systems.\nYou will analyze business requirements and design information systems solutions. You will collaborate with multi-functional teams to understand business needs, identify system enhancements, and drive system implementation projects. Your solid experience in business analysis, system design, and project management will enable you to deliver innovative and effective technology products. You will collaborate with Product owners and developers to maintain an efficient and consistent process, ensuring quality work from the team.\nRoles & Responsibilities:\nCollaborate with System Architects and Product Owners to manage business analysis activities for Veeva - Site Collaboration and Veeva Vault Study Training systems, ensuring alignment with engineering and product goals.\nCapture the voice of the customer to define business processes and product needs.\nCollaborate with Veeva - Site Collaboration and Veeva Vault Study Training business partners, Amgen Engineering teams and Veeva consultants to prioritize release scopes and refine the Product backlog .\nSupport the implementation and integrations of Veeva - Site Collaboration and Veeva Vault Study Training systems with other Amgen systems.\nEnsure non-functional requirements are included and prioritized in the Product and Release Backlogs.\nFacilitate the breakdown of Epics into Features and Sprint-Sized User Stories and participate in backlog reviews with the development team.\nClearly express features in User Stories/requirements so all team members and collaborators understand how they fit into the product backlog .\nEnsure Acceptance Criteria and Definition of Done are well-defined.\nStay focused on software development to ensure it meets requirements, providing proactive feedback to customers.\nDevelop and implement effective product demonstrations for internal and external partners .\nHelp develop and maintain a product roadmap that clearly outlines the planned features and enhancements, timelines, and achievements.\nIdentify and manage risks associated with the systems, requirement validation, and user acceptance.\nDevelop & maintain documentations of configurations, processes, changes, communication plans and training plans for end users.\nEnsure operational excellence, cybersecurity, and compliance.\nCollaborate with geographically dispersed teams, including those in the US and other international locations.\nFoster a culture of collaboration, innovation, and continuous improvement .\n\nBasic Qualifications:\nMasters degree with 4 - 6 years of experience in Computer Science/Information Systems experience with Agile Software Development methodologies OR\nBachelors degree with 6 - 8 years of experience in Computer Science/Information Systems experience with Agile Software Development methodologies OR\nDiploma with 10 - 12 years of experience in Computer Science/Information Systems experience with Agile Software Development methodologies\nPreferred Qualifications:\nExperience with Agile software development methodologies (Scrum).\nGood communication skills and the ability to collaborate with senior leadership with confidence and clarity.\nStrong knowledge of clinical trial processes especially Site Collaboration and Study Training process.\nFamiliarity with regulatory requirements for Clinical Trials (e.g. 21 CFR Part11, ICH ).\nHas experience with writing user requirements and acceptance criteria in agile project management systems such as JIRA.\n\n\nGood-to-Have Skills:\nFamiliarity with Veeva Clinical Platform, especially Veeva - Site Collaboration and Veeva Vault Study Training systems .\nExperience in managing product features for PI planning and developing product roadmaps and user journeys.\nExperience maintaining SaaS (software as a system) solutions and COTS (Commercial off the shelf) solutions.\nTechnical thought leadership.\nAble to communicate technical or complex subject matters in business terms.\nJira Align experience.\nExperience with AWS Services (like EC2, S3), Salesforce, Jira, and API gateway, etc.\nSAFe for Teams certification (preferred).\nCertifications in Veeva products (Preferred).\nCertified Business Analysis Professional (Preferred).\n\n\nSoft Skills:\nAble to work under minimal supervision .\nSkilled in providing oversight and mentoring team members. Demonstrated ability in effectively delegating work.\nExcellent analytical and gap/fit assessment skills.\nStrong verbal and written communication skills.\nAbility to work effectively with global, virtual teams .\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nTeam-oriented, with a focus on achieving team goals.\nStrong presentation and public speaking skills.",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Business Analysis', 'API gateway', 'Agile', 'AWS', 'Jira', 'Salesforce']",2025-06-12 14:54:18
"Sr. Product Manager, AI",Conga,3 - 5 years,Not Disclosed,['Bengaluru( Kadubeesanahalli )'],"Job Title: Sr. Product Manager\nLocation: Bangalore\nReports to: Manager, Product Management\n\nA quick snapshot\n\nAs a Product Manager on the Conga Discovery AI team, you will help define, build, and launch AI-driven metadata extraction solutions on top of the Conga platform. Youll bring your experience in AIand ideally Contract Lifecycle Management (CLM)to shape products that help enterprises discover, process, and analyze critical business data. This role is hands-on, requiring you to be comfortable demonstrating features, collaborating on implementations, and working closely with scrum teams to drive value for our customers.\n\nWhy it’s a big deal\n\nMetadata extraction is at the core of how businesses understand their documents and processes. By leveraging Discovery AI, you’ll help enterprises transform manual, time-consuming tasks into automated workflows that reduce errors, improve compliance, and deliver actionable insights. Your role will be central to creating and refining solutions that can scale to handle the most complex enterprise needs.\n\nAre you the person we’re looking for?\n\nRelevant Experience. You should have more than 5 years of experience in Product Management in B2B SaaS, preferably with data extraction, AI, or document automation products.\n\nDemonstrate. A success in conceptualizing and launching new product features from initial idea to market adoption.\n\nAI or machine learning. Knowledge of fundamentals and an interest in applying them to solve real-world business challenges.\n\nCLM. Exposure or understanding in CLM is a strong plus, as it ties closely into many metadata extraction use cases.\n\nResearch and Creativity. Conduct market and user research to identify new opportunities for AI-driven features.\n\nCustomer feedback. You will gather continuous customer feedback to iterate and prioritize feature development that delivers tangible customer value.\n\nMaintain strong partnerships. With professional services and support teams to address implementation details and customer escalations.\n\nDemo. You should confidently demo features to internal stakeholders, customers, and prospects to showcase Discovery AI capabilities.\n\nAnalyze and prioritize. You will use data analytics, user feedback, and market insights to guide product decisions and roadmap priorities.\n\nBalance. customer requirements, technical feasibility, and time-to-market considerations in a fast-paced environment.\n\nCustomer experience. You will regularly engage with customers to understand their needs and pain points, ensuring Discovery AI addresses real-world challenges.\n\nDocument. New workflows, provide training materials or guidelines, and gather post-launch feedback.\n\nEducation. Bachelor’s degree in Engineering or equivalent; a higher degree is a plus.\n\nHere’s what will give you an edge\n\nChampion the Customer. You appreciate that customers are the heart of the business, and you’re dedicated to delivering solutions that solve their problems while meeting them where they are.\n\nNatural collaborator. You thrive in an Agile, cross-functional setting, seeking input from engineers, designers, data scientists, and peers to make well-rounded product decisions.\n\nPassion. Your genuine enthusiasm for AI and data-driven solutions is evident in your work. You love delving into technical details, exploring new possibilities, and shaping products that redefine how companies operate.\n\nStrong Communication. You will communicate releases, risks, and timelines effectively to leadership and cross-functional stakeholders.\n\nCollaborate. With marketing to position and promote new features that drive adoption and user satisfaction.\n\nLeadership skills. You will work closely with scrum teams to ensure clear backlog priorities, smooth sprint planning, and timely delivery.",Industry Type: Software Product,Department: Product Management,"Employment Type: Full Time, Permanent","['Product Strategy', 'Product Management', 'Product Portfolio', 'Product Life Cycle Management', 'Product Planning']",2025-06-12 14:54:20
Senior Software Engineer-7916,WebMD,5 - 10 years,Not Disclosed,['Navi Mumbai'],"Position: Senior Software Engineer (Data Engineer)\nNo. of Positions: 1\nAbout WebMD:\nHeadquartered in El Segundo, Calif., Internet Brands is a fully integrated online media and software services\norganization focused on four high-value vertical categories: Health, Automotive, Legal, and Home/Travel. The\ncompanys award-winning consumer websites lead their categories and serve more than 250 million monthly\nvisitors, while a full range of web presence offerings has established deep, long-term relationships with SMB and\nenterprise clients. Internet Brands,powerful, proprietary operating platform provides the flexibility and\nscalability to fuel the companys continued growth. Internet Brands is a portfolio company of KKR and Temasek.\nWebMD Health Corp., an Internet Brands Company, is the leading provider of health information services, serving\npatients, physicians, health care professionals, employers, and health plans through our public and private online\nportals, mobile platforms, and health-focused publications. The WebMD Health Network includes WebMD\nHealth, Medscape, Jobson Healthcare Information, prIME Oncology, MediQuality, Frontline, QxMD, Vitals\nConsumer Services, MedicineNet, eMedicineHealth, RxList, OnHealth, Medscape Education, and other owned\nWebMD sites. WebMD, Medscape, CME Circle, Medpulse, eMedicine®, MedicineNet®, theheart.org®, and\nRxList® are among the trademarks of WebMD Health Corp. or its subsidiaries.\nFor Company details, visit our website: www.webmd.com / www.internetbrands.com\nAll qualified applicants will receive consideration for employment without regard to race, color, religion, sex,\nsexual orientation, gender identity, national origin, disability, or veteran status\nEducation: B.E. Computer Science/IT degree (or any other engineering discipline)\nExperience: 5+ years\nWork timings: 2:00 PM to 11:00 PM IST.\nDescription:\nWe are seeking an experienced and passionate Senior Software Developer to join our team. In this role,\nyou will work closely with cross-functional teams, developers, stakeholders, and business units to\ngather and analyze business requirements, design, build and implement ETL Solutions, and maintain\nthe infrastructure. The ideal candidate will have a strong background in business analysis, SQL, Unix,\nPython, ETL Tools to ensure the successful execution of projects.\nResponsibilities:\nLead requirements gathering sessions with key stakeholders to understand business needs and\nobjectives.\nCollaborate with and across Agile teams to design, develop, test, implement and support ETL\nprocesses for data transformation and preparation.\nManage data pipelines for analytics and operational use.\nEnsure data quality, data accuracy and integrity across multiple sources and systems.\nPerform unit tests and conduct reviews with other team members to make sure your code is\nrigorously designed, elegantly coded, and effectively tuned for performance.\nShould be able to come up with multiple approaches to any ETL\nproblem statement/solution/technical challenge and take well informed decision to pick the\nbest solution.\nAutomate ETL Processes using Cron and/or using Job Scheduler tools like AirFlow.\nAdhere to company standards and Serve as a key contributor to the design and development of\nexception handling, code/data standardization procedures, resolution steps and Quality Assurance\ncontrols.\nMaintain a version repository and ensure version control.\nCreate visual aids such as diagrams, charts, and screenshots to enhance documentation.\nWork with Infrastructure/systems team and developers to ensure all modules are up-to- date and\nare compatible with the code.",,,,"['ETL', 'SQL', 'UNIX', 'Python']",2025-06-12 14:54:22
Test Lead,Infosys,5 - 10 years,10-20 Lacs P.A.,[],"Role & responsibilities\n\nThe Test Lead oversees the testing strategy and execution for the Microsoft Fabric migration and Power BI reporting solutions. This offshore role ensures quality, reliability, and client satisfaction through rigorous validation.\nThe successful candidate will have a strong testing background and coordination skills.\nResponsibilities\nDevelop and execute the testing strategy for Microsoft Fabric and Power BI deliverables.\nValidate data migration, pipeline functionality, and report accuracy against requirements.\nCoordinate with the Offshore Project Manager to align testing with development milestones.\nCollaborate with onsite technical leads to validate results and resolve defects. • Oversee offshore testers, ensuring comprehensive coverage and quality standards.\nProactively identify risks and articulate solutions to minimize delivery issues.\nSkills\nBachelors degree in IT, computer science, or a related field.\n5+ years of experience in test leadership for data platforms and BI solutions.\nKnowledge of Microsoft Fabric, Power BI, and data migration testing.\nProficiency with testing tools (e.g., Azure DevOps, Selenium) and SQL.\nStrong communication and stakeholder management skills.\nDetail-oriented with a focus on quality and continuous improvement\n1. JD for Data Modeler\nThe Data Modeler designs and implements data models for Microsoft Fabric and Power BI, supporting the migration from Oracle/Informatica. This offshore role ensures optimized data structures for performance and reporting needs. The successful candidate will bring expertise in data modeling and a collaborative approach.\nResponsibilities\nDevelop conceptual, logical, and physical data models for Microsoft Fabric and Power BI solutions.\nImplement data models for relational, dimensional, and data lake environments on target platforms.\nCollaborate with the Offshore Data Engineer and Onsite Data Modernization Architect to ensure model alignment.\nDefine and govern data modeling standards, tools, and best practices.\nOptimize data structures for query performance and scalability.\nProvide updates on modeling progress and dependencies to the Offshore Project Manager.\nSkills\nBachelor’s or master’s degree in computer science, data science, or a related field.\n5+ years of data modeling experience with relational and NoSQL platforms.\nProficiency with modeling tools (e.g., Erwin, ER/Studio) and SQL.\nExperience with Microsoft Fabric, data lakes, and BI data structures.\nStrong analytical and communication skills for team collaboration.\nAttention to detail with a focus on performance and consistency.\nmanagement, communication, and presentation",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Test lead', 'Migration', 'power bi', 'microsoft fabric']",2025-06-12 14:54:24
Business Technology Solutions Associate Consultant,ZS,4 - 6 years,Not Disclosed,"['Pune', 'Gurugram']","Undertake primary ownership in driving self and team effort across all phases of a project lifecycle;\nTranslate business requirements into technical terms and drive team effort to design, build and manage technology solutions that solve business problems;\nApply appropriate development methodologies (eg: agile, waterfall) and best practices (eg: mid-development client reviews, embedded QA procedures, unit testing) to ensure successful and timely completion;\nPartner with Project lead/ Program lead in delivering projects and assist in project management responsibility like - project plans, people management, staffing and risk mitigation;",,,,"['Compliance', 'Data management', 'Staffing', 'MIS', 'Project management', 'Consulting', 'Financial planning', 'Data processing', 'Scheduling', 'SQL']",2025-06-12 14:54:27
Associate - Business Information Management,Axtria,5 - 10 years,Not Disclosed,['Gurugram'],"Position Summary \n\nThis is the Requisition for Employee Referrals Campaign and JD is Generic.\n\nWe are looking for Associates with 5+ years of experience in delivering solutions around Data Engineering, Big data analytics and data lakes, MDM, BI, and data visualization. Experienced to Integrate and standardize structured and unstructured data to enable faster insights using cloud technology. Enabling data-driven insights across the enterprise.\n\n Job Responsibilities \n\n\nHe/she should be able to design implement and deliver complex Data Warehousing/Data Lake, Cloud Data Management, and Data Integration project assignments.\n\nTechnical Design and Development – Expertise in any of the following skills.\n\nAny ETL tools (Informatica, Talend, Matillion, Data Stage), andhosting technologies like the AWS stack (Redshift, EC2) is mandatory.\n\nAny BI toolsamong Tablau, Qlik & Power BI and MSTR.\n\nInformatica MDM, Customer Data Management.\n\nExpert knowledge of SQL with the capability to performance tune complex SQL queries in tradition and distributed RDDMS systems is must.\n\nExperience across Python, PySpark and Unix/Linux Shell Scripting.\n\nProject Managementis\n\nmust to have. Should be able create simple to complex project plans in Microsoft Project Plan and think in advance about potential risks and mitigation plans as per project plan.\n\nTask Management – Should be able to onboard team on the project plan and delegate tasks to accomplish milestones as per plan. Should be comfortable in discussing and prioritizing work items with team members in an onshore-offshore model.\n\nHandle Client Relationship – Manage client communication and client expectations independently or with support of reporting manager. Should be able to deliver results back to the Client as per plan. Should have excellent communication skills.\n\n\n Education \n\nBachelor of Technology\nMaster's Equivalent - Engineering\n\n Work Experience \n\nOverall, 5- 7years of relevant experience inData Warehousing, Data management projects with some experience in the Pharma domain.\n\nWe are hiring for following roles across Data management tech stacks -\n\nETL toolsamong Informatica, IICS/Snowflake,Python& Matillion and other Cloud ETL.\n\nBI toolsamong Power BI and Tableau.\n\nMDM - Informatica/ Raltio, Customer Data Management.\n\nAzure cloud Developer using Data Factory and Databricks\n\nData Modeler-Modelling of data - understanding source data, creating data models for landing, integration.\n\nPython/PySpark -Spark/ PySpark Design, Development, and Deployment",Industry Type: Analytics / KPO / Research,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['aws stack', 'sql', 'etl tool', 'data visualization', 'sql queries', 'data management', 'amazon redshift', 'bi', 'data warehousing', 'pyspark', 'spark', 'etl', 'data lake', 'snowflake', 'python', 'big data analytics', 'datastage', 'talend', 'power bi', 'data engineering', 'tableau', 'mdm', 'aws', 'informatica', 'unix']",2025-06-12 14:54:29
Senior Software Engineer Python,Growexx,5 - 10 years,Not Disclosed,['Ahmedabad'],"built on top of the Revelation Open Insights analytics platform. This role is ideal for an engineer who thrives in complex, data-intensive environments and has a passion for modernizing lega operational stability. Key Responsibilities\nLead the maintenance, enhancement, and refactoring of the Python-based Legacy Registry system.\nCollaborate with data engineers, DevOps, and platform architects to ensure seamless integration with Revelation Open Insights.\nAnalyze and optimize legacy code for performance, scalability, and maintainability.\nDesign and implement automated testing strategies and CI/CD pipelines for legacy services.\nTroubleshoot and resolve production issues, ensuring high system availability and data integrity.\nDocument system architecture, workflows, and technical decisions to support long-term maintainability.\nParticipate in roadmap planning and contribute to modernization strategies.\nKey Skills\nDeep understanding of legacy system architecture, data modelling, and refactoring techniques.\nExperience working with SQL databases (e.g., PostgreSQL, MySQL) and data integration pipelines.\nFamiliarity with containerization (Docker), orchestration (Kubernetes), and CI/CD tools (e.g., GitHub Actions, Jenkins).\nStrong debugging, profiling, and performance tuning skills.\nExperience with enterprise data platforms or analytics systems.\nFamiliarity with the Revelation Open Insights platform or similar data intelligence tools.\nExposure to data governance, metadata management, or registry systems.\nEducation and Experience\nB.Tech or B.E. or MCA or BCA\n5+ years of professional experience in Python development, with a strong focus on backend systems\nAnalytical and Personal Skills Must have good logical reasoning and analytical skills\nAbility to break big goals to small incremental actions\nExcellent Communication and collaboration skills Demonstrate Ownership and Accountability of their work\nGreat attention to details\nDemonstrate ownership of tasks Positive and Cheerful outlook in life Work with the problem solver engineers team (Doc / PDF Only, Max file size 2 MB) By using this form you agree with the storage and handling of your data by this website. *\nYou cannot copy content of this page\nReconciliation Automation Data Sheet\nThis field is for validation purposes and should be left unchanged.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Performance tuning', 'System architecture', 'Automation', 'metadata', 'Postgresql', 'MySQL', 'Debugging', 'Analytics', 'SQL', 'Python']",2025-06-12 14:54:31
Tag Implementation Lead,Decision Foundry,8 - 10 years,Not Disclosed,['Bengaluru'],"Welcome to Decision Foundry - Data Analytics Division!\nWe are proud to introduce ourselves as a certified ""Great Place to Work,"" where we prioritize creating an exceptional work environment. As a global company, we embrace a diverse culture, fostering inclusivity across all levels.\nOriginating from a well-established 19-year web analytics company, we remain dedicated to our employee-centric approach. By valuing our team members, we aim to enhance engagement and drive collective success.\nWe are passionate about harnessing the power of data analytics to transform decision-making processes. Our mission is to empower data-driven decisions that contribute to a better world. In our workplace, you will enjoy the freedom to experiment and explore innovative ideas, leading to outstanding client service and value creation.\nWe win as an organization through our core tenets. They include:\nOne Team. One Theme.\nWe sign it. We deliver it.\nBe Accountable and Expect Accountability.\nRaise Your Hand or Be Willing to Extend it\nAbout the Role\nWe are looking for a seasoned Tag Implementation Lead to build and lead a new practice focused on digital tagging and data collection. This is a hands-on and strategic role where you will take ownership of client-side and server-side tagging implementations, build a team, and collaborate closely with clients and internal stakeholders to deliver best-in-class solutions.\nYou will work at the intersection of data engineering, marketing technology, and analytics, enabling businesses to track, measure, and optimize user interactions effectively.\nKey Responsibilities\nTag Management Strategy: Define and execute end-to-end tag implementation strategies for clients.\nClient & Server-Side Tagging: Implement and troubleshoot both client-side (browser-based) and server-side (cloud/CDN-based) tags.\nTool Ownership: Lead implementations using tools like Google Tag Manager (GTM), Adobe Launch, Tealium, or similar.\nClient Collaboration: Work directly with clients to understand requirements, propose tagging strategies, and manage deliverables.\nTeam Building: Hire, mentor, and manage a team of tag implementation engineers.\nDocumentation: Maintain detailed documentation on tagging setups, data layers, events, and variable mappings.\nData Quality Assurance: Establish QA frameworks to validate tag accuracy and data integrity across web and mobile platforms.\nCross-functional Collaboration: Work closely with web developers, analysts, and marketing teams to ensure accurate data capture.\n\n\n8-10 years of relevant experience in digital analytics or tag implementation roles.\nStrong expertise in Google Tag Manager (GTM).\nSolid understanding of JavaScript, browser debugging tools (e.g., Chrome DevTools), and DOM",Industry Type: Management Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Manager Quality Assurance', 'Google Analytics', 'Web analytics', 'Debugging', 'Javascript', 'Data collection', 'Agile', 'Data quality', 'data integrity', 'Team building']",2025-06-12 14:54:34
Senior DevOps Engineer,Velotio Technologies,5 - 7 years,Not Disclosed,['Pune'],"Velotio Technologies is a product engineering company working with innovative startups and enterprises. We are a certified Great Place to Work and recognized as one of the best companies to work for in India. We have provided full-stack product development for 110+ startups across the globe building products in the cloud-native, data engineering, B2B SaaS, IoT & Machine Learning space. Our team of 400+ elite software engineers solves hard technical problems while transforming customer ideas into successful products.\n\n\nWe are looking for a Senior DevOps Engineer , who will play a critical role in designing, implementing, and optimizing infrastructure and automation solutions to support a scalable, secure, and highly available environment. The candidate will work c",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Automation', 'Product engineering', 'Version control', 'Infrastructure management', 'Postgresql', 'MySQL', 'Disaster recovery', 'Machine learning', 'Budgeting', 'Python']",2025-06-12 14:54:36
"Spark, Java, Kafka- Hyderabad",Cognizant,12 - 15 years,Not Disclosed,['Hyderabad'],"Skill: Java, Spark, Kafka\nExperience: 10 to 16 years\nLocation: Hyderabad\n As Data Engineer, you will :\n       Support in designing and rolling out the data architecture and infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources\n       Identify data source, design  and implement data schema/models and integrate data that meet the requirements of the business stakeholders",,,,"['hive', 'cloudera', 'modeling', 'scala', 'data warehousing', 'apache pig', 'data pipeline', 'data architecture', 'scalability', 'sql', 'java', 'data modeling', 'spark', 'mysql', 'hadoop', 'etl', 'big data', 'hbase', 'python', 'oozie', 'data processing', 'airflow', 'elt', 'data engineering', 'nosql', 'mapreduce', 'kafka', 'feasibility analysis', 'hdfs', 'sqoop', 'aws']",2025-06-12 14:54:39
Senior JavaScript Software Engineer,Ciklum,6 - 10 years,Not Disclosed,"['Pune', 'Chennai']","Ciklum is looking for a Senior JavaScript Software Engineer to join our team full-time in India.\nWe are a custom product engineering company that supports both multinational organizations and scaling startups to solve their most complex business challenges. With a global team of over 4,000 highly skilled developers, consultants, analysts and product owners, we engineer technology that redefines industries and shapes the way people live.\n\nAbout the role:\nAs a Senior JavaScript Software Engineer, become a part of a cross-functional development team engineering experiences of tomorrow.\nClient for this project is a leading global provider of audit and assurance, consulting, financial advisory, risk advisory, tax, and related services. They are launching a digital transformation project to evaluate existing technology across the tax lifecycle and determine the best future state for that technology. This will include decomposing existing assets to determine functionality, assessment of those functionalities to determine the appropriate end state and building of new technologies to replace those functionalities.\n\nResponsibilities:\nParticipate in requirements analysis\nCollaborate with US and Vendors teams to produce software design and architecture\nWrite clean, scalable code using Angular with Typescript, HTML, CSS, and NET programming languages\nParticipate in pull request code review process\nTest and deploy applications and systems\nRevise, update, refactor and debug code\nDevelop, support and maintain applications and technology solutions\nEnsure that all development efforts meet or exceed client expectations. Applications should meet requirements of scope, functionality, and time and adhere to all defined and agreed upon standards\nBecome familiar with all development tools, testing tools, methodologies and processes\nBecome familiar with the project management methodology and processes\nEncourage collaborative efforts and camaraderie with on-shore and off-shore team members\nDemonstrate a strong working understanding of the best industry standards in software development and version controlling\nEnsure the quality and low bug rates of code released into production\nWork on agile projects, participate in daily SCRUM calls and provide task updates\nDuring design and key development phases, we might need to work a staggered shift as applicable to ensure appropriate overlap with US teams and project deliveries\n\nRequirements:\n  We know that sometimes, you cant tick every box. We would still love to hear from you if you think you will be a good fit\n6+ years of strong hands-on experience with JavaScript (ES6/ES2015+), HTML5, CSS3\n2+ years with hands-on experience with Typescript\n2+ years of hands-on experience with Angular 11+ component architecture, applying design patterns\nExperience with Angular 11+ and migrating to newer versions\nExperience with Angular State management or NgXs\nExperience with RxJS operators\nHands on experience with Kendo UI or Angular material or SpreadJS libraries\nExperience with Nx – Nrwl/Nx library for monorepos\nSkill for writing reusable components, Angular services, directives and pipes\nHands-on experience on C#, SQL Server, OOPS Concepts, Micro Services Architecture\nAt least two-year hands-on experience on .NET Core, ASP.NET Core Web API, SQL, NoSQL, Entity Framework 6 or above, Azure, Database performance tuning, Applying Design Patterns, Agile\n.Net back-end development with data engineering expertise. Experience with MS Fabric as a data platform/ Snowflake or similar tools would be a plus, but not a must need\nSkill for writing reusable libraries\nComfortable with Git & Git hooks using PowerShell, Terminal or a variation thereof\nFamiliarity with agile development methodologies\nExcellent Communication skills both oral & written\nExcellent troubleshooting and communication skills, ability to communicate clearly with US counterparts\n\nDesirable:\nExposure to micro-frontend architecture\nKnowledge on Yarn, Webpack, Mongo DB, NPM, Azure Devops Build/Release configuration\nSignalR, ASP.NET Core and WebSockets\nThis is an experienced level position, and we will train the qualified candidate in the required applications\nWillingness to work extra hours to meet deliverables\nExposure to Application Insights & Adobe Analytics\nUnderstanding of cloud infrastructure design and implementation\nExperience in CI/CD configuration\nGood knowledge of data analysis in enterprises\nExperience with Databricks, Snowflake\nExposure to Docker and its configurations, Experience with Kubernetes\n\nWhat's in it for you?\nCare: your mental and physical health is our priority. We ensure comprehensive company-paid medical insurance, as well as financial and legal consultation\nTailored education path: boost your skills and knowledge with our regular internal events (meetups, conferences, workshops), Udemy licence, language courses and company-paid certifications\nGrowth environment: share your experience and level up your expertise with a community of skilled professionals, locally and globally\nFlexibility: hybrid work mode at Chennai or Pune \nOpportunities: we value our specialists and always find the best options for them. Our Resourcing Team helps change a project if needed to help you grow, excel professionally and fulfil your potential\nGlobal impact: work on large-scale projects that redefine industries with international and fast-growing clients\nWelcoming environment: feel empowered with a friendly team, open-door policy, informal atmosphere within the company and regular team-building events\n\nAbout us:\nAt Ciklum, we are always exploring innovations, empowering each other to achieve more, and engineering solutions that matter. With us, you’ll work with cutting-edge technologies, contribute to impactful projects, and be part of a One Team culture that values collaboration and progress.\nIndia is a strategic innovation hub for Ciklum, with growing teams in Chennai and Pune leading advancements in EdgeTech, AR/VR, IoT, and beyond. Join us to collaborate on game-changing solutions and take your career to the next level.\nWant to learn more about us? Follow us on Instagram, Facebook, LinkedIn\n\nExplore, empower, engineer with Ciklum!\nExperiences of tomorrow. Engineered together\nInterested already?\nWe would love to get to know you! Submit your application. Can’t wait to see you at Ciklum.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Typescript', 'Angular', 'CSS', '.Net', 'HTML']",2025-06-12 14:54:42
AWS Cloud Tech Lead,ITC Infotech,5 - 9 years,Not Disclosed,['Pune'],"You will participate in the design, development, and deployment of scalable and robust applications using AWS Cloudfront, S3 buckets, Node.js, TypeScript, React.js, Next.js, Stencil.js, and Aurora Postgres SQL.\nImplement microservices that integrate with Databricks as a Data Lake and consume data products.\nKey Responsibilities\nCollaborate closely with other technical leads to align on standards, interfaces, and dependencies",,,,"['System architecture', 'Backend', 'Front end', 'Coding', 'Postgresql', 'Agile', 'Scrum', 'JIRA', 'AWS', 'microservices']",2025-06-12 14:54:44
Senior High Performance Computing Engineer,Amgen Inc,4 - 6 years,Not Disclosed,['Hyderabad'],"What you will do\nIn this vital role you will.\nRole Description:\nThe role is responsible for the design, integration, and management of high performance computing (HPC) systems that encompass both hardware and software components into the organizations network infrastructure. This individual will be responsible for all activities related to handling and supporting the Business and platforms including system administration, as well as incorporating new technologies under the challenge of a sophisticated and constantly evolving technology landscape. This role involves ensuring that all parts of a system work together seamlessly to meet the organizations requirements.\nRoles & Responsibilities:\nImplement, and manage cloud-based infrastructure that supports HPC environments that support data science (e.g. AI/ML workflows, Image Analysis).\nCollaborate with data scientists and ML engineers to deploy scalable machine learning models into production.\nEnsure the security, scalability, and reliability of HPC systems in the cloud.\nOptimize cloud resources for cost-effective and efficient use.\nKeep abreast of the latest in cloud services and industry standard processes.\nProvide technical leadership and guidance in cloud and HPC systems management.\nDevelop and maintain CI/CD pipelines for deploying resources to multi-cloud environments.\nMonitor and fix cluster operations/applications and cloud environments.\nDocument system design and operational procedures.\nBasic Qualifications:\nMasters degree with a 4 - 6 years of experience in Computer Science, IT or related field with hands-on HPC administration OR\nBachelors degree with 6 - 8 years of experience in Computer Science, IT or related field with hands-on HPC administration OR\nDiploma with 10-12 years of experience in Computer Science, IT or related field with hands-on HPC administration\nDemonstrable experience in cloud computing (preferably AWS) and cloud architecture.\nExperience with containerization technologies (Singularity, Docker) and cloud-based HPC solutions.\nExperience with infrastructure-as-code (IaC) tools such as Terraform, CloudFormation, Packer, Ansible and Git.\nExpert with scripting (Python or Bash) and Linux/Unix system administration (preferably Red Hat or Ubuntu).\nProficiency with job scheduling and resource management tools (SLURM, PBS, LSF, etc.).\nKnowledge of storage architectures and distributed file systems (Lustre, GPFS, Ceph).\nUnderstanding of networking architecture and security best practices.\nPreferred Qualifications:\nExperience supporting research in healthcare life sciences.\nExperience with Kubernetes (EKS) and service mesh architectures.\nKnowledge of AWS Lambda and event-driven architectures.\nExposure to multi-cloud environments (Azure, GCP).\nFamiliarity with machine learning frameworks (TensorFlow, PyTorch) and data pipelines.\nCertifications in cloud architecture (AWS Certified Solutions Architect, Google Cloud Professional Cloud Architect, etc.).\nExperience in an Agile development environment.\nPrior work with distributed computing and big data technologies (Hadoop, Spark).\nProfessional Certifications (please mention if the certification is preferred or mandatory for the role):\nRed Hat Certified Engineer (RHCE) or Linux Professional Institute Certification (LPIC)\nAWS Certified Solutions Architect Associate or Professional\nSoft Skills:\nStrong analytical and problem-solving skills.\nAbility to work effectively with global, virtual teams\nEffective communication and collaboration with cross-functional teams.\nAbility to work in a fast-paced, cloud-first environment.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['cloud computing', 'resource management', 'Ubuntu', 'Unix system administration', 'linux', 'unix production support', 'Python']",2025-06-12 14:54:46
Sr Software Engineer: Integration Engineer,HMH,5 - 7 years,Not Disclosed,['Pune'],"The Data Integration Engineer will play a key role in designing, building, and maintaining data integrations between core business systems such as Salesforce and SAP and our enterprise data warehouse on Snowflake. This position is ideal for an early-career professional (1 to 4 years of experience) eager to contribute to transformative data integration initiatives and learn in a collaborative, fast-paced environment.\n\nDuties & Responsibilities:\nCollaborate with cross-functional teams to understand business requirements and translate them into data integration solutions.\nDevelop and maintain ETL/ELT pipelines using modern tools like Informatica IDMC to connect source systems to Snowflake.\nEnsure data accuracy, consistency, and security in all integration workflows.\nMonitor, troubleshoot, and optimize data integration processes to meet performance and scalability goals.\nSupport ongoing integration projects, including Salesforce and SAP data pipelines, while adhering to best practices in data governance.\nDocument integration designs, workflows, and operational processes for effective knowledge sharing.\nAssist in implementing and improving data quality controls at the start of processes to ensure reliable outcomes.\nStay informed about the latest developments in integration technologies and contribute to team learning and improvement.",,,,"['GCP', 'Azure', 'IDMC', 'XML', 'CSV', 'JSON', 'SQL Server', 'AWS', 'data integration', 'data engineering']",2025-06-12 14:54:48
Senior High Performance Computing Engineer,Amgen Inc,6 - 8 years,Not Disclosed,['Hyderabad'],"What you will do\n\n\nIn this vital role you will be responsible for deploying, maintaining and supporting HPC infrastructure in a multi-cloud environment. Hands-on engineering which requires\n\ndeep technical expertise in HPC technology and standard methodologies.\nImplement, and manage cloud-based infrastructure that supports HPC environments that support data science (e.g. AI/ML workflows, Image Analysis).\nCollaborate with data scientists and ML engineers to deploy scalable machine learning models into production.\nEnsure the security, scalability, and reliability of HPC systems in the cloud.\nOptimize cloud resources for cost-effective and efficient use.\nStay ahead of with the latest in cloud services and industry standard processes.\nProvide technical leadership and guidance in cloud and HPC systems management.\nDevelop and maintain CI/CD pipelines for deploying resources to multi-cloud environments.\nMonitor and fix cluster operations/applications and cloud environments.\nDocument system design and operational procedures.\n\n\n\nMust-Have\n\nSkills:\nExpert with Linux/Unix system administration (RHEL, CentOS, Ubuntu, etc.).\nProficiency with job scheduling and resource management tools (SLURM, PBS, LSF, etc.).\nGood understanding of parallel computing, MPI, OpenMP, and GPU acceleration (CUDA, ROCm).\nKnowledge of storage architectures and distributed file systems (Lustre, GPFS, Ceph).\nExperience with containerization technologies (Singularity, Docker) and cloud-based HPC solutions.\nExpert in scripting languages (Python, Bash) and containerization technologies (Docker, Kubernetes).\nFamiliarity with automation tools (Ansible, Puppet, Chef) for system provisioning and maintenance.\nUnderstanding of networking protocols, high-speed interconnects, and security best practices.\nDemonstrable experience in cloud computing (AWS, Azure, GCP) and cloud architecture.\nExperience with infrastructure as code (IaC) tools like Terraform or CloudFormation and Git.\n\n\n\nWhat we expect of you\nWe are all different, yet we all use our unique contributions to serve patients. Expert knowledge in\n\nlarge Linux environments, networking, storage, and cloud related technologies. Also, the candidate will have\n\nexpertise in root-cause analysis and fix while working with a team and stakeholders.\n\nTop-level communication and documentation skills are required.\n\nExpertise in coding in\n\nPython, Bash, YAML is expected.\n\n\n\nGood-to-Have\n\nSkills:\nExperience with Kubernetes (EKS) and service mesh architectures.\nKnowledge of AWS Lambda and event-driven architectures.\nFamiliarity with AWS CDK, Ansible, or Packer for cloud automation.\nExposure to multi-cloud environments (Azure, GCP).\nBasic Qualifications:\nBachelors degree in computer science, IT, or related field with 6-8 years of hands-on HPC administration or a related field.\n\n\n\nProfessional Certifications (preferred):\nRed Hat Certified Engineer (RHCE) or Linux Professional Institute Certification (LPIC)\nAWS Certified Solutions Architect Associate or Professional\nPreferred Qualifications:\n\n\n\nSoft\n\nSkills:\nStrong analytical and problem-solving skills.\nAbility to work effectively with global, virtual teams\nEffective communication and collaboration with cross-functional teams.\nAbility to work in a fast-paced, cloud-first environment.\nShift Information: This position is required to be onsite and participate in 24/5 and weekend on call in rotation fashion and may require you to work a later shift. Candidates must be willing and able to work off hours, as required based on business requirements.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Performance Computing', 'python', 'cloud architecture', 'linux', 'bash', 'networking', 'linux internals', 'cloud computing', 'scripting languages']",2025-06-12 14:54:51
Data Scientist-Artificial Intelligence,IBM,3 - 7 years,Not Disclosed,['Bengaluru'],"As an Associate Data Scientist at IBM, you will work to solve business problems using leading edge and open-source tools such as Python, R, and TensorFlow, combined with IBM tools and our AI application suites. You will prepare, analyze, and understand data to deliver insight, predict emerging trends, and provide recommendations to stakeholders.\n\nIn your role, you may be responsible for\nImplementing and validating predictive and prescriptive models and creating and maintaining statistical models with a focus on big data & incorporating machine learning. techniques in your projects\nWriting programs to cleanse and integrate data in an efficient and reusable manner\nWorking in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors\nCommunicating with internal and external clients to understand and define business needs and appropriate modelling techniques to provide analytical solutions.\nEvaluating modelling results and communicating the results to technical and non-technical audiences\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nProof of Concept (POC) DevelopmentDevelop POCs to validate and showcase the feasibility and effectiveness of the proposed AI solutions.\nCollaborate with development teams to implement and iterate on POCs, ensuring alignment with customer requirements and expectations.\nHelp in showcasing the ability of Gen AI code assistant to refactor/rewrite and document code from one language to another, particularly COBOL to JAVA through rapid prototypes/ PoC\nDocument solution architectures, design decisions, implementation details, and lessons learned.\nCreate technical documentation, white papers, and best practice guides\n\n\nPreferred technical and professional experience\nStrong programming skills, with proficiency in Python and experience with AI frameworks such as TensorFlow, PyTorch, Keras or Hugging Face.\nUnderstanding in the usage of libraries such as SciKit Learn, Pandas, Matplotlib, etc. Familiarity with cloud platforms\nExperience and working knowledge in COBOL & JAVA would be preferred",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'scikit-learn', 'tensorflow', 'pytorch', 'keras', 'natural language processing', 'neural networks', 'predictive', 'huggingface', 'machine learning', 'prototype', 'artificial intelligence', 'sql', 'pandas', 'deep learning', 'r', 'java', 'cobol', 'data science', 'matplotlib', 'big data', 'statistics']",2025-06-13 05:56:07
Data Scientist,Tesco,1 - 3 years,Not Disclosed,['Bengaluru( Whitefield )'],"Job Summary:\n\nEnable data driven decision making across the Tesco business globally by developing analytics solutions using a combination of math, tech and business knowledge\n\nRoles and Responsibilities:\n- Identifying operational improvements and finding solutions by applying CI tools and techniques\n- Responsible for completing tasks and transactions within agreed KPI's",,,,"['Data Science', 'Advanced Excel', 'Data Analytics', 'Python', 'SQL', 'Applied Mathematics', 'Machine Learning', 'Statistics']",2025-06-13 05:56:09
Data Scientist - Python / Machine Learning,Blueberry Unicorn Services,6 - 11 years,Not Disclosed,"['Hyderabad', 'Bengaluru']","Working Hours : 2PM to 11PM IST\n\nMid-Level ML Engineers / Data Scientist Role : (4-5 years of experience )\n\n- Experience processing, filtering, and presenting large quantities (100K to Millions of rows) of data using Pandas and PySpark\n\n- Experience with statistical analysis, data modeling, machine learning, optimizations, regression modeling and forecasting, time series analysis, data mining, and demand modeling.\n\n- Experience applying various machine learning techniques and understanding the key parameters that affect their performance.\n\n- Experience with Predictive analytics (e.g., forecasting, time-series, neural networks) and Prescriptive analytics (e.g., stochastic optimization, bandits, reinforcement learning).\n\n- Experience with Python and Python packages like NumPy, Pandas and deep learning frameworks like TensorFlow, Pytorch and Keras\n\n- Experience in Big Data ecosystem with frameworks like Spark, PySpark , Unstructured DBs like Elasticsearch and MongoDB\n\n- Proficiency with TABLEAU or other web-based interfaces to create graphic-rich customizable plots, charts data maps etc.\n\n- Able to write SQL scripts for analysis and reporting (Redshift, SQL, MySQL).\n\n- Previous experience in ML, data scientist or optimization engineer role with a large technology company.\n\n- Experience in an operational environment developing, fast-prototyping, piloting, and launching analytic products.\n\n- Ability to develop experimental and analytic plans for data modeling processes, use of strong baselines, ability to accurately determine cause and effect relations.\n\n- Experience in creating data driven visualizations to describe an end-to-end system.\n\n- Excellent written and verbal communication skills. The role requires effective communication with colleagues from computer science, operations research, and business backgrounds.\n\n- Bachelors or Masters in Artificial Intelligence, Computer Science, Statistics, Applied Math, or a related field.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Machine Learning', 'Data Science', 'Data Scientist', 'Artificial Intelligence', 'Data Management', 'Big Data', 'Data Modeling', 'Spark', 'Numpy', 'Python', 'Predictive Analytics']",2025-06-13 05:56:11
Data Scientist,Paypal,2 - 5 years,Not Disclosed,['Bengaluru'],"The Company\nPayPal has been revolutionizing commerce globally for more than 25 years. Creating innovative experiences that make moving money, selling, and shopping simple, personalized, and secure, PayPal empowers consumers and businesses in approximately 200 markets to join and thrive in the global economy.\nWe operate a global, two-sided network at scale that connects hundreds of millions of merchants and consumers. We help merchants and consumers connect, transact, and complete payments, whether they are online or in person. PayPal is more than a connection to third-party payment networks. We provide proprietary payment solutions accepted by merchants that enable the completion of payments on our platform on behalf of our customers.\nOur beliefs are the foundation for how we conduct business every day. We live each day guided by our core values of Inclusion, Innovation, Collaboration, and Wellness. Together, our values ensure that we work together as one global team with our customers at the center of everything we do - and they push us to ensure we take care of ourselves, each other, and our communities.\nJob Summary:\nMeet your team:\n\nThis role sits within Credit card fraud risk strategy team of Global Fraud Prevention Org., focused on safeguarding our customers and business from evolving fraud threats. The team is responsible for developing and executing data-driven strategies to mitigate fraud in UK PPC portfolio.\n\nWhat do you need to know about the role:\n\nGlobal Fraud Prevention resides in the Global Risk Management (GRM) organization that supports various business lines in optimizing risk and rewards to enable profitable business growth.\n\nYou will be working closely with global fraud risk professionals focusing on managing and mitigating fraud risk in the UK PayPal Credit Portfolio,\n\nBe a part of this fraud revolution and enjoy the journey being with PayPal s growing team. Why You ll Love It here:\n\nImpact: Your work directly influences the success and growth of PayPal s credit offerings.\nLearning: Expect to level up your analytical and problem-solving skills every day with more challenges to solve\nGrowth: The credit card industry is constantly evolving, and you ll be right there on the cutting edge, sharpening your skills and new learnings along the way.\nCulture: We re a team of passionate professionals who love challenges and are always ready to celebrate a job well done.\nJob Description:\nYour way to impact:\nOwn the areas of Transaction Fraud risk policy: Work on Broad area of projects from Card risk strategies, acquisition and payment risk strategies, all depending on the business need.\nWork closely with Stakeholders: this includes Credit Risk, Product, finance teams to optimize fraud strategies and portfolio performance.\nProactively identify emerging fraud trends and propose mitigation strategies .\nMaintain and develop Monitoring and Alerting capabilities: to clearly monitor the PPC Card program health and simplify insights for key stakeholders.\nPresent regular updates to senior leaders: on Portfolio Health, highlights, lowlights, and actionable insights.\nYour day to day:\nIn this role you will have full ownership of portfolio and is responsible for end-to-end management of Fraud loss and decline rates.\nWorks independently and proficiently. Accountable for own results.\nCollaborate with different teams to develop strategies for fraud prevention, loss savings, and optimize transaction declines .\nAnalyze and assess risks to provide informed recommendations for mitigation strategies\nPrepare periodic KPI reports summarizing the business units risk and control environment for senior management\nWhat do you need to bring:\n2-5 years of domain expertise\nExcellent Problem-Solving Skills : Strong judgment and the ability to think strategically, creatively, and practically to address complex challenges.\nAdvanced Analytics expertise : Proficiency in SQL, Python, Advanced Excel, Tableau, and other analytics tools, with a proven track record of using them to solve real-world problems.\nExceptional communication skills : Outstanding written, verbal communication abilities, capable of translating complex technical concepts into clear, actionable insights for diverse audiences\nCollaboration Influence : Strong ability to collaborate across teams, build relationships, and drive results through influence and teamwork\nExperience in Payments / Transaction risk management / Credit / Fraud Risk is a strong plus.\n** We know the confidence gap and imposter syndrome can get in the way of meeting spectacular candidates. Please dont hesitate to apply.\nPreferred Qualification:\nSubsidiary:\nPayPal\nTravel Percent:\n0\nFor the majority of employees, PayPals balanced hybrid work model offers 3 days in the office for effective in-person collaboration and 2 days at your choice of either the PayPal office or your home workspace, ensuring that you equally have the benefits and conveniences of both locations.\nOur Benefits:\nWe have great benefits including a flexible work environment, employee shares options, health and life insurance and more. To learn more about our benefits please visit https://www.paypalbenefits.com .\nWho We Are:\nClick Here to learn more about our culture and community.\nCommitment to Diversity and Inclusion\nPayPal provides equal employment opportunity (EEO) to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, pregnancy, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state, or local law. In addition, PayPal will provide reasonable accommodations for qualified individuals with disabilities. If you are unable to submit an application because of incompatible assistive technology or a disability, please contact us at talentaccommodations@paypal.com .\nBelonging at PayPal:\nOur employees are central to advancing our mission, and we strive to create an environment where everyone can do their best work with a sense of purpose and belonging. Belonging at PayPal means creating a workplace with a sense of acceptance and security where all employees feel included and valued. We are proud to have a diverse workforce reflective of the merchants, consumers, and communities that we serve, and we continue to take tangible actions to cultivate inclusivity and belonging at PayPal.\nAny general requests for consideration of your skills, please Join our Talent Community .\nWe know the confidence gap and imposter syndrome can get in the way of meeting spectacular candidates. Please don t hesitate to apply.",Industry Type: FinTech / Payments,Department: Other,"Employment Type: Full Time, Permanent","['advanced analytics', 'PPC', 'Analytical', 'Diversity and Inclusion', 'Wellness', 'Advanced Excel', 'Risk management', 'Analytics', 'Monitoring', 'SQL']",2025-06-13 05:56:13
Data Scientist,Mastercard,4 - 8 years,Not Disclosed,['Gurugram'],"As consumer preference for digital payments continues to grow, ensuring a seamless and secure consumer experience is top of mind. Optimization Soltions team focuses on tracking of digital performance across all products and regions, understanding the factors influencing performance and the broader industry landscape. This includes delivering data-driven insights and business recommendations, engaging directly with key external stakeholders on implementing optimization solutions (new and existing), and partnering across the organization to drive alignment and ensure action is taken.\n\nThe Role:\n\nWork closely with global optimization solutions team to architect, develop, and maintain advanced reporting and data visualization capabilities on large volumes of data to support data insights and analytical needs across products, markets, and services\nThe candidate for this position will focus on Building solutions using Machine Learning and creating actionable insights to support product optimization and sales enablement.\nPrototype new algorithms, experiment, evaluate and deliver actionable insights.\nDrive the evolution of products with an impact focused on data science and engineering.\nDesigning machine learning systems and self-running artificial intelligence (AI) software to automate predictive models.\nPerform data ingestion, aggregation, and processing on high volume and high dimensionality data to drive and enable data unification and produce relevant insights.\nContinuously innovate and determine new approaches, tools, techniques & technologies to solve business problems and generate business insights & recommendations.\nApply knowledge of metrics, measurements, and benchmarking to complex and demanding solutions.\n\nAll about You\nA superior academic record at a leading university in Computer Science, Data Science, Technology, mathematics, statistics, or a related field or equivalent work experience\nExperience in data management, data mining, data analytics, data reporting, data product development and quantitative analysis\nStrong analytical skills with track record of translating data into compelling insights\nPrior experience working in a product development role.\nknowledge of ML frameworks, libraries, data structures, data modeling, and software architecture.\nproficiency in using Python/Spark, Hadoop platforms & tools (Hive, Impala, Airflow, NiFi), and SQL to build Big Data products & platforms\nExperience with Enterprise Business Intelligence Platform/Data platform ie Tableau, PowerBI is a plus.\nDemonstrated success interacting with stakeholders to understand technical needs and ensuring analyses and solutions meet their needs effectively.\nAbility to build a strong narrative on the business value of products and actively participate in sales enablement efforts.\nAble to work in a fast-paced, deadline-driven environment as part of a team and as an individual contributor.",Industry Type: Financial Services,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Data management', 'Data modeling', 'Information security', 'Machine learning', 'Data structures', 'Data mining', 'Business intelligence', 'SQL', 'Python']",2025-06-13 05:56:15
Data Scientist,Dynamic Yield,5 - 10 years,Not Disclosed,['Gurugram'],"Our Purpose\nMastercard powers economies and empowers people in 200+ countries and territories worldwide. Together with our customers, we re helping build a sustainable economy where everyone can prosper. We support a wide range of digital payments choices, making transactions secure, simple, smart and accessible. Our technology and innovation, partnerships and networks combine to deliver a unique set of products and services that help people, businesses and governments realize their greatest potential.\nTitle and Summary\nData Scientist\nWho is Mastercard?\nMastercard is a global technology company in the payments industry. Our mission is to connect and power an inclusive, digital economy that benefits everyone, everywhere by making transactions safe, simple, smart, and accessible. Using secure data and networks, partnerships, and passion, our innovations and solutions help individuals, financial institutions, governments, and businesses realize their greatest potential.\nOur decency quotient, or DQ, drives our culture and everything we do inside and outside of our company. With connections across more than 210 countries and territories, we are building a sustainable world that unlocks priceless possibilities for all.\n\nOur Team:\nAs consumer preference for digital payments continues to grow, ensuring a seamless and secure consumer experience is top of mind. Optimization Soltions team focuses on tracking of digital performance across all products and regions, understanding the factors influencing performance and the broader industry landscape. This includes delivering data-driven insights and business recommendations, engaging directly with key external stakeholders on implementing optimization solutions (new and existing), and partnering across the organization to drive alignment and ensure action is taken.\nAre you excited about Data Assets and the value they bring to an organization?\nAre you an evangelist for data-driven decision-making?\nAre you motivated to be part of a team that builds large-scale Analytical Capabilities supporting end users across 6 continents?\nDo you want to be the go-to resource for data science & analytics in the company?\n\n\nThe Role:\n\nWork closely with global optimization solutions team to architect, develop, and maintain advanced reporting and data visualization capabilities on large volumes of data to support data insights and analytical needs across products, markets, and services\nThe candidate for this position will focus on Building solutions using Machine Learning and creating actionable insights to support product optimization and sales enablement.\nPrototype new algorithms, experiment, evaluate and deliver actionable insights.\nDrive the evolution of products with an impact focused on data science and engineering.\nDesigning machine learning systems and self-running artificial intelligence (AI) software to automate predictive models.\nPerform data ingestion, aggregation, and processing on high volume and high dimensionality data to drive and enable data unification and produce relevant insights.\nContinuously innovate and determine new approaches, tools, techniques & technologies to solve business problems and generate business insights & recommendations.\nApply knowledge of metrics, measurements, and benchmarking to complex and demanding solutions.\n\nAll about You\nA superior academic record at a leading university in Computer Science, Data Science, Technology, mathematics, statistics, or a related field or equivalent work experience\nExperience in data management, data mining, data analytics, data reporting, data product development and quantitative analysis\nStrong analytical skills with track record of translating data into compelling insights\nPrior experience working in a product development role.\nknowledge of ML frameworks, libraries, data structures, data modeling, and software architecture.\nproficiency in using Python/Spark, Hadoop platforms & tools (Hive, Impala, Airflow, NiFi), and SQL to build Big Data products & platforms\nExperience with Enterprise Business Intelligence Platform/Data platform i.e. Tableau, PowerBI is a plus.\nDemonstrated success interacting with stakeholders to understand technical needs and ensuring analyses and solutions meet their needs effectively.\nAbility to build a strong narrative on the business value of products and actively participate in sales enablement efforts.\nAble to work in a fast-paced, deadline-driven environment as part of a team and as an individual contributor.\nCorporate Security Responsibility\n\nAll activities involving access to Mastercard assets, information, and networks comes with an inherent risk to the organization and, therefore, it is expected that every person working for, or on behalf of, Mastercard is responsible for information security and must:\nAbide by Mastercard s security policies and practices;\nEnsure the confidentiality and integrity of the information being accessed;\nReport any suspected information security violation or breach, and\nComplete all periodic mandatory security trainings in accordance with Mastercard s guidelines.",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Data management', 'Data modeling', 'Information security', 'Machine learning', 'Data structures', 'Data mining', 'Business intelligence', 'SQL', 'Python']",2025-06-13 05:56:17
Join us as a Data Scientist!!,Zensar,6 - 11 years,Not Disclosed,"['Hyderabad', 'Delhi / NCR']","-\nData Scientist\n\n-6+ years of experience in data science, with at least 2 years focused on LLMs or Generative AI.\n\n-Proven implementation experience in Data Science, Machine Learning, Deep Learning, and NLP for multiple domains.\n\n-Strong programming skills in Python, with experience in libraries such as Transformers (Hugging Face), PyTorch, or TensorFlow.\n\n-Hands-on experience with fine-tuning, prompt engineering, RAG (Retrieval-Augmented Generation), and LLM evaluation.\n\n-Familiarity with vector databases and embedding techniques.\n\n-Experience deploying models using APIs, Docker, and cloud platforms\n\n-Strong analytical, problem-solving, and communication skills.\n\n-Experience in ML Ops, Model deployment, Model lifecycle and management",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Pytorch', 'NLP', 'Machine Learning', 'Deep Learning', 'Tensorflow', 'Cloud Deployment', 'ML Ops', 'Generative AI', 'Hugging Face', 'LLM', 'Python']",2025-06-13 05:56:19
Data Scientist,HMG Technology,3 - 8 years,10-20 Lacs P.A.,['Bengaluru( Banaswadi )'],"Job Title: AI/ML Engineer (with LLM, Azure, Python & PySpark expertise)\nJob Description:\nWe are looking for a skilled and experienced AI/ML Engineer to join our data science and AI team. The ideal candidate will have a strong foundation in machine learning, artificial intelligence, and large language models (LLMs), along with deep proficiency in Python, PySpark, and Microsoft Azure services. You will be responsible for developing and deploying scalable AI solutions, working with big data frameworks, and leveraging cloud platforms to operationalize machine learning models.\nKey Responsibilities:\nArtificial Intelligence (AI) & Machine Learning (ML):\nDesign, develop, and optimize machine learning and AI models to solve business problems.\nPerform exploratory data analysis and feature engineering for model development.\nUse supervised, unsupervised, and reinforcement learning techniques where appropriate.\nBuild AI pipelines and integrate models into production systems.\nLarge Language Models (LLM):\nFine-tune and deploy LLMs (e.g., OpenAI, Hugging Face, or custom-trained models).\nDevelop prompt engineering strategies for LLM applications.\nImplement RAG (Retrieval-Augmented Generation) systems or LLMOps workflows.\nEvaluate LLM outputs for accuracy, bias, and performance.\nPython Programming:\nWrite efficient, reusable, and testable Python code for data processing, modeling, and API services.\nBuild automation scripts for data pipelines and model training workflows.\nUse popular libraries such as Scikit-learn, TensorFlow, PyTorch, Pandas, and NumPy.\nPySpark and Big Data:\nWork with large datasets using PySpark for data wrangling, transformation, and feature extraction.\nOptimize Spark jobs for performance and scalability.\nCollaborate with data engineering teams to implement end-to-end data pipelines.\nMicrosoft Azure:\nDeploy models and applications using Azure ML, Azure Databricks, Azure Functions, and Azure Synapse.\nManage compute resources, storage, and data security on Azure.\nUse Azure DevOps for CI/CD of ML pipelines and automation.\nCross-Functional Collaboration & Documentation:\nCollaborate with data engineers, product managers, and business stakeholders to align technical solutions with business needs.\nMaintain clear documentation of models, code, and workflows.\nPresent technical findings and model outcomes to both technical and non-technical audiences.\nRequired Skills & Qualifications:\nBachelor's or Masters degree in Computer Science, Data Science, Engineering, or a related field.\n3+ years of experience in AI/ML and data engineering roles.\nProficient in Python and PySpark.\nExperience with cloud platforms, especially Microsoft Azure.\nHands-on experience with LLMs (e.g., GPT, BERT, Claude, etc.).\nFamiliarity with ML frameworks like Scikit-learn, TensorFlow, or PyTorch.\nSolid understanding of ML lifecycle, MLOps, and deployment strategies.\nNice to Have:\nExperience with LLMOps and vector databases (e.g., FAISS, Pinecone).\nKnowledge of data governance and responsible AI practices.\nAzure certifications (e.g., Azure AI Engineer Associate, Azure Data Scientist Associate).\nExperience with REST APIs and containerization (Docker, Kubernetes).",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Large Language Model', 'Azure Cloud', 'Artificial Intelligence', 'Machine Learning', 'Pyspark', 'Deep Learning', 'Python']",2025-06-13 05:56:20
Data Scientist,NatWest Markets,5 - 10 years,Not Disclosed,['Bengaluru'],"Join us as a Data Scientist\nYou ll design and implement data science tools and methods which harness our data in order to drive market leading purposeful customer solutions\nWe ll look to you to actively participate in the data community to identify and deliver opportunities to support the bank s strategic direction through better use of data\nThis is an opportunity to promote data literacy education with business stakeholders supporting them to foster a data driven culture and to make a real impact with your work\nWere offering this role at associate level\nWhat youll do\nAs a Data Scientist, you ll bring together statistical, mathematical, machine-learning and software engineering skills to consider multiple solutions, techniques and algorithms to develop and implement ethically sound models end-to-end. We ll look to you to understand the needs of business stakeholders, form hypotheses and identify suitable data and analytics solutions to meet those needs in order to support the achievement of our business strategy.\nYou ll also be:\nUsing data translation skills to work closely with business stakeholders to define detailed business questions, problems or opportunities which can be supported through analytics\nApplying a software engineering and product development lens to business problems, creating, scaling and deploying software driven products and services\nWorking in an Agile way within multi-disciplinary data and analytics teams to achieve agreed project and scrum outcomes\nSelecting, building, training and testing machine learning models considering model valuation, model risk, governance and ethics, making sure that models are ready to implement and scale\nIteratively building and prototyping data analysis pipelines to provide insights that will ultimately lead to production deployment\nThe skills youll need\nYou ll need a strong academic background in a STEM discipline such as Mathematics, Physics, Engineering or Computer Science. You ll have an experience of atleast five years with statistical modelling and machine learning techniques.\nWe ll also look for financial services knowledge, and an ability to identify wider business impact, risk or opportunities and make connections across key outputs and processes\nYou ll also demonstrate:\nThe ability to use data to solve business problems from hypotheses through to resolution\nExperience using Python, Tableau, SQL and software engineering fundamentals\nExperience of of analytics in fraud prevention and detection\nExperience of monitoring and maintaining model performance through developing new dashboards and reports, improve existing dashboards and in-house Python packages\nExperience of exploratory data analysis\nGood communication skills with the ability to proactively engage with a wide range of stakeholders",Industry Type: Banking,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Data analysis', 'Machine learning', 'Agile', 'Scrum', 'Analytics', 'Monitoring', 'Financial services', 'SQL', 'Python']",2025-06-13 05:56:22
Data Scientist,Grid Dynamics,10 - 20 years,Not Disclosed,['Hyderabad'],"Role & responsibilitiMes\n\nCandiate needs to be 8+ Years of Experience\n\nDetails on tech stack\nPython\nPrompt engineering\nBest practices for prompt engineering\nHow LLM can be used in applications for a variety of tasks\nNLP\nUnderstanding of typical NLP problems: classification, NER, summarization, question answering, sentiment analysis, etc.\nTheoretical intuitive understanding of how Transformers work (tokenization, attention, etc).\nWord and sentence embeddings\nVector search\nVector databases, performance tuning\nDocument chunking techniques\nLLM applications development\nLangChain, LlamaIndex\nChain of Thoughts, DSP, and other techniques\nAgents and tools\nGoogle cloud (GCP)\nNice to have requirements to the candidate\nPreferable, the engineers are expected to have IT services/consulting experience.\nProficient in developing LLM-powered systems using advanced prompt engineering techniques, RAG and agentic design patterns. Experienced with frameworks like LangChain, LlamaIndex, and DSPy.\nFamiliar with evaluation approaches and metrics for different types of LLM-based systems.\nExperienced with keyword and vector search methods, including understanding of their underlying algorithms. Familiar with popular vector search engines.\nCompetent in various document understanding models and techniques to parse complex documents and implement effective chunking strategies for RAG systems.\nFamiliar with LLM and embedding models fine-tuning techniques.\nCompetent in using joint vision-language and generative models to solve various problems related to image generation, visual question answering, and multi-modal search. Familiar with diffusion models and associated techniques like LoRA, Dreambooth, and ControlNet.\nUnderstanding of the challenges and risks associated with the development of Generative AI systems and how to mitigate them.\nFamiliar with various architecture design patterns for different types of LLM-based applications such as chatbots, text2sql, document understanding, etc. Familiar with various approaches to scalability and cost reduction in Generative AI systems.\nAbility to stay updated with the latest advancements in Generative AI and integrate emerging technologies to drive innovation and improve the performance of AI systems.\nFamiliar with Responsible AI principles and Human-AI interaction design best practices.\n\n\nPreferred candidate profile",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Lora', 'Natural Language Processing', 'Deep Learning', 'Python']",2025-06-13 05:56:24
Data Scientist-Artificial Intelligence,IBM,5 - 7 years,Not Disclosed,['Bengaluru'],"Work with broader team to build, analyze and improve the AI solutions.\nYou will also work with our software developers in consuming different enterprise applications\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nResource should have 5-7 years of experience. Sound knowledge of Python and should know how to use the ML related services.\nProficient in Python with focus on Data Analytics Packages.\nStrategy Analyse large, complex data sets and provide actionable insights to inform business decisions.\nStrategy Design and implementing data models that help in identifying patterns and trends. Collaboration Work with data engineers to optimize and maintain data pipelines.\nPerform quantitative analyses that translate data into actionable insights and provide analytical, data-driven decision-making. Identify and recommend process improvements to enhance the efficiency of the data platform. Develop and maintain data models, algorithms, and statistical models\n\n\nPreferred technical and professional experience\nExperience with conversation analytics. Experience with cloud technologies\nExperience with data exploration tools such as Tableu",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['algorithms', 'python', 'data analytics', 'tableau', 'ml', 'hive', 'data analysis', 'natural language processing', 'pyspark', 'data warehousing', 'machine learning', 'artificial intelligence', 'sql', 'pandas', 'deep learning', 'java', 'data science', 'spark', 'kafka', 'hadoop', 'big data', 'aws', 'etl']",2025-06-13 05:56:25
Data Scientist-Artificial Intelligence,IBM,3 - 7 years,Not Disclosed,['Bengaluru'],"An AI Data Scientist at IBM is not just a job title - it’s a mindset. You’ll leverage the watsonx,AWS Sagemaker,Azure Open AI platform to co-create AI value with clients, focusing on technology patterns to enhance repeatability and delight clients.\n\nWe are seeking an experienced and innovative AI Data Scientist to be specialized in foundation models and large language models. In this role, you will be responsible for architecting and delivering AI solutions using cutting-edge technologies, with a strong focus on foundation models and large language models. You will work closely with customers, product managers, and development teams to understand business requirements and design custom AI solutions that address complex challenges. Experience with tools like Github Copilot, Amazon Code Whisperer etc. is desirable.\n\nSuccess is our passion, and your accomplishments will reflect this, driving your career forward, propelling your team to success, and helping our clients to thrive.\n\nDay-to-Day Duties:\nProof of Concept (POC) DevelopmentDevelop POCs to validate and showcase the feasibility and effectiveness of the proposed AI solutions. Collaborate with development teams to implement and iterate on POCs, ensuring alignment with customer requirements and expectations.\nHelp in showcasing the ability of Gen AI code assistant to refactor/rewrite and document code from one language to another, particularly COBOL to JAVA through rapid prototypes/ PoC\nDocumentation and Knowledge SharingDocument solution architectures, design decisions, implementation details, and lessons learned. Create technical documentation, white papers, and best practice guides. Contribute to internal knowledge sharing initiatives and mentor new team members.\nIndustry Trends and InnovationStay up to date with the latest trends and advancements in AI, foundation models, and large language models. Evaluate emerging technologies, tools, and frameworks to assess their potential impact on solution design and implementation\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nStrong programming skills, with proficiency in Python and experience with AI frameworks such as TensorFlow, PyTorch, Keras or Hugging Face. Understanding in the usage of libraries such as SciKit Learn, Pandas, Matplotlib, etc. Familiarity with cloud platforms (e.g. Kubernetes, AWS, Azure, GCP) and related services is a plus.\nExperience and working knowledge in COBOL & JAVA would be preferred\nHaving experience in Code generation, code matching & code translation leveraging LLM capabilities would be a Big plus (e.g. Amazon Code Whisperer, Github Copilot etc.) * Soft\n\nSkills:\nExcellent interpersonal and communication skills. Engage with stakeholders for analysis and implementation. Commitment to continuous learning and staying updated with advancements in the field of AI.\nGrowth mindsetDemonstrate a growth mindset to understand clients' business processes and challenges.\nExperience in python and pyspark will be added advantage\n\n\nPreferred technical and professional experience\nExperienceProven experience in designing and delivering AI solutions, with a focus on foundation models, large language models, exposure to open source, or similar technologies. Experience in natural language processing (NLP) and text analytics is highly desirable. Understanding of machine learning and deep learning algorithms.\nStrong track record in scientific publications or open-source communities\nExperience in full AI project lifecycle, from research and prototyping to deployment in production environments",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'machine learning', 'tensorflow', 'pytorch', 'keras', 'kubernetes', 'github', 'natural language processing', 'scikit-learn', 'pyspark', 'microsoft azure', 'artificial intelligence', 'text analytics', 'pandas', 'deep learning', 'java', 'code generation', 'cobol', 'gcp', 'matplotlib', 'aws']",2025-06-13 05:56:27
Data Scientist-Artificial Intelligence,IBM,10 - 15 years,Not Disclosed,['Bengaluru'],"We're seeking a results-driven and collaborative Software Development Manager to lead the design and development of IBM Consulting Advantage Platform. As a management leader, you'll collaborate with peers and stakeholders to ensure business continuity. You'll also be responsible for building and leading an impactful team of Developers & QA engineers, focusing on software developments, productivity improvements and fostering a culture of continuous learning and improvement.\nIn this role, you will be responsible for:\nLead a team of engineers to meet release dates along with committed deliverables on-time and with quality\nBalance priorities and work assignments across team members following agile processes to meet delivery schedules\nInterface with product management and offering managers to understand customer requirements and business prioritization\nDrive development activities, monitor progress, collaborate to align dependencies, remove blockers for team members and manage risks\nDevelop and implement effective strategies for software development, testing, and deployment\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\n10+ years of professional experience; 5+ years as team lead/manager\nExcellent organizational skills including attention to details, time management, and multi-tasking skills\nHands-on experience Experienced building Microservices & REST APIs using Java, and other related technologies\nExperience with Front End Development programming languages and design Frameworks\nStrong project management, organizational, problem-solving, communication, and collaboration skills\n\n\nPreferred technical and professional experience\nHands-on experience with SpringBoot, ReactJS, NodeJS etc\nExperience in working on a production SaaS application with SOC2 certification\nKnowledge of Containerisation technologies such as Kubernetes & Docker, and CI/CD pipelines such as Tekton, ArgoCD etc.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['continuous integration', 'ci/cd', 'microservices', 'java', 'project management', 'kubernetes', 'docker', 'ansible', 'sql', 'react.js', 'git', 'devops', 'linux', 'jenkins', 'html', 'shell scripting', 'rest', 'python', 'github', 'maven', 'microsoft azure', 'javascript', 'spring boot', 'node.js', 'saas', 'terraform', 'aws']",2025-06-13 05:56:29
Data Scientist,Leading Automobile Manufacturing Company...,5 - 10 years,Not Disclosed,['Chennai'],"Kindly share your resume on sv17@svmanagement.com\nResponsibility:\nWork with different user groups/ departments\nIdentify processes where Analytics driven decision making can create powerful impact\nDesign original analysis that helps generate relevant insights\nEstablishes credibility by thought partnering with business and service teams on analytics topics; takes positions and draws conclusions on a range of external and internal issues\nCommunicates analytical insights through sophisticated synthesis and packaging of results (including PPT slides, dashboards, mailers and alerts)\nCollect, synthesize, analyze team learning & inputs into new best practices and methodologies\nWork with IT teams for implementation of solutions in a production environment\nWork on development of internal capability on the subject\nKeep abreast of most recent developments in the Analytics space and identify new tools and capabilities relevant to company needs.\nAptitude to constantly learn and explore new analytical advancements\nContributes to development of new topic- and sector-related analytics products (development in scope for separate proprietary data & tools team)\nDevelops topic and content related to analytics work for trainings\nProfile:\nExperience in designing analytical solutions using machine learning algorithms\nKnowledge of advanced Excel for preliminary data analysis and good presentation skills\nProficient Coding Knowledge in Python is essential. Developing visualizations in Tableau is desirable\nAdditional coding knowledge in R & Visual Basic will be an advantage\nProficient in web analytics and predictive analytics\nGraduate/certificate in Business Analytics from premier Institute would be an advantage",Industry Type: Automobile,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'R', 'Visual Basic', 'Machine Learning', 'Python', 'Web Analytics', 'Tableau', 'Data Analytics', 'Predictive Analytics']",2025-06-13 05:56:31
Principle Data Scientist,Hilabs,5 - 7 years,Not Disclosed,['Pune'],"The HiLabs Story\nHiLabs is a leading provider of AI-powered solutions to clean dirty data, unlocking its hidden potential for healthcare transformation. HiLabs is committed to transforming the healthcare industry through innovation, collaboration, and a relentless focus on improving patient outcomes.\nHiLabs Team\nMultidisciplinary industry leaders\nHealthcare domain experts\nAI/ML and data science experts\nProfessionals hailing from the worlds best universities, business schools, and engineering institutes including Harvard, Yale, Carnegie Mellon, Duke, Georgia Tech, Indian Institute of Management (IIM), and Indian Institute of Technology (IIT).\nJob Title : Lead/Senior Data Scientist\n\nJob Location : Pune\n\nJob summary: HiLabs is looking for highly motivated and skilled Lead/Sr. Data Scientist focused on the application of emerging technologies. The candidates must be well versed with Python, Scala, Spark, SQL and AWS platform. The individuals who will join the new Evolutionary Platform team should be continually striving to advance AI/ML excellence and technology innovation. The mission is to power the next generation of the digital product and services through innovation, collaboration, and transparency. You will be a technology leader and doer who enjoys working in a dynamic, fast- paced environment.\nResponsibilities:\nLeverage AI/ML techniques and solutions to identify and mathematically interpret complex healthcare problems.\nFull-stack development of data pipelines involving Big Data.\nDesign and development of robust application/data pipelines using Python, Scala, Spark, and SQL\nLead a team of Data Scientists, developers as well as clinicians to strategize, design and evaluate AI based solutions to healthcare problems.\nIncrease efficiency and improve the quality of solutions offered.\nManaging the complete ETL pipeline development process from conception to deployment\nCollaborating with and guiding the team on writing, building, and deployment of data software\nFollowing best design and development practices to ensure high quality code.\nDesign, build and maintain efficient, secure, reusable, and reliable code\nPerform code reviews, testing, and debugging\nDesired Profile:\nBachelors or Master s degrees in computer science, Mathematics, or any other quantitative discipline from Premium/Tier 1 institutions\n5 to 7 years of experience in developing robust ETL data pipelines and implementing advanced AI/ML algorithms (GenAI is a plus).\nStrong experience working with technologies like Python, Scala, Spark, Apache Solr, MySQL, Airflow, AWS etc.\nExperience working with Relational databases like MySQL, SQLServer, Oracle etc.\nGood understanding of large system architecture and design\nUnderstands the core concepts of Machine Learning and the math behind it.\nExperience working in AWS/Azure cloud environment\nExperience using Version Control tools such as Bitbucket/GIT code repository\nExperience using tools like Maven/Jenkins, JIRA\nExperience working in an Agile software delivery environment, with exposure to continuous integration and continuous delivery tools\nGreat collaboration and interpersonal skills\nAbility to work with team members and lead by example in code, feature development, and knowledge sharing\nHiLabs is an equal opportunity employer (EOE). No job applicant or employee shall receive less favorable treatment or be disadvantaged because of their gender, marital or family status, color, race, ethnic origin, religion, disability, or age; nor be subject to less favorable treatment or be disadvantaged on any other basis prohibited by applicable law.\nHiLabs is proud to be an equal opportunity workplace dedicated to pursuing and hiring a diverse and inclusive workforce to support individual growth and superior business results.\n\nHiLabs Total Rewards\nCompetitive Salary, Accelerated Incentive Policies, H1B sponsorship, Comprehensive benefits package that includes ESOPs, financial contribution for your ongoing professional and personal development, medical coverage for you and your loved ones, 401k, PTOs & a collaborative working environment, Smart mentorship, and highly qualified multidisciplinary, incredibly talented professionals from highly renowned and accredited medical schools, business schools, and engineering institutes.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['System architecture', 'Maven', 'MySQL', 'Debugging', 'Agile', 'h1b', 'Oracle', 'Apache', 'SQL', 'Python']",2025-06-13 05:56:33
Full Stack Data Scientist,Vimo Getinsured,2 - 7 years,Not Disclosed,['Gurugram( Sector 61 Gurgaon )'],"About the Role\nAs a Data Science Engineer, you will need strong technical skills in data modeling, machine learning, data engineering, and software development. You will have the ability to conduct literature reviews and critically evaluate research papers to identify applicable techniques. Additionally, you should be able to design and implement efficient and scalable data processing pipelines, perform exploratory data analysis, and collaborate with other teams to integrate data science models into production systems. Passion for conversational AI and a desire to solve some of the most complex problems in the Natural Language Processing space are essential. You will work on highly scalable, stable, and automated deployments, aiming for high performance. Taking on the challenge of building and scaling a truly remarkable AI platform to impact the lives of millions of customers will be part of your responsibilities. Working in a challenging yet enjoyable environment, where learning new things is the norm, you should think of solutions beyond boundaries. You should also drive outcomes with full ownership, deeply believe in customer obsession, and thrive in a fast-paced environment of learning and innovation.\nYou will work in a challenging, consumer-facing problem space, where you can make an immediate impact. You will get to work with the latest technologies, learn to use new tools and get the opportunity to have your say in the final product. Youll work alongside a great team in an open, collaborative environment. We are part of Vimo, a well-funded, stable mid-size company with excellent salaries, medical/dental/vision coverage, and perks. Vimo is an Equal Opportunity Employer.",,,,"['python', 'Langchain', 'Neural Networks', 'LLM', 'Linux', 'Data Structures', 'Natural Language Processing', 'Jupyter Notebook', 'Machine Learning', 'Deep Learning', 'Numpy', 'Data Science', 'pandas', 'Nltk', 'Langgraph', 'Transformers', 'BERT', 'langsmith']",2025-06-13 05:56:35
Gen AI and Machine Learning Data Scientist,Diverse Lynx,4 - 5 years,Not Disclosed,['Hyderabad'],"Hiring for Gen AI and Machine Learning Data Scientist-Pan India-\nProficiency in programming languages such as Python or R .\nGood understanding of statistical principles for ML applications\nExperience with data manipulation and analysis using libraries such as pandas, NumPy, and SciPy.\nExceptional knowledge of Deep Learning techniques , model architectures, and parameter fine-tuning\nVery good knowledge of GenAI with good understanding of foundation models and transformer architecture\nFamiliarity with data visualization tools (e.g., Matplotlib, Seaborn).\nExperience with SQL and relational databases.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['deep learning', 'Architecture', 'data manipulation', 'Machine learning', 'Programming', 'data visualization', 'SQL', 'Python']",2025-06-13 05:56:36
Data Scientist,Virtana Corp,3 - 8 years,Not Disclosed,"['Pune', 'Chennai']","Position Overview:\nWe are seeking a Data Scientist Engineer with experience bringing highly scalable enterprise SaaS applications to market. This is a uniquely impactful opportunity to help drive our business forward and directly contribute to long-term growth at Virtana.\nIf you thrive in a fast-paced environment, take initiative, embrace proactivity and collaboration, and you re seeking an environment for continuous learning and improvement, we d love to hear from you!\nVirtana is a remote first work environment so you ll be able to work from the comfort of your home while collaborating with teammates on a variety of connectivity tools and technologies.\nJob Location- Pune/ Chennai/ Remote\nRole Responsibilities:\nResearch and test machine learning approaches for analyzing large-scale distributed computing applications.\nImplement different models AI and ML algorithms for prototype and production systems.\nTest and refine the models and algorithms with live customer data to improve accuracy and efficacy.\nWork with other functional teams to integrate implemented systems into the SaaS platform\nSuggest innovative and creative concepts and ideas that would improve the overall platform\nQualifications:\nThe ideal candidate must have the following qualifications:\n3+ years experience in practical implementation and deployment of ML based systems preferred.\nBS/B Tech or M Tech/ MS (preferred) in Applied Mathematics or Statistics, or CS/Engineering with strong mathematical/statistical background\nStrong quantitative and analytical skills, especially statistical and ML techniques, including familiarity with different supervised and unsupervised learning algorithms\nImplementation experiences and deep knowledge of Classification, Time Series Analysis, Pattern Recognition, Reinforcement Learning, Deep Learning, Dynamic Programming and Optimization\nExperience in working on modeling graph structures related to spatiotemporal systems\nProgramming skills in Python\nExperience in developing and deploying on cloud (AWS or Google or Azure)\nExperience in understanding and usage of LLM models and Prompt engineering is preferred.\nGood verbal and written communication skills\nFamiliarity with well-known ML frameworks such as Pandas, Keras and TensorFlow",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Usage', 'Prototype', 'Time series analysis', 'Artificial Intelligence', 'IT operations management', 'Machine learning', 'Cloud', 'Cash flow', 'Pattern recognition', 'Python']",2025-06-13 05:56:38
Data Scientist,Mindpro Technologies,4 - 9 years,5-12 Lacs P.A.,"['Karur', 'Dharwad']","Greetings From Mind Pro Technologies Pvt ltd (www.mindprotech.com)\n\nJob Title : Data Scientist\nWork Location : Karur (Tamil Nadu) or Dharwad (Karnataka )\nNp : 15days or Less\n\n\nJOB DESCRIPTION:\n Must have At least 4+ Years of experience in Python with Data Science.\n Must have worked on at least one Live project.\nExperience in relevant field such as Statistics, Computer Science or Applied Math or Operational Research.\nMust have Masters in (Maths/Statistics or Applied Mathematics/Machine Learning etc.)\nHistory of successfully performing customer implementations\nStrong customer facing skills, and previous consulting experience.\nExperience of handling high frequency streaming data for real time analysis and reporting.\nFamiliarity with - Natural Language Processing, Statistical Analysis (distribution analysis, correlation, variance, deep learning.\nExperience in tools like AWS, IBM Watson is a plus.\nExperience with open source technologies is a must.\nExcellent communication\nAbility to lead & build strong teams\nAbility to work in an ambiguous environment\n\nDesired Skills and Experience\nLanguages/Tools: Python/R.\nApproaches: Machine Learning\nConcepts: Supervised ANN, Bayesian, Gaussian, Vector Quantization, Logistic Model, Statistical, Predictive Modeling, Minimum Message Length, SVM, Random Forest, Ensembles, ANOVA, Decision Trees, Hidden Markov Models\nUnsupervised ANN, ARL, Clustering Hierarchical, Cluster Analysis\nReinforcement\nGen AI, LLM, LSTM, RNN, CNN, KNN\nBig Data (Good to have): Hadoop /Kafka / Storm / Spark streaming\nOS: Linux, Windows 32/64 bits.\n\nNote:  should know supervised and unsupervised learning,   semi-supervised learning, neural networks concepts, and how ML algorithms works with training and testing data. Experience on particular data set to train, test and roll-out for production use\n\nTool sets : Python, R, MATLAB or  any AI frame work, Neural network, Gen AI, LLM\nContact Details:\n\nRecruitment Team\nMindpro Technologies Pvt Ltd (www.mindprotech.com)\n+91-04324-240904 / +91-9600672304",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Gen AI', 'Statistical Modeling', 'LLM', 'Predictive Modeling', 'Artificial Intelligence', 'Natural Language Processing', 'Neural Networks', 'Machine Learning', 'Deep Learning', 'Python']",2025-06-13 05:56:40
Data Scientist,Callaway Digital Technologies,6 - 9 years,Not Disclosed,['Hyderabad'],"JOB OVERVIEW\nThe ideal candidate will be responsible for analyzing and interpreting large data sets related to finance, sales and supply chain operations to optimize business processes, identify opportunities for improvement, and provide strategic insights to support decision-making. The Data Scientist will work closely with cross-functional teams to identify key business questions, design and implement statistical models, and develop innovative data-driven solutions.\nKey Responsibilities:",,,,"['Statistical Modeling', 'Machine Learning', 'Python', 'Data Visualization', 'Azzure', 'R Program', 'SQL']",2025-06-13 05:56:42
Data Scientist,PS Human Resources And Consultants,3 - 6 years,7.5-15 Lacs P.A.,['Pune'],"Data Scientist\n\n\n\n\nResponsibilities\nDesign and implement AI agent workflows. Develop end-to-end intelligent pipelines and multi-agent systems (e.g., LangGraph/LangChain workflows) that coordinate multiple LLM-powered agents to solve complex tasks. Create graph-based or state-machine architectures for AI agents, chaining prompts and tools as needed.\n\nBuild and fine-tune generative models. Develop, train, and fine-tune advanced generative models (transformers, diffusion models, VAEs, GANs, etc.) on domain-specific data. Deploy and optimize foundation models (such as GPT, LLaMA, Mistral) in production, adapting them to our use cases through prompt engineering and supervised fine-tuning.\n\nDevelop data pipelines. Build robust data collection, preprocessing, and synthetic data generation pipelines to feed training and inference workflows. Implement data cleansing, annotation, and augmentation processes to ensure high-quality inputs for model training and evaluation.\n\nImplement LLM-based agents and automation. Integrate generative AI agents (e.g., chatbots, AI copilots, content generators) into business processes to automate data processing and decision-making tasks. Use Retrieval-Augmented Generation (RAG) pipelines and external knowledge sources to enhance agent capabilities. Leverage multimodal inputs when applicable.\n\nOptimize performance and safety. Continuously evaluate and improve model/system performance. Use GenAI-specific benchmarks and metrics (e.g., BLEU, ROUGE, TruthfulQA) to assess results, and iterate to optimize accuracy, latency, and resource efficiency. Implement safeguards and monitoring to mitigate issues like bias, hallucination, or inappropriate outputs.\n\nCollaborate and document. Work closely with product managers, engineers, and other stakeholders to gather requirements and integrate AI solutions into production systems. Document data workflows, model architectures, and experimentation results. Maintain code and tooling (prompt libraries, model registries) to ensure reproducibility and knowledge sharing.\n\nRequired Skills & Qualifications\nEducation: Bachelors or Masters degree in Computer Science, Data Science, Artificial Intelligence, or a related quantitative fieldanalyticsvidhya.com (or equivalent practical experience). A strong foundation in algorithms, statistics, and software engineering is expected.\n\nProgramming proficiency: Expert-level skills in Pythoncoursera.org, with hands-on experience in machine learning and deep learning frameworks (PyTorch, TensorFlow)analyticsvidhya.com. Comfortable writing production-quality code and using version control, testing, and code review workflows.\n\nGenerative model expertise: Demonstrated ability to build, fine-tune, and deploy large-scale generative modelsanalyticsvidhya.com. Familiarity with transformer architectures and generative techniques (LLMs, diffusion models, GANs)analyticsvidhya.comanalyticsvidhya.com. Experience working with model repositories and fine-tuning frameworks (Hugging Face, etc.).\n\nLLM and agent frameworks: Strong understanding of LLM-based systems and agent-oriented AI patterns. Experience with frameworks like LangGraph/LangChain or similar multi-agent platformsgyliu513.medium.com. Knowledge of agent communication standards (e.g., MCP/Agent Protocol)gyliu513.medium.comblog.langchain.dev to enable interoperability between AI agents.\n\nAI integration and MLOps: Experience integrating AI components with existing systems via APIs and services. Proficiency in retrieval-augmented generation (RAG) setups, vector databases, and prompt engineeringanalyticsvidhya.com. Familiarity with machine learning deployment and MLOps tools (Docker, Kubernetes, MLflow, KServe, etc.) for managing end-to-end automation and scalable workflowsanalyticsvidhya.com.\n\nFamiliarity with GenAI tools: Hands-on experience with state-of-the-art GenAI models and APIs (OpenAI GPT, Anthropic, Claude, etc.) and with popular libraries (Hugging Face Transformers, LangChain, etc.). Awareness of the current GenAI tooling ecosystem and best practices.\n\nSoft skills: Excellent problem-solving and analytical abilities. Strong communication and teamwork skills to collaborate across data, engineering, and business teams. Attention to detail and a quality-oriented mindset. (See Ideal Candidate below for more on personal attributes.)\n\nIdeal Candidate:\n\nInnovative, problem-solver: You are a creative thinker who enjoys tackling open-ended challenges. You have a solutions-oriented mindset and proactively experiment with new ideas and techniquesanalyticsvidhya.com.\n\nSystems thinker: You understand how different components (data, models, services) fit together in a large system. You can architect end-to-end AI solutions with attention to reliability, scalability, and integration points.\n\nCollaborative communicator: You work effectively in multidisciplinary teams. You are able to explain complex technical concepts to non-technical stakeholders and incorporate feedback. You value knowledge sharing and mentorship.\n\nAdaptable learner: The generative AI landscape evolves rapidly. You are passionate about staying current with the latest research and tools. You embrace continuous learning and are eager to upskill and try new libraries or platformsanalyticsvidhya.com.\n\nEthical and conscientious: You care about the real-world impact of AI systems. You take responsibility for the quality and fairness of models, and proactively address concerns like data privacy, bias, and security.",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Langchain', 'Machine Learning', 'AI Agent workflo', 'Python', 'Langgraph']",2025-06-13 05:56:44
Data Scientist,An Indian NBFC,3 - 8 years,Not Disclosed,['Chennai'],"Responsibilities:\nCollect, clean, and analyze large sets of structured and unstructured data to extract meaningful insights and trends\nDevelop and implement advanced machine learning algorithms to solve complex business problems\nSupport moving models to production, by creating high quality code modules that can be seamlessly integrated into existing systems (both on-prem and cloud)\nCommunicate complex findings to both technical and non-technical audiences through effective data visualization and storytelling.\nCollaborate with cross-functional teams to identify data-driven opportunities and translate business requirements into actionable data solutions.\nSupport the development and maintenance of data pipelines and infrastructure\nStay up-to-date with industry trends and advancements in Data Science and Machine Learning technologies.\n\nSkills Required:\nStrong foundation in statistics, and machine learning algorithms\nStrong proficiency in programming languages like Python and SQL.\nExcellent problem-solving and analytical skills.\nAbility to work independently and as part of a team.\nShould have built production models using at least 2 of the ML techniques: Clustering, Regression, Classification\nExperience in Banking & Financial Services is preferred.\nExperience working on cloud platforms (e.g., AWS, GCP) is preferred.\nA passion for data and a curiosity to explore new trends and technologies",Industry Type: NBFC,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Pipeline', 'Data Extraction', 'Model Building', 'Artificial Intelligence', 'Cloud', 'Machine Learning']",2025-06-13 05:56:45
Data Scientist,Simplify Healthcare,5 - 10 years,Not Disclosed,"['Pune( Hadapsar, Kharadi, Keshav Nagar, Vishrantwadi, Dhanori, Mundhwa, Viman Nagar )']","Engineer/Sr. Engineer Data Science\nLocation: Pune, India\n\nCompany Overview:\nSimplify Healthcare is one of the fastest-growing healthcare technology solutions providers serving the US health insurance (Payer) industry. Headquartered in Chicago with a Global Delivery Centre in Pune, we are trusted by 65+ payer organizations and supported by a team of 800+ professionals.\nWe specialize in delivering SaaS-based enterprise software solutions focused on product and benefits configuration, provider lifecycle management, and more. In 2023, we launched Simplify Health Cloud, our flagship Payer Platform, establishing our position as a leader in cloud-native, low-code configurable platforms for the healthcare sector.\nWith our strategic acquisition of Virtical.ai in 2024, we’re accelerating innovation through AI integration, particularly in areas such as LLMs, conversational AI, and cloud-based intelligence. Our proprietary Simplify App Fabric™ enables fast, secure, and low-code development for modern Payer solutions.\nOur innovation has earned us repeated recognition in Deloitte Technology Fast 500™, Inc. 5000, and reports by IDC and Gartner.",,,,"['Speech Recognition', 'Artificial Intelligence', 'Natural Language Processing', 'Conversational Ai', 'Chatbot', 'Cognitive Services', 'Text Mining', 'Machine Learning', 'Azure Cognitive Services']",2025-06-13 05:56:47
Data Scientist,Dwplacesolutions,3 - 5 years,Not Disclosed,['Bengaluru'],We are seeking an experienced Data Scientist to join our team.\nThe ideal candidate will have a strong background in developing and deploying\nconversational AI solutions using Large Language Models (LLMs) and RASA\nframework.,Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Machine Learning', 'Tensorflow', 'R', 'Artificial Intelligence', 'Natural Language Processing', 'Neural Networks', 'Chatbot', 'Deep Learning', 'Python']",2025-06-13 05:56:49
Data Scientist,Jsg. Consulting. Pvt.Ltd.,3 - 5 years,9.6-10.8 Lacs P.A.,['Jaipur'],"Familiarity with MDM (Meter Data Management), HES, and utility billing systems.\nExposure to AMI events analysis, load curves, and customer behavior analytics.\nKnowledge of regulatory requirements, data retention, and data .",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['billing exceptions', 'load profiling', 'Machine Learning', 'Meter Data Management', 'Smart Metering', 'Hes']",2025-06-13 05:56:50
Senior Data Scientist | Snowflakes | Tableau | AI/ML,Cisco,0 - 2 years,Not Disclosed,['Bengaluru'],"Job posting may be removed earlier if the position is filled or if a sufficient number of applications are received.\n\nMeet the Team\n\nWe are a dynamic and innovative team of Data Engineers, Data Architects, and Data Scientists based in Bangalore, India. Our mission is to harness the power of data to provide actionable insights that empower executives to make informed, data-driven decisions. By analyzing and interpreting complex datasets, we enable the organization to understand the health of the business and identify opportunities for growth and improvement.\n\nYour Impact\n\nWe are seeking a highly experienced and skilled Senior Data Scientist to join our dynamic team. The ideal candidate will possess deep expertise in machine learning models, artificial intelligence (AI), generative AI, and data visualization. Proficiency in Tableau and other visualization tools is essential. This role requires hands-on experience with databases such as Snowflake and Teradata, as well as advanced knowledge in various data science and AI techniques. The successful candidate will play a pivotal role in driving data-driven decision-making and innovation within our organization.\n\nKey Responsibilities\nDesign, develop, and implement advanced machine learning models to solve complex business problems.\nApply AI techniques and generative AI models to enhance data analysis and predictive capabilities.\nUtilize Tableau and other visualization tools to create insightful and actionable dashboards for stakeholders.\nManage and optimize large datasets using Snowflake and Teradata databases.\nCollaborate with cross-functional teams to understand business needs and translate them into analytical solutions.\nStay updated with the latest advancements in data science, machine learning, and AI technologies.\nMentor and guide junior data scientists, fostering a culture of continuous learning and development.\nCommunicate complex analytical concepts and results to non-technical stakeholders effectively.\nKey Technologies &\n\nSkills:\nMachine Learning ModelsSupervised learning, unsupervised learning, reinforcement learning, deep learning, neural networks, decision trees, random forests, support vector machines (SVM), clustering algorithms, etc.\nAI TechniquesNatural language processing (NLP), computer vision, generative adversarial networks (GANs), transfer learning, etc.\nVisualization ToolsTableau, Power BI, Matplotlib, Seaborn, Plotly, etc.\nDatabasesSnowflake, Teradata, SQL, NoSQL databases.\nProgramming LanguagesPython (essential), R, SQL.\nPython LibrariesTensorFlow, PyTorch, scikit-learn, pandas, NumPy, Keras, SciPy, etc.\nData ProcessingETL processes, data warehousing, data lakes.\nCloud PlatformsAWS, Azure, Google Cloud Platform.\nMinimum Qualifications\nBachelor's or Master's degree in Computer Science, Statistics, Mathematics, Data Science, or a related field.\nMinimum of [X] years of experience as a Data Scientist or in a similar role.\nProven track record in developing and deploying machine learning models and AI solutions.\nStrong expertise in data visualization tools, particularly Tableau.\nExtensive experience with Snowflake and Teradata databases.\nExcellent problem-solving skills and the ability to work independently and collaboratively.\nExceptional communication skills with the ability to convey complex information clearly.\nPreferred Qualifications (Provide up to five (5) bullet points these can include soft skills)\nExcellent communication and collaboration skills to work effectively in cross-functional teams.\nAbility to translate business requirements into technical solutions.\nStrong problem-solving skills and the ability to work with complex datasets.\nExperience in statistical analysis and machine learning techniques.\nUnderstanding of business domains such as sales, financials, marketing, and telemetry.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['machine learning', 'artificial intelligence', 'sql', 'tableau', 'data visualization', 'snowflake', 'scipy', 'python', 'scikit-learn', 'data warehousing', 'numpy', 'pandas', 'tensorflow', 'data integration tools', 'matplotlib', 'pytorch', 'keras', 'machine learning algorithms', 'etl', 'nosql databases']",2025-06-13 05:56:52
Data Scientist Sr. Analyst,Accenture,5 - 10 years,Not Disclosed,['Kochi'],"Job Title - + +\n\n\n\nManagement Level:\n\n\n\nLocation:Kochi, Coimbatore, Trivandrum\n\n\n\nMust have skills:Big Data, Python or R\n\n\n\n\nGood to have skills:Scala, SQL\n\n\n\nJob\n\n\nSummary\n\nA Data Scientist is expected to be hands-on to deliver end to end vis a vis projects undertaken in the Analytics space. They must have a proven ability to drive business results with their data-based insights. They must be comfortable working with a wide range of stakeholders and functional teams. The right candidate will have a passion for discovering solutions hidden in large data sets and working with stakeholders to improve business outcomes.\n\n\n\nRoles and Responsibilities\nIdentify valuable data sources and collection processes\nSupervise preprocessing of structured and unstructured data\nAnalyze large amounts of information to discover trends and patterns for insurance industry.\nBuild predictive models and machine-learning algorithms\nCombine models through ensemble modeling\nPresent information using data visualization techniques\nCollaborate with engineering and product development teams\nHands-on knowledge of implementing various AI algorithms and best-fit scenarios\nHas worked on Generative AI based implementations\n\n\n\nProfessional and Technical Skills\n3.5-5 years experience in Analytics systems/program delivery; at least 2 Big Data or Advanced Analytics project implementation experience\nExperience using statistical computer languages (R, Python, SQL, Pyspark, etc.) to manipulate data and draw insights from large data sets; familiarity with Scala, Java or C++\nKnowledge of a variety of machine learning techniques (clustering, decision tree learning, artificial neural networks, etc.) and their real-world advantages/drawbacks\nKnowledge of advanced statistical techniques and concepts (regression, properties of distributions, statistical tests and proper usage, etc.) and experience with applications\nHands on experience in Azure/AWS analytics platform (3+ years)\nExperience using variations of Databricks or similar analytical applications in AWS/Azure\nExperience using business intelligence tools (e.g. Tableau) and data frameworks (e.g. Hadoop)\nStrong mathematical skills (e.g. statistics, algebra)\nExcellent communication and presentation skills\nDeploying data pipelines in production based on Continuous Delivery practices.\n\n\n\n\nAdditional Information\nMulti Industry domain experience\nExpert in Python, Scala, SQL\nKnowledge of Tableau/Power BI or similar self-service visualization tools\nInterpersonal and Team skills should be top notch\nNice to have leadership experience in the past\nQualification\n\n\n\nExperience:3.5 -5 years of experience is required\n\n\n\n\nEducational Qualification:Graduation (Accurate educational details should capture)",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'scala', 'sql', 'r', 'big data', 'advanced analytics', 'mathematics', 'data manipulation', 'presentation skills', 'microsoft azure', 'pyspark', 'power bi', 'machine learning', 'javascript', 'aws kinesis', 'tableau', 'decision tree', 'java', 'hadoop', 'data visualization', 'aws', 'statistics']",2025-06-13 05:56:54
Senior Data Scientist,Ericsson,3 - 8 years,Not Disclosed,['Bengaluru'],"About this Opportunity\nThe complexity of running and optimizing the next generation of wireless networks, such as 5G with distributed edge compute, will require Machine Learning (ML) and Artificial Intelligence (AI) technologies. Ericsson is setting up an AI Accelerator Hub in India to fast-track our strategy execution, using Machine Intelligence (MI) to drive thought leadership, automate, and transform Ericsson s offerings and operations. We collaborate with academia and industry to develop state-of-the-art solutions that simplify and automate processes, creating new value through data insights.\n\nAs a Senior Data Scientist, you will apply your knowledge of data science and ML tools backed with strong programming skills to solve real-world problems.\nResponsibilities:\n1. Lead AI/ML features/capabilities in product/business areas\n2. Define business metrics of success for AI/ML projects and translate them into model metrics\n3. Lead end-to-end development and deployment of Generative AI solutions for enterprise use cases\n4. Design and implement architectures for vector search, embedding models, and RAG systems\n5. Fine-tune and evaluate large language models (LLMs) for domain-specific tasks\n6. Collaborate with stakeholders to translate vague problems into concrete Generative AI use cases\n7. Develop and deploy generative AI solutions using AWS services such as SageMaker, Bedrock, and other AWS AI tools. Provide technical expertise and guidance on implementing GenAI models and best practices within the AWS ecosystem.\n8. Develop secure, scalable, and production-grade AI pipelines\n9. Ensure ethical and responsible AI practices\n10. Mentor junior team members in GenAI frameworks and best practices\n11. Stay current with research and industry trends in Generative AI and apply cutting-edge techniques\n12. Contribute to internal AI governance, tooling frameworks, and reusable components\n13. Work with large datasets including petabytes of 4G/5G networks and IoT data\n14. Propose/select/test predictive models and other ML systems\n15. Define visualization and dashboarding requirements with business stakeholders\n16. Build proof-of-concepts for business opportunities using AI/ML\n17. Lead functional and technical analysis to define AI/ML-driven business opportunities\n18. Work with multiple data sources and apply the right feature engineering to AI models\n19. Lead studies and creative usage of new/existing data sources",,,,"['Wireless', 'Computer science', 'Data analysis', 'cassandra', 'Neural networks', 'Artificial Intelligence', 'Machine learning', 'Telecommunication', 'data visualization', 'Python']",2025-06-13 05:56:56
"SENIOR, DATA SCIENTIST",Walmart,3 - 8 years,Not Disclosed,['Bengaluru'],"Position Summary...\nWhat youll do...\nAbout Team\nThe Catalog Data Science Team at Walmart Global Tech is focused on using the latest research in generative AI (GenAI), artificial intelligence (AI), machine learning (ML), statistics, deep learning, computer vision and optimization to implement solutions that ensure Walmart s product catalog is accurate, complete, and optimized for customer experience. Our team tackles complex data science and ML engineering challenges related to product classification, attribute extraction, trust & safety, and catalog optimization, empowering next-generation retail use cases.\nThe Data Science and ML Engineering community at Walmart Global Tech is active in most of the Hack events, utilizing the petabytes of data at our disposal, to build some of the coolest ideas. All the work we do at Walmart Global Tech will eventually benefit our operations & our associates, helping Customers Save Money to Live Better.\nWhat youll do:\nAs a Senior Data Scientist - ML Engineer, you ll have the opportunity to:\nDrive research initiatives and proof-of-concepts that push the state of the art in generative AI and large-scale machine learning.\nDesign and implement high-throughput, low-latency AI/ML pipelines and microservices that operate at global scale.\nOversee data ingestion, model training, evaluation, deployment and monitoring-ensuring performance, quality and reliability.\nCustomize and optimize LLMs for specific business use cases, balancing accuracy, latency and cost.\nPrototype novel generative AI solutions, integrate advancements into production, and collaborate with research partners.\nChampion best practices in data quality, lineage, governance and cost optimization across ML pipelines.\nMentor a team of ML engineers, establish coding standards, conduct design reviews, and foster a culture of continuous improvement.\nPresent your team s work at top-tier AI/ML conferences, publish scientific papers, and cultivate partnerships with universities and research labs.\nWhat youll bring\nPhD in Computer Science, Statistics, Applied Mathematics or related field with 3+ years experience in ML engineering-or Master s with 6+ years or Bachelor s with 8+ years.\nProven track record of leading and scaling AI/ML products in production environments.\nDeep expertise in generative AI, large-scale model deployment, and fine-tuning of transformer-based architectures.\nStrong programming skills in Python, or equivalent, and experience with big data frameworks (Spark, Hadoop) and ML platforms (TensorFlow, PyTorch).\nDemonstrated history of scientific publications or patents in AI/ML.\nExcellent communication skills, a growth mindset, and the ability to drive cross-functional collaboration.\nAbout Walmart Global Tech\n.\n.\nFlexible, hybrid work\n.\nBenefits\n.\nBelonging\n.\n.\nEqual Opportunity Employer\nWalmart, Inc., is an Equal Opportunities Employer - By Choice. We believe we are best equipped to help our associates, customers and the communities we serve live better when we really know them. That means understanding, respecting and valuing unique styles, experiences, identities, ideas and opinions - while being inclusive of all people.\nMinimum Qualifications...\nMinimum Qualifications:Option 1- Bachelors degree in Statistics, Economics, Analytics, Mathematics, Computer Science, Information Technology, or related field and 3 years experience in an analytics related field. Option 2- Masters degree in Statistics, Economics, Analytics, Mathematics, Computer Science, Information Technology, or related field and 1 years experience in an analytics related field. Option 3 - 5 years experience in an analytics or related field.\nPreferred Qualifications...\nPrimary Location...\n\n\n",Industry Type: Retail,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Prototype', 'Networking', 'Coding', 'Machine learning', 'Continuous improvement', 'Information technology', 'Monitoring', 'Analytics', 'Python']",2025-06-13 05:56:57
Senior Data Scientist - Multi-Agent AI Systems,Capgemini,9 - 14 years,Not Disclosed,"['Pune', 'Bengaluru']","Role & responsibilities\nWe are seeking an exceptional Data Scientist with specialized expertise in developing multi-agent AI systems. In this role, you will design, implement, and optimize complex AI ecosystems where multiple intelligent agents collaborate to solve sophisticated problems. You will leverage your deep understanding of generative AI, retrieval-augmented generation (RAG), and prompt engineering to create cutting-edge solutions that push the boundaries of artificial intelligence.\nKey Responsibilities\nDesign and develop generative AI-based multi-agent systems that can collaborate, communicate, and coordinate to achieve complex objectives\nArchitect and implement RAG-based chatbot solutions that effectively leverage knowledge bases and external data sources\nCreate sophisticated prompt engineering strategies to optimize AI agent behavior and inter-agent communication\nBuild, train, and fine-tune generative AI models for various applications within multi-agent systems\nDevelop robust evaluation frameworks to measure and improve multi-agent system performance\nImplement efficient knowledge sharing mechanisms between AI agents\nWrite clean, efficient, and well-documented Python code for production-ready AI systems\nCollaborate with cross-functional teams to integrate multi-agent systems into broader product ecosystems\nStay at the forefront of AI research and incorporate state-of-the-art techniques into our solutions\n\nPreferred candidate profile\nMaster's or PhD in Computer Science, Machine Learning, Artificial Intelligence, or related field\n4+ years of professional experience in data science or machine learning engineering\nExtensive experience with Python programming and related data science/ML libraries\nDemonstrated expertise in developing and deploying generative AI models (e.g., LLMs, diffusion models)\nProven experience building RAG-based systems and implementing vector databases\nStrong background in prompt engineering for large language models\nExperience designing and implementing generative AI-based multi-agent architectures\nExcellent problem-solving skills and ability to optimize complex AI systems\n\nPreferred Qualifications\nExperience with LangChain, AutoGPT, CrewAI, or similar frameworks for building agent-based systems\nFamiliarity with orchestration tools for managing complex AI workflows\nKnowledge of agent communication protocols and collaborative problem-solving frameworks\nExperience with distributed systems and cloud computing platforms (AWS, GCP, Azure)\nContributions to open-source AI projects or research publications in relevant fields\nExperience with knowledge graphs and semantic reasoning systems\nFamiliarity with MLOps practices and deployment of AI systems at scale",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Crewai', 'Langchain', 'AutoGPT']",2025-06-13 05:56:59
Senior Data Scientist,Capgemini,5 - 9 years,Not Disclosed,['Gurugram'],"At Capgemini Invent, we believe difference drives change. As inventive transformation consultants, we blend our strategic, creative and scientific capabilities,collaborating closely with clients to deliver cutting-edge solutions. Join us to drive transformation tailored to our client's challenges of today and tomorrow.Informed and validated by science and data. Superpowered by creativity and design. All underpinned by technology created with purpose.\n\n \n\nYour role \n\nAs a Senior Data Scientist, you are expected to develop and implement Artificial Intelligence based solutions across various disciplines for the Intelligent Industry vertical of Capgemini Invent. You are expected to work as an individual contributor or along with a team to help design and develop ML/NLP models as per the requirement. You will work closely with the Product Owner, Systems Architect and other key stakeholders right from conceptualization till the implementation of the project. You should take ownership while understanding the client requirement, the data to be used, security & privacy needs and the infrastructure to be used for the development and implementation.\n\nThe candidate will be responsible for executing data science projects independently to deliver business outcomes and is expected to demonstrate domain expertise, develop, and execute program plans and proactively solicit feedback from stakeholders to identify improvement actions. This role requires a strong technical background, excellent problem-solving skills, and the ability to work collaboratively with stakeholders from different functional and business teams.\nThe role also requires the candidate to collaborate on ML asset creation and eager to learn and impart trainings to fellow data science professionals. We expect thought leadership from the candidate, especially on proposing to build a ML/NLP asset based on expected industry requirements. Experience in building Industry specific (e.g. Manufacturing, R&D, Supply Chain, Life Sciences etc), production ready AI Models using microservices and web-services is a plus.\n\nProgramming Languages Python NumPy, SciPy, Pandas, MatPlotLib, Seaborne\nDatabases RDBMS (MySQL, Oracle etc.), NoSQL Stores (HBase, Cassandra etc.)\nML/DL Frameworks SciKitLearn, TensorFlow (Keras), PyTorch,\nBig data ML Frameworks - Spark (Spark-ML, Graph-X), H2O\nCloud Azure/AWS/GCP\n\n \n\nYour Profile \n\nPredictive and Prescriptive modelling using Statistical and Machine Learning algorithms including but not limited to Time Series, Regression, Trees, Ensembles, Neural-Nets (Deep & Shallow CNN, LSTM, Transformers etc.). Experience with open-source OCR engines like Tesseract, Speech recognition, Computer Vision, face recognition, emotion detection etc. is a plus.\nUnsupervised learning Market Basket Analysis, Collaborative Filtering, Dimensionality Reduction, good understanding of common matrix decomposition approaches like SVD. Various Clustering approaches Hierarchical, Centroid-based, Density-based, Distribution-based, Graph-based clustering like Spectral.\nNLP Information Extraction, Similarity Matching, Sentiment Analysis, Text Clustering, Semantic Analysis, Document Summarization, Context Mapping/Understanding, Intent Classification, Word Embeddings, Vector Space Models, experience with libraries like NLTK, Spacy, Stanford Core-NLP is a plus. Usage of Transformers for NLP and experience with LLMs like (ChatGPT, Llama) and usage of RAGs (vector stores like LangChain & LangGraps), building Agentic AI applications.\nModel Deployment ML pipeline formation, data security and scrutiny check and ML-Ops for productionizing a built model on-premises and on cloud.\n\nRequired Qualifications\nMasters degree in a quantitative field such as Mathematics, Statistics, Machine Learning, Computer Science or Engineering or a bachelors degree with relevant experience.\nGood experience in programming with languages such as Python/Java/Scala, SQL and experience with data visualization tools like Tableau or Power BI.\n\nPreferred Experience\nExperienced in Agile way of working, manage team effort and track through JIRA\nExperience in Proposal, RFP, RFQ and pitch creations and delivery to the big forum.\nExperience in POC, MVP, PoV and assets creations with innovative use cases\nExperience working in a consulting environment is highly desirable.\nPresupposition\n\nHigh Impact client communication\nThe job may also entail sitting as well as working at a computer for extended periods of time. Candidates should be able to effectively communicate by telephone, email, and face to face.\n\n \n\nWhat you will love about working here \nWe recognize the significance of flexible work arrangements to provide support. Be it remote work, or flexible work hours, you will get an environment to maintain healthy work life balance.\nAt the heart of our mission is your career growth. Our array of career growth programs and diverse professions are crafted to support you in exploring a world of opportunities.\nEquip yourself with valuable certifications in the latest technologies such as Generative AI.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['numpy', 'sql', 'java', 'python', 'pandas', 'scala', 'poc', 'nltk', 'dl', 'artificial intelligence', 'tensorflow', 'spacy', 'spark', 'gcp', 'pytorch', 'keras', 'mysql', 'hbase', 'ml', 'jira', 'scipy', 'rdbms', 'oracle', 'mvp', 'microsoft azure', 'power bi', 'nosql', 'tableau', 'cassandra', 'matplotlib', 'agile', 'aws']",2025-06-13 05:57:01
Assistant Data Scientist,Rocket Software,0 - 1 years,Not Disclosed,['Pune'],"Face to Face interview in Pune . Please apply only if you are available for a Face to Face interview .\n\nJob highlights\n\nRequired Qualifications . 0 -2 years of relevant industry experience or fresh graduates are welcome to apply.\nBasic experience or understanding in applying Data Science methodologies to extract, process, and transform data from multiple sources.\nPreferred Qualifications . Bachelors degree in Data Science , AI, Statistics ,Computer Science, Economics, or a directly related field.\n\nEssential Duties and Responsibilities\n\nAssist in developing, fine-tuning, and deploying machine learning models.\nAid in consulting with key internal and external stakeholders to understand and frame model requirements and potential applications.\nParticipate in the development of sound analytic plans based on available data sources, business partner needs, and required timelines.\nWork with software engineers in integrating trained models into end-user applications.\nHelp manage deliverables across multiple projects in a deadline-driven environment.\nPresent results, insights, and recommendations to both technical and non-technical stakeholders.\n\nRequired Qualifications\n\n0 -2 years of relevant industry experience or fresh graduates are welcome to apply.\nGood knowledge of Python and Linux, familiarity with ML frameworks, and a willingness to learn.\nDemonstrated problem-solving abilities and creative thinking.\nBasic experience or understanding in applying Data Science methodologies to extract, process, and transform data from multiple sources.\nExcellent communication and interpersonal skills.\nMust be comfortable working in a team-oriented environment.\n\nPreferred Qualifications\n\nBachelor's degree in Statistics, Computer Science, Economics, or a directly related field.\nMasters degree or current enrollment in a Masters program in Statistics, Computer Science, Mathematics, Economics, or directly related fields is a plus.\nDemonstrated passion for continued learning and innovation.\nAs a Data Science Assistant, we expect not just skills and qualifications, but also an enthusiasm for learning and growing within our team. We value those who are adaptable, innovative, and ready to take on challenges in a fast-paced work environment.\n\nDiversity, Inclusion & Equity\n\nAt Rocket we are committed to an inclusive workplace environment, where every Rocketeer can thrive by bringing their full selves to work. Being a Rocketeer means you are part of our movement to continually drive inclusivity, diversity and equity in our workforce.",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'NLP', 'Natural Language Processing', 'Artificial Intelligence', 'Machine Learning', 'Python']",2025-06-13 05:57:03
"PRINCIPAL, DATA SCIENTIST",Walmart,10 - 15 years,Not Disclosed,['Bengaluru'],"Position Summary...\nWhat youll do...\nAbout Team\nWalmart s Enterprise Business Services (EBS) is a powerhouse of several exceptional teams delivering world-class technology solutions and services making a profound impact at every level of Walmart.\nAs a key part of Walmart Global Tech, our teams set the bar for operational excellence and leverage emerging technology to support millions of customers, associates, and stakeholders worldwide. Each time an associate turns on their laptop, a customer makes a purchase, a new supplier is onboarded, the company closes the books, physical and legal risk is avoided, and when we pay our associates consistently and accurately, that is EBS. Joining EBS means embarking on a journey of limitless growth, relentless innovation, and the chance to set new industry standards that shape the future of Walmart.\nWhat you will do\nYou will work with the multiple teams and guide them on technical aspects, set quality standards and participate in design discussion and drive technical decisions\nLead the end-to-end lifecycle of AI/ML projects, from ideation to deployment, ensuring alignment with Walmarts strategic goals.\nDesign and implement scalable cloud-based machine learning and data science solutions, leveraging, GCP, or other cloud platforms.\nDevelop novel algorithms and leverage state-of-the-art AI frameworks (e.g., TensorFlow, PyTorch, HuggingFace) to solve complex problems in indirect procurement optimization, customer personalization, and operational efficiency.\nBuild highly parallelized compute environments for processing large-scale datasets, optimizing performance across CPU and GPU architectures.\nCollaborate with diverse teams across engineering, business, and operations to understand requirements and integrate data science solutions seamlessly.\nAdvocate for best practices in software development, including CI/CD, unit testing, and documentation, to ensure robust and reliable systems.\nMentor junior data scientists and contribute to building a culture of innovation and learning within the data science community at Walmart.\nCode Reviews across teams\nEngage with Product Management and Business to drive the agenda, set your priorities and deliver awesome products.\nDrive design, development, implementation and documentation\nBuild, test and deploy cutting edge solutions at scale, impacting associates of Walmart worldwide.\nInteract with Walmart engineering teams across geographies to leverage expertise and contribute to the tech community.\nDrive the success of the implementation by applying technical skills, to design and build enhanced processes and technical solutions in support of strategic initiatives.\nYou will use your engineering experience and technical skill to develop highly scalable and robust solutions. You will work with Engineering Lead/architect.\nWork closely with the Architects and cross functional teams and follow established practices for the delivery of solutions meeting QCD (Quality, Cost & Delivery). Within the established architectural guidelines.\nWork with senior leadership to chart out the future roadmap of the products\nParticipate in hiring and build teams enabling them to be high performing agile teams.\nYou will help and participate with the teams that leverage and contribute to open source technologies to Make impact on a global scale\nInteract closely for requirements with Business owners and technical teams both within India and across the globe.\nWhat you will bring\nB.Tech. / B.E. / M.Tech. / M.S. in Computer Science or relevant discipline\n10+ years of experience in design and development of highly -scalable applications and platform development\nWork in a highly collaborative environment with a multidisciplinary team.\nWork with senior data scientists to design, architect, and build AI/ML model and model systems.\nWork with machine learning engineers to deploy, operate, and optimize scalable solutions\nWork with product managers to design user journeys, feedback loop and analyze user telemetry.\nCreate opportunities to develop yourself with an end-to-end AI/ML product experience.\nWork with a set of robust work standards to ensure we build trustworthy AI/ML solutions\nHosted & Participated Architecture Review & Design/Code Review events.\nHands on System Designing experience.\nStrong computer science fundamentals: data structures, algorithms, design patterns.\nExtensive hands-on experience building services using these technologies (Scala, Java, Springboot, Microservices ,NodeJs)\nHands-on experience in web technologies like React JS/Angular Js, Java script, Type script, CSS\nGood Knowledge in messaging systems: Kafka/RabbitMQ\nWorking knowledge of SQL and NoSQL database technologies.\nKnowledge on Linux platform\nKnowledge on unit testing frameworks (Junit, Jest , Spock etc) and code quality control platforms like Sonar\nKnowledge on cloud platforms any cloud platforms like IAAS/PAAS\nCI/CD development environments/tools: Git, Maven, Gradle, Docker, Kubernetes, Jenkins, Azure DevOps\nExperience in implementing Distributed Cache(Redis/Hazlecast)\nWell-Versed with Logging and Metrics tools and technologies (ELK/Splunk/Grafana)\nKnowledge in search engines like Lucene/Solr\nDemonstrated end-to-end ownership for development and design of least one cloud based project.\nStrong hands on development skills to prototype technical solutions.\nStrong desire to drive change, and ability to adapt to change quickly. Willing to learn new and emerging technologies.\nExceptional communication and interpersonal skills - including negotiation, facilitation, and consensus building skills; ability to influence and persuade, without direct control.\nPractitioner of Agile (Scrum) methodology\nAbout Walmart Global Tech\n.\n.\nFlexible, hybrid work\n.\nBenefits\n.\nBelonging\n.\n.\nEqual Opportunity Employer\nWalmart, Inc., is an Equal Opportunities Employer - By Choice. We believe we are best equipped to help our associates, customers and the communities we serve live better when we really know them. That means understanding, respecting and valuing unique styles, experiences, identities, ideas and opinions - while being inclusive of all people.\nMinimum Qualifications...\nMinimum Qualifications:Option 1: Bachelors degree in Statistics, Economics, Analytics, Mathematics, Computer Science, Information Technology or related field and 5 years experience in an analytics related field. Option 2: Masters degree in Statistics, Economics, Analytics, Mathematics, Computer Science, Information Technology or related field and 3 years experience in an analytics related field. Option 3: 7 years experience in an analytics or related field.\nPreferred Qualifications...",Industry Type: Retail,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Product management', 'Maven', 'Linux', 'Networking', 'Data structures', 'Unit testing', 'Open source', 'Information technology', 'Analytics', 'SQL']",2025-06-13 05:57:05
"STAFF, DATA SCIENTIST",Walmart,4 - 9 years,Not Disclosed,['Bengaluru'],"Position Summary... Drives the execution of multiple business plans and projects by identifying customer and operational needs; developing and communicating business plans and priorities; removing barriers and obstacles that impact performance; providing resources; identifying performance standards; measuring progress and adjusting performance accordingly; developing contingency plans; and demonstrating adaptability and supporting continuous learning. Provides supervision and development opportunities for associates by selecting and training; mentoring; assigning duties; building a team-based work environment; establishing performance expectations and conducting regular performance evaluations; providing recognition and rewards; coaching for success and improvement; and ensuring Belonging awareness. Promotes and supports company policies, procedures, mission, values, and standards of ethics and integrity by training and providing direction to others in their use and application; ensuring compliance with them; and utilizing and supporting the Open Door Policy. Ensures business needs are being met by evaluating the ongoing effectiveness of current plans, programs, and initiatives; consulting with business partners, managers, co-workers, or other key stakeholders; soliciting, evaluating, and applying suggestions for improving efficiency and cost-effectiveness; and participating in and supporting community outreach events.\nWhat youll do...\nAbout the Team :\nCentroid team at Walmart serves as the backbone of Walmarts end-to-end supply chain strategy. They are entrusted with the task of designing and implementing a long-term supply chain strategy that uses advanced data analytics and data science. Their primary objective is to ensure that Walmart provides top-tier customer service while supporting the increasing demand over time and simultaneously operating at low and efficient costs.\nThe team utilizes sophisticated data analysis methods to understand patterns, identify potential bottlenecks, and predict future trends. This enables them to optimize processes, make informed business decisions, and enhance overall operational efficiency.\nOne of Centroids key responsibilities also includes the creation of a Digital Twin Simulation platform for Walmarts supply chain. This innovative tool allows the team to test and validate all future strategies and tactical decisions before they are launched operationally. It also enables a deep assessment of long-term strategic sensitivity.\nIn essence, the Centroid teams work is integral to ensuring Walmarts supply chain is robust, flexible, and capable of adapting to ever-changing market demands. Their work helps to keep Walmart at the forefront of retail supply chain management, delivering exceptional service to customers while maintaining efficient operational costs.\nWhat Youll do :\nDevelop and manage advanced data analytics models to optimize supply chain strategies, balancing customer satisfaction with operational cost and asset efficiency.\nLeverage data analytics to identify opportunities for improvement and drive impactful results through collaboration with cross-functional teams.\nEstablish relationships across Walmart functional areas to identify best practices, solicit data/input, coordinate interdisciplinary initiatives, and rally support for data-driven recommendations.\nSecure alignment and support from relevant business partners and management for data-centric projects, leading discussions to drive necessary change.\nUtilize all available data resources effectively to ensure successful project outcomes.\nCommunicate data insights clearly and persuasively through emails, verbal discussions, and presentations, tailoring communication methods to the audience for maximum impact.\nCollaborate with multiple supply chain business teams to proactively identify, assess, and leverage cost-saving and service improvement opportunities through advanced data analytics.\nUtilize advanced analytics models to derive insights that will inform policy design across various supply chain areas, laying out multiple scenarios and performing sensitivity analysis.\nCollaborate with Data Scientists and Engineers to productionize and scale advanced analytics models as needed.\nDevelop and present compelling data-driven narratives/documents/visuals to influence key stakeholders in their decision-making.\nProvide coaching and training support to other team members in the supply chain area, leveraging your expertise in advanced data analytics.\nWhat Youll bring :\nStrong analytical acumen with technical expertise in Advanced Data Analytics and modelling\nExpert in SQL, - BigQuery like cloud data platforms.\nExpert in programming in Python, (or R)\nExperience in using data visualization tools like Tableau and Looker and be able to drive powerful insights.\nExperience working with large data sets and distributed computing tools (Map/Reduce, Hadoop, Hive, and/or Spark)\nExperience in operating from a cloud environment such as Google Could Platform or Microsoft Azure.\nAbility to work in a fast-paced, iterative development environment.\nStrong communication skills, both written and verbal, plus ability to work with cross functional teams of technical and non-technical members.\nStrong ability to understand the business and have good stakeholder management capabilities.\nExperience of working in cross-functional environment and leading or mentoring teams.\nAbout Walmart Global Tech\n.\n.\nFlexible, hybrid work\n.\nBenefits\n.\nBelonging\n.\n.\nEqual Opportunity Employer\nWalmart, Inc., is an Equal Opportunities Employer - By Choice. We believe we are best equipped to help our associates, customers and the communities we serve live better when we really know them. That means understanding, respecting and valuing unique styles, experiences, identities, ideas and opinions - while being inclusive of all people.\nMinimum Qualifications...\nMinimum Qualifications:Option 1: Bachelors degree in Statistics, Economics, Analytics, Mathematics, Computer Science, Information Technology or related field and 4 years experience in an analytics related field. Option 2: Masters degree in Statistics, Economics, Analytics, Mathematics, Computer Science, Information Technology or related field and 2 years experience in an analytics related field. Option 3: 6 years experience in an analytics or related field.\nPreferred Qualifications...\nPrimary Location... G, 1, 3, 4, 5 Floor, Building 11, Sez, Cessna Business Park, Kadubeesanahalli Village, Varthur Hobli , India",Industry Type: Retail,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Supply chain', 'Data analysis', 'Supply chain management', 'Networking', 'Analytical', 'Consulting', 'Programming', 'Analytics', 'Python', 'SQL']",2025-06-13 05:57:07
"STAFF, DATA SCIENTIST",Walmart,5 - 10 years,Not Disclosed,['Bengaluru'],"Position Summary...\nWhat youll do...\nAbout Team\nThe Catalog Data Science Team at Walmart Global Tech is focused on using the latest research in generative AI (GenAI), artificial intelligence (AI), machine learning (ML), statistics, deep learning, computer vision and optimization to implement solutions that ensure Walmart s product catalog is accurate, complete, and optimized for customer experience. Our team tackles complex data science and ML engineering challenges related to product classification, attribute extraction, trust & safety, and catalog optimization, empowering next-generation retail use cases.\nThe Data Science and ML Engineering community at Walmart Global Tech is active in most of the Hack events, utilizing the petabytes of data at our disposal, to build some of the coolest ideas. All the work we do at Walmart Global Tech will eventually benefit our operations & our associates, helping Customers Save Money to Live Better.\nWhat youll do:\nWe are looking for a Staff Machine Learning Engineer who can help build large scale AI/ML/Optimization products. Expected qualities include ability to build, deploy, maintain and troubleshoot large scale systems.\nAs a Staff ML Engineer, you ll have the opportunity to\nDrive research initiatives and proof-of-concepts that push the state of the art in generative AI and large-scale machine learning.\nDesign and implement high-throughput, low-latency AI/ML pipelines and microservices that operate at global scale.\nOversee data ingestion, model training, evaluation, deployment and monitoring-ensuring performance, quality and reliability.\nCustomize and optimize LLMs for specific business use cases, balancing accuracy, latency and cost.\nPrototype novel generative AI solutions, integrate advancements into production, and collaborate with research partners.\nChampion best practices in data quality, lineage, governance and cost optimization across ML pipelines.\nMentor a team of ML engineers, establish coding standards, conduct design reviews, and foster a culture of continuous improvement.\nPresent your team s work at top-tier AI/ML conferences, publish scientific papers, and cultivate partnerships with universities and research labs.\nWhat youll bring:\nPhD in Computer Science, Statistics, Applied Mathematics or related field with 5+ years experience in ML engineering-or Master s with 8+ years or Bachelor s with 10+ years.\nProven track record of leading and scaling AI/ML products in production environments.\nDeep expertise in generative AI, large-scale model deployment, and fine-tuning of transformer-based architectures.\nStrong programming skills in Python, or equivalent, and experience with big data frameworks (Spark, Hadoop) and ML platforms (TensorFlow, PyTorch).\nDemonstrated history of scientific publications or patents in AI/ML.\nExcellent communication skills, a growth mindset, and the ability to drive cross-functional collaboration.\nAbout Walmart Global Tech\n.\n.\nFlexible, hybrid work\n.\nBenefits\n.\nBelonging\n.\n.\nEqual Opportunity Employer\nWalmart, Inc., is an Equal Opportunities Employer - By Choice. We believe we are best equipped to help our associates, customers and the communities we serve live better when we really know them. That means understanding, respecting and valuing unique styles, experiences, identities, ideas and\nMinimum Qualifications...\nMinimum Qualifications:Option 1: Bachelors degree in Statistics, Economics, Analytics, Mathematics, Computer Science, Information Technology or related field and 4 years experience in an analytics related field. Option 2: Masters degree in Statistics, Economics, Analytics, Mathematics, Computer Science, Information Technology or related field and 2 years experience in an analytics related field. Option 3: 6 years experience in an analytics or related field.\nPreferred Qualifications...\nPrimary Location...",Industry Type: Retail,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Prototype', 'Networking', 'Coding', 'Machine learning', 'Continuous improvement', 'Information technology', 'Monitoring', 'Analytics', 'Python']",2025-06-13 05:57:09
Sr. Data Scientist-Stratup-Mid-Size companies Exp.@ Bangalore_Urgent,"A leader in this space, we deliver world...",8 - 13 years,Not Disclosed,['Bengaluru'],"Senior Data Scientist\n\nLocation: Onsite Bangalore\nExperience: 8+ years\n\nRole Overview\n\nWe are seeking a Senior Data Scientist with a strong foundation in machine learning, deep learning, and statistical modeling, with the ability to translate complex operational problems into scalable AI/ML solutions. In addition to core data science responsibilities, the role involves building production-ready backends in Python and contributing to end-to-end model lifecycle management. Exposure to computer vision is a plus, especially for industrial use cases like identification, intrusion detection, and anomaly detection.\n\nKey Responsibilities\n\nDevelop, validate, and deploy machine learning and deep learning models for forecasting, classification, anomaly detection, and operational optimization\nBuild backend APIs using Python (FastAPI, Flask) to serve ML/DL models in production environments\nApply advanced computer vision models (e.g., YOLO, Faster R-CNN) to object detection, intrusion detection, and visual monitoring tasks\nTranslate business problems into analytical frameworks and data science solutions\nWork with data engineering and DevOps teams to operationalize and monitor models at scale\nCollaborate with product, domain experts, and engineering teams to iterate on solution design\nContribute to technical documentation, model explainability, and reproducibility practices\n\n\nRequired Skills\n\nStrong proficiency in Python for data science and backend development\nExperience with ML/DL libraries such as scikit-learn, TensorFlow, or PyTorch\nSolid knowledge of time-series modeling, forecasting techniques, and anomaly detection\nExperience building and deploying APIs for model serving (FastAPI, Flask)\nFamiliarity with real-time data pipelines using Kafka, Spark, or similar tools\nStrong understanding of model validation, feature engineering, and performance tuning\nAbility to work with SQL and NoSQL databases, and large-scale datasets\nGood communication skills and stakeholder engagement experience\n\n\nGood to Have\n\nExperience with ML model deployment tools (MLflow, Docker, Airflow)\nUnderstanding of MLOps and continuous model delivery practices\nBackground in aviation, logistics, manufacturing, or other industrial domains\nFamiliarity with edge deployment and optimization of vision models\n\n\nQualifications\n\nMasters or PhD in Data Science, Computer Science, Applied Mathematics, or related field\n7+ years of experience in machine learning and data science, including end-to-end deployment of models in production",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['scikit-learn', 'time-series modeling', 'ML/DL libraries', 'data science', 'Python', 'Airflow', 'Kafka', 'MLflow', 'logistics', 'anomaly detection', 'aviation', 'SQL', 'PyTorch', 'NoSQL', 'MLOps', 'forecasting techniques', 'Docker', 'manufacturing', 'FastAPI', 'Spark', 'TensorFlow', 'Flask']",2025-06-13 05:57:11
Senior Data Scientist,Cradlepoint,3 - 8 years,Not Disclosed,['Bengaluru'],"Join our Team\nAbout this Opportunity\nThe complexity of running and optimizing the next generation of wireless networks, such as 5G with distributed edge compute, will require Machine Learning (ML) and Artificial Intelligence (AI) technologies. Ericsson is setting up an AI Accelerator Hub in India to fast-track our strategy execution, using Machine Intelligence (MI) to drive thought leadership, automate, and transform Ericsson s offerings and operations. We collaborate with academia and industry to develop state-of-the-art solutions that simplify and automate processes, creating new value through data insights.\nWhat you will do\nAs a Senior Data Scientist, you will apply your knowledge of data science and ML tools backed with strong programming skills to solve real-world problems.\nResponsibilities:\n1. Lead AI/ML features/capabilities in product/business areas\n2. Define business metrics of success for AI/ML projects and translate them into model metrics\n3. Lead end-to-end development and deployment of Generative AI solutions for enterprise use cases\n4. Design and implement architectures for vector search, embedding models, and RAG systems\n5. Fine-tune and evaluate large language models (LLMs) for domain-specific tasks\n6. Collaborate with stakeholders to translate vague problems into concrete Generative AI use cases\n7. Develop and deploy generative AI solutions using AWS services such as SageMaker, Bedrock, and other AWS AI tools. Provide technical expertise and guidance on implementing GenAI models and best practices within the AWS ecosystem.\n8. Develop secure, scalable, and production-grade AI pipelines\n9. Ensure ethical and responsible AI practices\n10. Mentor junior team members in GenAI frameworks and best practices\n11. Stay current with research and industry trends in Generative AI and apply cutting-edge techniques\n12. Contribute to internal AI governance, tooling frameworks, and reusable components\n13. Work with large datasets including petabytes of 4G/5G networks and IoT data\n14. Propose/select/test predictive models and other ML systems\n15. Define visualization and dashboarding requirements with business stakeholders\n16. Build proof-of-concepts for business opportunities using AI/ML\n17. Lead functional and technical analysis to define AI/ML-driven business opportunities\n18. Work with multiple data sources and apply the right feature engineering to AI models\n19. Lead studies and creative usage of new/existing data sources\nWhat you will bring\n1. Bachelors/Masters/Ph.D. in Computer Science, Data Science, AI, ML, Electrical Engineering, or related disciplines from reputed institutes\n2. 3+ years of applied ML/AI production-level experience\n3. Strong programming skills (R/Python)\n4. Proven ability to lead AI/ML projects end-to-end\n5. Strong grounding in mathematics, probability, and statistics\n6. Hands-on experience with data analysis, visualization techniques, and ML frameworks (Python, R, H2O, Keras, TensorFlow, Spark ML)\n7. Experience with semi-structured/unstructured data for AI/ML models\n8. Strong understanding of building AI models using Deep Neural Networks\n9. Experience with Big Data technologies (Hadoop, Cassandra)\n10. Ability to source and combine data from multiple sources for ML models\nPreferred Qualifications:\n1. Good communication skills in English\n2. Certifying MI MOOCs, a plus\n3. Domain knowledge in Telecommunication/IoT, a plus\n4. Experience with data visualization and dashboard creation, a plus\n5. Knowledge of Cognitive models, a plus\n6. Experience in partnering and collaborative co-creation in a global matrix organization.\nWhy join Ericsson\n\n\nWhat happens once you apply\nPrimary country and city: India (IN) || Bangalore\nReq ID: 766481",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Wireless', 'Computer science', 'Data analysis', 'cassandra', 'Neural networks', 'Artificial Intelligence', 'Machine learning', 'Telecommunication', 'data visualization', 'Python']",2025-06-13 05:57:13
Senior Data Scientist,eka.care,3 - 5 years,Not Disclosed,['Bengaluru'],"EkaCare (Orbi Health) is a well-funded startup working on a suite of technologies in the healthcare domain ranging from AI-powered EMR for doctors to one of the most comprehensive personal health record (PHR) applications for consumers. EkaCare seeks enthusiastic senior candidates to develop Large Language Models around medical/clinical data.\n\nWe look forward to a candidate with\nPassion for problem-solving and taking end-to-end ownership of projects\nExtensive knowledge and prior work experience in machine learning (specifically in developing LLMs)\nDesire for a high-paced start-up ride\n\nKey Responsibilities :\nFormulate and implement data-driven solutions in the HealthTech domain:\nBuilding LLMs around healthcare data, wherein the work would involve creating datasets, continual pre-training, supervised fine-tuning, and preference alignment of models.\nDeveloping product-led AI solutions for various healthcare entities.\n\nQualifications / Requirements\nMaster / PhD degree in a relevant academic discipline (Preferred)\n3-5 years of industry experience in building ML production-level pipelines.\nExtensive experience with LLMs (production-level deployment and fine-tuning)\nStrong track record of project delivery\n\nExperience Required: 3-5 years\n\nFull Time Employee Benefits:\nInsurance Benefits - Medical Insurance, Accidental Insurance\nParental Support - Maternity Benefit, Paternity Benefit Program\nMobility Benefits - Relocation benefits, Transfer Support Policy, Travel Policy\nRetirement Benefits - Employee PF Contribution, Flexible PF Contribution, Gratuity, NPS, Leave Encashment\nOther Benefits - Car Lease, Salary Advance Policy",Industry Type: Medical Services / Hospital,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Training', 'Machine learning', 'Leasing', 'Healthcare', 'Deployment', 'Medical insurance', 'Project delivery', 'Supervision', 'clinical data']",2025-06-13 05:57:15
Senior Data Scientist,Hindustan Unilever (HUL),2 - 5 years,Not Disclosed,['Bengaluru'],"Job Title: Senior Data Scientist\nLocation: Bangalore\nJob Title: Assistant Manager - Security Engineering\nLocation: UniOps Bangalore\nABOUT UNILEVER:\nEvery individual here can bring their purpose to life through their work. Join us and you ll be surrounded by inspiring leaders and supportive peers. Among them, you ll channel your purpose, bring fresh ideas to the table, and simply be you. As you work to make a real impact on the business and the world, we ll work to help you become a better you.\nABOUT UNIOPS:\nUnilever Operations (UniOps) is the global technology and operations engine of Unilever offering business services, technology, and enterprise solutions. UniOps serves over 190 locations and through a network of specialized service lines and partners delivers insights and innovations, user experiences and end-to-end seamless delivery making Unilever Purpose Led and Future Fit\nBackground\nFor Unilever to remain competitive in the future, the business needs to continue on the path to become data intelligent. The Data Analytics team will persevere to make Unilever Data Intelligent, powering key decisions with data, insights, advanced analytics and AI. Our ambition is to enable democratization of data, information and insights as a completely agile organization that builds fantastic careers for our people and is accountable for delivering great work that maximizes impact and delivers growth.\nThis Data Analytics function endeavours to create clear accountability for all aspects of Data Strategy, Data Management, Information Management, Analytics, and Insights. We are accountable for impact of solutions, maintaining market relevance and minimising unnecessary overlaps in analytics products, ensuring simplicity and that our solutions better meet the needs of our users. We partner with the Digital and Data Legal Counsel to ensure that our Data Defence (Privacy, Governance, Quality, etc) is well structured and sufficiently robust to use data and AI correctly throughout the enterprise. We democratize information across the business, while supporting the culture shift required for data driven decision making.\nOur vision is to make Unilever data intelligent, partnering with the business to power key decisions with data, advanced analytics and AI to accelerate growth. Our 5 strategies to achieve this are:\nAccelerate simplify access to relevant data, information and insights Build in-house, leading-edge data, information, insights analytics capability Lead the data insights culture and careers to empower employees across Unilever Rapidly embed analytics products, solutions and services to drive growth Advance Information Automation at Scale\nThe Senior Data Scientist is an exciting role in the Data Foundation. This team builds state of the art machine learning algorithms, maximising the impact of analytic solutions in driving enterprise performance. Typical initiatives include optimizing trade promotion investments, accurately forecasting customer demand, using NLP to glean insight on consumer trends from search data, and making individual assortment recommendations for each of the millions of stores that sell Unilever products.\nMain Purpose of the Job:\nThe Senior Data Scientist improves business performance in the functional area of Unilever they serve, through the application of world class data science capability. They own delivery of data science on moderate projects or specific modules of a major global initiative.\nKey accountabilities:\nInteract with relevant teams to identify business challenges where data science can help\nApply comprehensive data science knowledge to propose optimal techniques for key business challenges\nCreate detailed data science proposals and project plans, flagging any limitations of proposed solution\nDesign and prototype experimental solutions, particularly machine learning models\nDesign scaled solutions and ensure high quality and timely delivery\nFacilitate industrialization and ongoing operation of solutions through well organised code, clear documentation and collaboration with ML Ops resources\nGovern the work of 3rd party vendors where needed to support delivery, while maximising creation of Unilever IP\nRepresent Data Science in cross-functional governance of projects, engaging with stakeholders up to Director level\nHighlight recent developments in data science capability which could solve additional challenges\nLead a team of up 1-2 data scientists / interns, providing career mentorship and line management\nProvide technical guidance to data scientists across DA, particularly on the projects you lead\nSupport the growth of DA s data science capability by contributing to activities such as tool and vendor selection, best practice definition, recruitment, and creation of training materials\nBuild the reputation of DA s data science capability within Unilever and externally, through activities such as community engagement (e. g. Yammer), publications or blogs\nProvide ad-hoc immediate support to the business when needed (for example Covid-19 crisis support)\nDepending on the specific project, the Senior Data Scientist can expect 60-90% of their work to be hands-on prototyping solutions, with the remainder spent planning and designing, overseeing and reviewing work of project staff, interfacing with stakeholders and managing team members.\nExperience and qualifications required:\nStandards of Leadership Required in This Role\nPersonal Mastery (Data-science and advanced analytics)\nAgility\nBusiness acumen\nPassion for High Performance\nKey Skills Required\nProfessional Skills\nMachine learning - Expert\nStatistical modelling - Expert\nForecasting - Expert\nOptimisation techniques and tools - Fully Operational\nPython coding - Fully Operational\nData science platform tools e. g. MS Azure, Databricks - Fully Operational\nDeep learning (and applications to NLP Computer Vision) - Fully Operational\nCollaborative development using Git repos - Fully Operational\nAutomated Machine Learning platforms - Foundational knowledge\nWhile a broad data science technical background is required, the role will benefit from deeper skills (for example graduate studies or prior work experience) in one of the following areas, optimization, simulation, forecasting, natural language processing, computer vision or geospatial analysis.\nGeneral Skills\nProject Management - Expert\nCommunication / presentation skills - Expert\n3rd party resource management - Expert\nCPG Industry analytics - Expert\nStrong communication and stakeholder engagement skills are essential, including the ability to influence peers and senior business stakeholders across Unilever.\nRelevant Experience:\nMinimum of B. E. in a relevant technical field (e. g. Computer Science, Engineering, Statistics, Operations Research); preferably a postgraduate (Masters or Doctorate) degree\nAt least 4 years building data science solutions to solve business problems, preferably in the CPG industry (less experience may be acceptable if balanced by strong post-grad qualifications)\nExperience with open source languages (eg. Python) and preferably with distributed computing (PySpark)\nExperience deploying solutions in a modern cloud-based architecture\nExperience managing the work of team members and 3rd party resource vendors\nExperience presenting insights and influencing decisions of senior non-technical stakeholders\nKey interfaces\nInternal\nUnilever operational, marketing, customer development, supply chain, product finance teams\nInternal DA teams (Engagement teams; Data CoE; Solution Factory; BDL Factory; Information Factory; Tech Transformation)\nWider Unilever analytics and data science professionals\nExternal\n3rd party Data Science vendors\nUniversities\nIndustry bodies",Industry Type: Advertising & Marketing,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Supply chain', 'Operations research', 'Automation', 'data science', 'Data management', 'Project management', 'Information management', 'Resource management', 'Forecasting', 'Recruitment']",2025-06-13 05:57:16
Senior Data Scientist with GCP,TVS Next,5 - 7 years,Not Disclosed,['Bengaluru'],"What you’ll do:\nUtilize advanced mathematical, statistical, and analytical expertise to research, collect, analyze, and interpret large datasets from internal and external sources to provide insight and develop data driven solutions across the company\nBuild and test predictive models including but not limited to credit risk, fraud, response, and offer acceptance propensity\nResponsible for the development, testing, validation, tracking, and performance enhancement of statistical models and other BI reporting tools leading to new innovative origination strategies within marketing, sales, finance, and underwriting",,,,"['analytical', 'scikit-learn', 'searching', 'bi', 'pyspark', 'numpy', 'sql', 'analytics', 'apache', 'automation', 'data science', 'spark', 'gcp', 'bigquery', 'data visualization', 'xgboost', 'programming', 'reporting', 'ml', 'advanced analytics', 'python', 'data processing', 'predictive', 'jupyter notebook', 'bert', 'pandas', 'matplotlib', 'statistics']",2025-06-13 05:57:18
Data Scientist-Advanced Analytics,IBM,3 - 7 years,Not Disclosed,['Kochi'],"We are seeking a highly skilled Advanced Analytics Specialist to join our dynamic team. The successful candidate will be responsible for leveraging advanced analytics techniques to derive actionable insights, inform business decisions, and drive strategic initiatives. This role requires a deep understanding of data analysis, statistical modeling, machine learning, and data visualization.\nIn this role, you will be responsible for architecting and delivering AI solutions using cutting-edge technologies, with a strong focus on foundation models and large language models. You will work closely with customers, product managers, and development teams to understand business requirements and design custom AI solutions that address complex challenges. Experience with tools like Github Copilot, Amazon Code Whisperer etc. is desirable.\nSuccess is our passion, and your accomplishments will reflect this, driving your career forward, propelling your team to success, and helping our clients to thrive.\nDay-to-Day Duties:\nProof of Concept (POC) DevelopmentDevelop POCs to validate and showcase the feasibility and effectiveness of the proposed AI solutions. Collaborate with development teams to implement and iterate on POCs, ensuring alignment with customer requirements and expectations.\nHelp in showcasing the ability of Gen AI code assistant to refactor/rewrite and document code from one language to another, particularly COBOL to JAVA through rapid prototypes/ PoC\nDocumentation and Knowledge SharingDocument solution architectures, design decisions, implementation details, and lessons learned. Create technical documentation, white papers, and best practice guides. Contribute to internal knowledge sharing initiatives and mentor new team members.\nIndustry Trends and InnovationStay up to date with the latest trends and advancements in AI, foundation models, and large language models. Evaluate emerging technologies, tools, and frameworks to assess their potential impact on solution design and implementation\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nDevelop and implement advanced analytical models and algorithms to solve complex business problems, analyze large datasets to uncover trends, patterns, and insights that drive business performance.\nCollaborate with cross-functional teams to identify key business challenges and opportunities, Create and maintain data pipelines and workflows to ensure the accuracy and integrity of data, Design and deliver insightful reports and dashboards to communicate findings to stakeholders.\nStay up to date with the latest advancements in analytics, machine learning, and data science. Provide technical expertise and mentorship to junior team members.\nQualificationsBachelor’s or master’s degree in data science, Statistics, Mathematics, Computer Science, or a related field. Proven experience in advanced analytics, data science, or a similar role. Proficiency in programming languages such as Python, R, or SQL. Experience with data visualization tools like Tableau, Power BI, or similar.\nStrong understanding of statistical modelling and machine learning algorithms. Excellent analytical, problem-solving, and critical thinking skills. Ability to communicate complex analytical concepts to non-technical stakeholders. Experience with big data technologies (e.g., Hadoop, Spark) is a plus\n\n\nPreferred technical and professional experience\nFamiliarity with cloud-based analytics platforms (e.g., AWS, Azure).\nKnowledge of natural language processing (NLP) and deep learning techniques.\nExperience with project management and agile methodologies",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analysis', 'machine learning', 'statistical modeling', 'data visualization', 'machine learning algorithms', 'advanced analytics', 'python', 'github', 'natural language processing', 'power bi', 'microsoft azure', 'sql', 'r', 'tableau', 'java', 'data science', 'spark', 'hadoop', 'aws']",2025-06-13 05:57:20
Lead Data Scientist,Grab,8 - 13 years,Not Disclosed,['Bengaluru'],"Get to know the team\nGrabFin is an aggregate of FinTech businesses spread across 6 countries in S.E. Asia, in the Lending, Payments and Insurance domains. We are excited to provide innovative financial services to all participants of the Grab Ecosystem be it our Drivers, Consumers or Merchants. Our products are built on fundamental market insights combined with data science and engineering to bring the best product market fit across the cross section of our user base. This understanding of our ecosystem combined with world class engineering execution continues to create tremendous value for our customers.\nThe data scientist will work in a relatively flat team structure with an independent goal of building and manage critical data science models daily. You can expect to solve hard technical problems and grow into an expert on both batch and real-time Data Science use cases. You will have experience with technology and data science.\nYou will be reporting to Senior Manager, Data Science.\nThis role is onsite based in Bangalore.\nYoull develop credit risk scoring models for consumer loans, including PD, LGD, and collection models. Youll work with alternative data sources to boost model signal and accuracy. Your role will involve full ownership of the end-to-end model lifecycle from building and validation to deployment and maintenance. Youll collaborate with business, risk, and operations teams to shape solutions and influence product strategy with your insights. This is an individual contributor role suited for professionals with 8+ years of experience.\nThe Critical Tasks You Will Perform\nBuild predictive models using a mix of machine learning and traditional analytics methods to segregate between Good vs Bad borrowers\nBuild Machine learning & Deep learning models to estimate losses from of a given portfolio.\nValidate models on new datasets, based on in-market performance.\nEngineer predictive features from internal data assets to build refined customer profiles. Identify external data assets to bring into the model mix.\nDrive model governance by collaborating with risk policy, compliance, and audit teams to ensure adherence to regulatory expectations.\nIdentify model gaps or performance drifts and lead model refresh cycles.\nPresent findings to senior leadership with clear articulation of risk trade-offs and growth.\nTranslate model insights into strategic recommendations (e.g., policy changes, pricing levers, customer targeting strategies).\nSolve previously unsolved analytics problems using best in class data analytics and machine learning methodologies.\nRead more\nSkills you need\nThe Essential Skills You Need\n8+ years of experience.\nStrong understanding of credit business - lifecycle of a loan, collections process, and credit KPIs like NPL, ECL.\nExpert in building machine learning and predictive models in Python and Spark is an absolute must.\nSQL, Presto, Hive proficiency.\nSound knowledge of machine learning concepts. Illustrative machine learning concepts/methods are: Bagging, Boosting, Regularisation, Online Learning, Recommendation Engines\nExperience with LLMs, and Generative AI\nExperience with model deployment pipelines - using MLFlow, Airflow, or other MLOps tools.\nDemonstrated experience building machine learning models\nUnderstand the trade-offs between model performance and our needs.\nStrong problem-solving mindset is critical for success in this role.\nRead more\nWhat we offer\nAbout Grab and Our Workplace\nGrab is Southeast Asias leading superapp. From getting your favourite meals delivered to helping you manage your finances and getting around town hassle-free, weve got your back with everything. In Grab, purpose gives us joy and habits build excellence, while harnessing the power of Technology and AI to deliver the mission of driving Southeast Asia forward by economically empowering everyone, with heart, hunger, honour, and humility.\nRead more\nLife at Grab\nLife at Grab\nWe care about your well-being at Grab, here are some of the global benefits we offer:\nWe have your back with Term Life Insurance and comprehensive Medical Insurance.\nWith GrabFlex, create a benefits package that suits your needs and aspirations.\nCelebrate moments that matter in life with loved ones through Parental and Birthday leave, and give back to your communities through Love-all-Serve-all (LASA) volunteering leave\nWe have a confidential Grabber Assistance Programme to guide and uplift you and your loved ones through lifes challenges.\nWhat we stand for at Grab\nWe are committed to building an inclusive and equitable workplace that enables diverse Grabbers to grow and perform at their best. As an equal opportunity employer, we consider all candidates fairly and equally regardless of nationality, ethnicity, religion, age, gender identity, sexual orientation, family commitments, physical and mental impairments or disabilities, and other attributes that make them unique.\n#LI-DNI\nRead more",Industry Type: IT Services & Consulting,"Department: UX, Design & Architecture","Employment Type: Full Time, Permanent","['Loans', 'data science', 'Machine learning', 'Manager Technology', 'Medical insurance', 'Financial services', 'SQL', 'Python', 'Auditing', 'ECL']",2025-06-13 05:57:22
Senior Data Scientist,Codetru Software Solutions,8 - 12 years,Not Disclosed,['Hyderabad'],"Senior Data ScientistLocation: Hyderabad, IndiaExperience: 8-10 YearsWe are seeking a highly motivated, driven, and experienced Senior Data Scientist to join our dynamic team. As a go-getter with a passion for uncovering insights from complex data, you will play a pivotal role in shaping our data strategy and driving business decisions. The ideal candidate is a proactive problem-solver who thrives in a fast-paced environment and possesses a deep understanding of the entire data lifecycle, from extraction to model deployment.\n\nNote: This is purely a Technical role not Managerial.\n\nKey Responsibilities\nLead and execute end-to-end data science projects, from conception and data collection to model building and delivering actionable insights.\nDesign, build, and maintain robust and scalable ETL pipelines to process large volumes of structured and unstructured data from our data lake.\nUtilize advanced SQL and Python (Pandas, NumPy) for data extraction, manipulation, and in-depth analysis to identify critical trends, patterns, and opportunities.\nDevelop and implement a variety of machine learning algorithms, such as regression, classification, clustering, and forecasting models, to solve key business challenges.\nCreate compelling and intuitive data visualizations and dashboards using tools like Tableau or Power BI to communicate complex findings to both technical and non-technical stakeholders.\nMentor junior data scientists and contribute to the team's technical growth and best practices.\n\nMust-Have Qualifications & Skills\n\nExperience: A minimum of 8-10 years of hands-on experience in a data science or related role.\nSQL and Visualization: Expert-level proficiency in SQL for complex querying and proven experience with data visualization tools such as Tableau, Power BI, or Looker.\nData Engineering: Strong, hands-on experience building and managing ETL processes and working extensively within a data lake environment.\nPython and Data Analysis: Mastery of Python and its core data science libraries, especially Pandas, for data wrangling, exploration, and identifying hidden patterns.\nMachine Learning: In-depth theoretical knowledge and practical application of various ML algorithms, including supervised and unsupervised learning techniques. A portfolio of successfully deployed models is a strong plus.\nAttitude: A proactive, self-starting go-getter with excellent problem-solving skills and the drive to take ownership of projects from start to finish.",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Pandas', 'Fast Api', 'Machine Learning', 'Numpy', 'Python', 'Power Bi', 'Tableau', 'SQL', 'Flask']",2025-06-13 05:57:23
Senior Data Scientist,Virtana Corp,5 - 10 years,Not Disclosed,"['Pune', 'Chennai']","Position Overview:\nWe are seeking a Senior Data Scientist Engineer with experience bringing highly scalable enterprise SaaS applications to market. This is a uniquely impactful opportunity to help drive our business forward and directly contribute to long-term growth at Virtana.\nIf you thrive in a fast-paced environment, take initiative, embrace proactivity and collaboration, and you re seeking an environment for continuous learning and improvement, we d love to hear from you!\nVirtana is a remote first work environment so you ll be able to work from the comfort of your home while collaborating with teammates on a variety of connectivity tools and technologies.\nRole Responsibilities:\nResearch and test machine learning approaches for analyzing large-scale distributed computing applications.\nDevelop production-ready implementations of proposed solutions across different models AI and ML algorithms, including testing on live customer data to improve accuracy, efficacy, and robustness\nWork closely with other functional teams to integrate implemented systems into the SaaS platform\nSuggest innovative and creative concepts and ideas that would improve the overall platform.\nJob Location - Pune, Chennai or Remote\nQualifications:\nThe ideal candidate must have the following qualifications:\n6 + years experience in practical implementation and deployment of large customer-facing ML based systems.\nMS or M Tech (preferred) in applied mathematics/statistics; CS or Engineering disciplines are acceptable but must have with strong quantitative and applied mathematical skills\nIn-depth working, beyond coursework, familiarity with classical and current ML techniques, both supervised and unsupervised learning techniques and algorithms\nImplementation experiences and deep knowledge of Classification, Time Series Analysis, Pattern Recognition, Reinforcement Learning, Deep Learning, Dynamic Programming and Optimization\nExperience in working on modeling graph structures related to spatiotemporal systems\nProgramming skills in Python is a must\nExperience in understanding and usage of LLM models and Prompt engineering is preferred.\nExperience in developing and deploying on cloud (AWS or Google or Azure)\nGood verbal and written communication skills\nFamiliarity with well-known ML frameworks such as Pandas, Keras, TensorFlow\nAbout Virtana:\nVirtana delivers the industry s only unified software multi-cloud management platform that allows organizations to monitor infrastructure, de-risk cloud migrations, and reduce cloud costs by 25% or more.\nOver 200 Global 2000 enterprise customers, such as AstraZeneca, Dell, Salesforce, Geico, Costco, Nasdaq, and Boeing, have valued Virtana s software solutions for over a decade.\nOur modular platform for hybrid IT digital operations includes Infrastructure Performance Monitoring and Management (IPM), Artificial Intelligence for IT Operations (AIOps), Cloud Cost Management (Fin Ops), and Workload Placement Readiness Solutions. Virtana is simplifying the complexity of hybrid IT environments with a single cloud-agnostic platform across all the categories listed above. The $30B IT Operations Management (ITOM) Software market is ripe for disruption, and Virtana is uniquely positioned for success.\nCompany Profitable Growth and Recognition\nIn FY2023 (Fiscal year ending January 2023), Virtana earned:\nBest CEO, Best CEO for Women, and Best CEO for Diversity by Comparably\nTwo years in a row YoY Profitable Annual Recurring Revenue (ARR) Growth\nTwo consecutive years of +EBITDA, 78% YoY EBITDA growth, or 20% of Revenue\nPositive Cash Flow, 171% YoY cash flow growth\n\nYou can schedule with us through Calendly at https: / / calendly.com / bimla-dhirayan / zoom-meeting-virtana",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Usage', 'Time series analysis', 'Artificial Intelligence', 'IT operations management', 'Machine learning', 'Cloud', 'Cash flow', 'Pattern recognition', 'Performance monitoring', 'Python']",2025-06-13 05:57:25
Lead Data Scientist,Tezo,8 - 12 years,Not Disclosed,['Hyderabad'],"ql-editor "">\nTezo is a new generation Digital & AI solutions provider, with a history of creating remarkable outcomes for our customers. We bring exceptional experiences using cutting-edge analytics, data proficiency, technology, and digital excellence.\n\nTezo is seeking passionate AI Engineers who are excited about harnessing the power of Generative AI to transform our company and provide cutting-edge solutions for our clients. Join us in revolutionizing enterprises by building intelligent, generative solutions that leverage AI/ML. If youve ever dreamed of contributing to impactful projects on a large scale, this is the opportunity for you!\nIn this role, you will be an integral part of the Machine Learning Platforms/Data Science team, focusing on developing, testing, and deploying generative AI models.\n\nWhat Makes Our AI/ML Practice Unique:\nPurpose-driven: We actively respond to our customers evolving needs with innovative solutions.\nCollaborative: We foster a positive and engaging work environment where collective ideas thrive.\nAccountable: We take ownership of our performance, both individually and as a team.\nService Excellence: We maximize our potential through continuous learning and improvement.\nTrusted: We empower individuals to make informed decisions and take calculated risks.\n\nJob Summary:\nWe are looking for a dedicated Lead Data Scientist with a strong background in Generative AI to join our team. You will support product, leadership, and client teams by providing insights derived from advanced data analysis and generative modeling.\nIn this role, you will collaborate closely with the development team, architects, and product owners to build efficient generative models and manage their lifecycle using the appropriate technology stack.\nCore Requirements:\n\nAt least 6 years of experience working with geographically distributed teams\n2+ years of experience working in a client-facing role on AI/ML .\nDemonstrable experience in leading a substantive area of work, or line management of a team.\nProven experience in building production grade Retrieval-Augmented Generation (RAG) solutions with hands on experience with advanced RAG techniques for retrieval, re-ranking etc.\nBuild GenAI applications using LangChain, LlamaIndex and familiarity with Vector Stores and Large Language Models.\nExperience in fine-tuning Large Language Models (LLMs) for business use cases will be preferred.\nMinimum of 4 years of experience in developing end-to-end classical machine learning and NLP projects.\nDemonstrated experience in deploying ML solutions in production using cloud services like Azure,AWS.\nBusiness Understanding, Stakeholder management and Team leading skills.\nStrong practical expertise in Python and SQL needed for data science projects.\nJoin us at Tezo to be part of a dynamic team committed to driving innovation through Generative AI solutions!",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data analysis', 'Team leading', 'data science', 'Cloud Services', 'Machine learning', 'Stakeholder management', 'Business understanding', 'Analytics', 'SQL', 'Python']",2025-06-13 05:57:27
Lead Data Scientist (AI/ML) - Immediate hiring - Chennai/Pune,Optimum Solutions,10 - 20 years,Not Disclosed,"['Pune', 'Chennai']","Experience:\nMinimum 10+ years in AI/ML or data science, with at least 5 years in a leadership role.\nProven experience in banking or financial services is highly preferred.\nHands-on with AI/ML frameworks (e.g., TensorFlow, PyTorch, Scikit-learn) and tools (Python, SQL, Spark).\nExperience in Azure ML tools: Databricks etc.\nExperience with end-to-end model lifecycle management and MLOps.\n\nMinimum Qualification: Masters Degree/PhD in Computer Science, Econometrics, Statistics, or related fields.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Artificial Intelligence', 'Machine Learning', 'Data Scientist', 'Tensorflow', 'Pyspark', 'Azure Databricks', 'Deep Learning', 'Scikit-Learn', 'Data Science', 'Pytorch', 'Azure Data Lake', 'Azure Machine Learning', 'Python']",2025-06-13 05:57:29
Lead - Data Scientist - Mumbai,One of the Leading Electronics Manufactu...,10 - 15 years,37.5-45 Lacs P.A.,"['Mumbai', 'Mumbai Suburban', 'Mumbai (All Areas)']","Role & responsibilities\nDevelop and implement a comprehensive analytics strategy to support the organization's business objectives.\nLead cross-functional projects using advanced data modeling and analysis techniques to discover insights that will guide strategic decisions and uncover optimization opportunities.\nBuild, develop and maintain data models, reporting systems, data automation systems, dashboards and performance metrics support that support key business decisions on Microsoft Power BI and Snowflake.\nDriving key business impacting processes like demand forecast generation and SNOP processes.\nGenerating sales recommendation for trade sales team.\nOptimizing business process like network, inventory, etc., with data modeling and predicting / prescribing algorithms.\nOversee the design and delivery of reports and insights that analyze business functions and key operations and performance metrics.\nManage and optimize processes for data intake, validation, mining and engineering as well as modeling, visualization and communication deliverables.\nAnticipate future demands of initiatives related to people, technology, budget and business within your department and design/implement solutions to meet these needs.\nApply advanced analytics techniques to improve commercial routines and processes.\nOrganize and drive successful completion of data insight initiatives through effective management of analyst and data employees and effective collaboration with stakeholders.\nCommunicate results and business impacts of insight initiatives to stakeholders within and outside of the company.\nAbility to look, analyse, critically think, and communicate insights to support data-driven decisions.\n\n\nPreferred candidate profile\nWith 10+ years of experience in a position monitoring, managing, manipulating and drawing insights from data, and someone with at least 3 years of experience leading a team\nExperience with data visualization tools: Power BI, Tableau, Raw, chart.js, etc.\nExperience in understanding and managing the data flow across multiple systems to datawarehouse.\nWorking knowledge of data mining principles: predictive analytics, mapping, collecting data from multiple data systems on premises and cloud-based data sources.\nUnderstanding of and experience using analytical concepts and statistical techniques: hypothesis development, designing tests/experiments, analyzing data, drawing conclusions, and developing actionable recommendations for business units.",Industry Type: Electronics Manufacturing,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['predictive modeling', 'Statistical Modeling', 'data scientist', 'Clustering', 'Predictive Analytics', 'Logistic Regression', 'Decision Tree', 'Time Series Analysis', 'Linear Regression', 'Advanced Analytics', 'Data Modeling']",2025-06-13 05:57:31
"Data Scientist,VP",NatWest Markets,10 - 12 years,Not Disclosed,"['Gurugram', 'Bengaluru']","Join us as a Data Scientist\nIn this role, you ll drive and embed the design and implementation of data science tools and methods, which harness our data to drive market-leading purpose customer solutions\nDay-to-day, you ll act as a subject matter expert and articulate advanced data and analytics opportunities, bringing them to life through data visualisation\nIf you re ready for a new challenge, and are interested in identifying opportunities to support external customers by using your data science expertise, this could be the role for you\nWere offering this role at vice president level\nWhat you ll do\nWe re looking for someone to understand the requirements and needs of our business stakeholders. You ll develop good relationships with them, form hypotheses, and identify suitable data and analytics solutions to meet their needs and to achieve our business strategy.\nYou ll be maintaining and developing external curiosity around new and emerging trends within data science, keeping up to date with emerging trends and tooling and sharing updates within and outside of the team.\nYou ll also be responsible for:\nProactively bringing together statistical, mathematical, machine-learning and software engineering skills to consider multiple solutions, techniques, and algorithms\nImplementing ethically sound models end-to-end and applying software engineering and a product development lens to complex business problems\nWorking with and leading both direct reports and wider teams in an Agile way within multi-disciplinary data to achieve agreed project and Scrum outcomes\nUsing your data translation skills to work closely with business stakeholders to define business questions, problems or opportunities that can be supported through advanced analytics\nSelecting, building, training, and testing complex machine models, considering model valuation, model risk, governance, and ethics throughout to implement and scale models\nThe skills you ll need\nTo be successful in this role, you ll need evidence of project implementation and work experience gained in a data-analysis-related field as part of a multi-disciplinary team. We ll also expect you to hold an undergraduate or a master s degree in Data science, Statistics, Computer science, or related field .\nYou ll also need an experience of 10 years with statistical software, database languages, big data technologies, cloud environments and machine learning on large data sets. And we ll look to you to bring the ability to demonstrate leadership, self-direction and a willingness to both teach others and learn new techniques.\nAdditionally, you ll need:\nExperience of deploying machine learning models into a production environment\nProficiency in Python and relevant libraries such as Pandas, NumPy, Scikit-learn coupled with experience in data visualisation tools.\nExtensive work experience with AWS Sage maker , including expertise in statistical data analysis, machine learning models, LLMs, and data management principles\nEffective verbal and written communication skills , the ability to adapt communication style to a specific audience and mentoring junior team members",Industry Type: Banking,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Data analysis', 'data science', 'Data management', 'Machine learning', 'Agile', 'Scrum', 'SAGE', 'Business strategy', 'Python']",2025-06-13 05:57:32
Data Scientist Specialist (GenAI),Rarr Technologies,7 - 12 years,Not Disclosed,"['Pune', 'Chennai', 'Bengaluru']","Role & responsibilities:\nOutline the day-to-day responsibilities for this role.\n\nPreferred candidate profile:\nSpecify required role expertise, previous job experience, or relevant certifications.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Langchain', 'Artificial Intelligence', 'Natural Language Processing', 'Python', 'RAG', 'Machine Learning', 'Deep Learning']",2025-06-13 05:57:34
Data Scientist ( Developer ),Mobile Programming,4 - 7 years,Not Disclosed,['Hyderabad'],"Candidate Skill:\nTechnical Skills -\nPython | R | AI/ML | TensorFlow | Keras | PyTorch | Scikit-learn | SQL | Data Visualization | Deep Learning | Cloud Platforms (AWS, Azure, GCP) | Docker | Kubernetes\nWe are looking for a talented Data Scientist (Developer) to join our team in Hyderabad. The ideal candidate will have a strong background in AI/ML, with hands-on experience in developing data-driven solutions.\nYou will collaborate with cross-functional teams to build and deploy machine learning models that solve real-world problems, improve business processes, and drive decision-making.\nKey Responsibilities:\nDevelop machine learning models and algorithms to extract insights and make data-driven decisions.\nImplement advanced data analytics and machine learning models using AI/ML techniques.\nWork closely with product and business teams to understand business requirements and translate them into machine learning solutions.\nCollaborate with data engineers to collect, clean, and process large datasets.\nPerform exploratory data analysis (EDA) to identify trends, patterns, and anomalies in data.\nDesign and implement data pipelines to automate the processing and analysis of data.\nTest and validate models to ensure they perform optimally in production.\nOptimize existing models for improved performance and scalability.\nCommunicate results to non-technical stakeholders through clear visualizations and reports.\nStay up-to-date with the latest developments in the field of AI/ML.\nRequired Skills and Qualifications:\n4-7 years of experience in Data Science, with a minimum of 3 years in AI/ML development.\nProficiency in programming languages such as Python and R for building machine learning models.\nStrong understanding of machine learning algorithms, including supervised and unsupervised learning, deep learning, and reinforcement learning.\nHands-on experience with AI/ML frameworks such as TensorFlow, Keras, PyTorch, or Scikit-learn.\nExperience with data wrangling, feature engineering, and model evaluation techniques.\nFamiliarity with data visualization tools such as Matplotlib, Seaborn, or Tableau.\nStrong knowledge of SQL for querying large datasets.\nFamiliarity with cloud platforms (e.g., AWS, Azure, GCP) and tools for model deployment and scaling.\nExperience with Docker and Kubernetes for model deployment is a plus.\nExcellent problem-solving skills and ability to work in a fast-paced environment.\nStrong communication skills to present technical concepts to non-technical stakeholders.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Azure', 'Scikit-learn', 'machine learning', 'artificial intelligence', 'Deep Learning', 'SQL', 'R', 'PyTorch', 'Docker', 'GCP', 'Data Visualization', 'Keras', 'AWS', 'Python', 'TensorFlow', 'Kubernetes']",2025-06-13 05:57:36
Sr. Data Engineer (Analyst- BI/Visualization),Visa,5 - 10 years,Not Disclosed,['Bengaluru'],"We are looking for a seasoned individual contributor who is comfortable engaging with business owners, data enthusiasts, and technology teams. Successful candidates have strong experience in developing complex data solutions for business intelligence applications. This role will be hands-on and focused on building data pipelines, business intelligence solutions at scale and with a focus on sustained operational excellence.\nHelp improve Visa s decision-making by making data more accessible and relevant to key partners\nDevelop deep partnerships with engineering, finance, and product teams to deliver on major cross-functional solution development\nWork with a team of talented data and analytics engineers with the ability to not only keep up with, but also pioneer, in this space.\nCreate extract, transform, load (ETL) processes for easy ingestion and use\nDevelop visualizations to make your sophisticated analyses accessible to a broad audience\nFind opportunities to create, automate and scale repeatable financial and statistical analysis\nProvide technical leadership in a team that generates business insights based on big data, identify impactful recommendations, and communicate the findings to clients\nBrainstorm innovative ways to use our unique data to answer business problems\nCollaborate with and influence leadership so that your teams work directly impacts company strategy and direction.\nCommunicate effectively to all levels of the organization, including executives.\n\n\nBasic Qualifications\n5 or more years of work experience with a Bachelors Degree or an Advanced Degree (e.g. Masters, MBA, JD, MD, or PhD)\nPreferred Qualifications\n5 or more years of work experience with a Bachelor s Degree or more than 2 years of work experience with an Advanced Degree (e.g. Masters, MBA, JD, MD)\n4+ years experience in data-based decision-making or quantitative analysis\nMaster s degree in Statistics, Operations Research, Applied Mathematics, Economics, Data Science, Business Analytics, Computer Science, or a related technical field\nExposure to Financial Services/ Payments data analytics and ETL development.\nExperience and expertise with data visualization using Tableau, Power BI or similar tools\nExperience in implementing ETL pipelines in Spark, Python, HIVE or SAS that process transaction and account level data and standardization of data pipelines.\nAdvanced experience in writing and optimizing efficient SQL queries with Python, Spark, Hive, Scala handling Large Data Sets in Big-Data Environments.\nExperience with Unix/Shell and exposure to Scheduling tools like Oozie and Airflow.\nStrong written, verbal, and interpersonal skills needed to effectively communicate technical insights and recommendations with business customers and leadership team.\nGood business acumen to orient data analysis to business needs of clients.\nAbility to translate data and technical concepts into requirements documents, business cases and user stories.\nAbility to learn new tools and paradigms as data science continues to evolve at Visa and elsewhere.\nDemonstrated intellectual and analytical rigor, team oriented, energetic, collaborative, diplomatic, and flexible style.\nPrevious experience with product valuations, financial engineering, customer lifetime values or net present value methodologies",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Unix', 'Computer science', 'Data analysis', 'SAS', 'Business analytics', 'Analytical', 'Scheduling', 'Business intelligence', 'Financial services', 'Python']",2025-06-13 05:57:38
Senior Data Engineering Analyst,Optum,4 - 7 years,Not Disclosed,['Bengaluru'],"Job Description\n\nExperience 4 to 7 years.\nExperience in any ETL tools [e.g. DataStage] with implementation experience in large Data Warehouse\nProficiency in programming languages such as Python etc.\nExperience with data warehousing solutions (e.g., Snowflake, Redshift) and big data technologies (e.g., Hadoop, Spark).\nStrong knowledge of SQL and database management systems.\nFamiliarity with cloud platforms (e.g., AWS, Azure, GCP) and data pipeline orchestration tools (e.g. Airflow).\nProven ability to lead and develop high-performing teams, with excellent communication and interpersonal skills.\nStrong analytical and problem-solving abilities, with a focus on delivering actionable insights.\nResponsibilities\nDesign, develop, and maintain advanced data pipelines and ETL processes using niche technologies.\nCollaborate with cross-functional teams to understand complex data requirements and deliver tailored solutions.\nEnsure data quality and integrity by implementing robust data validation and monitoring processes.\nOptimize data systems for performance, scalability, and reliability.\nDevelop comprehensive documentation for data engineering processes and systems.",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['ETL', 'SQL', 'Python', 'Azure', 'Datastage', 'Snowflake', 'Ab Initio', 'Informatica', 'Teradata', 'AWS']",2025-06-13 05:57:40
Data Modeler,Synechron,0 - 2 years,Not Disclosed,['Bengaluru'],"Synechron is seeking a knowledgeable and proactive Data Modeler to guide the design and development of data structures that support our clients' business objectives. In this role, you will collaborate with cross-functional teams to translate business requirements into scalable and efficient data models, ensuring data accuracy, consistency, and integrity. You will contribute to creating sustainable and compliant data architectures that leverage emerging technologies such as cloud, IoT, mobile, and blockchain. Your work will be instrumental in enabling data-driven decision-making and operational excellence across projects.Software Required\n\nSkills:\nStrong understanding of data modeling concepts, methodologies, and tools Experience with data modeling for diverse technology platforms including cloud, mobile, IoT, and blockchain Familiarity with database management systems (e.g., relational, NoSQL) Knowledge of SDLC and Agile development practices Proficiency in modeling tools such as ERwin, PowerDesigner, or similar Preferred Skills:\nExperience with data integration tools and ETL processes Knowledge of data governance and compliance standards Familiarity with cloud platforms (AWS, Azure, GCP) and how they impact data architectureOverall Responsibilities Collaborate with business analysts, data engineers, and stakeholders to understand data requirements and translate them into robust data models Design logical and physical data models optimized for performance, scalability, and maintainability Develop and maintain documentation for data structures, including data dictionaries and metadata Conduct reviews of data models and code to ensure adherence to quality standards and best practices Assist in designing data security and privacy measures in alignment with organizational policies Stay informed about emerging data modeling trends and incorporate best practices into project delivery Support data migration, integration, and transformation activities as needed Provide technical guidance and mentorship related to data modeling standardsTechnical Skills (By Category) Data Modeling & Data Management: EssentialLogical/physical data modeling, ER diagrams, data dictionaries PreferredDimensional modeling, data warehousing, master data management Programming Languages: PreferredSQL (expertise in writing complex queries) OptionalPython, R for data analysis and scripting Databases & Data Storage Technologies: EssentialRelational databases (e.g., Oracle, SQL Server, MySQL) PreferredNoSQL (e.g., MongoDB, Cassandra), cloud-native data stores Cloud Technologies: PreferredBasic understanding of cloud data solutions (AWS, Azure, GCP) Frameworks & Libraries: Not typically required, but familiarity with data integration frameworks is advantageous Development Tools & Methodologies: EssentialData modeling tools (ERwin, PowerDesigner), version control (Git), Agile/Scrum workflows Security & Compliance: Knowledge of data security best practices, regulatory standards like GDPR, HIPAAExperience Minimum of 8+ years of direct experience in data modeling, data architecture, or related roles Proven experience designing data models for complex systems across multiple platforms (cloud, mobile, IoT, blockchain) Experience working in Agile environments using tools like JIRA, Confluence, Git Preference for candidates with experience supporting data governance and data quality initiativesNoteEquivalent demonstrated experience in relevant projects or certifications can qualify candidates.Day-to-Day Activities Participate in daily stand-ups and project planning sessions Collaborate with cross-functional teams to understand and analyze business requirements Create, review, and refine data models and associated documentation Develop data schemas, dictionaries, and standards to ensure consistency Support data migration, integration, and performance tuning activities Conduct peer reviews and provide feedback on data models and solutions Keep current with the latest industry developments in data architecture and modeling Troubleshoot and resolve data-related technical issuesQualifications Bachelors or Masters degree in Computer Science, Data Science, Information Technology, or related fields Demonstrated experience with data modeling tools and techniques in diverse technological environments Certifications related to data modeling, data management, or cloud platforms (preferred)Professional Competencies Strong analytical and critical thinking skills to develop optimal data solutions Effective communication skills for translating technical concepts to non-technical stakeholders Ability to work independently and in collaborative team environments Skilled problem solver able to handle complex data challenges Adaptability to rapidly evolving technologies and project requirements Excellent time management and prioritization skills to deliver quality outputs consistently",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data modeling', 'modeling tools', 'relational databases', 'scrum', 'agile', 'confluence', 'hipaa', 'data warehousing', 'data architecture', 'erwin', 'sql', 'git', 'gcp', 'mysql', 'etl', 'mongodb', 'jira', 'python', 'oracle', 'microsoft azure', 'sql server', 'nosql', 'gdpr', 'cassandra', 'aws', 'data integration', 'sdlc']",2025-06-13 05:57:42
IN Senior Associate GenAI S/W Engineer- Data and Analytics,PwC Service Delivery Center,1 - 7 years,Not Disclosed,['Bengaluru'],"Not Applicable\nSpecialism\nData, Analytics & AI\nManagement Level\nSenior Associate\n& Summary\nAt PwC, our people in data and analytics engineering focus on leveraging advanced technologies and techniques to design and develop robust data solutions for clients. They play a crucial role in transforming raw data into actionable insights, enabling informed decisionmaking and driving business growth.\n\nIn data engineering at PwC, you will focus on designing and building data infrastructure and systems to enable efficient data processing and analysis. You will be responsible for developing and implementing data pipelines, data integration, and data transformation solutions.\nWhy PWC\n& Summary\nJob Overview\nWe are seeking a highly skilled and versatile polyglot Full Stack Developer with expertise in modern frontend and backend technologies, cloudbased solutions, AI/ML and Gen AI. The ideal candidate will have a strong foundation in fullstack development, cloud platforms (preferably Azure), and handson experience in Gen AI, AI and machine learning technologies.\nKey Responsibilities\nDevelop and maintain web applications using Angular / React.js , .NET , and Python .\nDesign, deploy, and optimize Azure native PaaS and SaaS services, including but not limited to Function Apps , Service Bus , Storage Accounts , SQL Databases , Key vaults, ADF, Data Bricks and REST APIs with Open API specifications.\nImplement security best practices for data in transit and rest. Authentication best practices SSO, OAuth 2.0 and Auth0.\nUtilize Python for developing data processing and advanced AI/ML models using libraries like pandas , NumPy , scikitlearn and Langchain , Llamaindex , Azure OpenAI SDK\nLeverage Agentic frameworks like Crew AI, Autogen etc.\nWell versed with RAG and Agentic Architecture.\nStrong in Design patterns Architectural, Data, Object oriented\nLeverage azure serverless components to build highly scalable and efficient solutions.\nCreate, integrate, and manage workflows using Power Platform , including Power Automate , Power Pages , and SharePoint .\nApply expertise in machine learning , deep learning , and Generative AI to solve complex problems.\nPrimary Skills\nProficiency in React.js , .NET , and Python .\nStrong knowledge of Azure Cloud Services , including serverless architectures and data security.\nExperience with Python Data Analytics libraries\npandas\nNumPy\nscikitlearn\nMatplotlib\nSeaborn\nExperience with Python Generative AI Frameworks\nLangchain\nLlamaIndex\nCrew AI\nAutoGen\nFamiliarity with REST API design , Swagger documentation , and authentication best practices .\nSecondary Skills\nExperience with Power Platform tools such as Power Automate, Power Pages, and SharePoint integration.\nKnowledge of Power BI for data visualization (preferred).\nPreferred Knowledge Areas Nice to have\nIndepth understanding of Machine Learning , deep learning, supervised, unsupervised algorithms.\nMandatory skill sets\nAI, ML\nPreferred skill sets\nAI, ML\nYears of experience required\n3 7 years\nEducation qualification\nBE/BTECH, ME/MTECH, MBA, MCA\nEducation\nDegrees/Field of Study required Bachelor of Technology, Master of Business Administration, Bachelor of Engineering, Master of Engineering\nDegrees/Field of Study preferred\nRequired Skills\nGame AI\nAccepting Feedback, Accepting Feedback, Active Listening, Agile Scalability, Amazon Web Services (AWS), Analytical Thinking, Apache Airflow, Apache Hadoop, Azure Data Factory, Communication, Creativity, Data Anonymization, Data Architecture, Database Administration, Database Management System (DBMS), Database Optimization, Database Security Best Practices, Databricks Unified Data Analytics Platform, Data Engineering, Data Engineering Platforms, Data Infrastructure, Data Integration, Data Lake, Data Modeling, Data Pipeline {+ 28 more}\nTravel Requirements\nGovernment Clearance Required?",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Front end', 'Architecture', 'Data modeling', 'data security', 'Machine learning', 'data visualization', 'Apache', 'SQL', 'Python', 'Data architecture']",2025-06-13 05:57:43
IN Senior Associate Azure Data Engineer Data & Analaytics,PwC Service Delivery Center,2 - 5 years,Not Disclosed,['Bengaluru'],"Not Applicable\nSpecialism\nData, Analytics & AI\nManagement Level\nSenior Associate\n& Summary\nAt PwC, our people in data and analytics engineering focus on leveraging advanced technologies and techniques to design and develop robust data solutions for clients. They play a crucial role in transforming raw data into actionable insights, enabling informed decisionmaking and driving business growth.\n\nIn data engineering at PwC, you will focus on designing and building data infrastructure and systems to enable efficient data processing and analysis. You will be responsible for developing and implementing data pipelines, data integration, and data transformation solutions.\n& Summary A career within Data and Analytics services will provide you with the opportunity to help organisations uncover enterprise insights and drive business results using smarter data analytics. We focus on a collection of organisational technology capabilities, including business intelligence, data management, and data assurance that help our clients drive innovation, growth, and change within their organisations in order to keep up with the changing nature of customers and technology. We make impactful decisions by mixing mind and machine to leverage data, understand and navigate risk, and help our clients gain a competitive edge.\nResponsibilities\nDesign, develop, and optimize data pipelines and ETL processes using PySpark or Scala to extract, transform, and load large volumes of structured and unstructured data from diverse sources.\nImplement data ingestion, processing, and storage solutions on Azure cloud platform, leveraging services such as Azure Databricks, Azure Data Lake Storage, and Azure Synapse Analytics.\nDevelop and maintain data models, schemas, and metadata to support efficient data access, query performance, and analytics requirements.\nMonitor pipeline performance, troubleshoot issues, and optimize data processing workflows for scalability, reliability, and costeffectiveness.\nImplement data security and compliance measures to protect sensitive information and ensure regulatory compliance.\nRequirement\nProven experience as a Data Engineer, with expertise in building and optimizing data pipelines using PySpark, Scala, and Apache Spark.\nHandson experience with cloud platforms, particularly Azure, and proficiency in Azure services such as Azure Databricks, Azure Data Lake Storage, Azure Synapse Analytics, and Azure SQL Database.\nStrong programming skills in Python and Scala, with experience in software development, version control, and CI/CD practices.\nFamiliarity with data warehousing concepts, dimensional modeling, and relational databases (e.g., SQL Server, PostgreSQL, MySQL).\nExperience with big data technologies and frameworks (e.g., Hadoop, Hive, HBase) is a plus.\nMandatory skill sets\nSpark, Pyspark, Azure\nPreferred skill sets\nSpark, Pyspark, Azure\nYears of experience required\n4 8\nEducation qualification\nB.Tech / M.Tech / MBA / MCA\nEducation\nDegrees/Field of Study required Bachelor of Technology, Master of Business Administration\nDegrees/Field of Study preferred\nRequired Skills\nMicrosoft Azure\nAccepting Feedback, Accepting Feedback, Active Listening, Agile Scalability, Amazon Web Services (AWS), Analytical Thinking, Apache Airflow, Apache Hadoop, Azure Data Factory, Communication, Creativity, Data Anonymization, Data Architecture, Database Administration, Database Management System (DBMS), Database Optimization, Database Security Best Practices, Databricks Unified Data Analytics Platform, Data Engineering, Data Engineering Platforms, Data Infrastructure, Data Integration, Data Lake, Data Modeling, Data Pipeline {+ 27 more}\nTravel Requirements\nGovernment Clearance Required?",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data management', 'Data modeling', 'Postgresql', 'MySQL', 'Database administration', 'Agile', 'Apache', 'Business intelligence', 'SQL', 'Python']",2025-06-13 05:57:45
Data Engineer,Kinara Capital,0 - 1 years,3-5 Lacs P.A.,['Bengaluru'],"1\nAbout Company\nKinara Capital is a FinTech NBFC dedicated to driving Financial Inclusion\nin the MSME sector. Our mission is to transform lives, livelihoods, and\nlocal economies by providing fast and flexible loans without property\ncollateral to small business entrepreneurs. Led by a women-majority\nmanagement team, Kinara Capital values diversity and inclusion and\nfosters a collaborative working environment.\nKinara Capital is the only company from India recognized globally by the\nWorld Bank/IFC with a gold award in 2019 as Bank of the Year-Asia’ for\nour innovative work in SME financing. Kinara Capital is an RBI-registered\nSystemically Important NBFC.\nHeadquartered in Bangalore, we have 110 branches across Karnataka,\nGujarat, Maharashtra, Andhra Pradesh, Telangana, Tamil Nadu, and UT\nPuducherry with more than 1000 employees. https://kinaracapital.com/\nTitle:\nData Engineer\nTeam:\nData Warehouse Team\nPurpose of Job:\nThis is a hands-on coding and designing position and we are looking for\nan exceptionally talented data engineer who has exposure in\nimplementing AWS services to build data pipelines, api integration and\ndesigning data warehouse.\nJob Responsibilities:\nExcellent coding skills in Python, PySpark, SQL.\nHave extensive experience in Spark ecosystem and has\nworked on both real time and batch processing\nHave experience in AWS Glue, EMR, DMS, Lambda, S3,\nDynamoDB, Step functions, Airflow, RDS, Aurora etc.\nExperience with modern Database systems such as\nRedshift, Presto, Hive etc.\nWorked on building data lakes in the past on S3 or\nApache Hudi\nSolid understanding of Data Warehousing Concepts\nGood to have experience on tools such as Kafka or Kinesis\nGood to have AWS Developer Associate or Solutions\nArchitect Associate Certification\nQualifications:\nAt least a bachelor’s degree in Science, Engineering, Applied\nMathematics.\nOther Requirements:\nLearning Attitude and good communication skills\nReport to:\nLead Data Engineer\nPlace of work:\nHead office, Bangalore.\nJob Type:\nFull Time\nNo. of Posts:\n2",Industry Type: FinTech / Payments,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Amazon Redshift', 'Data Warehousing', 'Spark', 'Python', 'SQL', 'Snowflake', 'ETL']",2025-06-13 05:57:47
Data Engineer Sr. Analyst,Accenture,2 - 3 years,Not Disclosed,['Kochi'],"Job Title - + +\n\n\n\nManagement Level:\n\n\n\nLocation:Kochi, Coimbatore, Trivandrum\n\n\n\nMust have skills:Python/Scala, Pyspark/Pytorch\n\n\n\n\nGood to have skills:Redshift\n\n\n\nJob\n\n\nSummary\n\nYoull capture user requirements and translate them into business and digitally enabled solutions across a range of industries. Your responsibilities will include:\n\n\n\nRoles and Responsibilities\nDesigning, developing, optimizing, and maintaining data pipelines that adhere to ETL principles and business goals\nSolving complex data problems to deliver insights that helps our business to achieve their goals.\nSource data (structured unstructured) from various touchpoints, format and organize them into an analyzable format.\nCreating data products for analytics team members to improve productivity\nCalling of AI services like vision, translation etc. to generate an outcome that can be used in further steps along the pipeline.\nFostering a culture of sharing, re-use, design and operational efficiency of data and analytical solutions\nPreparing data to create a unified database and build tracking solutions ensuring data quality\nCreate Production grade analytical assets deployed using the guiding principles of CI/CD.\n\n\nProfessional and Technical Skills\nExpert in Python, Scala, Pyspark, Pytorch, Javascript (any 2 at least)\nExtensive experience in data analysis (Big data- Apache Spark environments), data libraries (e.g. Pandas, SciPy, Tensorflow, Keras etc.), and SQL. 2-3 years of hands-on experience working on these technologies.\nExperience in one of the many BI tools such as Tableau, Power BI, Looker.\nGood working knowledge of key concepts in data analytics, such as dimensional modeling, ETL, reporting/dashboarding, data governance, dealing with structured and unstructured data, and corresponding infrastructure needs.\nWorked extensively in Microsoft Azure (ADF, Function Apps, ADLS, Azure SQL), AWS (Lambda,Glue,S3), Databricks analytical platforms/tools, Snowflake Cloud Datawarehouse.\n\n\n\n\nAdditional Information\nExperience working in cloud Data warehouses like Redshift or Synapse\nCertification in any one of the following or equivalent\nAWS- AWS certified data Analytics- Speciality\nAzure- Microsoft certified Azure Data Scientist Associate\nSnowflake- Snowpro core- Data Engineer\nDatabricks Data Engineering\n\nQualification\n\n\n\nExperience:3.5 -5 years of experience is required\n\n\n\n\nEducational Qualification:Graduation (Accurate educational details should capture)",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['scala', 'pyspark', 'pytorch', 'python', 'microsoft azure', 'glue', 'amazon redshift', 'sql', 'tensorflow', 'sql azure', 'spark', 'keras', 'big data', 'etl', 'scipy', 'snowflake', 'data analysis', 'azure data lake', 'power bi', 'data engineering', 'javascript', 'pandas', 'data bricks', 'tableau', 'lambda expressions', 'aws']",2025-06-13 05:57:49
Data Engineer - Senior Analyst,Accenture,2 - 3 years,Not Disclosed,['Kochi'],"Job Title - + +\n\n\n\nManagement Level:\n\n\n\nLocation:Kochi, Coimbatore, Trivandrum\n\n\n\nMust have skills:Python/Scala, Pyspark/Pytorch\n\n\n\n\nGood to have skills:Redshift\n\n\n\nExperience:3.5 -5 years of experience is required\n\n\n\n\nEducational Qualification:Graduation (Accurate educational details should capture)\n\n\n\nJob\n\n\nSummary\n\nYoull capture user requirements and translate them into business and digitally enabled solutions across a range of industries. Your responsibilities will include:\n\n\n\nRoles and Responsibilities\nDesigning, developing, optimizing, and maintaining data pipelines that adhere to ETL principles and business goals\nSolving complex data problems to deliver insights that helps our business to achieve their goals.\nSource data (structured unstructured) from various touchpoints, format and organize them into an analyzable format.\nCreating data products for analytics team members to improve productivity\nCalling of AI services like vision, translation etc. to generate an outcome that can be used in further steps along the pipeline.\nFostering a culture of sharing, re-use, design and operational efficiency of data and analytical solutions\nPreparing data to create a unified database and build tracking solutions ensuring data quality\nCreate Production grade analytical assets deployed using the guiding principles of CI/CD.\n\n\nProfessional and Technical Skills\nExpert in Python, Scala, Pyspark, Pytorch, Javascript (any 2 at least)\nExtensive experience in data analysis (Big data- Apache Spark environments), data libraries (e.g. Pandas, SciPy, Tensorflow, Keras etc.), and SQL. 2-3 years of hands-on experience working on these technologies.\nExperience in one of the many BI tools such as Tableau, Power BI, Looker.\nGood working knowledge of key concepts in data analytics, such as dimensional modeling, ETL, reporting/dashboarding, data governance, dealing with structured and unstructured data, and corresponding infrastructure needs.\nWorked extensively in Microsoft Azure (ADF, Function Apps, ADLS, Azure SQL), AWS (Lambda,Glue,S3), Databricks analytical platforms/tools, Snowflake Cloud Datawarehouse.\n\n\n\n\nAdditional Information\nExperience working in cloud Data warehouses like Redshift or Synapse\nCertification in any one of the following or equivalent\nAWS- AWS certified data Analytics- Speciality\nAzure- Microsoft certified Azure Data Scientist Associate\nSnowflake- Snowpro core- Data Engineer\nDatabricks Data Engineering\n\nQualification\n\n\n\nExperience:3.5 -5 years of experience is required\n\n\n\n\nEducational Qualification:Graduation (Accurate educational details should capture)",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['scala', 'pyspark', 'pytorch', 'python', 'microsoft azure', 'glue', 'amazon redshift', 'sql', 'tensorflow', 'sql azure', 'spark', 'keras', 'big data', 'etl', 'scipy', 'snowflake', 'data analysis', 'azure data lake', 'power bi', 'data engineering', 'javascript', 'pandas', 'data bricks', 'tableau', 'lambda expressions', 'aws']",2025-06-13 05:57:51
Data Engineer Sr. Analyst,Accenture,5 - 7 years,Not Disclosed,['Kochi'],"Job Title - + +\n\n\n\nManagement Level:\n\n\n\nLocation:Kochi, Coimbatore, Trivandrum\n\n\n\nMust have skills:Databricks including Spark-based ETL, Delta Lake\n\n\n\n\nGood to have skills:Pyspark\n\n\n\nJob\n\n\nSummary\n\nWe are seeking a highly skilled and experienced Senior Data Engineer to join our growing Data and Analytics team. The ideal candidate will have deep expertise in Databricks and cloud data warehousing, with a proven track record of designing and building scalable data pipelines, optimizing data architectures, and enabling robust analytics capabilities. This role involves working collaboratively with cross-functional teams to ensure the organization leverages data as a strategic asset. Your responsibilities will include:\n\n\n\nRoles and Responsibilities\nDesign, build, and maintain scalable data pipelines and ETL processes using Databricks and other modern tools.\nArchitect, implement, and manage cloud-based data warehousing solutions on Databricks (Lakehouse Architecture)\nDevelop and maintain optimized data lake architectures to support advanced analytics and machine learning use cases.\nCollaborate with stakeholders to gather requirements, design solutions, and ensure high-quality data delivery.\nOptimize data pipelines for performance and cost efficiency.\nImplement and enforce best practices for data governance, access control, security, and compliance in the cloud.\nMonitor and troubleshoot data pipelines to ensure reliability and accuracy.\nLead and mentor junior engineers, fostering a culture of continuous learning and innovation.\nExcellent communication skills\nAbility to work independently and along with client based out of western Europe.\n\n\n\nProfessional and Technical Skills\n3.5-5 years of experience in Data Engineering roles with a focus on cloud platforms.\nProficiency in Databricks, including Spark-based ETL, Delta Lake, and SQL.\nStrong experience with one or more cloud platforms (AWS preferred).\nHandson Experience with Delta lake, Unity Catalog, and Lakehouse architecture concepts.\nStrong programming skills in Python and SQL; experience with Pyspark a plus.\nSolid understanding of data modeling concepts and practices (e.g., star schema, dimensional modeling).\nKnowledge of CI/CD practices and version control systems (e.g., Git).\nFamiliarity with data governance and security practices, including GDPR and CCPA compliance.\n\n\n\n\nAdditional Information\nExperience with Airflow or similar workflow orchestration tools.\nExposure to machine learning workflows and MLOps.\nCertification in Databricks, AWS\nFamiliarity with data visualization tools such as Power BI\n\n(do not remove the hyperlink)Qualification\n\n\n\nExperience:3.5 -5 years of experience is required\n\n\n\n\nEducational Qualification:Graduation (Accurate educational details should capture)",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data warehousing', 'sql', 'data modeling', 'python', 'data bricks', 'hive', 'kubernetes', 'catalog', 'pyspark', 'data architecture', 'docker', 'ansible', 'git', 'java', 'spark', 'devops', 'hadoop', 'etl', 'big data', 'data lake', 'airflow', 'power bi', 'cloud platforms', 'machine learning', 'data engineering', 'aws']",2025-06-13 05:57:53
"Senior Data Engineer ( T-SQL & SSIS,Data Warehousing & ETL Specialist)",Synechron,5 - 10 years,Not Disclosed,"['Hyderabad', 'Pune', 'Bengaluru']","Job Summary\nSynechron is seeking a highly skilled Senior Data Engineer specializing in T-SQL and SSIS to lead and advance our data integration and warehousing initiatives. In this role, you will design, develop, and optimize complex ETL processes and database solutions to support enterprise data needs. Your expertise will enable efficient data flow, ensure data integrity, and facilitate actionable insights, contributing to our organizations commitment to data-driven decision-making and operational excellence.\nSoftware Requirements\nOverall Responsibilities\nTechnical Skills (By Category)\nExperience Requirements\nDay-to-Day Activities\nQualifications\nProfessional Competencies",,,,"['Data Engineering', 'T-SQL', 'Azure Data Factory', 'query optimization', 'performance tuning', 'database security', 'AWS Glue', 'Data Warehousing', 'SSIS', 'ETL']",2025-06-13 05:57:55
Senior Data Engineer,Robert Bosch Engineering and Business Solutions Private Limited,4 - 8 years,Not Disclosed,['Bengaluru'],"As a Data engineer in our team, you work with large scale manufacturing data coming from our globally distributed plants. You will focus on building efficient, scalable data-driven applications.\nThe data sets produced by these applications - whether data streams or data at rest - need to be highly available, reliable, consistent and quality-assured so that they can serve as input to wide range of other use cases and downstream applications.\nWe run these applications on Azure databricks, you will be building applications, you will also contribute to scaling the platform including topics such as automation and observability.\nFinally, you are expected to interact with customers and other technical teams e. g. for requirements clarification definition of data models.\nPrimary responsibilities:\nBe a key contributor to the Bosch hybrid cloud data platform (on-prem cloud)\nDesigning building data pipelines on a global scale, ranging from small to huge datasets\nDesign applications and data models based on deep business understanding and customer requirements\nDirectly work with architects and technical leadership to design implement applications and / or architectural components\nArchitectural proposal and estimation for the application, technical leadership to the team\nCoordination/Collaboration with central teams for tasks and standards\nDevelop data integration workflow in Azure\nDeveloping streaming application using scala.\nIntegrating the end-to-end Azure Databricks pipeline to take data from source systems to target system ensuring the quality and consistency of data.\nDefining data quality and validation checks.\nConfiguring data processing and transformation.\nWriting unit test cases for data pipelines.\nDefining and implementing data quality and validation check.\nTuning pipeline configurations for optimal performance.\nParticipate in Peer review and PR review for the code written by team members",Industry Type: Automobile,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Automation', 'Architecture', 'Technical leadership', 'Data processing', 'Workflow', 'Data quality', 'Test cases', 'Unit testing', 'Downstream']",2025-06-13 05:57:56
Data Analyst,Pripton Innovations,0 - 5 years,Not Disclosed,[],We are looking for a Data Analyst with expertise in ERP systems (NetSuite and Workday Financials) and data warehousing (Snowflake) to support our ongoing ERP migration and data-driven decision-making.,Industry Type: Analytics / KPO / Research,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Analysis', 'ERP System', 'Netsuite', 'Snowflake']",2025-06-13 05:57:58
Data Analyst,Sara Business Solution,0 - 2 years,1.44-1.8 Lacs P.A.,['Karur'],"Responsibilities:\n* Analyze data using advanced tools and techniques.\n* Support agri-data projects with insights from the field.\nPassion for agriculture, rural development, or farming\nBasic computer skills (Excel, Google Sheets, etc.)",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent",['any degree with Agriculture background'],2025-06-13 05:58:00
Lead Data Engineer,Acuity Knowledge Partners,8 - 13 years,Not Disclosed,"['Pune', 'Gurugram', 'Bengaluru']","Preferred candidate profile\n\n9+ years of overall experience in software development with a focus on data projects using Python, PySpark, and associated frameworks.\nProven experience as a Data Engineer with experience in Azure cloud.\nExperience implementing solutions using Azure cloud services, Azure Data Factory, Azure Lake Gen 2, Azure Databases, Azure Data Fabric, API Gateway management, Azure Functions.",,,,"['Pyspark', 'Azure Cloud', 'Python', 'SQL']",2025-06-13 05:58:01
"Senior Python Developer (Machine Learning,Data Analysis,Visualization)",Synechron,3 - 5 years,Not Disclosed,"['Pune', 'Hinjewadi']","Software Requirements\nRequired Skills:\nProficiency in Python (version 3.6+) with experience in data analysis, manipulation, and scripting\nKnowledge of SQL for data extraction, transformation, and database querying\nExperience with data visualization tools such as PowerBI, Tableau, or QlikView\nFamiliarity with AI and Machine Learning frameworks such as TensorFlow, Keras, PyTorch, or equivalent",,,,"['Python', 'PostgreSQL', 'MySQL', 'Data Analysis', 'Data Visualization', 'Oracle', 'ETL', 'Machine Learning']",2025-06-13 05:58:03
Data Engineer - Ranchi,Teqfocus Consulting,0 - 2 years,Not Disclosed,['Ranchi'],"Job Title: Data Engineer\nExperience: 1+ Years (Freshers with relevant training & certification may apply)\nLocation: Ranchi (Work from Office)\n\nJob Summary:\nWe are looking for a Data Engineer with at least 1 year of hands-on experience in data engineering practices. The ideal candidate will work closely with our data and analytics teams to build robust and scalable data pipelines. Experience with Snowflake is a plus.\n\nKey Responsibilities:\nDesign, build, and maintain data pipelines using modern data engineering tools.\nTransform and clean data from multiple sources for reporting and analytics.\nOptimize data pipelines for performance and scalability.\nCollaborate with cross-functional teams including BI, analytics, and application developers.\nMonitor, troubleshoot, and maintain data workflows.\n\nRequired Skills:\nStrong understanding of data warehousing concepts.\nProficiency in SQL and Python.\nKnowledge of ETL tools and processes.\nFamiliarity with cloud platforms such as AWS, Snowflake, Databricks, Azure or GCP.\nExposure to data visualization tools is a plus.\n\nGood to have any of below certification:\nSnowflake SnowPro Core Certification\nSnowflake Advanced: Data Engineer Certification\nGoogle Cloud Professional Data Engineer\nMicrosoft Certified: Azure Data Engineer Associate\nAWS Certified Data Analytics Specialty\n\nQualifications:\nBachelor's degree in Computer Science, Information Technology, or related field.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Snowflake', 'AWS', 'Data Bricks', 'GCP', 'Microsoft Azure', 'Python']",2025-06-13 05:58:05
Data Engineer,Accenture,5 - 10 years,Not Disclosed,['Bengaluru'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL processes to migrate and deploy data across systems. Your day will involve working on data solutions and ensuring data integrity and quality.\nRoles & Responsibilities:- Expected to be an SME, collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Develop and maintain data pipelines for efficient data processing.- Implement ETL processes to migrate and deploy data across systems.- Ensure data quality and integrity throughout the data solutions.- Collaborate with cross-functional teams to optimize data processes.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Strong understanding of data engineering principles.- Experience with cloud-based data solutions like AWS or Azure.- Knowledge of SQL and NoSQL databases.- Hands-on experience with data modeling and schema design.\nAdditional Information:- The candidate should have a minimum of 5 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Bengaluru office.- A 15 years full-time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analytics', 'data engineering', 'sql', 'etl', 'aws', 'hive', 'python', 'data processing', 'microsoft azure', 'pyspark', 'data warehousing', 'data integrity', 'knowledge of sql', 'nosql', 'database design', 'data quality', 'data modeling', 'spark', 'hadoop', 'big data', 'etl process', 'nosql databases']",2025-06-13 05:58:06
Data Engineer,Accenture,15 - 20 years,Not Disclosed,['Bengaluru'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Neo4j, Stardog\n\n\n\n\nGood to have skills :JavaMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions that facilitate data generation, collection, and processing. Your typical day will involve creating data pipelines, ensuring data quality, and implementing ETL processes to migrate and deploy data across various systems. You will collaborate with cross-functional teams to understand their data needs and provide effective solutions, ensuring that the data infrastructure is robust and scalable to meet the demands of the organization.\nRoles & Responsibilities:- Expected to be an SME.- Collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Mentor junior team members to enhance their skills and knowledge in data engineering.- Continuously evaluate and improve data processes to enhance efficiency and effectiveness.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Neo4j.- Good To Have\n\n\n\n\nSkills:\nExperience with Java.- Strong understanding of data modeling and graph database concepts.- Experience with data integration tools and ETL processes.- Familiarity with data quality frameworks and best practices.- Proficient in programming languages such as Python or Scala for data manipulation.\nAdditional Information:- The candidate should have minimum 5 years of experience in Neo4j.- This position is based at our Bengaluru office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['scala', 'java', 'data modeling', 'python', 'neo4j', 'hive', 'pyspark', 'data warehousing', 'sql', 'spark', 'hadoop', 'data visualization', 'etl', 'big data', 'data manipulation', 'airflow', 'machine learning', 'data engineering', 'data quality', 'tableau', 'mapreduce', 'kafka', 'sqoop', 'aws', 'etl process']",2025-06-13 05:58:08
Data Engineer,Accenture,3 - 8 years,Not Disclosed,['Bengaluru'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Informatica MDM\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions that facilitate data generation, collection, and processing. Your typical day will involve creating data pipelines, ensuring data quality, and implementing ETL processes to effectively migrate and deploy data across various systems, contributing to the overall efficiency and reliability of data management within the organization.\nRoles & Responsibilities:- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Collaborate with cross-functional teams to gather requirements and translate them into technical specifications.- Monitor and optimize data pipelines for performance and reliability.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Informatica MDM.- Good To Have\n\n\n\n\nSkills:\nExperience with data warehousing concepts and practices.- Strong understanding of data modeling techniques and best practices.- Familiarity with SQL and database management systems.- Experience in implementing data governance and data quality frameworks.\nAdditional Information:- The candidate should have minimum 3 years of experience in Informatica MDM.- This position is based at our Bengaluru office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data warehousing', 'sql', 'data modeling', 'etl', 'informatica mdm', 'hive', 'python', 'data management', 'data engineering', 'data quality', 'tableau', 'spark', 'mdm', 'data governance', 'data warehousing concepts', 'technical specifications', 'hadoop', 'big data', 'informatica', 'etl process']",2025-06-13 05:58:10
Data Engineer,Accenture,12 - 15 years,Not Disclosed,['Bengaluru'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Data Engineering\n\n\n\n\nGood to have skills :Java Enterprise EditionMinimum\n\n\n\n12 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions that facilitate data generation, collection, and processing. Your typical day will involve creating data pipelines, ensuring data quality, and implementing ETL processes to migrate and deploy data across various systems. You will collaborate with cross-functional teams to understand their data needs and provide effective solutions, ensuring that the data infrastructure is robust and scalable to meet the demands of the organization.\nRoles & Responsibilities:- Expected to be an SME.- Collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Expected to provide solutions to problems that apply across multiple teams.- Mentor junior team members to enhance their skills and knowledge in data engineering.- Continuously evaluate and improve data processes to enhance efficiency and effectiveness.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Data Engineering.- Strong understanding of data modeling and database design principles.- Experience with ETL tools and frameworks.- Familiarity with cloud platforms such as AWS, Azure, or Google Cloud.- Knowledge of data warehousing concepts and technologies.\nAdditional Information:- The candidate should have minimum 12 years of experience in Data Engineering.- This position is based at our Bengaluru office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data engineering', 'database design', 'data modeling', 'design principles', 'aws', 'hive', 'python', 'scala', 'microsoft azure', 'data warehousing', 'java collections', 'sql', 'data quality', 'java', 'gcp', 'etl tool', 'spark', 'data warehousing concepts', 'hadoop', 'etl', 'big data', 'etl process']",2025-06-13 05:58:12
Data Engineer,Accenture,2 - 7 years,Not Disclosed,['Bengaluru'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Google Cloud Data Services\n\n\n\n\nGood to have skills :GCP Dataflow, Data EngineeringMinimum\n\n\n\n2 year(s) of experience is required\n\n\n\n\nEducational Qualification :standard 15 years\n\n\nSummary:As a Data Engineer, you will be responsible for designing, developing, and maintaining data solutions for data generation, collection, and processing. Your role involves creating data pipelines, ensuring data quality, and implementing ETL processes to migrate and deploy data across systems. You will play a crucial part in the data management process.\nRoles & Responsibilities:- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work-related problems.- Develop and maintain data solutions for data generation, collection, and processing.- Create data pipelines to streamline data flow.- Ensure data quality and integrity throughout the data lifecycle.- Implement ETL processes for data migration and deployment.- Collaborate with cross-functional teams to optimize data processes.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Google Cloud Data Services.- Good To Have\n\n\n\n\nSkills:\nExperience with Data Engineering and GCP Dataflow.- Strong understanding of cloud-based data services.- Experience in designing and implementing data pipelines.- Knowledge of ETL processes and data migration techniques.\nAdditional Information:- The candidate should have a minimum of 2 years of experience in Google Cloud Data Services.- This position is based at our Bengaluru office.- A standard 15 years of education is required.\n\nQualification\n\nstandard 15 years",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['gcp', 'etl', 'data services', 'google', 'data engineering', 'hive', 'data management', 'data warehousing', 'data migration', 'business intelligence', 'sql', 'plsql', 'data modeling', 'spark', 'hadoop', 'big data', 'python', 'sql server', 'data quality', 'tableau', 'aws', 'ssis', 'data flow', 'informatica', 'etl process']",2025-06-13 05:58:14
Data Engineer,Accenture,15 - 20 years,Not Disclosed,['Bengaluru'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Data Modeling Techniques and Methodologies\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n12 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL processes to migrate and deploy data across systems. Your day will involve working on data architecture and collaborating with cross-functional teams to optimize data processes.\nRoles & Responsibilities:- Expected to be an SME.- Collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Expected to provide solutions to problems that apply across multiple teams.- Lead data modeling initiatives to design and implement data structures.- Optimize data storage and retrieval processes.- Develop and maintain data pipelines for efficient data flow.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Data Modeling Techniques and Methodologies.- Strong understanding of database management systems.- Experience with data warehousing and ETL processes.- Knowledge of data governance and compliance.- Hands-on experience with data visualization tools.\nAdditional Information:- The candidate should have a minimum of 12 years of experience in Data Modeling Techniques and Methodologies.- This position is based at our Bengaluru office.PFB Education details- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['database management system', 'data warehousing', 'data modeling', 'data visualization', 'etl', 'hive', 'python', 'data architecture', 'data engineering', 'sql', 'database management', 'data quality', 'tableau', 'spark', 'data governance', 'data structures', 'hadoop', 'big data', 'data flow', 'etl process']",2025-06-13 05:58:16
Data Engineer,Accenture,15 - 20 years,Not Disclosed,['Pune'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Neo4j, Stardog\n\n\n\n\nGood to have skills :JavaMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions that facilitate data generation, collection, and processing. Your typical day will involve creating data pipelines, ensuring data quality, and implementing ETL processes to migrate and deploy data across various systems. You will collaborate with cross-functional teams to understand their data needs and provide effective solutions, ensuring that the data infrastructure is robust and scalable to meet the demands of the organization.\nRoles & Responsibilities:- Expected to be an SME.- Collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Mentor junior team members to enhance their skills and knowledge in data engineering.- Continuously evaluate and improve data processes to enhance efficiency and effectiveness.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Neo4j.- Good To Have\n\n\n\n\nSkills:\nExperience with Java.- Strong understanding of data modeling and graph database concepts.- Experience with data integration tools and ETL processes.- Familiarity with data quality frameworks and best practices.- Proficient in programming languages such as Python or Scala for data manipulation.\nAdditional Information:- The candidate should have minimum 5 years of experience in Neo4j.- This position is based at our Bengaluru office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['scala', 'java', 'data modeling', 'python', 'neo4j', 'hive', 'pyspark', 'data warehousing', 'sql', 'spark', 'hadoop', 'data visualization', 'etl', 'big data', 'data manipulation', 'airflow', 'machine learning', 'data engineering', 'data quality', 'tableau', 'mapreduce', 'kafka', 'sqoop', 'aws', 'etl process']",2025-06-13 05:58:18
Data Engineer,Accenture,15 - 20 years,Not Disclosed,['Bhubaneswar'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :PySpark, Microsoft Azure Databricks, Microsoft Azure Analytics Services, Microsoft Azure Data Services\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions that facilitate data generation, collection, and processing. Your typical day will involve creating data pipelines, ensuring data quality, and implementing ETL processes to migrate and deploy data across various systems. You will collaborate with cross-functional teams to understand data requirements and deliver effective solutions that meet business needs.\nRoles & Responsibilities:- Expected to be an SME.- Collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Mentor junior team members to enhance their skills and knowledge.- Continuously evaluate and improve data processes to enhance efficiency.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in PySpark, Microsoft Azure Databricks, Microsoft Azure Data Services, Microsoft Azure Analytics Services.- Strong experience in designing and implementing data pipelines.- Proficient in data modeling and database design.- Familiarity with data warehousing concepts and technologies.- Experience with data quality and data governance practices.\nAdditional Information:- The candidate should have minimum 5 years of experience in PySpark.- This position is based at our Bengaluru office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['azure databricks', 'data services', 'pyspark', 'microsoft azure', 'data modeling', 'hive', 'python', 'analytics services', 'azure analytics', 'data warehousing', 'data engineering', 'sql', 'database design', 'data quality', 'spark', 'data governance', 'data warehousing concepts', 'etl', 'etl process']",2025-06-13 05:58:20
Data Engineer,Accenture,15 - 20 years,Not Disclosed,['Bengaluru'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Data Architecture Principles\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n12 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions for data generation, collection, and processing. You will create data pipelines, ensure data quality, and implement ETL processes to migrate and deploy data across systems. Your day will involve working on various data-related tasks and collaborating with teams to optimize data processes.\nRoles & Responsibilities:- Expected to be an SME.- Collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Expected to provide solutions to problems that apply across multiple teams.- Develop innovative data solutions to meet business requirements.- Optimize data pipelines for efficiency and scalability.- Implement data governance policies to ensure data quality and security.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Data Architecture Principles.- Strong understanding of data modeling and database design.- Experience with ETL tools and processes.- Knowledge of cloud platforms and big data technologies.- Good To Have\n\n\n\n\nSkills:\nData management and governance expertise.\nAdditional Information:- The candidate should have a minimum of 12 years of experience in Data Architecture Principles.- This position is based at our Bengaluru office.Education information - - A 15 years full-time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data management', 'data architecture', 'database design', 'data architecture principles', 'data modeling', 'hive', 'python', 'big data technologies', 'cloud platforms', 'data engineering', 'sql', 'data quality', 'etl tool', 'spark', 'data governance', 'hadoop', 'etl', 'big data', 'etl process']",2025-06-13 05:58:22
S&C Global Network - AI - Life Sciences -Data Science Sr. Manager,Accenture,11 - 15 years,Not Disclosed,['Bengaluru'],"JR:\n\n\n\nR00229254\n\n\n\nExperience:\n\n\n\n11-15 Years\n\n\n\n\nEducational Qualification:\n\n\n\nBachelors or Masters degree in Statistics, Data Science, Applied Mathematics, Business Analytics, Computer Science, Information Systems, or other Quantitative field.\n\n\n\n---------------------------------------------------------------------\n\n\n\nJob Title -\n\n\n\nS&C Global Network - AI - Healthcare Analytics - Senior Manager\n\n\n\nManagement Level:\n\n\n\n6-Senior Manager\n\n\n\nLocation:\n\n\n\nBangalore/Gurgaon\n\n\n\nMust-have skills:R,Phython,SQL,Spark,Tableau ,Power BI\n\n\n\n\nGood to have skills:Ability to leverage design thinking, business process optimization, and stakeholder management skills.\n\n\n\nJob\n\n\nSummary:\n\nThis role involves driving strategic initiatives, managing business transformations, and leveraging industry expertise to create value-driven solutions.\n\n\n\n\nRoles & Responsibilities:\n\nProvide strategic advisory services, conduct market research, and develop data-driven recommendations to enhance business performance.\n\nAs part of our Data & AI practice, you will join a worldwide network of smart and driven colleagues experienced in leading AI/ML/Statistical tools, methods and applications. From data to analytics and insights to actions, our forward-thinking consultants provide analytically-informed, issue-based insights at scale to help our clients improve outcomes and achieve high performance.\n\n\n\nWHATS IN IT FOR YOU\nAn opportunity to work on high-visibility projects with top Pharma clients around the globe.\nPotential to Co-create with leaders in strategy, industry experts, enterprise function practitioners, and business intelligence professionals to shape and recommend innovative solutions that leverage emerging technologies.\nAbility to embed responsible business into everythingfrom how you service your clients to how you operate as a responsible professional.\nPersonalized training modules to develop your strategy & consulting acumen to grow your skills, industry knowledge, and capabilities.\nOpportunity to thrive in a culture that is committed to accelerating equality for all. Engage in boundaryless collaboration across the entire organization.\n\n\n\n\nWhat you would do in this role\nLead proposals, and business development efforts and coordinate with other colleagues to cross-sell/ up-sell Life Sciences offerings to existing as well as potential clients.\nLead client discussions, developing new industry Point of View (PoV), re-usable assets (tools)\nCollaborate closely with cross-functional teams including Data engineering, technology, and business stakeholders to identify opportunities for leveraging data to drive business solutions.\nLead and manage teams to deliver transformative and innovative client projects.\nGuide teams on analytical and AI methods and approaches\nManage client relationships to foster trust, deliver value, and build the Accenture brand\nDrive consulting practice innovation and thought leadership in your area of specialization\nSupport strategies and operating models focused on some business units and assess likely competitive responses. Also, assess implementation readiness and points of greatest impact.\nExecute a transformational change plan aligned with the clients business strategy and context for change. Engage stakeholders in the change journey and build commitment to change.\n\n\n\n\n\nProfessional & Technical\n\n\n\n\nSkills:\n\n\n- Relevant experience in the required domain.\n\n- Strong analytical, problem-solving, and communication skills.\n\n- Ability to work in a fast-paced, dynamic environment.\nProven experience in cross-sell/ up-sell\nLeverage ones hands-on experience of working across one or more of these areas such as real-world evidence data, R&D clinical data, and digital marketing data.\nExperience with handling Datasets like Komodo, RAVE, IQVIA, Truven, Optum, SHS, Specialty Pharmacy, PSP, etc.\nExperience in building and deployment of Statistical Models/Machine Learning including Segmentation & predictive modeling, hypothesis testing, multivariate statistical analysis, time series techniques, and optimization.\nExcellent analytical and problem-solving skills, with a data-driven mindset.\nAbility to solve complex business problems and deliver client delight.\nStrong writing skills to build points of view on current industry trends.\nGood Client handling skills; able to demonstrate thought leadership & problem-solving skills.\n\n\n\n\n\nAdditional Information:\n\n- Opportunity to work on innovative projects.\n\n- Career growth and leadership exposure.\n\n\n\n\n\nAbout Our Company | AccentureQualification\n\n\n\nExperience:\n\n\n\n11-15 Years\n\n\n\n\nEducational Qualification:\n\n\n\nBachelors or Masters degree in Statistics, Data Science, Applied Mathematics, Business Analytics, Computer Science, Information Systems, or other Quantitative field.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'power bi', 'sql', 'tableau', 'r', 'hypothesis testing', 'time series', 'business analytics', 'machine learning', 'data engineering', 'business intelligence', 'artificial intelligence', 'data science', 'computer science', 'spark', 'predictive modeling', 'segmentation', 'statistics', 'ml']",2025-06-13 05:58:24
S&C GN - Data&AI - Life Sciences - Analyst,Accenture,2 - 7 years,Not Disclosed,['Bengaluru'],"Management Level:Ind & Func AI Decision Science Analyst\n\n\n\n\nJob Location:Bangalore / Gurgaon\n\n\n\nMust-have\n\n\n\n\nSkills:\nLife Sciences/Pharma/Healthcare projects and delivering successful outcomes, commercial, clinical, Statistical Models/Machine Learning including Segmentation & predictive modeling, hypothesis testing, multivariate statistical analysis, time series techniques, and optimization.\n\n\n\nGood-to-have\n\n\n\n\nSkills:\nProficiency in Programming languages such as R, Python, SQL, Spark, AWS, Azure, or Google Cloud for deploying and scaling language models, Data Visualization tools like Tableau, Power BI\n\n\n\nJob\n\n\nSummary\n\nWe are seeking an experienced and visionary - Accenture S&C Global Network - Data & AI practice help our clients grow their business in entirely new ways. Analytics enables our clients to achieve high performance through insights from data - insights that inform better decisions and strengthen customer relationships. From strategy to execution, Accenture works with organizations to develop analytic capabilities - from accessing and reporting on data to predictive modelling - to outperform the competition.\n\n\n\nKey Responsibilities\nSupport delivery of small to medium-sized teams to deliver consulting projects for global clients.\nAn opportunity to work on high-visibility projects with top Pharma clients around the globe.\nPersonalized training modules to develop your strategy & consulting acumen to grow your skills, industry knowledge, and capabilities.\nResponsibilities may include strategy, implementation, process design, and change management for specific modules.\nWork with the team or as an Individual contributor on the project assigned which includes a variety of skills to be utilized from Data Engineering to Data Science\nDevelop assets and methodologies, point-of-view, research, or white papers for use by the team and the larger community.\nWork on variety of projects in Data Modeling, Data Engineering, Data Visualization, Data Science etc.,\nAcquire new skills that have utility across industry groups.\n\n\n\n\n\nAdditional Information\nAbility to solve complex business problems and deliver client delight.\nStrong writing skills to build points of view on current industry trends.\nGood communication, interpersonal, and presentation skills\n\n\nAbout Our Company | Accenture (do not remove the hyperlink)\n\n\nQualification\n\n\n\nExperience:Proven experience (2+ years) in working on Life Sciences/Pharma/Healthcare projects and delivering successful outcomes.\n\n\n\n\nEducational Qualification:Bachelors or Masters degree in Statistics, Data Science, Applied Mathematics, Business Analytics, Computer Science, Information Systems, or other Quantitative field.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['pharmaceutical', 'python', 'sql', 'life sciences', 'aws', 'presentation skills', 'microsoft azure', 'power bi', 'time series', 'machine learning', 'artificial intelligence', 'tableau', 'r', 'data science', 'gcp', 'spark', 'predictive modeling', 'statistical modeling', 'data visualization', 'statistics']",2025-06-13 05:58:26
Senior PySpark Data Engineer,Synechron,7 - 12 years,Not Disclosed,"['Pune', 'Hinjewadi']","Job Summary\nSynechron is seeking an experienced and technically proficient Senior PySpark Data Engineer to join our data engineering team. In this role, you will be responsible for developing, optimizing, and maintaining large-scale data processing solutions using PySpark. Your expertise will support our organizations efforts to leverage big data for actionable insights, enabling data-driven decision-making and strategic initiatives.\nSoftware Requirements\nRequired Skills:\nProficiency in PySpark\nFamiliarity with Hadoop ecosystem components (e.g., HDFS, Hive, Spark SQL)\nExperience with Linux/Unix operating systems\nData processing tools like Apache Kafka or similar streaming platforms\nPreferred Skills:\nExperience with cloud-based big data platforms (e.g., AWS EMR, Azure HDInsight)\nKnowledge of Python (beyond PySpark), Java or Scala relevant to big data applications\nFamiliarity with data orchestration tools (e.g., Apache Airflow, Luigi)\nOverall Responsibilities\nDesign, develop, and optimize scalable data processing pipelines using PySpark.\nCollaborate with data engineers, data scientists, and business analysts to understand data requirements and deliver solutions.\nImplement data transformations, aggregations, and extraction processes to support analytics and reporting.\nManage large datasets in distributed storage systems, ensuring data integrity, security, and performance.\nTroubleshoot and resolve performance issues within big data workflows.\nDocument data processes, architectures, and best practices to promote consistency and knowledge sharing.\nSupport data migration and integration efforts across varied platforms.\nStrategic Objectives:\nEnable efficient and reliable data processing to meet organizational analytics and reporting needs.\nMaintain high standards of data security, compliance, and operational durability.\nDrive continuous improvement in data workflows and infrastructure.\nPerformance Outcomes & Expectations:\nEfficient processing of large-scale data workloads with minimum downtime.\nClear, maintainable, and well-documented code.\nActive participation in team reviews, knowledge transfer, and innovation initiatives.\nTechnical Skills (By Category)\nProgramming Languages:\nRequired: PySpark (essential); Python (needed for scripting and automation)\nPreferred: Java, Scala\nDatabases/Data Management:\nRequired: Experience with distributed data storage (HDFS, S3, or similar) and data warehousing solutions (Hive, Snowflake)\nPreferred: Experience with NoSQL databases (Cassandra, HBase)\nCloud Technologies:\nRequired: Familiarity with deploying and managing big data solutions on cloud platforms such as AWS (EMR), Azure, or GCP\nPreferred: Cloud certifications\nFrameworks and Libraries:\nRequired: Spark SQL, Spark MLlib (basic familiarity)\nPreferred: Integration with streaming platforms (e.g., Kafka), data validation tools\nDevelopment Tools and Methodologies:\nRequired: Version control systems (e.g., Git), Agile/Scrum methodologies\nPreferred: CI/CD pipelines, containerization (Docker, Kubernetes)\nSecurity Protocols:\nOptional: Basic understanding of data security practices and compliance standards relevant to big data management\nExperience Requirements\nMinimum of 7+ years of experience in big data environments with hands-on PySpark development.\nProven ability to design and implement large-scale data pipelines.\nExperience working with cloud and on-premises big data architectures.\nPreference for candidates with domain-specific experience in finance, banking, or related sectors.\nCandidates with substantial related experience and strong technical skills in big data, even from different domains, are encouraged to apply.\nDay-to-Day Activities\nDevelop, test, and deploy PySpark data processing jobs to meet project specifications.\nCollaborate in multi-disciplinary teams during sprint planning, stand-ups, and code reviews.\nOptimize existing data pipelines for performance and scalability.\nMonitor data workflows, troubleshoot issues, and implement fixes.\nEngage with stakeholders to gather new data requirements, ensuring solutions are aligned with business needs.\nContribute to documentation, standards, and best practices for data engineering processes.\nSupport the onboarding of new data sources, including integration and validation.\nDecision-Making Authority & Responsibilities:\nIdentify performance bottlenecks and propose effective solutions.\nDecide on appropriate data processing approaches based on project requirements.\nEscalate issues that impact project timelines or data integrity.\nQualifications\nBachelors degree in Computer Science, Information Technology, or related field. Equivalent experience considered.\nRelevant certifications are preferred: Cloudera, Databricks, AWS Certified Data Analytics, or similar.\nCommitment to ongoing professional development in data engineering and big data technologies.\nDemonstrated ability to adapt to evolving data tools and frameworks.\nProfessional Competencies\nStrong analytical and problem-solving skills, with the ability to model complex data workflows.\nExcellent communication skills to articulate technical solutions to non-technical stakeholders.\nEffective teamwork and collaboration in a multidisciplinary environment.\nAdaptability to new technologies and emerging trends in big data.\nAbility to prioritize tasks effectively and manage time in fast-paced projects.\nInnovation mindset, actively seeking ways to improve data infrastructure and processes.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['PySpark', 'S3', 'Unix operating systems', 'Spark SQL', 'Luigi', 'HDFS', 'AWS EMR', 'Apache Airflow', 'Hive', 'Linux', 'Azure HDInsight', 'Apache Kafka', 'AWS']",2025-06-13 05:58:27
Senior - AWS Data Engineering,KPMG India,4 - 8 years,Not Disclosed,['Gurugram'],"KPMG India is looking for Senior - AWS Data Engineering to join our dynamic team and embark on a rewarding career journey Designs and builds scalable data pipelines using AWS servicesOptimizes data ingestion, storage, and processingCollaborates with data scientists and analystsEnsures performance, security, and compliance",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Networking', 'Focus', 'Manager Technology', 'professional services', 'AWS', 'international clients']",2025-06-13 05:58:29
Senior Data Engineer,Randstad Global,5 - 8 years,Not Disclosed,['Hyderabad'],"Key Responsibilities\n\nDesign, develop, and maintain data pipelines and ETL processes focused on data transformation using SQL, Python, and DBT.\nDevelop complex SQL queries for data extraction, transformation, and loading (ETL), with focus on aggregation, cleansing, and modeling.\nLeverage Python to automate transformation tasks and implement custom logic in data workflows.\nBuild and manage DBT models for reusable, maintainable, and scalable data pipelines.\nAssemble reusable and scalable datasets aligned with business needs.\nEnsure data accuracy, consistency, and completeness through thorough validation, testing, and documentation.\nDevelop and maintain data storage solutions and implement transformation logic for analysis and reporting.\nCollaborate with cross-functional stakeholders to understand data requirements and provide actionable insights.\nTroubleshoot and resolve data-related technical issues and identify opportunities to improve data quality and reliability.\nDocument technical specifications and data transformation processes.\nAdhere to information security policies, ensure data compliance with PII, GDPR, and other relevant regulations.\nRequired Skills & Experience\n\nHands-on experience with Google Cloud Platform (GCP) and Google BigQuery.\nStrong expertise in SQL, database design, and query optimization.\nProven experience in designing and implementing ETL pipelines and data transformation flows.\nTechnical proficiency in data modeling, data mining, and segmentation techniques.\nExcellent numerical, analytical, and problem-solving skills.\nStrong attention to detail and a critical mindset for evaluating information.\nEffective stakeholder management and ability to handle multiple priorities.\nExcellent verbal and written communication skills.\nAbility to self-manage workload and work collaboratively within a team.\nDesirable Skills\n\nProficiency in SQL and Python programming.\nHands-on experience with DBT and version control tools like GitLab.\nFamiliarity with complex financial data models.\nExperience with data visualization tools such as Tableau or Google Data Studio.\nExposure to Salesforce and Salesforce Einstein Analytics.\nUnderstanding of Agile/Scrum methodologies.\nBachelor's degree in Computer Science, Information Technology, or a related field.\nData engineering certification and basic knowledge of AI/ML fundamentals are a plus.",Industry Type: Recruitment / Staffing,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Python', 'Airflow', 'Google Cloud Platforms', 'ETL', 'SQL']",2025-06-13 05:58:31
Senior Data Engineer - Azure,Blend360 India,6 - 11 years,Not Disclosed,['Hyderabad'],"Job Description\nAs a Senior Data Engineer, your role is to spearhead the data engineering teams and elevate the team to the next level! You will be responsible for laying out the architecture of the new project as well as selecting the tech stack associated with it. You will plan out the development cycles deploying AGILE if possible and create the foundations for good data stewardship with our new data products!\nYou will also set up a solid code framework that needs to be built to purpose yet have enough flexibility to adapt to new business use cases tough but rewarding challenge!\n\nResponsibilities\nCollaborate with several stakeholders to deeply understand the needs of data practitioners to deliver at scale\nLead Data Engineers to define, build and maintain Data Platform\nWork on building Data Lake in Azure Fabric processing data from multiple sources\nMigrating existing data store from Azure Synapse to Azure Fabric\nImplement data governance and access control\nDrive development effort End-to-End for on-time delivery of high-quality solutions that conform to requirements, conform to the architectural vision, and comply with all applicable standards.\nPresent technical solutions, capabilities, considerations, and features in business terms. Effectively communicate status, issues, and risks in a precise and timely manner.\nFurther develop critical initiatives, such as Data Discovery, Data Lineage and Data Quality\nLeading team and Mentor junior resources\nHelp your team members grow in their role and achieve their career aspirations\nBuild data systems, pipelines, analytical tools and programs\nConduct complex data analysis and report on results\n\n\nQualifications\n5+ Years of Experience as a data engineer or similar role in Azure Synapses, ADF or\nrelevant exp in Azure Fabric\nDegree in Computer Science, Data Science, Mathematics, IT, or similar fie",Industry Type: Advertising & Marketing,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Data analysis', 'Data modeling', 'Analytical', 'Agile', 'data governance', 'Data quality', 'Data mining', 'SQL', 'Python']",2025-06-13 05:58:32
Senior Data Engineer - Azure,Blend360 India,6 - 11 years,Not Disclosed,['Hyderabad'],"Job Description\nAs a Senior Data Engineer, your role is to spearhead the data engineering teams and elevate the team to the next level! You will be responsible for laying out the architecture of the new project as well as selecting the tech stack associated with it. You will plan out the development cycles deploying AGILE if possible and create the foundations for good data stewardship with our new data products!\nYou will also set up a solid code framework that needs to be built to purpose yet have enough flexibility to adapt to new business use cases tough but rewarding challenge!\n\nResponsibilities\nCollaborate with several stakeholders to deeply understand the needs of data practitioners to deliver at scale\nLead Data Engineers to define, build and maintain Data Platform\nWork on building Data Lake in Azure Fabric processing data from multiple sources\nMigrating existing data store from Azure Synapse to Azure Fabric\nImplement data governance and access control\nDrive development effort End-to-End for on-time delivery of high-quality solutions that conform to requirements, conform to the architectural vision, and comply with all applicable standards.\nPresent technical solutions, capabilities, considerations, and features in business terms. Effectively communicate status, issues, and risks in a precise and timely manner.\nFurther develop critical initiatives, such as Data Discovery, Data Lineage and Data Quality\nLeading team and Mentor junior resources\nHelp your team members grow in their role and achieve their career aspirations\nBuild data systems, pipelines, analytical tools and programs\nConduct complex data analysis and report on results",Industry Type: Advertising & Marketing,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Access control', 'Data analysis', 'Team leading', 'Architecture', 'Analytical', 'Agile', 'data governance', 'Data processing', 'Mentor', 'Data quality']",2025-06-13 05:58:34
"Senior Manager, Senior Data Engineer",Merck Sharp & Dohme (MSD),6 - 11 years,Not Disclosed,['Hyderabad'],"Senior Manager, Data Engineer\nThe Opportunity\nBased in Hyderabad, join a global healthcare biopharma company and be part of a 130- year legacy of success backed by ethical integrity, forward momentum, and an inspiring mission to achieve new milestones in global healthcare.\nBe part of an organisation driven by digital technology and data-backed approaches that support a diversified portfolio of prescription medicines, vaccines, and animal health products.\nDrive innovation and execution excellence. Be a part of a team with passion for using data, analytics, and insights to drive decision-making, and which creates custom software, allowing us to tackle some of the worlds greatest health threats.\nOur Technology Centers focus on creating a space where teams can come together to deliver business solutions that save and improve lives. An integral part of our company s IT operating model, Tech Centers are globally distributed locations where each IT division has employees to enable our digital transformation journey and drive business outcomes. These locations, in addition to the other sites, are essential to supporting our business and strategy.\nA focused group of leaders in each Tech Center helps to ensure we can manage and improve each location, from investing in growth, success, and well-being of our people, to making sure colleagues from each IT division feel a sense of belonging to managing critical emergencies. And together, we must leverage the strength of our team to collaborate globally to optimize connections and share best practices across the Tech Centers.\nRole Overview\nResponsibilities\nDesigns, builds, and maintains data pipeline architecture - ingest, process, and publish data for consumption.\nBatch processes collected data, formats data in an optimized way to bring it analyze-ready\nEnsures best practices sharing and across the organization\nEnables delivery of data-analytics projects\nDevelops deep knowledge of the companys supported technology; understands the whole complexity/dependencies between multiple teams, platforms (people, technologies)\nCommunicates intensively with other platform/competencies to comprehend new trends and methodologies being implemented/considered within the company ecosystem\nUnderstands the customer and stakeholders business needs/priorities and helps building solutions that support our business goals\nEstablishes and manages the close relationship with customers/stakeholders\nHas overview of the date engineering market development to be able to come up/explore new ways of delivering pipelines to increase their value/contribution\nBuilds community of practice leveraging experience from delivering complex analytics projects\nIs accountable for ensuring that the team delivers solutions with high quality standards, timeliness, compliance and excellent user experience\nContributes to innovative experiments, specifically to idea generation, idea incubation and/or experimentation, identifying tangible and measurable criteria\nQualifications:\nBachelor s degree in Computer Science, Data Science, Information Technology, Engineering or a related field.\n3+ plus years of experience as a Data Engineer or in a similar role, with a strong portfolio of data projects.\n3+ plus years experience SQL skills, with the ability to write and optimize queries for large datasets.\n1+ plus years experience and proficiency in Python for data manipulation, automation, and pipeline development.\nExperience with Databricks including creating notebooks and utilizing Spark for big data processing.\nStrong experience with data warehousing solution (such as Snowflake), including schema design and performance optimization.\nExperience with data governance and quality management tools, particularly Collibra DQ.\nStrong analytical and problem-solving skills, with an attention to detail.\nSAP Basis experience working on SAP S/4HANA deployments on Cloud platforms (example: AWS, GCP or Azure).\nOur technology teams operate as business partners, proposing ideas and innovative solutions that enable new organizational capabilities. We collaborate internationally to deliver services and solutions that help everyone be more productive and enable innovation.\nWho we are:\nWhat we look for:\n#HYDIT\nCurrent Employees apply HERE\nCurrent Contingent Workers apply HERE\nSearch Firm Representatives Please Read Carefully\nEmployee Status:\nRegular\nRelocation:\nVISA Sponsorship:\nTravel Requirements:\nFlexible Work Arrangements:\nHybrid\nShift:\nValid Driving License:\nHazardous Material(s):\n\nRequired Skills:\nBusiness, Business, Business Intelligence (BI), Business Management, Contractor Management, Cost Reduction, Database Administration, Database Optimization, Data Engineering, Data Flows, Data Infrastructure, Data Management, Data Modeling, Data Optimization, Data Quality, Data Visualization, Design Applications, ETL Tools, Information Management, Management Process, Operating Cost Reduction, Senior Program Management, Social Collaboration, Software Development, Software Development Life Cycle (SDLC) {+ 1 more}\n\nPreferred Skills:\nJob Posting End Date:\n08/13/2025\n*A job posting is effective until 11:59:59PM on the day BEFORE the listed job posting end date. Please ensure you apply to a job posting no later than the day BEFORE the job posting end date.",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Automation', 'Data management', 'Data modeling', 'Analytical', 'Healthcare', 'Data processing', 'Business intelligence', 'Information technology', 'Analytics', 'SQL']",2025-06-13 05:58:35
Hadoop Data Engineer/ Senior Software Engineer,Hsbc,2 - 11 years,Not Disclosed,['Pune'],"Some careers shine brighter than others.\nIf you re looking for a career that will help you stand out, join HSBC and fulfil your potential. Whether you want a career that could take you to the top, or simply take you in an exciting new direction, HSBC offers opportunities, support and rewards that will take you further.\nHSBC is one of the largest banking and financial services organisations in the world, with operations in 64 countries and territories. We aim to be where the growth is, enabling businesses to thrive and economies to prosper, and, ultimately, helping people to fulfil their hopes and realise their ambitions.\nWe are currently seeking an experienced professional to join our team in the role of Software Engineer\nIn this role you will be\nExpertise in Scala-Spark/Python-Spark development and should be able to Work with Agile application dev team to implement data strategies.\nDesign and implement scalable data architectures to support the banks data needs.\nDevelop and maintain ETL (Extract, Transform, Load) processes.\nEnsure the data infrastructure is reliable, scalable, and secure.\nOversee the integration of diverse data sources into a cohesive data platform.\nEnsure data quality, data governance, and compliance with regulatory requirements.\nMonitor and optimize data pipeline performance.\nTroubleshoot and resolve data-related issues promptly.\nImplement monitoring and alerting systems for data processes.\nTroubleshoot and resolve technical issues optimizing system performance ensuring reliability.\nCreate and maintain technical documentation for new and existing system ensuring that information is accessible to the team.\nImplementing and monitoring solutions that identify both system bottlenecks and production issues.\n\n\n\n\n\n\n\n\n\n\nRequirements\n\n\n\nTo be successful in this role, you should meet the following requirements:\nExperience in data engineering or related field and hands-on experience of building and maintenance of ETL Data pipelines\nGood experience in Designing and Developing Spark Applications using Scala or Python.\nGood experience with database technologies (SQL, NoSQL), data warehousing solutions, and big data technologies (Hadoop, Spark)\nProficiency in programming languages such as Python, Java, or Scala.\nOptimization and Performance Tuning of Spark Applications\nGIT Experience on creating, merging and managing Repos.\nPerform unit testing and performance testing.\nGood understanding of ETL processes and data pipeline orchestration tools like Airflow, Control-M.\nStrong problem-solving skills and ability to work under pressure.\nExcellent communication and interpersonal skills.",Industry Type: Consumer Electronics & Appliances,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Performance tuning', 'Performance testing', 'Agile', 'Control-M', 'Data quality', 'Unit testing', 'Financial services', 'SQL', 'Python', 'Technical documentation']",2025-06-13 05:58:37
"Senior Staff Engineer, Big Data Engineer",Nagarro,9 - 13 years,Not Disclosed,['India'],"We're Nagarro.\nWe are a Digital Product Engineering company that is scaling in a big way! We build products, services, and experiences that inspire, excite, and delight. We work at scale across all devices and digital mediums, and our people exist everywhere in the world (18000+ experts across 38 countries, to be exact). Our work culture is dynamic and non-hierarchical. We're looking for great new colleagues. That's where you come in!\n\nREQUIREMENTS:\nTotal experience 9+ years.\nExcellent knowledge and experience in Big data engineer.\nStrong working experience with architecture and development in Apache Spark, Spark, Python, Azure Databricks, Data Pipelines, Azure Devops, Kafka, SQL Server/NoSQL.\nStrong expertise in Python, Django Rest Framework, Databricks and PostgreSQL.\nHands on experience in building data pipelines and building data frameworks for unit testing, data lineage tracking, and automation.\nFamiliarity with streaming technologies (e.g., Kafka, Kinesis, Flink).\nExperience with building and maintaining a cloud system.\nFamiliarity with data modeling, data warehousing, and building distributed systems.\nExpertise in Spanner for high-availability, scalable database solutions.\nKnowledge of data governance and security practices in cloud-based environments.\nProblem-solving mindset with the ability to tackle complex data engineering challenges.\nStrong communication and teamwork skills, with the ability to mentor and collaborate effectively.\n\nRESPONSIBILITIES:\nWriting and reviewing great quality code\nUnderstanding the clients business use cases and technical requirements and be able to convert them in to technical design which elegantly meets the requirements\nMapping decisions with requirements and be able to translate the same to developers\nIdentifying different solutions and being able to narrow down the best option that meets the clients requirements\nDefining guidelines and benchmarks for NFR considerations during project implementation\nWriting and reviewing design document explaining overall architecture, framework, and high-level design of the application for the developers\nReviewing architecture and design on various aspects like extensibility, scalability, security, design patterns, user experience, NFRs, etc., and ensure that all relevant best practices are followed\nDeveloping and designing the overall solution for defined functional and non-functional requirements; and defining technologies, patterns, and frameworks to materialize it\nUnderstanding and relating technology integration scenarios and applying these learnings in projects\nResolving issues that are raised during code/review, through exhaustive systematic analysis of the root cause, and being able to justify the decision taken\nCarrying out POCs to make sure that suggested design/technologies meet the requirements",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Big Data', 'Spark', 'Data Bricks', 'Python', 'Pyspark', 'Django Framework', 'Azure Databricks', 'SQL Server']",2025-06-13 05:58:38
Big Data Lead,Hexaware Technologies,8 - 13 years,18-25 Lacs P.A.,"['Chennai', 'Bengaluru', 'Mumbai (All Areas)']","As an Azure Data Engineer, we are looking for candidates who possess expertise in the following:\nDatabricks\nData Factory\nSQL\nPyspark/Spark\n\nRoles and Responsibilities:",,,,"['Databricks', 'Sql', 'Python']",2025-06-13 05:58:40
"Data Engineer - Snowflake, Azure Data Factory (ADF)",Suzva Software Technologies,0 - 1 years,Not Disclosed,['Mumbai'],"We are seeking an experienced Data Engineer to join our team for a 6-month contract assignment. The ideal candidate will work on data warehouse development, ETL pipelines, and analytics enablement using Snowflake, Azure Data Factory (ADF), dbt, and other tools.\n\nThis role requires strong hands-on experience with data integration platforms, documentation, and pipeline optimizationespecially in cloud environments such as Azure and AWS.\n\n#KeyResponsibilities\nBuild and maintain ETL pipelines using Fivetran, dbt, and Azure Data Factory\n\nMonitor and support production ETL jobs\n\nDevelop and maintain data lineage documentation for all systems\n\nDesign data mapping and documentation to aid QA/UAT testing\n\nEvaluate and recommend modern data integration tools\n\nOptimize shared data workflows and batch schedules\n\nCollaborate with Data Quality Analysts to ensure accuracy and integrity of data flows\n\nParticipate in performance tuning and improvement recommendations\n\nSupport BI/MDM initiatives including Data Vault and Data Lakes\n\n#RequiredSkills\n7+ years of experience in data engineering roles\n\nStrong command of SQL, with 5+ years of hands-on development\n\nDeep experience with Snowflake, Azure Data Factory, dbt\n\nStrong background with ETL tools (Informatica, Talend, ADF, dbt, etc.)\n\nBachelor's in CS, Engineering, Math, or related field\n\nExperience in healthcare domain (working with PHI/PII data)\n\nFamiliarity with scripting/programming (Python, Perl, Java, Linux-based environments)\n\nExcellent communication and documentation skills\n\nExperience with BI tools like Power BI, Cognos, etc.\n\nOrganized, self-starter with strong time-management and critical thinking abilities\n\n#NiceToHave\nExperience with Data Lakes and Data Vaults\n\nQA & UAT alignment with clear development documentation\nMulti-cloud experience (especially Azure, AWS)",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Java', 'Azure', 'Power BI', 'UAT', 'Perl', 'QA', 'Azure Data Factory', 'Linux', 'Cognos', 'Snowflake', 'ETL', 'AWS', 'Python']",2025-06-13 05:58:41
Data Catalogue - Analyst,AstraZeneca India Pvt. Ltd,1 - 8 years,Not Disclosed,['Chennai'],"Job Title: Data Catalogue Analyst\nCareer Leve : C3\nIntroduction to role:\nAre you ready to make a significant impact in the world of data management? As a Data Catalogue Analyst, youll play a crucial role in ensuring that data is findable, accessible, and fit for use across various business units. Youll be responsible for capturing metadata and developing our data catalogue, supporting the Commercial and Enabling Units business areas. This is your chance to contribute to meaningful work that drives excellence and breakthroughs.\nAccountabilities:\nSupport the Data Catalogue Principal to define Information Asset Registers across business areas to help profile information risk/value\nParticipate in projects to mitigate and control identified priority risk areas\nTake responsibility for nominated markets/business areas, develop domain knowledge and leverage internal customer relationships to respond to localised use cases\nAct as point of contact for nominated business areas or markets\nSupport initiatives to enhance the reusability and transparency of our data by making it available in our global data catalogue\nSupport the capture of user requirements for functionality and usability, and document technical requirements\nWork with IT partners to capture metadata for relevant data sets and lineage, and populate the catalogue\nWork with data stewards and business users to enrich catalogue entries with business data dictionary, business rules, glossaries\nComplete monitoring controls to assure metadata quality remains at a high level\nSupport catalogue principles and data governance leads for tool evaluation and UAT\nEssential Skills/Experience:\nDemonstrable experience of working in a data management, data governance or data engineering domain\nStrong business and system analysis skills\nDemonstrable experience with Data Catalogue, Search and Automation software (Collibra, Informatica, Talend etc)\nAbility to interpret and communicate technical information into business language and in alignment with AZ business\nSolid grasp of metadata harvesting methodologies and ability to create business and technical metadata sets.\nStrong engagement, communication and collaborator management skills, including excellent organizational, presentation and influencing skills\nHigh level of proficiency with common business applications (Excel, Visio, Word, PowerPoint & SAP business user)\nDesirable Skills/Experience:\nDemonstrable experience of working with Commercial or Finance data and systems (Veeva, Reltio, SAP) and consumption\nDomain knowledge of life sciences/pharmaceuticals; manufacturing; corporate finance; or sales & marketing\nExperience with data quality and profiling software\nExperience of working in a complex, diverse global organization\nAstraZeneca offers an environment where you can apply your skills to genuinely impact patients lives. With a focus on innovation and growth, youll be part of a team that challenges norms and embraces intelligent risks. Our collaborative community thrives on sharing knowledge and celebrating successes together. Here, youll find opportunities to learn from diverse perspectives, drive change, and contribute to our digital transformation journey.\nReady to take the next step in your career? Apply now and become a key player in shaping the future at AstraZeneca!\n11-Jun-2025\n11-Jun-2025",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Automation', 'SAP', 'Data management', 'Senior Analyst', 'Life sciences', 'Corporate finance', 'Data quality', 'Visio', 'Monitoring', 'Recruitment']",2025-06-13 05:58:43
"Data Eng, Mgmt & Governance Analyst",Accenture,3 - 5 years,Not Disclosed,['Gurugram'],"Skill required: Data Management - Structured Query Language (SQL)\n\n\n\n\nDesignation: Data Eng, Mgmt & Governance Analyst\n\n\n\n\nQualifications:BE/BTech\n\n\n\n\nYears of Experience:3 to 5 years\n\n\n\nAbout AccentureCombining unmatched experience and specialized skills across more than 40 industries, we offer Strategy and Consulting, Technology and Operations services, and Accenture Song all powered by the worlds largest network of Advanced Technology and Intelligent Operations centers. Our 699,000 people deliver on the promise of technology and human ingenuity every day, serving clients in more than 120 countries. Visit us at www.accenture.com\n\n\n\n\nWhat would you do\nData & AIDomain-specific language used in programming and designed for querying and modifying data and managing databases.\n\n\n\n\nWhat are we looking for\nSQL Data Visualization Adaptable and flexible Commitment to quality Ability to work well in a team Strong analytical skills Agility for quick learning\n\n\n\nRoles and Responsibilities: Draft, review and negotiate the supplier/buyside agreements and similar/related documentation with Accenture suppliers, to procure various goods and services including but not limited to Contactors, Human Resources Support, IT & Telecom, Marketing & Communications, Workplace Support (Facilities & Services), Software as a Service etc. in accordance with Accentures suppliers contracting standards, applicable laws, and business requirements. Customize the existing templates in exceptional cases to suit the business requirements thereby ensuring compliance to applicable local laws and Accentures suppliers contracting standards. Review the supplier templates and ensure that the deviations to the Accentures suppliers contracting standards are timely identified and highlighted to the business whenever they pose as risks to Accenture operations. Participate in negotiations by representing company s interests and interface directly with client/ vendor negotiating teams with suppliers, third parties, subcontractors etc., to agree to contractual terms in accordance with Accentures suppliers contracting standards, applicable laws, and stakeholder requirements. Liaise and effectively collaborate with internal stakeholders such as deal teams, Solution Architects, Procurement, HR, Workplace, Finance, Marketing & Communications etc., as well as with external parties such as suppliers, external counsel etc. to ensure contractual risks are clearly identified and addressed in compliance with Accenture s policies and standards. Work closely with the stakeholders to help them understand the contractual clauses in terms of interpretation and its applicability in the contract basis the business opportunity. Advise the Business from legal perspective to address the potential contractual risks that may pose as risks to Accenture business operations. Conduct gap analysis and create legal risk assessment by identifying and flagging potential risks to Accenture and/or clauses which are non-negotiable. Provide recommendations to Business and other related stakeholders to sensitize them on the extent of risk Accenture exposes itself in context of the services and to minimize or mitigate such risks effectively. Structure the legal transactions to be most advantageous from a contracting and business perspective and escalate accordingly to the SME/leadership on the deal etc.\n\nQualification\n\nBE,BTech",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data management', 'data analysis', 'gap analysis', 'sql', 'data visualization', 'project management', 'python', 'data analytics', 'documentation', 'business analysis', 'power bi', 'business intelligence', 'database management', 'tableau', 'saas', 'advanced excel', 'agile', 'business operations']",2025-06-13 05:58:45
Sr Data Engineer - Lead,Clifyx Technology,7 - 11 years,Not Disclosed,['Bengaluru'],"Developing Scala Spark pipelines that are resilient, modular and tested.\nHelp automate and scale governance through technology enablement\nEnable users finding the ""right data for the ""right use case\nParticipate in identifying and proposing solutions to data quality issues, and data management solutions\nSupport technical implementation of solutions through data pipeline development\nMaintain technical processes and procedures for data management\nVery good understanding of MS Azure Data Lake and associated setups\nETL knowledge to build semantic layers for reporting\nCreation / modification of pipelines based on source and target systems\nUser and access Management and Training end users",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Training', 'Usage', 'Data management', 'Access management', 'spark', 'Billing', 'SCALA', 'Manager Technology', 'Data quality', 'Testing']",2025-06-13 05:58:46
Senior Developer / Lead Data Engineer - Incorta,KPI Partners,4 - 9 years,Not Disclosed,"['Hyderabad', 'Pune', 'Bengaluru']","About Us:\nKPI Partners is a leading provider of data analytics and performance management solutions, dedicated to helping organizations harness the power of their data to drive business success. Our team of experts is at the forefront of the data revolution, delivering innovative solutions to our clients. We are currently seeking a talented and experienced Senior Developer / Lead Data Engineer with expertise in Incorta to join our dynamic team.\n\n",,,,"['python', 'oracle', 'data analytics', 'analytical', 'scala', 'pyspark', 'microsoft azure', 'cloud platforms', 'data pipeline', 'relational databases', 'data engineering', 'sql server', 'sql', 'analytics', 'java', 'data modeling', 'collaboration', 'data integration tools', 'mysql', 'etl', 'aws', 'programming', 'communication skills']",2025-06-13 05:58:48
Data Techology Senior Associate,MSCI Services,4 - 8 years,Not Disclosed,['Pune'],"As data engineers, we build scalable systems to process data in various formats and volumes, ranging from megabytes to terabytes. Our systems perform quality checks, match data across various sources, and release it in multiple formats. We leverage the latest technologies, sources, and tools to process the data. Some of the exciting technologies we work with include Snowflake, Databricks, and Apache Spark.\nYour skills and experience that will help you excel\nCore Java, Spring Boot, Apache Spark, Spring Batch, Python. Exposure to sql databases like Oracle, Mysql, Microsoft Sql is a must. Any experience / knowledge / certification on Cloud technology preferrably Microsoft Azure or Google cloud platform is good to have. Exposures to non sql databases like Neo4j or Document database is again good to have.\n  What we offer you\nTransparent compensation schemes and comprehensive employee benefits, tailored to your location, ensuring your financial security, health, and overall we'llbeing.\nFlexible working arrangements, advanced technology, and collaborative workspaces.\nA culture of high performance and innovation where we experiment with new ideas and take responsibility for achieving results.\nA global network of talented colleagues, who inspire, support, and share their expertise to innovate and deliver for our clients.\nGlobal Orientation program to kickstart your journey, followe'd by access to our Learning@MSCI platform, LinkedIn Learning Pro and tailored learning opportunities for ongoing skills development.\nMulti-directional career paths that offer professional growth and development through new challenges, internal mobility and expanded roles.\nWe actively nurture an environment that builds a sense of inclusion belonging and connection, including eight Employee Resource Groups. All Abilities, Asian Support Network, Black Leadership Network, Climate Action Network, Hola! MSCI, Pride & Allies, Women in Tech, and Women s Leadership Forum.",Industry Type: Financial Services,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['CVS', 'Core Java', 'Bloomberg', 'spring batch', 'MySQL', 'Oracle', 'Analytics', 'Downstream', 'Python', 'Recruitment']",2025-06-13 05:58:49
Tech Lead-Data Engineering,Ameriprise Financial,7 - 10 years,Not Disclosed,['Hyderabad'],"Key Responsibilities\nDesign and develop high-volume, data engineering solutions for mission-critical systems with quality.\nMaking enhancements to various applications that meets business and auditing requirements.\nResearch and evaluate alternative solutions and make recommendations on improving the product to meet business and information risk requirements.\nEvaluate service level issues and suggested enhancements to diagnose and address underlying system problems and inefficiencies.\nParticipate in full development lifecycle activities for the product (coding, testing, release activities).\nSupport Release activities on weekends as required.\nSupport any application issues reported during weekends.\nCoordinating day-To-day activities for multiple projects with onshore and offshore team members. Ensuring the availability of platform in lower environments\n\nRequired Qualifications\n7+ years of overall IT experience, which includes hands on experience in Big Data technologies.\nMandatory - Hands on experience in Python and PySpark.\nBuild pySpark applications using Spark Dataframes in Python using Jupyter notebook and PyCharm(IDE).\nWorked on optimizing spark jobs that processes huge volumes of data.\nHands on experience in version control tools like Git.\nWorked on Amazon s Analytics services like Amazon EMR, Amazon Athena, AWS Glue.\nWorked on Amazon s Compute services like Amazon Lambda, Amazon EC2 and Amazon s Storage service like S3 and few other services like SNS.\nExperience/knowledge of bash/shell scripting will be a plus.\nHas built ETL processes to take data, copy it, structurally transform it etc. involving a wide variety of formats like CSV, TSV, XML and JSON.\nExperience in working with fixed width, delimited , multi record file formats etc.\nGood to have knowledge of datawarehousing concepts - dimensions, facts, schemas- snowflake, star etc.\nHave worked with columnar storage formats- Parquet,Avro,ORC etc. Well versed with compression techniques - Snappy, Gzip.\nGood to have knowledge of AWS databases (atleast one) Aurora, RDS, Redshift, ElastiCache, DynamoDB.\nHands on experience in tools like Jenkins to build, test and deploy the applications\nAwareness of Devops concepts and be able to work in an automated release pipeline environment.\nExcellent debugging skills.\n\nPreferred Qualifications\nExperience working with US Clients and Business partners.\nKnowledge on Front end frameworks.\nExposure to BFSI domain is a good to have.\nHands on experience on any API Gateway and management platform.\nAWMPO AWMPS Presidents Office\nTechnology",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Front end', 'Version control', 'Coding', 'XML', 'Shell scripting', 'Debugging', 'JSON', 'Asset management', 'Analytics', 'Python']",2025-06-13 05:58:51
Lead Data Engineer - Azure,Blend360 India,7 - 12 years,Not Disclosed,['Hyderabad'],"Job Description\nAs a Lead Data Engineer, your role is to spearhead the data engineering teams and elevate the team to the next level! You will be responsible for laying out the architecture of the new project as well as selecting the tech stack associated with it. You will plan out the development cycles deploying AGILE if possible and create the foundations for good data stewardship with our new data products!\nYou will also set up a solid code framework that needs to be built to purpose yet have enough flexibility to adapt to new business use cases tough but rewarding challenge!\n\nResponsibilities\nCollaborate with several stakeholders to deeply understand the needs of data practitioners to deliver at scale\nLead Data Engineers to define, build and maintain Data Platform\nWork on building Data Lake in Azure Fabric processing data from multiple sources\nMigrating existing data store from Azure Synapse to Azure Fabric\nImplement data governance and access control\nDrive development effort End-to-End for on-time delivery of high-quality solutions that conform to requirements, conform to the architectural vision, and comply with all applicable standards.\nPresent technical solutions, capabilities, considerations, and features in business terms. Effectively communicate status, issues, and risks in a precise and timely manner.\nFurther develop critical initiatives, such as Data Discovery, Data Lineage and Data Quality\nLeading team and Mentor junior resources\nHelp your team members grow in their role and achieve their career aspirations\nBuild data systems, pipelines, analytical tools and programs\nConduct complex data analysis and report on results\n\n\nQualifications\n7+ Years of Experience as a data engineer or similar role in Azure Synapses, ADF or\nrelevant exp in Azure Fabric\nDegree in Computer Science, Data Science, Mathematics, IT, or similar fiel",Industry Type: Advertising & Marketing,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Data analysis', 'Data modeling', 'Analytical', 'Agile', 'data governance', 'Data quality', 'Data mining', 'SQL', 'Python']",2025-06-13 05:58:52
Senior Data Engineer,Grid Dynamics,8 - 13 years,15-25 Lacs P.A.,['Bengaluru'],"We are looking for an enthusiastic and technology-proficient Big Data Engineer, who is eager to participate in the design and implementation of a top-notch Big Data solution to be deployed at massive scale.\nOur customer is one of the world's largest technology companies based in Silicon Valley with operations all over the world. On this project we are working on the bleeding-edge of Big Data technology to develop high performance data analytics platform, which handles petabytes datasets.\nEssential functions\nParticipate in design and development of Big Data analytical applications.\nDesign, support and continuously enhance the project code base, continuous integration pipeline, etc.\nWrite complex ETL processes and frameworks for analytics and data management.\nImplement large-scale near real-time streaming data processing pipelines.\nWork inside the team of industry experts on the cutting edge Big Data technologies to develop solutions for deployment at massive scale.\nQualifications\nStrong coding experience with Scala, Spark,Hive, Hadoop.\nIn-depth knowledge of Hadoop and Spark, experience with data mining and stream processing technologies (Kafka, Spark Streaming, Akka Streams).\nUnderstanding of the best practices in data quality and quality engineering.\nExperience with version control systems, Git in particular.\nDesire and ability for quick learning of new tools and technologies.\nWould be a plus\nKnowledge of Unix-based operating systems (bash/ssh/ps/grep etc.).\nExperience with Github-based development processes.\nExperience with JVM build systems (SBT, Maven, Gradle).\nWe offer\nOpportunity to work on bleeding-edge projects\nWork with a highly motivated and dedicated team\nCompetitive salary\nFlexible schedule\nBenefits package - medical insurance, sports\nCorporate social events\nProfessional development opportunities\nWell-equipped office\nAbout us\nGrid Dynamics (NASDAQ: GDYN) is a leading provider of technology consulting, platform and product engineering, AI, and advanced analytics services. Fusing technical vision with business acumen, we solve the most pressing technical challenges and enable positive business outcomes for enterprise companies undergoing business transformation. A key differentiator for Grid Dynamics is our 8 years of experience and leadership in enterprise AI, supported by profound expertise and ongoing investment in data, analytics, cloud & DevOps, application modernization and customer experience. Founded in 2006, Grid Dynamics is headquartered in Silicon Valley with offices across the Americas, Europe, and India.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Hive', 'SCALA', 'Hadoop', 'Big Data', 'Spark']",2025-06-13 05:58:54
S&C Global Network - AI - CG&S - Data Engineer Consultant,Accenture,3 - 7 years,Not Disclosed,['Bengaluru'],"Job Title:Industry & Function AI Data Engineer + S&C GN\n\n\n\nManagement Level:09 - Consultant\n\n\n\nLocation:Primary - Bengaluru, Secondary - Gurugram\n\n\n\nMust-Have Skills:Data Engineering expertise, Cloud platforms:AWS, Azure, GCP, Proficiency in Python, SQL, PySpark and ETL frameworks\n\n\n\nGood-to-Have Skills:LLM Architecture, Containerization tools:Docker, Kubernetes, Real-time data processing tools:Kafka, Flink, Certifications like AWS Certified Data Analytics Specialty, Google Professional Data Engineer,Snowflake,DBT,etc.\n\n\n\nJob\n\n\nSummary:\n\nAs a Data Engineer, you will play a critical role in designing, implementing, and optimizing data infrastructure to power analytics, machine learning, and enterprise decision-making. Your work will ensure high-quality, reliable data is accessible for actionable insights. This involves leveraging technical expertise, collaborating with stakeholders, and staying updated with the latest tools and technologies to deliver scalable and efficient data solutions.\n\n\n\n\nRoles & Responsibilities:\nBuild and Maintain Data Infrastructure:Design, implement, and optimize scalable data pipelines and systems for seamless ingestion, transformation, and storage of data.\nCollaborate with Stakeholders:Work closely with business teams, data analysts, and data scientists to understand data requirements and deliver actionable solutions.\nLeverage Tools and Technologies:Utilize Python, SQL, PySpark, and ETL frameworks to manage large datasets efficiently.\nCloud Integration:Develop secure, scalable, and cost-efficient solutions using cloud platforms such as Azure, AWS, and GCP.\nEnsure Data Quality:Focus on data reliability, consistency, and quality using automation and monitoring techniques.\nDocument and Share Best Practices:Create detailed documentation, share best practices, and mentor team members to promote a strong data culture.\nContinuous Learning:Stay updated with the latest tools and technologies in data engineering through professional development opportunities.\n\n\n\n\n\nProfessional & Technical\n\n\n\n\nSkills:\n\nStrong proficiency in programming languages such as Python, SQL, and PySpark\nExperience with cloud platforms (AWS, Azure, GCP) and their data services\nFamiliarity with ETL frameworks and data pipeline design\nStrong knowledge of traditional statistical methods, basic machine learning techniques.\nKnowledge of containerization tools (Docker, Kubernetes)\nKnowing LLM, RAG & Agentic AI architecture\nCertification in Data Science or related fields (e.g., AWS Certified Data Analytics Specialty, Google Professional Data Engineer)\n\n\n\n\n\nAdditional Information:\n\nThe ideal candidate has a robust educational background in data engineering or a related field and a proven track record of building scalable, high-quality data solutions in the Consumer Goods sector.\n\nThis position offers opportunities to design and implement cutting-edge data systems that drive business transformation, collaborate with global teams to solve complex data challenges and deliver measurable business outcomes and enhance your expertise by working on innovative projects utilizing the latest technologies in cloud, data engineering, and AI.\n\n\n\nAbout Our Company | Accenture\n\nQualification\n\n\n\nExperience:Minimum 3-7 years in data engineering or related fields, with a focus on the Consumer Goods Industry\n\n\n\n\nEducational Qualification:Bachelors or Masters degree in Computer Science, Information Systems, Engineering, or a related field",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'pyspark', 'data engineering', 'sql', 'machine learning algorithms', 'kubernetes', 'snowflake', 'data analytics', 'microsoft azure', 'cloud platforms', 'machine learning', 'apache flink', 'artificial intelligence', 'docker', 'pipeline', 'data science', 'gcp', 'kafka', 'aws', 'etl', 'etl scripts']",2025-06-13 05:58:55
Deputy Manager - Data Engineer - Analytics,IBM,2 - 7 years,Not Disclosed,['Bengaluru'],"Develop, test and support future-ready data solutions for customers across industry verticals\nDevelop, test, and support end-to-end batch and near real-time data flows/pipelines\nDemonstrate understanding in data architectures, modern data platforms, big data, analytics, cloud platforms, data governance and information management and associated technologies\nCommunicates risks and ensures understanding of these risks.\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nMinimum of 2+ years of related experience required\nExperience in modeling and business system designs\nGood hands-on experience on DataStage, Cloud based ETL Services\nHave great expertise in writing TSQL code\nWell versed with data warehouse schemas and OLAP techniques\n\n\nPreferred technical and professional experience\nAbility to manage and make decisions about competing priorities and resources.\nAbility to delegate where appropriate\nMust be a strong team player/leader\nAbility to lead Data transformation project with multiple junior data engineers\nStrong oral written and interpersonal skills for interacting and throughout all levels of the organization.\nAbility to clearly communicate complex business problems and technical solutions.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['information management', 'cloud platforms', 'data architecture', 'data governance', 'big data', 'schema', 'python', 'data analytics', 'datastage', 'microsoft azure', 'warehouse', 't-sql', 'data engineering', 'ansible', 'docker', 'sql', 'java', 'devops', 'linux', 'olap', 'jenkins', 'shell scripting', 'etl', 'aws']",2025-06-13 05:58:57
Data Engineer-Data Platforms-Google,IBM,5 - 7 years,Not Disclosed,['Bengaluru'],"A career in IBM Consulting is rooted by long-term relationships and close collaboration with clients across the globe.You'll work with visionaries across multiple industries to improve the hybrid cloud and Al journey for the most innovative and valuable companies in the world. Your ability to accelerate impact and make meaningful change for your clients is enabled by our strategic partner ecosystem and our robust technology platforms across the IBM portfolio; including Software and Red Hat.\n\nIn your role, you will be responsible for:\nSkilled Multiple GCP services - GCS, BigQuery, Cloud SQL, Dataflow, Pub/Sub, Cloud Run, Workflow, Composer, Error reporting, Log explorer etc.\nMust have Python and SQL work experience & Proactive, collaborative and ability to respond to critical situation\nAbility to analyse data for functional business requirements & front face customer\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\n5 to 7 years of relevant experience working as technical analyst with Big Query on GCP platform.\nSkilled in multiple GCP services - GCS, Cloud SQL, Dataflow, Pub/Sub, Cloud Run, Workflow, Composer, Error reporting, Log explorer\nAmbitious individual who can work under their own direction towards agreed targets/goals and with creative approach to work\nYou love collaborative environments that use agile methodologies to encourage creative design thinking and find innovative ways to develop with cutting edge technologies.\nEnd to End functional knowledge of the data pipeline/transformation implementation that the candidate has done, should understand the purpose/KPIs for which data transformation was done\n\n\nPreferred technical and professional experience\nExperience with AEM Core Technologies OSGI Services, Apache Sling ,Granite Framework., Java Content Repository API, Java 8+, Localization\nFamiliarity with building tools, Jenkin and Maven , Knowledge of version control tools, especially Git, Knowledge of Patterns and Good Practices to design and develop quality and clean code, Knowledge of HTML, CSS, and JavaScript , jQuery\nFamiliarity with task management, bug tracking, and collaboration tools like JIRA and Confluence",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sql', 'java', 'html', 'python', 'javascript', 'hive', 'css', 'confluence', 'aem', 'data warehousing', 'apache sling', 'jquery', 'gen', 'git', 'gcp', 'spark', 'jenkins', 'bigquery', 'data transformation', 'hadoop', 'big data', 'etl', 'jira', 'cloud sql', 'maven', 'airflow', 'osgi', 'granite', 'agile', 'sqoop', 'aws']",2025-06-13 05:58:59
Data Engineer-Data Platforms-Google,IBM,5 - 7 years,Not Disclosed,['Bengaluru'],"Skilled Multiple GCP services - GCS, BigQuery, Cloud SQL, Dataflow, Pub/Sub, Cloud Run, Workflow, Composer, Error reporting, Log explorer etc.\nMust have Python and SQL work experience & Proactive, collaborative and ability to respond to critical situation\nAbility to analyse data for functional business requirements & front face customer\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\n5 to 7 years of relevant experience working as technical analyst with Big Query on GCP platform.\nSkilled in multiple GCP services - GCS, Cloud SQL, Dataflow, Pub/Sub, Cloud Run, Workflow, Composer, Error reporting, Log explorer\nYou love collaborative environments that use agile methodologies to encourage creative design thinking and find innovative ways to develop with cutting edge technologies\nAmbitious individual who can work under their own direction towards agreed targets/goals and with creative approach to work\n\n\nPreferred technical and professional experience\nCreate up to 3 bullets maxitive individual with an ability to manage change and proven time management\nProven interpersonal skills while contributing to team effort by accomplishing related results as needed\nUp-to-date technical knowledge by attending educational workshops, reviewing publications (encouraging then to focus on required skills)",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sql', 'gcp', 'bigquery', 'cloud sql', 'python', 'hive', 'gen', 'java', 'postgresql', 'spark', 'linux', 'mysql', 'hadoop', 'big data', 'pubsub', 'airflow', 'application engine', 'machine learning', 'sql server', 'dataproc', 'cloud storage', 'bigtable', 'agile', 'sqoop', 'aws', 'data flow']",2025-06-13 05:59:00
Data Engineer-Data Platforms,IBM,2 - 5 years,Not Disclosed,['Bengaluru'],"As a Data Engineer at IBM, you'll play a vital role in the development, design of application, provide regular support/guidance to project teams on complex coding, issue resolution and execution.\n\nYour primary responsibilities include:\nLead the design and construction of new solutions using the latest technologies, always looking to add business value and meet user requirements.\nStrive for continuous improvements by testing the build solution and working under an agile framework.\nDiscover and implement the latest technologies trends to maximize and build creative solutions\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nExperience with Apache Spark (PySpark)In-depth knowledge of Spark’s architecture, core APIs, and PySpark for distributed data processing.\nBig Data TechnologiesFamiliarity with Hadoop, HDFS, Kafka, and other big data tools. Data Engineering\n\nSkills:\nStrong understanding of ETL pipelines, data modeling, and data warehousing concepts.\nStrong proficiency in PythonExpertise in Python programming with a focus on data processing and manipulation. Data Processing FrameworksKnowledge of data processing libraries such as Pandas, NumPy.\nSQL ProficiencyExperience writing optimized SQL queries for large-scale data analysis and transformation.\nCloud PlatformsExperience working with cloud platforms like AWS, Azure, or GCP, including using cloud storage systems\n\n\nPreferred technical and professional experience\nDefine, drive, and implement an architecture strategy and standards for end-to-end monitoring.\nPartner with the rest of the technology teams including application development, enterprise architecture, testing services, network engineering,\nGood to have detection and prevention tools for Company products and Platform and customer-facing",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'data processing', 'pyspark', 'data modeling', 'spark', 'data analysis', 'microsoft azure', 'data warehousing', 'cloud platforms', 'numpy', 'data engineering', 'sql', 'pandas', 'spring boot', 'etl pipelines', 'java', 'gcp', 'kafka', 'data warehousing concepts', 'hadoop', 'big data', 'aws', 'etl']",2025-06-13 05:59:02
Data Engineer II,Flipkart,1 - 3 years,Not Disclosed,['Bengaluru'],"Skills Required :\nKafka, Spark Streaming. Proficiency in one of the programming languages preferably Java, Scala or Python.\nEducation/Qualification :\nBachelor's Degree in Computer Science, Engineering, Technology or related field\nDesirable Skills :\nKafka, Spark Streaming. Proficiency in one of the programming languages preferably Java, Scala or Python.",Industry Type: Courier / Logistics,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'java', 'Scala', 'Kafka', 'Spark Streaming', 'Python']",2025-06-13 05:59:04
Sales Excellence - COE - Data Engineering Specialist,Accenture,5 - 8 years,Not Disclosed,['Bengaluru'],"Job Title -\n\n\n\nSales Excellence - COE - Data Engineering Specialist\n\n\n\nManagement Level:\n\n\n\n9-Team Lead/Consultant\n\n\n\nLocation:\n\n\n\nMumbai, MDC2C\n\n\n\nMust-have skills:Sales\n\n\n\n\nGood to have skills:Data Science, SQL, Automation, Machine Learning\n\n\n\nJob\n\n\nSummary:\n\nApply deep statistical tools and techniques to find relationships between variables\n\n\n\n\nRoles & Responsibilities:\n\n- Apply deep statistical tools and techniques to find relationships between variables.\n\n- Develop intellectual property for analytical methodologies and optimization techniques.\n\n- Identify data requirements and develop analytic solutions to solve business issues.\n\nJob Title - Analytics & Modelling Specialist\n\nManagement Level :9-Specialist\n\nLocation:Bangalore/ Gurgaon/Hyderabad/Mumbai\n\nMust have skills:Python, Data Analysis, Data Visualization, SQL\nGood to have skills:Machine Learning\n\nJob\n\n\nSummary:\n\nThe Center of Excellence (COE) makes sure that the sales and pricing methods and offerings of Sales Excellence are effective.\n\n- The COE supports salespeople through its business partners and Analytics and Sales Operations teams.\n\nThe Data Engineer helps manage data sources and environments, utilizing large data sets and maintaining their integrity to create models and apps that deliver insights to the organization.\nRoles & Responsibilities:\n\nBuild and manage data models that bring together data from different sources.\n\nHelp consolidate and cleanse data for use by the modeling and development teams.\n\nStructure data for use in analytics applications.\n\nLead a team of Data Engineers effectively.\nProfessional & Technical\n\n\n\n\nSkills:\nA bachelors degree or equivalent\n\nTotal experience Range:5-8 years in the relevant field\n\nA minimum of 3 years of GCP experience with exposure to machine learning/data science\n\nExperience in configuration the machine learning workflow in GCP.\n\nA minimum of 5 years Advanced SQL knowledge and experience working with relational databases\n\nA minimum of 3 years Familiarity and hands on experience in different SQL objects like stored procedures, functions, views etc.,\n\nA minimum of 3 years Building of data flow components and processing systems to extract, transform, load and integrate data from various sources.\n\nA minimum of 3 years Hands on experience in advanced excel topics such as cube functions, VBA Automation, Power Pivot etc.\n\nA minimum of 3 years Hands on experience in Python\nAdditional Information:\n\nUnderstanding of sales processes and systems.\n\nMasters degree in a technical field.\n\nExperience with quality assurance processes.\n\nExperience in project management.\n\nYou May Also Need:\n\nAbility to work flexible hours according to business needs.\n\nMust have good internet connectivity and a distraction-free environment for working at home, in accordance with local guidelines.\n\n\n\n\nProfessional & Technical\n\n\n\n\nSkills:\n\n\n- Relevant experience in the required domain.\n\n- Strong analytical, problem-solving, and communication skills.\n\n- Ability to work in a fast-paced, dynamic environment.\n\n\n\n\nAdditional Information:\n\n- Opportunity to work on innovative projects.\n\n- Career growth and leadership exposure.\n\n\n\n\n\nAbout Our Company | AccentureQualification\n\n\n\nExperience:8 to 10 Years\n\n\n\n\nEducational Qualification:\n\n\n\nB.Com",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'data analysis', 'sales', 'sql', 'data visualization', 'hive', 'advance sql', 'ssas', 'dbms', 'machine learning', 'data engineering', 'power pivot', 'sql server', 'vba automation', 'data science', 'gcp', 'spark', 'advanced excel', 'hadoop', 'ssis', 'etl', 'big data', 'data flow', 'sql joins']",2025-06-13 05:59:05
Data Platform Engineer,Accenture,7 - 12 years,Not Disclosed,['Bengaluru'],"Project Role :Data Platform Engineer\n\n\n\n\n\nProject Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification : Engineering graduate preferably Computer Science graduate 15 years of full time education\n\n\nSummary:As a Data Platform Engineer, you will be responsible for assisting with the blueprint and design of the data platform components using Databricks Unified Data Analytics Platform. Your typical day will involve collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\nRoles & Responsibilities:- Assist with the blueprint and design of the data platform components using Databricks Unified Data Analytics Platform.- Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.- Develop and maintain data pipelines and ETL processes using Databricks Unified Data Analytics Platform.- Design and implement data security and access controls for the data platform.- Troubleshoot and resolve issues related to the data platform and data pipelines.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nExperience with Databricks Unified Data Analytics Platform.- Must To Have\n\n\n\n\nSkills:\nStrong understanding of data modeling and database design principles.- Good To Have\n\n\n\n\nSkills:\nExperience with cloud-based data platforms such as AWS or Azure.- Good To Have\n\n\n\n\nSkills:\nExperience with data security and access controls.- Good To Have\n\n\n\n\nSkills:\nExperience with data visualization tools such as Tableau or Power BI.\nAdditional Information:- The candidate should have a minimum of 7.5 years of experience in Databricks Unified Data Analytics Platform.- The ideal candidate will possess a strong educational background in computer science or a related field, along with a proven track record of delivering impactful data-driven solutions.- This position is based at our Bangalore, Hyderabad, Chennai and Pune Offices.- Mandatory office (RTO) for 2- 3 days and have to work on 2 shifts (Shift A- 10:00am to 8:00pm IST and Shift B - 12:30pm to 10:30 pm IST)\n\nQualification\n\nEngineering graduate preferably Computer Science graduate 15 years of full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['database design', 'data modeling', 'data analytics', 'microsoft azure', 'design principles', 'hive', 'sql', 'java', 'spark', 'design patterns', 'oops', 'mysql', 'hadoop', 'etl', 'big data', 'c#', 'rest', 'python', 'data security', 'power bi', 'javascript', 'sql server', 'data bricks', 'tableau', 'kafka', 'sqoop', 'aws']",2025-06-13 05:59:07
Data Platform Engineer,Accenture,7 - 12 years,Not Disclosed,['Bengaluru'],"Project Role :Data Platform Engineer\n\n\n\n\n\nProject Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Platform Engineer, you will assist with the data platform blueprint and design, collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. You will play a crucial role in shaping the data platform components.\nRoles & Responsibilities:- Expected to be an SME, collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Lead the implementation of data platform solutions.- Conduct performance tuning and optimization of data platform components.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Strong understanding of cloud-based data platforms.- Experience in designing and implementing data pipelines.- Knowledge of data governance and security best practices.\nAdditional Information:- The candidate should have a minimum of 7.5 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Bengaluru office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['hive', 'data analytics', 'data modeling', 'spark', 'data governance', 'python', 'amazon redshift', 'data warehousing', 'microsoft azure', 'emr', 'machine learning', 'sql', 'nosql', 'amazon ec2', 'java', 'kafka', 'mysql', 'hadoop', 'sqoop', 'big data', 'aws', 'etl']",2025-06-13 05:59:09
Data Platform Engineer,Accenture,5 - 10 years,Not Disclosed,['Pune'],"Project Role :Data Platform Engineer\n\n\n\n\n\nProject Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Platform Engineer, you will assist with the data platform blueprint and design, collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. You will play a crucial role in the development and maintenance of the data platform components, contributing to the overall success of the project.\nRoles & Responsibilities:- Expected to be an SME, collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Assist with the data platform blueprint and design.- Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.- Develop and maintain data platform components.- Contribute to the overall success of the project.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Strong understanding of statistical analysis and machine learning algorithms.- Experience with data visualization tools such as Tableau or Power BI.- Hands-on implementing various machine learning algorithms such as linear regression, logistic regression, decision trees, and clustering algorithms.- Solid grasp of data munging techniques, including data cleaning, transformation, and normalization to ensure data quality and integrity.\nAdditional Information:- The candidate should have a minimum of 5 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Bengaluru office.- 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analytics', 'tableau', 'machine learning algorithms', 'statistics', 'data munging', 'python', 'natural language processing', 'power bi', 'machine learning', 'sql', 'data bricks', 'data quality', 'r', 'data modeling', 'data science', 'predictive modeling', 'text mining', 'logistic regression']",2025-06-13 05:59:10
Data Platform Engineer,Accenture,12 - 15 years,Not Disclosed,['Bengaluru'],"Project Role :Data Platform Engineer\n\n\n\n\n\nProject Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\n\n\n\nMust have skills :Collibra Data Governance\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n12 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Platform Engineer, you will assist with the data platform blueprint and design, encompassing the relevant data platform components. Your typical day will involve collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models, while also engaging in discussions to refine and enhance the overall data architecture. You will be involved in various stages of the data platform lifecycle, ensuring that all components work harmoniously to support the organization's data needs and objectives.\nRoles & Responsibilities:- Expected to be an SME.- Collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Expected to provide solutions to problems that apply across multiple teams.- Facilitate knowledge sharing sessions to enhance team capabilities and foster a culture of continuous improvement.- Monitor and evaluate team performance, providing constructive feedback to ensure alignment with project goals.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Collibra Data Governance.- Strong understanding of data governance frameworks and best practices.- Experience with data integration tools and techniques.- Familiarity with data modeling concepts and methodologies.- Ability to analyze and interpret complex data sets to inform decision-making.\nAdditional Information:- The candidate should have minimum 12 years of experience in Collibra Data Governance.- This position is based at our Bengaluru office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'data architecture', 'sql', 'data modeling', 'data governance', 'data analysis', 'oracle', 'data management', 'data warehousing', 'business analysis', 'machine learning', 'business intelligence', 'javascript', 'sql server', 'data quality', 'tableau', 'java', 'html', 'mysql', 'etl', 'informatica']",2025-06-13 05:59:12
Data Platform Engineer,Accenture,7 - 12 years,Not Disclosed,['Pune'],"Project Role :Data Platform Engineer\n\n\n\n\n\nProject Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Platform Engineer, you will assist with the data platform blueprint and design, collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. You will play a crucial role in the development and maintenance of the data platform components, contributing to the overall success of the project.\nRoles & Responsibilities:- Expected to be an SME, collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Assist with the data platform blueprint and design.- Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.- Develop and maintain data platform components.- Contribute to the overall success of the project.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Strong understanding of statistical analysis and machine learning algorithms.- Experience with data visualization tools such as Tableau or Power BI.- Hands-on implementing various machine learning algorithms such as linear regression, logistic regression, decision trees, and clustering algorithms.- Solid grasp of data munging techniques, including data cleaning, transformation, and normalization to ensure data quality and integrity.\nAdditional Information:- The candidate should have a minimum of 7.5 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Bengaluru office.- 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analytics', 'tableau', 'machine learning algorithms', 'statistics', 'data munging', 'python', 'natural language processing', 'power bi', 'machine learning', 'sql', 'data bricks', 'data quality', 'r', 'data modeling', 'data science', 'predictive modeling', 'text mining', 'logistic regression']",2025-06-13 05:59:14
Data Platform Engineer,Accenture,7 - 12 years,Not Disclosed,['Bengaluru'],"Project Role :Data Platform Engineer\n\n\n\n\n\nProject Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Platform Engineer, you will assist with the data platform blueprint and design, collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. You will play a crucial role in the development and maintenance of the data platform components, contributing to the overall success of the project.\nRoles & Responsibilities:- Expected to be an SME, collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Assist with the data platform blueprint and design.- Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.- Develop and maintain data platform components.- Contribute to the overall success of the project.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Strong understanding of statistical analysis and machine learning algorithms.- Experience with data visualization tools such as Tableau or Power BI.- Hands-on implementing various machine learning algorithms such as linear regression, logistic regression, decision trees, and clustering algorithms.- Solid grasp of data munging techniques, including data cleaning, transformation, and normalization to ensure data quality and integrity.\nAdditional Information:- The candidate should have a minimum of 7.5 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Bengaluru office.- 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analytics', 'tableau', 'machine learning algorithms', 'statistics', 'data munging', 'python', 'natural language processing', 'power bi', 'machine learning', 'sql', 'data bricks', 'data quality', 'r', 'data modeling', 'data science', 'predictive modeling', 'text mining', 'logistic regression']",2025-06-13 05:59:15
Data Platform Engineer,Accenture,3 - 8 years,Not Disclosed,['Bengaluru'],"Project Role :Data Platform Engineer\n\n\n\n\n\nProject Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Platform Engineer, you will assist with the data platform blueprint and design, collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. You will play a crucial role in shaping the data platform components.\nRoles & Responsibilities:- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Collaborate with Integration Architects and Data Architects to design and implement data platform components.- Ensure seamless integration between various systems and data models.- Develop and maintain data platform blueprints.- Optimize data platform performance and scalability.- Provide technical guidance and support to team members.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Strong understanding of data platform architecture and design principles.- Experience with cloud-based data platforms like AWS or Azure.- Hands-on experience with data integration tools and technologies.- Knowledge of data governance and security best practices.\nAdditional Information:- The candidate should have a minimum of 3 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Bengaluru office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analytics', 'microsoft azure', 'platform architecture', 'design principles', 'aws', 'kubernetes', 'c++', 'oracle', 'enterprise architecture', 'microservices', 'docker', 'infrastructure architecture', 'java', 'data modeling', 'gcp', 'design patterns', 'data governance', 'agile', 'hadoop']",2025-06-13 05:59:17
Hadoop & UNIX Shell Scripting Engineer (Data Management),Synechron,5 - 10 years,Not Disclosed,['Bengaluru'],"Job Summary\nSynechron is seeking a dedicated and technically skilled Hadoop Shell Scripting Engineer to manage and optimize our Hadoop ecosystem. The role involves developing automation utilities, troubleshooting complex issues, and collaborating with vendors for platform enhancements. Your expertise will directly support enterprise data processing, performance tuning, and cloud migration initiatives, ensuring reliable and efficient data infrastructure that aligns with organizational goals.\nSoftware Requirements\nRequired Skills:\nStrong proficiency in UNIX Shell scripting with hands-on experience in developing automation utilities\nIn-depth understanding of Hadoop architecture and ecosystem components (HDFS, Hive, Spark)\nExperience with SQL querying and database systems\nFamiliarity with Git and enterprise version control practices\nWorking knowledge of DevOps and CI/CD tools and processes\nPreferred Skills:\nExperience with Python scripting for automation and utility development\nKnowledge of Java programming language\nFamiliarity with cloud platforms (AWS, Azure, GCP) related to Hadoop ecosystem support\nExposure to Hadoop vendor support and collaboration processes (e.g., Cloudera)\nOverall Responsibilities\nDevelop, maintain, and enhance scripts and utilities to automate Hadoop cluster management and data processing tasks\nServe as the Level 3 point of contact for issues related to Hadoop and Spark platforms\nPerform performance tuning and capacity planning to support enterprise data workloads\nConduct proof-of-concept tests for emerging technologies and evaluate their suitability for cloud migration projects\nCollaborate with vendor support teams and internal stakeholders for issue resolution, feature requests, and platform improvements\nReview and validate all changes going into production to ensure stability and performance\nContinuously analyze process inefficiencies and develop new automation utilities to enhance productivity\nAssist in capacity management and performance monitoring activities\nTechnical Skills (By Category)\nProgramming Languages:\nEssential: UNIX Shell scripting\nPreferred: Python, Java\nDatabases & Data Management:\nEssential: Knowledge of SQL querying and database systems\nPreferred: Experience with Hive, HDFS\nCloud Technologies:\nPreferred: Basic familiarity with cloud platforms for Hadoop ecosystem support and migration\nFrameworks & Libraries:\nNot specifically applicable; focus on scripting and platform tools\nDevelopment Tools & Methodologies:\nEssential: Git, version control, DevOps practices, CI/CD pipelines\nPreferred: Automation frameworks, monitoring tools\nSecurity Protocols:\nNot explicitly specified but familiarity with secure scripting and data access controls is advantageous\nExperience Requirements\nMinimum of 5+ years of hands-on experience working with Hadoop clusters and scripting in UNIX shell\nProven experience in managing enterprise Hadoop/Spark environments\nExperience in performance tuning, capacity planning, and utility development\nExposure to cloud migrations or proof-of-concept evaluations is a plus\nBackground in data engineering or platform support roles preferred\nDay-to-Day Activities\nDevelop and enhance UNIX shell scripts for Hadoop automation and utility management\nTroubleshoot and resolve complex platform issues as the Level 3 point of contact\nWork with application teams to optimize queries and data workflows\nEngage with vendor support teams for platform issues and feature requests\nPerform system performance reviews, capacity assessments, and tuning activities\nLead initiatives for process automation, efficiency improvement, and new technology evaluations\nDocument procedures, scripts, and platform configurations\nParticipate in team meetings, provide technical feedback, and collaborate across teams on platform health\nQualifications\nEducational Requirements:\nBachelor's degree in Computer Science, Information Technology, or related field\nEquivalent professional experience in data engineering, platform support, or Hadoop administration\nCertifications (Preferred):\nCertificates in Hadoop ecosystem, Linux scripting, or cloud platform certifications\nTraining & Professional Development:\nOngoing learning related to big data platforms, automation, and cloud migration\nProfessional Competencies\nStrong analytical and troubleshooting skills\nExcellent written and verbal communication skills\nProven ability to work independently with minimal supervision\nCollaborative team player with a positive attitude\nAbility to prioritize tasks effectively and resolve issues swiftly\nAdaptability to evolving technologies and environments\nFocus on quality, security, and process improvement",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Hadoop', 'Java', 'Automation', 'DevOps', 'Data Management', 'CI/CD', 'Performance Tuning', 'UNIX Shell Scripting', 'Python', 'SQL']",2025-06-13 05:59:19
"Data Engineer II, SCOT - AIM",Amazon,3 - 8 years,Not Disclosed,['Bengaluru'],"SCOTs Automated Inventory Management (AIM) team seeks talented individuals passionate about solving complex problems and driving impactful business decisions for our executives. The AIM team owns critical Tier 1 metrics for Amazon Retail stores, providing key insights to improve store health monitoring. We focus on enhancing selection, product availability, inventory efficiency, and inventory readiness to fulfill customer orders (FastTrack) while enabling accelerated delivery Speed Fulfillment Worldwide. This improves both Customer Experience (CX) and Long-Term Free Cash Flow (LTFCF) outcomes. Our approach involves creating standardized, scalable, and automated systems and tools to identify and reduce supply chain defects in our systems and inputs, while driving operational leverage and scaling.\n\nAs a Data Engineer, you will analyze large-scale business data, solve real-world problems, and develop metrics and business cases to delight our customers worldwide. You will work closely with Scientists, Engineers, and Product Managers to build scalable, high-impact products, architect data pipelines, and transform data into actionable insights to manage business at scale. We are looking for people who are motivated by thinking big, moving fast, and exploring business insights. If you love to implement solutions to hard problems while working hard, having fun, and making history, this may be the opportunity for you.\n\nAbout the team\nSupply Chain Optimization Technologies (SCOT) is the name of a complex group of systems designed to make the best decisions when it comes to forecasting, buying, placing, and shipping inventory. Functionally these teams work together to drive in-stock, drive placement, drive inventory removal and manage the customer experience.\n\n3+ years of data engineering experience\n4+ years of SQL experience\nExperience with data modeling, warehousing and building ETL pipelines Experience with AWS technologies like Redshift, S3, AWS Glue, EMR, Kinesis, FireHose, Lambda, and IAM roles and permissions\nExperience with non-relational databases / data stores (object storage, document or key-value stores, graph databases, column-family databases)\nExperience with big data technologies such as: Hadoop, Hive, Spark, EMR",,,,"['Supply chain', 'data engineer ii', 'Data modeling', 'Inventory management', 'Cash flow', 'Customer experience', 'Forecasting', 'Operations', 'Monitoring', 'SQL']",2025-06-13 05:59:20
Data Engineer II,Amazon,3 - 8 years,Not Disclosed,['Bengaluru'],"Amazon s Consumer Payments organization is seeking a highly quantitative, experienced Data Engineer to drive growth through analytics, automation of data pipelines, and enhancement of self-serve experiences. . You will succeed in this role if you are an organized self-starter who can learn new technologies quickly and excel in a fast-paced environment. In this position, you will be a key contributor and sparring partner, developing analytics and insights that global executive management teams and business leaders will use to define global strategies and deep dive businesses.\nYou will be part the team that is focused on acquiring new merchants from around the world to payments around the world. The position is based in India but will interact with global leaders and teams in Europe, Japan, US, and other regions. You should be highly analytical, resourceful, customer focused, team oriented, and have an ability to work independently under time constraints to meet deadlines. You will be comfortable thinking big and diving deep. A proven track record in taking on end-to-end ownership and successfully delivering results in a fast-paced, dynamic business environment is strongly preferred.\nResponsibilities include but not limited to:\nDesign, develop, implement, test, and operate large-scale, high-volume, high-performance data structures for analytics and Reporting.\nImplement data structures using best practices in data modeling, ETL/ELT processes, and SQL, AWS Redshift, and OLAP technologies, Model data and metadata for ad hoc and pre-built reporting.\nWork with product tech teams and build robust and scalable data integration (ETL) pipelines using SQL, Python and Spark.\nContinually improve ongoing reporting and analysis processes, automating or simplifying self-service support for customers.\nInterface with business customers, gathering requirements and delivering complete reporting solutions.\nCollaborate with Analysts, Business Intelligence Engineers and Product Managers to implement algorithms that exploit rich data sets for statistical analysis, and machine learning.\nParticipate in strategic tactical planning discussions, including annual budget processes.\nCommunicate effectively with product / business / tech-teams / other Data teams.\n3+ years of data engineering experience\nExperience with data modeling, warehousing and building ETL pipelines Experience with AWS technologies like Redshift, S3, AWS Glue, EMR, Kinesis, FireHose, Lambda, and IAM roles and permissions\nExperience with non-relational databases / data stores (object storage, document or key-value stores, graph databases, column-family databases)",,,,"['Automation', 'metadata', 'Data modeling', 'Machine learning', 'Data structures', 'OLAP', 'Business intelligence', 'Analytics', 'SQL', 'Python']",2025-06-13 05:59:22
Data Engineer--Operations,Robert Bosch Engineering and Business Solutions Private Limited,2 - 6 years,Not Disclosed,['Bengaluru'],"As a Data engineer in Operations, you will work on the operational management, monitoring, and support of scalable data pipelines running in Azure Databricks, Hadoop and Radium. You will ensure the reliability, performance, and availability of data workflows and maintain production environments. You will collaborate closely with data engineers, architects, and platform teams to implement best practices in data pipeline operations and incident management to ensure data availability and data completeness.\nPrimary responsibilities:\nOperational support and incident management for Azure Databricks, Hadoop, Radium data pipelines.\nCollaborating with data engineering and platform teams to define and enforce operational standards, SLAs, and best practices.\nDesigning and implementing monitoring, alerting, and logging solutions for Azure Databricks pipelines.\nCoordinating with central teams to ensure compliance with organizational operational standards and security policies.\nDeveloping and maintaining runbooks, SOPs, and troubleshooting guides for pipeline issues.\nManaging the end-to-end lifecycle of data pipeline incidents, including root cause analysis and remediation.\nOverseeing pipeline deployments, rollbacks, and change management using CI/CD tools such as Azure DevOps.\nEnsuring data quality and validation checks are effectively monitored in production.\nWorking closely with platform and infrastructure teams to address pipeline and environment-related issues.\nProviding technical feedback and mentoring junior operations engineers.\nConducting peer reviews of operational scripts and automation code.\nAutomating manual operational tasks using Scala and Python scripts.\nManaging escalations and coordinating critical production issue resolution.\nParticipating in post-mortem reviews and continuous improvement initiatives for data pipeline operations.",Industry Type: Automobile,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Automation', 'Change management', 'SCALA', 'Incident management', 'Data quality', 'Troubleshooting', 'Operations', 'Monitoring', 'Python']",2025-06-13 05:59:23
Cloud Data Engineer,Wipro,8 - 12 years,Not Disclosed,"['Hyderabad', 'Pune', 'Bengaluru']",Role: Cloud Data Engineer\nLocation: Wipro PAN India\nHybrid 3 days in Wipro office\n\nJD:\nStrong - SQL\nStrong - Python,,,,"['SQL', 'Python', 'AZURE', 'GCP', 'AWS']",2025-06-13 05:59:25
Data Analyst,Avnet,2 - 7 years,Not Disclosed,['Bengaluru'],"Job Summary:\nDesigns and prepares reports, dashboards, and summaries for statistical analysis and planning purposes. Analyzes business issues using data from internal and external sources to provide insight to decision-makers. Identifies and interprets trends and patterns.\n\nPrincipal Responsibilities:",,,,"['Training', 'Statistical analysis', 'Business analysis', 'Finance', 'Business process mapping', 'Data Analyst', 'Management', 'Business intelligence', 'Principal', 'Remedy']",2025-06-13 05:59:27
Data Analyst,Avnet Emea,2 - 7 years,Not Disclosed,['Bengaluru'],"Job Summary:\nDesigns and prepares reports, dashboards, and summaries for statistical analysis and planning purposes. Analyzes business issues using data from internal and external sources to provide insight to decision-makers. Identifies and interprets trends and patterns.\n\nPrincipal Responsibilities:\nCollects, compiles and analyzes data from various databases and performs statistical analysis for internal customer groups.\nDevelops reports and/or creates dashboards providing financial related information needed to make informed business decisions.\nDesigns, creates and implements templates to collect, display and analyze data for assigned projects\nCommunicates complex data in comprehensible ways. Evaluates information from multiple sources and clearly indicates quality of final analysis.\nDevelops reports and other tools to deliver internal company information enabling business users to make informed decisions.\nGathers, aggregates models, and analyzes information from multiple external sources regarding company financial performance, customer insights, competitor profiling, competitive threats, potential product or technical expansion, industry trends and other such business intelligence aspects.\nEstablishes standards and procedures for a variety of processes, conducting business analysis resulting in detailed creation and maintenance of business process mapping, requirements and training materials documentation.\nMay participate in or lead project teams.\nIdentifies, investigates and participates in opportunities to improve processes and procedures, to include various key performance metrics.\nOther duties as assigned.\n\nJob Level Specifications:\nFoundational knowledge of specialized disciplines, industry practices and standards, acquired via academic instruction and/or relevant work experience of substantially the same level.\nDevelops solutions to defined tasks, typical assignments and projects. May be solved by the application of specialized foundational knowledge, using existing approaches and solutions.\nWork is usually performed independently and requires the exercise of judgment and discretion. Receives initial direction although work may be reviewed for accuracy and quality.\nCollaborates with immediate management and team members within the department or function.\nActions typically affect own work assignments and department. Erroneous decisions or failure to accomplish work may require some assistance or resources to remedy.\n\nWork Experience:\nTypically less than 2 years with bachelors or equivalent.\n\nEducation and Certification(s):\nBachelors degree or equivalent experience from which comparable knowledge and job skills can be obtained.\n\nDistinguishing Characteristics:\nPosition may require the ability to travel.\nThe above statements are intended to describe the general nature and level of work being performed. They are not intended to be construed as an exhaustive list of all responsibilities, duties, and skills.",Industry Type: Electronics Manufacturing,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Training', 'Statistical analysis', 'Business analysis', 'Finance', 'Business process mapping', 'Data Analyst', 'Management', 'Business intelligence', 'Principal', 'Remedy']",2025-06-13 05:59:28
Data Engineer,Capgemini,6 - 9 years,Not Disclosed,['Gurugram'],"\nesign, implement, and maintain data pipelines for data ingestion, processing, and transformation in Azure.\nWork together with data scientists and analysts to understand the needs for data and create effective data workflows.\nCreate and maintain data storage solutions including Azure SQL Database, Azure Data Lake, and Azure Blob Storage.\nUtilizing Azure Data Factory or comparable technologies, create and maintain ETL (Extract, Transform, Load) operations.\nImplementing data validation and cleansing procedures will ensure the quality, integrity, and dependability of the data.\nImprove the scalability, efficiency, and cost-effectiveness of data pipelines.\nMonitoring and resolving data pipeline problems will guarantee consistency and availability of the data.\nWorks in the area of Software Engineering, which encompasses the development, maintenance and optimization of software solutions/applications.1. Applies scientific methods to analyse and solve software engineering problems.2. He/she is responsible for the development and application of software engineering practice and knowledge, in research, design, development and maintenance.3. His/her work requires the exercise of original thought and judgement and the ability to supervise the technical and administrative work of other software engineers.4. The software engineer builds skills and expertise of his/her software engineering discipline to reach standard software engineer skills expectations for the applicable role, as defined in Professional Communities.5. The software engineer collaborates and acts as team player with other software engineers and stakeholders.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['azure data lake', 'azure data factory', 'sql', 'azure blob storage', 'sql azure', 'hive', 'azure databricks', 'python', 'data validation', 'pyspark', 'data warehousing', 'power bi', 'data engineering', 'spark', 'data ingestion', 'software engineering', 'hadoop', 'etl', 'big data', 'aws', 'sql database']",2025-06-13 05:59:30
Data Engineer,Capgemini,6 - 9 years,Not Disclosed,['Hyderabad'],"\nDesign, implement, and maintain data pipelines for data ingestion, processing, and transformation in Azure.\nWork together with data scientists and analysts to understand the needs for data and create effective data workflows.\nCreate and maintain data storage solutions including Azure SQL Database, Azure Data Lake, and Azure Blob Storage.\nUtilizing Azure Data Factory or comparable technologies, create and maintain ETL (Extract, Transform, Load) operations.\nImplementing data validation and cleansing procedures will ensure the quality, integrity, and dependability of the data.\nImprove the scalability, efficiency, and cost-effectiveness of data pipelines.\nMonitoring and resolving data pipeline problems will guarantee consistency and availability of the data.\nWorks in the area of Software Engineering, which encompasses the development, maintenance and optimization of software solutions/applications.1. Applies scientific methods to analyse and solve software engineering problems.2. He/she is responsible for the development and application of software engineering practice and knowledge, in research, design, development and maintenance.3. His/her work requires the exercise of original thought and judgement and the ability to supervise the technical and administrative work of other software engineers.4. The software engineer builds skills and expertise of his/her software engineering discipline to reach standard software engineer skills expectations for the applicable role, as defined in Professional Communities.5. The software engineer collaborates and acts as team player with other software engineers and stakeholders.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['azure data lake', 'azure data factory', 'sql', 'azure blob storage', 'sql azure', 'hive', 'azure databricks', 'python', 'data validation', 'pyspark', 'data warehousing', 'power bi', 'data engineering', 'spark', 'data ingestion', 'software engineering', 'hadoop', 'etl', 'big data', 'aws', 'sql database']",2025-06-13 05:59:32
Data Engineer,Accenture,2 - 3 years,Not Disclosed,['Kochi'],"Job Title - Data Engineer Sr.Analyst ACS SONG\n\n\n\nManagement Level:Level 10 Sr. Analyst\n\n\n\nLocation:Kochi, Coimbatore, Trivandrum\n\n\n\nMust have skills:Python/Scala, Pyspark/Pytorch\n\n\n\n\nGood to have skills:Redshift\n\n\n\n\n\n\n\nJob\n\n\nSummary\n\nYoull capture user requirements and translate them into business and digitally enabled solutions across a range of industries. Your responsibilities will include:\n\n\n\nRoles and Responsibilities\n\nDesigning, developing, optimizing, and maintaining data pipelines that adhere to ETL principles and business goals\n\nSolving complex data problems to deliver insights that helps our business to achieve their goals.\n\nSource data (structured unstructured) from various touchpoints, format and organize them into an analyzable format.\n\nCreating data products for analytics team members to improve productivity\n\nCalling of AI services like vision, translation etc. to generate an outcome that can be used in further steps along the pipeline.\n\nFostering a culture of sharing, re-use, design and operational efficiency of data and analytical solutions\n\nPreparing data to create a unified database and build tracking solutions ensuring data quality\n\nCreate Production grade analytical assets deployed using the guiding principles of CI/CD.\n\n\n\n\nProfessional and Technical Skills\n\nExpert in Python, Scala, Pyspark, Pytorch, Javascript (any 2 at least)\n\nExtensive experience in data analysis (Big data- Apache Spark environments), data libraries (e.g. Pandas, SciPy, Tensorflow, Keras etc.), and SQL. 2-3 years of hands-on experience working on these technologies.\n\nExperience in one of the many BI tools such as Tableau, Power BI, Looker.\n\nGood working knowledge of key concepts in data analytics, such as dimensional modeling, ETL, reporting/dashboarding, data governance, dealing with structured and unstructured data, and corresponding infrastructure needs.\n\nWorked extensively in Microsoft Azure (ADF, Function Apps, ADLS, Azure SQL), AWS (Lambda,Glue,S3), Databricks analytical platforms/tools, Snowflake Cloud Datawarehouse.\n\n\n\n\nAdditional Information\n\nExperience working in cloud Data warehouses like Redshift or Synapse\n\nCertification in any one of the following or equivalent\n\nAWS- AWS certified data Analytics- Speciality\n\nAzure- Microsoft certified Azure Data Scientist Associate\n\nSnowflake- Snowpro core- Data Engineer\n\nDatabricks Data Engineering\n\n\nAbout Our Company | Accenture (do not remove the hyperlink)\n\n\n\nQualification\n\n\n\nExperience:3.5 -5 years of experience is required\n\n\n\n\nEducational Qualification:Graduation (Accurate educational details should capture)",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['scala', 'pyspark', 'pytorch', 'python', 'microsoft azure', 'glue', 'amazon redshift', 'sql', 'tensorflow', 'sql azure', 'spark', 'keras', 'big data', 'etl', 'snowflake', 'scipy', 'data analysis', 'azure data lake', 'power bi', 'data engineering', 'javascript', 'data bricks', 'pandas', 'tableau', 'lambda expressions', 'aws']",2025-06-13 05:59:34
Data Engineer,Accenture,3 - 8 years,Not Disclosed,['Hyderabad'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Informatica Data Quality\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions that facilitate data generation, collection, and processing. Your typical day will involve creating data pipelines, ensuring data quality, and implementing ETL processes to effectively migrate and deploy data across various systems. You will collaborate with team members to enhance data workflows and contribute to the overall efficiency of data management practices.\nRoles & Responsibilities:- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Assist in the design and implementation of data architecture to support data initiatives.- Monitor and optimize data pipelines for performance and reliability.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Informatica Data Quality.- Strong understanding of data integration techniques and ETL processes.- Experience with data profiling and data cleansing methodologies.- Familiarity with database management systems and SQL.- Knowledge of data governance and data quality best practices.\nAdditional Information:- The candidate should have minimum 3 years of experience in Informatica Data Quality.- This position is based at our Hyderabad office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['informatica data quality', 'sql', 'etl', 'data integration', 'etl process', 'hive', 'python', 'data management', 'data architecture', 'data engineering', 'data cleansing', 'database management', 'profiling', 'spark', 'data governance', 'hadoop', 'big data', 'informatica', 'data profiling']",2025-06-13 05:59:35
Data Engineer,Accenture,3 - 8 years,Not Disclosed,['Navi Mumbai'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :Business AgilityMinimum\n\n\n\n12 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions that facilitate data generation, collection, and processing. Your typical day will involve creating data pipelines, ensuring data quality, and implementing ETL processes to migrate and deploy data across various systems. You will collaborate with cross-functional teams to understand data requirements and deliver effective solutions that meet business needs. Additionally, you will monitor and optimize data workflows to enhance performance and reliability, ensuring that data is accessible and actionable for stakeholders.\nRoles & Responsibilities:- Need Databricks resource with Azure cloud experience- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Collaborate with data architects and analysts to design scalable data solutions.- Implement best practices for data governance and security throughout the data lifecycle.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Good To Have\n\n\n\n\nSkills:\nExperience with Business Agility.- Strong understanding of data modeling and database design principles.- Experience with data integration tools and ETL processes.- Familiarity with cloud platforms and services related to data storage and processing.\nAdditional Information:- The candidate should have minimum 3 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Pune office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data analytics', 'database design', 'data modeling', 'design principles', 'etl', 'hive', 'python', 'data warehousing', 'power bi', 'machine learning', 'data engineering', 'sql server', 'sql', 'data bricks', 'data quality', 'tableau', 'spark', 'data governance', 'hadoop', 'big data', 'aws', 'ssis', 'etl process']",2025-06-13 05:59:37
Data Engineer,Accenture,3 - 8 years,Not Disclosed,['Chennai'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :Business AgilityMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions that facilitate data generation, collection, and processing. Your typical day will involve creating data pipelines, ensuring data quality, and implementing ETL processes to migrate and deploy data across various systems. You will collaborate with cross-functional teams to understand data requirements and deliver effective solutions that meet business needs. Additionally, you will monitor and optimize data workflows to enhance performance and reliability, ensuring that data is accessible and actionable for stakeholders.\nRoles & Responsibilities:- Need Databricks resource with Azure cloud experience- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Collaborate with data architects and analysts to design scalable data solutions.- Implement best practices for data governance and security throughout the data lifecycle.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Good To Have\n\n\n\n\nSkills:\nExperience with Business Agility.- Strong understanding of data modeling and database design principles.- Experience with data integration tools and ETL processes.- Familiarity with cloud platforms and services related to data storage and processing.\nAdditional Information:- The candidate should have minimum 3 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Pune office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data analytics', 'database design', 'data modeling', 'design principles', 'etl', 'hive', 'python', 'data warehousing', 'power bi', 'machine learning', 'data engineering', 'sql server', 'sql', 'data bricks', 'data quality', 'tableau', 'spark', 'data governance', 'hadoop', 'big data', 'aws', 'ssis', 'etl process']",2025-06-13 05:59:39
Data Engineer,Accenture,3 - 8 years,Not Disclosed,['Pune'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions that facilitate data generation, collection, and processing. Your typical day will involve creating data pipelines, ensuring data quality, and implementing ETL processes to effectively migrate and deploy data across various systems. You will collaborate with cross-functional teams to understand data requirements and contribute to the overall data strategy of the organization, ensuring that data solutions are efficient, scalable, and aligned with business objectives.\nRoles & Responsibilities:- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Collaborate with stakeholders to gather and analyze data requirements.- Design and implement robust data pipelines to support data processing and analytics.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Strong understanding of data modeling and database design principles.- Experience with ETL tools and data integration techniques.- Familiarity with cloud platforms and services related to data storage and processing.- Knowledge of programming languages such as Python or Scala for data manipulation.\nAdditional Information:- The candidate should have minimum 3 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Pune office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'data analytics', 'database design', 'data modeling', 'design principles', 'hive', 'scala', 'data manipulation', 'data processing', 'pyspark', 'data warehousing', 'data engineering', 'sql', 'data quality', 'tableau', 'etl tool', 'spark', 'hadoop', 'etl', 'big data', 'data integration', 'etl process']",2025-06-13 05:59:41
Data Engineer,Accenture,3 - 8 years,Not Disclosed,['Indore'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :Business AgilityMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions that facilitate data generation, collection, and processing. Your typical day will involve creating data pipelines, ensuring data quality, and implementing ETL processes to migrate and deploy data across various systems. You will collaborate with cross-functional teams to understand data requirements and deliver effective solutions that meet business needs. Additionally, you will monitor and optimize data workflows to enhance performance and reliability, ensuring that data is accessible and actionable for stakeholders.\nRoles & Responsibilities:- Need Databricks resource with Azure cloud experience- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Collaborate with data architects and analysts to design scalable data solutions.- Implement best practices for data governance and security throughout the data lifecycle.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Good To Have\n\n\n\n\nSkills:\nExperience with Business Agility.- Strong understanding of data modeling and database design principles.- Experience with data integration tools and ETL processes.- Familiarity with cloud platforms and services related to data storage and processing.\nAdditional Information:- The candidate should have minimum 3 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Pune office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analytics', 'database design', 'data modeling', 'design principles', 'etl', 'hive', 'python', 'data warehousing', 'power bi', 'machine learning', 'data engineering', 'sql server', 'sql', 'data bricks', 'data quality', 'tableau', 'spark', 'data governance', 'hadoop', 'big data', 'aws', 'ssis', 'etl process']",2025-06-13 05:59:43
Data Engineer,Accenture,15 - 20 years,Not Disclosed,['Hyderabad'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Microsoft Azure Data Services\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n2 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions that facilitate data generation, collection, and processing. Your typical day will involve creating data pipelines, ensuring data quality, and implementing ETL processes to effectively migrate and deploy data across various systems, contributing to the overall efficiency and reliability of data management within the organization.\nRoles & Responsibilities:- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Collaborate with cross-functional teams to gather requirements and deliver data solutions that meet business needs.- Monitor and optimize data pipelines for performance and reliability.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Microsoft Azure Data Services.- Good To Have\n\n\n\n\nSkills:\nExperience with Azure Data Factory, Azure SQL Database, and Azure Synapse Analytics.- Strong understanding of data modeling and database design principles.- Experience with data integration and ETL tools.- Familiarity with data governance and data quality best practices.\nAdditional Information:- The candidate should have minimum 2 years of experience in Microsoft Azure Data Services.- This position is based at our Hyderabad office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data services', 'microsoft azure', 'database design', 'data modeling', 'design principles', 'python', 'data management', 'azure synapse', 'azure data factory', 'data engineering', 'sql', 'data quality', 'sql azure', 'etl tool', 'data governance', 'etl', 'data integration', 'etl process', 'sql database']",2025-06-13 05:59:44
Data Engineer,Accenture,3 - 8 years,Not Disclosed,['Indore'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Google BigQuery\n\n\n\n\nGood to have skills :Microsoft SQL Server, Google Cloud Data ServicesMinimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will be responsible for designing, developing, and maintaining data solutions for data generation, collection, and processing. You will create data pipelines, ensure data quality, and implement ETL processes to migrate and deploy data across systems.\nRoles & Responsibilities:- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Develop and maintain data pipelines.- Ensure data quality throughout the data lifecycle.- Implement ETL processes for data migration and deployment.- Collaborate with cross-functional teams to understand data requirements.- Optimize data storage and retrieval processes.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Google BigQuery.- Strong understanding of data engineering principles.- Experience with cloud-based data services.- Knowledge of SQL and database management systems.- Hands-on experience with data modeling and schema design.\nAdditional Information:- The candidate should have a minimum of 3 years of experience in Google BigQuery.- This position is based at our Mumbai office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data engineering', 'sql', 'data modeling', 'bigquery', 'etl', 'schema', 'hive', 'data services', 'python', 'amazon redshift', 'data warehousing', 'google', 'data migration', 'knowledge of sql', 'sql server', 'database design', 'data quality', 'tableau', 'spark', 'etl tool', 'hadoop', 'big data', 'aws', 'etl process']",2025-06-13 05:59:46
Data Engineer,Accenture,3 - 8 years,Not Disclosed,['Bhubaneswar'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :Business AgilityMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions that facilitate data generation, collection, and processing. Your typical day will involve creating data pipelines, ensuring data quality, and implementing ETL processes to migrate and deploy data across various systems. You will collaborate with cross-functional teams to understand data requirements and deliver effective solutions that meet business needs. Additionally, you will monitor and optimize data workflows to enhance performance and reliability, ensuring that data is accessible and actionable for stakeholders.\nRoles & Responsibilities:- Need Databricks resource with Azure cloud experience- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Collaborate with data architects and analysts to design scalable data solutions.- Implement best practices for data governance and security throughout the data lifecycle.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Good To Have\n\n\n\n\nSkills:\nExperience with Business Agility.- Strong understanding of data modeling and database design principles.- Experience with data integration tools and ETL processes.- Familiarity with cloud platforms and services related to data storage and processing.\nAdditional Information:- The candidate should have minimum 3 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Pune office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analytics', 'database design', 'data modeling', 'design principles', 'etl', 'hive', 'python', 'data warehousing', 'power bi', 'machine learning', 'data engineering', 'sql server', 'sql', 'data bricks', 'data quality', 'tableau', 'spark', 'data governance', 'hadoop', 'big data', 'aws', 'ssis', 'etl process']",2025-06-13 05:59:48
Data Engineer,Accenture,3 - 8 years,Not Disclosed,['Bhubaneswar'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :Business AgilityMinimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions that facilitate data generation, collection, and processing. Your typical day will involve creating data pipelines, ensuring data quality, and implementing ETL processes to migrate and deploy data across various systems. You will collaborate with cross-functional teams to understand data requirements and deliver effective solutions that meet business needs. Additionally, you will monitor and optimize data workflows to enhance performance and reliability, ensuring that data is accessible and actionable for stakeholders.\nRoles & Responsibilities:- Need Databricks resource with Azure cloud experience- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Collaborate with data architects and analysts to design scalable data solutions.- Implement best practices for data governance and security throughout the data lifecycle.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Good To Have\n\n\n\n\nSkills:\nExperience with Business Agility.- Strong understanding of data modeling and database design principles.- Experience with data integration tools and ETL processes.- Familiarity with cloud platforms and services related to data storage and processing.\nAdditional Information:- The candidate should have minimum 3 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Pune office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data analytics', 'database design', 'data modeling', 'design principles', 'etl', 'hive', 'python', 'data warehousing', 'power bi', 'machine learning', 'data engineering', 'sql server', 'sql', 'data bricks', 'data quality', 'tableau', 'spark', 'data governance', 'hadoop', 'big data', 'aws', 'ssis', 'etl process']",2025-06-13 05:59:50
Data Engineer,Accenture,3 - 8 years,Not Disclosed,['Pune'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :Business AgilityMinimum\n\n\n\n12 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions that facilitate data generation, collection, and processing. Your typical day will involve creating data pipelines, ensuring data quality, and implementing ETL processes to migrate and deploy data across various systems. You will collaborate with cross-functional teams to understand data requirements and deliver effective solutions that meet business needs. Additionally, you will monitor and optimize data workflows to enhance performance and reliability, ensuring that data is accessible and actionable for stakeholders.\nRoles & Responsibilities:- Need Databricks resource with Azure cloud experience- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Collaborate with data architects and analysts to design scalable data solutions.- Implement best practices for data governance and security throughout the data lifecycle.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Good To Have\n\n\n\n\nSkills:\nExperience with Business Agility.- Strong understanding of data modeling and database design principles.- Experience with data integration tools and ETL processes.- Familiarity with cloud platforms and services related to data storage and processing.\nAdditional Information:- The candidate should have minimum 3 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Pune office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data analytics', 'database design', 'data modeling', 'design principles', 'etl', 'hive', 'python', 'data warehousing', 'power bi', 'machine learning', 'data engineering', 'sql server', 'sql', 'data bricks', 'data quality', 'tableau', 'spark', 'data governance', 'hadoop', 'big data', 'aws', 'ssis', 'etl process']",2025-06-13 05:59:52
Data Engineer,Accenture,5 - 10 years,Not Disclosed,['Chennai'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL processes to migrate and deploy data across systems. Your day will involve working on data architecture and engineering tasks to support business operations and decision-making.\nRoles & Responsibilities:- Expected to be an SME, collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Develop and maintain data pipelines for efficient data processing.- Implement ETL processes to ensure seamless data migration and deployment.- Collaborate with cross-functional teams to design and optimize data solutions.- Conduct data quality assessments and implement improvements for data integrity.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Strong understanding of data architecture principles.- Experience in designing and implementing data solutions.- Proficient in SQL and other data querying languages.- Knowledge of cloud platforms such as AWS or Azure.\nAdditional Information:- The candidate should have a minimum of 5 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Chennai office.- A 15 years full-time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analytics', 'sql', 'data architecture principles', 'etl', 'aws', 'hive', 'python', 'data processing', 'airflow', 'microsoft azure', 'pyspark', 'data warehousing', 'data integrity', 'data migration', 'data engineering', 'data quality', 'spark', 'hadoop', 'business operations', 'big data', 'etl process']",2025-06-13 05:59:54
Data Engineer,Accenture,3 - 8 years,Not Disclosed,['Chennai'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :Business AgilityMinimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions that facilitate data generation, collection, and processing. Your typical day will involve creating data pipelines, ensuring data quality, and implementing ETL processes to migrate and deploy data across various systems. You will collaborate with cross-functional teams to understand data requirements and deliver effective solutions that meet business needs. Additionally, you will monitor and optimize data workflows to enhance performance and reliability, ensuring that data is accessible and actionable for stakeholders.\nRoles & Responsibilities:- Need Databricks resource with Azure cloud experience- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Collaborate with data architects and analysts to design scalable data solutions.- Implement best practices for data governance and security throughout the data lifecycle.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Good To Have\n\n\n\n\nSkills:\nExperience with Business Agility.- Strong understanding of data modeling and database design principles.- Experience with data integration tools and ETL processes.- Familiarity with cloud platforms and services related to data storage and processing.\nAdditional Information:- The candidate should have minimum 3 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Pune office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analytics', 'database design', 'data modeling', 'design principles', 'etl', 'hive', 'python', 'data warehousing', 'power bi', 'machine learning', 'data engineering', 'sql server', 'sql', 'data bricks', 'data quality', 'tableau', 'spark', 'data governance', 'hadoop', 'big data', 'aws', 'ssis', 'etl process']",2025-06-13 05:59:55
Data Engineer,Accenture,3 - 8 years,Not Disclosed,['Coimbatore'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Talend ETL\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL processes to migrate and deploy data across systems. Be involved in the end-to-end data management process.\nRoles & Responsibilities:- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work-related problems.- Develop and maintain data pipelines for efficient data processing.- Ensure data quality and integrity throughout the data lifecycle.- Implement ETL processes to extract, transform, and load data.- Collaborate with cross-functional teams to optimize data solutions.- Conduct data analysis to identify trends and insights.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Talend ETL.- Strong understanding of data integration and ETL processes.- Experience with data modeling and database design.- Knowledge of SQL and database querying languages.- Hands-on experience with data warehousing concepts.\nAdditional Information:- The candidate should have a minimum of 3 years of experience in Talend ETL.- This position is based at our Hyderabad office.- A 15 years full-time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['sql', 'talend etl', 'etl', 'data integration', 'etl process', 'hive', 'python', 'data analysis', 'data management', 'talend', 'data processing', 'data warehousing', 'knowledge of sql', 'data engineering', 'database design', 'data quality', 'data modeling', 'spark', 'data warehousing concepts', 'hadoop']",2025-06-13 05:59:57
Data Engineer,Accenture,3 - 8 years,Not Disclosed,['Hyderabad'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :Business AgilityMinimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions that facilitate data generation, collection, and processing. Your typical day will involve creating data pipelines, ensuring data quality, and implementing ETL processes to migrate and deploy data across various systems. You will collaborate with cross-functional teams to understand data requirements and deliver effective solutions that meet business needs. Additionally, you will monitor and optimize data workflows to enhance performance and reliability, ensuring that data is accessible and actionable for stakeholders.\nRoles & Responsibilities:- Need Databricks resource with Azure cloud experience- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Collaborate with data architects and analysts to design scalable data solutions.- Implement best practices for data governance and security throughout the data lifecycle.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Good To Have\n\n\n\n\nSkills:\nExperience with Business Agility.- Strong understanding of data modeling and database design principles.- Experience with data integration tools and ETL processes.- Familiarity with cloud platforms and services related to data storage and processing.\nAdditional Information:- The candidate should have minimum 3 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Pune office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data analytics', 'database design', 'data modeling', 'design principles', 'etl', 'hive', 'python', 'data warehousing', 'power bi', 'machine learning', 'data engineering', 'sql server', 'sql', 'data bricks', 'data quality', 'tableau', 'spark', 'data governance', 'hadoop', 'big data', 'aws', 'ssis', 'etl process']",2025-06-13 05:59:59
Data Engineer,Accenture,3 - 8 years,Not Disclosed,['Navi Mumbai'],"Project Role :Data Engineer\n\n\n\n\n\nProject Role Description :Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :Business AgilityMinimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Engineer, you will design, develop, and maintain data solutions that facilitate data generation, collection, and processing. Your typical day will involve creating data pipelines, ensuring data quality, and implementing ETL processes to migrate and deploy data across various systems. You will collaborate with cross-functional teams to understand data requirements and deliver effective solutions that meet business needs. Additionally, you will monitor and optimize data workflows to enhance performance and reliability, ensuring that data is accessible and actionable for stakeholders.\nRoles & Responsibilities:- Need Databricks resource with Azure cloud experience- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Collaborate with data architects and analysts to design scalable data solutions.- Implement best practices for data governance and security throughout the data lifecycle.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Good To Have\n\n\n\n\nSkills:\nExperience with Business Agility.- Strong understanding of data modeling and database design principles.- Experience with data integration tools and ETL processes.- Familiarity with cloud platforms and services related to data storage and processing.\nAdditional Information:- The candidate should have minimum 3 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Pune office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analytics', 'database design', 'data modeling', 'design principles', 'etl', 'hive', 'python', 'data warehousing', 'power bi', 'machine learning', 'data engineering', 'sql server', 'sql', 'data bricks', 'data quality', 'tableau', 'spark', 'data governance', 'hadoop', 'big data', 'aws', 'ssis', 'etl process']",2025-06-13 06:00:01
"Data Engineer, Devices",Amazon,5 - 10 years,Not Disclosed,['Noida'],"Are you a highly skilled data engineer and project leaderDo you think big, enjoy complexity and building solutions that scaleAre you curious to know what you could achieve in a company that pushes the boundaries of modern technologyIf you answered yes and you have a background in FinTech you ll love this role and Amazon s data obsessed culture.\n\nAmazon Devices and Services Fintech is the global team that designs and builds the financial planning and analysis tools for wide variety of Amazon s new and established organizations. From Kindle to Ring and even new and exciting companies like Kuiper (our new interstellar satellite play) this team enjoys a wide variety of complex and interesting problem spaces. They are almost like FinTech consultants embedded in Amazon.\n\nThis team are looking for a Data Engineer to build and enhance the businesses finance systems with TM1 at its core. You will manage all aspects from requirements gathering, technical design, development, deployment, and integration to solve budgeting, planning, performance management and reporting challenges\n\n\nDesign and implement next generation financial solutions assisted by almost unlimited access to AWS resources including EC2, RDS, Redshift, Stepfunctions, EMR, Lambda and 3rd party software TM1.\nBuild and deliver high quality data pipelines capable of scaling from running for a single month of data during month end close to 150 and more months when doing restatements.\nContinually improve ongoing reporting and analysis processes and infrastructure, automating or simplifying self-service capabilities for customers.\nDive deep to resolve problems at their root, looking for failure patterns and suggesting and implementing fixes or enhancements.\nPrepare runbooks, methods of procedures, tutorials, training videos on best practices for global delivery.\nSolve unique challenges presented by the massive data volume and diverse data sets working for one of the largest companies in the wo 5+ years data engineering experience.\nExtensive experience writing SQL queries and stored procedures.\nExperience with big data tools and distributed computing.\nFinance experience, exhibiting knowledge of financial reporting, budgeting and forecasting functions and processes.\nBachelors degree. Experience with non-relational databases / data stores (object storage, document or key-value stores, graph databases, column-family databases)\nExperience with AWS technologies like Redshift, S3, AWS Glue, EMR, Kinesis, FireHose, Lambda, and IAM roles and permissions.\nExperience with programming languages such as python, java shell scripts.\nExperience with IBM Planning Analytics/TM1 both scripting processes and writing rules.\nExperience with design delivery of formal training curriculum and programs.\nProject management, scoping, reporting, and scheduling experience.",,,,"['Performance management', 'Financial reporting', 'Project management', 'Financial planning', 'Scheduling', 'Stored procedures', 'Budgeting', 'Forecasting', 'Analytics', 'Python']",2025-06-13 06:00:02
S&C GN - Data&AI - Life Sciences - Analyst,Accenture,2 - 7 years,Not Disclosed,['Gurugram'],"Management Level:Ind & Func AI Decision Science Analyst\n\n\n\n\nJob Location:Bangalore / Gurgaon\n\n\n\nMust-have\n\n\n\n\nSkills:\nLife Sciences/Pharma/Healthcare projects and delivering successful outcomes, commercial, clinical, Statistical Models/Machine Learning including Segmentation & predictive modeling, hypothesis testing, multivariate statistical analysis, time series techniques, and optimization.\n\n\n\nGood-to-have\n\n\n\n\nSkills:\nProficiency in Programming languages such as R, Python, SQL, Spark, AWS, Azure, or Google Cloud for deploying and scaling language models, Data Visualization tools like Tableau, Power BI\n\n\n\nExperience:Proven experience (2+ years) in working on Life Sciences/Pharma/Healthcare projects and delivering successful outcomes.\n\n\n\n\nEducational Qualification:Bachelors or Masters degree in Statistics, Data Science, Applied Mathematics, Business Analytics, Computer Science, Information Systems, or other Quantitative field.\n\n\n\nJob\n\n\nSummary\n\nWe are seeking an experienced and visionary - Accenture S&C Global Network - Data & AI practice help our clients grow their business in entirely new ways. Analytics enables our clients to achieve high performance through insights from data - insights that inform better decisions and strengthen customer relationships. From strategy to execution, Accenture works with organizations to develop analytic capabilities - from accessing and reporting on data to predictive modelling - to outperform the competition.\n\n\n\nKey Responsibilities\nSupport delivery of small to medium-sized teams to deliver consulting projects for global clients.\nAn opportunity to work on high-visibility projects with top Pharma clients around the globe.\nPersonalized training modules to develop your strategy & consulting acumen to grow your skills, industry knowledge, and capabilities.\nResponsibilities may include strategy, implementation, process design, and change management for specific modules.\nWork with the team or as an Individual contributor on the project assigned which includes a variety of skills to be utilized from Data Engineering to Data Science\nDevelop assets and methodologies, point-of-view, research, or white papers for use by the team and the larger community.\nWork on variety of projects in Data Modeling, Data Engineering, Data Visualization, Data Science etc.,\nAcquire new skills that have utility across industry groups.\n\n\n\n\n\nAdditional Information\nAbility to solve complex business problems and deliver client delight.\nStrong writing skills to build points of view on current industry trends.\nGood communication, interpersonal, and presentation skills\n\n\nAbout Our Company | Accenture (do not remove the hyperlink)\n\n\nQualification\n\n\n\nExperience:Proven experience (2+ years) in working on Life Sciences/Pharma/Healthcare projects and delivering successful outcomes.\n\n\n\n\nEducational Qualification:Bachelors or Masters degree in Statistics, Data Science, Applied Mathematics, Business Analytics, Computer Science, Information Systems, or other Quantitative field.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['pharmaceutical', 'python', 'sql', 'life sciences', 'aws', 'presentation skills', 'microsoft azure', 'power bi', 'time series', 'machine learning', 'artificial intelligence', 'tableau', 'r', 'data science', 'gcp', 'spark', 'predictive modeling', 'statistical modeling', 'data visualization', 'statistics']",2025-06-13 06:00:04
"Lead Engineer, Data Engineering (J2EE/Angular/React/React Full Stack)",S&P Global Market Intelligence,10 - 15 years,Not Disclosed,"['Mumbai', 'Maharastra']","About the Role:\nGrade Level (for internal use): 11\nThe Team\nYou will be an expert contributor and part of the Rating Organizations Data Services Product Engineering Team. This team, who has a broad and expert knowledge on Ratings organizations critical data domains, technology stacks and architectural patterns, fosters knowledge sharing and collaboration that results in a unified strategy. All Data Services team members provide leadership, innovation, timely delivery, and the ability to articulate business value. Be a part of a unique opportunity to build and evolve S&P Ratings next gen analytics platform.\nResponsibilities:\nArchitect, design, and implement innovative software solutions to enhance S&P Ratings' cloud-based analytics platform.\nMentor a team of engineers (as required), fostering a culture of trust, continuous growth, and collaborative problem-solving.\nCollaborate with business partners to understand requirements, ensuring technical solutions align with business goals.\nManage and improve existing software solutions, ensuring high performance and scalability.\nParticipate actively in all Agile scrum ceremonies, contributing to the continuous improvement of team processes.\nProduce comprehensive technical design documents and conduct technical walkthroughs.\nExperience & Qualifications:\nBachelors degree in computer science, Information Systems, Engineering, equivalent or more is required\nProficient with software development lifecycle (SDLC) methodologies like Agile, Test-driven development\n10+ years of experience with 4+ years designing/developing enterprise products, modern tech stacks and data platforms\n4+ years of hands-on experience contributing to application architecture & designs, proven software/enterprise integration design patterns and full-stack knowledge including modern distributed front end and back-end technology stacks\n5+ years full stack development experience in modern web development technologies, Java/J2EE, UI frameworks like Angular, React, SQL, Oracle, NoSQL Databases like MongoDB\nExperience designing transactional/data warehouse/data lake and data integrations with Big data eco system leveraging AWS cloud technologies\nThorough understanding of distributed computing\nPassionate, smart, and articulate developer\nQuality first mindset with a strong background and experience with developing products for a global audience at scale\nExcellent analytical thinking, interpersonal, oral and written communication skills with strong ability to influence both IT and business partners\nSuperior knowledge of system architecture, object-oriented design, and design patterns.\nGood work ethic, self-starter, and results-oriented\nExcellent communication skills are essential, with strong verbal and writing proficiencies\nExp. with Delta Lake systems like Databricks using AWS cloud technologies and PySpark is a plus\nAdditional Preferred Qualifications:\nExperience working AWS\nExperience with SAFe Agile Framework\nBachelor's/PG degree in Computer Science, Information Systems or equivalent.\nHands-on experience contributing to application architecture & designs, proven software/enterprise integration design principles\nAbility to prioritize and manage work to critical project timelines in a fast-paced environment\nExcellent Analytical and communication skills are essential, with strong verbal and writing proficiencies\nAbility to train and mentor",Industry Type: Banking,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'AWS cloud technologies', 'PySpark', 'J2EE', 'React Full Stack', 'Databricks', 'React', 'Angular']",2025-06-13 06:00:05
Training & Internship - Data Analytics,SSS Grameen Services,3 months duration,Unpaid,[],"This is a remote position.\n\nThere are internships and Projects for\n- Final Year University students (BBA/MBA)\n- Freshers & housewives\nPassionate students willing to learn and hone their skills are welcome to apply.\n\nRequirements\nWhat Next:\n- Apply with Resume, bonafide certificate, UID Aadhaar, Tentative area of project ( Analytics / AI / Sustainability / Cybersecurity etc.,))\nAppear for\nTechnical Screening #1 (Python or R Coding)\nTechnical Screening #2 (Coding test on ML algorithms like SVM and its implementation or similar)\nAptitude #3 (for Industry Usecases)\n\n- Technical assessment result\nPass - direct Project internship 2-3 months\n- Closure: Letter of internship, Project Report, Completion certificate (apply for respective UGC credits from your University)\n\n\nBenefits\nStipend: nil\nBenefits: Certificate of internship and Project\nMode: LIVE Virtual Remote\n",Industry Type: Management Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Training', 'Technical training', 'LMS', 'Coding', 'Soft skills training', 'Data analytics', 'Internship', 'Python', 'Testing', 'Recruitment']",2025-06-13 06:00:07
Fresher Data Ops,Sakon,0 - 1 years,Not Disclosed,['Pune'],"Walk-In Drive for Interns and Associate Engineers Saturday, 14th June 2025\n\nDear Candidate,We are pleased to invite you to participate in our walk-in interview drive for the roles of Interns and Associate Engineers.\nDate: Saturday, 14th June 2025 Reporting Time: 10:30 AM Please bring the following:\nA copy of your updated resume\nA valid ID proof for entry\nRoles & Responsibilities:\nDesign and develop Data Integration Pipelines\nExecute data ingestion processes, ensuring the timely and accurate transfer of data into our systems\nTroubleshoot and resolve data ingestion issues in a timely manner\nCollaborate with cross-functional teams to understand data requirements and sources\nWork closely in data Engineering and Automation team to optimize and streamline data ingestion workflows\nStay informed about industry best practices and emerging technologies in data management\nInterview Process:\nGroup Discussion\nTechnical test\nTechnical Interview\nWe look forward to meeting you and wish you all the best for the interview.\nBest regards,Anuradha DhalLead-TA",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Python', 'Communication Skills', 'Computer Science', 'SQL']",2025-06-13 06:00:08
DE&A - Core - Big Data Engineering - ETL Orchestration DE&A - Core,Zensar,5 - 9 years,Not Disclosed,"['Kolkata', 'Mumbai', 'New Delhi', 'Hyderabad', 'Pune', 'Chennai', 'Bengaluru']","Design of complex ETL interfaces with agnostic tool set for various source/target types (SAP BODS preferred)\nPerformance Tuning & Troubleshooting skills across all technologies used\nStrong in DB and SQLs with decent data modeling skills\nAbility to lead developers and QA and adhere to committed timelines\nAgile experience is preferred\nAbility to work with upstream, downstream, and reporting system stakeholders and convert business requirements into technical requirements (JIRA stories)\n10 years of experience in:\n- ETL tools\n- Repository maintenance\n- Migration of jobs/workflows/dataflows\n- Job monitoring, break fix, user maintenance\n- CDC (Change Data Capture)\n- Oracle Golden Gate\n- Working with different sources/targets (Oracle, SAP, Files as source; SQL Server, file uploads as targets)\nRole Description :\n- Responsible for planning, developing, implementing, and managing data warehouse project deliverables\n- Design documents and high-level data mapping for ETL specifications\n- Create transformations in ETL jobs to achieve data cleansing and standardizations during initial data load\n- Provide support for integration testing, defect fixing, and deployment\nProficient knowledge in:\n- Transformations like Query Push down, SQL Table Comparison, Pivot, Look up, etc.\n- Databases like Oracle, SQL Queries, SQL Server\n- Dimensional modeling, data mining, and data warehouse concepts\nExcellent analytical skills\nKnowledge to transfer data from Oracle, SQL Server tables, and Web Services either incrementally (Delta Load) or full load to the data warehouse on a periodic basis\nTroubleshooting existing ETL jobs and improving performance of existing jobs\nCreating and loading data into aggregate tables using transformations\nAbility to perform tasks individually and independently\nDesign of complex ETL interfaces with agnostic tool set for various source/target types (SAP BODS preferred)\nPerformance Tuning & Troubleshooting skills across all technologies used\nStrong in DB and SQLs with decent data modeling skills\nAbility to lead developers and QA and adhere to committed timelines\nAgile experience is preferred\nAbility to work with upstream, downstream, and reporting system stakeholders and convert business requirements into technical requirements (JIRA stories)\n10 years of experience in:\n- ETL tools\n- Repository maintenance\n- Migration of jobs/workflows/dataflows\n- Job monitoring, break fix, user maintenance\n- CDC (Change Data Capture)\n- Oracle Golden Gate\n- Working with different sources/targets (Oracle, SAP, Files as source; SQL Server, file uploads as targets)\nRole Description :\n- Responsible for planning, developing, implementing, and managing data warehouse project deliverables\n- Design documents and high-level data mapping for ETL specifications\n- Create transformations in ETL jobs to achieve data cleansing and standardizations during initial data load\n- Provide support for integration testing, defect fixing, and deployment\nProficient knowledge in:\n- Transformations like Query Push down, SQL Table Comparison, Pivot, Look up, etc.\n- Databases like Oracle, SQL Queries, SQL Server\n- Dimensional modeling, data mining, and data warehouse concepts\nExcellent analytical skills\nKnowledge to transfer data from Oracle, SQL Server tables, and Web Services either incrementally (Delta Load) or full load to the data warehouse on a periodic basis\nTroubleshooting existing ETL jobs and improving performance of existing jobs\nCreating and loading data into aggregate tables using transformations\nAbility to perform tasks individually and independently",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Performance tuning', 'SAP', 'Data modeling', 'Integration testing', 'Agile', 'data mapping', 'Troubleshooting', 'Data mining', 'JIRA', 'Monitoring']",2025-06-13 06:00:10
Data Engineer,Grid Dynamics,4 - 9 years,Not Disclosed,['Bengaluru'],"Qualifications we are looking for\nMaster/Bachelor degree in Computer Science, Electrical Engineering, Information Systems or other technical discipline; advanced degree preferred.\nMinimum of 7+ years of software development experience (with a concentration in data centric initiatives), with demonstrated expertise in leveraging standard development best practice methodologies.\nMinimum 4+ years of experience in Hadoop using Core Java Programming, Spark, Scala, Hive and Go lang\nExpertise in Object Oriented Programming Language Java\nExperience using CI/CD Process, version control and bug tracking tools.\nExperience in handling very large data volume in Real Time and batch mode.\nExperience with automation of job execution and validation\nStrong knowledge of Database concepts\nStrong team player.\nStrong communication skills with proven ability to present complex ideas and document in a clear and concise way.\nQuick learner; self-starter, detailed and in-depth.",Industry Type: IT Services & Consulting,Department: Other,"Employment Type: Full Time, Permanent","['Data Engineering', 'SCALA', 'Bigdata Technologies', 'Spark']",2025-06-13 06:00:11
Big Data Developer/ Senior Big Data Developer,Grid Dynamics,5 - 10 years,Not Disclosed,"['Hyderabad', 'Chennai', 'Bengaluru']","About us\nGrid Dynamics (NASDAQ: GDYN) is a leading provider of technology consulting, platform and product engineering, AI, and advanced analytics services. Fusing technical vision with business acumen, we solve the most pressing technical challenges and enable positive business outcomes for enterprise companies undergoing business transformation. A key differentiator for Grid Dynamics is our 8 years of experience and leadership in enterprise AI, supported by profound expertise and ongoing investment in data, analytics, cloud & DevOps, application modernization and customer experience. Founded in 2006, Grid Dynamics is headquartered in Silicon Valley with offices across the Americas, Europe, and India.\n\nRole & responsibilities\nWe are looking for an enthusiastic and technology-proficient Big Data Engineer, who is eager to participate in the design and implementation of a top-notch Big Data solution to be deployed at massive scale.\nOur customer is one of the world's largest technology companies based in Silicon Valley with operations all over the world. On this project we are working on the bleeding-edge of Big Data technology to develop high performance data analytics platform, which handles petabytes datasets.\nEssential functions\nParticipate in design and development of Big Data analytical applications.\nDesign, support and continuously enhance the project code base, continuous integration pipeline, etc.\nWrite complex ETL processes and frameworks for analytics and data management.\nImplement large-scale near real-time streaming data processing pipelines.\nWork inside the team of industry experts on the cutting edge Big Data technologies to develop solutions for deployment at massive scale.\nQualifications\nStrong coding experience with Scala, Spark,Hive, Hadoop.\nIn-depth knowledge of Hadoop and Spark, experience with data mining and stream processing technologies (Kafka, Spark Streaming, Akka Streams).\nUnderstanding of the best practices in data quality and quality engineering.\nExperience with version control systems, Git in particular.\nDesire and ability for quick learning of new tools and technologies.\nWould be a plus\nKnowledge of Unix-based operating systems (bash/ssh/ps/grep etc.).\nExperience with Github-based development processes.\nExperience with JVM build systems (SBT, Maven, Gradle).\nWe offer\nOpportunity to work on bleeding-edge projects\nWork with a highly motivated and dedicated team\nCompetitive salary\nFlexible schedule\nBenefits package - medical insurance, sports\nCorporate social events\nProfessional development opportunities\nWell-equipped office",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Big Data', 'SCALA', 'Hadoop', 'Spark']",2025-06-13 06:00:13
Data Bricks,PwC India,7 - 12 years,Not Disclosed,['Bengaluru'],"Job Summary:\n\nWe are seeking a talented Data Engineer with strong expertise in Databricks, specifically in Unity Catalog, PySpark, and SQL, to join our data team. Youll play a key role in building secure, scalable data pipelines and implementing robust data governance strategies using Unity Catalog.\n\nKey Responsibilities:",,,,"['DataBricks', 'Data Bricks', 'Pyspark', 'Delta Lake', 'Databricks Engineer', 'Unity Catalog', 'SQL']",2025-06-13 06:00:14
S&C GN - Data&AI - CMT Eng - Consultant,Accenture,2 - 7 years,Not Disclosed,['Bengaluru'],"Job Title - S&C Global Network - AI - CMT DE- Consultant\n\n\n\nManagement Level:9- Consultant\n\n\n\nLocation:Open\n\n\n\nMust-have skills:Data Engineering\n\n\n\n\nGood to have skills:Ability to leverage design thinking, business process optimization, and stakeholder management skills.\n\n\n\nJob\n\n\nSummary:\n\nWe are looking for a passionate and results-driven\n\n\n\nData Engineerto join our growing data team. You will be responsible for designing, building, and maintaining scalable data pipelines and infrastructure that support data-driven decision-making across the organization.\n\n\n\n\nRoles & Responsibilities:\n\nDesign, build, and maintain robust, scalable, and efficient data pipelines (ETL/ELT).\nWork with structured and unstructured data across a wide variety of data sources.\nCollaborate with data analysts, data scientists, and business stakeholders to understand data requirements.\nOptimize data systems and architecture for performance, scalability, and reliability.\nMonitor data quality and support initiatives to ensure clean, accurate, and consistent data.\nDevelop and maintain data models and metadata.\nImplement and maintain best practices in data governance, security, and compliance.\n\n\n\n\nProfessional & Technical\n\n\n\n\nSkills:\n\n2+ years in data engineering or related fields\nProficiency in SQL and experience with relational databases (e.g., PostgreSQL, MySQL).\nStrong programming skills in Python, Scala, or Java.\nExperience with big data technologies such as Spark, Hadoop, or Hive.\nFamiliarity with cloud platforms like AWS, Azure, or GCP, especially services like S3, Redshift, BigQuery, or Azure Data Lake.\nExperience with orchestration tools like Airflow, Luigi, or similar.\nSolid understanding of data warehousing concepts and data modeling techniques.\nGood problem-solving skills and attention to detail.\nExperience with modern data stack tools like dbt, Snowflake, or Databricks.\nKnowledge of CI/CD pipelines and version control (e.g., Git).\nExposure to containerization (Docker, Kubernetes) and infrastructure as code (Terraform, CloudFormation).\n\n\n\n\nAdditional Information: - The ideal candidate will possess a strong educational background in quantitative discipline and experience in working with Hi-Tech clients\n\n- This position is based at our Bengaluru (preferred) and other AI Accenture locations.\n\n\n\n\n\nAbout Our Company | Accenture\n\nQualification\n\n\n\nExperience:4+ years\n\n\n\n\nEducational Qualification:Btech/ BE",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'scala', 'data engineering', 'sql', 'java', 'hive', 'continuous integration', 'kubernetes', 'snowflake', 'amazon redshift', 'airflow', 'microsoft azure', 'ci/cd', 'aws cloudformation', 'docker', 'data bricks', 'data modeling', 'spark', 'gcp', 'data warehousing concepts', 'terraform', 'hadoop', 'aws']",2025-06-13 06:00:16
Data Science,Global Banking Organization,5 - 10 years,Not Disclosed,['Bengaluru'],"Key Skills: Machine Learning, Data Science, Azure, Python, Hadoop.\nRoles and Responsibilities:\nStrong understanding of Math, Statistics, and the theoretical foundations of Statistical & Machine Learning, including Parametric and Non-parametric models.\nApply advanced data mining techniques to curate, process, and transform raw data into reliable datasets.\nUse various statistical techniques and ML methods to perform predictive modeling/classification for problems related to clients, distribution, sales, client profiles, and segmentation, and provide actionable insights for business decision-making.\nDemonstrate expertise in the full Machine Learning lifecycle--feature engineering, training, validation, scaling, deployment, scoring, monitoring, and feedback loops.\nProficiency in Python visualization libraries such as matplotlib and seaborn.\nExperience with cloud computing infrastructure like Azure, including Machine Learning Studio, Azure Data Factory, Synapse, Python, and PySpark.\nAbility to develop, test, and deploy models on cloud/web platforms.\nExcellent knowledge of Deep Learning Architectures, including Convolutional Neural Networks and Transformer/LLM Foundation Models.\nStrong expertise in supervised and adversarial learning techniques.\nRobust working knowledge of deep learning frameworks such as TensorFlow, Keras, and PyTorch.\nExcellent Python coding skills.\nExperience with version control tools (Git, GitHub/GitLab) and data version control.\nExperience in end-to-end model deployment and productionization.\nDemonstrated proficiency in deploying, scaling, and optimizing ML models in production environments with low latency, high availability, and cost efficiency.\nSkilled in model interpretability and CI/CD for ML using tools like MLflow and Kubeflow, with the ability to implement automated monitoring, logging, and retraining strategies.\nExperience Requirement:\n5-12 years of experience in designing and deploying deep learning and machine learning solutions.\nProven track record of delivering AI/ML solutions in real-world business applications at scale.\nHands-on experience working in cross-functional teams including data engineers, product managers, and business stakeholders.\nExperience mentoring junior data scientists and providing technical leadership within a data science team.\nExperience working with big data tools and environments such as Hadoop, Spark, or Databricks is a plus.\nPrior experience in managing model lifecycle in enterprise production environments including drift detection and retraining pipelines.\nEducation: B.Tech.",Industry Type: Financial Services,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Azure', 'Hadoop.', 'Machine Learning', 'Python']",2025-06-13 06:00:18
S&C GN - Data&AI - Retail - Consultant,Accenture,4 - 9 years,Not Disclosed,['Bengaluru'],"Job Title - Retail Specialized Data Scientist Level 9 SnC GN Data & AI\n\n\n\nManagement Level:09 - Consultant\n\n\n\nLocation:Bangalore / Gurgaon / Mumbai / Chennai / Pune / Hyderabad / Kolkata\n\n\n\nMust have skills:\nA solid understanding of retail industry dynamics, including key performance indicators (KPIs) such as sales trends, customer segmentation, inventory turnover, and promotions.\nStrong ability to communicate complex data insights to non-technical stakeholders, including senior management, marketing, and operational teams.\nMeticulous in ensuring data quality, accuracy, and consistency when handling large, complex datasets.\nGather and clean data from various retail sources, such as sales transactions, customer interactions, inventory management, website traffic, and marketing campaigns.\nStrong proficiency in Python for data manipulation, statistical analysis, and machine learning (libraries like Pandas, NumPy, Scikit-learn).\nExpertise in supervised and unsupervised learning algorithms\nUse advanced analytics to optimize pricing strategies based on market demand, competitor pricing, and customer price sensitivity.\n\n\n\n\nGood to have skills:\nFamiliarity with big data processing platforms like Apache Spark, Hadoop, or cloud-based platforms such as AWS or Google Cloud for large-scale data processing.\nExperience with ETL (Extract, Transform, Load) processes and tools like Apache Airflow to automate data workflows.\nFamiliarity with designing scalable and efficient data pipelines and architecture.\nExperience with tools like Tableau, Power BI, Matplotlib, and Seaborn to create meaningful visualizations that present data insights clearly.\n\n\nJob\n\n\nSummary: The Retail Specialized Data Scientist will play a pivotal role in utilizing advanced analytics, machine learning, and statistical modeling techniques to help our retail business make data-driven decisions. This individual will work closely with teams across marketing, product management, supply chain, and customer insights to drive business strategies and innovations. The ideal candidate should have experience in retail analytics and the ability to translate data into actionable insights.\n\n\n\n\nRoles & Responsibilities:\nLeverage Retail Knowledge:Utilize your deep understanding of the retail industry (merchandising, customer behavior, product lifecycle) to design AI solutions that address critical retail business needs.\nGather and clean data from various retail sources, such as sales transactions, customer interactions, inventory management, website traffic, and marketing campaigns.\nApply machine learning algorithms, such as classification, clustering, regression, and deep learning, to enhance predictive models.\nUse AI-driven techniques for personalization, demand forecasting, and fraud detection.\nUse advanced statistical methods help optimize existing use cases and build new products to serve new challenges and use cases.\nStay updated on the latest trends in data science and retail technology.\nCollaborate with executives, product managers, and marketing teams to translate insights into business actions.\n\n\n\n\nProfessional & Technical Skills:\nStrong analytical and statistical skills.\nExpertise in machine learning and AI.\nExperience with retail-specific datasets and KPIs.\nProficiency in data visualization and reporting tools.\nAbility to work with large datasets and complex data structures.\nStrong communication skills to interact with both technical and non-technical stakeholders.\nA solid understanding of the retail business and consumer behavior.\nProgramming Languages:Python, R, SQL, Scala\nData Analysis Tools:Pandas, NumPy, Scikit-learn, TensorFlow, Keras\nVisualization Tools:Tableau, Power BI, Matplotlib, Seaborn\nBig Data Technologies:Hadoop, Spark, AWS, Google Cloud\nDatabases:SQL, NoSQL (MongoDB, Cassandra)\n\n\n\n\nAdditional Information: -\n\nQualification\n\n\n\nExperience:Minimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification:Bachelors or Master's degree in Data Science, Statistics, Computer Science, Mathematics, or a related field.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'machine learning', 'artificial intelligence', 'data visualization', 'statistics', 'algorithms', 'data manipulation', 'scikit-learn', 'scala', 'numpy', 'unsupervised learning', 'sql', 'pandas', 'tensorflow', 'spark', 'consumer behavior', 'keras', 'hadoop', 'aws', 'reporting tools', 'retail business']",2025-06-13 06:00:19
S&C GN - Data&AI - Hi Tech - Data Science - Consultant,Accenture,3 - 7 years,Not Disclosed,['Bengaluru'],"Job Title - S&C Global Network - AI - Hi Tech - Data Science Consultant\n\n\n\nManagement Level:9-Team Lead/Consultant\n\n\n\nLocation:Hyderabad, HDC2A\n\n\n\nMust-have skills:Data Science\n\n\n\n\nGood to have skills:Ability to leverage design thinking, business process optimization, and stakeholder management skills.\n\n\n\nJob\n\n\nSummary:\n\nThis role involves driving strategic initiatives, managing business transformations, and leveraging industry expertise to create value-driven solutions.\n\n\n\n\nRoles & Responsibilities:\n\nProvide strategic advisory services, conduct market research, and develop data-driven recommendations to enhance business performance.\n\n\n\nWHATS IN IT FOR YOU\nWe are seeking a skilled and experienced Data Scientist to join our Hi-Tech practice.\nThe ideal candidate should have hands-on experience in data science within industries such as semiconductors, enterprise technology, consumer technology, medical technology.\nAs a Data Scientist, you will be responsible for developing AI models/applying GenAI techniques in areas such as marketing & consumer analytics, predictive asset maintenance, production optimization, supply chain, sales & channel partner program analytics, and connected products.\n\n\n\nWhat you would do in this role\nDevelop and implement AI models and GenAI applications to address business challenges in semiconductors, enterprise technology, consumer technology, medical technology and related industries.\nCollaborate with cross-functional teams to gather requirements, design solutions, and deploy models into production environments.\nDevelop and implement GenAI based solutions through contextual prompt engineering and prompt tuning and supporting solution architects on the design of GenAI-powered solutions/assets.\nUtilize your expertise in PLM/ERP/CRM/Contact Center systems to integrate data sources and ensure seamless operation of AI solutions.\nDesign and develop machine learning models using Python, with proficiency in NLP and Computer Vision techniques.\nArchitect functional solutions and provide technical guidance to enhance the performance and scalability of AI systems.\nLeverage cloud platforms, with preference for Azure/GCP, and experience with AWS is also valued.\nStay updated on emerging technologies and industry trends, contributing to continuous improvement initiatives within the organization.\n\n\n\n\n\nAbout Our Company | Accenture\n\nQualification\n\n\n\n\nProfessional & Technical\n\n\n\n\nSkills:\n\n\n- Relevant experience in the required domain.\n\n- Strong analytical, problem-solving, and communication skills.\n\n- Ability to work in a fast-paced, dynamic environment.\nProven track record of developing AI models in areas such as channel analytics, marketing & customer experience, supply chain analytics, predictive maintenance, production optimization, and connected products.\nStrong proficiency in Python programming, with experience in NLP and Computer Vision.\nExposure to PLM/ERP/CRM systems and understanding of their integration with AI solutions.\nExperience with cloud platforms, preferably Azure/GCP, and familiarity with AWS.\nKnowledge of LLM exposure and experience with tools such as ChatGPT, Llama 2, Claude 2, Hugging Face, etc. for prompt engineering, prompt tuning, etc will be an advantage.\n\n\n\n\nAdditional Information:\n\n- Opportunity to work on innovative projects.\n\n- Career growth and leadership exposure.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'erp', 'natural language processing', 'data science', 'computer vision', 'appium', 'ai solutions', 'cucumber', 'microsoft azure', 'machine learning', 'artificial intelligence', 'eclipse', 'plm', 'deep learning', 'tensorflow', 'java', 'gcp', 'ai techniques', 'aws', 'testng', 'bdd framework', 'crm']",2025-06-13 06:00:21
S&C GN - Data&AI - CMT Eng - Consultant,Accenture,3 - 8 years,Not Disclosed,['Bengaluru'],"Job Title - S&C Global Network - AI - CMT AI ML Consultant\n\n\n\nManagement Level:9- Consultant\n\n\n\nLocation:Open\n\n\n\nMust-have skills:Gen AI ML\n\n\n\n\nGood to have skills:Ability to leverage design thinking, business process optimization, and stakeholder management skills.\n\n\n\nJob\n\n\nSummary:\n\nThis role involves driving strategic initiatives, managing business transformations, and leveraging industry expertise to create value-driven solutions.\n\n\n\n\nRoles & Responsibilities:\n\nProvide strategic advisory services, conduct market research, and develop data-driven recommendations to enhance business performance.\n\n\n\nWHATS IN IT FOR YOU\nAccenture has committed to invest USD 3Billion into GenAI in the next 3 years. We will continually invest in your learning and growth. You'll work with Accentures highly skilled and experienced practitioners, and Accenture will support you in growing your own career path and interests.\nYoull be part of a diverse, vibrant, global Accenture Data and AI community, continually pushing the boundaries of business capabilities.\n\n\n\nWhat you would do in this role\n\n\n\nML Maturity Assessment:\nConduct comprehensive assessments of the organization's ML maturity, identifying strengths, weaknesses, and areas for improvement.\nProvide strategic recommendations to enhance the overall ML capability and align it with business objectives.\n\n\n\nML Ops Roadmap & Processes:\nDevelop ML Ops roadmaps and establish robust processes for the end-to-end machine learning lifecycle, including data preparation, model training, deployment, and monitoring.\nImplement best practices in ML Ops to ensure efficiency, scalability, and reliability of ML systems.\n\n\n\nImplementation of Gen AI Solutions:\nLead the design and implementation of state-of-the-art Generative AI solutions, leveraging deep learning frameworks such as TensorFlow and PyTorch.\nDrive innovation in Gen AI, staying abreast of the latest advancements and incorporating cutting-edge technologies into solutions.\n\n\n\nIncubating and Designing:\nProactively identify opportunities for ML and / or Gen AI applications within the organization.\nWork closely with cross-functional teams to incubate and design bespoke ML solutions tailored to business requirements.\n\n\n\nTechnical Leadership:\nProvide technical leadership and mentorship to data scientists, engineers, and other team members.\nCollaborate with stakeholders to ensure alignment between technical solutions and business objectives.\n\n\n\nCollaboration and Communication:\nCollaborate with business stakeholders to understand their needs and translate them into ML and Gen AI requirements.\nEffectively communicate complex technical concepts to non-technical audiences.\n\n\n\n\nProfessional & Technical\n\n\n\n\nSkills:\n\n\n- Relevant experience in the required domain.\n\n- Strong analytical, problem-solving, and communication skills.\n\n- Ability to work in a fast-paced, dynamic environment.\nProven expertise in conducting ML maturity assessments and developing ML Ops roadmaps.\nHands-on experience in operationalizing the Machine learning system on a cloud and / or an On-Prem platform.\nExperience in implementing Generative AI solutions, including incubation, design, and deployment will be a big plus.\nProficiency in deep learning frameworks such as TensorFlow and PyTorch.\nGood knowledge of ML Ops best practices and processes.\nExcellent problem-solving skills and ability to design scalable and reliable ML architectures.\nStrong leadership and communication skills, with a track record of leading successful ML initiatives.\nExperience in Telecom or Hi Tech or Software and platform industry desirable\nTools & Techniques\nTensorFlow, PyTorch, Scikit-learn, Keras\nNumPy, Pandas\nMatplotlib, Seaborn\nTensorFlow Serving, Docker and Kubernetes\nGood software engineering practices, including code modularization, documentation, and testing.\nExperience with open API , Integration architecture , microservices\n\n\n\n\nAdditional Information:\n\n- Opportunity to work on innovative projects.\n\n- Career growth and leadership exposure.\n\n\n\n\n\nAbout Our Company | Accenture\n\nQualification\n\n\n\nExperience:4+ years\n\n\n\n\nEducational Qualification:Btech/ BE",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['deep learning frameworks', 'machine learning', 'assessment', 'gen', 'ml', 'kubernetes', 'hi', 'python', 'scikit-learn', 'ai solutions', 'numpy', 'integration architecture', 'docker', 'microservices', 'pandas', 'deep learning', 'tensorflow', 'seaborn', 'matplotlib', 'open api', 'pytorch', 'keras', 'telecom']",2025-06-13 06:00:23
AWS Data Engineer_ Capgemini_ Pan India Location,Capgemini,5 - 10 years,Not Disclosed,"['Chennai', 'Bengaluru', 'Mumbai (All Areas)']","Role & responsibilities\nAWS S3, Glue, API Gateway, Crawler, Athena, , Lambda, Dynamic DB, Redshift is an advantage\nExperience/knowledge with streaming technologies is must preferably Kafka\nShould have knowledge/experience with SQL\nGood analytical skills\nFamiliar working on Linux platforms\nHave good understanding on pros and cons and the cost impact of the AWS services being leveraged\nGood Communication skills.\n\nPrimary Skills\nAWS S3, Glue, Athena, Python or Pyspark\nShould have knowledge/experience with SQL\n\nSecondary Skills\nExcellent verbal and written communication and interpersonal skills\nAbility to work independently and within a team environment",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Lamda', 'AWS', 'Athena', 'SQL', 'Python', 'S', 'Amazon Redshift', 'Aws Glue']",2025-06-13 06:00:25
S&C GN - Data&AI - Life Sciences - Consultant,Accenture,4 - 9 years,Not Disclosed,['Bengaluru'],"Management Level:Ind&Func AI Decision Science Consultant\n\n\n\n\nJob Location:Bangalore / Gurgaon\n\n\n\nMust-have\n\n\n\n\nSkills:\nExcellent understanding of Pharma data sets commercial, clinical, Leverage ones hands on experience of working across one or more of these areas such as real-world evidence data, Statistical Models/Machine Learning including Segmentation & predictive modeling, hypothesis testing, multivariate statistical analysis, time series techniques, and optimization.\n\n\n\nGood-to-have\n\n\n\n\nSkills:\nProgramming languages such as R, Python, SQL, Spark, AWS, Azure, or Google Cloud for deploying and scaling language models, Data Visualization tools like Tableau, Power BI.\n\n\n\nExperience:Proven experience (4+ years) in working on Life Sciences/Pharma/Healthcare projects and delivering successful outcomes.\n\n\n\n\nEducational Qualification:Bachelors or Masters degree in Statistics, Data Science, Applied Mathematics, Business Analytics, Computer Science, Information Systems, or other Quantitative field.\n\n\n\nJob\n\n\nSummary\n\nThis role involves driving strategic initiatives, managing business transformations, and leveraging industry expertise to create value-driven solutions. Provide strategic advisory services, conduct market research, and develop data-driven recommendations to enhance business performance.\n\n\n\nKey Responsibilities\nAn opportunity to work on high-visibility projects with top Pharma clients around the globe.\nPersonalized training modules to develop your strategy & consulting acumen to grow your skills, industry knowledge, and capabilities.\nProvide Subject matter expertise in various sub-segments of the LS industry.\nSupport delivery of small to medium-sized teams to deliver consulting projects for global clients.\nResponsibilities may include strategy, implementation, process design, and change management for specific modules.\nWork with the team or as an Individual contributor on the project assigned which includes a variety of skills to be utilized from Data Engineering to Data Science\nDevelop assets and methodologies, point-of-view, research, or white papers for use by the team and the larger community.\nAcquire new skills that have utility across industry groups.\nSupport strategies and operating models focused on some business units and assess likely competitive responses. Also, assess implementation readiness and points of greatest impact.\n\n\n\n\n\nAdditional Information\nProficient in Excel, MS Word, PowerPoint, etc.\nAbility to solve complex business problems and deliver client delight.\nStrong writing skills to build points of view on current industry trends.\nGood communication, interpersonal, and presentation skills\n\n\nAbout Our Company | Accenture (do not remove the hyperlink)\n\nQualification\n\n\n\nExperience:Proven experience (4+ years) in working on Life Sciences/Pharma/Healthcare projects and delivering successful outcomes.\n\n\n\n\nEducational Qualification:Bachelors or Masters degree in Statistics, Data Science, Applied Mathematics, Business Analytics, Computer Science, Information Systems, or other Quantitative field.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['pharmaceutical', 'python', 'sql', 'life sciences', 'aws', 'microsoft azure', 'power bi', 'time series', 'machine learning', 'data engineering', 'artificial intelligence', 'tableau', 'r', 'data science', 'gcp', 'spark', 'predictive modeling', 'statistical modeling', 'data visualization', 'statistics']",2025-06-13 06:00:26
S&C Global Network - AI - CG&S - Consultant Data Science,Accenture,4 - 8 years,Not Disclosed,['Bengaluru'],"Job Title: Industry & Function AI Decision Science Consultant S&C Global Network\n\n\n\nManagement Level: 9 Consultant\n\n\n\nLocation: Primary - Bengaluru, Secondary - Gurugram\n\n\n\nMust-Have\n\n\n\n\nSkills:\nData Science, AI, ML, Experience with cloud platforms such as AWS, Azure, or Google Cloud, Hands-on experience in programming languages like Python, R, PySpark, and SQL\n\n\n\nGood-to-Have\n\n\n\n\nSkills:\nDeep Learning Techniques (e.g. RNN, CNN), Visualization tools like Power BI and Tableau, Exposure to tools like ChatGPT, Llama 2, Hugging Face, etc.\n\n\n\nJob\n\n\nSummary:\n\nAs an Industry & Function AI Decision Science Consultant, you will leverage your expertise in data science and Consumer Goods domain knowledge to design and deliver AI-driven solutions. Your role will include strategic analysis, project delivery, solution development, and technical execution to empower businesses with actionable insights and enable automated and augmented decision-making.\n\n\n\n\nRoles & Responsibilities:\nConduct strategic analysis of the AI, analytics, and data maturity landscape for clients in the Consumer Goods domain\nLead data science engagements, manage delivery teams, and build innovative AI capabilities\nDevelop and implement advanced analytics solutions tailored to client requirements\nUtilize languages like Python, PySpark, R, and SQL for data wrangling and machine learning model development\nLeverage cloud technologies (Azure, AWS, GCP) to integrate and implement AI solutions\nTranslate complex data into compelling narratives for effective data storytelling\nMentor junior team members and contribute to thought leadership\n\n\n\n\n\nProfessional & Technical\n\n\n\n\nSkills:\n\nProficiency in Python, R, PySpark, and SQL\nStrong knowledge of traditional statistical methods, machine learning techniques, and deep learning\nHands-on experience in Consumer Goods & Services domain\nCloud integration skills with platforms like AWS, Azure, or Google Cloud\nExperience with optimization techniques (exact and evolutionary)\nCertifications like AWS Certified Data Analytics Specialty or Google Professional Data Engineer\nFamiliarity with visualization tools like Tableau and Power BI\nExposure to large language models (e.g., ChatGPT, Llama 2)\nFamiliarity with version control systems like Git.\n\n\n\n\n\nAdditional Information:\nThe ideal candidate will have a strong educational background in data science, computer science, or a related field, along with a proven track record of delivering impactful AI-driven solutions in the Consumer Goods industry.\n\n\n\n\nAbout Our Company | Accenture\n\nQualification\n\n\n\nExperience: Minimum 4-8 years of hands-on experience in data science with a focus on the Consumer Goods industry\n\n\n\n\nEducational Qualification:Bachelors or Masters degree in Statistics, Economics, Mathematics, Computer Science, or equivalent degree with Data Science specialization (from a premier institute)",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'artificial intelligence', 'deep learning', 'data science', 'ml', 'advanced analytics', 'cnn', 'data analytics', 'pyspark', 'microsoft azure', 'power bi', 'machine learning', 'sql', 'r', 'tableau', 'git', 'rnn', 'gcp', 'machine learning algorithms', 'aws', 'consumer goods', 'statistics']",2025-06-13 06:00:28
S&C Global Network - AI - CG&S - Consultant Data Science,Accenture,4 - 8 years,Not Disclosed,['Bengaluru'],"Job Title: Industry & Function AI Decision Science Consultant S&C Global Network\n\n\n\nManagement Level: 9 Consultant\n\n\n\nLocation: Primary - Bengaluru, Secondary - Gurugram\n\n\n\nMust-Have\n\n\n\n\nSkills:\nData Science, AI, ML, Experience with cloud platforms such as AWS, Azure, or Google Cloud, Hands-on experience in programming languages like Python, R, PySpark, and SQL\n\n\n\nGood-to-Have\n\n\n\n\nSkills:\nDeep Learning Techniques (e.g. RNN, CNN), Visualization tools like Power BI and Tableau, Exposure to tools like ChatGPT, Llama 2, Hugging Face, etc.\n\n\n\nJob\n\n\nSummary:\n\nAs an Industry & Function AI Decision Science Consultant, you will leverage your expertise in data science and Consumer Goods domain knowledge to design and deliver AI-driven solutions. Your role will include strategic analysis, project delivery, solution development, and technical execution to empower businesses with actionable insights and enable automated and augmented decision-making.\n\n\n\n\nRoles & Responsibilities:\nConduct strategic analysis of the AI, analytics, and data maturity landscape for clients in the Consumer Goods domain\nLead data science engagements, manage delivery teams, and build innovative AI capabilities\nDevelop and implement advanced analytics solutions tailored to client requirements\nUtilize languages like Python, PySpark, R, and SQL for data wrangling and machine learning model development\nLeverage cloud technologies (Azure, AWS, GCP) to integrate and implement AI solutions\nTranslate complex data into compelling narratives for effective data storytelling\nMentor junior team members and contribute to thought leadership\n\n\n\n\n\nProfessional & Technical\n\n\n\n\nSkills:\n\nProficiency in Python, R, PySpark, and SQL\nStrong knowledge of traditional statistical methods, machine learning techniques, and deep learning\nHands-on experience in Consumer Goods & Services domain\nCloud integration skills with platforms like AWS, Azure, or Google Cloud\nExperience with optimization techniques (exact and evolutionary)\nCertifications like AWS Certified Data Analytics Specialty or Google Professional Data Engineer\nFamiliarity with visualization tools like Tableau and Power BI\nExposure to large language models (e.g., ChatGPT, Llama 2)\nFamiliarity with version control systems like Git.\n\n\n\n\n\nAdditional Information:\nThe ideal candidate will have a strong educational background in data science, computer science, or a related field, along with a proven track record of delivering impactful AI-driven solutions in the Consumer Goods industry.\n\n\n\n\nAbout Our Company | Accenture\n\nQualification\n\n\n\nExperience: Minimum 4-8 years of hands-on experience in data science with a focus on the Consumer Goods industry\n\n\n\n\nEducational Qualification:Bachelors or Masters degree in Statistics, Economics, Mathematics, Computer Science, or equivalent degree with Data Science specialization (from a premier institute)",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'artificial intelligence', 'deep learning', 'data science', 'ml', 'advanced analytics', 'cnn', 'data analytics', 'pyspark', 'microsoft azure', 'power bi', 'machine learning', 'sql', 'r', 'tableau', 'git', 'rnn', 'gcp', 'machine learning algorithms', 'aws', 'consumer goods', 'statistics']",2025-06-13 06:00:30
S&C Global Network - AI - CG&S - Consultant Data Science,Accenture,4 - 8 years,Not Disclosed,['Bengaluru'],"Job Title: Industry & Function AI Decision Science Consultant S&C Global Network\n\n\n\nManagement Level: 9 Consultant\n\n\n\nLocation: Primary - Bengaluru, Secondary - Gurugram\n\n\n\nMust-Have\n\n\n\n\nSkills:\nData Science, AI, ML, Experience with cloud platforms such as AWS, Azure, or Google Cloud, Hands-on experience in programming languages like Python, R, PySpark, and SQL\n\n\n\nGood-to-Have\n\n\n\n\nSkills:\nDeep Learning Techniques (e.g. RNN, CNN), Visualization tools like Power BI and Tableau, Exposure to tools like ChatGPT, Llama 2, Hugging Face, etc.\n\n\n\nJob\n\n\nSummary:\n\nAs an Industry & Function AI Decision Science Consultant, you will leverage your expertise in data science and Consumer Goods domain knowledge to design and deliver AI-driven solutions. Your role will include strategic analysis, project delivery, solution development, and technical execution to empower businesses with actionable insights and enable automated and augmented decision-making.\n\n\n\n\nRoles & Responsibilities:\nConduct strategic analysis of the AI, analytics, and data maturity landscape for clients in the Consumer Goods domain\nLead data science engagements, manage delivery teams, and build innovative AI capabilities\nDevelop and implement advanced analytics solutions tailored to client requirements\nUtilize languages like Python, PySpark, R, and SQL for data wrangling and machine learning model development\nLeverage cloud technologies (Azure, AWS, GCP) to integrate and implement AI solutions\nTranslate complex data into compelling narratives for effective data storytelling\nMentor junior team members and contribute to thought leadership\n\n\n\n\n\nProfessional & Technical\n\n\n\n\nSkills:\n\nProficiency in Python, R, PySpark, and SQL\nStrong knowledge of traditional statistical methods, machine learning techniques, and deep learning\nHands-on experience in Consumer Goods & Services domain\nCloud integration skills with platforms like AWS, Azure, or Google Cloud\nExperience with optimization techniques (exact and evolutionary)\nCertifications like AWS Certified Data Analytics Specialty or Google Professional Data Engineer\nFamiliarity with visualization tools like Tableau and Power BI\nExposure to large language models (e.g., ChatGPT, Llama 2)\nFamiliarity with version control systems like Git.\n\n\n\n\n\nAdditional Information:\nThe ideal candidate will have a strong educational background in data science, computer science, or a related field, along with a proven track record of delivering impactful AI-driven solutions in the Consumer Goods industry.\n\n\n\n\nAbout Our Company | Accenture\n\nQualification\n\n\n\nExperience: Minimum 4-8 years of hands-on experience in data science with a focus on the Consumer Goods industry\n\n\n\n\nEducational Qualification:Bachelors or Masters degree in Statistics, Economics, Mathematics, Computer Science, or equivalent degree with Data Science specialization (from a premier institute)",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'artificial intelligence', 'deep learning', 'data science', 'ml', 'advanced analytics', 'cnn', 'data analytics', 'pyspark', 'microsoft azure', 'power bi', 'machine learning', 'sql', 'r', 'tableau', 'git', 'rnn', 'gcp', 'machine learning algorithms', 'aws', 'consumer goods', 'statistics']",2025-06-13 06:00:32
IN_Manager_Azure Data Engineer_Data & Analytics_Advisory_Bangalore,PwC Service Delivery Center,4 - 8 years,Not Disclosed,['Bengaluru'],"Not Applicable\nSpecialism\nSAP\n& Summary\nAt PwC, our people in data and analytics engineering focus on leveraging advanced technologies and techniques to design and develop robust data solutions for clients. They play a crucial role in transforming raw data into actionable insights, enabling informed decisionmaking and driving business growth.\n\nIn data engineering at PwC, you will focus on designing and building data infrastructure and systems to enable efficient data processing and analysis. You will be responsible for developing and implementing data pipelines, data integration, and data transformation solutions.\nAt PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purposeled and valuesdriven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us .\n& Summary A career within Data and Analytics services will provide you with the opportunity to help organisations uncover enterprise insights and drive business results using smarter data analytics. We focus on a collection of organisational technology capabilities, including business intelligence, data management, and data assurance that help our clients drive innovation, growth, and change within their organisations in order to keep up with the changing nature of customers and technology. We make impactful decisions by mixing mind and machine to leverage data, understand and navigate risk, and help our clients gain a competitive edge.\nResponsibilities\nDesign, develop, and optimize data pipelines and ETL processes using PySpark or Scala to extract, transform, and load large volumes of structured and unstructured data from diverse sources. Implement data ingestion, processing, and storage solutions on Azure cloud platform, leveraging services such as Azure Databricks, Azure Data Lake Storage, and Azure Synapse Analytics. Develop and maintain data models, schemas, and metadata to support efficient data access, query performance, and analytics requirements. Monitor pipeline performance, troubleshoot issues, and optimize data processing workflows for scalability, reliability, and costeffectiveness. Implement data security and compliance measures to protect sensitive information and ensure regulatory compliance. Requirement Proven experience as a Data Engineer, with expertise in building and optimizing data pipelines using PySpark, Scala, and Apache Spark. Handson experience with cloud platforms, particularly Azure, and proficiency in Azure services such as Azure Databricks, Azure Data Lake Storage, Azure Synapse Analytics, and Azure SQL Database. Strong programming skills in Python and Scala, with experience in software development, version control, and CI/CD practices. Familiarity with data warehousing concepts, dimensional modeling, and relational databases (e.g., SQL Server, PostgreSQL, MySQL).\nExperience with big data technologies and frameworks (e.g., Hadoop, Hive, HBase) is a plus.\nMandatory skill sets\nSpark, Pyspark, Azure\nPreferred skill sets\nSpark, Pyspark, Azure\nYears of experience required\n4 8\nEducation qualification\nB.Tech / M.Tech / MBA / MCA\nEducation\nDegrees/Field of Study required Bachelor of Engineering, Master of Business Administration, Master of Engineering\nDegrees/Field of Study preferred\nRequired Skills\nPython (Programming Language)\nAccepting Feedback, Accepting Feedback, Active Listening, Agile Scalability, Amazon Web Services (AWS), Analytical Thinking, Apache Airflow, Apache Hadoop, Azure Data Factory, Coaching and Feedback, Communication, Creativity, Data Anonymization, Data Architecture, Database Administration, Database Management System (DBMS), Database Optimization, Database Security Best Practices, Databricks Unified Data Analytics Platform, Data Engineering, Data Engineering Platforms, Data Infrastructure, Data Integration, Data Lake, Data Modeling {+ 32 more}\nNo",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['SAP', 'Data management', 'Data modeling', 'Postgresql', 'MySQL', 'Database administration', 'Agile', 'Apache', 'Business intelligence', 'Python']",2025-06-13 06:00:34
Senior / Lead Data Engineer,Atlas,1 - 2 years,Not Disclosed,['Pune'],"1. Designing and developing data pipelines: Lead data engineers are responsible for designing and developing data pipelines that move data from various sources to storage and processing systems.\n2. Building and maintaining data infrastructure: Lead data engineers are responsible for building and maintaining data infrastructure, such as data warehouses, data lakes, and data marts.\n3. Ensuring data quality and integrity: Lead data engineers are responsible for ensuring data quality and integrity, by setting up data validation processes and implementing data quality checks.\n4. Managing data storage and retrieval: Lead data engineers are responsible for managing data storage and retrieval, by designing and implementing data storage systems, such as NoSQL databases or Hadoop clusters.\n5. Developing and maintaining data models: Lead data engineers are responsible for developing and maintaining data models, such as data dictionaries and entity-relationship diagrams, to ensure consistency in data architecture.\n6. Managing data security and privacy: Lead data engineers are responsible for managing data security and privacy, by implementing security measures, such as access controls and encryption, to protect sensitive data.\n7. Leading and managing a team: Lead data engineers may be responsible for leading and managing a team of data engineers, providing guidance and support for their work.",Industry Type: Industrial Equipment / Machinery,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent",['Senior Lead'],2025-06-13 06:00:36
Azure Senior Data Engineers,IT Services & Consulting,5 - 9 years,14-24 Lacs P.A.,['Bengaluru'],"Job Title: Senior Data Engineer Azure\nLocation: Bengaluru\nExperience: 6+ years (3+ years on Azure data services preferred)\nDepartment: Data Engineering / IT\nJob Summary:\nWe are seeking a highly skilled Senior Data Engineer with expertise in Microsoft Azure to design, develop, and optimize data pipelines, data lakes, and warehouse solutions. The ideal candidate will play a key role in building scalable and secure data platforms to support business intelligence, analytics, and machine learning use cases.\nKey Responsibilities:\nDesign, build, and maintain scalable data pipelines using Azure Data Factory, Databricks, Synapse Analytics, and related tools.\nDevelop and optimize ETL/ELT processes for structured and unstructured data.\nImplement data lake and data warehouse solutions following best practices and security standards.\nCollaborate with data scientists, analysts, and business stakeholders to understand data requirements.\nEnsure data quality, lineage, and governance using tools like Purview, Azure Monitor, and Data Catalog.\nMonitor and troubleshoot performance issues across data flows and batch processing pipelines.\nSupport real-time data integration and streaming solutions using Azure Event Hubs, Stream Analytics, or Kafka.\nMaintain and enhance CI/CD pipelines for data solutions using Azure DevOps or GitHub Actions.\nLead and mentor junior engineers in best practices for Azure data engineering.\nRequired Skills & Qualifications:\nBachelor’s or Master’s degree in Computer Science, Engineering, or related field.\n6+ years of experience in data engineering roles, with 3+ years working with Azure data services.\nProficiency in SQL, Python or Scala.\nExperience with tools like Azure Data Factory, Azure Synapse Analytics, Azure Databricks, Azure SQL Database, and Azure Blob Storage.\nStrong understanding of data modeling, data warehousing, and data lake architecture.\nFamiliarity with DevOps practices, infrastructure-as-code (IaC) tools (ARM, Bicep, Terraform), and CI/CD pipelines.\nKnowledge of data governance and data security best practices on cloud platforms.\nExcellent communication and documentation skills.\nPreferred Qualifications:\nAzure certification (e.g., Azure Data Engineer Associate, Azure Solutions Architect, etc.)\nExperience with big data frameworks (e.g., Spark, Hadoop).\nKnowledge of machine learning pipelines or MLOps in Azure.\nExperience with Power BI or integration with other visualization tools.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Azure Senior Data Engineers', 'Azure Data Factory', 'azure']",2025-06-13 06:00:37
Senior Data Engineer -Bangalore,Happiest Minds Technologies,6 - 10 years,Not Disclosed,['Bengaluru'],"Job Overview:\nThe primary purpose of this role is to translate business requirements and functional specifications into logical program designs and to deliver dashboards, schema, data pipelines, and software solutions. This includes developing, configuring, or modifying data components within various complex business and/or enterprise application solutions in various computing environments. You will partner closely with multiple Business partners, Product Owners, Data Strategy, Data Platform, Data Science and Machine Learning (MLOps) teams to drive innovative data products for end users. Additionally, you will help shape overall solution & data products, develop scalable solutions through best-in-class engineering practices.",,,,"['NoSQL', 'big data systems', 'Data Pipeline', 'MongoDB', 'SQL', 'Hive', 'GIT', 'Hadoop', 'Kafka', 'Agile', 'MQL', 'Ci/Cd']",2025-06-13 06:00:39
Senior Data Engineer,Suzva Software Technologies,6 - 8 years,Not Disclosed,"['Mumbai', 'Delhi / NCR', 'Bengaluru']","JobOpening Senior Data Engineer (Remote, Contract 6 Months)\nRemote | Contract Duration: 6 Months | Experience: 6-8 Years\n\nWe are hiring a Senior Data Engineer for a 6-month remote contract position. The ideal candidate is highly skilled in building scalable data pipelines and working within the Azure cloud ecosystem, especially Databricks, ADF, and PySpark. You'll work closely with cross-functional teams to deliver enterprise-level data engineering solutions.\n\n#KeyResponsibilities\nBuild scalable ETL pipelines and implement robust data solutions in Azure.\n\nManage and orchestrate workflows using ADF, Databricks, ADLS Gen2, and Key Vaults.\n\nDesign and maintain secure and efficient data lake architecture.\n\nWork with stakeholders to gather data requirements and translate them into technical specs.\n\nImplement CI/CD pipelines for seamless data deployment using Azure DevOps.\n\nMonitor data quality, performance bottlenecks, and scalability issues.\n\nWrite clean, organized, reusable PySpark code in an Agile environment.\n\nDocument pipelines, architectures, and best practices for reuse.\n\n#MustHaveSkills\nExperience: 6+ years in Data Engineering\n\nTech Stack: SQL, Python, PySpark, Spark, Azure Databricks, ADF, ADLS Gen2, Azure DevOps, Key Vaults\n\nCore Expertise: Data Warehousing, ETL, Data Pipelines, Data Modelling, Data Governance\n\nAgile, SDLC, Containerization (Docker), Clean coding practices\n\n#GoodToHaveSkills\nEvent Hubs, Logic Apps\n\nPower BI\n\nStrong logic building and competitive programming background\n\nMode: Remote\n\nDuration: 6 Months\nLocations : Mumbai, Delhi / NCR, Bengaluru , Kolkata, Chennai, Hyderabad, Ahmedabad, Pune, Remote",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Databricks', 'Data Modelling', 'Data Pipelines', 'ADF', 'PySpark', 'Data Warehousing', 'ETL', 'Data Governance']",2025-06-13 06:00:40
Senior Data Engineer- (Big data and Data Pipelines),Findem,5 - 9 years,Not Disclosed,"['New Delhi', 'Bengaluru']","What is Findem:\n\nFindem is the only talent data platform that combines 3D data with AI. It automates and consolidates top-of-funnel activities across your entire talent ecosystem, bringing together sourcing, CRM, and analytics into one place. Only 3D data connects people and company data over time - making an individual s entire career instantly accessible in a single click, removing the guesswork, and unlocking insights about the market and your competition no one else can. Powered by 3D data, Findem s automated workflows across the talent lifecycle are the ultimate competitive advantage. Enabling talent teams to deliver continuous pipelines of top, diverse candidates while creating better talent experiences, Findem transforms the way companies plan, hire, and manage talent. Learn more at www.findem.ai\n\nExperience - 5 - 9 years\n\nWe are looking for an experienced Big Data Engineer, who will be responsible for building, deploying and managing various data pipelines, data lake and Big data processing solutions using Big data and ETL technologies.\n\nLocation- Delhi, India\nHybrid- 3 days onsite\nResponsibilities\nBuild data pipelines, Big data processing solutions and data lake infrastructure using various Big data and ETL technologies\nAssemble and process large, complex data sets that meet functional non-functional business requirements ETL from a wide variety of sources like MongoDB, S3, Server-to-Server, Kafka etc., and processing using SQL and big data technologies\nBuild analytical tools to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics Build interactive and ad-hoc query self-serve tools for analytics use cases\nBuild data models and data schema for performance, scalability and functional requirement perspective Build processes supporting data transformation, metadata, dependency and workflow management\nResearch, experiment and prototype new tools/technologies and make them successful\nSkill Requirements\nMust have-Strong in Python/Scala\nMust have experience in Big data technologies like Spark, Hadoop, Athena / Presto, Redshift, Kafka etc\nExperience in various file formats like parquet, JSON, Avro, orc etc\nExperience in workflow management tools like airflow Experience with batch processing, streaming and message queues\nAny of visualization tools like Redash, Tableau, Kibana etc\nExperience in working with structured and unstructured data sets\nStrong problem solving skills\nGood to have\nExposure to NoSQL like MongoDB\nExposure to Cloud platforms like AWS, GCP, etc\nExposure to Microservices architecture\nExposure to Machine learning techniques\nThe role is full-time and comes with full benefits. We are globally headquartered in the San Francisco Bay Area with our India headquarters in Bengaluru.\n\nEqual Opportunity",Industry Type: Management Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SAN', 'metadata', 'Prototype', 'Machine learning', 'Schema', 'JSON', 'Analytics', 'SQL', 'CRM', 'Python']",2025-06-13 06:00:42
Senior Data Engineer,PulseData labs Pvt Ltd,7 - 10 years,Not Disclosed,['Bengaluru'],"Company name: PulseData labs Pvt Ltd (captive Unit for URUS, USA)\n\nAbout URUS\nWe are the URUS family (US), a global leader in products and services for Agritech.\n\nSENIOR DATA ENGINEER\nThis role is responsible for the design, development, and maintenance of data integration and reporting solutions. The ideal candidate will possess expertise in Databricks and strong skills in SQL Server, SSIS and SSRS, and experience with other modern data engineering tools such as Azure Data Factory. This position requires a proactive and results-oriented individual with a passion for data and a strong understanding of data warehousing principles.\n\nResponsibilities\nData Integration\nDesign, develop, and maintain robust and efficient ETL pipelines and processes on Databricks.\nTroubleshoot and resolve Databricks pipeline errors and performance issues.\nMaintain legacy SSIS packages for ETL processes.\nTroubleshoot and resolve SSIS package errors and performance issues.\nOptimize data flow performance and minimize data latency.\nImplement data quality checks and validations within ETL processes.\nDatabricks Development\nDevelop and maintain Databricks pipelines and datasets using Python, Spark and SQL.\nMigrate legacy SSIS packages to Databricks pipelines.\nOptimize Databricks jobs for performance and cost-effectiveness.\nIntegrate Databricks with other data sources and systems.\nParticipate in the design and implementation of data lake architectures.\nData Warehousing\nParticipate in the design and implementation of data warehousing solutions.\nSupport data quality initiatives and implement data cleansing procedures.\nReporting and Analytics\nCollaborate with business users to understand data requirements for department driven reporting needs.\nMaintain existing library of complex SSRS reports, dashboards, and visualizations.\nTroubleshoot and resolve SSRS report issues, including performance bottlenecks and data inconsistencies.\nCollaboration and Communication\nComfortable in entrepreneurial, self-starting, and fast-paced environment, working both independently and with our highly skilled teams.\nCollaborate effectively with business users, data analysts, and other IT teams.\nCommunicate technical information clearly and concisely, both verbally and in writing.\nDocument all development work and procedures thoroughly.\nContinuous Growth\nKeep abreast of the latest advancements in data integration, reporting, and data engineering technologies.\nContinuously improve skills and knowledge through training and self-learning.\nThis job description reflects managements assignment of essential functions; it does not prescribe or restrict the tasks that may be assigned.\n\nRequirements\nBachelor's degree in computer science, Information Systems, or a related field.\n7+ years of experience in data integration and reporting.\nExtensive experience with Databricks, including Python, Spark, and Delta Lake.\nStrong proficiency in SQL Server, including T-SQL, stored procedures, and functions.\nExperience with SSIS (SQL Server Integration Services) development and maintenance.\nExperience with SSRS (SQL Server Reporting Services) report design and development.\nExperience with data warehousing concepts and best practices.\nExperience with Microsoft Azure cloud platform and Microsoft Fabric desirable.\nStrong analytical and problem-solving skills.\nExcellent communication and interpersonal skills.\nAbility to work independently and as part of a team.\nExperience with Agile methodologies.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Etl Development', 'Azure Databricks', 'Spark', 'SQL Server', 'Databricks Engineer', 'Data Warehousing', 'Pythonspark']",2025-06-13 06:00:44
Data Engineer-Data Integration,IBM,2 - 5 years,Not Disclosed,['Pune'],"As Data Engineer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\n\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise seach applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modeling results\n\n\n Your primary responsibilities include: \nDevelop & maintain data pipelines for batch & stream processing using informatica power centre or cloud ETL/ELT tools.\nLiaise with business team and technical leads, gather requirements, identify data sources, identify data quality issues, design target data structures, develop pipelines and data processing routines, perform unit testing and support UAT.\nWork with data scientist and business analytics team to assist in data ingestion and data-related technical issues.\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nExpertise in Data warehousing/ information Management/ Data Integration/Business Intelligence using ETL tool Informatica PowerCenter\nKnowledge of Cloud, Power BI, Data migration on cloud skills.\nExperience in Unix shell scripting and python\nExperience with relational SQL, Big Data etc\n\n\nPreferred technical and professional experience\nKnowledge of MS-Azure Cloud\nExperience in Informatica PowerCenter\nExperience in Unix shell scripting and python",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['information management', 'data warehousing', 'business intelligence', 'etl', 'data integration', 'python', 'informatica powercenter', 'power bi', 'relational sql', 'data migration', 'azure cloud', 'sql', 'elastic search', 'unix shell scripting', 'splunk', 'agile', 'big data', 'informatica']",2025-06-13 06:00:46
Data Engineer-Data Platforms,IBM,5 - 10 years,Not Disclosed,['Navi Mumbai'],"As a Big Data Engineer, you will develop, maintain, evaluate, and test big data solutions. You will be involved in data engineering activities like creating pipelines/workflows for Source to Target and implementing solutions that tackle the clients needs.\n\nYour primary responsibilities include:\nDesign, build, optimize and support new and existing data models and ETL processes based on our clients business requirements.\nBuild, deploy and manage data infrastructure that can adequately handle the needs of a rapidly growing data driven organization.\nCoordinate data access and security to enable data scientists and analysts to easily access to data whenever they need too\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nMust have 5+ years exp in Big Data -Hadoop Spark -Scala ,Python\nHbase, Hive Good to have Aws -S3,\nathena ,Dynomo DB, Lambda, Jenkins GIT\nDeveloped Python and pyspark programs for data analysis.\nGood working experience with python to develop Custom Framework for generating of rules (just like rules engine).\nDeveloped Python code to gather the data from HBase and designs the solution to implement using Pyspark. Apache Spark DataFrames/RDD's were used to apply business transformations and utilized Hive Context objects to perform read/write operations\n\n\nPreferred technical and professional experience\nUnderstanding of Devops.\nExperience in building scalable end-to-end data ingestion and processing solutions\nExperience with object-oriented and/or functional programming languages, such as Python, Java and Scala",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['scala', 'hadoop spark', 'spark', 'big data', 'python', 'hive', 'cloudera', 'pyspark', 'sql', 'java', 'git', 'postgresql', 'devops', 'jenkins', 'data ingestion', 'mysql', 'hadoop', 'etl', 'hbase', 'data analysis', 'dynamo db', 'oozie', 'microsoft azure', 'impala', 'data engineering', 'lambda expressions', 'kafka', 'sqoop', 'aws']",2025-06-13 06:00:47
Data Engineer Specialist,Accenture,3 - 4 years,Not Disclosed,['Kochi'],"Job Title - + +\n\n\n\nManagement Level :\n\n\n\nLocation:Kochi, Coimbatore, Trivandrum\n\n\n\nMust have skills:Python, Pyspark\n\n\n\n\nGood to have skills:Redshift\n\n\n\nJob\n\n\nSummary: We are seeking a highly skilled and experienced Senior Data Engineer to join our growing Data and Analytics team. The ideal candidate will have deep expertise in Databricks and cloud data warehousing, with a proven track record of designing and building scalable data pipelines, optimizing data architectures, and enabling robust analytics capabilities. This role involves working collaboratively with cross-functional teams to ensure the organization leverages data as a strategic asset. Your responsibilities will include:\n\n\n\n\nRoles & Responsibilities\nDesign, build, and maintain scalable data pipelines and ETL processes using Databricks and other modern tools.\nArchitect, implement, and manage cloud-based data warehousing solutions on Databricks (Lakehouse Architecture)\nDevelop and maintain optimized data lake architectures to support advanced analytics and machine learning use cases.\nCollaborate with stakeholders to gather requirements, design solutions, and ensure high-quality data delivery.\nOptimize data pipelines for performance and cost efficiency.\nImplement and enforce best practices for data governance, access control, security, and compliance in the cloud.\nMonitor and troubleshoot data pipelines to ensure reliability and accuracy.\nLead and mentor junior engineers, fostering a culture of continuous learning and innovation.\nExcellent communication skills\nAbility to work independently and along with client based out of western Europe\n\n\n\n\nProfessional & Technical\n\n\n\n\nSkills:\nDesigning, developing, optimizing, and maintaining data pipelines that adhere to ETL principles and business goals\nSolving complex data problems to deliver insights that helps our business to achieve their goals.\nSource data (structured unstructured) from various touchpoints, format and organize them into an analyzable format.\nCreating data products for analytics team members to improve productivity\nCalling of AI services like vision, translation etc. to generate an outcome that can be used in further steps along the pipeline.\nFostering a culture of sharing, re-use, design and operational efficiency of data and analytical solutions\nPreparing data to create a unified database and build tracking solutions ensuring data quality\nCreate Production grade analytical assets deployed using the guiding principles of CI/CD.\n\n\n\nProfessional and Technical Skills\nExpert in Python, Scala, Pyspark, Pytorch, Javascript (any 2 at least)\nExtensive experience in data analysis (Big data- Apache Spark environments), data libraries (e.g. Pandas, SciPy, Tensorflow, Keras etc.), and SQL. 3-4 years of hands-on experience working on these technologies.\nExperience in one of the many BI tools such as Tableau, Power BI, Looker.\nGood working knowledge of key concepts in data analytics, such as dimensional modeling, ETL, reporting/dashboarding, data governance, dealing with structured and unstructured data, and corresponding infrastructure needs.\nWorked extensively in Microsoft Azure (ADF, Function Apps, ADLS, Azure SQL), AWS (Lambda,Glue,S3), Databricks analytical platforms/tools, Snowflake Cloud Datawarehouse.\n\n\n\n\nAdditional Information\nExperience working in cloud Data warehouses like Redshift or Synapse\nCertification in any one of the following or equivalent\nAWS- AWS certified data Analytics- Speciality\nAzure- Microsoft certified Azure Data Scientist Associate\nSnowflake- Snowpro core- Data Engineer\nDatabricks Data Engineering\n\nQualification\n\n\n\nExperience:5-8 years of experience is required\n\n\n\n\nEducational Qualification:Graduation (Accurate educational details should capture)",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['scala', 'pyspark', 'pytorch', 'python', 'data bricks', 'glue', 'amazon redshift', 'data warehousing', 'sql', 'tensorflow', 'sql azure', 'spark', 'keras', 'big data', 'etl', 'snowflake', 'scipy', 'data analysis', 'azure data lake', 'microsoft azure', 'power bi', 'javascript', 'pandas', 'tableau', 'lambda expressions', 'aws']",2025-06-13 06:00:49
Azure Cloud Data Engineering Consultant,Optum,7 - 10 years,17-27.5 Lacs P.A.,['Gurugram'],"Primary Responsibilities:\nDesign and develop applications and services running on Azure, with a strong emphasis on Azure Databricks, ensuring optimal performance, scalability, and security.\nBuild and maintain data pipelines using Azure Databricks and other Azure data integration tools.\nWrite, read, and debug Spark, Scala, and Python code to process and analyze large datasets.\nWrite extensive query in SQL and Snowflake\nImplement security and access control measures and regularly audit Azure platform and infrastructure to ensure compliance.\nCreate, understand, and validate design and estimated effort for given module/task, and be able to justify it.\nPossess solid troubleshooting skills and perform troubleshooting of issues in different technologies and environments.\nImplement and adhere to best engineering practices like design, unit testing, functional testing automation, continuous integration, and delivery.\nMaintain code quality by writing clean, maintainable, and testable code.\nMonitor performance and optimize resources to ensure cost-effectiveness and high availability.\nDefine and document best practices and strategies regarding application deployment and infrastructure maintenance.\nProvide technical support and consultation for infrastructure questions.\nHelp develop, manage, and monitor continuous integration and delivery systems.\nTake accountability and ownership of features and teamwork.\nComply with the terms and conditions of the employment contract, company policies and procedures, and any directives.\nRequired Qualifications:\nB.Tech/MCA (Minimum 16 years of formal education)\nOverall 7+ years of experience.\nMinimum of 3 years of experience in Azure (ADF), Databricks and DevOps.\n5 years of experience in writing advanced level SQL.\n2-3 years of experience in writing, reading, and debugging Spark, Scala, and Python code.\n3 or more years of experience in architecting, designing, developing, and implementing cloud solutions on Azure.\nProficiency in programming languages and scripting tools.\nUnderstanding of cloud data storage and database technologies such as SQL and NoSQL.\nProven ability to collaborate with multidisciplinary teams of business analysts, developers, data scientists, and subject-matter experts.\nFamiliarity with DevOps practices and tools, such as continuous integration and continuous deployment (CI/CD) and Teraform.\nProven proactive approach to spotting problems, areas for improvement, and performance bottlenecks.\nProven excellent communication, writing, and presentation skills.\nExperience in interacting with international customers to gather requirements and convert them into solutions using relevant skills.\nPreferred Qualifications:\nKnowledge of AI/ML or LLM (GenAI).\nKnowledge of US Healthcare domain and experience with healthcare data.\nExperience and skills with Snowflake.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Data Factory', 'Azure Databricks', 'ETL', 'SQL', 'Python', 'Airflow', 'Pyspark', 'Snowflake', 'SCALA', 'Spark', 'Data Bricks']",2025-06-13 06:00:50
Data Engineer || Paisabazaar || Gurgaon,Paisabazaar,3 - 5 years,Not Disclosed,['Gurugram'],"Qualifications for Data Engineer :\n3+ Years of experience in building and optimizing big data solutions required to fulfill business and technology requirements.\n4+ years of technical expertise in areas of design and implementation using big data technology Hadoop, Hive, Spark, Python/Java.\nStrong analytic skills to understand and create solutions for business use cases.\nEnsure best practices to implement data governance principles, data quality checks on each data layer.",,,,"['Pyspark', 'Data Engineering', 'Hadoop', 'Hive', 'Java', 'Scala Programming', 'Big Data', 'SQL', 'Azure Cloud', 'GCP', 'Spark', 'AWS', 'Python']",2025-06-13 06:00:52
Data Science & AI Engineer,Blue Altair,5 - 8 years,Not Disclosed,['Pune'],"Greetings from Blue Altair!\nJob Overview:\nWe are seeking an experienced and highly skilled Data Science and AI Engineer to join our dynamic team. The ideal candidate will have 5+ years of experience working on cutting-edge data science and AI technologies across various cloud platforms with a strong focus to work on LLMs and SLMs. The role demands a professional capable of performing in a client-facing environment, as well as mentoring and guiding junior team members.\n\nTitle: Consultant/Sr. Consultant - Data Science Engineer\nExperience: 5-8 years\nLocation: Pune/Bangalore (Hybrid)\n\nRoles and responsibilities:\nDevelop, implement, and optimize machine learning models and AI algorithms to solve complex business problems.\nDesign, build, and fine-tune AI models, particularly focusing on LLMs and SLMs, using state-of-the-art techniques and architectures.\nApply advanced techniques in prompt engineering, model fine-tuning, and optimization to tailor models for specific business needs.\nDeploy and manage machine learning models and pipelines on cloud platforms (AWS, GCP, Azure, etc.).\nWork closely with clients to understand their data and AI needs and provide tailored solutions.\nCollaborate with cross-functional teams to integrate AI solutions into broader software architectures.\nMentor junior team members and provide guidance in implementing best practices in data science and AI development.\nStay up-to-date with the latest trends and advancements in data science, AI, and cloud technologies.\nPrepare technical documentation and present insights to both technical and non-technical stakeholders.\n\nRequirement:\n5+ years of experience in data science, machine learning, and AI technologies.\nProven experience working with cloud platforms such as Google Cloud, Microsoft Azure, or AWS.\nExpertise in programming languages such as Python, R, Julia, and AI frameworks like TensorFlow, PyTorch, Scikit-learn, Hugging face Transformers.\nKnowledge of data visualization tools (e.g., Matplotlib, Seaborn, Tableau)\nSolid understanding of data engineering concepts including ETL, data pipelines, and databases (SQL, NoSQL).\nExperience with MLOps practices and deployment of models in production environments.\nFamiliarity with NLP (Natural Language Processing) tasks and working with large-scale datasets.\nHands-on experience with generative AI models like GPT, Gemini, Claude, Mistral etc.\nClient-facing experience with strong communication skills to manage and engage stakeholders.\nStrong problem-solving skills and analytical mindset.\nAbility to work independently and as part of a team and mentor and provide technical leadership to junior team members.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['LLMs', 'Artificial Intelligence', 'MLOps', 'RAG', 'Natural Language Processing', 'Neural Networks', 'LLM', 'Machine Learning', 'AI Models', 'Data Science', 'PyTorch', 'SLM', 'AI Automation']",2025-06-13 06:00:54
SNOWFLAKE DATA ENGINEER,Capgemini,6 - 11 years,Not Disclosed,['Chennai'],"Your Role \n\nWorks in the area of Software Engineering, which encompasses the development, maintenance and optimization of software solutions/applications.\n1. Applies scientific methods to analyse and solve software engineering problems.\n2. He/she is responsible for the development and application of software engineering practice and knowledge, in research, design, development and maintenance.\n3. His/her work requires the exercise of original thought and judgement and the ability to supervise the technical and administrative work of other software engineers.\n4. The software engineer builds skills and expertise of his/her software engineering discipline to reach standard software engineer skills expectations for the applicable role, as defined in Professional Communities.\n5. The software engineer collaborates and acts as team player with other software engineers and stakeholders.\nWorks in the area of Software Engineering, which encompasses the development, maintenance and optimization of software solutions/applications.1. Applies scientific methods to analyse and solve software engineering problems.2. He/she is responsible for the development and application of software engineering practice and knowledge, in research, design, development and maintenance.3. His/her work requires the exercise of original thought and judgement and the ability to supervise the technical and administrative work of other software engineers.4. The software engineer builds skills and expertise of his/her software engineering discipline to reach standard software engineer skills expectations for the applicable role, as defined in Professional Communities.5. The software engineer collaborates and acts as team player with other software engineers and stakeholders.\n\n Your Profile \n4+ years of experience in data architecture, data warehousing, and cloud data solutions.\nMinimum 3+ years of hands-on experience with End to end Snowflake implementation.\nExperience in developing data architecture and roadmap strategies with knowledge to establish data governance and quality frameworks within Snowflake\nExpertise or strong knowledge in Snowflake best practices, performance tuning, and query optimisation.\nExperience with cloud platforms like AWS or Azure and familiarity with Snowflakes integration with these environments.\nStrong knowledge in at least one cloud(AWS or Azure) is mandatory\nSolid understanding of SQL, Python, and scripting for data processing and analytics.\nExperience in leading teams and managing complex data migration projects.\nStrong communication skills, with the ability to explain technical concepts to non-technical stakeholders.\n\nKnowledge on new Snowflake features,AI capabilities and industry trends to drive innovation and continuous improvement.\n\n  \n\n Skills (competencies) \n\nVerbal Communication",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['snowflake', 'python', 'microsoft azure', 'sql', 'aws', 'head hunting', 'screening', 'performance tuning', 'data processing', 'data warehousing', 'hrsd', 'data architecture', 'data migration', 'sourcing', 'talent acquisition', 'it recruitment', 'technical recruitment', 'recruitment', 'data governance']",2025-06-13 06:00:56
Data Engineer-Data Platforms,IBM,2 - 5 years,Not Disclosed,['Mumbai'],"As a Data Engineer at IBM, you'll play a vital role in the development, design of application, provide regular support/guidance to project teams on complex coding, issue resolution and execution.\nYour primary responsibilities include\nLead the design and construction of new solutions using the latest technologies, always looking to add business value and meet user requirements.\nStrive for continuous improvements by testing the build solution and working under an agile framework.\nDiscover and implement the latest technologies trends to maximize and build creative solutions\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nExperience with Apache Spark (PySpark)In-depth knowledge of Spark’s architecture, core APIs, and PySpark for distributed data processing.\nBig Data TechnologiesFamiliarity with Hadoop, HDFS, Kafka, and other big data tools. Data Engineering\n\nSkills:\nStrong understanding of ETL pipelines, data modeling, and data warehousing concepts.\nStrong proficiency in PythonExpertise in Python programming with a focus on data processing and manipulation. Data Processing FrameworksKnowledge of data processing libraries such as Pandas, NumPy.\nSQL ProficiencyExperience writing optimized SQL queries for large-scale data analysis and transformation.\nCloud PlatformsExperience working with cloud platforms like AWS, Azure, or GCP, including using cloud storage systems\n\n\nPreferred technical and professional experience\nDefine, drive, and implement an architecture strategy and standards for end-to-end monitoring.\nPartner with the rest of the technology teams including application development, enterprise architecture, testing services, network engineering,\nGood to have detection and prevention tools for Company products and Platform and customer-facing",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'data processing', 'pyspark', 'data modeling', 'spark', 'data analysis', 'microsoft azure', 'data warehousing', 'cloud platforms', 'numpy', 'data engineering', 'sql', 'pandas', 'spring boot', 'etl pipelines', 'java', 'gcp', 'kafka', 'data warehousing concepts', 'hadoop', 'big data', 'aws', 'etl']",2025-06-13 06:00:57
Data Engineer-Data Platforms-Google,IBM,2 - 5 years,Not Disclosed,['Hyderabad'],"As an Associate Software Developer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\n\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\n\nIn this role, your responsibilities may include:\nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise seach applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modeling results\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nDevelop/Convert the database (Hadoop to GCP) of the specific objects (tables, views, procedures, functions, triggers, etc.) from one database to another database platform Implementation of a specific Data Replication mechanism (CDC, file data transfer, bulk data transfer, etc.).\nExpose data as API Participation in modernization roadmap journey Analyze discovery and analysis outcomes Lead discovery and analysis workshops/playbacks Identification of the applications dependencies, source, and target database incompatibilities.\nAnalyze the non-functional requirements (security, HA, RTO/RPO, storage, compute, network, performance bench, etc.).\nPrepare the effort estimates, WBS, staffing plan, RACI, RAID etc. .\nLeads the team to adopt right tools for various migration and modernization method\n\n\nPreferred technical and professional experience\nYou thrive on teamwork and have excellent verbal and written communication skills.\nAbility to communicate with internal and external clients to understand and define business needs, providing analytical solutions\nAbility to communicate results to technical and non-technical audiences",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['elastic search', 'gcp', 'splunk', 'hadoop', 'big data', 'hive', 'python', 'data management', 'presentation skills', 'microsoft azure', 'machine learning', 'javascript', 'sql', 'docker', 'java', 'git', 'spark', 'linux', 'jenkins', 'html', 'mysql', 'aws']",2025-06-13 06:00:59
Data Engineer-Business Intelligence,IBM,3 - 8 years,Not Disclosed,['Pune'],"Provide expertise in analysis, requirements gathering, design, coordination, customization, testing and support of reports, in client’s environment\nDevelop and maintain a strong working relationship with business and technical members of the team\nRelentless focus on quality and continuous improvement\nPerform root cause analysis of reports issues\nDevelopment / evolutionary maintenance of the environment, performance, capability and availability.\nAssisting in defining technical requirements and developing solutions\nEffective content and source-code management, troubleshooting and debugging.\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nCognos Developer & Admin Required. EducationThe resource should be full time MCA/M. Tech/B. Tech/B.E. and should preferably have relevant certifications\nExperienceThe resource should have a minimum of 3 years of experience of working in the in BI DW projects in areas pertaining to reporting and visualization using cognos.\nThe resources shall have worked in at least two projects where they were involved in developing reporting/ visualization\nHe shall have good understanding of UNIX.\nShould be well conversant in English and should have excellent writing, MIS, communication, time management and multi-tasking skill\n\n\nPreferred technical and professional experience\nExperience with various cloud and integration platforms (e.g. AWS, Google, Azure)\nAgile mindset – ability to process changes of priorities and requests, ownership, critical thinking\nExperience with an ETL/Data Integration tool (eg. IBM InfoSphere DataStage, Azure Data Factory, Informatica PowerCenter)",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['informatica powercenter', 'microsoft azure', 'mis', 'bi dw', 'etl', 'datastage', 'bi', 'azure data factory', 'cognos', 'root cause analysis', 'business intelligence', 'sql', 'dw', 'ibm datastage', 'troubleshooting', 'debugging', 'agile', 'data visualization', 'aws', 'data integration', 'informatica', 'unix']",2025-06-13 06:01:01
Data Engineer-Business Intelligence,IBM,5 - 10 years,Not Disclosed,['Hyderabad'],"Provide expertise in analysis, requirements gathering, design, coordination, customization, testing and support of reports, in client’s environment\nDevelop and maintain a strong working relationship with business and technical members of the team\nRelentless focus on quality and continuous improvement\nPerform root cause analysis of reports issues\nDevelopment / evolutionary maintenance of the environment, performance, capability and availability.\nAssisting in defining technical requirements and developing solutions\nEffective content and source-code management, troubleshooting and debugging\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\n5+ years of experience with BI tools, with expertise and/or certification in at least one major BI platform – Tableau preferred.\nAdvanced knowledge of SQL, including the ability to write complex stored procedures, views, and functions.\nProven capability in data storytelling and visualization, delivering actionable insights through compelling presentations.\nExcellent communication skills, with the ability to convey complex analytical findings to non-technical stakeholders in a clear, concise, and meaningful way.\n5.Identifying and analyzing industry trends, geographic variations, competitor strategies, and emerging customer behavior\n\n\nPreferred technical and professional experience\nTroubleshooting capabilities to debug Data controls Capable of converting business requirements into workable model.\nGood communication skills, willingness to learn new technologies, Team Player, Self-Motivated, Positive Attitude.\nMust have thorough understanding of SQL & advance SQL (Joining & Relationships)",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['advance sql', 'sql', 'bi tools', 'debugging', 'troubleshooting', 'python', 'data analysis', 'data analytics', 'bi', 'data warehousing', 'power bi', 'business analysis', 'machine learning', 'business intelligence', 'sql server', 'qlikview', 'tableau', 'r', 'data visualization', 'etl', 'ssis']",2025-06-13 06:01:03
Cloud Data Platform Engineer - C,Capgemini,3 - 7 years,Not Disclosed,['Mumbai'],"\nA Data Platform Engineer specialises in the design, build, and maintenance of cloud-based data infrastructure and platforms for data-intensive applications and services. They develop Infrastructure as Code and manage the foundational systems and tools for efficient data storage, processing, and management. This role involves architecting robust and scalable cloud data infrastructure, including selecting and implementing suitable storage solutions, data processing frameworks, and data orchestration tools. Additionally, a Data Platform Engineer ensures the continuous evolution of the data platform to meet changing data needs and leverage technological advancements, while maintaining high levels of data security, availability, and performance. They are also tasked with creating and managing processes and tools that enhance operational efficiency, including optimising data flow and ensuring seamless data integration, all of which are essential for enabling developers to build, deploy, and operate data-centric applications efficiently.\n\n - Grade Specific \nAn expert on the principles and practices associated with data platform engineering, particularly within cloud environments, and demonstrates proficiency in specific technical areas related to cloud-based data infrastructure, automation, and scalability.Key responsibilities encompass:Team Leadership and ManagementSupervising a team of platform engineers, with a focus on team dynamics and the efficient delivery of cloud platform solutions.Technical Guidance and Decision-MakingProviding technical leadership and making pivotal decisions concerning platform architecture, tools, and processes. Balancing hands-on involvement with strategic oversight.Mentorship and Skill DevelopmentGuiding team members through mentorship, enhancing their technical proficiencies, and nurturing a culture of continual learning and innovation in platform engineering practices.In-Depth Technical ProficiencyPossessing a comprehensive understanding of platform engineering principles and practices, and demonstrating expertise in crucial technical areas such as cloud services, automation, and system architecture.Community ContributionMaking significant contributions to the development of the platform engineering community, staying informed about emerging trends, and applying this knowledge to drive enhancements in capability.\n\n Skills (competencies)",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['cloud services', 'cloud platform', 'cloud data flow', 'system architecture', 'data flow', 'hive', 'snowflake', 'python', 'airflow', 'microsoft azure', 'data warehousing', 'data engineering', 'sql', 'docker', 'spark', 'gcp', 'kafka', 'hadoop', 'sqoop', 'bigquery', 'big data', 'aws', 'etl', 'data integration']",2025-06-13 06:01:05
Data Engineer-Data Platforms,IBM,2 - 5 years,Not Disclosed,['Mumbai'],"Experience with Scala object-oriented/object function Strong SQL background.\nExperience in Spark SQL, Hive, Data Engineer.\nSQL Experience with data pipelines & Data Lake Strong background in distributed comp.\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nSQL Experience with data pipelines & Data Lake Strong background in distributed comp\nExperience with Scala object-oriented/object function Strong SQL background\n\n\nPreferred technical and professional experience\nCore Scala Development Experience",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['hive', 'scala', 'sql', 'spark', 'data lake', 'amazon redshift', 'pyspark', 'data warehousing', 'emr', 'java', 'data modeling', 'mysql', 'hadoop', 'big data', 'etl', 'python', 'microsoft azure', 'machine learning', 'data engineering', 'sql server', 'nosql', 'amazon ec2', 'kafka', 'sqoop', 'aws']",2025-06-13 06:01:06
Data Engineer-Data Platforms,IBM,2 - 5 years,Not Disclosed,['Pune'],"As a Data Engineer at IBM, you'll play a vital role in the development, design of application, provide regular support/guidance to project teams on complex coding, issue resolution and execution.\nYour primary responsibilities include:\nLead the design and construction of new solutions using the latest technologies, always looking to add business value and meet user requirements.\nStrive for continuous improvements by testing the build solution and working under an agile framework.\nDiscover and implement the latest technologies trends to maximize and build creative solutions\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nExperience with Apache Spark (PySpark)In-depth knowledge of Spark’s architecture, core APIs, and PySpark for distributed data processing.\nBig Data TechnologiesFamiliarity with Hadoop, HDFS, Kafka, and other big data tools. Data Engineering\n\nSkills:\nStrong understanding of ETL pipelines, data modeling, and data warehousing concepts.\nStrong proficiency in PythonExpertise in Python programming with a focus on data processing and manipulation. Data Processing FrameworksKnowledge of data processing libraries such as Pandas, NumPy.\nSQL ProficiencyExperience writing optimized SQL queries for large-scale data analysis and transformation.\nCloud PlatformsExperience working with cloud platforms like AWS, Azure, or GCP, including using cloud storage systems\n\n\nPreferred technical and professional experience\nDefine, drive, and implement an architecture strategy and standards for end-to-end monitoring.\nPartner with the rest of the technology teams including application development, enterprise architecture, testing services, network engineering,\nGood to have detection and prevention tools for Company products and Platform and customer-facing",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'data processing', 'pyspark', 'data modeling', 'spark', 'data analysis', 'microsoft azure', 'data warehousing', 'cloud platforms', 'numpy', 'data engineering', 'sql', 'pandas', 'spring boot', 'etl pipelines', 'java', 'gcp', 'kafka', 'data warehousing concepts', 'hadoop', 'big data', 'aws', 'etl']",2025-06-13 06:01:08
Data Engineer-Data Platforms,IBM,2 - 5 years,Not Disclosed,['Navi Mumbai'],"As a Data Engineer at IBM, you'll play a vital role in the development, design of application, provide regular support/guidance to project teams on complex coding, issue resolution and execution.\n\nYour primary responsibilities include:\nLead the design and construction of new solutions using the latest technologies, always looking to add business value and meet user requirements.\nStrive for continuous improvements by testing the build solution and working under an agile framework.\nDiscover and implement the latest technologies trends to maximize and build creative solutions\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nExperience with Apache Spark (PySpark)In-depth knowledge of Spark’s architecture, core APIs, and PySpark for distributed data processing.\nBig Data TechnologiesFamiliarity with Hadoop, HDFS, Kafka, and other big data tools. Data Engineering\n\nSkills:\nStrong understanding of ETL pipelines, data modeling, and data warehousing concepts.\nStrong proficiency in PythonExpertise in Python programming with a focus on data processing and manipulation. Data Processing FrameworksKnowledge of data processing libraries such as Pandas, NumPy.\nSQL ProficiencyExperience writing optimized SQL queries for large-scale data analysis and transformation.\nCloud PlatformsExperience working with cloud platforms like AWS, Azure, or GCP, including using cloud storage systems\n\n\nPreferred technical and professional experience\nDefine, drive, and implement an architecture strategy and standards for end-to-end monitoring.\nPartner with the rest of the technology teams including application development, enterprise architecture, testing services, network engineering,\nGood to have detection and prevention tools for Company products and Platform and customer-facing",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'data processing', 'pyspark', 'data modeling', 'spark', 'data analysis', 'microsoft azure', 'data warehousing', 'cloud platforms', 'numpy', 'data engineering', 'sql', 'pandas', 'spring boot', 'etl pipelines', 'java', 'gcp', 'kafka', 'data warehousing concepts', 'hadoop', 'big data', 'aws', 'etl']",2025-06-13 06:01:10
Data Engineer-Data Platforms-AWS,IBM,4 - 9 years,Not Disclosed,['Kochi'],"As Data Engineer, you will develop, maintain, evaluate and test big data solutions. You will be involved in the development of data solutions using Spark Framework with Python or Scala on Hadoop and Azure Cloud Data Platform\n\nResponsibilities:\nExperienced in building data pipelines to Ingest, process, and transform data from files, streams and databases. Process the data with Spark, Python, PySpark and Hive, Hbase or other NoSQL databases on Azure Cloud Data Platform or HDFS\nExperienced in develop efficient software code for multiple use cases leveraging Spark Framework / using Python or Scala and Big Data technologies for various use cases built on the platform\nExperience in developing streaming pipelines\nExperience to work with Hadoop / Azure eco system components to implement scalable solutions to meet the ever-increasing data volumes, using big data/cloud technologies Apache Spark, Kafka, any Cloud computing etc\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nMinimum 4+ years of experience in Big Data technologies with extensive data engineering experience in Spark / Python or Scala;\nMinimum 3 years of experience on Cloud Data Platforms on Azure;\nExperience in DataBricks / Azure HDInsight / Azure Data Factory, Synapse, SQL Server DB\nGood to excellent SQL skills\nExposure to streaming solutions and message brokers like Kafka technologies\n\n\nPreferred technical and professional experience\nCertification in Azure and Data Bricks or Cloudera Spark Certified developers",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'big data technologies', 'data engineering', 'sql', 'spark', 'hive', 'cloudera', 'scala', 'pyspark', 'microsoft azure', 'azure data factory', 'azure hdinsight', 'azure cloud', 'sql server', 'nosql', 'data bricks', 'apache', 'kafka', 'hadoop', 'big data', 'aws', 'cloud computing', 'hbase', 'nosql databases']",2025-06-13 06:01:11
Data Engineer-Data Integration,IBM,2 - 5 years,Not Disclosed,['Pune'],"As Data Engineer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nExpertise in Data warehousing/ information Management/ Data Integration/Business Intelligence using ETL tool Informatica PowerCenter\nKnowledge of Cloud, Power BI, Data migration on cloud skills.\nExperience in Unix shell scripting and python\nExperience with relational SQL, Big Data etc\n\n\nPreferred technical and professional experience\nKnowledge of MS-Azure Cloud\nExperience in Informatica PowerCenter\nExperience in Unix shell scripting and python",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['information management', 'data warehousing', 'business intelligence', 'etl', 'data integration', 'python', 'data management', 'informatica powercenter', 'power bi', 'relational sql', 'data migration', 'azure cloud', 'sql', 'unix shell scripting', 'java', 'etl tool', 'big data', 'informatica', 'unix']",2025-06-13 06:01:13
Data Platform Engineer,Accenture,3 - 8 years,Not Disclosed,['Pune'],"Project Role :Data Platform Engineer\n\n\n\n\n\nProject Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Platform Engineer, you will assist with the data platform blueprint and design, collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. You will play a crucial role in shaping the data platform components.\nRoles & Responsibilities:- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Collaborate with Integration Architects and Data Architects to design and implement data platform components.- Ensure seamless integration between various systems and data models.- Develop and maintain data platform blueprints.- Provide technical expertise in data platform design and implementation.- Troubleshoot and resolve data platform related issues.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Strong understanding of data platform architecture and design principles.- Experience in implementing and optimizing data pipelines.- Knowledge of cloud-based data solutions.- Hands-on experience with data platform security and compliance measures.\nAdditional Information:- The candidate should have a minimum of 3 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Pune office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analytics', 'java', 'data modeling', 'platform architecture', 'design principles', 'hive', 'python', 'oracle', 'enterprise architecture', 'microsoft azure', 'data warehousing', 'sql', 'docker', 'infrastructure architecture', 'spark', 'gcp', 'design patterns', 'hadoop', 'agile', 'aws', 'big data']",2025-06-13 06:01:15
Data Platform Engineer,Accenture,7 - 12 years,Not Disclosed,['Hyderabad'],"Project Role :Data Platform Engineer\n\n\n\n\n\nProject Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Data Platform Engineer, you will assist with the data platform blueprint and design, collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. You will play a crucial role in the development and maintenance of the data platform components, contributing to the overall success of the organization.\nRoles & Responsibilities:- Expected to be an SME, collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Assist with the data platform blueprint and design.- Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.- Develop and maintain data platform components.- Contribute to the overall success of the organization.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Strong understanding of statistical analysis and machine learning algorithms.- Experience with data visualization tools such as Tableau or Power BI.- Hands-on implementing various machine learning algorithms such as linear regression, logistic regression, decision trees, and clustering algorithms.- Solid grasp of data munging techniques, including data cleaning, transformation, and normalization to ensure data quality and integrity.\nAdditional Information:- The candidate should have a minimum of 7.5 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Hyderabad office.- 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analytics', 'tableau', 'machine learning algorithms', 'statistics', 'data munging', 'python', 'natural language processing', 'power bi', 'machine learning', 'sql', 'data bricks', 'data quality', 'r', 'data modeling', 'data science', 'predictive modeling', 'text mining', 'logistic regression']",2025-06-13 06:01:17
Data Platform Engineer,Accenture,7 - 12 years,Not Disclosed,['Pune'],"Project Role :Data Platform Engineer\n\n\n\n\n\nProject Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification : Engineering graduate preferably Computer Science graduate 15 years of full time education\n\n\nSummary:As a Data Platform Engineer, you will be responsible for assisting with the blueprint and design of the data platform components using Databricks Unified Data Analytics Platform. Your typical day will involve collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\nRoles & Responsibilities:- Assist with the blueprint and design of the data platform components using Databricks Unified Data Analytics Platform.- Collaborate with Integration Architects and Data Architects to ensure cohesive integration between systems and data models.- Develop and maintain data pipelines and ETL processes using Databricks Unified Data Analytics Platform.- Design and implement data security and access controls for the data platform.- Troubleshoot and resolve issues related to the data platform and data pipelines.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nExperience with Databricks Unified Data Analytics Platform.- Must To Have\n\n\n\n\nSkills:\nStrong understanding of data modeling and database design principles.- Good To Have\n\n\n\n\nSkills:\nExperience with cloud-based data platforms such as AWS or Azure.- Good To Have\n\n\n\n\nSkills:\nExperience with data security and access controls.- Good To Have\n\n\n\n\nSkills:\nExperience with data visualization tools such as Tableau or Power BI.\nAdditional Information:- The candidate should have a minimum of 7.5 years of experience in Databricks Unified Data Analytics Platform.- The ideal candidate will possess a strong educational background in computer science or a related field, along with a proven track record of delivering impactful data-driven solutions.- This position is based at our Bangalore, Hyderabad, Chennai and Pune Offices.- Mandatory office (RTO) for 2- 3 days and have to work on 2 shifts (Shift A- 10:00am to 8:00pm IST and Shift B - 12:30pm to 10:30 pm IST)\n\nQualification\n\nEngineering graduate preferably Computer Science graduate 15 years of full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['database design', 'data modeling', 'data analytics', 'microsoft azure', 'design principles', 'hive', 'sql', 'java', 'spark', 'design patterns', 'oops', 'mysql', 'hadoop', 'etl', 'big data', 'c#', 'rest', 'python', 'data security', 'power bi', 'javascript', 'sql server', 'data bricks', 'tableau', 'kafka', 'sqoop', 'aws']",2025-06-13 06:01:19
Data Platform Engineer,Accenture,7 - 12 years,Not Disclosed,['Hyderabad'],"Project Role :Data Platform Engineer\n\n\n\n\n\nProject Role Description :Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification : Engineering graduate preferably Computer Science graduate 15 years of full time education\n\n\nSummary:As a Data Platform Engineer, you will assist with the data platform blueprint and design, collaborating with Integration Architects and Data Architects to ensure cohesive integration between systems and data models. You will play a crucial role in shaping the data platform components.\nRoles & Responsibilities:- Expected to be an SME- Collaborate and manage the team to perform- Responsible for team decisions- Engage with multiple teams and contribute on key decisions- Provide solutions to problems for their immediate team and across multiple teams- Lead data platform blueprint and design- Implement data platform components effectively- Ensure seamless integration between systems and data models\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform- Strong understanding of data platform architecture- Experience in data integration and data modeling- Knowledge of cloud platforms like AWS or Azure- Hands-on experience with SQL and NoSQL databases\nAdditional Information:- The candidate should have a minimum of 7.5 years of experience in Databricks Unified Data Analytics Platform- This position is based at our Hyderabad office- An Engineering graduate preferably Computer Science graduate with 15 years of full-time education is required\n\nQualification\n\nEngineering graduate preferably Computer Science graduate 15 years of full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['sql', 'data modeling', 'data analytics', 'platform architecture', 'aws', 'kubernetes', 'continuous integration', 'openshift', 'docker', 'microservices', 'iot', 'java', 'git', 'gcp', 'devops', 'linux', 'jenkins', 'mysql', 'hadoop', 'big data', 'microsoft azure', 'cloud platforms', 'nosql', 'agile', 'data integration']",2025-06-13 06:01:20
Cloud Data Engineer - GCP,Synechron,2 - 3 years,Not Disclosed,"['Hyderabad', 'Gachibowli']","Job Summary\nSynechron is seeking a highly motivated and skilled Senior Cloud Data Engineer GCP to join our cloud solutions team. In this role, you will collaborate closely with clients and internal stakeholders to design, implement, and manage scalable, secure, and high-performance cloud-based data solutions on Google Cloud Platform (GCP). You will leverage your technical expertise to ensure the integrity, security, and efficiency of cloud data architectures, enabling the organization to derive maximum value from cloud data assets. This role contributes directly to our mission of delivering innovative digital transformation solutions and supports the organizations strategic objectives of scalable and sustainable cloud infrastructure.\nSoftware Requirements\nOverall Responsibilities\nTechnical Skills (By Category)\nExperience Requirements\nDay-to-Day Activities\nQualifications\nProfessional Competencies",,,,"['GCP', 'Jenkins', 'Java', 'NoSQL', 'Bash scripts', 'Data Studio', 'Data Management', 'CI/CD', 'Apache Beam', 'MongoDB', 'Cloud Build']",2025-06-13 06:01:22
"Data Engineer, Alexa AI Developer Tech",Amazon,3 - 8 years,Not Disclosed,['Pune'],"Alexa+ is our next-generation assistant powered by generative AI. Alexa+ is more conversational, smarter, personalized, and gets things done.\n\nOur goal is make Alexa+ an instantly familiar personal assistant that is always ready to help or entertain on any device. At the core of this vision is Alexa AI Developer Tech, a close-knit team that s dedicated to providing software developers with the tools, primitives, and services they need to easily create engaging customer experiences that expand the wealth of information, products and services available on Alexa+.\n\nYou will join a growing organization working on top technology using Generative AI and have an enormous opportunity to make an impact on the design, architecture, and implementation of products used every day, by people you know.\n\nWe re working hard, having fun, and making history; come join us!\n\n\nWork with a team of product and program managers, engineering leaders, and business leaders to build data architectures and platforms to support business\nDesign, develop, and operate high-scalable, high-performance, low-cost, and accurate data pipelines in distributed data processing platforms\nRecognize and adopt best practices in data processing, reporting, and analysis: data integrity, test design, analysis, validation, and documentation\nKeep up to date with big data technologies, evaluate and make decisions around the use of new or existing software products to design the data architecture\nDesign, build and own all the components of a high-volume data warehouse end to end.\nProvide end-to-end data engineering support for project lifecycle execution (design, execution and risk assessment)\nContinually improve ongoing reporting and analysis processes, automating or simplifying self-service support for customers\nInterface with other technology teams to extract, transform, and load (ETL) data from a wide variety of data sources\nOwn the functional and nonfunctional scaling of software systems in your ownership area. *Implement big data solutions for distributed computing.\n\nAbout the team\nAlexa AI Developer Tech is an organization within Alexa on a mission to empower developers to create delightful and engaging experiences by making Alexa more natural, accurate, conversational, and personalized. 3+ years of data engineering experience\n4+ years of SQL experience\nExperience with data modeling, warehousing and building ETL pipelines Experience with AWS technologies like Redshift, S3, AWS Glue, EMR, Kinesis, FireHose, Lambda, and IAM roles and permissions\nExperience with non-relational databases / data stores (object storage, document or key-value stores, graph databases, column-family databases)",,,,"['Data modeling', 'Business design', 'Risk assessment', 'Test design', 'Architectural design', 'Data processing', 'data integrity', 'Design analysis', 'SQL', 'Data architecture']",2025-06-13 06:01:23
"Data Engineer II, TFAW/Sherlock",Amazon,3 - 8 years,Not Disclosed,['Hyderabad'],"As a Data Engineer you will be working on building and maintaining complex data pipelines, assemble large and complex datasets to generate business insights and to enable data driven decision making and support the rapidly growing and dynamic business demand for data.\nYou will have an opportunity to collaborate and work with various teams of Business analysts, Managers, Software Dev Engineers, and Data Engineers to determine how best to design, implement and support solutions. You will be challenged and provided with tremendous growth opportunity in a customer facing, fast paced, agile environment.\n\n\nDesign, implement and support an analytical data platform solutions for data driven decisions and insights\nDesign data schema and operate internal data warehouses SQL/NOSQL database systems\nWork on different data model designs, architecture, implementation, discussions and optimizations\nInterface with other teams to extract, transform, and load data from a wide variety of data sources using AWS big data technologies like EMR, RedShift, Elastic Search etc.\nWork on different AWS technologies such as S3, RedShift, Lambda, Glue, etc.. and Explore and learn the latest AWS technologies to provide new capabilities and increase efficiency\nWork on data lake platform and different components in the data lake such as Hadoop, Amazon S3 etc.\nWork on SQL technologies on Hadoop such as Spark, Hive, Impala etc..\nHelp continually improve ongoing analysis processes, optimizing or simplifying self-service support for customers\nMust possess strong verbal and written communication skills, be self-driven, and deliver high quality results in a fast-paced environment.\nRecognize and adopt best practices in reporting and analysis: data integrity, test design, analysis, validation, and documentation.\nEnjoy working closely with your peers in a group of talented engineers and gain knowledge.\nBe enthusiastic about building deep domain knowledge on various Amazon s business domains.\nOwn the development and maintenance of ongoing metrics, reports, analyses, dashboards, etc. to drive key business decisions. 3+ years of data engineering experience\n4+ years of SQL experience\nExperience with data modeling, warehousing and building ETL pipelines Experience with AWS technologies like Redshift, S3, AWS Glue, EMR, Kinesis, FireHose, Lambda, and IAM roles and permissions\nExperience with non-relational databases / data stores (object storage, document or key-value stores, graph databases, column-family databases)",,,,"['NoSQL', 'data engineer ii', 'Business Analyst', 'Analytical', 'Schema', 'Test design', 'Agile', 'data integrity', 'Design analysis', 'SQL']",2025-06-13 06:01:25
Data Analyst,Growing Soonicorn in Auto Components Spa...,3 - 5 years,Not Disclosed,['Bengaluru'],"Dear Candidates,\n\nGreetings!!\n\nWe are hiring for one of the Globalized Product Based & Motor Vehicle Manufacturing MNC.\n\nJob Type: FTE\nJob Role:- Data Analyst\nExperience: 3 to 5 Years\nLocation: Bangalore\nWork Mode: Work from office\nNotice Period: Immediate to 30 days\nBudget: As Per Market Standards\nMandatory Skills:- Azure Data bricks, Power BI, Tableau, SQL, Python\n\n\n\nInterested candidates can share their updated resume on Gurpreet@selectiveglobalsearch.com",Industry Type: Electronic Components / Semiconductors,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Power Bi', 'Azure Databricks', 'Tableau', 'SQL', 'Python', 'Data Visualization']",2025-06-13 06:01:27
Data Analyst-Having Stratup-Mid-Size companies Exp.@ Bangalore_Urgent,"A leader in this space, we deliver world...",8 - 13 years,Not Disclosed,['Bengaluru'],"Data Analyst\n\nLocation: Bangalore\nExperience: 8 - 15 Yrs\nType: Full-time\n\nRole Overview\n\nWe are seeking a skilled Data Analyst to support our platform powering operational intelligence across airports and similar sectors. The ideal candidate will have experience working with time-series datasets and operational information to uncover trends, anomalies, and actionable insights. This role will work closely with data engineers, ML teams, and domain experts to turn raw data into meaningful intelligence for business and operations stakeholders.\n\nKey Responsibilities\n\nAnalyze time-series and sensor data from various sources\nDevelop and maintain dashboards, reports, and visualizations to communicate key metrics and trends.\nCorrelate data from multiple systems (vision, weather, flight schedules, etc) to provide holistic insights.\nCollaborate with AI/ML teams to support model validation and interpret AI-driven alerts (e.g., anomalies, intrusion detection).\nPrepare and clean datasets for analysis and modeling; ensure data quality and consistency.\nWork with stakeholders to understand reporting needs and deliver business-oriented outputs.\n\n\nQualifications & Required Skills\n\nBachelors or Masters degree in Data Science, Statistics, Computer Science, Engineering, or a related field.\n5+ years of experience in a data analyst role, ideally in a technical/industrial domain.\nStrong SQL skills and proficiency with BI/reporting tools (e.g., Power BI, Tableau, Grafana).\nHands-on experience analyzing structured and semi-structured data (JSON, CSV, time-series).\nProficiency in Python or R for data manipulation and exploratory analysis.\nUnderstanding of time-series databases or streaming data (e.g., InfluxDB, Kafka, Kinesis).\nSolid grasp of statistical analysis and anomaly detection methods.\nExperience working with data from industrial systems or large-scale physical infrastructure.\n\n\nGood-to-Have Skills\n\nDomain experience in airports, smart infrastructure, transportation, or logistics.\nFamiliarity with data platforms (Snowflake, BigQuery, Custom-built using open-source).\nExposure to tools like Airflow, Jupyter Notebooks and data quality frameworks.\nBasic understanding of AI/ML workflows and data preparation requirements.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Kafka', 'SQL', 'airports', 'InfluxDB', 'Airflow', 'structured Data', 'time-series', 'JSON', 'Tableau', 'Grafana', 'R', 'AI/ML', 'Kinesis', 'Snowflake', 'time-series databases', 'Data Preparation', 'Python', 'smart infrastructure', 'BigQuery', 'streaming data', 'Power BI', 'CSV', 'transportation', 'logistic', 'reporting tools']",2025-06-13 06:01:29
Data Analyst,Fraoula,3 - 5 years,Not Disclosed,"['Mumbai', 'Bengaluru', 'Delhi / NCR']","We are seeking a highly motivated and detail-oriented Data Analyst to join our growing team. The ideal candidate will be passionate about transforming raw data into actionable insights, helping us make informed strategic decisions. You will play a crucial role in collecting, processing, and performing statistical analyses on large datasets, translating complex findings into clear, concise reports and visualizations for various stakeholders.\n\nKey Responsibilities:\n-Data Collection & Cleaning: Source, collect, and clean data from various internal and external databases, ensuring data accuracy, completeness, and consistency.\n-Data Analysis & Modeling: Perform in-depth statistical analysis, identify trends, patterns, and anomalies in data. Develop and implement data models to predict outcomes and optimize performance.\n-Reporting & Visualization: Create compelling and intuitive dashboards, reports, and presentations using data visualization tools (e.g., Power BI, Tableau) to communicate insights to technical and non-technical audiences.\n\nLocation-Delhi NCR,Bangalore,Chennai,Pune,Kolkata,Ahmedabad,Mumbai,Hyderabad,Remote",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['SQL', 'Python', 'Power BI', 'Data Analysis', 'Data Visualization', 'Tableau', 'Looker', 'Data Cleaning', 'Statistical Analysis']",2025-06-13 06:01:30
Data Analyst,LTIMindtree,7 - 12 years,Not Disclosed,"['Chennai', 'Bengaluru', 'Delhi / NCR']","Role & responsibilities\nPrimary Skill SQL, Business Intelligence Tools, Banking products exp, DBMS( Redshift OR Oracle),Cloud Exp (AWS Preferred).\nSecondary Skill Data Management, Agile exp.\nTotal Exp 7+\nNotice Period : 30 Days\nJob Location : Pan India\n\nShare your CV on\npallavi.bhalerao@alphacom.in",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Business Intelligence Tools', 'DBMS', 'SQL', 'Banking Products']",2025-06-13 06:01:32
Data Analyst,FedEx,2 - 4 years,Not Disclosed,"['Gurugram', 'Bengaluru']","Role & responsibilities :\n\nCollect, analyze, and interpret complex data sets using Python and SQL to support business objectives.\nCollaborate with stakeholders to understand business needs, formulate analytic solutions, and provide actionable insights.\nDevelop and maintain data models and reports to track key performance indicators (KPIs) and business metrics.\nCreate meaningful data visualizations to communicate findings, trends, and actionable insights to non-technical stakeholders.\nConduct exploratory data analysis and identify patterns, trends, and opportunities for business improvement.\nSupport data quality initiatives, ensuring accuracy and consistency across data sources.\nUtilize statistical and quantitative techniques to support problem-solving and business optimization efforts.\n\n\n\n\nPreferred candidate profile\n\nPython: Proficiency in data manipulation, data analysis libraries (Pandas, NumPy),and data visualization libraries (Matplotlib, Seaborn).\nSQL: Strong command of SQL for data extraction, transformation, and complex queries.\nBusiness Acumen: Ability to understand business context and objectives, aligning analytics with organizational goals.\nQuantitative Aptitude: Strong analytical and problem-solving skills, with a keen attention to detail.\nData Visualization: Basic skills in data visualization to effectively communicate insights.\nStatistical Analysis: Foundational understanding of statistical methods (e.g., regression, hypothesis testing).\nCommunication Skills: Ability to distill complex data insights into clear,actionable recommendations for stakeholders.",Industry Type: Courier / Logistics (Logistics Tech),Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Analysis', 'SQL', 'Python', 'Power Bi', 'Business Insights', 'Tableau', 'Data Analytics']",2025-06-13 06:01:34
Data Engineer sr Associate,Product base company,8 - 13 years,Not Disclosed,['Hyderabad( Gachibowli )'],"Bachelors degree in computer science, engineering, or a related field. Master’s degree preferred.\nData: 5+ years of experience with data analytics and data warehousing. Sound knowledge of data warehousing concepts.\nSQL: 5+ years of hands-on experience on SQL and query optimization for data pipelines.\nELT/ETL: 5+ years of experience in Informatica/ 3+ years of experience in IICS/IDMC\nMigration Experience: Experience Informatica on prem to IICS/IDMC migration\nCloud: 5+ years’ experience working in AWS cloud environment\nPython: 5+ years of hands-on experience of development with Python\nWorkflow: 4+ years of experience in orchestration and scheduling tools (e.g. Apache Airflow)\nAdvanced Data Processing: Experience using data processing technologies such as Apache Spark or Kafka\nTroubleshooting: Experience with troubleshooting and root cause analysis to determine and remediate potential issues\nCommunication: Excellent communication, problem-solving and organizational and analytical skills\nAble to work independently and to provide leadership to small teams of developers.\nReporting: Experience with data reporting (e.g. MicroStrategy, Tableau, Looker) and data cataloging tools (e.g. Alation)\nExperience in Design and Implementation of ETL solutions with effective design and optimized performance, ETL Development with industry standard recommendations for jobs recovery, fail over, logging, alerting mechanisms. Role & responsibilities\n\n\nPreferred candidate profile",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Airflow', 'Unix', 'IICS', 'Informatica', 'Python', 'Informatica Cloud', 'Data Engineer', 'AWS', 'SQL']",2025-06-13 06:01:35
DE&A - AIML - Data Science - Data Science (Other) DE&A - AIML,Zensar,15 - 18 years,Not Disclosed,"['Kolkata', 'Mumbai', 'New Delhi', 'Hyderabad', 'Pune', 'Chennai', 'Bengaluru']","Experience: 15+ years overall | Minimum 10 full-cycle AI/ML project implementations , including GenAI experience\nRole Summary:\nWe are seeking a AI Architect to lead strategic AI transformation initiatives. This role demands deep hands-on experience in AI, Machine Learning (ML), and Generative AI (GenAI) , along with the ability to engage directly with C-level stakeholders , align technical delivery with business objectives, and drive enterprise-wide adoption of advanced AI solutions.\nThe ideal candidate is a techno-strategic leader who can take AI/ML/GenAI projects from ideation to production building architectures, leading cross-functional teams, and ensuring regulatory and operational alignment in BFSI environments.\nKey\nConsulting & Business Alignment\nPartner with senior business and IT leadership , including CIOs, CDOs, and COOs , to identify high-impact use cases across retail banking, insurance, credit, and capital markets.\nTranslate complex BFSI challenges into technically feasible and scalable AI/ML/GenAI solutions.\nCreate strategic roadmaps, capability assessments, and PoV/PoC execution plans that align with business KPIs and regulatory needs.\nSolution Architecture & Delivery Leadership\nDesign and lead delivery of AI/ML/GenAI pipelines covering data ingestion, model training, validation, deployment, and monitoring.\nBuild and scale GenAI-based solutions like LLM-driven chatbots, intelligent document processing, RAG pipelines, summarization tools , and virtual assistants.\nArchitect cloud-native AI platforms using AWS (SageMaker, Bedrock) , Azure (ML, OpenAI) , or GCP (Vertex AI, BigQuery, LangChain) .\nDefine and implement MLOps and LLMOps frameworks for versioning, retraining, CI/CD, and production observability.\nEnsure adherence to Responsible AI principles , including explainability, bias mitigation, auditability, and regulatory compliance\nEngineering & Integration\nWork closely with data engineering teams to acquire, transform, and pipeline data from core banking systems, CRMs, claims systems, and real-time feeds.\nDesign architecture for data lakes, feature stores, and vector databases supporting AI and GenAI use cases.\nEnable seamless integration of AI capabilities into enterprise workflows, customer platforms, and decision engines via APIs and microservices.\nRequired Skills & Experience:\n15+ years of experience in AI/ML, data engineering, and cloud architecture.\nMinimum of 10 end-to-end AI/ML project implementations from use case discovery through to productionization.\nProven expertise in: (Any One)\nAI/ML frameworks : scikit-learn, XGBoost, TensorFlow, PyTorch\nGenAI/LLM platforms : OpenAI, Cohere, Mistral, LangChain, Hugging Face, vector DBs (Pinecone, FAISS, Chroma)\nCloud platforms : AWS, Azure, GCP - including AI/ML & GenAI native services\nMLOps/LLMOps tools : MLflow, Kubeflow, SageMaker Pipelines, Vertex AI Pipelines\nStrong experience with data security, governance, model risk management , and AI compliance frameworks relevant to BFSI.\nAbility to lead large cross-functional teams and engage both technical teams and senior stakeholders.\nExperience: 15+ years overall | Minimum 10 full-cycle AI/ML project implementations , including GenAI experience\nRole Summary:\nWe are seeking a AI Architect to lead strategic AI transformation initiatives. This role demands deep hands-on experience in AI, Machine Learning (ML), and Generative AI (GenAI) , along with the ability to engage directly with C-level stakeholders , align technical delivery with business objectives, and drive enterprise-wide adoption of advanced AI solutions.\nThe ideal candidate is a techno-strategic leader who can take AI/ML/GenAI projects from ideation to production building architectures, leading cross-functional teams, and ensuring regulatory and operational alignment in BFSI environments.\nKey\nConsulting & Business Alignment\nPartner with senior business and IT leadership , including CIOs, CDOs, and COOs , to identify high-impact use cases across retail banking, insurance, credit, and capital markets.\nTranslate complex BFSI challenges into technically feasible and scalable AI/ML/GenAI solutions.\nCreate strategic roadmaps, capability assessments, and PoV/PoC execution plans that align with business KPIs and regulatory needs.\nSolution Architecture & Delivery Leadership\nDesign and lead delivery of AI/ML/GenAI pipelines covering data ingestion, model training, validation, deployment, and monitoring.\nBuild and scale GenAI-based solutions like LLM-driven chatbots, intelligent document processing, RAG pipelines, summarization tools , and virtual assistants.\nArchitect cloud-native AI platforms using AWS (SageMaker, Bedrock) , Azure (ML, OpenAI) , or GCP (Vertex AI, BigQuery, LangChain) .\nDefine and implement MLOps and LLMOps frameworks for versioning, retraining, CI/CD, and production observability.\nEnsure adherence to Responsible AI principles , including explainability, bias mitigation, auditability, and regulatory compliance\nEngineering & Integration\nWork closely with data engineering teams to acquire, transform, and pipeline data from core banking systems, CRMs, claims systems, and real-time feeds.\nDesign architecture for data lakes, feature stores, and vector databases supporting AI and GenAI use cases.\nEnable seamless integration of AI capabilities into enterprise workflows, customer platforms, and decision engines via APIs and microservices.\nRequired Skills & Experience:\n15+ years of experience in AI/ML, data engineering, and cloud architecture.\nMinimum of 10 end-to-end AI/ML project implementations from use case discovery through to productionization.\nProven expertise in: (Any One)\nAI/ML frameworks : scikit-learn, XGBoost, TensorFlow, PyTorch\nGenAI/LLM platforms : OpenAI, Cohere, Mistral, LangChain, Hugging Face, vector DBs (Pinecone, FAISS, Chroma)\nCloud platforms : AWS, Azure, GCP - including AI/ML & GenAI native services\nMLOps/LLMOps tools : MLflow, Kubeflow, SageMaker Pipelines, Vertex AI Pipelines\nStrong experience with data security, governance, model risk management , and AI compliance frameworks relevant to BFSI.\nAbility to lead large cross-functional teams and engage both technical teams and senior stakeholders.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Solution architecture', 'Architect', 'Bfsi', 'data security', 'Consulting', 'Machine learning', 'Risk management', 'Operations', 'Monitoring', 'Core banking']",2025-06-13 06:01:37
Data Engineer - Python Programming,Leading Client,5 - 7 years,Not Disclosed,['Indore'],"We are looking for a skilled Data Engineer with strong hands-on experience in Clickhouse, Kubernetes, SQL, Python, and FastAPI, along with a good understanding of PostgreSQL.\nThe ideal candidate will be responsible for building and maintaining efficient data pipelines, optimizing query performance, and developing APIs to support scalable data services.\n\n- Design, build, and maintain scalable and efficient data pipelines and ETL processes.\n\n- Develop and optimize Clickhouse databases for high-performance analytics.\n\n- Create RESTful APIs using FastAPI to expose data services.\n\n- Work with Kubernetes for container orchestration and deployment of data services.\n\n- Write complex SQL queries to extract, transform, and analyze data from PostgreSQL and Clickhouse.\n\n- Collaborate with data scientists, analysts, and backend teams to support data needs and ensure data quality.\n\n- Monitor, troubleshoot, and improve performance of data infrastructure.\n\n- Strong experience in Clickhouse - data modeling, query optimization, performance tuning.\n\n- Expertise in SQL - including complex joins, window functions, and optimization.\n\n- Proficient in Python, especially for data processing (Pandas, NumPy) and scripting.\n\n- Experience with FastAPI for creating lightweight APIs and microservices.\n\n- Hands-on experience with PostgreSQL - schema design, indexing, and performance.\n\n- Solid knowledge of Kubernetes managing containers, deployments, and scaling.\n\n- Understanding of software engineering best practices (CI/CD, version control, testing).\n\n- Experience with cloud platforms like AWS, GCP, or Azure.\n\n- Knowledge of data warehousing and distributed data systems.\n\n- Familiarity with Docker, Helm, and monitoring tools like Prometheus/Grafana.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'PostgreSQL', 'Data Modeling', 'Data Warehousing', 'Data Analytics', 'ETL', 'Python', 'SQL']",2025-06-13 06:01:39
Data Engineer - Python Programming,Leading Client,5 - 7 years,Not Disclosed,['Pune'],"We are looking for a skilled Data Engineer with strong hands-on experience in Clickhouse, Kubernetes, SQL, Python, and FastAPI, along with a good understanding of PostgreSQL.\nThe ideal candidate will be responsible for building and maintaining efficient data pipelines, optimizing query performance, and developing APIs to support scalable data services.\n\n- Design, build, and maintain scalable and efficient data pipelines and ETL processes.\n\n- Develop and optimize Clickhouse databases for high-performance analytics.\n\n- Create RESTful APIs using FastAPI to expose data services.\n\n- Work with Kubernetes for container orchestration and deployment of data services.\n\n- Write complex SQL queries to extract, transform, and analyze data from PostgreSQL and Clickhouse.\n\n- Collaborate with data scientists, analysts, and backend teams to support data needs and ensure data quality.\n\n- Monitor, troubleshoot, and improve performance of data infrastructure.\n\n- Strong experience in Clickhouse - data modeling, query optimization, performance tuning.\n\n- Expertise in SQL - including complex joins, window functions, and optimization.\n\n- Proficient in Python, especially for data processing (Pandas, NumPy) and scripting.\n\n- Experience with FastAPI for creating lightweight APIs and microservices.\n\n- Hands-on experience with PostgreSQL - schema design, indexing, and performance.\n\n- Solid knowledge of Kubernetes managing containers, deployments, and scaling.\n\n- Understanding of software engineering best practices (CI/CD, version control, testing).\n\n- Experience with cloud platforms like AWS, GCP, or Azure.\n\n- Knowledge of data warehousing and distributed data systems.\n\n- Familiarity with Docker, Helm, and monitoring tools like Prometheus/Grafana.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'PostgreSQL', 'Data Modeling', 'Data Warehousing', 'Data Analytics', 'ETL', 'Python', 'SQL']",2025-06-13 06:01:40
Data Engineer - Python Programming,Leading Client,5 - 7 years,Not Disclosed,"['Mumbai', 'Any Location']","We are looking for a skilled Data Engineer with strong hands-on experience in Clickhouse, Kubernetes, SQL, Python, and FastAPI, along with a good understanding of PostgreSQL.\nThe ideal candidate will be responsible for building and maintaining efficient data pipelines, optimizing query performance, and developing APIs to support scalable data services.\n\n- Design, build, and maintain scalable and efficient data pipelines and ETL processes.\n\n- Develop and optimize Clickhouse databases for high-performance analytics.\n\n- Create RESTful APIs using FastAPI to expose data services.\n\n- Work with Kubernetes for container orchestration and deployment of data services.\n\n- Write complex SQL queries to extract, transform, and analyze data from PostgreSQL and Clickhouse.\n\n- Collaborate with data scientists, analysts, and backend teams to support data needs and ensure data quality.\n\n- Monitor, troubleshoot, and improve performance of data infrastructure.\n\n- Strong experience in Clickhouse - data modeling, query optimization, performance tuning.\n\n- Expertise in SQL - including complex joins, window functions, and optimization.\n\n- Proficient in Python, especially for data processing (Pandas, NumPy) and scripting.\n\n- Experience with FastAPI for creating lightweight APIs and microservices.\n\n- Hands-on experience with PostgreSQL - schema design, indexing, and performance.\n\n- Solid knowledge of Kubernetes managing containers, deployments, and scaling.\n\n- Understanding of software engineering best practices (CI/CD, version control, testing).\n\n- Experience with cloud platforms like AWS, GCP, or Azure.\n\n- Knowledge of data warehousing and distributed data systems.\n\n- Familiarity with Docker, Helm, and monitoring tools like Prometheus/Grafana.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'PostgreSQL', 'Data Modeling', 'Data Warehousing', 'Data Analytics', 'ETL', 'Python', 'SQL']",2025-06-13 06:01:42
Data Engineer - Python Programming,Leading Client,5 - 7 years,Not Disclosed,['Nagpur'],"We are looking for a skilled Data Engineer with strong hands-on experience in Clickhouse, Kubernetes, SQL, Python, and FastAPI, along with a good understanding of PostgreSQL.\nThe ideal candidate will be responsible for building and maintaining efficient data pipelines, optimizing query performance, and developing APIs to support scalable data services.\n\n- Design, build, and maintain scalable and efficient data pipelines and ETL processes.\n\n- Develop and optimize Clickhouse databases for high-performance analytics.\n\n- Create RESTful APIs using FastAPI to expose data services.\n\n- Work with Kubernetes for container orchestration and deployment of data services.\n\n- Write complex SQL queries to extract, transform, and analyze data from PostgreSQL and Clickhouse.\n\n- Collaborate with data scientists, analysts, and backend teams to support data needs and ensure data quality.\n\n- Monitor, troubleshoot, and improve performance of data infrastructure.\n\n- Strong experience in Clickhouse - data modeling, query optimization, performance tuning.\n\n- Expertise in SQL - including complex joins, window functions, and optimization.\n\n- Proficient in Python, especially for data processing (Pandas, NumPy) and scripting.\n\n- Experience with FastAPI for creating lightweight APIs and microservices.\n\n- Hands-on experience with PostgreSQL - schema design, indexing, and performance.\n\n- Solid knowledge of Kubernetes managing containers, deployments, and scaling.\n\n- Understanding of software engineering best practices (CI/CD, version control, testing).\n\n- Experience with cloud platforms like AWS, GCP, or Azure.\n\n- Knowledge of data warehousing and distributed data systems.\n\n- Familiarity with Docker, Helm, and monitoring tools like Prometheus/Grafana.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Python Programming', 'Data Engineering', 'PostgreSQL', 'Data Modeling', 'Data Warehousing', 'Data Analytics', 'ETL', 'Python', 'SQL']",2025-06-13 06:01:43
Data Engineer - Python Programming,Leading Client,5 - 7 years,Not Disclosed,['Ahmedabad'],"We are looking for a skilled Data Engineer with strong hands-on experience in Clickhouse, Kubernetes, SQL, Python, and FastAPI, along with a good understanding of PostgreSQL.\nThe ideal candidate will be responsible for building and maintaining efficient data pipelines, optimizing query performance, and developing APIs to support scalable data services.\n\n- Design, build, and maintain scalable and efficient data pipelines and ETL processes.\n\n- Develop and optimize Clickhouse databases for high-performance analytics.\n\n- Create RESTful APIs using FastAPI to expose data services.\n\n- Work with Kubernetes for container orchestration and deployment of data services.\n\n- Write complex SQL queries to extract, transform, and analyze data from PostgreSQL and Clickhouse.\n\n- Collaborate with data scientists, analysts, and backend teams to support data needs and ensure data quality.\n\n- Monitor, troubleshoot, and improve performance of data infrastructure.\n\n- Strong experience in Clickhouse - data modeling, query optimization, performance tuning.\n\n- Expertise in SQL - including complex joins, window functions, and optimization.\n\n- Proficient in Python, especially for data processing (Pandas, NumPy) and scripting.\n\n- Experience with FastAPI for creating lightweight APIs and microservices.\n\n- Hands-on experience with PostgreSQL - schema design, indexing, and performance.\n\n- Solid knowledge of Kubernetes managing containers, deployments, and scaling.\n\n- Understanding of software engineering best practices (CI/CD, version control, testing).\n\n- Experience with cloud platforms like AWS, GCP, or Azure.\n\n- Knowledge of data warehousing and distributed data systems.\n\n- Familiarity with Docker, Helm, and monitoring tools like Prometheus/Grafana.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Python Programming', 'Data Engineering', 'PostgreSQL', 'Data Modeling', 'Data Warehousing', 'Data Analytics', 'ETL', 'Python', 'SQL']",2025-06-13 06:01:45
Data Engineer - Python Programming,Leading Client,5 - 7 years,Not Disclosed,['Delhi / NCR'],"We are looking for a skilled Data Engineer with strong hands-on experience in Clickhouse, Kubernetes, SQL, Python, and FastAPI, along with a good understanding of PostgreSQL.\nThe ideal candidate will be responsible for building and maintaining efficient data pipelines, optimizing query performance, and developing APIs to support scalable data services.\n\n- Design, build, and maintain scalable and efficient data pipelines and ETL processes.\n\n- Develop and optimize Clickhouse databases for high-performance analytics.\n\n- Create RESTful APIs using FastAPI to expose data services.\n\n- Work with Kubernetes for container orchestration and deployment of data services.\n\n- Write complex SQL queries to extract, transform, and analyze data from PostgreSQL and Clickhouse.\n\n- Collaborate with data scientists, analysts, and backend teams to support data needs and ensure data quality.\n\n- Monitor, troubleshoot, and improve performance of data infrastructure.\n\n- Strong experience in Clickhouse - data modeling, query optimization, performance tuning.\n\n- Expertise in SQL - including complex joins, window functions, and optimization.\n\n- Proficient in Python, especially for data processing (Pandas, NumPy) and scripting.\n\n- Experience with FastAPI for creating lightweight APIs and microservices.\n\n- Hands-on experience with PostgreSQL - schema design, indexing, and performance.\n\n- Solid knowledge of Kubernetes managing containers, deployments, and scaling.\n\n- Understanding of software engineering best practices (CI/CD, version control, testing).\n\n- Experience with cloud platforms like AWS, GCP, or Azure.\n\n- Knowledge of data warehousing and distributed data systems.\n\n- Familiarity with Docker, Helm, and monitoring tools like Prometheus/Grafana.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Python Programming', 'Data Engineering', 'PostgreSQL', 'Data Modeling', 'Data Warehousing', 'Data Analytics', 'ETL', 'Python', 'SQL']",2025-06-13 06:01:46
Data Engineer - Databricks,KPI Partners,3 - 6 years,Not Disclosed,"['Hyderabad', 'Pune', 'Bengaluru']","About KPI Partners.\nKPI Partners is a leading provider of data analytics solutions, dedicated to helping organizations transform data into actionable insights. Our innovative approach combines advanced technology with expert consulting, allowing businesses to leverage their data for improved performance and decision-making.\n\nJob Description.\nWe are seeking a skilled and motivated Data Engineer with experience in Databricks to join our dynamic team. The ideal candidate will be responsible for designing, building, and maintaining scalable data pipelines and data processing solutions that support our analytics initiatives. You will collaborate closely with data scientists, analysts, and other engineers to ensure the consistent flow of high-quality data across our platforms.",,,,"['python', 'data analytics', 'analytical', 'scala', 'pyspark', 'microsoft azure', 'data warehousing', 'data pipeline', 'data architecture', 'data engineering', 'sql', 'data bricks', 'cloud', 'analytics', 'data quality', 'data modeling', 'gcp', 'teamwork', 'integration', 'aws', 'etl', 'programming', 'communication skills', 'etl scripts']",2025-06-13 06:01:48
Data Engineer-Having Stratup-Mid-Size company Exp.@ Bangalore_Urgent,"As a leader in this space, we deliver wo...",8 - 13 years,Not Disclosed,['Bengaluru'],"Data Engineer\n\nLocation: Bangalore - Onsite\nExperience: 8 - 15 years\nType: Full-time\n\nRole Overview\n\nWe are seeking an experienced Data Engineer to build and maintain scalable, high-performance data pipelines and infrastructure for our next-generation data platform. The platform ingests and processes real-time and historical data from diverse industrial sources such as airport systems, sensors, cameras, and APIs. You will work closely with AI/ML engineers, data scientists, and DevOps to enable reliable analytics, forecasting, and anomaly detection use cases.\nKey Responsibilities\nDesign and implement real-time (Kafka, Spark/Flink) and batch (Airflow, Spark) pipelines for high-throughput data ingestion, processing, and transformation.\nDevelop data models and manage data lakes and warehouses (Delta Lake, Iceberg, etc) to support both analytical and ML workloads.\nIntegrate data from diverse sources: IoT sensors, databases (SQL/NoSQL), REST APIs, and flat files.\nEnsure pipeline scalability, observability, and data quality through monitoring, alerting, validation, and lineage tracking.\nCollaborate with AI/ML teams to provision clean and ML-ready datasets for training and inference.\nDeploy, optimize, and manage pipelines and data infrastructure across on-premise and hybrid environments.\nParticipate in architectural decisions to ensure resilient, cost-effective, and secure data flows.\nContribute to infrastructure-as-code and automation for data deployment using Terraform, Ansible, or similar tools.\n\n\nQualifications & Required Skills\n\nBachelors or Master’s in Computer Science, Engineering, or related field.\n6+ years in data engineering roles, with at least 2 years handling real-time or streaming pipelines.\nStrong programming skills in Python/Java and SQL.\nExperience with Apache Kafka, Apache Spark, or Apache Flink for real-time and batch processing.\nHands-on with Airflow, dbt, or other orchestration tools.\nFamiliarity with data modeling (OLAP/OLTP), schema evolution, and format handling (Parquet, Avro, ORC).\nExperience with hybrid/on-prem and cloud platforms (AWS/GCP/Azure) deployments.\nProficient in working with data lakes/warehouses like Snowflake, BigQuery, Redshift, or Delta Lake.\nKnowledge of DevOps practices, Docker/Kubernetes, Terraform or Ansible.\nExposure to data observability, data cataloging, and quality tools (e.g., Great Expectations, OpenMetadata).\nGood-to-Have\nExperience with time-series databases (e.g., InfluxDB, TimescaleDB) and sensor data.\nPrior experience in domains such as aviation, manufacturing, or logistics is a plus.\n\nRole & responsibilities\n\n\nPreferred candidate profile",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['aviation', 'Data Modeling', 'Python', 'OLAP', 'Cloud', 'ORC', 'logistics', 'Avro', 'Terraform', 'Snowflake', 'manufacturing', 'AWS', 'Parquet', 'Java', 'Azure', 'BigQuery', 'Data', 'Redshift', 'SQL', 'TimescaleDB', 'GCP', 'InfluxDB', 'dbt', 'Ansible', 'OLTP', 'Kubernetes']",2025-06-13 06:01:50
Senior Data Science Consultant - Optimization Projects,Leading Client,8 - 10 years,Not Disclosed,['Nagpur'],"Role Summary :\n\nWe are seeking a highly skilled Senior Data Science Consultant with 8+ years of experience to lead an internal optimization initiative.\n\nThe ideal candidate should have a strong background in data science, operations research, and mathematical optimization, with a proven track record of applying these skills to solve complex business problems.\n\nThis role requires a blend of technical depth, business acumen, and collaborative communication.\n\nA background in internal efficiency/operations improvement or cost/resource optimization projects is highly desirable.\n\nKey Responsibilities :\n\n- Lead and contribute to internal optimization-focused data science projects from design to deployment.\n\n- Develop and implement mathematical models to optimize resource allocation, process performance, and decision-making.\n\n- Use techniques such as linear programming, mixed-integer programming, heuristic and metaheuristic algorithms.\n\n- Collaborate with business stakeholders to gather requirements and translate them into data science use cases.\n\n- Build robust data pipelines and use statistical and machine learning methods to drive insights.\n\n- Communicate complex technical findings in a clear, concise manner to both technical and non-technical audiences.\n\n- Mentor junior team members and contribute to knowledge sharing and best practices within the team.\n\nRequired Skills And Qualifications :\n\n- Masters or PhD in Data Science, Computer Science, Operations Research, Applied Mathematics, or related fields.\n\n- Minimum 8 years of relevant experience in data science, with a strong focus on optimization.\n\n- Expertise in Python (NumPy, Pandas, SciPy, Scikit-learn), SQL, and optimization libraries such as PuLP, Pyomo, Gurobi, or CPLEX.\n\n- Experience with end-to-end lifecycle of internal optimization projects.\n\n- Strong analytical and problem-solving skills.\n\n- Excellent communication and stakeholder management abilities.\n\nPreferred Qualifications :\n\n- Experience working on internal company projects focused on logistics, resource planning, workforce optimization, or cost reduction.\n\n- Exposure to tools/platforms like Databricks, Azure ML, or AWS SageMaker.\n\n- Familiarity with dashboards and visualization tools like Power BI or Tableau.\n\n- Prior experience in consulting or internal centers of excellence (CoE) is a plus.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Data Scientist', 'Operations Research', 'Pandas', 'Numpy', 'Scikit-Learn', 'Python', 'SQL']",2025-06-13 06:01:51
Senior Data Science Consultant - Optimization Projects,Leading Client,8 - 10 years,Not Disclosed,['Delhi / NCR'],"Role Summary :\n\nWe are seeking a highly skilled Senior Data Science Consultant with 8+ years of experience to lead an internal optimization initiative.\n\nThe ideal candidate should have a strong background in data science, operations research, and mathematical optimization, with a proven track record of applying these skills to solve complex business problems.\n\nThis role requires a blend of technical depth, business acumen, and collaborative communication.\n\nA background in internal efficiency/operations improvement or cost/resource optimization projects is highly desirable.\n\nKey Responsibilities :\n\n- Lead and contribute to internal optimization-focused data science projects from design to deployment.\n\n- Develop and implement mathematical models to optimize resource allocation, process performance, and decision-making.\n\n- Use techniques such as linear programming, mixed-integer programming, heuristic and metaheuristic algorithms.\n\n- Collaborate with business stakeholders to gather requirements and translate them into data science use cases.\n\n- Build robust data pipelines and use statistical and machine learning methods to drive insights.\n\n- Communicate complex technical findings in a clear, concise manner to both technical and non-technical audiences.\n\n- Mentor junior team members and contribute to knowledge sharing and best practices within the team.\n\nRequired Skills And Qualifications :\n\n- Masters or PhD in Data Science, Computer Science, Operations Research, Applied Mathematics, or related fields.\n\n- Minimum 8 years of relevant experience in data science, with a strong focus on optimization.\n\n- Expertise in Python (NumPy, Pandas, SciPy, Scikit-learn), SQL, and optimization libraries such as PuLP, Pyomo, Gurobi, or CPLEX.\n\n- Experience with end-to-end lifecycle of internal optimization projects.\n\n- Strong analytical and problem-solving skills.\n\n- Excellent communication and stakeholder management abilities.\n\nPreferred Qualifications :\n\n- Experience working on internal company projects focused on logistics, resource planning, workforce optimization, or cost reduction.\n\n- Exposure to tools/platforms like Databricks, Azure ML, or AWS SageMaker.\n\n- Familiarity with dashboards and visualization tools like Power BI or Tableau.\n\n- Prior experience in consulting or internal centers of excellence (CoE) is a plus.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Data Scientist', 'Operations Research', 'Pandas', 'Numpy', 'Scikit-Learn', 'Python', 'SQL']",2025-06-13 06:01:53
Senior Data Science Consultant - Optimization Projects,Leading Client,8 - 10 years,Not Disclosed,['Ahmedabad'],"Role Summary :\n\nWe are seeking a highly skilled Senior Data Science Consultant with 8+ years of experience to lead an internal optimization initiative.\n\nThe ideal candidate should have a strong background in data science, operations research, and mathematical optimization, with a proven track record of applying these skills to solve complex business problems.\n\nThis role requires a blend of technical depth, business acumen, and collaborative communication.\n\nA background in internal efficiency/operations improvement or cost/resource optimization projects is highly desirable.\n\nKey Responsibilities :\n\n- Lead and contribute to internal optimization-focused data science projects from design to deployment.\n\n- Develop and implement mathematical models to optimize resource allocation, process performance, and decision-making.\n\n- Use techniques such as linear programming, mixed-integer programming, heuristic and metaheuristic algorithms.\n\n- Collaborate with business stakeholders to gather requirements and translate them into data science use cases.\n\n- Build robust data pipelines and use statistical and machine learning methods to drive insights.\n\n- Communicate complex technical findings in a clear, concise manner to both technical and non-technical audiences.\n\n- Mentor junior team members and contribute to knowledge sharing and best practices within the team.\n\nRequired Skills And Qualifications :\n\n- Masters or PhD in Data Science, Computer Science, Operations Research, Applied Mathematics, or related fields.\n\n- Minimum 8 years of relevant experience in data science, with a strong focus on optimization.\n\n- Expertise in Python (NumPy, Pandas, SciPy, Scikit-learn), SQL, and optimization libraries such as PuLP, Pyomo, Gurobi, or CPLEX.\n\n- Experience with end-to-end lifecycle of internal optimization projects.\n\n- Strong analytical and problem-solving skills.\n\n- Excellent communication and stakeholder management abilities.\n\nPreferred Qualifications :\n\n- Experience working on internal company projects focused on logistics, resource planning, workforce optimization, or cost reduction.\n\n- Exposure to tools/platforms like Databricks, Azure ML, or AWS SageMaker.\n\n- Familiarity with dashboards and visualization tools like Power BI or Tableau.\n\n- Prior experience in consulting or internal centers of excellence (CoE) is a plus.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Data Scientist', 'Operations Research', 'Pandas', 'Numpy', 'Scikit-Learn', 'Python', 'SQL']",2025-06-13 06:01:54
Data Engineer,Talent Aspire,2 - 7 years,Not Disclosed,"['Chandigarh', 'Bengaluru']","As the Data Engineer, you will play a pivotal role in shaping our data infrastructure and\nexecuting against our strategy. You will ideate alongside engineering, data and our clients to\ndeploy data products with an innovative and meaningful impact to clients. You will design, build, and maintain scalable data pipelines and workflows on AWS. Additionally, your expertise in AI and machine learning will enhance our ability to deliver smarter, more predictive solutions.\n\nKey Responsibilities\nCollaborate with other engineers, customers to brainstorm and develop impactful data\nproducts tailored to our clients.\nLeverage AI and machine learning techniques to integrate intelligent features into our\nofferings.\nDevelop, and optimize end-to-end data pipelines on AWS\nFollow best practices in software architecture and development.\nImplement effective cost management and performance optimization strategies.\nDevelop and maintain systems using Python, SQL, PySpark, and Django for front-end\ndevelopment.\nWork directly with clients and end-users and address their data needs\nUtilize databases and tools including and not limited to, Postgres, Redshift, Airflow, and\nMongoDB to support our data ecosystem.\nLeverage AI frameworks and libraries to integrate advanced analytics into our solutions.\nQualifications\n\nExperience:\nMinimum of 3 years of experience in data engineering, software development, or\nrelated roles.\nProven track record in designing and deploying AWS cloud infrastructure\nsolutions\nAt least 2 years in data analysis and mining techniques to aid in descriptive and\ndiagnostic insights\nExtensive hands-on experience with Postgres, Redshift, Airflow, MongoDB, and\nreal-time data workflows.\n\nTechnical Skills:\nExpertise in Python, SQL, and PySpark\nStrong background in software architecture and scalable development practices.\nTableau, Metabase or similar viz tools experience\nWorking knowledge of AI frameworks and libraries is a plus.\nLeadership & Communication:\nDemonstrates ownership and accountability for delivery with a strong\ncommitment to quality.\nExcellent communication skills with a history of effective client and end-user\nengagement.\nStartup & Fintech Mindset:\nAdaptability and agility to thrive in a fast-paced, early-stage startup environment.\nPassion for fintech innovation and a strong desire to make a meaningful impact\non the future of finance.",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Engineering', 'performance optimization strategies', 'PySpark', 'Django', 'cost management', 'AWS', 'AI frameworks', 'Python', 'SQL']",2025-06-13 06:01:56
Lead Data Analyst - Power BI,Bot Consulting,5 - 8 years,Not Disclosed,['Jaipur'],"We are seeking an experienced and proactive Lead Data Analyst \\u2013 Power BI to lead the development of scalable analytics solutions and guide our growing data team in Jaipur. The ideal candidate will bring strong expertise in Power BI, SQL, Python, and experience with cloud data platforms such as Snowflake. You will be responsible for designing data models, leading dashboard/reporting initiatives, mentoring junior analysts, and enabling business stakeholders to make data-driven decisions.\nRoles & Responsibilities:\nLead the development and enhancement of interactive Power BI dashboards, reports, and data visualizations tailored to business requirements.\nArchitect and optimize data models in Power BI for performance and scalability (including DAX and Power Query transformations).\nBuild and manage robust end-to-end data pipelines, including data extraction, transformation, and loading (ETL/ELT).\nCollaborate with cross-functional stakeholders to translate business needs into technical solutions and actionable insights.\nPerform advanced data analysis using SQL and Python to uncover trends, patterns, and opportunities.\nEnsure data governance, quality, and consistency across all reporting assets and data platforms.\nMentor junior analysts and contribute to best practices in reporting, documentation, and code review.\nAct as a bridge between business and engineering teams, ensuring alignment and impact from analytics projects.\nWork with cloud data warehouses such as Snowflake or similar platforms for scalable analytics.\nSkills & Qualifications\n6+ years of experience in Data Analytics, Business Intelligence, or Data Engineering roles.\nProven expertise in Power BI, including dashboard development, DAX, data modeling, and Power Query.\nAdvanced proficiency in SQL and ability to work with large, complex datasets.\nProgramming experience in Python for data manipulation, automation, or machine learning (preferred).\nStrong understanding of ETL/ELT concepts, data warehousing, and modern cloud data platforms (Snowflake preferred).\nBachelors or Masters degree in Computer Science, Data Science, Engineering, or a related field.\nExcellent analytical thinking, problem-solving, and attention to detail.\nStrong communication skills and the ability to present data insights to non-technical stakeholders.\nPreferred Qualifications\nHands-on experience with Snowflake, Redshift, or BigQuery.\nFamiliarity with Airflow, DBT, or other orchestration tools.\nPower BI Certification (eg, PL-300: Microsoft Power BI Data Analyst).\nExperience with Agile methodologies and managing sprint-based BI deliverables.\nExposure to version control (Git) and CI/CD practices in data analytics projects.\nSigns You May Be a Great Fit\nImpact: Play a pivotal role in shaping a rapidly growing venture studio.\nCulture: Thrive in a collaborative, innovative environment that values creativity and ownership.\nGrowth: Access professional development opportunities and mentorship.\nBenefits: Competitive salary, health/we'llness packages, and flexible work options",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['IT services', 'Data analysis', 'Automation', 'Data modeling', 'Analytical', 'microsoft', 'Business intelligence', 'Analytics', 'SQL', 'Data extraction']",2025-06-13 06:01:57
nior Data Engineer,M360 Research,5 - 8 years,Not Disclosed,['Bengaluru'],"M3 Global Research, an M3 company, is seeking a Senior Data Engineer to join our data engineering team. This role will focus on building and maintaining robust data pipelines, working closely with stakeholders to ensure data solutions align with business objectives, and utilizing tools like Power BI for data visualization and reporting. The ideal candidate has strong analytical skills, a passion for data-driven decision-making, and excellent communication abilities to work effectively with stakeholders across the organization.\nEssential Duties and Responsibilities:\nDesign, develop, and maintain high-quality, secure data pipelines and processes to manage and transform data efficiently.\nLead the architecture and implementation of data models, schemas, and integrations that support business intelligence and reporting needs.\nCollaborate with cross-functional teams to understand data requirements and deliver optimal data solutions that align with business goals.\nMaintain and enhance data infrastructure, including data warehouses, lakes, and integration tools.\nProvide guidance on best practices for data management, security, and compliance.\nSupport Power BI and other visualization tools, ensuring consistent and reliable access to data insights.\nOversee the delivery of data initiatives, ensuring they meet project milestones, KPIs, and deadlines.\nEssential Job Functions:\nMaintain regular and punctual attendance.\nWork cooperatively and communicate effectively with team members and stakeholders.\nComply with all company policies and procedures.\nSupervisory Responsibility:\nYes\nOutcomes:\nDeliver high-quality, reliable data solutions.\nProvide stakeholders with clear and actionable insights through Power BI reports.\nEnsure data pipelines and ETL processes are optimized and running efficiently.\nFoster strong relationships with stakeholders to ensure their data needs are met.\nCompetencies:\nAttention to detail.\nAnalytical thinking and problem-solving skills.\nStrong communication and interpersonal skills to engage effectively with stakeholders.\nAbility to work in a fast-paced, agile environment.\nKnowledge and Skills:\nProficiency with data engineering tools and technologies (eg, SQL, Python, ETL tools).\nStrong experience with Power BI for data visualization and reporting.\nFamiliarity with cloud-based data platforms (eg, AWS, Azure, Google Cloud).\nExperience with data modeling, data warehousing, and designing scalable data architectures.\nStrong knowledge of database systems (eg, SQL Server, Oracle, PostgreSQL).\nExperience working in an Agile development environment.\nExcellent communication skills to work effectively with both technical and non-technical stakeholders.\nAbility to multi-task and manage multiple projects simultaneously.\nProblem-solving mindset with a desire to continuously improve data processes.\nMinimum Experience:\n5+ years of experience in data engineering or related fields.\n2+ years of experience with Power BI or similar data visualization tools.\nEducation and Training Required:\nbachelors degree in computer science, Data Science, or a related field, or equivalent experience",Industry Type: Analytics / KPO / Research,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data management', 'ISO', 'Analytical', 'Healthcare', 'Market research', 'Oracle', 'Business intelligence', 'Japanese', 'Recruitment', 'SQL']",2025-06-13 06:01:59
Data Engineer- MS Fabric,InfoCepts,5 - 10 years,Not Disclosed,"['Kolkata', 'Mumbai', 'New Delhi', 'Hyderabad', 'Pune', 'Chennai', 'Bengaluru']","Position: Data Engineer - MS Fabric\nPurpose of the Position: As an MS Fabric Data engineer you will be responsible for designing, implementing, and managing scalable data pipelines. Strong experience in implementation and management of lake House using MS Fabric Azure Tech stack (ADLS Gen2, ADF, Azure SQL) .\nProficiency in data integration techniques, ETL processes and data pipeline architectures. Well versed in Data Quality rules, principles and implementation.\nLocation: Bangalore/ Pune/ Nagpur/ Chennai\nType of Employment: FTE",,,,"['Performance tuning', 'Data modeling', 'Coding', 'XML', 'Scheduling', 'Data quality', 'JSON', 'Business intelligence', 'Data architecture', 'Python']",2025-06-13 06:02:01
Data Engineer,Clifyx Technology,5 - 10 years,Not Disclosed,['Bengaluru'],"2\nData Engineer\nAzure Synapse/ADF , Workiva\nTo manage and maintain the associated Connector, Chains, Tables and Queries, making updates, as needed, as new metrics or requirements are identified\nDevelop functional and technical requirements for any changes impacting wData (Workiva Data)\nConfigure and unit test any changes impacting wData (connector, chains, tables, queries\nPromote wData changes",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['hive', 'python', 'oracle', 'azure data lake', 'informatica powercenter', 'azure synapse', 'microsoft azure', 'data warehousing', 'power bi', 'azure data factory', 'data engineering', 'sql server', 'sql', 'plsql', 'sql azure', 'spark', 'oracle adf', 'hadoop', 'etl', 'ssis', 'big data', 'informatica', 'unix']",2025-06-13 06:02:02
Mid-Level Data Engineer,BMW Techworks India,6 - 8 years,Not Disclosed,"['Chennai', 'Bengaluru']","What awaits you/ Job Profile\nProvide estimates for requirements, analyses and develop as per the requirement.\nDeveloping and maintaining data pipelines and ETL (Extract, Transform, Load) processes to extract data efficiently and reliably from various sources, transform it into a usable format, and load it into the appropriate data repositories.\nCreating and maintaining logical and physical data models that align with the organizations data architecture and business needs. This includes defining data schemas, tables, relationships, and indexing strategies for optimal data retrieval and analysis.\nCollaborating with cross-functional teams and stakeholders to ensure data security, privacy, and compliance with regulations.\nCollaborate with downstream application to understand their needs and build the data storage and optimize as per their need.\nWorking closely with other stakeholders and Business to understand data requirements and translate them into technical solutions.\nFamiliar with Agile methodologies and have prior experience working with Agile teams using Scrum/Kanban\nLead Technical discussions with customers to find the best possible solutions.\nProactively identify and implement opportunities to automate tasks and develop reusable frameworks.\nOptimizing data pipelines to improve performance and cost, while ensuring a high quality of data within the data lake.\nMonitoring services and jobs for cost and performance, ensuring continual operations of data pipelines, and fixing of defects.\nConstantly looking for opportunities to optimize data pipelines to improve performance\nWhat should you bring along\nMust Have:\nHand on Expertise of 6- 8 years in AWS services like S3, Lambda, Glue, Athena, RDS, Step functions, SNS, SQS, API Gateway, Security, Access and Role permissions, Logging and monitoring Services.\nGood hand on knowledge on Python, Spark, Hive and Unix, AWS CLI\nPrior experience in working with streaming solution like Kafka\nPrior experience in implementing different file storage types like Delta-lake / Ice-berg.\nExcellent knowledge in Data modeling and Designing ETL pipeline.\nMust have strong knowledge in using different databases such as MySQL, Oracle and Writing complex queries.\nStrong experience working in a continuous integration and Deployment process.\nNice to Have:\nHand on experience in the Terraform, GIT, GIT Actions. CICD pipeline and Amazon Q.\nMust have technical skill\nPyspark, AWS ,SQL, Kafka, Glue, IAM. S3, Lambda, Step Function, Athena\nGood to have Technical skills\nTerraform, GIT, GIT Actions. CICD pipeline , AI",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Unix', 'Data modeling', 'data security', 'MySQL', 'Agile', 'Scrum', 'Oracle', 'Monitoring', 'SQL', 'Python']",2025-06-13 06:02:04
Databricks Data Engineer,Mumba Technologies,6 - 10 years,22.5-25 Lacs P.A.,['Bengaluru'],"Mandatory Skills & Experience:\n6 to 8 years of experience in data engineering, with strong experience in Oracle\nDWH/ODS environments.\nMinimum 3+ years hands-on experience in Databricks (including PySpark, SQL,\nDelta Lake, Workflows).\nStrong understanding of Lakehouse architecture, cloud data platforms, and big\ndata processing.\n\nProven experience in migrating data warehouse and ETL workloads from Oracle to\ncloud platforms.\nExperience with PL/SQL, query tuning, and reverse engineering legacy systems.\nExposure to Pentaho and/or TIBCO Data Virtualization/Integration tools.\nExperience with CI/CD pipelines, version control (e.g., Git), and automated\ntesting.\nFamiliarity with data governance, security policies, and compliance in cloud\nenvironments.\nStrong communication and documentation skills.\n\nPreferred Skills (Advantage):\nExperience in cloud migration projects (AWS/Azure).\nKnowledge of Delta Lake, Unity Catalog, and Databricks workflows.\nExposure to Kafka for real-time data streaming.\nExperience with ETL tools like Pentaho or Tibco will be an added advantage.\nAWS/Azure/Databricks certifications\nTools & Technologies:\nDatabricks, Oracle, Hadoop (HDFS, Hive, Sqoop), AWS (S3, EMR, Glue, Lamda, RDS)\nPySpark, SQL, Python, Kafka\nCI/CD (Jenkins, GitHub Actions), Orchestration (Airflow, Control-M)\nJIRA, Confluence, Git (GitHub/Bitbucket)\nCloud Certifications (Preferred):\nDatabricks Certified Data Engineer\nAWS Certified Solutions Architect/Developer",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'Delta Lake', 'Databricks', 'Oracle DWH', 'Data warehouse', 'SQL']",2025-06-13 06:02:05
Data Engineer,Orangemint Technologies,3 - 5 years,20-25 Lacs P.A.,['Bengaluru'],"Company name: PulseData labs Pvt Ltd (captive Unit for URUS, USA)\nAbout URUS We are the URUS family (US), a global leader in products and services for Agritech.\nSENIOR DATA ENGINEER This role is responsible for the design, development, and maintenance of data integration and reporting solutions. The ideal candidate will possess expertise in Databricks and strong skills in SQL Server, SSIS and SSRS, and experience with other modern data engineering tools such as Azure Data Factory. This position requires a proactive and results-oriented individual with a passion for data and a strong understanding of data warehousing principles.\nResponsibilities Data Integration\nDesign, develop, and maintain robust and efficient ETL pipelines and processes on Databricks.\nTroubleshoot and resolve Databricks pipeline errors and performance issues.\nMaintain legacy SSIS packages for ETL processes.\nTroubleshoot and resolve SSIS package errors and performance issues.\nOptimize data flow performance and minimize data latency.\nImplement data quality checks and validations within ETL processes.\nDatabricks Development\nDevelop and maintain Databricks pipelines and datasets using Python, Spark and SQL.\nMigrate legacy SSIS packages to Databricks pipelines.\nOptimize Databricks jobs for performance and cost-effectiveness.\nIntegrate Databricks with other data sources and systems.\nParticipate in the design and implementation of data lake architectures.\nData Warehousing\nParticipate in the design and implementation of data warehousing solutions.\nSupport data quality initiatives and implement data cleansing procedures.\nReporting and Analytics\nCollaborate with business users to understand data requirements for department driven reporting needs.\nMaintain existing library of complex SSRS reports, dashboards, and visualizations.\nTroubleshoot and resolve SSRS report issues, including performance bottlenecks and data inconsistencies.\nCollaboration and Communication\nComfortable in entrepreneurial, self-starting, and fast-paced environment, working both independently and with our highly skilled teams.\nCollaborate effectively with business users, data analysts, and other IT teams.\nCommunicate technical information clearly and concisely, both verbally and in writing.\nDocument all development work and procedures thoroughly.\nContinuous Growth\nKeep abreast of the latest advancements in data integration, reporting, and data engineering technologies.\nContinuously improve skills and knowledge through training and self-learning.\nThis job description reflects managements assignment of essential functions; it does not prescribe or restrict the tasks that may be assigned.\nRequirements\nBachelor's degree in computer science, Information Systems, or a related field.\n2+ years of experience in data integration and reporting.\nExtensive experience with Databricks, including Python, Spark, and Delta Lake.\nStrong proficiency in SQL Server, including T-SQL, stored procedures, and functions.\nExperience with SSIS (SQL Server Integration Services) development and maintenance.\nExperience with SSRS (SQL Server Reporting Services) report design and development.\nExperience with data warehousing concepts and best practices.\nExperience with Microsoft Azure cloud platform and Microsoft Fabric desirable.\nStrong analytical and problem-solving skills.\nExcellent communication and interpersonal skills.\nAbility to work independently and as part of a team.\nExperience with Agile methodologies.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Etl Development', 'Azure Databricks', 'Databricks Engineer', 'Spark', 'SQL Server', 'Data Warehousing', 'Pythonspark']",2025-06-13 06:02:07
Data Engineer-Pyspark,A leading technology services and consul...,5 - 10 years,Not Disclosed,"['Pune', 'Chennai', 'Bengaluru']","Our client is Global IT Service & Consulting Organization\nExp-5+ yrs\n\nSkil Apache Spark\n\nLocation- Bangalore, Hyderabad, Pune, Chennai, Coimbatore, Gr. Noida\n\n\nExcellent Knowledge on Spark; The professional must have a thorough understanding Spark framework, Performance Tuning etc\nExcellent Knowledge and hands-on experience of at least 4+ years in Scala or PySpark\nExcellent Knowledge of the Hadoop eco System- Knowledge of Hive mandatory\nStrong Unix and Shell Scripting Skills\nExcellent Inter-personal skills and for experienced candidates Excellent leadership skills\nMandatory for anyone to have Good knowledge of any of the CSPs like Azure,AWS or GCP; Certifications on Azure will be additional Pl",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Pyspark', 'Data Engineering', 'Cloud', 'Microsoft Azure', 'Python', 'GCP', 'data engineer', 'Hadoop', 'AWS']",2025-06-13 06:02:08
Data Engineer,Axis Finance (AFL),7 - 11 years,Not Disclosed,"['Mumbai', 'Mumbai (All Areas)']","Key Responsibilities:\nShould have experience in below\nDesign, develop, and implement a Data Lake House architecture on AWS, ensuring scalability, flexibility, and performance.\nBuild ETL/ELT pipelines for ingesting, transforming, and processing structured and unstructured data.\nCollaborate with cross-functional teams to gather data requirements and deliver data solutions aligned with business needs.\nDevelop and manage data models, schemas, and data lakes for analytics, reporting, and BI purposes.\nImplement data governance practices, ensuring data quality, security, and compliance.\nPerform data integration between on-premise and cloud systems using AWS services.\nMonitor and troubleshoot data pipelines and infrastructure for reliability and scalability.\nSkills and Qualifications:\n7 + years of experience in data engineering, with a focus on cloud data platforms.\nStrong experience with AWS services: S3, Glue, Redshift, Athena, Lambda, IAM, RDS, and EC2.\nHands-on experience in building data lakes, data warehouses, and lake house architectures.\nShould have experience in ETL/ELT pipelines using tools like AWS Glue, Apache Spark, or similar.\nExpertise in SQL and Python or Java for data processing and transformations.\nFamiliarity with data modeling and schema design in cloud environments.\nUnderstanding of data security and governance practices, including IAM policies and data encryption.\nExperience with big data technologies (e.g., Hadoop, Spark) and data streaming services (e.g., Kinesis, Kafka).\nHave lending domain knowledge will be added advantage\nPreferred Skills:\nExperience with Databricks or similar platforms for data engineering.\nFamiliarity with DevOps practices for deploying data solutions on AWS (CI/CD pipelines).\nKnowledge of API integration and cloud data migration strategies.",Industry Type: NBFC,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['pipeline tools', 'lending domain', 'AWS', 'data models', 'spark', 'devops', 'databricks', 'date engineering platforms', 'hadoop', 'data lake house', 'API integration']",2025-06-13 06:02:10
Data Streaming Engineer,Data Streaming Engineer,5 - 10 years,Not Disclosed,"['Pune', 'Chennai', 'Bengaluru']","Hello Candidates,\n\nWe are Hiring !!\n\nJob Position - Data Streaming Engineer\nExperience - 5+ years\nLocation - Mumbai, Pune , Chennai , Bangalore\nWork mode - Hybrid ( 3 days WFO)\n\nJOB DESCRIPTION\n\nRequest for Data Streaming Engineer Data Streaming @ offshore :\n• Flink , Python Language.\n• Data Lake Systems. (OLAP Systems).\n• SQL (should be able to write complex SQL Queries)\n• Orchestration (Apache Airflow is preferred).\n• Hadoop (Spark and Hive: Optimization of Spark and Hive apps).\n• Snowflake (good to have).\n• Data Quality (good to have).\n• File Storage (S3 is good to have)\n\nNOTE - Candidates can share their resume on - shrutia.talentsketchers@gmail.com",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Flink', 'Apache Airflow', 'Data Quality', 'Hadoop', 'Snowflake', 'Data Lake', 'orchastration', 'Python', 'SQL']",2025-06-13 06:02:11
Data Engineer,Supersourcing,4 - 8 years,Not Disclosed,"['Noida', 'Pune', 'Bengaluru']","Job Description-\nWe are hiring a Data Engineer with strong expertise in JAVA, Apache Spark and AWS Cloud. You will design and develop high-performance, scalable applications and data pipelines for cloud-based environments.\n\nKey Skills:\n3+ years in Java (Java 8+), Spring Boot, and REST APIs\n3+ years in Apache Spark (Core, SQL, Streaming)\nStrong hands-on with AWS services: S3, EC2, Lambda, Glue, EMR\nExperience with microservices, CI/CD, and Git\nGood understanding of distributed systems and performance tuning\n\nNice to Have:\nExperience with Kafka, Airflow, Docker, or Kubernetes\nAWS Certification (Developer/Architect)",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Java', 'Spark', 'Pyspark', 'SQL']",2025-06-13 06:02:13
Data Engineer (5-10 Years) | @ Banking | Bangalore & Mumbai,Net Connect,5 - 10 years,Not Disclosed,"['Bengaluru', 'Mumbai (All Areas)']","Job Summary\nWe are seeking a skilled Data Engineer to design, develop, and optimize scalable data pipelines and infrastructure. The ideal candidate will have expertise in relational databases, data modeling, cloud migration, and automation, working closely with cross-functional teams to drive data-driven decision-making.\n\nKey Responsibilities",,,,"['Data Modeling', 'ETL', 'Scripting', 'SQL', 'Azure Data Factory', 'Informatica']",2025-06-13 06:02:14
Data Science Analyst,Cigna Medical Group,2 - 4 years,Not Disclosed,['Bengaluru'],"Key responsibilities\nDeliver quality analytics, from data preparation, data analysis, data exploration, data quality assessment, data manipulation, method selection, design & application, insights generation and visualisation\nDevelop and implement basic machine learning models and algorithms under the guidance of senior data scientists to extract insights and solve business problems\nProactive learning and acquisition of key analytical, technical and commercial skills and business knowledge to become a proficient Analyst working under the supervision of the senior/lead data science analysts.\nKPIs: Timeliness, accuracy, manager and client feedback (Internal and external as required)\nCollaborate with internal stakeholders and demonstrate the ability to transform client questions and problems into analytical solutions\nActive team member in providing the required support to help business understand and optimise use of analytical products and / or solutions\nBuild industry knowledge on the advancements in the field of analytics, data science and GenAI\nComply with the IM Cigna and CHSI Policies, procedures and processes, and continuously demonstrate Cigna Data and Analytics culture.\nKey activities\nWorking in a team to support end-to-end analytical projects\nLiaising with stakeholders to determine objectives / scope of upcoming projects\nData exploration, cleansing and manipulation\nDetermining appropriate type of analysis and undertaking analysis/modelling\nExtracting insights\nClear presentation of insights via spreadsheets, PowerPoint presentations, self-service analytical visualisation tools\nParticipate in client meetings\nOngoing stakeholder interaction (internal and external as required) on project progress\nContribute to the Feedback process (between stakeholders and the team) to ensure continuous improvement with team\nParticipate and contribute in learning forums such as Analytics Community and sharing knowledge with wider team\nExperience and education required\n2-4+ years experience in a technical analytics environment, carrying out data analytics and data science/AI projects and initiatives\nTertiary qualifications in engineering, mathematics, actuarial studies, statistics, physics, or a related discipline\nKnowledge of technical analytics discipline, including data preparation and foundational analytics concepts\nExperience with successfully managing both internal and external stakeholders, delivering against projects, tasks and activities in a dynamic deadline driven environment\nCommercial acumen to understand business needs and be able to suggest the commercial impacts of different analytics solutions or approaches\nCoding and modelling experience in SQL / R / Python and / or Cloud data platforms e.g. AWS\nExperience in visualization and data management tools is an added advantage\nExperience in GenAI/ LLMs is an added advantage\nExperience working with complex datasets\nAttention to detail and self driven continuous learning\nParticipation in external data hackathons and competitions will be an added advantage",Industry Type: Medical Services / Hospital,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data analysis', 'Data management', 'Coding', 'Analytical', 'Healthcare', 'Actuarial', 'Data quality', 'Continuous improvement', 'SQL', 'Python']",2025-06-13 06:02:16
Sr. Data Science,Disa Consulting Services,7 - 9 years,Not Disclosed,"['Hyderabad', 'Chennai', 'Bengaluru']","Title: SR DATA SCIENCE\nExperience: 7 - 9 Years\nNotice Period: Immediate to 15 days\nWork Mode: Remote, Hybrid\nLocation: Hyderabad & Chennai\n\nRequirements:\nExperience designing and developing automated pipelines that utilize some combination of RAG pipelines, automated generation of prompts to LLMs, and/or multi-agent Agentic inference engines.\nAll skills / experience of above listed data scientist roles plus:\nArchitected and designed end to end pipelines that utilize LLMs and Autonomous Agents.\nDesigned and implemented methods for assuring quality and governing the output of Gen AI or Agentic solutions.\nDesigned and developed APIs exposing services of services that consume LLMs.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Restfull Api', 'Azure Cloud', 'LLM', 'Machine Learning', 'SQL', 'Python']",2025-06-13 06:02:18
Data Quality Analyst,Yallas Technology Solutions Opc,5 - 10 years,Not Disclosed,[],"Title: Data Quality Analyst/Developer\nDuration: 6 months to 1 year contract\nLocation:  Remote\nNotice period - Immediate to 7 days\nUAN /EPFO Report Required\n\nWork Experience:\n5 + years of this experience - Experience doing Data Emendation\nDesign/Develop Rules, monitoring mechanisms, notification\nDesign/Develop UI, Workflows, security\nDesign/Develop analytics (overall DQ reporting, usage statistics, etc).\nDesign/Develop migration activities to migrate existing DQ assets between our existing DQ platform and new DQ platform.\nDesign integration with MDM & Catalog (as needed)\nMonitor system performance and suggest optimization strategies (as needed).\nWork with DT to maintain system - patches, backups, etc.\nWork with LYB's Data Stewards to support their governance activities.\nTesting\n\nThe DQ Analyst/Developer should have experience with IMDC (for the sake of our example) cloud DQ and observability, JSON (depending on tool) Deep SQL skills, Integration tools/methodologies - API as well as ETL, Data Analysis, Snowflake or Databricks knowledge (for lineage), Power BI (nice to have), SAP ECC knowledge (nice to have), experience with cloud platforms (Azure, AWS, Google).\nIf you are interested please share required details along with resume\nFull Name:\nCurrent or Previous organization:\nCurrent Location:\nTotal Experience:\nRelevant experience as Python Developer:\nhow many years of experience In Azure, AWS, Google\nHow many years of experience in UI, Workflows, security\nWorking as full time or contract:\nReason for job change:\nAny other offers inhand:\nCurrent CTC:\nexpected CTC:\nNotice Period:\nemail id:\ncontact Number :\nDomain name:\nare you ok to work Cotractual role?:\nshare your aadhar or pan card for the verification",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Temporary/Contractual","['Data quality analyst', 'cloud data quality', 'Azure', 'data quality developer', 'JSON', 'google', 'Informatica', 'AWS']",2025-06-13 06:02:19
Senior Manager Marketing Data Analytics,Factspan Analytics,9 - 14 years,Not Disclosed,['Bengaluru'],"Position: Senior Manager marketing Analytics\nBengaluru, Karnataka\n\nFactspan Overview: Factspan is a pure play data and analytics services organization. We partner with fortune 500 enterprises to build an analytics center of excellence, generating insights and solutions from raw data to solve business challenges, make strategic recommendations and implement new processes that help them succeed. With offices in Seattle, Washington and Bengaluru, India; we use a global delivery model to service our customers. Our customers include industry leaders from Retail, Financial Services, Hospitality, and technology sectors.",,,,"['Market Mix Modelling', 'People Management Skills', 'Marketing Analytics', 'Mmm', 'Stakeholder Management', 'Delivery Management']",2025-06-13 06:02:21
Data Product Owner,Capgemini,9 - 14 years,Not Disclosed,['Hyderabad'],"\n\nThe Product Owner III will be responsible for defining and prioritizing features and user stories, outlining acceptance criteria, and collaborating with cross-functional teams to ensure successful delivery of product increments. This role requires strong communication skills to effectively engage with stakeholders, gather requirements, and facilitate product demos.\n\nThe ideal candidate should have a deep understanding of agile methodologies, experience in the insurance sector, and possess the ability to translate complex needs into actionable tasks for the development team.\n\n Key Responsibilities: \nDefine and communicate the  vision, roadmap, and backlog  for data products.\nManages team backlog items and prioritizes based on business value.\nPartners with the business owner to understand needs, manage scope and add/eliminate user stories while contributing heavy influence to build an effective strategy.\nTranslate business requirements into scalable data product features.\nCollaborate with data engineers, analysts, and business stakeholders to prioritize and deliver impactful solutions.\nChampion  data governance , privacy, and compliance best practices.\nAct as the voice of the customer to ensure usability and adoption of data products.\nLead Agile ceremonies (e.g., backlog grooming, sprint planning, demos) and maintain a clear product backlog.\nMonitor data product performance and continuously identify areas for improvement.\nSupport the integration of AI/ML solutions and advanced analytics into product offerings.\n\n\n  \n\n Required Skills & Experience: \nProven experience as a Product Owner, ideally in data or analytics domains.\nStrong understanding of  data engineering ,  data architecture , and  cloud platforms  (AWS, Azure, GCP).\nFamiliarity with  SQL , data modeling, and modern data stack tools (e.g., Snowflake, dbt, Airflow).\nExcellent  stakeholder management  and communication skills across technical and non-technical teams.\nStrong  business acumen  and ability to align data products with strategic goals.\nExperience with  Agile/Scrum methodologies  and working in cross-functional teams.\nAbility to  translate data insights into compelling stories and recommendations .\n\n\n",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data architecture', 'business acumen', 'data engineering', 'stakeholder management', 'agile methodology', 'snowflake', 'advanced analytics', 'microsoft azure', 'cloud platforms', 'user stories', 'demo', 'sql', 'data modeling', 'gcp', 'sprint planning', 'scrum', 'product performance', 'agile', 'aws', 'ml']",2025-06-13 06:02:23
S&C Global Network - AI - Life Sciences -Data Science Consultant,Accenture,4 - 9 years,Not Disclosed,['Gurugram'],"Job Title -\n\n\n\nS&C Global Network - AI - Healthcare Analytics - Consultant\n\n\n\nManagement Level:\n\n\n\n9-Team Lead/Consultant\n\n\n\nLocation:\n\n\n\nBangalore/Gurgaon\n\n\n\nMust-have skills:R,Phython,SQL,Spark,Tableau ,Power BI\n\n\n\n\nGood to have skills:Ability to leverage design thinking, business process optimization, and stakeholder management skills.\n\n\n\nJob\n\n\nSummary:\n\nThis role involves driving strategic initiatives, managing business transformations, and leveraging industry expertise to create value-driven solutions.\n\n\n\n\nRoles & Responsibilities:\n\nProvide strategic advisory services, conduct market research, and develop data-driven recommendations to enhance business performance.\n\n\n\nWHATS IN IT FOR YOU\nAn opportunity to work on high-visibility projects with top Pharma clients around the globe.\nPotential to Co-create with leaders in strategy, industry experts, enterprise function practitioners, and business intelligence professionals to shape and recommend innovative solutions that leverage emerging technologies.\nAbility to embed responsible business into everythingfrom how you service your clients to how you operate as a responsible professional.\nPersonalized training modules to develop your strategy & consulting acumen to grow your skills, industry knowledge, and capabilities.\nOpportunity to thrive in a culture that is committed to accelerating equality for all. Engage in boundaryless collaboration across the entire organization.\n\n\n\n\nWhat you would do in this role\nSupport delivery of small to medium-sized teams to deliver consulting projects for global clients.\nResponsibilities may include strategy, implementation, process design, and change management for specific modules.\nWork with the team or as an Individual contributor on the project assigned which includes a variety of skills to be utilized from Data Engineering to Data Science\nProvide Subject matter expertise in various sub-segments of the LS industry.\nDevelop assets and methodologies, point-of-view, research, or white papers for use by the team and the larger community.\nAcquire new skills that have utility across industry groups.\nSupport strategies and operating models focused on some business units and assess likely competitive responses. Also, assess implementation readiness and points of greatest impact.\nCo-lead proposals, and business development efforts and coordinate with other colleagues to create consensus-driven deliverables.\nExecute a transformational change plan aligned with the clients business strategy and context for change. Engage stakeholders in the change journey and build commitment to change.\nMake presentations wherever required to a known audience or client on functional aspects of his or her domain.\n\n\n\nWho are we looking for\nBachelors or Masters degree in Statistics, Data Science, Applied Mathematics, Business Analytics, Computer Science, Information Systems, or other Quantitative field.\nProven experience (4+ years) in working on Life Sciences/Pharma/Healthcare projects and delivering successful outcomes.\nExcellent understanding of Pharma data sets commercial, clinical, RWE (Real World Evidence) & EMR (Electronic medical records)\nLeverage ones hands on experience of working across one or more of these areas such as real-world evidence data, R&D clinical data, digital marketing data.\nHands-on experience with handling Datasets like Komodo, RAVE, IQVIA, Truven, Optum etc.\nHands-on experience in building and deployment of Statistical Models/Machine Learning including Segmentation & predictive modeling, hypothesis testing, multivariate statistical analysis, time series techniques, and optimization.\nProficiency in Programming languages such as R, Python, SQL, Spark, etc.\nAbility to work with large data sets and present findings/insights to key stakeholders; Data management using databases like SQL.\nExperience with any of the cloud platforms like AWS, Azure, or Google Cloud for deploying and scaling language models.\nExperience with any of the Data Visualization tools like Tableau, Power BI, Qlikview, Spotfire is good to have.\nExcellent analytical and problem-solving skills, with a data-driven mindset.\nProficient in Excel, MS Word, PowerPoint, etc.\nAbility to solve complex business problems and deliver client delight.\nStrong writing skills to build points of view on current industry trends.\nGood communication, interpersonal, and presentation skills\n\n\n\n\n\nProfessional & Technical\n\n\n\n\nSkills:\n\n\n- Relevant experience in the required domain.\n\n- Strong analytical, problem-solving, and communication skills.\n\n- Ability to work in a fast-paced, dynamic environment.\n\n\n\n\nAdditional Information:\n\n- Opportunity to work on innovative projects.\n\n- Career growth and leadership exposure.\n\n\n\n\n\nAbout Our Company | Accenture\nQualification\n\n\n\nExperience:\n\n\n\n4-8 Years\n\n\n\n\nEducational Qualification:\n\n\n\nBachelors or Masters degree in Statistics, Data Science, Applied Mathematics, Business Analytics, Computer Science, Information Systems, or other Quantitative field.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'sql', 'tableau', 'r', 'spark', 'spotfire', 'power bi', 'microsoft azure', 'time series', 'emr', 'machine learning', 'data engineering', 'artificial intelligence', 'qlikview', 'data science', 'gcp', 'predictive modeling', 'segmentation', 'life sciences', 'data visualization', 'aws', 'statistics']",2025-06-13 06:02:24
S&C GN - Data&AI - Life Sciences - Consultant,Accenture,4 - 9 years,Not Disclosed,['Gurugram'],"Management Level:Ind&Func AI Decision Science Consultant\n\n\n\n\nJob Location:Bangalore / Gurgaon\n\n\n\nMust-have\n\n\n\n\nSkills:\nExcellent understanding of Pharma data sets commercial, clinical, Leverage ones hands on experience of working across one or more of these areas such as real-world evidence data, Statistical Models/Machine Learning including Segmentation & predictive modeling, hypothesis testing, multivariate statistical analysis, time series techniques, and optimization.\n\n\n\nGood-to-have\n\n\n\n\nSkills:\nProgramming languages such as R, Python, SQL, Spark, AWS, Azure, or Google Cloud for deploying and scaling language models, Data Visualization tools like Tableau, Power BI.\n\n\n\n\nJob\n\n\nSummary\n\nThis role involves driving strategic initiatives, managing business transformations, and leveraging industry expertise to create value-driven solutions. Provide strategic advisory services, conduct market research, and develop data-driven recommendations to enhance business performance.\n\n\n\nKey Responsibilities\nAn opportunity to work on high-visibility projects with top Pharma clients around the globe.\nPersonalized training modules to develop your strategy & consulting acumen to grow your skills, industry knowledge, and capabilities.\nProvide Subject matter expertise in various sub-segments of the LS industry.\nSupport delivery of small to medium-sized teams to deliver consulting projects for global clients.\nResponsibilities may include strategy, implementation, process design, and change management for specific modules.\nWork with the team or as an Individual contributor on the project assigned which includes a variety of skills to be utilized from Data Engineering to Data Science\nDevelop assets and methodologies, point-of-view, research, or white papers for use by the team and the larger community.\nAcquire new skills that have utility across industry groups.\nSupport strategies and operating models focused on some business units and assess likely competitive responses. Also, assess implementation readiness and points of greatest impact.\n\n\n\n\n\nAdditional Information\nProficient in Excel, MS Word, PowerPoint, etc.\nAbility to solve complex business problems and deliver client delight.\nStrong writing skills to build points of view on current industry trends.\nGood communication, interpersonal, and presentation skills\n\n\nAbout Our Company | Accenture (do not remove the hyperlink)\n\nQualification\n\n\n\nExperience:Proven experience (4+ years) in working on Life Sciences/Pharma/Healthcare projects and delivering successful outcomes.\n\n\n\n\nEducational Qualification:Bachelors or Masters degree in Statistics, Data Science, Applied Mathematics, Business Analytics, Computer Science, Information Systems, or other Quantitative field.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['pharmaceutical', 'python', 'sql', 'life sciences', 'aws', 'microsoft azure', 'power bi', 'time series', 'machine learning', 'data engineering', 'artificial intelligence', 'tableau', 'r', 'data science', 'gcp', 'spark', 'predictive modeling', 'statistical modeling', 'data visualization', 'statistics']",2025-06-13 06:02:26
"Senior data engineer - Python, Pyspark, AWS - 5+ years Gurgaon",One of the largest insurance providers.,5 - 10 years,Not Disclosed,['Gurugram'],"Senior data engineer - Python, Pyspark, AWS - 5+ years Gurgaon\n\nSummary: An excellent opportunity for someone having a minimum of five years of experience with expertise in building data pipelines. A person must have experience in Python, Pyspark and AWS.\n\nLocation- Gurgaon (Hybrid)\n\nYour Future Employer- One of the largest insurance providers.\n\nResponsibilities-\nTo design, develop, and maintain large-scale data pipelines that can handle large datasets from multiple sources.\nReal-time data replication and batch processing of data using distributed computing platforms like Spark, Kafka, etc.\nTo optimize the performance of data processing jobs and ensure system scalability and reliability.\nTo collaborate with DevOps teams to manage infrastructure, including cloud environments like AWS.\nTo collaborate with data scientists, analysts, and business stakeholders to develop tools and platforms that enable advanced analytics and reporting.\n\nRequirements-\nHands-on experience with AWS services such as S3, DMS, Lambda, EMR, Glue, Redshift, RDS (Postgres) Athena, Kinesics, etc.\nExpertise in data modeling and knowledge of modern file and table formats.\nProficiency in programming languages such as Python, PySpark, and SQL/PLSQL for implementing data pipelines and ETL processes.\nExperience data architecting or deploying Cloud/Virtualization solutions (Like Data Lake, EDW, Mart ) in the enterprise.\nCloud/hybrid cloud (preferably AWS) solution for data strategy for Data lake, BI and Analytics.\nWhat is in for you-\nA stimulating working environment with equal employment opportunities.\nGrowing of skills while working with industry leaders and top brands.\nA meritocratic culture with great career progression.\n\nReach us- If you feel that you are the right fit for the role please share your updated CV at randhawa.harmeen@crescendogroup.in\n\nDisclaimer- Crescendo Global specializes in Senior to C-level niche recruitment. We are passionate about empowering job seekers and employers with an engaging memorable job search and leadership hiring experience. Crescendo Global does not discriminate on the basis of race, religion, color, origin, gender, sexual orientation, age, marital status, veteran status, or disability status.",Industry Type: Insurance,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Pyspark', 'Data Pipeline', 'AWS', 'Data Ingestion', 'Data Engineering', 'Data Processing']",2025-06-13 06:02:27
Senior AWS Data Engineer,Exavalu,7 - 12 years,Not Disclosed,[],"7-12 years of experience using AWS Data Landscape and data Ingestion pipeline.\nAble to understand and explain the data ingestion from different sources like file, database, applications etc\nBuild and enhance the Python, PySpark Based Framework for ingestion.Data engineering experience in AWS Data Services like Glue, EMR, Airflow, CloudWatch, Lambda, Step functions, Event triggers.\nAble to work as a senior engineer with sole interaction point with different business functional teams.\nRequirements\n7 to 12 years of experience in ETL and Data Engineering roles.\nAWS Glue, PySpark, and Amazon Redshift.\nStrong command of SQL and procedural programming in cloud or enterprise databases.\nDeep understanding of data warehousing concepts and data modeling.\nProven ability to deliver efficient, we'll-documented, and scalable data pipelines on AWS.\nFamiliarity with Airflow, AWS Lambda, and other orchestration tools is a plus.\nAWS Certification (eg, AWS Data Analytics Specialty) is an advantage.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data services', 'Data modeling', 'amazon redshift', 'Database', 'Programming', 'Data analytics', 'AWS', 'Data warehousing', 'SQL', 'Python']",2025-06-13 06:02:29
Senior Databricks Data Engineer,"Creative Capsule, LLC",5 - 10 years,Not Disclosed,['Panaji'],"This position will primarily be responsible for designing, developing, and maintaining robust ETL/ELT pipelines for data ingestion, transformation, and storage. The role also involves designing and developing scalable data solutions.\nYou will work with a team responsible for ensuring the availability, reliability, and performance of data systems and requires a good understanding of infrastructure management, cost optimization, and performance tuning in Databricks environments. The candidate will design, develop, and maintain scalable data pipelines of Databricks on cloud platforms (Databricks, Azure, AWS, or GCP).\nThe person will also work with a collaborative team responsible for driving client performance by combining data-driven insights and strategic thinking to solve business challenges. The candidate should have strong organizational, critical thinking, and communication skills to interact effectively with stakeholders.\nResponsibilities:\nDesign, develop, and manage end-to-end data pipelines on Databricks using Spark, Delta Lake, and related technologies\nImplement and optimize ETL/ELT workflows for ingesting, transforming, and storing large volumes of structured and semi-structured data\nManage and monitor Databricks infrastructure, including cluster configuration, auto-scaling, job execution, and resource utilization\nIdentify and implement cost-saving strategies across Databricks workspaces through efficient pipeline design, job scheduling, and infrastructure tuning\nOrchestrate workflows using tools like Apache Airflow, Azure Data Factory, or similar orchestration frameworks\nCollaborate with data architects and platform engineers to ensure efficient data architecture and performance tuning\nMonitor data quality, integrity, and lineage across all pipelines and proactively troubleshoot data issues\nDevelop, maintain, and optimize data models and storage layers that support BI/reporting and analytics use cases\nPartner with business stakeholders to translate business needs into technical specifications and scalable data solutions\nMaintain comprehensive documentation of pipeline architecture, configurations, and operational best practices\nTechnical Qualifications:\nExperience in infrastructure management on Databricks: cluster setup, scaling, security roles, workspace configuration\nExperience in cost optimization techniques in Databricks (e.g., job cluster vs. all-purpose cluster usage, cost/performance tradeoffs)\nExperience with orchestration tools such as Apache Airflow, Azure Data Factory, or AWS Glue Workflows\nExperience with relational (e.g., PostgreSQL, SQL Server) and NoSQL (e.g., MongoDB) databases\nStrong proficiency in Apache Spark and Delta Lake within the Databricks ecosystem\nProficient in Python and SQL for data processing and pipeline development\nFamiliarity with data warehousing, data lakes, and distributed data processing frameworks\nUnderstanding of BI tools like Power BI, Tableau, or Looker is a plus\nPersonal Skills:\nStrong analytical skills: ability to read business requirements, analyze problems, and propose solutions\nAbility to identify alternatives and find optimal solutions\nAbility to follow through and ensure logical implementation\nQuick learner with the ability to adapt to new concepts and software\nAbility to work effectively in a team environment\nStrong time management skills, capable of handling multiple tasks and competing deadlines\nEffective written and verbal communication skills\nEducation and Work Experience:\nBackground in Computer Science, Information Technology, Data Science, or a related field preferred\nMinimum 5 years of experience in Data Engineering with at least 2 years of hands-on experience with Databricks (Azure, AWS, or GCP)\nCertification in Databricks, Azure Data Engineering, or any related data technology is an added advantage",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Performance tuning', 'Infrastructure management', 'Postgresql', 'Scheduling', 'Data quality', 'Information technology', 'Analytics', 'SQL', 'Python']",2025-06-13 06:02:31
Senior Data Engineer,ANS Group,2 - 6 years,Not Disclosed,['Ahmedabad'],"ANS Group is looking for Senior Data Engineer\nThe job responsibilities of a Senior Data Engineer may include:1\n\nDesigning and implementing scalable and reliable data pipelines, data models, and data infrastructure for processing large and complex datasets\n\n2\n\nDeveloping and maintaining databases, data warehouses, and data lakes that store and manage the organization's data\n\n3\n\nDeveloping and implementing data integration and ETL (Extract, Transform, Load) processes to ensure that data flows smoothly and accurately between different systems and data sources\n\n4\n\nEnsuring data quality, consistency, and accuracy through data profiling, cleansing, and validation\n\n5\n\nBuilding and maintaining data processing and analytics systems that support business intelligence, machine learning, and other data-driven applications\n\n6\n\nOptimizing the performance and scalability of data systems and infrastructure to ensure that they can handle the organization's growing data needs\n\nTo be a successful Senior Data Engineer, one must have in-depth knowledge of database architecture, data modeling, data integration, and ETL processes\n\nThey should also be proficient in programming languages such as Python, Java, or SQL and have experience working with big data technologies like Hadoop, Spark, and NoSQL databases\n\nStrong communication and leadership skills",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'data warehousing', 'data pipeline', 'machine learning', 'data engineering', 'sql', 'nosql', 'database design', 'data quality', 'java', 'data modeling', 'spark', 'leadership', 'hadoop', 'etl', 'data integration', 'programming', 'data lake', 'etl process', 'communication skills', 'data profiling']",2025-06-13 06:02:32
Senior Data Engineer with Dot Net,Swits Digital,4 - 7 years,Not Disclosed,['Gurugram'],"Job Title: Senior Data Engineer with .NET\n\nExperience: 6+ Years\n\nLocation: Gurgaon, India\n\n\n\nRequired Skills:\n\n\n\n\n5+ years of experience in distributed computing (Spark) and software development\n\n\n\n3+ years of hands-on experience in Spark-Scala\n\n\n\n5+ years of experience in Data Engineering\n\n\n\n5+ years of experience in Python\n\n\n\n5+ years of experience in .NET Core\n\n\n\nExperience with Dapper, SQL, XUnit, NUnit, and RabbitMQ\n\n\n\nProficiency in working with databases (preferably Postgres)\n\n\n\nSolid understanding of Object-Oriented Programming principles\n\n\n\nExperience in Agile development environments (Scrum/Kanban)\n\n\n\nExperience with version control tools (preferably Git)\n\n\n\nCI/CD pipeline experience\n\n\n\nStrong exposure to automated testing including Integration/Delta, Load, and Performance testing\n\n\n\n\nGood to Have Skills:\n\n\n\n\nExperience with Docker and containerized deployments\n\n\n\nExperience with Kubernetes\n\n\n\nFamiliarity with Airflow\n\n\n\nExperience working in cloud environments (GCP and Azure)\n\n\n\nExposure to TeamCity CI and Octopus Deploy",Industry Type: Recruitment / Staffing,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Automation testing', 'GIT', 'Version control', 'spark', 'Agile development', 'Performance testing', 'Deployment', 'Object oriented programming', 'SQL', 'Python']",2025-06-13 06:02:34
Senior Data Engineer - Azure,Blend360 India,6 - 11 years,Not Disclosed,['Hyderabad'],"As a Senior Data Engineer, your role is to spearhead the data engineering teams and elevate the team to the next level! You will be responsible for laying out the architecture of the new project as well as selecting the tech stack associated with it. You will plan out the development cycles deploying AGILE if possible and create the foundations for good data stewardship with our new data products!\nYou will also set up a solid code framework that needs to be built to purpose yet have enough flexibility to adapt to new business use cases tough but rewarding challenge!\n\nResponsibilities\nCollaborate with several stakeholders to deeply understand the needs of data practitioners to deliver at scale\nLead Data Engineers to define, build and maintain Data Platform\nWork on building Data Lake in Azure Fabric processing data from multiple sources\nMigrating existing data store from Azure Synapse to Azure Fabric\nImplement data governance and access control\nDrive development effort End-to-End for on-time delivery of high-quality solutions that conform to requirements, conform to the architectural vision, and comply with all applicable standards.\nPresent technical solutions, capabilities, considerations, and features in business terms. Effectively communicate status, issues, and risks in a precise and timely manner.\nFurther develop critical initiatives, such as Data Discovery, Data Lineage and Data Quality\nLeading team and Mentor junior resources\nHelp your team members grow in their role and achieve their career aspirations\nBuild data systems, pipelines, analytical tools and programs\nConduct complex data analysis and report on results",Industry Type: Industrial Automation,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Access control', 'Data analysis', 'Team leading', 'Architecture', 'Analytical', 'Agile', 'data governance', 'Data processing', 'Mentor', 'Data quality']",2025-06-13 06:02:35
Senior Data Engineer - Azure,Blend360 India,5 - 11 years,Not Disclosed,['Hyderabad'],"As a Senior Data Engineer, your role is to spearhead the data engineering teams and elevate the team to the next level! You will be responsible for laying out the architecture of the new project as well as selecting the tech stack associated with it. You will plan out the development cycles deploying AGILE if possible and create the foundations for good data stewardship with our new data products!\nYou will also set up a solid code framework that needs to be built to purpose yet have enough flexibility to adapt to new business use cases tough but rewarding challenge!\n\nResponsibilities\nCollaborate with several stakeholders to deeply understand the needs of data practitioners to deliver at scale\nLead Data Engineers to define, build and maintain Data Platform\nWork on building Data Lake in Azure Fabric processing data from multiple sources\nMigrating existing data store from Azure Synapse to Azure Fabric\nImplement data governance and access control\nDrive development effort End-to-End for on-time delivery of high-quality solutions that conform to requirements, conform to the architectural vision, and comply with all applicable standards.\nPresent technical solutions, capabilities, considerations, and features in business terms. Effectively communicate status, issues, and risks in a precise and timely manner.\nFurther develop critical initiatives, such as Data Discovery, Data Lineage and Data Quality\nLeading team and Mentor junior resources\nHelp your team members grow in their role and achieve their career aspirations\nBuild data systems, pipelines, analytical tools and programs\nConduct complex data analysis and report on results\n\n\n5+ Years of Experience as a data engineer or similar role in Azure Synapses, ADF or\nrelevant exp in Azure Fabric\nDegree in Computer Science, Data Science, Mathematics, IT, or similar field\nMust have experience e",Industry Type: Industrial Automation,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Data analysis', 'Data modeling', 'Analytical', 'Agile', 'data governance', 'Data quality', 'Data mining', 'SQL', 'Python']",2025-06-13 06:02:37
Data Engineer / Sr. Data Entry Specialist,Techwaukee,2 - 4 years,Not Disclosed,[],Role Purpose\nThe purpose of this role is to execute the process and drive the performance of the team on the key metrices of the process.\nJob Details\nCountry/Region: India\nEmployment Type: Remote\nWork Type: Contract,,,,"['Product management', 'ERP', 'Data modeling', 'XML', 'MySQL', 'JSON', 'Informatica', 'Information technology', 'SQL', 'Python']",2025-06-13 06:02:39
Senior Data engineer,Wissen Technology,10 - 17 years,Not Disclosed,['Pune'],"Wissen Technology is Hiring fo r Senior Data engineer\n\nAbout Wissen Technology:\nWissen Technology is a globally recognized organization known for building solid technology teams, working with major financial institutions, and delivering high-quality solutions in IT services. With a strong presence in the financial industry, we provide cutting-edge solutions to address complex business challenges\n\nRole Overview : We are looking for a highly skilled and experienced Senior Data Engineer to join our growing data engineering team in Bangalore . In this role, you will be responsible for designing, developing, and maintaining scalable and efficient data pipelines and data architectures. You will collaborate closely with cross-functional teams to ensure data accessibility, reliability, and performance to support business intelligence, analytics, and data science initiatives\n\nExperience : 9-13 Years\nLocation: Bangalore\nKey Responsibilities\nDesign , build, and maintain scalable and reliable data pipelines for ingesting, transforming, and loading structured and unstructured data from diverse sources.\nDevelop optimized SQL queries and modular scripts for data manipulation and transformation.\nCreate and maintain data models and schemas to support advanced analytics, reporting, and operational processes.\nAutomate routine data engineering tasks and data quality checks using scripting (Python, Shell, etc.) and orchestration tools.\nCollaborate with engineering, product, and data science teams to understand data requirements and deliver high-quality solutions.\nAct as a liaison between engineering and business teams to translate business needs into technical solutions.\nImplement monitoring, alerting, and troubleshooting mechanisms for data pipelines and dashboards to ensure data integrity and availability.\nDefine and implement best practices for data validation, data governance, and compliance.\nManage test data and QA environments, supporting test data management processes.\nWork in an Agile environment and contribute to continuous improvement initiatives across the data engineering landscape.\n\n\nRequired Skills and Qualification\nBachelors or masters degree in computer science , Information Systems, or a related field.\n9+ years of professional experience in data engineering or a related field.\nStrong expertise in SQL development and performance tuning for both RDBMS and cloud-based databases.\nHands-on experience with cloud platforms, particularly AWS (e.g., S3, Glue, Lambda, Redshift, EMR).\nExperience with modern data warehouse technologies like Snowflake and Amazon Redshift .\nStrong background in data modeling , ETL/ELT architecture, and data warehousing concepts.\nProficiency in programming languages such as Python , Scala , or Java .\nExperience working with",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Telecom', 'Performance tuning', 'Manager Quality Assurance', 'Data management', 'Consulting', 'Manager Technology', 'Healthcare', 'Business intelligence', 'Monitoring', 'Python']",2025-06-13 06:02:40
Senior Data Engineer,Ensemble Health Partners,5 - 9 years,Not Disclosed,[],"Company Overview:\nAs Ensemble Health Partners Company, we're at the forefront of innovation, leveraging cutting-edge technology to drive meaningful impact in the Revenue Cycle Management landscape. Our future-forward technology combines tightly integrated data ingestion, workflow automation and business intelligence solutions on a modern cloud architecture. We have the second-largest share in the RCM space in the US Market with 10000+ professionals working in the organization. With 10 Technology Patents in our name, we believe the best results come from a combination of skilled and experienced team, proven and repeatable processes, and modern and flexible technologies. As a leading player in the industry, we offer an environment that fosters growth, creativity, and collaboration, where your expertise will be valued, and your contributions will make a difference.\n\nRole & responsibilities :\nExperience : 5-9 Years\nLocation : remote/wfh\n\nPosition Summary :\nDesign and maintain scalable data pipelines, manage ETL processes and data warehouses, ensure data quality and governance, collaborate with cross-functional teams, support machine learning deployment, lead projects, mentor juniors, work with big data and cloud technologies, and bring expertise in Spark, Databricks, Streaming/Reactive/Event-driven systems, Agentic programming, and LLM application development.\n\nRequired Skills :\nSpark, Databricks, Streaming/Reactive /Event driven, Agentic programming & LLM Application Experience\n5+ years of coding experience with Microsoft SQL.\n3+ years working with big data technologies including but not limited to Databricks, Apache Spark, Python, Microsoft Azure (Data Factory, Dataflows, Azure Functions, Azure Service Bus) with a willingness and ability to learn new ones\nExcellent understanding of engineering fundamentals: testing automation, code reviews, telemetry, iterative delivery and DevOps\nExperience with polyglot storage architectures including relational, columnar, key-value, graph or equivalent\nExperience with Delta tables as well as Parquet files stored in ADLS\nExperience delivering applications using componentized and distributed architectures using event driven patterns\nDemonstrated ability to communicate effectively to both technical and non-technical, globally distributed audiences\nSolid foundations in formal architecture, design patterns and best practices\nExperience working with healthcare datasets\n\nWhy Join US?\nWe adapt emerging technologies to practical uses to deliver concrete solutions that bring maximum impact to providers bottom line. We currently have 10 Technology Patents in our name.\nWe offer you a great organization to work for, where you will get to do best work of your career and grow with the team that is shaping the future of Revenue Cycle Management.\nWe have our strong focus on Learning and development. We have the best Industry standard professional development policies to support the learning goals of our associates.\nWe have flexible/ remote working/ working from home options\nBenefits\nHealth Benefits and Insurance Coverage for family and parents. Accidental Insurance for the associate.\nCompliant with all Labor Laws- Maternity benefits, Paternity Leaves.\nCompany Swags- Welcome Packages, Work Anniversary Kits\nExclusive Referral Policy\nProfessional Development Program and Reimbursements.\nRemote work flexibility to work from home.\nPlease share your resume on yash.arora@ensemblehp.com with current ctc, expected ctc, notice period.",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Data Factory', 'Pyspark', 'Azure Functions', 'Azure Databricks']",2025-06-13 06:02:42
Sr. Data Engineer - Snowflake,Freelancer Monika,5 - 10 years,15-25 Lacs P.A.,['Pune'],"Role & responsibilities\nDesigned and implemented end-to-end data pipeline using DBT, Snowflake  \nCreated and structure DBT models like staging, transformation, marts, YAML configurations for models and tests, dbt  seeds.  \n\nHands-on experience on DBT Jinja templating, macro development, dbt jobs and snapshot management for Slowly  changing dimensions.  \n\nDevelop python script for data cleaning, transformation and automation of repetitive task.  \nExperienced in loading structured and semi-structured data from AWS S3 to Snowflake by designing file formats,  \nconfiguring storage integration, and automating data loads using Snow pipe.  \nDesigned scalable incremental models for handling large datasets, reducing resource usage\n\nPreferred candidate profile\nCandidate must have 5+ Yrs experience.\nEarly joiner, who can join within a month",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Snowflake', 'DBT', 'Data Engineer', 'Snowflake Sql', 'Snow Flake Schema', 'Data Build Tool', 'Snowflake Db', 'ETL', 'SQL']",2025-06-13 06:02:44
Senior Azure Data Engineer,Cloud Angles Digital Transformation,8 - 12 years,Not Disclosed,['Hyderabad'],"Job Summary:\nWe are seeking a highly skilled Data Engineer with expertise in leveraging Data Lake architecture and the Azure cloud platform to develop, deploy, and optimise data-driven solutions. . You will play a pivotal role in transforming raw data into actionable insights, supporting strategic decision-making across the organisation.\nResponsibilities\nDesign and implement scalable data science solutions using Azure Data Lake, Azure Data Bricks, Azure Data Factory and related Azure services.\nDevelop, train, and deploy machine learning models to address business challenges.\nCollaborate with data engineering teams to optimise data pipelines and ensure seamless data integration within Azure cloud infrastructure.\nConduct exploratory data analysis (EDA) to identify trends, patterns, and insights.\nBuild predictive and prescriptive models to support decision-making processes.\nExpertise in developing end-to-end Machine learning lifecycle utilizing crisp-DM which includes of data collection, cleansing, visualization, preprocessing, model development, model validation and model retraining\nProficient in building and implementing RAG systems that enhance the accuracy and relevance of model outputs by integrating retrieval mechanisms with generative models.\nEnsure data security, compliance, and governance within the Azure cloud ecosystem.\nMonitor and optimise model performance and scalability in production environments.\nPrepare clear and concise documentation for developed models and workflows.\nSkills Required:\nGood experience in using Pyspark, Python, MLops (Optional), ML flow (Optional), Azure Data Lake Storage. Unity Catalog\nWorked and utilized data from various RDBMS like MYSQL, SQL Server, Postgres and NoSQL databases like MongoDB, Cassandra, Redis and graph DB like Neo4j, Grakn.\nProven experience as a Data Engineer with a strong focus on Azure cloud platform and Data Lake architecture.\nProficiency in Python, Pyspark,\nHands-on experience with Azure services such as Azure Data Lake, Azure Synapse Analytics, Azure Machine Learning, Azure Databricks, and Azure Functions.\nStrong knowledge of SQL and experience in querying large datasets from Data Lakes.\nFamiliarity with data engineering tools and frameworks for data ingestion and transformation in Azure.\nExperience with version control systems (e.g., Git) and CI/CD pipelines for machine learning projects.\nExcellent problem-solving skills and the ability to work collaboratively in a team environment.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Data Factory', 'Azure Data Engineering', 'Azure Databricks', 'Pyspark', 'Azure Data Lake', 'Python']",2025-06-13 06:02:46
"Walk-In Senior Data Engineer - DataStage, Azure & Power BI",Net Connect,6 - 10 years,5-11 Lacs P.A.,['Hyderabad( Madhapur )'],"Greetings from NCG!\n\nWe have a opening for Snowflake Developer role in Hyderabad office!\nBelow JD for your reference\n\nJob Description:\n\nWe are hiring an experienced Senior Data Engineer with strong expertise in IBM DataStage, , and . The ideal candidate will be responsible for end-to-end data integration, transformation, and reporting solutions that drive business decisions.",,,,"['Azure Data Factory', 'Datastage', 'Etl Datastage']",2025-06-13 06:02:47
Data Engineer (C2H),First Mile Consulting,4 - 8 years,Not Disclosed,"['Hyderabad', 'Pune', 'Chennai', 'Bengaluru']","Very strong in python, pyspark and SQL. Good experience in any cloud . They use AWS but any cloud experience is ok. They will train on other things but if candidates have experience with ETL (like AWS Airflow), datalakes like Snowflake",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Temporary/Contractual","['Pyspark', 'Cloud', 'Data engineer', 'Python', 'Sql', 'Airflow']",2025-06-13 06:02:49
Analyst - Direct Display,Merkle B2b,0 - 2 years,Not Disclosed,['Chennai'],"The purpose of this role is to assist with the planning, reviewing and optimisation of Display campaigns whilst supporting the team in reporting and managing client accounts.\nJob Description:\nKey responsibilities:Focuses on day-to-day executionProactively reviews and manages client data to ensure optimal performance on all campaignsTracks and reports on campaign results, gathers data analysis and participates in weekly callsGenerates campaign reports and is responsible for pacing, QA and traffickingDevelops and maintains accurate project plans for client status updates\nLocation:\nChennai\nBrand:\nParagon\nTime Type:\nFull time\nContract Type:\nPermanent",Industry Type: Management Consulting,Department: Other,"Employment Type: Full Time, Permanent","['digital marketing', 'data analysis', 'quality control', 'branding', 'business development', 'advertising', 'sales', 'brand management', 'assurance', 'quality analysis', 'marketing', 'promotions', 'qc', 'campaigns', 'quality assurance', 'marketing communication', 'social media marketing', 'reporting']",2025-06-13 06:02:51
Data Science Lead,Protiviti India,9 - 14 years,25-40 Lacs P.A.,['Mumbai (All Areas)'],"Role & responsibilities\n8+ year bachelors or master’s degree from reputed University with concentration on finance, economics or other quantitative field such as statistics or engineering.\nManage multiple client engagements in Financial Services locally in India\nActively drive pre-sales, sales activities primarily for FS clients locally in Data Science Domain\nUnderstand client requirements in detail and create technical & commercial proposal\nDrive client conversations specifically for business development activities",,,,"['Data Science', 'Natural Language Processing', 'Presales', 'Machine Learning', 'AWS', 'GCP', 'Cloud Platform', 'Python']",2025-06-13 06:02:53
Data Analyst,BP INCORPORATE INTERNATIONAL.,1 - 3 years,Not Disclosed,['Pune'],"Grade IResponsible for supporting the delivery of business analysis and consulting processes and procedures for the defined specialism using basic technical capabilities, developing working relationships to provide support with queries, issues and ad-hoc requests and assisting with quality assurance services. Specialisms: Business Analysis; Data Management and Data Science; Digital Innovation.\nEntity:\nTechnology\n\nITS Group\n\nYou will work with\nBeing part of a digital delivery data group supporting bp Solutions, you will apply your domain knowledge and familiarity with domain data processes to support the organisation. Part of bp s Production Operations business, bp Solutions has hubs in London, Pune, and Houston. The data team provides daily operational data management, data engineering and analytics support to this organisation across a broad range of activity from facilities and subsea engineering to logistics.\nLet me tell you about the role\nA data analyst collects, processes, and performs analyses on a variety of datasets. Their key responsibilities include interpreting sophisticated data sets to identify trends and patterns, using analytical tools and methods to generate actionable insights, and crafting visualizations and reports to communicate those insights and recommendations to support decision-making. Data analysts collaborate closely with business domain collaborators to understand their data analysis needs, ensure data accuracy, write and recommend data-driven solutions and tackle value impacting business problems.\nYou might be a good fit for this role if you:\nHave strong domain knowledge in at least one of; facilities or subsea engineering, maintenance and reliability, operations, logistics.\nStrong analytical skills and proven capability in applying analytical techniques and Python scripting to solve practical problems.\nAre curious, and keen to apply new technologies, trends methods to improve existing standards and the capabilities of the Subsurface community.\nAre well organized and self-motivated, you balance proactive and reactive approaches and across multiple priorities to complete tasks on time.\nApply judgment and common sense - you use insight and good judgment to inform actions and respond to situations as they arise.\nWhat you will deliver\nBe a link between asset teams and Technology, combining in-depth understanding of one or more relevant domains with data analytics skills\nProvide actionable, data-driven insights by combining deep statistical skills, data manipulation capabilities and business insight.\nProactively identify impactful opportunities and autonomously complete data analysis.\nYou apply existing data analytics strategies relevant to your immediate scope.\nClean, pre-process and analyse both structured and unstructured data\nDevelop data visualisations to analyse and interrogate broad datasets (e.g. with tools such as Microsoft PowerBI, Spotfire or similar).\nPresent results to peers and senior management, influencing decision making\nWhat you will need to be successful (experience and qualifications)\nEssential\nMSc or equivalent experience in a quantitative field, preferably statistics.\nhave strong domain knowledge in at least one of; facilities or subsea engineering, maintenance and reliability, operations, logistics.\nHands-on experience carrying out data analytics, data mining and product analytics in complex, fast-paced environments.\nApplied knowledge of data analytics and data pipelining tools and approaches across all data lifecycle stages.\nDeep understanding of a few and a high-level understanding of several commonly available statistics approaches.\nAdvanced SQL knowledge.\nAdvanced scripting experience in R or python.\nAbility to write and maintain moderately sophisticated data pipelines.\nCustomer-centric and pragmatic approach. Focus on value delivery and swift execution, while maintaining attention to detail.\nGood communication and social skills, with the ability to effectively communicate ideas, expectations, and feedback to team members, partners, and customers. Foster collaboration and teamwork\nDesired\nAdvanced analytics degree.\nExperience applying analytics to support engineering turnarounds\nExperience with big data technologies (e.g. Hadoop, Hive, and Spark) is a plus.\nAbout bp\nOur purpose is to deliver energy to the world, today and tomorrow. For over 100 years, bp has focused on discovering, developing, and producing oil and gas in the nations where we operate. We are one of the few companies globally that can provide governments and customers with an integrated energy offering. Delivering our strategy sustainably is fundamental to achieving our ambition to be a net zero company by 2050 or sooner!\n\nTravel Requirement\nUp to 10% travel should be expected with this role\n\nRelocation Assistance:\nThis role is eligible for relocation within country\n\nRemote Type:\nThis position is a hybrid of office/remote working\n\nSkills:",Industry Type: Oil & Gas,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data analysis', 'Manager Quality Assurance', 'Data management', 'Business analysis', 'Consulting', 'microsoft', 'Data mining', 'Analytics', 'SQL', 'Logistics']",2025-06-13 06:02:54
Data Analyst,Innovature Software Labs,2 - 4 years,Not Disclosed,"['Kochi', 'Chennai']",">\nJob Category: Software\nJob Type: Full Time\nJob Location: Infopark - Kochi\nExperience: 2 - 4 Years\nDesignation: Data Analyst\nKey Responsibilities\nAnalyze large volumes of analytical data (sales, customer, product) to identify patterns, trends, and insights.\nDevelop and maintain data reports and dashboards using business intelligence tools (e.g., Tableau, Power BI, etc.)\nCollaborate with business teams to understand key business objectives and data needs.\nAnalyze customer behavior, product usage, and historical purchase data to identify cross-selling opportunities.\nEnsure data models and structures are well-designed to facilitate analysis and reporting.\nAct as a bridge between data engineers and business stakeholders , ensuring data is aligned with business needs.\nSkill set\nProficient in data engineering tools and languages such as Python, SQL, and Java.\nStrong knowledge of business intelligence (BI) tools such as Tableau, Power BI or similar for creating reports and dashboards.\nAbility to work with databases(SQL NoSQL) and perform complex data queries.\nFamiliarity with data wrangling, ETL processes and data cleaning techniques.\nClear understanding of data warehousing cloud based analytics .\nAbility to apply data insights to influence marketing strategies, sales tactics, and product offerings.\nExperience\n2-4 years of experience in a data analyst role with experience in analyzing customer data, sales data, and product data.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Usage', 'Sales', 'Analytical', 'power bi', 'Data Analyst', 'Business intelligence', 'Data warehousing', 'Analytics', 'SQL', 'Python']",2025-06-13 06:02:56
Data Analyst,Cushman Wakefield,1 - 2 years,Not Disclosed,['Gurugram'],"Job Title\nData Analyst\nJob Description Summary\nThis role focuses on developing advanced analytics systems, uncovering growth opportunities through data analysis, and creating insightful reports and visualizations. It will involve close collaboration with managers to understand business needs, define KPIs, and deliver actionable insights that enhance performance and efficiency.\nKey responsibilities include data mining, predictive modeling, system evaluation, and maintaining robust data infrastructure to support strategic decision-making.\n\n\n\n\n\n\nINCO: Cushman Wakefield",,,,"['data cleansing', 'advanced analytics', 'Data analysis', 'Scalability', 'Revenue enhancement', 'Infrastructure', 'Manager Technology', 'Predictive modeling', 'Data Analyst', 'Data mining']",2025-06-13 06:02:58
Data Analyst,C&W Services,1 - 4 years,Not Disclosed,['Gurugram'],"Job Title\nData Analyst\nJob Description Summary\nThis role focuses on developing advanced analytics systems, uncovering growth opportunities through data analysis, and creating insightful reports and visualizations. It will involve close collaboration with managers to understand business needs, define KPIs, and deliver actionable insights that enhance performance and efficiency.\nKey responsibilities include data mining, predictive modeling, system evaluation, and maintaining robust data infrastructure to support strategic decision-making.\nWork closely with the Program manager (s) and department heads to understand and maintain focus on their analytics needs, including critical metrics and KPIs, and deliver actionable insights to relevant decision-makers\nProactively analyze data to answer key questions for stakeholders or yourself, with an eye on what drives business performance, and investigate and communicate which areas need improvement in efficiency and productivity\nCreate and maintain rich insights to facilitate cross sell and upsell decisions through deep data mining\nDefine and implement data acquisition and integration logic, selecting an appropriate combination of methods and tools within the defined technology stack to ensure optimal scalability and performance of the solution\nDevelop and maintain databases by acquiring data from primary and secondary sources\nProviding technical expertise in data storage structures, data mining, and data cleansing.\nBuild predictive analysis and churn analysis to impact revenue enhancement and retention\n\n\n\n\n\n\nINCO: Cushman & Wakefield",Industry Type: Real Estate,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data cleansing', 'advanced analytics', 'Data analysis', 'Scalability', 'Revenue enhancement', 'Infrastructure', 'Manager Technology', 'Predictive modeling', 'Data Analyst', 'Data mining']",2025-06-13 06:02:59
Data Techology Senior Associate,MSCI Services,2 - 8 years,Not Disclosed,['Pune'],"The Data Technology team at MSCI is responsible for meeting the data requirements across various business areas, including Index, Analytics, and Sustainability. Our team collates data from multiple sources such as vendors (e.g., Bloomberg, Reuters), website acquisitions, and web scraping (e.g., financial news sites, company websites, exchange websites, filings). This data can be in structured or semi-structured formats. We normalize the data, perform quality checks, assign internal identifiers, and release it to downstream applications.\nYour Key Responsibilities\nAs data engineers, we build scalable systems to process data in various formats and volumes, ranging from megabytes to terabytes. Our systems perform quality checks, match data across various sources, and release it in multiple formats. We leverage the latest technologies, sources, and tools to process the data. Some of the exciting technologies we work with include Snowflake, Databricks, and Apache Spark.\nYour skills and experience that will help you excel\nCore Java, Spring Boot, Apache Spark, Spring Batch, Python. Exposure to sql databases like Oracle, Mysql, Microsoft Sql is a must. Any experience / knowledge / certification on Cloud technology preferrably Microsoft Azure or Google cloud platform is good to have. Exposures to non sql databases like Neo4j or Document database is again good to have.\nAbout MSCI\nWhat we offer you\nTransparent compensation schemes and comprehensive employee benefits, tailored to your location, ensuring your financial security, health, and overall wellbeing.\nFlexible working arrangements, advanced technology, and collaborative workspaces.\nA culture of high performance and innovation where we experiment with new ideas and take responsibility for achieving results.\nA global network of talented colleagues, who inspire, support, and share their expertise to innovate and deliver for for ongoing skills development.\nMulti-directional career paths that offer professional growth and development through new challenges, internal mobility and expanded roles.\nWe actively nurture an environment that builds a sense of inclusion belonging and connection, including eight Employee Resource Groups. All Abilities, Asian Support Network, Black Leadership Network, Climate Action Network, Hola! MSCI, Pride & Allies, Women in Tech, and Women s Leadership Forum.\n.\nMSCI Inc. is an equal opportunity employer. It is the policy of the firm to ensure equal employment opportunity without discrimination or harassment on the basis of race, color, religion, creed, age, sex, gender, gender identity, sexual orientation, national origin, citizenship, disability, marital and civil partnership/union status, pregnancy (including unlawful discrimination on the basis of a legally protected parental leave), veteran status, or any other characteristic protected by law. MSCI is also committed to working with and providing reasonable accommodations to individuals with disabilities. If you are an individual with a disability and would like to request a reasonable accommodation for . Please note, this e-mail is intended only for individuals who are requesting a reasonable workplace accommodation; it is not intended for other inquiries.\nTo all recruitment agencies\n.\nNote on recruitment scams",Industry Type: NGO / Social Services / Industry Associations,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['CVS', 'Core Java', 'Bloomberg', 'spring batch', 'MySQL', 'Oracle', 'Analytics', 'Downstream', 'Python', 'Recruitment']",2025-06-13 06:03:01
Lead Data Engineer,Shyftlabs,5 - 10 years,Not Disclosed,['Noida'],"Position Overview\nWe are looking for an experienced Lead Data Engineer to join our dynamic team. If you are passionate about building scalable software solutions, and work collaboratively with cross-functional teams to define requirements and deliver solutions we would love to hear from you.\nJob Responsibilities:\nDevelop and maintain data pipelines and ETL/ELT processes using Python\nDesign and implement scalable, high-performance applications\nWork collaboratively with cross-functional teams to define requirements and deliver solutions\nDevelop and manage near real-time data streaming solutions using Pub, Sub or Beam.\nContribute to code reviews, architecture discussions, and continuous improvement initiatives\nMonitor and troubleshoot production systems to ensure reliability and performance\nBasic Qualifications:\n5+ years of professional software development experience with Python\nStrong understanding of software engineering best practices (testing, version control, CI/CD)\nExperience building and optimizing ETL/ELT processes and data pipelines\nProficiency with SQL and database concepts\nExperience with data processing frameworks (e.g., Pandas)\nUnderstanding of software design patterns and architectural principles\nAbility to write clean, well-documented, and maintainable code\nExperience with unit testing and test automation\nExperience working with any cloud provider (GCP is preferred)\nExperience with CI/CD pipelines and Infrastructure as code\nExperience with Containerization technologies like Docker or Kubernetes\nBachelors degree in Computer Science, Engineering, or related field (or equivalent experience)\nProven track record of delivering complex software projects\nExcellent problem-solving and analytical thinking skills\nStrong communication skills and ability to work in a collaborative environment\nPreferred Qualifications:\nExperience with GCP services, particularly Cloud Run and Dataflow\nExperience with stream processing technologies (Pub/Sub)\nFamiliarity with big data technologies (Airflow)\nExperience with data visualization tools and libraries\nKnowledge of CI/CD pipelines with Gitlab and infrastructure as code with Terraform\nFamiliarity with platforms like Snowflake, Bigquery or Databricks,.\nGCP Data engineer certification",Industry Type: Management Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Software design', 'Version control', 'Analytical', 'Data processing', 'Unit testing', 'data visualization', 'Continuous improvement', 'SQL', 'Python']",2025-06-13 06:03:02
Lead Data Engineer - Azure,Blend360 India,7 - 12 years,Not Disclosed,['Hyderabad'],"As a Lead Data Engineer, your role is to spearhead the data engineering teams and elevate the team to the next level! You will be responsible for laying out the architecture of the new project as well as selecting the tech stack associated with it. You will plan out the development cycles deploying AGILE if possible and create the foundations for good data stewardship with our new data products!\nYou will also set up a solid code framework that needs to be built to purpose yet have enough flexibility to adapt to new business use cases tough but rewarding challenge!\n\nResponsibilities\nCollaborate with several stakeholders to deeply understand the needs of data practitioners to deliver at scale\nLead Data Engineers to define, build and maintain Data Platform\nWork on building Data Lake in Azure Fabric processing data from multiple sources\nMigrating existing data store from Azure Synapse to Azure Fabric\nImplement data governance and access control\nDrive development effort End-to-End for on-time delivery of high-quality solutions that conform to requirements, conform to the architectural vision, and comply with all applicable standards.\nPresent technical solutions, capabilities, considerations, and features in business terms. Effectively communicate status, issues, and risks in a precise and timely manner.\nFurther develop critical initiatives, such as Data Discovery, Data Lineage and Data Quality\nLeading team and Mentor junior resources\nHelp your team members grow in their role and achieve their career aspirations\nBuild data systems, pipelines, analytical tools and programs\nConduct complex data analysis and report on results\n\n\n7+ Years of Experience as a data engineer or similar role in Azure Synapses, ADF or\nrelevant exp in Azure Fabric\nDegree in Computer Science, Data Science, Mathematics, IT, or similar field\nMust have experience e",Industry Type: Industrial Automation,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Data analysis', 'Data modeling', 'Analytical', 'Agile', 'data governance', 'Data quality', 'Data mining', 'SQL', 'Python']",2025-06-13 06:03:04
Lead Data Engineer,Moreyeahs,7 - 9 years,Not Disclosed,['Indore'],"We are seeking a highly skilled and experienced Lead Data Engineer (7+ years) to join our dynamic team. As a Lead Data Engineer, you will play a crucial role in designing, developing, and maintaining our data infrastructure. You will be responsible for ensuring the efficient and reliable collection, storage, and transformation of large-scale data to support business intelligence, analytics, and data-driven decision-making.\n\nKey Responsibilities :\n\nData Architecture & Design :\n- Lead the design and implementation of robust data architectures that support data warehousing (DWH), data integration, and analytics platforms.\n- Develop and maintain ETL (Extract, Transform, Load) pipelines to ensure the efficient processing of large datasets.\n\nETL Development :\n- Design, develop, and optimize ETL processes using tools like Informatica Power Center, Intelligent Data Management Cloud (IDMC), or custom Python scripts.\n- Implement data transformation and cleansing processes to ensure data quality and consistency across the enterprise.\n\nData Warehouse Development :\n- Build and maintain scalable data warehouse solutions using Snowflake, Databricks, Redshift, or similar technologies.\n\n- Ensure efficient storage, retrieval, and processing of structured and semi-structured data.\n\nBig Data & Cloud Technologies :\n- Utilize AWS Glue and PySpark for large-scale data processing and transformation.\n- Implement and manage data pipelines using Apache Airflow for orchestration and scheduling.\n- Leverage cloud platforms (AWS, Azure, GCP) for data storage, processing, and analytics.\n\nData Management & Governance :\n- Establish and enforce data governance and security best practices.\n- Ensure data integrity, accuracy, and availability across all data platforms.\n- Implement monitoring and alerting systems to ensure data pipeline reliability.\n\nCollaboration & Leadership :\n\n- Work closely with data Stewards, analysts, and business stakeholders to understand data requirements and deliver solutions that meet business needs.\n- Mentor and guide junior data engineers, fostering a culture of continuous learning and development within the team.\n- Lead data-related projects from inception to delivery, ensuring alignment with business objectives and timelines.\n\nDatabase Management :\n- Design and manage relational databases (RDBMS) to support transactional and analytical workloads.\n- Optimize SQL queries for performance and scalability across various database platforms.\n\nRequired Skills & Qualifications :\nEducation: Bachelors or Masters degree in Computer Science, Information Systems, Engineering, or a related field.\n\nExperience :\n- Minimum of 7+ years of experience in data engineering, ETL, and data warehouse development.\n- Proven experience with ETL tools like Informatica Power Center or IDMC.\n- Strong proficiency in Python and PySpark for data processing.\n- Experience with cloud-based data platforms such as AWS Glue, Snowflake, Databricks, or Redshift.\n- Hands-on experience with SQL and RDBMS platforms (e.g., Oracle, MySQL, PostgreSQL).\n- Familiarity with data orchestration tools like Apache Airflow.\nTechnical Skills :\n- Advanced knowledge of data warehousing concepts and best practices.\n- Strong understanding of data modeling, schema design, and data governance.\n- Proficiency in designing and implementing scalable ETL pipelines.\n- Experience with cloud infrastructure (AWS, Azure, GCP) for data storage and processing.\n\nSoft Skills :\n- Excellent communication and collaboration skills.\n- Ability to lead and mentor a team of engineers.\n- Strong problem-solving and analytical thinking abilities.\n- Ability to manage multiple projects and prioritize tasks effectively.\n\nPreferred Qualifications :\n- Experience with machine learning workflows and data science tools.\n- Certification in AWS, Snowflake, Databricks, or relevant data engineering technologies.\n- Experience with Agile methodologies and DevOps practices.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data modeling', 'RDBMS', 'MySQL', 'Agile', 'Informatica', 'Oracle', 'Apache', 'Business intelligence', 'Analytics', 'Python']",2025-06-13 06:03:06
Lead Data Engineer,anblicks,6 - 8 years,Not Disclosed,['Ahmedabad'],"Job Summary:\nWe are seeking a Senior Data Engineer with hands-on experience building scalable data pipelines using Microsoft Fabric. The role focuses on delivering ingestion, transformation, and enrichment workflows across medallion architecture.\nKey Responsibilities:\nDevelop and maintain data pipelines using Microsoft Fabric Data Factory and OneLake.\nDesign and build ingestion and transformation pipelines for structured and unstructured data.\nImplement frameworks for metadata tagging, version control, and batch tracking.\nEnsure security, quality, and compliance of data pipelines.\nContribute to CI/CD integration, observability, and documentation.\nCollaborate with data architects and analysts to meet business requirements.\nQualifications:\n6+ years of experience in data engineering; 2+ years working on Microsoft Fabric or Azure Data services.\nHands-on with tools like Azure Data Factory, Fabric, Databricks, or Synapse.\nStrong SQL and data processing skills (e.g., PySpark, Python).\nExperience with data cataloging, lineage, and governance frameworks.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['metadata', 'data services', 'Version control', 'Compliance', 'Architecture', 'Data processing', 'microsoft', 'SQL', 'Python']",2025-06-13 06:03:07
Lead Data Engineer (Immediate joiner),Decision Point,4 - 9 years,15-30 Lacs P.A.,"['Gurugram', 'Chennai']","Role & responsibilities\n• Assume ownership of Data Engineering projects from inception to completion.\nImplement fully operational Unified Data Platform solutions in production environments using technologies like Databricks, Snowflake, Azure Synapse etc.\nShowcase proficiency in Data Modelling and Data Architecture\nUtilize modern data transformation tools such as DBT (Data Build Tool) to streamline and automate data pipelines (nice to have).",,,,"['Pyspark', 'Azure Databricks', 'SQL', 'Azure Synapse', 'Python', 'Etl Pipelines', 'Airflow', 'Bigquery', 'Advance Sql', 'Azure Cloud', 'GCP', 'Data Modeling', 'Data Architecture', 'AWS']",2025-06-13 06:03:09
Principal Machine Learning Engineer,Paypal,0 - 7 years,Not Disclosed,['Bengaluru'],"The Company\nPayPal has been revolutionizing commerce globally for more than 25 years. Creating innovative experiences that make moving money, selling, and shopping simple, personalized, and secure, PayPal empowers consumers and businesses in approximately 200 markets to join and thrive in the global economy.\nWe operate a global, two-sided network at scale that connects hundreds of millions of merchants and consumers. We help merchants and consumers connect, transact, and complete payments, whether they are online or in person. PayPal is more than a connection to third-party payment networks. We provide proprietary payment solutions accepted by merchants that enable the completion of payments on our platform on behalf of our customers.\nOur beliefs are the foundation for how we conduct business every day. We live each day guided by our core values of Inclusion, Innovation, Collaboration, and Wellness. Together, our values ensure that we work together as one global team with our customers at the center of everything we do - and they push us to ensure we take care of ourselves, each other, and our communities.\nJob Summary:\nWhat you need to know about the role\n\nThis job will drive the strategic vision and development of cutting-edge machine learning models and algorithms to solve complex problems. You will work closely with data scientists, software engineers, and product teams to enhance services through innovative AI/ML solutions. Your role will involve building scalable ML pipelines, ensuring data quality, and deploying models into production environments to drive business insights and improve customer experiences.\n\nYour Way to Impact\n\nAs a Principal Machine Learning Engineer, you ll lead mission-critical initiatives that define PayPal s AI edge from large-scale model fine-tuning to the architecture of foundational AI systems. Your leadership will enable AI-native capabilities across personalization, fraud detection, customer experience, and internal productivity. You will shape how PayPal delivers trust, speed, and intelligence in every user interaction.\n\nMeet Our Team\nYou ll work within the core Applied Intelligence team, a cross-functional hub driving AI-first innovation across PayPal s product and platform landscape. Your team s work powers smarter workflows and more seamless experiences for our global customer base. This is a hands-on leadership role where you ll help align strategy, technology, and business outcomes.\nJob Description:\nYour Day to Day\nDefine and drive strategic vision for model development and ML applications across business domains.\nLead architecture and experimentation for foundational model pipelines.\nManage end-to-end lifecycle from data prep and training to deployment and monitoring.\nCollaborate with product, infra, and engineering leaders to ship impactful solutions.\nGuide model evaluation frameworks, bias detection, and performance monitoring practices.\nMentor technical leads and contribute to thought leadership internally and externally.\nWhat You Need to Bring\nMinimum of 15 years of relevant experience with a Bachelor s degree or equivalent.\nDeep expertise in building and fine-tuning advanced ML models at scale.\nStrong experience with cloud-native ML solutions (e.g., SageMaker, Vertex AI).\nProven success in leading multi-functional ML projects from research to production.\nStrong communication and strategic planning abilities to align tech with business.\nPreferred Qualification:\nSubsidiary:\nPayPal\nTravel Percent:\n0\nFor the majority of employees, PayPals balanced hybrid work model offers 3 days in the office for effective in-person collaboration and 2 days at your choice of either the PayPal office or your home workspace, ensuring that you equally have the benefits and conveniences of both locations.\nOur Benefits:\nWe have great benefits including a flexible work environment, employee shares options, health and life insurance and more. To learn more about our benefits please visit https://www.paypalbenefits.com .\nWho We Are:\nClick Here to learn more about our culture and community.\nCommitment to Diversity and Inclusion\nPayPal provides equal employment opportunity (EEO) to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, pregnancy, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state, or local law. In addition, PayPal will provide reasonable accommodations for qualified individuals with disabilities. If you are unable to submit an application because of incompatible assistive technology or a disability, please contact us at talentaccommodations@paypal.com .\nBelonging at PayPal:\nOur employees are central to advancing our mission, and we strive to create an environment where everyone can do their best work with a sense of purpose and belonging. Belonging at PayPal means creating a workplace with a sense of acceptance and security where all employees feel included and valued. We are proud to have a diverse workforce reflective of the merchants, consumers, and communities that we serve, and we continue to take tangible actions to cultivate inclusivity and belonging at PayPal.\nAny general requests for consideration of your skills, please Join our Talent Community .\nWe know the confidence gap and imposter syndrome can get in the way of meeting spectacular candidates. Please don t hesitate to apply.",Industry Type: FinTech / Payments,Department: Other,"Employment Type: Full Time, Permanent","['Architecture', 'User interaction', 'Diversity and Inclusion', 'Machine learning', 'Strategic planning', 'Manager Technology', 'Wellness', 'Data quality', 'Customer experience', 'Fraud detection']",2025-06-13 06:03:10
Senior Data Engineer,Dfcs Technologies,8 - 13 years,Not Disclosed,['Chennai'],"Architect & Build Scalable Systems: Design and implement a petabyte-scale lakehouse\nArchitectures to unify data lakes and warehouses. Real-Time Data Engineering: Develop and optimize streaming pipelines using Kafka, Pulsar, and Flink.\n\nRequired Candidate profile\nData engineering experience with large-scale systems• Expert proficiency in Java for data-intensive applications. Handson experience with lakehouse architectures, stream processing, & event streaming",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Java', 'Terraform', 'Delta Lake', 'Data Engineer', 'Streaming', 'Streaming Framework', 'Pyspark', 'Apache Iceberg', 'lakehouse', 'Apache Flink', 'Kafka', 'Spark Streaming', 'Stream Processing', 'Spark Core', 'Apache Storm', 'Apache Pulsar', 'Kafta', 'Data Pipeline', 'Streaming Kafka', 'Clickhouse', 'Spark', 'AWS', 'Kafka Streams', 'Streams']",2025-06-13 06:03:12
Senior Data Engineer,Binary Infoways,5 - 9 years,12-19.2 Lacs P.A.,['Hyderabad'],"Responsibilities:\n* Design, develop & maintain data pipelines using Airflow, Python & SQL.\n* Optimize performance through Spark & Splunk analytics.\n* Collaborate with cross-functional teams on big data initiatives.\n* AWS",Industry Type: BPM / BPO,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Airflow', 'Big Data Technologies', 'ETL', 'AWS', 'Python', 'Glue', 'Snowflake', 'Spark', 'Splunk', 'SQL']",2025-06-13 06:03:14
Azure Data Engineer ( Azure Databricks),Apex One,4 - 8 years,Not Disclosed,"['Hyderabad', 'Bengaluru']","Job Summary\nWe are seeking a skilled Azure Data Engineer with 4 years of overall experience, including at least 2 years of hands-on experience with Azure Databricks (Must). The ideal candidate will have strong expertise in building and maintaining scalable data pipelines and working across cloud-based data platforms.\nKey Responsibilities\nDesign, develop, and optimize large-scale data pipelines using Azure Data Factory, Azure Databricks, and Azure Synapse.\nImplement data lake solutions and work with structured and unstructured datasets in Azure Data Lake Storage (ADLS).\nCollaborate with data scientists, analysts, and engineering teams to design and deliver end-to-end data solutions.\nDevelop ETL/ELT processes and integrate data from multiple sources.\nMonitor, debug, and optimize workflows for performance and cost-efficiency.\nEnsure data governance, quality, and security best practices are maintained.\nMust-Have Skills\n4+ years of total experience in data engineering.\n2+ years of experience with Azure Databricks (PySpark, Notebooks, Delta Lake).\nStrong experience with Azure Data Factory, Azure SQL, and ADLS.\nProficient in writing SQL queries and Python/Scala scripting.\nUnderstanding of CI/CD pipelines and version control systems (e.g., Git).\nSolid grasp of data modeling and warehousing concepts.",Industry Type: Management Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure', 'Azure Data Factory', 'SQL queries', 'PySpark', 'Delta Lake', 'Azure Databricks', 'Notebooks', 'Azure SQL']",2025-06-13 06:03:15
"Data Engineer, Product Analytics",Meta,2 - 7 years,Not Disclosed,['Bengaluru'],"Apply to this job\nAs a Data Engineer at Meta, you will shape the future of people-facing and business-facing products we build across our entire family of applications (Facebook, Instagram, Messenger, WhatsApp, Reality Labs, Threads). Your technical skills and analytical mindset will be utilized designing and building some of the worlds most extensive data sets, helping to craft experiences for billions of people and hundreds of millions of businesses worldwide.In this role, you will collaborate with software engineering, data science, and product management teams to design/build scalable data solutions across Meta to optimize growth, strategy, and user experience for our 3 billion plus users, as well as our internal employee community.You will be at the forefront of identifying and solving some of the most interesting data challenges at a scale few companies can match. By joining Meta, you will become part of a world-class data engineering community dedicated to skill development and career growth in data engineering and beyond.Data Engineering: You will guide teams by building optimal data artifacts (including datasets and visualizations) to address key questions. You will refine our systems, design logging solutions, and create scalable data models. Ensuring data security and quality, and with a focus on efficiency, you will suggest architecture and development approaches and data management standards to address complex analytical problems.Product leadership: You will use data to shape product development, identify new opportunities, and tackle upcoming challenges. Youll ensure our products add value for users and businesses, by prioritizing projects, and driving innovative solutions to respond to challenges or opportunities.Communication and influence: You wont simply present data, but tell data-driven stories. You will convince and influence your partners using clear insights and recommendations. You will build credibility through structure and clarity, and be a trusted strategic partner.\nData Engineer, Product Analytics Responsibilities\nCollaborate with engineers, product managers, and data scientists to understand data needs, representing key data insights in a meaningful way\nDesign, build, and launch collections of sophisticated data models and visualizations that support multiple use cases across different products or domains\nDefine and manage Service Level Agreements for all data sets in allocated areas of ownership\nSolve challenging data integration problems, utilizing optimal Extract, Transform, Load (ETL) patterns, frameworks, query techniques, sourcing from structured and unstructured data sources\nImprove logging\nAssist in owning existing processes running in production, optimizing complex code through advanced algorithmic concepts\nOptimize pipelines, dashboards, frameworks, and systems to facilitate easier development of data artifacts\nInfluence product and cross-functional teams to identify data opportunities to drive impact\nMinimum Qualifications\nBachelors degree in Computer Science, Computer Engineering, relevant technical field, or equivalent\n2+ years of experience where the primary responsibility involves working with data. This could include roles such as data analyst, data scientist, data engineer, or similar positions\n2+ years of experience with SQL, ETL, data modeling, and at least one programming language (e.g., Python, C++, C#, Scala or others.)\nPreferred Qualifications\nMasters or Ph.D degree in a STEM field\nAbout Meta\n.\n\n\nEqual Employment Opportunity\n.\n\nMeta is committed to providing reasonable accommodations for qualified individuals with disabilities and disabled veterans in our job application procedures. If you need assistance or an accommodation due to a disability, fill out the Accommodations request form .",Industry Type: Internet,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Product management', 'Computer science', 'C++', 'Data management', 'Data modeling', 'data security', 'Analytical', 'Analytics', 'SQL', 'Python']",2025-06-13 06:03:17
Data Engineer - GCP,Happiest Minds Technologies,5 - 10 years,8-18 Lacs P.A.,"['Pune', 'Bengaluru']","Key Responsibilities:\n• Data Pipeline Development: Designing, building, and maintaining robust data pipelines to move data from various sources (e.g., databases, external APIs, logs) to centralized data systems, such as data lakes or warehouses.\n• Data Integration: Integrating data from multiple sources and ensuring it's processed in a consistent, usable format. This involves transforming, cleaning, and validating data to meet the needs of products, analysts and data scientists.\n• Database Management: Creating, managing, and optimizing databases for storing large amounts of structured and unstructured data. Ensuring high availability, scalability, and security of data storage solutions.\nIdentifying and resolving issues related to the speed and efficiency of data systems. This could include optimizing queries, storage systems, and improving overall system architecture.\n• : Automating routine tasks, such as data extraction, transformation, and loading (ETL), to ensure smooth data flows with minimal manual intervention.\n• : Working closely with Work closely with product managers, UX/UI designers, and other stakeholders to understand data requirements and ensure data is in the right format for analysis and modeling.\n• : Ensuring data integrity and compliance with data governance policies, including data quality standards, privacy regulations (e.g., GDPR), and security protocols.\n: Continuously monitoring data pipelines and databases for any disruptions or errors and troubleshooting any issues that arise to ensure continuous data flow.\n• : Staying up to date with emerging data tools, technologies, and best practices in order to improve data systems and infrastructure.\n• : Documenting data systems, pipeline processes, and data architectures, providing clear instructions for the team to follow, and ensuring that the architecture is understandable for stakeholders.",,,,"['Data Engineering', 'gcp', 'Python', 'SQL', 'Pyspark', 'Bigquery', 'Google Cloud Platforms']",2025-06-13 06:03:19
Gcp Data Engineer,Saama Technologies,3 - 8 years,Not Disclosed,"['Pune', 'Chennai', 'Coimbatore']","We are looking for immediate joiners only.\nPosition: GCP Data Engineer\nWe are seeking a skilled and experienced GCP Data Engineer to join our dynamic team. The ideal candidate will have a strong background in Google Cloud Platform (GCP), BigQuery, Dataform, and data warehouse concepts. Experience with Airflow/Cloud Composer and cloud computing knowledge will be a significant advantage.\nResponsibilities:\n- Designing, developing, and maintaining data pipelines and workflows on the Google Cloud Platform.",,,,"['Pyspark', 'GCP', 'Python', 'SQL', 'Google Cloud Platforms']",2025-06-13 06:03:20
Expert Cloud Data Engineer - (AWS/Python/Pyspark/Kafka/SQL),BMW Techworks India,9 - 13 years,15-27.5 Lacs P.A.,"['Chennai', 'Bengaluru']","Job Description\nRole Expert Cloud Data Engineer\nLocation Pune/Chennai\nExperience: 9 to 12 years and above.\nWhat awaits you/ Job Profile\nYou will lead strategic data engineering initiatives, ensuring that cloud data solutions align with business objectives. You will oversee large-scale projects, mentor teams, and drive innovation in cloud-based data engineering.\nKey Responsibilities:\nArchitect and implement enterprise-level data solutions that are scalable and secure.\nLead cross-functional teams to execute large-scale data projects.\nDevelop data governance strategies and lifecycle management best practices.\nExplore and integrate cutting-edge cloud and AI technologies to enhance data capabilities.\nMentor junior and senior engineers, fostering a culture of technical excellence.\nConduct high-level data analysis and make strategic recommendations.\nDrive cost optimization and performance tuning across cloud-based data platforms.\nWhat should you bring along\n8+ years of experience in data engineering, AWS Cloud Architecture, and DevOps.\nStrong understanding of data security, compliance, and governance in cloud environments.\nAbility to evaluate and implement emerging technologies in data engineering.\nExcellent knowledge of Terraform and GitHub Actions.\nExperienced in being in the lead of a feature team.\nMust have technical skill\nAdvanced Python, PySpark, SQL, Java/Scala (preferred)\nAWS (advanced expertise in Glue, Redshift, Athena, Kinesis, EMR), Cloud Architect Certification\nKafka, Flink, Spark Streaming\nGood to have technical skills\nTerraform, AWS CloudFormation\nGitHub Actions, Jenkins, Kubernetes\nIntegration of ML models into cloud data pipelines\nData Governance & Security: Role-based access control (RBAC), encryption, compliance frameworks",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'Terraform', 'Kafka', 'AWS', 'Python']",2025-06-13 06:03:22
Data Engineer I,Swiss Re,1 - 3 years,Not Disclosed,['Bengaluru'],"About the Role:\nAs a Data Engineer, you will be responsible for implementing data pipelines and analytics\nsolutions to support key decision-making processes in our Life Health Reinsurance business. You will become part of a project that is leveraging cutting edge technology that applies Big Data and Machine Learning to solve new and emerging problems for Swiss Re. You will be expected to gain a full understanding of the reinsurance data and business logic required to deliver analytics solutions.\nKey responsibilities include:\nWork closely with Product Owners and Engineering Leads to understand requirements and evaluate the implementation effort.\nDevelop and maintain scalable data transformation pipelines\nImplement analytics models and visualizations to provide actionable data insights\nCollaborate within a global development team to design and deliver solutions.\nAbout the Team:\nLife Health Data Analytics Engineering is a key tech partner for our Life Health Reinsurance division, supporting in the transformation of the data landscape and the creation of innovative analytical products and capabilities. A large globally distributed team working in an agile development landscape, we deliver solutions to make better use of our reinsurance data and enhance our ability to make data-driven decisions across the business value chain.\nAbout You:\nAre you eager to disrupt the industry with us and make an impactDo you wish to have your talent recognized and rewardedThen join our growing team and become part of the next wave of data\ninnovation. Key qualifications include:\nBachelors degree level or equivalent in Computer Science, Data Science or similar discipline\nAt least 1-3 years of experience working with large scale software systems\nProficient in Python/PySpark\nProficient in SQL (Spark SQL preferred)\nPalantir Foundry experience is a strong plus.\nExperience working with large data sets on enterprise data platforms and distributed computing (Spark/Hive/Hadoop preferred)\nExperience with JavaScript/HTML/CSS a plus\nExperience working in a Cloud environment such as AWS or Azure is a plus\nStrong analytical and problem-solving skills\nEnthusiasm to work in a global and multicultural environment of internal and external professionals\nStrong interpersonal and communication skills, demonstrating a clear and articulate standard of written and verbal communication in complex environments\nAbout Swiss Re\n.\n\n\n\nIf you are an experienced professional returning to the workforce after a career break, we encourage you to apply for open positions that match your skills and experience.\nKeywords:\nReference Code: 134085",Industry Type: Insurance,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Reinsurance', 'data science', 'spark', 'Analytical', 'Machine learning', 'Javascript', 'HTML', 'SQL', 'Python']",2025-06-13 06:03:24
Jr Data Engineer,BMW Techworks India,4 - 6 years,Not Disclosed,"['Chennai', 'Bengaluru']","What awaits you/ Job Profile\nAnalyze, Organize data and Build data systems with multiple data sources\nWorking closely with business and market team for the Data analysis.\nWhat should you bring along\nBuild data systems and pipelines\nStandardize the data assets which maximize data reusability\nEvaluate business needs and objectives\nImplementing data pipeline to ingest and cleanse raw data from various source systems including SaaS, Cloud based and On-prep databases\nCombine raw information from different sources\nExplore ways to enhance data quality and reliability\nImplementing functional requirements including incremental loads, data snapshots and identity resolution\nOptimizing data pipelines to improve performance and cost, while ensuring a high quality of data within the data lake:\nMonitoring services and jobs for cost and performance, ensuring continual operations of data pipelines, and fixing of defects.\nConstantly looking for opportunities to optimize data pipelines to improve performance.\nMust have technical skill\nMust have coding skills in Spark/Pyspark, Python and SQL\nMust have Knowledge of AWS tools, Glue, Athena ,step functions and S3\nGood to have technical skills\nGood knowledge of CI/CD (Github / Github Actions, Terraform)\nKnowledge on Agile methodologies.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data analysis', 'github', 'Coding', 'Agile', 'Data quality', 'Cost', 'AWS', 'Monitoring', 'SQL', 'Python']",2025-06-13 06:03:26
Staff Software Engineer - Cloud Data Pipeline,Calix,12 - 15 years,Not Disclosed,['Bengaluru'],"This position is based in Bangalore, India.\n\nCalix is leading a service provider transformation to deliver a differentiated subscriber experience around the Smart Home and Business, while monetizing their network using Role based Cloud Services, Telemetry, Analytics, Automation, and the deployment of Software Driven Adaptive networks.\nAs part of a high performing global team, the right candidate will play a significant role as Calix Cloud Data Engineer involved in architecture design, implementation, technical leadership in data ingestion, extraction, and transformation domain.\nResponsibilities and Duties:\nTechnical leadership in all phases of software design and development in meeting requirements of service stability, reliability, scalability, and security.\nHiring, training, provide technical direction and lead discussions and coordinate deliverables across multiple engineering teams globally.\nWork closely with Cloud product owners to understand, analyze product requirements, provide feedback, coordinate resources and deliver a complete solution.\nDrive evaluation and selection of best fit, efficient, cost-effective solution stack for the Calix Cloud data platform.\nDrive development of scalable data pipeline infrastructure and services for enabling operationally efficient analytics solutions for Calix Cloud suite of products.\nCreate and extend data lake solution to enable data science workbenches and implement quality systems to ensure data quality, consistency, security, compliance, and lineage.\nDrive continuous optimization of the data pipelines with automation, and tools.\nHave a Test first mindset and use modern DevSecOps practices for Agile development.\nCollaborate with senior leadership to translate platform opportunities into an actionable roadmap, track progress, and deliver new platform capabilities on-time and on-budget.\nTriage and resolve customer escalations and technical issues.\nQualifications:\n12 + years of highly technical, hands-on software engineering experience with at least 7 years of Cloud based solution development\n3+ experience leading and mentoring engineering team with strong technical direction and delivering high quality software on schedule, including delivery for large, cross-functional projects and working with geographically distributed teams\nStrong, creative problem-solving skills and ability to abstract and share details to create meaningful articulation.\nPassionate about delivering high quality software solutions and enabling automation in all phases.\nGood understanding of big data engineering challenges and proven experience with data platform engineering (batch and streaming, ingestion, storage, processing, management, governance, integration, consumption patterns)\nExperience in designing and performance tuning batch-based, low latency real-time streaming and event-based data solutions (Kafka, Spark, Flink or similar frameworks).\nPractical experience of architecting with GCP Cloud platform and services and especially the Data ecosystem: BigQuery, Datastream, DataProc, Composer etc.\nDeep understanding of Data Cataloging, Data Governance, Data Privacy principles and frameworks to integrate into the Data engineering flows.\nAdvanced knowledge of Data Lake technologies, data storage formats (Parquet, ORC, Avro) and query engines and associated concepts for consumption layers.\nExperience implementing solutions that adhere to best practices and guidelines for different privacy and compliance practices around data (GDPR, CCPA).\nHands on expert level on one or more of the following programming languages - Python, Java\nOrganized and goal-focused, ability to deliver in a fast-paced environment.\nBS degree in Computer Science, Engineering, Mathematics, or relevant industry standard experience to match",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Performance tuning', 'Automation', 'Software design', 'Quality systems', 'data governance', 'Data quality', 'data privacy', 'Analytics', 'Python']",2025-06-13 06:03:27
Aws Data Engineer,Hiring for Leading MNC Company,4 - 6 years,Not Disclosed,"['Hyderabad', 'Pune', 'Bengaluru']","Warm Greetings from SP Staffing!!\n\nRole:AWS Data Engineer\nExperience Required :4 to 6 yrs\nWork Location :Bangalore/Pune/Hyderabad/Chennai\n\nRequired Skills,\n\nPyspark\nAWS Glue\n\nInterested candidates can send resumes to nandhini.spstaffing@gmail.com",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'Aws Glue', 'python', 'EMR', 'Aws Emr', 'AWS Data Engineer', 'Aws Lambda', 'Lakehouse', 'spark', 'Data Engineer', 'ETL', 'AWS', 'Athena', 'gateway']",2025-06-13 06:03:29
Remote Data Visualization Engineer 36Lakhs CTC|| Kandi Srinivasa Reddy,Integra Technologies,8 - 11 years,35-37.5 Lacs P.A.,"['Kolkata', 'Ahmedabad', 'Bengaluru']","Dear Candidate,\nWe are seeking a Data Visualization Engineer to turn complex data into clear, engaging visual insights that empower business decisions. This role involves working closely with analysts and stakeholders to build interactive dashboards and reports.\n\nKey Responsibilities:\nDesign and develop visualizations using tools like Power BI, Tableau, or Looker.\nTranslate data into compelling stories and business insights.\nOptimize dashboard performance and usability for end users.\nCollaborate with data engineering and analytics teams.\nImplement visualization standards and data governance practices.\nRequired Skills & Qualifications:\nProficiency in data visualization tools (Power BI, Tableau, D3.js).\nStrong knowledge of SQL and data modeling.\nUnderstanding of UX principles in data presentation.\nExperience working with large datasets and cloud-based data platforms.\nKnowledge of scripting languages (Python, R) is a plus.\nSoft Skills:\nStrong troubleshooting and problem-solving skills.\nAbility to work independently and in a team.\nExcellent communication and documentation skills.\n\nNote: If interested, please share your updated resume and preferred time for a discussion. If shortlisted, our HR team will contact you.\n\nKandi Srinivasa Reddy\nDelivery Manager\nIntegra Technologies",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Tableau', 'Quicksight', 'Data Visualization', 'Dashboard Development', 'Business Intelligence', 'Power Bi', 'Bi Tools', 'Dashboarding', 'Business Objects', 'Reporting Tools', 'SQL', 'Microstrategy', 'Dashboards', 'QlikView']",2025-06-13 06:03:31
GCP Data Engineer,TVS Next,3 - 5 years,Not Disclosed,['Bengaluru'],"What you’ll be doing:\nAssist in developing machine learning models based on project requirements\nWork with datasets by preprocessing, selecting appropriate data representations, and ensuring data quality.\nPerforming statistical analysis and fine-tuning using test results.\nSupport training and retraining of ML systems as needed.\nHelp build data pipelines for collecting and processing data efficiently.",,,,"['kubernetes', 'pyspark', 'data pipeline', 'sql', 'docker', 'cloud', 'tensorflow', 'java', 'spark', 'gcp', 'pytorch', 'bigquery', 'programming', 'ml', 'cloud sql', 'cd', 'python', 'airflow', 'cloud spanner', 'cloud pubsub', 'application engine', 'machine learning', 'apache flink', 'data engineering', 'dataproc', 'kafka', 'cloud storage', 'terraform', 'bigtable']",2025-06-13 06:03:33
Azure Data Engineer,JRD Systems,7 - 12 years,Not Disclosed,"['Hyderabad', 'Bengaluru']","Cloud Data Engineer\n\nThe Cloud Data Engineer will be responsible for developing the data lake platform and all applications on Azure cloud. Proficiency in data engineering, data modeling, SQL, and Python programming is essential. The Data Engineer will provide design and development solutions for applications in the cloud.\nEssential Job Functions:\nUnderstand requirements and collaborate with the team to design and deliver projects.\nDesign and implement data lake house projects within Azure.\nDevelop application lifecycle utilizing Microsoft Azure technologies.\nParticipate in design, planning, and necessary documentation.\nEngage in Agile ceremonies including daily standups, scrum, retrospectives, demos, and code reviews.\nHands-on experience with Python/SQL development and Azure data pipelines.\nCollaborate with the team to develop and deliver cross-functional products.\nKey Skills:\na. Data Engineering and SQL\nb. Python\nc. PySpark\nd. Azure Data Lake and ADF\ne. Databricks\nf. CI/CD\ng. Strong communication\nOther Responsibilities:\nDocument and maintain project artifacts.\nMaintain comprehensive knowledge of industry standards, methodologies, processes, and best practices.\nComplete training as required for Privacy, Code of Conduct, etc.\nPromptly report any known or suspected loss, theft, or unauthorized disclosure or use of PI to the General Counsel/Chief Compliance Officer or Chief Information Officer.\nAdhere to the company's compliance program.\nSafeguard the company's intellectual property, information, and assets.\nOther duties as assigned.\nMinimum Qualifications and Job Requirements:\nBachelor's degree in Computer Science.\n7 years of hands-on experience in designing and developing distributed data pipelines.\n5 years of hands-on experience in Azure data service technologies.\n5 years of hands-on experience in Python, SQL, Object-oriented programming, ETL, and unit testing.\nExperience with data integration with APIs, Web services, Queues.\nExperience with Azure DevOps and CI/CD as well as agile tools and processes including JIRA, Confluence.\n*Required: Azure data engineering associate and databricks data engineering certification",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'Delta Table', 'Azure Databricks', 'SQL', 'Python', 'SCALA', 'Big Data', 'Kafka', 'Azure Data Lake', 'Spark', 'ETL', 'Data Bricks']",2025-06-13 06:03:35
Tech. PM - Data Engineering-Data Analytics@ Gurgaon/Blore_Urgent,A global leader in delivering innovative...,5 - 10 years,Not Disclosed,"['Gurugram', 'Bengaluru']","Job Title - Technical Project Manager\n\nLocation - Gurgaon/ Bangalore\n\nNature of Job - Permanent\n\nDepartment - data analytics\n\nWhat you will be doing\n\n\nDemonstrated client servicing and business analytics skills with at least 5 - 9 years of experience as data engineer, BI developer, data analyst, technical project manager, program manager etc.\nTechnical project management- drive BRD, project scope, resource allocation, team\ncoordination, stakeholder communication, UAT, Prod fix, change requests, project governance\nSound knowledge of banking industry (payments, retail operations, fraud etc.)\nStrong ETL experience or experienced Teradata developer\nManaging team of business analysts, BI developers, ETL developers to ensure that projects are completed on time\nResponsible for providing thought leadership and technical advice on business issues\nDesign methodological frameworks and solutions.\n\n\nWhat were looking for\n\n\nBachelors/masters degree in computer science/data science/AI/statistics, Certification in Gen AI. Masters degree Preferred.\nManage multiple projects, at a time, from inception to delivery\nSuperior problem-solving, analytical, and quantitative skills\nEntrepreneurial mindset, coupled with a “can do” attitude\nDemonstrated ability to collaborate with cross-functional, cross-border teams and coach / mentor colleagues.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Technical Project Manager', 'Data Engineering', 'multiple projects', 'Technical project management', 'Data Analytics', 'project scope', 'ETL Pipeline', 'team coordination', 'resource allocation', 'Prod fix', 'drive BRD', 'program manager', 'Big data']",2025-06-13 06:03:36
Data Analyst (1yr+ exp) - Mumbai,Aeke Consultancy,1 - 2 years,1.5-2.25 Lacs P.A.,['Mumbai (All Areas)'],"Hiring: Data Analyst (Bcom candidates only)\nSeeking candidates with 1+ year experience in Power BI, Excel, and Macros.\nBudget: 18,000/month\nLocation: Mumbai\nKindly share your CV via WhatsApp at 9076492644. Calls will not be entertained.",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Power Bi', 'Data Analysis', 'Excel Macros']",2025-06-13 06:03:38
Data Engineer,Mobile Programming,5 - 10 years,Not Disclosed,['Mumbai'],"Candidate Skill:Technical Skills - Data Engineering | ETL | SQL | Python | AWS | Azure | Google Cloud | Hadoop | Spark | Kafka | Data Warehousing | Data Modeling | NoSQL | Data Quality\nWe are looking for an experienced Data Engineer to join our team in Mumbai. As a Data Engineer, you will be responsible for designing, building, and maintaining efficient data pipelines that transform raw data into actionable insights. You will work closely with data scientists and analysts to ensure that data is accessible, reliable, and optimized for analysis. Your role will also involve handling large datasets, ensuring data quality, and implementing data processing frameworks.\nKey Responsibilities:Design and build scalable data pipelines for processing, transforming, and integrating large datasets.Develop and maintain ETL processes to extract, transform, and load data from multiple sources into the data warehouse.Collaborate with data scientists and analysts to ensure that the data is optimized for analysis and modeling.Ensure the quality, integrity, and security of data throughout its lifecycle.\nWork with cloud-based technologies for data storage and processing (AWS, Azure, GCP).Implement data processing frameworks for efficient handling of structured and unstructured data.Troubleshoot and resolve issues related to data pipelines and workflows.Automate data integration processes and ensure data consistency and accuracy across systems.\nRequired Skills:\n5+ years of experience in Data Engineering with hands-on experience in data pipeline development.Strong expertise in ETL processes, data integration, and data warehousing.Proficiency in SQL, Python, and other programming languages for data manipulation.Experience with cloud technologies such as AWS, Azure, or Google Cloud.Knowledge of big data technologies like Hadoop, Spark, or Kafka is a plus.Strong understanding of data modeling, data quality, and data governance.\nFamiliarity with NoSQL databases (e.g., MongoDB, Cassandra) and relational databases.Strong analytical and problem-solving skills with the ability to work with large, complex datasets.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Azure', 'Hadoop', 'Kafka', 'SQL', 'Data Quality', 'NoSQL', 'GCP', 'Spark', 'Data Warehousing', 'Data Modeling', 'ETL', 'AWS', 'Python']",2025-06-13 06:03:39
"Staff Engr - Data Science(Advanced OOPS,Python,PySpark,Databricks)",The TJX Companies Inc,5 - 10 years,Not Disclosed,['Hyderabad'],"TJX Companies\nAt TJX Companies, every day brings new opportunities for growth, exploration, and achievement. You ll be part of our vibrant team that embraces diversity, fosters collaboration, and prioritizes your development. Whether you re working in our four global Home Offices, Distribution Centers or Retail Stores TJ Maxx, Marshalls, Homegoods, Homesense, Sierra, Winners, and TK Maxx, you ll find abundant opportunities to learn, thrive, and make an impact. Come join our TJX family a Fortune 100 company and the world s leading off-price retailer.\nJob Description:\nAbout TJX:\nAt TJX, is a Fortune 100 company that operates off-price retailers of apparel and home fashions. TJX India - Hyderabad is the IT home office in the global technology organization of off-price apparel and home fashion retailer TJX, established to deliver innovative solutions that help transform operations globally. At TJX, we strive to build a workplace where our Associates contributions are welcomed and are embedded in our purpose to provide excellent value to our customers every day. At TJX India, we take a long-term view of your career. We have a high-performance culture that rewards Associates with career growth opportunities, preferred assignments, and upward career advancement. We take well-being very seriously and are committed to offering a great work-life balance for all our Associates.\nWhat you ll discover\nInclusive culture and career growth opportunities\nA truly Global IT Organization that collaborates across North America, Europe, Asia and Australia, click here to learn more\nChallenging, collaborative, and team-based environment\nWhat you ll do\nThe Global Supply Chain - Logistics Team is responsible for managing various supply chain logistics related solutions within TJX IT. The organization delivers capabilities that enrich the customer experience and provide business value. We seek a motivated, talented Staff E ngineer with good understanding of cloud base, database and BI concepts to help architect enterprise reporting solutions across global buying, planning and allocations .\nWhat you ll need\nThe Global Supply Chain - Logistics Team thrives on strong relationships with our business partners and working diligently to address their needs which supports TJX growth and operational stability. On this tightly knit and fast-paced solution delivery team you will be constantly challenged to stretch and think outside the box .\nYou will be working with product teams, architecture and business partners to strategically plan and deliver the product features by connecting the technical and business worlds. You will need to break down complex problems into steps that drive product development while keeping product quality and security as the priority. You will be responsible for most architecture, design and technical decisions within the assigned scope.\nKey Responsibilities:\nDesign, develop, test and deploy AI solutions using Azure AI services to meet business requirements , working collaboratively with architects and other engineers.\nTrain, fine-tune, and evaluate AI models, including large language models (LLMs), ensuring they meet performance criteria and integrate seamlessly into new or existing solutions.\nDevelop and integrate APIs to enable smooth interaction between AI models and other applications, facilitating efficient model serving.\nCollaborate effectively with cross-functional teams, including data scientists, software engineers, and business stakeholders, to deliver comprehensive AI solutions.\nOptimize AI and ML model performance through techniques such as hyperparameter tuning and model compression to enhance efficiency and effectiveness.\nMonitor and maintain AI systems, providing technical support and troubleshooting to ensure continuous operation and reliability.\nCreate comprehensive documentation for AI solutions, including design documents, user guides, and operational procedures, to support development and maintenance.\nStay updated with the latest advancements in AI, machine learning, and cloud technologies, demonstrating a commitment to continuous learning and improvement.\nDesign, code, deploy, and support software components, working collaboratively with AI architects and engineers to build impactful systems and services.\nLead medium complex initiatives, prioritizing and assigning tasks, providing guidance, and resolving issues to ensure successful project delivery.\nMinimum Qualifications\nBachelors degree in computer science, engineering, or related field\n8 + years of experience in data /software engineering, design, implementation and architecture.\nAt least 5+ years of hands-on experience in developing AI/ML solutions, with a focus on deploying them in a cloud environment .\nDeep understanding of AI and ML algorithms with focus on Operations Research / Optimization knowledge ( preferably M etaheuristics / Genetic Algorithms) .\nStrong programming skills in Python with advanced OOPS concepts.\nGood understanding of structured, semi structured, and unstructured data, Data modelling, Data analysis, ETL and ELT .\nProficiency with Databricks & PySpark .\nExperience with MLOps practices including CI/CD for machine learning models.\nKnowledge of security best practices for deploying AI solutions, including data encryption and access control.\nKnowledge of ethical considerations in AI, including bias detection and mitigation strategies.\nThis role operates in an Agile/Scrum environment and requires a solid understanding of the full software lifecycle, including functional requirement gathering, design and development, testing of software applications, and documenting requirements and technical specifications.\nFully Owns Epics with decreasing guidance. Takes initiative through identifying gaps and opportunities.\nStrong communication and influence skills. Solid team leadership with mentorship skills\nAbility to understand the work environment and competing priorities in conjunction with developing/meeting project goals .\nShows a positive, open-minded, and can-do attitude .\nExperience in the following technologies:\nAdvanced Python programming ( OOPS)\nOperations Research / Optimization knowledge ( preferably M etaheuristics / Genetic Algorithms)\nDatabricks with Pyspark\nAzure / Cloud knowledge\nGithub / version control\nFunctional knowledge on Supply Chain / Logistics is preferred.\nIn addition to our open door policy and supportive work environment, we also strive to provide a competitive salary and benefits package. TJX considers all applicants for employment without regard to race, color, religion, gender, sexual orientation, national origin, age, disability, gender identity and expression, marital or military status, or based on any individuals status in any group or class protected by applicable federal, state, or local law. TJX also provides reasonable accommodations to qualified individuals with disabilities in accordance with the Americans with Disabilities Act and applicable state and local law.\nAddress:\nSalarpuria Sattva Knowledge City, Inorbit Road\nLocation:\nAPAC Home Office Hyderabad IN",Industry Type: Retail,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Supply chain', 'Data analysis', 'Operations research', 'Architecture', 'OOPS', 'Cloud', 'Machine learning', 'Technical support', 'Python', 'Logistics']",2025-06-13 06:03:41
Data Engineer,Infoobjects Inc.,3 - 6 years,Not Disclosed,['Jaipur'],"Role & responsibilities:\nDesign, develop, and maintain robust ETL/ELT pipelines to ingest and process data from multiple sources.\nBuild and maintain scalable and reliable data warehouses, data lakes, and data marts.\nCollaborate with data scientists, analysts, and business stakeholders to understand data needs and deliver solutions.\nEnsure data quality, integrity, and security across all data systems.\nOptimize data pipeline performance and troubleshoot issues in a timely manner.\nImplement data governance and best practices in data management.\nAutomate data validation, monitoring, and reporting processes.\n\n\n\nPreferred candidate profile:\nBachelor's or Masters degree in Computer Science, Engineering, Information Systems, or related field.\nProven experience (X+ years) as a Data Engineer or similar role.\nStrong programming skills in Python, Java, or Scala.\nProficiency with SQL and working knowledge of relational databases (e.g., PostgreSQL, MySQL).\nHands-on experience with big data technologies (e.g., Spark, Hadoop).\nFamiliarity with cloud platforms such as AWS, GCP, or Azure (e.g., S3, Redshift, BigQuery, Data Factory).\nExperience with orchestration tools like Airflow or Prefect.\nKnowledge of data modeling, warehousing, and architecture design principles.\nStrong problem-solving skills and attention to detail.\n\nPerks and benefits\nFree Meals\nPF and Gratuity\nMedical and Term Insurance",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SCALA', 'Kafka', 'AWS', 'Python', 'Pyspark', 'Java', 'Postgresql', 'Hadoop', 'Spark', 'ETL', 'SQL']",2025-06-13 06:03:42
Data Engineer,Atyeti,2 - 4 years,Not Disclosed,['Pune'],"Role & responsibilities\n\nDevelop and Maintain Data Pipelines: Design, develop, and manage scalable ETL pipelines to process large datasets using PySpark, Databricks, and other big data technologies.\nData Integration and Transformation: Work with various structured and unstructured data sources to build efficient data workflows and integrate them into a central data warehouse.\nCollaborate with Data Scientists & Analysts: Work closely with the data science and business intelligence teams to ensure the right data is available for advanced analytics, machine learning, and reporting.",,,,"['Azure Synapse', 'Pyspark', 'ETL', 'Python']",2025-06-13 06:03:44
Data Engineer,Hinduja Tech,6 - 10 years,Not Disclosed,['Pune'],"Education: Bachelors or masters degree in computer science, Information Technology, Engineering, or a related field.Experience: 6-10 years\n8+ years of experience in data engineering or a related field.\nStrong hands-on experience with Azure Databricks, Spark, Python/Scala, CICD, Scripting for data processing.\nExperience working in multiple file formats like Parquet, Delta, and Iceberg.\nKnowledge of Kafka or similar streaming technologies for real-time data ingestion.",,,,"['Data Engineer', 'Azure Databricks', 'ETL', 'Pyspark', 'AWS', 'Python', 'SQL']",2025-06-13 06:03:46
Data Engineer,Luxoft,5 - 8 years,Not Disclosed,['Pune'],"Project description\nAre you passionate about leveraging the latest technologies for strategic changeDo you enjoy problem solving in clever waysAre you organized enough to drive change across complex data systemsIf so, you could be the right person for this role.\nAs an experienced data engineer, you will join a global data analytics team in our Group Chief Technology Officer / Enterprise Architecture organization supporting our strategic initiatives which ranges from portfolio health to integration.\n\nResponsibilities\n\nHelp Group Enterprise Architecture team to develop our suite of EA tools and workbenches\n\nWork in the development team to support the development of portfolio health insights\n\nBuild data applications from cloud infrastructure to visualization layer\n\nProduce clear and commented code\n\nProduce clear and comprehensive documentation\n\nPlay an active role with technology support teams and ensure deliverables are completed or escalated on time\n\nProvide support on any related presentations, communications, and trainings\n\nBe a team player, working across the organization with skills to indirectly manage and influence\n\nBe a self-starter willing to inform and educate others\n\nSkills\nMust have\n\nB.Sc./M.Sc. degree in computing or similar\n\n5-8+ years' experience as a Data Engineer, ideally in a large corporate environment\n\nIn-depth knowledge of SQL and data modelling/data processing\n\nStrong experience working with Microsoft Azure\n\nExperience with visualisation tools like PowerBI (or Tableau, QlikView or similar)\n\nExperience working with Git, JIRA, GitLab\n\nStrong flair for data analytics\n\nStrong flair for IT architecture and IT architecture metrics\n\nExcellent stakeholder interaction and communication skills\n\nUnderstanding of performance implications when making design decisions to deliver performant and maintainable software.\n\nExcellent end-to-end SDLC process understanding.\n\nProven track record of delivering complex data apps on tight timelines\n\nFluent in English both written and spoken.\n\nPassionate about development with focus on data and cloud\n\nAnalytical and logical, with strong problem solving skills\n\nA team player, comfortable with taking the lead on complex tasks\n\nAn excellent communicator who is adept in, handling ambiguity and communicating with both technical and non-technical audiences\n\nComfortable with working in cross-functional global teams to effect change\n\nPassionate about learning and developing your hard and soft professional skills\n\nNice to have\n\nExperience working in the financial industry\n\nExperience in complex metrics design and reporting\n\nExperience in using artificial intelligence for data analytics\n\nOther\n\nLanguages\n\nEnglishC1 Advanced\n\nSeniority\n\nSenior",Industry Type: Legal,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analytics', 'data processing', 'microsoft azure', 'sql', 'data modeling', 'screening', 'it architecture', 'hiring', 'power bi', 'hrsd', 'knowledge of sql', 'data engineering', 'artificial intelligence', 'sourcing', 'qlikview', 'talent acquisition', 'tableau', 'git', 'recruitment', 'gitlab', 'sdlc', 'jira']",2025-06-13 06:03:47
Data Engineer,Luxoft,5 - 10 years,Not Disclosed,['Pune'],"Project description\nYou'll be working in the GM Business Analytics team located in Pune. The successful candidate will be a member of the global Distribution team, which has team members in London and Pune.\n\nWe work as part of a global team providing analytical solutions for IB distribution/sales people. Solutions deployed should be extensible globally with minimal localization.\n\nResponsibilities\n\nAre you passionate about data and analyticsAre you keen to be part of the journey to modernize a data warehouse/ analytics suite of application(s). Do you take pride in the quality of software delivered for each development iteration\n\nWe're looking for someone like that to join us and\n\nbe a part of a high-performing team on a high-profile project.\n\nsolve challenging problems in an elegant way\n\nmaster state-of-the-art technologies\n\nbuild a highly responsive and fast updating application in an Agile & Lean environment\n\napply best development practices and effectively utilize technologies\n\nwork across the full delivery cycle to ensure high-quality delivery\n\nwrite high-quality code and adhere to coding standards\n\nwork collaboratively with diverse team(s) of technologists\n\nYou are:\n\nCurious and collaborative, comfortable working independently, as well as in a team\n\nFocused on delivery to the business\n\nStrong in analytical skills. For example, the candidate must understand the key dependencies among existing systems in terms of the flow of data among them. It is essential that the candidate learns to understand the 'big picture' of how IB industry/business functions.\n\nAble to quickly absorb new terminology and business requirements\n\nAlready strong in analytical tools, technologies, platforms, etc. The candidate must also demonstrate a strong desire for learning and self-improvement.\n\nOpen to learning home-grown technologies, support current state infrastructure and help drive future state migrations. imaginative and creative with newer technologies\n\nAble to accurately and pragmatically estimate the development effort required for specific objectives\n\nYou will have the opportunity to work under minimal supervision to understand local and global system requirements, design and implement the required functionality/bug fixes/enhancements. You will be responsible for components that are developed across the whole team and deployed globally.\n\nYou will also have the opportunity to provide third-line support to the application's global user community, which will include assisting dedicated support staff and liaising with the members of other development teams directly, some of which will be local and some remote.\n\nSkills\nMust have\n\nA bachelor's or master's degree, preferably in Information Technology or a related field (computer science, mathematics, etc.), focusing on data engineering.\n\n5+ years of relevant experience as a data engineer in Big Data is required.\n\nStrong Knowledge of programming languages (Python / Scala) and Big Data technologies (Spark, Databricks or equivalent) is required.\n\nStrong experience in executing complex data analysis and running complex SQL/Spark queries.\n\nStrong experience in building complex data transformations in SQL/Spark.\n\nStrong knowledge of Database technologies is required.\n\nStrong knowledge of Azure Cloud is advantageous.\n\nGood understanding and experience with Agile methodologies and delivery.\n\nStrong communication skills with the ability to build partnerships with stakeholders.\n\nStrong analytical, data management and problem-solving skills.\n\nNice to have\n\nExperience working on the QlikView tool\n\nUnderstanding of QlikView scripting and data model\n\nOther\n\nLanguages\n\nEnglishC1 Advanced\n\nSeniority\n\nSenior",Industry Type: Legal,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analysis', 'data management', 'big data technologies', 'sql', 'spark', 'python', 'scala', 'mathematics', 'business analytics', 'data engineering', 'azure cloud', 'qlikview', 'data bricks', 'computer science', 'database creation', 'data transformation', 'agile', 'big data', 'agile methodology']",2025-06-13 06:03:49
Data Engineer,Diverse Lynx,3 - 6 years,Not Disclosed,['Noida'],"Responsibilities\nAs part of the Client delivery team, your primary role would be to interface with the client for quality assurance, issue resolution and ensuring high customer satisfaction.\nYou will understand requirements, create and review designs, validate the architecture and ensure high levels of service offerings to clients in the technology domain.\nYou will participate in project estimation, provide inputs for solution delivery, conduct technical risk planning, perform code reviews and unit test plan reviews.\nYou will lead and guide your teams towards developing optimized high quality code deliverables, continual knowledge management and adherence to the organizational guidelines and processes.\nYou would be a key contributor to building efficient programs/ systems and if you think you fit right in to help our clients navigate their next in their digital transformation journey, this is the place for you!If you think you fit right in to help our clients navigate their next in their digital transformation journey, this is the place for you!",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Manager Quality Assurance', 'Project estimation', 'Architecture', 'Customer satisfaction', 'Test planning', 'Issue resolution', 'Unit testing', 'Management', 'Solution delivery', 'digital transformation']",2025-06-13 06:03:51
Data Engineer,Shyftlabs,5 - 10 years,Not Disclosed,['Noida'],"Position Overview\nWe are looking for an experienced Lead Data Engineer to join our dynamic team. If you are passionate about building scalable software solutions, and work collaboratively with cross-functional teams to define requirements and deliver solutions we would love to hear from you.\nJob Responsibilities:\nDevelop and maintain data pipelines and ETL/ELT processes using Python\nDesign and implement scalable, high-performance applications\nWork collaboratively with cross-functional teams to define requirements and deliver solutions\nDevelop and manage near real-time data streaming solutions using Pub, Sub or Beam.\nContribute to code reviews, architecture discussions, and continuous improvement initiatives\nMonitor and troubleshoot production systems to ensure reliability and performance\nBasic Qualifications:\n5+ years of professional software development experience with Python\nStrong understanding of software engineering best practices (testing, version control, CI/CD)\nExperience building and optimizing ETL/ELT processes and data pipelines\nProficiency with SQL and database concepts\nExperience with data processing frameworks (e.g., Pandas)\nUnderstanding of software design patterns and architectural principles\nAbility to write clean, well-documented, and maintainable code\nExperience with unit testing and test automation\nExperience working with any cloud provider (GCP is preferred)\nExperience with CI/CD pipelines and Infrastructure as code\nExperience with Containerization technologies like Docker or Kubernetes\nBachelors degree in Computer Science, Engineering, or related field (or equivalent experience)\nProven track record of delivering complex software projects\nExcellent problem-solving and analytical thinking skills\nStrong communication skills and ability to work in a collaborative environment\nPreferred Qualifications:\nExperience with GCP services, particularly Cloud Run and Dataflow\nExperience with stream processing technologies (Pub/Sub)\nFamiliarity with big data technologies (Airflow)\nExperience with data visualization tools and libraries\nKnowledge of CI/CD pipelines with Gitlab and infrastructure as code with Terraform\nFamiliarity with platforms like Snowflake, Bigquery or Databricks,.\nGCP Data engineer certification",Industry Type: Management Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Software design', 'Version control', 'Analytical', 'Data processing', 'Unit testing', 'data visualization', 'Continuous improvement', 'SQL', 'Python']",2025-06-13 06:03:53
Data Engineer,Mobio Solutions,3 - 5 years,Not Disclosed,['Ahmedabad'],"Job Overview:\nWe are looking for a skilled and experienced Data Engineer to join our team. The ideal candidate will have a strong background in Azure Data Factory, Databricks, Pyspark, Python , Azure SQL and other Azure cloud services, and will be responsible for building and managing scalable data pipelines, data lakes, and data warehouses . Experience with Azure Synapse Analytics, Microsoft Fabric or PowerBI will be considered a strong advantage.\nKey Responsibilities:\nDesign, develop, and manage robust and scalable ETL/ELT pipelines using Azure Data Factory and Databricks\nWork with PySpark and Python to transform and process large datasets\nBuild and maintain data lakes and data warehouses on Azure Cloud\nCollaborate with data architects, analysts, and stakeholders to gather and translate requirements into technical solutions\nEnsure data quality, consistency, and integrity across systems\nOptimize performance and cost of data pipelines and cloud infrastructure\nImplement best practices for security, governance, and monitoring of data pipelines\nMaintain and document data workflows and architecture\nRequired Skills & Qualifications:\n3-5 years of experience in Data Engineering\nStrong hands-on experience with:\nAzure Data Factory (ADF)\nAzure Databricks\nAzure SQL\nPySpark and Python\nAzure Storage (Blob, Data Lake Gen2)\nHands-on experience with data warehouse/Lakehouse/data lake architecture\nFamiliarity with Delta Lake, MLflow, and Unity Catalog is a plus\nGood understanding of SQL and performance tuning\nKnowledge of CI/CD in Azure for data pipelines\nExcellent problem-solving skills and ability to work independently\nPreferred Skills:\nExperience with Azure Synapse Analytics\nFamiliarity with Microsoft Fabric\nWorking knowledge of Power BI for data visualization and dashboarding\nExposure to DevOps and infrastructure as code (IaC) in Azure\nUnderstanding of data governance and security best practices\nDatabricks certification (e.g., Databricks Certified Data Engineer Associate/Professional)",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Performance tuning', 'Cloud', 'data governance', 'Data quality', 'data visualization', 'microsoft', 'Analytics', 'Monitoring', 'SQL', 'Python']",2025-06-13 06:03:55
Data Engineer - Asset Lending,Investec Global Services,2 - 5 years,Not Disclosed,['Mumbai'],"About the role :\nPrimary function of the role is to deliver high quality data engineering solutions to business and end users across Asset Lending (Asset Finance, Working Capital and Asset Based Lending businesses either directly via self-service data products, or by working closely with the Analytics team, providing modelled data warehouses on which they can add reporting and analytics.\nReporting to the Head of ccc Technology, this role will fill a crucial role in bridging the gap between business needs, the requirements from the data analytics team and translating these into engineering delivery.\nKey Responsibilities :\nWork closely with end-users and Data Analysts to understand the business and their data requirements\nCarry out ad hoc data analysis and data wrangling using Synapse Analytics and Databricks\nBuilding dynamic meta-data driven data ingestion patterns using Azure Data Factory and Databricks\nBuild and maintain the Enterprise Data Warehouse (using Data Vault 2.0 methodology)\nBuild and maintain business focused data products and data marts\nBuild and maintain Azure Analysis Services databases and cubes\nShare support and operational duties within the wider engineering and data teams\nWork with Architecture and Engineering teams to deliver on these projects. and ensure that supporting code and infrastructure follows best practices outlined by these teams.\nHelp define test criteria to establish clear conditions for success and ensure alignment with business objectives.\nManage their user stories and acceptance criteria through to production into day-to-day support\nAssist in the testing and validation of new requirements and processes to ensure they meet business need\nWhat are we looking for?\nExcellent data analysis and exploration using T-SQL\nStrong SQL programming (stored procedures, functions)\nExtensive experience with SQL Server and SSIS\nKnowledge and experience of data warehouse modelling methodologies (Kimball, dimensional modelling, Data Vault 2.0)\nExperience in Azure one or more of the following: Data Factory, Databricks, Synapse Analytics, ADLS Gen2\nExperience in building robust and performant ETL processes\nBuild and maintain Analysis Services databases and cubes (both multidimensional and tabular)\nExperience in using source control & ADO\nUnderstanding and experience of deployment pipelines\n",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data analysis', 'Agile', 'Stored procedures', 'SSIS', 'Operations', 'Data warehousing', 'Analytics', 'Analysis services', 'SQL', 'google maps']",2025-06-13 06:03:56
Data Engineer,Smartavya Analytica,3 - 5 years,Not Disclosed,['Pune'],"Job Title: Data Engineer\nLocation: Pune, India (On-site)\nExperience: 3 5 years\nEmployment Type: Full-time\n\nJob Summary\nWe are looking for a hands-on Data Engineer who can design and build modern Lakehouse solutions on Microsoft Azure. You will own data ingestion from source-system APIs through Azure Data Factory into OneLake, curate bronze silver gold layers on Delta Lake, and deliver dimensional models that power analytics at scale.",,,,"['Azure Data Factory', 'Azure Synapse', 'Adls Gen2', 'Data Lake', 'Fabric']",2025-06-13 06:03:58
Microsoft Fabric Data engineer,Bahwan CyberTek,12 - 14 years,20-30 Lacs P.A.,"['Indore', 'Hyderabad']","Microsoft Fabric Data engineer\n\nCTC Range 12 14 Years\nLocation – Hyderabad/Indore\nNotice Period - Immediate\n* Primary Skill\n\nMicrosoft Fabric\nSecondary Skill 1\n\nAzure Data Factory (ADF)\n12+ years of experience in Microsoft Azure Data Engineering for analytical projects.\nProven expertise in designing, developing, and deploying high-volume, end-to-end ETL pipelines for complex models, including batch, and real-time data integration frameworks using Azure, Microsoft Fabric and Databricks.\nExtensive hands-on experience with Azure Data Factory, Databricks (with Unity Catalog), Azure Functions, Synapse Analytics, Data Lake, Delta Lake, and Azure SQL Database for managing and processing large-scale data integrations.\nExperience in Databricks cluster optimization and workflow management to ensure cost-effective and high-performance processing.\nSound knowledge of data modelling, data governance, data quality management, and data modernization processes.\nDevelop architecture blueprints and technical design documentation for Azure-based data solutions.\nProvide technical leadership and guidance on cloud architecture best practices, ensuring scalable and secure solutions.\nKeep abreast of emerging Azure technologies and recommend enhancements to existing systems.\nLead proof of concepts (PoCs) and adopt agile delivery methodologies for solution development and delivery.\nwww.yash.com\n\n'Information transmitted by this e-mail is proprietary to YASH Technologies and/ or its Customers and is intended for use only by the individual or entity to which it is addressed, and may contain information that is privileged, confidential or exempt from disclosure under applicable law. If you are not the intended recipient or it appears that this mail has been forwarded to you without proper authority, you are notified that any use or dissemination of this information in any manner is strictly prohibited. In such cases, please notify us immediately at info@yash.com and delete this mail from your records.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Databricks', 'Azure Data Factory', 'Azure Synapse', 'Microsoft Fabric', 'Microsoft Azure Data Engineering', 'Pyspark', 'Data Bricks', 'SQL', 'Azure Data Lake', 'Microsoft Azure', 'Spark', 'ETL', 'Python']",2025-06-13 06:04:00
Data Engineer For a US based IT Company based in Hyderabad,GLOBAL INSTITUTE FOR STAFFING & TRAINING...,5 - 10 years,16-20 Lacs P.A.,['Hyderabad( Kokapeta Village )'],"We are Hiring Data Engineer for a US based IT Company Based in Hyderabad. Candidates with minimum 5 Years of experience in Data Engineering can apply.\n\nThis job is for 1 year contract only\n\nJob Title: Data Engineer\nLocation: Hyderabad\nCTC: Upto 20 LPA\nExperience: 5+ Years\n\nJob Overview:\nWe are looking for a seasoned Senior Data Engineer with deep hands-on experience in Talend and IBM DataStage to join our growing enterprise data team. This role will focus on designing and optimizing complex data integration solutions that support enterprise-wide analytics, reporting, and compliance initiatives.\nIn this senior-level position, you will collaborate with data architects, analysts, and key stakeholders to facilitate large-scale data movement, enhance data quality, and uphold governance and security protocols.\n\nKey Responsibilities:\nDevelop, maintain, and enhance scalable ETL pipelines using Talend and IBM DataStage\nPartner with data architects and analysts to deliver efficient and reliable data integration solutions\nReview and optimize existing ETL workflows for performance, scalability, and reliability\nConsolidate data from multiple sourcesboth structured and unstructuredinto data lakes and enterprise platforms\nImplement rigorous data validation and quality assurance procedures to ensure data accuracy and integrity\nAdhere to best practices for ETL development, including source control and automated deployment\nMaintain clear and comprehensive documentation of data processes, mappings, and transformation rules\nSupport enterprise initiatives around data migration, modernization, and cloud transformation\nMentor junior engineers and participate in code reviews and team learning sessions\nRequired Qualifications:\nMinimum 5 years of experience in data engineering or ETL development\nProficient with Talend (Open Studio and/or Talend Cloud) and IBM DataStage\nStrong skills in SQL, data profiling, and performance tuning\nExperience handling large datasets and complex data workflows\nSolid understanding of data warehousing, data modeling, and data lake architecture\nFamiliarity with version control systems (e.g., Git) and CI/CD pipelines\nStrong analytical and troubleshooting skills\nEffective verbal and written communication, with strong documentation habits\n\nPreferred Qualifications:\nPrior experience in banking or financial services\nExposure to cloud platforms such as AWS, Azure, or Google Cloud\nKnowledge of data governance tools (e.g., Collibra, Alation)\nAwareness of data privacy regulations (e.g., GDPR, CCPA)\nExperience working in Agile/Scrum environments\n\nFor further assistance contact/whatsapp: 9354909518 or write to priya@gist.org.in",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Temporary/Contractual","['Data Engineering', 'Kafka', 'Snowflake', 'Java', 'Spark', 'mongo', 'Ci/Cd', 'Data Pipeline', 'Agile', 'Scrum', 'Data Modeling', 'Talend', 'Oracle', 'AWS', 'Data Governance', 'Gdpr', 'azure', 'Python', 'Shell Scripting', 'Postgresql', 'Ibm Datastage', 'Workflow', 'Data Framework', 'GCPA', 'SQL', 'GIT', 'Amazon Redshift', 'GCP', 'Data Lake', 'Data Warehousing', 'ETL']",2025-06-13 06:04:01
Azure Data Engineer,Jurist Associates,5 - 10 years,7-12 Lacs P.A.,"['Kochi', 'Hyderabad', 'Bengaluru']","Design, build, and maintain scalable and efficient data pipelines using Azure services such as Azure Data Factory (ADF), Azure Databricks, and Azure Synapse Analytics. Develop and optimize ETL/ELT workflows for ingestion, cleansing, transformation,\n\nRequired Candidate profile\nStrong understanding of data warehouse architecture, data lakes, and big data frameworks. Candidates who have atleast 5 years of experience should only apply.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Data Engineering', 'Azure Data Factory', 'GenAI Tools', 'Azure Data Warehouse', 'Azure data lake Gen2', 'Azure Synapse Analytics', 'Azure Databricks', 'Azure Data Lake', 'Nosql Databases', 'Data Modeling', 'Semantic Analytics', 'ML/DL Models']",2025-06-13 06:04:03
Data Engineer,Konrad Group,3 - 7 years,15-30 Lacs P.A.,['Gurugram( Sector 42 Gurgaon )'],"Who We Are\n\nKonrad is a next generation digital consultancy. We are dedicated to solving complex business problems for our global clients with creative and forward-thinking solutions. Our employees enjoy a culture built on innovation and a commitment to creating best-in-class digital products in use by hundreds of millions of consumers around the world. We hire exceptionally smart, analytical, and hard working people who are lifelong learners.\nAbout The Role\nAs a Data Engineer youll be tasked with designing, building, and maintaining scalable data platforms and pipelines. Your deep knowledge of data platforms such as Azure Fabric, Databricks, and Snowflake will be essential as you collaborate closely with data analysts, scientists, and other engineers to ensure reliable, secure, and efficient data solutions.\n\nWhat Youll Do\n\nDesign, build, and manage robust data pipelines and data architectures.\nImplement solutions leveraging platforms such as Azure Fabric, Databricks, and Snowflake.\nOptimize data workflows, ensuring reliability, scalability, and performance.\nCollaborate with internal stakeholders to understand data needs and deliver tailored solutions.\nEnsure data security and compliance with industry standards and best practices.\nPerform data modelling, data extraction, transformation, and loading (ETL/ELT).\nIdentify and recommend innovative solutions to enhance data quality and analytics capabilities.\n\nQualifications\n\nBachelors degree or higher in Computer Science, Data Engineering, Information Technology, or a related field.\nAt least 3 years of professional experience as a Data Engineer or similar role.\nProficiency in data platforms such as Azure Fabric, Databricks, and Snowflake.\nHands-on experience with data pipeline tools, cloud services, and storage solutions.\nStrong programming skills in SQL, Python, or related languages.\nExperience with big data technologies and concepts (Spark, Hadoop, Kafka).\nExcellent analytical, troubleshooting, and problem-solving skills.\nAbility to effectively communicate technical concepts clearly to non-technical stakeholders.\nAdvanced English\n\nNice to have\n\nCertifications related to Azure Data Engineering, Databricks, or Snowflake.\nFamiliarity with DevOps practices and CI/CD pipelines.\n\nPerks and Benefits\n\nComprehensive Health & Wellness Benefits Package \nSocials, Outings & Retreats\nCulture of Learning & Development\nFlexible Working Hours\nWork from Home Flexibility\nService Recognition Programs\n\nKonrad is committed to maintaining a diverse work environment and is proud to be an equal opportunity employer. All qualified applicants, regardless of race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status will receive consideration for employment. If you have any accessibility requirements or concerns regarding the hiring process or employment with us, please notify us so we can provide suitable accommodation.\nWhile we sincerely appreciate all applications, only those candidates selected for an interview will be contacted.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Hadoop', 'Azure Data Factory', 'Azure Databricks', 'Spark', 'Fabric', 'Python']",2025-06-13 06:04:04
Data Engineer - SSIS - 5+ Years - Gurugram (Hybrid),Crescendo Global,5 - 10 years,Not Disclosed,['Gurugram'],"Data Engineer - SSIS - 5+ Years - Gurugram (Hybrid)\n\nAre you a skilled Data Engineer with expertise in SSIS and 5+ years of experience? Do you have a passion for analytics and want to work in a hybrid setup in Gurugram? Our client is seeking a talented individual to join their team and contribute to their data engineering projects.\n\nLocation : Gurugram (Hybrid)\n\nYour Future EmployerOur client is a leading organization in the analytics domain, known for fostering an inclusive and diverse work environment. They are committed to providing their employees with opportunities for growth and development.\n\nResponsibilities\nDesign, develop, and maintain data pipelines using SSIS for efficient data processing\nCollaborate with cross-functional teams to understand data requirements and provide effective data solutions\nOptimize data pipelines for performance and scalability\nEnsure data quality and integrity throughout the data engineering process\n\nRequirements\n5+ years of experience in data engineering with a strong focus on SSIS\nProficiency in data warehousing concepts and ETL processes\nHands-on experience with SQL databases and data modeling4.Strong analytical and problem-solving skills\nBachelor's degree in Computer Science, Engineering, or related field\n\nWhat's in it for you : In this role, you will have the opportunity to work on challenging projects and enhance your expertise in data engineering. The organization offers a competitive compensation package and a supportive work environment where your contributions are valued.\n\nReach us : If you feel this opportunity is well aligned with your career progression plans, please feel free to reach me with your updated profile at rohit.kumar@crescendogroup.in\n\nDisclaimer : Crescendo Global specializes in Senior to C-level niche recruitment. We are passionate about empowering job seekers and employers with an engaging memorable job search and leadership hiring experience. Crescendo Global does not discriminate on the basis of race, religion, color, origin, gender, sexual orientation, age, marital status, veteran status or disability status.\n\nNote : We receive a lot of applications on a daily basis so it becomes a bit difficult for us to get back to each candidate. Please assume that your profile has not been shortlisted in case you don't hear back from us in 1 week. Your patience is highly appreciated.\n\nScammers can misuse Crescendo Globals name for fake job offers. We never ask for money, purchases, or system upgrades. Verify all opportunities at www.crescendo-global.com and report fraud immediately. Stay alert!\n\nProfile keywords : Data Engineer, SSIS, Data Warehousing, ETL, SQL, Analytics",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Engineering', 'SSIS', 'SQL', 'Analytics']",2025-06-13 06:04:06
Data Engineer,Fortune India 500 Chemicals Firm,12 - 18 years,Not Disclosed,['Mumbai (All Areas)'],"Skills:\nData Management: Expertise in data warehousing, SQL/NoSQL, cloud platforms (AWS, Azure, GCP)\nETL Tools: Proficient in Informatica, Talend, Azure Data Factory\nModelling: Strong in dimensional modelling, star/snowflake schema\nGovernance & Compliance: Knowledge of GDPR, HIPAA, data governance frameworks\nLanguages: T-SQL, PL/SQL\nSoft Skills: Effective communicator, strong analytical and problem-solving skills\nKey Responsibilities:\nArchitecture: Designed scalable, high-performance data warehouse architectures and data models\nETL & Integration: Led ETL design/development for structured/unstructured data across platforms\nGovernance: Defined data quality standards and collaborated on data governance policy implementation\nCollaboration: Interfaced with BI, data science, and business teams to align data strategies\nPerformance & Security: Optimized queries/ETL jobs and ensured data security and compliance\nDocumentation: Maintained standards and documentation for architecture, ETL, and workflows",Industry Type: Chemicals,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Warehousing', 'GCP', 'Snowflake', 'Microsoft Azure', 'Dimensional Modeling', 'Data Modeling', 'ETL', 'AWS']",2025-06-13 06:04:07
DBA DATA Engineer || 12 Lakhs CTC,Robotics Technologies,6 - 10 years,12 Lacs P.A.,['Hyderabad( Banjara hills )'],"Dear Candidate,\nWe are seeking a skilled and experienced DBA Data Engineer to join our growing data team. The ideal candidate will play a key role in designing, implementing, and maintaining our databases and data pipeline architecture. You will collaborate with software engineers, data analysts, and DevOps teams to ensure efficient data flow, data integrity, and optimal database performance across all systems. This role requires a strong foundation in database administration, SQL performance tuning, data modeling, and experience with both on-prem and cloud-based environments.\n\nRequirements:\nBachelors degree in Computer Science, Information Systems, or a related field.\n6+ years of experience in database administration and data engineering.\nProven expertise in RDBMS (Oracle, MySQL, and PostgreSQL) and NoSQL systems (MongoDB, Cassandra).\nExperience managing databases in cloud environments (AWS, Azure, or GCP).\nProficiency in ETL processes and tools (e.g., Apache NiFi, Talend, Informatica, AWS Glue).\nStrong experience with scripting languages such as Python, Bash, or PowerShell.\n\nDBA Data Engineer Roles & Responsibilities:\nDesign and maintain scalable and high-performance database architectures.\nMonitor and optimize database performance using tools like CloudWatch, Oracle Enterprise Manager, pgAdmin, Mongo Compass, or Dynatrace.\nDevelop and manage ETL/ELT pipelines to support business intelligence and analytics.\nEnsure data integrity and security through best practices in backup, recovery, and encryption.\nAutomate regular database maintenance tasks using scripting and scheduled jobs.\nImplement high availability, failover, and disaster recovery strategies.\nConduct performance tuning of queries, stored procedures, indexes, and table structures.\nCollaborate with DevOps to automate database deployments using CI/CD and IaC tools (e.g., Terraform, AWS CloudFormation).\nDesign and implement data models, including star/snowflake schemas for data warehousing.\nDocument data flows, data dictionaries, and database configurations.\nManage user access and security policies using IAM roles or database-native permissions.\nAnalyze existing data systems and propose modernization or migration plans (on-prem to cloud, SQL to NoSQL, etc.).\nUse AWS RDS, Amazon Redshift, Azure SQL Database, or Google BigQuery as needed.\nStay up-to-date with emerging database technologies and make recommendations.\n\nMust-Have Skills:\nDeep knowledge of SQL and database performance tuning.\nHands-on experience with database migrations and replication strategies.\nFamiliarity with data governance, data quality, and compliance frameworks (GDPR, HIPAA, etc.).\nStrong problem-solving and troubleshooting skills.\nExperience with data streaming platforms such as Apache Kafka, AWS Kinesis, or Apache Flink is a plus.\nExperience with data lake and data warehouse architectures.\nExcellent communication and documentation skills.\n\nSoft Skills:\nProblem-Solving: Ability to analyze complex problems and develop effective solutions.\nCommunication Skills: Strong verbal and written communication skills to effectively collaborate with cross-functional teams.\nAnalytical Thinking: Ability to think critically and analytically to solve technical challenges.\nTime Management: Capable of managing multiple tasks and deadlines in a fast-paced environment.\nAdaptability: Ability to quickly learn and adapt to new technologies and methodologies.\n\nInterview Mode : F2F for who are residing in Hyderabad / Zoom for other states\nLocation : 43/A, MLA Colony,Road no 12, Banjara Hills, 500034\nTime : 2 - 4pm",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Database Administration', 'Database Migration', 'Dba Skills', 'High Availability', 'Db Administration', 'Hadr', 'Database Security', 'Disaster Recovery', 'Dr Testing', 'DR', 'Luw', 'Db Upgrade', 'Brtools', 'UDDI', 'Os Migration', 'Dbase', 'DBMS', 'IBM DB2', 'Db Migration', 'Disaster Recovery Planning', 'Db Dba', 'Backup And Recovery']",2025-06-13 06:04:09
Data Engineer - Financial Analytics Specialist,RWS Group,5 - 8 years,Not Disclosed,['Bengaluru'],"Job Purpose -\nWe are seeking a Data Engineer Financial Analytics Specialist to join our Data and Analytics team at RWS. This role combines advanced technical skills in SQL and ETL with analytical thinking and financial business acumen. You will be instrumental in designing and implementing data transformations and pipelines to support complex business requirements, particularly in financial data systems. This is a hands-on technical role with a significant focus on problem-solving, mathematical reasoning, and creating solutions for intricate business processes.\nThis position is ideal for someone with a quantitative background in computer science, mathematics, or a related field, who thrives on developing innovative solutions, optimizing data workflows, and ensuring data accuracy. We are looking for a candidate who is self-motivated, detail-oriented, and possesses exceptional technical skills, alongside a strong sense of ownership and accountability. The primary focus of this role will be financial data projects within a collaborative and dynamic team environment.\n\nAbout Group Technology-\nGroup Technology enables the organization to achieve its strategic direction whilst driving shareholder value. The division establishes common standards and IT governance across the business. It further develops and manages core applications enabling smooth operational running of the organization across all functions. We drive and deliver future roadmaps aligned to the overall strategic direction of the business. Group Technology support services to over 7500 end users across the globe, manage the information security operation and safeguard all our assets. Our core Group Technology functions include Technical Architecture, Network & Voice, IT Security, Service Delivery, Solutions Delivery and Asset Management. Group Technology has a global presence across all regions with over 400 staff.\nOur Data Engineering team is responsible for developing, building, maintaining, and managing data pipelines. This requires working with large datasets, databases, and the software used to analyse them including cloud systems like AWS or Azure.\n\nJob Overview\nKey Responsibilities\nWe are seeking a Data Engineer Financial Analytics Specialist to join our Data and Analytics team at RWS. This role combines advanced technical skills in SQL and ETL with analytical thinking and financial business acumen. You will be instrumental in designing and implementing data transformations and pipelines to support complex business requirements, particularly in financial data systems. This is a hands-on technical role with a significant focus on problem-solving, mathematical reasoning, and creating solutions for intricate business processes.\nThis position is ideal for someone with a quantitative background in computer science, mathematics, or a related field, who thrives on developing innovative solutions, optimizing data workflows, and ensuring data accuracy. We are looking for a candidate who is self-motivated, detail-oriented, and possesses exceptional technical skills, alongside a strong sense of ownership and accountability. The primary focus of this role will be financial data projects within a collaborative and dynamic team environment.\nKey Responsibilities\nWrite advanced T-SQL queries, stored procedures, and functions to handle complex data transformations, ensuring optimal performance and scalability.\nPerform query performance optimization, including indexing strategies and tuning for large datasets.\nDesign, develop, and maintain ETL/ELT pipelines using tools like SSIS, ADF, or equivalent, ensuring seamless data integration and transformation.\nBuild and automate data workflows to support accurate and efficient data operations.\nCollaborate with stakeholders to understand financial business processes and requirements.\nDevelop tailored data solutions to address challenges such as cost allocation, weighted distributions, and revenue recognition using mathematical and logical reasoning.\nConduct root cause analysis for data issues, resolving them swiftly to ensure data accuracy and reliability.\nApply critical thinking and problem-solving to transform business logic into actionable data workflows.\nPartner with cross-functional teams to ensure data solutions align with business objectives.\nDocument data workflows, pipelines, and processes to support ongoing maintenance and scalability.\nRequired Skills and Experiences\n\nMinimum 5+ years of experience require in the similar role or same skillset\nAdvanced proficiency in T-SQL and SQL Server, including tools like SSMS, SQL Profiler, and SQL Agent.\nProven experience developing and optimizing complex queries, stored procedures, and ETL pipelines.\nProficiency with SSIS for data integration and transformation.\nStrong critical thinking and mathematical reasoning skills to design efficient solutions for business problems.\nDemonstrated ability to handle complex financial data challenges, including proportional distributions and other mathematical transformations.\nSolid understanding of financial concepts and processes, enabling seamless translation of business needs into data solutions.\nAbility to work effectively with business stakeholders, translating requirements into technical solutions.\nExcellent interpersonal and communication skills, fostering collaboration across teams.\nFamiliarity with Python, C#, or similar scripting languages for backend integration.\nExperience with cloud ETL tools such as Azure Data Factory (ADF), Fivetran, or Airbyte.\nKnowledge of data governance and compliance principles.\n\nShift Timings-\nThere will be UK shift timings - 1:30 PM 9:30 PM (IST)\n\nRWS Values -\nGet the 3Ps right Partner, Pioneer, Progress and well Deliver together as One RWS.\nFor further information, please visit: RWS\nRWS embraces DEI and promotes equal opportunity, we are an Equal Opportunity Employer and prohibit discrimination and harassment of any kind. RWS is committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment. All employment decisions at RWS are based on business needs, job requirements and individual qualifications, without regard to race, religion, nationality, ethnicity, sex, age, disability, or sexual orientation. RWS will not tolerate discrimination based on any of these characteristics\nRecruitment Agencies: RWS Holdings PLC does not accept agency resumes. Please do not forward any unsolicited resumes to any RWS employees. Any unsolicited resume received will be treated as the property of RWS and Terms & Conditions associated with the use of such resume will be considered null and void.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['T-SQL', 'Analytical Skills', 'Finance', 'SSIS', 'SQL', 'Critical Thinking', 'Azure', 'Analytical Ability', 'SSRS', 'ETL']",2025-06-13 06:04:10
JavaScript Engineer For Training AI Data,G2i Inc,3 - 8 years,Not Disclosed,[],"Evaluating the quality of AI-generated code, including human-readable summaries of your rationale.\nBuilding and evaluating React components, hooks, and modern JavaScript solutions.\nSolving coding problems and writing functional and efficient JavaScript/React code.\nWriting robust test cases to confirm code works efficiently and effectively.\nCreating instructions to help others and reviewing code before it goes into the model.\nEngaging in a variety of projects, from evaluating code snippets to developing full mobile applications using chatbots.\nPay Rates\nCompensation rates average at $30/hr and can go up to $50+/hr. Expectations are 15+ hours per week; however, there is no upper limit. You can work as much as you want and will be paid weekly per hour of work done on the platform.\nContract Length\nThis is a long-term contract with no end date. We expect to have work for the next 2 years. You can end the contract at any time, but we hope you will commit to 12 months of work.\nFlexible Schedules\nDevelopers can set their own hours. Ideal candidates will be interested in spending 40 hours a week. You will be assigned to projects, so strong performers will adapt to the urgency of projects and stay engaged, but we are incredibly flexible on working hours. You can take a 3-hour lunch with no problem. Instead of tracking your hours, you are paid according to time spent on the platform, calculated in the coding exercises.\nInterview Process\nApply using this Ashby form.\nIf you seem like a good fit, well send an async RLHF code review that will take 35 minutes and must be finished within 72 hours of us sending it.\nYou ll receive credentials to the RLHF platform. We re doing regular calls to answer any further questions about onboarding, as well as providing a support team at your disposal.\nYou ll perform a simulated production-level task (RLHF task) on the platform. This will be the final stage, which will ultimately determine your leveling and which project you ll be assigned. Successful completion of this process provides you with an opportunity to work on projects as they become available.\nTech Stack Priorities\nThe current priority for this team is frontend engineers who are well versed in JavaScript, React, and modern web development frameworks and libraries.\nRequired Qualifications\n3+ years of experience in a software engineering/software development role.\nStrong proficiency with JavaScript/React and frontend development.\nComplete fluency in the English language.\nAbility to articulate complex technical concepts clearly and engagingly.\nExcellent attention to detail and ability to maintain consistency in writing. Solid understanding of grammar, punctuation, and style guidelines.\nNice To Haves:\nBachelors or Masters degree in Computer Science.\nExperience with modern JavaScript frameworks and libraries (Next.js, Vue, Angular).\nFamiliarity with frontend testing frameworks (Jest, React Testing Library, Cypress).\nKnowledge of state management solutions (Redux, Context API, MobX).\nExperience with TypeScript and modern frontend tooling.\nRecognized accomplishments or contributions to the coding community or in projects.\nProven analytical skills with an ability to approach problems creatively.\nAdept communication skills, especially when understanding and discussing project requirements.\nA commitment to continuous learning and staying updated with the latest coding advancements and best practices.\nEnthusiasm for teaching AI models and experience with technical writing!",Industry Type: Software Product,Department: Other,"Employment Type: Full Time, Permanent","['Computer science', 'Analytical skills', 'Front end', 'Technical writing', 'Coding', 'Web development', 'Javascript', 'Test cases', 'Mobile applications', 'Software engineering']",2025-06-13 06:04:12
Power BI - Data Analyst,Hakkda,3 - 6 years,Not Disclosed,['Jaipur'],"ABOUT HAKKODA\n\nHakkoda, an IBM Company, is a modern data consultancy that empowers data driven organizations to realize the full value of the Snowflake Data Cloud. We provide consulting and managed services in data architecture, data engineering, analytics and data science. We are renowned for bringing our clients deep expertise, being easy to work with, and being an amazing place to work! We are looking for curious and creative individuals who want to be part of a fast-paced, dynamic environment, where everyone s input and efforts are valued. We hire outstanding individuals and give them the opportunity to thrive in a collaborative atmosphere that values learning, growth, and hard work. Our team is distributed across North America, Latin America, India and Europe. If you have the desire to be a part of an exciting, challenging, and rapidly-growing Snowflake consulting services company, and if you are passionate about making a difference in this world, we would love to talk to you!.\n\nWe are looking for a skilled and motivated Data Analyst / Data Engineer to join our growing data team in Jaipur. The ideal candidate should have hands-on experience with SQL, Python, Power BI , and familiarity with Snowflake is a strong advantage. You will play a key role in building data pipelines, delivering analytical insights, and enabling data-driven decision-making across the organization.\nRole Description:\nDevelop and manage robust data pipelines and workflows for data integration, transformation, and loading.\nDesign, build, and maintain interactive Power BI dashboards and reports based on business needs.\nOptimize existing Power BI reports for performance, usability, and scalability .\nWrite and optimize complex SQL queries for data analysis and reporting.\nUse Python for data manipulation, automation, and advanced analytics where applicable.\nCollaborate with business stakeholders to understand requirements and deliver actionable insights .\nEnsure high data quality, integrity, and governance across all reporting and analytics layers.\nWork closely with data engineers, analysts, and business teams to deliver scalable data solutions .\nLeverage cloud data platforms like Snowflake for data warehousing and analytics (good to have).\nQualifications\n3-6 years of professional experience in data analysis or data engineering.\nBachelor s degree in computer science , Engineering, Data Science, Information Technology , or a related field.\nStrong proficiency in SQL with the ability to write complex queries and perform data modeling.\nHands-on experience with Power BI for data visualization and business intelligence reporting.\nProgramming knowledge in Python for data processing and analysis.\nGood understanding of ETL/ELT , data warehousing concepts, and cloud-based data ecosystems.\nExcellent problem-solving skills , attention to detail, and analytical thinking.\nStrong communication and interpersonal skills to work effectively with cross-functional teams .\nPreferred / Good to Have\nExperience working with large datasets and cloud platforms like Snowflake, Redshift, or BigQuery.\nFamiliarity with workflow orchestration tools (e.g., Airflow) and version control systems (e.g., Git).\nPower BI Certification (e.g., PL-300: Microsoft Power BI Data Analyst).\nExposure to Agile methodologies and end-to-end BI project life cycles.\nBenefits:\n\n- Health Insurance\n- Paid leave\n- Technical training and certifications\n- Robust learning and development opportunities\n- Incentive\n- Toastmasters\n- Food Program\n- Fitness Program\n- Referral Bonus Program\n\nHakkoda is committed to fostering diversity, equity, and inclusion within our teams. A diverse workforce enhances our ability to serve clients and enriches our culture. We encourage candidates of all races, genders, sexual orientations, abilities, and experiences to apply, creating a workplace where everyone can succeed and thrive.\n\nReady to take your career to the next level? Apply today and join a team that s shaping the future!!\n\nHakkoda is an IBM subsidiary which has been acquired by IBM and will be integrated in the IBM organization. Hakkoda will be the hiring entity. By Proceeding with this application, you understand that Hakkoda will share your personal information with other IBM subsidiaries involved in your recruitment process, wherever these are located. More information on how IBM protects your personal information, including the safeguards in case of cross-border data transfer, are available here.",Industry Type: Analytics / KPO / Research,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Automation', 'Data modeling', 'Consulting', 'Agile', 'Workflow', 'microsoft', 'Information technology', 'Analytics', 'SQL', 'Python']",2025-06-13 06:04:13
Devops AWS DATA Engineeer|| Technical Analyst || 12Lakhs CTC,Robotics Technologies,8 - 9 years,11-12 Lacs P.A.,['Hyderabad( Banjara hills )'],"We are seeking a highly skilled Devops Engineer to join our dynamic development team. In this role, you will be responsible for designing, developing, and maintaining both frontend and backend components of our applications using Devops and associated technologies.\nYou will collaborate with cross-functional teams to deliver robust, scalable, and high-performing software solutions that meet our business needs. The ideal candidate will have a strong background in devops, experience with modern frontend frameworks, and a passion for full-stack development.\n\nRequirements:\nBachelor's degree in Computer Science Engineering, or a related field.\n8 to 9+ years of experience in full-stack development, with a strong focus on DevOps.\n\nDevOps with AWS Data Engineer - Roles & Responsibilities:\nUse AWS services like EC2, VPC, S3, IAM, RDS, and Route 53.\nAutomate infrastructure using Infrastructure as Code (IaC) tools like Terraform or AWS CloudFormation.\nBuild and maintain CI/CD pipelines using tools AWS CodePipeline, Jenkins,GitLab CI/CD.\nCross-Functional Collaboration\nAutomate build, test, and deployment processes for Java applications.\nUse Ansible, Chef, or AWS Systems Manager for managing configurations across environments.\nContainerize Java apps using Docker.\nDeploy and manage containers using Amazon ECS, EKS (Kubernetes), or Fargate.\nMonitoring & Logging using Amazon CloudWatch,Prometheus + Grafana,E\nStack (Elasticsearch, Logstash, Kibana),AWS X-Ray for distributed tracing manage access with IAM roles/policies.\nUse AWS Secrets Manager / Parameter Store for managing credentials.\nEnforce security best practices, encryption, and audits.\nAutomate backups for databases and services using AWS Backup, RDS Snapshots, and S3 lifecycle rules.\nImplement Disaster Recovery (DR) strategies.\nWork closely with development teams to integrate DevOps practices.\nDocument pipelines, architecture, and troubleshooting runbooks.\nMonitor and optimize AWS resource usage.\nUse AWS Cost Explorer, Budgets, and Savings Plans.\n\nMust-Have Skills:\nExperience working on Linux-based infrastructure.\nExcellent understanding of Ruby, Python, Perl, and Java.\nConfiguration and managing databases such as MySQL, Mongo.\nExcellent troubleshooting.\nSelecting and deploying appropriate CI/CD tools\nWorking knowledge of various tools, open-source technologies, and cloud services.\nAwareness of critical concepts in DevOps and Agile principles.\nManaging stakeholders and external interfaces.\nSetting up tools and required infrastructure.\nDefining and setting development, testing, release, update, and support processes for DevOps operation.\nHave the technical skills to review, verify, and validate the software code developed in the project.\nInterview Mode : F2F for who are residing in Hyderabad / Zoom for other states\nLocation : 43/A, MLA Colony,Road no 12, Banjara Hills, 500034\nTime : 2 - 4pm",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Iac', 'Devops', 'Jenkins', 'AWS', 'Kubernetes', 'RDS', 'Aws Cloudformation', 'Amazon Cloudwatch', 'Prometheus', 'Ci/Cd', 'Grafana', 'DR', 'Cloud Trail', 'Docker', 'IAM', 'Ansible / Chef', 'fargate', 'Gitlab', 'Monitoring', 'Python']",2025-06-13 06:04:15
Hadoop Data Engineer,Envision Technology Solutions,3 - 8 years,5-15 Lacs P.A.,"['New Delhi', 'Hyderabad', 'Gurugram']","Primary Skill – Hadoop, Hive, Python, SQL, Pyspark/Spark.\nLocation –Hyderabad / Gurgaon;",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Hadoop', 'Hive', 'Spark', 'Python', 'SQL']",2025-06-13 06:04:16
"Applied Scientist, Alexa Sensitive Content Intelligence (ASCI)",Amazon,3 - 8 years,Not Disclosed,['Bengaluru'],"Alexa is the voice activated digital assistant powering devices like Amazon Echo, Echo Dot, Echo Show, and Fire TV, which are at the forefront of this latest technology wave. To preserve our customers experience and trust, the Alexa Privacy team creates policies and builds services and tools through Machine Learning techniques to detect and mitigate sensitive content across Alexa. We are looking for an experienced Senior Applied Scientist to build industry-leading technologies in attribute extraction and sensitive content detection across all languages and countries.\n\nAn Applied Scientist will be in a team of exceptional scientists to develop novel algorithms and modeling techniques to advance the state of the art in Natural Language Processing (NLP) or Computer Vision (CV) related tasks. They will work in a hybrid, fast-paced organization where scientists, engineers, and product managers work together to build customer facing experiences. They will collaborate with and mentor other scientists to raise the bar of scientific research in Amazon. Their work will directly impact our customers in the form of products and services that make use of speech, language, and computer vision technologies.\n\nWe are looking for candidate with strong technical experiences and a passion for building scientific driven solutions in a fast-paced environment. This Senior Applied Scientist should have good understanding of NLP models (e.g. LSTM, transformer based models) or CV models (e.g. CNN, AlexNet, ResNet) and where to apply them in different business cases. They should leverage exceptional technical expertise, a sound understanding of the fundamentals of Computer Science, and practical experience of building large-scale distributed systems to creating reliable, scalable, and high-performance products. In addition to technical depth, they must possess exceptional communication skills and understand how to influence key stakeholders.\n\nThis Applied Scientist will be joining a select group of people making history producing one of the most highly rated products in Amazons history, so if you are looking for a challenging and innovative role where you can solve important problems while growing as a leader, this may be the place for you.\n\n\nThis Applied Scientist will lead the science solution design, run experiments, research new algorithms, and find new ways of optimizing customer experience. They will set examples for the team on good science practice and standards. Besides theoretical analysis and innovation, they will work closely with talented engineers and ML scientists to put algorithms and models into practice.\n\nThis Applied Scientists work will also directly impact the trust customers place in Alexa, globally. They will contribute directly to our growth by hiring smart and motivated scientists to establish teams that can deliver swiftly and predictably, adjusting in an agile fashion to deliver what our customers need.\n\nA day in the life\nYou will be working with a group of talented scientists on researching algorithm and running experiments to test scientific proposal/solutions to improve our sensitive contents detection and mitigation. This will involve collaboration with partner teams including engineering, PMs, data annotators, and other scientists to discuss data quality, policy, and model development. You will mentor other scientists, review and guide their work, help develop roadmaps for the team. You work closely with partner teams across Alexa to deliver platform features that require cross-team leadership.\n\nAbout the team\nThe mission of the Alexa Sensitive Content Intelligence (ASCI) team is to (1) minimize negative surprises to customers caused by sensitive content, (2) detect and prevent potential brand-damaging interactions, and (3) build customer trust through appropriate interactions on sensitive topics.\nThe term sensitive content includes within its scope a wide range of categories of content such as offensive content (e.g., hate speech, racist speech), profanity, content that is suitable only for certain age groups, politically polarizing content, and religiously polarizing content. The term content refers to any material that is exposed to customers by Alexa (including both 1P and 3P experiences) and includes text, speech, audio, and video. 3+ years of building models for business application experience\nPhD, or Masters degree and 4+ years of CS, CE, ML or related field experience\nExperience in patents or publications at top-tier peer-reviewed conferences or journals\nExperience programming in Java, C++, Python or related language\nExperience in any of the following areas: algorithms and data structures, parsing, numerical optimization, data mining, parallel and distributed computing, high-performance computing Experience using Unix/Linux\nExperience in professional software development",,,,"['Unix', 'Computer science', 'Computer vision', 'C++', 'Linux', 'Machine learning', 'Agile', 'Data structures', 'Data mining', 'Python']",2025-06-13 06:04:18
Data Modeler - SQL/Erwin,Leading Client,7 - 10 years,Not Disclosed,['Pune'],"Job Summary :\n\nWe are looking for a highly skilled Data Engineer / Data Modeler with strong experience in Snowflake, DBT, and GCP to support our data infrastructure and modeling initiatives.\n\nThe ideal candidate should possess excellent SQL skills, hands-on experience with Erwin Data Modeler, and a strong background in modern data architectures and data modeling techniques.\n\nKey Responsibilities :\n\n- Design and implement scalable data models using Snowflake and Erwin Data Modeler.\n\n- Create, maintain, and enhance data pipelines using DBT and GCP (BigQuery, Cloud Storage, Dataflow).\n\n- Perform reverse engineering on existing systems (e.g., Sailfish/DDMS) using DBeaver or similar tools to understand and rebuild data models.\n\n- Develop efficient SQL queries and stored procedures for data transformation, quality, and validation.\n\n- Collaborate with business analysts and stakeholders to gather data requirements and convert them into physical and logical models.\n\n- Ensure performance tuning, security, and optimization of the Snowflake data warehouse.\n\n- Document metadata, data lineage, and business logic behind data structures and flows.\n\n- Participate in code reviews, enforce coding standards, and provide best practices for data modeling and governance.\n\nMust-Have Skills :\n\n- Snowflake architecture, schema design, and data warehouse experience.\n\n- DBT (Data Build Tool) for data transformation and pipeline development.\n\n- Strong expertise in SQL (query optimization, complex joins, window functions, etc.)\n\n- Hands-on experience with Erwin Data Modeler (logical and physical modeling).\n\n- Experience with GCP (BigQuery, Cloud Composer, Cloud Storage).\n\n- Experience in reverse engineering legacy systems like Sailfish or DDMS using DBeaver.\n\nGood To Have :\n\n- Experience with CI/CD tools and DevOps for data environments.\n\n- Familiarity with data governance, security, and privacy practices.\n\n- Exposure to Agile methodologies and working in distributed teams.\n\n- Knowledge of Python for data engineering tasks and orchestration scripts.\n\nSoft Skills :\n\n- Excellent problem-solving and analytical skills.\n\n- Strong communication and stakeholder management.\n\n- Self-driven with the ability to work independently in a remote setup.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SQL', 'Data Quality', 'Data Build Tool', 'Data Security', 'Snowflake DB', 'Data Modeling', 'Data Warehousing', 'Data Analytics', 'Data Governance', 'Erwin']",2025-06-13 06:04:20
Data Modeler - SQL/Erwin,Leading Client,7 - 10 years,Not Disclosed,['Indore'],"Job Summary :\n\nWe are looking for a highly skilled Data Engineer / Data Modeler with strong experience in Snowflake, DBT, and GCP to support our data infrastructure and modeling initiatives.\n\nThe ideal candidate should possess excellent SQL skills, hands-on experience with Erwin Data Modeler, and a strong background in modern data architectures and data modeling techniques.\n\nKey Responsibilities :\n\n- Design and implement scalable data models using Snowflake and Erwin Data Modeler.\n\n- Create, maintain, and enhance data pipelines using DBT and GCP (BigQuery, Cloud Storage, Dataflow).\n\n- Perform reverse engineering on existing systems (e.g., Sailfish/DDMS) using DBeaver or similar tools to understand and rebuild data models.\n\n- Develop efficient SQL queries and stored procedures for data transformation, quality, and validation.\n\n- Collaborate with business analysts and stakeholders to gather data requirements and convert them into physical and logical models.\n\n- Ensure performance tuning, security, and optimization of the Snowflake data warehouse.\n\n- Document metadata, data lineage, and business logic behind data structures and flows.\n\n- Participate in code reviews, enforce coding standards, and provide best practices for data modeling and governance.\n\nMust-Have Skills :\n\n- Snowflake architecture, schema design, and data warehouse experience.\n\n- DBT (Data Build Tool) for data transformation and pipeline development.\n\n- Strong expertise in SQL (query optimization, complex joins, window functions, etc.)\n\n- Hands-on experience with Erwin Data Modeler (logical and physical modeling).\n\n- Experience with GCP (BigQuery, Cloud Composer, Cloud Storage).\n\n- Experience in reverse engineering legacy systems like Sailfish or DDMS using DBeaver.\n\nGood To Have :\n\n- Experience with CI/CD tools and DevOps for data environments.\n\n- Familiarity with data governance, security, and privacy practices.\n\n- Exposure to Agile methodologies and working in distributed teams.\n\n- Knowledge of Python for data engineering tasks and orchestration scripts.\n\nSoft Skills :\n\n- Excellent problem-solving and analytical skills.\n\n- Strong communication and stakeholder management.\n\n- Self-driven with the ability to work independently in a remote setup.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SQL', 'Data Quality', 'Data Build Tool', 'Data Security', 'Snowflake DB', 'Data Modeling', 'Data Warehousing', 'Data Analytics', 'Data Governance', 'Erwin']",2025-06-13 06:04:21
Data Modeler - SQL/Erwin,Leading Client,7 - 10 years,Not Disclosed,[],"Job Summary :\n\nWe are looking for a highly skilled Data Engineer / Data Modeler with strong experience in Snowflake, DBT, and GCP to support our data infrastructure and modeling initiatives.\n\nThe ideal candidate should possess excellent SQL skills, hands-on experience with Erwin Data Modeler, and a strong background in modern data architectures and data modeling techniques.\n\nKey Responsibilities :\n\n- Design and implement scalable data models using Snowflake and Erwin Data Modeler.\n\n- Create, maintain, and enhance data pipelines using DBT and GCP (BigQuery, Cloud Storage, Dataflow).\n\n- Perform reverse engineering on existing systems (e.g., Sailfish/DDMS) using DBeaver or similar tools to understand and rebuild data models.\n\n- Develop efficient SQL queries and stored procedures for data transformation, quality, and validation.\n\n- Collaborate with business analysts and stakeholders to gather data requirements and convert them into physical and logical models.\n\n- Ensure performance tuning, security, and optimization of the Snowflake data warehouse.\n\n- Document metadata, data lineage, and business logic behind data structures and flows.\n\n- Participate in code reviews, enforce coding standards, and provide best practices for data modeling and governance.\n\nMust-Have Skills :\n\n- Snowflake architecture, schema design, and data warehouse experience.\n\n- DBT (Data Build Tool) for data transformation and pipeline development.\n\n- Strong expertise in SQL (query optimization, complex joins, window functions, etc.)\n\n- Hands-on experience with Erwin Data Modeler (logical and physical modeling).\n\n- Experience with GCP (BigQuery, Cloud Composer, Cloud Storage).\n\n- Experience in reverse engineering legacy systems like Sailfish or DDMS using DBeaver.\n\nGood To Have :\n\n- Experience with CI/CD tools and DevOps for data environments.\n\n- Familiarity with data governance, security, and privacy practices.\n\n- Exposure to Agile methodologies and working in distributed teams.\n\n- Knowledge of Python for data engineering tasks and orchestration scripts.\n\nSoft Skills :\n\n- Excellent problem-solving and analytical skills.\n\n- Strong communication and stakeholder management.\n\n- Self-driven with the ability to work independently in a remote setup.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SQL', 'Data Quality', 'Data Build Tool', 'Data Security', 'Snowflake DB', 'Data Modeling', 'Data Warehousing', 'Data Analytics', 'Data Governance', 'Erwin']",2025-06-13 06:04:22
Data Modeler - SQL / Erwin,Leading Client,7 - 10 years,Not Disclosed,[],"Employment Type : Contract (Remote).\n\nJob Summary :\n\nWe are looking for a highly skilled Data Engineer / Data Modeler with strong experience in Snowflake, DBT, and GCP to support our data infrastructure and modeling initiatives.\n\nThe ideal candidate should possess excellent SQL skills, hands-on experience with Erwin Data Modeler, and a strong background in modern data architectures and data modeling techniques.\n\nKey Responsibilities :\n\n- Design and implement scalable data models using Snowflake and Erwin Data Modeler.\n\n- Create, maintain, and enhance data pipelines using DBT and GCP (BigQuery, Cloud Storage, Dataflow).\n\n- Perform reverse engineering on existing systems (e.g., Sailfish/DDMS) using DBeaver or similar tools to understand and rebuild data models.\n\n- Develop efficient SQL queries and stored procedures for data transformation, quality, and validation.\n\n- Collaborate with business analysts and stakeholders to gather data requirements and convert them into physical and logical models.\n\n- Ensure performance tuning, security, and optimization of the Snowflake data warehouse.\n\n- Document metadata, data lineage, and business logic behind data structures and flows.\n\n- Participate in code reviews, enforce coding standards, and provide best practices for data modeling and governance.\n\nMust-Have Skills :\n\n- Snowflake architecture, schema design, and data warehouse experience.\n\n- DBT (Data Build Tool) for data transformation and pipeline development.\n\n- Strong expertise in SQL (query optimization, complex joins, window functions, etc.)\n\n- Hands-on experience with Erwin Data Modeler (logical and physical modeling).\n\n- Experience with GCP (BigQuery, Cloud Composer, Cloud Storage).\n\n- Experience in reverse engineering legacy systems like Sailfish or DDMS using DBeaver.\n\nGood To Have :\n\n- Experience with CI/CD tools and DevOps for data environments.\n\n- Familiarity with data governance, security, and privacy practices.\n\n- Exposure to Agile methodologies and working in distributed teams.\n\n- Knowledge of Python for data engineering tasks and orchestration scripts.\n\nSoft Skills :\n\n- Excellent problem-solving and analytical skills.\n\n- Strong communication and stakeholder management.\n\n- Self-driven with the ability to work independently in a remote setup.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SQL', 'Data Quality', 'Data Build Tool', 'Data Security', 'Snowflake DB', 'Data Modeling', 'Data Warehousing', 'Data Analytics', 'Data Governance', 'Erwin']",2025-06-13 06:04:24
Data Modeler - SQL / Erwin,Leading Client,7 - 10 years,Not Disclosed,[],"Employment Type : Contract (Remote).\n\nJob Summary :\n\nWe are looking for a highly skilled Data Engineer / Data Modeler with strong experience in Snowflake, DBT, and GCP to support our data infrastructure and modeling initiatives.\n\nThe ideal candidate should possess excellent SQL skills, hands-on experience with Erwin Data Modeler, and a strong background in modern data architectures and data modeling techniques.\n\nKey Responsibilities :\n\n- Design and implement scalable data models using Snowflake and Erwin Data Modeler.\n\n- Create, maintain, and enhance data pipelines using DBT and GCP (BigQuery, Cloud Storage, Dataflow).\n\n- Perform reverse engineering on existing systems (e.g., Sailfish/DDMS) using DBeaver or similar tools to understand and rebuild data models.\n\n- Develop efficient SQL queries and stored procedures for data transformation, quality, and validation.\n\n- Collaborate with business analysts and stakeholders to gather data requirements and convert them into physical and logical models.\n\n- Ensure performance tuning, security, and optimization of the Snowflake data warehouse.\n\n- Document metadata, data lineage, and business logic behind data structures and flows.\n\n- Participate in code reviews, enforce coding standards, and provide best practices for data modeling and governance.\n\nMust-Have Skills :\n\n- Snowflake architecture, schema design, and data warehouse experience.\n\n- DBT (Data Build Tool) for data transformation and pipeline development.\n\n- Strong expertise in SQL (query optimization, complex joins, window functions, etc.)\n\n- Hands-on experience with Erwin Data Modeler (logical and physical modeling).\n\n- Experience with GCP (BigQuery, Cloud Composer, Cloud Storage).\n\n- Experience in reverse engineering legacy systems like Sailfish or DDMS using DBeaver.\n\nGood To Have :\n\n- Experience with CI/CD tools and DevOps for data environments.\n\n- Familiarity with data governance, security, and privacy practices.\n\n- Exposure to Agile methodologies and working in distributed teams.\n\n- Knowledge of Python for data engineering tasks and orchestration scripts.\n\nSoft Skills :\n\n- Excellent problem-solving and analytical skills.\n\n- Strong communication and stakeholder management.\n\n- Self-driven with the ability to work independently in a remote setup.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SQL', 'Data Quality', 'Data Build Tool', 'Data Security', 'Snowflake DB', 'Data Modeling', 'Data Warehousing', 'Data Analytics', 'Data Governance', 'Erwin']",2025-06-13 06:04:26
Data Modeler - SQL/Erwin,Leading Client,7 - 10 years,Not Disclosed,[],"Employment Type : Contract (Remote).\n\nJob Summary :\n\nWe are looking for a highly skilled Data Engineer / Data Modeler with strong experience in Snowflake, DBT, and GCP to support our data infrastructure and modeling initiatives.\n\nThe ideal candidate should possess excellent SQL skills, hands-on experience with Erwin Data Modeler, and a strong background in modern data architectures and data modeling techniques.\n\nKey Responsibilities :\n\n- Design and implement scalable data models using Snowflake and Erwin Data Modeler.\n\n- Create, maintain, and enhance data pipelines using DBT and GCP (BigQuery, Cloud Storage, Dataflow).\n\n- Perform reverse engineering on existing systems (e.g., Sailfish/DDMS) using DBeaver or similar tools to understand and rebuild data models.\n\n- Develop efficient SQL queries and stored procedures for data transformation, quality, and validation.\n\n- Collaborate with business analysts and stakeholders to gather data requirements and convert them into physical and logical models.\n\n- Ensure performance tuning, security, and optimization of the Snowflake data warehouse.\n\n- Document metadata, data lineage, and business logic behind data structures and flows.\n\n- Participate in code reviews, enforce coding standards, and provide best practices for data modeling and governance.\n\nMust-Have Skills :\n\n- Snowflake architecture, schema design, and data warehouse experience.\n\n- DBT (Data Build Tool) for data transformation and pipeline development.\n\n- Strong expertise in SQL (query optimization, complex joins, window functions, etc.)\n\n- Hands-on experience with Erwin Data Modeler (logical and physical modeling).\n\n- Experience with GCP (BigQuery, Cloud Composer, Cloud Storage).\n\n- Experience in reverse engineering legacy systems like Sailfish or DDMS using DBeaver.\n\nGood To Have :\n\n- Experience with CI/CD tools and DevOps for data environments.\n\n- Familiarity with data governance, security, and privacy practices.\n\n- Exposure to Agile methodologies and working in distributed teams.\n\n- Knowledge of Python for data engineering tasks and orchestration scripts.\n\nSoft Skills :\n\n- Excellent problem-solving and analytical skills.\n\n- Strong communication and stakeholder management.\n\n- Self-driven with the ability to work independently in a remote setup.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SQL', 'Data Quality', 'Data Build Tool', 'Data Security', 'Snowflake DB', 'Data Modeling', 'Data Warehousing', 'Data Analytics', 'Data Governance', 'Erwin']",2025-06-13 06:04:27
Digital Marketing Data Science Manager,Abinbev Gcc Services,6 - 10 years,Not Disclosed,['Bengaluru'],"AB InBev GCC was incorporated in 2014 as a strategic partner for Anheuser-Busch InBev. The center leverages the power of data and analytics to drive growth for critical business functions such as operations, finance, people, and technology. The teams are transforming Operations through Tech and Analytics.\nDo You Dream Big?\nWe Need You.\n\nJob Description",,,,"['NLP', 'Python', 'SQL', 'R', 'Power Bi', 'MFDL', 'Azure Cloud']",2025-06-13 06:04:29
Big data Developer,Diverse Lynx,3 - 5 years,Not Disclosed,['Bengaluru'],"Total Yrs. of Experience 8+ Yrs Relevant Yrs. of experience 4+ Yrs Detailed JD (Roles and Responsibilities)\nGather operational Client on business processes and policies from multiple sources\nPrepare periodical and ad-hoc reports using operational data\nDevelop semantic core to align data with business processes\nSupport operations teams work streams for data processing, analysis and reporting\nAnalyse data and Create dashboards for the senior management\nDesign and implement optimal processes\nRegression testing of the releases\nMandatory skills\nBig Data : Spark, Hive, DataBricks\nLanguage : SQL, JAVA /Python\nBI Analytics: Power BI (DAX), Tableau , Dataiku\nOperating System : Unix\nExperience with Data Migration, Data Engineering, Data Analysis\nDesired/ Secondary skills\nBig Data : SCALA, HADOOP\nTools: DB Visualizer, JIRA, GIT, Bitbucket, Control-M\nStrong problem-solving skills and the ability to work independently and in a team environment.\nExcellent communication skills and the ability to work effectively with cross-functional teams.\nDomain eCommerce / Retail Max Vendor Rate in Per Day (Currency in relevance to work location) 11000 INR/Day Delivery Anchor for tracking the sourcing statistics, technical evaluation, interviews and feedback etc. Prem_dason@infosys.com Work Location given in ECMS ID Bangalore (Hyd, Pune, Trivandrum locations also ok)",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Unix', 'Data analysis', 'Data migration', 'Control-M', 'Data processing', 'Regression testing', 'Operations', 'Analytics', 'SQL', 'Python']",2025-06-13 06:04:30
Data Architect,Diverse Lynx,8 - 13 years,Not Disclosed,['Bengaluru'],"Data Architect-\nTotal Yrs. of Experience* 15+ Relevant Yrs. of experience* 8+ Detailed JD *(Roles and Responsibilities)\nLeadership qualities and ability to lead a team of 8 data engineers + PowerBI resources\nShould be able to engage with business users and IT to provide consultation on data and visualization needs\nExcellent communication, articulation, and presentation skills\nExposure to data architecture, ETL architecture\nDesign, develop, and maintain scalable data pipelines using Python, ADF, and Databricks\nImplement ETL process to extract, transform, and load data from various sources into Snowflake\nEnsure data is processed efficiently and is made available for analytics and reporting\n8+ years of experience in data engineering, with a focus on Python, ADF, Snowflake, Databricks, and ETL processes.\nProficiency in SQL and experience with cloud-based data storage and processing.\nStrong problem-solving skills and the ability to work in a fast-paced environment\nExperience with Agile methodologies and working in a collaborative team environment.\nCertification in Snowflake, Azure, or other relevant technologies is an added advantage\nBachelors degree in computer science engineering, Information Systems or equivalent field\nMandatory skills* Python, Snowflake, Azure Data Factory, Databricks, SQL Desired skills* 1. Strong Oral and written communication\n2. Proactive and accountable of the deliverables quality and timely submission Domain* Retail Work Location* India\nLocation- PAN India\nYrs of Exp-15+Yrs",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Retail', 'Focus', 'Data Architect', 'Cloud', 'Agile', 'Analytics', 'SQL', 'Python', 'Data architecture']",2025-06-13 06:04:32
Data Visualization Engineer,Maimsd Technology,4 - 7 years,Not Disclosed,['Pune'],"Experience : 4 - 7 Yrs\n\nEmployment Type : Full Time, Permanent\n\nWorking mode : Regular\n\nNotice Period : Immediate - 15 Day\n\nAbout the Role :\n\nWe are seeking a skilled Data Visualization Engineer to join our team and transform raw data into actionable insights. You will play a crucial role in designing, developing, and maintaining interactive dashboards and reports using Power BI, Power Apps, Power Query, and Power Automate.\n\nResponsibilities :\n\n- Data Visualization : Create compelling and informative dashboards and reports using Power BI, effectively communicating complex data to stakeholders.\n\n- Data Integration : Import data from various sources (e.g., databases, spreadsheets, APIs) using Power Query and ensure data quality and consistency.\n\n- Data Modeling : Develop robust data models in Power BI to support complex analysis and reporting requirements.\n\n- Power Apps Development : Create custom applications using Power Apps to enable data-driven decision-making and automate workflows.\n\n- Power Automate Integration : Automate repetitive tasks and workflows using Power Automate to improve efficiency and reduce errors.\n\n- Cloud Data Analytics : Leverage cloud-based data analytics platforms to process and analyze large datasets.\n\n- Collaboration : Work closely with data analysts, data scientists, and business users to understand their requirements and deliver effective visualizations.\n\nQualifications :\n\nExperience : 4-7 years of experience in data visualization and business intelligence.\n\nTechnical Skills :\n\n- Proficiency in Power BI, Power Apps, Power Query, and Power Automate.\n\n- Strong understanding of data modeling and ETL processes.\n\n- Experience working with cloud-based data analytics platforms.\n\n- Familiarity with SQL and other data query languages.\n\nSoft Skills :\n\n- Excellent communication and interpersonal skills.\n\n- Strong problem-solving and analytical abilities.\n\n- Attention to detail and ability to deliver high-quality work",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Visualization', 'Reporting Analytics', 'Power BI', 'Dashboard Design', 'Power Automate', 'Data Modeling', 'ETL', 'Power Query M', 'Data Integration', 'SQL']",2025-06-13 06:04:33
Golang-Lead Software Engineer/ Senior Software/Software Engineer,Tech Mahindra,4 - 9 years,Not Disclosed,"['Hyderabad', 'Pune', 'Bengaluru']","Demonstrable ability to write Python/Golang and SQL. You are happy to learn new programming languages and frameworks as necessary.\nYou are interested in, contemporary approaches to service design, including the use of containers and container orchestration technologies, streaming data platforms, APIs and in-memory/NoSQL stores.\nYou are familiar with working in a devops based software development workflow, including building, testing, and continuous integration/deployment. You are also happy to be evolve along with the development process and contribute to its success.\nYou have the ability to communicate with a range of stakeholders, including subject matter experts, data scientists, software engineers and enterprise devops and security professionals.\nYou are keen to engage with best practices for code review, version control, and change control, balancing the need for a quality codebase with the unique and particular demands of scale up stage software engineering.\nYou have experience or are keen to engage with productionising machine learning technologies.Role & responsibilities",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Golang', 'devops', 'API', 'SQL']",2025-06-13 06:04:35
Gcp Data Engineer,Estuate Software,6 - 11 years,Not Disclosed,['Hyderabad'],"Design, build,\nJob Title: Data Engineer / Integration Engineer\nJob Summary:\nWe are seeking a highly skilled Data Engineer / Integration Engineer to join our team. The ideal candidate will have expertise in Python, workflow orchestration, cloud platforms (GCP/Google BigQuery), big data frameworks (Apache Spark or similar), API integration, and Oracle EBS. The role involves designing, developing, and maintaining scalable data pipelines, integrating various systems, and ensuring data quality and consistency across platforms. Knowledge of Ascend.io is a plus.\nKey Responsibilities:\nDesign, build, and maintain scalable data pipelines and workflows.\nDevelop and optimize ETL/ELT processes using Python and workflow automation tools.\nImplement and manage data integration between various systems, including APIs and Oracle EBS.\nWork with Google Cloud Platform (GCP) or Google BigQuery (GBQ) for data storage, processing, and analytics.\nUtilize Apache Spark or similar big data frameworks for efficient data processing.\nDevelop robust API integrations for seamless data exchange between applications.\nEnsure data accuracy, consistency, and security across all systems.\nMonitor and troubleshoot data pipelines, identifying and resolving performance issues.\nCollaborate with data analysts, engineers, and business teams to align data solutions with business goals.\nDocument data workflows, processes, and best practices for future reference.\nRequired Skills & Qualifications:\nStrong proficiency in Python for data engineering and workflow automation.\nExperience with workflow orchestration tools (e.g., Apache Airflow, Prefect, or similar).\nHands-on experience with Google Cloud Platform (GCP) or Google BigQuery (GBQ).\nExpertise in big data processing frameworks, such as Apache Spark.\nExperience with API integrations (REST, SOAP, GraphQL) and handling structured/unstructured data.\nStrong problem-solving skills and ability to optimize data pipelines for performance.\nExperience working in an agile environment with CI/CD processes.\nStrong communication and collaboration skills.\nPreferred Skills & Nice-to-Have:\nExperience with Ascend.io platform for data pipeline automation.\nKnowledge of SQL and NoSQL databases.\nFamiliarity with Docker and Kubernetes for containerized workloads.\nExposure to machine learning workflows is a plus.\nWhy Join Us?\nOpportunity to work on cutting-edge data engineering projects.\nCollaborative and dynamic work environment.\nCompetitive compensation and benefits.\nProfessional growth opportunities with exposure to the latest technologies.\n\nHow to Apply:\nInterested candidates can apply by sending their resume to 8892751405 / deekshith.naidu@estuate.com or through Naukri",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Airflow', 'GCP', 'Bigquery', 'SQL']",2025-06-13 06:04:36
AWS Data Engineer,Exavalu,2 - 5 years,Not Disclosed,[],"Exavalu is looking for AWS Data Engineer to join our dynamic team and embark on a rewarding career journey\nBe responsible for the planning, implementation, and growth of the AWS cloud infrastructure\nBuild, release, and manage the configuration of all production systems\nManage a continuous integration and deployment methodology for server-based technologies\nWork alongside architecture and engineering teams to design and implement any scalable software services\nEnsure necessary system security by using best in class cloud security solutionsImplement continuous integration/continuous delivery (CI/CD) pipelines when necessary\nRecommend process and architecture improvements\nTroubleshoot the system and solve problems across all platform and application domains\nOversee pre-production acceptance testing to ensure the high quality of a companys services and products",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent",['AWS Data Engineer'],2025-06-13 06:04:37
Data Engineering Automation Tester,Swits Digital,3 - 6 years,Not Disclosed,['Gurugram'],"Job Title: Data Engineering Automation Tester\n\nExperience: 6+ Years\n\nLocation: Gurgaon, India\n\n\nMandatory Skills:\n\n\n\n\nStrong experience in distributed computing (Spark) and software development.\n\n\n\nProficiency in working with databases (preferably Postgres).\n\n\n\nSolid understanding of Object-Oriented Programming and development principles.\n\n\n\nExperience in Agile development methodologies (Scrum/Kanban).\n\n\n\nHands-on experience with version control tools (preferably Git).\n\n\n\nExposure to CI/CD pipelines.\n\n\n\nStrong background in automated testing including Integration/Delta, Load, and Performance testing.\n\n\n\nExtensive experience in database testing (preferably Postgres).\n\n\n\n\nGood to Have Skills:\n\n\n\n\nExposure to Docker and containerized environments.\n\n\n\nExperience with Spark-Scala.\n\n\n\nKnowledge of Data Engineering principles.\n\n\n\nExperience in Python and .NET Core.\n\n\n\nFamiliarity with Kubernetes.\n\n\n\nExperience with Airflow.\n\n\n\nWorking knowledge of cloud platforms (GCP and Azure).\n\n\n\nExperience with TeamCity CI and Octopus Deploy.",Industry Type: Recruitment / Staffing,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Database testing', 'Automation', 'Automation testing', 'GIT', 'Version control', 'spark', 'Agile development', 'Performance testing', 'Object oriented programming', 'Python']",2025-06-13 06:04:39
GCP Data Engineer,Swits Digital,2 - 7 years,Not Disclosed,['Chennai'],"Job Title: GCP Data Engineer\nLocation: Chennai, India\nJob type: FTE\nMandatory Skills: Google Cloud Platform - Biq Query, Data Flow, Dataproc, Data Fusion, TERRAFORM, Tekton,Cloud SQL, AIRFLOW, POSTGRES, Airflow PySpark, Python, API\nJob Description:\n2+Years in GCP Services, Biq Query, Data Flow, Dataproc, DataPlex,DataFusion, Terraform, Tekton, Cloud SQL, Redis Memory, Airflow, Cloud Storage\n2+ Years inData Transfer Utilities\n2+ Years in Git / any other version control tool\n2+ Years in Confluent Kafka\n1+ Years of Experience in API Development\n2+ Years in Agile Framework\n4+ years of strong experience in python, Pyspark development.\n4+ years of shell scripting to develop the adhoc jobsfor data importing/exporting",Industry Type: Recruitment / Staffing,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Version control', 'GIT', 'GCP', 'Shell scripting', 'query', 'Agile', 'cloud storage', 'SQL', 'Python']",2025-06-13 06:04:41
Azure Data Factory (ADF) Engineer,Arieotech Solutions,3 - 5 years,Not Disclosed,['Pune'],"We are seeking an experienced Azure Data Factory Engineer to design, develop, and manage data pipelines using Azure Data Factory. The ideal candidate will possess hands-on expertise in ADF components and activities, and have practical knowledge of incremental data loading, file management, API integration, and cloud storage solutions. This role involves automating data workflows, optimizing performance, and ensuring the seamless flow of data within our cloud environment.\nKey Responsibilities:\nDesign and Develop Data Pipelines: Build and maintain scalable data pipelines using Azure Data Factory, ensuring efficient and reliable data movement and transformation.\nIncremental Data Loads: Implement and manage incremental data loading processes to ensure that only updated or new data is processed, optimizing data pipeline performance and reducing resource consumption.\nFile Management: Handle data ingestion and management from various file sources, including CSV, JSON, and Parquet formats, ensuring data accuracy and consistency.\nAPI Integration: Develop and configure data pipelines to interact with RESTful APIs for data extraction and integration, handling authentication and data retrieval processes effectively.\nCloud Storage Management: Work with Azure Blob Storage and Azure Data Lake Storage to manage and utilize cloud storage solutions, ensuring data is securely stored and easily accessible.\nADF Automation: Leverage Azure Data Factory s automation capabilities to schedule and monitor data workflows, ensuring timely execution and error-free operations.\nPerformance Optimization: Continuously monitor and optimize data pipeline performance, troubleshoot issues, and implement best practices to enhance efficiency.\nCollaboration: Work closely with data engineers, analysts, and other stakeholders to gather requirements, provide technical guidance, and ensure successful data integration solutions.\nQualifications:\nEducational Background: Bachelor s degree in Computer Science, Information Technology, or a related field (B. E, B.Tech, MCA, MCS). Advanced degrees or certifications are a plus.\nExperience: Minimum 3-5 years of hands-on experience with Azure Data Factory, including designing and implementing complex data pipelines.\nTechnical Skills:\nStrong knowledge of ADF components and activities, including datasets, pipelines, data flows, and triggers.\nProficiency in incremental data loading techniques and optimization strategies.\nExperience working with various file formats and handling large-scale data files.\nProven ability to integrate and interact with APIs for data retrieval and processing.\nHands-on experience with Azure Blob Storage and Azure Data Lake Storage.\nFamiliarity with ADF automation features and scheduling.\nSoft Skills:\nStrong problem-solving and analytical skills.\nExcellent communication and collaboration abilities.\nAbility to work independently and manage multiple tasks effectively.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Analytical skills', 'Automation', 'Storage management', 'cloud storage', 'JSON', 'Scheduling', 'Management', 'Information technology', 'Data extraction']",2025-06-13 06:04:42
GCP Data Engineer,Product based Fortune Global 500 MNC in ...,4 - 8 years,6-16 Lacs P.A.,"['Hyderabad', 'Chennai']","Role & responsibilities\nBachelors degree or four or more years of work experience.\nFour or more years of work experience.\nExperience with Data Warehouse concepts and Data Management life cycle.\nExperience in any DBMS\nExperience in Shell scripting, Spark, Scala.\nExperience in GCP/Big Query, composer, Airflow.\nExperience in real time streaming\nExperience in DevOps",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['GCP', 'Bigquery', 'Cloud Storage', 'Pubsub', 'Data Flow', 'Dataproc', 'Spark', 'Composing', 'Dataprep', 'Python']",2025-06-13 06:04:44
Aws Data Engineer,Astrosoft Technologies,2 - 4 years,6.5-10 Lacs P.A.,['Hyderabad( Gachibowli )'],"Company: AstroSoft Technologies (https://www.astrosofttech.com/)\nAstrosoft is an award-winning company that specializes in the areas of Data, Analytics, Cloud, AI/ML, Innovation, Digital. We have a customer first mindset and take extreme ownership in delivering solutions and projects for our customers and have consistently been recognized by our clients as the premium partner to work with. We bring to bear top tier talent, a robust and structured project execution framework, our significant experience over the years and have an impeccable record in delivering solutions and projects for our clients.\nFounded in 2004, Headquarters in FL,USA, Corporate Office - India, Hyderabad\nBenefits from Astrosoft Technologies\nH1B Sponsorship (Depends on Project & Performance)\nLunch & Dinner (Every day)\nHealth Insurance Coverage- Group\nIndustry Standards Leave Policy\nSkill Enhancement Certification\nHybrid Mode\nRole & responsibilities\nJob Title: AWS Engineer\nRequired Skills:\nMinimum of 2+ years direct experience with AWS Data Engineer\nStrong Experience in AWS Services like Redshift & ETL Glue, Spark, Python, Lambda,Kafka,S3, EMR Etc experience is a must.\nMonitoring tools - Cloudwatch\nExperience in Development & Support Projects as well.\nStrong verbal and written communication skills\nStrong experience and understanding of streaming architecture and development practices using kafka, spark, flink etc,\nStrong AWS development experience using S3, SNS, SQS, MWAA (Airflow) Glue, DMS and EMR.\nStrong knowledge of one or more programing languages Python/Java/Scala (ideally Python)\nExperience using Terraform to build IAC components in AWS.\nStrong experience with ETL Tools in AWS; ODI experience is as plus.\nStrong experience with Database Platforms: Oracle, AWS Redshift\nStrong experience in SQL tuning, tuning ETL solutions, physical optimization of databases.\nVery familiar with SRE concepts which includes evaluating and implementing monitoring and observability tools like Splunk, Data Dog, CloudWatch and other job, log or dashboard concepts for customer support and application health checks.\nAbility to collaborate with our business partners to understand and implement their requirements.\nExcellent interpersonal skills and be able to build consensus across teams.\nStrong critical thinking and ability to think out-of-the box.\nSelf-motivated and able to perform under pressure.\nAWS certified (preferred)\nQualifications:\nEducation: Bachelor's degree in Computer Science, Information Technology, or a related field (or equivalent work experience).\nExperience:\nProven experience as an AWS Support Engineer or in a similar role.\nHands-on experience with a wide range of AWS services (e.g., EC2, S3, RDS, Lambda, CloudFormation, IAM).\nSoft Skills:\nExcellent communication and interpersonal skills.\nStrong problem-solving and analytical skills.\nAbility to work independently and as part of a team.\nCustomer-focused mindset with a commitment to delivering high-quality support.\nWhat We Offer:\nCompetitive salary and benefits package.\nOpportunities for professional growth and development.\nA collaborative and supportive work environment.\nAccess to the latest AWS technologies and training resources.\nIf you are passionate about cloud technology and enjoy helping customers solve complex technical challenges, we would love to hear from you!\nAcknowledge the mail with your updated cv,\nJulekha.Gousiya@astrosofttech.com\n\n\n\nDetails As we discussed, Please revert with your acknowledgment.\nTotal Experience-\nAws\nRedshift\nGlue-\nCurrent Location-\nCurrent Company-\nC-CTC-\nEx-CTC –\nOffer –\nNP –\nReady to Relocate Hyderabad (Y/N) – Yes\n(Hybrid) –(Y/N) - Yes",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['AWS', 'Glue', 'redshift', 'Spark']",2025-06-13 06:04:45
Azure Data Engineer,Arges Global,2 - 5 years,8-18 Lacs P.A.,['Pune( Baner )'],"Scope of Work:\nCollaborate with the lead Business / Data Analyst to gather and analyse business requirements for data processing and reporting solutions.\nMaintain and run existing Python code, ensuring smooth execution and troubleshooting any issues that arise.\nDevelop new features and enhancements for data processing, ingestion, transformation, and report building.\nImplement best coding practices to improve code quality, maintainability, and efficiency.\nWork within Microsoft Fabric to manage data integration, warehousing, and analytics, ensuring optimal performance and reliability.\nSupport and maintain CI/CD workflows using Git-based deployments or other automated deployment tools, preferably in Fabric.\nDevelop complex business rules and logic in Python to meet functional specifications and reporting needs.\nParticipate in an agile development environment, providing feedback, iterating on improvements, and supporting continuous integration and delivery processes.\nRequirements:\nThis person will be an individual contributor responsible for programming, maintenance support, and troubleshooting tasks related to data movement, processing, ingestion, transformation, and report building.\nAdvanced-level Python developer.\nModerate-level experience in working in Microsoft Fabric environment (at least one and preferably two or more client projects in Fabric).\nWell-versed with understanding of modelling, databases, data warehousing, data integration, and technical elements of business intelligence technologies.\nAbility to understand business requirements and translate them into functional specifications for reporting applications.\nExperience in GIT-based deployments or other CI/CD workflow options, preferably in Fabric.\nStrong verbal and written communication skills.\nAbility to perform in an agile environment where continual development is prioritized.\nWorking experience in the financial industry domain and familiarity with financial accounting terms and statements like general ledger, balance sheet, and profit & loss statements would be a plus.\nAbility to create Power BI dashboards, KPI scorecards, and visual reports would be a plus.\nDegree in Computer Science or Information Systems, along with a good understanding of financial terms or working experience in banking/financial institutions, is preferred.",Industry Type: Financial Services (Asset Management),Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Power Bi', 'Microsoft Azure', 'Python', 'Azure Data Factory', 'Microsoft Fabric', 'Azure Databricks', 'Azure Data Lake']",2025-06-13 06:04:47
Snowflake Data Engineer,Prudent Globaltech Solutions,7 - 12 years,16-27.5 Lacs P.A.,['Hyderabad( Madhapur )'],"Job Description Data Engineer\nWe are seeking a highly skilled Data Engineer with extensive experience in Snowflake, Data Build Tool (dbt), Snaplogic, SQL Server, PostgreSQL, Azure Data Factory, and other ETL tools. The ideal candidate will have a strong ability to optimize SQL queries and a good working knowledge of Python. A positive attitude and excellent teamwork skills are essential.\n\nRole & responsibilities\nData Pipeline Development: Design, develop, and maintain scalable data pipelines using Snowflake, DBT, Snaplogic, and ETL tools.",,,,"['Snowflake', 'Azure Data Factory', 'ADF', 'Data Engineer', 'ETL', 'SQL']",2025-06-13 06:04:49
Azure Data Engineer,CODERZON Technologies Pvt Ltd,3 - 8 years,6-18 Lacs P.A.,['Kochi'],"Looking for a Data Engineer with 3+ yrs exp in Azure Data Factory, Synapse, Data Lake, Databricks, SQL, Python, Spark, CI/CD. Preferred: DP-203 cert, real-time data tools (Kafka, Stream Analytics), data governance (Purview), Power BI.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Data Factory', 'Azure Synapse', 'Pyspark', 'Azure Databricks', 'Azure Data Lake', 'SQL Azure', 'Python']",2025-06-13 06:04:50
S&C Global Network - AI - Healthcare Analytics - Senior Analyst,Accenture,3 - 5 years,Not Disclosed,['Bengaluru'],"Job Title -\n\n\n\nS&C Global Network - AI - Healthcare Analytics - Senior Analyst\n\n\n\nManagement Level:\n\n\n\n10-Senior Analyst\n\n\n\nLocation:\n\n\n\nGurgaon/Bangalore/Mumbai\n\n\n\nMust-have skills:Phython, Spark,SQL, Tableau, Power BI\n\n\n\n\nGood to have skills:Ability to leverage design thinking, business process optimization, and stakeholder management skills.\n\n\n\nJob\n\n\nSummary:\n\nThis role involves driving strategic initiatives, managing business transformations, and leveraging industry expertise to create value-driven solutions.\n\n\n\n\nRoles & Responsibilities:\n\nProvide strategic advisory services, conduct market research, and develop data-driven recommendations to enhance business performance.\n\n\n\nWHATS IN IT FOR YOU\nConduct data wrangling and analysis on healthcare claims, provider datasets, and publicly available health data.\nDevelop predictive models using data science and AI techniques to address client needs.\nUtilize natural language processing (NLP) capabilities to extract insights from unstructured data sources.\nCollaborate with cross-functional teams to implement analytics solutions effectively.\nTranslate complex data findings into clear, concise, and actionable strategies.\n\n\n\nWhat you would do in this role\nWork with Managers to get Client's business requirements and deliver Analytics driven solution.\nDuties and Responsibilities\nSr. Data Scientist responsible for generating actionable recommendations well-supported by quantitative analysis to help our clients address their ongoing problems.\nPresent analytic findings & opportunities for improvement to senior management and summarize key findings, and aid in the dissemination of metrics throughout the organization.\nBuild knowledge base and disseminate information on applications of variety of analytical techniques.\nDevelop statistical models and delivery of analytic offerings and solutions in health domain areas.\n\n\n\n\n\nProfessional & Technical\n\n\n\n\nSkills:\n\n\n- Relevant experience in the required domain.\n\n- Strong analytical, problem-solving, and communication skills.\n\n- Ability to work in a fast-paced, dynamic environment.\nExperience in resolving complex data issues in creative and effective ways.\nStrong people management skills both at client site and an offshore environment\nExcellent communication skills and ability to interact with all levels of end users, stakeholders, and technical resources.\nAdept with using Statistical (like forecasting/modeling, optimization models), Machine Learning (GBM, Decision Trees etc.), AI techniques (Deep Learning)\nGood exposure to consulting experience/basic understanding of business problems and suggesting solutions.\n\n\nTechnical\n\n\n\n\nSkills:\nProficient in data handling suites PYTHON, Spark, SQL, or similar packages\nExcellent written and oral communication skills with ability to clearly communicate ideas and results to non-technical businesspeople.\nStrong aptitude, ability, motivation, and interest in placing quantitative analysis in the context of health care business for providers / payers/Public health systems\nExperience working with cloud providers (e.g. AWS, Azure, GCP)\n\n\n\n\n\nAdditional Information:\n\n- Opportunity to work on innovative projects.\n\n- Career growth and leadership exposure.\n\n\n\n\n\nAbout Our Company | Accenture\n\nQualification\n\n\n\nExperience:\n\n\n\n3-5 Years in Healthcare Analytics\n\n\n\n\nEducational Qualification:\n\n\n\nBachelor's / masters degree in computer science, statistics, applied mathematics, or a related field.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'power bi', 'sql', 'tableau', 'spark', 'switching', 'advanced analytics', 'network engineering', 'data analytics', 'data analysis', 'microsoft azure', 'networking', 'machine learning', 'data science', 'gcp', 'healthcare analytics', 'network analysis', 'data handling', 'aws', 'ccna']",2025-06-13 06:04:52
Big Data Developer,Techstar Group,7 - 10 years,Not Disclosed,['Hyderabad'],"Responsibilities of the Candidate :\n\n- Be responsible for the design and development of big data solutions. Partner with domain experts, product managers, analysts, and data scientists to develop Big Data pipelines in Hadoop\n\n- Be responsible for moving all legacy workloads to a cloud platform\n\n- Work with data scientists to build Client pipelines using heterogeneous sources and provide engineering services for data PySpark science applications\n\n- Ensure automation through CI/CD across platforms both in cloud and on-premises\n\n- Define needs around maintainability, testability, performance, security, quality, and usability for the data platform\n\n- Drive implementation, consistent patterns, reusable components, and coding standards for data engineering processes\n\n- Convert SAS-based pipelines into languages like PySpark, and Scala to execute on Hadoop and non-Hadoop ecosystems\n\n- Tune Big data applications on Hadoop and non-Hadoop platforms for optimal performance\n\n- Apply an in-depth understanding of how data analytics collectively integrate within the sub-function as well as coordinate and contribute to the objectives of the entire function.\n\n- Produce a detailed analysis of issues where the best course of action is not evident from the information available, but actions must be recommended/taken.\n\n- Assess risk when business decisions are made, demonstrating particular consideration for the firm's reputation and safeguarding Citigroup, its clients, and assets, by driving compliance with applicable laws, rules, and regulations, adhering to Policy, applying sound ethical judgment regarding personal behavior, conduct, and business practices, and escalating, managing and reporting control issues with transparency\n\nRequirements :\n\n- 6+ years of total IT experience\n\n- 3+ years of experience with Hadoop (Cloudera)/big data technologies\n\n- Knowledge of the Hadoop ecosystem and Big Data technologies Hands-on experience with the Hadoop eco-system (HDFS, MapReduce, Hive, Pig, Impala, Spark, Kafka, Kudu, Solr)\n\n- Experience in designing and developing Data Pipelines for Data Ingestion or Transformation using Java Scala or Python.\n\n- Experience with Spark programming (Pyspark, Scala, or Java)\n\n- Hands-on experience with Python/Pyspark/Scala and basic libraries for machine learning is required.\n\n- Proficient in programming in Java or Python with prior Apache Beam/Spark experience a plus.\n\n- Hand on experience in CI/CD, Scheduling and Scripting\n\n- Ensure automation through CI/CD across platforms both in cloud and on-premises\n\n- System level understanding - Data structures, algorithms, distributed storage & compute\n\n- Can-do attitude on solving complex business problems, good interpersonal and teamwork skills",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Big Data', 'Hive', 'Data Engineering', 'Data Pipeline', 'PySpark', 'Hadoop', 'Kafka', 'HDFS', 'Spark', 'Python']",2025-06-13 06:04:54
Data Architect,Exavalu,7 - 12 years,Not Disclosed,[],"We are seeking an experienced Azure Data Architect to lead the design, implementation, and optimization of scalable, secure, and cost-effective cloud data solutions on Microsoft Azure. The ideal candidate will have deep expertise in data architecture, cloud computing, and modern data platforms, with a proven ability to align technology strategies with business goals.\nKey Responsibilities:\nDesign and implement modern data platform solutions using Azure services including Azure Data Lake, Azure Synapse Analytics, Azure Data Factory, and Azure Databricks.\nDefine data architecture frameworks, standards, and principles.\nLead data modernization and cloud migration initiatives.\nBuild secure and scalable data pipelines to ingest, transform, and store large datasets.\nCollaborate with data engineers, data scientists, and business stakeholders to understand data requirements and provide architectural guidance.\nOptimize performance, security, and cost of data platforms.\nEvaluate and recommend new Azure services and tools based on evolving project needs.\nEnsure data governance, data quality, and compliance standards are met.\nRequirements Required Qualifications:\nBachelors or Masters degree in Computer Science, Information Systems, or a related field.\n7+ years of experience in data architecture or data engineering roles.\n3+ years of hands-on experience with Azure data services.\nStrong knowledge of relational and non-relational databases (SQL Server, Cosmos DB, etc).\nProficiency with data modeling, data warehousing, and ETL processes.\nFamiliarity with big data and analytics tools like Databricks, Spark, and Synapse.\nExperience with scripting and programming languages (Python, SQL, PowerShell).\nExcellent understanding of security, identity, and governance in Azure.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Cloud computing', 'Architecture', 'Data modeling', 'Data quality', 'Cosmos', 'Analytics', 'SQL', 'Python', 'Data architecture']",2025-06-13 06:04:55
Data Architect,Armakuni,8 - 12 years,Not Disclosed,['Ahmedabad'],"DataArchitecture Design: Develop and maintain a comprehensive data architecture strategy that aligns with the business objectives and technology landscape.\nDataModeling:Createand managelogical, physical, and conceptual data models to support various business applications and analytics. DatabaseDesign: Design and implement database solutions, including data warehouses, data lakes, and operational databases.\nDataIntegration: Oversee the integration of data from disparate sources into unified, accessible systems using ETL/ELT processes. DataGovernance:Implementand enforce data governance policies and procedures to ensure data quality, consistency, and security.\nTechnologyEvaluation: Evaluate and recommend data management tools, technologies, and best practices to improve data infrastructure and processes.\nCollaboration: Work closely with data engineers, data scientists, business analysts, and other stakeholders to understand data requirements and deliver effective solutions. Trusted by the world s leading brands\nDocumentation:Createand maintain documentation related to data architecture, data flows, data dictionaries, and system interfaces. PerformanceTuning: Optimize database performance through tuning, indexing, and query optimization.\nSecurity: Ensure data security and privacy by implementing best practices for data encryption, access controls, and compliance with relevant regulations (e.g., GDPR, CCPA)\n\nRequirements:\nHelpingproject teams withsolutions architecture,troubleshooting, and technical implementation assistance.\nExperiencewithbig data technologies (e.g., Hadoop, Spark, Kafka, Airflow).\nExpertisewithcloud platforms (e.g., AWS, Azure, Google Cloud) and their data services.\nKnowledgeofdataintegration tools (e.g., Informatica, Talend, FiveTran, Meltano).\nUnderstandingofdatawarehousing concepts and tools (e.g., Snowflake, Redshift, Synapse, BigQuery). Experiencewithdata governanceframeworks and tools.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['query optimization', 'Data management', 'data security', 'Postgresql', 'MySQL', 'Data quality', 'Informatica', 'Troubleshooting', 'Analytics', 'Data architecture']",2025-06-13 06:04:57
Data Science Trainee,Lanxess,1 - 3 years,Not Disclosed,['Thane'],"Contract Type: Regular 12 months\n\n\n\n\nIf the chemistry is right, we can make a difference at LANXESS: speed up sports, make beverages last longer, add more color to leisure time and much more.\n\nAs a leading specialty chemicals group, we develop and produce chemical intermediates, additives, specialty chemicals and high-tech plastics. With more than 13,000 employees. Be part of it!",,,,"['Training', 'deep learning', 'C', 'data science', 'Finance', 'Machine learning', 'Javascript', 'Packaging', 'SQL', 'Python']",2025-06-13 06:04:59
Data Science (SSE) | FINJO I766,Omni Recruit,3 - 8 years,Not Disclosed,['Mumbai (All Areas)'],"Python Developer\nWork from Office\nLocation : Airoli , Navi Mumbai\n& :\n3.55 years of relevant experience in Python\nMinimum 3.5 years in Python programming\nAt least 1+ year in machine learning and natural language processing (NLP)\nMinimum 1.5 years with LLMs and GenAI\nAt least 2 years of experience with any database\n1+ year of experience deploying ML models/Python applications on Azure or AWS",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Machine Learning', 'Python']",2025-06-13 06:05:01
Data Solutions Architect (Immediate Joiner),Expro,7 - 12 years,Not Disclosed,['Hyderabad( HITEC City )'],"Overall Purpose of the Job\nWe are seeking an experienced and innovative Data Solutions Architect to join our team. The ideal candidate will have a strong background in designing and implementing data systems, ensuring seamless integration and scalability of data solutions, and leading the design of data architectures that meet business needs. This role requires proficiency in leveraging the Azure IoT Framework to enable IoT-driven data solutions. As a Data Solutions Architect, you will work closely with cross-functional teams to create robust, secure, and scalable data solutions that empower the business to leverage data for strategic decision-making.\n\nRole & responsibilities\nDesign and implement scalable, secure, and high-performance data architectures that meet business requirements.\nLead the integration of Azure IoT Framework into the architecture, enabling real-time data ingestion, processing, and analysis from IoT devices.\nCollaborate with stakeholders (business, IT, and data science teams) to understand data needs and translate them into effective solutions.\nOversee the full data lifecycle, including data collection, transformation, storage, and consumption.\nEvaluate and recommend tools, platforms, and technologies for data storage, processing, and analytics, ensuring that the solutions are aligned with business goals.\nDefine data integration strategies, ensuring that data from disparate sources, including IoT devices and sensors, can be ingested, processed, and unified effectively.\nCreate and enforce data governance practices, ensuring compliance with data privacy, security, and regulatory requirements.\nLead and mentor teams in the development and implementation of data solutions, ensuring adherence to architectural best practices and design patterns.\nEnsure the scalability, reliability, and performance of data systems to handle increasing volumes of data, including large datasets from IoT sources.\nStay updated on emerging trends and technologies in data management, cloud platforms, Azure IoT, and data engineering.\n\nRequired Skills & Qualifications:\n\nBachelors or masters degree in computer science, Data Engineering, Information Technology, or a related field.\nProven experience (typically 5+ years) in designing, implementing, and managing large-scale data architectures.\nExpertise in Azure IoT Framework, including services such as Azure IoT Hub, Azure Stream Analytics, and Azure Digital Twins.\nStrong understanding of data management, ETL processes, and database design (e.g., SQL, NoSQL, data lakes, and data warehouses).\nHands-on experience with modern data technologies and tools such as Hadoop, Spark, Kafka, and ETL frameworks.\nProficiency with data governance, security, and privacy standards.\nFamiliarity with machine learning, AI integration, and analytics platforms is a plus.\nStrong leadership skills with the ability to mentor and guide technical teams.\nExcellent problem-solving abilities and a deep understanding of system architecture and distributed systems.\nExcellent communication skills, both written and verbal, with the ability to translate complex technical concepts into business-friendly terms.\n\nPreferred Skills:\nExperience with cloud-native data platforms and services (e.g., Azure Synapse Analytics, Azure Databricks).\nKnowledge of data visualization and business intelligence tools (e.g., Tableau, Power BI, Looker).\nExperience with DevOps and CI/CD practices for data pipelines and data-driven applications.\nFamiliarity with microservices architectures and APIs.\nCertifications in Azure technologies (e.g., Microsoft Certified: Azure Solutions Architect Expert, Microsoft Certified: Azure IoT Developer).\nExperience with edge computing and real-time data processing for IoT solutions.\nRequired Immediate joiner.",Industry Type: Oil & Gas,Department: Other,"Employment Type: Full Time, Permanent","['Datafactory', 'Azure Event Hub', 'SQL Server', 'Data Bricks', 'Azure IoT Hub', 'Sap Data Services']",2025-06-13 06:05:02
Data Architect,Calibo,12 - 16 years,Not Disclosed,[],"About the Role:\n\nWe are looking for a highly skilled Data Engineering Architect with strong Data Engineering pipeline implementation experience to serve as the lead Solution/Technical Architect and Subject Matter Expert for customer experience data solutions across multiple data sources. The ideal candidate will collaborate with the Enterprise Architect and the client IT team to establish and implement strategic initiatives.\n\nResponsibilities and Technical Skills:\n12+ years of relevant experience in designing and Architecting ETL, ELT, Reverse ETL, Data Management or Data Integration, Data Warehouse, Data Lake, and Data Migration.\nMust have expertise in building complex ETL pipelines and large Data Processing, Data Quality and Data security\nExperience in delivering quality work on time with multiple, competing priorities.\nExcellent troubleshooting and problem-solving skills must be able to consistently identify critical elements, variables and alternatives to develop solutions.\nExperience in identifying, analyzing and translating business requirements into conceptual, logical and physical data models in complex, multi-application environments.\nExperience with Agile and Scaled Agile Frameworks.\nExperience in identifying and documenting data integration issues, and challenges such as duplicate data, non-conformed data, and unclean data. Multiple platform development experience.\nStrong experience in performance tuning of ETL processes using Data Platforms\nMust have experience in handling Data formats like Delta Tables, Parquet files, Iceberg etc.\nExperience in Cloud technologies such as AWS/Azure or Google Cloud.\nApache Spark design and development experience using Scala, Java, Python or Data Frames with Resilient Distributed Datasets (RDDs).\nDevelopment experience in databases like Oracle, AWS Redshift, AWS RDS, Postgres Databricks and/or Snowflake.\nHands-on professional work experience with Python is highly desired.\nExperience in Hadoop ecosystem tools for real-time or batch data ingestion.\nStrong communication and teamwork skills to interface with development team members, business analysts, and project management. Excellent analytical skills.\nIdentification of data sources, internal and external, and defining a plan for data management as per business data strategy.\nCollaborating with cross-functional teams for the smooth functioning of the enterprise data system.\nManaging end-to-end data architecture, from selecting the platform, designing the technical architecture, and developing the application to finally testing and implementing the proposed solution.\nPlanning and execution of big data solutions using Databricks, Big Data, Hadoop, Big Query, Snowflake, MongoDB, DynamoDB, PostgreSQL and SQL Server\nHands-on experience in defining and implementing various Machine Learning models for different business needs.\nIntegrating technical functionality, ensuring data accessibility, accuracy, and security.\nProgramming / Scripting Languages like Python / Java / Go, Microservices\nMachine Learning / AI tools like Scikit-learn / TensorFlow / PyTorch",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['cloud', 'ETL', 'AWS', 'Data Handling', 'Spark']",2025-06-13 06:05:04
Director of Data Governance and Operations,Sailpoint Technologies,10 - 12 years,Not Disclosed,['Pune'],"SailPoint is the leader in identity security for the cloud enterprise. Our identity security solutions secure and enable thousands of companies worldwide, giving our customers unmatched visibility into the entirety of their digital workforce, ensuring workers have the right access to do their job - no more, no less.\nBuilt on a foundation of AI and ML, our Identity Security Cloud Platform delivers the right level of access to the right identities and resources at the right time matching the scale, velocity, and changing needs of today s cloud-oriented, modern enterprise.\nAbout the role:\nWe are seeking a dynamic and experienced Director of Data Governance and Operations to lead our data governance and data quality initiatives . Reporting to the VP of IT, this leadership role will be pivotal in driving data-driven decision-making across our Go-To-Market (GTM) operations and Finance organizations. The ideal candidate will possess a deep passion for data governance, a proven track record of developing and executing successful data quality strategies, and a strong understanding of how data fuels AI innovation.\nEssential Duties and Responsibilities\nDevelop and champion a comprehensive data strategy aligned with SailPoint s overall business objectives , with a particular focus on GTM and Finance.\nEstablish and maintain a robust data governance framework, including policies, standards, and procedures, to ensure data quality, accuracy, and compliance.\nPartner with GTM and Finance leadership to identify data needs, develop data solutions, and drive the adoption of data-driven insights to improve performance.\nLead efforts to monitor and improve data quality across key systems and data sources, implementing data cleansing and validation processes.\nEnsure data readiness for AI initiatives, collaborating with Enterprise Applications and D ata E ngineering teams to prepare and structure data for AI model development and deployment.\nSalesforce Expertise: Leverage in-depth knowledge of Salesforce to optimize data management, reporting, and analytics within the platform.\nBuild, mentor, and manage a high-performing team of data professionals, fostering a culture of collaboration, innovation, and continuous improvement.\nCollaborate effectively with stakeholders across IT, GTM, Finance, and other departments to ensure alignment on data strategy and priorities.\nStay abreast of emerging trends in data management, AI, and analytics, and identify opportunities to leverage new technologies to enhance SailPoint s data capabilities.\nRequired Qualifications\n15 + years of experience in data management, data governance, or data strategy roles, with increasing levels of responsibility.\nProven track record of developing and executing successful data strategies for GTM operations and Finance.\nStrong experience with Master Data Management (MDM)\nDeep understanding of data governance principles , practices , tools, and technologies.\nIn-depth knowledge of Salesforce data model, reporting, and analytics capabilities.\nExperience with Salesforce administration.\nStrong understanding of AI and machine learning concepts, and the importance of data readiness for AI.\nExcellent leadership, communication, and interpersonal skills.\nAbility to influence and collaborate effectively with stakeholders at all levels of the organization.\nExperience building and managing high-performing teams.\nStrong analytical and problem-solving skills.\nSalesforce certifications are highly desirable.\nBachelor s degree in a relevant field (e.g., Computer Science, Data Science, Business Analytics).\nWhat success looks like\n30-Day Plan (Orientation and Integration):\nObjective: Establish foundational knowledge and integrate into the team.\nTraining Learning:\nIntr oduction to SailPoint Enterprise Systems and Data Platform\nReview company policies, procedures, and compliance requirements\nRoles Responsibilities:\nUnderstand job expectations, key performance indicators (KPIs), and deliverables.\nReview IT team structure and individual roles\nReview GTM and Finance team structure and roles and responsibilities of key stakeholders\nRelationship Building:\nMeet team members, key stakeholders, and cross-department colleagues.\nParticipate in onboarding activities and one-on-one meetings with IT leaders .\nImmediate Contributions:\nAssess and compile problem statement on data governance and data quality\nAssess and compile data flow diagrams for the key GTM and Finance metrics\nAssist with data requests to get familiar with SailPoint internal data needs , e.g. ACV/ARR\nInitiate recruiting of data analyst\n60-Day Plan ( Data Leadership and Strategic Thinking )\nObjective: Expertise on SailPoint business data and define strategy for data governance and data quality\nEstablish structure and recurring cadence with key business stakeholders and IT system owners on data governance\nDefine SailPoint Master Data and process to ensure data integrity\nEstablish metrics to measure data quality\nRecommend industry best practice for data governance and data quality\nDe fine roadmap\nData Governance and MDM initiatives\nData repository and metadata for AI\nProject Involvement:\nSailPoint ARR/ACV Reporting\nData found ation for AI\nTeam Building and Relationship Building :\nComplete recruiting of new data analyst\nDemonstrate ability to engage business and IT stakeholders and partnership on key data initiatives\n90 -180 Day Plan ( Execution on Roadmap )\nObjective: Establish leadership and make impact to key data initiatives\nData Quality\nComplete the top 2 initiatives for data quality for Salesforce\nData Governance and MDM\nPublish the master data definition and framework\nCollaborate with business stakeholders and IT system owners to implement system validation and business operations to ensure data integrity\nAI\nCollaborate with IT system owners and Data Engineering team to build metadata for AI consumption\nDeliver data for AI model training and AI search\nValidate the AI output to ensure accuracy",Industry Type: IT Services & Consulting,Department: Strategic & Top Management,"Employment Type: Full Time, Permanent","['data cleansing', 'Enterprise applications', 'Business analytics', 'Analytical', 'data governance', 'Data Analyst', 'Data quality', 'Continuous improvement', 'Business operations', 'Salesforce']",2025-06-13 06:05:05
Data Architect - AWS,Qentelli,4 - 8 years,Not Disclosed,['Hyderabad'],"Job Summary:\nWe are looking for a highly skilled AWS Data Architect to design and implement scalable, secure, and high-performing data architecture solutions on AWS. The ideal candidate will have hands-on experience in building data lakes, data warehouses, and data pipelines, along with a solid understanding of data governance and cloud security best practices.\n  Roles and Responsibilities:\nDesign and implement data architecture solutions on AWS using services such as S3, Redshift, Glue, Lake Formation, Athena, and Lambda.\nDevelop scalable ETL/ELT workflows and data pipelines using AWS Glue, Apache Spark, or AWS Data Pipeline.\nDefine and implement data governance, security, and compliance strategies, including IAM policies, encryption, and data cataloging.\nCreate and manage data lakes and data warehouses that are scalable, cost-effective, and secure.\nCollaborate with data engineers, analysts, and business stakeholders to develop robust data models and reporting solutions.\nEvaluate and recommend tools, technologies, and best practices to optimize data architecture and ensure high-quality solutions.\nEnsure data quality, performance tuning, and optimization for large-scale data storage and processing\n  Required Skills and Qualifications:\nProven experience in AWS data services such as S3, Redshift, Glue, etc.\nStrong knowledge of data modeling, data warehousing, and big data architecture.\nHands-on experience with ETL/ELT tools and data pipeline frameworks.\nGood understanding of data security and compliance in cloud environments.\nExcellent problem-solving skills and ability to work collaboratively with cross-functional teams.\nStrong verbal and written communication skills.\n  Preferred Skills:\nAWS Certified Data Analytics – Specialty or AWS Solutions Architect Certification.\nExperience in performance tuning and optimizing large datasets.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['cloud security', 'performance tuning', 'glue', 'amazon redshift', 'big data technologies', 'verbal communication', 'data warehousing', 'data architecture', 'aws glue', 's', 'data modeling', 'lambda expressions', 'spark', 'written communication', 'data governance', 'athena', 'aws', 'etl', 'communication skills']",2025-06-13 06:05:07
Azure Data Factory,Swift Staffing,8 - 12 years,6.5-14 Lacs P.A.,"['Mumbai', 'Hyderabad', 'Pune']","Job Description:\n5+ years in data engineering with at least 2 years on Azure Synapse.\nStrong SQL, Spark, and Data Lake integration experience.\nFamiliarity with Azure Data Factory, Power BI, and DevOps pipelines.\nExperience in AMS or managed services environments is a plus.\nDetailed JD\nDesign, develop, and maintain data pipelines using Azure Synapse Analytics.\nCollaborate with customer to ensure SLA adherence and incident resolution.\nOptimize Synapse SQL pools for performance and cost.\nImplement data security, access control, and compliance measures.\nParticipate in calibration and transition phases with client stakeholders",Industry Type: Recruitment / Staffing,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Azure Data Factory', 'Azure Synapse', 'Data Engineering', 'Azure Databricks', 'SQL Azure', 'Power Bi', 'Devops']",2025-06-13 06:05:09
Data Architect Telecom Domain databrick BSS OSS,fast growing Data Driven IT solutions an...,10 - 20 years,45-55 Lacs P.A.,"['Noida', 'Hyderabad', 'Gurugram']","Data Architect Telecom Domain\nTo design comprehensive data architecture and technical solutions specifically for telecommunications industry challenges, leveraging TMforum frameworks and modern data platforms. To work closely with customers, and technology partners to deliver data solutions that address complex telecommunications business requirements including customer experience management, network optimization, revenue assurance, and digital transformation initiatives.\nResponsibilities:\nDesign and articulate enterprise-scale telecom data architectures incorporating TMforum standards and frameworks, including SID (Shared Information/Data Model), TAM (Telecom Application Map), and eTOM (enhanced Telecom Operations Map)\nDevelop comprehensive data models aligned with TMforum guidelines for telecommunications domains such as Customer, Product, Service, Resource, and Partner management\nCreate data architectures that support telecom-specific use cases including customer journey analytics, network performance optimization, fraud detection, and revenue assurance\nDesign solutions leveraging Microsoft Azure and Databricks for telecom data processing and analytics\nConduct technical discovery sessions with telecom clients to understand their OSS/BSS architecture, network analytics needs, customer experience requirements, and digital transformation objectives\nDesign and deliver proof of concepts (POCs) and technical demonstrations showcasing modern data platforms solving real-world telecommunications challenges\nCreate comprehensive architectural diagrams and implementation roadmaps for telecom data ecosystems spanning cloud, on-premises, and hybrid environments\nEvaluate and recommend appropriate big data technologies, cloud platforms, and processing frameworks based on telecom-specific requirements and regulatory compliance needs.\nDesign data governance frameworks compliant with telecom industry standards and regulatory requirements (GDPR, data localization, etc.)\nStay current with the latest advancements in data technologies including cloud services, data processing frameworks, and AI/ML capabilities\nContribute to the development of best practices, reference architectures, and reusable solution components for accelerating proposal development\nQualifications:\nBachelor's or Master's degree in Computer Science, Telecommunications Engineering, Data Science, or a related technical field\n10+ years of experience in data architecture, data engineering, or solution architecture roles with at least 5 years in telecommunications industry\nDeep knowledge of TMforum frameworks including SID (Shared Information/Data Model), eTOM, TAM, and their practical implementation in telecom data architectures\nDemonstrated ability to estimate project efforts, resource requirements, and implementation timelines for complex telecom data initiatives\nHands-on experience building data models and platforms aligned with TMforum standards and telecommunications business processes\nStrong understanding of telecom OSS/BSS systems, network management, customer experience management, and revenue management domains\nHands-on experience with data platforms including Databricks, and Microsoft Azure in telecommunications contexts\nExperience with modern data processing frameworks such as Apache Kafka, Spark and Airflow for real-time telecom data streaming\nProficiency in Azure cloud platform and its respective data services with an understanding of telecom-specific deployment requirements\nKnowledge of system monitoring and observability tools for telecommunications data infrastructure\nExperience implementing automated testing frameworks for telecom data platforms and pipelines\nFamiliarity with telecom data integration patterns, ETL/ELT processes, and data governance practices specific to telecommunications\nExperience designing and implementing data lakes, data warehouses, and machine learning pipelines for telecom use cases\nProficiency in programming languages commonly used in data processing (Python, Scala, SQL) with telecom domain applications\nUnderstanding of telecommunications regulatory requirements and data privacy compliance (GDPR, local data protection laws)\nExcellent communication and presentation skills with ability to explain complex technical concepts to telecom stakeholders\nStrong problem-solving skills and ability to think creatively to address telecommunications industry challenges\nGood to have TMforum certifications or telecommunications industry certifications\nRelevant data platform certifications such as Databricks, Azure Data Engineer are a plus\nWillingness to travel as required\nif you will all or most of the criteria contact bdm@intellisearchonline.net M 9341626895",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Telecom Bss', 'Data Architect', 'Telecom OSS', 'ETOM', 'Data Bricks']",2025-06-13 06:05:10
Senior Lead business execution consultant,Wells Fargo,7 - 12 years,Not Disclosed,['Bengaluru'],"About this role:\nWells Fargo is seeking a Senior Lead business execution consultant\n\nIn this role, you will:\nAct as a Business Execution advisor to leadership to drive performance and initiatives, and develop and implement information delivery or presentations to key stakeholders and senior management",,,,"['Business execution', 'Business Implementation', 'Data Engineering', 'NLP', 'generative AI', 'Data Mining', 'machine learning', 'Strategic Planning', 'agentic AI']",2025-06-13 06:05:12
FP&A Analyst / Sr. Analyst / Manager - Chennai,2coms,3 - 8 years,Not Disclosed,['Chennai'],"SUMMARY\nFP&A Analyst / Sr. Analyst / Manager\n\nExperience: 6+ Years\n\nLocation: Chennai\n\nWork Arrangement: On-site\n\nRESPONSIBILITIES:\n\nThis position offers a unique opportunity to support our Corporate FP&A team. You will be instrumental in maintaining data integrity, conducting financial reporting and analysis, and ensuring efficient process execution. Your role will be pivotal in upholding the accuracy of essential data resources that contribute to all FP&A reports and processes.\n\nThe ideal candidate for this role will:\n\nTake charge of administrative operational cadences and have the potential to enhance existing processes, which includes organizing static meetings, managing communications related to scenario updates, and conducting daily validation checks of data.\nCreate standard template views utilized by the Global FP&A teams to deliver key results and performance insights, such as ensuring all Anaplan GSheet saved views are up to date and refreshing, and sending the monthly Close review excel template to Int’l at the beginning of each month.\nPreserve and generate source of truth materials to maintain data integrity and alignment of results across the broader FP&A team, achieved by maintaining monthly validation files and collaborating with the US based Corporate FP&A analyst to create the quarterly E-Binder TOC for the earnings team to reference in SmartSheets.\nConduct preliminary forecast/trend analysis and schedule creation for processes driven by the Corporate FP&A team, including rolling forward and refreshing all Earnings P0/P1 schedules and driving the quarterly EBITDA, SBC, Gains/Losses forecasts through partnership with Accounting.\nServe as the initial point of contact for all Earnings, BOD, and annual Operating Plan deck summaries, including staging the first pass proposals for all decks and managing all processes associated with final touches.\n\nRequirements\nRequirements:\n\nBachelor’s degree in Finance, Accounting, Business, or related field\nDemonstrated experience in financial planning and analysis\nProficiency in data analysis and financial modeling\nStrong communication and presentation skills\nAdvanced proficiency in Microsoft Excel and other financial software\nAbility to thrive in a fast-paced, dynamic environment\nStrong attention to detail and accuracy\nRelevant certifications (e.g., CFA, CPA) preferred",Industry Type: Banking,Department: Finance & Accounting,"Employment Type: Full Time, Permanent","['data analysis', 'software', 'forecasting', 'presentation skills', 'accounting', 'analysis', 'budgeting', 'excel', 'planning', 'annual operating plan', 'fpa', 'financial reporting', 'financial modelling', 'financial planning', 'financial planning and analysis', 'reporting', 'finance', 'communication skills']",2025-06-13 06:05:13
VKYC Analyst,Wikilabs India,0 - 5 years,Not Disclosed,['Mumbai (All Areas)( Kalina )'],"VKYC Analyst - Operations and Support\n\nKey Responsibilities:\nKYC applications timely review, with a feedback in line with laid down Bank Policy and Procedures\nEnsure that onboardings are originated in compliance with established policies and procedures\nAssist to enhance the quality of vKYC applications\nEnsure regulatory compliance\nMaintain working knowledge of relevant legislation, statutory instruments, codes of practice, and organization policies, and ensure adherence\n\nQualification Requirements:\n1+ year relevant experience preferably at a high-growth tech startup in the financial services space\nAbility to analyze documentation, as long as outstanding attention to details\nAbility to read large volumes of documents effectively and extract necessary information\nIn-depth understanding of the risk mitigation strategies (KYC)\nStrong in oral communication, analytical and report writing skills\nSense of teamwork, cooperative, and adaptable to change.\nAbility to work under pressure and problem-solving abilities\nBachelors Degree\nWilling to work Nights and Weekends on a rotating shift basis\n\nWe consider as a plus:\nprevious startup experience\nknowledge of the credit card or consumer finance industry in fintech",Industry Type: FinTech / Payments,"Department: Customer Success, Service & Operations","Employment Type: Full Time, Permanent","['Good Communication In English', 'Video Kyc', 'Document Verification', 'VKYC', 'Video Know Your Customer', 'Credit Cards']",2025-06-13 06:05:15
Fresher - Analyst,NLB Services,0 - 1 years,Not Disclosed,['Noida'],"Location: Noida\nShift Timings: Flexible (Rotational Shifts)\nAbout the Role:\nWe are seeking motivated and adaptable freshers to join our team in the Banking and Financial Services sector. This is an excellent opportunity for individuals looking to start their careers in a dynamic industry, with multiple process roles available, including research, remediation, keying, and data entry.\nKey Responsibilities:\nConduct research and analysis to support various banking and financial processes.\nPerform tasks such as remediation, keying, data entry, or other activities depending on the process requirements, which may involve check adjustments, returns, exceptions, or other related areas.\nEnsure data confidentiality and integrity throughout the workflow.\nWork collaboratively with team members and other departments to meet project deadlines.\nAdapt to assigned tasks within the process as per business needs.\nConsistently meet performance metrics and quality standards.\n\nTraining and Support:\nComprehensive process-specific training will be provided to help you succeed in your role.\nPerks and Benefits:\n2-way cab pick-up and drop-off facility for a stress-free commute.\nOne complimentary meal provided during the shift.\nExposure to multiple functional areas, enhancing your professional growth.\nSupportive work environment focused on learning and development.",Industry Type: Banking,"Department: BFSI, Investments & Trading","Employment Type: Full Time, Permanent","['Banking Process', 'Banking Operations']",2025-06-13 06:05:16
"Senior, Software Engineer",Walmart,6 - 9 years,Not Disclosed,['Bengaluru'],"Position Summary...\nWhat youll do...\nAbout Team:\nThis is the team that powers services and applications to manage Customer Orders, Trip Lifecycle, Delivery Partners Profile and Work Assignment in a friction-less, predictableway. Build reusable Saa S products and services that manage Customer accounts and Identities and power end-to-end account creation, login, session, profile and membership journey.\nWhat youll do:\nDesign ; develop highly scalable services and solve complex software systems problems by leveraging state-of-the-art technology Coding, unit testing, building high performance and scalable applications that meet the needs of millions of Walmart-International customers, in the areas of supply chain management ; Customer experience. Gain exposure to various emerging technologies used in E-commerce platforms. Provide technical direction, architecture leadership and expertise to the team. Participate in medium- to large-scale, complex, cross-functional projects by reviewing project, product and business requirements; translating requirements into technical solutions; gathering requested information (for example, design documents, product requirement); designing robust and scalable architectures; writing and developing code; conducting unit testing; communicating status and issues to team members and stakeholders; collaborating with cross functional teams; troubleshooting open issues and bug-fixes; enhancing design to prevent re-occurrences of defects; ensuring on-time delivery;\nWhat youll bring:\n6 to 9 years of total experience of which 6+ years in Backend engineering platform development. 5+ years of experience in Java technologies, Distributed systems and large-scale application development and design. Experience with a containerization technology and Microservice Well versed in CI/CD Work with Java, Multithreading, Data Structures, Algorithm, Design Patterns and develop robust high- performance and scalable applications. Extremely strong technical background with the capability of being hands-on and ability to mentor top individual technical talent\nAbout Walmart Global Tech\nImagine working in an environment where one line of code can make life easier for hundreds of millions of people. Thats what we do at Walmart Global Tech. Were a team of software engineers, data scientists, cybersecurity experts and service professionalswithin the worlds leading retailer who make an epic impact and are at the forefront of the next retail disruption. People are why we innovate, and people power our innovations. We are people-led and tech-empowered.\nWe train our team in the skillsets of the future and bring in experts like you to help us grow. We have roles for those chasingtheir first opportunity as well as those looking for the opportunity that will define their career. Here, you can kickstart a great career in tech, gain new skills and experience for virtually every industry, or leverage your expertise to innovate at scale,impact millions and reimagine the future of retail.\nFlexible, hybrid work\nWe use a hybrid way of working with primary in office presence coupled with an optimal mix of virtual presence. We use our campuses to collaborate and be together in person, as business needs require and for development and networking opportunities. This approachhelps us make quicker decisions, remove location barriers across our global team, be more flexible in our personal lives.\nBenefits\nBeyond our great compensation package, you can receive incentive awards for your performance. Other great perks include ahost of best-in-class benefits maternity and parental leave, PTO, health benefits, and much more.\nBelonging\n.\nAt Walmart, our vision is everyone included. By fostering a workplace culture where everyone isand feelsincluded, everyone wins. Our associates and customers reflect the makeup of all 19 countries where we operate. By making Walmart a welcoming place where all people feel like they belong, were able to engage associates, strengthen our business, improve our ability to serve customers, and support the communities where we operate.\n\nEqual Opportunity Employer:\nWalmart, Inc. is an Equal Opportunity Employer By Choice. We believe we are best equipped to help our associates, customers and the communities we serve live better when we really know them. That means understanding, respecting and valuing uniquestyles, experiences, identities, ideas and opinions while being welcoming of all people.\nMinimum Qualifications...\nMinimum Qualifications:Option 1: Bachelors degree in computer science, information technology, engineering, information systems, cybersecurity, or related area and 3years experience in software engineering or related area at a technology, retail, or data-driven company.\nOption 2: 5 years experience in software engineering or related area at a technology, retail, or data-driven company.\nPreferred Qualifications...\nCertification in Security+, GISF, CISSP, CCSP, or GSEC, Master s degree in computer science, information technology, engineering, information systems, cybersecurity, or related area and 1 year s experience leading information security or cybersecurity projects\nInformation Technology - CISCO Certification - Certification Primary Location... BLOCK- 1, PRESTIGE TECH PACIFIC PARK, SY NO. 38/1, OUTER RING ROAD KADUBEESANAHALLI, , India\n\n\n",Industry Type: Retail,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Supply chain management', 'Multithreading', 'Networking', 'Coding', 'Information security', 'Data structures', 'Application development', 'Unit testing', 'Troubleshooting', 'Information technology']",2025-06-13 06:05:18
Sr Manager of Software Engineering,JPMorgan Chase Bank,14 - 20 years,Not Disclosed,['Bengaluru'],"When you mentor and advise multiple technical teams and move financial technologies forward, it s a big challenge with big impact. You were made for this.\n\n\nAs a Senior Manager of Software Engineering at JPMorgan Chase within the Consumer and Community Banking Technology Team, you serve in a leadership role by providing technical coaching and advisory for multiple technical teams, as well as anticipate the needs and potential dependencies of other functions within the firm. As an expert in your field, your insights influence budget and technical considerations to advance operational efficiencies and functionalities.\n\nJob responsibilities\n\n\n\nProvide direction, oversight, and coaching for a team of entry-level to mid-level software engineers working on basic to moderately complex tasks.\n\nBe accountable for decisions affecting team resources, budget, tactical operations, and the execution and implementation of processes and procedures.\n\nLead the design, development, testing, and implementation of data visualization projects to support business objectives.\n\nCollaborate with data analysts, data scientists, and business stakeholders to understand data requirements and translate them into effective visual solutions.\n\nWork in an Agile development environment with team members, including Product Managers, UX Designers, QA Engineers, and other Software Engineers.\n\nValidate the technical feasibility of UI/UX designs and provide regular technical guidance to support business and technical teams, contractors, and vendors.\n\nDevelop secure, high-quality production code, review and debug code written by others, and drive decisions influencing product design, application functionality, and technical operations.\n\nServe as a subject matter expert in one or more areas of focus and actively contribute to the engineering community as an advocate of firmwide frameworks, tools, and practices of the Software Development Life Cycle.\n\nInfluence peers and project decision-makers to consider the use and application of leading-edge technologies.\n\nDevelop and maintain dashboards, reports, and interactive visualizations using tools such as Tableau, ensuring data accuracy and integrity by implementing best practices in data visualization and management.\n\nStay current with industry trends and emerging technologies in data visualization and analytics, communicate complex data insights clearly to various audiences, including senior management, and manage a team of data visualization associates, providing guidance and mentorship to junior team members.\n\n\n\nRequired qualifications, capabilities, and skills\n\n\n\nFormal training or certification in software engineering concepts and 5+ years of applied experience.\n\n5+ Years of experience as a Web/UI Lead Architect\n\nProficiency in Javascript, Typescript, HTML, CSS\n\nExpert knowledge in ReactJs, Redux, React hooks.\n\nStrong understanding of front-end coding and development technologies\n\nHands-on practical experience delivering system design, application development, testing, and operational stability\n\nAdvanced knowledge of software applications and technical processes with considerable in-depth knowledge in UI and Web Technologies\n\nAbility to tackle design and functionality problems independently with little to no oversight\n\nPractical cloud native experience\n\nExperience in Computer Science, Computer Engineering, Mathematics, or a related technical field\n\n\n\nPreferred qualifications, capabilities, and skills\n\n\n\nFull stack development with Node/. NET/Java\n\nFamiliarity with working in event driven environments\n\nA good understanding of cross-browser compatibility issues and their solutions along with Typescript\n\nExperience working with Databases and ability to write SQL queries along with experience with messaging platforms\n\nBachelor s degree in data science, Computer Science, Information Systems, Statistics, or a related field.\n\nProblem solver and solution oriented. Strong written and verbal communication skills. Jira and Agile practices\n\nExperience with big data technologies and machine learning is a plus.",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Front end', 'Coding', 'Javascript', 'Agile', 'System design', 'HTML', 'Application development', 'JIRA', 'Analytics']",2025-06-13 06:05:19
Sr SW Engineer - ML,Visa,2 - 7 years,Not Disclosed,['Bengaluru'],"This position is for a Senior Data Engineer with solid development experience who will focus on creating new capabilities for AI as a Service while maturing our code base and development processes. In this position, you are first a passionate and talented developer that can work in a dynamic environment as a member of Agile Scrum teams. Your strong technical leadership, problem-solving abilities, coding, testing and debugging skills is just a start. You must be dedicated to filling product backlog and delivering production-ready code. You must be willing to go beyond the routine and prepared to do a little bit of everything.\nEssential Functions\nCollaborate with project teams, data science teams and development teams to drive the technical roadmap and guide development and implementation of new data driven business solutions.\nDrive technical standard and best practices, and continuously improve AI Platform engineering scalability.\nArchitecture and design of AI Platform services including Machine Learning Engines, In Memory Computing Systems, Streaming Computing Systems, Distributed Data Systems and etc, in Golang, Java, and Python.\nCoordinate the implementation among development teams to ensure system performance, security, scalability and availability.\nCoaching and mentoring junior team members and evolving team talent pipeline.\n\n\nBasic Qualifications\n2+ years of relevant work experience and a Bachelors degree, OR 5+ years of relevant work experience\n\nPreferred Qualifications\n3 or more years of work experience with a Bachelor s Degree or more than 2 years of work experience with an Advanced Degree (e.g. Masters, MBA, JD, MD)",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Basic', 'data science', 'Agile scrum', 'Architecture', 'Coding', 'Debugging', 'Machine learning', 'Technical leadership', 'Business solutions', 'Python']",2025-06-13 06:05:22
DVP - Data Science,Skywings Advisors,7 - 12 years,20-35 Lacs P.A.,['Mumbai (All Areas)'],"Lead the end-to-end analytics lifecycle,from data exploration to actionable insights&product development\nDesign,develop&implement machine learning models for predictive analytics,natural language processing (NLP)\nCloud Architecture & Engineering\n\nRequired Candidate profile\n8+ years of experience in AI and Data Science roles\nStrong expertise in machine learning frameworks like TensorFlow,PyTorch,Hugging Face\ndata engineering tools such as BigQuery, Kafka, Snowflake.",Industry Type: Insurance,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Artificial Intelligence', 'Data Mining', 'Machine Learning', 'Ml']",2025-06-13 06:05:23
Azure Data Modeler,Software Company,5 - 10 years,Not Disclosed,"['Pune', 'Ahmedabad', 'Mumbai (All Areas)']","5+ yrs in Azure data modeling, Azure (Databricks, Data Lake), SQL, Spark, Python. Exp in Salesforce, Oracle, SQL Server to Azure migration. Strong in ETL, star/snowflake schemas, governance, & cross-functional collaboration. Join within 30 days.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Azure', 'Azure Databricks', 'Azure Data Lake', 'salesforce', 'SQL Server', 'Oracle', 'azure Data modeling']",2025-06-13 06:05:25
Big Data Developer,Binary Infoways,6 - 10 years,12-22 Lacs P.A.,['Hyderabad'],"AWS (EMR, S3, Glue, Airflow, RDS, Dynamodb, similar)\nCICD (Jenkins or another)\nRelational Databases experience (any)\nNo SQL databases experience (any)\nMicroservices or Domain services or API gateways or similar\nContainers (Docker, K8s, similar)",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'AWS', 'Python', 'Airflow', 'Java', 'Big Data', 'EMR', 'SQL', 'Jenkins', 'Glue', 'SCALA', 'Big Data Technologies', 'Spark']",2025-06-13 06:05:26
Business Analyst,Accenture,3 - 8 years,Not Disclosed,['Bengaluru'],"Project Role :Business Analyst\n\n\n\n\n\nProject Role Description :Analyze an organization and design its processes and systems, assessing the business model and its integration with technology. Assess current state, identify customer requirements, and define the future state and/or business solution. Research, gather and synthesize information.\n\n\n\nMust have skills :Business Analysis\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Business Analyst, you will analyze an organization and design its processes and systems, assessing the business model and its integration with technology. You will assess the current state, identify customer requirements, and define the future state and/or business solution. Research, gather, and synthesize information to drive business decisions.\nRoles & Responsibilities:- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work-related problems.- Develop detailed business requirements and user stories.- Conduct stakeholder interviews to gather business requirements.- Create process flow diagrams and business process models.- Collaborate with cross-functional teams to ensure project success.- Assist in the implementation and testing of business solutions.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Business Analysis.- Strong understanding of business process modeling.- Experience with Agile methodologies.- Knowledge of data analysis and interpretation.- Hands-on experience with requirement gathering tools.\nAdditional Information:- The candidate should have a minimum of 3 years of experience in Business Analysis.- This position is based at our Bengaluru office.- A 15 years full-time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['business process modeling', 'data analysis', 'business analysis', 'user stories', 'agile methodology', 'process flow diagram', 'project management', 'gap analysis', 'business solutions', 'wireframing', 'sql', 'product management', 'brd', 'fsd', 'flow diagrams', 'frd', 'agile', 'requirement analysis']",2025-06-13 06:05:28
Business Analyst,Accenture,15 - 20 years,Not Disclosed,['Bengaluru'],"Project Role :Business Analyst\n\n\n\n\n\nProject Role Description :Analyze an organization and design its processes and systems, assessing the business model and its integration with technology. Assess current state, identify customer requirements, and define the future state and/or business solution. Research, gather and synthesize information.\n\n\n\nMust have skills :Hotel and Lodging\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Business Process Designer, you will analyze, develop, and improve workflows, identify inefficiencies in existing processes, propose solutions to optimize effectiveness, collaborate with business users to define product requirements, and design continuous monitoring and feedback collection for process refinement.\nRoles & Responsibilities:- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Develop and document detailed workflows for business processes.- Identify inefficiencies in current processes and propose optimization solutions.- Collaborate with business users to define product requirements.- Design continuous monitoring mechanisms for process improvement.- Provide training and support to users on new processes.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Hotel and Lodging.- Strong understanding of process optimization techniques.- Experience in workflow analysis and improvement.- Knowledge of continuous monitoring and feedback mechanisms.- Hands-on experience in process documentation.\nAdditional Information:- Should have a minimum of overall 4 years of relevant profession experience- Should have 15 years of full-time education- Location:Bangalore (primary), Gurgaon (secondary)\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['continuous monitoring', 'process optimization', 'business analysis', 'workflow analysis', 'process documentation', 'kubernetes', 'continuous integration', 'python', 'documentation', 'process improvement', 'ansible', 'docker', 'sql', 'providing training', 'devops', 'jenkins', 'aws']",2025-06-13 06:05:30
Business Analyst,Accenture,15 - 20 years,Not Disclosed,['Bengaluru'],"Project Role :Business Analyst\n\n\n\n\n\nProject Role Description :Analyze an organization and design its processes and systems, assessing the business model and its integration with technology. Assess current state, identify customer requirements, and define the future state and/or business solution. Research, gather and synthesize information.\n\n\n\nMust have skills :Openlink Endur Functional\n\n\n\n\nGood to have skills :Agile Testing, Test Automation StrategyMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Business Analyst, you will analyze an organization and design its processes and systems, assessing the business model and its integration with technology. You will assess the current state, identify customer requirements, and define the future state and/or business solution. Research, gather, and synthesize information to drive business decisions.\nRoles & Responsibilities:- gathering, functional analysis, functional design and end to end configuration of product set up in Endur - Adept in handling business changes and managing client expectations - Provide comprehensive document of changes, coordinate user testing, user training - Effective management of tasks, timelines, progress reporting on tasks- Manage communication across multiple teams and be a good team player\nProfessional & Technical\n\n\n\n\nSkills:\n- Thorough understanding of Agile and waterfall delivery methodology - Should have experience in managing an end to end delivery of a module - Strong functional understanding of Gas, power, coal, emissions/renewables trading - Should have deep experience in latest Endur version with setup experience on all key modules - Should have good experience in writing basic sqls, understanding of the Endur data model - Should have basic knowledge on test automation- Must To Have\n\n\n\n\nSkills:\nProficiency in Openlink Endur Functional.- Strong understanding of business process analysis and optimization.- Experience in system integration and technology assessment.- Knowledge of Agile methodologies for project management.- Good To Have\n\n\n\n\nSkills:\nExperience with Agile Testing and Test Automation Strategy.\nAdditional Information:- The candidate should have a minimum of 6 years of experience in Openlink Endur Functional.- This position is based at our Bengaluru office.- A 15 years full-time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['openlink endur', 'business process analysis', 'emission', 'waterfall', 'agile', 'project management', 'automation testing', 'functional analysis', 'agile testing', 'business analysis', 'user stories', 'functional design', 'user training', 'system integration', 'brd', 'frd', 'agile methodology']",2025-06-13 06:05:31
Business Analyst,Accenture,5 - 10 years,Not Disclosed,['Navi Mumbai'],"Project Role :Business Analyst\n\n\n\n\n\nProject Role Description :Analyze an organization and design its processes and systems, assessing the business model and its integration with technology. Assess current state, identify customer requirements, and define the future state and/or business solution. Research, gather and synthesize information.\n\n\n\nMust have skills :Property and Casualty Insurance\n\n\n\n\nGood to have skills :Business AnalysisMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Business Analyst, you will analyze an organization and design its processes and systems, assessing the business model and its integration with technology. You will assess the current state, identify customer requirements, and define the future state and/or business solution. Additionally, you will research, gather, and synthesize information to contribute to the success of the organization.\nRoles & Responsibilities:- Expected to be an SME, collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Conduct thorough analysis of business processes and systems.- Identify areas for improvement and propose solutions.- Collaborate with stakeholders to gather and document business requirements.- Create and maintain project documentation.- Assist in the development and execution of test plans.- Conduct user acceptance testing and provide feedback.- Support the implementation of new processes and systems.- Provide training and support to end-users.- Stay up-to-date with industry trends and best practices.- Assist in the development and implementation of change management strategies.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Property and Casualty Insurance, Business Analysis.- Strong understanding of statistical analysis and machine learning algorithms.- Experience with data visualization tools such as Tableau or Power BI.- Hands-on implementing various machine learning algorithms such as linear regression, logistic regression, decision trees, and clustering algorithms.- Solid grasp of data munging techniques, including data cleaning, transformation, and normalization to ensure data quality and integrity.- Strong understanding of business process modeling and analysis.- Experience in conducting stakeholder interviews and workshops.- Ability to translate business requirements into functional specifications.- Familiarity with data analysis and visualization tools.- Ability to work effectively in a cross-functional team environment.\nAdditional Information:- The candidate should have a minimum of 5 years of experience in Property and Casualty Insurance and Business Analysis.- This position is based at our Bengaluru office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['business process modeling', 'property and casualty insurance', 'business analysis', 'machine learning algorithms', 'statistics', 'data analysis', 'bi', 'power bi', 'machine learning', 'data cleansing', 'business requirement analysis', 'tableau', 'data visualization', 'data munging']",2025-06-13 06:05:33
Business Analyst,Accenture,15 - 20 years,Not Disclosed,['Bengaluru'],"Project Role :Business Analyst\n\n\n\n\n\nProject Role Description :Analyze an organization and design its processes and systems, assessing the business model and its integration with technology. Assess current state, identify customer requirements, and define the future state and/or business solution. Research, gather and synthesize information.\n\n\n\nMust have skills :Openlink Endur Functional\n\n\n\n\nGood to have skills :OpenLink EndurMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification :1:Any graduation, 2:15-year fulltime education\nProject Role :Business Analyst\n\nProject Role Description :Analyze an organization and design its processes and systems, assessing the business model and its integration with technology. Assess current state, identify customer requirements, and define the future state and/or business solution. Research, gather and synthesize information.\nMust have Skills :Openlink Endur FunctionalGood to Have Skills :OpenLink EndurJob :Key Responsibilities :1gathering, functional analysis, functional design and end to end configuration of product set up in Endur 2:Adept in handling business changes and managing client expectations 3:Provide comprehensive document of changes, coordinate user testing, user training 4:Effective management of tasks, timelines, progress reporting on tasks 5:Manage communication across multiple teams and be a good team player\nTechnical Experience :1 Experience and understanding of Agile and waterfall delivery methodology 2 Should have experience in managing an end to end delivery of a module 3 Strong functional understanding of Gas, power, coal, emissions/renewables trading 4 Should have deep experience in latest Endur version with setup experience on all key modules 5 Dotnet, Endur, ETRM/CTRM strong programming skills, good software engineering skillset\nProfessional Attributes :1 Candidate must be able to work in small teams, or alone,for project delivery 2 Candidate should be able to exhibit strong analytical and problem-solving skills, to deliver high quality solutions to clients 3 Candidate should be able to liaise with other teams to resolve cross functional issue\nEducational Qualification:1:Any graduation, 2:15-year fulltime educationAdditional Info :\n\nQualification\n\n1:Any graduation, 2:15-year fulltime education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['openlink endur', 'emission', 'openlink', 'waterfall', 'agile', 'ctrm', 'project management', 'etrm', 'functional analysis', 'gap analysis', 'business analysis', 'user stories', 'functional design', 'sql', 'user training', 'brd', 'project delivery', '.net', 'frd', 'scrum', 'requirement analysis']",2025-06-13 06:05:34
Business Analyst,Accenture,5 - 10 years,Not Disclosed,['Navi Mumbai'],"Project Role :Business Analyst\n\n\n\n\n\nProject Role Description :Analyze an organization and design its processes and systems, assessing the business model and its integration with technology. Assess current state, identify customer requirements, and define the future state and/or business solution. Research, gather and synthesize information.\n\n\n\nMust have skills :Property and Casualty Insurance\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Business Analyst, you will analyze an organization and design its processes and systems, assessing the business model and its integration with technology. You will assess the current state, identify customer requirements, and define the future state and/or business solution. Additionally, you will research, gather, and synthesize information to contribute to the success of the organization.\nRoles & Responsibilities:- Expected to be an SME, collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Conduct thorough analysis of business processes and systems.- Identify areas for improvement and propose solutions.- Collaborate with stakeholders to gather and document business requirements.- Create and maintain project documentation.- Assist in the development and execution of test plans.- Conduct user acceptance testing and provide feedback.- Support the implementation of new processes and systems.- Provide training and support to end-users.- Stay up-to-date with industry trends and best practices.- Assist in the development and implementation of change management strategies.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Property and Casualty Insurance.- Strong understanding of business analysis methodologies and techniques.- Experience in conducting business process analysis and improvement.- Knowledge of requirements gathering and documentation.- Familiarity with project management principles and practices.- Good To Have\n\n\n\n\nSkills:\nExperience with Agile methodologies.- Experience with data analysis and visualization tools.- Knowledge of insurance industry regulations and compliance.- Excellent communication and interpersonal skills.- Ability to work independently and in a team environment.\nAdditional Information:- The candidate should have a minimum of 5 years of experience in Property and Casualty Insurance.- This position is based at our Bengaluru office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['project management', 'business analysis', 'business process analysis', 'user acceptance testing', 'agile methodology', 'data analysis', 'project documentation', 'documentation', 'change management', 'sql', 'providing training', 'business requirement analysis', 'brd', 'frd', 'agile']",2025-06-13 06:05:36
SW Business Analysis Team Analyst,Accenture,8 - 10 years,Not Disclosed,['Bengaluru'],"Skill required: Tech for Operations - Business Analysis\n\n\n\n\nDesignation: SW Business Analysis Team Analyst\n\n\n\n\nQualifications:Any Graduation\n\n\n\n\nYears of Experience:8 to 10 years\n\n\n\nAbout AccentureCombining unmatched experience and specialized skills across more than 40 industries, we offer Strategy and Consulting, Technology and Operations services, and Accenture Song all powered by the worlds largest network of Advanced Technology and Intelligent Operations centers. Our 699,000 people deliver on the promise of technology and human ingenuity every day, serving clients in more than 120 countries. Visit us at www.accenture.com\n\n\n\n\nWhat would you do\nYou will be part of the Technology for Operations team that acts as a trusted advisor and partner to Accenture Operations. The team provides innovative and secure technologies to help clients build an intelligent operating model, driving exceptional results. We work closely with the sales, offering and delivery teams to identify and build innovative solutions.The Tech For Operations (TFO) team provides innovative and secure technologies to help clients build an intelligent operating model, driving exceptional results. Works closely with the sales, offering and delivery teams to identify and build innovative solutions. Major sub deals include AHO(Application Hosting Operations), ISMT (Infrastructure Management), Intelligent AutomationIn this role as a Business Requirement Analysis, you will be required to identify and analyze specific and well-documented requirements for business needs related to processes, organizations systems, applications, etc including the ability to understand and map the requirements to the given business problems. The role involves defining the scope of the project, the timescales, and the resources needed to complete it.\n\n\n\n\nWhat are we looking for\nProblem-solving skillsResults orientationStrong analytical skillsWritten and verbal communicationNumerical ability\n\n\n\nRoles and Responsibilities: In this role you are required to do analysis and solving of moderately complex problems Typically creates new solutions, leveraging and, where needed, adapting existing methods and procedures The person requires understanding of the strategic direction set by senior management as it relates to team goals Primary upward interaction is with direct supervisor or team leads Generally interacts with peers and/or management levels at a client and/or within Accenture The person should require minimal guidance when determining methods and procedures on new assignments Decisions often impact the team in which they reside and occasionally impact other teams Individual would manage medium-small sized teams and/or work efforts (if in an individual contributor role) at a client or within Accenture Please note that this role may require you to work in rotational shifts\n\nQualification\n\nAny Graduation",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['service operations', 'business analysis', 'sales', 'business requirement analysis', 'bigquery', 'site execution', 'hive', 'bar bending schedule', 'python', 'project management', 'contractor billing', 'client billing', 'sql', 'electrical maintenance', 'gcp', 'site engineering', 'data flow']",2025-06-13 06:05:38
Business Interlock Analyst,Accenture,3 - 5 years,Not Disclosed,['Bengaluru'],"Skill required: Talent Development - Instructor-Led Training (ILT)\n\n\n\n\nDesignation: Business Interlock Analyst\n\n\n\n\nQualifications:Any Graduation\n\n\n\n\nYears of Experience:3 to 5 years\n\n\n\nAbout AccentureCombining unmatched experience and specialized skills across more than 40 industries, we offer Strategy and Consulting, Technology and Operations services, and Accenture Song all powered by the worlds largest network of Advanced Technology and Intelligent Operations centers. Our 699,000 people deliver on the promise of technology and human ingenuity every day, serving clients in more than 120 countries. Visit us at www.accenture.com\n\n\n\n\nWhat would you do\nImprove workforce performance and productivity, boosts business agility, increases revenue and reduces costsTalent Development processThe practice of training and learning material between an instructor and learners, either individuals or groups. Instructors can also be referred to as a facilitator, who may be knowledgeable and experienced in the learning material, but can also be used more for their facilitation skills and ability to deliver material to learners.\n\n\n\n\nWhat are we looking for\nCustomer Support OperationsTelecom Dispute ManagementAbility to perform under pressureAbility to meet deadlinesCollaboration and interpersonal skillsWritten and verbal communicationCustomer service operation / retailNetwork Services - TelecomCustomer AnalyticsCustomer Communications\n\n\n\nRoles and Responsibilities: In this role you are required to do analysis and solving of lower-complexity problems Your day to day interaction is with peers within Accenture before updating supervisors In this role you may have limited exposure with clients and/or Accenture management You will be given moderate level instruction on daily work tasks and detailed instructions on new assignments The decisions you make impact your own work and may impact the work of others You will be an individual contributor as a part of a team, with a focused scope of work Please note that this role may require you to work in rotational shifts\n\nQualification\n\nAny Graduation",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['business analysis', 'human resource management', 'talent development', 'talent management', 'recruitment', 'hr generalist activities', 'corporate hr', 'strategic hr', 'business hr', 'talent acquisition', 'employee engagement', 'performance management', 'talent engagement']",2025-06-13 06:05:40
Business Analyst,Accenture,5 - 10 years,Not Disclosed,['Bengaluru'],"Project Role :Business Analyst\n\n\n\n\n\nProject Role Description :Analyze an organization and design its processes and systems, assessing the business model and its integration with technology. Assess current state, identify customer requirements, and define the future state and/or business solution. Research, gather and synthesize information.\n\n\n\nMust have skills :Microsoft Dynamics 365 ERP Technical\n\n\n\n\nGood to have skills :Microsoft Dynamics AX TechnicalMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Business Analyst, you will analyze an organization and design its processes and systems, assessing the business model and its integration with technology. Assess current state, identify customer requirements, and define the future state and/or business solution. Research, gather and synthesize information.\nRoles & Responsibilities:- Expected to be an SME.- Collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Develop and implement strategies for process improvement.- Conduct business process modeling and simulation.- Facilitate workshops and meetings to gather requirements.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Microsoft Dynamics 365 ERP Technical.- Good To Have\n\n\n\n\nSkills:\nExperience with Microsoft Dynamics AX Technical.- Strong understanding of ERP systems and business processes.- Knowledge of data analysis and interpretation.- Experience in system integration and customization.\nAdditional Information:- The candidate should have a minimum of 5 years of experience in Microsoft Dynamics 365 ERP Technical.- This position is based at our Hyderabad office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['erp', 'data analysis', 'microsoft dynamics', 'erp systems', 'microsoft dynamics ax', 'business process modeling', 'project management', 'business analysis', 'process improvement', 'user stories', 'sql', 'system integration', 'brd', 'frd', 'scrum', 'agile', 'requirement analysis']",2025-06-13 06:05:42
Business Analyst,Accenture,7 - 12 years,Not Disclosed,['Bengaluru'],"Project Role :Business Analyst\n\n\n\n\n\nProject Role Description :Analyze an organization and design its processes and systems, assessing the business model and its integration with technology. Assess current state, identify customer requirements, and define the future state and/or business solution. Research, gather and synthesize information.\n\n\n\nMust have skills :Microsoft Dynamics 365 ERP Technical\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Business Analyst, you will analyze an organization and design its processes and systems, assessing the business model and its integration with technology. You will assess the current state, identify customer requirements, and define the future state and/or business solution. Research, gather, and synthesize information to contribute to key decisions and solutions.\nRoles & Responsibilities:- Expected to be an SME, collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Lead requirements gathering sessions with stakeholders.- Develop functional specifications and system design documents.- Conduct gap analysis and recommend solutions.- Facilitate user acceptance testing and provide post-implementation support.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Microsoft Dynamics 365 ERP Technical.- Strong understanding of system integration and data migration.- Experience in configuring and customizing Dynamics 365 ERP modules.- Knowledge of SQL and database management.- Hands-on experience in troubleshooting and resolving technical issues.\nAdditional Information:- The candidate should have a minimum of 7.5 years of experience in Microsoft Dynamics 365 ERP Technical.- This position is based at our Hyderabad office.- A 15 years full-time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['erp', 'microsoft dynamics', 'data migration', 'sql', 'system integration', 'design documents', 'project management', 'gap analysis', 'system design', 'business analysis', 'user stories', 'database management', 'brd', 'user acceptance testing', 'troubleshooting', 'scrum', 'agile']",2025-06-13 06:05:43
Business Analyst,Amazon,1 - 6 years,Not Disclosed,['Bengaluru'],"Amazon India is seeking highly analytical and detail oriented candidate to join the Consumables Category team in Bangalore. As part of the team, the analyst will analyze business decisions, troubleshoot issues, and communicate the findings effectively across different teams. This role will support regular operations of a part of the E-commerce business.\nAn ideal analyst is one who enjoys discovering and solving complicated problems, can quickly learn complex systems, likes to work with numbers, and takes pride in organizing and communicating his work. 1+ years of data analytics or automation experience\n1+ years of capacity planning, operations planning, business analysis or similar experience\nBachelors degree\nKnowledge of data pipelining and extraction using SQL\nExperience with data mining tools like SQL, SAS, SPSS, or similar Knowledge of SQL and Excel at a moderate or advanced level",,,,"['Automation', 'SAS', 'Business Analyst', 'Business analysis', 'Operations planning', 'Analytical', 'SPSS', 'Data mining', 'SQL', 'Capacity planning']",2025-06-13 06:05:45
Analyst,HARMAN,2 - 7 years,Not Disclosed,['Bengaluru'],"JOB IDENTIFICATION\nBusiness Title: Analyst\nBusiness Unit: Symphony Health Solutions\nLocation: Bangalore\nJOB SUMMARY\nWe are looking for a candidate with relevant analytics experience including working with larger datasets with healthcare background as a preference, Basic SQL and Advance Excel skills.\nAnalyst performs complex data analysis independently, preparing comprehensive reports and presentations of data analysis findings for the client or data vendor per established service level agreements. Job functions include inputs to processes, executing programs, assessing data accuracy, drawing research conclusions, and formatting and presenting output. This position is an expert in researching, resolving and documenting client/vendor inquiries. Work in a fast paced dynamic environment.",,,,"['Data analysis', 'Service level', 'Senior Analyst', 'Process improvement', 'Agile', 'Healthcare', 'Manager Quality Control', 'Downstream', 'Auditing', 'SQL']",2025-06-13 06:05:47
Hiring Business Analyst with MSTR or Dataiku - Bangalore/ Chennai !!!!,Tech Mahindra,8 - 13 years,Not Disclosed,"['Chennai', 'Bengaluru']","A Business Analyst in the Financial Crime Surveillance Operations (FCSO) Data & Reporting PO Team understands the core concepts, principles, processes or procedures of Data & MI.\nExperienced in using MSTR reports & Dataiku. The FCSO Business Analyst gathers requirements from various stakeholders and creates user stories for the squad to understand and take it for delivery. They must have strong analytical skills, understand the strategic framework & make sense of data.\n\n1.Core Business Analysis Skills\nRequirement Gathering\nDocumentation\nGap analysis\n\n2. Data & MI Expertise\nData Analysis\nData mapping & Metrics understanding\nFCSO Process knowledge (Good to have)\n\n3. Technical Skills\nQuery Databases\nFamiliarity with BI Tools like Dataiku, MSTR\n\n4. Agile & Delivery management\n\nUnderstanding of Scrum for collaborating with Squads\nUser Story Creation\nBacklog Management\nStakeholder Management",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Microstrategy', 'Business Analytics', 'Dataiku', 'Business Analysis']",2025-06-13 06:05:49
I&F Decision Sci Practitioner Sr Analyst,Accenture,5 - 8 years,Not Disclosed,['Navi Mumbai'],"Skill required: Delivery - Warranty Management\n\n\n\n\nDesignation: I&F Decision Sci Practitioner Sr Analyst\n\n\n\n\nQualifications:Any Graduation\n\n\n\n\nYears of Experience:5 to 8 years\n\n\n\nAbout AccentureCombining unmatched experience and specialized skills across more than 40 industries, we offer Strategy and Consulting, Technology and Operations services, and Accenture Song all powered by the worlds largest network of Advanced Technology and Intelligent Operations centers. Our 699,000 people deliver on the promise of technology and human ingenuity every day, serving clients in more than 120 countries. Visit us at www.accenture.com\n\n\n\n\nWhat would you do\nData & AIDefine warranty offerings; run outsourced after-sales warranty support and entitlement programs; evaluate customer feedback and planned versus actual costs of warranty coverage; use warranty data analytics to reduce cost and improve product quality; increase recoveries from suppliers and design and deploy warranty solutions.\n\n\n\n\nWhat are we looking for\nWarranty Analytics Automotive Warranty Scripting Data Analysis & Interpretation Business Intelligence Commitment to quality Adaptable and flexible Agility for quick learning Ability to work well in a team Written and verbal communication Data Engineering/SQL Databricks ML\n\n\n\nRoles and Responsibilities: In this role you are required to do analysis and solving of increasingly complex problems Your day-to-day interactions are with peers within Accenture You are likely to have some interaction with clients and/or Accenture management You will be given minimal instruction on daily work/tasks and a moderate level of instruction on new assignments Decisions that are made by you impact your own work and may impact the work of others In this role you would be an individual contributor and/or oversee a small work effort and/or team\n\nQualification\n\nAny Graduation",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['warranty management', 'data analytics', 'data analysis', 'business intelligence', 'sql', 'hive', 'python', 'service operations', 'pyspark', 'data warehousing', 'microsoft azure', 'machine learning', 'data engineering', 'tableau', 'data science', 'data modeling', 'spark', 'hadoop', 'big data', 'aws', 'etl']",2025-06-13 06:05:50
"Engineer, Principal/Manager - Machine Learning, AI",Qualcomm,8 - 13 years,Not Disclosed,['Bengaluru'],"General Summary:\nAs a leading technology innovator, Qualcomm pushes the boundaries of what's possible to enable next-generation experiences and drives digital transformation to help create a smarter, connected future for all. As a Qualcomm Systems Engineer, you will research, design, develop, simulate, and/or validate systems-level software, hardware, architecture, algorithms, and solutions that enables the development of cutting-edge technology. Qualcomm Systems Engineers collaborate across functional teams to meet and exceed system-level requirements and standards.\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 8+ years of Systems Engineering or related work experience.\nOR\nMaster's degree in Engineering, Information Systems, Computer Science, or related field and 7+ years of Systems Engineering or related work experience.\nOR\nPhD in Engineering, Information Systems, Computer Science, or related field and 6+ years of Systems Engineering or related work experience.\nPrincipal Engineer Machine Learning\nWe are looking for a Principal AI/ML Engineer with expertise in model inference, optimization, debugging, and hardware acceleration. This role will focus on building efficient AI inference systems, debugging deep learning models, optimizing AI workloads for low latency, and accelerating deployment across diverse hardware platforms.\nIn addition to hands-on engineering, this role involves cutting-edge research in efficient deep learning, model compression, quantization, and AI hardware-aware optimization techniques. You will explore and implement state-of-the-art AI acceleration methods while collaborating with researchers, industry experts, and open-source communities to push the boundaries of AI performance.\nThis is an exciting opportunity for someone passionate about both applied AI development and AI research, with a strong focus on real-world deployment, model interpretability, and high-performance inference.\nEducation & Experience:\n20+ years of experience in AI/ML development, with at least 5 years in model inference, optimization, debugging, and Python-based AI deployment.\nMasters or Ph.D. in Computer Science, Machine Learning, AI\nLeadership & Collaboration\nLead a team of AI engineers in Python-based AI inference development.\nCollaborate with ML researchers, software engineers, and DevOps teams to deploy optimized AI solutions.\nDefine and enforce best practices for debugging and optimizing AI models\nKey Responsibilities\nModel Optimization & Quantization\nOptimize deep learning models using quantization (INT8, INT4, mixed precision etc), pruning, and knowledge distillation.\nImplement Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT) for deployment.\nFamiliarity with TensorRT, ONNX Runtime, OpenVINO, TVM\nAI Hardware Acceleration & Deployment\nOptimize AI workloads for Qualcomm Hexagon DSP, GPUs (CUDA, Tensor Cores), TPUs, NPUs, FPGAs, Habana Gaudi, Apple Neural Engine.\nLeverage Python APIs for hardware-specific acceleration, including cuDNN, XLA, MLIR.\nBenchmark models on AI hardware architectures and debug performance issues\nAI Research & Innovation\nConduct state-of-the-art research on AI inference efficiency, model compression, low-bit precision, sparse computing, and algorithmic acceleration.\nExplore new deep learning architectures (Sparse Transformers, Mixture of Experts, Flash Attention) for better inference performance.\nContribute to open-source AI projects and publish findings in top-tier ML conferences (NeurIPS, ICML, CVPR).\nCollaborate with hardware vendors and AI research teams to optimize deep learning models for next-gen AI accelerators.\nDetails of Expertise:\nExperience optimizing LLMs, LVMs, LMMs for inference\nExperience with deep learning frameworks: TensorFlow, PyTorch, JAX, ONNX.\nAdvanced skills in model quantization, pruning, and compression.\nProficiency in CUDA programming and Python GPU acceleration using cuPy, Numba, and TensorRT.\nHands-on experience with ML inference runtimes (TensorRT, TVM, ONNX Runtime, OpenVINO)\nExperience working with RunTimes Delegates (TFLite, ONNX, Qualcomm)\nStrong expertise in Python programming, writing optimized and scalable AI code.\nExperience with debugging AI models, including examining computation graphs using Netron Viewer, TensorBoard, and ONNX Runtime Debugger.\nStrong debugging skills using profiling tools (PyTorch Profiler, TensorFlow Profiler, cProfile, Nsight Systems, perf, Py-Spy).\nExpertise in cloud-based AI inference (AWS Inferentia, Azure ML, GCP AI Platform, Habana Gaudi).\nKnowledge of hardware-aware optimizations (oneDNN, XLA, cuDNN, ROCm, MLIR, SparseML).\nContributions to open-source community\nPublications in International forums conferences journals",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Machine Learning', 'AWS Inferentia', 'Azure ML', 'AI/ML', 'ONNX Runtime', 'OpenVINO', 'GCP AI', 'TVM', 'XLA', 'MLIR', 'TensorRT', 'Python']",2025-06-13 06:05:52
S&C Global Network - AI - Auto & Industrial - Analyst,Accenture,1 - 3 years,Not Disclosed,['Bengaluru'],"Management Level:Ind & Func AI Decision Science Analyst\n\n\n\nLocation:Bengaluru (Bangalore), Gurugram (Gurgaon), Hyderabad, Chennai.\n\n\n\nMust-have skills:Programming languages -Python/R, Generative AI, Large Language Models (LLMs), ML libraries such as Scikit-learn, TensorFlow, Torch, Lang Chain, or OpenAI API, RAG Applications.\n\n\n\n\nGood to have skills:Big data technologies such as Spark or Hadoop,AI model explainability(XAI),bias detection and AI ethics. Familiarity with Edge AI and deploying models on embedded devices for industrial automation. Experience with Reinforcement Learning (RL) and AI-driven optimization techniques.\n\n\n\nJob\n\n\nSummary\n\nWe are looking for a Data Scientist / AI Specialist with 1-3 years of experience to join our team and work on client projects in the Automotive & Industrial sectors. This role will involve leveraging traditional Machine Learning (ML), Generative AI (GenAI), Agentic AI, and Autonomous AI Systems to drive innovation, optimize processes, and enhance decision-making in complex industrial environments.\n\nPrior experience in the Auto/Industrial industry is a plus, but we welcome candidates from any domain with a strong analytical mindset and a passion for applying AI to real-world business challenges.\n\n\n\n\nRoles & Responsibilities:\nDevelop, deploy and monitor AI/ML models in production environments & enterprise systems, including predictive analytics, anomaly detection, and process optimization for clients.\nWork with Generative AI models (e.g., GPT, Stable Diffusion, DALLE) for applications such as content generation, automated documentation, code synthesis, and intelligent assistants.\nImplement Agentic AI systems, including AI-powered automation, self-learning agents, and decision-support systems for industrial applications.\nDesign and build Autonomous AI solutions for tasks like predictive maintenance, supply chain optimization, and robotic process automation (RPA).\nWork with structured and unstructured data from various sources, including IoT sensors, manufacturing logs, and customer interactions.\nOptimize and fine-tune LLMs (Large Language Models) for specific business applications, ensuring ethical and explainable AI use.\nUtilize MOps and AI orchestration tools to streamline model deployment, monitoring, and retraining cycles.\nCollaborate with cross-functional teams, including engineers, business analysts, and domain experts, to align AI solutions with business objectives.\nStay updated with cutting-edge AI research in Generative AI, Autonomous AI, and Multi-Agent Systems.\n\n\n\n\nProfessional & Technical\n\n\n\n\nSkills:\n\n1-3 years of experience in Data Science, Machine Learning, or AI-related roles.\nProficiency in Python (preferred) or R, and experience with ML libraries such as Scikit-learn, TensorFlow, Torch, Lang Chain, or OpenAI API.\nStrong understanding of Generative AI, Large Language Models (LLMs), and their practical applications.\nHands-on experience in fine-tuning and deploying foundation models (e.g., OpenAI, Llama, Claude, Gemini, etc.).\nExperience with Vector Databases (e.g., FAISS, Chroma, Weaviate, Pinecone) for retrieval-augmented generation (RAG) applications.\nKnowledge of Autonomous AI Agents (e.g., AutoGPT, BabyAGI) and multi-agent orchestration frameworks.\nExperience working with SQL and NoSQL databases.\nFamiliarity with cloud platforms (AWS, Azure, or GCP) for AI/ML model deployment.\nStrong problem-solving and analytical thinking abilities.\nAbility to communicate complex AI concepts to technical and non-technical stakeholders.\nBonus:Experience in Automotive, Industrial, or Manufacturing AI applications (e.g., predictive maintenance, quality inspection, digital twins).\n\n\n\n\nAdditional Information:\nBachelor/Masters degree in Statistics/Economics/ Mathematics/ Computer Science or related disciplines with an excellent academic record /MBA from top-tier universities.\nExcellent Communication and Interpersonal Skills.\n\n\n\n\n\nAbout Our Company | Accenture\n\n\n\n\n\nQualification\n\n\n\nExperience:Minimum 1-3 years of relevant Data Science, Machine Learning or AI-related roles., Exposure to Industrial & Automotive Firms or Professional Services.\n\n\n\n\nEducational Qualification: Bachelor/Masters degree in Statistics/Economics/ Mathematics/ Computer Science or related disciplines with an excellent academic record or MBA from top-tier universities.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'machine learning', 'sql', 'tensorflow', 'data science', 'image processing', 'c++', 'scikit-learn', 'microsoft azure', 'artificial intelligence', 'nosql', 'deep learning', 'r', 'spark', 'gcp', 'computer vision', 'network analysis', 'hadoop', 'api', 'big data', 'aws', 'opencv']",2025-06-13 06:05:53
S&C Global Network - AI - CDP - Analyst,Accenture,8 - 10 years,Not Disclosed,['Gurugram'],"Job Title -\n\n\n\nS&C Global Network - AI - RTCDP - Consultant\n\n\n\nManagement Level:\n\n\n\n9-Team Lead/Consultant\n\n\n\nLocation:\n\n\n\nBengaluru\n\n\n\nMust-have skills:Web Analytics Tools\n\n\n\n\nGood to have skills:Ability to leverage design thinking, business process optimization, and stakeholder management skills.\n\n\n\nJob\n\n\nSummary:\n\nThis role involves driving strategic initiatives, managing business transformations, and leveraging industry expertise to create value-driven solutions.\n\n\n\n\nRoles & Responsibilities:\n\nProvide strategic advisory services, conduct market research, and develop data-driven recommendations to enhance business performance.\n\n\n\n\nProfessional & Technical\n\n\n\n\nSkills:\n\n\n- Relevant experience in the required domain.\n\n- Strong analytical, problem-solving, and communication skills.\n\n- Ability to work in a fast-paced, dynamic environment.\n\n\n\nTechnical\n\n\n\n\nSkills:\n\nAny CDP platforms experience e.g., Lytics CDP platform developer, or/and\nSegment CDP platform developer, or/and\nAdobe Experience Platform (Real time CDP) developer, or/and\nCustom CDP developer on any cloud\nGA4/GA360, or/and Adobe Analytics\nGoogle Tag Manager, and/or Adobe Launch, and/or any Tag Manager Tool\nGoogle Ads, DV360, Campaign Manager, Facebook Ads Manager, The Trading desk etc.\nDeep Cloud experiecne (GCP, AWS, Azure)\nAdvance level Python, SQL, Shell Scripting experience\nData Migration, DevOps, MLOps, Terraform Script programmer\n\n\n\nSoft\n\n\n\n\nSkills:\n\nStrong problem solving skills\nGood team player\nAttention to details\nGood communication skills\n\n\n\n\n\nAdditional Information:\n\n- Opportunity to work on innovative projects.\n\n- Career growth and leadership exposure.\n\n\n\nWHATS IN IT FOR YOU\n\nAs part of our Analytics practice, you will join a worldwide network of over 20k+ smart and driven colleagues experienced in leading AI/ML/Statistical tools, methods and applications. From data to analytics and insights to actions, our forward-thinking consultants provide analytically-informed, issue-based insights at scale to help our clients improve outcomes and achieve high performance.\n\n\n\nWhat you would do in this role\n\nA Consultant/Manager for Customer Data Platforms serves as the day-to-day marketing technology point of contact and helps our clients get value out of their investment into a Customer Data Platform (CDP) by developing a strategic roadmap focused on personalized activation. You will be working with a multidisciplinary team of Solution Architects, Data Engineers, Data Scientists, and Digital Marketers.\n\n\n\nKey Duties and Responsibilities:\nBe a platform expert in one or more leading CDP solutions. Developer level expertise on Lytics, Segment, Adobe Experience Platform, Amperity, Tealium, Treasure Data etc. Including custom build CDPs\nDeep developer level expertise for real time even tracking for web analytics e.g., Google Tag Manager, Adobe Launch etc.\nProvide deep domain expertise in our clients business and broad knowledge of digital marketing together with a Marketing Strategist industry\nDeep expert level knowledge of GA360/GA4, Adobe Analytics, Google Ads, DV360, Campaign Manager, Facebook Ads Manager, The Trading desk etc.\nAssess and audit the current state of a clients marketing technology stack (MarTech) including data infrastructure, ad platforms and data security policies together with a solutions architect.\nConduct stakeholder interviews and gather business requirements\nTranslate business requirements into BRDs, CDP customer analytics use cases, structure technical solution\nPrioritize CDP use cases together with the client.\nCreate a strategic CDP roadmap focused on data driven marketing activation.\nWork with the Solution Architect to strategize, architect, and document a scalable CDP implementation, tailored to the clients needs.\nProvide hands-on support and platform training for our clients.\nData processing, data engineer and data schema/models expertise for CDPs to work on data models, unification logic etc.\nWork with Business Analysts, Data Architects, Technical Architects, DBAs to achieve project objectives - delivery dates, quality objectives etc.\nBusiness intelligence expertise for insights, actionable recommendations.\nProject management expertise for sprint planning\n\n\n\n\n\nAbout Our Company | Accenture\n\nQualification\n\n\n\nExperience:\n\n\n\n8 to 10 Years\n\n\n\n\nEducational Qualification:\n\n\n\nB.Com",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['digital marketing', 'display video', 'google ads', 'campaign management', 'facebook ads manager', 'trading', 'python', 'adobe analytics', 'adobe', 'microsoft azure', 'sql', 'gcp', 'web analytics', 'devops', 'segmentation', 'web analytics tools', 'shell scripting', 'network analysis', 'aws', 'seo']",2025-06-13 06:05:55
Sr. Manager,Amazon,10 - 15 years,Not Disclosed,['Bengaluru'],"Gift Cards mission is to drive Amazon s flywheel by inspiring people around the globe to gift or transact with confidence. We are looking for Sr. Manager, Data science, to transform how millions of customers discover, purchase, and share the perfect gift through data science innovation at Amazon Gift Cards. This leader leads a multi-disciplinary team driving innovation and operational excellence.\n\nThis role is pivotal to our mission, combining strategic vision, hands-on leadership, and scientific expertise. You will manage a diverse team of data scientists, business intelligence and data engineers, working collaboratively to deliver high-impact scientific solutions and scalable systems to improve customer experience and drive business growth. The role focuses on identifying and solving ambiguous, high-value problems using state-of-the-art methodologies in machine learning (ML) as well as developing self-service data platforms for internal analytics.\n\n\nStrategic Leadership: Define and execute the scientific vision for Gift cards, aligning with the broader business goals. Drive cross-\norganizational initiatives, identifying opportunities to leverage data science and advanced analytics to solve critical business challenges. Partner with senior leaders to prioritize programs and align resources with strategic objectives.\nTeam Development: Hire, develop, and mentor a diverse team of scientists and engineers, fostering a culture of innovation, learning and excellence.\nScience and Innovation: Design and implement state-of-the-art ML models, analytics systems, and scalable architectures to support e-commerce. Explore novel scientific approaches and contribute to advancements in areas such as sales automation, product recommendation, pricing optimization, and customer segmentation. Stay abreast of industry developments to inspire and implement\nstate-of-the-art solutions, particularly in GenAI.\nStakeholder Engagement: Represent scientific and technical initiatives to senior leadership, articulating trade-offs and driving consensus. Collaborate across geographies and functions to increase penetration of Payment Products, and leverage data to recommend deprecations or new ways of operating. Together, improving customer experience and operational scalability.\nOperational Excellence: Develop the data foundation and metrics to enhance operational transparency and decision-making.\nManage day-to-day technical operations, ensuring quality and efficiency across data pipelines, tools, and models. Masters in Computer Science or a related field.\n10+ years experience in machine learning, AI, or a related field.\n7+ years of leading and managing science teams that deliver large impact.\nProven track record of leading and managing science teams that deliver large impact.\nExpert knowledge of machine learning algorithms and techniques.\nFamiliarity with large-scale data processing and storage systems.\nExcellent communication and interpersonal skills.\nAbility to work in a fast-paced, collaborative environment PhD in Computer Science or a related field.\nWorked with product teams for building vision/strategy for a space.\n2+ years of hands-on experience as a Senior Scientist",,,,"['Computer science', 'Stakeholder Engagement', 'customer segmentation', 'Technical operations', 'Penetration', 'Operational excellence', 'Machine learning', 'Data processing', 'Strategic leadership', 'Business intelligence']",2025-06-13 06:05:57
Senior - Internal Audit,KPMG India,2 - 5 years,Not Disclosed,['Bengaluru'],"Skills and Expertise: The consultant should have expertise in the following:\n1. Designing, developing, and deploying AI-driven solutions using Python.\n2. Developing and deploying Generative AI models for tasks such as text generation, image synthesis, or data augmentation.\n3. Designing workflows to fine-tune Large Language Models (LLMs) like GPT or BERT for specific use cases.\n4. Experience with Natural Language Processing (NLP) techniques and libraries such as spaCy, NLTK, or Hugging Face Transformers.\n5. Proficiency in computer vision techniques and libraries such as OpenCV, TensorFlow, or PyTorch.\n6. Implementing and optimizing AI algorithms for performance and scalability.\n7. Knowledge of reinforcement learning and its applications.\n8. Developing custom API endpoints for model deployment and real-time data processing. Ensuring API security and scalability for handling large volumes of requests efficiently.\n9. Integrating third-party APIs to enhance AI functionality (e.g., speech-to-text, language translation, or image recognition).\n10. Utilizing deep learning frameworks such as PyTorch or TensorFlow for building neural networks.\n11. Strong understanding of AI/ML concepts, including frameworks like scikit-learn and TensorFlow. Experience with Machine Learning libraries for advanced modeling.\n12. Experience with AI model deployment and serving using tools like Docker, Kubernetes, or cloud services (AWS, Azure, GCP).\n13. Collaborating with data scientists and analysts to integrate machine learning models into production.\n14. Hands-on experience with Python, knowledge of Data Analytics to proficiently write and optimize scripts.\n15. Writing complex SQL queries to extract, transform, and analyze data from relational/non-relational databases.\n16. Proficiency in version control tools such as Git.\n17. Knowledge of data visualization tools like Power BI for creating dashboards and reports (good to have).\n18. Exposure to ServiceNow for IT service management and automation (good to have).\n19. Familiarity with Microsoft Azure cloud services for AI and data processing (good to have).\n20. Good communication and presentation skills.\n.",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer vision', 'Automation', 'Manager Internal Audit', 'Version control', 'GCP', 'Neural networks', 'Machine learning', 'Data processing', 'Natural language processing', 'Python']",2025-06-13 06:05:58
Deloitte - Senior Consultant - Databricks,Deloitte,6 - 10 years,Not Disclosed,"['Pune', 'Bengaluru', 'Delhi / NCR']","What impact will you make?\nEvery day, your work will make an impact that matters, while you thrive in a dynamic culture of inclusion, collaboration and high performance. As the undisputed leader in professional services, Deloitte is where you will find unrivaled opportunities to succeed and realize your full potential\n\nThe Team\nDeloittes AI&D practice can help you uncover and unlock the value buried deep inside vast amounts of data. Our global network provides strategic guidance and implementation services to help companies manage data from disparate sources and convert it into accurate, actionable information that can support fact-driven decision-making and generate an insight-driven advantage. Our practice addresses the continuum of opportunities in business intelligence & visualization, data management, performance management and next-generation analytics and technologies, including big data, cloud, cognitive and machine learning.\n\nWork youll do\nLocation: Bangalore/Mumbai/Pune/Delhi/Chennai/Hyderabad/Kolkata\n\nRoles: Databricks Data Engineering Senior Consultant\nWe are seeking highly skilled Databricks Data Engineers to join our data modernization team. You will play a pivotal role in designing, developing, and maintaining robust data solutions on the Databricks platform. Your experience in data engineering, along with a deep understanding of Databricks, will be instrumental in building solutions to drive data-driven decision-making across a variety of customers.\nMandatory Skills: Databricks, Spark, Python / SQL\n\nResponsibilities\n•        Design, develop, and optimize data workflows and notebooks using Databricks to ingest, transform, and load data from various sources into the data lake.\n•        Build and maintain scalable and efficient data processing workflows using Spark (PySpark or Spark SQL) by following coding standards and best practices.\n•        Collaborate with technical and business stakeholders to understand data requirements and translate them into technical solutions.\n•        Develop data models and schemas to support reporting and analytics needs.\n•        Ensure data quality, integrity, and security by implementing appropriate checks and controls.\n•        Monitor and optimize data processing performance, identifying, and resolving bottlenecks.\n•        Stay up to date with the latest advancements in data engineering and Databricks technologies.\nQualifications\n•        Bachelors or masters degree in any field\n•        6-10 years of experience in designing, implementing, and maintaining data solutions on Databricks\n•        Experience with at least one of the popular cloud platforms – Azure, AWS or GCP\n•        Experience with ETL (Extract, Transform, Load) and ELT (Extract, Load, Transform) processes\n•        Knowledge of data warehousing and data modelling concepts\n•        Experience with Python or SQL\n•        Experience with Delta Lake\n•        Understanding of DevOps principles and practices\n•        Excellent problem-solving and troubleshooting skills\n•        Strong communication and teamwork skills\n\nYour role as a leader\nAt Deloitte India, we believe in the importance of leadership at all levels. We expect our people to embrace and live our purpose by challenging themselves to identify issues that are most important for our clients, our people, and for society and make an impact that matters.\nIn addition to living our purpose, Senior Consultant across our organization:\nDevelop high-performing people and teams through challenging and meaningful opportunities\nDeliver exceptional client service; maximize results and drive high performance from people while fostering collaboration across businesses and borders\nInfluence clients, teams, and individuals positively, leading by example and establishing confident relationships with increasingly senior people\nUnderstand key objectives for clients and Deloitte; align people to objectives and set priorities and direction.\nActs as a role model, embracing and living our purpose and values, and recognizing others for the impact they make\n\nHow you will grow\nAt Deloitte, our professional development plan focuses on helping people at every level of their career to identify and use their strengths to do their best work every day. From entry-level employees to senior leaders, we believe there is always room to learn. We offer opportunities to help build excellent skills in addition to hands-on experience in the global, fast-changing business world. From on-the-job learning experiences to formal development programs at Deloitte University, our professionals have a variety of opportunities to continue to grow throughout their career. Explore Deloitte University, The Leadership Centre.\n\nBenefits\nAt Deloitte, we know that great people make a great organization. We value our people and offer employees a broad range of benefits. Learn more about what working at Deloitte can mean for you.\n\nOur purpose\nDeloitte is led by a purpose: To make an impact that matters. Every day, Deloitte people are making a real impact in the places they live and work. We pride ourselves on doing not only what is good for clients, but also what is good for our people and the\nCommunities in which we live and work—always striving to be an organization that is held up as a role model of quality, integrity, and positive change. Learn more about Deloitte's impact on the world\n\nRecruiter tips\nWe want job seekers exploring opportunities at Deloitte to feel prepared and confident. To help you with your interview, we suggest that you do your research: know some background about the organization and the business area you are applying to. Check out recruiting tips from Deloitte professionals.",Industry Type: Management Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['Azure Databricks', 'Aws Databricks', 'Data Bricks', 'Python', 'SQL', 'Pyspark']",2025-06-13 06:06:00
"Associate Staff Engineer, Frontend React",Nagarro,5 - 7 years,Not Disclosed,['Bengaluru'],"We're Nagarro.\n\nWe are a Digital Product Engineering company that is scaling in a big way! We build products, services, and experiences that inspire, excite, and delight. We work at scale across all devices and digital mediums, and our people exist everywhere in the world (18000+ experts across 38 countries, to be exact). Our work culture is dynamic and non-hierarchical. We're looking for great new colleagues. That's where you come in!\n\nREQUIREMENTS:\nTotal Experience 5+ years.\nHands on working experience in front-end or full-stack development experience, with building production apps in React.js and Next.js.\nHands-on expertise writing unit/integration tests for React components (Jest, React Testing Library, etc.)\nSolid grasp of state-management patterns and libraries (Redux, React Context, Zustand, etc.).\nStrong understanding of RESTful APIs, asynchronous programming (Promises, async/await), and modern build tools (Webpack, Vite, or Turbopack).\nPractical experience with Git, pull-request workflows, and collaborative development tools (GitHub, GitLab, Bitbucket).\nAdvanced proficiency in JavaScript (ES6+) and TypeScript.\nProblem-solving mindset with the ability to tackle complex data engineering challenges. \nStrong communication and teamwork skills, with the ability to mentor and collaborate effectively.\n\nRESPONSIBILITIES:\nWriting and reviewing great quality code\nUnderstanding the clients business use cases and technical requirements and be able to convert them in to technical design which elegantly meets the requirements\nMapping decisions with requirements and be able to translate the same to developers.\nIdentifying different solutions and being able to narrow down the best option that meets the clients requirements.\nDefining guidelines and benchmarks for NFR considerations during project implementation\nWriting and reviewing design document explaining overall architecture, framework, and high-level design of the application for the developers.\nReviewing architecture and design on various aspects like extensibility, scalability, security, design patterns, user experience, NFRs, etc., and ensure that all relevant best practices are followed.\nDeveloping and designing the overall solution for defined functional and non-functional requirements; and defining technologies, patterns, and frameworks to materialize it\nUnderstanding and relating technology integration scenarios and applying these learnings in projects\nResolving issues that are raised during code/review, through exhaustive systematic analysis of the root cause, and being able to justify the decision taken.\nCarrying out POCs to make sure that suggested design/technologies meet the requirements.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Typescript', 'Javascript', 'React.Js']",2025-06-13 06:06:02
Business Analyst,SLK Software,5 - 10 years,Not Disclosed,['Bengaluru'],"purpose of this role is to work independently with customers to understand the customer business model, needs / requirements and translate them into software requirements document which would be used by IT Design & Development teams for solutioning, provide functional support & clarifications to QA and technical teams throughout the project life cycle, assist the Business teams during User Acceptance Testing and involve in post production support & verifications.\n  KEY RESPONSIBILITIES AND ACCOUNTABILITIES\nUnderstand Business needs and elicit and document requirements\nReview with stakeholders and achieve agreement on requirements\nAchieve consensus on business needs, usability, performance and feasibility.\nFor internal customers being supported\ni) Develop and maintain knowledge & learning.\nii) Proliferate offerings to sales team\niii) Advise QA and Development teams on troubleshooting analysis\nFollow standard documentation processes, focusing on objectives of the customer, development and QA.\nDevelop artefacts through case studies, white papers and lessons learnt.\nFeasibility analysis on prospective business\nSupport project planning and status reporting.\nSharing of knowledge and learning amongst peers.\nEDUCATION QUALIFICATION\nDegree B.E\nCertifications (if any) Optional\nMINIMUM EXPERIENCE REQUIRED\nOverall (in years) 5+ years\nRelevant (in years) 3+ years\nDOMAIN/ FUNCTIONAL SKILLS\nAnalytical skills, problem solving skills\nStrong BFSI domain knowledge\nStakeholder management, communication skills, learning oriented, consensus building\nBA tools and techniques",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Post production', 'Automation', 'Business Analyst', 'Production support', 'Project planning', 'Investment management', 'Troubleshooting', 'Stakeholder management', 'User acceptance testing', 'Analytics']",2025-06-13 06:06:04
Programmer/Analyst 5,Lam Research,10 - 17 years,Not Disclosed,['Bengaluru'],"The Group You ll Be A Part Of\nLAM HR-Applications team is looking for a passionate, engaging Sr HR Applications Architect to join our growing team. This role will perform Technology evaluation, Identification, Solution Design, Execute the design for entire stack of HR-Applications echo-system and perform Production Support.\nThe Global Information Systems Group is dedicated to the success of Lam through providing best-in-class and innovative information system solutions and services. Together, we support users globally with data, information, and systems to achieve their business objectives.\nThe Impact You ll Make\nDesigns, develops, modifies, debugs and evaluates programs for functional areas, including but not limited to finance, human resources, manufacturing and marketing. Analyzes existing programs or formulates logic for new systems, devises logic procedures, prepares flowcharting, performs coding and tests/debugs programs. Develops conversion and system implementation plans. Prepares and obtains approval of system and programming documentation. Recommends changes in development, maintenance and system standards. Trains users in conversion and implementation of system. May be internal or external, client-focused, working in conjunction with Professional Services and outsourcing functions. May include company-wide, web-enabled solutions.\nWhat You ll Do\nLead design and implementation of the HR systems of the organization across HR technologies\nInterface with business stakeholders, assess feasibility of the requirements and guide the Technology Leads and Implementation teams to align the solution development\nFront-run the transformation and migration initiative in HR Applications COE ensuring a scalable solution to accommodate future enhancements and adoption to all BU s of Lam\nExplore new technologies and practices, be a part of the core team building an HR COE and define the standards and best practices\nAct as a SPOC/L3 for the current product support related activities and the\nHR-Echo System\nCross- training teams on knowledge transfer across business functions\nWho We re Looking For\nExcellent grasp of HR systems (both SAAS & On-Premises) technical and functional\nProven experience leading System Transformation, Integrations, Data Migrations, Implementations, Assessments and Process re-engineering.\n14+ years of experience as an HR Applications Architect\nVery strong communication and collaboration skills\nFlexible to travel and work hours.\nPreferred Qualifications",Industry Type: Electronic Components / Semiconductors,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Architect', 'Production support', 'Coding', 'Flex', 'Manager Technology', 'Process re-engineering', 'HR', 'Outsourcing', 'Product support', 'Team building']",2025-06-13 06:06:05
Programmer/Analyst 4,Lam Research,8 - 13 years,Not Disclosed,['Bengaluru'],"The Group You ll Be A Part Of\nThe Global Information Systems Group is dedicated to the success of Lam through providing best-in-class and innovative information system solutions and services. Together, we support users globally with data, information, and systems to achieve their business objectives.\nThe Impact You ll Make\nAbout the Role: We are looking for an experienced and highly motivated Siemens Teamcenter PLM Development Lead to join our team. The ideal candidate will have extensive experience with Siemens Teamcenter and a strong background in PLM development and integration. This role involves the development and implementation of Teamcenter solutions, working closely with cross-functional teams to ensure seamless integration and optimal performance.\nWhat You ll Do\nKey Responsibilities:\nLead the development and implementation of Siemens Teamcenter PLM solutions.\nCollaborate with cross-functional teams to integrate Teamcenter with other enterprise systems such as ERP, CAD, and MES.\nDesign and develop customizations and configurations to meet business requirements.\nProvide technical leadership and guidance to the development team.\nEnsure the quality and performance of Teamcenter solutions through rigorous testing and validation.\nDevelop and maintain documentation for Teamcenter configurations, customizations, and processes.\nConduct training sessions for end-users and provide ongoing support to ensure effective use of Teamcenter.\nStay updated with the latest Teamcenter features and best practices to continuously improve system performance and user experience.\nWho We re Looking For\nMinimum of 8 years of related experience with a Bachelor s degree; or 6 years and a Master s degree; or a PhD with 3 years experience; or equivalent experience.\nPreferred Qualifications\nQualifications:\nBachelors degree in Computer Science, Information Systems, Engineering, or a related field.\nMinimum of 7 years of experience working with Siemens Teamcenter.\nStrong understanding of PLM concepts and business processes.\nExperience with Teamcenter configuration, customization, and integration.\nProficiency in programming languages such as Java, C++, or Python.\nExcellent problem-solving skills and attention to detail.\nStrong communication and interpersonal skills.\nAbility to work independently and as part of a team.\nExperience in secure SDLC development practices\nOur Commitment\nWe believe it is important for every person to feel valued, included, and empowered to achieve their full potential. By bringing unique individuals and viewpoints together, we achieve extraordinary results.\nLam Research (""Lam"" or the ""Company"") is an equal opportunity employer. Lam is committed to and reaffirms support of equal opportunity in employment and non-discrimination in employment policies, practices and procedures on the basis of race, religious creed, color, national origin, ancestry, physical disability, mental disability, medical condition, genetic information, marital status, sex (including pregnancy, childbirth and related medical conditions), gender, gender identity, gender expression, age, sexual orientation, or military and veteran status or any other category protected by applicable federal, state, or local laws. It is the Companys intention to comply with all applicable laws and regulations. Company policy prohibits unlawful discrimination against applicants or employees.\nLam offers a variety of work location models based on the needs of each role. Our hybrid roles combine the benefits of on-site collaboration with colleagues and the flexibility to work remotely and fall into two categories - On-site Flex and Virtual Flex. On-site Flex you ll work 3+ days per week on-site at a Lam or customer/supplier location, with the opportunity to work remotely for the balance of the week. Virtual Flex you ll work 1-2 days per week on-site at a Lam or customer/supplier location, and remotely the rest of the time.",Industry Type: Electronic Components / Semiconductors,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['PLM', 'Computer science', 'C++', 'ERP', 'Interpersonal skills', 'CAD', 'Flex', 'Siemens', 'SDLC', 'Python']",2025-06-13 06:06:07
Software Engineering Senior Analyst,ManipalCigna Health Insurance,2 - 6 years,Not Disclosed,['Hyderabad'],"Software Engineering Senior Analyst - HIH - Evernorth\nAbout Evernorth:\nEvernorth Health Services, a division of The Cigna Group (NYSE: CI), creates pharmacy, care, and benefits solutions to improve health and increase vitality. We relentlessly innovate to make the prediction, prevention, and treatment of illness and disease more accessible to millions of people.\n\nPosition Summary:\nCigna, a leading Health Services company, is looking for Data Engineers in our Engineering Enablement Office (EEO) organization. The Data Engineer is responsible for the delivery of test data business need starting from understanding the data requirements to manufacturing test data for a work initiative. This role requires you to be fluent in some of the critical technologies with proficiency in others and have a hunger to learn on the job and add value to the business. Critical attributes of being a Data Engineer, among others, is Ownership Accountability. In addition to Delivery, the Data Engineer should have an automation first and continuous improvement mindset.\nDescription Responsibilities:\nThe Data Engineer will be responsible for determining the best approach to create test data. It includes account, enrolment, claims, and provider setup applications. This team member must have the ability to engage in test data requirements analysis with Integration Solution Manager (ISM) and Quality Engineer (QE) teams. This team player will also be responsible to collaborate with ISM QE team to explore opportunities to automate test data setup/mining processes.\nResponsible for test data creation (data manufacturing)\nUnderstands various back end and front-end architecture components required for job executions.\nDetermines priorities for test data creation , validation triage.\nUnderstand test data mapping with test scenarios.\nManage test data catalog and self-service mining tools.\nManage data cleanup activities, renewal identification and planning.\nDeveloping subject matter expertise and building knowledge of supported applications\nAnalyzing and communicating test data challenges and risks effectively to identify practical solutions.\nCompleting work governed by best practices, standards and processes and continuously learning about Agile to effectively integrate best practices into delivery activities.\nBe fluent in particular areas and have proficiency in many areas.\nHave a passion to learn.\nTake ownership and accountability.\nUnderstands when to automate and when not to.\nHave a desire to simplify.\nBe entrepreneurial / business minded.\nHave a quality mindset, not just code quality but also to ensure ongoing data quality by monitoring data to identify problems before they have business impact.\nTake risks and champion new ideas.\nExperience Required:\n4+ years being part of Agile teams\n2+ years of experience in a Test data account, enrollment, claims, and provider setup in healthcare domain\n2+ years of experience in Healthcare\nExperience Desired:\n3-6 years of IT experience in similar role\nAbility to analyse, interpret, and organize large amounts of data.\nProblem-solving, and analytical skills\nTime Management skills\nCigna application flow and business knowledge\nEducation and Training Required:\nKnowledge and/or experience with Health care information domains is a plus.\nComputer science - Good to have\nPrimary Skills:\nAdvance SQL knowledge\nJira (Sprint/Kanban)\nConfluence for documentation\nProgramming Logic and Algorithms\nVBA (Excel Macros)\nBuild and maintain integrations with data sources and APIs\nStrong knowledge of database systems, data modeling techniques, and SQL proficiency\nAutomation Skills\nEfficient at least in one Programming language (Python/Java) or Scripting language (JavaScript)\nExpertise at least in one Automation Framework [Robot Framework /Behave / Pytest Framework/ BDD Cucumber / TestNG / Cypress\nGood Exposure on integrating the Test Automation Reporting with CI Tools (Jenkins/Azure)\nHands on experience on the platforms [ GUI Automation Non-GUI Automation [ DB / API / MQ ( Kafka)\nTableau data catalogue and Dashboard (Good to have)\nProficiency with ETL tools commonly used in data engineering (like Databricks)\nAutomation in Cloud (experience a plus)\nAI Machine Learning (experience a plus)\nAdditional Skills:\nExcellent troubleshooting skills\nStrong communication skills\nWork in an agile CI/CD environment (Jenkins experience a plus)\nFamiliarity with cloud platforms and services (like AWS, Azure)",Industry Type: Insurance,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Automation', 'Data modeling', 'Javascript', 'Agile', 'Healthcare', 'Troubleshooting', 'Macros', 'Monitoring', 'SQL', 'Python']",2025-06-13 06:06:09
Senior ETL Engineer/Consultant Specialist,Hsbc,3 - 6 years,Not Disclosed,['Hyderabad'],"Some careers shine brighter than others.\nIf you re looking for a career that will help you stand out, join HSBC and fulfil your potential. Whether you want a career that could take you to the top, or simply take you in an exciting new direction, HSBC offers opportunities, support and rewards that will take you further.\nHSBC is one of the largest banking and financial services organisations in the world, with operations in 64 countries and territories. We aim to be where the growth is, enabling businesses to thrive and economies to prosper, and, ultimately, helping people to fulfil their hopes and realise their ambitions.\nWe are currently seeking an experienced professional to join our team in the role of Consultant Specialist\nIn this role you will be\nDesign and Develop ETL Processes: Lead the design and implementation of ETL processes using all kinds of batch/streaming tools to extract, transform, and load data from various sources into GCP.\nCollaborate with stakeholders to gather requirements and ensure that ETL solutions meet business needs.\nData Pipeline Optimization: Optimize data pipelines for performance, scalability, and reliability, ensuring efficient data processing workflows.\nMonitor and troubleshoot ETL processes, proactively addressing issues and bottlenecks.\nData Integration and Management: I ntegrate data from diverse sources, including databases, APIs, and flat files, ensuring data quality and consistency.\nManage and maintain data storage solutions in GCP (e. g. , BigQuery, Cloud Storage) to support analytics and reporting.\nGCP Dataflow Development: Write Apache Beam based Dataflow Job for data extraction, transformation, and analysis, ensuring optimal performance and accuracy.\nCollaborate with data analysts and data scientists to prepare data for analysis and reporting.\nAutomation and Monitoring: Implement automation for ETL workflows using tools like Apache Airflow or Cloud Composer, enhancing efficiency and reducing manual intervention.\nSet up monitoring and alerting mechanisms to ensure the health of data pipelines and compliance with SLAs.\nData Governance and Security: Apply best practices for data governance, ensuring compliance with industry regulations (e. g. , GDPR, HIPAA) and internal policies.\nCollaborate with security teams to implement data protection measures and address vulnerabilities.\nDocumentation and Knowledge Sharing: Document ETL processes, data models, and architecture to facilitate knowledge sharing and onboarding of new team members.\nConduct training sessions and workshops to share expertise and promote best practices within the team.\n\n\n\n\n\n\n\n\n\n\n\nRequirements\n\n\n\nTo be successful in this role, you should meet the following requirements:\nEducation: Bachelor s degree in Computer Science, Information Systems, or a related field.\nExperience: Minimum of 5 years of industry experience in data engineering or ETL development, with a strong focus on Data Stage and GCP.\nProven experience in designing and managing ETL solutions, including data modeling, data warehousing, and SQL development.\nTechnical Skills: Strong knowledge of GCP services (e. g. , BigQuery, Dataflow, Cloud Storage, Pub/Sub) and their application in data engineering.\nExperience of cloud-based solutions, especially in GCP, cloud certified candidate is preferred.\nExperience and knowledge of Bigdata data processing in batch mode and streaming mode, proficient in Bigdata eco systems, e. g. Hadoop, HBase, Hive, MapReduce, Kafka, Flink, Spark, etc.\nFamiliarity with Java Python for data manipulation on Cloud/Bigdata platform.\nAnalytical Skills: Strong problem-solving skills with a keen attention to detail.\nAbility to analyze complex data sets and derive meaningful insights.\nBenefits: Competitive salary and comprehensive benefits package.\nOpportunity to work in a dynamic and collaborative environment on cutting-edge data projects.\nProfessional development opportunities to enhance your skills and advance your career.\nIf you are a passionate data engineer with expertise in ETL processes and a desire to make a significant impact within our organization, we encourage you to apply for this exciting opportunity!",Industry Type: Consumer Electronics & Appliances,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Automation', 'Data modeling', 'HIPAA', 'Data quality', 'Apache', 'Monitoring', 'Analytics', 'Financial services', 'Python']",2025-06-13 06:06:10
"Senior Staff Engineer, Mobile Flutter",Nagarro,10 - 15 years,Not Disclosed,[],"Total Experience 10+ years.\nStrong working experience in Dart and Flutter.\nSolid understanding of Mobile App Architecture (MVVM, BLoC, Provider, GetX).\nExperience with local databases like Sqflite or alternatives.\nHands-on experience in unit testing and test automation for Flutter apps.\nProven experience in building and deploying apps to the App Store and Google Play Store.\nFamiliarity with Git, GitHub/GitLab, CI/CD tools (eg, Jenkins, Bitrise, GitHub Actions).\nDeep knowledge of mobile app lifecycle, design principles, and clean architecture patterns (MVVM, MVC, etc)\nExpertise in mobile app performance optimization and security best practices.\nExperience in API integration , RESTful services, and handling JSON data.\nProficient in Version Control Systems like Git.\nProblem-solving mindset with the ability to tackle complex data engineering challenges.\nStrong communication and teamwork skills, with the ability to mentor and collaborate effectively.\nRESPONSIBILITIES:\nWriting and reviewing great quality code\nUnderstanding the clients business use cases and technical requirements and be able to convert them into technical design which elegantly meets the requirements\nMapping decisions with requirements and be able to translate the same to developers.\nIdentifying different solutions and being able to narrow down the best option that meets the clients requirements.\nDefining guidelines and benchmarks for NFR considerations during project implementation\nWriting and reviewing design document explaining overall architecture, framework, and high-level design of the application for the developers.\nReviewing architecture and design on various aspects like extensibility, scalability, security, design patterns, user experience, NFRs, etc, and ensure that all relevant best practices are followe'd.\nDeveloping and designing the overall solution for defined functional and non-functional requirements; and defining technologies, patterns, and frameworks to materialize it\nUnderstanding and relating technology integration scenarios and applying these learnings in projects\nResolving issues that are raised during code/review, through exhaustive systematic analysis of the root cause, and being able to justify the decision taken.\nCarrying out POCs to make sure that suggested design/technologies meet the requirements\n\n\nBachelor s or master s degree in computer science, Information Technology, or a related field.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'github', 'Version control', 'GIT', 'MVVM', 'JSON', 'MVC', 'Unit testing', 'High level design', 'Information technology']",2025-06-13 06:06:12
"Senior Staff Engineer, Frontend React",Nagarro,10 - 15 years,Not Disclosed,[],"Total Experience 10+ years.\nHands on working experience in front-end or full-stack development experience, with building production apps in React.js and Next.js.\nHands-on expertise writing unit/integration tests for React components (Jest, React Testing Library, etc)\nSolid grasp of state-management patterns and libraries (Redux, React Context, Zustand, etc).\nStrong understanding of RESTful APIs, asynchronous programming (Promises, async/await), and modern build tools (Webpack, Vite, or Turbopack).\nPractical experience with Git, pull-request workflows, and collaborative development tools (GitHub, GitLab, Bitbucket).\nAdvanced proficiency in JavaScript (ES6+) and TypeScript.\nProblem-solving mindset with the ability to tackle complex data engineering challenges.\nStrong communication and teamwork skills, with the ability to mentor and collaborate effectively.\nRESPONSIBILITIES:\nWriting and reviewing great quality code\nUnderstanding the clients business use cases and technical requirements and be able to convert them in to technical design which elegantly meets the requirements\nMapping decisions with requirements and be able to translate the same to developers.\nIdentifying different solutions and being able to narrow down the best option that meets the clients requirements.\nDefining guidelines and benchmarks for NFR considerations during project implementation\nWriting and reviewing design document explaining overall architecture, framework, and high-level design of the application for the developers.\nReviewing architecture and design on various aspects like extensibility, scalability, security, design patterns, user experience, NFRs, etc, and ensure that all relevant best practices are followe'd.\nDeveloping and designing the overall solution for defined functional and non-functional requirements; and defining technologies, patterns, and frameworks to materialize it\nUnderstanding and relating technology integration scenarios and applying these learnings in projects\nResolving issues that are raised during code/review, through exhaustive systematic analysis of the root cause, and being able to justify the decision taken.\nCarrying out POCs to make sure that suggested design/technologies meet the requirements.\n\n\nBachelor s or master s degree in computer science, Information Technology, or a related field.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'github', 'Usage', 'GIT', 'Project implementation', 'Architecture', 'Scalability', 'Technical design', 'High level design', 'Information technology']",2025-06-13 06:06:14
"Senior Staff Engineer, Mobile - Flutter",Nagarro,10 - 15 years,Not Disclosed,[],"Total Experience 10+ years.\nStrong working experience in Dart and Flutter.\nSolid understanding of Mobile App Architecture (MVVM, BLoC, Provider, GetX).\nExperience with local databases like Sqflite or alternatives.\nHands-on experience in unit testing and test automation for Flutter apps.\nProven experience in building and deploying apps to the App Store and Google Play Store.\nFamiliarity with Git, GitHub/GitLab, CI/CD tools (eg, Jenkins, Bitrise, GitHub Actions).\nDeep knowledge of mobile app lifecycle, design principles, and clean architecture patterns (MVVM, MVC, etc)\nExpertise in mobile app performance optimization and security best practices.\nExperience in API integration , RESTful services, and handling JSON data.\nProficient in Version Control Systems like Git.\nProblem-solving mindset with the ability to tackle complex data engineering challenges.\nStrong communication and teamwork skills, with the ability to mentor and collaborate effectively.\nRESPONSIBILITIES:\nWriting and reviewing great quality code\nUnderstanding the clients business use cases and technical requirements and be able to convert them into technical design which elegantly meets the requirements\nMapping decisions with requirements and be able to translate the same to developers.\nIdentifying different solutions and being able to narrow down the best option that meets the clients requirements.\nDefining guidelines and benchmarks for NFR considerations during project implementation\nWriting and reviewing design document explaining overall architecture, framework, and high-level design of the application for the developers.\nReviewing architecture and design on various aspects like extensibility, scalability, security, design patterns, user experience, NFRs, etc, and ensure that all relevant best practices are followe'd.\nDeveloping and designing the overall solution for defined functional and non-functional requirements; and defining technologies, patterns, and frameworks to materialize it\nUnderstanding and relating technology integration scenarios and applying these learnings in projects\nResolving issues that are raised during code/review, through exhaustive systematic analysis of the root cause, and being able to justify the decision taken.\nCarrying out POCs to make sure that suggested design/technologies meet the requirements\n\n\nBachelor s or master s degree in computer science, Information Technology, or a related field.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'github', 'Version control', 'GIT', 'MVVM', 'JSON', 'MVC', 'Unit testing', 'High level design', 'Information technology']",2025-06-13 06:06:16
Application Lead,Accenture,5 - 10 years,Not Disclosed,['Bengaluru'],"Project Role :Application Lead\n\n\n\n\n\nProject Role Description :Lead the effort to design, build and configure applications, acting as the primary point of contact.\n\n\n\nMust have skills :PySpark\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification : Engineering graduate preferably Computer Science graduate 15 years of full time education\n\n\nSummary:As an Application Lead, you will be responsible for leading the effort to design, build, and configure applications, acting as the primary point of contact. Your typical day will involve working with PySpark and collaborating with cross-functional teams to deliver high-quality solutions.\nRoles & Responsibilities:- Lead the design, development, and deployment of PySpark-based applications, ensuring high-quality solutions are delivered on time and within budget.- Collaborate with cross-functional teams, including business analysts, data scientists, and software developers, to ensure that applications meet business requirements and are scalable and maintainable.- Act as the primary point of contact for all application-related issues, providing technical guidance and support to team members and stakeholders.- Ensure that applications are designed and developed in accordance with industry best practices, including coding standards, testing methodologies, and deployment processes.- Stay up-to-date with the latest trends and technologies in PySpark and related fields, and apply this knowledge to improve the quality and efficiency of application development.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nStrong experience in PySpark.- Good To Have\n\n\n\n\nSkills:\nExperience with other big data technologies such as Hadoop, Hive, and Spark.- Solid understanding of software development principles, including object-oriented programming, design patterns, and agile methodologies.- Experience with database technologies such as SQL and NoSQL.- Experience with cloud platforms such as AWS or Azure.- Strong problem-solving and analytical skills, with the ability to troubleshoot complex issues and provide effective solutions.\nAdditional Information:- The candidate should have a minimum of 5 years of experience in PySpark.- The ideal candidate will possess a strong educational background in computer science or a related field, along with a proven track record of delivering high-quality software solutions.- This position is based at our Bangalore, Hyderabad, Chennai and Pune Offices.- Mandatory office (RTO) for 2- 3 days and have to work on 2 shifts (Shift A- 10:00am to 8:00pm IST and Shift B - 12:30pm to 10:30 pm IST)\n\nQualification\n\nEngineering graduate preferably Computer Science graduate 15 years of full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['hive', 'software development', 'pyspark', 'spark', 'hadoop', 'visualforce', 'sfdc', 'ado.net', 'microsoft azure', 'triggers', 'sql', 'nosql', 'application development', 'apex', 'salesforce', 'sales force development', 'salesforce crm', 'data loader', 'design patterns', 'agile', 'aws', 'agile methodology']",2025-06-13 06:06:18
Application Lead,Accenture,15 - 20 years,Not Disclosed,['Bengaluru'],"Project Role :Application Lead\n\n\n\n\n\nProject Role Description :Lead the effort to design, build and configure applications, acting as the primary point of contact.\n\n\n\nMust have skills :Palantir Foundry\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\nProject Role :Lead Data Engineer\n\nProject Role Description :Design, build and enhance applications to meet business process and requirements in Palantir foundry.Work experience :Minimum 6 years\nMust have Skills :Palantir Foundry , PySpark, TypeScript (for customizing Workshop Forms & UI)Good to Have Skills :Experience in Pyspark, python and SQLKnowledge on Big Data tools & TechnologiesOrganizational and project management experience.Job & Key Responsibilities :Responsible for designing , developing, testing, and supporting data pipelines and applications on Palantir foundry.Configure and customize Workshop to design and implement workflows and ontologies.Configure and customize Workshop applications, including designing Forms, Workflows, and Ontology-based interactions.Write TypeScript to create dynamic and interactive Forms in Workshop for user-driven data entry and validation.Collaborate with data engineers and stakeholders to ensure successful deployment and operation of Palantir foundry applications.Work with stakeholders including the product owner, data, and design teams to assist with data-related technical issues and understand the requirements and design the data pipeline.Work independently, troubleshoot issues and optimize performance.Communicate design processes, ideas, and solutions clearly and effectively to team and client. Assist junior team members in improving efficiency and productivity.\nTechnical Experience :Proficiency in PySpark, Python and Sql with demonstrable ability to write & optimize SQL and spark jobs.Hands on experience on Palantir foundry related services like Data Connection, Code repository, Contour , Data lineage & Health checks.Good to have working experience with workshop , ontology , slate.Hands-on experience in data engineering and building data pipelines (Code/No Code) for ELT/ETL data migration, data refinement and data quality checks on Palantir Foundry.Experience in TypeScript to create and customize Forms in Workshop, including form validation, user interactions, and data binding with Ontology.Experience in ingesting data from different external source systems using data connections and sync.Good Knowledge on Spark Architecture and hands on experience on performance tuning & code optimization.Proficient in managing both structured and unstructured data, with expertise in handling various file formats such as CSV, JSON, Parquet, and ORC.Experience in developing and managing scalable architecture & managing large data sets.Good understanding of data loading mechanism and adeptly implement strategies for capturing CDC.Nice to have test driven development and CI/CD workflows.Experience in version control software such as Git and working with major hosting services (e. g. Azure DevOps, GitHub, Bitbucket, Gitlab).Implementing code best practices involves adhering to guidelines that enhance code readability, maintainability, and overall quality.\nEducational Qualification:15 years of full-term education\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['pyspark', 'sql', 'spark', 'typescript', 'python', 'hive', 'continuous integration', 'palantir', 'csv', 'data migration', 'bitbucket', 'azure devops', 'parquet', 'git', 'java', 'devops', 'json', 'hadoop', 'big data', 'orc', 'github', 'software testing', 'performance tuning', 'data quality', 'gitlab', 'sqoop', 'sql knowledge']",2025-06-13 06:06:20
Application Lead,Accenture,15 - 20 years,Not Disclosed,['Indore'],"Project Role :Application Lead\n\n\n\n\n\nProject Role Description :Lead the effort to design, build and configure applications, acting as the primary point of contact.\n\n\n\nMust have skills :AWS Glue\n\n\n\n\nGood to have skills :Data Engineering, PySpark, AWS BigDataMinimum\n\n\n\n12 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an Application Lead, you will lead the effort to design, build, and configure applications, acting as the primary point of contact. Your day will involve overseeing the application development process and ensuring successful project delivery.\nRoles & Responsibilities:- Expected to be an SME- Collaborate and manage the team to perform- Responsible for team decisions- Engage with multiple teams and contribute on key decisions- Expected to provide solutions to problems that apply across multiple teams- Lead the application development process- Coordinate with stakeholders to gather requirements- Ensure timely project delivery\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in AWS Glue, Data Engineering, PySpark, AWS BigData- Strong understanding of cloud computing principles- Experience in designing and implementing data pipelines- Knowledge of ETL processes and data transformation- Familiarity with data warehousing concepts\nAdditional Information:- The candidate should have a minimum of 12 years of experience in AWS Glue- This position is based at our Bengaluru office- A 15 years full-time education is required\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['pyspark', 'aws big data', 'cloud computing', 'data engineering', 'aws glue', 'hive', 'scala', 'amazon redshift', 'data warehousing', 'sql', 'plsql', 'java', 'spark', 'hadoop', 'etl', 'big data', 'python', 'oracle', 'big data analytics', 'application development', 'project delivery', 'athena', 'sqoop', 'aws', 'unix']",2025-06-13 06:06:21
Lead Assistant Vice President - Business Consulting,Hsbc,14 - 16 years,Not Disclosed,['Bengaluru'],"Some careers have more impact than others.\nHSBC is one of the largest banking and financial services organizations in the world, with operations in 62 countries and territories. We aim to be where the growth is, enabling businesses to thrive and economies to prosper, and, ultimately, helping people to fulfil their hopes and realize their ambitions.\nWe are currently seeking an experienced professional to join our team in the role of Lead Assistant Vice President - Business Consulting\nPrincipal responsibilities\nAbility to convert business problem to an analytical problem and then finding pertinent solutions\nOverall business understanding of BFSI domain\nProviding high-quality analysis and recommendations to business problems.\nEfficient project management and delivery\nAbility to conceptualize data driven solutions for the business problem at hand for multiple businesses/region to facilitate efficient decision making\nFocus on driving efficiency gains and enhancement of processes.\nUse of data to improve customer outcomes through the provision of insight and challenge\nAlso drive business benefit through self-initiated projects\nThe role may require one to liaise with different stakeholders business teams, business analysts, product owners, data engineers, platform developers etc.\nThe person should demonstrate leadership qualities in terms of bringing the junior members up the learning curve and handholding them whenever necessary\nWorking with an Agile mindset, using regular project management tools like Jira, Confluence, GitHub etc. to share updates and remaining on top of the work\nComplete adherence to overall Management of Risk/Internal Controls/Compliance for self and the team\nRequirements\nMaster s degree from reputed university in Computer Science, Information Technology or any other quantitative fields\nExperience in managing significant data volume with in-depth knowledge of relational database management along with Consulting experience in the data analytics domain.\nExcellent Project Management skill is required.\nThorough knowledge in SQL and Python must. Having knowledge/experience in Pyspark would be preferred. Knowledge of SAS is a plus\nKnowledge of Big Data is required.\nKnowledge in Google Cloud Platform or any On-Prem and corresponding tooling solutions would be preferred\nAdvanced knowledge of MS Excel including macro based excel sheets.\nStrong analytical skills with business analysis experience or equivalent.\nKnowledge and understanding of financial-services/ banking-operations preferred.\nAdvanced capability in translating complex and/or technical work and insights into straightforward business language.\nExcellent interpersonal skills with highly developed capacity to constructively challenge and influence management.\nProven problem solving skills with ability to consider alternative and innovative solutions\nBesides strong written and verbal communication skills the incumbent is also expected to have strong presentation skills\nPersonal data held by the Bank relating to employment applications will be used in accordance with our Privacy Statement, which is available on our website.",Industry Type: Consumer Electronics & Appliances,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'SAS', 'Business analysis', 'Project management', 'Consulting', 'Agile', 'Information technology', 'Financial services', 'SQL', 'Python']",2025-06-13 06:06:23
Starburst Engineer,Luxoft,0 - 4 years,Not Disclosed,['Pune'],"Project description\nYou will be working in a global team that manages and performs a global technical control.\nYou'll be joining Assets Management team which is looking after asset management data foundation and operates a set of in-house developed tooling.\nAs an IT engineer you'll play an important role in ensuring the development methodology is followed, and lead technical design discussions with the architects. Our culture centers around partnership with our businesses, transparency, accountability and empowerment, and passion for the future.\n\nResponsibilities\n\nDesign, develop, and maintain scalable data solutions using Starburst.\n\nCollaborate with cross-functional teams to integrate Starburst with existing data sources and tools.\n\nOptimize query performance and ensure data security and compliance.\n\nImplement monitoring and alerting systems for data platform health.\n\nStay updated with the latest developments in data engineering and analytics.\n\nSkills\nMust have\n\nBachelor's degree or Masters in a related technical field; or equivalent related professional experience.\n\nPrior experience as a Software Engineer applying new engineering principles to improve existing systems including leading complex, well defined projects.\n\nStrong knowledge of Big-Data Languages including:\n\nSQL\n\nHive\n\nSpark/Pyspark\n\nPresto\n\nPython\n\nStrong knowledge of Big-Data Platforms, such as:o The Apache Hadoop ecosystemo AWS EMRo Qubole or Trino/Starburst\n\nGood knowledge and experience in cloud platforms such as AWS, GCP, or Azure.\n\nContinuous learner with the ability to apply previous experience and knowledge to quickly master new technologies.\n\nDemonstrates the ability to select among technology available to implement and solve for need.\n\nAble to understand and design moderately complex systems.\n\nUnderstanding of testing and monitoring tools.\n\nAbility to test, debug, fix issues within established SLAs.\n\nExperience with data visualization tools (e.g., Tableau, Power BI).\n\nUnderstanding of data governance and compliance standards.\n\nNice to have\n\nData Architecture & EngineeringDesign and implement efficient and scalable data warehousing solutions using Azure Databricks and Microsoft Fabric.\n\nBusiness Intelligence & Data VisualizationCreate insightful Power BI dashboards to help drive business decisions.\n\nOther\n\nLanguages\n\nEnglishC1 Advanced\n\nSeniority\n\nSenior",Industry Type: Legal,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['hive', 'python', 'pyspark', 'sql', 'spark', 'azure databricks', 'microsoft azure', 'data warehousing', 'power bi', 'data architecture', 'business intelligence', 'data engineering', 'dashboards', 'tableau', 'apache', 'gcp', 'debugging', 'hadoop', 'data visualization', 'aws', 'engineering design', 'presto']",2025-06-13 06:06:25
Business Analyst,Accenture,15 - 20 years,Not Disclosed,['Mumbai'],"Project Role :Business Analyst\n\n\n\n\n\nProject Role Description :Analyze an organization and design its processes and systems, assessing the business model and its integration with technology. Assess current state, identify customer requirements, and define the future state and/or business solution. Research, gather and synthesize information.\n\n\n\nMust have skills :Business Analysis, Cucumber (Software)\n\n\n\n\nGood to have skills : Hands-on Exp. on SQL , . Jira (XRAY) and ConfluenceMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Business Analyst, you will analyze an organization and design its processes and systems, assessing the business model and its integration with technology. You will assess the current state, identify customer requirements, and define the future state and/or business solution. Research, gather, and synthesize information to drive business decisions.\nRoles & Responsibilities:- Expected to be an SME, collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Lead requirement gathering sessions with stakeholders.- Create detailed business requirements documentation.- Conduct gap analysis to identify areas for process improvement.- Facilitate communication between business and technology teams.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Business Analysis, Data Analysis & Interpretation, Scrum.- Strong understanding of project management methodologies.- Experience in process mapping and modeling.- Excellent communication and interpersonal skills.- Ability to prioritize and manage multiple tasks simultaneously.- Hands-on experience in SQL- Strong experience using Jira and Confluence.- Strong analytic skills.- Knowledge of all phases of IT software development and implementation life cycle.- Capable to effectively interact with technical team.- Team spirit - Like to explain and share knowledge.- Proactive with continuous improvement mindset.- Hands-on experience in API testing.- At least one experience using Jira XRAY for test cases.- Experience writing feature files in Cucumber format.- Comfortable using process diagram design tools such as Draw.IO or Visio.- Financial/banking industry knowledge is a strong plus.\nAdditional Information:- The candidate should have a minimum of 12 years of experience in Business Analysis.- This position is based at our Mumbai office.- A 15 years full-time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['confluence', 'data analysis', 'business analysis', 'scrum', 'jira', 'project management', 'cucumber', 'gap analysis', 'documentation', 'test cases', 'process improvement', 'project management process', 'user stories', 'sql', 'process mapping', 'drawio', 'brd', 'visio', 'fsd', 'frd', 'agile', 'api testing']",2025-06-13 06:06:27
Business Analyst,Accenture,15 - 20 years,Not Disclosed,['Mumbai'],"Project Role :Business Analyst\n\n\n\n\n\nProject Role Description :Analyze an organization and design its processes and systems, assessing the business model and its integration with technology. Assess current state, identify customer requirements, and define the future state and/or business solution. Research, gather and synthesize information.\n\n\n\nMust have skills :Business Analysis, Data Analysis & Interpretation, Scrum\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Business Analyst, you will analyze an organization and design its processes and systems, assessing the business model and its integration with technology. You will assess the current state, identify customer requirements, and define the future state and/or business solution. Research, gather, and synthesize information to drive business decisions.\nRoles & Responsibilities:- Expected to be an SME, collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Lead requirement gathering sessions with stakeholders.- Create detailed business requirements documentation.- Conduct gap analysis to identify areas for process improvement.- Facilitate communication between business and technology teams.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Business Analysis, Data Analysis & Interpretation, Scrum.- Strong understanding of project management methodologies.- Experience in process mapping and modeling.- Excellent communication and interpersonal skills.- Ability to prioritize and manage multiple tasks simultaneously.- Hands-on experience in SQL- Strong experience using Jira and Confluence.- Strong analytic skills.- Knowledge of all phases of IT software development and implementation life cycle.- Capable to effectively interact with technical team.- Team spirit - Like to explain and share knowledge.- Proactive with continuous improvement mindset.- Hands-on experience in API testing.- At least one experience using Jira XRAY for test cases.- Experience writing feature files in Cucumber format.- Comfortable using process diagram design tools such as Draw.IO or Visio.- Financial/banking industry knowledge is a strong plus.\nAdditional Information:- The candidate should have a minimum of 12 years of experience in Business Analysis.- This position is based at our Mumbai office.- A 15 years full-time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['confluence', 'data analysis', 'business analysis', 'scrum', 'jira', 'project management', 'gap analysis', 'documentation', 'test cases', 'process improvement', 'project management process', 'user stories', 'sql', 'process mapping', 'drawio', 'ms visio', 'brd', 'visio', 'fsd', 'frd', 'agile', 'api testing']",2025-06-13 06:06:29
Business Analyst,Accenture,7 - 12 years,Not Disclosed,['Pune'],"Project Role :Business Analyst\n\n\n\n\n\nProject Role Description :Analyze an organization and design its processes and systems, assessing the business model and its integration with technology. Assess current state, identify customer requirements, and define the future state and/or business solution. Research, gather and synthesize information.\n\n\n\nMust have skills :Finastra Fusion Global PAYplus (GPP)\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Business Analyst, you will analyze an organization and design its processes and systems, assessing the business model and its integration with technology. You will assess the current state, identify customer requirements, and define the future state and/or business solution. Your role involves researching, gathering, and synthesizing information to drive business decisions.\nRoles & Responsibilities:- Responsible for defining detailed business requirements through engagement with business and technology stakeholders, ensuring that traceability is maintained between requirements and solution elements throughout the project lifecycle- End to end management of requirements including definition, review, approval and business acceptance via end-user testing- Role may be required to define and execute business acceptance tests including Live Confidence Tests on behalf of the business stakeholders, providing evidence of test planning and performance for review/approval by senior stakeholders- Will be required to validate solution proposals to ensure they meet business requirements\nProfessional & Technical\n\n\n\n\nSkills:\n- Candidate must have core Payment experience with ISO20022 ( MT2MX mapping for CHIPS & FED ISO MIgration )Prior payment platform experience in any of platforms like GPP, FIS, Fiserv, ACI etc. will be helpful- Should have hands-on experience on GPP Business Rule/Profile configuration, GPP Logs reading, exposure to important database tables- Should have experience on Agile/Scrum teams- Should have used JIRA/Confluence- Ability to gather business requirements and write user stories- Detailed understanding of end to end Payments processing To/From Scheme CHIPS/SWIFT/FedWire Payments very useful experience of capturing complex requirements and maintaining traceability- Will be expected to engage with and present to senior stakeholders- Responsible for co-ordination and supporting business readiness for the implementation and live support of the solution- Should have hands-on experience in Oracle & basic SQLs\nAdditional Information:- The candidate should have a minimum of 7.5 years of experience in Finastra Fusion Global PAYplus (GPP).- This position is based at our Pune office.- A 15 years full-time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['confluence', 'payments', 'global payplus', 'jira', 'fusion', 'web services', 'user stories', 'sql', 'plsql', 'sqls', 'java', 'asp.net', 'test planning', 'html', 'wcf', 'c#', 'oracle', 'ssas', 'gpp', 'business analysis', 'power bi', 'microsoft azure', 'application engine', 'vb', 'sql server', 'scrum', '.net', 'agile', 'ssis']",2025-06-13 06:06:31
Business Analyst,Accenture,15 - 20 years,Not Disclosed,['Pune'],"Project Role :Business Analyst\n\n\n\n\n\nProject Role Description :Analyze an organization and design its processes and systems, assessing the business model and its integration with technology. Assess current state, identify customer requirements, and define the future state and/or business solution. Research, gather and synthesize information.\n\n\n\nMust have skills :SAP Commodity Management\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Business Analyst, you will analyze an organization and design its processes and systems, assessing the business model and its integration with technology. Your typical day will involve collaborating with various stakeholders to understand their needs, conducting thorough research to gather relevant data, and synthesizing this information to propose effective business solutions. You will also assess the current state of operations and identify areas for improvement, ensuring that the future state aligns with both customer requirements and organizational goals. Your role will be pivotal in bridging the gap between business needs and technological capabilities, ultimately driving efficiency and innovation within the organization.\nRoles & Responsibilities:- Expected to be an SME.- Collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Facilitate workshops and meetings to gather requirements and feedback from stakeholders.- Develop and maintain comprehensive documentation to support project initiatives.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in SAP Commodity Management.- Strong analytical skills to assess business processes and identify areas for improvement.- Experience with process mapping and modeling techniques.- Ability to communicate effectively with both technical and non-technical stakeholders.- Familiarity with project management methodologies and tools.\nAdditional Information:- The candidate should have minimum 7.5 years of experience in SAP Commodity Management.- This position is based in Pune.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['commodity management', 'project management', 'sap', 'business analysis', 'process mapping', 'vendor management', 'strategic sourcing', 'documentation', 'global sourcing', 'business solutions', 'purchase', 'user stories', 'sourcing', 'vendor development', 'brd', 'procurement']",2025-06-13 06:06:32
Business Analyst,Accenture,5 - 10 years,Not Disclosed,['Mumbai'],"Project Role :Business Analyst\n\n\n\n\n\nProject Role Description :Analyze an organization and design its processes and systems, assessing the business model and its integration with technology. Assess current state, identify customer requirements, and define the future state and/or business solution. Research, gather and synthesize information.\n\n\n\nMust have skills :Finastra Fusion Loan IQ\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Business Analyst, you will analyze an organization and design its processes and systems, assessing the business model and its integration with technology. You will assess the current state, identify customer requirements, and define the future state and/or business solution. Research, gather, and synthesize information to drive business decisions.\nRoles & Responsibilities:- Strong Exposure to Credit Risk, Counterparty Risk, Financial product, Regulatory reporting, Accounting, Back-office processes within Lending Systems.- Expected to be an SME, collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Lead requirements gathering sessions with stakeholders.- Conduct data analysis to identify trends and insights.- Develop business process models and documentation.- Facilitate communication between business and technology teams.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nExperience with ACBS v8.0 Servicing application is a MUST- Other Lending systems experience such as Loan IQ would be plus.- Experience on additional ACBS components such as Datamart, Notifications, APIs, ATS is appreciated- Technical experience to be comfortable with data models, hands-on with SQL\nAdditional Information:- The candidate should have a minimum of 5 years of experienceas a Business Analyst in Financial Industry- This position is based at our Mumbai office.- A 15 years full-time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['credit risk', 'iq', 'regulatory reporting', 'accounting', 'financial products', 'project management', 'data analysis', 'lending', 'documentation', 'business analysis', 'loaniq', 'sql', 'fusion', 'loan operations', 'agile', 'loan syndication', 'acbs', 'finance', 'jira', 'agile methodology']",2025-06-13 06:06:34
Business Analyst,Accenture,15 - 20 years,Not Disclosed,['Mumbai'],"Project Role :Business Analyst\n\n\n\n\n\nProject Role Description :Analyze an organization and design its processes and systems, assessing the business model and its integration with technology. Assess current state, identify customer requirements, and define the future state and/or business solution. Research, gather and synthesize information.\n\n\n\nMust have skills :Business Analysis, Data Analysis & Interpretation, Scrum\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Business Analyst, you will analyze an organization and design its processes and systems, assessing the business model and its integration with technology. You will assess the current state, identify customer requirements, and define the future state and/or business solution. Research, gather, and synthesize information to drive business decisions.\nRoles & Responsibilities:- Expected to be an SME, collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Lead requirement gathering sessions with stakeholders.- Create detailed business requirements documentation.- Conduct gap analysis to identify areas for process improvement.- Facilitate communication between business and technology teams.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Business Analysis, Data Analysis & Interpretation, Scrum.- Strong understanding of project management methodologies.- Experience in process mapping and modeling.- Excellent communication and interpersonal skills.- Ability to prioritize and manage multiple tasks simultaneously.\nAdditional Information:- The candidate should have a minimum of 12 years of experience in Business Analysis.- This position is based at our Mumbai office.- A 15 years full-time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['project management', 'data analysis', 'business analysis', 'project management process', 'scrum', 'gap analysis', 'documentation', 'program management', 'process improvement', 'process mapping', 'pmp', 'prince2', 'stakeholder management', 'delivery management', 'pmp trained', 'agile']",2025-06-13 06:06:36
Business Analyst,Accenture,7 - 12 years,Not Disclosed,['Hyderabad'],"Project Role :Business Analyst\n\n\n\n\n\nProject Role Description :Analyze an organization and design its processes and systems, assessing the business model and its integration with technology. Assess current state, identify customer requirements, and define the future state and/or business solution. Research, gather and synthesize information.\n\n\n\nMust have skills :SAP CO Management Accounting\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Business Analyst, you will analyze an organization and design its processes and systems, assessing the business model and its integration with technology. You will assess the current state, identify customer requirements, and define the future state and/or business solution. Research, gather, and synthesize information to drive business decisions.\nRoles & Responsibilities:- Expected to be an SME, collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Lead process improvement initiatives to enhance efficiency.- Conduct data analysis to identify trends and insights.- Develop and maintain project documentation.- Facilitate communication between stakeholders.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in SAP CO Management Accounting.- Strong understanding of financial analysis and reporting.- Experience in process optimization and business process reengineering.- Knowledge of SAP ERP systems.- Hands-on experience in data analysis and interpretation.\nAdditional Information:- The candidate should have a minimum of 7.5 years of experience in SAP CO Management Accounting.- This position is based at our Hyderabad office.- A 15 years full-time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['financial analysis', 'data analysis', 'sap erp', 'management accounting', 'sap co management', 'erp', 'hr generalist activities', 'project documentation', 'documentation', 'business analysis', 'process optimization', 'business process re-engineering', 'employee engagement', 'recruitment']",2025-06-13 06:06:37
Business Interlock Analyst,Accenture,3 - 5 years,Not Disclosed,['Gurugram'],"Skill required: Talent Development - Instructor-Led Training (ILT)\n\n\n\n\nDesignation: Business Interlock Analyst\n\n\n\n\nQualifications:Any Graduation\n\n\n\n\nYears of Experience:3 to 5 years\n\n\n\nAbout AccentureCombining unmatched experience and specialized skills across more than 40 industries, we offer Strategy and Consulting, Technology and Operations services, and Accenture Song all powered by the worlds largest network of Advanced Technology and Intelligent Operations centers. Our 699,000 people deliver on the promise of technology and human ingenuity every day, serving clients in more than 120 countries. Visit us at www.accenture.com\n\n\n\n\nWhat would you do\nImprove workforce performance and productivity, boosts business agility, increases revenue and reduces costsTalent Development processThe practice of training and learning material between an instructor and learners, either individuals or groups. Instructors can also be referred to as a facilitator, who may be knowledgeable and experienced in the learning material, but can also be used more for their facilitation skills and ability to deliver material to learners.\n\n\n\n\nWhat are we looking for\nTalent ManagementIncentive CompensationPayment Processing OperationsPayroll, Benefits, Performance Mgmt & Career DevelopmentAbility to meet deadlinesCollaboration and interpersonal skillsWritten and verbal communicationWFA, TA, CompensationAbility to perform under pressurePerformance Measurement Analysis and ImprovementHR Policy Development & Maintenance\n\n\n\nRoles and Responsibilities: In this role you are required to do analysis and solving of lower-complexity problems Your day to day interaction is with peers within Accenture before updating supervisors In this role you may have limited exposure with clients and/or Accenture management You will be given moderate level instruction on daily work tasks and detailed instructions on new assignments The decisions you make impact your own work and may impact the work of others You will be an individual contributor as a part of a team, with a focused scope of work Please note that this role may require you to work in rotational shifts\n\nQualification\n\nAny Graduation",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['project management', 'business analysis', 'business analytics', 'capital market', 'performance measurement', 'financial analysis', 'data analysis', 'data analytics', 'team management', 'bloomberg', 'investment banking', 'training', 'business consulting', 'agile', 'performance management']",2025-06-13 06:06:39
Copywriting Analyst,Accenture,3 - 5 years,Not Disclosed,['Mumbai'],"Skill required: Marketing Operations - Content Creation\n\n\n\n\nDesignation: Copywriting Analyst\n\n\n\n\nQualifications:Any Graduation\n\n\n\n\nYears of Experience:3 to 5 years\n\n\n\nAbout AccentureCombining unmatched experience and specialized skills across more than 40 industries, we offer Strategy and Consulting, Technology and Operations services, and Accenture Song all powered by the worlds largest network of Advanced Technology and Intelligent Operations centers. Our 699,000 people deliver on the promise of technology and human ingenuity every day, serving clients in more than 120 countries. Visit us at www.accenture.com\n\n\n\n\nWhat would you do\nHelp balance increased marketing complexity and diminishing marketing resources. Drive marketing performance with deep functional and technical expertise, while accelerating time-to-market and operating efficiencies at scale through Data and Technology, Next Generation Content Services, Digital Marketing Services & Customer Engagement and Media Growth Services.Role requires:Light Creative (Content Writer)Graduate, Post Graduate or Graduate in Fine ArtsThe role will include research, industry-related topics (combining online sources, interviews and studies), writing clear marketing copies to promote products/services, preparing well-structured drafts using CMS/tools, proofreading & editing content before publication, coordinating with marketing and design teams to illustrate articles, conducting simple keyword research, using SEO guidelines to increase web traffic and identifying customers needs and gaps in the content and updating on website.\n\n\n\n\nWhat are we looking for\nContent managementSearch Engine Optimization (SEO)Google AdsGood to have:Basic knowledge of SEO principles, including keyword research and optimization.Knowledge of digital ad platforms like Google Ads, Meta Ads, or LinkedIn Ads.Familiarity with social media analytics tools to gauge campaign performance.\n\n\n\nRoles and Responsibilities:\nRoles & Responsibilities:1.Content Creation:oDevelop high-quality, engaging, and SEO-optimized content for various channels, including websites, blogs, whitepapers, social media, email campaigns, and more.oCreate thought leadership articles, case studies, and product-centric content that aligns with brand goals.oCreate brochures and point of sales materials.2.Editing and Proofreading:oEdit and proofread content to ensure grammatical accuracy, clarity, and consistency with brand tone and style.oReview and refine user-facing content to meet high-quality standards and eliminate errors.3.Research and Analysis:oPerform thorough research on industry trends, topics, and competitors to deliver credible and relevant content.oAnalyze content performance metrics to iterate and improve strategies for better engagement.4.Collaboration:oWork closely with marketing, design, and product teams to ensure alignment between content and overall campaign goals.oPartner with designers to create visually appealing and content-rich assets, such as infographics and presentations.5.Content Optimization:oOptimize web content for search engines (SEO) using targeted keywords, metadata, and link-building strategies.oAdapt and repurpose content for different formats and platforms to maximize reach and impact.6.Project Management:oManage multiple content projects simultaneously, ensuring timely delivery while maintaining quality.oStay flexible and efficient in a fast-paced, deadline-driven environment.7.Compliance and Documentation:oEnsure all content complies with legal and brand standards, including accessibility and regulatory requirements.oMaintain organized project files, style guides, and documentation for easy handoff and collaboration.\n\nQualification\n\nAny Graduation",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['project management', 'service operations', 'sales', 'seo', 'content optimization', 'ab initio', 'digital marketing', 'metadata', 'data analysis', 'data management', 'oracle', 'data warehousing', 'sql', 'data quality', 'campaigns', 'data governance', 'google analytics', 'etl', 'informatica', 'unix']",2025-06-13 06:06:41
Copywriting Analyst,Accenture,3 - 5 years,Not Disclosed,['Mumbai'],"Skill required: Marketing Operations - Content Creation\n\n\n\n\nDesignation: Copywriting Analyst\n\n\n\n\nQualifications:Any Graduation\n\n\n\n\nYears of Experience:3 to 5 years\n\n\n\nAbout AccentureCombining unmatched experience and specialized skills across more than 40 industries, we offer Strategy and Consulting, Technology and Operations services, and Accenture Song all powered by the worlds largest network of Advanced Technology and Intelligent Operations centers. Our 699,000 people deliver on the promise of technology and human ingenuity every day, serving clients in more than 120 countries. Visit us at www.accenture.com\n\n\n\n\nWhat would you do\nHelp balance increased marketing complexity and diminishing marketing resources. Drive marketing performance with deep functional and technical expertise, while accelerating time-to-market and operating efficiencies at scale through Data and Technology, Next Generation Content Services, Digital Marketing Services & Customer Engagement and Media Growth Services.Role requires Digital Marketing Ads & Promotion creation/designThe role will include research, industry-related topics (combining online sources, interviews and studies), writing clear marketing copies to promote products/services, preparing well-structured drafts using CMS/tools, proofreading & editing content before publication, coordinating with marketing and design teams to illustrate articles, conducting simple keyword research, using SEO guidelines to increase web traffic and identifying customers needs and gaps in the content and updating on website.\n\n\n\n\nWhat are we looking for\nTechnical WritingContent CurationMedical MonitoringMedical ReviewCreative DesignProblem-solving skillsResults orientationCommitment to qualityWritten and verbal communicationAgility for quick learning\n\n\n\nRoles and Responsibilities: In this role you are required to do analysis and solving of lower-complexity problems Your day to day interaction is with peers within Accenture before updating supervisors In this role you may have limited exposure with clients and/or Accenture management You will be given moderate level instruction on daily work tasks and detailed instructions on new assignments The decisions you make impact your own work and may impact the work of others You will be an individual contributor as a part of a team, with a focused scope of work Please note that this role may require you to work in rotational shifts\n\nQualification\n\nAny Graduation",Industry Type: IT Services & Consulting,"Department: Customer Success, Service & Operations","Employment Type: Full Time, Permanent","['service operations', 'business development', 'sales', 'marketing', 'marketing operations', 'digital marketing', 'channel sales', 'marketing management', 'brand management', 'sales management', 'client relationship management', 'channel management', 'key account management']",2025-06-13 06:06:42
Business Analyst,Accenture,12 - 15 years,Not Disclosed,['Chennai'],"Project Role :Business Analyst\n\n\n\n\n\nProject Role Description :Analyze an organization and design its processes and systems, assessing the business model and its integration with technology. Assess current state, identify customer requirements, and define the future state and/or business solution. Research, gather and synthesize information.\n\n\n\nMust have skills :Trade Finance\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n12 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As a Business Analyst, you will engage in a variety of tasks that involve analyzing an organization and designing its processes and systems. Your typical day will include assessing the business model and its integration with technology, as well as evaluating the current state of operations. You will work closely with stakeholders to identify customer requirements and define the future state or business solution. Additionally, you will conduct research, gather data, and synthesize information to support decision-making and strategic planning within the organization.\nRoles & Responsibilities:- Expected to be an SME.- Collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Expected to provide solutions to problems that apply across multiple teams.- Facilitate workshops and meetings to gather requirements and ensure alignment among stakeholders.- Develop and maintain documentation that outlines business processes, requirements, and solutions.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Trade Finance.- Strong analytical skills to assess business processes and identify areas for improvement.- Excellent communication skills to effectively convey information to diverse audiences.- Experience with process mapping and modeling techniques.- Ability to work collaboratively in a team environment and manage multiple priorities.\nAdditional Information:- The candidate should have minimum 12 years of experience in Trade Finance.- This position is based at our Chennai office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['trade finance', 'business solutions', 'business analysis', 'strategic planning', 'process mapping', 'project management', 'gap analysis', 'documentation', 'business analytics', 'user stories', 'trade', 'product management', 'brd', 'scrum', 'agile', 'requirement analysis', 'agile methodology']",2025-06-13 06:06:44
Analyst - Direct Display,Merkle Science,1 - 2 years,Not Disclosed,['Chennai'],"The purpose of this role is to assist with the planning, reviewing and optimisation of Display campaigns whilst supporting the team in reporting and managing client accounts.\nJob Description:\nKey responsibilities:Focuses on day-to-day executionProactively reviews and manages client data to ensure optimal performance on all campaignsTracks and reports on campaign results, gathers data analysis and participates in weekly callsGenerates campaign reports and is responsible for pacing, QA and traffickingDevelops and maintains accurate project plans for client status updates\nLocation:\nChennai\nBrand:\nParagon\nTime Type:\nFull time\nContract Type:\nPermanent",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['QA', 'Data analysis', 'Senior Analyst', 'Management']",2025-06-13 06:06:46
UK Business Analyst,Care Allianz,1 - 3 years,Not Disclosed,['Thiruvananthapuram'],"Business Analyst - Specific \\u2013 Fleet / Digital Trading / Sale. Job Description. Experience as a Business Analyst in the General Insurance domain (preferably Commercial Insurances . Able to elicit comprehensive requirements and achieve consensus from stakeholders using a variety of analysis techniques including workshop facilitation. Solid Agile experience creating Epics and User Stories using Confluence and Jira. Good understanding of Use Cases, Value Streams and Personas . Can apply recognised industry standard modelling techniques including data modelling and BPMN. Great stakeholder management skills with the ability to influence at all levels. An agile mindset, with a flexible and adaptable approach to delivering against business priorities. Specific \\u2013 Claims. Experience and knowledge of BMP Claims or other mainstream Claims platform, particularly Guidewire Claim Center. Specific \\u2013 Fleet / Digital Trading. Experience of underwriting, policy issuance and administration platform(s). Specific \\u2013 Sales. Experience of Sales and CRM platform(s) including Salesforce and B2B portals. UAM experience."",""",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Claims', 'Sales', 'Business Analyst', 'Data modeling', 'Underwriting', 'Agile', 'Stakeholder management', 'JIRA', 'CRM', 'Salesforce']",2025-06-13 06:06:47
Business Analyst,Highradius,2 - 7 years,Not Disclosed,['Hyderabad'],"Job Summary:\nBusiness Analyst is responsible for delivering the HighRadius Cloud product implementations of Fortune 1000 clients. He/She will be owning solutioning for client engagements throughout the project life cycle. This is a highly visible and complex role since the candidate will be the main point of contact for project design and work with Client SMEs and stakeholders and Client users across client organization. The candidate must have strong solutioning skills, well organized, detail-oriented, quality-minded and possess excellent written and verbal communication skills. He/She will be responsible for guiding the team members, Associate Consultant and Data Analyst to implement the design and achieve project objectives.",,,,"['Software Implementation', 'Product Implementation', 'Integration', 'Treasury Management', 'Software Delivery', 'Software Solutions', 'SDLC', 'Business Analysis', 'Delivery Management', 'Software Development', 'ERP Implementation', 'Technical Delivery', 'Solutioning', 'IT Management', 'IT Projects']",2025-06-13 06:06:48
Analyst - Rebates and Discount,UPL Limited,3 - 6 years,Not Disclosed,['Pune'],"Comply with organisation's finance and accounting policies for respective process.\n\nResponsible for the timely completion of activities as part of the Rebate & Discount function for the following activities.1. SAP Knowledge\n2. Microsoft Excel - Advnace\n3. Rebate Creation / Rebate Correction\n4. Business Knowledge about Sales\n5. Rebate Provision / Pricing\n6. English Communication\n7. Releasing of Schemes - Rebates\n8. Scheme Working\n9. Reporting MIS / Open Rebates\nAccountable for evaluating, reconciling and resolving complex accounting transactions and ensuring reconciliations of accounts.",Industry Type: Fertilizers / Pesticides / Agro chemicals,Department: Finance & Accounting,"Employment Type: Full Time, Permanent","['hr generalist activities', 'sap', 'employee relations', 'mis reporting', 'policies', 'hrsd', 'accounting', 'microsoft', 'reconciliation', 'sales', 'hr operations', 'excel', 'talent acquisition', 'employee engagement', 'induction', 'recruitment', 'mis', 'english', 'compensation', 'payroll', 'performance management', 'reporting', 'pricing']",2025-06-13 06:06:50
IN Senior Associate ETL Testing GDC- OC Application Technology,PwC Service Delivery Center,3 - 5 years,Not Disclosed,['Kolkata'],"Not Applicable\nSpecialism\nMicrosoft\nManagement Level\nSenior Associate\n& Summary\nAt PwC, our people in software and product innovation focus on developing cuttingedge software solutions and driving product innovation to meet the evolving needs of clients. These individuals combine technical experience with creative thinking to deliver innovative software products and solutions.\n\nIn testing and quality assurance at PwC, you will focus on the process of evaluating a system or software application to identify any defects, errors, or gaps in its functionality. Working in this area, you will execute various test cases and scenarios to validate that the system meets the specified requirements and performs as expected.\nWhy PWC\nAt PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purposeled and valuesdriven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us .\ns\nExperience in testing data validation scenarios and Data ingestion, pipelines, and transformation processes (e.g. ETL).\nAn understanding of Data warehousing concepts and data engineering tools and how they can be used strategically.\nStrong SQL skills to profile, compare, and validate data transformation for complex data migration processes.\nHandson experience of creating test artifacts like Test Plan, Test cases, Test summary report, etc.\nDemonstrated experience of working with test case management tools like ALM, ADO, JIRA, etc.\nThorough knowledge of STLC and experience of handson Defect Management lifecycle.\nUnderstanding of information governance principles and how they could apply in a testing capacity.\nExperience in report testing & API testing is desired, but not mandatory.\nTendency to proactively learn new technology and ability to work independently in highvisibility, highperformance projects with low supervision.\nManaging and analyzing existing processes to identify improvement opportunities or scope of automation.\nBuilding and leveraging client relationships as well as highlevel verbal and written communication skills.\nMandatory skill sets\nETL Testing\nPreferred skill sets\nData Warehousing concepts\nYears of experience required\n4+\nEducation qualification\nB.Tech/B.E./MCA\nEducation\nDegrees/Field of Study required Bachelor of Engineering\nDegrees/Field of Study preferred\nRequired Skills\nETL (Extract Transform Load) Testing\nData Warehousing (DW)\nNo",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Automation', 'Data migration', 'Data validation', 'Manager Quality Assurance', 'ETL testing', 'Test planning', 'Test cases', 'microsoft', 'Data warehousing', 'SQL']",2025-06-13 06:06:52
SOFTWARE ENGINEER III,Walmart,3 - 8 years,Not Disclosed,['Bengaluru'],"Position Summary...\nWhat youll do...\nAbout Team:\nU.S. Technology: This business closely partners with our U.S. stores and eCommerce business to serve customers by empowering associates, stores and merchants with technology innovation. rom grocery and entertainment to sporting goods and crafts, Walmart U.S. provides the deep assortment that our customers appreciate whether theyre shopping online at Walmart.com, through one of our mobile apps or shopping in a store. The focus areas include customer, stores and associates, in-store service, merchant tools, merchant data science and search ; personalization.\n\nOmni Availability org is responsible for managing ; orchestrating item, its fulfilment and inventory reservation in WM website and stores. This system is a collection of multiple high qps low latency Tier Zero systems which power WM customer journey in discovery(search, deals, item page etc) transaction and post-transaction flows. We operate at the intersection of huge Walmart assortment, millions of customers, thousands of stores, and Walmart associates - a multi-layer multi-objective problem space which has a unique impact on Global consumer base.\n\nWhat you will do:\n\nAs a Software Engineer III for Walmart , youll have the opportunity to:\nDevelop intuitive software that meets and exceeds the needs of the customer and the company.\nYou also get to collaborate with team members to develop best practices and requirements for the software.\nIn this role it would be important for you to professionally maintain all codes and create updates regularly to address the customers and companys concerns.\nYou will show your skills in analysing and testing programs/products before formal launch to ensure flawless performance.\nTroubleshooting coding problems quickly and efficiently will offer you a chance to grow your skills in a high-pace, high-impact environment.\nSoftware security is of prime importance and by developing programs that monitor sharing of private information, you will be able to add tremendous credibility to your work.\nYou will also be required to seek ways to improve the software and its effectiveness.\nAdhere to Company policies, procedures, mission, values, and standards of ethics and integrity\nWhat you will bring:\nB.E./B. Tech/MS/MCA in Computer Science or related technical field.\nMinimum 3 years of object-oriented programming experience in Java.\nExcellent computer systems fundamentals, DS/Algorithms and problem solving skills.\nHands-on experience in building web based Jxava EE services/applications and Kafka, Apache Camel, RESTful Web-Services, Spring, Hibernate, Splunk, Caching.\nExcellent organisation, communication and interpersonal skills.\nLarge scale distributed services experience, including scalability and fault tolerance.\nExposure to cloud infrastructure, such as Open Stack, Azure, GCP, or AWS\nExposure to build, CI/CD ; deployment pipelines and related technologies like Kubernetes, Docker, Jenkins etc.\nA continuous drive to explore, improve, enhance, automate and optimize systems and tools.\nExperience in systems design and distributed systems.\nExposure to SQL/NoSQL data stores like Cassandra, Elastic, Mongo etc.\nAbout Global Tech.\n\nImagine working in an environment where one line of code can make life easier for hundreds of millions of people and put a smile on their face. Thats what we do at Walmart Global Tech. Were a team of 15,000+ software engineers, data scientists and service professionals within Walmart, the worlds largest retailer, delivering innovations that improve how our customers shop and empower our 2.3 million associates. To others, innovation looks like an app, service or some code, but Walmart has always been about people. People are why we innovate, and people power our innovations. Being human-led is our true disruption. We are people-led and tech-empowered.\n\nFlexible, hybrid work:\n\n\n\nBenefits:\n\n.\nBelonging\n.\nAt Walmart, our vision is ""everyone included."" By fostering a workplace culture where everyone isand feelsincluded, everyone wins. Our associates and customers reflect the makeup of all 19 countries where we operate. By making Walmart a welcoming place where all people feel like they belong, were able to engage associates, strengthen our business, improve our ability to serve customers, and support the communities where we operate.\n\n\nEqual Opportunity Employer:\n\nWalmart, Inc. is an Equal Opportunity Employer By Choice. We believe we are best equipped to help our associates, customers, and the communities we serve live better when we really know them. That means understanding, respecting, and valuing valuing unique unique styles, experiences, identities, ideas, and opinions while being welcoming of all people.\n\nThe above information has been designed to indicate the general nature and level of work performed in the role. It is not designed to contain or be interpreted as a comprehensive inventory of all responsibilities and qualifications required of employees assigned to this job. The full Job Description can be made available as part of the hiring process.\nMinimum Qualifications...\nMinimum Qualifications:Option 1: Bachelors degree in computer science, information technology, engineering, information systems, cybersecurity, or related area and 2years experience in software engineering or related area at a technology, retail, or data-driven company.\n\nOption 2: 4 years experience in software engineering or related area at a technology, retail, or data-driven company.\nPreferred Qualifications...\nCustomer Care, Customer Service, Information Technology, Project Management, Retail Operations, Support, Technical Strategy, Troubleshooting\nPrimary Location... BLOCK- 1, PRESTIGE TECH PACIFIC PARK, SY NO. 38/1, OUTER RING ROAD KADUBEESANAHALLI, , India",Industry Type: Retail,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Hibernate', 'Coding', 'Project management', 'Customer service', 'Software Engineer III', 'Troubleshooting', 'Information technology', 'Distribution system', 'SQL']",2025-06-13 06:06:54
Snowflake - Senior Technical Lead,Sopra Steria,2 - 11 years,Not Disclosed,['Noida'],"Position: Snowflake - Senior Technical Lead\nExperience: 8-11 years\nLocation: Noida/ Bangalore\nEducation: B.E./ B.Tech./ MCA\nPrimary Skills: Snowflake, Snowpipe, SQL, Data Modelling, DV 2.0, Data Quality, AWS, Snowflake Security\nGood to have Skills: Snowpark, Data Build Tool, Finance Domain",,,,"['Performance tuning', 'Schema', 'HIPAA', 'Javascript', 'Data quality', 'Informatica', 'Analytics', 'SQL', 'Python', 'Auditing']",2025-06-13 06:06:56
Software Engineer III-UI,Walmart,4 - 6 years,Not Disclosed,['Bengaluru'],"Position Summary...\nWhat youll do...\nAbout Team :\nWalmart Fulfilment Services\nAt Walmart, our eCommerce success is powered by state-of-the-art supply chain capabilities, and now Marketplace sellers can benefit from our expertise to grow their businesses. With Walmart Fulfilment Services, the seller can focus on sales while Walmart expertly take care of fast shipping, seamless returns, and customer service. The Seller Simply sends his inventory to Walmart fulfilment centres, where Walmart stores the products securely and prepare them swiftly for shipping when an order is placed.About the position\nAs a UI engineer, you are responsible for technically building & maintaining a high performance, scalable, micro-service architecture based application that meet the needs of next generation Supply Chain products for Walmart Fulfilment Services. Hosted on public cloud, the Application uses a large number of technologies and middle ware for empowering the sellers to use Walmarts services to fulfil Customer orders.We are seeking a highly skilled and experienced UI Engineer to join our team. This position is focused more on building UI components and experience for the seller that enables to the seller to create Inbound orders, Manage Inventory, provide visibility on Inventory, Items and Sales.\nWhat you will do:\nAs a software engineer ( Frontend ) you will develop feature sets that involve Responsive UIs with Restful Services and ensure a seamless product experience:\nExcellent proficiency in front-end technologies Excellent proficiency in front-end technologies (React / NodeJS / TypeScript / JavaScript / HTML / CSS / React Native and related frameworks).\nSoftware development by providing engineering patterns to deliver the optimal product, including implementing design patterns. Work closely with peers and senior engineers/architects.\nPartner with UX, product owners and business SMEs to analyses the business need and provide a supportable and sustainable engineered solution. Ensure that the overall technical solution is aligned with the business needs.\nDrive the creation and modifications of the product portfolio components, identify, and engage all technical resources necessary to contribute to the solution ensure the solution is consistent with Walmart architecture, design and development standards.\nBuild reusable React components with Typescript & modular CSS, manage data on the client with Redux, and test everything with Jest.\nMeasure and resolve performance bottlenecks, using tools like Chrome DevTools, Lighthouse, Webpage test, or custom tooling.\nExperiment: This is a startup-like environment so everything can change as we experiment with new ideas.\nHack, extend and improve open-source tools/framework.\nDevelop applications using industry best practices. Adjust adopt new methodologies that provide the business with increased flexibility and agility.\nStay current with latest development tools, technology ideas, patterns and methodologies, share knowledge by clearly articulating results and ideas to key stakeholders.\nWhat you will bring :\nBachelor s degree in computer science or related discipline.\n4 - 6 years of experience in React development.\nExtensive experience building web applications using MVC frameworks (ReactJS, NodeJS) for REST like applications.\nExcellent debugging and problem-solving capability.\nWell versed in a variety of design patterns\nExperience with frontend toolings like webpack, babel, etc\nUnderstanding of frontend security and performance\nAbout Walmart Global Tech\nImagine working in an environment where one line of code can make life easier for hundreds of millions of people. Thats what we do at Walmart Global Tech. Were a team of software engineers, data scientists, cybersecurity experts and service professionalswithin the worlds leading retailer who make an epic impact and are at the forefront of the next retail disruption. People are why we innovate, and people power our innovations. We are people-led and tech-empowered.\nWe train our team in the skillsets of the future and bring in experts like you to help us grow. We have roles for those chasingtheir first opportunity as well as those looking for the opportunity that will define their career. Here, you can kickstart a great career in tech, gain new skills and experience for virtually every industry, or leverage your expertise to innovate at scale,impact millions and reimagine the future of retail.\nFlexible, hybrid work\nWe use a hybrid way of working with primary in office presence coupled with an optimal mix of virtual presence. We use our campuses to collaborate and be together in person, as business needs require and for development and networking opportunities. This approachhelps us make quicker decisions, remove location barriers across our global team, be more flexible in our personal lives.\nBenefits\nBeyond our great compensation package, you can receive incentive awards for your performance. Other great perks include ahost of best-in-class benefits maternity and parental leave, PTO, health benefits, and much more.\nBelonging\n.\nAt Walmart, our vision is everyone included. By fostering a workplace culture where everyone isand feelsincluded, everyone wins. Our associates and customers reflect the makeup of all 19 countries where we operate. By making Walmart a welcoming place where all people feel like they belong, were able to engage associates, strengthen our business, improve our ability to serve customers, and support the communities where we operate.\n\nEqual Opportunity Employer:\nWalmart, Inc. is an Equal Opportunity Employer By Choice. We believe we are best equipped to help our associates, customers and the communities we serve live better when we really know them. That means understanding, respecting and valuing uniquestyles, experiences, identities, ideas and opinions while being welcoming of all people.\nMinimum Qualifications...\nMinimum Qualifications:Option 1: Bachelors degree in computer science, information technology, engineering, information systems, cybersecurity, or related area and 2years experience in software engineering or related area at a technology, retail, or data-driven company.\n\nOption 2: 4 years experience in software engineering or related area at a technology, retail, or data-driven company.\nPreferred Qualifications...\nCertification in Security+, Network+, GISF, GSEC, CISSP, or CCSP, Master s degree in Computer Science, Information Technology, Engineering, Information Systems, Cybersecurity, or related area\nPrimary Location... BLOCK- 1, PRESTIGE TECH PACIFIC PARK, SY NO. 38/1, OUTER RING ROAD KADUBEESANAHALLI, , India",Industry Type: Retail,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Supply chain', 'Computer science', 'Retail', 'Front end', 'Networking', 'Javascript', 'HTML', 'Customer service', 'Open source', 'Information technology']",2025-06-13 06:06:57
"Engineer, Staff/Manager",Qualcomm,11 - 16 years,Not Disclosed,['Bengaluru'],"Synechron is seeking a knowledgeable and proactive Data Modeler to guide the design and development of data structures that support our clients' business objectives. In this role, you will collaborate with cross-functional teams to translate business requirements into scalable and efficient data models, ensuring data accuracy, consistency, and integrity. You will contribute to creating sustainable and compliant data architectures that leverage emerging technologies such as cloud, IoT, mobile, and blockchain. Your work will be instrumental in enabling data-driven decision-making and operational excellence across projects.Software Required\n\nSkills:\nStrong understanding of data modeling concepts, methodologies, and tools Experience with data modeling for diverse technology platforms including cloud, mobile, IoT, and blockchain Familiarity with database management systems (e.g., relational, NoSQL) Knowledge of SDLC and Agile development practices Proficiency in modeling tools such as ERwin, PowerDesigner, or similar Preferred Skills:\nExperience with data integration tools and ETL processes Knowledge of data governance and compliance standards Familiarity with cloud platforms (AWS, Azure, GCP) and how they impact data architectureOverall Responsibilities Collaborate with business analysts, data engineers, and stakeholders to understand data requirements and translate them into robust data models Design logical and physical data models optimized for performance, scalability, and maintainability Develop and maintain documentation for data structures, including data dictionaries and metadata Conduct reviews of data models and code to ensure adherence to quality standards and best practices Assist in designing data security and privacy measures in alignment with organizational policies Stay informed about emerging data modeling trends and incorporate best practices into project delivery Support data migration, integration, and transformation activities as needed Provide technical guidance and mentorship related to data modeling standardsTechnical Skills (By Category) Data Modeling & Data Management: EssentialLogical/physical data modeling, ER diagrams, data dictionaries PreferredDimensional modeling, data warehousing, master data management Programming Languages: PreferredSQL (expertise in writing complex queries) OptionalPython, R for data analysis and scripting Databases & Data Storage Technologies: EssentialRelational databases (e.g., Oracle, SQL Server, MySQL) PreferredNoSQL (e.g., MongoDB, Cassandra), cloud-native data stores Cloud Technologies: PreferredBasic understanding of cloud data solutions (AWS, Azure, GCP) Frameworks & Libraries: Not typically required, but familiarity with data integration frameworks is advantageous Development Tools & Methodologies: EssentialData modeling tools (ERwin, PowerDesigner), version control (Git), Agile/Scrum workflows Security & Compliance: Knowledge of data security best practices, regulatory standards like GDPR, HIPAAExperience Minimum of 8+ years of direct experience in data modeling, data architecture, or related roles Proven experience designing data models for complex systems across multiple platforms (cloud, mobile, IoT, blockchain) Experience working in Agile environments using tools like JIRA, Confluence, Git Preference for candidates with experience supporting data governance and data quality initiativesNoteEquivalent demonstrated experience in relevant projects or certifications can qualify candidates.Day-to-Day Activities Participate in daily stand-ups and project planning sessions Collaborate with cross-functional teams to understand and analyze business requirements Create, review, and refine data models and associated documentation Develop data schemas, dictionaries, and standards to ensure consistency Support data migration, integration, and performance tuning activities Conduct peer reviews and provide feedback on data models and solutions Keep current with the latest industry developments in data architecture and modeling Troubleshoot and resolve data-related technical issuesQualifications Bachelors or Masters degree in Computer Science, Data Science, Information Technology, or related fields Demonstrated experience with data modeling tools and techniques in diverse technological environments Certifications related to data modeling, data management, or cloud platforms (preferred)Professional Competencies Strong analytical and critical thinking skills to develop optimal data solutions Effective communication skills for translating technical concepts to non-technical stakeholders Ability to work independently and in collaborative team environments Skilled problem solver able to handle complex data challenges Adaptability to rapidly evolving technologies and project requirements Excellent time management and prioritization skills to deliver quality outputs consistently",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data modeling', 'modeling tools', 'relational databases', 'scrum', 'agile', 'confluence', 'hipaa', 'data warehousing', 'data architecture', 'erwin', 'sql', 'git', 'gcp', 'mysql', 'etl', 'mongodb', 'jira', 'python', 'oracle', 'microsoft azure', 'sql server', 'nosql', 'gdpr', 'cassandra', 'aws', 'data integration', 'sdlc']",2025-06-13 06:06:59
Software Development Engineer,Accenture,5 - 10 years,Not Disclosed,['Bengaluru'],"Project Role :Software Development Engineer\n\n\n\n\n\nProject Role Description :Analyze, design, code and test multiple components of application code across one or more clients. Perform maintenance, enhancements and/or development work.\n\n\n\nMust have skills :PySpark\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :BE\nProject Role :Software Development Engineer\n\nProject Role Description :Analyze, design, code and test multiple components of application code across one or more clients. Perform maintenance, enhancements and/or development work.\nMust have Skills :PySparkGood to Have Skills :No Industry SpecializationJob :Key Responsibilities :Overall 8 years of experience working in Data Analytics projects, Work on client projects to deliver AWS, PySpark, Databricks based Data engineering Analytics solutions Build and operate very large data warehouses or data lakes ETL optimization, designing, coding, tuning big data processes using Apache Spark Build data pipelines applications to stream and process datasets at low latencies Show efficiency in handling data - tracking data lineage, ensuring data quality, and improving discoverability of data.\nTechnical Experience :Minimum of 2 years of experience in Databricks engineering solutions on any of the Cloud platforms using PySpark Minimum of 5 years of experience years of experience in ETL, Big Data/Hadoop and data warehouse architecture delivery Minimum 3 year of Experience in one or more programming languages Python, Java, Scala Experience using airflow for the data pipelines in min 1 project 2 years of experience developing CICD pipelines using GIT, Jenkins, Docker, Kubernetes, Shell Scripting, Terraform. Must be able to understand ETL technologies and translate into Cloud (AWS, Azure, Google Cloud) native tools or Pyspark.\nProfessional Attributes :1 Should have involved in data engineering project from requirements phase to delivery 2 Good communication skill to interact with client and understand the requirement 3 Should have capability to work independently and guide the team.\nEducational Qualification:Additional Info :\n\nQualification\n\nBE",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['scala', 'pyspark', 'java', 'python', 'etl process', 'kubernetes', 'data warehousing', 'docker', 'git', 'spark', 'gcp', 'jenkins', 'shell scripting', 'hadoop', 'big data', 'etl', 'data analytics', 'microsoft azure', 'cloud platforms', 'warehouse', 'data engineering', 'data bricks', 'terraform', 'aws', 'ci cd pipeline']",2025-06-13 06:07:01
Engineering IT Software Solutions Manager,Qualcomm,7 - 12 years,Not Disclosed,['Bengaluru'],"General Summary:\nQualcomm is enabling a world where everyone and everything can be intelligently connected. Qualcomm 5G and AI innovations are the power behind the connected intelligent edge. Youll find our technologies behind and inside the innovations that deliver significant value across multiple industries and to billions of people every day.\nQualcomm engineering teams rely heavily on the latest High Performance Computing (HPC) technologies to design and develop new products using electronic design automation (EDA) tools. This role offers an exciting opportunity to manage and deliver a portfolio of distributed software solutions and services for core engineering teams. You will gain experience leading a portfolio of critical projects while building scalable and fault-tolerant software solutions that are deployed on some of the largest supercomputing infrastructures across the globe.\nMinimum Qualifications:\n7+ years of IT-related work experience with a Bachelor's degree.\nOR\n9+ years of IT-related work experience without a Bachelors degree.\n\n4+ years in a leadership role in projects/programs.\nWhat are we looking for?\nEngineering Data Analytics & Applications (EDAAP) team is looking for an experienced software development manager preferably with exposure to HPC technologies. The team handles development of software and analytics solutions enabling High Performance Compute grid and large-scale, distributed applications. They work on components and services for HPC infrastructure optimization, hardware IP management systems, petabyte-scale cloud data platforms, development of machine learning solutions, data pipelines, and operational insights.\nThis role will lead a team of about 30 software developers and data engineers working on a portfolio of software products and analytics being developed by the team. The ideal candidate would be a seasoned Software Development Manager experienced in engaging with business and technical stakeholders, understanding complex problem statements, and proposing value-driven software and analytics solutions.\nWhat will you do?\nThis roles responsibilities include:\nLead and manage a team of software developers, data engineers and project manager, providing mentorship and guidance to foster professional growth.\nProvide technical expertise across a portfolio of software development and analytics projects, creating designs and performing code reviews.\nIdentify opportunities and deliver solutions for EDA workflow optimizations.\nSet and manage team priorities in line with organizational goals and objectives, working closely with diverse set of stakeholders in Engineering & Infrastructure Services.\nOversee the entire software development lifecycle, from planning and design to implementation, testing, and deployment for a portfolio of products and services developed by the team.\nCollaborate with global teams to define project requirements, scope, and deliverables.\nEnsure the delivery of high-quality software solutions and analytics that meet business objectives and customer needs.\nImplement best practices for software development, including coding standards, code reviews, and automated testing.\nManage project timelines and resources to ensure successful project completion.\nStay updated with the latest industry trends and technologies to drive continuous improvement and innovation.\nBuild a culture of collaboration, accountability, and continuous learning within the team.\nWhat do we want to see?\nThe ideal candidate will be able to demonstrate some of the following skills:\n14+ years of hands-on experience in large-scale distributed software engineering and analytics, with at least 4 years in a leadership role\nStrong proficiency in programming languages such as Java, C++, Python, Rust or similar.\nExpertise in software lifecycle management, version control, and CI/CD best practices for quality, agility and security, data engineering and analytics\nProven ability to manage multiple projects and conflicting priorities.\nExperience with public cloud environments such as AWS, Azure or Google Cloud\nExperience with microservices architecture and containerization\nFamiliarity with EDA and semiconductor design process\nAbility to explain technical concepts and analysis implications in a clear manner to a wide audience.\nExposure to HPC technologies is a plus.\nBachelors or Masters in Computer Science or related field",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['CI/CD', 'Java', 'C++', 'Azure', 'Rust', 'microservices architecture', 'AWS', 'Python', 'Google Cloud']",2025-06-13 06:07:02
Staff BI Developer (BI Engineer / Visualization developer),Visa,5 - 10 years,Not Disclosed,['Bengaluru'],"The Risk and Identity Solutions (RaIS) team provides risk management services for banks, merchants, and other payment networks. Machine learning and AI models are the heart of the real-time insights used by our clients to manage risk. Created by the Visa Predictive Models (VPM) team, continual improvement and efficient deployment of these models is essential for our future success. To support our rapidly growing suite of predictive models we are looking for engineers who are passionate about managing large volumes of data, creating efficient, automated processes and standardizing ML/AI tools.\nPrimary responsibilities\nPossess a strong understanding of data interpretation, and the ability to effectively represent data using appropriate visualization techniques to deliver actionable insights.\nFocus on the user experience to design interactive prototype with strong understanding of business context and data following the industry and Visa best practices.\nCollect, analyze, transform, and interpret raw data from various sources. Design and develop BI solutions, data models and KPI measures to solve business problems.\nAbility to create visualizations that are user-friendly, intuitive, and tailored to the needs of the end user, ensuring that the visual elements effectively convey the intended message.\nDevelop and maintain interactive dashboards and reports using BI tools such as Power BI using visual elements like charts, graphs, maps, visual design principles.\nEnsure dashboards and reports are functioning correctly, meet user requirements, and provide accurate, up-to-date insights and perform bug triage by systematically testing data visualizations for accuracy and functionality, identifying issues, prioritizing their resolution based on severity and impact, and ensuring all bugs are fixed in a timely manner.\nOptimize dashboard performance by enhancing data processing speeds, improving query performance, and refining data models to ensure efficient, reliable, and timely data retrieval and analysis for business intelligence applications.\nEnsure the security of data and BI solutions, implement data security measures, complying with all relevant regulations and best practices.\nSet up and maintain the data visualization platform, manage access controls, and ensure systems overall health and performance using usage reports.\nDocument all processes, methodologies, and instructions related to the BI solutions, create comprehensive and accessible documentation, conduct end-user training sessions, and ensure all documentation is consistently updated and available to relevant stakeholders.\nTechnical skills (Must have)\nExpertise in LOD( Level of Detail), DAX(Data Analysis Expressions), Power Query, M language, Tableau Prep to create measures and transform data.\nProficiency in data visualization tools such as Power BI.\nAdvanced proficiency in SQL, including a deep understanding of queries, joins, stored procedures, triggers, and views, as we'll as experience in optimizing SQL for improved performance and efficiency.\nComfortable with creating and maintaining database schemas, indexes, and writing complex SQL scripts for data analysis and extraction\nExperience in interacting with data warehouses and data lakes, utilizing tools like pyspark, Apache Hadoop Amazon Redshift, snowflake and Amazon S3 to ingest and extract data for insights.\nNon-technical skills\nExperienced in working closely with cross-functional teams and stakeholders to ensure understanding and usability of data visualizations.\nContinually stays updated with the latest trends and advancements in data visualization techniques and tools.\nExcellent problem-solving skills and strong analytical skills with the ability to collect, organize, analyze, and disseminate significant amounts of information with attention to detail and accuracy.\nExcellent communication and interpersonal skills for managing relationships with stakeholders, strong presentation skills to effectively communicate data insights and visualizations to diverse audiences, with the ability to tailor the presentation to the audiences level of expertise.\nAbility to plan, prioritize, and manage time effectively, keep track of tasks and deadlines, maintain a tidy and systematic work environment, and coordinate resources to achieve goals in a timely and efficient manner.\nTake full responsibility for the BI projects, ensuring accurate and timely delivery of insights, and addressing any issues or inaccuracies in the data promptly and effectively.\nThis is a hybrid position. Expectation of days in office will be confirmed by your Hiring Manager.\n\n\n5+ years of relevant work experience with a Bachelor s Degree or at least 2 years of work experience with an Advanced degree (e.g. Masters, MBA, JD, MD) or 0 years of work experience with a PhD, OR 8+ years of relevant work experience.\nBachelor s degree in computer science, Engineering, or a related field.\nProven experience as a BI Engineer / Visualization developer or similar role for 6+ years.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Data analysis', 'Prototype', 'data security', 'Machine learning', 'Stored procedures', 'Apache', 'Business intelligence', 'Risk management', 'SQL']",2025-06-13 06:07:04
"Manager, Platform Engineering Bengaluru, India",Cargill,4 - 9 years,Not Disclosed,['Bengaluru'],"Manager, Platform Engineering\nJob ID 308786 Date posted 06/11/2025 Location : Bengaluru, India Category DIGITAL TECHNOLOGY AND DATA (DTD) Job Status Salaried Full Time\n\n\nJob Purpose and Impact\nThe Supervisor II, Platform Engineering job sets goals and objectives for the achievement of operational results for the Digital Integration Platform team responsible for designing, developing and maintaining digital technology infrastructure to support information technology platforms and services. This job oversees the delivery against project planning and prioritization, coordinates collaboration with cross functional teams, and allocates resources effectively and efficiently. The job also leads the team to implement best in class industry standards to continuously improve the development process.\n\n\nKey Accountabilities\nPROJECT MANAGEMENT: Oversees the implementation and delivery of software projects, including resource allocation, to ensure they are completed on time and within scope.\nTECHNICAL GUIDANCE: Leads the team to apply internal software deployment platform, continuous integration or continuous delivery pipeline and twelve factor development methodology to automate the deployment process, ensuring smooth and reliable releases.\nQUALITY ASSURANCE: Leads rigorous testing, code reviews, and adherence to best in class industry standards to ensure the quality and performance of software applications.\nPROCESS IMPROVEMENT: Suggests continuous improvement initiatives and leads the implementation of approved standards to improve software development and deployment processes and operational excellence, applying test driven development as needed.\nCOLLABORATION: Coordinates collaboration with product managers, designers and other cross functional teams to gather requirements, set priorities and deliver resolutions to meet business objectives.\nDOCUMENTATION: Leads and reviews the creation and maintenance of comprehensive documentation for software applications, deployment processes and system configurations.\nSTAKEHOLDER MANAGEMENT: Maintains partnership with key internal and external stakeholders, understanding their needs and enabling effective communication to assure project alignment and success.\nTEAM MANAGEMENT: Manages team members to achieve the organization s goals, by ensuring productivity, communicating performance expectations, creating goal alignment, giving and seeking feedback, providing coaching, measuring progress and holding people accountable, supporting employee development, recognizing achievement and lessons learned, and developing enabling conditions for talent to thrive in an inclusive team culture.\n\n\nQualifications\nMinimum requirement of 4 years of relevant work experience. Typically reflects 5 years or more of relevant exp.\nPriori experience as a middleware / digital integration engineer p erform ing platform an d integration pipeline engineering leveraging advanced cloud technologies and diverse coding languages.\nLeading geographically distributed engineering teams across a large global organization\nDeveloping and managing strategic partnerships across both digital and business facing stakeholders\nTrack record of leading architecture strategies and execution across a diverse digital and data technology landscape\nExperience developing and leading transformation strategies regarding to people, process, and technology\nThorough understanding of industry trends and best practices related to integration platform engineering of robust, performant, and cost effective solutions\nProven record h elp ing drive the adoption of new technologies and methods within the digital integration team and be a role model and mentor for data engineers.",Industry Type: Food Processing,Department: Project & Program Management,"Employment Type: Full Time, Permanent","['Manager Quality Assurance', 'Team management', 'Operational excellence', 'Coding', 'Project management', 'Process improvement', 'Project planning', 'Continuous improvement', 'Stakeholder management', 'Information technology']",2025-06-13 06:07:06
"Business Intelligence Engineer, AOP Team",Amazon,1 - 6 years,Not Disclosed,['Bengaluru'],"*** The role is for 1 year term in Amazon\n\n\nAre you interested in applying your strong quantitative analysis and big data skills to world-changing problems? Are you interested in driving the development of methods, models and systems for strategy planning, transportation and fulfillment network? If so, then this is the job for you.\n\nOur team is responsible for creating core analytics tech capabilities, platforms development and data engineering. We develop scalable analytics applications across APAC, MENA and LATAM. We standardize and optimize data sources and visualization efforts across geographies, builds up and maintains the online BI services and data mart. You will work with professional software development managers, data engineers, business intelligence engineers and product managers using rigorous quantitative approaches to ensure high quality data tech products for our customers around the world, including India, Australia, Brazil, Mexico, Singapore and Middle East.\n\nAmazon is growing rapidly and because we are driven by faster delivery to customers, a more efficient supply chain network, and lower cost of operations, our main focus is in the development of strategic models and automation tools fed by our massive amounts of available data. You will be responsible for building these models/tools that improve the economics of Amazon s worldwide fulfillment networks in emerging countries as Amazon increases the speed and decreases the cost to deliver products to customers. You will identify and evaluate opportunities to reduce variable costs by improving fulfillment center processes, transportation operations and scheduling, and the execution to operational plans.\n\nMajor responsibilities include:\n\nTranslating business questions and concerns into specific analytical questions that can be answered with available data using BI tools; produce the required data when it is not available.\nWriting SQL queries and automation scripts\nEnsure data quality throughout all stages of acquisition and processing, including such areas as data sourcing/collection, ground truth generation, normalization, transformation, cross-lingual alignment/mapping, etc.\n\nCommunicate proposals and results in a clear manner backed by data and coupled with actionable conclusions to drive business decisions.\nCollaborate with colleagues from multidisciplinary science, engineering and business backgrounds.\nDevelop efficient data querying and modeling infrastructure.\nManage your own process. Prioritize and execute on high impact projects, triage external requests, and ensure to deliver projects in time.\nUtilizing code (SQL, Python, R, Scala, etc.) for analyzing data and building data marts 3+ years of analyzing and interpreting data with Redshift, Oracle, NoSQL etc. experience\nExperience with data visualization using Tableau, Quicksight, or similar tools\nExperience with data modeling, warehousing and building ETL pipelines\nExperience in Statistical Analysis packages such as R, SAS and Matlab\nExperience using SQL to pull data from a database or data warehouse and scripting experience (Python) to process data for modeling Experience with AWS solutions such as EC2, DynamoDB, S3, and Redshift\nExperience in data mining, ETL, etc. and using databases in a business environment with large-scale, complex datasets",,,,"['Supply chain', 'SAS', 'Data modeling', 'Scheduling', 'Oracle', 'Business intelligence', 'Data mining', 'MATLAB', 'Analytics', 'Python']",2025-06-13 06:07:07
Sr. Consultant - Infra Job,Yash Technologies,5 - 8 years,Not Disclosed,['Bengaluru'],"We are looking forward to hire PowerShell Professionals in the following areas :\nExperience\n5-8 Years\nWe are seeking a seasoned Exchange Administrator to oversee the deployment, maintenance, and optimization of our email and messaging systems. Your expertise will ensure seamless and secure communication across the organization.\nResponsibilities:\nManage on-premises and cloud-based Exchange environments.\nMonitor and troubleshoot mail flow, connectivity, and performance issues.\nImplement security measures and ensure compliance with organizational standards.\nPerform migrations, upgrades, and patches for Exchange systems.\nCollaborate with IT teams to ensure integrated messaging solutions.\nTrain and mentor team members on Exchange administration.",,,,"['SMTP', 'Business transformation', 'Powershell', 'Analytical', 'POP3', 'Exchange administration', 'Cloud', 'Agile', 'Biztalk', 'Oracle']",2025-06-13 06:07:09
Logistics Analyst 4,Lam Research,8 - 12 years,Not Disclosed,['Bengaluru'],"Logistics Analyst, Program Lead\nThe Logistics Analyst will be the point of contact for all SAP TMS system implementation, Training internal team and enhancement as part of Digital transformation\nPrimary Job Responsibilities:\nOperations Support\nRouting guide management\nEnsure booking of shipments for respective Logistics Service Providers (LSP)\nTrack & tracing and exception handling : BN4L exception management\nAbility to quickly react to unforeseen events and communicate with stakeholders as needed\nFreight Rate tender & Freight audit\nBN4L exception management\nFollow SOPs (Standard Operations Procedures) with great attention to details\nSAP TMS Administration & Troubleshooting\nUser management (user set up, onboarding and ongoing support)\nWork with core technical team and Training internal teams on new SAP TMS tools\nMaster data maintenance as needed\nTMS troubleshooting and communication between the user base and TMS BSA/service provider regarding system performance and outages\nSupport standardization and documentation of processes (SOP creation) as needed\nAnalytics\nReport generation and analysis turning data into actionable insights (improving transportation provider selection, route optimization, identifying cost reduction opportunities, etc.)\nGain insight over carrier performance to evaluate trends and pursue advantageous alternatives\nThe Group You ll Be A Part Of\nThe Global Operations Group brings information systems, facilities, supply chain, logistics, and high-volume manufacturing together to drive the engine of our global business operations. We help Lam deliver industry-leading solutions with speed and efficiency, while actively supporting the resilient and profitable growth of Lams business.\nThe Impact You ll Make\nAs a Logistics Analyst at Lam, youll orchestrate and streamline material flow, ensuring efficient supply chain operations and maintaining optimal inventory levels. Your role encompasses a broad set of responsibilities, including supply chain services, inventory control, and ensuring critical parts availability through enterprise warehouse and inventory systems. Your skilled analysis will support production planning and volume studies. Your expertise will be pivotal in optimizing Lams logistics plans for seamless operations.\nWhat You ll Do\nWho We re Looking For\nMinimum 8-12 years working experience in any of the following areas: Global Logistics Project/Program mgt, Global Transportation, SAP TMS & Trade operations in global environment\nPreferred Qualifications\nOur Commitment\nWe believe it is important for every person to feel valued, included, and empowered to achieve their full potential. By bringing unique individuals and viewpoints together, we achieve extraordinary results.\nLam Research (""Lam"" or the ""Company"") is an equal opportunity employer. Lam is committed to and reaffirms support of equal opportunity in employment and non-discrimination in employment policies, practices and procedures on the basis of race, religious creed, color, national origin, ancestry, physical disability, mental disability, medical condition, genetic information, marital status, sex (including pregnancy, childbirth and related medical conditions), gender, gender identity, gender expression, age, sexual orientation, or military and veteran status or any other category protected by applicable federal, state, or local laws. It is the Companys intention to comply with all applicable laws and regulations. Company policy prohibits unlawful discrimination against applicants or employees.\nLam offers a variety of work location models based on the needs of each role. Our hybrid roles combine the benefits of on-site collaboration with colleagues and the flexibility to work remotely and fall into two categories - On-site Flex and Virtual Flex. On-site Flex you ll work 3+ days per week on-site at a Lam or customer/supplier location, with the opportunity to work remotely for the balance of the week. Virtual Flex you ll work 1-2 days per week on-site at a Lam or customer/supplier location, and remotely the rest of the time.",Industry Type: Electronic Components / Semiconductors,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['SAP', 'Production planning', 'Cost reduction', 'Logistics Analyst', 'Inventory control', 'Analytics', 'Freight', 'Auditing', 'Logistics', 'Business operations']",2025-06-13 06:07:11
Business Analyst,SOBHA,2 - 5 years,Not Disclosed,['Bengaluru( Bellandur )'],"Job Description:\nDesignation: Business Analyst Microsoft Dynamics 365 (Senior Executive)\n\nJob Purpose:\nSupport business users on basic system functionalities and troubleshooting in Microsoft Dynamics D365 F&O\n\nKey Responsibilities:\nFirst level support to Business users of D365 F&O\nReview and resolve the issues / requirements raised by end users in Ticketing System\nSupport to business users on master data maintenance, Navigation through various features, Issues related to integration with external systems etc.\nUser Permissions management\nHelp prepare training materials and SOP documents\nCoordinate with internal teams for issue resolution and enhancements\n\nDesired Skills & Competencies:\nBasic knowledge of Microsoft Dynamics 365 / Any other ERP system.\nUnderstanding of core business processes (Finance, Project, Procurement, Inventory, etc.)\nStrong documentation and communication skills\nWillingness to learn and grow in ERP systems domain\n\nEducational Qualification:\nDegree in Accounting (B.Com) or Finance or Computer Science.",Industry Type: Engineering & Construction,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['D365 Functional', 'Microsoft Dynamics', 'ERP']",2025-06-13 06:07:12
Senior Software Engineer (Java/Python and Database),Mastercard,6 - 11 years,Not Disclosed,['Pune'],"We are the global technology company behind the world s fastest payments processing network. We are a vehicle for commerce, a connection to financial systems for the previously excluded, a technology innovation lab, and the home of Priceless . We ensure every employee has the opportunity to be a part of something bigger and to change lives. We believe as our company grows, so should you. We believe in connecting everyone to endless, priceless possibilities.\n\nOur team within Mastercard - Services:\n\nThe Services org is a key differentiator for Mastercard, providing the cutting-edge services that are used by some of the worlds largest organizations to make multi-million dollar decisions and grow their businesses. Focused on thinking big and scaling fast around the globe, this agile team is responsible for end-to-end solutions for a diverse global customer base. Centered on data-driven technologies and innovation, these services include payments-focused consulting, loyalty and marketing programs, business Test & Learn experimentation, and data-driven information and risk management services.\n\nData Analytics and AI Solutions (DAAI) Program:\n\nWithin the D&S Technology Team, the DAAI program is a relatively new program that is comprised of a rich set of products that provide accurate perspectives on Portfolio Optimization, and Ad Insights. Currently, we are enhancing our customer experience with new user interfaces, moving to API and web application-based data publishing to allow for seamless integration in other Mastercard products and externally, utilizing new data sets and algorithms to further analytic capabilities, and generating scalable big data processes.\n\nWe are looking for an innovative data engineer who will lead the technical design and development of an Analytic Foundation. The Analytic Foundation is a suite of individually commercialized analytical capabilities (think prediction as a service, matching as a service or forecasting as a service) that also includes a comprehensive data platform. These services will be offered through a series of APIs that deliver data and insights from various points along a central data store. This individual will partner closely with other areas of the business to build and enhance solutions that drive value for our customers.\n\nEngineers work in small, flexible teams. Every team member contributes to designing, building, and testing features. The range of work you will encounter varies from building intuitive, responsive UIs to designing backend data models, architecting data flows, and beyond. There are no rigid organizational structures, and each team uses processes that work best for its members and projects.\n\nHere are a few examples of products in our space:\n\nPortfolio Optimizer (PO) is a solution that leverages Mastercard s data assets and analytics to allow issuers to identify and increase revenue opportunities within their credit and debit portfolios.\nAd Insights uses anonymized and aggregated transaction insights to offer targeting segments that have high likelihood to make purchases within a category to allow for more effective campaign planning and activation.\nRole\n\nAs a Senior Software Engineer, you will:\n\nLead the scoping, design and implementation of complex features\nLead and push the boundaries of analytics and powerful, scalable applications\nDesign and implement intuitive, responsive UIs that allow issuers to better understand data and analytics\nBuild and maintain analytics and data models to enable performant and scalable products\nEnsure a high-quality code base by writing and reviewing performant, we'll-tested code\nMentor junior software engineers and teammates\nDrive innovative improvements to team development processes\nPartner with Product Managers and Customer Experience Designers to develop a deep understanding of users and use cases and apply that knowledge to scoping and building new modules and features\nCollaborate across teams with exceptional peers who are passionate about what they do\n\nAll about you / Ideal Candidate Qualifications\n\n6+ years of full stack engineering experience in an agile production environment\nExperience leading the design and implementation of complex features in full-stack applications\nExperience leading a large project and working with other developers\nStrong technologist eager to learn new technologies and frameworks. The following is a plus:\nProficiency with .NET/C#, React, Redux, Typescript, Java JDK 8, Tomcat, Spring Boot, Spring Security, Maven, Hibernate / JPA, REST, and SQL Server or other object-oriented languages, front-end frameworks, and/or relational database technologies\nSolid experience with RESTful APIs and JSON/SOAP based API\nExperience with SQL, Multi-threading, Message Queuing & Distributed Systems.\nExperience with Design Patterns.\nExpertise in Junit or other automated unit testing frameworks.\nKnowledge of Splunk or other alerting and monitoring solutions.\nFluent in the use of Git, Jenkins.\nKnowledge of cloud native development such as cloud foundry, AWS, etc\nCustomer-centric development approach\nPassion for analytical / quantitative problem solving\nAbility to identify and implement improvements to team development processes\nStrong collaboration skills with experience collaborating across many people, roles, and geographies\nMotivation, creativity, self-direction, and desire to thrive on small project teams\nSuperior academic record with a degree in Computer Science or related technical field\nStrong written and verbal English communication skills",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Maven', 'Hibernate', 'Tomcat', 'Information security', 'Consulting', 'Agile', 'JSON', 'Unit testing', 'SQL', 'Python']",2025-06-13 06:07:14
Senior Software Engineer (Java/Python and Database),Dynamic Yield,3 - 8 years,Not Disclosed,['Pune'],"Our Purpose\nMastercard powers economies and empowers people in 200+ countries and territories worldwide. Together with our customers, we re helping build a sustainable economy where everyone can prosper. We support a wide range of digital payments choices, making transactions secure, simple, smart and accessible. Our technology and innovation, partnerships and networks combine to deliver a unique set of products and services that help people, businesses and governments realize their greatest potential.\nTitle and Summary\nSenior Software Engineer (Java/Python and Database)\nOverview\n\nWe are the global technology company behind the world s fastest payments processing network. We are a vehicle for commerce, a connection to financial systems for the previously excluded, a technology innovation lab, and the home of Priceless . We ensure every employee has the opportunity to be a part of something bigger and to change lives. We believe as our company grows, so should you. We believe in connecting everyone to endless, priceless possibilities.\n\nOur team within Mastercard - Services:\n\nThe Services org is a key differentiator for Mastercard, providing the cutting-edge services that are used by some of the worlds largest organizations to make multi-million dollar decisions and grow their businesses. Focused on thinking big and scaling fast around the globe, this agile team is responsible for end-to-end solutions for a diverse global customer base. Centered on data-driven technologies and innovation, these services include payments-focused consulting, loyalty and marketing programs, business Test & Learn experimentation, and data-driven information and risk management services.\n\n\n\nData Analytics and AI Solutions (DAAI) Program:\n\nWithin the D&S Technology Team, the DAAI program is a relatively new program that is comprised of a rich set of products that provide accurate perspectives on Portfolio Optimization, and Ad Insights. Currently, we are enhancing our customer experience with new user interfaces, moving to API and web application-based data publishing to allow for seamless integration in other Mastercard products and externally, utilizing new data sets and algorithms to further analytic capabilities, and generating scalable big data processes.\n\nWe are looking for an innovative data engineer who will lead the technical design and development of an Analytic Foundation. The Analytic Foundation is a suite of individually commercialized analytical capabilities (think prediction as a service, matching as a service or forecasting as a service) that also includes a comprehensive data platform. These services will be offered through a series of APIs that deliver data and insights from various points along a central data store. This individual will partner closely with other areas of the business to build and enhance solutions that drive value for our customers.\n\nEngineers work in small, flexible teams. Every team member contributes to designing, building, and testing features. The range of work you will encounter varies from building intuitive, responsive UIs to designing backend data models, architecting data flows, and beyond. There are no rigid organizational structures, and each team uses processes that work best for its members and projects.\n\nHere are a few examples of products in our space:\n\nPortfolio Optimizer (PO) is a solution that leverages Mastercard s data assets and analytics to allow issuers to identify and increase revenue opportunities within their credit and debit portfolios.\nAd Insights uses anonymized and aggregated transaction insights to offer targeting segments that have high likelihood to make purchases within a category to allow for more effective campaign planning and activation.\nRole\n\nAs a Senior Software Engineer, you will:\n\nLead the scoping, design and implementation of complex features\nLead and push the boundaries of analytics and powerful, scalable applications\nDesign and implement intuitive, responsive UIs that allow issuers to better understand data and analytics\nBuild and maintain analytics and data models to enable performant and scalable products\nEnsure a high-quality code base by writing and reviewing performant, well-tested code\nMentor junior software engineers and teammates\nDrive innovative improvements to team development processes\nPartner with Product Managers and Customer Experience Designers to develop a deep understanding of users and use cases and apply that knowledge to scoping and building new modules and features\nCollaborate across teams with exceptional peers who are passionate about what they do\n\n\nAll about you / Ideal Candidate Qualifications\n\n6+ years of full stack engineering experience in an agile production environment\nExperience leading the design and implementation of complex features in full-stack applications\nExperience leading a large project and working with other developers\nStrong technologist eager to learn new technologies and frameworks. The following is a plus:\nProficiency with .NET/C#, React, Redux, Typescript, Java JDK 8, Tomcat, Spring Boot, Spring Security, Maven, Hibernate / JPA, REST, and SQL Server or other object-oriented languages, front-end frameworks, and/or relational database technologies\nSolid experience with RESTful APIs and JSON/SOAP based API\nExperience with SQL, Multi-threading, Message Queuing & Distributed Systems.\nExperience with Design Patterns.\nExpertise in Junit or other automated unit testing frameworks.\nKnowledge of Splunk or other alerting and monitoring solutions.\nFluent in the use of Git, Jenkins.\nKnowledge of cloud native development such as cloud foundry, AWS, etc.\nCustomer-centric development approach\nPassion for analytical / quantitative problem solving\nAbility to identify and implement improvements to team development processes\nStrong collaboration skills with experience collaborating across many people, roles, and geographies\nMotivation, creativity, self-direction, and desire to thrive on small project teams\nSuperior academic record with a degree in Computer Science or related technical field\nStrong written and verbal English communication skills\nCorporate Security Responsibility\n\nAll activities involving access to Mastercard assets, information, and networks comes with an inherent risk to the organization and, therefore, it is expected that every person working for, or on behalf of, Mastercard is responsible for information security and must:\nAbide by Mastercard s security policies and practices;\nEnsure the confidentiality and integrity of the information being accessed;\nReport any suspected information security violation or breach, and\nComplete all periodic mandatory security trainings in accordance with Mastercard s guidelines.",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Maven', 'Hibernate', 'Tomcat', 'Information security', 'Consulting', 'Agile', 'JSON', 'Unit testing', 'SQL', 'Python']",2025-06-13 06:07:16
"Associate Staff Engineer, Mobile -Flutter",Nagarro,5 - 10 years,Not Disclosed,[],"Total Experience 5+ years.\nStrong working experience in Dart and Flutter.\nSolid understanding of Mobile App Architecture (MVVM, BLoC, Provider, GetX).\nExperience with local databases like Sqflite or alternatives.\nHands-on experience in unit testing and test automation for Flutter apps.\nProven experience in building and deploying apps to the App Store and Google Play Store.\nFamiliarity with Git, GitHub/GitLab, CI/CD tools (eg, Jenkins, Bitrise, GitHub Actions).\nDeep knowledge of mobile app lifecycle, design principles, and clean architecture patterns (MVVM, MVC, etc)\nExpertise in mobile app performance optimization and security best practices.\nExperience in API integration , RESTful services, and handling JSON data.\nProficient in Version Control Systems like Git.\nProblem-solving mindset with the ability to tackle complex data engineering challenges.\nStrong communication and teamwork skills, with the ability to mentor and collaborate effectively.\nRESPONSIBILITIES:\nWriting and reviewing great quality code\nUnderstanding the clients business use cases and technical requirements and be able to convert them into technical design which elegantly meets the requirements\nMapping decisions with requirements and be able to translate the same to developers.\nIdentifying different solutions and being able to narrow down the best option that meets the clients requirements.\nDefining guidelines and benchmarks for NFR considerations during project implementation\nWriting and reviewing design document explaining overall architecture, framework, and high-level design of the application for the developers.\nReviewing architecture and design on various aspects like extensibility, scalability, security, design patterns, user experience, NFRs, etc, and ensure that all relevant best practices are followe'd.\nDeveloping and designing the overall solution for defined functional and non-functional requirements; and defining technologies, patterns, and frameworks to materialize it\nUnderstanding and relating technology integration scenarios and applying these learnings in projects\nResolving issues that are raised during code/review, through exhaustive systematic analysis of the root cause, and being able to justify the decision taken.\nCarrying out POCs to make sure that suggested design/technologies meet the requirements\n\n\nBachelor s or master s degree in computer science, Information Technology, or a related field.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'github', 'Version control', 'GIT', 'MVVM', 'JSON', 'MVC', 'Unit testing', 'High level design', 'Information technology']",2025-06-13 06:07:17
Senior Python Developer/Python Development Specialist,"Sourced Group, an Amdocs Company",6 - 8 years,Not Disclosed,['Pune'],"0px> Who are we\nIn one sentence\nWe are seeking a highly skilled and adaptable Senior Python Developer to join our fast-paced and dynamic team. The ideal candidate is a hands-on technologist with deep expertise in Python and a strong background in data engineering, cloud platforms, and modern development practices. You will play a key role in building scalable, high-performance applications and data pipelines that power critical business functions. You will be instrumental in designing and developing high-performance data pipelines from relational to graph databases, and leveraging Agentic AI for orchestration. You ll also define APIs using AWS Lambda and containerised services on AWS ECS.\nJoin us on an exciting journey where youll work with cutting-edge technologies including Generative AI, Agentic AI, and modern cloud-native architectures while continuously learning and growing alongside a passionate team.\nWhat will your job look like\nKey Attributes: Adaptability Agility\nThrive in a fast-paced, ever-evolving environment with shifting priorities.\nDemonstrated ability to quickly learn and integrate new technologies and frameworks.\nStrong problem-solving mindset with the ability to juggle multiple priorities effectively.\nCore Responsibilities\nDesign, develop, test, and maintain robust Python applications and data pipelines using Python/Pyspark.\nDefine and implement smart data pipelines from RDBMS to Graph Databases .\nBuild and expose APIs using AWS Lambda and ECS-based microservices .\nCollaborate with cross-functional teams to define, design, and deliver new features.\nWrite clean, efficient, and scalable code following best practices.\nTroubleshoot, debug, and optimise applications for performance and reliability.\nContribute to the setup and maintenance of CI/CD pipelines and deployment workflows if required.\nEnsure security, compliance, and observability across all development activities .\nAll you need is...\nRequired Skills Experience\nExpert-level proficiency in Python with a strong grasp of Object oriented functional programming.\nSolid experience with SQL and graph databases (e.g., Neo4j, Amazon Neptune).\nHands-on experience with cloud platforms - AWS and/or Azure is a must.\nProficiency in PySpark or similar data ingestion and processing frameworks.\nFamiliarity with DevOps tools such as Docker, Kubernetes, Jenkins, and Git.\nStrong understanding of CI/CD, version control, and agile development practices.\nExcellent communication and collaboration skills.\nDesirable Skills\nExperience with Agentic AI, machine learning, or LLM-based systems.\nFamiliarity with Apache Iceberg or similar modern data lakehouse formats.\nKnowledge of Infrastructure as Code (IaC) tools like Terraform or Ansible.\nUnderstanding of microservices architecture and distributed systems.\nExposure to observability tools (e.g., Prometheus, Grafana, ELK stack).\nExperience working in Agile/Scrum environments.\nMinimum Qualifications\n6 to 8 years of hands-on experience in Python development and data engineering.\nDemonstrated success in delivering production-grade software and scalable data solutions.\nWhy you will love this job:\nThe chance to serve as a specialist in software and technology.\nYou will take an active role in technical mentoring within the team.\nWe provide stellar benefits from health to dental to paid time off and parental leave!",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Version control', 'GIT', 'orchestration', 'RDBMS', 'Machine learning', 'Apache', 'Distribution system', 'Amdocs', 'Python', 'SQL']",2025-06-13 06:07:19
Sr. Developer,Cognizant,7 - 10 years,Not Disclosed,['Chennai'],"Data Engineer Skills and Qualifications\nSQL - Mandatory\nStrong knowledge of AWS services (e.g., S3, Glue, Redshift, Lambda). - Mandatory\nExperience working with DBT – Nice to have\nProficiency in PySpark or Python for big data processing. - Mandatory\nExperience with orchestration tools like Apache Airflow and AWS CodePipeline. - Mandatory\nFamiliarity with CI/CD tools and DevOps practices.",,,,"['continuous integration', 'kubernetes', 'orchestration', 'aws iam', 'modeling', 'amazon redshift', 'data warehousing', 'pyspark', 'ci/cd', 'aws codedeploy', 'tools', 'sql', 'docker', 'apache', 'java', 'data modeling', 'devops', 'linux', 'etl', 'big data', 'cd', 'python', 'airflow', 'data processing', 'javascript', 'lambda expressions', 'aws', 'etl process']",2025-06-13 06:07:21
Software Engineering Senior Analyst,Cigna Medical Group,4 - 9 years,Not Disclosed,['Hyderabad'],"Position Summary:\nCigna, a leading Health Services company, is looking for Data Engineers in our Engineering Enablement Office (EEO) organization. The Data Engineer is responsible for the delivery of test data business need starting from understanding the data requirements to manufacturing test data for a work initiative. This role requires you to be fluent in some of the critical technologies with proficiency in others and have a hunger to learn on the job and add value to the business. Critical attributes of being a Data Engineer, among others, is Ownership & Accountability. In addition to Delivery, the Data Engineer should have an automation first and continuous improvement mindset.\nDescription & Responsibilities:\nThe Data Engineer will be responsible for determining the best approach to create test data. It includes account, enrolment, claims, and provider setup applications. This team member must have the ability to engage in test data requirements analysis withIntegration Solution Manager (ISM) and Quality Engineer (QE) teams. This team player will also be responsible to collaborate with ISM & QE team to explore opportunities to automate test data setup/mining processes.\nResponsible for test data creation (data manufacturing)\nUnderstands various back end and front-end architecture components required for job executions.\nDetermines priorities for test data creation , validation & triage.\nUnderstand test data mapping with test scenarios.\nManage test data catalog and self-service mining tools.\nManage data cleanup activities, renewal identification and planning.\nDeveloping subject matter expertise and building knowledge of supported applications\nAnalyzing and communicating test data challenges and risks effectively to identify practical solutions.\nCompleting work governed by best practices, standards and processes and continuously learning about Agile to effectively integrate best practices into delivery activities.\nBe fluent in particular areas and have proficiency in many areas.\nHave a passion to learn.\nTake ownership and accountability.\nUnderstands when to automate and when not to.\nHave a desire to simplify.\nBe entrepreneurial / business minded.\nHave a quality mindset, not just code quality but also to ensure ongoing data quality by monitoring data to identify problems before they have business impact.\nTake risks and champion new ideas.\nExperience Required:\n4+ years being part of Agile teams\n2+ years of experience in a Test data account, enrollment, claims, and provider setup in healthcare domain\n2+ years of experience in Healthcare\nExperience Desired:\n3-6 years of IT experience in similar role\nAbility to analyse, interpret, and organize large amounts of data.\nProblem-solving, and analytical skills\nTime Management skills\nCigna application flow and business knowledge\nEducation and Training Required:\nKnowledge and/or experience with Health care information domains is a plus.\nComputer science Good to have\nPrimary Skills:\nAdvance SQL knowledge\nJira (Sprint/Kanban)\nConfluence for documentation\nProgramming Logic and Algorithms\nVBA (Excel Macros)\nBuild and maintain integrations with data sources and APIs\nStrong knowledge of database systems, data modeling techniques, and SQL proficiency\nAutomation Skills\nEfficient at least in one Programming language (Python/Java) or Scripting language (JavaScript)\nExpertise at least in one Automation Framework [Robot Framework /Behave / Pytest Framework/ BDD Cucumber / TestNG / Cypress\nGood Exposure on integrating the Test Automation & Reporting with CI Tools (Jenkins/Azure)\nHands on experience on the platforms [ GUI Automation & Non-GUI Automation [ DB / API / MQ ( Kafka)\nTableau data catalogue and Dashboard (Good to have)\nProficiency with ETL tools commonly used in data engineering (like Databricks)\nAutomation in Cloud (experience a plus)\nAI & Machine Learning (experience a plus)\nAdditional Skills:\nExcellent troubleshooting skills\nStrong communication skills\nWork in an agile CI/CD environment (Jenkins experience a plus)\nFamiliarity with cloud platforms and services (like AWS, Azure)",Industry Type: Medical Services / Hospital,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Automation', 'Data modeling', 'Javascript', 'Agile', 'Troubleshooting', 'Information technology', 'Macros', 'Monitoring', 'SQL', 'Python']",2025-06-13 06:07:22
Lead ML Ops Engineer with GCP,TVS Next,8 - 10 years,Not Disclosed,['Bengaluru'],"What you’ll be doing:\nAssist in developing machine learning models based on project requirements\nWork with datasets by preprocessing, selecting appropriate data representations, and ensuring data quality.\nPerforming statistical analysis and fine-tuning using test results.\nSupport training and retraining of ML systems as needed.\nHelp build data pipelines for collecting and processing data efficiently.",,,,"['hive', 'kubernetes', 'data pipeline', 'sql', 'docker', 'tensorflow', 'java', 'product management', 'gcp', 'spark', 'pytorch', 'bigquery', 'hadoop', 'big data', 'programming', 'hbase', 'ml', 'cloud sql', 'python', 'airflow', 'cloud spanner', 'cloud pubsub', 'machine learning', 'data engineering', 'ops', 'mapreduce', 'kafka', 'cloud storage', 'hdfs', 'bigtable', 'aws']",2025-06-13 06:07:24
Legal - Intern,Coditas Technologies,0 years,Not Disclosed,['Pune( Viman Nagar )'],"Company Introduction\n\nCoditas is an offshore product development organization comprised of passionate engineers, design thinkers, data scientists, cloud professionals, and other top industry professionals. We offer services spanning the entire length and breadth of software development, including cutting-edge technologies such as Artificial Intelligence, Machine Learning, and Generative AI. With over 200 clients worldwide, we are partners with multi-billion dollar and Fortune 500 clients such as JPMorgan Chase, BCG, KPMG, Reliance, HDFC, IDFC, SunPharma, and many more. Coditas has experienced fast-paced growth thanks to an engineering-driven culture and steadfast philosophies around writing clean code, designing intuitive user experiences, and letting the work speak for itself.\n\nRoles and Responsibilities\n\nDocument Review: Conduct thorough reviews of legal documents to ensure accuracy, compliance,\nAgreements Abstract: Summarize and extract key terms and conditions from contracts and agreements for easy reference and analysis\nData Maintenance: Maintain and update legal databases and records to ensure data integrity and accessibility for ongoing legal processes.\nDrafting of Agreements: Prepare and draft various legal agreements, ensuring they are legally sound and tailored to meet specific business needs and requirements.\nData Protection Compliance: Assist in reviewing, updating, and drafting privacy notices, data processing agreements, and internal policies to ensure alignment with DPDPA and GDPR requirements.\n\nTechnical Skills\n\nAnalytical and research skills\nSearching for information online and offline\nCreating and delivering presentations\nUnderstanding of Data Protection Laws: Basic understanding of data privacy principles under the DPDPA (India) and GDPR (EU); prior academic exposure or certifications would be an advantage\nExperience Range: 0 to 1 years\nDegree: Completed Five or Three years of Law Degree from a government-recognized University in Law.\nGood communication skills written and verbal.\nA positive attitude toward your job in general.\nKnowledge of MS Office, Google Office suites\nAptitude for numbers and strong problem-solving and analytical skills\nAbility to work effectively with people at all levels of the organization\n\n\nA full-time employment opportunity may be extended upon successful completion of the six-month internship, subject to satisfactory performance.",Industry Type: IT Services & Consulting,Department: Legal & Regulatory,"Employment Type: Full Time, Permanent","['Agreement reviewing', 'Data Privacy', 'Gdpr', 'Drafting', 'Legal Documentation', 'Contract Drafting', 'DPDPA', 'Contract Review', 'Alternative Dispute Resolution', 'Drafting Agreements']",2025-06-13 06:07:26
Fullstack Python Developer,Fia Technology Services,0 - 3 years,4-7 Lacs P.A.,['Gurugram'],"Role & responsibilities :\nDesign, develop, test, and maintain Python-based applications.\nBuild RESTful APIs and integrate with third-party services.\nWork with databases (e.g., PostgreSQL, MongoDB, MySQL) for data modeling and queries.\nWrite clean, maintainable, and well-documented code.\nCollaborate with frontend developers, DevOps, and QA teams.\nParticipate in code reviews and technical discussions.",,,,"['Python', 'Rabbit MQ', 'GIT', 'Postgresql', 'Django', 'MySQL', 'Kafka', 'Mysql Database', 'MongoDB', 'Flask']",2025-06-13 06:07:27
ELK Support,Kyndryl,0 - 4 years,Not Disclosed,['Mumbai'],"Who We Are\nAt Kyndryl, we design, build, manage and modernize the mission-critical technology systems that the world depends on every day. So why work at Kyndryl? We are always moving forward – always pushing ourselves to go further in our efforts to build a more equitable, inclusive world for our employees, our customers and our communities.\n\nThe Role\nAre you ready to dive headfirst into the captivating world of data engineering at Kyndryl? As a Data Engineer, you'll be the visionary behind our data platforms, crafting them into powerful tools for decision-makers. Your role? Ensuring a treasure trove of pristine, harmonized data is at everyone's fingertips.\n\nAs a Data Engineer at Kyndryl, you'll be at the forefront of the data revolution, crafting and shaping data platforms that power our organization's success. This role is not just about code and databases; it's about transforming raw data into actionable insights that drive strategic decisions and innovation.\n\nIn this role, you'll be engineering the backbone of our data infrastructure, ensuring the availability of pristine, refined data sets. With a well-defined methodology, critical thinking, and a rich blend of domain expertise, consulting finesse, and software engineering prowess, you'll be the mastermind of data transformation.\n\nYour journey begins by understanding project objectives and requirements from a business perspective, converting this knowledge into a data puzzle. You'll be delving into the depths of information to uncover quality issues and initial insights, setting the stage for data excellence. But it doesn't stop there. You'll be the architect of data pipelines, using your expertise to cleanse, normalize, and transform raw data into the final dataset—a true data alchemist.\n\nArmed with a keen eye for detail, you'll scrutinize data solutions, ensuring they align with business and technical requirements. Your work isn't just a means to an end; it's the foundation upon which data-driven decisions are made – and your lifecycle management expertise will ensure our data remains fresh and impactful.\n\nSo, if you're a technical enthusiast with a passion for data, we invite you to join us in the exhilarating world of data engineering at Kyndryl. Let's transform data into a compelling story of innovation and growth.\n\nYour Future at Kyndryl\nEvery position at Kyndryl offers a way forward to grow your career. We have opportunities that you won’t find anywhere else, including hands-on experience, learning opportunities, and the chance to certify in all four major platforms. Whether you want to broaden your knowledge base or narrow your scope and specialize in a specific sector, you can find your opportunity here.",,,,"['analytical', 'glue', 'data mining', 'dbms', 'data pipeline', 'analytics', 'cloud', 'elastic search', 'load', 'data modeling', 'postgresql', 'data ingestion', 'etl', 'kibana', 'mongodb', 'data lake', 'communication skills', 'data analytics', 'extraction', 'transformation', 'data engineering', 'nosql', 'dataproc', 'data bricks', 'kafka', 'aws']",2025-06-13 06:07:29
Application Lead,Accenture,5 - 10 years,Not Disclosed,['Hyderabad'],"Project Role :Application Lead\n\n\n\n\n\nProject Role Description :Lead the effort to design, build and configure applications, acting as the primary point of contact.\n\n\n\nMust have skills :PySpark\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification : Engineering graduate preferably Computer Science graduate 15 years of full time education\n\n\nSummary:As an Application Lead, you will be responsible for leading the effort to design, build, and configure applications, acting as the primary point of contact. Your typical day will involve working with PySpark and collaborating with cross-functional teams to deliver high-quality solutions.\nRoles & Responsibilities:- Lead the design, development, and deployment of PySpark-based applications, ensuring high-quality solutions are delivered on time and within budget.- Collaborate with cross-functional teams, including business analysts, data scientists, and software developers, to ensure that applications meet business requirements and are scalable and maintainable.- Act as the primary point of contact for all application-related issues, providing technical guidance and support to team members and stakeholders.- Ensure that applications are designed and developed in accordance with industry best practices, including coding standards, testing methodologies, and deployment processes.- Stay up-to-date with the latest trends and technologies in PySpark and related fields, and apply this knowledge to improve the quality and efficiency of application development.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nStrong experience in PySpark.- Good To Have\n\n\n\n\nSkills:\nExperience with other big data technologies such as Hadoop, Hive, and Spark.- Solid understanding of software development principles, including object-oriented programming, design patterns, and agile methodologies.- Experience with database technologies such as SQL and NoSQL.- Experience with cloud platforms such as AWS or Azure.- Strong problem-solving and analytical skills, with the ability to troubleshoot complex issues and provide effective solutions.\nAdditional Information:- The candidate should have a minimum of 5 years of experience in PySpark.- The ideal candidate will possess a strong educational background in computer science or a related field, along with a proven track record of delivering high-quality software solutions.- This position is based at our Bangalore, Hyderabad, Chennai and Pune Offices.- Mandatory office (RTO) for 2- 3 days and have to work on 2 shifts (Shift A- 10:00am to 8:00pm IST and Shift B - 12:30pm to 10:30 pm IST)\n\nQualification\n\nEngineering graduate preferably Computer Science graduate 15 years of full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['hive', 'software development', 'pyspark', 'spark', 'hadoop', 'visualforce', 'sfdc', 'ado.net', 'microsoft azure', 'triggers', 'sql', 'nosql', 'application development', 'apex', 'salesforce', 'sales force development', 'salesforce crm', 'data loader', 'design patterns', 'agile', 'aws', 'agile methodology']",2025-06-13 06:07:31
AI ML Technical Lead,Globallogic,8 - 13 years,Not Disclosed,['Pune'],"Role Summary: The Technical Lead/Manager will be responsible for leading the AI CoE team, driving the implementation of AI-powered solutions for the organization. They will coordinate across multiple teams, work closely with stakeholders, and oversee the technical aspects of AI/ML projects, ensuring the effective application of generative AI technologies to the product suite.\nKey Responsibilities:\nLead the AI CoE team, ensuring alignment with business objectives.\nOversee the design, development, and deployment of AI/ML models for financial use cases.",,,,"['Artificial Intelligence', 'Machine Learning', 'SQL', 'R', 'Python', 'Tensorflow', 'Natural Language Processing', 'Neural Networks', 'Scikit-Learn', 'Deep Learning', 'Pytorch', 'Data Science', 'Image Processing', 'Aiml', 'Keras', 'Computer Vision']",2025-06-13 06:07:32
Application Lead,Accenture,15 - 20 years,Not Disclosed,['Hyderabad'],"Project Role :Application Lead\n\n\n\n\n\nProject Role Description :Lead the effort to design, build and configure applications, acting as the primary point of contact.\n\n\n\nMust have skills :Microsoft Fabric\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an Application Lead, you will lead the effort to design, build, and configure applications, acting as the primary point of contact. Your typical day will involve collaborating with various teams to ensure project milestones are met, facilitating discussions to address challenges, and guiding your team through the development process. You will also engage in strategic planning to align application development with organizational goals, ensuring that the solutions provided are effective and efficient.\nRoles & Responsibilities:- Expected to be an SME.- Collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Mentor junior team members to enhance their skills and knowledge.- Facilitate regular team meetings to discuss progress and address any roadblocks.\nProfessional & Technical\n\n\n\n\nSkills:\nLead and manage a team of data engineers, providing guidance, mentorship, and support.Foster a collaborative and innovative team culture. Work closely with stakeholders to understand data requirements and business objectives.Translate business requirements into technical specifications for the Data Warehouse.Lead the design of data models, ensuring they meet business needs and adhere to best practices.Collaborate with the Technical Architect to design dimensional models for optimal performance.Design and implement data pipelines for ingestion, transformation, and loading (ETL/ELT) using Fabric Data Factory Pipeline and Dataflows Gen2.Develop scalable and reliable solutions for batch data integration across various structured and unstructured data sources.Oversee the development of data pipelines for smooth data flow into the Fabric Data Warehouse.Implement and maintain data solutions in Fabric Lakehouse and Fabric Warehouse.Monitor and optimize pipeline performance, ensuring minimal latency and resource efficiency.Tune data processing workloads for large datasets in Fabric Warehouse and Lakehouse.Exposure in ADF and DataBricks\nAdditional Information:- The candidate should have minimum 5 years of experience in Microsoft Fabric.- This position is based in Hyderabad.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['elt', 'application development', 'sql', 'data bricks', 'oracle adf', 'hive', 'python', 'oracle', 'qxdm', 'data warehousing', 'microsoft azure', 'sql server', 'plsql', 'unix shell scripting', 'spark', 'odi', 'ssrs', 'hadoop', 'etl', 'big data', 'ssis', 'aws', 'informatica', 'unix']",2025-06-13 06:07:34
Application Lead,Accenture,7 - 12 years,Not Disclosed,['Coimbatore'],"Project Role :Application Lead\n\n\n\n\n\nProject Role Description :Lead the effort to design, build and configure applications, acting as the primary point of contact.\n\n\n\nMust have skills :Data Analytics\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\nProject Role :Application Lead\n\nProject Role Description :Lead the effort to design, build and configure applications, acting as the primary point of contact. Must have skills :Retail, CPG, Supply chain, marketing data Analytics\nGood to have skills :NAMinimum 7.5 year(s) of experience is required\nEducational Qualification :Application Lead\n\n\nSummary:Candidate needs to have an outside-in view of how Retail/Consumer goods services industry is evolving to support clients in their transformation journey .As an Application Lead who is experienced in Retail and Consumer goods Data Analytics , you will lead the effort to design, build, and configure applications, acting as the primary point of contact. You will be responsible for overseeing the entire application development process and ensuring its successful implementation. Your role will involve collaborating with cross-functional teams, making key decisions, and providing solutions to problems. With your expertise in Data Retail and Consumer Goods Analytics, you will contribute to the development of innovative and efficient applications.\nRoles & Responsibilities:- Expected to be proficient in either of Retail, CPG, Supply chain, marketing data Analytics and or data engineering.\nExpected to be an SME, collaborate, and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute to key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Lead the effort to design, build, and configure applications.- Act as the primary point of contact for application-related matters.- Oversee the entire application development process.- Ensure successful implementation of applications.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Data Retail and Consumer goods and services Analytics.- Strong understanding of statistical analysis and machine learning algorithms.-Extracting, cleaning, and manipulating large datasets from various sources required for delivery- Utilizing advanced data processing techniques and data visualization tools to present the reporting in a clear and compelling manner- Hands-on implementing various machine learning algorithms such as linear regression, logistic regression, decision trees, and clustering algorithms.- Solid grasp of data munging techniques, including data cleaning, transformation, and normalization to ensure data quality and integrity.\nAdditional Information:- The candidate should have a minimum of 7.5 years of experience in Retail and Consumer goods and services Data Analytics.- This position is based at our Pune office.- An Application Lead education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['retail', 'marketing', 'cpg', 'consumer goods', 'statistics', 'python', 'data analytics', 'analytics services', 'supply chain', 'machine learning', 'data engineering', 'application development', 'sql', 'data quality', 'data visualization', 'machine learning algorithms', 'data munging']",2025-06-13 06:07:36
Scala Developer/ Lead Developer,Synechron,4 - 9 years,Not Disclosed,['Pune'],Role & responsibilities\n\nExperience Range : 4 to 8 Years.\nWork Location : Pune.\n\nCandidate should have :\nStrong Scala Skills (with a focus on the functional programming paradigm)\nSolid grasp of software development methods,,,,"['SCALA', 'Spark', 'Hadoop']",2025-06-13 06:07:38
S&C Global Network - AI - Healthcare Analytics - Consultant,Accenture,4 - 8 years,Not Disclosed,['Bengaluru'],"Job Title -\n\n\n\nS&C Global Network - AI - Healthcare Analytics - Consultant\n\n\n\nManagement Level:\n\n\n\n9-Team Lead/Consultant\n\n\n\nLocation:\n\n\n\nGurgaon/Bangalore/Mumbai\n\n\n\nMust-have skills:Phython, Spark,SQL, Tableau, Power BI\n\n\n\n\nGood to have skills:Ability to leverage design thinking, business process optimization, and stakeholder management skills.\n\n\n\nJob\n\n\nSummary:\n\nThis role involves driving strategic initiatives, managing business transformations, and leveraging industry expertise to create value-driven solutions.\n\n\n\n\nRoles & Responsibilities:\n\nProvide strategic advisory services, conduct market research, and develop data-driven recommendations to enhance business performance.\n\n\n\nWHATS IN IT FOR YOU\nConduct data wrangling and analysis on healthcare claims, provider datasets, and publicly available health data.\nDevelop predictive models using data science and AI techniques to address client needs.\nUtilize natural language processing (NLP) capabilities to extract insights from unstructured data sources.\nCollaborate with cross-functional teams to implement analytics solutions effectively.\nTranslate complex data findings into clear, concise, and actionable strategies.\n\n\n\nWhat you would do in this role\nWork with Managers to get Client's business requirements and deliver Analytics driven solution.\nDuties and Responsibilities\nSr. Data Scientist responsible for generating actionable recommendations well-supported by quantitative analysis to help our clients address their ongoing problems.\nPresent analytic findings & opportunities for improvement to senior management and summarize key findings, and aid in the dissemination of metrics throughout the organization.\nBuild knowledge base and disseminate information on applications of variety of analytical techniques.\nDevelop statistical models and delivery of analytic offerings and solutions in health domain areas.\n\n\n\n\n\nProfessional & Technical\n\n\n\n\nSkills:\n\n\n- Relevant experience in the required domain.\n\n- Strong analytical, problem-solving, and communication skills.\n\n- Ability to work in a fast-paced, dynamic environment.\n\n\n\n\nAdditional Information:\n\n- Opportunity to work on innovative projects.\n\n- Career growth and leadership exposure.\n\n\n\n\n\nAbout Our Company | Accenture\n\nQualification\n\n\n\nExperience:\n\n\n\n4-8 Years in Healthcare Analytics\n\n\n\n\nEducational Qualification:\n\n\n\nBachelor's / masters degree in computer science, statistics, applied mathematics, or a related field.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'power bi', 'sql', 'tableau', 'spark', 'natural language processing', 'business process optimization', 'artificial intelligence', 'strategic advisory services', 'strategic initiatives', 'stakeholder management', 'business transformation', 'healthcare analytics']",2025-06-13 06:07:40
Application Developer,Accenture,5 - 10 years,Not Disclosed,['Bengaluru'],"Project Role :Application Developer\n\n\n\n\n\nProject Role Description :Design, build and configure applications to meet business process and application requirements.\n\n\n\nMust have skills :Data Engineering\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an Application Developer, you will be involved in designing, building, and configuring applications to meet business process and application requirements. Your typical day will revolve around creating innovative solutions to address various business needs and ensuring seamless application functionality.\nRoles & Responsibilities:- Expected to be an SME- Collaborate and manage the team to perform- Responsible for team decisions- Engage with multiple teams and contribute on key decisions- Provide solutions to problems for their immediate team and across multiple teams- Lead the development and implementation of data engineering solutions- Optimize and maintain data pipelines for optimal performance- Collaborate with data scientists and analysts to understand data requirements\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Data Engineering- Strong understanding of ETL processes- Experience with cloud-based data platforms such as AWS or Azure- Knowledge of data modeling and database design- Experience with big data technologies like Hadoop or Spark\nAdditional Information:- The candidate should have a minimum of 5 years of experience in Data Engineering- This position is based at our Bengaluru office- A 15 years full-time education is required\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data engineering', 'spark', 'hadoop', 'aws', 'etl process', 'python', 'oracle', 'datastage', 'microsoft azure', 'data warehousing', 'sql server', 'application development', 'sql', 'plsql', 'database design', 'java', 'unix shell scripting', 'data modeling', 'leadership development', 'etl', 'informatica', 'unix']",2025-06-13 06:07:42
Google Cloud Platform (GCP) Architect,Tech Mahindra,9 - 14 years,Not Disclosed,['Bengaluru'],"Job Title: Google Cloud Platform (GCP) Architect - Vertex AI Generative AIResponsibilities:Design, implement, and maintain highly scalable cloud-based solutions using GCP services, with a focus on Vertex AI for building and deploying AI models\nCreate and maintain architectures leveraging GCP products such as BigQuery, Kubernetes Engine (GKE), Cloud Functions, Pub/Sub, and others\nWork with teams to define strategies for implementing machine learning (ML) workflows, optimizing model performance, and managing data pipelines\nUtilize Vertex AI to build end-to-end ML workflows, from data collection and preprocessing to model training, deployment, and monitoring\nLeverage Generative AI models and techniques to deliver cutting-edge AI solutions, particularly in content generation, data augmentation, or natural language processing\nExperience on Agentic AI FrameworksImplement best practices in deploying and scaling machine learning models in the cloud\nPartner with cross-functional teams (data scientists, engineers, product managers) to understand business requirements and deliver technical solutions\nDrive continuous improvement and innovation in cloud-native AI solutions\nSkills Experience: 10+ Years of IT experienceRequired:Deep Knowledge of GCP Ecosystem:Strong proficiency with GCP services including but not limited to:Vertex AI for end-to-end AI/ML lifecycle management\nBigQuery for large-scale data processing\nGoogle Kubernetes Engine (GKE) for container orchestration\nCloud Functions, Cloud Pub/Sub, Cloud Storage, and others\nExperience designing, deploying, and optimizing GCP-based architectures in production environments\nExpertise in Vertex AI:Experience using Vertex AI for building, training, and deploying machine learning models at scale\nProficiency with Vertex AI Workbench, Pipelines, and Model Monitoring\nFamiliarity with AutoML and custom model training on Vertex AI\nExperience with Generative AI:Hands-on experience with Generative AI techniques and models, such as GPT (Generative Pre-trained Transformer), GANs (Generative Adversarial Networks), or other advanced natural language models\nFamiliarity with applying Generative AI in real-world scenarios, such as content generation, AI-driven chatbots, synthetic data generation, or AI-enhanced user experiences\nExperience with Agentic AI:Experience of Agentic AI Framework (e\ng\nGoogle Agentspace)Strong Programming and Scripting Skills:Proficiency in Python (required for machine learning tasks), along with experience in ML libraries like TensorFlow, PyTorch, or Scikit-learn\nFamiliarity with SQL for querying data in BigQuery\nCloud Security and Governance:Strong understanding of cloud security practices (IAM roles, service accounts, encryption)\nExperience with GCP security tools (e\ng\n, Cloud Identity, Security Command Center)\nCertifications:Google Cloud Professional Cloud Architect or Google Cloud Professional Data Engineer certification\nVertex AI or AI/ML-related certifications\nEducation:Masters degree/PhD in Computer Science Data Science/AI ML or a related technical field or equivalent practical experience",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'orchestration', 'Machine learning', 'Data collection', 'Data processing', 'Natural language processing', 'Continuous improvement', 'Monitoring', 'SQL', 'Python']",2025-06-13 06:07:44
Architect (AI and Cloud),Rakuten Symphony,10 - 18 years,Not Disclosed,['Bengaluru( Kadubeesanahalli )'],"Why should you choose us? Rakuten Symphony is reimagining telecom, changing supply chain norms and disrupting outmoded thinking that threatens the industrys pursuit of rapid innovation and growth. Based on proven modern infrastructure practices, its open interface platforms make it possible to launch and operate advanced mobile services in a fraction of the time and cost of conventional approaches, with no compromise to network quality or security. Rakuten Symphony has operations in Japan, the United States, Singapore, India, South Korea, Europe, and the Middle East Africa region. For more information, visit: https://symphony.rakuten.com\n\nBuilding on the technology Rakuten used to launch Japans newest mobile network, we are taking our mobile offering global. To support our ambitions to provide an innovative cloud-native telco platform for our customers, Rakuten Symphony is looking to recruit and develop top talent from around the globe. We are looking for individuals to join our team across all functional areas of our business from sales to engineering, support functions to product development. Lets build the future of mobile telecommunications together!\n\nAbout Rakuten Rakuten Group, Inc. (TSE: 4755) is a global leader in internet services that empower individuals, communities, businesses and society. Founded in Tokyo in 1997 as an online marketplace, Rakuten has expanded to offer services in ecommerce, fintech, digital content and communications to approximately 1.5 billion members around the world. The Rakuten Group has over 27,000 employees, and operations in 30 countries and regions. For more information visit https://global.rakuten.com/corp/\n\nJob Summary:\nThe AI Architect is a senior technical leader responsible for designing and implementing the overall AI infrastructure and architecture for the organization. This role will define the technical vision for AI initiatives, select appropriate technologies and platforms, and ensure that AI systems are scalable, reliable, secure, and aligned with business requirements. The AI Architect will work closely with CTO Office, product manager, engineering manager, data scientists, machine learning engineers, and other stakeholders to build a robust and efficient AI ecosystem.\n\nMandatory Skills:\nCloud Computing Platforms (AWS, Azure, GCP).\nAI/ML Frameworks (TensorFlow, PyTorch, scikit-learn) .\nData Engineering Tools (Spark, Hadoop, Kafka).\nMicroservices Architecture.\nAI/ML as a service Deployment.\nDevOps Principles (CI/CD/CT).\nStrong understanding of AI/ML algorithms and techniques\n\nRoles & Responsibilities:\nDefine the overall AI architecture and infrastructure strategy for the organization.\nSelect appropriate technologies and platforms for AI development and deployment. • Design scalable, reliable, and secure AI systems.\nDevelop and maintain architectural blueprints and documentation.\nProvide technical leadership and guidance to tech lead, engineering manager, data scientists, machine learning engineers, and other stakeholders.\nEnsure that AI systems are aligned with business requirements and industry best practices. Evaluate new AI technologies and trends.\nCollaborate with security and compliance teams to ensure that AI systems meet regulatory requirements.\nCollaborate with CTO Office to ensure the AI strategy implemented aligned with overall business unit strategy.",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Architect', 'Artificial Intelligence', 'Cloud', 'Microservice Based Architecture', 'Tensorflow', 'Pytorch', 'Ai', 'Machine Learning', 'Solution Architect', 'Scikit-Learn', 'Technical Architecture']",2025-06-13 06:07:46
S&C Global Network - AI - Auto & Industrial - Consultant,Accenture,3 - 6 years,Not Disclosed,['Gurugram'],"Management Level:Ind & Func AI Decision Science Consultant\n\n\n\nLocation:Bengaluru (Bangalore), Gurugram (Gurgaon), Hyderabad, Chennai.\n\n\n\nMust-have skills:Programming languages -Python/R, Generative AI, Large Language Models (LLMs), ML libraries such as Scikit-learn, TensorFlow, Torch, Lang Chain, or OpenAI API, RAG Applications.\n\n\n\n\nGood to have skills:Big data technologies such as Spark or Hadoop,AI model explainability(XAI),bias detection and AI ethics. Familiarity with Edge AI and deploying models on embedded devices for industrial automation. Experience with Reinforcement Learning (RL) and AI-driven optimization techniques.\n\n\n\nJob\n\n\nSummary\n\nWe are looking for a Data Scientist / AI Specialist with 3-6 years of experience to join our team and work on client projects in the Automotive & Industrial sectors. This role will involve leveraging traditional Machine Learning (ML), Generative AI (GenAI), Agentic AI, and Autonomous AI Systems to drive innovation, optimize processes, and enhance decision-making in complex industrial environments.\n\nPrior experience in the Auto/Industrial industry is a plus, but we welcome candidates from any domain with a strong analytical mindset and a passion for applying AI to real-world business challenges.\n\n\n\n\nRoles & Responsibilities:\nDevelop, deploy and monitor AI/ML models in production environments & enterprise systems, including predictive analytics, anomaly detection, and process optimization for clients.\nWork with Generative AI models (e.g., GPT, Stable Diffusion, DALLE) for applications such as content generation, automated documentation, code synthesis, and intelligent assistants.\nImplement Agentic AI systems, including AI-powered automation, self-learning agents, and decision-support systems for industrial applications.\nDesign and build Autonomous AI solutions for tasks like predictive maintenance, supply chain optimization, and robotic process automation (RPA).\nWork with structured and unstructured data from various sources, including IoT sensors, manufacturing logs, and customer interactions.\nOptimize and fine-tune LLMs (Large Language Models) for specific business applications, ensuring ethical and explainable AI use.\nUtilize MOps and AI orchestration tools to streamline model deployment, monitoring, and retraining cycles.\nCollaborate with cross-functional teams, including engineers, business analysts, and domain experts, to align AI solutions with business objectives.\nStay updated with cutting-edge AI research in Generative AI, Autonomous AI, and Multi-Agent Systems.\n\n\n\n\nProfessional & Technical\n\n\n\n\nSkills:\n\n3-6 years of experience in Data Science, Machine Learning, or AI-related roles.\nProficiency in Python (preferred) or R, and experience with ML libraries such as Scikit-learn, TensorFlow, Torch, Lang Chain, or OpenAI API.\nStrong understanding of Generative AI, Large Language Models (LLMs), and their practical applications.\nHands-on experience in fine-tuning and deploying foundation models (e.g., OpenAI, Llama, Claude, Gemini, etc.).\nExperience with Vector Databases (e.g., FAISS, Chroma, Weaviate, Pinecone) for retrieval-augmented generation (RAG) applications.\nKnowledge of Autonomous AI Agents (e.g., AutoGPT, BabyAGI) and multi-agent orchestration frameworks.\nExperience working with SQL and NoSQL databases.\nFamiliarity with cloud platforms (AWS, Azure, or GCP) for AI/ML model deployment.\nStrong problem-solving and analytical thinking abilities.\nAbility to communicate complex AI concepts to technical and non-technical stakeholders.\nBonus:Experience in Automotive, Industrial, or Manufacturing AI applications (e.g., predictive maintenance, quality inspection, digital twins).\n\n\n\n\nAdditional Information:\nBachelor/Masters degree in Statistics/Economics/ Mathematics/ Computer Science or related disciplines with an excellent academic record /MBA from top-tier universities.\nExcellent Communication and Interpersonal Skills.\n\n\n\n\n\nAbout Our Company | AccentureQualification\n\n\n\nExperience:Minimum 3-6 years of relevant Data Science, Machine Learning or AI-related roles., Exposure to Industrial & Automotive Firms or Professional Services.\n\n\n\n\nEducational Qualification: Bachelor/Master degree in Statistics/Economics/ Mathematics/ Computer Science or related disciplines with an excellent academic record or MBA from top-tier universities.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'machine learning', 'sql', 'tensorflow', 'data science', 'scikit-learn', 'microsoft azure', 'communication and interpersonal skills', 'artificial intelligence', 'nosql', 'r', 'spark', 'gcp', 'supply chain optimization', 'hadoop', 'api', 'robotic process automation', 'big data', 'aws']",2025-06-13 06:07:48
S&C Global Network - AI - Auto & Industrial - Consultant,Accenture,3 - 6 years,Not Disclosed,['Bengaluru'],"Management Level:Ind & Func AI Decision Science Consultant\n\n\n\nLocation:Bengaluru (Bangalore), Gurugram (Gurgaon), Hyderabad, Chennai.\n\n\n\nMust-have skills:Programming languages -Python/R, Generative AI, Large Language Models (LLMs), ML libraries such as Scikit-learn, TensorFlow, Torch, Lang Chain, or OpenAI API, RAG Applications.\n\n\n\n\nGood to have skills:Big data technologies such as Spark or Hadoop,AI model explainability(XAI),bias detection and AI ethics. Familiarity with Edge AI and deploying models on embedded devices for industrial automation. Experience with Reinforcement Learning (RL) and AI-driven optimization techniques.\n\n\n\nJob\n\n\nSummary\n\nWe are looking for a Data Scientist / AI Specialist with 3-6 years of experience to join our team and work on client projects in the Automotive & Industrial sectors. This role will involve leveraging traditional Machine Learning (ML), Generative AI (GenAI), Agentic AI, and Autonomous AI Systems to drive innovation, optimize processes, and enhance decision-making in complex industrial environments.\n\nPrior experience in the Auto/Industrial industry is a plus, but we welcome candidates from any domain with a strong analytical mindset and a passion for applying AI to real-world business challenges.\n\n\n\n\nRoles & Responsibilities:\nDevelop, deploy and monitor AI/ML models in production environments & enterprise systems, including predictive analytics, anomaly detection, and process optimization for clients.\nWork with Generative AI models (e.g., GPT, Stable Diffusion, DALLE) for applications such as content generation, automated documentation, code synthesis, and intelligent assistants.\nImplement Agentic AI systems, including AI-powered automation, self-learning agents, and decision-support systems for industrial applications.\nDesign and build Autonomous AI solutions for tasks like predictive maintenance, supply chain optimization, and robotic process automation (RPA).\nWork with structured and unstructured data from various sources, including IoT sensors, manufacturing logs, and customer interactions.\nOptimize and fine-tune LLMs (Large Language Models) for specific business applications, ensuring ethical and explainable AI use.\nUtilize MOps and AI orchestration tools to streamline model deployment, monitoring, and retraining cycles.\nCollaborate with cross-functional teams, including engineers, business analysts, and domain experts, to align AI solutions with business objectives.\nStay updated with cutting-edge AI research in Generative AI, Autonomous AI, and Multi-Agent Systems.\n\n\n\n\nProfessional & Technical\n\n\n\n\nSkills:\n\n3-6 years of experience in Data Science, Machine Learning, or AI-related roles.\nProficiency in Python (preferred) or R, and experience with ML libraries such as Scikit-learn, TensorFlow, Torch, Lang Chain, or OpenAI API.\nStrong understanding of Generative AI, Large Language Models (LLMs), and their practical applications.\nHands-on experience in fine-tuning and deploying foundation models (e.g., OpenAI, Llama, Claude, Gemini, etc.).\nExperience with Vector Databases (e.g., FAISS, Chroma, Weaviate, Pinecone) for retrieval-augmented generation (RAG) applications.\nKnowledge of Autonomous AI Agents (e.g., AutoGPT, BabyAGI) and multi-agent orchestration frameworks.\nExperience working with SQL and NoSQL databases.\nFamiliarity with cloud platforms (AWS, Azure, or GCP) for AI/ML model deployment.\nStrong problem-solving and analytical thinking abilities.\nAbility to communicate complex AI concepts to technical and non-technical stakeholders.\nBonus:Experience in Automotive, Industrial, or Manufacturing AI applications (e.g., predictive maintenance, quality inspection, digital twins).\n\n\n\n\nAdditional Information:\nBachelor/Masters degree in Statistics/Economics/ Mathematics/ Computer Science or related disciplines with an excellent academic record /MBA from top-tier universities.\nExcellent Communication and Interpersonal Skills.\n\n\n\n\n\nAbout Our Company | AccentureQualification\n\n\n\nExperience:Minimum 3-6 years of relevant Data Science, Machine Learning or AI-related roles., Exposure to Industrial & Automotive Firms or Professional Services.\n\n\n\n\nEducational Qualification: Bachelor/Master degree in Statistics/Economics/ Mathematics/ Computer Science or related disciplines with an excellent academic record or MBA from top-tier universities.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'machine learning', 'sql', 'tensorflow', 'data science', 'scikit-learn', 'microsoft azure', 'communication and interpersonal skills', 'artificial intelligence', 'nosql', 'r', 'spark', 'gcp', 'supply chain optimization', 'hadoop', 'api', 'robotic process automation', 'big data', 'aws']",2025-06-13 06:07:49
AI Product Specialist,Capco,3 - 5 years,Not Disclosed,"['Kolkata', 'Mumbai', 'New Delhi', 'Hyderabad', 'Pune', 'Chennai', 'Bengaluru']","We are seeking a detail-oriented and strategic AI Product Specialist to support the implementation and governance of AI solutions within our HR ecosystem, primarily leveraging SAP SuccessFactors. This role is instrumental in coordinating cross-functional teams, overseeing AI feature testing, ensuring responsible AI practices, and tracking performance metrics to support data-driven HR decisions.\nKey Responsibilities:\nAI Implementation Integration:\nLead and support the rollout of AI-driven features and functionalities within SAP SuccessFactors including virtual agents, predictive analytics, and intelligent recommendations.\nCollaborate with HR, IT, and vendor teams to integrate AI capabilities aligned with business needs.\nEvaluate and advise on agentic AI capabilities and their potential integration into HR workflows.\nResponsible AI Governance\nEnsure compliance with internal and external AI governance standards (e.g., Responsible AI frameworks, country compliance).\nReview and document RAI risks, mitigation actions, and audit trails throughout the development lifecycle.\nTesting Quality Assurance\nDesign and oversee test cases for AI features, including validation of outcomes across diverse user scenarios.\nMonitor for bias, accuracy, performance, and user acceptance of AI outputs.\nCross-functional Coordination\nCollaborate with product managers, data scientists, HR stakeholders and compliance teams to ensure successful end to end implementation.\nManage communication between stakeholders to ensure AI product milestones are met.\nMetrics Reporting\nDefine and track KPIs to measure the performance and adoption of AI features.\nSupport the development and visualization of AI dashboards to provide insights on usage, fairness, and efficiency. Continuous Improvement\nServe as a liaison between business stakeholders and technical team to ensure AI implementations meet both user expectations and regulatory requirements.\nGather user feedback and partner with vendors to enhance AI functionality and user experience.\nStay current on emerging AI trends in HR tech and recommend innovations that align with business goals.",Industry Type: Banking,Department: Consulting,"Employment Type: Full Time, Permanent","['Usage', 'SAP', 'Manager Quality Assurance', 'Management consulting', 'Cross functional coordination', 'HR', 'Test cases', 'Continuous improvement', 'Financial services', 'Auditing']",2025-06-13 06:07:51
Survey Programmer,Iqvia Biotech,4 - 8 years,Not Disclosed,['Bengaluru'],"Provides high quality, on-time input to client projects in the life sciences field. Assignments range in complexity from basic analysis and problem solving to assisting in the development of more complex solutions. May serve as project leader for small teams or work streams.\n\nEssential Functions\nDevelop online survey using effective survey programming tools, viz. Decipher, Confirmit, Sawtooth etc.\nAssist in complex custom scripts using jQuery/ JavaScript\nAssists with the review and analysis of client requirements or problems and assists in the development of proposals and client solutions.\nAssists in the development of detailed documentation and specifications.\nPerforms quantitative or qualitative analyses to assist in the identification of client issues and the development of client specific solutions.\nAssists in the design/structure and completion of presentations that are appropriate to the characteristics or needs of the audience.\nDevelops, and may present, complete client deliverables within known/identified frameworks and methodologies.\nProactively develops a basic knowledge of consulting methodologies and the life sciences market through the delivery of consulting engagements and participation in formal and informal learning opportunities.\nEngagement based responsibilities are assigned and managed by Senior Consultants, Engagement Managers or Principals.\nStrong analytical and problem-solving skills with experience in data interpretation\nAbility to work in a fast-paced environment and manage multiple projects simultaneously\nQualifications\nBachelors Degree required\n4-8 years of related experience required\nWorks willingly and effectively with others in and across the organization to accomplish team goals.\nKnowledge of data processing/ analysis tools, viz. SPSS, Wincross is a good to have skill.\nKnowledge and understanding of the fundamental processes of business, their interaction, and the impact of external/internal influences on decision making, growth and decline.\nKnowledge of consulting methods, tools and techniques, related to one s functional area.\nKnowledge of current events and developments within an industry and major competitors.\nEffective time & team management skills.",Industry Type: Biotechnology,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Team management', 'jQuery', 'Analytical', 'Consulting', 'Javascript', 'Healthcare', 'Data processing', 'Clinical research', 'Life sciences', 'SPSS']",2025-06-13 06:07:52
Survey Programmer,Iqvia Biotech,4 - 8 years,Not Disclosed,['Bengaluru'],"Job Overview\nProvides high quality, on-time input to client projects in the life sciences field. Assignments range in complexity from basic analysis and problem solving to assisting in the development of more complex solutions. May serve as project leader for small teams or work streams.\n\nEssential Functions\nDevelop online survey using effective survey programming tools, viz. Decipher, Confirmit, Sawtooth etc.\nAssist in complex custom scripts using jQuery/ JavaScript\nAssists with the review and analysis of client requirements or problems and assists in the development of proposals and client solutions.\nAssists in the development of detailed documentation and specifications.\nPerforms quantitative or qualitative analyses to assist in the identification of client issues and the development of client specific solutions.\nAssists in the design/structure and completion of presentations that are appropriate to the characteristics or needs of the audience.\nDevelops, and may present, complete client deliverables within known/identified frameworks and methodologies.\nProactively develops a basic knowledge of consulting methodologies and the life sciences market through the delivery of consulting engagements and participation in formal and informal learning opportunities.\nEngagement based responsibilities are assigned and managed by Senior Consultants, Engagement Managers or Principals.\nStrong analytical and problem-solving skills with experience in data interpretation\nAbility to work in a fast-paced environment and manage multiple projects simultaneously\nQualifications\nBachelors Degree required\n4-8 years of related experience required\nWorks willingly and effectively with others in and across the organization to accomplish team goals.\nKnowledge of data processing/ analysis tools, viz. SPSS, Wincross is a good to have skill.\nKnowledge and understanding of the fundamental processes of business, their interaction, and the impact of external/internal influences on decision making, growth and decline.\nKnowledge of consulting methods, tools and techniques, related to one s functional area.\nKnowledge of current events and developments within an industry and major competitors.\nEffective time & team management skills.\n. We create intelligent connections to accelerate the development and commercialization of innovative medical treatments to help improve patient outcomes and population health worldwide . Learn more at https://jobs.iqvia.com",Industry Type: Medical Devices & Equipment,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Team management', 'jQuery', 'Analytical', 'Consulting', 'Javascript', 'Healthcare', 'Data processing', 'Clinical research', 'Life sciences', 'SPSS']",2025-06-13 06:07:54
S&C Global Network - AI - Financial Services Analytics - Consultant,Accenture,8 - 13 years,Not Disclosed,['Mumbai'],"Entity:- Accenture Strategy & Consulting\n\n\n\nTeam:- Global Network - Data & AI\n\n\n\nPractice:- Banking & Financial Services Analytics\n\n\n\nTitle:- Consultant Level 9\n\n\n\nJob location:- Bengaluru, Gurugram, Mumbai\n\n\n\nAbout S&C - Global Network Data & AI:- Accenture Global Network - Data & AI practice help our clients grow their business in entirely new ways. Analytics enables our clients to achieve high performance through insights from data - insights that inform better decisions and strengthen customer relationships. From strategy to execution, Accenture works with organizations to develop analytic capabilities - from accessing and reporting on data to predictive modelling to outperform the competition.\n\n\n\nRole Overview:- As an experienced financial services professional, your market-leading industry, management and technology expertise will provide critical solutions that answer unparalleled strategic, operational, technology and sourcing demands. And, in doing so, you'll improve the future of the global financial services industry.\n\nAccenture serves the world's leading financial services organisations across three industry groups\nCentral Banks:Monetary authorities, reserve banks across globe\nBanking :Retail and commercial banks and diversified financial institutions.\nCapital Markets :Investment banks, broker/dealers, asset & wealth management firms, depositories, exchanges, clearing & settlement organisations.\n\n\n\nWhats in it for you\nYoull be part of a diverse, vibrant, global Accenture data science community, continually pushing the boundaries of analytical capabilities\nGet to work with top financial clients globally\nBuild new skills, grow existing skills, develop new areas of expertise within functional and technical areas of business\nGet access to resources that will allow you to leverage the latest technologiesand bring innovation to life with the worlds most recognizable companies\n\n\n\n\nWhat you would do in this role\nWork closely with clients to understand their business goals and challenges.\nDevelop data strategies aligned with business objectives and industry best practices.\nCollaborate with business stakeholders to understand analytical requirements and deliver actionable insights\nIdentification of key business questions through data collection and ETL, and from performing analyses and using a wide range of statistical, machine learning, and applied mathematical techniques to delivery insights\nDevelop and implement predictive models to assess and manage financial risks, leveraging advanced data science techniques.\nBuild predictive models to assess and maximize Customer Lifetime Value, enabling targeted retention and acquisition strategies.\nHelp in responding to RFPs and design POVs\nWorking across client teams to develop and architect Generative AI solutions using ML and GenAI\nImplement data security measures to safeguard sensitive financial information and ensure adherence to industry standards.\n\nQualification\n\n\n\nWho we are looking for\n3-8+ years of experience in Data Scienece preferably with financial services clients\nBachelors or masters degree in a relevant field Computer Science / Statistics / Data Science / Econometrics / Economics / Engineering from reputed institute or MBA from Tier1 colleges\nProficiency in programming languages such as Python, R, Scala,pyspark\nPossess strong analytical skills to derive meaningful insights from complex data sets\nExperience with data visualization tools (Tableau, Power BI, etc.)\nKnowledge of Data Science and Machine Learning concepts and algorithms such as clustering, regression, classification, forecasting, hyperparameters optimization, NLP, computer vision, speech processing; understanding of ML model lifecycle would be an asset\nExperience in Supervisory analytics (like Network Analytics, IFRS9 / Basel, Risk Analytics, Balance of Payments Analytics, Licensing analytics, AML etc) is a plus\nStrong domain experience in one of the following:\nCentral banks (Monetary / Regulatory / Compliance / BASEL),\nCommercial Banking,\nAsset & Wealth management\nProven experience in one of data engineering, data governance, data science roles\nExperience in Generative AI or Central / Supervisory banking is a plus.\nExperience with any Cloud Technologies (MS Azure, GCP, AWS)\nFamiliarity with Deep Learning concepts & tools (H2O, TensorFlow, PyTorch, Keras, Theano, etc.) is a plus.\nExcellent communication and client-facing skills\nAbility to work independently and collaboratively in a team\nProject management skills and the ability to manage multiple tasks concurrently\nStrong written and oral communication skills\n\n\n\n\nAccenture is an equal opportunities employer and welcomes applications from all sections of society and does not discriminate on grounds of race, religion or belief, ethnic or national origin, disability, age, citizenship, marital, domestic or civil partnership status, sexual orientation, gender identity, or any other basis as protected by applicable law.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'commercial banking', 'tensorflow', 'wealth management', 'asset', 'ms azure', 'management skills', 'scala', 'investment banking', 'pyspark', 'power bi', 'machine learning', 'capital market', 'financial services', 'deep learning', 'r', 'tableau', 'gcp', 'pytorch', 'keras', 'aws']",2025-06-13 06:07:55
Infrastructure Consulting Practitioner,Accenture,3 - 8 years,Not Disclosed,['Bengaluru'],"Project Role :Infrastructure Consulting Practitioner\n\n\n\n\n\nProject Role Description :Support clients to design, build and run an agile, scalable IT infrastructure that is standardized and optimized to deliver operational efficiencies, improve employee and workplace performance and meet dynamic business demands. Apply infrastructure improvements across the workplace, network, data center and operations as well as provide resources to manage and run the infrastructure on a managed or capacity services basis.\n\n\n\nMust have skills :Data Engineering\n\n\n\n\nGood to have skills :Microsoft SQL Server, Python (Programming Language), Snowflake Data WarehouseMinimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an Infrastructure Consulting Practitioner, you will support clients to design, build and run an agile, scalable IT infrastructure that is standardized and optimized to deliver operational efficiencies, improve employee and workplace performance, and meet dynamic business demands. Apply infrastructure improvements across the workplace, network, data center, and operations as well as provide resources to manage and run the infrastructure on a managed or capacity services basis.Resource should be ready to work Australia shift timing\nRoles & Responsibilities:- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work-related problems.- Implement infrastructure improvements to enhance operational efficiencies.- Design and build scalable IT infrastructure solutions.- Optimize infrastructure to meet dynamic business demands.- Manage and run infrastructure on a managed services basis.- Provide resources for infrastructure operations.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Data Engineering.- Good To Have\n\n\n\n\nSkills:\nExperience with Snowflake Data Warehouse.- Strong understanding of infrastructure design and optimization.- Knowledge of network infrastructure management.- Experience in IT infrastructure scalability.\nAdditional Information:- The candidate should have a minimum of 3 years of experience in Data Engineering.- This position is based at our Bengaluru office.- A 15 years full-time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['snowflake', 'python', 'infrastructure design', 'data engineering', 'sql server', 'kubernetes', 'microsoft azure', 'networking', 'warehouse', 'docker', 'ansible', 'infrastructure architecture', 'sql', 'devops', 'linux', 'jenkins', 'aws', 'cloud computing', 'it infrastructure']",2025-06-13 06:07:57
Application Developer,Accenture,3 - 8 years,Not Disclosed,['Bengaluru'],"Project Role :Application Developer\n\n\n\n\n\nProject Role Description :Design, build and configure applications to meet business process and application requirements.\n\n\n\nMust have skills :Data Engineering\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an Application Developer, you will be responsible for designing, building, and configuring applications to meet business process and application requirements. Your typical day will involve collaborating with team members to develop innovative solutions and ensure seamless application functionality.\nRoles & Responsibilities:- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Develop and implement efficient data pipelines for data processing.- Optimize data storage and retrieval processes to enhance performance.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Data Engineering.- Strong understanding of ETL processes and data modeling.- Experience with cloud platforms such as AWS or Azure.- Knowledge of programming languages like Python or Java.\nAdditional Information:- The candidate should have a minimum of 3 years of experience in Data Engineering and flink- The candidate must have Flink knowledge.- This position is based at our Bengaluru office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['java', 'data modeling', 'data engineering', 'apache flink', 'etl process', 'web services', 'jsp', 'hibernate', 'sql', 'microservices', 'spring', 'apache', 'spring mvc', 'j2ee', 'mysql', 'etl', 'rest', 'python', 'oracle', 'data processing', 'microsoft azure', 'javascript', 'application development', 'spring boot', 'aws']",2025-06-13 06:07:59
Application Developer,Accenture,5 - 10 years,Not Disclosed,['Indore'],"Project Role :Application Developer\n\n\n\n\n\nProject Role Description :Design, build and configure applications to meet business process and application requirements.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an Application Developer, you will design, build, and configure applications to meet business process and application requirements. You will be responsible for ensuring that the applications are developed and implemented effectively to support the organization's needs. This role requires expertise in the Databricks Unified Data Analytics Platform and a strong understanding of application development principles.\nRoles & Responsibilities:- Expected to be an SME, collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Develop and maintain applications based on business requirements.- Perform code reviews and ensure adherence to coding standards.- Collaborate with cross-functional teams to gather requirements and design solutions.- Troubleshoot and debug application issues.- Create technical documentation for reference and reporting purposes.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Databricks Unified Data Analytics Platform.- Strong understanding of application development principles.- Experience in designing and building applications to meet business requirements.- Knowledge of programming languages such as Java, Python, or Scala.- Familiarity with cloud platforms such as AWS or Azure.- Experience with database technologies and SQL.- Good To Have\n\n\n\n\nSkills:\nExperience with data engineering and ETL processes.- Knowledge of containerization technologies such as Docker or Kubernetes.\nAdditional Information:- The candidate should have a minimum of 5 years of experience in Databricks Unified Data Analytics Platform.- This position is based at our Bengaluru office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'data analytics', 'application development', 'java', 'aws', 'kubernetes', 'rest', 'css', 'scala', 'microsoft azure', 'hibernate', 'data engineering', 'javascript', 'jquery', 'sql', 'docker', 'spring', 'spring boot', 'solution design', 'troubleshooting', 'code review', 'html', 'etl', 'technical documentation']",2025-06-13 06:08:01
Technology Architect,Accenture,12 - 15 years,Not Disclosed,['Bengaluru'],"Project Role :Technology Architect\n\n\n\n\n\nProject Role Description :Review and integrate all application requirements, including functional, security, integration, performance, quality and operations requirements. Review and integrate the technical architecture requirements. Provide input into final decisions regarding hardware, network products, system software and security.\n\n\n\nMust have skills :Snowflake Data Warehouse\n\n\n\n\nGood to have skills :Data EngineeringMinimum\n\n\n\n12 year(s) of experience is required\n\n\n\n\nEducational Qualification :MBA TechnologyData related specializations MCA Advanced Degrees in STEM\n\n\nSummary:As a Technology Architect, you will engage in a dynamic environment where you will review and integrate all application requirements, ensuring that functional, security, integration, performance, quality, and operations needs are met. Your day will involve collaborating with various teams to assess technical architecture requirements and providing critical input into decisions regarding hardware, network products, system software, and security measures. You will play a pivotal role in shaping the technology landscape of the organization, ensuring that all components work harmoniously to achieve business objectives.\nRoles & Responsibilities:- Expected to be an SME.- Collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Expected to provide solutions to problems that apply across multiple teams.- Facilitate knowledge sharing and mentoring within the team to enhance overall performance.- Conduct regular assessments of technology solutions to ensure alignment with business goals.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Snowflake Data Warehouse.- Good To Have\n\n\n\n\nSkills:\nExperience with Data Engineering.- Strong understanding of data modeling and ETL processes.- Experience with cloud-based data solutions and architecture.- Familiarity with data governance and compliance standards.\nAdditional Information:- The candidate should have minimum 12 years of experience in Snowflake Data Warehouse.- This position is based at our Bengaluru office.- An MBA Technology or MCA with advanced degrees in STEM is required.\n\nQualification\n\nMBA TechnologyData related specializations MCA Advanced Degrees in STEM",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['snowflake', 'warehouse', 'data modeling', 'etl', 'etl process', 'c#', 'oracle', 'system software', 'microsoft azure', 'technology architecture', 'hibernate', 'technology solutions', 'sql server', 'sql', 'microservices', 'plsql', 'spring', 'technical architecture', 'java', 'asp.net', 'j2ee', 'aws']",2025-06-13 06:08:02
"Salesforce Developer, Ads Tech",Amazon,2 - 7 years,Not Disclosed,['Bengaluru'],"Amazon Advertising operates at the intersection of e-Commerce and Advertising, offering a range of digital advertising solutions with the goal of helping our customers discover and shop for anything they want to buy. Advertising is an extension of the shopping experience and as consumers watch more video than ever, a viewing revolution is underway in how people consume video and connect with brands.\n\nThe Amazon Advertising business is growing at a fast pace and this team s mission is to apply technology to accelerate that growth through best-in-class software engineering, data engineering, and business intelligence. We are part of the Sales Technology team within Global Sales Operations team is to deliver best-in-class products and data solutions to improve Amazon Advertising Sales team effectiveness worldwide. We are building sales enablement products for Amazon Advertising front line sales team. We want to ensure that the sales team has all the information about the customer and their potential to up-sell at their fingertips and they are not wasting time in inefficiencies.\n\nAmazon Advertising is looking for a Sr.Salesforce Developer interested in revolutionizing customer relationship management (CRM) for the Ads Sales team. This role requires ownership of the end-to-end product/feature development, including customer/stakeholder engagement, requirements definition, project management/communication, and development to operate at scale. The ideal candidate has experience building CRM products and Application Programming Interfaces (API) from scratch with minimal guidance.\n\nBuild solutions in Salesforce.com leveraging Force.com, VisualForce, APEX, Lightning Web Components, Web Services, and APIs\nOwn all aspects of architecture, end-to-end design, development, security, design/development best practices, integration and deployment (CI/CD), release architecture, and automation regression testing\nExperience with integrations and knowledge of integration patterns\nSupport cross-systems integrations across native Salesforce ecosystem, as well as other cloud technologies including AWS\nPartner with Sales, Operations, Product, and external customers to deliver tailored solutions that support business requirements, create a superlative user experience, and drive tangible business value\nParticipate in the planning and analysis of requirements for system enhancements, translating requirements into well-architecture solutions that best use the Salesforce platform and products\nProactively identify opportunities to leverage Salesforce.com in innovative ways to improve existing business processes and support rapid scaling\n\n\nBuild solutions in Salesforce.com leveraging Force.com, VisualForce, APEX, Lightning Web Components, Web Services, and APIs\nOwn all aspects of architecture, end-to-end design, development, security, design/development best practices, integration and deployment (CI/CD), release architecture, and automation regression testing\nExperience with integrations and knowledge of integration patterns\nSupport cross-systems integrations across native Salesforce ecosystem, as well as other cloud technologies including AWS\nPartner with Sales, Operations, Product, and external customers to deliver tailored solutions that support business requirements, create a superlative user experience, and drive tangible business value\nParticipate in the planning and analysis of requirements for system enhancements, translating requirements into well-architecture solutions that best use the Salesforce platform and products\nProactively identify opportunities to leverage Salesforce.com in innovative ways to improve existing business processes and support rapid scaling Experience programming with at least one modern language such as Python, Ruby, Golang, Java, C++, C#, Rust\nExperience in automating, deploying, and supporting large-scale infrastructure\nExperience with Linux/Unix\nExperience with CI/CD pipelines build processes Experience with distributed systems at scale",,,,"['Unix', 'C++', 'Automation', 'Linux', 'Project management', 'Regression testing', 'Business intelligence', 'Ruby', 'CRM', 'Python']",2025-06-13 06:08:05
Engagement Manager,Fractal Analytics,10 - 15 years,Not Disclosed,['Bengaluru'],"Brief About Fractal\nFractal Analytics is Leading Fortune 500 companies leverage Big Data, analytics and technology to drive smarter, faster and more accurate decisions in every aspect of their business.\nFortune 500 companies recognize analytics is a competitive advantage to understand s and make better decisions. We deliver insight, innovation and impact to them through predictive analytics and visual storytelling.\nBrief About The Role\nAn Engagement Manager has complete ownership and accountability for successful delivery of client projects and enabling growth for a particular client account. Own client outcomes across engagements, typically managing multiple projects in parallel and cross-sell new consulting work. An engagement Manager plays a pivotal role in leading and executing large global programs for various clients and bring thought leadership to build a roadmap/analytics strategy . Their roles and responsibilities include the following:",,,,"['advanced analytics', 'Staffing', 'Analytical', 'Manager Program Management', 'Machine learning', 'Business planning', 'SOW', 'Management', 'Forecasting', 'Operations']",2025-06-13 06:08:06
AI/ML,Larsen & Toubro (L&T),2 - 4 years,Not Disclosed,"['Chennai', 'Bengaluru']","Experience Required\n\n2 to 4 years of experience in AI/ML model development, deployment, and optimization. Hands-on experience in building machine learning pipelines and working with large datasets\n\nDomain Experience (Functional)\nExperience in domains such as natural language processing (NLP), computer vision, predictive analytics, or recommendation systems. Exposure to industry-specific AI applications (e.g., healthcare, finance, retail, manufacturing) is a plus.\n\nQualification\nBachelors or Masters degree in Computer Science, Artificial Intelligence, Data Science, Mathematics, or a related field\n\nRoles & Responsibilities\nDesign, develop, and deploy machine learning and deep learning models.\nCollaborate with data engineers and domain experts to collect, clean, and preprocess data.\nConduct experiments, evaluate model performance, and iterate for improvement.\nIntegrate AI models into production systems and monitor their performance.\nStay updated with the latest research and advancements in AI/ML.\nDocument model development processes and contribute to knowledge sharing.\n\nTechnical Skills\n\nProficient in Python and core ML libraries: TensorFlow, PyTorch, Scikit-learn.\nStrong with Pandas, NumPy for data handling.\nSolid grasp of ML algorithms, statistics, and model evaluation.\nFamiliar with cloud platforms (AWS/Azure/GCP).\nExperience with Git and basic CI/CD for model deployment",Industry Type: Engineering & Construction,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Npl', 'Aiml', 'Tensorflow', 'Ci/Cd', 'Machine Learning', 'Deep Learning', 'Scikit-Learn', 'Numpy', 'Pytorch', 'GCP', 'Pandas', 'Microsoft Azure', 'AWS', 'Python']",2025-06-13 06:08:08
Sr Software Engineer (Full Stack),Johnson Controls,4 - 7 years,Not Disclosed,['Pune'],"Job Title-Senior Software Engineer (AI Engineering)\nPosting Title- Senior Software Engineer (Full Stack - AI Data Engineering)\nJob Code/Job Profile/Job Level- 172\nPreferred Locations-\nIndia (Pune)\nIntroduction\nThe future is being built today and Johnson Controls is making that future more productive, more secure and more sustainable. We are harnessing the power of cloud, AI/ML and Data analytics, the Internet of Things (IoT), and user design thinking to deliver on the promise of intelligent buildings and smart cities that connect communities in ways that make people s lives and the world better.",,,,"['Computer science', 'Object oriented design', 'Machine learning', 'Javascript', 'Engineering Manager', 'HTML', 'Scrum', 'Open source', 'Monitoring', 'Python']",2025-06-13 06:08:10
Senior AI Engineer,Reltio,5 - 10 years,Not Disclosed,['Bengaluru'],"Job Title: Senior AI Engineer\nLocation: Bengaluru, India - (Hybrid)\nAt Reltio , we believe data should fuel business success. Reltio s AI-powered data unification and management capabilities encompassing entity resolution, multi-domain master data management (MDM), and data products transform siloed data from disparate sources into unified, trusted, and interoperable data. Reltio Data Cloud delivers interoperable data where and when its needed, empowering data and analytics leaders with unparalleled business responsiveness. Leading enterprise brands across multiple industries around the globe rely on our award-winning data unification and cloud-native MDM capabilities to improve efficiency, manage risk and drive growth.\nAt Reltio, our values guide everything we do. With an unyielding commitment to prioritizing our Customer First , we strive to ensure their success. We embrace our differences and are Better Together as One Reltio. We are always looking to Simplify and Share our knowledge when we collaborate to remove obstacles for each other. We hold ourselves accountable for our actions and outcomes and strive for excellence. We Own It . Every day, we innovate and evolve, so that today is Always Better Than Yesterday . If you share and embody these values, we invite you to join our team at Reltio and contribute to our mission of excellence.\nReltio has earned numerous awards and top rankings for our technology, our culture and our people. Reltio was founded on a distributed workforce and offers flexible work arrangements to help our people manage their personal and professional lives. If you re ready to work on unrivaled technology where your desire to be part of a collaborative team is met with a laser-focused mission to enable digital transformation with connected data, let s talk!\nJob Summary:\nAs a Senior AI Engineer at Reltio, you will be a core part of the team responsible for building intelligent systems that enhance data quality, automate decision-making, and drive entity resolution at scale. You will work with cross-functional teams to design and deploy advanced AI/ML solutions that are production-ready, scalable, and embedded into our flagship data platform.\nThis is a high-impact engineering role with exposure to cutting-edge problems in entity resolution, deduplication, identity stitching, record linking, and metadata enrichment .\nJob Duties and Responsibilities:\nDesign, implement, and optimize state-of-the-art AI/ML models for solving real-world data management challenges such as entity resolution, classification, similarity matching, and anomaly detection.\nWork with structured, semi-structured, and unstructured data to extract signals and engineer intelligent features for large-scale ML pipelines.\nDevelop scalable ML workflows using Spark, MLlib, PyTorch, TensorFlow, or MLFlow , with seamless integration into production systems.\nTranslate business needs into technical design and collaborate with data scientists, product managers, and platform engineers to operationalize models.\nContinuously monitor and improve model performance using feedback loops, A/B testing, drift detection, and retraining strategies.\nConduct deep dives into customer data challenges and apply innovative machine learning algorithms to address accuracy, speed, and bias.\nActively contribute to research and experimentation efforts, staying updated with latest AI trends in graph learning, NLP, probabilistic modeling , etc.\nDocument designs and present outcomes to both technical and non-technical stakeholders , fostering transparency and knowledge sharing.\nSkills You Must Have:\nBachelor s or Master s degree in Computer Science, Machine Learning, Artificial Intelligence , or related field. PhD is a plus.\n5+ years of hands-on experience in developing and deploying machine learning models in production environments.\nProficiency in Python (NumPy, scikit-learn, pandas, PyTorch/TensorFlow) and experience with large-scale data processing tools ( Spark, Kafka, Airflow ).\nStrong understanding of ML fundamentals , including classification, clustering, feature selection, hyperparameter tuning, and evaluation metrics.\nDemonstrated experience working with entity resolution, identity graphs, or data deduplication .\nFamiliarity with containerized environments (Docker, Kubernetes) and cloud platforms (AWS, GCP, Azure)\nStrong debugging, analytical, and communication skills with a focus on delivery and impact.\nAttention to detail, ability to work independently, and a passion for staying updated with the latest advancements in the field of data science\nSkills Good to Have:\nExperience with knowledge graphs, graph-based ML, or embedding techniques .\nExposure to deep learning applications in data quality, record matching, or information retrieval .\nExperience building explainable AI solutions in regulated domains.\nPrior work in SaaS, B2B enterprise platforms , or data infrastructure companies .\nWhy Join Reltio*\nHealth Wellness:\nComprehensive Group medical insurance, including your parent,s with additional top-up options.\nAccidental Insurance\nLife insurance\nFree online unlimited doctor consultations\nAn Employee Assistance Program (EAP)\nWork-Life Balance:\n36 annual leaves, which include Sick leaves - 18, Earned Leaves - 18\n26 weeks of maternity leave, 15 days of paternity leave\nVery unique to Reltio - 01 week of additional off as recharge week every year globally\nSupport for home office setup:\nHome office setup allowance.\nStay Connected, Work Flexibly: Mobile Internet Reimbursement\nNo need to pack a lunch we ve got you covered with a free meal. And many more ..",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'metadata', 'Data management', 'Analytical', 'Debugging', 'Machine learning', 'Data processing', 'Data quality', 'Analytics', 'Python']",2025-06-13 06:08:12
Sr Staff Machine Learning Engineer,Xoom,12 - 15 years,Not Disclosed,['Bengaluru'],"Job Summary\nThis job will oversee the strategic direction and execution of machine learning projects. You will work closely with data scientists, software engineers, and product teams to enhance services through innovative AI/ML solutions. Your role will involve building scalable ML pipelines, ensuring data quality, and deploying models into production environments to drive business insights and improve customer experiences.\n\nYour Way to Impact\nAs a Sr. Staff Machine Learning Engineer, you ll be instrumental in scaling PayPal s AI capabilities through advanced fine-tuning and foundational model development. Your work will directly enhance how we serve our customers with intelligent, adaptive, and personalized experiences. From conversational agents to smart automation, the models you build will be embedded across PayPal s ecosystem, impacting millions of users globally.\nMeet Our Team\nYou ll be part of the Applied Intelligence organization, working alongside product, platform, and infrastructure teams. We power next-gen experiences through a unified AI stack accelerating time-to-value for internal users and building intuitive solutions for consumers and merchants. You ll be joining a team that values experimentation, scalability, and operational excellence in AI systems.\nJob Description\nYour Day to Day\nFine-tune and optimize advanced machine learning and foundational models.\nBuild reusable training pipelines and inference systems for production-grade deployment.\nCollaborate with cross-functional teams to integrate models into product experiences and backend workflows.\nContinuously monitor, test, and improve model performance in live environments.\nMentor junior engineers and contribute to team-wide knowledge sharing.\nParticipate in internal forums, technical design reviews, and possibly external publications.\nWhat You Need to Bring\nBachelor s degree or equivalent, with at least 12-15 years of relevant experience.\nStrong hands-on experience with ML frameworks like PyTorch, TensorFlow, or Hugging Face.\nExperience fine-tuning large models (e.g., LLMs, vision-language models) using SFT, LoRA, RLHF, etc.\nProven experience deploying ML models on cloud platforms such as AWS, GCP, or Azure.\nDeep knowledge of MLOps practices and tooling (e.g., model registries, CI/CD for ML).\nPreferred Qualification\nSubsidiary\nPayPal\nTravel Percent\n0\nFor the majority of employees, PayPals balanced hybrid work model offers 3 days in the office for effective in-person collaboration and 2 days at your choice of either the PayPal office or your home workspace, ensuring that you equally have the benefits and conveniences of both locations.\nOur Benefits\nWe have great benefits including a flexible work environment, employee shares options, health and life insurance and more. To learn more about our benefits please visit https//www.paypalbenefits.com .\nWho We Are\nClick Here to learn more about our culture and community.\nCommitment to Diversity and Inclusion\nPayPal provides equal employment opportunity (EEO) to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, pregnancy, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state, or local law. In addition, PayPal will provide reasonable accommodations for qualified individuals with disabilities. .\nBelonging at PayPal\nOur employees are central to advancing our mission, and we strive to create an environment where everyone can do their best work with a sense of purpose and belonging. Belonging at PayPal means creating a workplace with a sense of acceptance and security where all employees feel included and valued. We are proud to have a diverse workforce reflective of the merchants, consumers, and communities that we serve, and we continue to take tangible actions to cultivate inclusivity and belonging at PayPal.\nAny general requests for consideration of your skills, please Join our Talent Community .\nWe know the confidence gap and imposter syndrome can get in the way of meeting spectacular candidates. Please don t hesitate to apply.",Industry Type: Internet,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Backend', 'Automation', 'Operational excellence', 'GCP', 'Technical design', 'Diversity and Inclusion', 'Machine learning', 'Wellness', 'model development', 'Data quality']",2025-06-13 06:08:13
"Senior Software Engineer, Supply Chain and Retail Technology",Wayfair,12 - 17 years,Not Disclosed,['Bengaluru'],"Candidates for this position are preferred to be based in Bangalore, India and will be expected to\ncomply with their teams hybrid work schedule requirements.\nWe are looking for a passionate Backend Software Engineer to join the Fulfilment Optimisation team. This team builds the platforms that determine how customer orders are fulfilled, optimising for Wayfair profitability and customer delight. A big part of our work revolves around enhancing and scaling customer-facing platforms that provide fulfillment information on our websites, starting at the top of the customer funnel on the search pages all the way through orders being delivered. Throughout this customer journey, we are responsible for maintaining an accurate representation of our dynamic supply chain, determining how different products will fit into boxes, predicting how these boxes will flow through warehouses and trucks, and ultimately surfacing the information our customers need to inform their decision and the details our suppliers and carriers require to successfully execute on the promises made to our customers. We do all of this in milliseconds, thousands of times per second.\nWhat You ll Do:\nPartner with your business stakeholders to provide them with transparency, data, and resources to make informed decisions\nBe a technical leader within and across the teams you work with.\nDrive high impact architectural decisions and hands-on development, including inception, design, execution, and delivery following good design and coding practices.\nObsessively focus on production readiness for the team including testing, monitoring, deployment, documentation and proactive troubleshooting.\nIdentify risks and gaps in technical approaches and propose solutions to meet team and project goals.\nCreate proposals and action plans to garner support across the organization.\nInfluence and contribute to the team s strategy and roadmap.\nTenacity for learning - curious, and constantly pushing the boundary of what is possible.\nWe Are a Match Because You Have:\nA Bachelor s Degree in Computer Science or a related engineering discipline\nAt least 12 years of experience in a senior engineer or technical lead role.\nShould have mentored 10-12 people.\nExperience developing and designing scalable distributed systems with deep understanding of architectural and design patterns, object oriented design, modern\nprogram languages.\nExcellent communication skills and ability to work effectively with engineers, product managers, data scientists, analysts and business stakeholders.\nPassion for mentoring and leading peer engineers.\nExperience designing APIs and micro services.\nExperience working on cloud technologies specifically GCP is a plus.\nDeep understanding of data processing and data pipelines.\nCommon open source platforms, tools and framework, eg: Kafka, Kubernetes, Containerization, Java microservices, GraphQL APIs, Aerospike etc\nDesigning and developing recommendation systems and productionalizing ML models for real time decisions, large-scale data processing and event-driven systems and technologies is a plus.\n.",Industry Type: Retail,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Supply chain', 'Computer science', 'Object oriented design', 'Backend', 'Coding', 'Troubleshooting', 'Open source', 'Distribution system', 'Monitoring', 'Logistics']",2025-06-13 06:08:15
Senior AI/ML Engineer (Generative AI Expertise),Vee Healthtek,5 - 10 years,Not Disclosed,"['Salem', 'Chennai', 'Bengaluru']","Note : 5 days work from Office. Atleast one round must be F2F.\n\nJob Summary: We are seeking a highly skilled Senior AI/ML Engineer with extensive experience in Generative AI to lead the design, implementation, and optimization of advanced AI/ML solutions. The ideal candidate will serve as a subject matter expert (SME), mentor a team of AI/ML specialists and collaborating with cross-functional teams to deliver impactful solutions.\n\nMust-Have Skills:",,,,"['Pytorch', 'Tensorflow', 'Generative Ai', 'Pandas', 'Numpy', 'Keras']",2025-06-13 06:08:16
AI ML Engineer,Globallogic,5 - 10 years,Not Disclosed,['Pune'],"reliability\nRole Summary: The AI/ML Engineers will be responsible for developing, testing, and deploying machine learning models and AI algorithms that can be integrated into the product suite. The ideal candidate will have expertise in integrating large language models (LLMs) with external knowledge bases, fine-tuning model architectures for specific tasks, and optimizing retrieval strategies to improve the accuracy and relevance of generated content.\nKey Responsibilities:",,,,"['Artificial Intelligence', 'Machine Learning', 'R', 'Python', 'Tensorflow', 'Natural Language Processing', 'Scikit-Learn', 'SQL', 'Pytorch', 'Huggingface', 'Large Language Model', 'Opencv', 'Keras', 'Spacy']",2025-06-13 06:08:18
"Staff Engineer, Nodejs",Nagarro,7 - 12 years,Not Disclosed,[],"Total Experience 7+ years.\nExcellent knowledge developing scalable and highly available Restful APIs using NodeJS technologies.\nThorough understanding of React.js and its core principles and experience with popular React.js workflows (such as Flux or Redux or Context API or Data Structures).\nFamiliarity with common programming tools such as RESTful APIs, TypeScript, version control software, and remote deployment tools, CI/CD tools.\nUnderstanding of linter libraries (TSLINT, Prettier etc) and Unit testing using Jest, Enzyme, Jasmine or equivalent framework.\nStrong proficiency in JavaScript, including DOM manipulation and the JavaScript object model. Proficient with the latest versions of ECMAScript (JavaScript or TypeScript).\nUnderstanding of containerization, experienced in Dockers, Kubernetes.\nExposed to API gateway integrations like 3Scale.\nUnderstanding of Single-Sign-on or token-based authentication (Rest, JWT, OAuth).\nPossess expert knowledge of task/message queues include but not limited to: AWS, Microsoft Azure, Pushpin. and Kafka.\nPractical experience with GraphQL is good to have.\nWriting tested, idiomatic, and documented JavaScript, HTML and CSS.\nExperiencing in Developing responsive web-based UI.\nHave experience on Styled Components, Tailwind CSS, Material UI and other CSS-in-JS techniques.\nProblem-solving mindset with the ability to tackle complex data engineering challenges.\nStrong communication and teamwork skills, with the ability to mentor and collaborate effectively.\nRESPONSIBILITIES:\nWriting and reviewing great quality code\nUnderstanding the clients business use cases and technical requirements and be able to convert them in to technical design which elegantly meets the requirements\nMapping decisions with requirements and be able to translate the same to developers.\nIdentifying different solutions and being able to narrow down the best option that meets the clients requirements.\nDefining guidelines and benchmarks for NFR considerations during project implementation\nWriting and reviewing design document explaining overall architecture, framework, and high-level design of the application for the developers.\nReviewing architecture and design on various aspects like extensibility, scalability, security, design patterns, user experience, NFRs, etc, and ensure that all relevant best practices are followe'd.\nDeveloping and designing the overall solution for defined functional and non-functional requirements; and defining technologies, patterns, and frameworks to materialize it\nUnderstanding and relating technology integration scenarios and applying these learnings in projects\nResolving issues that are raised during code/review, through exhaustive systematic analysis of the root cause, and being able to justify the decision taken.\nCarrying out POCs to make sure that suggested design/technologies meet the requirements\n\n\nBachelor s or master s degree in computer science, Information Technology, or a related field.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'DOM', 'Version control', 'Architecture', 'Javascript', 'Data structures', 'HTML', 'Unit testing', 'High level design', 'Information technology']",2025-06-13 06:08:20
"Staff Engineer, Mobile -Flutter",Nagarro,7 - 12 years,Not Disclosed,[],"Total Experience 7+ years.\nStrong working experience in Dart and Flutter.\nSolid understanding of Mobile App Architecture (MVVM, BLoC, Provider, GetX).\nExperience with local databases like Sqflite or alternatives.\nHands-on experience in unit testing and test automation for Flutter apps.\nProven experience in building and deploying apps to the App Store and Google Play Store.\nFamiliarity with Git, GitHub/GitLab, CI/CD tools (eg, Jenkins, Bitrise, GitHub Actions).\nDeep knowledge of mobile app lifecycle, design principles, and clean architecture patterns (MVVM, MVC, etc)\nExpertise in mobile app performance optimization and security best practices.\nExperience in API integration , RESTful services, and handling JSON data.\nProficient in Version Control Systems like Git.\nProblem-solving mindset with the ability to tackle complex data engineering challenges.\nStrong communication and teamwork skills, with the ability to mentor and collaborate effectively.\nRESPONSIBILITIES:\nWriting and reviewing great quality code\nUnderstanding the clients business use cases and technical requirements and be able to convert them into technical design which elegantly meets the requirements\nMapping decisions with requirements and be able to translate the same to developers.\nIdentifying different solutions and being able to narrow down the best option that meets the clients requirements.\nDefining guidelines and benchmarks for NFR considerations during project implementation\nWriting and reviewing design document explaining overall architecture, framework, and high-level design of the application for the developers.\nReviewing architecture and design on various aspects like extensibility, scalability, security, design patterns, user experience, NFRs, etc, and ensure that all relevant best practices are followe'd.\nDeveloping and designing the overall solution for defined functional and non-functional requirements; and defining technologies, patterns, and frameworks to materialize it\nUnderstanding and relating technology integration scenarios and applying these learnings in projects\nResolving issues that are raised during code/review, through exhaustive systematic analysis of the root cause, and being able to justify the decision taken.\nCarrying out POCs to make sure that suggested design/technologies meet the requirements\n\n\nBachelor s or master s degree in computer science, Information Technology, or a related field.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'github', 'Version control', 'GIT', 'MVVM', 'JSON', 'MVC', 'Unit testing', 'High level design', 'Information technology']",2025-06-13 06:08:21
Support Engineer,Amazon,2 - 7 years,Not Disclosed,['Hyderabad'],"We support a fast-paced environment where each day brings new challenges and opportunities.\nAs a Support Engineer, you will play a pivotal role in ensuring the stability, compliance, and operational excellence of our enterprise Data Warehouse (DW) environment.\nIn this role, you will be responsible for monitoring and maintaining production data pipelines, proactively identifying and resolving issues that impact data quality, availability, or timeliness.\nYou ll collaborate closely with data engineers and cross-functional teams to troubleshoot incidents, implement scalable solutions, and enhance the overall resilience of our data infrastructure.\nA key aspect of this role involves supporting our data compliance and governance initiatives, ensuring systems align with internal policies and external regulatory standards such as GDPR.\nYou will help enforce access controls, manage data retention policies, and support audit readiness through strong logging and monitoring practices.\nYou ll also lead efforts to automate manual support processes, improving team efficiency and reducing operational risk.\nAdditionally, you will be responsible for maintaining clear, up-to-date documentation and runbooks for operational procedures and issue resolution, promoting consistency and knowledge sharing across the team.\nWe re looking for a self-motivated, quick-learning team player with a strong sense of ownership and a can-do attitude, someone who thrives in a dynamic, high-impact environment and is eager to make meaningful contributions to our data operations.\n2+ years of software development, or 2+ years of technical support experience\nExperience scripting in modern program languages\nExperience troubleshooting and debugging technical systems Knowledge of web services, distributed systems, and web application development\nExperience troubleshooting maintaining hardware software RAID\nExperience with REST web services, XML, JSON",,,,"['XML', 'Debugging', 'Machine learning', 'Agile', 'JSON', 'Distribution system', 'Technical support', 'Monitoring', 'Product support', 'Auditing']",2025-06-13 06:08:23
Support Engineer,Amazon,2 - 7 years,Not Disclosed,['Hyderabad'],"We support a fast-paced environment where each day brings new challenges and opportunities.\nAs a Support Engineer, you will play a pivotal role in ensuring the stability, compliance, and operational excellence of our enterprise Data Warehouse (DW) environment.\nIn this role, you will be responsible for monitoring and maintaining production data pipelines, proactively identifying and resolving issues that impact data quality, availability, or timeliness.\nYou ll collaborate closely with data engineers and cross-functional teams to troubleshoot incidents, implement scalable solutions, and enhance the overall resilience of our data infrastructure.\nA key aspect of this role involves supporting our data compliance and governance initiatives, ensuring systems align with internal policies and external regulatory standards such as GDPR.\nYou will help enforce access controls, manage data retention policies, and support audit readiness through strong logging and monitoring practices.\nYou ll also lead efforts to automate manual support processes, improving team efficiency and reducing operational risk.\nAdditionally, you will be responsible for maintaining clear, up-to-date documentation and runbooks for operational procedures and issue resolution, promoting consistency and knowledge sharing across the team.\nWe re looking for a self-motivated, quick-learning team player with a strong sense of ownership and a can-do attitude, someone who thrives in a dynamic, high-impact environment and is eager to make meaningful contributions to our data operations.\n2+ years of software development, or 2+ years of technical support experience\nBachelors degree in engineering or equivalent\nExperience troubleshooting and debugging technical systems\nExperience scripting in modern program languages\nExperience with SQL databases (querying and analyzing) Good to have experience with AWS technologies stack including Redshift, RDS, S3, EMR or similar solutions build around Hive/Spark etc\nGood to have experience with reporting tools like Tableau, OBIEE or other BI packages.\nKnowledge of software engineering best practices across the development lifecycle is a plus",,,,"['Debugging', 'Machine learning', 'Agile', 'Data quality', 'Technical support', 'Monitoring', 'Product support', 'Reporting tools', 'SQL', 'Auditing']",2025-06-13 06:08:25
Business Analytics Lead Analyst,Cigna Medical Group,5 - 8 years,Not Disclosed,['Hyderabad'],"The job profile for this position is Business Analytics Lead Analyst. The Customer Experience & Operations Enablement Analytics organization offers solutions that provide data, reporting, and actionable insights to internal/external business partners to improve customer experience, reduce cost, measure business performance, and inform business decisions. The Business Analytics Lead Analyst will be responsible for dashboard and report creation as well as the ability to pull data to meet adhoc measurement needs. The individual will be able to create prototypes of reporting needs, and support manual report/scorecard creation where needed when automated dashboards are not feasible . The analytics lead analyst will be comfortable working directly with the Operations teams to learn about their process and where the data and reporting fits in. Looking for candidates that can work directly with operations team members to understand requirements and do their own development and testing.\nResponsibilities Include:\nUsing SQL to write queries to answer questions and perform ETL tasks to create datasets.\nUtilizing Tableau or other similar Data Visualization tools to automate scorecards and reports\nUsing Business Intelligence tools to create self-service reporting for business partners.\nConducting self-driven data exploration and documentation of tables, schemas, and tests.\nUsing SQL to query data structures to help inform our business partners.\nExamining and interpreting the data to discover the weaknesses and identify the root causes\nCompleting ad hoc requests for business partners data needs.\nIdentifying and implementing automation to consolidate similar or repeated ad hoc requests.\nUnderstanding business needs to better inform reporting and analytics duties.\nGiving guidance on any recurring problems or issues\nCompleting proposals in cooperation and conjunction with experts on the subject (SME).\nRefactoring reporting to enhance performance, provide deeper insight, and answer questions.\nUpdating project documents as well as status reports.\nQualifications:\nRequired experience:\n5 -8 years of relevant analytics experience with focus on\nProficiency with Structured Query Language (SQL) and Oracle.\nExperience with Business Intelligence Software (Tableau, PowerBI , Looker, etc.)\n3-5 years of experience with:\nScripting language (Python, Powershell , VBA).\nBig Data Platforms (Databricks, Hadoop, AWS).\nExcellent verbal, written and interpersonal communication skills a must .\nProblem-solving, consulting skills, teamwork, leadership, and creativity skills a must .\nAnalytical mind with outstanding ability to collect and analyze data.\nExpertise in contact center or workforce planning operations preferred.\nProficiency in Agile practices (Jira) preferred.",Industry Type: Medical Services / Hospital,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Automation', 'Business analytics', 'Analytical', 'Pharmacy', 'Consulting', 'Workforce planning', 'Agile', 'Oracle', 'Business intelligence', 'SQL']",2025-06-13 06:08:27
Business Analyst,Kuku Fm,1 - 3 years,Not Disclosed,['Bengaluru'],"About the Role\nWe are looking for a data-driven Business Analyst to join our high-impact team. As a key enabler of data-backed decisions, you will work closely with cross-functional stakeholders product, marketing, operations, and leadership to drive insights and strategic initiatives in a fast-paced tech environment. This role demands strong analytical capabilities, an ownership mindset, and the ability to translate complex data into actionable business recommendations.\nResponsibilities:\nClosely work with business leaders and founders office to discover insights and solve high-impact problems across business verticals using data.\nLead day-to-day analysis requirements and present insights in a structured form.\nBe on top of data to help develop actionable insights from weekly / monthly business performance reviews.\nCollaborate across different teams to reach the root of a problem/insight.\nLeverage multiple sources of information (primary data analysis, qualitative research, etc.) to generate deeper insights.\nDesign, run and measure experiments to test the business hypotheses in collaboration with Product and Engineering teams.\nMonitor and forecast key business metrics regularly.\nRequirements:\nYou have 1-3 years of experience in high-growth tech startups, management consulting, PE/VC funds.\nYou have hands-on experience of using SQL on a daily basis to extract data and are comfortable with excel.\nYou have hands-on experience in building dashboards on visualization platforms like Tableau, Power BI, periscope data, cluvio etc.\nBreathe data and have exceptional problem-solving and presentation skills.\nPreferably have some basic knowledge of python / R (not a deal-breaker).\n\nWhy Join Us?\nOpportunity to work in a fast-growing audio and content platform.\nExposure to multi-language marketing and global user base strategies.\nA collaborative work environment with a data-driven and innovative approach.\nCompetitive salary and growth opportunities in marketing and growth strategy.\nAbout KUKU\nFounded in 2018, KUKU is India s leading storytelling platform, offering a vast digital library of audio stories, short courses, and microdramas. KUKU aims to be India s largest cultural exporter of stories, culture and history to the world with a firm belief in Create In India, Create For The World .\nWe deliver immersive entertainment and education through our OTT platforms: Kuku FM, Guru, Kuku TV, and more. With a mission to provide high-quality, personalized stories across genres from entertainment across multiple formats and languages, KUKU continues to push boundaries and redefine India s entertainment industry.\nWebsite: www.kukufm.com\nAndroid App: Google Play\niOS App: App Store\nLinkedIn: KUKU\nReady to make an impact? Apply now!",Industry Type: Film / Music / Entertainment,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Qualitative research', 'Data analysis', 'Analytical', 'Management consulting', 'IOS app', 'Marketing operations', 'Product marketing', 'SQL', 'Android', 'Python']",2025-06-13 06:08:29
PCI Analyst,Yum! India,6 - 8 years,Not Disclosed,[],"PCI Analyst\n\nLocation-Remote\nShift Timings- 11:00 AM IST to 8:00 PM IST\n\nThe PCI Analyst plays a critical role in ensuring PCI DSS compliance across assigned divisions. This role partners with BISO teams and markets to gather, review, and submit evidence required for PCI assessments. The PCI Analyst maintains ongoing compliance readiness and contributes to the overall compliance strategy by collaborating with assessors and internal teams.\n\nRole & responsibilities\nAssist in planning, coordinating, and executing PCI DSS assessments and audits, including internal assessments and third-party audits.\nPartner with Brand/Market & BISO teams to gather, validate, and submit evidence for PCI assessments.\nWork to complete PCI DSS Service Provider and/or Merchant assessments in global markets, including but not limited to the UK, US, and Israel.\nEnsure ongoing PCI DSS compliance readiness across assigned divisions.\nCollaborate with internal teams to respond promptly and effectively to assessor requests.\nProvide guidance on PCI DSS requirements and evidence collection processes.\nStay informed of changes in PCI standards and industry best practices and communicate relevant updates to internal stakeholders.\nAssist with training and awareness initiatives related to payment card security and compliance.\n\nPreferred candidate profile\n\nCertification as a PCI DSS QSA, AQSA, or ISA.\nBachelors degree in Cyber security, Compliance, or a related field.\n6-8 years of hands-on experience in PCI compliance or related roles.\nIn-depth knowledge of PCI DSS requirements and compliance frameworks.\nStrong organizational and collaboration skills, with a focus on accuracy and efficiency.\nExperience supporting PCI DSS assessments (SAQ or ROC).\nKnowledge of security controls and technologies (e.g., firewalls, encryption, vulnerability scanning).\nIndustry certifications such as CISA, CISSP, PCI ISA, or CompTIA Security+",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['PCI DSS', 'Cyber Security', 'Compliance', 'ISA', 'Governance', 'audit', 'QSA']",2025-06-13 06:08:30
Business Analyst,TechStar Group,7 - 10 years,8-14 Lacs P.A.,"['Navi Mumbai', 'Bengaluru', 'Mumbai (All Areas)']","Job Description:\nBusiness Analyst - 7 to 10years' experience in Capital markets or global treasury.\nKnowledge of capital markets & financial instruments Equities, Derivatives, Fixed Income, FX.\nUnderstanding of Investment Banking and Asset Management Front, Middle & Back Office functions.\nAsset management experience of Trading Compliance rule coding & testing is strongly preferred, but other experience within Asset Management industry is acceptable Financial industry knowledge within investments and distribution highly desired (i.e., understanding of investment process, capital markets, fixed income, equities Sound knowledge on Database SQL querying.\nShould be a strong team player Excellent communication skills - written & verbal.\nShould be able to create good documentation and correspond on functionalities & issues concisely.\nShould be able to articulate well during discussions.\nShould be able to work with tight deadlines Confident of interacting with business users and various stakeholders.\n\nRole & responsibilities\n\nShould be able to work with tight deadlines • Confident of interacting with business users and various stakeholders.\nSkilled at using MS Excel, Word, PowerPoint & Visio.\n\n\n\nPreferred candidate profile",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Capital Market', 'Investment Banking', 'Asset Management', 'Derivatives', 'Fixed Income', 'Treasury', 'SQL']",2025-06-13 06:08:32
Quality Assurance - ETL QA Engineer - Lead,Kearney-Cervello,6 - 10 years,Not Disclosed,['Bengaluru'],"About the Role:\nAs a Lead ETL QA Engineer, you will drive the QA strategy and execution for a major data pipeline modernization initiative on Azure and Snowflake. This role requires a deep understanding of data quality frameworks, test planning, and stakeholder engagement. The candidate should possess leadership capabilities and be hands-on with SQL and automation.\n\nThe job responsibilities are as follows:",,,,"['Azure Data Factory', 'Quality Assurance', 'ETL Testing', 'SQL', 'Database Testing', 'Sql Database Testing']",2025-06-13 06:08:34
Quantexa / Associate Director,Hsbc,10 - 15 years,Not Disclosed,['Pune'],"Some careers shine brighter than others.\nIf you re looking for a career that will help you stand out, join HSBC and fulfil your potential. Whether you want a career that could take you to the top, or simply take you in an exciting new direction, HSBC offers opportunities, support and rewards that will take you further.\nHSBC is one of the largest banking and financial services organisations in the world, with operations in 64 countries and territories. We aim to be where the growth is, enabling businesses to thrive and economies to prosper, and, ultimately, helping people to fulfil their hopes and realise their ambitions.\nWe are currently seeking an experienced professional to join our team in the role of Lead Consultant Specialist.\nIn this role, you will:\nThe role requires a strong leader to oversee the solution design and implementation of change whilst at the same time ensuring production is resilient and performant. This includes interaction with the bank s architects and other systems and technical teams, end users and stakeholders.\nThe person is expected to oversee and guide the day-to-day activities of the technical team, with the help of his more experienced colleagues, ensure the team follows good practice etc. In addition, this person will be able to suggest and plan best technical solutions, undertake problem solving, etc balancing pragmatism vs long tern best practice. Thus role includes opportunities for hands on development and analysis and not just team management.\nRole covers a mix of change and run responsibilities. Current team size is around 45 people located in UK, India, China, Poland and Mexico but mostly India.\nShould be flexible in working hours, ready to work in shift and On call.\nRequirements\nTo be successful in this role, you should meet the following requirements:\nBackground in hands-on technical development, with at least 10+ years of industry experience in a data engineering or engineering equivalent managed a team developers\nStrong emotional intelligence, able to work professionally under pressure.\nHave the gravitas to represent the platform in senior meetings\nStrong communication skills, with the ability to convey technical detail in a non technical language\nNeeds to be a practitioner and proponent of Agile and Dev Ops\nProficiency in Hadoop, Spark, Scala, Python, or a programming language associated with data engineering.\nExpertise building and deploying production level data processing batch systems maintained by application support teams.\nExperience with a variety of modern development tooling (e.g. Git, Gradle, Nexus) and technologies supporting automation and DevOps (e.g. Jenkins, Docker)\nExperience working in an Agile environment\nA strong technical communication ability with demonstrable experience of working in rapidly changing client environments.\nKnowledge of testing libraries of common programming languages (such as ScalaTest or equivalent). Knows the difference between different test types (unit test, integration test) and can cite specific examples of what they have written themselves.\nGood understanding of CDP 7.1, HDFS filesystems, Unix, Unix Shell Scripting, Elasticsearch\nExperience in understanding and analyzing complex business requirements and carry out the system design accordingly.\nQuantexa Data Engineering Certification (Preferred)\nExperience in managed services AWS EKS, Azure EKS (Preferred)\nExperience in Angular (Preferred)\nMicroservices (OCP, Kubernetes) (Preferred)",Industry Type: Consumer Electronics & Appliances,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Automation', 'Application support', 'Team management', 'Managed services', 'GIT', 'Agile', 'System design', 'Unix shell scripting', 'Financial services', 'Python']",2025-06-13 06:08:35
Scala/Spark Developer - Chennai,Photon,8 - 9 years,Not Disclosed,"['Kolkata', 'Mumbai', 'New Delhi', 'Hyderabad', 'Pune', 'Chennai', 'Bengaluru']","Develop, test, and deploy data processing applications using Apache Spark and Scala.\nOptimize and tune Spark applications for better performance on large-scale data sets.\nWork with the Cloudera Hadoop ecosystem (e.g., HDFS, Hive, Impala, HBase, Kafka) to build data pipelines and storage solutions.\nCollaborate with data scientists, business analysts, and other developers to understand data requirements and deliver solutions.",,,,"['cloudera', 'Version control', 'spark', 'Postgresql', 'Data processing', 'Application development', 'data integrity', 'Apache', 'Analytics', 'SQL']",2025-06-13 06:08:37
CE4 WIRELESS RB,Zensar,5 - 8 years,Not Disclosed,"['Kolkata', 'Mumbai', 'New Delhi', 'Hyderabad', 'Pune', 'Chennai', 'Bengaluru']","d Skills (Good to have as value add to this role) o Python and API o Ekahau Pro/ sidekick usage o ISE deployment and configuration experience Education &/ Additional Cortication s o Typically requires BSc Computer Science or equivalent plus 5-8 years of relevant work experience. Advanced degree strongly preferred. o Cisco certifications (CCNP, CCIE etc.) would be highly desirable.\nd Skills (Good to have as value add to this role) o Python and API o Ekahau Pro/ sidekick usage o ISE deployment and configuration experience Education &/ Additional Cortication s o Typically requires BSc Computer Science or equivalent plus 5-8 years of relevant work experience. Advanced degree strongly preferred. o Cisco certifications (CCNP, CCIE etc.) would be highly desirable.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Wireless', 'Computer science', 'Usage', 'Ccie', 'Deployment', 'cisco', 'CCNP', 'Python']",2025-06-13 06:08:39
Python Fullstack Developer,CGI,2 - 5 years,Not Disclosed,['Bengaluru'],"Role & responsibilities\nExcellent Knowledge & Understanding of Python, Pandas and Oracle, SQL.\nGood Knowledge & Understanding of data modelling.\nFlair for learning new tools & technology.\n2-5 years of experience in Banking IT, with a good understanding of the Corporate and Institutional\nBanking activity. Knowledge of Capital Markets an asset.\nGood knowledge and understanding of Windows Batch or PowerShell scripting.\nGood knowledge in systems, application frameworks, database optimization, and experience being\nresponsible for the success of software development projects.\nProven record interpreting and fulfilling requirements by developing high performing, scalable and\nmaintainable solutions with multiple technologies.\nHands-on experience with SDLC methodologies and best practices including Waterfall Process, Agile\nmethodologies, deployment automation, code reviews, and test-driven development.\nStrong coordination and organizational skills\nExcellent communication skills and multi-tasking capabilities.\nBeing aware of new technologies and frameworks.\nExperience and knowledge of Python language features like basic data types, functions with keywords\nargs, default args, variable length args.\nExperience in using Python database API with any relational database like Oracle/SQL Server using pure\nPython or native database driver.\nExperience in using built in collection types [for example: sequence types, dict, sets].\nExperience in using text manipulation facilities like regex, string methods from the standard library.\nExperience in using file and directory manipulation modules like paths, globs from the standard library.\nKnowledge of widely used libraries in data science like NumPy, Pandas is a must.\nVisualization is a good to have but not on the mandatory path.\nKnowledge and Experience in using collections from the collections module will be good to have though\nnot mandatory.\nFamiliarity with date manipulation using modules like datetime.\n•\nKnowledge of Generators will be good to have though not mandatory\n\nPreferred candidate profile",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Django', 'Pandas', 'Oracle', 'Python', 'SQL']",2025-06-13 06:08:40
Principal Machine Learning Engineer,Xoom,15 - 18 years,Not Disclosed,['Bengaluru'],"Job Summary\nWhat you need to know about the role\n\nThis job will drive the strategic vision and development of cutting-edge machine learning models and algorithms to solve complex problems. You will work closely with data scientists, software engineers, and product teams to enhance services through innovative AI/ML solutions. Your role will involve building scalable ML pipelines, ensuring data quality, and deploying models into production environments to drive business insights and improve customer experiences.\n\nYour Way to Impact\n\nAs a Principal Machine Learning Engineer, you ll lead mission-critical initiatives that define PayPal s AI edge from large-scale model fine-tuning to the architecture of foundational AI systems. Your leadership will enable AI-native capabilities across personalization, fraud detection, customer experience, and internal productivity. You will shape how PayPal delivers trust, speed, and intelligence in every user interaction.\n\nMeet Our Team\nYou ll work within the core Applied Intelligence team, a cross-functional hub driving AI-first innovation across PayPal s product and platform landscape. Your team s work powers smarter workflows and more seamless experiences for our global customer base. This is a hands-on leadership role where you ll help align strategy, technology, and business outcomes.\nJob Description\nYour Day to Day\nDefine and drive strategic vision for model development and ML applications across business domains.\nLead architecture and experimentation for foundational model pipelines.\nManage end-to-end lifecycle from data prep and training to deployment and monitoring.\nCollaborate with product, infra, and engineering leaders to ship impactful solutions.\nGuide model evaluation frameworks, bias detection, and performance monitoring practices.\nMentor technical leads and contribute to thought leadership internally and externally.\nWhat You Need to Bring\nMinimum of 15 years of relevant experience with a Bachelor s degree or equivalent.\nDeep expertise in building and fine-tuning advanced ML models at scale.\nStrong experience with cloud-native ML solutions (e.g., SageMaker, Vertex AI).\nProven success in leading multi-functional ML projects from research to production.\nStrong communication and strategic planning abilities to align tech with business.\nPreferred Qualification\nSubsidiary\nPayPal\nTravel Percent\n0\nFor the majority of employees, PayPals balanced hybrid work model offers 3 days in the office for effective in-person collaboration and 2 days at your choice of either the PayPal office or your home workspace, ensuring that you equally have the benefits and conveniences of both locations.\nOur Benefits\nWe have great benefits including a flexible work environment, employee shares options, health and life insurance and more. To learn more about our benefits please visit https//www.paypalbenefits.com .\nWho We Are\nClick Here to learn more about our culture and community.\nCommitment to Diversity and Inclusion\nPayPal provides equal employment opportunity (EEO) to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, pregnancy, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state, or local law. In addition, PayPal will provide reasonable accommodations for qualified individuals with disabilities. .\nBelonging at PayPal\nOur employees are central to advancing our mission, and we strive to create an environment where everyone can do their best work with a sense of purpose and belonging. Belonging at PayPal means creating a workplace with a sense of acceptance and security where all employees feel included and valued. We are proud to have a diverse workforce reflective of the merchants, consumers, and communities that we serve, and we continue to take tangible actions to cultivate inclusivity and belonging at PayPal.\nAny general requests for consideration of your skills, please Join our Talent Community .\nWe know the confidence gap and imposter syndrome can get in the way of meeting spectacular candidates. Please don t hesitate to apply.",Industry Type: Internet,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Architecture', 'User interaction', 'Diversity and Inclusion', 'Machine learning', 'Strategic planning', 'Manager Technology', 'Wellness', 'Data quality', 'Customer experience', 'Fraud detection']",2025-06-13 06:08:42
Machine Learning Ops Engineer,Quadrangle,6 - 11 years,Not Disclosed,"['Pune', 'Bengaluru', 'Delhi / NCR']","Location: Bangalore/Noida/Pune/Gurgaon\nEducation: B.E. / B. Tech / M.E. / M. Tech / MCA\nJob Responsibilities:\nModel Deployment and Management:\nDrive ML prototypes into production ensuring seamless deployment and management on cloud at scale.\nMonitor real-time performance of deployed models, analyze data, and proactively address performance issues.\nTroubleshoot and resolve production issues related to ML model deployment, performance, and scalability.\nCollaboration and Integration:\nCollaborate with DevOps engineers to manage cloud compute resources for ML model deployment and performance optimization.\nWork closely with ML scientists, software engineers, data engineers, and other stakeholders to implement best practices for MLOps, including CI/CD pipelines, version control, model versioning, and automated deployment.\nInnovation and Continuous Improvement:\nStay updated with the latest advancements in MLOps technologies and recommend new tools and techniques.\nContribute to the continuous improvement of team processes and workflows.\nShare knowledge and expertise to promote a collaborative learning environment.\nDevelopment and Documentation:\nBuild software to run and support machine-learning models.\nDevelop and maintain documentation, standard operating procedures, and guidelines related to MLOps processes.\nParticipate in fast iteration cycles and adapt to evolving project requirements.\nBusiness Solutions and Strategy:\nPropose solutions and strategies to business challenges.\nCollaborate with Data Science team, Front End Developers, DBA, and DevOps teams to shape architecture and detailed designs.\nMentorship:\nConduct code reviews and mentor junior team members.\nFoster strong interpersonal skills, excellent communication skills, and collaboration skills within the team.\nMandatory Skills:\nProgramming Languages: Proficiency in Python (3.x) and SQL.\nML Frameworks and Libraries: Extensive knowledge of ML frameworks, libraries, data structures, data modeling, and software architecture.\nDatabases: Proficiency in SQL and NoSQL databases.\nMathematics and Algorithms: In-depth knowledge of mathematics, statistics, and algorithms.\nML Modules and REST API: Proficient with ML modules and REST API.\nVersion Control: Hands-on experience with version control applications (GIT).\nModel Deployment and Monitoring: Experience with model deployment and monitoring.\nData Processing: Ability to turn unstructured data into useful information (e.g., auto-tagging images, text-to-speech conversions).\nProblem-Solving: Analytically agile with strong problem-solving capabilities.\nLearning Agility: Quick to learn new concepts and eager to explore and build new features.\nQualifications:\nEducation: Bachelors or Master’s degree in Computer Science, Data Science, or a related field.\nExperience: Minimum of 6 years of hands-on experience in MLOps, deploying and managing machine learning models in production environments, preferably in cloud-based environments.Role & responsibilities\n\n\nPreferred candidate profile",Industry Type: Recruitment / Staffing,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['MLOPS', 'ML operations', 'ML ops']",2025-06-13 06:08:43
Machine Learning Engineer - Python/Tensorflow,Vayuz Technologies,4 - 5 years,Not Disclosed,['Bengaluru'],"Key Responsibilities :\n- Conduct feature engineering, data analysis, and data exploration to extract valuable insights.\n- Develop and optimize Machine Learning models to achieve high accuracy and performance.\n- Design and implement Deep Learning models, including Artificial Neural Networks (ANN), Convolutional Neural Networks (CNN), and Reinforcement Learning techniques.\n- Handle real-time imbalanced datasets and apply appropriate techniques to improve model fairness and robustness.\n- Deploy models in production environments and ensure continuous monitoring, improvement, and updates based on feedback.\n- Collaborate with cross-functional teams to align ML solutions with business goals.\n- Utilize fundamental statistical knowledge and mathematical principles to ensure the reliability of models.\n- Bring in the latest advancements in ML and AI to drive innovation.\n\nRequirements :\n- 4-5 years of hands-on experience in Machine Learning and Deep Learning.\n- Strong expertise in feature engineering, data exploration, and data preprocessing.\n- Experience with imbalanced datasets and techniques to improve model generalization.\n- Proficiency in Python, TensorFlow, Scikit-learn, and other ML frameworks.\n- Strong mathematical and statistical knowledge with problem-solving skills.\n- Ability to optimize models for high accuracy and performance in real-world scenarios.\n\nPreferred Qualifications :\n- Experience with Big Data technologies (Hadoop, Spark, etc.)\n- Familiarity with containerization and orchestration tools (Docker, Kubernetes).\n- Experience in automating ML pipelines with MLOps practices.\n- Experience in model deployment using cloud platforms (AWS, GCP, Azure) or MLOps tools.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Machine Learning', 'Tensorflow', 'Azure', 'MLOps', 'GCP', 'Big Data', 'Neural Networks', 'AWS', 'Scikit-Learn', 'Deep Learning', 'Python']",2025-06-13 06:08:45
Senior Product Manager - AI,Cheq Digital,3 - 6 years,Not Disclosed,['Bengaluru'],"Defineand drive the vision, strategy, and roadmap for CheQs credit advisory experience.\nWorkclosely with data scientists to translate credit data into context-aware,AI-driven recommendations.\nCollaboratewith engineering, design, marketing, and analytics to ship features that areintelligent, intuitive, and useful.\nOwnproduct outcomes focused on user engagement, habit formation, and financialimprovement.\nDesignand run experiments to validate hypotheses and improve user flows, insights,and nudges.\nEnsureresponsible use of data and AI, aligning with privacy and compliance standards.\nShapecore user journeys by identifying friction and driving seamless, personalizedinterventions.\n  What you'll need\n3 to 4 years of product management experience,ideally in consumer-tech, fintech, or data-led environments.\nHands-on experience with data-driven featuresor platforms that leverage AI/ML for personalization or decision-making.\nAbility to work with data scientists andinterpret model outputs in the context of user needs.\nStrong analytical thinking, structuredproblem-solving, and crisp communication.\nA user-first mindset with the ability tobalance short-term velocity with long-term value.\nBias for action and comfort with building inevolving, fast-paced environments.\n\nWhy should you join CheQ\n\nYou define your work\nWe acknowledge that your work does not define you. Its you who will define your work here. we'do not encourage trade-offs between work and life.\n\nPropelled by courage & care\nwe'dare each other with the art of possible and then watch each others back delivering the solution with speed, agility, heart and rigor.\n\nLearn with the best\nWith a strong leadership team from diverse backgrounds, you can expect to get the best of many worlds.\n\nAnd much more\nIndustry competitive compensation.\nWork on real problems of India that will create Impact at scale.\nWork with all the jazz and fancy that new and innovative technologies bring.",Industry Type: Financial Services,Department: Product Management,"Employment Type: Full Time, Permanent","['Product management', 'Senior Executive', 'Finance', 'Shaping', 'Engineering Design', 'Management', 'Credit management', 'Advisory', 'Analytics', 'RAM']",2025-06-13 06:08:46
Vice President Senior KDB/Q Developer,Barclays,3 - 13 years,Not Disclosed,['Pune'],"Join us as a Vice President Senior Senior KDB/Q Developer at Barclays. To build and maintain the systems that collect, store, process, and analyse data, such as data pipelines, data warehouses and data lakes to ensure that all data is accurate, accessible, and secure. The ideal candidate is responsible and accountable for building and maintenance of data architectures pipelines that enable the transfer and processing of durable, complete and consistent data, design and implementation of data warehouse and data lakes that manage the appropriate data volumes and velocity and adhere to the required security measures. The candidate is also expected for the development of processing and analysis algorithms fit for the intended data complexity and volumes in collaboration with data scientist to build and deploy machine learning models. To contribute or set strategy, drive requirements and make recommendations for change.\n\nTo be successful as a Vice President Senior KDB/Q Developer, you should have experience with:\n\nExcellent Q/Kdb+ programming skills on Linux.\nDevelopment experience in front office / electronic trading systems\nStrong knowledge of agile and SDLC processes.\nExperience with Maven / Java / Git / Team City / Jira / Confluence.\n\nSome other highly valued skills may include:\n\nStrong academic record with a numerate degree (2:1 or higher) - e.g., computer science, maths, physics, engineering.\nFull-lifecycle development on at least 1 large commercial system, with ideal candidate focusing on significant development of large distributed Kdb+ systems.\nYou may be assessed on key critical skills relevant for success in role, such as risk and controls, change and transformation, business acumen, strategic thinking and digital and technology, as well as job-specific technical skill.\n\nThis role is for Pune location.\nPurpose of the role\nTo build and maintain the systems that collect, store, process, and analyse data, such as data pipelines, data warehouses and data lakes to ensure that all data is accurate, accessible, and secure.\nAccountabilities\nBuild and maintenance of data architectures pipelines that enable the transfer and processing of durable, complete and consistent data.\nDesign and implementation of data warehoused and data lakes that manage the appropriate data volumes and velocity and adhere to the required security measures.\nDevelopment of processing and analysis algorithms fit for the intended data complexity and volumes.\nCollaboration with data scientist to build and deploy machine learning models.\nVice President Expectations\nTo contribute or set strategy, drive requirements and make recommendations for change. Plan resources, budgets, and policies; manage and maintain policies/ processes; deliver continuous improvements and escalate breaches of policies/procedures..\nIf managing a team, they define jobs and responsibilities, planning for the department s future needs and operations, counselling employees on performance and contributing to employee pay decisions/changes. They may also lead a number of specialists to influence the operations of a department, in alignment with strategic as well as tactical priorities, while balancing short and long term goals and ensuring that budgets and schedules meet corporate requirements..\nIf the position has leadership responsibilities, People Leaders are expected to demonstrate a clear set of leadership behaviours to create an environment for colleagues to thrive and deliver to a consistently excellent standard. The four LEAD behaviours are: L - Listen and be authentic, E - Energise and inspire, A - Align across the enterprise, D - Develop others..\nOR for an individual contributor, they will be a subject matter expert within own discipline and will guide technical direction. They will lead collaborative, multi-year assignments and guide team members through structured assignments, identify the need for the inclusion of other areas of specialisation to complete assignments. They will train, guide and coach less experienced specialists and provide information affecting long term profits, organisational risks and strategic decisions..\nAdvise key stakeholders, including functional leadership teams and senior management on functional and cross functional areas of impact and alignment.\nManage and mitigate risks through assessment, in support of the control and governance agenda.\nDemonstrate leadership and accountability for managing risk and strengthening controls in relation to the work your team does.\nDemonstrate comprehensive understanding of the organisation functions to contribute to achieving the goals of the business.\nCollaborate with other areas of work, for business aligned support areas to keep up to speed with business activity and the business strategies.\nCreate solutions based on sophisticated analytical thought comparing and selecting complex alternatives. In-depth analysis with interpretative thinking will be required to define problems and develop innovative solutions.\nAdopt and include the outcomes of extensive research in problem solving processes.\nSeek out, build and maintain trusting relationships and partnerships with internal and external stakeholders in order to accomplish key business objectives, using influencing and negotiating skills to achieve outcomes.",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Maven', 'Front office', 'electronic trading', 'Analytical', 'Machine learning', 'Vice President', 'Agile', 'Counselling', 'SDLC']",2025-06-13 06:08:48
Senior Developer,Arctic Wolf Networks,5 - 10 years,Not Disclosed,['Bengaluru'],"At Arctic Wolf, were redefining the cybersecurity landscape. With our employee Pack members, spread out globally, committed to setting new industry standards. Our accomplishments speak for themselves, from our recognition in the Forbes Cloud 100 , CNBC Disruptor 50 , Fortune Future 50 , and Fortune Cyber 60 to winning the 2024 CRN Products of the Year award. We re proud to be named a Leader in the IDC MarketScape for Worldwide Managed Detection and Response Services and earning a Customers Choice distinction from Gartner Peer Insights . Our Aurora Platform also received CRN s Products of the Year award in the inaugural Security Operations Platform category. Join a company that s not only leading, but also shaping, the future of security operations.\nOur mission is simple: End Cyber Risk. We re looking for a Senior Developer to be a part of making this happen.\n\nAbout the Role:\nThe Senior Software Developer will contribute to our Arctic Wolf Labs department, by contributing to initiatives and developing innovative systems, services, and frameworks for the next generation security research initiatives of Arctic Wolf Labs. We are looking for experienced developers to join our pack and work with us to continuously evolve and expand the capabilities of our security products and services.\nArctic Wolf Labs is the research-focused division at Arctic Wolf focused on advancing innovation in the field of security operations. The mission of Arctic Wolf Labs is to develop cutting-edge technology and tools that are designed to enhance the company s core mission to end cyber risk, while also bringing comprehensive security intelligence to Arctic Wolf s customer base and the security community-at-large. Leveraging the more than two trillion security events the Arctic Wolf Security Operations Cloud ingests, parses, enriches, and analyzes each week, Arctic Wolf Labs is responsible for performing threat research on new and emerging adversaries, developing advanced threat detection models, and driving improvement in the speed, scale, and detection abilities of Arctic Wolf s solution offerings. The Arctic Wolf Labs team comprises security and threat intelligence researchers, data scientists, and security development engineers with deep domain knowledge in artificial intelligence (AI), security RD, as well as advanced threat offensive and defensive methods and technologies. Security Research Services Development partners with these groups to understand requirements, design implement scalable, fault-tolerant solutions, and build the next generation of security capabilities for Arctic Wolf.\n\nYou Will:\nOwn RD of distributed, highly scalable, and fault-tolerant microservices\nUse test-driven development techniques to develop beautiful, efficient, and secure code\nCreate and scale high-performance services in a real-time threat analysis pipeline\nDocument and maintain software functionality and specifications\nIdentify problems proactively and propose novel solutions to solve them\nContinuously learn and expand your technical horizons\nWere looking for someone with:\n5+ years of experience with architecting, and developing new highly scalable and fault-tolerant distributed services for high throughput and low latency from ground up in a complex production system\nExcellent understanding of key design patterns (creational, structural, behavioural) and architectural patterns including MVC, Microservices Architecture, Functions as a service (lambda functions), serverless design, Async Programming Patterns, Reactive and Event Driven Programming\nExperience with SQL and NoSQL databases (MongoDB, Elastic Search), KV stores (Redis etc.), Message Queues (RabbitMQ, Kafka etc.), Event-Driven Compute Frameworks (Step Functions, Lambda Functions), Stream and Batch Data Processing (Apache Spark, Flink) in production\nExcellent understanding of Object-oriented, Reactive and Event-Driven Programming concepts and willingness to pick up new object-oriented languages suitable for the job\n5+ years of experience in building and shipping scalable microservices and APIs\nDemonstrable experience with integrating, refactoring, and troubleshooting services in a large production system\nExperience with developing and deploying in the cloud preferably AWS using IAC (infrastructure as code)\nExperience with JavaScript frontend and backend development frameworks (React, Angular, Node etc.)\nFamiliarity with CI / CD tools including Jenkins, Bamboo, Terraform etc.\nExperience architecting, designing, developing, testing, and delivering software in an agile environment\nGood understanding of functional programming concepts a plus\nFamiliarity with cybersecurity concepts is an asset\nAll wolves receive compelling compensation and benefits packages, including:\nEquity for all employees\nFlexible annual leave, paid holidays and volunteer days\nTraining and career development programs\nComprehensive private benefits plan including medical insurance for you and your family, life insurance (3x compensation), and personal accident insurance.\nFertility support and paid parental leave\n\nAt Arctic Wolf, we foster a collaborative and inclusive work environment that thrives on diversity of thought, background, and culture. This is reflected in our multiple awards, including Top Workplace USA (2021-2024), Best Places to Work - USA (2021-2024), Great Place to Work - Canada (2021-2024), Great Place to Work - UK (2024), and Kununu Top Company - Germany (2024). Our commitment to bold growth and shaping the future of security operations is matched by our dedication to customer satisfaction, with over 7,000 customers worldwide and more than 2,000 channel partners globally. As we continue to expand globally and enhance our technology, Arctic Wolf remains the most trusted name in the industry.\nOur Values\nArctic Wolf recognizes that success comes from delighting our customers, so we work together to ensure that happens every day. We believe in diversity and inclusion, and truly value the unique qualities and unique perspectives all employees bring to the organization. And we appreciate that by protecting people s and organizations sensitive data and seeking to end cyber risk we get to work in an industry that is fundamental to the greater good.\nWe celebrate unique perspectives by creating a platform for all voices to be heard through our Pack Unity program. We encourage all employees to join or create a new alliance. See more about our Pack Unity here .\nWe also believe and practice corporate responsibility, and have recently joined the Pledge 1% Movement, ensuring that we continue to give back to our community. We know that through our mission to End Cyber Risk we will continue to engage and give back to our communities.\nArctic Wolf is an Equal Opportunity Employer and considers applicants for employment without regard to race, color, religion, sex, orientation, national origin, age, disability, genetics, or any other basis forbidden under federal, provincial, or local law. Arctic Wolf is committed to fostering a welcoming, accessible, respectful, and inclusive environment ensuring equal access and participation for people with disabilities. As such, we strive to make our entire employee experience as accessible as possible and provide accommodations as required for candidates and employees with disabilities and/or other specific needs where possible. Please let us know if you require any accommodations by emailing recruiting@arcticwolf.com.\n\nSecurity Requirements\nConducts duties and responsibilities in accordance with AWN s Information Security policies, standards, processes, and controls to protect the confidentiality, integrity and availability of AWN business information (in accordance with our employee handbook and corporate policies).\nBackground checks are required for this position.",Industry Type: Emerging Technologies,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Career development', 'Backend', 'Information security', 'Javascript', 'Agile', 'Data processing', 'Medical insurance', 'Troubleshooting', 'Security operations', 'SQL']",2025-06-13 06:08:49
"F2F Interview Sigmoid, B'lore- 14 June Sr Java Developer",Astar Data,3 - 8 years,Not Disclosed,['Bengaluru'],"You will be responsible for building a highly-scalable and extensible robust application. This position reports to the Engineering Manager.\n\nResponsibilities:\nAlign Sigmoid with key Client initiatives\nInterface daily with customers across leading Fortune 500 companies to understand strategic requirements\nAbility to understand business requirements and tie them to technology solutions • Open to work from client location as per the demand of the project / customer. • Facilitate in Technical Aspects\nDevelop and evolve highly scalable and fault-tolerant distributed components using Java technologies.\nExcellent experience in Application development and support, integration development and quality assurance.\nProvide technical leadership and manage it day to day basis\nInterface daily with customers across leading Fortune 500 companies to understand strategic requirements\nStay up-to-date on the latest technology to ensure the greatest ROI for customer & Sigmoid\nHands on coder with good understanding on enterprise level code.\nDesign and implement APIs, abstractions and integration patterns to solve challenging distributed computing problems\nExperience in defining technical requirements, data extraction, data transformation, automating jobs, productionizing jobs, and exploring new big data technologies within a Parallel Processing environment\nCulture\nMust be a strategic thinker with the ability to think unconventional / out:of:box. • Analytical and solution driven orientation.\nRaw intellect, talent and energy are critical.\nEntrepreneurial and Agile : understands the demands of a private, high growth company.\nAbility to be both a leader and hands on ""doer"".\n\nQualifications: -\nYears of track record of relevant work experience and a computer Science or a related technical discipline is required\nExperience in development of Enterprise scale applications and capable in developing framework, design patterns etc. Should be able to understand and tackle technical challenges and propose comprehensive solutions.\nExperience with functional and object-oriented programming, Java or Python is a must.\nExperience in Springboot ,API,SQL\nGood to have GIT , Airflow ,Node JS, Python , Angular Python\nExperience with database modeling and development, data mining and warehousing. • Unit, Integration and User Acceptance Testing.\nEffective communication skills (both written and verbal)\nAbility to collaborate with a diverse set of engineers, data scientists and product managers\nComfort in a fast-paced start-up environment.\nExperience in Agile methodology.\nProficient with SQL and its variation among popular databases. • Experience working with large, complex data sets from a variety of sources.\n\n\nPlease carry your Laptop.\nLocation- Sigmoid, Bangalore,Address: 2nd Floor, Tower-2, SJR I PARK, Rd Number 9, EPIP Zone, Whitefield, Bengaluru, Karnataka 560066\nDates- 14 June  2025 (Saturday)\nTime- 10:30  AM\nSkills :- Java,Springboot,DSA is mandatoryRole -  Sr Java Developer \nExperience :- 4 -6 yrsQualification :- Min 4 yrs post 12th std is mandatory BE/MCA preferred (BSC or BCA Candidates please ignore this mail)\n\nNo. of rounds :-4 \nRound 1 - Coding & DSA\nRound 2 - Technical round\nRound 3 - Culture Fitment\nRound 4 - HR \nPlease carry a Govt Id card (Aadhar/PAN) and your resume for the interview.\nKindly ignore this mail if you have already attended the interview",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Core Java', 'Spring Boot', 'Microservices', 'DSA']",2025-06-13 06:08:50
"Principal Analyst, Insights",Zeta Global,3 - 4 years,Not Disclosed,['Bengaluru'],"Description: Principal Data Analyst will be responsible for analyzing complex datasets, identifying opportunities for process improvements, and implementing automation solutions to streamline workflows. This role requires a deep understanding of data analytics, process automation tools, and excellent problem-solving skills. The ideal candidate will be proactive, detail-oriented, and able to work collaboratively with cross-functional teams to drive data-driven initiatives.\nWhat you ll do:\nAnalyze large and complex datasets to identify trends, patterns, and insights that drive business decisions.\nDevelop, implement, and maintain automated processes to improve data accuracy, efficiency,and reporting capabilities.\nCollaborate with stakeholders to understand business requirements and translate them into technical solutions.\nDesign and build automated dashboards and reports to provide real-time insights to various departments.\nUtilize data visualization tools to present findings in a clear and actionable manner.\nContinuously monitor and refine automated processes to ensure optimal performance and scalability.\nStay updated with industry trends and best practices in data analytics and process automation.\nMentor and provide guidance to junior data analysts on best practices and technical skills.\nWho you are:\nA great communicator who can convey complex technical features in simple terms.\nAble to multitask and prioritize among several high-profile clients.\nHave a high degree of creativity, self-motivation, and drive.\nEagerness to work in a startup team environment that will be rapidly changing.\nEnthusiastic team player with a penchant for collaboration and knowledge sharing.\nWillingness to do whatever it takes to get the job done.\nNerdy but loveable.\nData driven, technical, self-starting and curious.\nWhat you need:\nBachelor s or Master s degree in data science, Computer Science, Statistics, or a related field.\nMinimum of 3-4 years of experience in data analysis, with a focus on process automation.\nA minimum of 2 years of work experience in analytics (minimum of 1 year with a Ph.D.)\nExperience with data querying languages (e.g. SQL), scripting languages (e.g. Python), and/or statistical/mathematical software (e.g. R) Experience with combining and consolidating disparate datasets in apps such as Big Query, Data Bricks\nProficiency in programming languages such as Python, R, or SQL.\nExtensive experience with data visualization tools such as Tableau, Power BI or similar.\nStrong knowledge of process automation tools and platforms (e.g., Alteryx, UiPath, Microsoft Power Automate).\nExperience with database management systems (e.g., SQL Server, MySQL, PostgreSQL).\nExcellent analytical and problem-solving skills.\nAbility to work effectively in a fast-paced, collaborative environment.\nStrong communication skills, with the ability to convey complex data insights to non-technical stakeholders.\nExperience with machine learning and predictive analytics is a plus.\nBonus if you have:\nMaster s or Ph.D. Degree in a quantitative field.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['SQL', 'UiPath', 'R', 'Power BI', 'PostgreSQL', 'MySQL', 'Alteryx', 'Tableau', 'Python']",2025-06-13 06:08:52
Senior Developer - Celonis,Abinbev Gcc Services,4 - 9 years,Not Disclosed,['Bengaluru'],"AB InBev GCC was incorporated in 2014 as a strategic partner for Anheuser-Busch InBev. The center leverages the power of data and analytics to drive growth for critical business functions such as operations, finance, people, and technology. The teams are transforming Operations through Tech and Analytics.\nDo You Dream Big?\nWe Need You.\nJob Description\nJob Title: Senior Developer - Celonis",,,,"['Celonis', 'Process Mining', 'SQL']",2025-06-13 06:08:53
IT Analyst(Finance)-5+yrs-Bangalore/Pune/Hyderabad-Hybrid,Databuzz ltd,5 - 10 years,Not Disclosed,"['Hyderabad', 'Pune', 'Bengaluru']","Databuzz is Hiring for IT Analyst(Finance)-5+yrs-Bangalore/Pune/Hyderabad-Hybrid\n\nPlease mail your profile to haritha.jaddu@databuzzltd.com with the below details, If\nyou are Interested.\n\nAbout DatabuzzLTD:\nDatabuzz is One stop shop for data analytics specialized in Data Science, Big Data, Data Engineering, AI & ML, Cloud Infrastructure and Devops. We are an MNC based in both UK and INDIA. We are a ISO 27001 & GDPR complaint company.\n\nCTC -\nECTC -\nNotice Period/LWD - (Candidate serving notice period will be preferred)\n\nPosition: IT Analyst(Finance)\nLocation: Bangalore/Pune/Hyderabad\nExp -5+ yrs\n\nMandatory skills :\nCandidate should have Strong knowledge on Functionalities in AR,GL,CE,AP (R12)\nGood understanding on Order to Cash Flow\nExperienced in implementation/ Configuration/ Customization / Support\nExperienced in localizations\nExperience in implementing integrations with external systems\nGood documentation skills and presentation skills to prepare MD050 / PPT and do the solution reviews with various teams\nAble to write SQLs on tables and pull the required data\nUnderstanding of Fusion finance.\n\n\nRegards,\nHaritha\nTalent Acquisition specialist\nharitha.jaddu@databuzzltd.com",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['AR', 'Ce', 'R12', 'GL', 'Ap']",2025-06-13 06:08:55
Senior Etl Informatica Developer,VBeyond,6 - 8 years,19-25 Lacs P.A.,"['Noida', 'Chennai', 'Bengaluru']","We are seeking a highly skilled and experienced Senior ETL & Reporting QA Analyst to join our dynamic team. The ideal candidate will bring strong expertise in ETL and Report Testing, with a solid command of SQL, and hands-on experience in Informatica, as well as BI Reporting tools. A strong understanding of the Insurance domain is crucial to this role. This position will be instrumental in ensuring the accuracy, reliability, and performance of our data pipelines and reporting solutions.\n\nKey Responsibilities:\nDesign, develop, and execute detailed test plans and test cases for ETL processes, data migration, and data warehousing solutions.\nPerform data validation and data reconciliation using complex SQL queries across various source and target systems.\nValidate Informatica ETL workflows and mappings to ensure accurate data transformation and loading.\nConduct end-to-end report testing and dashboard validations using Cognos (preferred), or comparable BI tools such as Tableau or Power BI.\nCollaborate with cross-functional teams including Business Analysts, Developers, and Data Engineers to understand business requirements and transform them into comprehensive test strategies.\nIdentify, log, and track defects to closure using test management tools and actively participate in defect triage meetings.\nMaintain and enhance test automation scripts and frameworks where applicable.\nEnsure data integrity, consistency, and compliance across reporting environments, particularly in the insurance domain context.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Informatica', 'ETL', 'Power Bi', 'Insurance Domain', 'Tableau', 'SQL']",2025-06-13 06:08:57
Business Analyst - Alternative Investments,Vichara Technologies,6 - 10 years,25-35 Lacs P.A.,"['Bengaluru', 'Delhi / NCR', 'Mumbai (All Areas)']","Create documentation and user stories.\nWork with engineering teams to review upcoming and backlog Jira tickets.\nProvide guidance on design decisions in areas including Credit and tech including Snowflake and Streamlit\nDevelop reporting in powerBI\n\nRequired Candidate profile\n5+ years of experience as a Business analyst especially in Alternative assets, Credit, CLO, Real Estate etc.\nExperience creating complex dashboards in powerBI\nExposure to Snowflake and Streamlit",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Investment Banking', 'Power Bi', 'User Stories', 'Private Equity', 'Data Warehousing', 'streamlit', 'Alternative Investments', 'JIRA', 'Investment Management', 'Analytics', 'Business Analysis', 'Loan Syndication', 'SQL', 'private credit', 'Real Estate', 'Snowflake', 'Private Debt', 'Dashboards', 'Argus', 'Python']",2025-06-13 06:08:58
S&C Global Network - AI - CDP - Marketing Analytics - Consultant,Accenture,8 - 10 years,Not Disclosed,['Gurugram'],"Job Title -\n\n\n\nS&C Global Network - AI - CDP - Marketing Analytics - Consultant\n\n\n\nManagement Level:\n\n\n\n9-Team Lead/Consultant\n\n\n\nLocation:\n\n\n\nGurugram, DDC1A, NonSTPI\n\n\n\nMust-have skills:Data Analytics\n\n\n\n\nGood to have skills:Ability to leverage design thinking, business process optimization, and stakeholder management skills.\n\n\n\nJob\n\n\nSummary:\n\nThis role involves driving strategic initiatives, managing business transformations, and leveraging industry expertise to create value-driven solutions.\n\n\n\n\nRoles & Responsibilities:\n\nProvide strategic advisory services, conduct market research, and develop data-driven recommendations to enhance business performance.\n\n\n\nWHATS IN IT FOR YOU\n\nAs part of our Analytics practice, you will join a worldwide network of over 20k+ smart and driven colleagues experienced in leading AI/ML/Statistical tools, methods and applications. From data to analytics and insights to actions, our forward-thinking consultants provide analytically-informed, issue-based insights at scale to help our clients improve outcomes and achieve high performance.\n\n\n\nWhat you would do in this role\n\nA Consultant/Manager for Customer Data Platforms serves as the day-to-day marketing technology point of contact and helps our clients get value out of their investment into a Customer Data Platform (CDP) by developing a strategic roadmap focused on personalized activation. You will be working with a multidisciplinary team of Solution Architects, Data Engineers, Data Scientists, and Digital Marketers.\n\n\n\nKey Duties and Responsibilities:\nBe a platform expert in one or more leading CDP solutions. Developer level expertise on Lytics, Segment, Adobe Experience Platform, Amperity, Tealium, Treasure Data etc. Including custom build CDPs\nDeep developer level expertise for real time even tracking for web analytics e.g., Google Tag Manager, Adobe Launch etc.\nProvide deep domain expertise in our clients business and broad knowledge of digital marketing together with a Marketing Strategist industry\nDeep expert level knowledge of GA360/GA4, Adobe Analytics, Google Ads, DV360, Campaign Manager, Facebook Ads Manager, The Trading desk etc.\nAssess and audit the current state of a clients marketing technology stack (MarTech) including data infrastructure, ad platforms and data security policies together with a solutions architect.\nConduct stakeholder interviews and gather business requirements\nTranslate business requirements into BRDs, CDP customer analytics use cases, structure technical solution\nPrioritize CDP use cases together with the client.\nCreate a strategic CDP roadmap focused on data driven marketing activation.\nWork with the Solution Architect to strategize, architect, and document a scalable CDP implementation, tailored to the clients needs.\nProvide hands-on support and platform training for our clients.\nData processing, data engineer and data schema/models expertise for CDPs to work on data models, unification logic etc.\nWork with Business Analysts, Data Architects, Technical Architects, DBAs to achieve project objectives - delivery dates, quality objectives etc.\nBusiness intelligence expertise for insights, actionable recommendations.\nProject management expertise for sprint planning\n\n\n\n\n\nProfessional & Technical\n\n\n\n\nSkills:\n\n\n- Relevant experience in the required domain.\n\n- Strong analytical, problem-solving, and communication skills.\n\n- Ability to work in a fast-paced, dynamic environment.\nExperience with A/B testing tools is a plus.\nMust have programming experience in PySpark, Python, Shell Scripts.\nRDBMS, TSQL, NoSQL experience is must.\nManage large volumes of structured and unstructured data, extract & clean data to make it amenable for analysis.\nExperience in deployment and operationalizing the code is an added advantage.\nExperience with source control systems such as Git, Bitbucket, and Jenkins build and continuous integration tools.\nProficient in Excel, MS word, PowerPoint, etc\n\n\n\nTechnical\n\n\n\n\nSkills:\n\nAny CDP platforms experience e.g., Lytics CDP platform developer, or/and\nSegment CDP platform developer, or/and\nAdobe Experience Platform (Real time CDP) developer, or/and\nCustom CDP developer on any cloud\nGA4/GA360, or/and Adobe Analytics\nGoogle Tag Manager, and/or Adobe Launch, and/or any Tag Manager Tool\nGoogle Ads, DV360, Campaign Manager, Facebook Ads Manager, The Trading desk etc.\nDeep Cloud experiecne (GCP, AWS, Azure)\nAdvance level Python, SQL, Shell Scripting experience\nData Migration, DevOps, MLOps, Terraform Script programmer\n\n\n\nSoft\n\n\n\n\nSkills:\n\nStrong problem solving skills\nGood team player\nAttention to details\nGood communication skills\n\n\n\n\n\nAdditional Information:\n\n- Opportunity to work on innovative projects.\n\n- Career growth and leadership exposure.\n\n\n\n\n\nAbout Our Company | Accenture\n\nQualification\n\n\n\nExperience:\n\n\n\n8-10Years\n\n\n\n\nEducational Qualification:\n\n\n\nAny Degree",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['digital marketing', 'python', 'data analytics', 'pyspark', 'campaign management', 'trading', 'adobe analytics', 'rdbms', 'adobe', 'microsoft azure', 'display video', 't-sql', 'google ads', 'nosql', 'sql', 'git', 'gcp', 'devops', 'jenkins', 'segmentation', 'facebook ads manager', 'shell scripting', 'aws']",2025-06-13 06:09:00
Artificial Intelligence Architect,Emerson,10 - 20 years,Not Disclosed,['Pune'],"Role & responsibilities\nDesign robust and scalable AI/ML architectures that support the development and deployment of machine learning models and AI solutions.\nDevelop and guide the implementation of end-to-end AI/ML solutions, including model development, data processing, and system integration.\nEvaluate and recommend the latest AI/ML technologies, frameworks, and tools to enhance system capabilities and performance.\nCollaborate with software engineers and other development teams to integrate AI/ML solutions into existing systems and applications. Ensure seamless operation and performance.\nWork with cross-functional teams, including developers, data scientists, machine learning engineers, and business stakeholders, to understand requirements and design solutions that align with business objectives.\n\nPreferred candidate profile\nBachelors degree in computer science, Data Science, Statistics, or a related field or a master's degree or higher is preferred.\nMore than 3 years of experience in designing and implementing AI/ML architectures, with a proven track record of successful projects.\nExtensive experience with machine learning frameworks (e.g., Go, TensorFlow, PyTorch), programming languages C#, .Net, NodeJS and data processing tools.\nStrong understanding of system architecture principles, including distributed systems, microservices, and cloud computing.\nExperience with Microsoft Azure cloud services and their AI/ML offerings\nExperience with event-handling systems such as Kafka\nExperience with big data technologies and data engineering practices.\nExcellent verbal and written communication skills, with the ability to convey complex technical concepts to non-technical stakeholders.",Industry Type: Industrial Automation,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Aiml', 'Ml', 'Python', 'Tensorflow', 'Pytorch', 'Architecture', 'Artificial Intelligence', '.Net', 'Machine Learning', 'Scikit-Learn']",2025-06-13 06:09:02
AI/ML Computational Science Assoc Mgr,Accenture,10 - 14 years,Not Disclosed,['Navi Mumbai'],"Skill required: Data Scientist - Data Science\n\n\n\n\nDesignation: AI/ML Computational Science Assoc Mgr\n\n\n\n\nQualifications:Any Graduation\n\n\n\n\nYears of Experience:10 to 14 years\n\n\n\nAbout AccentureCombining unmatched experience and specialized skills across more than 40 industries, we offer Strategy and Consulting, Technology and Operations services, and Accenture Song all powered by the worlds largest network of Advanced Technology and Intelligent Operations centers. Our 699,000 people deliver on the promise of technology and human ingenuity every day, serving clients in more than 120 countries. Visit us at www.accenture.com\n\n\n\n\nWhat would you do\nData & AIAn interdisciplinary field about scientific methods, processes and systems to extract knowledge or insights from data in various forms, either structured or unstructured.\n\n\n\n\nWhat are we looking for\nGenerative AI Python (Programming Language) Ability to handle disputes Ability to manage multiple stakeholders Ability to meet deadlines Ability to perform under pressure Adaptable and flexible\n\n\n\nRoles and Responsibilities: In this role you are required to do analysis and solving of moderately complex problems Typically creates new solutions, leveraging and, where needed, adapting existing methods and procedures The person requires understanding of the strategic direction set by senior management as it relates to team goals Primary upward interaction is with direct supervisor or team leads Generally, interacts with peers and/or management levels at a client and/or within Accenture The person should require minimal guidance when determining methods and procedures on new assignments Decisions often impact the team in which they reside and occasionally impact other teams Individual would manage medium-small sized teams and/or work efforts (if in an individual contributor role) at a client or within Accenture\n\nQualification\n\nAny Graduation",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['service operations', 'data science', 'vendor development', 'procurement', 'supply chain management', 'vendor management', 'client relationship management', 'project management', 'material management', 'logistics management', 'business development', 'inventory management']",2025-06-13 06:09:03
GN CG&S Growth Strategy -Alco Bev Manager,Accenture,5 - 10 years,Not Disclosed,['Mumbai'],"Job Title - Growth Strategy (AlcoBev) Manager S&C GN\n\n\n\nManagement Level :07 - Manager\n\n\n\nLocation:Bangalore / Gurgaon / Mumbai\n\n\n\nMust have skills:\n\n\n\n\nDeep expertise in the Alcoholic Beverages (AlcoBev) industry, including market dynamics, regulations, and commercial processes\nExperience in\n\n\n\nRoute-to-Market (RTM), Distribution & Trade Management, Revenue Growth Management (RGM), and Pricing Strategy within AlcoBev\nStrong understanding of\n\n\n\nexcise laws, taxation structures, and state-wise regulatory frameworks for alcoholic beverages\nExperience in working with\n\n\n\ndistributors, wholesalers, and on-trade/off-trade channels\nAbility to design and implement\n\n\n\ngrowth strategies for sales, channel expansion, and brand portfolio optimization\n\n\n\n\nGood to have skills:\nExposure to\n\n\n\ndigital commerce, D2C models, and omnichannel strategies in the AlcoBev industry\nFamiliarity with\n\n\n\nDistributor Management Systems (DMS), CRM, Trade Promotion Management (TPM), and pricing tools\nUnderstanding of\n\n\n\ndata analytics, demand forecasting, and AI-driven sales strategies\n\n\n\n\nJob\n\n\nSummary: As a Manager in Growth Strategy (AlcoBev), you will work with global and regional Alcoholic Beverage companies to develop market expansion strategies, optimize route-to-market models, and enhance commercial processes.\n\nYou will be responsible for solving complex industry challenges, driving regulatory compliance strategies, and improving distributor management and trade effectiveness. Your role will involve working closely with senior stakeholders across sales, marketing, and supply chain functions to create data-driven, scalable growth solutions.\n\n\n\n\nRoles & Responsibilities:\nDevelop\n\n\n\ngrowth and market entry strategies for AlcoBev brands across\n\n\n\nemerging and developed markets\nOptimize\n\n\n\nRoute-to-Market (RTM) and distributor management models to improve reach and efficiency\nNavigate\n\n\n\nstate-wise excise regulations, licensing processes, and trade policies to drive compliance and growth\nDesign\n\n\n\npricing and revenue growth strategies, factoring in\n\n\n\nexcise duties, MRP regulations, and discount structures\nEnhance\n\n\n\ntrade promotion effectiveness by integrating data-driven insights into sales and distribution strategies\nLeverage\n\n\n\ndata analytics and AI-driven insights to improve demand forecasting, territory planning, and sales execution\nCollaborate with cross-functional teams to integrate\n\n\n\ne-commerce, D2C, and omnichannel capabilities into growth plans\nMentor and develop a team of consultants specializing in AlcoBev industry strategies\n\n\n\n\nProfessional & Technical\n\n\n\n\nSkills:\n\nStrong knowledge of\n\n\n\nstate-wise excise regulations, taxation structures, and compliance requirements\nExperience in\n\n\n\nmanaging distributor relationships and trade negotiations\nAbility to develop\n\n\n\npricing, promotion, and assortment strategies based on regulatory and market constraints\nFamiliarity with\n\n\n\nDMS, CRM, TPM, and analytics tools used in AlcoBev sales & marketing\nStrong\n\n\n\nanalytical skills to assess\n\n\n\nmarket trends, competitor strategies, and consumer demand patterns\nAbility to\n\n\n\ninfluence senior stakeholders, including leadership teams at leading AlcoBev companies\n\n\n\n\n\nAdditional Information:\n\n\n\n\nWork with leading AlcoBev brands globally, solving key\n\n\n\nmarket entry, pricing, and regulatory challenges\nExposure to\n\n\n\ncutting-edge digital transformation strategies in the AlcoBev sector\nOpportunity to\n\n\n\ncollaborate with industry experts, data scientists, and commercial strategists\nBe part of an inclusive and collaborative\n\n\n\nindustry-focused strategy consulting team\n\nAbout Our Company | Accenture (do not remove the hyperlink)Qualification\n\n\n\nExperience:\n\n\n\n\nMinimum 8-12 years of experience, with at least\n\n\n\n5+ years of hands-on experience in the\n\n\n\nAlcoBev industry or in Consulting with experience in the AlcoBev industry\nPrior experience in\n\n\n\nsales, distribution, pricing, or regulatory functions at a leading AlcoBev company or consulting firm (worked on alcobev projects)\nExposure to\n\n\n\nstate-wise regulatory challenges, distributor models, and licensing processes\n\n\n\n\nEducational Qualification:\nMBA from a Tier 1 Business School",Industry Type: IT Services & Consulting,Department: Strategic & Top Management,"Employment Type: Full Time, Permanent","['excise', 'regulatory frameworks', 'beverage', 'regulations', 'taxation', 'data analytics', 'distribution system', 'rtm', 'dms', 'distribution', 'sales', 'growth strategy', 'tpm', 'marketing', 'route3', 'revenue', 'trade promotion management', 'trade management', 'digital commerce', 'crm']",2025-06-13 06:09:05
S&C Global Network - AI CMT Eng - Specialist,Accenture,3 - 8 years,Not Disclosed,['Gurugram'],"Job Title - S&C Global Network - AI - CMT AI ML Consultant\n\n\n\nManagement Level:9- Consultant\n\n\n\nLocation:Open\n\n\n\nMust-have skills:Gen AI ML\n\n\n\n\nGood to have skills:Ability to leverage design thinking, business process optimization, and stakeholder management skills.\n\n\n\nJob\n\n\nSummary:\n\nThis role involves driving strategic initiatives, managing business transformations, and leveraging industry expertise to create value-driven solutions.\n\n\n\n\nRoles & Responsibilities:\n\nProvide strategic advisory services, conduct market research, and develop data-driven recommendations to enhance business performance.\n\n\n\nWHATS IN IT FOR YOU\nAccenture has committed to invest USD 3Billion into GenAI in the next 3 years. We will continually invest in your learning and growth. You'll work with Accentures highly skilled and experienced practitioners, and Accenture will support you in growing your own career path and interests.\nYoull be part of a diverse, vibrant, global Accenture Data and AI community, continually pushing the boundaries of business capabilities.\n\n\n\nWhat you would do in this role\n\n\n\nML Maturity Assessment:\nConduct comprehensive assessments of the organization's ML maturity, identifying strengths, weaknesses, and areas for improvement.\nProvide strategic recommendations to enhance the overall ML capability and align it with business objectives.\n\n\n\nML Ops Roadmap & Processes:\nDevelop ML Ops roadmaps and establish robust processes for the end-to-end machine learning lifecycle, including data preparation, model training, deployment, and monitoring.\nImplement best practices in ML Ops to ensure efficiency, scalability, and reliability of ML systems.\n\n\n\nImplementation of Gen AI Solutions:\nLead the design and implementation of state-of-the-art Generative AI solutions, leveraging deep learning frameworks such as TensorFlow and PyTorch.\nDrive innovation in Gen AI, staying abreast of the latest advancements and incorporating cutting-edge technologies into solutions.\n\n\n\nIncubating and Designing:\nProactively identify opportunities for ML and / or Gen AI applications within the organization.\nWork closely with cross-functional teams to incubate and design bespoke ML solutions tailored to business requirements.\n\n\n\nTechnical Leadership:\nProvide technical leadership and mentorship to data scientists, engineers, and other team members.\nCollaborate with stakeholders to ensure alignment between technical solutions and business objectives.\n\n\n\nCollaboration and Communication:\nCollaborate with business stakeholders to understand their needs and translate them into ML and Gen AI requirements.\nEffectively communicate complex technical concepts to non-technical audiences.\n\n\n\n\nProfessional & Technical\n\n\n\n\nSkills:\n\n\n- Relevant experience in the required domain.\n\n- Strong analytical, problem-solving, and communication skills.\n\n- Ability to work in a fast-paced, dynamic environment.\nProven expertise in conducting ML maturity assessments and developing ML Ops roadmaps.\nHands-on experience in operationalizing the Machine learning system on a cloud and / or an On-Prem platform.\nExperience in implementing Generative AI solutions, including incubation, design, and deployment will be a big plus.\nProficiency in deep learning frameworks such as TensorFlow and PyTorch.\nGood knowledge of ML Ops best practices and processes.\nExcellent problem-solving skills and ability to design scalable and reliable ML architectures.\nStrong leadership and communication skills, with a track record of leading successful ML initiatives.\nExperience in Telecom or Hi Tech or Software and platform industry desirable\nTools & Techniques\nTensorFlow, PyTorch, Scikit-learn, Keras\nNumPy, Pandas\nMatplotlib, Seaborn\nTensorFlow Serving, Docker and Kubernetes\nGood software engineering practices, including code modularization, documentation, and testing.\nExperience with open API , Integration architecture , microservices\n\n\n\n\nAdditional Information:\n\n- Opportunity to work on innovative projects.\n\n- Career growth and leadership exposure.\n\n\n\n\n\nAbout Our Company | Accenture\n\nQualification\n\n\n\nExperience:4+ years\n\n\n\n\nEducational Qualification:Btech/ BE",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['deep learning frameworks', 'machine learning', 'assessment', 'gen', 'ml', 'kubernetes', 'hi', 'scikit-learn', 'ai solutions', 'numpy', 'integration architecture', 'docker', 'microservices', 'pandas', 'tensorflow', 'seaborn', 'matplotlib', 'open api', 'business transformation', 'pytorch', 'keras', 'telecom']",2025-06-13 06:09:07
GN - SONG - Design and Digital Products - UI/UX - Manager,Accenture,12 - 14 years,Not Disclosed,['Gurugram'],"Job Title -\n\n\n\nGN - SONG - Design and Digital Products - UI/UX - Manager\n\n\n\nManagement Level:\n\n\n\n7-Manager\n\n\n\nLocation:\n\n\n\nGurugram, DDC1\n\n\n\nMust-have skills:UI/UX Design\n\n\n\n\nGood to have skills:Ability to leverage design thinking, business process optimization, and stakeholder management skills.\n\n\n\nJob\n\n\nSummary:\n\nThis role involves driving strategic initiatives, managing business transformations, and leveraging industry expertise to create value-driven solutions.\n\n\n\n\nRoles & Responsibilities:\n\nProvide strategic advisory services, conduct market research, and develop data-driven recommendations to enhance business performance.\n\nYou will work closely with our clients as consulting professionals who design, build and implement experiences that can help enhance business performance & drive customer satisfaction. As part of the team, you will drive the following:\n\n\n\nKey Responsibilities\n\n\n\n\nUser Advocacy and Research:\nAct as a champion for user needs, ensuring designs are empathetic and solve real-world problems. Conduct UX research, create personas, and benchmark best practices to inform design strategies.\nAssess existing user experiences to identify gaps and recommend improvements.\n\n\n\n\nStrategic UI/UX Design:\nDesign and create visually appealing, intuitive, and functional interfaces for client products and systems.\nDesign and maintain UI components such as navigation menus, CTAs, tabs, carousels, in-app banners, and ensure cohesive user experiences across platforms.\n\n\n\n\nPrototyping and Testing:\nCreate wireframes, prototypes, and high-fidelity designs using industry-standard tools. Iterate designs based on user feedback and analytics to continuously improve the experience.\nConduct user testing to gather feedback on prototypes, ensuring usability, accessibility, and desirability.\n\n\n\n\nCollaboration:\nWork closely with development teams to ensure implement designs accurately and alignment with business and technical requirements.\n\n\n\n\nLeadership and Team Building:\nLead and inspire a multidisciplinary design team, fostering a culture of creativity and accountability.\nMentor team members to develop their skills and contribute to their professional growth.\nPromote collaboration and motivation within the team to achieve project goals.\n\n\n\n\nClient Management and Engagement:\nBuild and maintain strong relationships with clients, understanding their objectives, delivering tailored solutions and AI-enhanced solutions.\nPresent design concepts, strategies, and progress updates to key stakeholders effectively.\n\n\n\n\nBring Your Best Skills Forward\n\n\n\n\nUser-Centric Vision: Ability to apply behavioral science and nuanced insights to create impactful digital experiences.\n\n\n\n\nCommunication Excellence: Strong presentation and storytelling skills to influence stakeholders and drive change.\n\n\n\n\nAdaptability and Problem Solving: Proven ability to lead in dynamic environments and address complex challenges effectively.\n\n\n\n\nStrategic Leadership: Expertise in shaping design strategies and aligning them with business goals.\n\n\n\n\nTeam Collaboration: Capacity to build and lead high-performing teams with diverse skills and backgrounds.\n\n\n\n\nAI Collaboration: Skills in working with AI engineers and data scientists to integrate intelligent features into user interfaces.\n\n\n\n\n\nProfessional & Technical\n\n\n\n\nSkills:\n\n\n- Relevant experience in the required domain.\n\n- Strong analytical, problem-solving, and communication skills.\n\n- Ability to work in a fast-paced, dynamic environment.\n\n\n\n\nAdditional Information:\n\n- Opportunity to work on innovative projects.\n\n- Career growth and leadership exposure.\n\n\n\n\n\nAbout Our Company | AccentureQualification\n\n\n\nExperience:\n\n\n\n12-14Years\n\n\n\n\nEducational Qualification:\n\n\n\nAny Degree",Industry Type: IT Services & Consulting,"Department: UX, Design & Architecture","Employment Type: Full Time, Permanent","['presentation skills', 'story writing', 'ui', 'ui/ux', 'ux design', 'ux', 'adobe xd', 'prototyping', 'user interface designing', 'gui testing', 'process optimization', 'photoshop', 'prototype', 'ux research', 'visual design', 'stakeholder management', 'wireframe', 'html', 'illustrator']",2025-06-13 06:09:09
I&F Decision Science Practitioner Specialist,Accenture,7 - 11 years,Not Disclosed,['Mumbai'],"Skill required: Data Scientist - Data Science\n\n\n\n\nDesignation: Specialist\n\n\n\n\nQualifications:Any Graduation\n\n\n\n\nYears of Experience:7 - 11 Years\n\n\n\nAbout AccentureCombining unmatched experience and specialized skills across more than 40 industries, we offer Strategy and Consulting, Technology and Operations services, and Accenture Song all powered by the worlds largest network of Advanced Technology and Intelligent Operations centers. Our 699,000 people deliver on the promise of technology and human ingenuity every day, serving clients in more than 120 countries. Visit us at www.accenture.com\n\n\n\n\nWhat would you do\nYou will be aligned with our Insights & Intelligence vertical and help us generate insights by leveraging the latest Artificial Intelligence (AI) and Analytics techniques to deliver value to our clients. You will also help us apply your expertise in building world-class solutions, conquering business problems, addressing technical challenges using AI Platforms and technologies. You will be required to utilize the existing frameworks, standards, patterns to create architectural foundation and services necessary for AI applications that scale from multi-user to enterprise-class and demonstrate yourself as an expert by actively blogging, publishing research papers, and creating awareness in this emerging area. You will be working as a part of Marketing & Customer Analytics team which provides a set of processes that measure, manage and analyze marketing activities in order to provide actionable insights and recommendations to marketing organizations in terms of optimizing ROI & performance efficiency in operations.Customer analytics is a process by which data from customer behavior is used to help make key business decisions via market segmentation and predictive analytics. This information is used by businesses for direct marketing, site selection, and customer relationship management. You should have exposure to digital marketing, A/B testing, MVT, Google Analytics/Site Catalyst. You will be a core member of Accenture Operations global Applied Intelligence group, an energetic, strategic, high-visibility and high-impact team, to innovate and transform the Accenture Operations business using machine learning, advanced analytics to support data-driven decisioning. The objectives of the team include but are not limited toLeading team of data scientists to build and deploy data science models to uncover deeper insights, predict future outcomes, and optimize business processes for clients. Refining and improving data science models based on feedback, new data, and evolving business needs. Analyze available data to identify opportunities for enhancing brand equity, improving retail margins, achieving profitable growth, and expanding market share for clients.\n\n\n\n\nWhat are we looking for\nExtensive experience in leading Data Science and Advanced Analytics delivery teams Strong statistical programming experience - Python, R, SAS, S-plus, MATLAB, STATA or SPSS. Experience working with large data sets and big data tools like Snowflake, AWS, Spark, etc. Solid knowledge in at least one of the following Supervised and Unsupervised Learning, Classification, Regression, Clustering, Neural Networks, Ensemble Modelling (random forest, boosted tree, etc.), Multivariate Statistics, Non-parametric Methods, Reliability Models, Markov Models, Stochastic models, Bayesian Models Experience in atleast one of these business domainsCPG, Retail, Marketing Analytics, Customer Analytics, Digital Marketing, eCommerce, Health, Supply Chain Extensive experience in client engagement and business development Ability to work in a global collaborative team environment\n\n\n\nRoles and Responsibilities: In this role you are required to do analysis and solving of moderately complex problems Typically creates new solutions, leveraging and, where needed, adapting existing methods and procedures The person requires understanding of the strategic direction set by senior management as it relates to team goals Primary upward interaction is with direct supervisor or team leads Generally, interacts with peers and/or management levels at a client and/or within Accenture The person should require minimal guidance when determining methods and procedures on new assignments Decisions often impact the team in which they reside and occasionally impact other teams Individual would manage medium-small sized teams and/or work efforts (if in an individual contributor role) at a client or within Accenture Please note that this role may require you to work in rotational shifts\n\nQualification\n\nAny Graduation",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['client engagement', 'neural networks', 'business development', 'data science', 'clustering', 'digital marketing', 'matlab', 'snowflake', 'python', 'adobe analytics', 'sas', 'stata', 'spss', 'ensemble', 'unsupervised learning', 'r', 'spark', 'google analytics', 'statistical programming', 'aws']",2025-06-13 06:09:11
"Tech Recruiter, Experienced Talent Recruiter",Bain,5 - 8 years,Not Disclosed,['New Delhi'],"MANAGER, EXPERIENCED TALENT RECRUITING (ETR)\nNEW DELHI, BAIN AND COMPANY\n\nWHAT MAKES US A GREAT PLACE TO WORK\nWe are proud to be consistently recognized as one of the world s best places to work. We are currently the #1 ranked consulting firm on Glassdoor s Best Places to Work list and have maintained a spot in the top four on Glassdoor s list since its founding in 2009.\nExtraordinary teams are at the heart of our business strategy, but these don t happen by chance. They require intentional focus on bringing together a broad set of backgrounds, cultures, experiences, perspectives, and skills in a supportive and inclusive work environment. We hire people with exceptional talent and create an environment in which every individual can thrive professionally and personally.",,,,"['Interpersonal skills', 'Consulting', 'Manager Technology', 'Scheduling', 'HR', 'Business strategy', 'Management', 'Stakeholder management', 'MS Office', 'Recruitment']",2025-06-13 06:09:13
Media AdTech Specialist,Capgemini,5 - 10 years,Not Disclosed,['Kolkata'],"Provide ad operations and/or AdTech operations expertise\nExecute and help implement an AdTech compliance program\nStrong understanding of Programmatic Ad Eco system and Retail Media bidding advertisement.\nYouTube, Connected TV and Video Ads advertisement and hands on ad set up experience.\nDesign and implement advertising solutions tailored for retail media needs.\nBuild operational systems that enhances the productivity of our Ad Operations team\nCollaborate with other departments and stakeholders to identify and solve complex problems\nContinuously testing and improving software solutions to ensure optimal performance and user experience\nCreate and manage strong relationships with DSPs and other relevant players in the ad tech space that can help our clients achieve their objectives.\nSpearhead initiatives to refine and expand digital media services.\nCollaborate with a diverse team of experts to drive innovation. (data scientists, developers, engineers, clients and stakeholders).\nEnsure seamless integration and service delivery.\nApply the latest industry trends and best practices to achieve our clients outcomes\nStay abreast of media regulations and trends affecting digital advertising.\nBuild highly performant AdTech platforms that will support our future growth in the Ads Space\n\nQualifications:\n5 years experience in advertising operations (AdTech ops) and/or revenue operations (Revops).\nStrong understanding of DSPs, digital advertising ecosystems, ad networks, and/or advertising exchanges.\nDemonstrated excellence in client relationship management.\nDemonstrated ability to build and work across teams.\nExperience in Technical Solutions Architecture and design leadership.\nManage multiple projects and prioritize tasks effectively\nBroad knowledge across multiple technology areas Marketing Operation, Ecommerce Domain and Retail Media.\nStrong organizational skills and attention to detail.\nAbility to work independently and as part of a team.\n\nTools:\nFacebook Ads Manager, Pinterest Ad Manager, Instagram ads, DV360, Programmatic, Campaign Manager 360, Google Ad Manager, TTD\nPower-Bi, Excel, PowerPoint will be a plus point.\nYouTube, Videos, CTV related ad platforms.",Industry Type: IT Services & Consulting,Department: Marketing & Communication,"Employment Type: Full Time, Permanent","['DSP', 'Programmatic Buying', 'DV360', 'Facebook Ads Manager', 'Bidding', 'Display Video', 'Google Ads', 'Media Buying', 'Atl', 'Media Planning', 'Pinterest', 'Campaign Management', 'Btl']",2025-06-13 06:09:14
BI Admin,IBM,3 - 6 years,Not Disclosed,['Mumbai'],"Role Overview:\nSeeking a Qlik Sense expert to build enterprise dashboards and enable self-service BI for stakeholders across business functions. All this will be applicable for Tableau as well.\nKey Responsibilities:\nDesign and develop interactive Qlik Sense dashboards and visualizations for business stakeholders.\nAdminister and manage Qlik Sense Enterprise environments including QMC (Qlik Management Console) and Hub.\nSet up and manage user access, security rules, and custom roles within Qlik Sense.\nMonitor system performance and troubleshoot issues related to Qlik Sense services and apps.\nSchedule and manage data reload tasks using Qlik Sense and external tools (e.g., Qlik CLI, QRS API).\nCollaborate with data engineering and BI teams to optimize data models for performance and usability.\nMigrate and consolidate reports from legacy BI tools (e.g., Business Objects) into Qlik Sense.\nImplement governance standards for visualization development and data usage.\nSupport version upgrades, patching, and integration with enterprise identity providers (LDAP/SAML).\nManage publishing streams, apps, and tasks for various business units and project teams.\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\n\nRole OverviewThis needs to applicable for Tableau as well.\nSeeking a Qlik Sense expert to build enterprise dashboards and enable self-service BI for stakeholders across business functions.\nKey Responsibilities:\nDesign and develop interactive Qlik Sense dashboards and visualizations for business stakeholders.\nAdminister and manage Qlik Sense Enterprise environments including QMC (Qlik Management Console) and Hub.\nSet up and manage user access, security rules, and custom roles within Qlik Sense.\nMonitor system performance and troubleshoot issues related to Qlik Sense services and apps.\nSchedule and manage data reload tasks using Qlik Sense and external tools (e.g., Qlik CLI, QRS API).\nCollaborate with data engineering and BI teams to optimize data models for performance and usability.\nMigrate and consolidate reports from legacy BI tools (e.g., Business Objects) into Qlik Sense.\nImplement governance standards for visualization development and data usage.\nSupport version upgrades, patching, and integration with enterprise identity providers (LDAP/SAML).\nManage publishing streams, apps, and tasks for various business units and project teams.\nSkills Required:\nStrong hands-on in Qlik Sense scripting and visualizations\nExperience with Section Access and publishing apps\nUnderstanding of QVD architecture and ETL\nStrong knowledge of QMC, security rules, section access, and governance best practices.\nProficient in Qlik scripting, set analysis, and data modeling techniques.\nFamiliarity with Active Directory, REST APIs, and Qlik CLI for automation.\nPrior work in BFSI dashboards preferred",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['tableau', 'data modeling', 'ldap', 'set analysis', 'data visualization', 'rest', 'cli', 'python', 'data analysis', 'data analytics', 'qlikview development', 'bi', 'data warehousing', 'dashboards', 'business intelligence', 'qlikview', 'sql', 'bi tools', 'active directory', 'bfsi', 'business objects', 'saml', 'etl']",2025-06-13 06:09:16
Application Developer,Accenture,3 - 8 years,Not Disclosed,['Chennai'],"Project Role :Application Developer\n\n\n\n\n\nProject Role Description :Design, build and configure applications to meet business process and application requirements.\n\n\n\nMust have skills :Data Engineering\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an Application Developer, you will engage in the design, construction, and configuration of applications tailored to fulfill specific business processes and application requirements. Your typical day will involve collaborating with team members to understand project needs, developing innovative solutions, and ensuring that applications function seamlessly to support organizational goals. You will also participate in testing and refining applications to enhance user experience and efficiency, while staying updated on industry trends and best practices to continuously improve your contributions.\nRoles & Responsibilities:- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Assist in the documentation of application processes and workflows to ensure clarity and consistency.- Engage in code reviews and provide constructive feedback to peers to foster a culture of continuous improvement.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Data Engineering.- Strong understanding of data modeling and database design principles.- Experience with ETL processes and data integration techniques.- Familiarity with cloud platforms and services related to data engineering.- Ability to work with big data technologies and frameworks.\nAdditional Information:- The candidate should have minimum 3 years of experience in Data Engineering.- This position is based at our Chennai office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data engineering', 'database design', 'data modeling', 'design principles', 'etl', 'c#', 'rest', 'css', 'web services', 'big data technologies', 'hibernate', 'javascript', 'jquery', 'application development', 'sql', 'microservices', 'spring', 'java', 'j2ee', 'json', 'code review', 'html', 'mysql', 'data integration']",2025-06-13 06:09:17
Application Developer,Accenture,15 - 20 years,Not Disclosed,['Chennai'],"Project Role :Application Developer\n\n\n\n\n\nProject Role Description :Design, build and configure applications to meet business process and application requirements.\n\n\n\nMust have skills :Data Engineering\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an Application Developer, you will design, build, and configure applications to meet business process and application requirements. A typical day involves collaborating with various teams to understand their needs, developing solutions, and ensuring that applications function seamlessly to support business operations. You will engage in problem-solving and decision-making processes, contributing to the overall success of projects and initiatives.\nRoles & Responsibilities:- Expected to be an SME.- Collaborate and manage the team to perform.- Responsible for team decisions.- Engage with multiple teams and contribute on key decisions.- Provide solutions to problems for their immediate team and across multiple teams.- Facilitate knowledge sharing sessions to enhance team capabilities.- Monitor project progress and ensure alignment with business objectives.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Data Engineering.- Strong understanding of data pipeline architecture and ETL processes.- Experience with cloud platforms such as AWS or Azure.- Familiarity with data warehousing solutions and big data technologies.- Ability to work with various data storage solutions, including SQL and NoSQL databases.\nAdditional Information:- The candidate should have minimum 5 years of experience in Data Engineering.- This position is based at our Chennai office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sql', 'data pipeline architecture', 'etl', 'data engineering', 'etl process', 'golang', 'data warehousing', 'dbms', 'jquery', 'redis', 'apache tomcat', 'plsql', 'java', 'apache', 'json', 'html', 'mysql', 'big data', 'python', 'oracle', 'microsoft azure', 'javascript', 'sql server', 'nosql', 'application development', 'aws']",2025-06-13 06:09:19
Application Developer,Accenture,3 - 8 years,Not Disclosed,['Pune'],"Project Role :Application Developer\n\n\n\n\n\nProject Role Description :Design, build and configure applications to meet business process and application requirements.\n\n\n\nMust have skills :Data Engineering\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an Application Developer, you will engage in the design, construction, and configuration of applications tailored to fulfill specific business processes and application requirements. Your typical day will involve collaborating with team members to understand project needs, developing innovative solutions, and ensuring that applications function seamlessly to support organizational goals. You will also participate in testing and refining applications to enhance user experience and efficiency, while staying updated on industry trends and best practices to continuously improve your contributions.\nRoles & Responsibilities:- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Assist in the documentation of application processes and workflows to ensure clarity and consistency.- Engage in code reviews and provide constructive feedback to peers to foster a culture of continuous improvement.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Data Engineering.- Strong understanding of data modeling and ETL processes.- Experience with cloud platforms such as AWS or Azure for data storage and processing.- Familiarity with programming languages such as Python or Java for application development.- Knowledge of database management systems, including SQL and NoSQL databases.\nAdditional Information:- The candidate should have minimum 3 years of experience in Data Engineering and Flink.- The candidate must have Flink knowledge.- This position is based at our Pune office.- A 15 years full time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['java', 'data modeling', 'data engineering', 'apache flink', 'etl process', 'web services', 'jsp', 'hibernate', 'sql', 'microservices', 'spring', 'apache', 'spring mvc', 'j2ee', 'code review', 'mysql', 'etl', 'rest', 'python', 'microsoft azure', 'javascript', 'nosql', 'application development', 'spring boot', 'kafka', 'aws']",2025-06-13 06:09:21
Application Developer,Accenture,3 - 8 years,Not Disclosed,['Hyderabad'],"Project Role :Application Developer\n\n\n\n\n\nProject Role Description :Design, build and configure applications to meet business process and application requirements.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:Data Engineering Sr. Advisor demonstrates expertise in data engineering technologies with the focus on engineering, innovation, strategic influence and product mindset. This individual will act as key contributor of the team to design, build, test and deliver large-scale software applications, systems, platforms, services or technologies in the data engineering space. This individual will have the opportunity to work directly with partner IT and business teams, owning and driving major deliverables across all aspects of software delivery.The candidate will play a key role in automating the processes on Databricks and AWS. They collaborate with business and technology partners in gathering requirements, develop and implement. The individual must have strong analytical and technical skills coupled with the ability to positively influence on delivery of data engineering products. The applicant will be working in a team that demands innovation, cloud-first, self-service-first, and automation-first mindset coupled with technical excellence. The applicant will be working with internal and external stakeholders and customers for building solutions as part of Enterprise Data Engineering and will need to demonstrate very strong technical and communication skills.Delivery Intermediate delivery skills including the ability to deliver work at a steady, predictable pace to achieve commitments, decompose work assignments into small batch releases and contribute to tradeoff and negotiation discussions.Domain Expertise Demonstrated track record of domain expertise including the ability to understand technical concepts necessary to do the job effectively, demonstrate willingness, cooperation, and concern for business issues and possess in-depth knowledge of immediate systems worked on.Problem Solving Proven problem solving skills including debugging skills, allowing you to determine source of issues in unfamiliar code or systems and the ability to recognize and solve repetitive problems rather than working around them, recognize mistakes using them as learning opportunities and break down large problems into smaller, more manageable ones& Responsibilities:The candidate will be responsible to deliver business needs end to end from requirements to development into production.Through hands-on engineering approach in Databricks environment, this individual will deliver data engineering toolchains, platform capabilities and reusable patterns.The applicant will be responsible to follow software engineering best practices with an automation first approach and continuous learning and improvement mindset.The applicant will ensure adherence to enterprise architecture direction and architectural standards.The applicant should be able to collaborate in a high-performing team environment, and an ability to influence and be influenced by others.Experience Required:More than 12 years of experience in software engineering, building data engineering pipelines, middleware and API development and automationMore than 3 years of experience in Databricks within an AWS environmentData Engineering experienceExperience Desired:Expertise in Agile software development principles and patternsExpertise in building streaming, batch and event-driven architectures and data pipelinesPrimary\n\n\n\n\nSkills:\nCloud-based security principles and protocols like OAuth2, JWT, data encryption, hashing data, secret management, etc.Expertise in Big data technologies such as Spark, Hadoop, Databricks, Snowflake, EMR, GlueGood understanding of Kafka, Kafka Streams, Spark Structured streaming, configuration-driven data transformation and curationExpertise in building cloud-native microservices, containers, Kubernetes and platform-as-a-service technologies such as OpenShift, CloudFoundryExperience in multi-cloud software-as-a-service products such as Databricks, SnowflakeExperience in Infrastructure-as-Code (IaC) tools such as terraform and AWS cloudformationExperience in messaging systems such as Apache ActiveMQ, WebSphere MQ, Apache Artemis, Kafka, AWS SNSExperience in API and microservices stack such as Spring Boot, Quarkus, Expertise in Cloud technologies such as AWS Glue, Lambda, S3, Elastic Search, API Gateway, CloudFrontExperience with one or more of the following programming and scripting languages Python, Scala, JVM-based languages, or JavaScript, and ability to pick up new languagesExperience in building CI/CD pipelines using Jenkins, Github ActionsStrong expertise with source code management and its best practicesProficient in self-testing of applications, unit testing and use of mock frameworks, test-driven development (TDD)Knowledge on Behavioral Driven Development (BDD) approachAdditional\n\n\n\n\nSkills:\nAbility to perform detailed analysis of business problems and technical environmentsStrong oral and written communication skillsAbility to think strategically, implement iteratively and estimate financial impact of design/architecture alternativesContinuous focus on an on-going learning and development\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data analytics', 'big data technologies', 'data engineering', 'cloud technologies', 'agile software development', 'continuous integration', 'kubernetes', 'python', 'scala', 'ci/cd', 'javascript', 'data bricks', 'lambda expressions', 'spark', 'kafka', 'software engineering', 'aws', 'big data']",2025-06-13 06:09:23
Technology Architect,Accenture,8 - 11 years,Not Disclosed,['Kolkata'],"Project Role :Technology Architect\n\n\n\n\n\nProject Role Description :Review and integrate all application requirements, including functional, security, integration, performance, quality and operations requirements. Review and integrate the technical architecture requirements. Provide input into final decisions regarding hardware, network products, system software and security.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :Cloud Data ArchitectureMinimum\n\n\n\n5 year(s) of experience is required\n\n\n\n\nEducational Qualification :BE or MCA\n\n\nSummary:As a Technology Architect, you will be responsible for reviewing and integrating all application requirements, including functional, security, integration, performance, quality, and operations requirements. Your typical day will involve reviewing and integrating technical architecture requirements, providing input into final decisions regarding hardware, network products, system software, and security, and utilizing Databricks Unified Data Analytics Platform to deliver impactful data-driven solutions.\nRoles & Responsibilities6 or more years of experience in implementing data ingestion pipelines from multiple sources and creating end to end data pipeline on Databricks platform. 2 or more years of experience using Python, PySpark or Scala. Experience in Databricks on cloud. Exp in any of AWS, Azure or GCPe, ETL, data engineering, data cleansing and insertion into a data warehouse\nMust have Skills like Databricks, Cloud Data Architecture, Python Programming Language, Data Engineering.\nProfessional AttributesExcellent writing, communication and presentation skills. Eagerness to learn and develop self on an ongoing basis. Excellent client facing and interpersonal skills.\n\nQualification\n\nBE or MCA",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'data analytics', 'data architecture', 'data engineering', 'data bricks', 'hive', 'scala', 'system software', 'pyspark', 'microsoft azure', 'technology architecture', 'sql', 'pipeline', 'technical architecture', 'java', 'spark', 'kafka', 'data ingestion', 'sqoop', 'hadoop', 'aws', 'etl', 'big data']",2025-06-13 06:09:24
Technology Architect,Accenture,8 - 13 years,Not Disclosed,['Coimbatore'],"Project Role :Technology Architect\n\n\n\n\n\nProject Role Description :Review and integrate all application requirements, including functional, security, integration, performance, quality and operations requirements. Review and integrate the technical architecture requirements. Provide input into final decisions regarding hardware, network products, system software and security.\n\n\n\nMust have skills :Databricks Unified Data Analytics Platform\n\n\n\n\nGood to have skills :Cloud Data ArchitectureMinimum\n\n\n\n7.5 year(s) of experience is required\n\n\n\n\nEducational Qualification : BE or MCA\n\n\nSummary:As a Technology Architect, you will be responsible for reviewing and integrating all application requirements, including functional, security, integration, performance, quality, and operations requirements. Your typical day will involve reviewing and integrating technical architecture requirements, providing input into final decisions regarding hardware, network products, system software, and security, and utilizing Databricks Unified Data Analytics Platform to deliver impactful data-driven solutions.\nRoles & ResponsibilitiesShould have a minimum of 8 years of experience in Databricks Unified Data Analytics Platform. Should have strong educational background in technology and information architectures, along with a proven track record of delivering impactful data-driven solutions. Strong requirement analysis and technical solutioning skill in Data and Analytics Client facing role in terms of running solution workshops, client visits, handled large RFP pursuits and managed multiple stakeholders.\nTechnical Experience10 or more years of experience in implementing data ingestion pipelines from multiple sources and creating end to end data pipeline on Databricks platform. 3 or more years of experience using Python, PySpark or Scala. Experience in Databricks on cloud. Exp in any of AWS, Azure or GCPe, ETL, data engineering, data cleansing and insertion into a data warehouse\nMust have Skills like Databricks, Cloud Data Architecture, Python Programming Language, Data Engineering.\nProfessional AttributesExcellent writing, communication and presentation skills. Eagerness to learn and develop self on an ongoing basis. Excellent client facing and interpersonal skills.\n\nQualification\n\nBE or MCA",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'data analytics', 'data architecture', 'data engineering', 'data bricks', 'scala', 'system software', 'pyspark', 'microsoft azure', 'technology architecture', 'hibernate', 'sql', 'spring', 'technical architecture', 'java', 'information architecture', 'requirement analysis', 'aws', 'etl', 'rfp']",2025-06-13 06:09:26
Application Developer,Accenture,3 - 8 years,Not Disclosed,['Hyderabad'],"Project Role :Application Developer\n\n\n\n\n\nProject Role Description :Design, build and configure applications to meet business process and application requirements.\n\n\n\nMust have skills :Apache Hadoop\n\n\n\n\nGood to have skills :NAMinimum\n\n\n\n3 year(s) of experience is required\n\n\n\n\nEducational Qualification :15 years full time education\n\n\nSummary:As an Application Developer, you will be responsible for designing, building, and configuring applications to meet business process and application requirements. Your typical day will involve collaborating with team members to develop innovative solutions and enhance application functionality.\nRoles & Responsibilities:- Expected to perform independently and become an SME.- Required active participation/contribution in team discussions.- Contribute in providing solutions to work related problems.- Develop and implement software solutions to meet business requirements.- Collaborate with cross-functional teams to enhance application functionality.- Conduct code reviews and provide technical guidance to team members.- Troubleshoot and debug applications to ensure optimal performance.- Stay updated on industry trends and technologies to drive continuous improvement.\nProfessional & Technical\n\n\n\n\nSkills:\n- Must To Have\n\n\n\n\nSkills:\nProficiency in Apache Hadoop.- Strong understanding of distributed computing principles.- Experience with data processing frameworks like MapReduce and Spark.- Hands-on experience in designing and implementing scalable data pipelines.- Knowledge of Hadoop ecosystem components such as HDFS, YARN, and Hive.\nAdditional Information:- The candidate should have a minimum of 3 years of experience in Apache Hadoop.- This position is based at our Hyderabad office.- A 15 years full-time education is required.\n\nQualification\n\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['hive', 'distributed computing', 'spark', 'hadoop', 'mapreduce', 'web services', 'jsp', 'hibernate', 'microservices', 'sql', 'spring', 'react.js', 'apache', 'java', 'j2ee', 'code review', 'rest', 'python', 'data processing', 'javascript', 'application development', 'spring boot', 'node.js', 'troubleshooting', 'yarn']",2025-06-13 06:09:27
IT Manager - Clinical & Medical,Medtronic,12 - 16 years,Not Disclosed,['Hyderabad'],"A Day in the Life We believe that when people from different cultures, genders, and points of view come together, innovation is the result and everyone wins. Medtronic walks the walk, creating an inclusive culture where you can thrive.\n\nThe OSHPI (OU Strategy, Healthcare IT and Product Innovation) Clinical IT Manager role is responsible for creating the best team of Technologists, Data Engineers, and Application Developers to support the Clinical IT organization. This individual will be building and developing a team that provides technical solutions for the entire Clinical IT technology stack. We expect this team to grow significantly over time due to Clinical s need to modernize their enterprise technology stack.\n\nThis team will be collaborating with the US based Clinical IT team and global stakeholders to deliver solutions supporting the global Clinical organization. This team works with other Global IT employees and external service providers to ensure projects are successfully delivered and supported.",,,,"['Product innovation', 'Project management', 'Analytical', 'Employee engagement', 'Manager Technology', 'Clinical research', 'Healthcare', 'Continuous improvement', 'Operations', 'clinical data']",2025-06-13 06:09:29
"Java, Springboot Developer/Consultant Specialist",Hsbc,2 - 6 years,Not Disclosed,['Pune'],"Some careers shine brighter than others.\nIf you re looking for a career that will help you stand out, join HSBC and fulfil your potential. Whether you want a career that could take you to the top, or simply take you in an exciting new direction, HSBC offers opportunities, support and rewards that will take you further.\nHSBC is one of the largest banking and financial services organisations in the world, with operations in 64 countries and territories. We aim to be where the growth is, enabling businesses to thrive and economies to prosper, and, ultimately, helping people to fulfil their hopes and realise their ambitions.\nWe are currently seeking an experienced professional to join our team in the role of Consultant Specialist\nIn this role, you will:\nHighly skilled and experienced Developer Engineer with expertise in Java, Java 8, Microservices, Springboot 3. 0. 0, postgres, JPA, UI -React, Typescript, JS, Apache Flink, Apache Beam, MongoDB.\nStrong knowledge on Google Cloud Platform (GCP) services such as Dataflow, Big Query/Clickhouse, Cloud storage, Pub/Sub, Google Cloud Storage (GCS), and Composer.\nThe ideal candidate should also have hands-on experience with Apache Airflow, Google Kubernetes Engine (GKE), and Python for scripting and automation , automation testing.\nYou will play a critical role in designing, developing, and maintaining scalable, high-performance pipelines and cloud-native solutions\nStrong focus on real-time stream processing using Apache Flink.\nCollaborate with cross-functional teams to define, design, and deliver new features and enhancements.\nMonitor and optimize the performance of data pipelines and applications.\nEnsure data quality, integrity, and security across all data pipelines and storage solutions.\nProvide technical guidance and mentorship to junior team members.\nStay up-to-date with the latest data engineering technologies, best practices, and industry trends.\n\n\n\n\n\n\n\n\n\n\nRequirements\n\n\n\nTo be successful in this role, you should meet the following requirements:\n2 to 6 years of experience in development engineering, with a focus on ETL processes and data pipeline development.\nBachelor s or Master s degree in Computer Science, Engineering, or a related field.\nStrong expertise in Java, SQL for data extraction, transformation, and loading.\nStrong problem-solving, analytical skills and the ability to troubleshoot complex data and application issues.\nExcellent communication and collaboration skills, with the ability to work effectively in a team environment.\nFamiliarity with Helm charts for Kubernetes deployments.\nExperience with monitoring tools like Prometheus, Grafana, or Stackdriver.\nKnowledge of security best practices for cloud and Kubernetes environments.\nKnowledge of DevOps Skills will be an added advantage",Industry Type: Consumer Electronics & Appliances,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'GCP', 'JPA', 'Data quality', 'MongoDB', 'Apache', 'Financial services', 'SQL', 'Python', 'Data extraction']",2025-06-13 06:09:30
Senior Machine Learning Engineer - NLP/Python,Maimsd Technology,4 - 7 years,Not Disclosed,['Pune'],"Notice Period : Immediate - 15 Days\n\nAbout the Role :\n\nWe are seeking a highly skilled Senior Data Scientist to join our team and contribute to cutting-edge projects. As a key member of our data science team, you will leverage your expertise in machine learning, natural language processing, and data engineering to develop innovative solutions that drive business value.\n\nResponsibilities :\n\n- Machine Learning Model Development : Design, develop, and deploy advanced machine learning models, focusing on natural language processing tasks such as text classification, sentiment analysis, and language generation.\n\n- Data Engineering : Extract, transform, and load (ETL) data from various sources, ensuring data quality and consistency.\n\n- NLP Techniques : Apply state-of-the-art NLP techniques, including language models and text processing, to solve complex problems.\n\n- Python and ML Frameworks : Utilize Python programming language and popular ML frameworks like PyTorch or TensorFlow to build efficient and scalable models.\n\n- Cloud and Containerization : Leverage cloud platforms (GCP, AWS) and containerization technologies (Docker) for efficient deployment and management of ML models.\n\n- Database Management : Work with relational databases (Postgres, MySQL) to store, manage, and query large datasets.\n\n- Knowledge Graphs and ML Publications : Contribute to the development of knowledge graphs and stay updated with the latest advancements in the field through research and publications.\n\nQualifications :\n\nExperience : 4-7 years of hands-on experience in data science, with a strong focus on machine learning and natural language processing.\n\nTechnical Skills :\n\n- Proficiency in Python programming language.\n\n- Deep understanding of machine learning algorithms and techniques.\n\n- Expertise in NLP, including language models and text processing.\n\n- Familiarity with ML frameworks like PyTorch or TensorFlow.\n\n- Experience with cloud platforms (GCP, AWS) and containerization (Docker).\n\n- Knowledge of relational databases (Postgres, MySQL).\n\nSoft Skills :\n\n- Strong problem-solving and analytical skills.\n\n- Excellent communication and collaboration abilities.\n\n- Ability to work independently and as part of a team.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Machine Learning', 'Data Science', 'Tensorflow', 'Azure', 'NLP', 'Data Scientist', 'AWS', 'Python']",2025-06-13 06:09:32
Full stack Sr. Software Engineer,Amgen Inc,5 - 8 years,Not Disclosed,['Hyderabad'],"Lets do this. Let’s change the world. In this vital role We are seeking a highly skilled and hands-on Senior Software Engineer – Search to drive the development of intelligent, scalable search systems across our pharmaceutical organization. You'll work at the intersection of software engineering, AI, and life sciences to enable seamless access to structured and unstructured content—spanning research papers, clinical trial data, regulatory documents, and internal scientific knowledge. This is a high-impact role where your code directly accelerates innovation and decision-making in drug development and healthcare delivery",,,,"['Java', 'React.Js', 'Python', 'Typescript', 'Postgresql']",2025-06-13 06:09:33
Senior Artificial Intelligence Engineer - NLP,Rosemallow Technologies,3 - 5 years,Not Disclosed,['Pune'],"Key Responsibilities :\n\nWe are seeking a highly skilled and motivated AI Engineer to join our team. In this role, you will leverage your expertise in AI technologies and the Microsoft ecosystem to design, build, and deploy intelligent agents and automation solutions that enhance business processes and deliver value to our clients.\n\nCandidates must have extensive experience in the Microsoft environment. You will collaborate with cross-functional teams to create innovative solutions using Microsoft tools and platforms.\n\nResponsibilities :\n\nAgent Development :\n\n- Design, implement, and optimize AI agents using Microsoft Azure Framework and related technologies.\n\n- Develop custom AI solutions leveraging Power Automate, Azure OpenAI, and other Microsoft tools.\n\nSolution Integration :\n\n- Deploy AI solutions within client environments, ensuring scalability and seamless integration with existing systems.\n\n- Work with stakeholders to identify automation opportunities and tailor solutions to business needs.\n\nAI Algorithm and Model Implementation :\n\n- Design and implement machine learning algorithms, focusing on natural language processing (NLP) and conversational AI.\n\n- Perform data preprocessing, feature engineering, and model training to create high-performing solutions.\n\nCollaboration and Support :\n\n- Collaborate with cross-functional teams, including software engineers, data scientists, and product managers, to deliver integrated solutions.\n\n- Provide technical guidance and support to ensure the successful adoption and use of AI-driven tools.\n\nContinuous Improvement :\n\n- Stay updated on advancements in AI, machine learning, and Microsofts AI technologies.\n\n- Contribute to knowledge sharing by conducting training sessions and documenting best practices.\n\nPreferred Skills :\n\n- Strong knowledge of Azure OpenAI, Azure AI Search Index and open source libraries such as LangChain and LlamaIndex.\n\n- Proficiency in Python and its AI/ML libraries (e.g., TensorFlow, PyTorch, scikit-learn).\n\n- Familiarity with building and managing cloud-based solutions, preferably on Microsoft Azure.\n\n- Understanding of conversational AI technologies and chatbot frameworks.\n\n- Experience with data analysis tools and techniques to uncover insights and optimize models.\n\nRequirements :\n\n- Bachelors or Masters degree in Computer Science, Data Science, or a related field.\n\n- Proven experience in developing and deploying AI/ML models in real-world applications.\n\n- Strong programming skills, especially in Python, and familiarity with version control systems like Git.\n\n- Extensive experience in the Microsoft environment and related technologies.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Artificial Intelligence', 'Tensorflow', 'Solution Integration', 'NLP', 'PyTorch', 'Conversational AI', 'OpenAI', 'Machine Learning', 'Python']",2025-06-13 06:09:35
Senior Engineer,The TJX Companies Inc,5 - 10 years,Not Disclosed,['Hyderabad'],"TJX Companies\nAt TJX Companies, every day brings new opportunities for growth, exploration, and achievement. You ll be part of our vibrant team that embraces diversity, fosters collaboration, and prioritizes your development. Whether you re working in our four global Home Offices, Distribution Centers or Retail Stores TJ Maxx, Marshalls, Homegoods, Homesense, Sierra, Winners, and TK Maxx, you ll find abundant opportunities to learn, thrive, and make an impact. Come join our TJX family a Fortune 100 company and the world s leading off-price retailer.\nJob Description:\nAbout TJX:\nTJX is a Fortune 100 company that operates off-price retailers of apparel and home fashions. TJX India - Hyderabad is the IT home office in the global technology organization of off-price apparel and home fashion retailer TJX, established to deliver innovative solutions that help transform operations globally. At TJX, we strive to build a workplace where our Associates contributions are welcomed and are embedded in our purpose to provide excellent value to our customers every day. At TJX India, we take a long-term view of your career. We have a high-performance culture that rewards Associates with career growth opportunities, preferred assignments, and upward career advancement. We take well-being very seriously and are committed to offering a great work-life balance for all our Associates.\nWhat you will discover:\nInclusive culture and career growth opportunities\nGlobal IT Organization which collaborates across U.S., Canada, Europe and Australia\nChallenging, collaborative, and team-based environment\nWhat you will do:\nEnterprise Data & Analytics thrives on strong relationships with our business partners and working diligently to address their needs which support TJX India growth and operational stability. On this tightly knit and fast-paced solution delivery team you will be constantly challenged to stretch and think outside the box.\nYou will have a real opportunity to be a part of TJX s transformation to a data driven culture, with the autonomy to work with the business to unlock game changing insights that drive business value. We have modernized our technology stack to focus on the cloud and top tier tools. We are looking for someone who embraces the use of technology to build, manage, and govern data.\nAs a Senior Engineer at TJX India, you will work with a team that partners with the EDA Data Foundation team and delivers products for one or more customer analytics product teams.\nKey Responsibilities:\nAbility to understand the functional and technical design architecture implemented in Datalake on Snowflake\nAbility to lead team of ETL developers and guide them on business / Functional requirement and help in technical area\nExtract, transform and load from various sources system to snowflake using a combination of Talend, Azure Data factory, Snowflake, Data Lake Analytics, Data ingestion to one or more Azure services\nCollaborate effectively within Data Technology teams, Business Information teams to design and build optimized data flows from source to Data visualization\nDevelop and deploy performance optimization methodologies\nStrong in drafting functional and technical documentation\nSupport the Solutioning team from architectural design , testing and implementation\nDesign and architect end-to-end data engineering pipelines, ensuring optimal performance, reliability, and scalability\nDevelop and maintain data integration processes, including ETL/ELT workflows and data ingestion pipelines\nEvaluate and recommend emerging technologies and tools to enhance the data architecture and engineering capabilities\nProvide technical leadership and guidance to junior team members, fostering a culture of collaboration and innovation\nEnsure compliance with data governance and security policies, including data privacy regulations\nBe a part of data modernization projects providing direction on matters of overall design and technical direction, acts as the primary driver toward establishing guidelines and approaches\nExcellent communication and collaboration skills, with the ability to effectively bridge the gap between technical and non-technical audiences\nWhat You will Need (Minimum Qualifications):\nAt least 5 years in-depth, data engineering experience and execution of ETL (Talend) data pipelines, scripting and SQL queries\nA minimum of 2 years of experience in building enterprise level solution on Azure cloud environment with Azure Function, Azure Databricks, Azure Event Hub, Azure Event Grid, Azure data lake, Azure Data factory\nA minimum of 1 year of experience on hands-on coding in python and/or pyspark on Azure Databricks environment\n3 years of experience working in Agile and Scrum frameworks\nExtensive knowledge of Data warehousing and ability to understand star schema\nWork experience in relational database and hands-on on writing SQL queries, stored procedure extensively\nHighly proficient in Data analysis - analysing SQL, Python scripts, ETL/ELT transformation scripts\nImplement CI/CD pipelines for application deployment using Azure DevOps\nPreferred Qualifications:\nExperience in Talend (4+ years) for implementing large scale Data Engineering projects\nExperience in Data Engineering and DevOps knowledge\nExperience in programming languages like SQL, Python / Pyspark, Shell Scripting and Scala.\nExperience in Azure Functions, Azure Data Factory, Logic Apps and Spark platforms\nExperience in Datastage and other ETL tool is a plus\nBachelor s degree in computer science, Engineering, Mathematics, a technical field, or equivalent practical experience.\nCome Discover Different at TJX India. From opportunity and teamwork to growth, we think you ll find that it s so much more than a job. When you re a part of our global TJX family, you have the full support of a diverse, close-knit group of people dedicated to finding great deals and fantastic style. Best of all? They have a lot of fun doing it.\nWe care about our culture, but we also prioritize the tangible stuff (Competitive salaries: check. Solid benefits: check. Plenty of room for advancement: of course). It s our way of empowering you to make your career here.\nWe consider all applicants for employment without regard to race, color, religion, gender, sexual orientation, national origin, age, disability, gender identity and expression, marital or military status. We also provide reasonable accommodations to qualified individuals with disabilities in accordance with the Americans with Disabilities Act and applicable state and local law.\nIn addition to our open door policy and supportive work environment, we also strive to provide a competitive salary and benefits package. TJX considers all applicants for employment without regard to race, color, religion, gender, sexual orientation, national origin, age, disability, gender identity and expression, marital or military status, or based on any individuals status in any group or class protected by applicable federal, state, or local law. TJX also provides reasonable accommodations to qualified individuals with disabilities in accordance with the Americans with Disabilities Act and applicable state and local law.\nAddress:\nSalarpuria Sattva Knowledge City, Inorbit Road\nLocation:\nAPAC Home Office Hyderabad IN",Industry Type: Retail,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Data analysis', 'Coding', 'Datastage', 'Shell scripting', 'Agile', 'Scrum', 'Analytics', 'SQL', 'Python']",2025-06-13 06:09:37
Senior JavaScript Software Engineer,Ciklum,3 - 11 years,Not Disclosed,['Chennai'],"Description\nCiklum is looking for a Senior JavaScript Software Engineer to join our team full-time in India.\n\nWe are a custom product engineering company that supports both multinational organizations and scaling startups to solve their most complex business challenges. With a global team of over 4, 000 highly skilled developers, consultants, analysts and product owners, we engineer technology that redefines industries and shapes the way people live.\n\nAbout the role:\n\nAs a Senior JavaScript Software Engineer, become a part of a cross-functional development team engineering experiences of tomorrow.\n\nClient for this project is a leading global provider of audit and assurance, consulting, financial advisory, risk advisory, tax, and related services. They are launching a digital transformation project to evaluate existing technology across the tax lifecycle and determine the best future state for that technology. This will include decomposing existing assets to determine functionality, assessment of those functionalities to determine the appropriate end state and building of new technologies to replace those functionalities.\n\n\nResponsibilities\n\nParticipate in requirements analysis\n\nCollaborate with US and Vendors teams to produce software design and architecture\n\nWrite clean, scalable code using Angular with Typescript, HTML, CSS, and NET programming languages\n\nParticipate in pull request code review process\n\nTest and deploy applications and systems\n\nRevise, update, refactor and debug code\n\nDevelop, support and maintain applications and technology solutions\n\nEnsure that all development efforts meet or exceed client expectations. Applications should meet requirements of scope, functionality, and time and adhere to all defined and agreed upon standards\n\nBecome familiar with all development tools, testing tools, methodologies and processes\n\nBecome familiar with the project management methodology and processes\n\nEncourage collaborative efforts and camaraderie with on-shore and off-shore team members\n\nDemonstrate a strong working understanding of the best industry standards in software development and version controlling\n\nEnsure the quality and low bug rates of code released into production\n\nWork on agile projects, participate in daily SCRUM calls and provide task updates\n\nDuring design and key development phases, we might need to work a staggered shift as applicable to ensure appropriate overlap with US teams and project deliveries\n\n\n\nRequirements\nWe know that sometimes, you can t tick every box. We would still love to hear from you if you think you will be a good fit\n\n\n6+ years of strong hands-on experience with JavaScript (ES6/ES2015+), HTML5, CSS3\n\n2+ years with hands-on experience with Typescript\n\n2+ years of hands-on experience with Angular 11+ component architecture, applying design patterns\n\nExperience with Angular 11+ and migrating to newer versions\n\nExperience with Angular State management or NgXs\n\nExperience with RxJS operators\n\nHands on experience with Kendo UI or Angular material or SpreadJS libraries\n\nExperience with Nx - Nrwl/Nx library for monorepos\n\nSkill for writing reusable components, Angular services, directives and pipes\n\nHands-on experience on C#, SQL Server, OOPS Concepts, Micro Services Architecture\n\nAt least two-year hands-on experience on . NET Core, ASP. NET Core Web API, SQL, NoSQL, Entity Framework 6 or above, Azure, Database performance tuning, Applying Design Patterns, Agile\n\n. Net back-end development with data engineering expertise. Experience with MS Fabric as a data platform/ Snowflake or similar tools would be a plus, but not a must need\n\nSkill for writing reusable libraries\n\nComfortable with Git Git hooks using PowerShell, Terminal or a variation thereof\n\nFamiliarity with agile development methodologies\n\nExcellent Communication skills both oral written\n\nExcellent troubleshooting and communication skills, ability to communicate clearly with US counterparts\n\n\n\nDesirable\n\nExposure to micro-frontend architecture\n\nKnowledge on Yarn, Webpack, Mongo DB, NPM, Azure Devops Build/Release configuration\n\nSignalR, ASP. NET Core and WebSockets\n\nThis is an experienced level position, and we will train the qualified candidate in the required applications\n\nWillingness to work extra hours to meet deliverables\n\nExposure to Application Insights Adobe Analytics\n\nUnderstanding of cloud infrastructure design and implementation\n\nExperience in CI/CD configuration\n\nGood knowledge of data analysis in enterprises\n\nExperience with Databricks, Snowflake\n\nExposure to Docker and its configurations, Experience with Kubernetes\n\n\n\nWhats in it for you\n\nCare: your mental and physical health is our priority. We ensure comprehensive company-paid medical insurance, as well as financial and legal consultation\n\nTailored education path: boost your skills and knowledge with our regular internal events (meetups, conferences, workshops), Udemy licence, language courses and company-paid certifications\n\nGrowth environment: share your experience and level up your expertise with a community of skilled professionals, locally and globally\n\nFlexibility: hybrid work mode at Chennai or Pune\n\nOpportunities: we value our specialists and always find the best options for them. Our Resourcing Team helps change a project if needed to help you grow, excel professionally and fulfil your potential\n\nGlobal impact: work on large-scale projects that redefine industries with international and fast-growing clients\n\nWelcoming environment: feel empowered with a friendly team, open-door policy, informal atmosphere within the company and regular team-building events",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Performance tuning', 'Data analysis', 'Testing tools', 'Project management', 'Consulting', 'Javascript', 'HTML', 'Scrum', 'Troubleshooting', 'SQL']",2025-06-13 06:09:38
AI/ML Sr Software Engineer,Amgen Inc,5 - 9 years,Not Disclosed,['Hyderabad'],"Lets do this. Let’s change the world. In this vital role We are looking for a creative and technically skilled Senior Software Engineer/AI Engineer – Search to help design and build cutting-edge AI-powered search solutions for the pharmaceutical industry. In this role, you'll develop intelligent systems that surface the most relevant insights from clinical trials, scientific literature, regulatory documents, and internal knowledge assets. Your work will empower researchers, clinicians, and decision-makers with faster, smarter access to the right information.\n.",,,,"['Java', 'Artificial Intelligence', 'Aiml', 'Microservices', 'Python', 'Machine Learning']",2025-06-13 06:09:40
Machine Learning Engineer,Avani Infosoft,1 - 2 years,2.4-6.6 Lacs P.A.,['Bengaluru( Kamakshipalya )'],"Responsibilities:\n* Develop machine learning models using TensorFlow, NumPy & OpenCV.\n* Implement computer vision solutions with CNNs & object detection techniques.\n\n\nProvident fund",Industry Type: BPM / BPO,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Handling', 'Object Detection', 'Opencv', 'Deployment', 'Model Development', 'Tensorflow', 'Cnn', 'Computer Vision', 'Machine Learning', 'Numpy', 'Deep Learning']",2025-06-13 06:09:41
AI Delivery Lead,Tata Technologies,10 - 15 years,Not Disclosed,['Pune'],"10+ years of experience in machine learning, AI, or data science, with at least 3 years in a leadership role.Proven track record of delivering ML/AI projects at scale in an enterprise environment.Deep functional expertise in AI/ML, coupled with a solid understan ding of data science solution developmentExperience in managing teams and stakeholder expectations.Strong communication skills and demonstrated experience in stakeholder managementTechnical SkillsStrong expertise in ML frameworks and tools (e.g., TensorFlow, P yTorch, Scikit-learn).Good experience in Gen AI (various LLMs and its framework)Proficiency in programming languages like Python, R, or Java.Familiarity with cloud platforms (AWS, Azure, GCP) for ML workflows.Solid understanding of MLOps principles, including CI/CD pipelines, model deployment, and monitoring.Knowledge of big data technologies (e.g., Spark, Hadoop) and data engineering bes t practices.Preferred Certifications AWS/Azure/GCP Certified ML",,,,"['CI/CD pipelines', 'Java', 'Azure', 'R', 'GCP', 'Hadoop', 'Spark', 'AWS', 'Python']",2025-06-13 06:09:43
GCP Hadoop Developer/Lead,Onix,5 - 10 years,Not Disclosed,"['Pune', 'Bengaluru']","Job Summary\nWe are seeking a highly skilled Hadoop Developer / Lead Data Engineer to join our data engineering team based in Bangalore or Pune. The ideal candidate will have extensive experience with Hadoop ecosystem technologies and cloud-based big data platforms, particularly on Google Cloud Platform (GCP). This role involves designing, developing, and maintaining scalable data ingestion, processing, and transformation frameworks to support enterprise data needs.\n\nMinimum Qualifications\nBachelor's degree in computer science, Computer Information Systems, or related technical field.\n5-10 years of experience in software engineering or data engineering, with a strong focus on big data technologies.\nProven experience in implementing software development life cycles (SDLC) in enterprise environments.\nTechnical Skills & Expertise\nBig Data Technologies:\nExpertise in Hadoop platform, Hive, and related ecosystem tools.\nStrong experience with Apache Spark (using SQL, Scala, and/or Java).\nExperience with real-time data streaming using Kafka.\nProgramming Languages & Frameworks:\nProficient in PySpark and SQL for data processing and transformation.\nStrong coding skills in Python.\nCloud Technologies (Google Cloud Platform):\nExperience with BigQuery for data warehousing and analytics.\nFamiliarity with Cloud Composer (Airflow) for workflow orchestration.\nHands-on with DataProc for managed Spark and Hadoop clusters.\nResponsibilities\nDesign, develop, and implement scalable data ingestion and transformation pipelines using Hadoop and GCP services.\nBuild real-time and batch data processing solutions leveraging Spark, Kafka, and related technologies.\nEnsure data quality, governance, and lineage by implementing automated validation and classification frameworks.\nCollaborate with cross-functional teams to deploy and operationalize data analytics tools at enterprise scale.\nParticipate in production support and on-call rotations to maintain system reliability.\nFollow established SDLC practices to deliver high-quality, maintainable solutions.\nPreferred Qualifications\nExperience leading or mentoring data engineering teams.\nFamiliarity with CI/CD pipelines and DevOps best practices for big data environments.\nStrong communication skills with an ability to collaborate across teams.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'Bigquery', 'Hadoop', 'GCP', 'Python', 'Airflow', 'Kafka', 'Dataproc', 'SQL', 'Hive', 'cloud composer', 'Spark', 'Google Cloud Platforms']",2025-06-13 06:09:45
"Sr. Product Manager, AI",Conga,3 - 5 years,Not Disclosed,['Bengaluru( Kadubeesanahalli )'],"Job Title: Sr. Product Manager\nLocation: Bangalore\nReports to: Manager, Product Management\n\nA quick snapshot\n\nAs a Product Manager on the Conga Discovery AI team, you will help define, build, and launch AI-driven metadata extraction solutions on top of the Conga platform. Youll bring your experience in AIand ideally Contract Lifecycle Management (CLM)to shape products that help enterprises discover, process, and analyze critical business data. This role is hands-on, requiring you to be comfortable demonstrating features, collaborating on implementations, and working closely with scrum teams to drive value for our customers.\n\nWhy it’s a big deal\n\nMetadata extraction is at the core of how businesses understand their documents and processes. By leveraging Discovery AI, you’ll help enterprises transform manual, time-consuming tasks into automated workflows that reduce errors, improve compliance, and deliver actionable insights. Your role will be central to creating and refining solutions that can scale to handle the most complex enterprise needs.\n\nAre you the person we’re looking for?\n\nRelevant Experience. You should have more than 5 years of experience in Product Management in B2B SaaS, preferably with data extraction, AI, or document automation products.\n\nDemonstrate. A success in conceptualizing and launching new product features from initial idea to market adoption.\n\nAI or machine learning. Knowledge of fundamentals and an interest in applying them to solve real-world business challenges.\n\nCLM. Exposure or understanding in CLM is a strong plus, as it ties closely into many metadata extraction use cases.\n\nResearch and Creativity. Conduct market and user research to identify new opportunities for AI-driven features.\n\nCustomer feedback. You will gather continuous customer feedback to iterate and prioritize feature development that delivers tangible customer value.\n\nMaintain strong partnerships. With professional services and support teams to address implementation details and customer escalations.\n\nDemo. You should confidently demo features to internal stakeholders, customers, and prospects to showcase Discovery AI capabilities.\n\nAnalyze and prioritize. You will use data analytics, user feedback, and market insights to guide product decisions and roadmap priorities.\n\nBalance. customer requirements, technical feasibility, and time-to-market considerations in a fast-paced environment.\n\nCustomer experience. You will regularly engage with customers to understand their needs and pain points, ensuring Discovery AI addresses real-world challenges.\n\nDocument. New workflows, provide training materials or guidelines, and gather post-launch feedback.\n\nEducation. Bachelor’s degree in Engineering or equivalent; a higher degree is a plus.\n\nHere’s what will give you an edge\n\nChampion the Customer. You appreciate that customers are the heart of the business, and you’re dedicated to delivering solutions that solve their problems while meeting them where they are.\n\nNatural collaborator. You thrive in an Agile, cross-functional setting, seeking input from engineers, designers, data scientists, and peers to make well-rounded product decisions.\n\nPassion. Your genuine enthusiasm for AI and data-driven solutions is evident in your work. You love delving into technical details, exploring new possibilities, and shaping products that redefine how companies operate.\n\nStrong Communication. You will communicate releases, risks, and timelines effectively to leadership and cross-functional stakeholders.\n\nCollaborate. With marketing to position and promote new features that drive adoption and user satisfaction.\n\nLeadership skills. You will work closely with scrum teams to ensure clear backlog priorities, smooth sprint planning, and timely delivery.",Industry Type: Software Product,Department: Product Management,"Employment Type: Full Time, Permanent","['Product Strategy', 'Product Management', 'Product Portfolio', 'Product Life Cycle Management', 'Product Planning']",2025-06-13 06:09:47
Business Analyst,Maintwiz Technologies,2 - 4 years,Not Disclosed,['Chennai'],"MaintWiz is calling Business Analysts who have delivered requirements related documentation for SaaS products. Selected candidates will be working in Plant Maintenance and Industrial Asset Management.\nGather and document business requirements by working closely with customers, product managers, and technical teams.\nAnalyze maintenance workflows, asset hierarchies, and reliability practices to identify product and process improvement opportunities.\nTranslate functional needs into structured specifications, user stories, and wireframes for product development.\nConduct competitor benchmarking and industry research to guide product strategy and feature development.\nSupport customer onboarding and adoption by creating process maps, documentation, and training inputs.\nCollaborate with QA and development teams to validate feature implementation aligns with business intent.\nUse data analytics to derive insights on asset performance, work order efficiency, and user behavior.\nAct as a domain expert in maintenance and reliability, with a strong grasp of engineering fundamentals and operational challenges.\nExposure to AI based tools to improve productivity and speed-to-market\n\nProficiency in English, Hindi and one Regional Language Required",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Business Analysis', 'customer onboarding', 'Plant Maintenance', 'operational management', 'product development', 'SaaS', 'Industrial Asset Management']",2025-06-13 06:09:49
Analyst MICOE,Shell,4 - 9 years,Not Disclosed,['Chennai'],"Whats the role\nThe Analyst MICOE role requires an experienced designer of data visualizations and complex reporting with a strong understanding of the Financial metrics and how they influence business performance. This designer will work with an integrated team of developers, business analysts, Interface Managers in creating complex data visualizations using Microsoft Power BI and other tools. Once a solution is live, run and maintain the same, take care of year roll-over and minor changes/enhancements to the solution.\nWhat you will be doing\nSwift understanding of the business model, expectations from business, linking it to the business strategy and the way KPIs are measured.\nDesign and create data visualizations in Microsoft Power BI.\nExpertise in creating info links to ERPs or source Databases to fetch data into visualization tools.\nFacilitate design review sessions with senior analysts and key stakeholders to refine and elaborate the data visualizations.\nWork on validation and testing to ensure it meets the requirements.\nCreates the design specification, deployment plans, and other technical documents for respective design activities.\nCreate user reference document and Training videos on how to use.\nSupport to troubleshooting problems, providing workarounds etc.,\nEstimates the magnitude and time requirements to complete all tasks and provides accurate and timely updates to the team on progress.\nEnsure on time, high quality deliverables and meeting project milestones and deadlines (Project Plan On A Page) within budget with minimal supervision.\nAssists peers in the business application, development technologies etc.,\nParticipates in peer review of work products such as code, designs, and test plans produced by other team members.\nEnsure IRM compliance of tools, maintain evidence for the user access management and support any system audit.\nSupport the team in creation, run & maintain, tool enhancements and deployment of latest technologies and functionalities.\nWorking in a Global and Cross cultural environment & displaying strong personal effectiveness.\nWhat we need from you\nMinimum of 4 years of data visualization experience but can be relaxed for the right person with the required drive and appetite for action.\nMust have good experience in creating data models, complex visualizations of data based upon identified information needs, business model, Data set / ERP system and stakeholder requirements\nShould have global reporting system exposure (GSAP, GPMR, HANA & ECC).\nExperienced in data modelling, data extraction through SQL. Coding skills would be a plus (Python, VBA, R, etc.)\nMUST be a Power BI developer. Experienced in working on complex reporting/data visualizations using Microsoft Power BI\nStrong understanding of Data management (SQL/Azure).\nDemonstrated experience developing end to end data flow structures, resulting in intuitive BI dashboards with high uptake.\nCandidates should have in depth experience working with end users to refine identified business needs through in-depth design reviews and information sessions.\nCandidates should be results driven, detailed orientated and work well within a dynamic and creative team.\nWork exposure to MS Access, MS Excel and Power Point is essential.\nPossess good written and oral communication skills as well as presentation skills\nAbility to learn quickly and adapt to new environments.\nSelf-driven and motivated individual that takes pride in the development of high quality, on time solutions, run and maintain the existing tools and support team for other deliverables.",Industry Type: Power,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data visualization', 'data management', 'data modeling', 'ECC', 'ms access', 'GSAP', 'HANA', 'GPMR']",2025-06-13 06:09:50
BO Business Analyst,Exavalu,5 - 10 years,Not Disclosed,[],"Analyze and document current claims processes, workflows, and systems.\nGather, validate, and manage business requirements from claims stakeholders.\nCollaborate with IT, operations, and product teams to design solutions that improve claims handling efficiency, accuracy, and customer satisfaction.\nSupport the implementation of claims system enhancements or migrations (eg, Guidewire ClaimsCenter, Duck Creek, etc).\nIdentify gaps in current claims operations and propose process optimization or automation opportunities.\nAssist in UAT (User Acceptance Testing), defect tracking, and resolution.\nCreate and maintain process maps, business rules, and functional requirement documents.\nEnsure alignment between business needs and technical solutions, acting as a liaison between business and IT teams.\nSupport reporting and dashboard requirements for claims analytics.\nFacilitate workshops, stakeholder meetings, and status updates.\nRequired Skills and Qualifications:\n5+ years of experience as a Business Analyst, preferably within P&C insurance claims.\nStrong understanding of claims processes (FNOL, adjudication, settlement, subrogation, etc).\nExperience with claims platforms like Guidewire, Duck Creek, or similar.\nProficiency in documenting requirements, process flows, and business rules.\nAbility to liaise between technical and non-technical teams effectively.\nFamiliarity with Agile and Waterfall project delivery methodologies.\nStrong analytical, problem-solving, and communication skills.\nPreferred Skills:\nExposure to data analytics or BI tools (Tableau, Power BI, etc).\nKnowledge of regulatory and compliance aspects of claims management.\nCertification in Business Analysis (CBAP, CCBA) or Agile (Scrum Master, Product Owner).",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Automation', 'Process optimization', 'Claims', 'Business analysis', 'Analytical', 'Agile', 'IT operations', 'Project delivery', 'User acceptance testing', 'Analytics']",2025-06-13 06:09:51
Business Analyst,Exavalu,5 - 10 years,Not Disclosed,[],"Gather requirements, define functional specs/Use cases or User stories/Epics, and work with technical teams/Business stakeholders.\n5 to 15 years of experience in Life Insurance/Annuities and policy admin platforms like OIPA, Vantage, ALIP etc\nStrong problem-solving, communication, and analytical skills.\nALIP experience preferred but not mandatory\n",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Analytical skills', 'Administration', 'Usage', 'Business Analyst']",2025-06-13 06:09:53
Analyst,National Australia Bank (NAB),3 - 10 years,Not Disclosed,['Gurugram'],"NAB is looking for Analyst to join our dynamic team and embark on a rewarding career journey.\nThe Senior Analyst plays a crucial role in driving data-driven decision-making processes within the organization\nThis position involves analyzing complex data sets, generating actionable insights, and providing strategic recommendations to support key business initiatives\nKey Responsibilities:Data Analysis:Conduct in-depth analysis of large and complex datasets to extract meaningful insights",,,,"['Excel', 'Senior Analyst', 'Finance', 'Focus', 'Banking', 'Manager Technology', 'Recruitment']",2025-06-13 06:09:55
Analyst,Oliver Wyman,2 - 4 years,Not Disclosed,['Mumbai'],"Company: Marsh\nDescription:\nMarsh is seeking candidates for the following position based in the Mumbai(Powai) office.\nAnalyst / Senior Analyst - Insurance services (Grade B1/B2)\nWhat can you expect:\nExcellent exposure to complex diverse insurance handling work.\nOpportunity to enhance insurance knowledge and understanding build on client management skills.\nAs a new colleague, you will be provided with Business Overview/Insights, in-depth process training, roles responsibilities overview, expectations of various stakeholders to make you successful in this role.\nWithin the first 30 days, we expect you to gain good understanding of the role and requirement that it entails. Within 60 days, attain competency have a good understanding of process and systems finally within 90 days be able to handle the work processing with limited support\nWhat is in it for you\nHolidays (As Per the location - the final decision will depend on business requirements at that time)\nShared Transport (Provided the address falls in accepted service zone)\nWe will count on you to:\nProcess and self-Management:\nContribute to achieve the Service Level Agreements (SLAs), Key Performance Indicators (KPIs) and business objectives\nAdheres to Company policies and is in compliance at all times.\nMaintains performance standards.\nUpdates reports based on predefined templates on a regular basis to ensure accurate entry\nMaintains a basic understanding of the core aspects of relevant Insurance and related legislation\nCompleting all training related activities when assigned\nCognizant of completing all assigned activities within the stipulated time\nEnsuring all production targets are met in training\nEnsuring all production targets are met in BAU (Post training)\nEnsuring all Quality targets are met in training\nEnsuring all quality targets are met in BAU (Post training)\nCommunicate status of work, any issues with team managers and manager on time\nSchedule adherence is a must\nNo unplanned leaves\nReady to work in below shifts :\n6:30 PM to 3:30 AM\nYour hiring manager and HR will inform you the shift requirement for the team you are interviewing for.\nCompliance, Regulatory and Procedural Responsibilities\nEnsure that all statutory regulations and company procedures are followed to protect clients, colleagues and the business interest of the company\nAppropriate usage of Marsh Specialitys systems to monitor, record and retain information\nDemonstrate clear understanding of regulatory requirements\nProactively ensures compliance with regulatory and risks framework\nAdheres to policies, guidelines and operating procedures\nKeeps own knowledge and expertise up to date and relevant\nIdentifies and evaluates risks appropriately. Recognises how own actions impact on compliance\nWhat you need to have:\nPrior years of work experience\nGraduate in any stream\nFair understanding of Insurance Broking business and dynamics\nGood command over spoken and written English\nSuperior comprehension and articulation skills\nAbility to prioritize and organise tasks, work within stiff timelines\nEye for detail and innovative mindset\nBasic knowledge of MS Office\nWillingness to learn and adapt to changes\nWhat makes you stand out:\nInsurance certification from The Institutes, USA\nPost-Graduation/Certificate Courses in Insurance\nAny prior experience on any insurance process\nWhile the above are basic requirements mentioned, the role may not be limited to only these listed.\nMarsh, a business of Marsh McLennan (NYSE: MMC), is the world s top insurance broker and risk advisor. Marsh McLennan is a global leader in risk, strategy and people, advising clients in 130 countries across four businesses: Marsh, Guy Carpenter, Mercer and Oliver Wyman. With annual revenue of $24 billion and more than 90,000 colleagues, Marsh McLennan helps build the confidence to thrive through the power of perspective. For more information, visit marsh.com, or follow on LinkedIn and X.",Industry Type: Management Consulting,Department: Other,"Employment Type: Full Time, Permanent","['Training', 'Basic', 'insurance processing', 'Service level', 'Usage', 'Compliance', 'Senior Analyst', 'risk advisor', 'MS Office', 'Client management']",2025-06-13 06:09:56
Project Manager \\/ Business Analyst,Systems Plus Solutions Pvt Ltd,8 - 10 years,Not Disclosed,['Pune'],"We are seeking a highly organized and experienced Project Manager / Business Analyst who can effectively manage projects from initiation to closure while translating business needs into technical solutions. The ideal candidate will bridge the gap between stakeholders and technical teams, ensuring projects are delivered on time, within scope, and meet business objectives.\n\n\nKey Responsibilities:\nLead end-to-end project management activities following Agile/Scrum and traditional SDLC methodologies.\nWork closely with business stakeholders to gather, analyze, and document detailed requirements.\nFacilitate daily stand-ups, sprint planning, retrospectives, and stakeholder demos.\nDevelop project charters, work breakdown structures (WBS), project plans, and timelines.\nIdentify project risks, develop mitigation strategies, and maintain risk logs.\nEnsure clear communication across all project team members and stakeholders.\nTrack and report on project performance, adjusting plans as needed to meet delivery objectives.\nCoordinate across cross-functional teams including developers, QA, business users, and leadership.\nLead UAT (User Acceptance Testing) efforts and ensure solutions meet quality standards.\nManage change control processes and ensure traceability of requirements throughout the project lifecycle.\n\nRequired Skills Experience:\n8-10 years of experience in Project Management and/or Business Analysis.\nStrong understanding of Agile/Scrum frameworks and SDLC processes.\nProven ability to manage multiple projects simultaneously.\nExcellent written and verbal communication skills.\nProficient in tools such as Jira, Confluence, MS Project, or similar project management tools.\nStrong organizational, problem-solving, and analytical skills.\nExperience creating BRDs (Business Requirement Documents), FRDs (Functional Requirement Documents), and user stories.\n\n\nPreferred Qualifications:\nPMP, CSM (Certified Scrum Master), or PMI-ACP certification preferred.\nExperience in software, web development, or data platform projects is a plus.\nFamiliarity with tools such as Power BI, ServiceNow, or Salesforce is advantageous.",Industry Type: IT Services & Consulting,Department: Project & Program Management,"Employment Type: Full Time, Permanent","['PMP', 'Business analysis', 'Project management', 'Web development', 'Manager Business Analyst', 'WBS', 'Scrum', 'User acceptance testing', 'SDLC', 'Salesforce']",2025-06-13 06:09:58
Analyst - Direct Display,Aegis Media,1 - 3 years,Not Disclosed,['Chennai'],"The purpose of this role is to assist with the planning, reviewing and optimisation of Display campaigns whilst supporting the team in reporting and managing client accounts.\nJob Description:\nKey responsibilities:\nFocuses on day-to-day execution\nProactively reviews and manages client data to ensure optimal performance on all campaigns\nTracks and reports on campaign results, gathers data analysis and participates in weekly calls\nGenerates campaign reports and is responsible for pacing, QA and trafficking\nDevelops and maintains accurate project plans for client status updates\nLocation:\nChennai\nBrand:\nParagon\nTime Type:\nFull time\nContract Type:\nPermanent",Industry Type: Advertising & Marketing,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['QA', 'Data analysis', 'Senior Analyst', 'Management']",2025-06-13 06:09:59
Business Analyst,Manektech,4 - 8 years,Not Disclosed,['Ahmedabad'],"Job Overviews\nDesignation: Business Analyst\nLocation: Ahmedabad\nWork Mode: Work from Office\nVacancy: 1\nExperience: 4.0 To 8.0\n\nManekTech is looking for Business Analyst to join our dynamic team and embark on a rewarding career journey.\nEvaluating business processes, anticipating requirements, uncovering areas for improvement, and developing and implementing solutions.\nLeading ongoing reviews of business processes and developing optimization strategies.\nStaying up-to-date on the latest process and IT advancements to automate and modernize systems.\nConducting meetings and presentations to share ideas and findings.\nPerforming requirements analysis.\nDocumenting and communicating the results of your efforts.\nEffectively communicating your insights and plans to cross-functional team members and management.\nGathering critical information from meetings with various stakeholders and producing useful reports.\nWorking closely with clients, technicians, and managerial staff.\nProviding leadership, training, coaching, and guidance to junior staff.\nAllocating resources and maintaining cost efficiency.\nEnsuring solutions meet business needs and requirements.\nPerforming user acceptance testing.\nManaging projects, developing project plans, and monitoring performance.\nUpdating, implementing, and maintaining procedures.\nPrioritizing initiatives based on business needs and requirements.\nServing as a liaison between stakeholders and users.\nManaging competing resources and priorities.\nMonitoring deliverables and ensuring timely completion of projects.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['vendor management', 'project management', 'documentation', 'business analysis', 'budgeting', 'strategic planning', 'operations', 'people management skills', 'service delivery', 'leadership', 'user acceptance testing', 'requirement analysis', 'cost efficiency', 'reporting']",2025-06-13 06:10:01
Business Analyst,Purplle.com,1 - 4 years,Not Disclosed,['Mumbai'],"As a Business Analyst within our Operations team, you will play a pivotal role in leveraging data-driven insights to optimize processes, enhance operational efficiency, and drive strategic decision-making. Your responsibilities will involve utilizing a mix of technical skills and business acumen to interpret complex data sets, generate actionable insights, and support operational improvements.\n\nKey Responsibilities:\nCollaborate closely with cross-functional teams to understand operational requirements, identify opportunities for improvement, and define key performance indicators (KPIs) to measure success.\n\nAnalyze large datasets using SQL, Python, and advanced Excel techniques to extract, transform, and visualize data for operational reporting and decision making purposes.\n\nDevelop and maintain automated reports and dashboards using Power BI, Power Query, Tableau, Data Studios, Looker, and other visualization tools to communicate insights effectively.\n\nConduct in-depth analysis and interpretation of operational data to identify trends, patterns, and anomalies, providing actionable recommendations to drive operational excellence.\n\nUtilize strong aptitude and logical thinking to solve complex operational challenges and contribute to strategic initiatives that optimize workflows and enhance overall business performance.\n\nFoster strong stakeholder relationships through effective communication, presenting insights, and collaborating on operational strategies and solutions.\n\nRequired Skills and Qualifications:\nBachelor's degree in Business Administration, Data Science, Computer Science, or a related field.\n\nProficiency in SQL, Python, and advanced Excel for data analysis and manipulation.\n\nHands-on experience with Power BI, Power Query, Tableau, Data Studios, Looker, or similar visualization tools.\n\nStrong analytical and problem-solving abilities with a sharp aptitude for logical thinking.\n\nExcellent communication skills and the ability to effectively engage with stakeholders at all levels.\n\nProven track record of successfully managing and prioritizing multiple projects simultaneously in a fast-paced environment.\n\nStrong collaborative skills and the ability to work effectively in a team-oriented culture.\n\nPreferred Qualifications:\nExperience in the e-commerce industry or within a high-growth, dynamic environment.\n\nKnowledge of additional programming languages, statistical tools, or data modeling techniques.\n\nCertifications in business analysis, data visualization, or related fields.",Industry Type: Beauty & Personal Care,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'data analysis', 'bi', 'strong analytical skills', 'power bi', 'business analysis', 'analysis', 'dashboards', 'sql', 'business administration', 'data studio', 'excel', 'power query', 'tableau', 'data modeling', 'advanced excel', 'data visualization', 'communication skills']",2025-06-13 06:10:02
Analyst,Gallagher Service Center (GSC),1 - 6 years,Not Disclosed,['Pune'],"Role & responsibilities\n1) Hands on experience on Defined Benefit and Defined contribution scheme knowledge.\n2) Ability to learn and grasp scheme nuances for DB & DC pensions\n3) Identify and escalate unresolved issues by keeping supervisors in loop.\n4) Good communication skills with have proven ability in handling member/client calls and good email writing skills.\n5) Ability to prioritize items.\nPreferred candidate profile\n1) Should have worked on at the minimum, one of the core pension administration task such as Transfers, Leavers, Retirements, Divorce and Death or Payroll processing and Scheme rules.\n2) Candidate should be any Graduate or a Post Graduate (Non-technical)\n3) Excellent command over English (comprehension and written).\n4) Good academic record (50% or above).\n5) Basic knowledge in insurance or finance\n6) Open to rotational shifts.\n7) Ability to work in a team and exhibit collaboration.\n8) Should be able to adapt to organizational changes.\n9) Proficiency in Microsoft Office (especially Excel, Word).\n10) The candidate has to be a quick learner, high on energy and must have a flair for customer processes.\n\nGraduates only.\n\nVenue : 3rd Floor, Delta 2, Giga space, Viman Nagar, Pune 411014\n\nTime: 10AM - 2 PM\n\nInterested candidates kindly do the pre-registration for a smooth interview process on the below link.",Industry Type: Analytics / KPO / Research,"Department: BFSI, Investments & Trading","Employment Type: Full Time, Permanent","['UK pensions', 'Defined Benefits', 'Defined Contribution', 'Retirement', 'Pension Administration']",2025-06-13 06:10:04
Analyst - Affordable Housing,Castellan Real Estate Partners,2 - 3 years,Not Disclosed,[],"TITLE: Analyst - Affordable Housing\n\nLOCATION: India, (Remote)\n\nCOMPANY OVERVIEW:\nThe Company is a US based fully integrated real estate investment and development firm with business lines devoted to real estate investments, development, lending, and property management with an emphasis on affordable housing development. The Company has invested in equity and debt real estate transactions with a market value more than $2 billion. With a team of highly skilled professionals, the Company strives to combine unique vision, market knowledge, and a keen ability to execute any rehabilitation or new construction real estate developments including section 42 low-income housing tax credit deals.\n\nBUSINESS OVERVIEW:\nAffordable housing refers to rental and ownership housing units that are affordable to families and households at income levels lower than the areas median income. Development of affordable housing require funding from public (federal government, state government and local municipalities) and private sources (banks and other lenders). The team monitors funding availability announcements and prepares applications to request funds for acquisition of land, construction of housing and related costs.\nPOSITION OVERVIEW:\nThe analyst works on structuring of affordable housing deals and applying for funding to various governmental sources. The funding applications are MS Excel based and support documentation in MS Word and Adobe PDF. The role requires a high degree of initiative, responsibility, diligence, and professionalism. The candidate is expected to have excellent oral and written communication skills in English. The analyst will work remotely as part of a team and expected to be available from 3.30 PM IST to 12.30 midnight.\n\nRESPONSIBILITIES:\nPreparing MS Excel based funding applications (Funding sources include Low Income Housing Tax Credits, Tax-Exempt Bonds, State, County, City funding sources)\nPreparing support documentation for the funding applications\nMonitoring and tracking documents and checklists\nResearch of new funding opportunities / expansion opportunities\nWorking on other aspects of the development finance process, including predevelopment and construction closing, conversion and compliance\nGeneral support of all other initiatives and special projects as required.\n\nQUALIFICATIONS:\nGraduate / Post Graduate Degree in Business, Construction Management,\nMinimum 2 years of experience in Real Estate / Underwriting / Financial Modelling/ Financial Research\nBackground in Financial Analysis, Project/Infrastructure finance, Credit Analysis, Valuation, Balance Sheet Analysis is preferred.\nDegrees from US based colleges and/or work experience in the US are desirable.\nProficient in Microsoft Excel, Word, and Adobe PDF",Industry Type: Engineering & Construction,Department: Finance & Accounting,"Employment Type: Full Time, Permanent","['tax exempt', 'Credit Analysis', 'Construction Management', 'Compliance', 'Valuation', 'Funding', 'Underwriting', 'Checklist', 'Financial Modelling']",2025-06-13 06:10:05
Business Analyst,Guidehouse,3 - 5 years,Not Disclosed,['Thiruvananthapuram'],"Job Summary\nResponsible for conducting analysis in order to come up with solutions to business problems and help in developing more substantial business process roles to businesses and the clients. Bridges the gap between IT and the business by assessing processes, determine requirements and deliver data-driven recommendations and solutions to the stakeholders that technically, financially and functionally feasible.\nEntry-level technical individual contributor\n- Responsible for making minor changes in technical systems and processes to solve problems\n- Explains facts, policies and practices related to job area\nOrganization Impact & Communication\n- Achieves operational and/or billable targets within job area, which has some impact on results\n- Executes daily work thinking innovatively in their work product and solutions\n- Explains facts, policies and practices related to job area\n- Communicates with contacts typically within the department on matters that involve obtaining or providing information requiring some explanation or interpretation in order to reach agreement.\nInnovation & Complexity\n- Entry-level technical individual contributor\n- Assists on smaller, less complex projects or task-related activities\n- Problems faced are not typically difficult or complex, makes minor changes in processes to solve problems\nLeadership & Talent Management\n- Responsible for making minor changes in technical systems and processes to solve problems\n- Work is of limited scope, typically on smaller, less complex projects or task related activities. Work is closely supervised\nKnowledge & Experience\n- Requires broad theoretical job knowledge typically obtained through advanced education or specialized in skillset\n- Requires a University Degree and minimum 2 years of prior relevant experience; some roles may require graduate-level education Masters or Doctorate degree (Relevant experience may be substituted for formal education or advanced degree\n\nWhat You Will Do:\nUnderstand the requirements from product owners and coordinate with the developers\nCreate, edit, and manage Jira tickets, ensuring they are clear and prioritized for development efficiency\nNeed to play Scrum Master and BA role for small projects.\nAssist with and contribute to technical documentation that outlines backend systems and infrastructure processes\nCollaborate with Technical Product Analysts and Product Managers to derive data insights and improve developer experience\n\nWhat You Will Need:\n3-5 Years of experience as Business Analyst\nBachelors degree in computer science or IT.\nStrong collaborative skills and the ability to communicate effectively across technical teams.\nKeen attention to detail with excellent written communication skills.\nSQL knowledge and excel preferred\nHealthcare Domain knowledge.\n\nWhat Would Be Nice to Have:\nExposure to cloud infrastructure environments\nBasic understanding of Scrum/Agile methodologies\nExposure to Analytics projects.\nBasic statistics",Industry Type: Management Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Agile', 'scrum', 'SQL', 'Agile Methodology', 'JIRA']",2025-06-13 06:10:07
Data Scientist,Ford,1 - 8 years,Not Disclosed,['Chennai'],"Ford/GDIA Mission and Scope:\n\n\nCreating the future of smart mobility requires the highly intelligent use of data, metrics, and analytics. That s where you can make an impact as part of our Global Data Insight & Analytics team. We are the trusted advisers that enable Ford to clearly see business conditions, customer needs, and the competitive landscape. With our support, key decision-makers can act in meaningful, positive ways. Join us and use your data expertise and analytical skills to drive evidence-based, timely decision-making.\n\nThe Global Data Insights and Analytics (GDI&A) department at Ford Motors Company is looking for qualified people who can develop scalable solutions to complex real-world problems using Machine Learning, Big Data, Statistics, Econometrics, and Optimization. The goal of GDI&A is to drive evidence-based decision making by providing insights from data. Applications for GDI&A include, but are not limited to, Connected Vehicle, Smart Mobility, Advanced Operations, Manufacturing, Supply chain, Logistics, and Warranty Analytics.\n\nAbout the Role:\n\nYou will be part of the FCSD analytics team, playing a critical role in leveraging data science to drive significant business impact within Ford Customer Service Division. As a Data Scientist, you will translate complex business challenges into data-driven solutions. This involves partnering closely with stakeholders to understand problems, working with diverse data sources (including within GCP), developing and deploying scalable AI/ML models, and communicating actionable insights that deliver measurable results for Ford.\nQualifications:\n\n\nAt least 2 years of relevant professional experience applying data science techniques to solve business problems. This includes demonstrated hands-on proficiency with SQL and Python.\n\nBachelors or Masters degree in a quantitative field (e. g. , Statistics, Computer Science, Mathematics, Engineering, Economics).\n\nHands-on experience in conducting statistical data analysis (EDA, forecasting, clustering, hypothesis testing, etc. ) and applying machine learning techniques (Classification/Regression, NLP, time-series analysis, etc. ).\n\n\n\n\n\nTechnical Skills:\n\n\nProficiency in SQL, including the ability to write and optimize queries for data extraction and analysis.\n\nProficiency in Python for data manipulation (Pandas, NumPy), statistical analysis, and implementing Machine Learning models (Scikit-learn, TensorFlow, PyTorch, etc. ).\n\nWorking knowledge in a Cloud environment (GCP, AWS, or Azure) is preferred for developing and deploying models.\n\nExperience with version control systems, particularly Git.\n\nNice to have: Exposure to Generative AI / Large Language Models (LLMs).\n\n\n\n\n\nFunctional Skills:\n\n\nProven ability to understand and formulate business problem statements.\n\nAbility to translate Business Problem statements into data science problems.\n\nStrong problem-solving ability, with the capacity to analyze complex issues and develop effective solutions.\n\nExcellent verbal and written communication skills, with a demonstrated ability to translate complex technical information and results into simple, understandable language for non-technical audiences.\n\nStrong business engagement skills, including the ability to build relationships, collaborate effectively with stakeholders, and contribute to data-driven decision-making.\n\n\n\nBuild an in-depth understanding of the business domain and data sources, demonstrating strong business acumen.\n\nExtract, analyze, and transform data using SQL for insights.\n\nApply statistical methods and develop ML models to solve business problems.\n\nDesign and implement analytical solutions, contributing to their deployment, ideally leveraging Cloud environments.\n\nWork closely and collaboratively with Product Owners, Product Managers, Software Engineers, and Data Engineers within an agile development environment.\n\nIntegrate and operationalize ML models for real-world impact.\n\nMonitor the performance and impact of deployed models, iterating as needed.\n\nPresent findings and recommendations effectively to both technical and non-technical audiences to inform and drive business decisions.",Industry Type: Auto Components,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Supply chain', 'Data analysis', 'Analytical', 'Customer service', 'Econometrics', 'Forecasting', 'Analytics', 'SQL', 'Logistics', 'Data extraction']",2025-06-14 05:57:31
Data Scientist,Ford,1 - 8 years,Not Disclosed,['Chennai'],"Ford/GDIA Mission and Scope:\n\n\nCreating the future of smart mobility requires the highly intelligent use of data, metrics, and analytics. That s where you can make an impact as part of our Global Data Insight & Analytics team. We are the trusted advisers that enable Ford to clearly see business conditions, customer needs, and the competitive landscape. With our support, key decision-makers can act in meaningful, positive ways. Join us and use your data expertise and analytical skills to drive evidence-based, timely decision-making.\n\nThe Global Data Insights and Analytics (GDI&A) department at Ford Motors Company is looking for qualified people who can develop scalable solutions to complex real-world problems using Machine Learning, Big Data, Statistics, Econometrics, and Optimization. The goal of GDI&A is to drive evidence-based decision making by providing insights from data. Applications for GDI&A include, but are not limited to, Connected Vehicle, Smart Mobility, Advanced Operations, Manufacturing, Supply chain, Logistics, and Warranty Analytics.\n\nAbout the Role:\n\nYou would be part of FCSD analytics team.\n\nAs a Data Scientist on the team, you will collaborate within the team and work with business partners to understand business problems and explore data from various sources in GCP-Data Factory, wrangle them to develop solutions using AI/ML algorithms to provide actionable insights that deliver key results to Ford.\n\nThe potential candidate should have hands-on experience in building statistical/machine learning models adhering to the best practices of development and deployment in cloud environment. This role requires a solid problem-solving skill, business acumen, and passion for leveraging data science/AI skills to drive business results.\nQualifications:\n\n\nAt least 2 years of relevant work experience in solving business problems using data science\n\nBachelors/master s degree in quantitative domain, Statistics, Computer science, Mathematics, Engineering with MBA from a premier institute (BE, MS, MBA, BSc/MSc -Computer science/Statistics) or any other equivalent\n\n2+ years of experience with SQL, Python delivering analytical solutions in production environment.\n\nAt least 1 year of experience working in Cloud environment (GCP or AWS or Azure)\n\n2+ years of experience in conducting statistical data analysis (EDA, forecasting, clustering, etc. , ) and machine learning techniques (Classification/Regression, NLP)\n\n\nTechnical Skills:\n\n\nProficient in BigQuery/SQL, Python\n\nAdvanced SQL knowledge to handle large data, optimize queries.\n\nWorking knowledge in GCP environment (Big Query, Vertex AI) to develop and deploy machine Learning models\n\nNice to have: Exposure to Gen AI/LLM\n\n\nFunctional Skills:\n\n\nUnderstanding and formulating business problem statements\n\nConvert Business Problem statement into data science problems.\n\nSelf-motivated with excellent verbal and written skills\n\nHighly credible in organizational, time management and decision making.\n\nExcellent Problem-Solving and Interpersonal skills\n\nJob Responsibilities\n\n\nBuild an in-depth understanding of the business domain and data sources.\n\nExtract, Analyse data from database/data warehouse to gain insights, discover trends and patterns with clear objectives in mind.\n\nDesign and implement scalable analytical solutions in Google cloud environment.\n\nWork closely with Product Owner, Product Manager, Software engineers and Data engineers to build products in agile environment.\n\nOperationalize AI/ML/LLM models by integrating with upstream and downstream business processes.\n\nCommunicate results to business teams through effective presentations.\n\nWork with business partners through problem formulation, data management, solutions development, operationalization, and solutions management\n\nIdentify opportunities to build analytical solutions driving business value, leveraging various data sources.",Industry Type: Auto Components,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Supply chain', 'Computer science', 'Data analysis', 'Data management', 'Machine learning', 'Agile', 'Analytics', 'SQL', 'Python', 'Logistics']",2025-06-14 05:57:33
Data Scientist,Ford,1 - 8 years,Not Disclosed,['Chennai'],"The Global Data Insights and Analytics (GDI&A) department at Ford Motors Company is looking for qualified people who can develop scalable solutions to complex real-world problems using Machine Learning, Big Data, Statistics, Econometrics, and Optimization. The candidate should possess the ability to translate a business problem into an analytical problem, identify the relevant data sets needed for addressing the analytical problem, recommend, implement, and validate the best suited analytical algorithm(s), and generate/deliver insights to stakeholders. Candidates are expected to regularly refer to research papers and be at the cutting-edge with respect to algorithms, tools, and techniques. The role is that of an individual contributor; however, the candidate is expected to work in project teams of 2 to 3 people and interact with Business partners on regular basis.\n\nMasters degree in computer science, Operational research, Statistics, Applied mathematics, or in any other engineering discipline.\n\nProficient in querying and analyzing large datasets using BigQuery on GCP. Strong Python skills for data wrangling and automation.\n\n2+ years of hands-on experience in Python programming for data analysis, machine learning, and with libraries such as NumPy, Pandas, Matplotlib, Scikit-learn, TensorFlow, PyTorch, NLTK, spaCy, and Gensim.\n\n2+ years of experience with both supervised and unsupervised machine learning techniques.\n\n2+ years of experience with data analysis and visualization using Python packages such as Pandas, NumPy, Matplotlib, Seaborn, or data visualization tools like Dash or QlikSense.\n\n1+ years experience in SQL programming language and relational databases.\n\n\nUnderstand business requirements and analyze datasets to determine suitable approaches to meet analytic business needs and support data-driven decision-making by FCSD business team\n\nDesign and implement data analysis and ML models, hypotheses, algorithms and experiments to support data-driven decision-making\n\nApply various analytics techniques like data mining, predictive modeling, prescriptive modeling, math, statistics, advanced analytics, machine learning models and algorithms, etc. ; to analyze data and uncover meaningful patterns, relationships, and trends\n\nDesign efficient data loading, data augmentation and data analysis techniques to enhance the accuracy and robustness of data science and machine learning models, including scalable models suitable for automation\n\nResearch, study and stay updated in the domain of data science, machine learning, analytics tools and techniques etc. ; and continuously identify avenues for enhancing analysis efficiency, accuracy and robustness",Industry Type: Auto Components,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Automation', 'Data analysis', 'Analytical', 'Machine learning', 'Predictive modeling', 'data visualization', 'Data mining', 'SQL', 'Python']",2025-06-14 05:57:35
Data Scientist (AI For Computer Vision),Philips,3 - 8 years,Not Disclosed,['Bengaluru'],"Job Title Data Scientist (AI for Computer Vision)\nJob Description\nWe are seeking an experienced Data Scientist specializing in AI for Computer Vision to join our dynamic team. Your primary responsibilities will include developing, fine-tuning, and optimizing AI models for computer vision applications, driving innovation in various healthcare applications. You will work closely with cross-functional teams, including machine learning engineers, software developers, and product managers, to deliver state-of-the-art AI solutions.\nYour role:\nExplore and develop innovative Artificial Intelligence (AI) algorithms for healthcare applications\nCreate and refine AI algorithms for pre- and post-processing of images and videos, focusing on data from various imaging modalities\nDevelop and implement machine learning and deep learning techniques for segmentation, classification, and statistical modeling.\nDemonstrate expertise in image processing, object detection, segmentation, and classification.\nProficient in Python programming\nPossess a strong understanding of algorithms and frameworks such as TensorFlow, PyTorch, and Keras\nExperienced with version control systems (e.g., Git) and software development practices\nDevelop and Optimize Computer Vision Models: Design, train, and fine-tune DL models for real-world applications\nData Preparation & Engineering: Gather, clean, and preprocess large-scale image and video datasets for training and evaluation of computer vision models\nExperimentation & Model Evaluation: Conduct A/B testing and assess model performance using quantitative metrics (e.g., IoU, mAP, precision, recall)\nResearch & Innovation: Stay updated with the latest advancements in computer vision, deep learning, and related technologies\nDeployment & Scaling: Work with ML engineers to deploy models into production environments using cloud platforms (AWS, Azure) and frameworks like TensorFlow, PyTorch, and OpenCV\nCollaboration & Communication: Work closely with cross-functional teams to integrate computer vision solutions into business processes and applications.\n\nYoure the right fit if:\nBachelor s or master s Degree: In computer science, AI, Data Science, Machine Learning, or a related field\nExperience: 3+ years in machine learning, deep learning, or AI research, with at least 1 year of hands-on experience in developing computer vision-based AI applications\nProgramming Proficiency: Strong proficiency in Python and ML frameworks like TensorFlow and PyTorch\nDomain Knowledge: Knowledge of computer vision, natural language processing (NLP), or multimodal AI applications\nTechnical Skills: Familiarity with computer vision techniques and fine-tuning of models\nProblem-Solving Skills: Strong problem-solving skills and the ability to work in a fast-paced, research-driven environment\nMLOps Tools: Hands-on experience with MLOps tools (e.g., MLflow, Kubeflow, Docker, Kubernetes)\nEthical AI: Understanding of ethical AI and bias mitigation in computer vision models.\nPublications and Contributions: Strong publication record or contributions to open-source AI projects.\n\nHow we work together\nWe believe that we are better together than apart. For our office-based teams, this means working in-person at least 3 days per week.\nthis role is an office role.\n\nIf you re interested in this role and have many, but not all, of the experiences needed, we encourage you to apply. You may still be the right candidate for this or other opportunities at Philips. Learn more about our culture of impact with care here .\n#LI-EU\n#LI-Hybrid\n#LI-PHILIN",Industry Type: Medical Services / Hospital,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Computer vision', 'Version control', 'Image processing', 'Artificial Intelligence', 'Machine learning', 'Healthcare', 'Natural language processing', 'Open source', 'Python']",2025-06-14 05:57:37
Data Scientist 1,Paypal,25 - 30 years,Not Disclosed,['Bengaluru'],"The Company\nPayPal has been revolutionizing commerce globally for more than 25 years. Creating innovative experiences that make moving money, selling, and shopping simple, personalized, and secure, PayPal empowers consumers and businesses in approximately 200 markets to join and thrive in the global economy.\nWe operate a global, two-sided network at scale that connects hundreds of millions of merchants and consumers. We help merchants and consumers connect, transact, and complete payments, whether they are online or in person. PayPal is more than a connection to third-party payment networks. We provide proprietary payment solutions accepted by merchants that enable the completion of payments on our platform on behalf of our customers.\nOur beliefs are the foundation for how we conduct business every day. We live each day guided by our core values of Inclusion, Innovation, Collaboration, and Wellness. Together, our values ensure that we work together as one global team with our customers at the center of everything we do - and they push us to ensure we take care of ourselves, each other, and our communities.\nJob Summary:\nWhat you need to know about the role -\nWe are looking for Senior Data Scientist with experience of managing large portfolios to develop PayPal s Risk strategy within the Rewards, Loyalty and Honey Risk solutions team. This portfolio comprises PayPal s global marketing initiatives and campaigns, as well as customized experiences developed for the company s highest-priority strategic partnerships.\n\nMeet our team\nPayPal s Reward, Loyalty & Honey risk team fraud Risk team is responsible for assessing and managing buyer and seller side risk exposures for all global marketing initiatives. The team is also responsible for partnering with the corresponding Business Units and Product teams to align with and influence their strategic priorities, educate business partners about Risk management principles, and collaboratively optimize the Risk treatments and experiences for these unique products and partners.\nJob Description:\nThis role will be the end-to-end owner of the Reward payout risk solutions and is responsible for end-to-end management of marketing budget loss and decline rates. Day-to-day duties include data analysis, monitoring and forecasting, creating the logic for and implementing risk rules and strategies, collaborate with ML engineering, product and technology teams on attribute, model and platform requirements, mentoring juniors in team, and communicating with global stakeholders to ensure we deliver the best possible customer experience while meeting loss rate targets.\nIf you re interested in enriching PayPal rewarding experiences and think customer back, then this is the right team for to join!\nYour day to day :\nEach Senior Data Scientist on this team has full ownership of Reward payout portfolio and is responsible for end-to-end management of loss and decline rates for marketing budget.\nDay-to-day duties include data analysis, monitoring and forecasting, creating the logic for and implementing risk rules and strategies, collaborate with ML engineering, product and technology teams on attribute, model and platform requirements, communicating with global stakeholders to ensure we deliver the best possible customer experience while meeting loss rate targets.\nWorks independently and proficiently. Accountable for own results. Reviews are mainly for consultation and sharing ideas.\nWorks on multiple assignments simultaneously and in all areas of a standard project in the area of responsibility.\nWhat do you need to bring\nStrong analytical skills -- ability to build quick estimates using back-of-the-envelope analysis, structure (and, if needed, execute) more complex analyses, pull together business cases and forecasts to navigate through multi-dimensional sets of tradeoffs.\nEnthusiasm for data-driven problem solving within a fast-paced environment is a must. In addition, experience with Microsoft Excel or statistical software, working knowledge of SQL or other relational database languages, and hands-on experience in data analysis involving large data sets are strongly desired Work experience at the management consulting firms is a plus.\nPolished communication and influence skills - risk decision scientists need to collaborate cross-functionally with product managers, data scientists, business owners, and customers to learn from subject-matter experts, present findings in a clear and concise manner, and reach alignment on how to execute risk strategies. Demonstrated ability to influence groups and effectively resolve conflicts is required.\nAn innate intellectual curiosity, and a willingness to build awareness of current payments industry and risk management best practices. PayPal is constantly innovating by introducing new products and entering new markets, so successful risk analysts on this team must quickly get up speed on new content areas. You will be expected to become an expert in your specific domain.\nCan-do attitude, team player, energetic personality, ability to work well under pressure in a fast-paced and constantly changing environment to meet deadlines. The successful risk analyst is a self-starter who has the resilience to learn from their mistakes and reach their true potential.\nIdentify typical problems and issues during normal course of work and take proactive actions to solve them with minimum guidance. Recommends changes to policies and establishes procedures that affect immediate organization(s).\nExercises discretion in resolving a variety of issues in imaginative as well as practical ways.\nImpact of decision has moderate to large reach\nOffers insight for and contributes to improving existing technology, tools, processes, and business solutions. Adds value to brainstorming sessions\nBS/BA degree with 6+ years of related professional experience or master s degree with 4+ years of related experience.\nFocuses primarily on how to achieve overall analytic objectives of a project with speed and quality.\nSuggests ideas for operational plans and objectives\nClear subject matter expert within group / geography\nWorks independently and proficiently on multiple assignments simultaneously with speed and quality\nManage junior decision/data scientists.\nPreferred Qualification:\nSubsidiary:\nPayPal\nTravel Percent:\n0\nFor the majority of employees, PayPals balanced hybrid work model offers 3 days in the office for effective in-person collaboration and 2 days at your choice of either the PayPal office or your home workspace, ensuring that you equally have the benefits and conveniences of both locations.\nOur Benefits:\nWe have great benefits including a flexible work environment, employee shares options, health and life insurance and more. To learn more about our benefits please visit https://www.paypalbenefits.com .\nWho We Are:\nClick Here to learn more about our culture and community.\nCommitment to Diversity and Inclusion\nPayPal provides equal employment opportunity (EEO) to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, pregnancy, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state, or local law. In addition, PayPal will provide reasonable accommodations for qualified individuals with disabilities. If you are unable to submit an application because of incompatible assistive technology or a disability, please contact us at talentaccommodations@paypal.com .\nBelonging at PayPal:\nOur employees are central to advancing our mission, and we strive to create an environment where everyone can do their best work with a sense of purpose and belonging. Belonging at PayPal means creating a workplace with a sense of acceptance and security where all employees feel included and valued. We are proud to have a diverse workforce reflective of the merchants, consumers, and communities that we serve, and we continue to take tangible actions to cultivate inclusivity and belonging at PayPal.\nAny general requests for consideration of your skills, please Join our Talent Community .\nWe know the confidence gap and imposter syndrome can get in the way of meeting spectacular candidates. Please don t hesitate to apply.",Industry Type: FinTech / Payments,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data analysis', 'Excel', 'Management consulting', 'Subject Matter Expert', 'Risk management', 'Business solutions', 'Forecasting', 'Operations', 'Monitoring', 'SQL']",2025-06-14 05:57:40
Data Scientist 1,Xoom,1 - 3 years,Not Disclosed,['Bengaluru'],"Job Summary\nWhat you need to know about the role -\nWe are looking for Senior Data Scientist with experience of managing large portfolios to develop PayPal s Risk strategy within the Rewards, Loyalty and Honey Risk solutions team. This portfolio comprises PayPal s global marketing initiatives and campaigns, as well as customized experiences developed for the company s highest-priority strategic partnerships.\n\nMeet our team\nPayPal s Reward, Loyalty & Honey risk team fraud Risk team is responsible for assessing and managing buyer and seller side risk exposures for all global marketing initiatives. The team is also responsible for partnering with the corresponding Business Units and Product teams to align with and influence their strategic priorities, educate business partners about Risk management principles, and collaboratively optimize the Risk treatments and experiences for these unique products and partners.\nJob Description\nThis role will be the end-to-end owner of the Reward payout risk solutions and is responsible for end-to-end management of marketing budget loss and decline rates. Day-to-day duties include data analysis, monitoring and forecasting, creating the logic for and implementing risk rules and strategies, collaborate with ML engineering, product and technology teams on attribute, model and platform requirements, mentoring juniors in team, and communicating with global stakeholders to ensure we deliver the best possible customer experience while meeting loss rate targets.\nIf you re interested in enriching PayPal rewarding experiences and think customer back, then this is the right team for to join!\nYour day to day\nEach Senior Data Scientist on this team has full ownership of Reward payout portfolio and is responsible for end-to-end management of loss and decline rates for marketing budget.\nDay-to-day duties include data analysis, monitoring and forecasting, creating the logic for and implementing risk rules and strategies, collaborate with ML engineering, product and technology teams on attribute, model and platform requirements, communicating with global stakeholders to ensure we deliver the best possible customer experience while meeting loss rate targets.\nWorks independently and proficiently. Accountable for own results. Reviews are mainly for consultation and sharing ideas.\nWorks on multiple assignments simultaneously and in all areas of a standard project in the area of responsibility.\nWhat do you need to bring\nStrong analytical skills -- ability to build quick estimates using back-of-the-envelope analysis, structure (and, if needed, execute) more complex analyses, pull together business cases and forecasts to navigate through multi-dimensional sets of tradeoffs.\nEnthusiasm for data-driven problem solving within a fast-paced environment is a must. In addition, experience with Microsoft Excel or statistical software, working knowledge of SQL or other relational database languages, and hands-on experience in data analysis involving large data sets are strongly desired Work experience at the management consulting firms is a plus.\nPolished communication and influence skills - risk decision scientists need to collaborate cross-functionally with product managers, data scientists, business owners, and customers to learn from subject-matter experts, present findings in a clear and concise manner, and reach alignment on how to execute risk strategies. Demonstrated ability to influence groups and effectively resolve conflicts is required.\nAn innate intellectual curiosity, and a willingness to build awareness of current payments industry and risk management best practices. PayPal is constantly innovating by introducing new products and entering new markets, so successful risk analysts on this team must quickly get up speed on new content areas. You will be expected to become an expert in your specific domain.\nCan-do attitude, team player, energetic personality, ability to work well under pressure in a fast-paced and constantly changing environment to meet deadlines. The successful risk analyst is a self-starter who has the resilience to learn from their mistakes and reach their true potential.\nIdentify typical problems and issues during normal course of work and take proactive actions to solve them with minimum guidance. Recommends changes to policies and establishes procedures that affect immediate organization(s).\nExercises discretion in resolving a variety of issues in imaginative as well as practical ways.\nImpact of decision has moderate to large reach\nOffers insight for and contributes to improving existing technology, tools, processes, and business solutions. Adds value to brainstorming sessions\nBS/BA degree with 6+ years of related professional experience or master s degree with 4+ years of related experience.\nFocuses primarily on how to achieve overall analytic objectives of a project with speed and quality.\nSuggests ideas for operational plans and objectives\nClear subject matter expert within group / geography\nWorks independently and proficiently on multiple assignments simultaneously with speed and quality\nManage junior decision/data scientists.",Industry Type: Internet,Department: Other,"Employment Type: Full Time, Permanent","['Data analysis', 'Excel', 'Management consulting', 'Subject Matter Expert', 'Risk management', 'Business solutions', 'Forecasting', 'Operations', 'Monitoring', 'SQL']",2025-06-14 05:57:42
data scientist,Bluphlux,3 - 7 years,Not Disclosed,['Pune'],"Job Summary\nAs a Data Scientist at Bluphlux, you will play a pivotal role in leveraging advanced data analytics and machine learning techniques to enhance our AI-driven recruitment solutions. You will work closely with our engineering and product teams to develop and implement models that improve the accuracy and efficiency of our resume ranking system.\n\nKey Responsibilities\nDevelop and implement machine learning models to enhance our recruitment platform.\nAnalyze large datasets to extract meaningful insights and improve our algorithms.\nCollaborate with cross-functional teams to integrate data-driven solutions into our products.\nContinuously monitor and refine models to ensure optimal performance.\nStay updated with the latest advancements in data science and machine learning to drive innovation.\nRequired Qualifications\nBachelors or Masters degree in Data Science, Computer Science, Statistics, or a related field.\nProven experience as a Data Scientist or similar role.\nStrong knowledge of machine learning algorithms and data analysis techniques.\nProficiency in programming languages such as Python or R.\nExperience with data visualization tools and techniques.\nPreferred Skills\nExperience with natural language processing (NLP) and language models.\nFamiliarity with cloud platforms such as AWS or Azure.\nStrong problem-solving skills and attention to detail.\nExcellent communication and teamwork abilities.\nAlso important:\n\nIf the candidate says his notice period is more than 2 months then try to negotiate.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data analysis', 'data science', 'Machine learning', 'Programming', 'Natural language processing', 'Data analytics', 'data visualization', 'Python', 'Recruitment']",2025-06-14 05:57:44
Data Scientist,Barclays,1 - 7 years,Not Disclosed,['Pune'],"Join us as a Data Scientist at Barclays, where youll take part in the evolution of our digital landscape, driving innovation and excellence. Youll harness cutting-edge technology to revolutionise our digital offerings, ensuring unparalleled customer experiences. As a part of the Service Operations team, you will deliver technology stack, using strong analytical and problem solving skills to understand the business requirements and deliver quality solutions. Youll be working on complex technical problems that will involve detailed analytical skills and analysis. This will be done in conjunction with fellow engineers, business analysts and business stakeholders.\nTo be successful as a Data Scientist you should have experience with:\nEssential Skills\nSolid understanding of machine learning concepts and model deployment.\nPrior experience in a data science role, indicating a strong foundation in the field.\nAdvanced coding proficiency in Python, with the ability to design, test, and correct complex scripts.\nProficiency in SQL, for managing and manipulating data.\nWorking in an Agile manner and leading Agile teams using Jira.\nSome other highly valued skills include:\nExcellent modelling skills, as evidenced by an advanced degree or significant experience.\nStrong quantitative and statistical skills, enabling logical and methodical problem-solving.\nGood understanding and experience of big data technologies and the underlying approach.\nGood interpersonal skills for maintaining relationships with multiple business areas, including senior leadership and compliance.\nAbility to manage laterally and upwards across multiple discipline technical areas.\nVersion control using Bitbucket, Gitlab, etc.\nCloud experience (AWS, Azure or GCP)\nYou may be assessed on key critical skills relevant for success in role, such as risk and controls, change and transformation, business acumen, strategic thinking and digital and technology, as well as job-specific technical skills.\nThis role is based in Pune.\nPurpose of the role\nTo use innovative data analytics and machine learning techniques to extract valuable insights from the banks data reserves, leveraging these insights to inform strategic decision-making, improve operational efficiency, and drive innovation across the organisation.\nAccountabilities\nIdentification, collection, extraction of data from various sources, including internal and external sources.\nPerforming data cleaning, wrangling, and transformation to ensure its quality and suitability for analysis.\nDevelopment and maintenance of efficient data pipelines for automated data acquisition and processing.\nDesign and conduct of statistical and machine learning models to analyse patterns, trends, and relationships in the data.\nDevelopment and implementation of predictive models to forecast future outcomes and identify potential risks and opportunities.\nCollaborate with business stakeholders to seek out opportunities to add value from data through Data Science.\nAssistant Vice President Expectations\nTo advise and influence decision making, contribute to policy development and take responsibility for operational effectiveness. Collaborate closely with other functions/ business divisions.\nLead a team performing complex tasks, using well developed professional knowledge and skills to deliver on work that impacts the whole business function. Set objectives and coach employees in pursuit of those objectives, appraisal of performance relative to objectives and determination of reward outcomes\nIf the position has leadership responsibilities, People Leaders are expected to demonstrate a clear set of leadership behaviours to create an environment for colleagues to thrive and deliver to a consistently excellent standard. The four LEAD behaviours are: L - Listen and be authentic, E - Energise and inspire, A - Align across the enterprise, D - Develop others.\nOR for an individual contributor, they will lead collaborative assignments and guide team members through structured assignments, identify the need for the inclusion of other areas of specialisation to complete assignments. They will identify new directions for assignments and/ or projects, identifying a combination of cross functional methodologies or practices to meet required outcomes.\nConsult on complex issues; providing advice to People Leaders to support the resolution of escalated issues.\nIdentify ways to mitigate risk and developing new policies/procedures in support of the control and governance agenda.\nTake ownership for managing risk and strengthening controls in relation to the work done.\nPerform work that is closely related to that of other areas, which requires understanding of how areas coordinate and contribute to the achievement of the objectives of the organisation sub-function.\nCollaborate with other areas of work, for business aligned support areas to keep up to speed with business activity and the business strategy.\nEngage in complex analysis of data from multiple sources of information, internal and external sources such as procedures and practises (in other areas, teams, companies, etc). to solve problems creatively and effectively.\nCommunicate complex information. Complex information could include sensitive information or information that is difficult to communicate because of its content or its audience.\nInfluence or convince stakeholders to achieve outcomes.",Industry Type: Financial Services,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Coding', 'Analytical', 'Machine learning', 'Agile', 'Business strategy', 'Assistant Vice President', 'Operations', 'SQL', 'Service operations', 'Python']",2025-06-14 05:57:47
Data Scientist,Spearhead Professional,11 - 20 years,27.5-42.5 Lacs P.A.,"['New Delhi', 'Bengaluru', 'Mumbai (All Areas)']",Responsibilities:\nDevelop machine learning models using Python & PyTorch.\nOptimize data architecture for efficiency & accuracy.\nCollaborate with cross-functional teams on AI projects.,Industry Type: Not mentioned,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Architecture', 'Artificial Intelligence', 'Machine Learning', 'Deep Learning', 'Pytorch', 'Data Science', 'Natural Language Processing', 'Python']",2025-06-14 05:57:49
"Data Scientist - Noida , Pune , Chennai",R Systems International,4 - 8 years,Not Disclosed,"['Noida', 'Pune', 'Chennai']","Looking for experienced Data Scientist Candidates.\nJob Description\nLeveraging knowledge in analytical and statistical algorithms to assist stakeholders in improving their business\nPartnering on the design and implementation of statistical data quality procedures for existing and new data sources\nCommunicating complex data science solutions, concepts, and analyses to team members and business leaders",,,,"['Data Scientist', 'Artificial Intelligence', 'Natural Language Processing', 'LLM', 'Data Science', 'Machine Learning', 'Deep Learning']",2025-06-14 05:57:51
Data Scientist,Reuters,3 - 8 years,Not Disclosed,"['Mumbai', 'Hyderabad']","The Data Scientist organization within the Data and Analytics division is responsible for designing and implementing a unified data strategy that enables the efficient, secure, and governed use of data across the organization. We aim to create a trusted and customer-centric data ecosystem, built on a foundation of data quality, security, and openness, and guided by the Thomson Reuters Trust Principles. Our team is dedicated to developing innovative data solutions that drive business value while upholding the highest standards of data management and ethics.\nAbout the role:\nWork with low to minimum supervision to solve business problems using data and analytics.\nWork in multiple business domain areas including Customer Experience and Service, Operations, Finance, Sales and Marketing.\nWork with various business stakeholders, to understand and document requirements.\nDesign an analytical framework to provide insights into a business problem.\nExplore and visualize multiple data sets to understand data available for problem solving.\nBuild end to end data pipelines to handle and process data at scale.\nBuild machine learning models and/or statistical solutions.\nBuild predictive models.\nUse Natural Language Processing to extract insight from text.\nDesign database models (if a data mart or operational data store is required to aggregate data for modeling).\nDesign visualizations and build dashboards in Tableau and/or PowerBI\nExtract business insights from the data and models.\nPresent results to stakeholders (and tell stories using data) using power point and/or dashboards.\nWork collaboratively with other team members.\nAbout you:\nOverall 3+ years experience in technology roles.\nMust have a minimum of 1 years of experience working in the data science domain.\nHas used frameworks/libraries such as Scikit-learn, PyTorch, Keras, NLTK.\nHighly proficient in Python.\nHighly proficient in SQL.\nExperience with Tableau and/or PowerBI.\nHas worked with Amazon Web Services and Sagemaker.\nAbility to build data pipelines for data movement using tools such as Alteryx, GLUE, Informatica.\nProficient in machine learning, statistical modelling, and data science techniques.\nExperience with one or more of the following types of business analytics applications:\nPredictive analytics for customer retention, cross sales and new customer acquisition.\nPricing optimization models.\nSegmentation.\nRecommendation engines.\nExperience in one or more of the following business domains\nCustomer Experience and Service.\nFinance.\nOperations.\nGood presentation skills and the ability to tell stories using data and PowerPoint/Dashboard Visualizations.\nExcellent organizational, analytical and problem-solving skills.\nAbility to communicate complex results in a simple and concise manner at all levels within the organization.\nAbility to excel in a fast-paced, startup-like environment.\n#LI-SS5\nWhat s in it For You?\nHybrid Work Model: We ve adopted a flexible hybrid working environment (2-3 days a week in the office depending on the role) for our office-based roles while delivering a seamless experience that is digitally and physically connected.\nFlexibility & Work-Life Balance: Flex My Way is a set of supportive workplace policies designed to help manage personal and professional responsibilities, whether caring for family, giving back to the community, or finding time to refresh and reset. This builds upon our flexible work arrangements, including work from anywhere for up to 8 weeks per year, empowering employees to achieve a better work-life balance.\nCareer Development and Growth: By fostering a culture of continuous learning and skill development, we prepare our talent to tackle tomorrow s challenges and deliver real-world solutions. Our Grow My Way programming and skills-first approach ensures you have the tools and knowledge to grow, lead, and thrive in an AI-enabled future.\nIndustry Competitive Benefits: We offer comprehensive benefit plans to include flexible vacation, two company-wide Mental Health Days off, access to the Headspace app, retirement savings, tuition reimbursement, employee incentive programs, and resources for mental, physical, and financial wellbeing.\nCulture: Globally recognized, award-winning reputation for inclusion and belonging, flexibility, work-life balance, and more. We live by our values: Obsess over our Customers, Compete to Win, Challenge (Y)our Thinking, Act Fast / Learn Fast, and Stronger Together.\nSocial Impact: Make an impact in your community with our Social Impact Institute. We offer employees two paid volunteer days off annually and opportunities to get involved with pro-bono consulting projects and Environmental, Social, and Governance (ESG) initiatives.\nMaking a Real-World Impact: We are one of the few companies globally that helps its customers pursue justice, truth, and transparency. Together, with the professionals and institutions we serve, we help uphold the rule of law, turn the wheels of commerce, catch bad actors, report the facts, and provide trusted, unbiased information to people all over the world.\nAbout Us\nThomson Reuters informs the way forward by bringing together the trusted content and technology that people and organizations need to make the right decisions. We serve professionals across legal, tax, accounting, compliance, government, and media. Our products combine highly specialized software and insights to empower professionals with the data, intelligence, and solutions needed to make informed decisions, and to help institutions in their pursuit of justice, truth, and transparency. Reuters, part of Thomson Reuters, is a world leading provider of trusted journalism and news.\nWe are powered by the talents of 26,000 employees across more than 70 countries, where everyone has a chance to contribute and grow professionally in flexible work environments. At a time when objectivity, accuracy, fairness, and transparency are under attack, we consider it our duty to pursue them. Sound exciting? Join us and help shape the industries that move society forward.\nAs a global business, we rely on the unique backgrounds, perspectives, and experiences of all employees to deliver on our business goals. To ensure we can do that, we seek talented, qualified employees in all our operations around the world regardless of race, color, sex/gender, including pregnancy, gender identity and expression, national origin, religion, sexual orientation, disability, age, marital status, citizen status, veteran status, or any other protected classification under applicable law. Thomson Reuters is proud to be an Equal Employment Opportunity Employer providing a drug-free workplace.\nWe also make reasonable accommodations for qualified individuals with disabilities and for sincerely held religious beliefs in accordance with applicable law. More information on requesting an accommodation here .\nLearn more on how to protect yourself from fraudulent job postings here .\nMore information about Thomson Reuters can be found on thomsonreuters.com.",Industry Type: Internet,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Customer acquisition', 'Data management', 'Business analytics', 'Analytical', 'Consulting', 'Customer retention', 'Data quality', 'Operations', 'SQL', 'Service operations']",2025-06-14 05:57:53
Data Scientist (Supply Chain),Sparkcognition,5 - 10 years,Not Disclosed,['Bengaluru'],"Who We Are & Why Join Us\nAvathon is revolutionizing industrial AI with a powerful platform that enables businesses to harness the full potential of their operational data. Our technology seamlessly integrates and contextualizes siloed datasets, providing a 360-degree operational view that enhances decision-making and efficiency. With advanced capabilities like digital twins, natural language processing, normal behavior modeling, and machine vision, we create real-time virtual replicas of physical assets, enabling predictive maintenance, performance simulation, and operational optimization. Our AI-driven models empower companies with scalable solutions for anomaly detection, performance forecasting, and asset lifetime extension all tailored to the complexities of industrial environments.\nCutting-Edge AI Innovation - Join a team at the forefront of AI, developing groundbreaking solutions that shape the future.\nHigh-Growth Environment - Thrive in a fast-scaling startup where agility, collaboration, and rapid professional growth are the norm.\nMeaningful Impact - Work on AI-driven projects that drive real change across industries and improve lives.\n\nLearn more at: Avathon\nThe Applied Research team addresses hard, novel challenges that arise in product development or customer engagements. As a Data Scientist, Supply Chain , youll bring your knowledge and proven data science expertise to conduct research in machine learning-based solutions that improve our products and help our customers. Qualified candidates are deeply analytical with a keen understanding of artificial intelligence, machine learning, and data science. They are experienced researchers who know how to design worthwhile experiments and empirically derive conclusions. They have the ability, inclination, and experience to conduct research that solves practical problems. They have the communication skills to work closely with both research colleagues and customers.\n\nYou will:\nIndependently and effectively engage with internal product developers, external customers, and subject matter experts to understand and solve critical technical challenges through the application of cutting-edge artificial intelligence\nConduct research and design technical solutions that improve models for commercial and industrial applications, such as forecasting stochastic demand, economics based inventory optimization, manufacturing and network planning, transportation routing and resource scheduling, anomaly detection and prescriptive maintenance based on IOT data, and fraud detection\nConduct research across artificial intelligence areas including reinforcement learning, foundation models, graph neural networks, causal modeling, transferability and continuous learning\nPioneer procedures and/or automated toolsets to more efficiently and effectively perform data science activities\nContribute to AI product development and key data science research areas, internally and externally\nPropose new projects or initiatives that will yield business benefits and evaluate project plans and proposals\nEvaluate and respond to RFPs related to artificial intelligence\nConduct research and write patent applications and technical publications\nYou ll Have:\nExperience with research-level innovation in Data Science and ML, preferably with an advanced degree.\n5+ years of experience in one or more of Forecasting, Optimisation ( Inventory/ Supply chain/ Network/ Transportation),Procurement, experience on economic and Probabilistic modeling is a plus.\nStrong understanding of Data Science, including machine learning, statistics, probability, and modeling.\nSignificant experience with Data Science programming languages, such as Python, R, Matlab\nSignificant experience with machine learning frameworks, such as PyTorch, TensorFlow, Theano, and Keras.\nApplied knowledge of ML techniques/algorithms including linear models, neural networks, decision trees, Bayesian techniques, clustering, and anomaly detection\nSignificant experience managing large volumes of data (terabytes or more)\nExperience with leading project teams, specifically data science teams\nStrong written and verbal communications, ability to translate complex technical topics to internal and external stakeholders.\nAvathon is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, pregnancy, genetic information, disability, status as a protected veteran, or any other protected category under applicable federal, state, and local laws.\nAvathon is committed to providing reasonable accommodations throughout the recruiting process. If you need a reasonable accommodation, please contact us to discuss how we can assist you.",Industry Type: Emerging Technologies,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Procurement', 'Supply chain', 'Simulation', 'Neural networks', 'Analytical', 'Machine learning', 'Scheduling', 'MATLAB', 'Forecasting', 'Python']",2025-06-14 05:57:56
Data Scientist,Randomtrees,2 - 6 years,Not Disclosed,['Chennai'],"Job Summary: We are seeking a highly skilled and passionate Data Scientist with 3-6 years of experience to join our dynamic team in Chennai. The ideal candidate will possess a strong background in machine learning and deep learning methodologies, coupled with expert-level proficiency in PySpark and SQL for large-scale data manipulation and analysis. You will be instrumental in transforming complex data into actionable insights, building predictive models, and deploying robust data-driven solutions that directly impact our business objectives.\nKey Responsibilities:",,,,"['Pyspark', 'Machine Learning', 'Deep Learning', 'SQL']",2025-06-14 05:57:59
Data Scientist,Virtana Corp,3 - 8 years,Not Disclosed,"['Pune', 'Chennai']","We are seeking a Data Scientist Engineer with experience bringing highly scalable enterprise SaaS applications to market. This is a uniquely impactful opportunity to help drive our business forward and directly contribute to long-term growth at Virtana.\nIf you thrive in a fast-paced environment, take initiative, embrace proactivity and collaboration, and you re seeking an environment for continuous learning and improvement, we d love to hear from you!\nVirtana is a remote first work environment so you ll be able to work from the comfort of your home while collaborating with teammates on a variety of connectivity tools and technologies.\nJob Location- Pune/ Chennai/ Remote\nYou can schedule with us through Calendly at https: / / calendly.com / bimla-dhirayan / zoom-meeting-virtana\nRole Responsibilities:\nResearch and test machine learning approaches for analyzing large-scale distributed computing applications.\nImplement different models AI and ML algorithms for prototype and production systems.\nTest and refine the models and algorithms with live customer data to improve accuracy and efficacy.\nWork with other functional teams to integrate implemented systems into the SaaS platform\nSuggest innovative and creative concepts and ideas that would improve the overall platform\nQualifications:\nThe ideal candidate must have the following qualifications:\n3+ years experience in practical implementation and deployment of ML based systems preferred.\nBS/B Tech or M Tech/ MS (preferred) in Applied Mathematics or Statistics, or CS/Engineering with strong mathematical/statistical background\nStrong quantitative and analytical skills, especially statistical and ML techniques, including familiarity with different supervised and unsupervised learning algorithms\nImplementation experiences and deep knowledge of Classification, Time Series Analysis, Pattern Recognition, Reinforcement Learning, Deep Learning, Dynamic Programming and Optimization\nExperience in working on modeling graph structures related to spatiotemporal systems\nProgramming skills in Python\nExperience in developing and deploying on cloud (AWS or Google or Azure)\nExperience in understanding and usage of LLM models and Prompt engineering is preferred.\nGood verbal and written communication skills\nFamiliarity with well-known ML frameworks such as Pandas, Keras and TensorFlow",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Prototype', 'Time series analysis', 'Artificial Intelligence', 'IT operations management', 'Machine learning', 'Cloud', 'Cash flow', 'Pattern recognition', 'Python']",2025-06-14 05:58:01
Data Scientist,Algoleap Technologies,6 - 11 years,Not Disclosed,['Hyderabad'],"Proven implementation experience in Data Science, Machine Learning, Deep Learning, and NLP for multiple domains.\nStrong programming skills in Python, with experience in libraries such as Transformers (Hugging Face), PyTorch, or TensorFlow.\n6+ years of experience in data science, with at least 2 years focused on LLMs or Generative AI.\n\n  Hands-on experience with fine-tuning, prompt engineering, RAG (Retrieval-Augmented Generation), and LLM evaluation.",,,,"['deep learning', 'data science', 'Analytical', 'Machine learning', 'Cloud', 'Programming', 'Transformers', 'Deployment', 'Python']",2025-06-14 05:58:03
Data Scientist- Credit Risk Modelling,TransOrg,3 - 5 years,Not Disclosed,"['Bengaluru', 'Mumbai (All Areas)']","Domain: Retail Banking / Credit Cards\nLocation: Mumbai/ Bengaluru\nExperience: 3-5 years\nIndustry: Banking / Financial Services (Mandatory)\n\nWhy would you like to join us?\nTransOrg Analytics specializes in Data Science, Data Engineering and Generative AI, providing advanced analytics solutions to industry leaders and Fortune 500 companies across India, US, APAC and the Middle East. We leverage data science to streamline, optimize, and accelerate our clients' businesses.\nVisit at www.transorg.com to know more about us.\n\nWhat do we expect from you?\nBuild and validate credit risk models, including application scorecards and behavior scorecards (B-score), aligned with business and regulatory requirements.\nUse advanced machine learning algorithms such as Logistic Regression, XGBoost, and Clustering to develop interpretable and high-performance models.\nTranslate business problems into data-driven solutions using robust statistical and analytical methods.\nCollaborate with cross-functional teams including credit policy, risk strategy, and data engineering to ensure effective model implementation and monitoring.\nMaintain clear, audit-ready documentation for all models and comply with internal model governance standards.\nTrack and monitor model performance, proactively suggesting recalibrations or enhancements as needed.\n\nWhat do you need to excel at?\nWriting efficient and scalable code in Python, SQL, and PySpark for data processing, feature engineering, and model training.\nWorking with large-scale structured and unstructured data in a fast-paced, banking or fintech environment.\nDeploying and managing models using MLFlow, with a strong understanding of version control and model lifecycle management.\nUnderstanding retail banking products, especially credit card portfolios, customer behavior, and risk segmentation.\nCommunicating complex technical outcomes clearly to non-technical stakeholders and senior management.\nApplying a structured problem-solving approach and delivering insights that drive business value.\nWhat are we looking for?\nBachelors or masters degree in Statistics, Mathematics, Computer Science, or a related quantitative field.\n35 years of experience in credit risk modelling, preferably in retail banking or credit cards.\nHands-on expertise in Python, SQL, PySpark, and experience with MLFlow or equivalent MLOps tools.\nDeep understanding of machine learning techniques including Logistic Regression, XGBoost, and Clustering.\nProven experience in developing Application Scorecards and behavior Scorecards using real-world banking data.\nStrong documentation and compliance orientation, with an ability to work within regulatory frameworks.\nCuriosity, accountability, and a passion for solving real-world problems using data.\nCloud Knowledge, JIRA, GitHub(good to have)",Industry Type: Analytics / KPO / Research,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Natural Language Processing', 'Machine Learning', 'Python', 'SQL', 'Predictive Modeling', 'Logistic Regression', 'Linear Regression', 'Time Series Analysis', 'Statistical Modeling', 'Credit Risk Modelling', 'Deep Learning', 'Predictive Analytics']",2025-06-14 05:58:05
Data Scientist,Swits Digital,4 - 8 years,Not Disclosed,['Chennai'],"Job Title: Data Scientist\n\n\n\n\nLocation: Chennai\n\n\n\n\nExperience: 4-8 Years\n\n\n\n\nSkills Required:\n\n\n\n\n4 + years of hands-on experience in using machine learning/text mining tools and techniques such as Clustering /classification/ decision trees, Random forests, Support vector machines, Deep Learning, Neural networks, Reinforcement learning, and other numerical algorithms.\n\nLooking for qualified Data Scientists who can develop scalable solutions to complex real-world problems using Machine Learning, Big Data, Statistics, and Optimization. Potential candidates should have hands-on experience in applying first principles methods, machine learning, data mining, and text mining techniques to build analytics prototypes that work on massive datasets.\n\nCandidates should have experience in manipulating both structured and unstructured data in various formats, sizes, and storage-mechanisms. Candidates should have excellent problem-solving skills with an inquisitive mind to challenge existing practices.\n\nCandidates should have exposure to multiple programming languages and analytical tools and be flexible to using the requisite tools/languages for the problem at-hand.",Industry Type: Recruitment / Staffing,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Text mining', 'deep learning', 'Neural networks', 'Analytical', 'Machine learning', 'Programming', 'Data mining', 'big data', 'Statistics', 'Analytics']",2025-06-14 05:58:08
Data Scientist-Gen AI/MLOPS-7+Yrs-Gurgaon,This global operations management and an...,7 - 12 years,Not Disclosed,['Gurugram'],"Gen AI + DS + ML Ops\nJob Title: Generative AI and Data Science Engineer with MLOps Expertise\nLocation: Gurgaon, India\nEmployment Type: Full-time\n\nAbout the Role:\nWe are seeking a versatile and highly skilled Generative AI and Data Science Engineer with strong MLOps expertise. This role combines deep technical knowledge in data science and machine learning with a focus on designing and deploying scalable, production-level AI solutions. You will work with cross-functional teams to drive AI/ML projects from research and prototyping through to deployment and maintenance, ensuring model robustness, scalability, and efficiency.\n\nResponsibilities:\nGenerative AI Development and Data Science:\nDesign, develop, and fine-tune generative AI models for various applications such as natural language processing, image synthesis, and data augmentation.\nPerform exploratory data analysis (EDA) and statistical modeling to identify trends, patterns, and actionable insights.\nCollaborate with data engineering and product teams to create data pipelines for model training, testing, and deployment.\nApply data science techniques to optimize model performance and address real-world business challenges.\nMachine Learning Operations (MLOps):\nImplement MLOps best practices for managing and automating the end-to-end machine learning lifecycle, including model versioning, monitoring, and retraining.\nBuild, maintain, and optimize CI/CD pipelines for ML models to streamline development and deployment processes.\nEnsure scalability, robustness, and security of AI/ML systems in production environments.\nDevelop tools and frameworks for monitoring model performance and detecting anomalies post-deployment.\nResearch and Innovation:\nStay current with advancements in generative AI, machine learning, and MLOps technologies and frameworks.\nIdentify new methodologies, tools, and technologies that could enhance our AI and data science capabilities.\nEngage in R&D initiatives and collaborate with team members on innovative projects.\n\nRequirements:\nEducational Background:\nBachelors or Masters degree in Computer Science, Data Science, Engineering, or a related field. PhD is a plus.\nTechnical Skills:\nProficiency in Python and familiarity with machine learning libraries (e.g., TensorFlow, PyTorch, Keras, scikit-learn).\nStrong understanding of generative AI models (e.g., GANs, VAEs, transformers) and deep learning techniques.\nExperience with MLOps frameworks and tools such as MLflow, Kubeflow, Docker, and CI/CD platforms.\nKnowledge of data science techniques for EDA, feature engineering, statistical modeling, and model evaluation.\nFamiliarity with cloud platforms (e.g., AWS, Google Cloud, Azure) for deploying and scaling AI/ML models.\nSoft Skills:\nAbility to collaborate effectively across teams and communicate complex technical concepts to non-technical stakeholders.\nStrong problem-solving skills and the ability to innovate in a fast-paced environment.\n\nPreferred Qualifications:\nPrior experience in designing and deploying large-scale generative AI models.\nProficiency in SQL and data visualization tools (e.g., Tableau, Power BI).\nExperience with model interpretability and explainability frameworks.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Machine Learning', 'Gen AI', 'Data Scientist', 'Tensorflow', 'MLflow', 'Pytorch', 'Kubeflow', 'Docker', 'Cicd Pipeline', 'CI/CD', 'Machine Learning Operations', 'AWS', 'GANs', 'Python']",2025-06-14 05:58:10
Data Scientist - GEN AI,Rarr Technologies,6 - 8 years,Not Disclosed,['Bengaluru'],"At least 6 - 8 years overall experience\nAt least 4-5 years experience in Machine Learning / Deep Learning\nExtensive experience working on NLP\nConversant with Python programming\nGood knowledge of and experience working on Generative AI\nAdept in Prompt Engineering\nShould have a good understanding of microservices architecture and Data Engineering\nShould be able to understand user needs and map with corresponding technologies\nShould be able to architect Generative AI based solutions\nShould be able to guide the team technically\nShould be able to highlight technical limitations and shortcomings well in advance and suggest alternative approaches / solutions\nShould have working experience on Azure Cognitive Services\nExperience working on client facing roles and direct client interactions\nGood communication skills\nAi Ml, Nlp & Deep Learning, Data Science",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['deep learning', 'Architect', 'data science', 'Architecture', 'Machine learning', 'Programming', 'Python', 'microservices']",2025-06-14 05:58:12
Data Scientist with GCP Cloud,Foreign MNC,6 - 11 years,Not Disclosed,"['Hyderabad', 'Bengaluru', 'Delhi / NCR']","Project Role : AI / ML Engineer\nProject Role Description : Develops applications and systems that utilize AI tools, Cloud AI services, with proper cloud or on-prem application pipeline with production ready quality. Be able to apply GenAI models as part of the solution. Could also include but not limited to deep learning, neural networks, chatbots, image processing.\nMust have skills : Google Cloud Machine Learning Services\nGood to have skills : NA\nMinimum 5+ year(s) of experience is required\nEducational Qualification : BE\nSummary:\nAs an AI/ML Engineer, you will be responsible for developing applications and systems that utilize AI tools and Cloud AI services. Your typical day will involve applying GenAI models, developing cloud or on-prem application pipelines, and ensuring production-ready quality. You will also work with deep learning, neural networks, chatbots, and image processing.\n\nKey Responsibilities :\nA: Demonstrate various Google specific Designs using effective POCs, as per client requirements.\nB: Identifying applicability of Google Cloud AI services to use cases with ability to project both the business and tech benefits.\nC: Design, build and productionizes ML models to solve business challenges using Google Cloud technologies\nD: Lead and guide team of data scientists\nE: Coordinating and collaborating with cross-functional teams\n\nTechnical Experience :\nA: Min 3 + years of experience on GCP AWS ML\nB: Exposure to Google Gen AI Services\nC: Exposure to GCP and its Compute, Storage, Data, Network, Security services\nD: Expert programming skills in any one of Java, Python, Spark\nE: Knowledge of GKE, Kubeflow on GCP would be good to have\nG: Experience in Vertex AI for building and managing the ML models\nH: Experience in implementing MLOps\nAdditional Information:\n- The candidate should have a minimum of 5 +years of experience in Google Cloud Machine Learning Services.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Science', 'GCP', 'Natural Language Processing', 'Aws Sagemaker', 'Aiml', 'Computer Vision', 'Machine Learning', 'Deep Learning', 'Python']",2025-06-14 05:58:14
"SENIOR, DATA SCIENTIST",Walmart,1 - 5 years,Not Disclosed,['Bengaluru'],"The Catalog Data Science Team at Walmart Global Tech is focused on using the latest research in generative AI (GenAI), artificial intelligence (AI), machine learning (ML), statistics, deep learning, computer vision and optimization to implement solutions that ensure Walmart s product catalog is accurate, complete, and optimized for customer experience. Our team tackles complex data science and ML engineering challenges related to product classification, attribute extraction, trust & safety, and catalog optimization, empowering next-generation retail use cases.\nThe Data Science and ML Engineering community at Walmart Global Tech is active in most of the Hack events, utilizing the petabytes of data at our disposal, to build some of the coolest ideas. All the work we do at Walmart Global Tech will eventually benefit our operations & our associates, helping Customers Save Money to Live Better.\nWhat youll do:\nAs a Senior Data Scientist - ML Engineer, you ll have the opportunity to:\nDrive research initiatives and proof-of-concepts that push the state of the art in generative AI and large-scale machine learning.\nDesign and implement high-throughput, low-latency AI/ML pipelines and microservices that operate at global scale.\nOversee data ingestion, model training, evaluation, deployment and monitoring-ensuring performance, quality and reliability.\nCustomize and optimize LLMs for specific business use cases, balancing accuracy, latency and cost.\nPrototype novel generative AI solutions, integrate advancements into production, and collaborate with research partners.\nChampion best practices in data quality, lineage, governance and cost optimization across ML pipelines.\nMentor a team of ML engineers, establish coding standards, conduct design reviews, and foster a culture of continuous improvement.\nPresent your team s work at top-tier AI/ML conferences, publish scientific papers, and cultivate partnerships with universities and research labs.\nWhat youll bring\nPhD in Computer Science, Statistics, Applied Mathematics or related field with 3+ years experience in ML engineering-or Master s with 6+ years or Bachelor s with 8+ years.\nProven track record of leading and scaling AI/ML products in production environments.\nDeep expertise in generative AI, large-scale model deployment, and fine-tuning of transformer-based architectures.\nStrong programming skills in Python, or equivalent, and experience with big data frameworks (Spark, Hadoop) and ML platforms (TensorFlow, PyTorch).\nDemonstrated history of scientific publications or patents in AI/ML.\nExcellent communication skills, a growth mindset, and the ability to drive cross-functional collaboration",Industry Type: Retail,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Prototype', 'Networking', 'Coding', 'Machine learning', 'Continuous improvement', 'Information technology', 'Monitoring', 'Analytics', 'Python']",2025-06-14 05:58:17
"SENIOR, DATA SCIENTIST",Walmart,1 - 5 years,Not Disclosed,['Bengaluru'],"Position Summary...\nWhat youll do...\nAbout Team\nThe Catalog Data Science Team at Walmart Global Tech is focused on using the latest research in generative AI (GenAI), artificial intelligence (AI), machine learning (ML), statistics, deep learning, computer vision and optimization to implement solutions that ensure Walmart s product catalog is accurate, complete, and optimized for customer experience. Our team tackles complex data science and ML engineering challenges related to product classification, attribute extraction, trust & safety, and catalog optimization, empowering next-generation retail use cases.\nThe Data Science and ML Engineering community at Walmart Global Tech is active in most of the Hack events, utilizing the petabytes of data at our disposal, to build some of the coolest ideas. All the work we do at Walmart Global Tech will eventually benefit our operations & our associates, helping Customers Save Money to Live Better.\nWhat youll do:\nAs a Senior Data Scientist for Walmart Global Tech, you ll have the opportunity to\nDesign, develop, and deploy AI/ML, NLP, LLM models into production environments with a focus on reliability and scalability\nIntegrate data science solutions into current business processes.\nDevelop and recommend process standards and best practices in Machine Learning as applicable to the retail industry.\nSpearhead collaborations with other senior team members and stakeholders, leveraging your data science expertise to drive strategic decision-making and optimize business operations\nPromote and support company policies, procedures, mission, values, and standards of ethics and integrity.\nWhat youll bring:\nQualifications\nPhD with >3 years of relevant experience / 4-year bachelor s degree with > 8 years of experience / Master s degree with > 6 years of experience. Educational qualifications should be preferably in Computer Science or a strongly quantitative discipline.\nDemonstrated history of strong hands-on experience in AI/ML modelling\nPrior Experience in building Vision-based models\nProven records of scientific publications or intellectual property generation\nPrior Experience in programming skills across data science, statistical analysis, big data and ML stack\nStrong communication skills with inclination to high ownership and commitment\nProven track record of delivering high-impact AI/ML solutions to Production\nMandatory Skills: Machine Learning, NLP, Computer Vision , Python, R\nAdditional Qualifications: Good to have experience in areas such as Graph Neural Networks, LLM Optimization\nAbout Walmart Global Tech\n.\n.\nFlexible, hybrid work\n.\nBenefits\n.\nBelonging\n.\n.\nEqual Opportunity Employer\nWalmart, Inc., is an Equal Opportunities Employer - By Choice. We believe we are best equipped to help our associates, customers and the communities we serve live better when we really know them. That means understanding, respecting and valuing unique styles, experiences, identities, ideas and opinions - while being inclusive of all people.\nMinimum Qualifications...\nMinimum Qualifications:Option 1- Bachelors degree in Statistics, Economics, Analytics, Mathematics, Computer Science, Information Technology, or related field and 3 years experience in an analytics related field. Option 2- Masters degree in Statistics, Economics, Analytics, Mathematics, Computer Science, Information Technology, or related field and 1 years experience in an analytics related field. Option 3 - 5 years experience in an analytics or related field.\nPreferred Qualifications...\nPrimary Location...",Industry Type: Retail,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Computer vision', 'Networking', 'Neural networks', 'Artificial Intelligence', 'Intellectual property', 'Machine learning', 'Information technology', 'Analytics', 'Python']",2025-06-14 05:58:19
Data Scientist,Apollo Finvest India Limited,2 - 5 years,20-25 Lacs P.A.,['Mumbai (All Areas)( Andheri West )'],"Risk Analyst in NBFC or lending space, knowledge of unsecured personal loans and lending processes, Proficient in Python, SQL, PowerBI, and Excel for data analysis and modeling, Familiarity with RBI norms, compliance standards, and digital lending.",Industry Type: FinTech / Payments,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Portfolio Analysis', 'MIS Reporting', 'SQL', 'Data Science', 'NBFC', 'Power Bi', 'Risk', 'Workflow', 'Risk Modeling', 'Excel', 'Risk Analytics', 'RBI', 'API', 'AWS', 'Python']",2025-06-14 05:58:21
Data Scientist,Simplify Healthcare,5 - 10 years,Not Disclosed,"['Pune( Hadapsar, Kharadi, Keshav Nagar, Vishrantwadi, Dhanori, Mundhwa, Viman Nagar )']","Engineer/Sr. Engineer Data Science\nLocation: Pune, India\n\nCompany Overview:\nSimplify Healthcare is one of the fastest-growing healthcare technology solutions providers serving the US health insurance (Payer) industry. Headquartered in Chicago with a Global Delivery Centre in Pune, we are trusted by 65+ payer organizations and supported by a team of 800+ professionals.\nWe specialize in delivering SaaS-based enterprise software solutions focused on product and benefits configuration, provider lifecycle management, and more. In 2023, we launched Simplify Health Cloud, our flagship Payer Platform, establishing our position as a leader in cloud-native, low-code configurable platforms for the healthcare sector.\nWith our strategic acquisition of Virtical.ai in 2024, we’re accelerating innovation through AI integration, particularly in areas such as LLMs, conversational AI, and cloud-based intelligence. Our proprietary Simplify App Fabric™ enables fast, secure, and low-code development for modern Payer solutions.\nOur innovation has earned us repeated recognition in Deloitte Technology Fast 500™, Inc. 5000, and reports by IDC and Gartner.",,,,"['Speech Recognition', 'Artificial Intelligence', 'Natural Language Processing', 'Conversational Ai', 'Chatbot', 'Cognitive Services', 'Text Mining', 'Machine Learning', 'Azure Cognitive Services']",2025-06-14 05:58:23
Senior Data Scientist,Reuters,3 - 8 years,Not Disclosed,"['Mumbai', 'Hyderabad']","Senior Data Scientist - Enterprise Analytics\nWant to be part of the Data & Analytics organization, whose strategic goal is to create a world-class Data & Analytics company by building, embedding, and maturing a data-driven culture across Thomson Reuters.\nWe are looking for a highly motivated individual with strong organizational and technical skills for the position of Senior Data Scientist. You will play a critical role working on cutting edge of analytics, leveraging predictive models, machine learning and generative AI to drive business insights and facilitating informed decision-making and help Thomson Reuters rapidly scale data-driven initiatives.\nAbout the Role\nIn this opportunity as Senior Data Scientist, you will:\nEngage with stakeholders, business analysts and project team to understand the data requirements.\nWork in multiple business domain areas including Customer Service, Finance, Sales and Marketing.\nDesign analytical frameworks to provide insights into a business problem.\nExplore and visualize multiple data sets to understand data available and prepare data for problem solving.\nBuild machine learning models and/or statistical solutions.\nBuild predictive models, generative AI solutions.\nUse Natural Language Processing to extract insight from text.\nDesign database models (if a data mart or operational data store is required to aggregate data for modeling).\nDesign visualizations and build dashboards in Tableau and/or PowerBI.\nExtract business insights from the data and models.\nPresent results to stakeholders (and tell stories using data) using power point and/or dashboards.\nAbout You\nYoure a fit for the role of Senior Data Scientist if your background includes:\nExperience- 6-8 Years in the field of Machine Learning & AI\nMust have a minimum of 3 years of experience working in the data science domain\nDegree preferred in a quantitative field (Computer Science, Statistics, etc.)\nBoth technical and business acumen is required\nTechnical skills\nProficient in machine learning, statistical modelling, data science and generative AI techniques\nHighly proficient in Python and SQL\nExperience with Tableau and/or PowerBI\nHas worked with Amazon Web Services and Sagemaker\nAbility to build data pipelines for data movement using tools such as Alteryx, GLUE\nExperience Predictive analytics for customer retention, upsell/cross sell products and new customer acquisition, Customer Segmentation, Recommendation engines (customer and AWS Personalize), POC s in building Generative AI solutions (GPT, Llama etc.,)\nHands on with Prompt Engineering\nExperience in Customer Service, Finance, Sales and Marketing\nAdditional Technical skills include Familiarity with Natural Language Processing including Feature Extraction techniques, Word Embeddings, Topic Modeling, Sentiment Analysis, Classification, Sequence Models and Transfer Learning\nKnowledgeable of AWS APIs for Machine Learning\nHas worked with Snowflake extensively.\nGood presentation skills and the ability to tell stories using data and Powerpoint/Dashboard Visualizations.\nAbility to communicate complex results in a simple and concise manner at all levels within the organization.\nConsulting Experience with a premier consulting firm.\n#LI-SS5\nWhat s in it For You?\nHybrid Work Model: We ve adopted a flexible hybrid working environment (2-3 days a week in the office depending on the role) for our office-based roles while delivering a seamless experience that is digitally and physically connected.\nFlexibility & Work-Life Balance: Flex My Way is a set of supportive workplace policies designed to help manage personal and professional responsibilities, whether caring for family, giving back to the community, or finding time to refresh and reset. This builds upon our flexible work arrangements, including work from anywhere for up to 8 weeks per year, empowering employees to achieve a better work-life balance.\nCareer Development and Growth: By fostering a culture of continuous learning and skill development, we prepare our talent to tackle tomorrow s challenges and deliver real-world solutions. Our Grow My Way programming and skills-first approach ensures you have the tools and knowledge to grow, lead, and thrive in an AI-enabled future.\nIndustry Competitive Benefits: We offer comprehensive benefit plans to include flexible vacation, two company-wide Mental Health Days off, access to the Headspace app, retirement savings, tuition reimbursement, employee incentive programs, and resources for mental, physical, and financial wellbeing.\nCulture: Globally recognized, award-winning reputation for inclusion and belonging, flexibility, work-life balance, and more. We live by our values: Obsess over our Customers, Compete to Win, Challenge (Y)our Thinking, Act Fast / Learn Fast, and Stronger Together.\nSocial Impact: Make an impact in your community with our Social Impact Institute. We offer employees two paid volunteer days off annually and opportunities to get involved with pro-bono consulting projects and Environmental, Social, and Governance (ESG) initiatives.\nMaking a Real-World Impact: We are one of the few companies globally that helps its customers pursue justice, truth, and transparency. Together, with the professionals and institutions we serve, we help uphold the rule of law, turn the wheels of commerce, catch bad actors, report the facts, and provide trusted, unbiased information to people all over the world.\nAbout Us\nThomson Reuters informs the way forward by bringing together the trusted content and technology that people and organizations need to make the right decisions. We serve professionals across legal, tax, accounting, compliance, government, and media. Our products combine highly specialized software and insights to empower professionals with the data, intelligence, and solutions needed to make informed decisions, and to help institutions in their pursuit of justice, truth, and transparency. Reuters, part of Thomson Reuters, is a world leading provider of trusted journalism and news.\nWe are powered by the talents of 26,000 employees across more than 70 countries, where everyone has a chance to contribute and grow professionally in flexible work environments. At a time when objectivity, accuracy, fairness, and transparency are under attack, we consider it our duty to pursue them. Sound exciting? Join us and help shape the industries that move society forward.\nAs a global business, we rely on the unique backgrounds, perspectives, and experiences of all employees to deliver on our business goals. To ensure we can do that, we seek talented, qualified employees in all our operations around the world regardless of race, color, sex/gender, including pregnancy, gender identity and expression, national origin, religion, sexual orientation, disability, age, marital status, citizen status, veteran status, or any other protected classification under applicable law. Thomson Reuters is proud to be an Equal Employment Opportunity Employer providing a drug-free workplace.\nWe also make reasonable accommodations for qualified individuals with disabilities and for sincerely held religious beliefs in accordance with applicable law. More information on requesting an accommodation here .\nLearn more on how to protect yourself from fraudulent job postings here .\nMore information about Thomson Reuters can be found on thomsonreuters.com.",Industry Type: Internet,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Customer acquisition', 'Career development', 'Analytical', 'Consulting', 'Machine learning', 'Customer retention', 'Customer service', 'Operations', 'SQL']",2025-06-14 05:58:26
Senior/Lead Data Scientist,Tezo,6 - 10 years,Not Disclosed,['Hyderabad'],"Tezo is a new generation Digital & AI solutions provider, with a history of creating remarkable outcomes for our customers. We bring exceptional experiences using cutting-edge analytics, data proficiency, technology, and digital excellence.\nTezo is seeking passionate AI Engineers who are excited about harnessing the power of Generative AI to transform our company and provide cutting-edge solutions for our clients. Join us in revolutionizing enterprises by building intelligent, generative solutions that leverage AI/ML. If you've ever dreamed of contributing to impactful projects on a large scale, this is the opportunity for you! \nIn this role, you will be an integral part of the Machine Learning Platforms/Data Science team, focusing on developing, testing, and deploying generative AI models. \n\nWhat Makes Our AI/ML Practice Unique:\nPurpose-driven: We actively respond to our customers' evolving needs with innovative solutions. \nCollaborative: We foster a positive and engaging work environment where collective ideas thrive. \nAccountable: We take ownership of our performance, both individually and as a team. \nService Excellence: We maximize our potential through continuous learning and improvement. \nTrusted: We empower individuals to make informed decisions and take calculated risks. \n\nJob Summary:\nWe are looking for a dedicated Lead Data Scientist with a strong background in Generative AI to join our team. You will support product, leadership, and client teams by providing insights derived from advanced data analysis and generative modeling. \nIn this role, you will collaborate closely with the development team, architects, and product owners to build efficient generative models and manage their lifecycle using the appropriate technology stack. \n\nCore Requirements:\n\nAt least 6 years of experience working with geographically distributed teams \n2+ years of experience working in a client-facing role on AI/ML .\nDemonstrable experience in leading a substantive area of work, or line management of a team.\nProven experience in building production grade Retrieval-Augmented Generation (RAG) solutions with hands on experience with advanced RAG techniques for retrieval, re-ranking etc. \nBuild GenAI applications using LangChain, LlamaIndex and familiarity with Vector Stores and Large Language Models. \nExperience in fine-tuning Large Language Models (LLMs) for business use cases will be preferred. \nMinimum of 4 years of experience in developing end-to-end classical machine learning and NLP projects. \nDemonstrated experience in deploying ML solutions in production using cloud services like Azure,AWS. \nBusiness Understanding, Stakeholder management and Team leading skills. \nStrong practical expertise in Python and SQL needed for data science projects. \n\nJoin us at Tezo to be part of a dynamic team committed to driving innovation through Generative AI solutions!",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Generative Ai', 'Large Language Model', 'Machine Learning', 'NLP', 'Solutioning', 'Retrieval Augmented Generation']",2025-06-14 05:58:28
Senior Data Scientist,Cimet,5 - 10 years,Not Disclosed,['Jaipur'],"Purpose Of The Position:\nWe are looking for a highly experienced Senior Data Scientist with a deep understanding of AI/ML technologies, including Recommendation Systems, Chatbots, Generative AI, and Large Language Models (LLMs). The ideal candidate will have hands-on experience in applying these technologies to solve real-world problems, working with large datasets, and collaborating with cross-functional teams to deliver innovative data-driven solutions..\n\nKey Responsibilities:\nDesign, develop, and optimize recommendation systems to enhance user experience and engagement across platforms.\nBuild and deploy chatbots with advanced NLP capabilities for automating customer interactions and improving business processes.\nLead the development of Generative AI solutions, including content generation and automation.\nResearch and apply Large Language Models (LLMs) like GPT, BERT, and others to solve business-specific problems and create innovative solutions.\nCollaborate with engineering teams to integrate machine learning models into production systems, ensuring scalability and reliability.\nPerform data exploration, analysis, and feature engineering to improve model performance.\nStay updated on the latest advancements in AI and ML technologies, proposing new techniques and tools to enhance our product capabilities.\nMentor junior data scientists and engineers, providing guidance on best practices in AI/ML model development and deployment.\nCollaborate with product managers and business stakeholders to translate business goals into AI-driven solutions.\nWork on model interpretability, explainability, and ensure models are built in an ethical and responsible manner.\nRequired Skills And Qualifications:\n5+ years of experience in data science or machine learning, with a focus on building and deploying AI models.\nStrong expertise in designing and developing recommendation systems and working with collaborative filtering, matrix factorization, and content-based filtering techniques.\nHands-on experience with chatbots using Natural Language Processing (NLP) and conversational AI frameworks.\nIn-depth understanding of Generative AI, including transformer-based models and GANs (Generative Adversarial Networks).\nExperience working with Large Language Models (LLMs) such as GPT, BERT, T5, etc.\nProficiency in machine learning frameworks such as TensorFlow, PyTorch, and Scikit-learn.\nStrong programming skills in Python and libraries such as NumPy, Pandas, Hugging Face, and NLTK.\nExperience with cloud platforms like AWS, GCP, or Azure for deploying and scaling machine learning models.\nSolid understanding of data pipelines, ETL processes, and working with large datasets using SQL or NoSQL databases.\nKnowledge of MLOps and experience deploying models in production environments.\nStrong problem-solving skills and a deep understanding of statistical methods and algorithms.\nPreferred Qualifications:\nExperience with Reinforcement Learning and Recommender Systems personalization techniques.\nExperience of working with AWS Bedrock services.\nFamiliarity with ethical AI and model bias mitigation techniques.\nExperience with A/B testing, experimentation, and performance tracking for AI models in production.\nPrior experience mentoring junior data scientists and leading AI/ML projects.\nStrong communication and collaboration skills, with the ability to convey complex technical concepts to non-technical stakeholders.\n\nWhy Join Us?\nOpportunity to be part of a rapidly growing, innovative product-based company.\nCollaborate with a talented, driven team focused on building high-quality software solutions.\nCompetitive compensation and benefits package.",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Machine Learning', 'Tensorflow', 'Generative Ai', 'Natural Language Processing', 'Scikit-Learn', 'Nltk', 'Pytorch', 'MLOps', 'Cloud Computing', 'Statistical Modeling', 'Bert', 'Deploying Models', 'Ab Testing', 'AWS']",2025-06-14 05:58:31
Data Scientist,Value Innovation Labs,8 - 10 years,12-15 Lacs P.A.,['Chennai'],"We are seeking a skilled and experienced Machine Learning Engineer specialized in Natural\nLanguage Processing (NLP) and Computer Vision to join our team. The candidate should\nhave a strong foundation in ML and AI principles, coupled with a proven track record of\ndeveloping and deploying a minimum of 20+ models in these domains.\n\nResponsibilities:\nDevelop and implement state-of-the-art machine learning algorithms and models for NLP\nand computer vision applications.\nCollaborate with cross-functional teams to understand business requirements and\ntranslate them into technical solutions.\nConduct thorough data analysis, pre-processing, and feature engineering to extract\nmeaningful insights from structured and unstructured data.\nDesign and implement scalable and efficient machine learning pipelines for training,\nevaluation, and inference.\nExperiment with various deep learning architectures, hyper parameters, and optimization\ntechniques to improve model performance.\nStay updated with the latest advancements in ML and AI research and evaluate their\npotential applications in our domain.\nWork closely with software engineers to integrate ML models into production systems\nand ensure their robustness and scalability.\nPerform rigorous testing and validation of models to ensure their accuracy, reliability,\nand generalization capability.\nContinuously monitor and maintain deployed models, and implement necessary updates\nand enhancements as required.\nDocument code, methodologies, and findings effectively, and contribute to internal\nknowledge sharing initiatives.\nBachelor's/Masters/Ph.D. degree in Computer Science, Engineering, Mathematics, or a\nrelated field.\nStrong theoretical understanding of machine learning algorithms, deep learning\nframeworks, and statistical modeling techniques.\nProficiency in programming languages such as Python, along with experience in popular\nML libraries such as TensorFlow, PyTorch, or scikit-learn.\nHands-on experience in developing and deploying machine learning models for NLP and\ncomputer vision tasks, with a demonstrable portfolio of projects.\nSolid understanding of natural language processing techniques, including text\nclassification, sentiment analysis, named entity recognition, etc.\nFamiliarity with computer vision concepts such as image classification, object detection,\nimage segmentation, etc., and experience with frameworks like OpenCV or TensorFlow.\nExperience with cloud platforms (e.g., AWS, Azure, Google Cloud) and containerization\ntechnologies (e.g., Docker, Kubernetes) is a plus.\nExcellent problem-solving skills, attention to detail, and ability to work independently as\nwell as part of a team.\nStrong communication and interpersonal skills, with the ability to effectively collaborate\nwith stakeholders across different functional areas.\n\nWe need only Data Scientist Candidates.\n\nFor More Information Kindly call on\n7701968943(Sanchit)\nShare your updated cv on sanchit@mounttalent.com",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Tensorflow', 'Aws Cloud', 'Artificial Intelligence', 'Natural Language Processing', 'Neural Networks', 'Machine Learning', 'Deep Learning', 'SQL', 'Pytorch', 'Image Processing', 'Computer Vision', 'Google Cloud Platforms', 'Python']",2025-06-14 05:58:33
Senior/Lead Data Scientist,Tiger Analytics,6 - 11 years,Not Disclosed,"['Hyderabad', 'Chennai', 'Bengaluru']","Role & responsibilities\n\nCurious about the role? What your typical day would look like?As a Senior Data Scientist, your work is a combination of hands-on contribution to Loreum Ipsum, Loreum Ipsum, etc. More specifically, this will involve:\nLead and contribute to developing sophisticated machine learning models, predictive analytics, and statistical analyses to solve complex business problems.",,,,"['Data Science', 'Time Series Analysis', 'Machine Learning', 'Python', 'Time Series Forecasting', 'Regression', 'Clustering', 'neural nets', 'Optimization', 'SQL']",2025-06-14 05:58:35
Gen AI/ ML Data Scientist,A Leading Analytics Firm,5 - 8 years,Not Disclosed,"['Chennai', 'Delhi / NCR']","Exciting Opportunity: GenAI / ML Data Scientist Role!\nLocation: Chennai / Gurgaon / NCR\nNotice Period: 1530 Days\nExperience: 5–8 Years\nPosition: GenAI / ML Data Scientist (Team Manager Level)\nDomain: Generative AI, Data Engineering, NLP, Advanced Analytics\nWork Mode: Work from Office – 5 Days a Week\n\nExciting Career Opportunity!\n\nWe are a product-centric insight & automation services company globally, helping organizations around the world make better & faster decisions using the power of insight & intelligent automation. We build and operationalize next-gen strategies through Big Data, Artificial Intelligence, Machine Learning, Unstructured Data Processing, and Advanced Analytics. With our extensive experience in data sciences & analytics, we stand out from the competition in India. We firmly believe in Excellence Everywhere.\n\nJob / Role Information:\nDesignation: Machine Learning Engineer / Data Scientist (with expertise in Generative AI & Data Engineering)\nExperience: 5-8 Years\nFunction: AI & Data Science Enterprise Delivery\nLocation: Gurgaon/NCR/Chennai\nRole: Team Manager\n\nJob Description\n\nPurpose of the Job/Role\nThis role is responsible for managing client expectations and strategizing with various stakeholders to meet customer requirements.\n\nKEY RESPONSIBILITIES:\n\nData Science:\nDevelop machine learning models to support recommendation systems and NLP projects\nProvide actionable insights for product and service optimization\nData Engineering:\nBuild and maintain scalable ETL pipelines\nOptimize data storage solutions (data lakes, columnar formats) and ensure data accuracy for analytics\nData Analysis & Insight Generation:\nAnalyze complex datasets to uncover trends and patterns\nGenerate and present insights that drive strategic decisions and enhance client services\nStakeholder Collaboration:\nWork with product and service teams to understand data needs and translate them into technical solutions\nWorking Relationships:\nReporting to: VP Business Growth\nExternal Stakeholders: Clients\nSkills/Competencies Required:\nTechnical Skills:\nProficiency with Python (Pandas, NumPy), SQL, and Java\nExperience with LLMs, Lang Chain, and Generative AI technologies\nFamiliarity with ML frameworks (TensorFlow, PyTorch) and data engineering tools (Spark, Kafka)\nStrong data analysis skills and ability to present findings to both technical and non-technical stakeholders\nProficient in key data engineering concepts, such as data lakes, columnar formats, ETL tools, and BI tools\nKnowledge in Machine Learning, NLP, Recommender systems, personalization, Segmentation, microservices architecture, and API development\nAbility to adapt to a fast-paced, dynamic work environment and quickly learn new technologies\nSoft Skills:\nWork well in a team or independently\nExcellent written & verbal communication skills\nStrong critical thinking and questioning skills\nHigh degree of flexibility willing to fill in the gaps rather than relying on others\nStrong communication skills, especially in presenting data insights\nFlexibility, problem-solving, and a proactive approach in a fast-paced environment\nAcademic Qualifications & Experience Required:\n\nRequired Educational Qualification & Relevant Experience:\nBachelor's Degree in Computer Science, Data Analytics, Engineering, or a related field\nMinimum 3 to 5 years of experience in data science and data engineering\nStrong critical thinking abilities and capacity to work autonomously\nProficient understanding of key data engineering concepts (data lakes, columnar formats, ETL tools, and BI tools)\nHigh motivation, good work ethic, maturity, and personal initiative\nStrong oral and written communication skills\nEqual Opportunity Employer\nWe are committed to an inclusive work environment. We do not differentiate candidates on the basis of religion, caste, gender, language, disabilities, or ethnic group. We reserve the right to place or move any candidate to any location globally, in the best interest of our business.\nInterested?\nIf you're passionate about data science and ready to take your career to the next level, we want to hear from you!\nSend your resume to: smita.gurung@cielhr.com",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Tensorflow', 'Gen AI', 'Natural Language Processing', 'Machine Learning', 'Python', 'Pytorch']",2025-06-14 05:58:37
Hiring Senior Data Scientist/ Data Science Manager,Tredence,5 - 10 years,Not Disclosed,"['Kolkata', 'Pune', 'Bengaluru']","Job Description:\n\nGraduate degree in a quantitative field (CS, statistics, applied mathematics, machine learning, or related discipline)\n• Good programming skills in Python with strong working knowledge of Pythons numerical, data analysis, or AI frameworks such as NumPy, Pandas, Scikit-learn, etc.\n• Experience with LMs (Llama (1/2/3), T5, Falcon, Langchain or framework similar like Langchain)\n• Candidate must be aware of entire evolution history of NLP (Traditional Language Models to Modern Large Language Models), training data creation, training set-up and finetuning",,,,"['Data Science', 'Tensorflow', 'Pytorch', 'generative', 'python', 'Artificial Intelligence', 'Natural Language Processing', 'Machine Learning', 'Deep Learning']",2025-06-14 05:58:39
Senior Statistical Data Scientist,Pfizer,5 - 6 years,Not Disclosed,['Chennai'],"An Individual Contributor role\nProductive h ands on programming, supporting deliverables in the study / project / portfolio / standards team, of medium - high complex statistical programming deliverables to support assets and study teams\nPerforms tasks with limited supervision early in role and independently later in role.\nIs capable of handling standards/study programming specific activities independently including collaboration across stakeholders at various timezones\nEnsures adherence to high quality programming standards in their daily work\n\nWork Location Assignment: Flexible\n\nWork Location Assignment: Hybrid\nMedical\n#LI-PFE",,,,"['Statistical programming', 'Individual Contributor', 'Supervision']",2025-06-14 05:58:41
Sr. Data Scientist,SoulPage IT,3 - 5 years,Not Disclosed,['Hyderabad'],"Sr.Data Scientist - Soulpage IT Solutions\nHome\nSr.Data Scientist\nMay 28, 2024\nJob Title: Data Scientist\nWork Exp: 3+ yrs\nApproximate CTC: Industry Standards\nLocation: Hyderabad\nFunctional Area: IT Software System Programming\nQualification: B.Tech/B.E/M.Tech/B.Sc(math or statistics)/MSc(Statistics)\nJob Description:\nWe are looking for an experienced Data Scientist with a strong hold on Machine Learning and Deep Learning model building, deployment and has problem solving skills.\nSkills & Responsibilities:\nExcellent skills in Deep learning-based algorithms with image and text data and ML algorithms with structured datasets.\nStrong hold on computer vision libraries like OpenCV and Pillow and NLP libraries like Huggingface and Spacy.\nAbility to conceptualize and implement different deep neural network architectures in Python using Pytorch and Tensorflow.\nStrong mathematical statistical understanding behind the algorithms.\nIdeate, conceptualize and formulate Data Science use case of significant impact for the business\nDevelop and evaluate various Machine Learning models before zeroing in on the best one and ability to run Machine Learning models on huge amount of data\nDrive discussions with Business to ensure the complete understanding with respect to the data science use case; Gain and demonstrate Data and Domain knowledge\nEvaluate the Data Quality and Metadata information of Key Data Elements before any modelling effort to ensure minimal surprises in the later part of the project\nDesign holistic data science solution covering Descriptive, Predictive & Prescriptive analytics\nBuild reusable ML code for faster turnaround time to business problem solving\nExplain the Machine Learning model implementation to business stakeholders in a way that they can understand and appreciate the solution\nBuild storytelling dashboards to make all insights and model output available to end users in a form which is highly helpful for decision making\nManage relationship with business stakeholders acting as embedded data scientist constantly thinking about data science solutions to make business better\nKey Skills Required:\nPython, Pytorch, Tensorflow, OpenCV, Scikit Learn, Pandas and Numpy, Flask, Django, AWS, Deep Learning including CNNs, Transformers, and RNNs, and Statistical Modelling.\nAbout Company:\nSoulpage IT Solutions Pvt. Ltd.\nSOULPAGE is a Data Science Technology company based in Hyderabad, India. A simple organization with a strong commitment to customer service. We are committed to helping enterprises explore unventured technical avenues to tap unprecedented value creation. Our effort is to provide innovative software solutions using the latest technological advancements in the areas of automation, Data Science, AI, and application development for our clients to stay relevant and adapt to changing times.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer vision', 'Automation', 'metadata', 'Machine learning', 'System programming', 'Application development', 'Data quality', 'Customer service', 'Analytics', 'Python']",2025-06-14 05:58:43
Hiring Lead Data Scientist with Vodafone Intelligent Solutions,_VOIS,7 - 12 years,Not Disclosed,['Pune'],"Experience: 7-12years\nLocation and Work Mode: Pune\n\nJob Description:\n\nAbout this Role\nWe are seeking a highly skilled and motivated Data Scientist to join our team. This role is ideal for individuals who are passionate about leveraging data to drive innovation and create intelligent, scalable solutions. The successful candidate will work across a variety of data science domains, including diagnostic, predictive, and prescriptive analytics, and will contribute to the development of autonomous cognitive systems. This is an exciting opportunity to be part of a forward-thinking team that values continuous learning and collaboration.\n\nWho are you\nA data enthusiast with a strong foundation in machine learning, deep learning, and statistical modelling.\nA collaborative problem-solver who thrives in cross-functional teams and is comfortable engaging with stakeholders.\nA continuous learner with a passion for exploring new tools, technologies, and methodologies.\nAn effective communicator who can translate complex data insights into actionable business strategies.\nA self-starter with a proactive mindset and the ability to work independently in a dynamic environment.\n\nWhat you will do\nManage and process large-scale data on both on-premise and cloud platforms (preferably AWS).\nDesign and implement diagnostic models using decision theory and causal inference techniques (e.g., DAG, ADMG, Deterministic SME).\nBuild and productise diagnostic systems for scalable reuse across the organisation.\nDevelop robust predictive and prescriptive analytics models using AI techniques such as ML, DL, NLP, ES, and RL.\nCreate intelligent systems that determine the Next Best Action based on prescriptive analytics.\nDrive the development of autonomous cognitive systems that proactively recommend actions with minimal human intervention.\nApply advanced deep learning techniques including CNNs, RNNs, and MLPs to solve complex business problems.\nUtilise machine learning and deep learning libraries such as TensorFlow, PyTorch, Scikit-learn, NumPy, Pandas, Statsmodels, Theano, and XGBoost.\nCollaborate with stakeholders to understand business needs and deliver data-driven solutions.\nPresent insights and findings using visualisation tools like Tableau and Power BI.\n\nWhat skills you need\nProficiency in programming languages: Python, R, SQL\nExperience with frameworks: TensorFlow, Keras, Scikit-learn\nStrong understanding of statistical modelling: regression, classification, clustering, time series\nHands-on experience with cloud platforms: AWS (preferred), Azure\nFamiliarity with big data technologies and environments\nExcellent communication and stakeholder management skills\nStrong analytical and problem-solving abilities\nAt least one certification in AWS is preferred\n\nWhat skills you will learn\nAdvanced applications of causal inference and decision theory in real-world scenarios\nBuilding and deploying scalable AI systems in cloud environments\nDesigning autonomous systems that integrate feedback loops for continuous learning\nEnhancing stakeholder engagement through data storytelling and visualisation\nExposure to cutting-edge tools and frameworks in the AI and ML ecosystem",Industry Type: Telecom / ISP,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['NLP', 'Machine Learning', 'AWS', 'Deep Learning', 'SQL', 'Data Scientist', 'PLSQL']",2025-06-14 05:58:45
"Senior Data Scientist, Actimize",NICE,4 - 7 years,Not Disclosed,['Pune'],"So, what’s the role all about? \nWe are seeking a highly skilled and experienced Senior Data Scientist to join our dynamic team. The ideal candidate should have a minimum  4s of years of experience in  data science, with hands-on experience in developing and implementing Generative AI solutions. The Senior Data Scientist will be responsible for developing Machine Learning models and collaborating with cross-functional teams to solve complex business problems \nHow will you make an impact?",,,,"['algorithms', 'development', 'python', 'scikit-learn', 'methods', 'interpersonal skills', 'microsoft azure', 'power bi', 'engineering', 'numpy', 'machine learning', 'evaluation', 'pandas', 'analytics', 'deep learning', 'r', 'tableau', 'data science', 'gcp', 'science', 'machine learning algorithms', 'aws', 'programming', 'communication skills']",2025-06-14 05:58:48
Senior Data Scientist,Growby Exx Services,5 - 10 years,Not Disclosed,['Ahmedabad'],"Growexx is looking for smart and passionateData Scientist/Analyst, who will empower Marketing, Product, Sales teams to make strategic, data-driven decisions.\nKey Responsibilities\nMine, process, and analyse hit/event level web, product, sales, and digital marketing data. \nLeverage LLMs (Large Language Models) and traditional machine learning to mine, process, and analyze web, product, sales, and digital marketing event-level data.\nDevelop and fine-tune LLM-driven solutions for tasks such as text summarization, customer support automation, personalization, and user journey understanding.\nBuild and deploy predictive models and ML algorithms across structured and unstructured customer profile, journey, and usage datasets.\nDeploy LLM and ML models into production environments for activation across websites, product applications, and sales/marketing channels.\nDesign and implement model activation strategies, including A/B testing plans, benchmarking studies, and measurement of final business impact.\nConduct comprehensive evaluation of LLMs, including performance benchmarking (accuracy, latency, token usage, cost), prompt effectiveness testing, fine-tuning impact analysis, and safety/bias assessments.\nDesign, build, and deploy LLM-based agentic systems using frameworks such as LangChain, AutoGen, CrewAI, or custom orchestration for complex workflows (e.g., multi-agent collaboration, function-calling pipelines, dynamic task execution).\nIntegrate LLM agents with APIs, internal knowledge bases, retrieval systems (RAG architectures), and external tools to enable autonomous or semi-autonomous decision-making.\nPartner with data engineering teams to enhance and maintain the Customer360 data model, including creating new feature engineering requirements, improving taxonomy, and identifying and resolving data quality issues.\nCollaborate with cross-functional teams (Enterprise Data Warehouse, Salesforce MOPS, IT, Product, Marketing) to continuously improve data integration and quality for advanced modeling use cases.\nBuild a deep understanding of business models, objectives, challenges, and opportunities by working closely with leadership and key stakeholders.\nDocument model methodologies, evaluation frameworks, agent workflows, deployment architectures, and post-activation performance results in a structured and reproducible format.\nStay current with advancements in LLMs, agentic AI, retrieval-augmented generation (RAG), and ML technologies to recommend and implement innovative solutions.\nKey Skills\nExperience using Python, SciKit, SQL, Snowflake, product usage data, Jupyter Notebooks, Amazon SageMaker, Airflow, Github.\nProficient in data mining, advanced statistical analysis, feature engineering, and mathematical modeling.\nDeep experience with machine learning techniques including supervised, unsupervised, reinforcement learning, causal inference, and predictive modeling.\nSkilled across the full ML lifecycle: data preparation, feature creation and selection, model training, hyperparameter tuning, evaluation, and deployment for inference/prediction.\nExtensive hands-on experience with cookie-level advertising and digital marketing data (Google Ads, Bing, Epsilon, LinkedIn, Facebook) for demand generation KPIs such as ROAS, CTRs, impressions, multi-touch attribution (MTA).\nProven experience designing, fine-tuning, evaluating, and deploying Large Language Models (LLMs) and generative AI applications.\nExperience designing and deploying agentic systems using frameworks such as LangChain, AutoGen, CrewAI, and custom function-calling pipelines.\nExpertise integrating LLM agents with APIs, knowledge bases, retrieval systems (RAG architecture), and orchestrating dynamic multi-agent workflows.\nStrong understanding of evaluation metrics for LLMs, including prompt testing, token optimization, bias/safety analysis, latency, and cost benchmarks.\nDeep familiarity with cookie-level web and product behavior data (usage metrics, conversion funnels, bounce rates, sessions, hits/events, journey optimization).\nExpertise in designing and executing A/B, multivariate, and lift tests to measure activated ML/LLM model performance across digital and offline channels.\nSkilled in gathering business requirements, translating them into ML use cases, and clearly communicating methodologies and results to both technical and non-technical stakeholders.\nContinuous learner, keeping up-to-date with the latest advances in transformers, generative AI models, retrieval-augmented generation (RAG), and agentic AI frameworks.\nPreferred: practical experience in an engineering capacity building, testing, deploying, and optimizing ensemble ML and LLM solutions in production environments.\nEducation and Experience\nB Tech or B. E. (Computer Science / Information Technology)\n5 + years as a Data Scientist or similar roles.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Generative Ai', 'LLM', 'Machine Learning', 'Deep Learning']",2025-06-14 05:58:50
Data Scientist- Natural Language Processing - Pan India,Fortune Global 500 IT Services Firm,6 - 11 years,Not Disclosed,"['Bhubaneswar', 'Indore', 'Coimbatore']","Project Role : Responsible AI Engineer\nProject Role Description : Assess AI systems for adherence to predefined thresholds and benchmarks related to responsible, ethical and sustainable practices. Design and implement technology mitigation strategies for systems to ensure ethical and responsible standards are achieved.\nMust have skills : Natural Language Processing (NLP)\nGood to have skills : NA\nMinimum 5 year(s) of experience is required\nEducational Qualification : 15 years full time education\nSummary:\nAs an AI/ML Engineer specializing in Natural Language Processing (NLP), you will be responsible for developing applications and systems that utilize AI to improve performance and efficiency. Your typical day will involve working with deep learning, neural networks, chatbots, and NLP to create innovative solutions for our clients.\n\nRoles & Responsibilities:\n- Develops applications and systems that utilize AI to improve performance and efficiency, including but not limited to deep learning, neural networks, chatbots, natural language processing.\n- Design and develop NLP-based applications and systems using deep learning, neural networks, and chatbots.\n- Collaborate with cross-functional teams to identify business requirements and translate them into technical solutions.\n- Implement and optimize NLP algorithms and models to improve performance and accuracy.\n- Stay up-to-date with the latest advancements in NLP and AI technologies and integrate innovative approaches for sustained competitive advantage.\n- Communicate technical findings effectively to stakeholders, utilizing data visualization tools for clarity.\n- Build predictive models, develop advanced algorithms that extract and classify information from large datasets quantify model performance\n- Evaluate emerging technologies that may contribute to our analytical platform Identify and exploit new patterns in data using various techniques\n- Worked with ML data mining toolkits like NLP, Semantic Web, R, Core NLP, NLTK etc. Information retrieval libraries like Lucene, SOLR fast paced, test driven, collaborative and iterative programming environment\nProfessional & Technical Skills:\n- Able to engage with customers for solving business problems leveraging Artificial Intelligent, Machine Learning\n- Solid understanding of machine learning algorithms and statistical analysis. Also, good understanding of machine learning algorithms such as CRFs , SVM\n- Identifying applicability of Machine Learning, Natural Language Processing (NLP) to use cases with ability to project both the business, Technology benefits\n- Experience in implementing various NLP algorithms and models.\n- Identify ways of embedding, integrating Artificial Intelligent, Machine Learning services into the enterprise architecture seamlessly\n- Keep abreast of new technology innovations in the field of Machine Learning, Natural Language Processing (NLP) bring it\n- Working experience in any of the NLP application areas, Sematic Web and Ontologies Machine Translation, Sentiment Analysis Document Classification, Question Answer Matching, Text Summarization\n- Have worked with RNN, LSTM etc.\n- Work exp in creating NLP pipelines for processing large amount of document corpus\n- Expertise with Python\n\nAdditional Information:\n- The ideal candidate will possess a strong educational background in computer science, mathematics, or a related field, along with a proven track record of delivering impactful data-driven solutions.\n-Be a self-starter and a fast learner\n- Possess strong problem-solving skills with the ability to methodically analyze and resolve tech challenges\n- Possess strong written, verbal, communication, analytical, technical, inter-personal and presentation skills",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Natural Language Processing', 'Cnn', 'Artificial Intelligence', 'Neural Networks', 'Data Mining', 'Data Extraction', 'Text Mining', 'LLM', 'Machine Learning', 'Deep Learning', 'Data Science', 'Rnn', 'Image Processing', 'Computer Vision']",2025-06-14 05:58:52
Senior Data Scientist,Virtana Corp,6 - 11 years,Not Disclosed,"['Pune', 'Chennai']","We are seeking a Senior Data Scientist Engineer with experience bringing highly scalable enterprise SaaS applications to market. This is a uniquely impactful opportunity to help drive our business forward and directly contribute to long-term growth at Virtana.\nIf you thrive in a fast-paced environment, take initiative, embrace proactivity and collaboration, and you re seeking an environment for continuous learning and improvement, we d love to hear from you!\nVirtana is a remote first work environment so you ll be able to work from the comfort of your home while collaborating with teammates on a variety of connectivity tools and technologies.\nRole Responsibilities:\nResearch and test machine learning approaches for analyzing large-scale distributed computing applications.\nDevelop production-ready implementations of proposed solutions across different models AI and ML algorithms, including testing on live customer data to improve accuracy, efficacy, and robustness\nWork closely with other functional teams to integrate implemented systems into the SaaS platform\nSuggest innovative and creative concepts and ideas that would improve the overall platform.\nJob Location - Pune, Chennai or Remote\nQualifications:\nThe ideal candidate must have the following qualifications:\n6 + years experience in practical implementation and deployment of large customer-facing ML based systems.\nMS or M Tech (preferred) in applied mathematics/statistics; CS or Engineering disciplines are acceptable but must have with strong quantitative and applied mathematical skills\nIn-depth working, beyond coursework, familiarity with classical and current ML techniques, both supervised and unsupervised learning techniques and algorithms\nImplementation experiences and deep knowledge of Classification, Time Series Analysis, Pattern Recognition, Reinforcement Learning, Deep Learning, Dynamic Programming and Optimization\nExperience in working on modeling graph structures related to spatiotemporal systems\nProgramming skills in Python is a must\nExperience in understanding and usage of LLM models and Prompt engineering is preferred.\nExperience in developing and deploying on cloud (AWS or Google or Azure)\nGood verbal and written communication skills\nFamiliarity with well-known ML frameworks such as Pandas, Keras, TensorFlow",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Time series analysis', 'Artificial Intelligence', 'IT operations management', 'Machine learning', 'Cloud', 'Cash flow', 'Pattern recognition', 'Performance monitoring', 'Python']",2025-06-14 05:58:54
Data Scientist- Large Language Models,Fortune Global 500 IT Services Firm,5 - 10 years,Not Disclosed,['Bengaluru'],"Project Role : Responsible AI Engineer\nProject Role Description : Assess AI systems for adherence to predefined thresholds and benchmarks related to responsible, ethical and sustainable practices. Design and implement technology mitigation strategies for systems to ensure ethical and responsible standards are achieved.\nMust have skills : Large Language Models\nGood to have skills : NA\nMinimum 5 year(s) of experience is required\nEducational Qualification : 15 years full time education\nSummary:\nAs an AI Platform Engineer, you will be responsible for developing applications and systems that utilize AI to improve performance and efficiency. Your typical day will involve working with Large Language Models, deep learning, neural networks, chatbots, and natural language processing.\n\nRoles & Responsibilities:\n- Design and develop scalable and efficient AI-based applications and systems using Large Language Models, deep learning, neural networks, chatbots, and natural language processing.\n- Collaborate with cross-functional teams to identify and prioritize AI use cases, and develop solutions that meet business requirements.\n- Implement and maintain AI models and algorithms, and ensure their accuracy, reliability, and scalability.\n- Develop and maintain AI infrastructure, including data pipelines, model training and deployment pipelines, and monitoring and logging systems.\n- Stay updated with the latest advancements in AI and machine learning, and integrate innovative approaches for sustained competitive advantage.\n\nProfessional & Technical Skills:\n- Must To Have Skills: Experience with Large Language Models.\n- Strong understanding of deep learning, neural networks, chatbots, and natural language processing.\n- Experience with AI infrastructure, including data pipelines, model training and deployment pipelines, and monitoring and logging systems.\n- Proficiency in programming languages such as Python, Java, or C++.\n- Experience with cloud platforms such as AWS, Azure, or GCP.\n- Experience with containerization technologies such as Docker and Kubernetes.\n\nAdditional Information:\n- The ideal candidate will possess a strong educational background in computer science, artificial intelligence, or a related field, along with a proven track record of delivering impactful AI-driven solutions.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Large Language Models', 'Java', 'C++', 'Artificial Intelligence', 'Natural Language Processing', 'Neural Networks', 'Machine Learning', 'Deep Learning', 'Data Science', 'Azure Cloud', 'GCP', 'Image Processing', 'Computer Vision', 'AWS', 'Python']",2025-06-14 05:58:57
Sr Data Scientist,Johnson Controls,1 - 2 years,Not Disclosed,['Pune'],"Job Title Senior Data Scientist - Data Analytics\nHow you will do it\nAdvanced Analytics, LLMs Modeling\nDesign and implement advanced machine learning models including deep learning, time-series forecasting, recommendation engines, and LLM-based solutions (e. g. , GPT, LLaMA, Claude).\nDevelop use cases around enterprise search, document summarization, conversational AI, and automated knowledge retrieval using large language models.",,,,"['Computer science', 'Product engineering', 'orchestration', 'Artificial Intelligence', 'Machine learning', 'Forecasting', 'Monitoring', 'Analytics', 'SQL', 'Python']",2025-06-14 05:58:59
Data Analyst,Fiserv,3 - 5 years,Not Disclosed,['Thane'],"Hi,\n\nWe at Fiserv are actively looking for Data Analysts with expertise in Power BI, SQL and Python.\n\nBelow is the job description:\n\nWhat does a successful Data Analyst do at Fiserv ?\nData Analyst is responsible for identifying any issues or ways to improve the collection, distribution and consumption of data. The data analysts will also monitor performance and quality control plans to identify any issues or ways to improve data orchestrations. This role requires collaborating with architects and developers to implement effective automation processes.\n\nWhat will you do:\nAbility to manage time and priorities with multiple tasks and projects, to work with loosely defined requirements.\nAnalyze, query and manipulate financial and business level data.\nValidate data sets are in synch with sources. Perform reconciliations of defined data. Identify, compare, and resolve data quality problems. Evaluate large dataset for quality and accuracy.\nDetermine business impact level for data quality issues. Work with Programmers to correct data quality errors. Determine root cause for data quality errors and make recommendations for solutions.\nResearch and determine scope and complexity of issue to identify steps to fix issue.\nDevelop process improvements to enhance overall data quality and execute data cleanup measures. Maintain a record of original data and corrected data.\nEnsure adherence to data quality standards. Identify areas of improvement to achieve data quality.\nResolve all internal data exceptions in timely and accurate manner.\n\nWhat will you need to know:\nBachelors Degree or equivalent experience.\nMust have analytical, problem solving, and team building skills.\nAbility to work independently, prioritize tasks and solve problems.\nProficient in MS Power BI, SQL and Python.\nExcellent communication (verbal and written), interpersonal, organizational, collaboration, and trouble shooting skills.\n\nWhat would be great to have:\nExposure to Foundry or Snowflake a plus.\nExperience in VBA is a plus.\n\nWe welcome and encourage diversity in our workforce. Fiserv is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, colour, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protector veteran.\nExplore the possibilities of a career with Fiserv and Find your Forward with us !",Industry Type: FinTech / Payments,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Power Bi', 'Business Analytics', 'Data Analysis', 'SQL', 'Python', 'Analytics']",2025-06-14 05:59:01
Data Analyst (Python),Crisil,0 - 3 years,Not Disclosed,"['Hyderabad', 'Pune']","Tech Resources\nAny graduate/postgraduate (MBA)/BSc/BTech with sound grip on financial aspects\nGood communication skills, both written and oral\nWilling to work in 24*5 environment on rotational shifts (including night shifts)\nCertification or knowledge/experience in MS-office (Excel, Word, PowerPoint)\nPreferred with exposure of working on data analysis\nKnowledge in SQL, Python and VBA Macro is a must\nKnowledge of corporate finance / accountancy i.e., financial statements and annual reports is a plus",Industry Type: Financial Services,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['companies act', 'office', 'accounting', 'microsoft', 'corporate finance', 'sql', 'java', 'listing agreement', 'writing', 'powerpoint', 'communication skills', 'financial statements', 'python', 'data analysis', 'sebi', 'excel', 'r', 'vba', 'accountancy', 'compliance', 'secretarial activities', 'annual reports', 'word', 'finance', 'ms office']",2025-06-14 05:59:03
Data Analyst,Optum,1 - 3 years,3.25-7.5 Lacs P.A.,"['Noida', 'Gurugram']",Role & responsibilities\n\nLooking for 1+ year exp\nSQL\nAZURE\n\n\nPreferred candidate profile\n\nAI exposure\ngood communication,Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Analytics', 'azure', 'SQL']",2025-06-14 05:59:05
Data Analyst,Schneider Electric,2 - 3 years,7-8 Lacs P.A.,"['Gurugram', 'Chennai', 'Bengaluru']","Support the PIM Solutions at a Process standpoint on Run Operations - Open & consider assigned new tickets coming from Support tool (2929IT).\n\nStays updated on upcoming functionalities and enhancements, and keeps users informed.",Industry Type: Electronics Manufacturing (Electronic Manufacturing Services (EMS)),Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Power Bi', 'Data Analysis', 'Data Analytics', 'SQL', 'Data Visualization']",2025-06-14 05:59:07
Data Engineer,Visa,10 - 14 years,Not Disclosed,['Bengaluru'],"At Visa, the Corporate Information Technology, Billing & Incentives Platforms team, enables Visas revenue growth through flexible pricing engines and global revenue platforms built on next-generation technologies. This includes managing system requirements, evaluating cutting-edge technologies, design, development, integration, quality assurance, implementation and maintenance of corporate revenue applications. The team works closely with business owners of these services to deliver customer developed solutions, as well as implement industry leading packaged software. This team has embarked on a major transformational journey to build and implement best of breed revenue and billing applications to transform our business as well as technology.\nThe candidate should enjoy working on diverse technologies and should be excited to take initiatives to solve complex business problems and get the job done while taking on new challenges. You should thrive in team-oriented and fast-paced environments where each team-member is vital to the overall success of the projects.\nKey Responsibilities\nDevelop and maintain test automation scripts using PySpark for big data applications.\nCollaborate with data engineers and developers to understand data processing workflows and requirements.\nDesign and implement automated tests for data ingestion, processing, and transformation in a Hadoop ecosystem.\nPerform data validation, data integrity, and performance testing for Spark applications.\nUtilize Spark-specific concepts such as RDDs, Data Frames, Datasets, and Spark SQL in test automation.\nCreate and manage CI/CD pipelines for automated testing in a big data environment.\nIdentify, report, and track defects, and work with the development team to resolve issues.\nOptimize and tune Spark jobs for performance and scalability.\nMaintain and update test cases based on new features and changes in the application.\nDocument test plans, test cases, and test results comprehensively.\nPerform QA and manual testing for payments applications, ensuring compliance with business requirements and standards.\nWork with limited direction, usually within a complex environment, to drive delivery of solutions and meet service levels.\nProductively work with stakeholders in multiple countries and time zones.\nWith active engagement, collaboration, effective communication, quality, integrity, and reliable delivery develop and maintain a trusted and valued relationship with the team, customers and business partners.\n\n\nBasic Qualifications\nBachelors degree, OR 3+ years of relevant work experience\n\nPreferred Qualifications\nbachelor s degree in computer science, Information Technology or related field.\nRelevant certifications in Big Data, Spa",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Data validation', 'Manual testing', 'Manager Quality Assurance', 'Billing', 'Performance testing', 'Data processing', 'Test cases', 'Information technology', 'SQL']",2025-06-14 05:59:09
Data Annotation Specialist,Yamaha Motor Solutions,0 - 3 years,3-4.5 Lacs P.A.,['Faridabad'],"We are seeking a highly detail-oriented and technically adept 3D Data Annotation Specialist to join our growing team. This role is critical in shaping high-quality datasets for training cutting-edge AI and computer vision models, particularly in domains such as LiDAR data processing, and 3D object detection.\n\nRoles and Responsibilities\nQualifications:\nB.Tech in Computer Science, IT, or related field preferred (others may also apply strong analytical and software learning abilities required).\nStrong analytical and reasoning skills, with attention to spatial geometry and object relationships in 3D space.\nBasic understanding of 3D data formats (e.g., .LAS, .LAZ, .PLY) and visualization tools.\nAbility to work independently while maintaining high-quality standards.\nExcellent communication skills and the ability to collaborate in a fast-paced environment.\nAttention to detail and ability to work with precision in visual/manual tasks.\nGood understanding of basic geometry, coordinate systems, and file handling.\nPreferred Qualifications:\nPrior experience in 3D data annotation or LiDAR data analysis.\nExposure to computer vision workflows.\nComfortable working with large datasets and remote sensing data\nKey Responsibilities:\nAnnotate 3D point cloud data with precision using specialized tools [ Training would be provided]\nLabel and segment objects within LiDAR data, aerial scans, or 3D models.\nFollow annotation guidelines while applying logical and spatial reasoning to 3D environments.\nCollaborate with ML engineers and data scientists to ensure annotation accuracy and consistency.\nProvide feedback to improve annotation tools and workflow automation.\nParticipate in quality control reviews and conduct re-annotation as needed",Industry Type: Automobile,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['annotation', 'python', 'data analysis', 'data analytics', 'modeling', 'analytical', 'software', 'strong analytical skills', 'data processing', 'machine learning', 'sql', 'data cleansing', 'data entry operation', 'cloud', '3d', 'data extraction', 'automation', 'data science', 'data annotation', 'lidar', 'communication skills']",2025-06-14 05:59:11
Data Engineer+ Subject Matter Expert- Data Mining,Newton School,0 - 2 years,Not Disclosed,['Pune'],"Newton School of Technology is on a mission to transform technology education and bridge the employability gap. As India s first impact university, we are committed to revolutionizing\nlearning, empowering students, and shaping the future of the tech industry. Backed by\nrenowned professionals and industry leaders, we aim to solve the employability challenge\nand create a lasting impact on society. We are currently looking for a Data Engineer + Subject Matter Expert - Data Mining to join our Computer Science Department. This is a full-time academic role focused on data mining, analytics, and teaching/mentoring students in core data science and engineering topics.\n\nKey Responsibilities:\nDevelop and deliver comprehensive and engaging lectures for the undergraduate\n""Data Mining"", BigData and Data Analytics courses, covering the full syllabus from\nfoundational concepts to advanced techniques.\nInstruct students on the complete data lifecycle, including data preprocessing,\ncleaning, transformation, and feature engineering.\nTeach the theory, implementation, and evaluation of a wide range of algorithms for\nClassification, Association rules mining, Clustering and Anomaly Detections.\nDesign and facilitate practical lab sessions and assignments that provide students\nwith hands-on experience using modern data tools and software.\nDevelop and grade assessments, including assignments, projects, and examinations,\nthat effectively measure the Course Learning Objectives (CLOs).\nMentor and guide students on projects, encouraging them to work with real-world or\nbenchmark datasets (e.g., from Kaggle).\nStay current with the latest advancements, research, and industry trends in data\nengineering and machine learning to ensure the curriculum remains relevant and\ncutting-edge.\nContribute to the academic and research environment of the department and the\nuniversity.\n\nRequired Qualifications:\nA Ph.D. (or a Masters degree with significant, relevant industry experience) in\nComputer Science, Data Science, Artificial Intelligence, or a closely related field.\nDemonstrable expertise in the core concepts of data engineering and machine\nlearning as outlined in the syllabus.\nStrong practical proficiency in Python and its data science ecosystem, specifically\nScikit-learn, Pandas, NumPy, and visualization libraries (e.g., Matplotlib, Seaborn).\nProven experience in teaching, preferably at the undergraduate level, with an ability to\nmake complex topics accessible and engaging.\nExcellent communication and interpersonal skills.\n\nPreferred Qualifications:\nA strong record of academic publications in reputable data mining, machine learning,\nor AI conferences/journals.\nPrior industry experience as a Data Scientist, Big Data Engineer, Machine Learning\nEngineer, or in a similar role.\nExperience with big data technologies (e.g., Spark, Hadoop) and/or deep learning\nframeworks (e.g., TensorFlow, PyTorch).\nExperience in mentoring student teams for data science competitions or hackathons.\n\nPerks & Benefits:\nCompetitive salary packages aligned with industry standards.\nAccess to state-of-the-art labs and classroom facilities.\nTo know more about us, feel free to explore our website: Newton School of Technology\nWe look forward to the possibility of having you join our academic team and help shape the\nfuture of tech education!\nNewton School of Technology is on a mission to transform technology education and bridge the employability gap. As India s first impact university,\n...",Industry Type: Education / Training,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Interpersonal skills', 'data science', 'Artificial Intelligence', 'Machine learning', 'Manager Technology', 'Data analytics', 'Subject Matter Expert', 'Data mining', 'Python']",2025-06-14 05:59:13
Senior - Data Engineering Professional,KPMG India,2 - 7 years,Not Disclosed,['Bengaluru'],"2 - 7 years of experience in Python\nGood understanding of Big data ecosystems and frameworks such as Hadoop, Spark etc.\nExperience in developing data processing task using PySpark .\nExpertise in at least one popular cloud provider preferably AWS is a plus.\nGood knowledge of any RDBMS/NoSQL database with strong SQL writing skills\nExperience on Datawarehouse tools like Snowflake is a plus.\nExperience with any one ETL tool is a plus\nStrong analytical and problem-solving capability\nExcellent verbal and written communications skills\nClient facing skills: Solid experience working with clients directly, to be able to build trusted relationships with stakeholders\nAbility to collaborate effectively across global teams\nStrong understanding of data structures, algorithm, object-oriented design and design patterns\nExperience in the use of multi-dimensional data, data curation processes, and the measurement/improvement of data quality.\nGeneral knowledge of business processes, data flows and quantitative models that generate or consume data\nIndependent thinker, willing to engage, challenge and learn new technologies\nQualification\nBachelor s degree or master s in computer science or related field.\nCertification from professional bodies is a plus.\nSELECTION PROCESS\nCandidates should expect 3 - 4 rounds of personal or telephonic interviews to assess fitment and communication skills\n.",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Object oriented design', 'RDBMS', 'Analytical', 'Data processing', 'Data structures', 'Data quality', 'Data warehousing', 'SQL', 'Python']",2025-06-14 05:59:15
Sr. Data Engineer (Bigdata + Java/Python/Go),Visa,2 - 9 years,Not Disclosed,['Bengaluru'],"Visas Data and AI Platform team is at the forefront of transforming the payment industry by harnessing the power of big data, artificial intelligence, and cutting-edge open-source technologies. Our mission is to build scalable, high-performance data platforms and services that enable Visa s businesses to innovate and deliver unparalleled value to our customers worldwide.\nOur Work\nAt Visa, we are dedicated to developing robust data platforms that handle massive volumes of data, ensuring real-time processing, analytics, and low-latency responses. Our team leverages a variety of open-source software to create flexible, efficient, and reliable solutions that meet the dynamic needs of our business operations.\nBig Data Platforms: We design and implement platforms capable of processing and analyzing vast amounts of data generated by millions of transactions daily. These platforms support various use cases, from fraud detection to personalized customer experiences.\nOpen-Source Software: Our development approach heavily relies on open-source technologies, allowing us to build innovative solutions while contributing to the wider tech community. We use and contribute to projects like Apache Hadoop, Apache Druid, Apache Pinot, Apache Spark, Apache Flink, and Apache Kafka, among others.\nCutting-Edge Technologies: To achieve our goals, we incorporate state-of-the-art technologies in our stack. This includes leveraging streaming analytics, real-time data processing, and low-latency services to ensure that our platforms are always at the cutting edge of performance and reliability.\nWho We Are Looking For\nWe are seeking seasoned professionals with a deep understanding and expertise in large-scale distributed systems and platform development using open-source software. Ideal candidates will have:\nExpertise in Distributed Systems: Proven experience in designing, developing, and maintaining large-scale distributed systems.\nOpen-Source Contributions: Active involvement in the open-source community, particularly in streaming, analytical, and low-latency services.\nTechnical Proficiency: Strong proficiency in technologies such as Apache Druid, Apache Pinot, Apache Spark, Apache Flink, Apache Kafka, Temporal, and Yugabyte.\nInnovative Mindset: A passion for innovation and a drive to stay ahead of the curve by continuously exploring and implementing new technologies.\nJoin Us\nIf you are a passionate technologist with a track record of delivering high-quality, scalable solutions and contributing to the open-source community, we invite you to join our team. At Visa, you will have the opportunity to work on groundbreaking projects, collaborate with talented professionals, and make a significant impact on the future of payments.\nBy joining our Data Platform team, you will play a crucial role in enabling Visa s businesses to leverage data and AI to drive innovation, enhance security, and deliver exceptional customer experiences.\nThe candidate will:\nActively participate in all phases of software development lifecycle: analysis, technical design, planning, development, testing/CICD, release, post production/escalation support\nWork on a highly scalable, available and resilient multi-tenant Platforms that can host large scale and highly critical applications.\nDevelop large scale multi-tenant software components on the Platform in an Agile based methodology to provide self-service capabilities.\nCollaborating with partners from business and technology organizations, develop key deliverables for Data Platform Strategy - Scalability, optimization, operations, availability, roadmap.\nBuild partnerships with other technology teams to drive efficiency in their use of platforms and services that PaaS provide\nExpertise in applying the appropriate software engineering patterns to build robust and scalable systems.\nBe a self-starter, work independently and Mentor junior members in the scrum team. Stays current with new and evolving technologies\nBuilds strong commitment within the team to support the appropriate team priorities\nThis is a hybrid position. Expectation of days in office will be confirmed by your Hiring Manager.\n\n\nBasic Qualifications:\n3+ years of relevant work experience with a Bachelor s Degree or at least 2 years of work experience with an Advanced degree (e.g. Masters, MBA, JD, MD) or 0 years of work experience with a PhD, OR 3+ years of relevant wor",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Automation', 'GIT', 'Configuration management', 'Scrum', 'Agile methodology', 'Open source', 'Distribution system', 'Analytics', 'Python']",2025-06-14 05:59:17
Data Governance Analyst,HCLTech,4 - 6 years,Not Disclosed,"['Noida', 'Vijayawada', 'Chennai']","Key Responsibilities:\nUnderstand all of our data definitions and nuances (e.g., attribution window)\nMaintain and update glossary; Advise various internal stakeholders\nConsult data visualization team and analysts on what data to use for new reports and analyses\nRecommend development or enhancement of datasets for reporting and analyses questions\nDevelop and implement data governance policies and procedures to ensure data quality, availability, and integrity.",,,,"['Data Management', 'ETL', 'Data Governance', 'SQL', 'Python', 'Data Engineering', 'Data Modeling', 'Data Warehousing', 'Data Governance Analyst']",2025-06-14 05:59:20
Lead Data Engineer,Jio,6 - 11 years,Not Disclosed,['Mumbai (All Areas)'],"Overview of the Company:\nJio Platforms Ltd. is a revolutionary Indian multinational tech company, often referred to as India's biggest startup, headquartered in Mumbai. Launched in 2019, it's the powerhouse behind Jio, India's largest mobile network with over 400 million users. But Jio Platforms is more than just telecom. It's a comprehensive digital ecosystem, developing cutting-edge solutions across media, entertainment, and enterprise services through popular brands like JioMart, JioFiber, and JioSaavn.\nJoin us at Jio Platforms and be part of a fast-paced, dynamic environment at the forefront of India's digital transformation. Collaborate with brilliant minds to develop next-gen solutions that empower millions and revolutionize industries.\n\nTeam Overview:\nThe Data Platforms Team is the launchpad for a data-driven future, empowering the Reliance Group of Companies. We're a passionate group of experts architecting an enterprise-scale data mesh to unlock the power of big data, generative AI, and ML modelling across various domains. We don't just manage data we transform it into intelligent actions that fuel strategic decision-making. Imagine crafting a platform that automates data flow, fuels intelligent insights, and empowers the organization that's what we do.\nJoin our collaborative and innovative team, and be a part of shaping the future of data for India's biggest digital revolution! About the role.\n\nTitle: Lead Data Engineer\nLocation: Mumbai\n\nResponsibilities:\nEnd-to-End Data Pipeline Development: Design, build, optimize, and maintain robust data pipelines across cloud, on-premises, or hybrid environments, ensuring performance, scalability, and seamless data flow.\nReusable Components & Frameworks: Develop reusable data pipeline components and contribute to the team's data pipeline framework evolution.\nData Architecture & Solutions: Contribute to data architecture design, applying data modelling, storage, and retrieval expertise.\nData Governance & Automation: Champion data integrity, security, and efficiency through metadata management, automation, and data governance best practices.\nCollaborative Problem Solving: Partner with stakeholders, data teams, and engineers to define requirements, troubleshoot, optimize, and deliver data-driven insights.\nMentorship & Knowledge Transfer: Guide and mentor junior data engineers, fostering knowledge sharing and professional growth.\n\nQualification Details:\nEducation: Bachelor's degree or higher in Computer Science, Data Science, Engineering, or a related technical field.\nCore Programming: Excellent command of a primary data engineering language (Scala, Python, or Java) with a strong foundation in OOPS and functional programming concepts.\nBig Data Technologies: Hands-on experience with data processing frameworks (e.g., Hadoop, Spark, Apache Hive, NiFi, Ozone, Kudu), ideally including streaming technologies (Kafka, Spark Streaming, Flink, etc.).\nDatabase Expertise: Excellent querying skills (SQL) and strong understanding of relational databases (e.g., MySQL, PostgreSQL). Experience with NoSQL databases (e.g., MongoDB, Cassandra) is a plus.\nEnd-to-End Pipelines: Demonstrated experience in implementing, optimizing, and maintaining complete data pipelines, integrating varied sources and sinks including streaming real-time data.\nCloud Expertise: Knowledge of Cloud Technologies like Azure HDInsights, Synapse, EventHub and GCP DataProc, Dataflow, BigQuery.\nCI/CD Expertise: Experience with CI/CD methodologies and tools, including strong Linux and shell scripting skills for automation.\n\nDesired Skills & Attributes:\nProblem-Solving & Troubleshooting: Proven ability to analyze and solve complex data problems, troubleshoot data pipeline issues effectively.\nCommunication & Collaboration: Excellent communication skills, both written and verbal, with the ability to collaborate across teams (data scientists, engineers, stakeholders).\nContinuous Learning & Adaptability: A demonstrated passion for staying up-to-date with emerging data technologies and a willingness to adapt to new tools.",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Azure Cloud', 'Kafka', 'Python']",2025-06-14 05:59:22
Data Consultant-Data Scientist,Kyndryl,10 - 15 years,Not Disclosed,['Bengaluru'],"Who We Are\nAt Kyndryl, we design, build, manage and modernize the mission-critical technology systems that the world depends on every day. So why work at Kyndryl? We are always moving forward – always pushing ourselves to go further in our efforts to build a more equitable, inclusive world for our employees, our customers and our communities.\n\nThe Role\nAs a Data Scientist at Kyndryl you are the bridge between business problems and innovative solutions, using a powerful blend of well-defined methodologies, statistics, mathematics, domain expertise, consulting, and software engineering. You'll wear many hats, and each day will present a new puzzle to solve, a new challenge to conquer.\n\nYou will dive deep into the heart of our business, understanding its objectives and requirements – viewing them through the lens of business acumen, and converting this knowledge into a data problem. You’ll collect and explore data, seeking underlying patterns and initial insights that will guide the creation of hypotheses.",,,,"['python', 'transformers', 'natural language processing', 'data mining', 'consulting', 'microsoft azure', 'numpy', 'machine learning', 'artificial intelligence', 'nosql', 'cloud', 'r', 'tensorflow', 'data science', 'gcp', 'optimization techniques', 'pytorch', 'mysql', 'aws', 'programming', 'architecture', 'nosql databases', 'ml', 'statistics']",2025-06-14 05:59:24
Data Engineer - Databricks,KPI Partners,0 - 4 years,Not Disclosed,['Pune'],"About KPI Partners.\nKPI Partners is a leading provider of data analytics solutions, dedicated to helping organizations transform data into actionable insights. Our innovative approach combines advanced technology with expert consulting, allowing businesses to leverage their data for improved performance and decision-making.\n\nJob Description.\nWe are seeking a skilled and motivated Data Engineer with experience in Databricks to join our dynamic team. The ideal candidate will be responsible for designing, building, and maintaining scalable data pipelines and data processing solutions that support our analytics initiatives. You will collaborate closely with data scientists, analysts, and other engineers to ensure the consistent flow of high-quality data across our platforms.",,,,"['Computer science', 'Data modeling', 'Analytical', 'Consulting', 'Data processing', 'Data quality', 'Information technology', 'Analytics', 'SQL', 'Python']",2025-06-14 05:59:26
Data/Applied Scientist (Search),BAY Area Technology Solutions,3 - 5 years,Not Disclosed,"['Hyderabad', 'Bengaluru', 'Delhi / NCR']","Strong in Python and experience with Jupyter notebooks, Python packages like polars, pandas, numpy, scikit-learn, matplotlib, etc.\nMust have: Experience with machine learning lifecycle, including data preparation, training, evaluation, and deployment Must have: Hands-on experience with GCP services for ML & data science Must have: Experience with Vector Search and Hybrid Search techniques Must have: Experience with embeddings generation using models like BERT, Sentence Transformers, or custom models Must have: Experience in embedding indexing and retrieval (e.g., Elastic, FAISS, ScaNN, Annoy) Must have: Experience with LLMs and use cases like RAG (Retrieval-Augmented Generation) Must have: Understanding of semantic vs lexical search paradigms Must have: Experience with Learning to Rank (LTR) techniques and libraries (e.g., XGBoost, LightGBM with LTR support) Should be proficient in SQL and BigQuery for analytics and feature generation Should have experience with Dataproc clusters for distributed data processing using Apache Spark or PySpark Should have experience deploying models and services using Vertex AI, Cloud Run, or Cloud Functions Should be comfortable working with BM25 ranking (via Elasticsearch or OpenSearch) and blending with vector-based approaches Good to have: Familiarity with Vertex AI Matching Engine for scalable vector retrieval Good to have: Familiarity with TensorFlow Hub, Hugging Face, or other model repositories Good to have: Experience with prompt engineering, context windowing, and embedding optimization for LLM-based systems Should understand how to build end-to-end ML pipelines for search and ranking applications Must have: Awareness of evaluation metrics for search relevance (e.g., precision@k, recall, nDCG, MRR) Should have exposure to CI/CD pipelines and model versioning practices GCP Tools Experience: ML & AI: Vertex AI, Vertex AI Matching Engine, AutoML, AI Platform Storage: BigQuery, Cloud Storage, Firestore Ingestion: Pub/Sub, Cloud Functions, Cloud Run Search: Vector Databases (e.g., Matching Engine, Qdrant on GKE), Elasticsearch/OpenSearch Compute: Cloud Run, Cloud Functions, Vertex Pipelines, Cloud Dataproc (Spark/PySpark) CI/CD & IaC: GitLab/GitHub Actions\nLocation: Remote- Bengaluru,Hyderabad,Delhi / NCR,Chennai,Pune,Kolkata,Ahmedabad,Mumbai",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Python', 'pandas', 'Jupyter notebooks', 'machine learning lifecycle', 'scikit-learn', 'Sentence Transformers', 'matplotlib', 'polars', 'BERT', 'numpy']",2025-06-14 05:59:29
Senior Lead Data Architect,JPMorgan Chase Bank,11 - 19 years,Not Disclosed,['Hyderabad'],"You strive to be an essential member of a diverse team of visionaries dedicated to making a lasting impact. Don t pass up this opportunity to collaborate with some of the brightest minds in the field and deliver best-in-class solutions to the industry.\n\n\nAs a Senior Lead Data Architect at JPMorgan Chase within the Consumer and Community Banking Data Technology, you are an integral part of a team that works to develop high-quality data architecture solutions for various software applications, platform and data products. Drive significant business impact and help shape the global target state architecture through your capabilities in multiple data architecture domains.\n\nJob responsibilities\n\n\nRepresents the data architecture team at technical governance bodies and provides feedback regarding proposed improvements regarding data architecture governance practices\n\nEvaluates new and current technologies using existing data architecture standards and frameworks\n\nRegularly provides technical guidance and direction to support the business and its technical teams, contractors, and vendors\n\nDesign secure, high-quality, scalable solutions and reviews architecture solutions designed by others\n\nDrives data architecture decisions that impact data product platform design, application functionality, and technical operations and processes\n\nServes as a function-wide subject matter expert in one or more areas of focus\n\nActively contributes to the data engineering community as an advocate of firmwide data frameworks, tools, and practices in the Software Development Life Cycle\n\nInfluences peers and project decision-makers to consider the use and application of leading-edge technologies\n\nAdvises junior architects and technologists\n\n\nRequired qualifications, capabilities, and skills\n\n\n7+ years of hands-on practical experience delivering data architecture and system designs, data engineer, testing, and operational stability\n\nAdvanced knowledge of architecture, applications, and technical processes with considerable in-depth knowledge in data architecture discipline and solutions (e. g. , data modeling, native cloud data services, business intelligence, artificial intelligence, machine learning, data domain driven design, etc. )\n\nPractical cloud based data architecture and deployment experience, preferably AWS\n\nPractical SQL development experiences in cloud native relational databases, e. g. Snowflake, Athena, Postgres\n\nAbility to deliver various types of data models with multiple deployment targets, e. g. conceptual, logical and physical data models deployed as an operational vs. analytical data stores\n\nAdvanced in one or more data engineering disciplines, e. g. streaming, ELT, event processing\n\nAbility to tackle design and functionality problems independently with little to no oversight\n\nAbility to evaluate current and emerging technologies to select or recommend the best solutions for the future state data architecture\n\n\nPreferred qualifications, capabilities, and skills\n\n\nFinancial services experience, card and banking a big plus\n\nPractical experience in modern data processing technologies, e. g. , Kafka streaming, DBT, Spark, Airflow, etc.\n\nPractical experience in data mesh and/or data lake\n\nPractical experience in machine learning/AI with Python development a big plus\n\nPractical experience in graph and semantic technologies, e. g. RDF, LPG, Neo4j, Gremlin\n\nKnowledge of architecture assessments frameworks, e. g. Architecture Trade off Analysis",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data modeling', 'Analytical', 'Artificial Intelligence', 'Banking', 'Machine learning', 'Data processing', 'Business intelligence', 'Financial services', 'Python', 'Data architecture']",2025-06-14 05:59:31
Business Analytics/Data Scientist - SAS & SQL with BFSI Domain,Khushboo,4 - 9 years,Not Disclosed,['Hyderabad'],"hands-on Analytics experience with 2+ years of experience.\nhands-on experience with one or more data analytics tools including Python, R, SAS, and SQL, SPARK\nGood understanding of credit card industry, financial\nProficiency in Tableau is a plus",Industry Type: Financial Services,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['sas', 'Credit Risk Modelling', 'sql', 'Consumer Banking', 'Predictive Modeling', 'python', 'Credit Card Analytics', 'Statistical Modeling', 'credit card', 'Model Development']",2025-06-14 05:59:33
Senior Data Engineer,Ford,3 - 12 years,Not Disclosed,['Chennai'],"The Service360 Senior Data Engineer will be the trusted data advisor in GDI&A (Global Data Insights & Analytics) supporting the following teams: Ford Pro, FCSA (Ford Customer Service Analytics) and FCSD (Ford Customer Service Division) Business. This is an exciting opportunity that provides the Data Engineer a well-rounded experience. The position requires translation of the customer s Analytical needs into specific data products that should be built in the GCP environment by collaborating with the Product Owners, Technical Anchor and the Customers\n\nExperience working in GCP native (or equivalent) services like Big Query, Google Cloud Storage, PubSub, Dataflow, Dataproc etc.\n\nExperience working with Airflow for scheduling and orchestration of data pipelines.\n\nExperience working with Terraform to provision Infrastructure as Code.\n\n2 + years professional development experience in Java or Python.\n\nBachelor s degree in computer science or related scientific field.\n\nExperience in analysing complex data, organizing raw data, and integrating massive datasets from multiple data sources to build analytical domains and reusable data products.\n\nExperience in working with architects to evaluate and productionalize data pipelines for data ingestion, curation, and consumption.\n\nExperience in working with stakeholders to formulate business problems as technical data requirements, identify and implement technical solutions while ensuring key business drivers are captured in collaboration with product management.\n\n\nWork on a small agile team to deliver curated data products for the Product Organization.\n\nWork effectively with fellow data engineers, product owners, data champions and other technical experts.\n\nMinimum of 5 years of experience with progressive responsibilities in software development\n\nMinimum of 3 years of experience defining product vision, strategy, product roadmaps and creating and managing backlogs\n\nExperience wrangling, transforming and visualizing large data sets from multiple sources, using a variety of tools\n\nProficiency in SQL is a must have skill\n\nExcellent written and verbal communication skills\n\nMust be comfortable presenting to and interacting with cross-functional teams and customers\n\nDemonstrate technical knowledge and communication skills with the ability to advocate for well-designed solutions.\n\nDevelop exceptional analytical data products using both streaming and batch ingestion patterns on Google Cloud Platform with solid data warehouse principles.\n\nBe the Subject Matter Expert in Data Engineering with a focus on GCP native services and other well integrated third-party technologies.\n\nArchitect and implement sophisticated ETL pipelines, ensuring efficient data integration into Big Query from diverse batch and streaming sources.\n\nSpearhead the development and maintenance of data ingestion and analytics pipelines using cutting-edge tools and technologies, including Python, SQL, and DBT/Data form.\n\nEnsure the highest standards of data quality and integrity across all data processes.\n\nData workflow management using Astronomer and Terraform for cloud infrastructure, promoting best practices in Infrastructure as Code\n\nRich experience in Application Support in GCP. Experienced in data mapping, impact analysis, root cause analysis, and document data lineage to support robust data governance.\n\nDevelop comprehensive documentation for data engineering processes, promoting knowledge sharing and system maintainability.\n\nUtilize GCP monitoring tools to proactively address performance issues and ensure system resilience, while providing expert production support.\n\nProvide strategic guidance and mentorship to team members on data transformation initiatives, championing data utility within the enterprise.",Industry Type: Auto Components,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Product management', 'Computer science', 'Application support', 'Production support', 'Agile', 'Scheduling', 'Customer service', 'Analytics', 'SQL', 'Python']",2025-06-14 05:59:35
Supply Chain Analyst - Data Engineering,TE Connectivity,5 - 10 years,Not Disclosed,['Bengaluru'],,,,,"['azure databricks', 'supply chain', 'azure data factory', 'data engineering', 'data modeling', 'azure data warehouse', 'python', 'ssas', 'power bi', 'warehouse', 'dashboards', 'supply', 'sql server', 'sql', 'transportation', 'sql server integration services', 'data center', 'spark', 'ssrs', 'etl', 'ssis', 'msbi']",2025-06-14 05:59:37
Senior Data Engineer,Taskus,5 - 10 years,Not Disclosed,['Chennai'],"What We Offer:\nWhat youll do\nDevelop, maintain, and enhance new data sources and tables, contributing to data engineering efforts to ensure comprehensive and efficient data architecture.\nServes as the liaison between Data Engineer team and the Airport operation teams, developing new data sources and overseeing enhancements to existing database; being one of the main contact points for data requests, metadata, and statistical analysis",,,,"['BPO', 'Project management', 'Analytical', 'Social media', 'Javascript', 'HTML', 'Data quality', 'Outsourcing', 'Gaming', 'SQL']",2025-06-14 05:59:39
Hiring | Data Engineer- Azure | Sr Analyst/ Consultant | Bangalore,Global MNC,5 - 10 years,Not Disclosed,['Bengaluru'],"Skill required: Data Engineers- Azure\nDesignation: Sr Analyst/ Consultant\nJob Location: Bengaluru\nQualifications: BE/BTech\nYears of Experience: 4 - 11 Years\n\nOVERALL PURPOSE OF JOB\nUnderstand client requirements and build ETL solution using Azure Data Factory, Azure Databricks & PySpark. Build solution in such a way that it can absorb clients change request very easily. Find innovative ways to accomplish tasks and handle multiple projects simultaneously and independently. Works with Data & appropriate teams to effectively source required data. Identify data gaps and work with client teams to effectively communicate the findings to stakeholders/clients.\n\nResponsibilities:\nDevelop ETL solution to populate Centralized Repository by integrating data from various data sources.\nCreate Data Pipelines, Data Flow, Data Model according to the business requirement.\nProficient in implementing all transformations according to business needs.\nIdentify data gaps in data lake and work with relevant data/client teams to get necessary data required for dashboarding/reporting.\nStrong experience working on Azure data platform, Azure Data Factory, Azure Data Bricks.\nStrong experience working on ETL components and scripting languages like PySpark, Python.\nExperience in creating Pipelines, Alerts, email notifications, and scheduling jobs.\nExposure on development/staging/production environments.\nProviding support in creating, monitoring and troubleshooting the scheduled jobs.\nEffectively work with client and handle client interactions.\n\nSkills Required:\nBachelors' degree in Engineering or Science or equivalent graduates with at least 4-11 years of overall experience in data management including data integration, modeling & optimization.\nMinimum 4 years of experience working on Azure cloud, Azure Data Factory, Azure Databricks.\nMinimum 3-4 years of experience in PySpark, Python, etc. for data ETL.\nIn-depth understanding of data warehouse, ETL concept and modeling principles.\nStrong ability to design, build and manage data.\nStrong understanding of Data integration.\nStrong Analytical and problem-solving skills.\nStrong Communication & client interaction skills.\nAbility to design database to store huge data necessary for reporting & dashboarding.\nAbility and willingness to acquire knowledge on the new technologies, good analytical and interpersonal skills\nwith ability to interact with individuals at all levels.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Azure Data Factory', 'Data Management', 'Data Engineer', 'Azure Data Platform', 'Azure Data Bricks', 'Pyspark', 'Data Modelling', 'ETL', 'Python']",2025-06-14 05:59:41
Cognitive Data and Analytics Data Engineer,Dell Technologies,3 - 6 years,Not Disclosed,"['Hyderabad', 'Gurugram', 'Bengaluru']","Data Engineering Advisor\nData Science is all about breaking new ground to enable businesses to answer their most urgent questions. Pioneering massively parallel data-intensive analytic processing, our mission is to develop a whole new approach to generating meaning and value from petabyte-scale data sets and shape brand new methodologies, tools, statistical methods and models. What s more, we are in collaboration with leading academics, industry experts and highly skilled engineers to equip our customers to generate sophisticated new insights from the biggest of big data.\nJoin us to do the best work of your career and make a profound social impact as a Data Engineering Advisor on our Data Engineering Team in Bangalore /Hyderabad/Gurgaon.\n\nWhat you ll achieve\nAs a Data Engineer, you will build leading edge AI-Fueled, Data-Driven business solutions within Services Parts, Services Repair and Services Engineering Space.\n\nYou will:\nInteract with business leaders and internal customers to create and establish design standards and assurance processes for software, systems and applications development to ensure compatibility and operability.\nAnalyze business goals, defines project scope and identifies functional and technical requirements to produce accurate business insights.\nDevelop fundamentally new approaches to generate meaning from data, creating specifications for reports and analysis based on business needs.\nSupport existing AI and Analytics products by ensuring optimal operations and enhance for new functionalities.\n\nTake the first step towards your dream career\nEvery Dell Technologies team member brings something unique to the table. Here s what we are looking for with this role:\n\nEssential Requirements\nUses proactive outcomes-driven mindset\nUses detailed analytical data skills\nUses hands on experience developing coding data and analytics solutions\nUses hands on expert knowledge in SQL\nUses hands on expert knowledge in Python, Java, or other modern programming language\nDesirable Requirements\nTeradata macro programming\nData management concepts\n\n\nApplication closing date: 30th June 25",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Assurance', 'Data management', 'Coding', 'Analytical', 'Programming', 'Engineering Manager', 'Teradata', 'Business solutions', 'SQL', 'Python']",2025-06-14 05:59:44
Cognitive Data and Analytics Data Engineer,Dell Technologies,3 - 6 years,Not Disclosed,"['Hyderabad', 'Gurugram', 'Bengaluru']","Senior Analyst, Data Engineer\nData Science is all about breaking new ground to enable businesses to answer their most urgent questions. Pioneering massively parallel data-intensive analytic processing, our mission is to develop a whole new approach to generating meaning and value from petabyte-scale data sets and shape brand new methodologies, tools, statistical methods and models. What s more, we are in collaboration with leading academics, industry experts and highly skilled engineers to equip our customers to generate sophisticated new insights from the biggest of big data.\nJoin us to do the best work of your career and make a profound social impact as a Data Engineering Sr. Analyst on our Data Engineering Team in Bangalore / Hyderabad/Gurgaon.\nWhat you ll achieve\nAs a Data Engineer, you will build and support leading edge AI-Fueled, Data-Driven business solutions within Services Parts, Services Repair and Services Engineering Space.\n\nYou will:\nInteract with business leaders and internal customers to create and establish design standards and assurance processes for software, systems and applications development to ensure compatibility and operability.\nAnalyze business goals, defines project scope and identifies functional and technical requirements to produce accurate business insights.\nDevelop fundamentally new approaches to generate meaning from data, creating specifications for reports and analysis based on business needs.\nSupport existing AI and Analytics products by ensuring optimal operations and enhance for new functionalities.\n\nTake the first step towards your dream career\nEvery Dell Technologies team member brings something unique to the table. Here s what we are looking for with this role:\n\nEssential Requirements\nUses proactive outcomes-driven mindset\nUses detailed analytical data skills\nUses hands on experience developing coding data and analytics solutions\nUses hands on knowledge in SQL\nUses hands on knowledge in Python, Java, or other modern programming language\nDesirable Requirements\nTeradata macro programming\nData management concepts\n\n\nApplication closing date: 30th June 25",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Assurance', 'Data management', 'Coding', 'Senior Analyst', 'Analytical', 'Programming', 'Teradata', 'Business solutions', 'SQL', 'Python']",2025-06-14 05:59:46
Data Analysis - English Specialist Data Analysis - English Specialist,Zensar,2 - 6 years,Not Disclosed,"['Kolkata', 'Mumbai', 'New Delhi', 'Hyderabad', 'Pune', 'Chennai', 'Bengaluru']","Zensar Technologies is looking for Data Analysis - English Specialist Data Analysis - English Specialist to join our dynamic team and embark on a rewarding career journey A Data Analyst is responsible for collecting, analyzing, and interpreting data to identify trends, patterns, and insights that can drive informed business decisions\n\nThey play a crucial role in helping organizations understand their data, derive actionable insights, and optimize processes\n\nHere is a general job description for a Data Analyst:Data Collection and Preparation: Collect and gather relevant data from various sources, such as databases, spreadsheets, and external systems\n\nClean, validate, and transform the data to ensure accuracy and consistency\n\nData Analysis and Interpretation: Apply statistical techniques, data mining methods, and visualization tools to analyze large datasets\n\nIdentify trends, patterns, and correlations within the data and generate insights to support decision-making\n\nReporting and Visualization: Create clear and concise reports, dashboards, and visual representations of data using data visualization tools, such as Tableau, Power BI, or Excel\n\nPresent findings to stakeholders in a compelling and understandable manner\n\nData Quality and Integrity: Ensure data integrity and accuracy by conducting data validation, resolving discrepancies, and monitoring data quality\n\nImplement measures to maintain data privacy, security, and compliance with regulatory requirements\n\nBusiness Needs Assessment: Collaborate with stakeholders to understand their data analysis requirements and translate them into actionable analytics projects\n\nIdentify key performance indicators (KPIs) and metrics to measure business performance and success\n\nData-Driven Decision Making: Assist in making data-driven decisions by providing insights and recommendations based on data analysis\n\nSupport strategic planning, operational improvements, and process optimizations based on data-driven insights\n\nData Modeling and Forecasting: Develop and maintain data models, predictive models, and forecasting models to anticipate trends, predict outcomes, and support future planning\n\nUtilize statistical software, programming languages, or machine learning techniques as necessary\n\nContinuous Improvement: Stay updated with the latest data analysis techniques, tools, and trends\n\nContinuously improve data analysis processes, methodologies, and automation to enhance efficiency and effectiveness\n\nand Communication: Work closely with cross-functional teams, such as business analysts, data engineers, and data scientists, to align data analysis efforts with organizational goals\n\nCommunicate findings, insights, and recommendations to non-technical stakeholders in a clear and understandable manner\n\nDocumentation and Knowledge Sharing: Document data analysis methodologies, processes, and findings for future reference\n\nShare knowledge and best practices with the team to promote a culture of learning and data-driven decision-making\n\nSkills and Qualifications:Strong analytical skills with the ability to manipulate and analyze complex datasets\n\nProficiency in data analysis tools such as SQL, Excel, Python, R, or similar tools\n\nExperience with data visualization tools such as Tableau, Power BI, or similar tools\n\nKnowledge of statistical analysis techniques and methodologies\n\nFamiliarity with data modeling, predictive modeling, and forecasting techniques\n\nUnderstanding of database concepts and query languages\n\nExcellent attention to detail and problem-solving abilities\n\nStrong communication and presentation skills\n\nAbility to work independently and collaborate in a team environment\n\nFamiliarity with data privacy, security, and regulatory compliance\n\nPrior experience in data analysis or a related field is preferred",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Design engineering', 'Data analysis', 'Technology consulting', 'Focus', 'Agile', 'Conceptualization', 'Management']",2025-06-14 05:59:48
Data Science Assoc Analyst,Pepsico,2 - 4 years,Not Disclosed,['Hyderabad'],"Overview \n\nData Science Team works in developing Machine Learning (ML) and Artificial Intelligence (AI) projects. Specific scope of this role is to develop ML solution in support of ML/AI projects using big analytics toolsets in a CI/CD environment. Analytics toolsets may include DS tools/Spark/Databricks, and other technologies offered by Microsoft Azure or open-source toolsets. This role will also help automate the end-to-end cycle with Azure Pipelines.\n\nYou will be part of a collaborative interdisciplinary team around data, where you will be responsible of our continuous delivery of statistical/ML models. You will work closely with process owners, product owners and final business users. This will provide you the correct visibility and understanding of criticality of your developments.\n\n \n\n\n",,,,"['python', 'pyspark', 'machine learning', 'ml', 'statistics', 'hive', 'continuous integration', 'github', 'supply chain', 'natural language processing', 'ci/cd', 'microsoft azure', 'apache pig', 'artificial intelligence', 'sql', 'docker', 'data bricks', 'git', 'spark', 'gcp', 'devops', 'oracle adf', 'jenkins', 'aws']",2025-06-14 05:59:50
Data Bricks,PwC India,7 - 12 years,Not Disclosed,['Bengaluru'],"Job Summary:\n\nWe are seeking a talented Data Engineer with strong expertise in Databricks, specifically in Unity Catalog, PySpark, and SQL, to join our data team. Youll play a key role in building secure, scalable data pipelines and implementing robust data governance strategies using Unity Catalog.\n\nKey Responsibilities:",,,,"['DataBricks', 'Data Bricks', 'Pyspark', 'Delta Lake', 'Databricks Engineer', 'Unity Catalog', 'SQL']",2025-06-14 05:59:53
Data Science Analyst,ManipalCigna Health Insurance,2 - 4 years,Not Disclosed,['Bengaluru'],"Key responsibilities\nDeliver quality analytics, from data preparation, data analysis, data exploration, data quality assessment, data manipulation, method selection, design & application, insights generation and visualisation\nDevelop and implement basic machine learning models and algorithms under the guidance of senior data scientists to extract insights and solve business problems\nProactive learning and acquisition of key analytical, technical and commercial skills and business knowledge to become a proficient Analyst working under the supervision of the senior/lead data science analysts.\nKPIs: Timeliness, accuracy, manager and client feedback (Internal and external as required)\nCollaborate with internal stakeholders and demonstrate the ability to transform client questions and problems into analytical solutions\nActive team member in providing the required support to help business understand and optimise use of analytical products and / or solutions\nBuild industry knowledge on the advancements in the field of analytics, data science and GenAI\nComply with the IM Cigna and CHSI Policies, procedures and processes, and continuously demonstrate Cigna Data and Analytics culture.\nKey activities\nWorking in a team to support end-to-end analytical projects\nLiaising with stakeholders to determine objectives / scope of upcoming projects\nData exploration, cleansing and manipulation\nDetermining appropriate type of analysis and undertaking analysis/modelling\nExtracting insights\nClear presentation of insights via spreadsheets, PowerPoint presentations, self-service analytical visualisation tools\nParticipate in client meetings\nOngoing stakeholder interaction (internal and external as required) on project progress\nContribute to the Feedback process (between stakeholders and the team) to ensure continuous improvement with team\nParticipate and contribute in learning forums such as Analytics Community and sharing knowledge with wider team\nExperience and education required\n2-4+ years experience in a technical analytics environment, carrying out data analytics and data science/AI projects and initiatives\nTertiary qualifications in engineering, mathematics, actuarial studies, statistics, physics, or a related discipline\nKnowledge of technical analytics discipline, including data preparation and foundational analytics concepts\nExperience with successfully managing both internal and external stakeholders, delivering against projects, tasks and activities in a dynamic deadline driven environment\nCommercial acumen to understand business needs and be able to suggest the commercial impacts of different analytics solutions or approaches\nCoding and modelling experience in SQL / R / Python and / or Cloud data platforms e.g. AWS\nExperience in visualization and data management tools is an added advantage\nExperience in GenAI/ LLMs is an added advantage\nExperience working with complex datasets\nAttention to detail and self driven continuous learning\nParticipation in external data hackathons and competitions will be an added advantage",Industry Type: Insurance,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data analysis', 'Data management', 'Analytical', 'Machine learning', 'Healthcare', 'Actuarial', 'Data quality', 'Continuous improvement', 'Analytics', 'SQL']",2025-06-14 05:59:55
Data Engineer,Amazon,1 - 6 years,Not Disclosed,['Hyderabad'],"Business Data Technologies (BDT) makes it easier for teams across Amazon to produce, store, catalog, secure, move, and analyze data at massive scale. Our managed solutions combine standard AWS tooling, open-source products, and custom services to free teams from worrying about the complexities of operating at Amazon scale. This lets BDT customers move beyond the engineering and operational burden associated with managing and scaling platforms, and instead focus on scaling the value they can glean from their data, both for their customers and their teams.\n\nWe own the one of the biggest (largest) data lakes for Amazon where 1000 s of Amazon teams can search, share, and store EB (Exabytes) of data in a secure and seamless way; using our solutions, teams around the world can schedule/process millions of workloads on a daily basis. We provide enterprise solutions that focus on compliance, security, integrity, and cost efficiency of operating and managing EBs of Amazon data.\n\n\nCORE RESPONSIBILITIES:\nBe hands-on with ETL to build data pipelines to support automated reporting\nInterface with other technology teams to extract, transform, and load data from a wide variety of data sources\nImplement data structures using best practices in data modeling, ETL/ELT processes, and SQL, Redshift.\nModel data and metadata for ad-hoc and pre-built reporting\nInterface with business customers, gathering requirements and delivering complete reporting solutions\nBuild robust and scalable data integration (ETL) pipelines using SQL, Python and Spark.\nBuild and deliver high quality data sets to support business analyst, data scientists, and customer reporting needs.\nContinually improve ongoing reporting and analysis processes, automating or simplifying self-service support for customers\nParticipate in strategic & tactical planning discussions\n\nA day in the life\nAs a Data Engineer, you will be working with cross-functional partners from Science, Product, SDEs, Operations and leadership to translate raw data into actionable insights for stakeholders, empowering them to make data-driven decisions. Some of the key activities include:\n\nCrafting the Data Flow: Design and build data pipelines, the backbone of our data ecosystem.\nEnsure the integrity of the data journey by implementing robust data quality checks and monitoring processes.\n\nArchitect for Insights: Translate complex business requirements into efficient data models that optimize data analysis and reporting. Automate data processing tasks to streamline workflows and improve efficiency.\n\nBecome a data detective! ensuring data availability and performance 1+ years of data engineering experience\nExperience with SQL\nExperience with data modeling, warehousing and building ETL pipelines\nExperience with one or more query language (e.g., SQL, PL/SQL, DDL, MDX, HiveQL, SparkSQL, Scala)\nExperience with one or more scripting language (e.g., Python, KornShell) Experience with big data technologies such as: Hadoop, Hive, Spark, EMR\nExperience with any ETL tool like, Informatica, ODI, SSIS, BODI, Datastage, etc.\nKnowledge of cloud services such as AWS or equivalent",,,,"['Data analysis', 'Data modeling', 'Datastage', 'PLSQL', 'Data structures', 'Informatica', 'SSIS', 'Open source', 'Monitoring', 'Python']",2025-06-14 05:59:58
Enterprise Data Operations Analyst,Pepsico,4 - 9 years,Not Disclosed,['Gurugram'],"Overview \n\nAs a member of the data engineering team, you will be the key technical expert developing and overseeing PepsiCo's data product build & operations and drive a strong vision for how data engineering can proactively create a positive impact on the business. You'll be an empowered member of a team of data engineers who build data pipelines into various source systems, rest data on the PepsiCo Data Lake, and enable exploration and access for analytics, visualization, machine learning, and product development efforts across the company. As a member of the data engineering team, you will help lead the development of very large and complex data applications into public cloud environments directly impacting the design, architecture, and implementation of PepsiCo's flagship data products around topics like revenue management, supply chain, manufacturing, and logistics. You will work closely with process owners, product owners and business users. You'll be working in a hybrid environment with in-house, on-premise data sources as well as cloud and remote systems and help grow DevOps and DataOps culture.\n\n \n\n\n",,,,"['azure policy', 'azure devops', 'arm templates', 'api', 'rest', 'artifactory', 'kubernetes', 'sonar', 'docker', 'ansible', 'azure cli', 'pcf', 'git', 'gcp', 'devops', 'powershell', 'paas', 'jenkins', 'python', 'microsoft azure', 'groovy', 'automation engineering', 'infrastructure', 'scrum', 'terraform', 'agile', 'aws']",2025-06-14 06:00:00
Azure Databricks Data Engineer / Developer,Infosys,2 - 5 years,Not Disclosed,['Bengaluru'],"Educational Requirements\nMCA,MSc,Bachelor of Engineering,BBA,BCom,BSc\nService Line\nData & Analytics Unit\nResponsibilities\nA day in the life of an Infoscion\nAs part of the Infosys delivery team, your primary role would be to interface with the client for quality assurance, issue resolution and ensuring high customer satisfaction.\nYou will understand requirements, create and review designs, validate the architecture and ensure high levels of service offerings to clients in the technology domain.\nYou will participate in project estimation, provide inputs for solution delivery, conduct technical risk planning, perform code reviews and unit test plan reviews.\nYou will lead and guide your teams towards developing optimized high quality code deliverables, continual knowledge management and adherence to the organizational guidelines and processes.\nYou would be a key contributor to building efficient programs/ systems and if you think you fit right in to help our clients navigate their next in their digital transformation journey, this is the place for you!If you think you fit right in to help our clients navigate their next in their digital transformation journey, this is the place for you!\nAdditional Responsibilities:\nKnowledge of more than one technology\nBasics of Architecture and Design fundamentals\nKnowledge of Testing tools\nKnowledge of agile methodologies\nUnderstanding of Project life cycle activities on development and maintenance projects\nUnderstanding of one or more Estimation methodologies, Knowledge of Quality processes\nBasics of business domain to understand the business requirements\nAnalytical abilities, Strong Technical Skills, Good communication skills\nGood understanding of the technology and domain\nAbility to demonstrate a sound understanding of software quality assurance principles, SOLID design principles and modelling methods\nAwareness of latest technologies and trends\nExcellent problem solving, analytical and debugging skills\nTechnical and Professional Requirements:\nPrimary skills:Technology->Cloud Platform->Azure Development & Solution Architecting\nPreferred Skills:\nTechnology->Cloud Platform->Azure Development & Solution Architecting",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Databricks', 'Azure Development', 'Testing tools', 'agile methodologies', 'debugging', 'Cloud Platform', 'software quality assurance']",2025-06-14 06:00:03
Specialist Data/AI Engineering,ATT Communication Services,3 - 4 years,Not Disclosed,['Bengaluru'],"Overall Purpose\nThis position will interact on a consistent basis with other developers, architects, data product owners and source systems. This position requires multifaceted candidates who have experience in data analysis, visualization, good hands-on experience with BI Tools and relational databases, experience in data warehouse architecture (traditional and cloud).\nKey Roles and Responsibilities\nDevelop, understand, and enhance code in traditional data warehouse environments, data lake, and cloud environments like Snowflake, Azure, Databricks\nBuild new end-to-end business intelligence solutions. This includes data extraction, ETL processes applied on data to derive useful business insights, and best representing this data through dashboards.\nWrite complex SQL queries used to transform data using Python/Unix shell scripting\nUnderstand business requirements and create visual reports and dashboards using Power BI or Tableau.\nUpskill to different technologies, understand existing products and programs in place\nWork with other development and operations teams.\nFlexible with shifts and occasional weekend support.\nKey Competencies\nFull life-cycle experience on enterprise software development projects.\nExperience in relational databases/ data marts/data warehouses and complex SQL programming.\nExtensive experience in ETL, shell or python scripting, data modelling, analysis, and preparation\nExperience in Unix/Linux system, files systems, shell scripting.\nGood to have knowledge on any cloud platforms like AWS, Azure, Snowflake, etc.\nGood to have experience in BI Reporting tools Power BI or Tableau\nGood problem-solving and analytical skills used to resolve technical problems.\nMust possess a good understanding of business requirements and IT strategies.\nAbility to work independently but must be a team player. Should be able to drive business decisions and take ownership of their work.\nExperience in presentation design, development, delivery, and good communication skills to present analytical results and recommendations for action-oriented data driven decisions and associated operational and financial impacts.\nRequired/Desired Skills\nCloud Platforms - Azure, Snowflake, Databricks, Delta lake (Required 3-4 years)\nRDBMS and Data Warehousing (Required 7-10 Years)\nSQL Programming and ETL (Required 7-10 Years)\nUnix/Linux shell scripting (Required 2-3 years)\nPower BI / Tableau (Desired 3 years)\nPython or any other programming language (Desired 4 years)\nEducation & Qualifications\nUniversity Degree in Computer Science and/or Analytics\nMinimum Experience required: 7-10 years in relational database design & development, ETL development\nJob ID R-68650 Date posted 06/11/2025",Industry Type: Telecom / ISP,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Unix', 'Linux', 'RDBMS', 'Database design', 'Shell scripting', 'Business intelligence', 'Unix shell scripting', 'Analytics', 'SQL', 'Python']",2025-06-14 06:00:05
Big Data Engineer,Amazon,3 - 8 years,Not Disclosed,['Chennai'],"Amazon Retail Financial Intelligence Systems is seeking a seasoned and talented Senior Data Engineer to join the Fortune Platform team. Fortune is a fast growing team with a mandate to build tools to automate profit-and-loss forecasting and planning for the Physical Consumer business. We are building the next generation Business Intelligence solutions using big data technologies such as Apache Spark, Hive/Hadoop, and distributed query engines. As a Data Engineer in Amazon, you will be working in a large, extremely complex and dynamic data environment. You should be passionate about working with big data and are able to learn new technologies rapidly and evaluate them critically. You should have excellent communication skills and be able to work with business owners to translate business requirements into system solutions. You are a self-starter, comfortable with ambiguity, and working in a fast-paced and ever-changing environment. Ideally, you are also experienced with at least one of the programming languages such as Java, C++, Spark/Scala, Python, etc.\n\nMajor Responsibilities:\nWork with a team of product and program managers, engineering leaders, and business leaders to build data architectures and platforms to support business\nDesign, develop, and operate high-scalable, high-performance, low-cost, and accurate data pipelines in distributed data processing platforms\nRecognize and adopt best practices in data processing, reporting, and analysis: data integrity, test design, analysis, validation, and documentation\nKeep up to date with big data technologies, evaluate and make decisions around the use of new or existing software products to design the data architecture\nDesign, build and own all the components of a high-volume data warehouse end to end.\nProvide end-to-end data engineering support for project lifecycle execution (design, execution and risk assessment)\nContinually improve ongoing reporting and analysis processes, automating or simplifying self-service support for customers\nInterface with other technology teams to extract, transform, and load (ETL) data from a wide variety of data sources\nOwn the functional and nonfunctional scaling of software systems in your ownership area.\nImplement big data solutions for distributed computing.\n\n\nAs a DE on our team, you will be responsible for leading the data modelling, database design, and launch of some of the core data pipelines. You will have significant influence on our overall strategy by helping define the data model, drive the database design, and spearhead the best practices to delivery high quality products.\n\nAbout the team\nProfit intelligence systems measures, predicts true profit(/loss) for each item as a result of a specific shipment to an Amazon customer. Profit Intelligence is all about providing intelligent ways for Amazon to understand profitability across retail business. What are the hidden factors driving the growth or profitability across millions of shipments each day\n\nWe compute the profitability of each and every shipment that gets shipped out of Amazon. Guess what, we predict the profitability of future possible shipments too. We are a team of agile, can-do engineers, who believe that not only are moon shots possible but that they can be done before lunch. All it takes is finding new ideas that challenge our preconceived notions of how things should be done. Process and procedure matter less than ideas and the practical work of getting stuff done. This is a place for exploring the new and taking risks.\n\nWe push the envelope in using cloud services in AWS as well as the latest in distributed systems, forecasting algorithms, and data mining. 3+ years of data engineering experience\nExperience with data modeling, warehousing and building ETL pipelines\nExperience with SQL Experience with AWS technologies like Redshift, S3, AWS Glue, EMR, Kinesis, FireHose, Lambda, and IAM roles and permissions\nExperience with non-relational databases / data stores (object storage, document or key-value stores, graph databases, column-family databases)",,,,"['C++', 'Database design', 'Risk assessment', 'Agile', 'Business intelligence', 'Data mining', 'Distribution system', 'SQL', 'Python', 'Data architecture']",2025-06-14 06:00:07
Azure Data Engineer,Zensar,5 - 10 years,15-25 Lacs P.A.,"['Hyderabad', 'Pune', 'Bengaluru']","We are looking for a skilled Data Engineer with expertise in Azure Data Factory, ADLS, SQL, PySpark, Python and Azure Databricks to design, build and optimize data pipelines. Also ensuring efficient data ingestion, transformation and storage solutions.\n\nKey Responsibilities:\n\nDesing and Develop data pipelines using Azure Data Factory and SSIS\nManage Cloud Data Storage and Processing (AWS S3, ADLS etc)\nWrite complex SQL queries, optimize performance\nProcess large datasets using PySpark\nDevelop scripts for data processing, automation and API integration using Python\nDevelop Databrikcs notebook, manage workflows and implement delta lake for data transactions\nPipeline orchestration and Monitoring\nKnowledge on CI/CD using Azure DevOps/Github",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Data Factory', 'Azure Data Lake', 'Azure Databricks', 'Pyspark']",2025-06-14 06:00:10
Data Engineer - Python Programming,Leading Client,5 - 7 years,Not Disclosed,['Bengaluru'],"We are looking for a skilled Data Engineer with strong hands-on experience in Clickhouse, Kubernetes, SQL, Python, and FastAPI, along with a good understanding of PostgreSQL.\nThe ideal candidate will be responsible for building and maintaining efficient data pipelines, optimizing query performance, and developing APIs to support scalable data services.\n\n- Design, build, and maintain scalable and efficient data pipelines and ETL processes.\n\n- Develop and optimize Clickhouse databases for high-performance analytics.\n\n- Create RESTful APIs using FastAPI to expose data services.\n\n- Work with Kubernetes for container orchestration and deployment of data services.\n\n- Write complex SQL queries to extract, transform, and analyze data from PostgreSQL and Clickhouse.\n\n- Collaborate with data scientists, analysts, and backend teams to support data needs and ensure data quality.\n\n- Monitor, troubleshoot, and improve performance of data infrastructure.\n\n- Strong experience in Clickhouse - data modeling, query optimization, performance tuning.\n\n- Expertise in SQL - including complex joins, window functions, and optimization.\n\n- Proficient in Python, especially for data processing (Pandas, NumPy) and scripting.\n\n- Experience with FastAPI for creating lightweight APIs and microservices.\n\n- Hands-on experience with PostgreSQL - schema design, indexing, and performance.\n\n- Solid knowledge of Kubernetes managing containers, deployments, and scaling.\n\n- Understanding of software engineering best practices (CI/CD, version control, testing).\n\n- Experience with cloud platforms like AWS, GCP, or Azure.\n\n- Knowledge of data warehousing and distributed data systems.\n\n- Familiarity with Docker, Helm, and monitoring tools like Prometheus/Grafana.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'PostgreSQL', 'Data Modeling', 'Data Warehousing', 'Data Analytics', 'ETL', 'Python', 'SQL']",2025-06-14 06:00:12
Gcp Data Engineer,Tata Consultancy Services,7 - 10 years,Not Disclosed,"['Pune', 'Chennai', 'Bengaluru']","As a GCP data engineer the colleague should be able to designs scalable data architectures on Google Cloud Platform, using services like Big Query and Dataflow. They write and maintain code (Python, Java), ensuring efficient data models and seamless ETL processes. Quality checks and governance are implemented to maintain accurate and reliable data.\nSecurity is a priority, enforcing measures for storage, transmission, and processing, while ensuring compliance with data protection standards. Collaboration with cross-functional teams is key for understanding diverse data requirements. Comprehensive documentation is maintained for data processes, pipelines, and architectures.",,,,"['Big query', 'GCP', 'Bigquery', 'GCP Data Engineer - GCP', 'Python', 'Cloud Composer', 'Groovy', 'Data flow', 'SQL', 'air flow']",2025-06-14 06:00:14
Senior Data Science Consultant - Optimization Projects,Leading Client,8 - 10 years,Not Disclosed,['Bengaluru'],"Role Summary :\n\nWe are seeking a highly skilled Senior Data Science Consultant with 8+ years of experience to lead an internal optimization initiative.\n\nThe ideal candidate should have a strong background in data science, operations research, and mathematical optimization, with a proven track record of applying these skills to solve complex business problems.\n\nThis role requires a blend of technical depth, business acumen, and collaborative communication.\n\nA background in internal efficiency/operations improvement or cost/resource optimization projects is highly desirable.\n\nKey Responsibilities :\n\n- Lead and contribute to internal optimization-focused data science projects from design to deployment.\n\n- Develop and implement mathematical models to optimize resource allocation, process performance, and decision-making.\n\n- Use techniques such as linear programming, mixed-integer programming, heuristic and metaheuristic algorithms.\n\n- Collaborate with business stakeholders to gather requirements and translate them into data science use cases.\n\n- Build robust data pipelines and use statistical and machine learning methods to drive insights.\n\n- Communicate complex technical findings in a clear, concise manner to both technical and non-technical audiences.\n\n- Mentor junior team members and contribute to knowledge sharing and best practices within the team.\n\nRequired Skills And Qualifications :\n\n- Masters or PhD in Data Science, Computer Science, Operations Research, Applied Mathematics, or related fields.\n\n- Minimum 8 years of relevant experience in data science, with a strong focus on optimization.\n\n- Expertise in Python (NumPy, Pandas, SciPy, Scikit-learn), SQL, and optimization libraries such as PuLP, Pyomo, Gurobi, or CPLEX.\n\n- Experience with end-to-end lifecycle of internal optimization projects.\n\n- Strong analytical and problem-solving skills.\n\n- Excellent communication and stakeholder management abilities.\n\nPreferred Qualifications :\n\n- Experience working on internal company projects focused on logistics, resource planning, workforce optimization, or cost reduction.\n\n- Exposure to tools/platforms like Databricks, Azure ML, or AWS SageMaker.\n\n- Familiarity with dashboards and visualization tools like Power BI or Tableau.\n\n- Prior experience in consulting or internal centers of excellence (CoE) is a plus.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Data Scientist', 'Operations Research', 'Pandas', 'Numpy', 'Scikit-Learn', 'Python', 'SQL']",2025-06-14 06:00:16
Data Analysis - Japanese Specialist Data Analysis,Zensar,3 - 7 years,Not Disclosed,"['Kolkata', 'Mumbai', 'New Delhi', 'Hyderabad', 'Pune', 'Chennai', 'Bengaluru']","Zensar Technologies is looking for Data Analysis - Japanese Specialist Data Analysis - Japanese Specialist to join our dynamic team and embark on a rewarding career journey A Data Analyst is responsible for collecting, analyzing, and interpreting data to identify trends, patterns, and insights that can drive informed business decisions\n\nThey play a crucial role in helping organizations understand their data, derive actionable insights, and optimize processes\n\nHere is a general job description for a Data Analyst:Data Collection and Preparation: Collect and gather relevant data from various sources, such as databases, spreadsheets, and external systems\n\nClean, validate, and transform the data to ensure accuracy and consistency\n\nData Analysis and Interpretation: Apply statistical techniques, data mining methods, and visualization tools to analyze large datasets\n\nIdentify trends, patterns, and correlations within the data and generate insights to support decision-making\n\nReporting and Visualization: Create clear and concise reports, dashboards, and visual representations of data using data visualization tools, such as Tableau, Power BI, or Excel\n\nPresent findings to stakeholders in a compelling and understandable manner\n\nData Quality and Integrity: Ensure data integrity and accuracy by conducting data validation, resolving discrepancies, and monitoring data quality\n\nImplement measures to maintain data privacy, security, and compliance with regulatory requirements\n\nBusiness Needs Assessment: Collaborate with stakeholders to understand their data analysis requirements and translate them into actionable analytics projects\n\nIdentify key performance indicators (KPIs) and metrics to measure business performance and success\n\nData-Driven Decision Making: Assist in making data-driven decisions by providing insights and recommendations based on data analysis\n\nSupport strategic planning, operational improvements, and process optimizations based on data-driven insights\n\nData Modeling and Forecasting: Develop and maintain data models, predictive models, and forecasting models to anticipate trends, predict outcomes, and support future planning\n\nUtilize statistical software, programming languages, or machine learning techniques as necessary\n\nContinuous Improvement: Stay updated with the latest data analysis techniques, tools, and trends\n\nContinuously improve data analysis processes, methodologies, and automation to enhance efficiency and effectiveness\n\nand Communication: Work closely with cross-functional teams, such as business analysts, data engineers, and data scientists, to align data analysis efforts with organizational goals\n\nCommunicate findings, insights, and recommendations to non-technical stakeholders in a clear and understandable manner\n\nDocumentation and Knowledge Sharing: Document data analysis methodologies, processes, and findings for future reference\n\nShare knowledge and best practices with the team to promote a culture of learning and data-driven decision-making\n\nSkills and Qualifications:Strong analytical skills with the ability to manipulate and analyze complex datasets\n\nProficiency in data analysis tools such as SQL, Excel, Python, R, or similar tools\n\nExperience with data visualization tools such as Tableau, Power BI, or similar tools\n\nKnowledge of statistical analysis techniques and methodologies\n\nFamiliarity with data modeling, predictive modeling, and forecasting techniques\n\nUnderstanding of database concepts and query languages\n\nExcellent attention to detail and problem-solving abilities\n\nStrong communication and presentation skills\n\nAbility to work independently and collaborate in a team environment\n\nFamiliarity with data privacy, security, and regulatory compliance\n\nPrior experience in data analysis or a related field is preferred",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Design engineering', 'Data analysis', 'Technology consulting', 'Focus', 'Agile', 'Conceptualization', 'Management', 'Japanese']",2025-06-14 06:00:18
Data Engineer SRE - Shell Data Engineer SRE - Shell,Zensar,3 - 6 years,Not Disclosed,"['Hubli', 'Mangaluru', 'Mysuru', 'Bengaluru', 'Belgaum']","Role - Data Engineer / SRE\nMust Have - ADF, Databricks, SQL, Azure Functions\nGood to Have - Python, Azure DevOps, GitHub\nMust have a willingness to work in support and enhancement projects, on incidents, change management etc.,.\nRelevant 6+ yrs experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field.\nExperience in ITSM, worked on incidents, problem tickets, change management.\nExperience with Azure: ADLS, ADF, Databricks, Stream Analytics, SQL DW, COSMOS DB, Analysis Services, Azure Functions.\nExperience with relational SQL and NoSQL databases, including Postgres and Cassandra.\nExperience with object-oriented/object function scripting languages: Python, SQL, Scala, Spark-SQL etc.\nAdvanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.\nExperience building and optimizing big data data pipelines, architectures, and data sets.\nExperience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.\nStrong analytic skills related to working with unstructured datasets.\nBuild processes supporting data transformation, data structures, metadata, dependency, and workload management.\n\nRole - Data Engineer / SRE\nMust Have - ADF, Databricks, SQL, Azure Functions\nGood to Have - Python, Azure DevOps, GitHub\nMust have a willingness to work in support and enhancement projects, on incidents, change management etc.,.\nRelevant 6+ yrs experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field.\nExperience in ITSM, worked on incidents, problem tickets, change management.\nExperience with Azure: ADLS, ADF, Databricks, Stream Analytics, SQL DW, COSMOS DB, Analysis Services, Azure Functions.\nExperience with relational SQL and NoSQL databases, including Postgres and Cassandra.\nExperience with object-oriented/object function scripting languages: Python, SQL, Scala, Spark-SQL etc.\nAdvanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.\nExperience building and optimizing big data data pipelines, architectures, and data sets.\nExperience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.\nStrong analytic skills related to working with unstructured datasets.\nBuild processes supporting data transformation, data structures, metadata, dependency, and workload management.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Change management', 'metadata', 'NoSQL', 'cassandra', 'Data structures', 'Analytics', 'Analysis services', 'SQL', 'Python']",2025-06-14 06:00:21
SSAS Data Engineer - MDX / SQL,Leading Client,5 - 7 years,Not Disclosed,['Bengaluru'],"We are looking for an experienced SSAS Data Engineer with strong expertise in SSAS (Tabular and/or Multidimensional Models), SQL, MDX/DAX, and data modeling.\n\nThe ideal candidate will have a solid background in designing and developing BI solutions, working with large datasets, and building scalable SSAS cubes for reporting and analytics.\n\nExperience with ETL processes and reporting tools like Power BI is a strong plus.\n\nKey Responsibilities\n\n- Design, develop, and maintain SSAS models (Tabular and/or Multidimensional).\n\n- Build and optimize MDX or DAX queries for advanced reporting needs.\n\n- Create and manage data models (Star/Snowflake schemas) supporting business KPIs.\n\n- Develop and maintain ETL pipelines for efficient data ingestion (preferably using SSIS or similar tools).\n\n- Implement KPIs, aggregations, partitioning, and performance tuning in SSAS cubes.\n\n- Collaborate with data analysts, business stakeholders, and Power BI teams to deliver accurate and insightful reporting solutions.\n\n- Maintain data quality and consistency across data sources and reporting layers.\n\n- Implement RLS/OLS and manage report security and governance in SSAS and Power BI.\n\nPrimary :\n\nRequired Skills :\n\n- SSAS Tabular & Multidimensional.\n\n- SQL Server (Advanced SQL, Views, Joins, Indexes).\n\n- DAX & MDX.\n\n- Data Modeling & OLAP concepts.\n\nSecondary :\n\n- ETL Tools (SSIS or equivalent).\n\n- Power BI or similar BI/reporting tools.\n\n- Performance tuning & troubleshooting in SSAS and SQL.\n\n- Version control (TFS/Git), deployment best practices.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SSAS', 'MDX', 'Data Engineering', 'Power BI', 'Dimensional Modeling', 'Data Modeling', 'SQL Server', 'ETL', 'SSIS']",2025-06-14 06:00:23
Data Engineer,Xoriant,7 - 12 years,Not Disclosed,"['Pune', 'Bengaluru', 'Mumbai (All Areas)']","We are eagerly seeking candidates with 5 to 13 years experience for a Data Engineer / Lead, to join our dynamic team. The ideal candidate will play a pivotal role within the team to who is a skilled professional with exposure to Python, Spark, Hive, AWS. You will collaborate with internal teams to design, develop, deploy, and maintain software applications at scale\nRole: Data Engineer / Lead\nLocation: PAN India\nExperience: 5 to 13 years\nJob type: Full time\nWork type: Hybrid\nData Engineer with minimum 5 years of relevant professional experience\n• Should have expertise in Python Scripting and Big data technologies like Spark, Hive, Presto etc.\n• Experience with AWS services – IAM, EC2, S3, EMR, Lambda Functions, Step Functions, CloudWatch, Redshift, Athena, GLUE etc.\n• Hands-on experience with Databricks.\n• Proficient writing Spark jobs in Pyspark and Scala.\n• Experience writing queries with both SQL and NoSQL DB ((Hive, HBase, MongoDB, Elasticsearch, PostgreSQL etc.)\n• Should have good understanding around python data structures including data frames, datasets, RDD’s etc.\n• Experience in ML – integration of ML models\n• Experience with Data profiling, data migration.\n• Developing Hive UDF and Hive jobs\n• Proven hands-on Software Development experience\n• Experience with test-driven development\n• Preferred experience in the insurance domain\n• Must have good understanding of Data warehousing concepts.\n• Experience using CI/CD tools like GitHub Actions, Jenkins, Azure Devops etc.\n• Experienced working in Agile projects – Sprint planning, grooming and providing estimations.\n• Experience using JIRA, Confluence, VS Code or similar IDE’s, Jupyter notebooks etc.\n• Good communication and collaborative skills with internal and external teams\n• Flexibility and ability to work in onshore/offshore model involving multiple agile teams\n• Mentor and guide junior developers, review code, familiar with estimation techniques using story points\n• Strong analytical and problem-solving skills\nQualification you must require:\nBachelors or master’s with Computer Science or related field",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Snowflake', 'AWS', 'Data Bricks', 'Python']",2025-06-14 06:00:26
DE&A - Core - Cloud Data Engineering - Snowflake Data Engineering,Zensar,5 - 9 years,Not Disclosed,"['Kolkata', 'Mumbai', 'New Delhi', 'Hyderabad', 'Pune', 'Chennai', 'Bengaluru']","Designing and implementing data processing systems using Microsoft Fabric, Azure Data Analytics, Databricks and other distributed frameworks\n(ex\n\nHadoop, Spark, Snowflake, Airflow)\n\nWriting efficient and scalable code to process, transform, and clean large volumes of structured and unstructured data\nDesigning data pipelines: Snowflake Data Cloud uses data pipelines to ingest data into its system from sources like databases, cloud storage, or streaming platforms\n\nA Snowflake Data Engineer designs, builds, and fine-tunes these pipelines to make sure that all data is loaded into Snowflake correctly\n\nDesigning and implementing data processing systems using Microsoft Fabric, Azure Data Analytics, Databricks and other distributed frameworks\n(ex\n\nHadoop, Spark, Snowflake, Airflow)\n\nWriting efficient and scalable code to process, transform, and clean large volumes of structured and unstructured data\nDesigning data pipelines: Snowflake Data Cloud uses data pipelines to ingest data into its system from sources like databases, cloud storage, or streaming platforms\n\nA Snowflake Data Engineer designs, builds, and fine-tunes these pipelines to make sure that all data is loaded into Snowflake correctly",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['spark', 'Hadoop', 'cloud storage', 'Data processing', 'Data analytics', 'microsoft']",2025-06-14 06:00:28
Analyst,Merkle B2b,0 - 2 years,Not Disclosed,['Bengaluru'],"Key responsibilities:\nIntegrates disparate datasets, conducts data preparation for analyses\nApplies data science methods to provide insights and recommendations to clients\nDelivers analytic outcomes based on project timelines and key milestones\nMaintains knowledge of new trends in the data science industry\nDevelops and manages code used for analytics purposes\nCommunicates findings and insights\nKnowledge on SQL, Tableau\nLocation:\nBangalore\nBrand:\nIprospect\nTime Type:\nFull time\nContract Type:\nPermanent",Industry Type: Management Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['tableau', 'Usage', 'data science', 'Senior Analyst', 'Analytics', 'SQL']",2025-06-14 06:00:30
Data Analyst,Ramboll,6 - 10 years,Not Disclosed,['Gurugram'],"Transport Commercial / Data Analyst \nRole requires multitasking capabilities for effectively handling multiple opportunities at the same time, ensuring each preparation receives the necessary attention and meets the deadlines.\nWe are seeking motivated individuals to join the Transport Commercial team and help us to drive growth within the market. This position will involve a variety of tasks, working with the team to support: measuring and reporting on our performance, data analysis and supporting our market knowledge through CRM. \nKey responsibilities include:\nGathering and analyzing data from various sources such as CRM and additional databases to generate comprehensive reports and dashboards for the Transport Commercial team and the Transport Leadership Team\n Working with our team and Opportunity Owners to support and develop the use of tools and processes to support effective tendering, including the use of AI.\nConducting market intelligence tasks to support business decisions by performing online market research and developing tools to optimize this research turning data insights into effective business intelligence.\nImplementing automation processes to improve data quality and visualization through charts, views, and interactive dashboards, to support strategic planning and decision-making\n Supporting live tenders as necessary, in particular with the preparation of governance and progress report documentation.\nEstablish and nurture relationships with internal stakeholders.\n  Qualification\nThis role is ideal for an experienced Graduate passionate about managing business operations and driving growth within the Transport Commercial sector. If you have a proactive mindset, strong analytical skills, and a keen interest in this field, we encourage you to apply.\nBachelor's degree in Business Administration, Economics, Engineering, Data Science, or a related field.\nDemonstrated skills in Business development softwares such as MS Office, PowerBI and CRM systems. Experience on scripting tools (e.g., SQL, Python) would be advantageous.\nStrong writing and presentation skills.\nCapabilities to multitask, managing multiple opportunities simultaneously while meeting deadlines.\nExcellent networking skills and a global mindset to establish and nurture relationships.\nProactive approach and excellent collaboration skills.\nExperience in data analysis, market intelligence, and business decision support.\nFamiliarity with automation processes and data visualization techniques.\nPrevious experience in a similar role is preferred.\nInterest in business management and work-winning strategies.\nDemonstrate a global mindset and strong networking skills\nAdditional Information\nWhat we can offer you\nInvestment in your development\nLeaders you can count on, guided by our Leadership Principles\nBe valued for the unique person you are\nNever be short of inspiration from colleagues, clients, and projects\nThe long-term thinking of a foundation-owned company\nWork at the heart of sustainable change\nRamboll is a global architecture, engineering, and consultancy company. We believe that the purpose of sustainable change is to create a thriving world for both nature and people. So, that’s where we start – and how we work. At Ramboll, our core strength is our people, and our history is rooted in a clear vision of how a responsible company should act. Being open and curious is a cornerstone of our culture. We embrace an inclusive mindset that looks for fresh, diverse, and innovative perspectives. We respect, embrace, and invite diversity in all forms to actively cultivate an environment where everyone can flourish and realise their full potential.\nReady to join us?\nPlease submit your application. Be sure to include all relevant documents including your CV, cover letter, etc.\nThank you for taking the time to apply! We look forward to receiving your application.\nAbout Ramboll\nFounded in Denmark, Ramboll is a foundation-owned people company. We have more than 18,000 experts working across our global operations in 35 countries. Our experts are leaders in their fields, developing and delivering innovative solutions in diverse markets including Buildings, Transport, Planning & Urban Design, Water, Environment & Health, Energy, and Management Consulting. We invite you to contribute to a more sustainable future working in an open, collaborative, and empowering company. Combining local experience with global knowledge, we together shape the societies of tomorrow.\nEquality, diversity, and inclusion is at the heart of what we do\nWe believe in the strength of diversity and know that unique experiences and perspectives are vital for creating truly sustainable societies. Therefore, we are committed to providing an inclusive and supportive work environment where everyone can flourish and reach their potential. We welcome applications from candidates of all backgrounds and encourage you to contact our recruitment team to discuss any accommodations you need during the application process.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'data analysis', 'analytical', 'softwares', 'consulting', 'bi', 'presentation skills', 'power bi', 'business development', 'networking', 'tools', 'dashboards', 'business intelligence', 'research', 'sql', 'scripting', 'data quality', 'operations', 'data science', 'collaboration', 'writing', 'data visualization', 'ms office', 'crm']",2025-06-14 06:00:32
Data Analyst,Vipany Management Consulting,3 - 5 years,Not Disclosed,['Pune'],"Summary\nIn this role, you will be a part of the centralised global office based in India and work closely with each of our markets globally to understand the clients communication objectives, access multiple data sources and visualise it using Tableau / Datorama, support on of ETL process using MSSQL / Alteryx Flow and has sound knowledge of Excel VBA.\nKey responsibilities\nOwn the design, development, and maintenance of ongoing metrics, reports, analyses, dashboards on the key drivers of our business\nPartner with operations/business teams to consult, develop and implement KPIs, automated reporting/process solutions and data infrastructure improvements to meet business needs\nEnable effective decision making by retrieving and aggregating data from multiple sources and compiling it into a digestible and actionable format\nManage on-time delivery of regular client reports including:\no Building reports from data warehouse\no Review of completed reports for anomalies & discrepancies\no Troubleshooting data issues/discrepancies\no Ensure formatting & delivery parameters are met\nUpdating Tableau dashboards and Excel Dashboard as required for daily/weekly client reporting.\nInvestigate and understand the opportunities of new data sources in the context of integration into Tableau.\nUpdating Tableau/Excel/ or any similar dashboards for daily/weekly client reporting.\nSupport data cleansings & manipulation process including but not limited to:\no Taxonomy classification\no Conversion re-naming, grouping\no Removal of test/ghost impressions\nDesired Skills:-\n• Minimum 3 years of experience in Analytics\n• Strong verbal/written communication and data presentation skills, including an ability to effectively communicate with both business and technical teams.\n• Hands on experience in creating complex Excel reports, SQL Queries joining multiple datasets\n• Data Visualization tools such as Quick Sight / Tableau / Power BI / Datorama\n• An ability and interest in working in a fast-paced, ambiguous and rapidly-changing environment\n• Experience in developing requirements and formulating business metrics for reporting, familiarity with data visualization tools, e.g. Tableau, Power BI\nInterested candidates can reach out through:-\nKavya.p-8341137995/kavya.p@vipanyglobal.com\nHiring for only females",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Visualization', 'SQL', 'Power Bi', 'Data Analysis', 'Tableau', 'Data Reporting']",2025-06-14 06:00:35
Azure Senior Data Engineers,IT Services & Consulting,5 - 9 years,14-24 Lacs P.A.,['Bengaluru'],"Job Title: Senior Data Engineer Azure\nLocation: Bengaluru\nExperience: 6+ years (3+ years on Azure data services preferred)\nDepartment: Data Engineering / IT\nRoles and Responsibilities:\nThe Data Engineer will work on data engineering projects for various business units, focusing on delivery of complex data management solutions by leveraging industry best practices. They work with the project team to build the most efficient data pipelines and data management solutions that make data easily available for consuming applications and analytical solutions. A Data engineer is expected to possess strong technical skills.\nKey Characteristics\nTechnology champion who constantly pursues skill enhancement and has inherent curiosity to understand work from multiple dimensions.\nInterest and passion in Big Data technologies and appreciates the value that can be brought in with an effective data management solution.\nHas worked on real data challenges and handled high volume, velocity, and variety of data.\nExcellent analytical & problem-solving skills, willingness to take ownership and resolve technical challenges.\nContributes to community building initiatives like CoE, CoP.\nMandatory skills:\nAzure - Master\nELT - Skill\nData Modeling - Skill\nData Integration & Ingestion - Skill\nData Manipulation and Processing - Skill\nGITHUB, Action, Azure DevOps - Skill\nData factory, Databricks, SQL DB, Synapse, Stream Analytics, Glue, Airflow, Kinesis, Redshift, SonarQube, PyTest - Skill\nOptional skills:\nExperience in project management, running a scrum team.\nExperience working with BPC, Planning.\nExposure to working with external technical ecosystem.\nMKDocs documentation",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Live streaming data', 'Pyspark', 'SCALA', 'Data Bricks', 'SQL', 'batch processing data']",2025-06-14 06:00:37
Senior / Lead Data Engineer,Quincy Compressor,5 - 8 years,Not Disclosed,['Pune'],"Senior / Lead Data Engineer Job Details | our company Search by Keyword Search by Location Select how often (in days) to receive an alert: Select how often (in days) to receive an alert: Senior / Lead Data Engineer Research and Development Atlas Copco (India) Private Ltd. Date of posting: Jun 11, 2025 Passionate people create exceptional things\nDid you know that the solutions we develop are a key part of most industriesElectronics, medical research, renewable energy, food production, infrastructure and many more.\nWe re everywhere! Working with us means working with the latest technologies and groundbreaking, sustainable innovations. With our inclusive and caring environment, you get the support and inspiration you need to grow.\nHere, your ideas are embraced, and you never stop learning. Interested in being part of our team\nYour Role\nAtlas Copco is a global, industrial company based in Stockholm, Sweden, with almost 40, 000 employees and customers in more than 180 countries. Atlas Copco has been driven by an innovative spirit ever since the start in 1873.\nDid you know that Atlas Copco is the largest Air Compressor manufacturer in the world with its compressors being used in industries ranging from hospitals to offshore oil drillsAre you looking to be part of an organization which believes in investing in its people and contributing towards their organic growthWould you be interested to be part of the organization which is tasked with establishing, consolidating and unifying data and analytics services across the business areas of Atlas CopcoAre you an experienced data engineer who would like to use their experience to build and maintain enterprise-production-grade data pipelines, considering business requirements and platform standardsThis might be the perfect opportunity for you to make a difference.\nWho are we\nThe Data Analytics is the competence within Atlas Copcos Global Engineering Center India Air Power (GECIA) responsible for establishing, consolidating, and unifying data and analytics services across the business areas of Atlas Copco. We strive to become the business partner for trusted, transparent, and robust enterprise data to enable data driven decision making.\nYou would be a part of Data Analytics Team of Global Engineering Center (GECIA) of Atlas Copco India Ltd and supporting CTIS from our Hinjewadi location in Pune, India. As a Data Engineer within our Data Anlytics Competence team, based out of Pune, you will lead the management and organisation of data for our Sales and Marketing leading indicators team which is focused on creating data and analytics solution related to the leading indicators space.\nWhat are we looking for\nWe are looking for minimum 5-8 years of experienced Data Engineers to build and maintain enterprise, production-grade data pipelines, considering business requirements and data platform standards. You will be expected to liaise with internal customers/stakeholders, understand their requirements, and propose solutions that align with their expectations. While you will be part of the local team at Atlas Copco, Pune office, you will also have the opportunity to be part of global project deliveries with potential on-site work if required.\nAs a key member of the highly motivated Data Analytics Competence Team, your work will enable team members to deliver ""first time right"" application delivery.\nPrincipal Duties and Responsibilities\nYou will be responsible for the management and organization of data for our applications\nYou will handle ETL processes, and manage data pipelines for a variety of applications, these will include shop floor applications, enterprise applications and home-grown applications developed within the team.\nYou will manage the data cloud infrastructure in Azure Data Lakehouse. You will also manage data in SAP using both SAP HANA and API connections\nYou will interact with the customers/stakeholders to understand their requirements and propose solutions.\nYou will work closely with Application Owners and be responsible for end-to-end processes/deployment.\nYou will document technical functional application aspects and release notes to facilitate the aftercare of the application\nYou ll be expected to work across the data spectrum and not be afraid to support the team in data analytics and data science activities if your skillset allows whilst prioritising your key projects.\nImportant areas of Expertise\nSkills Experience:\nWe are looking for minimum 5-8 years of experienced Data EngineersThe ideal candidate will have a good blend of business and technical skills. Specific requirements for this position include: General skills: You have a Maters degree in computer science of equivalent by experience The ideal candidate has strong and demonstrable hands-on experience with data engineering on cloud-based data platforms\nRequired Skills\nStrong working knowledge of ETL processes, and data pipeline management SAP SAP HANA\nStrong capability with Python\nGood understanding of low code workflow automation tools (Power Automate) Strong SQL skills and ability to both design, build and extract data from SQL databases Excellent English communication skills (written, oral), with good listening capabilities. Good technical analytical, debugging, and problem-solving skills\nHas a reasonable balance between getting the job done vs technical debt\nEnjoys enabling seamless deployments in a fast-moving environment.\nEffective team player working in a team; willingness to put the needs of the team over their own\nExperience with Microsoft Power Platform low-code tools (Power Apps/Power Automate/Dataverse)\nPreferred Skills\nExperience with product development for the Microsoft Azure platform Experience with product development life cycle would be a plus Experience with agile development methodology (Scrum) Functional analysis skills and experience (Use cases, UML) is an asset. Experience in NoSQL databases would be an asset.\nExperience in data science activities such as advanced analytics and machine learning would be an asset.\n\nCompetences:\nYou have a passion for innovation and technologies, combined with strong technical and analytical skills.\nYou are customer oriented, enthusiastic, and professional\nYou have a can-do mindset, hands-on approach and are decisive. You are not afraid of making errors and are willing to learn by doing\nYou are a strong communicator with excellent collaboration skills and are customer focused\nYou have self-drive and passion You are flexible and prepared to work outside of business hours if required to meet a deadline.\nYou have a proactive attitude and always strive for continuous improvement\nYou are able to cooperate with different levels in the organization, with different people and cultures\nYou can maintain good relations with external parties\nYou are stress resistant.\nYou are result oriented and quality focused on terms of processes.\nIn return, we offer you What we offer\nFlexible working hours\nA modern infrastructure providing you with the latest tools and technologies at your disposal\nA challenging environment which contributes to your constant learning and professional growth\nTo be part of the data competence team which is constantly growing\nDepending on the country you are enrolled in group healthcare insurance plans covering your medical needs\nA chance to become part of a global, innovative company, supporting sustainable productivity\nYou get the opportunity to bring revolutionary ideas fostering innovation and execute qualified ideas\nA friendly culture with immense professional and personal development, education and opportunities for career growth\nFree access to LinkedIn Learning and many other internal and external trainings\nAtlas Copco offers trainings on a regular basis to acquire new skills\nFriendly and open culture of Swedish company. Very high visibility in the organization with ""No door"" culture, you can always talk to anyone in the organization\nLast Day to Apply\n25/06/2025\nDiverse by nature and inclusive by choice\nBright ideas come from all of us. The more unique perspectives we embrace, the more innovative we are. Together we build a culture where difference is valued and we share a deep sense of purpose and belonging.",Industry Type: Industrial Equipment / Machinery,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Enterprise applications', 'UML', 'Debugging', 'Healthcare', 'Scrum', 'Continuous improvement', 'SQL', 'Python']",2025-06-14 06:00:39
Lead Data Engineer,Whats On India Media,10 - 16 years,Not Disclosed,"['Bengaluru', 'Mumbai (All Areas)']","Gracenote, a Nielsen company, is dedicated to connecting audiences to the\nentertainment they love, powering a better media future for all people. Gracenote is the\ncontent data business unit of Nielsen that powers innovative entertainment experiences for\nthe worlds leading media companies. Our entertainment metadata and connected IDs\ndeliver advanced content navigation and discovery to connect consumers to the content\nthey love and discover new ones.\nGracenotes industry-leading datasets cover TV programs, movies, sports, music and\npodcasts in 80 countries and 35 languages.Common identifiers Universally adopted by the\nworlds leading media companies to deliver powerful cross-media entertainment\nexperiences. Machine driven, human validated best-in-class data and images fuel new\nsearch and discovery experiences across every screen.\nGracenote's Data Organization is a dynamic and innovative group that is essential in\ndelivering business outcomes through data, insights, predictive & prescriptive analytics. An\nextremely motivated team that values creativity, experimentation through continuous\nlearning in an agile and collaborative manner. From designing, developing and maintaining\ndata architecture that satisfies our business goals to managing data governance and\nregion-specific regulations, the data team oversees the whole data lifecycle.\nRole Overview:\nWe are seeking an experienced Senior Data Engineer with 10-12 years of experience to join\nour Video engineering team with Gracenote - a NielsenIQ Company. In this role, you will\ndesign, build, and maintain our data processing systems and pipelines. You will work closely\nwith Product managers, Architects, analysts, and other stakeholders to ensure data is\naccessible, reliable, and optimized for Business, analytical and operational needs.\n\nKey Responsibilities:\n\nDesign, develop, and maintain scalable data pipelines and ETL processes\nArchitect and implement data warehousing solutions and data lakes\nOptimize data flow and collection for cross-functional teams\nBuild infrastructure required for optimal extraction, transformation, and loading of data\nEnsure data quality, reliability, and integrity across all data systems\nCollaborate with data scientists and analysts to help implement models and algorithms\nIdentify, design, and implement internal process improvements: automating manual\nprocesses, optimizing data delivery, etc.\nCreate and maintain comprehensive technical documentation\nMentor junior engineers and provide technical leadership\nEvaluate and integrate new data management technologies and tools\nImplement Optimization strategies to enable and maintain sub second latency.\nOversee Data infrastructure to ensure robust deployment and monitoring of the\npipelines and processes.\nStay ahead of emerging trends in Data, cloud, integrating new research into practical\napplications.\nMentor and grow a team of junior data engineers.\n\nRequired qualification and Skills:\n\nExpert-level proficiency in Python, SQL, and big data tools (Spark, Kafka, Airflow).\nBachelor's degree in Computer Science, Engineering, or related field; Master's degree\npreferred\nExpert knowledge of SQL and experience with relational databases (e.g., PostgreSQL,\nRedshift, TIDB, MySQL, Oracle, Teradata)\nExtensive experience with big data technologies (e.g., Hadoop, Spark, Hive, Flink)\nProficiency in at least one programming language such as Python, Java, or Scala\nExperience with data modeling, data warehousing, and building ETL pipelines\nStrong knowledge of data pipeline and workflow management tools (e.g., Airflow, Luigi,\nNiFi)\nExperience with cloud platforms (AWS, Azure, or GCP) and their data services. AWS\nPreferred\nHands on Experience with building streaming pipelines with flink, Kafka, Kinesis. Flink\nPreferred.\nUnderstanding of data governance and data security principles\nExperience with version control systems (e.g., Git) and CI/CD practices\nProven leadership skills in grooming data engineering teams.\nPreferred Skills\nExperience with containerization and orchestration tools (Docker, Kubernetes)\nBasic knowledge of machine learning workflows and MLOps\nExperience with NoSQL databases (MongoDB, Cassandra, etc.)\nFamiliarity with data visualization tools (Tableau, Power BI, etc.)\nExperience with real-time data processing\nKnowledge of data governance frameworks and compliance requirements (GDPR, CCPA,\netc.)\nExperience with infrastructure-as-code tools (Terraform, CloudFormation)Role & responsibilities",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'Kafka Streams', 'AWS', 'GCP', 'Azure Cloud', 'Postgresql', 'MySQL']",2025-06-14 06:00:41
Senior Software / Data Engineer,Splore Human Centric Ai,4 - 9 years,Not Disclosed,['Bengaluru'],"What is Splore\n\n\n\nSplore is redefining how enterprises harness the power of generative AI and multi-agent systems. We work closely with established partners across industries like finance, legal, and tech, enabling them to solve real-world challenges and drive productivity. We integrate state-of-the-art AI technologies into existing business workflows, offering end-to-end solutions that enhance decision-making and streamline operations.\n\n\n\nBacked by industry leaders Temasek and Menyala, and powered by a team of AI and machine learning experts, Splore delivers AI applications to stay ahead in a rapidly evolving, data-driven landscape.\n\na data-driven world, providing precise, actionable insights that drive businesses to operate more effectively and make smarter decisions.\n\n\n\nWhat is the role\n\n\nAs the Senior Software / Data Engineer, you will act as the primary contact for the Engineering Director and Product Managers in steering platform development. You will drive the creation of our core / data platform and improve engineering effectiveness and processes by scaling the platform across target businesses. Overseeing a team of 2-4 engineers, you will build a strong engineering / data culture driving fast product innovation. Guiding and designing technical architecture, you will manage sprints and releases for the team.\n\n\n\nResponsibilities\n\n\nIn this role, you will:\n\n\nLead Architecture and Design: Architect, design, and oversee the implementation of various components of the core/data platform, ensuring integration with technologies developed by our AI/ML team. APIs for Front-End App, leveraging Cloud, Cloud Native, Edge Computing, and AI/ML technologies.\n\n\n\nMetrics and Optimization: Define key performance metrics for the platform, establish processes for tracking these metrics, and continuously optimize platform performance.\n\nEngineering Processes: Implement and refine engineering processes, utilizing the most effective tools and methodologies to enhance development efficiency and data management.\n\nCross-Functional Collaboration: Work closely with Product Management and Engineering teams to align AI, platform capabilities, and solutions with business and technical requirements.\n\nPlatform Development Lifecycle Management: Oversee the entire lifecycle of platform development, from conception through deployment and maintenance.\n\nTechnical Leadership and Oversight: Provide technical guidance to engineers, code reviews, ensuring high-quality technical delivery and effective problem-solving during implementation phases.\n\nArchitectural and Non-Functional Considerations: Ensure that the platform architecture adequately addresses key non-functional requirements such as high availability, scalability, maintainability, and extensibility\n\n\n\n\nAttributes\n\n\n\nWe are looking for a Senior Software / Data Engineer with the following:\n\n\nDealing with Ambiguity - You thrive in navigating dynamic environments, making informed decisions amid evolving scenarios and comfortably embracing uncertainty.\n\nCollaborates - Close partnership with our Product, Design and Engineering teams will be key in building our search engine!\n\nNimble Learning - Were looking for someone who thrives in a startup environment. Youre not afraid to get your hands dirty and learn through experimentation when faced with fresh challenges. Youre always on the pulse of the latest ML trends and immersing yourself in new technologies.\n\nFunctional/ Technical Skills - Demonstrated expertise in Backend Engineering with over 4 years of experience designing and building large-scale distributed systems and data platforms.\n\nProficient in Java and Python, with strong working knowledge of databases and modern UI technologies; experience with React is a plus.\n\nDeep understanding of technical architecture, system design, and software development best practices.\n\nProven track record of mentoring and leading engineers across various experience levels.\n\nHands-on experience with Data Engineering, platform development, data pipelines, real-time streaming, Data Lake architecture, and DevOps practices.\n\nBackground in startup environments, particularly in Data/AI startups or SaaS companies.\n\nFamiliarity with the MCP (Model Context Protocol) framework is a strong advantage.\n\nA passion for AI and emerging technologies is highly valued.",Industry Type: Internet,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Product management', 'Backend', 'Front end', 'Data management', 'Machine learning', 'Technical leadership', 'System design', 'Product design', 'Distribution system', 'Python']",2025-06-14 06:00:43
Specialist Digital Supply Chain Datamodeler/Data Engineer,Merck Sharp & Dohme (MSD),4 - 8 years,Not Disclosed,['Hyderabad'],"Our company is an innovative, global healthcare leader that is committed to improving health and well-being around the world with a diversified portfolio of prescription medicines, vaccines and animal health products. We continue to focus our research on conditions that affect millions of people around the world - diseases like Alzheimers, diabetes and cancer - while expanding our strengths in areas like vaccines and biologics.\nOur ability to excel depends on the integrity, knowledge, imagination, skill, diversity and teamwork of an individual like you. To this end, we strive to create an environment of mutual respect, encouragement and teamwork. As part of our global team, you ll have the opportunity to collaborate with talented and dedicated colleagues while developing and expanding your career.\nAs a Digital Supply Chain Data Modeler/Engineer, you will work as a member of the Digital Manufacturing Division team supporting Enterprise Orchestration Platform. You will be responsible for identifying, assessing, and solving complex business problems related to manufacturing and supply chain. You will receive training to achieve this, and you ll be amazed at the diversity of opportunities to develop your potential and grow professionally. You will collaborate with business stakeholders and determine analytical capabilities that will enable the creation of Insights-focused solutions that align to business needs and ensure that delivery of these solutions meet quality requirements.\nThe Opportunity\nBased in Hyderabad, joining a global healthcare biopharma company and be part of a 130- year legacy of success backed by ethical integrity, forward momentum, and an inspiring mission to achieve new milestones in global healthcare.\nBe part of an organization driven by digital technology and data-backed approaches that support a diversified portfolio of prescription medicines, vaccines, and animal health products.\nDrive innovation and execution excellence. Be a part of a team with passion for using data, analytics, and insights to drive decision-making, and which creates custom software, allowing us to tackle some of the worlds greatest health threats.\nOur Technology Centers focus on creating a space where teams can come together to deliver business solutions that save and improve lives. An integral part of our company s IT operating model, Tech Centers are globally distributed locations where each IT division has employees to enable our digital transformation journey and drive business outcomes. These locations, in addition to the other sites, are essential to supporting our business and strategy.\nA focused group of leaders in each Tech Centre helps to ensure we can manage and improve each location, from investing in growth, success, and well-being of our people, to making sure colleagues from each IT division feel a sense of belonging to managing critical emergencies. And together, we must leverage the strength of our team to collaborate globally to optimize connections and share best practices across the Tech Centers.\nAs Data modeler lead, you will be responsible for following\nDeliver divisional analytics initiatives with primary focus on datamodeling for all analytics, advanced analytics and AI/ML uses cases e,g Self Services , Business Intelligence & Analytics, Data exploration, Data Wrangling etc.\nHost and lead requirement/process workshop to understand the requirements of datamodeling .\nAnalysis of business requirements and work with architecture team to deliver & contribute to feasibility analysis, implementation plans and high-level estimates.\nBased on business process and analysis of data sources, deliver detailed ETL design with mapping of data model covering all areas of Data warehousing for all analytics use cases .\nCreation of data model & transformation mapping in modeling tool and deploy in databases including creation of schedule orchestration jobs .\nDeployment of Data modeling configuration to Target systems (SIT , UAT & Prod ) .\nUnderstanding of Product owership and management.\nLead Data model as a product for focus areas of Digital supply chain domain.\nCreation of required SDLC documentation as per project requirements.\nOptimization/industrialization of existing database and data transformation solution\nPrepare and update Data modeling and Data warehousing best practices along with foundational platforms.\nWork very closely with foundational product teams, Business, vendors and technology support teams to build team to deliver business initiatives .\nPosition Qualifications :\nEducation Minimum Requirement: - B.S. or M.S. in IT, Engineering, Computer Science, or related field.\nRequired Experience and Skills**:\n5+ years of relevant work experience, with demonstrated expertise in Data modeling in DWH, Data Mesh or any analytics related implementation; experience in implementing end to end DWH solutions involving creating design of DWH and deploying the solution\n3+ years of experience in creating logical & Physical data model in any modeling tool ( SAP Power designer, WhereScape etc ).\nExperience in creating data modeling standards, best practices and Implementation process.\nHigh Proficiency in Information Management, Data Analysis and Reporting Requirement Elicitation\nExperience working with extracting business rules to develop transformations, data lineage, and dimension data modeling\nExperience working with validating legacy and developed data model outputs\nDevelopment experience using WhereScape and various similar ETL/Data Modeling tools\nExposure to Qlik or similar BI dashboarding applications\nHas advanced knowledge of SQL and data transformation practices\nHas deep understanding of data modelling and preparation of optimal data structures\nIs able to communicate with business, data transformation team and reporting team\nHas knowledge of ETL methods, and a willingness to learn ETL technologies\nCan fluently communicate in English\nExperience in Redshift or similar databases using DDL, DML, Query optimization, Schema management, Security, etc\nExperience with Airflow or similar various orchestration tools\nExposure to CI/CD tools\nExposure to AWS modules such as S3, AWS Console, Glue, Spectrum, etc management\nIndependently support business discussions, analyze, and develop/deliver code\nPreferred Experience and Skills:\nExperience working on projects where Agile methodology is leveraged Understanding of data management best practices and data analytics Ability to lead requirements sessions with clients and project teams Strong leadership, verbal and written communication skills with ability to articulate results and issues to internal and client teams Demonstrated experience in the Life Science space Exposure to SAP and Rapid Response domain data is a plus",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['CVS', 'Data analysis', 'SAP', 'Database administration', 'Data structures', 'Healthcare', 'Business intelligence', 'Information technology', 'SDLC', 'SQL']",2025-06-14 06:00:46
"Scala Data Engr/Sr Data Pipeline Python, Spark Streaming German MNC",German MNC,6 - 9 years,Not Disclosed,['Bangalore/Bengaluru( Electronic City )'],"Full time with top German MNC for location Bangalore - Experience on SCALA is a must\n\nJob Overview:\nTo work on development, monitoring and maintenance of Data pipelines across clusters.\nPrimary responsibilities:\nDevelop, Monitor and Maintain data pipeline for various plants.\nCreate and maintain optimal data pipeline architecture.\nAssemble large, complex data sets that meet functional / non-functional business requirements.\nIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability.\nWork with stakeholders including the Data officers and stewards to assist with data-related technical issues and support their data infrastructure needs.\nWork on incidents highlighted by the data officers.\nIncident diagnosis, routing, evaluation & resolution.\nAnalyze the root cause of incidents.\nCreate incident closure report.\nQualifications\nQualifications\nBachelors degree in Computer Science, Electronics & Communication Engineering, a related technical field, or equivalent practical experience.\n6-8 years of experience in Spark, Scala software development.\nExperience in large-scale software development.\nExcellent software engineering skills (i.e., data structures, algorithms, software design).\nExcellent problem-solving, investigative, and troubleshooting skills.\nExperience in Kafka is mandatory\nAdditional Information\nSkills\nSelf-starter and empowered professional with strong execution and project management capabilities\nAbility to collaborate effectively, well developed inter personal relationships with all levels in the organization and outside contacts.\nOutstanding written and verbal communication skills.\nHigh Collaboration & a perseverance to drive performance & change\nAdditional information\nKey Competencies-\nDistributed computing systems\nExperience with CI/CD tools such as Jenkins or Github Actions\nExperience with Python programming\nWorking knowledge of Docker & Kubernetes\nExperience in developing data pipelines using spark & scala.\nExperience in debugging pipeline issues.\nExperience in writing python and shell scripts.\nIn-Depth Knowledge of SQL and Other Database Solutions\nHaving a strong understanding of Apache Hadoop-based analytics\nHands on experience on InteliJ, Github /Bitbucket, HUE.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Data Pipeline', 'SCALA', 'Spark Streaming', 'Python', 'Azure', 'Pipeline']",2025-06-14 06:00:48
Senior Data Engineer,Grab,4 - 9 years,Not Disclosed,['Bengaluru'],"Lending team at Grab is dedicated to building safe, secure, and loan products catering to all user segments across SEA. Our mission is to promote financial inclusion and support underbanked partners across the region. Data plays a pivotal role in our lending operations, guiding decisions across credit assessment, collections, reporting, and beyond\nYou will report to the Lead Data Engineer. This role is based in Bangalore.\nGet to Know the Role:\nAs the Data engineer in the Lending Data Engineering team, you will work with data modellers, product analytics, product managers, software engineers and business stakeholders across the SEA in understanding the business and data requirements. You will build and manage the data asset, including acquisition, storage, processing and use channels, and using some of the most scalable and resilient open source big data technologies like Flink, Airflow, Spark, Kafka, Trino and more on cloud infrastructure. You are encouraged to think out of the box and have fun exploring the latest patterns and designs.\nThe Critical Tasks You will Perform:\nDevelop scalable, reliable ETL pipelines to ingest data from diverse sources.\nBuild expertise in real-time data availability to support accurate real-time metric definitions.\nImplement data quality checks and governance best practices for data cleansing, assurance, and ETL operations. Use existing data platform tools to set up and manage pipelines.\nImprove data infrastructure performance to ensure, reliable insights for decision-making. Design next-gen data lifecycle management tools/frameworks for batch, real-time, API-based, and serverless use cases.\nBuild solutions using AWS services like Glue, Redshift, Athena, Lambda, S3, Step Functions, EMR, and Kinesis.\nUse tools like Amazon MSK/Kinesis for real-time data processing and metric tracking.\nRead more\nSkills you need\nEssential Skills Youll Need:\n4+ years of experience building scalable, secure, distributed, and data pipelines.\nProficiency in Python, Scala, or Java for data engineering solutions.\nKnowledge of big data technologies like Flink, Spark, Trino, Airflow, Kafka, and AWS services (EMR, Glue, Redshift, Kinesis, and Athena).\nSolid experience with SQL, data modelling, and schema design.\nHands-on with AWS storage and compute services (S3, DynamoDB, Athena, and Redshift Spectrum).\nExperience working with NoSQL, Columnar, and Relational databases.\nCurious and eager to explore new data technologies and solutions.\nFamiliarity with in-house and AWS-native tools for efficient pipeline development.\nDesign event-driven architectures using SNS, SQS, Lambda, or similar serverless technologies.\nExperience with data structures, algorithms, or ML concepts.\nRead more\nWhat we offer\nAbout Grab and Our Workplace\nGrab is Southeast Asias leading superapp. From getting your favourite meals delivered to helping you manage your finances and getting around town hassle-free, weve got your back with everything. In Grab, purpose gives us joy and habits build excellence, while harnessing the power of Technology and AI to deliver the mission of driving Southeast Asia forward by economically empowering everyone, with heart, hunger, honour, and humility.\nRead more\nLife at Grab\nLife at Grab\nWe care about your well-being at Grab, here are some of the global benefits we offer:\nWe have your back with Term Life Insurance and comprehensive Medical Insurance.\nWith GrabFlex, create a benefits package that suits your needs and aspirations.\nCelebrate moments that matter in life with loved ones through Parental and Birthday leave , and give back to your communities through Love-all-Serve-all (LASA) volunteering leave\nWe have a confidential Grabber Assistance Programme to guide and uplift you and your loved ones through lifes challenges.\nWhat We Stand For at Grab\nWe are committed to building an inclusive and equitable workplace that enables diverse Grabbers to grow and perform at their best. As an equal opportunity employer, we consider all candidates fairly and equally regardless of nationality, ethnicity, religion, age, gender identity, sexual orientation, family commitments, physical and mental impairments or disabilities, and other attributes that make them unique.\nRead more",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Assurance', 'NoSQL', 'Schema', 'Data structures', 'Data processing', 'Data quality', 'Open source', 'Analytics', 'SQL', 'Python']",2025-06-14 06:00:50
Senior Developer / Lead Data Engineer - Incorta,KPI Partners,5 - 10 years,Not Disclosed,['Pune'],"About Us:\n\n\nKPI Partners is a leading provider of data analytics and performance management solutions, dedicated to helping organizations harness the power of their data to drive business success. Our team of experts is at the forefront of the data revolution, delivering innovative solutions to our clients. We are currently seeking a talented and experienced Senior Developer / Lead Data Engineer with expertise in Incorta to join our dynamic team.",,,,"['Performance management', 'Data modeling', 'Analytical', 'MySQL', 'SCALA', 'Data quality', 'data visualization', 'Analytics', 'Python']",2025-06-14 06:00:52
Lead Data Engineer - SQL and AWS,Gartner for HR,6 - 11 years,Not Disclosed,['Gurugram'],"About the Role:\nGartner is looking for passionate and motivated Lead Data Engineers who are excited to foray into new technologies and help build / maintain data driven components for realizing business needs. This role is in Gartner Product Delivery Organization (PDO). PDO Data Engineering teams are high velocity agile teams responsible for developing and maintaining components crucial to customer-facing channels, data science teams, and reporting & analysis. These components include but are not limited to Spark jobs, REST APIs, AI/ML model training & inference, MLOps / devops pipelines, data transformation & quality pipelines, data lake & data catalogs, data streams etc.\nWhat you will do :\nAbility to lead and execute a mix of small/medium sized projects simultaneously\nOwns success, takes responsibility for successful delivery of solutions from development to production.\nMentor and guide team members\nExplore and create POC of new technologies / frameworks\nShould have significant experience working directly with Business users in problem solving\nExcellent Communication and Prioritization skills.\nShould be able to interact and coordinate well with other developers / teams to resolve operational issues.\nShould be self-motivated and a fast learner to ramp up quickly with a fair amount of help from team members.\nMust be able to estimate development tasks with high accuracy and deliver on time with high quality while following coding guidelines and best practices.\nIdentify systemic operational issues and resolve them.\nWhat you will need\n6+ years of post-college experience in data engineering, API development or related fields\nMust have\nDemonstrated experience in data engineering, data science, or machine learning. Experience working with data platforms - building and maintaining ETL flows and data stores for ML and reporting applications.\nSkills to transform data, prepare it for analysis, and analyze it - including structured and unstructured data\nAbility to transform business needs into technical solutions\nDemonstrated experience of cloud platforms (AWS, Azure, GCP, etc.)\nExperience with languages such as Python, Java, SQL\nExperience with tools such as Apache Spark, Databricks, AWS EMR\nExperience with Kanban or Agile Scrum development\nExperience with REST API development\nExperience collaboration tools such as Git, Jenkins, Jira, Confluence\nExperience with Data modeling and Database schema / table design\n#LI-SP7",Industry Type: Analytics / KPO / Research,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['GIT', 'Data modeling', 'Coding', 'Machine learning', 'Schema', 'Mentor', 'JIRA', 'SQL', 'Python', 'Recruitment']",2025-06-14 06:00:54
Senior Data Engineer,Tata Elxsi,8 - 13 years,25-40 Lacs P.A.,['Bengaluru'],"*Must-Have Skills:*\n\n* Azure Databricks / PySpark hands-on\n* SQL/PL-SQL advanced level\n* Snowflake – 2+ years\n* Spark/Data pipeline development – 2+ years\n* Azure Repos / GitHub, Azure DevOps\n* Unix Shell Scripting\n* Cloud technology experience\n\n*Key Responsibilities:*\n\n1. *Design, build, and manage data pipelines using Azure Databricks, PySpark, and Snowflake.\n2. *Analyze and resolve production issues (Tier 2 support with weekend/on-call rotation).\n3. *Write and optimize complex SQL/PL-SQL queries.\n4. *Collaborate on low-level and high-level design for data solutions.\n5. *Document all project deliverables and support deployment.\n\nGood to Have:\nKnowledge of Oracle, Qlik Replicate, GoldenGate, Hadoop\nJob scheduler tools like Control-M or Airflow\n\nBehavioral:\nStrong problem-solving & communication skills",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Pyspark', 'Azure Databricks', 'Cloud Technologies', 'Snowflake', 'Etl Development', 'Data Engineering', 'Github', 'Data Pipeline Development', 'Unix Shell Scripting', 'SQL', 'Apache Spark', 'Azure Repo', 'PLSQL', 'Oracle', 'Azure Devops']",2025-06-14 06:00:57
Data Analyst with Python,Cenduit India Services,5 - 10 years,Not Disclosed,['Bengaluru'],"Job Description\nEssential Functions.\n\nResearches information for the creation of global data elements or information in variety of areas or in a complex area.\nAnalyses and interprets researched information in relation to agreed business rules.\nAnalyses changes in data or information and incorporates in data being maintained as needed.\nUpdates existing global data systems/stores correctly with new and changed information.\nEnsures the information created is correctly transferred into relevant systems, databases and client offerings.\nQuality assures a variety of global data elements or information, providing feedback to originators and answering their queries.\nMay assess impact of data changes on regular client offerings.\nMay answer queries on global data (client and internal).\nMay assist in training of new data analysts.\nMay assist with scheduling and tracking creation of global reference data.\n\nQualifications\nBachelors Degree or equivalent, in biomedical subject Req\n1.5+ year s experience working in Excel and SQL, creating BI dashboard.\nExperience in Python OR VBA\nExperience in Power BI or tableau\nWilling to work on UK shift\nGood understanding of how data being created is used in client offerings.\nIQVIA is a leading global provider of clinical research services, commercial insights and healthcare intelligence to the life sciences and healthcare industries. We create intelligent connections to accelerate the development and commercialization of innovative medical treatments to help improve patient outcomes and population health worldwide .",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Healthcare', 'Clinical research', 'business rules', 'power bi', 'Scheduling', 'Life sciences', 'Data Analyst', 'biomedical', 'SQL', 'Python']",2025-06-14 06:00:59
Data Science Analyst (Standard),Infogain,3 - 8 years,Not Disclosed,['Gurugram'],"Job Description: Data Scientist\n1. Expertise in Data Science & AI/ML: 3-8 years experience designing, developing, and deploying scalable AI/ML solutions for Big Data, with proficiency in Python, SQL, TensorFlow, PyTorch, Scikit-learn, and Big Data ML libraries (e.g., Spark MLlib).\n2. Cloud Proficiency: Proven experience with cloud-based Big Data services (GCP preferred, AWS/Azure a plus) for AI/ML model deployment and Big Data pipelines.; understanding of data modeling, warehousing, and ETL in Big Data contexts.\n3. Analytical & Communication Skills: Ability to extract actionable insights from large datasets, apply statistical methods, and effectively communicate complex findings to both technical and non-technical audiences (visualization skills a plus).\n4. Educational Background: Bachelors or Masters degree in a quantitative field (Computer Science, Data Science, Engineering, Statistics, Mathematics).\nEXPERIENCE\n3-4.5 Years\nSKILLS\nPrimary Skill: Data Science\nSub Skill(s): Data Science\nAdditional Skill(s): Python, Data Science, SQL, TensorFlow, Pytorch",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'data services', 'data science', 'Data modeling', 'GCP', 'Analytical', 'Cloud', 'big data', 'SQL', 'Python']",2025-06-14 06:01:01
AWS Data engineer,Tata Consultancy Services,5 - 10 years,Not Disclosed,"['Hyderabad', 'Pune', 'Chennai']","Airflow Data Engineer in AWS platform\nJob Title Apache Airflow Data Engineer\nROLE” as per TCS Role\nMaster • 4-8 years of experience in AWS, Apache Airflow (on Astronomer platform), Python, Pyspark, SQL • Good hands-on knowledge on SQL and Data Warehousing life cycle is an absolute requirement. • Experience in creating data pipelines and orchestrating using Apache Airflow • Significant experience with data migrations and development of Operational Data Stores, Enterprise Data Warehouses, Data Lake and Data Marts. • Good to have: Experience with cloud ETL and ELT in one of the tools like DBT/Glue/EMR or Matillion or any other ELT tool • Excellent communication skills to liaise with Business & IT stakeholders. • Expertise in planning execution of a project and efforts estimation. • Exposure to working in Agile ways of working.",,,,"['Airflow', 'Pyspark', 'ETL', 'AWS', 'Python', 'Hive', 'AWS Glue', 'EMR', 'ETL Tool', 'Lambda', 'Athena', 'RedShift']",2025-06-14 06:01:03
Senior Data Engineer,Roku,5 - 10 years,Not Disclosed,['Bengaluru'],"id=""job_description_2_0"">\nRoku is the #1 TV streaming platform in the U.S., Canada, and Mexico, and weve set our sights on powering every television in the world. Roku pioneered streaming to the TV. Our mission is to be the TV streaming platform that connects the entire TV ecosystem. We connect consumers to the content they love, enable content publishers to build and monetize large audiences, and provide advertisers unique capabilities to engage consumers.\nFrom your first day at Roku, youll make a valuable - and valued - contribution. Were a fast-growing public company where no one is a bystander. We offer you the opportunity to delight millions of TV streamers around the world while gaining meaningful experience across a variety of disciplines.\nAbout the team\nThe mission of Rokus Data Engineering team is to develop a world-class big data platform so that internal and external customers can leverage data to grow their businesses. Data Engineering works closely with business partners and Engineering teams to collect metrics on existing and new initiatives that are critical to business success. As Senior Data Engineer working on Device metrics, you will design data models & develop scalable data pipelines to capturing different business metrics across different Roku Devices.\nAbout the role\nRoku pioneered streaming to the TV. We connect users to the streaming content they love, enable content publishers to build and monetise large audiences, and provide advertisers with unique capabilities to engage consumers. Roku streaming players and Roku TV models are available around the world through direct retail sales and licensing arrangements with TV brands and pay-TV operators.With tens of million players sold across many countries, thousands of streaming channels and billions of hours watched over the platform, building scalable, highly available, fault-tolerant, big data platform is critical for our success.This role is based in Bangalore, India and requires hybrid working, with 3 days in the office.\nWhat youll be doing\nBuild highly scalable, available, fault-tolerant distributed data processing systems (batch and streaming systems) processing over 10s of terabytes of data ingested every day and petabyte-sized data warehouse\nBuild quality data solutions and refine existing diverse datasets to simplified data models encouraging self-service\nBuild data pipelines that optimise on data quality and are resilient to poor quality data sources\nOwn the data mapping, business logic, transformations and data quality\nLow level systems debugging, performance measurement & optimization on large production clusters\nParticipate in architecture discussions, influence product roadmap, and take ownership and responsibility over new projects\nMaintain and support existing platforms and evolve to newer technology stacks and architectures\nWere excited if you have\nExtensive SQL Skills\nProficiency in at least one scripting language, Python is required\nExperience in big data technologies like HDFS, YARN, Map-Reduce, Hive, Kafka, Spark, Airflow, Presto, etc.\nProficiency in data modeling, including designing, implementing, and optimizing conceptual, logical, and physical data models to support scalable and efficient data architectures.\nExperience with AWS, GCP, Looker is a plus\nCollaborate with cross-functional teams such as developers, analysts, and operations to execute deliverables\n5+ years professional experience as a data or software engineer\nBS in Computer Science; MS in Computer Science preferred\n#LI-AR3\nBenefits\nRoku is committed to offering a diverse range of benefits as part of our compensation package to support our employees and their families. Our comprehensive benefits include global access to mental health and financial wellness support and resources. Local benefits include statutory and voluntary benefits which may include healthcare (medical, dental, and vision), life, accident, disability, commuter, and retirement options (401(k)/pension). Our employees can take time off work for vacation and other personal reasons to balance their evolving work and life needs. Its important to note that not every benefit is available in all locations or for every role. For details specific to your location, please consult with your recruiter.\nThe Roku Culture\nWe have a unique culture that we are proud of. We think of ourselves primarily as problem-solvers, which itself is a two-part idea. We come up with the solution, but the solution isnt real until it is built and delivered to the customer. That penchant for action gives us a pragmatic approach to innovation, one that has served us well since 2002.\nTo learn more about Roku, our global footprint, and how weve grown, visit https: / / www.weareroku.com / factsheet .\nBy providing your information, you acknowledge that you have read our Applicant Privacy Notice and authorize Roku to process your data subject to those terms.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Data modeling', 'Debugging', 'Healthcare', 'Data processing', 'Data quality', 'data mapping', 'Licensing', 'SQL', 'Python']",2025-06-14 06:01:05
Senior Data Engineer,Blackline,6 - 11 years,Not Disclosed,['Bengaluru'],"Get to Know Us:\n\nIts fun to work in a company where people truly believe in what theyre doing!\nAt BlackLine, were committed to bringing passion and customer focus to the business of enterprise applications.\nSince being founded in 2001, BlackLine has become a leading provider of cloud software that automates and controls the entire financial close process. Our vision is to modernize the finance and accounting function to enable greater operational effectiveness and agility, and we are committed to delivering innovative solutions and services to empower accounting and finance leaders around the world to achieve Modern Finance.\nBeing a best-in-class SaaS Company, we understand that bringing in new ideas and innovative technology is mission critical. At BlackLine we are always working with new, cutting edge technology that encourages our teams to learn something new and expand their creativity and technical skillset that will accelerate their careers.\nWork, Play and Grow at BlackLine!\n\nMake Your Mark:\n\nAs a member of the Data BI Engineering team you will primarily focus on advancing our Enterprise Data Platform to allow the organization to make data-driven decisions. The successful candidate will work closely with cross-functional teams to identify business requirements, design, and develop data models, data warehouses, and data visualization solutions that help support the organizations strategic goals.\nThe Senior Data Engineer will work in a dynamic environment and will be required to stay current with the latest trends and technologies in the business intelligence field. The ideal candidate will be able to pick up business domain and internal process knowledge and leverage that knowledge to think strategically, communicate effectively, and manage multiple projects simultaneously.\nThe team is also responsible for administering tools and platforms around reporting, analytics, and data visualization while promoting best practices. The role requires a strong combination of technical expertise, leadership skills, and a deep understanding of data engineering principles and best practices. We are looking for a driven, detail-oriented, and passionate engineer to come to join our team.\n\nYoull Get To:\n\nProvide technical expertise and leadership in technology direction, road-mapping, architecture definition, design, development, and delivery of enterprise-class solutions while adhering to timelines, coding standards, requirements, and quality.\nArchitect, design, develop, test, troubleshoot, debug, optimize, scale, perform the capacity planning, deploy, maintain, and improve software applications, driving the delivery of high-quality value and features to Blackline s customers.\nWork collaboratively across the company to design, communicate and further assist with adoption of best practices in architecture and implementation.\nDeliver robust architectural solutions for complex design problems.\nImplement, refine, and enforce data engineering best practices to ensure that delivered features meet performance, security, and maintainability expectations.\nResearch, test, benchmark, and evaluate new tools and technologies, and recommend ways to implement them in data platform. Identify and create solutions that are likely to contribute to the development of new company concepts while keeping in mind the business strategy, short- and long-term roadmap, and architectural considerations to support them in a highly scalable and easy extensible manner.\nActively participate in research, development, support, management, and other company initiatives designing solutions to optimally address current and future business requirements and infrastructure plans.\nInspire a forward-thinking team of developers, acting as an agent of change and evangelist for a quality-first culture within the organization. Mentor and coach key technical staff and guide them to solutions on complex design issues.\nAct as a conduit for questions and information flow when those outside of Engineering have ideas for new technology applications.\nSpeak in terms relevant to audience, translating technical concepts into non-technical language and vice versa. Facilitate consensus building while striving for win/win scenarios and elicit value-add contributions from all team members in group settings.\nMaintain a strong sense of business value and return on investment in planning, design, and communication.\nProactively identify issues, bottlenecks, gaps, or other areas of concern or opportunity and work to either directly affect change, or advocate for that change by working with peers and leadership to build consensus and act.\nPerform critical maintenance, deployment, and release support activities, including occasional off-hours support.\n\nWhat Youll Bring:\n\nBachelors or Masters degree in Computer Science, Data Science, or a related field. 6+ years as a data engineer. 6+ years of experience using RDBMS, SQL, NoSQL, Python, Java, or other programming languages is a plus. 6+ years of experience designing, developing, testing, and implementing Extract, Transform and Load (ELT/ETL) solutions using enterprise ELT/ETL tools and Open source. 3+ years working experience with SQL and familiarity with Snowflake data warehouse, strong working knowledge in stored procedures, CTEs, and UDFs, RBAC Knowledge of data integration and data quality best practices Familiarity with data security and privacy regulations. Experience in working in a startup-type environment, good team player, and can work independently with minimal supervision Experience with cloud-native architecture and data solutions. Strong working knowledge in data modeling, data partitioning, and query optimization Demonstrated knowledge of development processes and agile methodologies. Strong analytical and interpersonal skills, comfortable presenting complex ideas in simple terms. Proficient in managing large volumes of data. Strong analytical and interpersonal skills, comfortable presenting complex ideas in simple terms. Strong communication and collaboration skills, with the ability to work effectively with cross-functional teams. Experience in providing technical support and troubleshooting for data-related issues. Expertise with at least one cloud environment and building cloud native data services. Prior experience driving data governance, quality, security initiatives.\nWe re Even More Excited If You Have:\n\nExperience with Google Cloud or similar cloud provider\nSignificant experience with open source platforms and technologies.\nExperience with data science and machine learning tools and technologies is a plus.\n\nThrive at BlackLine Because You Are Joining:\n\nA technology-based company with a sense of adventure and a vision for the future. Every door at BlackLine is open. Just bring your brains, your problem-solving skills, and be part of a winning team at the worlds most trusted name in Finance Automation!\nA culture that is kind, open, and accepting. Its a place where people can embrace what makes them unique, and the mix of cultural backgrounds and varying interests cultivates diverse thought and perspectives.\nA culture where BlackLiners continued growth and learning is empowered. BlackLine offers a wide variety of professional development seminars and inclusive affinity groups to celebrate and support our diversity.\nBlackLine is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to sex, gender identity or expression, race, ethnicity, age, religious creed, national origin, physical or mental disability, ancestry, color, marital status, sexual orientation, military or veteran status, status as a victim of domestic violence, sexual assault or stalking, medical condition, genetic information, or any other protected class or category recognized by applicable equal employment opportunity or other similar laws.\nBlackLine recognizes that the ways we work and the workplace itself have shifted. We innovate in a workplace that optimizes a combination of virtual and in-person interactions to maximize collaboration and nurture our culture. Candidates who live within a reasonable commute to one of our offices will work in the office at least 2 days a week.\nBachelors or Masters degree in Computer Science, Data Science, or a related field. 6+ years as a data engineer. 6+ years of experience using RDBMS, SQL, NoSQL, Python, Java, or other programming languages is a plus. 6+ years of experience designing, developing, testing, and implementing Extract, Transform and Load (ELT/ETL) solutions using enterprise ELT/ETL tools and Open source. 3+ years working experience with SQL and familiarity with Snowflake data warehouse, strong working knowledge in stored procedures, CTEs, and UDFs, RBAC Knowledge of data integration and data quality best practices Familiarity with data security and privacy regulations. Experience in working in a startup-type environment, good team player, and can work independently with minimal supervision Experience with cloud-native architecture and data solutions. Strong working knowledge in data modeling, data partitioning, and query optimization Demonstrated knowledge of development processes and agile methodologies. Strong analytical and interpersonal skills, comfortable presenting complex ideas in simple terms. Proficient in managing large volumes of data. Strong analytical and interpersonal skills, comfortable presenting complex ideas in simple terms. Strong communication and collaboration skills, with the ability to work effectively with cross-functional teams. Experience in providing technical support and troubleshooting for data-related issues. Expertise with at least one cloud environment and building cloud native data services. Prior experience driving data governance, quality, security initiatives.",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Automation', 'Data modeling', 'RDBMS', 'Coding', 'Agile', 'Business intelligence', 'Open source', 'Technical support', 'SQL', 'Python']",2025-06-14 06:01:08
Senior Data Governance Engineer,Aviatrix Systems,5 - 10 years,Not Disclosed,['Bengaluru'],"As a Senior Data Governance Engineer with 5+ years of relevant experience, you will be instrumental in designing, building, and maintaining Aviatrixs data governance framework. This position will have a significant impact on our data infrastructure by promoting best practices, enhancing data transparency, and establishing policies that enable seamless cross-functional collaboration. You will work independently with various teams to implement data governance solutions and ensure our data meets the highest standards of quality and compliance. The role requires flexibility to align with USA working hours (including until midnight IST) to effectively collaborate with global teams.",,,,"['Python', 'SQL', 'Azure Cloud']",2025-06-14 06:01:10
Senior Manufacturing Data Engineer,Analog Devices,3 - 8 years,Not Disclosed,['Bengaluru'],"About Analog Devices\nAnalog Devices, Inc. (NASDAQ: ADI ) is a global semiconductor leader that bridges the physical and digital worlds to enable breakthroughs at the Intelligent Edge. ADI combines analog, digital, and software technologies into solutions that help drive advancements in digitized factories, mobility, and digital healthcare, combat climate change, and reliably connect humans and the world. With revenue of more than $9 billion in FY24 and approximately 24,000 people globally, ADI ensures todays innovators stay Ahead of Whats Possible . Learn more at www.analog.com and on LinkedIn and Twitter (X) .\nResponsibilities\nAs a Senior Manufacturing Data Engineer, you will collaborate with cross-functional teams, including data analysts, product managers, data scientists, and engineers, to deliver impactful data solutions for semiconductor manufacturing and automation.\nTranslate business and functional requirements into scalable data solutions aligned with Data Architecture guidelines for manufacturing and test environments.\nDevelop and maintain data pipelines to process manufacturing data, including formats such as STDF files, wafer metrology data, wafer sort results, tool logs, MES data, and FDC data, ensuring seamless integration into enterprise data systems.\nWrite scripts and programs to parse, extract, and transform data from diverse sources, improving data accessibility and quality across systems.\nSupport the integration of manufacturing and test data into cloud-based platforms such as Snowflake and Azure Data Lake, enabling advanced analytics and scalable processing.\nContribute to the standardization and optimization of data flows from tool logs, wafer tracking systems, and test equipment to improve reporting and analytics accuracy.\nPartner with senior team members to enhance data quality and completeness across MES, SPC, and test systems.\nPerform root cause analysis on data issues, ensuring timely resolution and adherence to SLA requirements.\nAssist in building subject matter expertise in MES platforms like Camstar/OpCenter and PROMIS, SPC tools, and data analytics platforms.\nParticipate in the development of frameworks for data pipeline observability, including alerting and monitoring systems.\nQualifications\n3+ years in Data Engineering, Data Science, or Data Analytics roles, with a preference for candidates with experience in semiconductor manufacturing.\nDegree in Computer Science, Electrical Engineering, Computer Engineering, or a related field.\nProficiency in SQL and Python.\nExperience with data modeling, ELT processes, and data integration techniques.\nFamiliarity with big data tools (e.g., Spark, Hadoop), containerization (e.g., Kubernetes), and cloud platforms like Snowflake, Azure, and DBT.\nExposure to analytics and visualization tools like Spotfire, Power BI, or Tableau.\nBasic understanding of statistical methods and techniques for anomaly detection and root cause analysis.\nExperience with scripting and programming for data parsing and transformation.",Industry Type: Consumer Electronics & Appliances,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Automation', 'Analog', 'Metrology', 'Healthcare', 'Licensing', 'Monitoring', 'Analytics', 'SQL', 'Python']",2025-06-14 06:01:12
Associate Specialist Data Science,Merck Sharp & Dohme (MSD),1 - 3 years,Not Disclosed,['Pune'],"We are seeking candidates with prior experience in the healthcare analytics or consulting sectors, prior hands-on experience in Data Science (building end-to-end ML models). It is preferred that you have a good understanding of Physician and Patient-level data (PLD) from leading vendors such as IQVIA, Komodo, and Optum. Familiarity with HCP Analytics, PLD analytics, concepts like persistence, compliance, line of therapy, etc., or Segmentation & Targeting is highly desirable. You will be part of a dynamic team that collaborates with our partners across therapeutic areas. Furthermore, effective communication skills are crucial, as this role requires interfacing with executive and business stakeholders.\nWho you are:\nYou understand the foundations of statistics and machine learning and can work in high performance computing/cloud environments, with experience/knowledge in aspects across statistical analysis, machine learning, model development, data engineering, data visualization, and data interpretation\nYou are self-motivated, and have demonstrated abilities to think independently as a data scientist\nYou structure your data science approach according to the necessary task, while appropriately applying the correct level of model complexity to the problem at hand\nYou have an agile mindset of continuous learning and will focus on integrating enterprise value into team culture\nYou are kind, collaborative, and capable of seeking and giving candid feedback that effectively contributes to a more seamless day-to-day execution of tasks\nKey Responsibilities:\nUnderstand the business requirements and support the manager to translate those to analytical problem statements.\nImplement the solution steps through SQL/Python, appropriate ML techniques without rigorous handholding.\nFollow technical requirements (Datasets, business rules, technical architecture) and industry best practices in every task.\nCollaborate with cross-functional teams to design and implement solutions that meet business requirements.\nPresent the findings to US DS stakeholders in a clear and concise manner and address feedback.\nAdopt a continuous learning mindset, both technical and functional.\nDevelop deep expertise in therapeutic area, with clear focus on commercial aspects.\nMinimum Qualifications:\nBachelor s degree with at least 1-3 years industry experience\nStrong Python/R, SQL, Excel skills\nStrong foundations of statistics and machine learning\nPreferred Qualifications:\nAdvanced degree in STEM (MS, MBA, PhD)\n2-3 years experience in healthcare analytics and consulting\nFamiliarity with Physician and Patient-Level data (e.g., claims, electronic health records) and data from common healthcare data vendors (IQVIA, Optum, Komodo, etc.)\nExperience in HCP & Patient Level Data analytics (e.g., HCP Segmentation & targeting, Patient Cohorts, knowledge of Lines of Therapy, Persistency, Compliance, etc.)\nProficiency in Data Science Concepts, Microsoft Excel and PowerPoint, and familiarity with Dataiku\nCurrent Employees apply HERE\nCurrent Contingent Workers apply HERE\nSearch Firm Representatives Please Read Carefully\nEmployee Status:\nRegular .",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Relationship management', 'Claims', 'Data modeling', 'Pharma', 'Analytical', 'Consulting', 'Healthcare', 'healthcare analytics', 'Business intelligence', 'SQL']",2025-06-14 06:01:14
Sr. Data Analyst,Slintel,3 - 8 years,Not Disclosed,[],"Our Mission:\n6sense is on a mission to revolutionize how B2B organizations create revenue by predicting customers most likely to buy and recommending the best course of action to engage anonymous buying teams. 6sense Revenue AI is the only sales and marketing platform to unlock the ability to create, manage and convert high-quality pipeline to revenue.\nOur People:\nPeople are the heart and soul of 6sense. We serve with passion and purpose. We live by our Being 6sense values of Accountability, Growth Mindset, Integrity, Fun and One Team. Every 6sensor plays a part in de ning the future of our industry-leading technology. 6sense is a place where difference-makers roll up their sleeves, take risks, act with integrity, and measure success by the value we create for our customers.\nWe want 6sense to be the best chapter of your career.\nPosition Overview:\nWe are seeking a highly skilled Sr. Data Analyst to focus on backend data support and governance. The ideal candidate will have a strong background in data engineering principles, SQL, and data modeling within modern cloud data platforms. This individual will play a key role in building and maintaining a scalable and trusted data infrastructure that supports reporting across the customer journey. A working knowledge of data governance frameworks and the ability to collaborate cross-functionally is essential.\nKey Responsibilities:\nBuild, maintain, and optimize data pipelines and models within our data warehouse to enable trusted downstream analytics.\nDevelop scalable, clean, and joinable datasets to support reporting across sales, marketing, customer success, and finance functions.\nCollaborate closely with RevOps, data engineering, and analytics stakeholders to ensure data is structured and aligned to business needs.\nSupport data governance by enforcing data definitions, naming conventions, and ownership models.\nMonitor and improve data quality, lineage, and integrity through proactive checks and documentation.\nTranslate raw data into reusable, governed tables and metrics to support self-service and centralized reporting use cases.\nAssist in standardizing metrics and business definitions to drive consistent reporting across systems and teams.\nQualifications:\nBachelor s degree in Computer Science, Data Science, Information Systems, or a related field. Master s preferred.\n3+ years of experience in a data analytics, analytics engineering, or backend reporting role.\nExpert-level SQL skills and experience working with cloud data warehouses (Snowflake, Redshift, BigQuery, etc.).\nSolid understanding of dimensional modeling, data architecture, and ELT pipeline development.\nFamiliarity with data governance tools, policies, or best practices.\nExperience with BI platforms (Looker, Tableau, Power BI, Sigma) is a plus.\nStrong organizational and communication skills; ability to translate technical requirements into business impact.\nOur Benefits:\nFull-time employees can take advantage of health coverage, paid parental leave, generous paid time-off and holidays, quarterly self-care days off, and stock options. We ll make sure you have the equipment and support you need to work and connect with your teams, at home or in one of our o ces.\nWe have a growth mindset culture that is represented in all that we do, from onboarding through to numerous learning and development initiatives including access to our LinkedIn Learning platform. Employee well-being is also top of mind for us. We host quarterly wellness education sessions to encourage self care and personal growth. From wellness days to ERG-hosted events, we celebrate and energize all 6sense employees and their backgrounds.\nEqual Opportunity Employer:\n6sense is an Equal Employment Opportunity and Affirmative Action Employers. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender perception or identity, national origin, age, marital status, protected veteran status, or disability status. .\nWe are aware of recruiting impersonation attempts that are not affiliated with 6sense in any way. A ll email communications from 6sense will originate from the @6sense.com domain . We will not initially contact you via text message and will never request payments . If you are uncertain whether you have been contacted by an official 6sense employee, reach out to jobs@ 6sense.com",Industry Type: Internet,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Backend', 'Data modeling', 'Senior Data Analyst', 'data governance', 'Wellness', 'Data quality', 'Downstream', 'SQL', 'Data architecture']",2025-06-14 06:01:16
Snowflake Data Engineer,LatentView,3 - 6 years,Not Disclosed,"['Chennai', 'Bengaluru']","Job Description:\n\nWe are seeking a highly experienced and skilled Senior Data Engineer to join our dynamic team. This role requires hands-on experience with databases such as Snowflake and Teradata, as well as advanced knowledge in various data science and AI techniques. The successful candidate will play a pivotal role in driving data-driven decision-making and innovation within our organization.",,,,"['Data Engineering', 'Snowflake', 'Tableau', 'Data Warehousing', 'Data Modeling', 'ETL']",2025-06-14 06:01:19
Hiring For Big Data Lead with Infogain,Infogain,12 - 15 years,Not Disclosed,['Singapore'],"BIG Data Lead\n\nExperience: 12-15 Years\nLocation: Singapore\nNotice Period: 0-15 Days\n\nJob Description:\nMandatory Skills :\n(SQL Server / Oracle / DB2 / Netezza) at-least good working knowledge in 2 of these DB\nApache Spark Streaming or Apache Flink\nKafka\nNOSQL databases - Cosmos DB, Document DB\nSpark, Dataframe API\nHive (HQL)\nScripting language Shell or bash\nCI CD\nExperience with at least one Cloud Infra provider (Azure/AWS)\nGood to have Skills :\nCertifications related to Data and Analytics",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Big Data', 'Spark', 'Data Modeling', 'Azure', 'Hive', 'Apache Flink', 'Hadoop', 'Kafka', 'Spark Streaming', 'AWS.', 'Shell Python']",2025-06-14 06:01:21
Data Analyst,Vishanz Business Services,4 - 8 years,Not Disclosed,"['Hyderabad', 'Chennai', 'Bengaluru']","We are looking for ""Data Analyst"" with Minimum 4 years experience\nContact- Yashra (95001 81847)\n\nRequired Candidate profile\nHands-on Exp in Data Analyst’s role, Python, SQL, Power BI. Pref. Business domains: Telecom/CPG/Life Science\nAdvanced SQL with experience in writing optimized queries for large datasets.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Analysis', 'Python', 'SQL', 'Power Bi']",2025-06-14 06:01:23
Engineer Data,Empower Pragati,2 - 6 years,Not Disclosed,['Bengaluru'],"Our vision for the future is based on the idea that transforming financial lives starts by giving our people the freedom to transform their own. We have a flexible work environment, and fluid career paths. We not only encourage but celebrate internal mobility. We also recognize the importance of purpose, well-being, and work-life balance. Within Empower and our communities, we work hard to create a welcoming and inclusive environment, and our associates dedicate thousands of hours to volunteering for causes that matter most to them.\nChart your own path and grow your career while helping more customers achieve financial freedom. Empower Yourself.\nThis role supports Empower s data and AI strategy, with a focus on building Responsible AI capabilities. The Data Engineer will design and implement scalable, ethical, and secure data pipelines and infrastructure that underpin AI/ML systems, ensuring high-quality data flows into model development, testing, monitoring, and governance workflows. The candidate will work across cloud (AWS) and on-premises environments, contributing to the lifecycle of data used for Responsible AI tooling, including bias detection, model transparency, and compliance tracking.\nESSENTIAL FUNCTIONS:\nDesign, build, and maintain data pipelines that support model development, testing, and monitoring, with a focus on AI governance and traceability.\nCollaborate with cross-functional teams (including Data Scientists, ML Engineers, and Risk) to understand data needs for AI use cases.\nIntegrate data quality, lineage, and metadata tracking into ETL pipelines to support Responsible AI workflows.\nSupport ingestion and transformation of structured and unstructured data (including NLP datasets) for AI model training and evaluation.\nDesign with compliance in mind: integrate secure handling of PII and support auditability in data flows.\nParticipate in technical design discussions focused on enabling transparency, fairness, and explainability in data workflows.\nTroubleshoot and resolve performance and data quality issues in distributed AI pipelines.\nContribute to reusable libraries or templates to support standardized data practices across AI projects.\nQUALIFICATIONS:\nBachelor s Degree in Computer Science , Information Systems, or related field.\n2-6 years of experience in data engineering, preferably in AI/ML environments.\nStrong Python and SQL skills with experience in data pipeline orchestration (e.g., Airflow, Step Functions).\nExperience with Big Data frameworks (e.g., Spark, Hadoop) and streaming data platforms (e.g., Kafka).\nExperience working in AWS environments with services like S3, Glue, Redshift, SageMaker, and Lake Formation.\nFamiliarity with machine learning workflows and data requirements (e.g., training/test splits, data versioning, feature stores).\nExperience integrating data validation, data lineage, or metadata tools (e.g., Great Expectations, Apache Atlas, Amundsen).\nUnderstanding of Responsible AI principles and experience supporting data aspects of fairness, bias, explainability, or model monitoring is a strong plus.\nExperience with JIRA and Agile methodologies.\nExperience in financial services or other highly regulated environments preferred.\nThis job description is not intended to be an exhaustive list of all duties, responsibilities and qualifications of the job. The employer has the right to revise this job description at any time. You will be evaluated in part based on your performance of the responsibilities and/or tasks listed in this job description. You may be required perform other duties that are not included on this job description. The job description is not a contract for employment, and either you or the employer may terminate employment at any time, for any reason.\nWe are an equal opportunity employer with a commitment to diversity. All individuals, regardless of personal characteristics, are encouraged to apply. All qualified applicants will receive consideration for employment without regard to age, race, color, national origin, ancestry, sex, sexual orientation, gender, gender identity, gender expression, marital status, pregnancy, religion, physical or mental disability, military or veteran status, genetic information, or any other status protected by applicable state or local law.",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data validation', 'development testing', 'Machine learning', 'Agile', 'model development', 'Data quality', 'Financial services', 'Monitoring', 'SQL']",2025-06-14 06:01:25
Data Engineer - Python Programming,Leading Client,5 - 7 years,Not Disclosed,['Jaipur'],"We are looking for a skilled Data Engineer with strong hands-on experience in Clickhouse, Kubernetes, SQL, Python, and FastAPI, along with a good understanding of PostgreSQL.\nThe ideal candidate will be responsible for building and maintaining efficient data pipelines, optimizing query performance, and developing APIs to support scalable data services.\n\n- Design, build, and maintain scalable and efficient data pipelines and ETL processes.\n\n- Develop and optimize Clickhouse databases for high-performance analytics.\n\n- Create RESTful APIs using FastAPI to expose data services.\n\n- Work with Kubernetes for container orchestration and deployment of data services.\n\n- Write complex SQL queries to extract, transform, and analyze data from PostgreSQL and Clickhouse.\n\n- Collaborate with data scientists, analysts, and backend teams to support data needs and ensure data quality.\n\n- Monitor, troubleshoot, and improve performance of data infrastructure.\n\n- Strong experience in Clickhouse - data modeling, query optimization, performance tuning.\n\n- Expertise in SQL - including complex joins, window functions, and optimization.\n\n- Proficient in Python, especially for data processing (Pandas, NumPy) and scripting.\n\n- Experience with FastAPI for creating lightweight APIs and microservices.\n\n- Hands-on experience with PostgreSQL - schema design, indexing, and performance.\n\n- Solid knowledge of Kubernetes managing containers, deployments, and scaling.\n\n- Understanding of software engineering best practices (CI/CD, version control, testing).\n\n- Experience with cloud platforms like AWS, GCP, or Azure.\n\n- Knowledge of data warehousing and distributed data systems.\n\n- Familiarity with Docker, Helm, and monitoring tools like Prometheus/Grafana.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'PostgreSQL', 'Data Modeling', 'Data Warehousing', 'Data Analytics', 'ETL', 'Python', 'SQL']",2025-06-14 06:01:28
AWS Data Engineer,Redserv Global Solutions,3 - 6 years,10-15 Lacs P.A.,[],"Roles and Responsibilities:\n\nWe are looking for an experienced AWS Cloud Data Engineer to join our Data Science & Analytics team to build, optimize, and maintain cloud-based data solutions. The ideal candidate will possess strong technical knowledge in data engineering on AWS, expertise in data integration, pipeline creation, performance optimization, and a strong understanding of DevOps methodologies.\nDesign, develop, and deploy scalable, high-performance data pipelines on AWS and scalable AWS infrastructure solutions.",,,,"['Data Engineer', 'AWS', 'Python']",2025-06-14 06:01:30
Data Engineer - Python Programming,Leading Client,5 - 7 years,Not Disclosed,['Surat'],"Job Description :\n\nWe are looking for a skilled Data Engineer with strong hands-on experience in Clickhouse, Kubernetes, SQL, Python, and FastAPI, along with a good understanding of PostgreSQL.\nThe ideal candidate will be responsible for building and maintaining efficient data pipelines, optimizing query performance, and developing APIs to support scalable data services.\n\n- Design, build, and maintain scalable and efficient data pipelines and ETL processes.\n\n- Develop and optimize Clickhouse databases for high-performance analytics.\n\n- Create RESTful APIs using FastAPI to expose data services.\n\n- Work with Kubernetes for container orchestration and deployment of data services.\n\n- Write complex SQL queries to extract, transform, and analyze data from PostgreSQL and Clickhouse.\n\n- Collaborate with data scientists, analysts, and backend teams to support data needs and ensure data quality.\n\n- Monitor, troubleshoot, and improve performance of data infrastructure.\n\n- Strong experience in Clickhouse - data modeling, query optimization, performance tuning.\n\n- Expertise in SQL - including complex joins, window functions, and optimization.\n\n- Proficient in Python, especially for data processing (Pandas, NumPy) and scripting.\n\n- Experience with FastAPI for creating lightweight APIs and microservices.\n\n- Hands-on experience with PostgreSQL - schema design, indexing, and performance.\n\n- Solid knowledge of Kubernetes managing containers, deployments, and scaling.\n\n- Understanding of software engineering best practices (CI/CD, version control, testing).\n\n- Experience with cloud platforms like AWS, GCP, or Azure.\n\n- Knowledge of data warehousing and distributed data systems.\n\n- Familiarity with Docker, Helm, and monitoring tools like Prometheus/Grafana.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Python Programming', 'Data Engineering', 'PostgreSQL', 'Data Modeling', 'Data Warehousing', 'Data Analytics', 'ETL', 'Python', 'SQL']",2025-06-14 06:01:32
Data Engineer - Python Programming,Leading Client,5 - 7 years,Not Disclosed,['Chennai'],"We are looking for a skilled Data Engineer with strong hands-on experience in Clickhouse, Kubernetes, SQL, Python, and FastAPI, along with a good understanding of PostgreSQL.\nThe ideal candidate will be responsible for building and maintaining efficient data pipelines, optimizing query performance, and developing APIs to support scalable data services.\n\n- Design, build, and maintain scalable and efficient data pipelines and ETL processes.\n\n- Develop and optimize Clickhouse databases for high-performance analytics.\n\n- Create RESTful APIs using FastAPI to expose data services.\n\n- Work with Kubernetes for container orchestration and deployment of data services.\n\n- Write complex SQL queries to extract, transform, and analyze data from PostgreSQL and Clickhouse.\n\n- Collaborate with data scientists, analysts, and backend teams to support data needs and ensure data quality.\n\n- Monitor, troubleshoot, and improve performance of data infrastructure.\n\n- Strong experience in Clickhouse - data modeling, query optimization, performance tuning.\n\n- Expertise in SQL - including complex joins, window functions, and optimization.\n\n- Proficient in Python, especially for data processing (Pandas, NumPy) and scripting.\n\n- Experience with FastAPI for creating lightweight APIs and microservices.\n\n- Hands-on experience with PostgreSQL - schema design, indexing, and performance.\n\n- Solid knowledge of Kubernetes managing containers, deployments, and scaling.\n\n- Understanding of software engineering best practices (CI/CD, version control, testing).\n\n- Experience with cloud platforms like AWS, GCP, or Azure.\n\n- Knowledge of data warehousing and distributed data systems.\n\n- Familiarity with Docker, Helm, and monitoring tools like Prometheus/Grafana.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Python Programming', 'Data Engineering', 'PostgreSQL', 'Data Modeling', 'Data Warehousing', 'Data Analytics', 'ETL', 'Python', 'SQL']",2025-06-14 06:01:34
Data Engineer - Python Programming,Leading Client,5 - 7 years,Not Disclosed,['Noida'],"Job Description :\n\nWe are looking for a skilled Data Engineer with strong hands-on experience in Clickhouse, Kubernetes, SQL, Python, and FastAPI, along with a good understanding of PostgreSQL.\nThe ideal candidate will be responsible for building and maintaining efficient data pipelines, optimizing query performance, and developing APIs to support scalable data services.\n\n- Design, build, and maintain scalable and efficient data pipelines and ETL processes.\n\n- Develop and optimize Clickhouse databases for high-performance analytics.\n\n- Create RESTful APIs using FastAPI to expose data services.\n\n- Work with Kubernetes for container orchestration and deployment of data services.\n\n- Write complex SQL queries to extract, transform, and analyze data from PostgreSQL and Clickhouse.\n\n- Collaborate with data scientists, analysts, and backend teams to support data needs and ensure data quality.\n\n- Monitor, troubleshoot, and improve performance of data infrastructure.\n\n- Strong experience in Clickhouse - data modeling, query optimization, performance tuning.\n\n- Expertise in SQL - including complex joins, window functions, and optimization.\n\n- Proficient in Python, especially for data processing (Pandas, NumPy) and scripting.\n\n- Experience with FastAPI for creating lightweight APIs and microservices.\n\n- Hands-on experience with PostgreSQL - schema design, indexing, and performance.\n\n- Solid knowledge of Kubernetes managing containers, deployments, and scaling.\n\n- Understanding of software engineering best practices (CI/CD, version control, testing).\n\n- Experience with cloud platforms like AWS, GCP, or Azure.\n\n- Knowledge of data warehousing and distributed data systems.\n\n- Familiarity with Docker, Helm, and monitoring tools like Prometheus/Grafana.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Python Programming', 'Data Engineering', 'PostgreSQL', 'Data Modeling', 'Data Warehousing', 'Data Analytics', 'ETL', 'Python', 'SQL']",2025-06-14 06:01:36
Data Engineer - Python Programming,Leading Client,5 - 7 years,Not Disclosed,['Lucknow'],"We are looking for a skilled Data Engineer with strong hands-on experience in Clickhouse, Kubernetes, SQL, Python, and FastAPI, along with a good understanding of PostgreSQL.\nThe ideal candidate will be responsible for building and maintaining efficient data pipelines, optimizing query performance, and developing APIs to support scalable data services.\n\n- Design, build, and maintain scalable and efficient data pipelines and ETL processes.\n\n- Develop and optimize Clickhouse databases for high-performance analytics.\n\n- Create RESTful APIs using FastAPI to expose data services.\n\n- Work with Kubernetes for container orchestration and deployment of data services.\n\n- Write complex SQL queries to extract, transform, and analyze data from PostgreSQL and Clickhouse.\n\n- Collaborate with data scientists, analysts, and backend teams to support data needs and ensure data quality.\n\n- Monitor, troubleshoot, and improve performance of data infrastructure.\n\n- Strong experience in Clickhouse - data modeling, query optimization, performance tuning.\n\n- Expertise in SQL - including complex joins, window functions, and optimization.\n\n- Proficient in Python, especially for data processing (Pandas, NumPy) and scripting.\n\n- Experience with FastAPI for creating lightweight APIs and microservices.\n\n- Hands-on experience with PostgreSQL - schema design, indexing, and performance.\n\n- Solid knowledge of Kubernetes managing containers, deployments, and scaling.\n\n- Understanding of software engineering best practices (CI/CD, version control, testing).\n\n- Experience with cloud platforms like AWS, GCP, or Azure.\n\n- Knowledge of data warehousing and distributed data systems.\n\n- Familiarity with Docker, Helm, and monitoring tools like Prometheus/Grafana.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Python Programming', 'Data Engineering', 'PostgreSQL', 'Data Modeling', 'Data Warehousing', 'Data Analytics', 'ETL', 'Python', 'SQL']",2025-06-14 06:01:38
Data Engineer - Python Programming,Leading Client,5 - 7 years,Not Disclosed,['Kolkata'],"Job Description :\n\nWe are looking for a skilled Data Engineer with strong hands-on experience in Clickhouse, Kubernetes, SQL, Python, and FastAPI, along with a good understanding of PostgreSQL.\nThe ideal candidate will be responsible for building and maintaining efficient data pipelines, optimizing query performance, and developing APIs to support scalable data services.\n\n- Design, build, and maintain scalable and efficient data pipelines and ETL processes.\n\n- Develop and optimize Clickhouse databases for high-performance analytics.\n\n- Create RESTful APIs using FastAPI to expose data services.\n\n- Work with Kubernetes for container orchestration and deployment of data services.\n\n- Write complex SQL queries to extract, transform, and analyze data from PostgreSQL and Clickhouse.\n\n- Collaborate with data scientists, analysts, and backend teams to support data needs and ensure data quality.\n\n- Monitor, troubleshoot, and improve performance of data infrastructure.\n\n- Strong experience in Clickhouse - data modeling, query optimization, performance tuning.\n\n- Expertise in SQL - including complex joins, window functions, and optimization.\n\n- Proficient in Python, especially for data processing (Pandas, NumPy) and scripting.\n\n- Experience with FastAPI for creating lightweight APIs and microservices.\n\n- Hands-on experience with PostgreSQL - schema design, indexing, and performance.\n\n- Solid knowledge of Kubernetes managing containers, deployments, and scaling.\n\n- Understanding of software engineering best practices (CI/CD, version control, testing).\n\n- Experience with cloud platforms like AWS, GCP, or Azure.\n\n- Knowledge of data warehousing and distributed data systems.\n\n- Familiarity with Docker, Helm, and monitoring tools like Prometheus/Grafana.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Python Programming', 'Data Engineering', 'PostgreSQL', 'Data Modeling', 'Data Warehousing', 'Data Analytics', 'ETL', 'Python', 'SQL']",2025-06-14 06:01:40
Senior Data Science Consultant - Optimization Projects,Leading Client,8 - 10 years,Not Disclosed,['Noida'],"Role Summary :\n\nWe are seeking a highly skilled Senior Data Science Consultant with 8+ years of experience to lead an internal optimization initiative.\n\nThe ideal candidate should have a strong background in data science, operations research, and mathematical optimization, with a proven track record of applying these skills to solve complex business problems.\n\nThis role requires a blend of technical depth, business acumen, and collaborative communication.\n\nA background in internal efficiency/operations improvement or cost/resource optimization projects is highly desirable.\n\nKey Responsibilities :\n\n- Lead and contribute to internal optimization-focused data science projects from design to deployment.\n\n- Develop and implement mathematical models to optimize resource allocation, process performance, and decision-making.\n\n- Use techniques such as linear programming, mixed-integer programming, heuristic and metaheuristic algorithms.\n\n- Collaborate with business stakeholders to gather requirements and translate them into data science use cases.\n\n- Build robust data pipelines and use statistical and machine learning methods to drive insights.\n\n- Communicate complex technical findings in a clear, concise manner to both technical and non-technical audiences.\n\n- Mentor junior team members and contribute to knowledge sharing and best practices within the team.\n\nRequired Skills And Qualifications :\n\n- Masters or PhD in Data Science, Computer Science, Operations Research, Applied Mathematics, or related fields.\n\n- Minimum 8 years of relevant experience in data science, with a strong focus on optimization.\n\n- Expertise in Python (NumPy, Pandas, SciPy, Scikit-learn), SQL, and optimization libraries such as PuLP, Pyomo, Gurobi, or CPLEX.\n\n- Experience with end-to-end lifecycle of internal optimization projects.\n\n- Strong analytical and problem-solving skills.\n\n- Excellent communication and stakeholder management abilities.\n\nPreferred Qualifications :\n\n- Experience working on internal company projects focused on logistics, resource planning, workforce optimization, or cost reduction.\n\n- Exposure to tools/platforms like Databricks, Azure ML, or AWS SageMaker.\n\n- Familiarity with dashboards and visualization tools like Power BI or Tableau.\n\n- Prior experience in consulting or internal centers of excellence (CoE) is a plus.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Data Scientist', 'Operations Research', 'Pandas', 'Numpy', 'Scikit-Learn', 'Python', 'SQL']",2025-06-14 06:01:43
Senior Data Science Consultant - Optimization Projects,Leading Client,8 - 10 years,Not Disclosed,['Lucknow'],"Role Summary :\n\nWe are seeking a highly skilled Senior Data Science Consultant with 8+ years of experience to lead an internal optimization initiative.\n\nThe ideal candidate should have a strong background in data science, operations research, and mathematical optimization, with a proven track record of applying these skills to solve complex business problems.\n\nThis role requires a blend of technical depth, business acumen, and collaborative communication.\n\nA background in internal efficiency/operations improvement or cost/resource optimization projects is highly desirable.\n\nKey Responsibilities :\n\n- Lead and contribute to internal optimization-focused data science projects from design to deployment.\n\n- Develop and implement mathematical models to optimize resource allocation, process performance, and decision-making.\n\n- Use techniques such as linear programming, mixed-integer programming, heuristic and metaheuristic algorithms.\n\n- Collaborate with business stakeholders to gather requirements and translate them into data science use cases.\n\n- Build robust data pipelines and use statistical and machine learning methods to drive insights.\n\n- Communicate complex technical findings in a clear, concise manner to both technical and non-technical audiences.\n\n- Mentor junior team members and contribute to knowledge sharing and best practices within the team.\n\nRequired Skills And Qualifications :\n\n- Masters or PhD in Data Science, Computer Science, Operations Research, Applied Mathematics, or related fields.\n\n- Minimum 8 years of relevant experience in data science, with a strong focus on optimization.\n\n- Expertise in Python (NumPy, Pandas, SciPy, Scikit-learn), SQL, and optimization libraries such as PuLP, Pyomo, Gurobi, or CPLEX.\n\n- Experience with end-to-end lifecycle of internal optimization projects.\n\n- Strong analytical and problem-solving skills.\n\n- Excellent communication and stakeholder management abilities.\n\nPreferred Qualifications :\n\n- Experience working on internal company projects focused on logistics, resource planning, workforce optimization, or cost reduction.\n\n- Exposure to tools/platforms like Databricks, Azure ML, or AWS SageMaker.\n\n- Familiarity with dashboards and visualization tools like Power BI or Tableau.\n\n- Prior experience in consulting or internal centers of excellence (CoE) is a plus.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Data Scientist', 'Operations Research', 'Pandas', 'Numpy', 'Scikit-Learn', 'Python', 'SQL']",2025-06-14 06:01:45
Data Engineer - Python Programming,Leading Client,5 - 7 years,Not Disclosed,['Hyderabad'],"We are looking for a skilled Data Engineer with strong hands-on experience in Clickhouse, Kubernetes, SQL, Python, and FastAPI, along with a good understanding of PostgreSQL.\nThe ideal candidate will be responsible for building and maintaining efficient data pipelines, optimizing query performance, and developing APIs to support scalable data services.\n\n- Design, build, and maintain scalable and efficient data pipelines and ETL processes.\n\n- Develop and optimize Clickhouse databases for high-performance analytics.\n\n- Create RESTful APIs using FastAPI to expose data services.\n\n- Work with Kubernetes for container orchestration and deployment of data services.\n\n- Write complex SQL queries to extract, transform, and analyze data from PostgreSQL and Clickhouse.\n\n- Collaborate with data scientists, analysts, and backend teams to support data needs and ensure data quality.\n\n- Monitor, troubleshoot, and improve performance of data infrastructure.\n\n- Strong experience in Clickhouse - data modeling, query optimization, performance tuning.\n\n- Expertise in SQL - including complex joins, window functions, and optimization.\n\n- Proficient in Python, especially for data processing (Pandas, NumPy) and scripting.\n\n- Experience with FastAPI for creating lightweight APIs and microservices.\n\n- Hands-on experience with PostgreSQL - schema design, indexing, and performance.\n\n- Solid knowledge of Kubernetes managing containers, deployments, and scaling.\n\n- Understanding of software engineering best practices (CI/CD, version control, testing).\n\n- Experience with cloud platforms like AWS, GCP, or Azure.\n\n- Knowledge of data warehousing and distributed data systems.\n\n- Familiarity with Docker, Helm, and monitoring tools like Prometheus/Grafana.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Python', 'Data Engineering', 'GCP', 'PostgreSQL', 'Clickhouse', 'Data Modeling', 'Data Warehousing', 'ETL', 'AWS', 'SQL', 'Kubernetes']",2025-06-14 06:01:47
Data Engineer,IT Service based company,5 - 8 years,15-22.5 Lacs P.A.,"['Noida', 'Bengaluru', 'Delhi / NCR']","HI Candidates,\n\nwe have an opportunities with one of the leading IT consulting Group for the data engineer role. Interested candidates can mail their CV's at Abhishek.saxena@mounttalent.com\n\n\nJob Description-\n\nWhat were looking for Data Engineer III with:\n5+ years of experience with ETL Process, Data warehouse architecture\n5+ Years of experience with Azure Data services i.e. ADF, ADLS Gen 2, Azure SQL dB, Synapse, Azure Databricks, Microsoft Fabric\n5+ years of experience designing business intelligence solutions\nStrong proficiency in SQL and Python/pyspark\nImplementation experience of Medallion architecture and delta lake (or lakehouse)\nExperience with cloud-based data platforms, preferably Azure\nFamiliarity with big data technologies and data warehousing concepts\nWorking knowledge of Azure DevOps and CICD (build and release)",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Snowflake', 'ETL', 'Azure Data Factory', 'Azure Synapse', 'Azure Databricks', 'Data Warehousing']",2025-06-14 06:01:50
Data Engineer - Python Programming,Leading Client,5 - 7 years,Not Disclosed,['Kanpur'],"We are looking for a skilled Data Engineer with strong hands-on experience in Clickhouse, Kubernetes, SQL, Python, and FastAPI, along with a good understanding of PostgreSQL.\nThe ideal candidate will be responsible for building and maintaining efficient data pipelines, optimizing query performance, and developing APIs to support scalable data services.\n\n- Design, build, and maintain scalable and efficient data pipelines and ETL processes.\n\n- Develop and optimize Clickhouse databases for high-performance analytics.\n\n- Create RESTful APIs using FastAPI to expose data services.\n\n- Work with Kubernetes for container orchestration and deployment of data services.\n\n- Write complex SQL queries to extract, transform, and analyze data from PostgreSQL and Clickhouse.\n\n- Collaborate with data scientists, analysts, and backend teams to support data needs and ensure data quality.\n\n- Monitor, troubleshoot, and improve performance of data infrastructure.\n\n- Strong experience in Clickhouse - data modeling, query optimization, performance tuning.\n\n- Expertise in SQL - including complex joins, window functions, and optimization.\n\n- Proficient in Python, especially for data processing (Pandas, NumPy) and scripting.\n\n- Experience with FastAPI for creating lightweight APIs and microservices.\n\n- Hands-on experience with PostgreSQL - schema design, indexing, and performance.\n\n- Solid knowledge of Kubernetes managing containers, deployments, and scaling.\n\n- Understanding of software engineering best practices (CI/CD, version control, testing).\n\n- Experience with cloud platforms like AWS, GCP, or Azure.\n\n- Knowledge of data warehousing and distributed data systems.\n\n- Familiarity with Docker, Helm, and monitoring tools like Prometheus/Grafana.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Python', 'Data Engineering', 'PostgreSQL', 'Clickhouse', 'Data Modeling', 'Data Warehousing', 'ETL', 'Kubernetes', 'SQL']",2025-06-14 06:01:52
Senior Data Engineer,Thinkapps Solutions,5 - 10 years,Not Disclosed,"['Kolkata', 'Hyderabad', 'Pune', 'Ahmedabad', 'Chennai', 'Bengaluru', 'Delhi / NCR', 'Mumbai (All Areas)']","•Experience in MongoDB database is important, other databases of SQL Server, PostgreSQL knowledge also required. Require understand & modifying the database objects based on business request. Must have deep understanding of preparing complex queries,",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['MongoDB', 'Data Bricks', 'PostgreSQL', 'SQL']",2025-06-14 06:01:54
Data Consultant - Databricks Data Engineer,Kyndryl,6 - 10 years,Not Disclosed,['Bengaluru'],"Who We Are\nAt Kyndryl, we design, build, manage and modernize the mission-critical technology systems that the world depends on every day. So why work at Kyndryl? We are always moving forward – always pushing ourselves to go further in our efforts to build a more equitable, inclusive world for our employees, our customers and our communities.\n\nThe Role\nAt Kyndryl, we design, build, manage and modernize the mission-critical technology systems that the world depends on every day. So why work at Kyndryl? We are always moving forward – always pushing ourselves to go further in our efforts to build a more equitable, inclusive world for our employees, our customers and our communities.",,,,"['python', 'sql queries', 'analytical', 'data mining', 'consulting', 'data processing', 'microsoft azure', 'pyspark', 'extraction', 'transformation', 'relational databases', 'stored procedures', 'sql', 'pipeline', 'java', 't', 'load', 'service delivery', 'spark', 'data ingestion', 'debugging', 'troubleshooting', 'client']",2025-06-14 06:01:57
Data Engineer,OSB India,3 - 5 years,Not Disclosed,['Bengaluru'],"Role & responsibilities :\n\n3+ years Data Engineering (SQL, Data warehousing) / ETL (SSIS) development experience is essential\n3 years data design experience in an MI / BI / Analytics environment (Kimball, lake house, data lake) is essential\n3 years experience of working in a structured Change Management project lifecycle is essential",,,,"['SSIS', 'SQL', 'MSBI', 'ETL']",2025-06-14 06:01:59
Senior Data Lead,Aqb Solutions,7 - 10 years,20-27.5 Lacs P.A.,['Kolkata'],"We are looking for a Senior Data Lead to lead enterprise-level data modernization and innovation. In this highly strategic role, you will design scalable, secure, and future-ready data architectures, modernize legacy systems, and provide trusted technical leadership across both technology and business teams. This is a unique opportunity to make a company-wide impact by influencing data strategy and enabling smarter, faster decision-making through data.\n\nKey Responsibilities\n\nArchitect & Design: Lead the development of robust, scalable data models, data management systems, and integration frameworks to ensure enterprise-wide data accuracy, consistency, and security.\nDomain Expertise: Act as a subject matter expert across key business functions such as Supply Chain, Product Engineering, Sales & Marketing, Manufacturing, Finance, and Legal.\nModernization Leadership: Drive the transformation of legacy systems and manage end-to-end cloud migrations with minimal business disruption.\nCollaboration: Partner with data engineers, scientists, analysts, and IT leaders to build high-performance, scalable data pipelines and transformation solutions.\nGovernance & Compliance: Establish and maintain data governance frameworks including metadata repositories, data dictionaries, and data lineage documentation.\nStrategic Advisory: Provide guidance on data architecture best practices, technology selection, and roadmap alignment to senior leadership and cross-functional teams.\nMentorship: Serve as a mentor and thought leader to junior data professionals, fostering a culture of innovation, knowledge sharing, and technical excellence.\nInnovation & Trends: Stay abreast of emerging technologies in cloud, data platforms, and AI/ML to identify and implement innovative solutions.\nCommunication: Translate complex technical concepts into clear, actionable insights for technical and non-technical audiences alike.\nRequired Qualifications\n10+ years of experience in data architecture, engineering, or enterprise data management roles.\nDemonstrated success leading large-scale data initiatives in life sciences or other highly regulated industries.\nDeep expertise in modern data architecture paradigms such as Data Lakehouse, Data Mesh, or Data Fabric.\nStrong hands-on experience with cloud platforms like AWS, Azure, or Google Cloud Platform (GCP).\nProficiency in data modeling, ETL/ELT frameworks, and enterprise integration patterns.\nDeep understanding of data governance, metadata management, master data management (MDM), and data quality practices.\nExperience with tools and platforms including but not limited to:\nData Integration: Informatica, Talend\nData Governance: Collibra\nModeling/Transformation: dbt\nCloud Platforms: Snowflake, Databricks\nExcellent problem-solving skills with the ability to translate business requirements into scalable data solutions.\nExceptional communication skills and experience engaging with both executive stakeholders and engineering teams.\nPreferred Qualifications (Nice to Have)\nExperience with AI/ML data pipelines or real-time streaming architectures.\nCertifications in cloud technologies (e.g., AWS Certified Solutions Architect, Azure Data Engineer).\nFamiliarity with regulatory frameworks such as GxP, HIPAA, or GDPR.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Architecture', 'Data Migration', 'Data Management', 'Data Modeling', 'Data Governance', 'Data Integration']",2025-06-14 06:02:01
PySpark Data Engineer,IT Services & Consulting,6 - 10 years,14-20 Lacs P.A.,"['Hyderabad', 'Bengaluru']","Job Description: PySpark Data Engineer:-\n1. API Development: Design, develop, and maintain robust APIs using FastAPI and RESTful principles for scalable backend systems.\n2. Big Data Processing: Leverage PySpark to process and analyze large datasets efficiently, ensuring optimal performance in big data environments.\n3. Full-Stack Integration: Develop seamless backend-to-frontend feature integrations, collaborating with front-end developers for cohesive user experiences.\n4. CI/CD Pipelines: Implement and manage CI/CD pipelines using GitHub Actions and Azure DevOps to streamline deployments and ensure system reliability.\n5. Containerization: Utilize Docker for building and deploying containerized applications in development and production environments.\n6. Team Leadership: Lead and mentor a team of developers, providing guidance, code reviews, and support to junior team members to ensure high-quality deliverables.\n7. Code Optimization: Write clean, maintainable, and efficient Python code, with a focus on scalability, reusability, and performance.\n8. Cloud Deployment: Deploy and manage applications on cloud platforms like Azure, ensuring high availability and fault tolerance.\n9. Collaboration: Work closely with cross-functional teams, including product managers and designers, to translate business requirements into technical solutions.\n10. Documentation: Maintain thorough documentation for APIs, processes, and systems to ensure transparency and ease of maintenance.\nHighlighted Skillset:-\nBig Data: Strong PySpark skills for processing large datasets.\nDevOps: Proficiency in GitHub Actions, CI/CD pipelines, Azure DevOps, and Docker.\nIntegration: Experience in backend-to-frontend feature connectivity.\nLeadership: Proven ability to lead and mentor development teams.\nCloud: Knowledge of deploying and managing applications in Azure or other cloud environments.\nTeam Collaboration: Strong interpersonal and communication skills for working in cross-functional teams.\nBest Practices: Emphasis on clean code, performance optimization, and robust documentation.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Pyspark', 'Docker', 'Github', 'FastAPI', 'Ci/Cd']",2025-06-14 06:02:04
Gcp Data Engineer,TELUS International,5 - 10 years,17-30 Lacs P.A.,[],"JD - Required skills:\n5+ years of industry experience in the field of Data Engineering support and enhancement.\nProficient in Google Cloud Platform (GCP) services such as Dataflow, BigQuery, Cloud Storage and Pub/Sub.\nStrong understanding of data pipeline architectures and ETL processes.\nExperience with Python programming language in terms of data processing.\nKnowledge of SQL and experience with relational databases.",,,,"['Data Pipeline', 'Bigquery', 'Data Flow', 'Gcp Cloud', 'Python', 'SQL']",2025-06-14 06:02:06
Big Data Engineer,Rarr Technologies,6 - 8 years,Not Disclosed,['Bengaluru'],"Job description\nProven experience working with data pipelines ETL BI regardless of the technology\nProven experience working with AWS including at least 3 of RedShift S3 EMR Cloud Formation DynamoDB RDS lambda\nBig Data technologies and distributed systems one of Spark Presto or Hive\nPython language scripting and object oriented\nFluency in SQL for datawarehousing RedShift in particular is a plus\nGood undesrtanding on datawarehousing and Data modelling concepts\nFamiliar with GIT Linux CICD pipelines is a plus\nStrong systemsprocess orientation with demonstrated analytical thinking organization skills and problemsolving skills\nAbility to selfmanage prioritize and execute tasks in a demanding environment\nStrong consultancy orientation and experience with the ability to form collaborative productive working relationships across diverse teams and cultures is a must\nWillingness and ability to train and teach others\nAbility to facilitate meetings and follow up with resulting action items\nPython Scripting, Etl, Big Data, Aws",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['GIT', 'Linux', 'Analytical', 'big data', 'Distribution system', 'AWS', 'Data warehousing', 'SQL', 'Python', 'Scripting']",2025-06-14 06:02:08
Senior Data Science Consultant - Optimization Projects,Leading Client,8 - 10 years,Not Disclosed,['Jaipur'],"Role Summary :\n\nWe are seeking a highly skilled Senior Data Science Consultant with 8+ years of experience to lead an internal optimization initiative.\n\nThe ideal candidate should have a strong background in data science, operations research, and mathematical optimization, with a proven track record of applying these skills to solve complex business problems.\n\nThis role requires a blend of technical depth, business acumen, and collaborative communication.\n\nA background in internal efficiency/operations improvement or cost/resource optimization projects is highly desirable.\n\nKey Responsibilities :\n\n- Lead and contribute to internal optimization-focused data science projects from design to deployment.\n\n- Develop and implement mathematical models to optimize resource allocation, process performance, and decision-making.\n\n- Use techniques such as linear programming, mixed-integer programming, heuristic and metaheuristic algorithms.\n\n- Collaborate with business stakeholders to gather requirements and translate them into data science use cases.\n\n- Build robust data pipelines and use statistical and machine learning methods to drive insights.\n\n- Communicate complex technical findings in a clear, concise manner to both technical and non-technical audiences.\n\n- Mentor junior team members and contribute to knowledge sharing and best practices within the team.\n\nRequired Skills And Qualifications :\n\n- Masters or PhD in Data Science, Computer Science, Operations Research, Applied Mathematics, or related fields.\n\n- Minimum 8 years of relevant experience in data science, with a strong focus on optimization.\n\n- Expertise in Python (NumPy, Pandas, SciPy, Scikit-learn), SQL, and optimization libraries such as PuLP, Pyomo, Gurobi, or CPLEX.\n\n- Experience with end-to-end lifecycle of internal optimization projects.\n\n- Strong analytical and problem-solving skills.\n\n- Excellent communication and stakeholder management abilities.\n\nPreferred Qualifications :\n\n- Experience working on internal company projects focused on logistics, resource planning, workforce optimization, or cost reduction.\n\n- Exposure to tools/platforms like Databricks, Azure ML, or AWS SageMaker.\n\n- Familiarity with dashboards and visualization tools like Power BI or Tableau.\n\n- Prior experience in consulting or internal centers of excellence (CoE) is a plus.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Data Scientist', 'Operations Research', 'Pandas', 'Numpy', 'Scikit-Learn', 'Python', 'SQL']",2025-06-14 06:02:10
Senior Data Science Consultant - Optimization Projects,Leading Client,8 - 10 years,Not Disclosed,['Chennai'],"Role Summary :\n\nWe are seeking a highly skilled Senior Data Science Consultant with 8+ years of experience to lead an internal optimization initiative.\n\nThe ideal candidate should have a strong background in data science, operations research, and mathematical optimization, with a proven track record of applying these skills to solve complex business problems.\n\nThis role requires a blend of technical depth, business acumen, and collaborative communication.\n\nA background in internal efficiency/operations improvement or cost/resource optimization projects is highly desirable.\n\nKey Responsibilities :\n\n- Lead and contribute to internal optimization-focused data science projects from design to deployment.\n\n- Develop and implement mathematical models to optimize resource allocation, process performance, and decision-making.\n\n- Use techniques such as linear programming, mixed-integer programming, heuristic and metaheuristic algorithms.\n\n- Collaborate with business stakeholders to gather requirements and translate them into data science use cases.\n\n- Build robust data pipelines and use statistical and machine learning methods to drive insights.\n\n- Communicate complex technical findings in a clear, concise manner to both technical and non-technical audiences.\n\n- Mentor junior team members and contribute to knowledge sharing and best practices within the team.\n\nRequired Skills And Qualifications :\n\n- Masters or PhD in Data Science, Computer Science, Operations Research, Applied Mathematics, or related fields.\n\n- Minimum 8 years of relevant experience in data science, with a strong focus on optimization.\n\n- Expertise in Python (NumPy, Pandas, SciPy, Scikit-learn), SQL, and optimization libraries such as PuLP, Pyomo, Gurobi, or CPLEX.\n\n- Experience with end-to-end lifecycle of internal optimization projects.\n\n- Strong analytical and problem-solving skills.\n\n- Excellent communication and stakeholder management abilities.\n\nPreferred Qualifications :\n\n- Experience working on internal company projects focused on logistics, resource planning, workforce optimization, or cost reduction.\n\n- Exposure to tools/platforms like Databricks, Azure ML, or AWS SageMaker.\n\n- Familiarity with dashboards and visualization tools like Power BI or Tableau.\n\n- Prior experience in consulting or internal centers of excellence (CoE) is a plus.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Data Scientist', 'Operations Research', 'Pandas', 'Numpy', 'Scikit-Learn', 'Python', 'SQL']",2025-06-14 06:02:12
Senior Consultant - Data Science,Arcadis,8 - 13 years,Not Disclosed,['Bengaluru'],"Qualification & Experience:\nMinimum of 8 years of experience as a Data Scientist/Engineer with demonstrated expertise in data engineering and cloud computing technologies.\nTechnical Responsibilities\nExcellent proficiency in Python, with a strong focus on developing advanced skills.\nExtensive exposure to NLP and image processing concepts.\nProficient in version control systems like Git.\nIn-depth understanding of Azure deployments.\nExpertise in OCR, ML model training, and transfer learning.\nExperience working with unstructured data formats such as PDFs, DOCX, and images. O\nStrong familiarity with data science best practices and the ML lifecycle.\nStrong experience with data pipeline development, ETL processes, and data engineering tools such as Apache Airflow, PySpark, or Databricks.\nFamiliarity with cloud computing platforms like Azure, AWS, or GCP, including services like Azure Data Factory, S3, Lambda, and BigQuery.\nTool Exposure: Advanced understanding and hands-on experience with Git, Azure, Python, R programming and data engineering tools such as Snowflake, Databricks, or PySpark.\nData mining, cleaning and engineering: Leading the identification and merging of relevant data sources, ensuring data quality, and resolving data inconsistencies.\nCloud Solutions Architecture: Designing and deploying scalable data engineering workflows on cloud platforms such as Azure, AWS, or GCP.\nData Analysis: Executing complex analyses against business requirements using appropriate tools and technologies.\nSoftware Development: Leading the development of reusable, version-controlled code under minimal supervision.\nBig Data Processing: Developing solutions to handle large-scale data processing using tools like Hadoop, Spark, or Databricks.\nPrincipal Duties & Key Responsibilities:\nLeading data extraction from multiple sources, including PDFs, images, databases, and APIs.\nDriving optical character recognition (OCR) processes to digitize data from images.\nApplying advanced natural language processing (NLP) techniques to understand complex data.\nDeveloping and implementing highly accurate statistical models and data engineering pipelines to support critical business decisions and continuously monitor their performance.\nDesigning and managing scalable cloud-based data architectures using Azure, AWS, or GCP services.\nCollaborating closely with business domain experts to identify and drive key business value drivers.\nDocumenting model design choices, algorithm selection processes, and dependencies.\nEffectively collaborating in cross-functional teams within the CoE and across the organization.\nProactively seeking opportunities to contribute beyond assigned tasks.\nRequired Competencies:\nExceptional communication and interpersonal skills.\nProficiency in Microsoft Office 365 applications.\nAbility to work independently, demonstrate initiative, and provide strategic guidance.\nStrong networking, communication, and people skills.\nOutstanding organizational skills with the ability to work independently and as part of a team.\nExcellent technical writing skills.\nEffective problem-solving abilities.\nFlexibility and adaptability to work flexible hours as required.\nKey competencies / Values:\nClient Focus: Tailoring skills and understanding client needs to deliver exceptional results.\nExcellence: Striving for excellence defined by clients, delivering high-quality work.\nTrust: Building and retaining trust with clients, colleagues, and partners.\nTeamwork: Collaborating effectively to achieve collective success.\nResponsibility: Taking ownership of performance and safety, ensuring accountability.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Big Data Processing', 'Software Development', 'Data cleaning', 'Data engineering', 'Data mining', 'cloud computing', 'OCR']",2025-06-14 06:02:14
Senior Data Science Consultant - Optimization Projects,Leading Client,8 - 10 years,Not Disclosed,['Surat'],"Role Summary :\n\nWe are seeking a highly skilled Senior Data Science Consultant with 8+ years of experience to lead an internal optimization initiative.\n\nThe ideal candidate should have a strong background in data science, operations research, and mathematical optimization, with a proven track record of applying these skills to solve complex business problems.\n\nThis role requires a blend of technical depth, business acumen, and collaborative communication.\n\nA background in internal efficiency/operations improvement or cost/resource optimization projects is highly desirable.\n\nKey Responsibilities :\n\n- Lead and contribute to internal optimization-focused data science projects from design to deployment.\n\n- Develop and implement mathematical models to optimize resource allocation, process performance, and decision-making.\n\n- Use techniques such as linear programming, mixed-integer programming, heuristic and metaheuristic algorithms.\n\n- Collaborate with business stakeholders to gather requirements and translate them into data science use cases.\n\n- Build robust data pipelines and use statistical and machine learning methods to drive insights.\n\n- Communicate complex technical findings in a clear, concise manner to both technical and non-technical audiences.\n\n- Mentor junior team members and contribute to knowledge sharing and best practices within the team.\n\nRequired Skills And Qualifications :\n\n- Masters or PhD in Data Science, Computer Science, Operations Research, Applied Mathematics, or related fields.\n\n- Minimum 8 years of relevant experience in data science, with a strong focus on optimization.\n\n- Expertise in Python (NumPy, Pandas, SciPy, Scikit-learn), SQL, and optimization libraries such as PuLP, Pyomo, Gurobi, or CPLEX.\n\n- Experience with end-to-end lifecycle of internal optimization projects.\n\n- Strong analytical and problem-solving skills.\n\n- Excellent communication and stakeholder management abilities.\n\nPreferred Qualifications :\n\n- Experience working on internal company projects focused on logistics, resource planning, workforce optimization, or cost reduction.\n\n- Exposure to tools/platforms like Databricks, Azure ML, or AWS SageMaker.\n\n- Familiarity with dashboards and visualization tools like Power BI or Tableau.\n\n- Prior experience in consulting or internal centers of excellence (CoE) is a plus.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Data Scientist', 'Operations Research', 'Pandas', 'Numpy', 'Scikit-Learn', 'Python', 'SQL']",2025-06-14 06:02:17
Senior Data Science Consultant - Optimization Projects,Leading Client,8 - 10 years,Not Disclosed,['Kolkata'],"Role Summary :\n\nWe are seeking a highly skilled Senior Data Science Consultant with 8+ years of experience to lead an internal optimization initiative.\n\nThe ideal candidate should have a strong background in data science, operations research, and mathematical optimization, with a proven track record of applying these skills to solve complex business problems.\n\nThis role requires a blend of technical depth, business acumen, and collaborative communication.\n\nA background in internal efficiency/operations improvement or cost/resource optimization projects is highly desirable.\n\nKey Responsibilities :\n\n- Lead and contribute to internal optimization-focused data science projects from design to deployment.\n\n- Develop and implement mathematical models to optimize resource allocation, process performance, and decision-making.\n\n- Use techniques such as linear programming, mixed-integer programming, heuristic and metaheuristic algorithms.\n\n- Collaborate with business stakeholders to gather requirements and translate them into data science use cases.\n\n- Build robust data pipelines and use statistical and machine learning methods to drive insights.\n\n- Communicate complex technical findings in a clear, concise manner to both technical and non-technical audiences.\n\n- Mentor junior team members and contribute to knowledge sharing and best practices within the team.\n\nRequired Skills And Qualifications :\n\n- Masters or PhD in Data Science, Computer Science, Operations Research, Applied Mathematics, or related fields.\n\n- Minimum 8 years of relevant experience in data science, with a strong focus on optimization.\n\n- Expertise in Python (NumPy, Pandas, SciPy, Scikit-learn), SQL, and optimization libraries such as PuLP, Pyomo, Gurobi, or CPLEX.\n\n- Experience with end-to-end lifecycle of internal optimization projects.\n\n- Strong analytical and problem-solving skills.\n\n- Excellent communication and stakeholder management abilities.\n\nPreferred Qualifications :\n\n- Experience working on internal company projects focused on logistics, resource planning, workforce optimization, or cost reduction.\n\n- Exposure to tools/platforms like Databricks, Azure ML, or AWS SageMaker.\n\n- Familiarity with dashboards and visualization tools like Power BI or Tableau.\n\n- Prior experience in consulting or internal centers of excellence (CoE) is a plus.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Data Scientist', 'Operations Research', 'Pandas', 'Numpy', 'Scikit-Learn', 'Python', 'SQL']",2025-06-14 06:02:19
Gcp Data Engineer,V2soft,3 - 8 years,Not Disclosed,"['Chennai', 'Sholinganallur']","Position Description:Bachelors Degree 2+Years in GCP Services - Biq Query, Data Flow, Dataproc, DataPlex,DataFusion, Terraform, Tekton, Cloud SQL, Redis Memory, Airflow, Cloud Storage 2+ Years inData Transfer Utilities 2+ Years in Git / any other version control tool 2+ Years in Confluent Kafka1+ Years of Experience in API Development 2+ Years in Agile Framework 4+ years of strongexperience in python, Pyspark development. 4+ years of shell scripting to develop the adhoc jobsfor data importing/exportingSkills Required:Google Cloud Platform - Biq Query, Data Flow, Dataproc, Data Fusion, TERRAFORM, Tekton,Cloud SQL, AIRFLOW, POSTGRES, Airflow PySpark, Python, API",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['GCP', 'Python', 'Airflow', 'Pyspark', 'Terraform']",2025-06-14 06:02:21
Backend Data Engineer,Squarepeg Hires,3 - 8 years,Not Disclosed,['Bengaluru'],"Voicecare AI is a Healthcare Administration General Intelligence (HAGI) company for the back-office and the RCM industry. We are building a safety focused large and small conversational language model for the healthcare industry. Our mission is to dramatically improve access, adherence, and outcomes for the patients and the healthcare workforce through the application of generative AI. We are a venture-backed company partnering with the top healthcare stakeholders in the country.\n\nWe are seeking a Back-end Data Engineer with deep expertise in GCP, strong Python coding skills, and a security-first mindset. This role is critical in designing, implementing, and optimizing secure and scalable data pipelines to support AI-driven healthcare applications. The ideal candidate will have a strong background in cloud data architecture, big data processing, and compliance with healthcare security standards.\n\nKey Responsibilities:\n\nData Pipeline Development & Optimization\nDesign and develop scalable, secure, and high-performance data pipelines on GCP.\nImplement ETL/ELT workflows using tools like Dataflow and BigQuery.\nOptimize data processing for latency, cost-efficiency, and compliance with healthcare standards.\nCloud Data Architecture & GCP Expertise\nArchitect and manage data storage solutions (BigQuery, Cloud Storage, Spanner, Firestore) with a focus on security and performance.\nLeverage GCP-native services to build fault-tolerant, distributed data architectures.\nImplement data governance and access controls using IAM, VPC, and encryption best practices.\nAutomation & Infrastructure as Code (IaC)\nDevelop and maintain Infrastructure as Code (IaC) using Terraform, Deployment Manager, or CloudFormation.\nAutomate data engineering workflows using Python, and CI/CD pipelines.\nPerformance Monitoring & Optimization\nImplement logging, monitoring, and alerting using Cloud Logging, Cloud Monitoring, and other advanced techniques\nContinuously monitor and optimize data queries, storage utilization, and processing speed.\nCollaboration & Cross-Functional Integration\nWork closely with AI/ML engineers, full-stack engineers, quality engineering and data engineers to integrate AI-driven insights into healthcare applications.\nProvide expertise on data modeling, schema design, and efficient query execution.\nSkills & Experience:\nGCP Expertise: Deep experience with BigQuery, Dataflow, Cloud Composer (Airflow), Vertex AI, Pub/Sub, Cloud SQL, Spanner, and Firestore.\nPython Programming: Strong proficiency in Python for data processing, automation, and API development.\nSecurity-First Mindset: Hands-on experience with data encryption, access controls, and regulatory compliance.\nETL/ELT & Data Pipelines: Expertise in building scalable and secure data processing pipelines.\nMonitoring & Optimization: Experience with Cloud Logging, Cloud Monitoring, and SQL performance tuning.\nHas worked across multiple tech-stacks, and thrived in collaborating with Data Science, Frontend/Applications and Infra teams.\nStrong knowledge of database technologies, both SQL and NoSQL and experience in optimizing database performance.\n3 years of hands-on engineering experience, with demonstrated growth\nHas shown entrepreneurial spirit (e.g. worked at an early stage startup, or has started and delivered on a new product from the ground-up).\nHealthcare Industry Knowledge (Preferred): Familiarity with healthcare data standards (FHIR, HL7), compliance regulations, and PHI security best practices.\nQualifications\nBachelor s or Master s degree in Computer Science, Data Engineering, or a related field.\n3+ years of experience in data engineering, cloud architecture, and security-first data processing.\nGCP Certifications preferred (e.g., Professional Data Engineer, Professional Cloud Architect, Security Engineer).\nExperience working in healthcare AI, health tech startups, or enterprise healthcare IT environments is a plus.\nStrong portfolio of cloud-based data engineering projects demonstrating security best practices.\nCandidates located in Bangalore is a plus\nCandidates from top universities is a plus",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Performance tuning', 'Automation', 'Backend', 'Data modeling', 'Coding', 'Infrastructure', 'Healthcare', 'SQL', 'Python']",2025-06-14 06:02:23
Data Engineer,Big 4 Accounting Firms,5 - 10 years,Not Disclosed,"['Gurugram', 'Bengaluru', 'Delhi / NCR']","Bachelors or higher degree in Computer Science or a related discipline; or equivalent (minimum 4 years work\nexperience).\n• At least 2+ years of consulting or client service delivery experience on Azure Data Solution\n• At least 2+ years of experience in developing data ingestion, data processing and analytical pipelines\nfor big data, relational databases such as SQL server and data warehouse solutions such as Azure\nSynapse\n• Extensive experience providing practical direction with using Azure Native services.\n• Extensive hands-on experience implementing data ingestion, ETL and data processing using Azure\nservices: ADLS, Azure Data Factory, Azure Functions, Azure Logic App Synapse/DW, Azure SQL DB,\nDatabricks etc.\n• Experience in Data Analysis, data debugging, problem solving skills and business requirement\nunderstanding.\n• Minimum of 2+ years of hands-on experience in Azure and Big Data technologies such as Java, Python, SQL,\nADLS/Blob, PySpark and SparkSQL, Databricks, HD Insight\n• Well versed in DevSecOps and CI/CD deployments\n• Experience in using Big Data File Formats and compression techniques.\n• Experience working with Developer tools such as Azure DevOps, Visual Studio Team Server, Git\n• Experience with private and public cloud architecture, pros/cons and migration considerations.Role & responsibilities\n\n\nPreferred candidate profile",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Azure Data Factory', 'Pyspark', 'Azure Databricks', 'SQL', 'Python']",2025-06-14 06:02:25
Senior Consultant Data Science,Arcadis,8 - 13 years,Not Disclosed,['Bengaluru'],"Qualification & Experience:\nMinimum of 8 years of experience as a Data Scientist/Engineer with demonstrated expertise in data engineering and cloud computing technologies.\nTechnical Responsibilities\nExcellent proficiency in Python, with a strong focus on developing advanced skills.\nExtensive exposure to NLP and image processing concepts.\nProficient in version control systems like Git.\nIn-depth understanding of Azure deployments.\nExpertise in OCR, ML model training, and transfer learning.\nExperience working with unstructured data formats such as PDFs, DOCX, and images. O\nStrong familiarity with data science best practices and the ML lifecycle.\nStrong experience with data pipeline development, ETL processes, and data engineering tools such as Apache Airflow, PySpark, or Databricks.\nFamiliarity with cloud computing platforms like Azure, AWS, or GCP, including services like Azure Data Factory, S3, Lambda, and BigQuery.\nTool Exposure: Advanced understanding and hands-on experience with Git, Azure, Python, R programming and data engineering tools such as Snowflake, Databricks, or PySpark.\nData mining, cleaning and engineering: Leading the identification and merging of relevant data sources, ensuring data quality, and resolving data inconsistencies.\nCloud Solutions Architecture: Designing and deploying scalable data engineering workflows on cloud platforms such as Azure, AWS, or GCP.\nData Analysis: Executing complex analyses against business requirements using appropriate tools and technologies.\nSoftware Development: Leading the development of reusable, version-controlled code under minimal supervision.\nBig Data Processing: Developing solutions to handle large-scale data processing using tools like Hadoop, Spark, or Databricks.\nPrincipal Duties & Key Responsibilities:\nLeading data extraction from multiple sources, including PDFs, images, databases, and APIs.\nDriving optical character recognition (OCR) processes to digitize data from images.\nApplying advanced natural language processing (NLP) techniques to understand complex data.\nDeveloping and implementing highly accurate statistical models and data engineering pipelines to support critical business decisions and continuously monitor their performance.\nDesigning and managing scalable cloud-based data architectures using Azure, AWS, or GCP services.\nCollaborating closely with business domain experts to identify and drive key business value drivers.\nDocumenting model design choices, algorithm selection processes, and dependencies.\nEffectively collaborating in cross-functional teams within the CoE and across the organization.\nProactively seeking opportunities to contribute beyond assigned tasks.\nRequired Competencies:\nExceptional communication and interpersonal skills.\nProficiency in Microsoft Office 365 applications.\nAbility to work independently, demonstrate initiative, and provide strategic guidance.\nStrong networking, communication, and people skills.\nOutstanding organizational skills with the ability to work independently and as part of a team.\nExcellent technical writing skills.\nEffective problem-solving abilities.\nFlexibility and adaptability to work flexible hours as required.\nKey competencies / Values:\nClient Focus: Tailoring skills and understanding client needs to deliver exceptional results.\nExcellence: Striving for excellence defined by clients, delivering high-quality work.\nTrust: Building and retaining trust with clients, colleagues, and partners.\nTeamwork: Collaborating effectively to achieve collective success.\nResponsibility: Taking ownership of performance and safety, ensuring accountability.\nPeople: Creating an inclusive environment that fosters individual growth and development.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Azure Data Factory', 'S3', 'Big Query', 'Azure', 'GCP', 'Hadoop', 'Spark', 'Databricks', 'ETL', 'Lambda', 'AWS']",2025-06-14 06:02:27
Data Engineer,BAY Area Technology Solutions,3 - 5 years,Not Disclosed,"['Mumbai', 'Delhi / NCR', 'Bengaluru']","Strong on programming languages like Python, Java\nMust have one cloud hands-on experience (GCP preferred)\nMust have: Experience working with Dockers\nMust have: Environments managing (e.g venv, pip, poetry, etc.)\nMust have: Experience with orchestrators like Vertex AI pipelines, Airflow, etc\nMust have: Data engineering, Feature Engineering techniques\nProficient in either Apache Spark or Apache Beam or Apache Flink\nMust have: Advance SQL knowledge\nMust be aware of Streaming concepts like Windowing , Late arrival , Triggers etc\nShould have hands-on experience on Distributed computing\nShould have working experience on Data Architecture design\nShould be aware of storage and compute options and when to choose what\nShould have good understanding on Cluster Optimisation/ Pipeline Optimisation strategies\nShould have exposure on GCP tools to develop end to end data pipeline for various scenarios (including ingesting data from traditional data bases as well as integration of API based data sources).\nShould have Business mindset to understand data and how it will be used for BI and Analytics purposes.\nShould have working experience on CI/CD pipelines, Deployment methodologies, Infrastructure as a code (eg. Terraform)\nGood to have, Hands-on experience on Kubernetes\nGood to have Vector based Database like Qdrant\nExperience in Working with GCP tools like:\nStorage : CloudSQL , Cloud Storage, Cloud Bigtable, Bigquery, Cloud Spanner, Cloud DataStore, Vector database\nIngest : Pub/Sub, Cloud Functions, AppEngine, Kubernetes Engine, Kafka, Micro services\nSchedule : Cloud Composer, Airflow\nProcessing: Cloud Dataproc, Cloud Dataflow, Apache Spark, Apache Flink\nCI/CD : Bitbucket+Jenkinjs / Gitlab ,Infrastructre as a tool : Terraform\nLocations : Mumbai, Delhi / NCR, Bengaluru , Kolkata, Chennai, Hyderabad, Ahmedabad, Pune, Remote",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Java', 'Docker', 'GCP', 'CI/CD', 'Poetry', 'Apache', 'Python']",2025-06-14 06:02:29
Data Engineer,Ortseam Technologies,5 - 10 years,Not Disclosed,[],"Job Description\n\nJob Title: Offshore Data Engineer\nBase Location: Bangalore\nWork Mode: Remote\nExperience: 5+ Years\n\nJob Description: We are looking for a skilled Offshore Data Engineer with strong experience in Python, SQL, and Apache Beam. Familiarity with Java is a plus. The ideal candidate should be self-driven, collaborative, and able to work in a fast-paced environment.\n\nKey Responsibilities:\n\nDesign and implement reusable, scalable ETL frameworks using Apache Beam and GCP Dataflow.\nDevelop robust data ingestion and transformation pipelines using Python and SQL.\nIntegrate Kafka for real-time data streams alongside batch workloads.\nOptimize pipeline performance and manage costs within GCP services.\nWork closely with data analysts, data architects, and product teams to gather and understand data requirements.\nManage and monitor BigQuery datasets, tables, and partitioning strategies.\nImplement error handling, resiliency, and observability mechanisms across pipeline components.\nCollaborate with DevOps teams to enable automated delivery (CI/CD) for data pipeline components.\n\nRequired Skills:\n\n5+ years of hands-on experience in Data Engineering or Software Engineering.\nProficiency in Python and SQL.\nGood understanding of Java (for reading or modifying codebases).\nExperience building ETL pipelines with Apache Beam and Google Cloud Dataflow.\nHands-on experience with Apache Kafka for stream processing.\nSolid understanding of BigQuery and data modeling on GCP.\nExperience with GCP services (Cloud Storage, Pub/Sub, Cloud Compose, etc.).\n\nGood to Have:\n\nExperience building reusable ETL libraries or framework components.\nKnowledge of data governance, data quality checks, and pipeline observability.\nFamiliarity with Apache Airflow or Cloud Composer for orchestration.\nExposure to CI/CD practices in a cloud-native environment (Docker, Terraform, etc.).\n\nTech stack: \n\nPython, SQL, Java, GCP (BigQuery, Pub/Sub, Cloud Storage, Cloud Compose, Dataflow), Apache Beam, Apache Kafka, Apache Airflow, CI/CD (Docker, Terraform)",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Bigquery', 'Kafka', 'SQL', 'GCP', 'Python', 'Airflow', 'Java', 'Pubsub', 'Data Engineering', 'Gcp Dataflow', 'Cloud Storage', 'Data Pipeline', 'Data Flow', 'Cicd Pipeline', 'Apache Beam', 'Google Cloud Platforms']",2025-06-14 06:02:32
Data Analyst (USA Healthcare System) - Technical Lead,BILVANTIS TECHNOLOGIES,8 - 13 years,Not Disclosed,['Hyderabad'],"Data Analyst (USA Healthcare System) - Technical Lead\nQualifications:\n8+ years of experience in data analysis, preferably in the healthcare or fintech industry, with a strong understanding of the US healthcare system, including the roles and interactions between providers, payers, and patients.\nStrong proficiency in SQL and experience with data visualization tools (e.g., Power BI, Sigma), with the ability to analyze and present complex healthcare data effectively.\nProficiency with Snowflake and DBT (40 % of the role), leveraging these tools to manage and analyze large datasets within the healthcare domain.\nBasic knowledge of Python for data analysis tasks (40 % of the role), including data manipulation and automation of routine tasks.\nExperience with Datawarehouse and MDM systems, particularly in managing provider data, including provider demographic services and credentialing services.\nExcellent analytical and problem-solving skills with keen attention to detail, particularly in identifying trends and patterns in healthcare data.\nStrong leadership and mentoring abilities. Ability to manage and prioritize multiple projects in a dynamic, fast-paced environment.\nUnderstanding of data privacy and compliance regulations, particularly in the healthcare industry, including key healthcare legislation and regulations such as the Affordable Care Act (ACA), Medicare, Medicaid, and the Health Insurance Portability and Accountability Act (HIPAA).\nKnowledge of third-party data systems such as Dun Bradstreet (DB) and the Centers for Medicare Medicaid Services (CMS) for provider data management.\nAbility to manage multiple tasks and projects simultaneously in a fast-paced environment, with a focus on improving provider data management processes and outcomes.\nAbility to manage multiple tasks and projects simultaneously in a fast-paced environment.\nKey Responsibilities:\nPartner with Product Owners, Product Managers, and the Director of DI (Provider) to define and prioritize data strategies that drive impactful product development in the US Healthcare Providers domain.\nAct as a data liaison, providing actionable insights to stakeholders and guiding the integration of diverse data sources into product workflows.\nPerform in-depth analyses of provider data to identify trends, patterns, and actionable insights that influence business strategies.\nImplement and lead comprehensive data quality assurance processes, ensuring the accuracy and consistency of provider data.\nSupport provider demographic services, network solutions and credentialing services by leveraging data from existing Datawarehouse, MDM data stores.\nWork closely with the IT and Data Engineering teams to ensure data is stored, processed, and accessed in a compliant and secure manner, incorporating data from identified sources.\nAssist in the development and implementation of data governance policies and procedures, ensuring the inclusion of data from relevant sources.\nMentor junior analysts and provide guidance on best practices in data analysis, governance, and visualization.\nStay up to date with industry trends and best practices in data analysis, provider data management, and healthcare fintech, and continuously identify new data sets to enhance outcomes.\nPreferred Qualifications:\nExperience with healthcare data standards and regulations (e.g., HIPAA), ensuring compliance and data integrity in all data management activities.\nFamiliarity with data governance frameworks and best practices, particularly in the context of healthcare data management.\nAdvanced degree in Data Science, Computer Science, Information Systems, or a related field.\nIn-depth knowledge of the US healthcare system, including:\nHealthcare Triangle: Understanding the roles and interactions between providers, payers, and patients.\nProvider Networks: Knowledge of how provider networks are structured and managed, including credentialing and contracting processes.\nRegulatory Environment: Awareness of key healthcare legislation and regulations, such as the Affordable Care Act (ACA), Medicare, Medicaid, and the Health Insurance Portability Act and Accountability Act (HIPAA).\nReimbursement Models: Understanding various reimbursement models, including fee-for-service, value-based care, and capitation.\nHealthcare Data Standards: Knowledge of standards such as ICD-10, CPT, and HL7.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Health insurance', 'Data analysis', 'Automation', 'Manager Quality Assurance', 'Data management', 'Analytical', 'Technical Lead', 'Healthcare', 'US healthcare', 'SQL']",2025-06-14 06:02:34
Senior Data Modeler - Technical Lead,BILVANTIS TECHNOLOGIES,7 - 10 years,Not Disclosed,['Hyderabad'],"Senior Data Modeler - Technical Lead:\n\nSenior Data Modeler designs and optimizes enterprise data models and data warehouse architectures for analytics and reporting. They collaborate with engineers and analysts to ensure scalability, data integrity, and governance best practices. Expertise in dimensional modeling, SQL optimization, and cloud data warehouses is essential.\n\nEXPERIENCE REQUIREMENTS:\n\nBachelor s degree in computer science, Information Systems, or a related field.\n8+ years of experience in data modeling, data architecture, and enterprise DWH development.\nStrong expertise in at least one Columnar MPP Cloud Data Warehouse (Snowflake, Azure Synapse, Redshift).\nExperience with ETL/ELT tools (Azure Data Factory, DBT, Fivetran) to support data model implementation.\nAdvanced SQL development skills, including query optimization, stored procedures, and indexing strategies.\nHands-on experience in dimensional modeling, data vault modeling, and OLAP architectures.\nExperience working with data modeling tools (Erwin, DBT, SQL DB Modeler, or similar).\nFamiliarity with data governance frameworks, data lineage tracking, and MDM solutions.\nExperience in agile development processes using Jira and Confluence.\n\nPreferred Qualifications:\n\nKnowledge of Python for data transformation and automation.\nDomain expertise in healthcare data (provider credentialing, claims processing, payer networks).\n\nKEY RESPONSIBILITIES:\n\nData Modeling Architecture:\nDesign and develop conceptual, logical, and physical data models to support enterprise-wide DWH, data marts, and OLAP cubes for analytics and reporting.\nImplement dimensional modeling (star/snowflake schemas) for analytical workloads and normalized models (3NF) for operational data stores (ODS).\nOptimize data structures for performance, scalability, and maintainability, ensuring they support complex analytical queries.\nCreate and maintain data dictionaries, metadata, and ER diagrams to document models effectively.\nDefine slowly changing dimensions (SCD), surrogate keys, fact tables, and aggregates to enhance analytical reporting.\nData Warehouse Development Optimization:\nArchitect and implement Enterprise Data Warehousing (EDW) solutions to consolidate and centralize business data.\nWork with Data Engineers to develop ETL/ELT strategies for ingesting, transforming, and storing data efficiently in cloud DWH environments.\nOptimize data partitioning, clustering, indexing, and query performance tuning within Snowflake, Redshift, or Azure Synapse.\nDevelop and maintain data pipelines to support real-time and batch data processing.\nDefine data retention, archiving, and purging strategies to optimize storage costs.\nData Governance, Quality, Compliance:\nEstablish data governance best practices to ensure accuracy, consistency, and security across all data assets.\nImplement data lineage tracking, cataloging, and MDM (Master Data Management) strategies to improve data discoverability.\nWork with compliance teams to enforce security, access control, and regulatory compliance (HIPAA, GDPR, SOC 2, etc.) in data management.\nDevelop data quality frameworks to monitor anomalies, enforce validation rules, and handle missing or inconsistent data.\nDocumentation Knowledge Sharing:\nCreate and maintain comprehensive documentation, including data flow diagrams, transformation logic, mapping documents, and ETL specifications.\nConduct data model reviews, knowledge-sharing sessions, and training for engineering and analytics teams.\nStay updated with industry trends in DWH architectures, cloud data platforms, and data modeling tools.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Performance tuning', 'Automation', 'SOC', 'Technical Lead', 'Data structures', 'Healthcare', 'Stored procedures', 'Analytics', 'SQL', 'Python']",2025-06-14 06:02:36
Big Data Engineer - Python/PySpark,Qcentrio,6 - 10 years,Not Disclosed,['Bengaluru'],"Work Location : Bangalore (CV Ramen Nagar location)\n\nNotice Period : Immediate - 30 days\n\nMandatory Skills : Big Data, Python, SQL, Spark/Pyspark, AWS Cloud\n\nJD and required Skills & Responsibilities :\n\n- Actively participate in all phases of the software development lifecycle, including requirements gathering, functional and technical design, development, testing, roll-out, and support.\n\n- Solve complex business problems by utilizing a disciplined development methodology.\n\n- Produce scalable, flexible, efficient, and supportable solutions using appropriate technologies.\n\n- Analyse the source and target system data. Map the transformation that meets the requirements.\n\n- Interact with the client and onsite coordinators during different phases of a project.\n\n- Design and implement product features in collaboration with business and Technology stakeholders.\n\n- Anticipate, identify, and solve issues concerning data management to improve data quality.\n\n- Clean, prepare, and optimize data at scale for ingestion and consumption.\n\n- Support the implementation of new data management projects and re-structure the current data architecture.\n\n- Implement automated workflows and routines using workflow scheduling tools.\n\n- Understand and use continuous integration, test-driven development, and production deployment frameworks.\n\n- Participate in design, code, test plans, and dataset implementation performed by other data engineers in support of maintaining data engineering standards.\n\n- Analyze and profile data for the purpose of designing scalable solutions.\n\n- Troubleshoot straightforward data issues and perform root cause analysis to proactively resolve product issues.\n\nRequired Skills :\n\n- 5+ years of relevant experience developing Data and analytic solutions.\n\n- Experience building data lake solutions leveraging one or more of the following AWS, EMR, S3, Hive & PySpark\n\n- Experience with relational SQL.\n\n- Experience with scripting languages such as Python.\n\n- Experience with source control tools such as GitHub and related dev process.\n\n- Experience with workflow scheduling tools such as Airflow.\n\n- In-depth knowledge of AWS Cloud (S3, EMR, Databricks)\n\n- Has a passion for data solutions.\n\n- Has a strong problem-solving and analytical mindset\n\n- Working experience in the design, Development, and test of data pipelines.\n\n- Experience working with Agile Teams.\n\n- Able to influence and communicate effectively, both verbally and in writing, with team members and business stakeholders\n\n- Able to quickly pick up new programming languages, technologies, and frameworks.\n\n- Bachelor's degree in computer science",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Python', 'Hive', 'PySpark', 'Hadoop', 'Big Data', 'Agile', 'Spark', 'AWS EMR', 'SQL']",2025-06-14 06:02:38
Data Engineer,Supersourcing,4 - 8 years,Not Disclosed,"['Noida', 'Pune', 'Bengaluru']","Job Description-\nWe are hiring a Data Engineer with strong expertise in JAVA, Apache Spark and AWS Cloud. You will design and develop high-performance, scalable applications and data pipelines for cloud-based environments.\n\nKey Skills:\n3+ years in Java (Java 8+), Spring Boot, and REST APIs\n3+ years in Apache Spark (Core, SQL, Streaming)\nStrong hands-on with AWS services: S3, EC2, Lambda, Glue, EMR\nExperience with microservices, CI/CD, and Git\nGood understanding of distributed systems and performance tuning\n\nNice to Have:\nExperience with Kafka, Airflow, Docker, or Kubernetes\nAWS Certification (Developer/Architect)",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Java', 'Spark', 'Pyspark', 'SQL']",2025-06-14 06:02:40
Data Analysts,Mainetti,3 - 8 years,Not Disclosed,['Chennai'],"Core Competencies\n1. Attention to Detail\nEnsures data accuracy and consistency across all reports and datasets.\n2. Data Integrity & Accountability\nTakes ownership of data quality; trusted to manage business-critical data (e.g., rebate reports).\n3. Analytical Thinking\nCapable of identifying patterns, anomalies, and actionable insights through data review.",,,,"['Sales Analysis', 'Power Bi Dashboards', 'Data Analysis', 'SQL Queries', 'Advanced Excel']",2025-06-14 06:02:43
Data Engineer,Devdolphins,2 - 7 years,12-24 Lacs P.A.,['Bengaluru'],"Hiring Data Engineer (2+ yrs exp) in Bangalore for onsite client role. Strong in PySpark & Databricks. Exp in ETL/ELT, SQL, data lakes and Azure cloud. Full-time, in-office, immediate joiners only. Salary: 12L - 24L.\n\n\nHealth insurance\nProvident fund",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Pyspark', 'Data Bricks', 'SQL']",2025-06-14 06:02:45
Data Annotation Specialist,Yamaha Motor Solutions,1 - 3 years,3-4.5 Lacs P.A.,['Faridabad'],"We are seeking a highly detail-oriented and technically adept 3D Data Annotation Specialist to join our growing team. This role is critical in shaping high-quality datasets for training cutting-edge AI and computer vision models, particularly in domains such as LiDAR data processing, and 3D object detection.\n\nRoles and Responsibilities\nQualifications:\nB.Tech in Computer Science, IT, or related field preferred (others may also apply strong analytical and software learning abilities required).\nStrong analytical and reasoning skills, with attention to spatial geometry and object relationships in 3D space.\nBasic understanding of 3D data formats (e.g., .LAS, .LAZ, .PLY) and visualization tools.\nAbility to work independently while maintaining high-quality standards.\nExcellent communication skills and the ability to collaborate in a fast-paced environment.\nAttention to detail and ability to work with precision in visual/manual tasks.\nGood understanding of basic geometry, coordinate systems, and file handling.\nPreferred Qualifications:\nPrior experience in 3D data annotation or LiDAR data analysis.\nExposure to computer vision workflows.\nComfortable working with large datasets and remote sensing data\nKey Responsibilities:\nAnnotate 3D point cloud data with precision using specialized tools [ Training would be provided]\nLabel and segment objects within LiDAR data, aerial scans, or 3D models.\nFollow annotation guidelines while applying logical and spatial reasoning to 3D environments.\nCollaborate with ML engineers and data scientists to ensure annotation accuracy and consistency.\nProvide feedback to improve annotation tools and workflow automation.\nParticipate in quality control reviews and conduct re-annotation as needed",Industry Type: Automobile,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['annotation', 'python', 'data analysis', 'data analytics', 'modeling', 'analytical', 'software', 'strong analytical skills', 'data processing', 'machine learning', 'sql', 'data cleansing', 'data entry operation', 'cloud', '3d', 'data extraction', 'automation', 'data science', 'data annotation', 'lidar', 'communication skills']",2025-06-14 06:02:47
Senior Data Engineer,Guidehouse,5 - 10 years,Not Disclosed,['Chennai'],"Job Posting\nWhat You Will Do:\nDesign, develop, and maintain robust, scalable, and efficient data pipelines and ETL/ELT processes.\nLead and execute data engineering projects from inception to completion, ensuring timely delivery and high quality.\nBuild and optimize data architectures for operational and analytical purposes.\nCollaborate with cross-functional teams to gather and define data requirements.\nImplement data quality, data governance, and data security practices.\nManage and optimize cloud-based data platforms (Azure\\AWS).\nDevelop and maintain Python/PySpark libraries for data ingestion, Processing and integration with both internal and external data sources.\nDesign and optimize scalable data pipelines using Azure data factory and Spark(Databricks)\nWork with stakeholders, including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.\nDevelop frameworks for data ingestion, transformation, and validation.\nMentor junior data engineers and guide best practices in data engineering.\nEvaluate and integrate new technologies and tools to improve data infrastructure.\nEnsure compliance with data privacy regulations (HIPAA, etc.).\nMonitor performance and troubleshoot issues across the data ecosystem.\nAutomated deployment of data pipelines using GIT hub actions \\ Azure devops\nWhat You Will Need:\nBachelors or masters degree in computer science, Information Systems, Statistics, Math, Engineering, or related discipline.\nMinimum 5 + years of solid hands-on experience in  data engineering and cloud services.\nExtensive working experience with advanced SQL and deep understanding of SQL.\nGood Experience in Azure data factory (ADF), Databricks , Python and PySpark.\nGood experience in modern data storage concepts data lake, lake house.\nExperience in other cloud services (AWS) and data processing technologies will be added advantage.\nAbility to enhance , develop and resolve defects in ETL process using cloud services.\nExperience handling large volumes (multiple terabytes) of incoming data from clients and 3rd party sources in various formats such as text, csv, EDI X12 files and access database.\nExperience with software development methodologies (Agile, Waterfall) and version control tools\nHighly motivated, strong problem solver, self-starter, and fast learner with demonstrated analytic and quantitative skills.\nGood communication skill.\nWhat Would Be Nice To Have:\nAWS ETL Platform – Glue , S3\nOne or more programming languages such as Java, .Net\nExperience in US health care domain and insurance claim processing.",Industry Type: Management Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Pyspark', 'databricks', 'SQL', 'azure']",2025-06-14 06:02:49
Senior Data Engineer,Infraveo Technologies,3 - 6 years,Not Disclosed,[],"We are seeking a Senior Data Engineer (PostgreSQL and d SQL syntax) to join our team.\n\nResponsibilties:\nWork closely with other talented database professionals & software engineer. Our vetting process means you can count on your team members to know what they are talking about.\nWork from home or work remotely from anywhere.\nFlexible work schedule: meaning you can work regular hours or whenever you work best\nWork-life balance is essential and highly valued at company. If you choose to work more than 40 hours, youll be compensated for the\nextra work.\nWork on interesting projects solving complex business problems with custom software.\n100 hours per year to focus on your professional development. We invest in your growth.\nProfit sharing bonus means as were successful, youre successful.\nVariable compensation opportunities for being part of on-call team rotation and performing client focused work outside of normal business hours.\nExcellent benefits package including medical insurance, dental, vision, 401(k) matching, FSA, disability, life insurance, and paid parental leave.\nRequirements\nBasic understanding of data modeling design patterns such as 3NF and star-schema.\nData warehouse design experience using medallion architecture and/or snowflake methodologies.\nExperience with Microsoft Fabric, Synapse or Snowflake.\nReporting tools such as Power BI.\nETL tools such as Azure Data Factory.\nPerformance tuning databases and long running queries.\nStored procedure development.\nDatabase migrations and/or database server consolidations, including Azure SQL solutions.\nConfigure Azure SQL databases, elastic pools, and Managed Instances.\nPerform health checks, assessments, and security audits for client databases.\nImplement replication and/or log shipping for disaster recovery or reporting scenarios.\nEvaluate disk I/O and network throughput for SQL performance.\n\nNice to Have Experience:\n\nAdvanced SQL syntax.\nSQL Server installation & configuration in production environments using best practices.\nAvailability Group & failover cluster configuration.\nExposure to MySQL and/or PostgreSQL environments.\nFamiliarity with Red Gate SQL Monitor or similar tools.\nRight-sizing VM for SQL Server.\nDevelop and maintain robust backup, recovery, and maintenance plans.\nHolding Microsoft certification(s) in the Data & AI solutions field are a plus.\nMicrosoft Certified: Fabric Analytics Engineer Associate (DP-600).\nMicrosoft Certified: Fabric Data Engineer Associate (DP-700).\nMicrosoft Certified: Power BI Data Analyst Associate (PL-300).\nMicrosoft Certified: Administering Microsoft Azure SQL Solutions (DP-300).",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Performance tuning', 'Data modeling', 'Postgresql', 'MySQL', 'Database', 'power bi', 'Medical insurance', 'microsoft', 'Warehouse design', 'SQL']",2025-06-14 06:02:51
Data Analyst/ Excel Specialist,Appletech,3 - 8 years,Not Disclosed,['Vadodara'],"Must have at least 3 years of experience in working with MS Excel and excellent command over various functions and formulas viz. VLOOKUP, HLOOKUP, Pivot Table, etc.\nShould be able to understand data.\nExtract Excel and CSV data from other software, combine multiple files and massage the data.\nUse various tools and processes to complete data migrations from other software packages into our product in a timely and accurate manner.\nParticipate in detailed design and product test execution as required.\nShould have excellent written English and able to communicate directly with the US-based clients.\n\n*Only those candidates should apply who are ready to work from our Vadodara Office.",Industry Type: IT Services & Consulting,Department: Finance & Accounting,"Employment Type: Full Time, Permanent","['Excel', 'VLOOKUP', 'Data Analysis', 'Pivot', 'HLOOKUP', 'Countif', 'Data Migration', 'Conditional Formatting']",2025-06-14 06:02:53
Lead Data Engineer,Wiser Solutions,2 - 3 years,Not Disclosed,['Vadodara'],"Job Description\nWhen looking to buy a product, whether it is in a brick and mortar store or online, it can be hard enough to find one that not only has the characteristics you are looking for but is also at a price that you are willing to pay. It can also be especially frustrating when you finally find one, but it is out of stock. Likewise, brands and retailers can have a difficult time getting the visibility they need to ensure you have the most seamless experience as possible in selecting their product. We at Wiser believe that shoppers should have this seamless experience, and we want to do that by providing the brands and retailers the visibility they need to make that belief a reality.\nOur goal is to solve a messy problem elegantly and cost effectively. Our job is to collect, categorize, and analyze lots of structured and semi-structured data from lots of different places every day (whether it s 20 million+ products from 500+ websites or data collected from over 300,000 brick and mortar stores across the country). We help our customers be more competitive by discovering interesting patterns in this data they can use to their advantage, while being uniquely positioned to be able to do this across both online and instore.\nWe are looking for a lead-level software engineer to lead the charge on a team of like-minded individuals responsible for developing the data architecture that powers our data collection process and analytics platform. If you have a passion for optimization, scaling, and integration challenges, this may be the role for you.\nWhat You Will Do\nThink like our customers - you will work with product and engineering leaders to define data solutions that support customers business practices.\nDesign/develop/extend our data pipeline services and architecture to implement your solutions - you will be collaborating on some of the most important and complex parts of our system that form the foundation for the business value our organization provides\nFoster team growth - provide mentorship to both junior team members and evangelizing expertise to those on others.\nImprove the quality of our solutions - help to build enduring trust within our organization and amongst our customers by ensuring high quality standards of the data we manage\nOwn your work - you will take responsibility to shepherd your projects from idea through delivery into production\nBring new ideas to the table - some of our best innovations originate within the team\nTechnologies We Use\nLanguages: SQL, Python\nInfrastructure: AWS, Docker, Kubernetes, Apache Airflow, Apache Spark, Apache Kafka, Terraform\nDatabases: Snowflake, Trino/Starburst, Redshift, MongoDB, Postgres, MySQL\nOthers: Tableau (as a business intelligence solution)\n\n\nQualifications\nBachelors/Master s degree in Computer Science or relevant technical degree\n10+ years of professional software engineering experience\nStrong proficiency with data languages such as Python a",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Linux', 'MySQL', 'Agile', 'Data structures', 'OLAP', 'Apache', 'Business intelligence', 'SQL', 'Python']",2025-06-14 06:02:55
Data Modeler - SQL/ Erwin,Leading Client,7 - 10 years,Not Disclosed,['Bengaluru'],"Job Title : Data Engineer / Data Modeler.\n\nLocation : Remote (India).\n\nEmployment Type : Contract (Remote).\n\nExperience Required : 7+ Years.\n\nJob Summary :\n\nWe are looking for a highly skilled Data Engineer / Data Modeler with strong experience in Snowflake, DBT, and GCP to support our data infrastructure and modeling initiatives.\n\nThe ideal candidate should possess excellent SQL skills, hands-on experience with Erwin Data Modeler, and a strong background in modern data architectures and data modeling techniques.\n\nKey Responsibilities :\n\n- Design and implement scalable data models using Snowflake and Erwin Data Modeler.\n\n- Create, maintain, and enhance data pipelines using DBT and GCP (BigQuery, Cloud Storage, Dataflow).\n\n- Perform reverse engineering on existing systems (e.g., Sailfish/DDMS) using DBeaver or similar tools to understand and rebuild data models.\n\n- Develop efficient SQL queries and stored procedures for data transformation, quality, and validation.\n\n- Collaborate with business analysts and stakeholders to gather data requirements and convert them into physical and logical models.\n\n- Ensure performance tuning, security, and optimization of the Snowflake data warehouse.\n\n- Document metadata, data lineage, and business logic behind data structures and flows.\n\n- Participate in code reviews, enforce coding standards, and provide best practices for data modeling and governance.\n\nMust-Have Skills :\n\n- Snowflake architecture, schema design, and data warehouse experience.\n\n- DBT (Data Build Tool) for data transformation and pipeline development.\n\n- Strong expertise in SQL (query optimization, complex joins, window functions, etc.)\n\n- Hands-on experience with Erwin Data Modeler (logical and physical modeling).\n\n- Experience with GCP (BigQuery, Cloud Composer, Cloud Storage).\n\n- Experience in reverse engineering legacy systems like Sailfish or DDMS using DBeaver.\n\nGood To Have :\n\n- Experience with CI/CD tools and DevOps for data environments.\n\n- Familiarity with data governance, security, and privacy practices.\n\n- Exposure to Agile methodologies and working in distributed teams.\n\n- Knowledge of Python for data engineering tasks and orchestration scripts.\n\nSoft Skills :\n\n- Excellent problem-solving and analytical skills.\n\n- Strong communication and stakeholder management.\n\n- Self-driven with the ability to work independently in a remote setup.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Modeling', 'Data Quality', 'Data Build Tool', 'Data Security', 'Snowflake DB', 'Data Warehousing', 'Data Analytics', 'Data Governance', 'Erwin', 'SQL']",2025-06-14 06:02:57
Data Analyst - Intermediate,Equifax Credit Information Services Private Limited,3 - 8 years,Not Disclosed,['Pune'],"Equifax is looking for a Data Analyst to join our team in India supporting the Canada business unit. As a Data Analyst at Equifax, you will work with stakeholders to use data and analytics to provide key insights and drive business decisions.\nThe ideal candidate will possess strong technical skills and a passion for uncovering solutions, business insights hidden within vast datasets, and have the ability to communicate findings clearly and concisely to both technical and non-technical audiences.\nKey Responsibilities Include :\n\nIND-Pune-Equifax Analytics-PEC\nFunction - Data and Analytics\nFull time",,,,"['GCP', 'Project management', 'Analytical', 'Healthcare', 'Manager Quality Control', 'Project delivery', 'Analytics', 'Financial services', 'SQL', 'Python']",2025-06-14 06:02:59
"Lead Data Analytics Engineer - Power BI, Snowflake, SQL, Python",Avalara Technologies,8 - 13 years,Not Disclosed,['Pune'],"What You'll Do\n\nThe Data Science Engineering team is looking for a Lead Data Analytics Engineer\nto join our team! You should be and gather our requirements, understanding complex product, business, and engineering challenges, composing and prioritizing research projects, and then building them in partnership with cloud engineers and architects, and using the work of our data engineering team. You have deep SQL experience, an understanding of modern data stacks and technology, experience with data and all things data-related, and experience guiding a team through technical and design challenges. You will report into the Sr. Manager, Cloud Software Engineering and be a part of the larger Data Engineering team.\n\nWhat Your Responsibilities Will Be",,,,"['Data Analytics', 'Power BI', 'Snowflake', 'data storytelling', 'data warehousing', 'Data Modeling', 'AWS', 'SQL', 'Python']",2025-06-14 06:03:02
Senior Azure Data Engineer,Bit Wave Solutions,7 - 12 years,Not Disclosed,[],"Position : Sr Azure Data Engineer\nLocation: Remote\nTime : CET Time\nRole & responsibilities\n\nWe are seeking a highly skilled Senior Data Engineer to join our dynamic team. The ideal candidate will have extensive experience in Microsoft Azure, Fabric Azure SQL, Azure Synapse, Python, and Power BI. Knowledge of Oracle DB and data replication tools will be preferred. This role involves designing, developing, and maintaining robust data pipelines and ensuring efficient data processing and integration across various platforms.\n\nCandidate understands stated needs & requirements of the stakeholders and produce high quality deliverables Monitors own work to ensure delivery within the desired performance standards. Understands the importance of delivery within expected time, budget and quality standards and displays concern in case of deviation. Good communication skills and a team player\n\nDesign and Development: Architect, develop, and maintain scalable data pipelines using Microsoft Fabric and Azure services, including Azure SQL and Azure Synapse.\n\nData Integration: Integrate data from multiple sources, ensuring data consistency, quality, and availability using data replication tools.\nData Management: Manage and optimize databases, ensuring high performance and reliability.\nETL Processes: Develop and maintain ETL processes to transform data into actionable insights.\nData Analysis: Use Python and other tools to analyze data, create reports, and provide insights to support business decisions.\nVisualization: Develop and maintain dashboards and reports in Power BI to visualize complex data sets.\nPerformance Tuning: Optimize database performance and troubleshoot any issues related to data processing and integration\n\nPreferred candidate profile\n\nMinimum 7 years of experience in data engineering or a related field.\nProven experience with Microsoft Azure services, Fabrics including Azure SQL and Azure Synapse.\nStrong proficiency in Python for data analysis and scripting.\nExtensive experience with Power BI for data visualization.\nKnowledge of Oracle DB and experience with data replication tools.\nProficient in SQL and database management.\nExperience with ETL tools and processes.\nStrong understanding of data warehousing concepts and architectures.\nFamiliarity with cloud-based data platforms and services.\nAnalytical Skills: Ability to analyze complex data sets and provide actionable insights.\nProblem-Solving: Strong problem-solving skills and the ability to troubleshoot data-related issues.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Azure Data Factory', 'Azure Synapse', 'Microsoft Fabric', 'Python', 'Power Bi', 'Powerapps', 'SQL']",2025-06-14 06:03:04
Sr. AWS Data Engineer,Agilisium,6 - 10 years,14-24 Lacs P.A.,['Hyderabad'],"Role & responsibilities\nJob Title: Data Engineer\nYears of experience: 6 to 10 years (Minimum 5 years of relevant experience)\nWork Mode: Work From Office Hyderabad\nNotice Period-Immediate to 30 Days only\nKey Skills:\nPython, SQL, AWS, Spark, Databricks - (Mandate)",,,,"['Pyspark', 'AWS', 'Python', 'SQL', 'Amazon Redshift', 'Data Bricks', 'Elastic Search']",2025-06-14 06:03:06
Sr Data Engineer,Algoleap Technologies,10 - 15 years,Not Disclosed,['Hyderabad'],"We are looking for a Senior Data Engineer to lead the design and implementation of scalable data infrastructure and engineering practices. This role will be critical in laying down the architectural foundations for advanced analytics and AI/ML use cases across global business units. Youll work closely with the Data Science Lead, Product Manager, and other cross-functional stakeholders to ensure data systems are robust, secure, and future-ready.\nKey Responsibilities:",,,,"['orchestration', 'Architecture', 'GCP', 'Data modeling', 'Machine learning', 'Data processing', 'Data quality', 'Open source', 'SQL', 'Python']",2025-06-14 06:03:08
Sr. Azure Data Platform Engineer,EMTA Infotech,5 - 10 years,18-30 Lacs P.A.,[],"Role Title: Sr. Azure Data Platform Engineer\nLocation: India\n\n1 remote role and 5 WFO Noida location candidates need to have L2 and L3 Support experience with the below)\n\nWe are seeking an Azure Data Platform Engineer with a strong focus on Administration and hands-on experience in Azure platform engineering services.\nIdeal candidates should have expertise in administering services such as:\nAzure Key Vault\n\nFunction App & Logic App\nEvent Hub\n\nApp Services\nAzure Data Factory (Administration)\n\nAzure Monitor & Log Analytics\nAzure Databricks (Administration)\n\nETL processes\nCosmos DB (Administration)\n\nAzure DevOps & CI/CD pipelines\nAzure Synapse Analytics (Administration)\n\nPython / Shell scripting\nAzure Data Lake Storage (ADLS)\n\nAzure Kubernetes Service (AKS)\n\nAdditional knowledge of Tableau and Power BI would be a plus.\n\nAlso, candidates should have hands-on experience managing and ensuring the stability, security, and performance of these platforms, with a focus on automation, monitoring, and incident management.\n\nProficient in distributed system architectures and Azure Data Engineering services like Event Hub, Data Factory, ADLS Gen2, Cosmos DB, Synapse, Databricks, APIM, Function App , Logic App, and App Services\nImplement, and manage infrastructure using IaC tools such as Azure Resource Manager (ARM) templates and Terraform.\nManage containerized applications using Docker and orchestrate them with Azure Kubernetes Service (AKS).\nSet up and manage monitoring, logging, and alerting systems using Azure Monitor, Log Analytics, and Application Insights.\nImplement disaster recovery (DR) strategies, backups, and failover mechanisms for critical workloads.\nAutomate infrastructure provisioning, scaling, and management for high availability and efficiency.\nExperienced in managing and maintaining clusters across Development, Test, Preproduction, and Production environments on Azure.\nSkilled in defining, scheduling, and monitoring job flows, with proactive alert setup.\nAdept at troubleshooting failed jobs in azure tools like Databricks and Data Factory, performing root cause analysis, and applying corrective measures.\nHands-on experience with distributed streaming tools like Event Hub.\nExpertise in designing and managing backup and disaster recovery solutions using Infrastructure as Code (IaC) with Terraform.\nStrong experience in automating processes using Python, shell scripting, and working with Jenkins and Azure DevOps.\nProficient in designing and maintaining Azure CI/CD pipelines for seamless code integration, testing, and deployment.\nExperienced in monitoring and troubleshooting VM resources such as memory, CPU, OS, storage, and network.\nSkilled at monitoring applications and advising developers on improving job and workflow performance.\nCapable of reviewing and resolving log file issues for system and application components.\nAdaptable to evolving technologies, with a strong sense of responsibility and accomplishment.\nKnowledgeable in agile methodologies for software delivery.\n5-15 years of experience with Azure and cloud platforms, leveraging cloud-native tools to build, manage, and optimize secure, scalable solutions.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Key Vault', 'Azure Databricks', 'Azure Data Factory', 'Azure Synapse Analytics', 'Azure Devops', 'Adls Gen2', 'Function Apps', 'Shell Scripting', 'Azure Logic Apps', 'Azure Monitoring', 'Iac', 'Cosmos', 'Log Analytics', 'APIM', 'Terraform', 'Event Hub', 'CI/CD', 'App Service Plan', 'Etl Process', 'ARM', 'Kubernetes']",2025-06-14 06:03:11
Senior Consultant Data Science,Arcadis,8 - 13 years,Not Disclosed,['Bengaluru'],"Qualification & Experience:\nMinimum of 8 years of experience as a Data Scientist/Engineer with demonstrated expertise in data engineering and cloud computing technologies.\nTechnical Responsibilities\nExcellent proficiency in Python, with a strong focus on developing advanced skills.\nExtensive exposure to NLP and image processing concepts.\nProficient in version control systems like Git.\nIn-depth understanding of Azure deployments.\nExpertise in OCR, ML model training, and transfer learning.\nExperience working with unstructured data formats such as PDFs, DOCX, and images. O\nStrong familiarity with data science best practices and the ML lifecycle.\nStrong experience with data pipeline development, ETL processes, and data engineering tools such as Apache Airflow, PySpark, or Databricks.\nFamiliarity with cloud computing platforms like Azure, AWS, or GCP, including services like Azure Data Factory, S3, Lambda, and BigQuery.\nTool Exposure: Advanced understanding and hands-on experience with Git, Azure, Python, R programming and data engineering tools such as Snowflake, Databricks, or PySpark.\nData mining, cleaning and engineering: Leading the identification and merging of relevant data sources, ensuring data quality, and resolving data inconsistencies.\nCloud Solutions Architecture: Designing and deploying scalable data engineering workflows on cloud platforms such as Azure, AWS, or GCP.\nData Analysis: Executing complex analyses against business requirements using appropriate tools and technologies.\nSoftware Development: Leading the development of reusable, version-controlled code under minimal supervision.\nBig Data Processing: Developing solutions to handle large-scale data processing using tools like Hadoop, Spark, or Databricks.\nPrincipal Duties & Key Responsibilities:\nLeading data extraction from multiple sources, including PDFs, images, databases, and APIs.\nDriving optical character recognition (OCR) processes to digitize data from images.\nApplying advanced natural language processing (NLP) techniques to understand complex data.\nDeveloping and implementing highly accurate statistical models and data engineering pipelines to support critical business decisions and continuously monitor their performance.\nDesigning and managing scalable cloud-based data architectures using Azure, AWS, or GCP services.\nCollaborating closely with business domain experts to identify and drive key business value drivers.\nDocumenting model design choices, algorithm selection processes, and dependencies.\nEffectively collaborating in cross-functional teams within the CoE and across the organization.\nProactively seeking opportunities to contribute beyond assigned tasks.\nRequired Competencies:\nExceptional communication and interpersonal skills.\nProficiency in Microsoft Office 365 applications.\nAbility to work independently, demonstrate initiative, and provide strategic guidance.\nStrong networking, communication, and people skills.\nOutstanding organizational skills with the ability to work independently and as part of a team.\nExcellent technical writing skills.\nEffective problem-solving abilities.\nFlexibility and adaptability to work flexible hours as required.\nKey competencies / Values:\nClient Focus: Tailoring skills and understanding client needs to deliver exceptional results.\nExcellence: Striving for excellence defined by clients, delivering high-quality work.\nTrust: Building and retaining trust with clients, colleagues, and partners.\nTeamwork: Collaborating effectively to achieve collective success.\nResponsibility: Taking ownership of performance and safety, ensuring accountability.\nPeople: Creating an inclusive environment that fosters individual growth and development.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Azure Data Factory', 'S3', 'Apache Airflow', 'Azure', 'BigQuery', 'Git', 'PySpark', 'Lambda']",2025-06-14 06:03:13
Senior Data Engineer,Ntrix Innovations,6 - 9 years,Not Disclosed,"['Hyderabad', 'Kondapur']","Manadatory Skills:\n• Airflow,\n• python,\n• AWS and\n• Big Data technologies like Spark",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Airflow', 'Spark', 'AWS', 'Python']",2025-06-14 06:03:15
Senior Consultant - Data Analytics,Arcadis,5 - 10 years,Not Disclosed,['Bengaluru'],"Qualifications and Experience:\nBS/MS or BCA/MCA or bachelors/masters degree in Math, Statistics, Computer Science, Engineering, or another technical field.\nExperience required: 5+ years.\nExperience in Data engineering with Python\\R\nExperience in SQL, MS-SQL Server, or other relational databases.\nAzure Cloud Service experience\\AWS\\Google Could Service (Optional)\nAPI automated Data extraction pipeline\nExperience in developing and maintaining integrated visualization reports in PowerBI (Optional)\nExperience with software deployment project lifecycle phases - requirements gathering, planning, testing, delivery, enhancements, support.\nExperience in Project Management (Preferred)\nExceptional communication skills and fluency in English (professional level).\nKey Skills / Attributes\nExceptional analytical and problem-solving skills, strong attention to detail, organization skills, and work ethic.\nSelf-motivated and team-oriented, with the ability to work successfully both independently and within a team.\nAbility to balance and address new challenges as they arise and an eagerness to take ownership of tasks.\nDrive to succeed and grow a career in the Project/Program Management\nPrincipal Duties & Key Responsibilities\nKey Duties & Responsibilities\nThis role will require to work independently or in team to solve data problems with unstructured data.\nCollaborate with other team members and other disciplines to deliver project requirements.\nWork independently to complete allocated activities to meet timeframe and quality objectives and meeting or exceeding client expectations.\nDevelop effective materials for clients, making sure that their messages are clearly conveyed through the appropriate channel, using the language that is suitable for the intended audience and readers, and would induce the desired response.\nActively contribute to Arcadis Global Communities of Practice relevant to Project Management, Data Visualization, and Power Platform, through knowledge shares and case study presentations.\nActively contribute to the Digital Advisory community of practice, through development of integrated solutions that embed GEC capabilities into core advisory business.\nData Engineering, Management, and Visualisation\nExperience in manipulating, transforming, and analysing data sets that are raw, large, and complex.\nDemonstrates ability to plan, gather, analyse, and document user and business information.\nIncorporates, integrates, and interfaces technical knowledge with business / systems requirements.\nUnderstanding of all aspects of an implementation project including, but not limited to planning, analysis and design, configuration, development, conversions, system testing, cutover and production support.\nProduce written deliverables for requirement specifications and support documentation: process mapping, meeting minutes, glossaries, data dictionary, technical design, system testing and implementation activities.\nCollect and organize data, data warehouse reports, spreadsheets, and databases for analytical reporting.\nStrong on database concepts, data modelling, stored procedures, complex query writing, performance optimization of SQL queries.\nExperience in creating automated data extraction pipeline from various sources like API, Databases in various formats.\nA problem solving, solution driven mindset with the ability to innovate within the constraints of a project time/cost/quality.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Analytics', 'project management', 'python', 'sql queries', 'performance tuning', 'power bi', 'triggers', 'data engineering', 'sql server', 'azure cloud', 'stored procedures', 'sql', 'plsql', 'data extraction', 'data modeling', 'database creation', 'ssrs', 'data visualization', 'ssis', 'statistics']",2025-06-14 06:03:17
Senior Data Engineer,Avalara Technologies,6 - 11 years,Not Disclosed,['Pune'],"What You'll Do\n\nThe Global Analytics & Insights (GAI) team is looking for a Senior Data Engineer to lead our build of the data infrastructure for Avalara's core data assets- empowering us with accurate, data to lead data backed decisions. As A Senior Data Engineer, you will help architect, implement, and maintain our data infrastructure using Snowflake, dbt (Data Build Tool), Python, Terraform, and Airflow. You will immerse yourself in our financial, marketing, and sales data to become an expert of Avalara's domain. You will have deep SQL experience, an understanding of modern data stacks and technology, a desire to build things the right way using modern software principles, and experience with data and all things data related.\n\nWhat Your Responsibilities Will Be\n\nWhat You'll Need to be Successful",,,,"['Data Engineering', 'salesforce', 'continuous integration', 'snowflake', 'advance sql', 'python', 'git', 'infrastructure', 'ci/cd', 'terraform', 'sql']",2025-06-14 06:03:19
"Azure Data Engineering (Synapse, ADF, T-SQL) - Technical Lead",BILVANTIS TECHNOLOGIES,7 - 12 years,Not Disclosed,['Hyderabad'],"We are looking for a highly self-motivated individual with Azure Data Engineering (Synapse, ADF, T-SQL) as a Technical Lead:\nWork Experience:\nExperience should have 8 Years to 12 Years of Azure Data Engineering.\n7+ years of experience in ETL and Data Warehousing development.\nExperience with data modelling and ETL. Strong hands-on experience in ETL process preferably Microsoft technologies.\nKnowledge and experience in Azure Synapse Analytics, Azure Synapse DWH, building pipelines in AZURE Synapse platform and TSQL and Azure Data Factory.\nExperience in data warehouse design and maintenance is plus.\nExperience in agile development processes using Jira and Confluence.\nUnderstanding on the SDLC.\nUnderstanding on the Agile methodologies.\nCommunication with customer and producing the Daily status report.\nShould have good oral and written communication.\nShould be proactive and adaptive.\nSkills and languages:\n\nProficiency in written and spoken English; working knowledge of another UN language would be an asset.\nExpected Deliverables:\nCandidate should help IST in data extraction process from ERP to Datawarehouse.\nStrong knowledge on TSQL and writing Stored procedures.\nShould help building data models in different area of ERP based data sets.\nCandidate should build pipeline to establish integration of data flow among different sources of IT applications.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['T-SQL', 'ERP', 'Agile', 'Technical Lead', 'Stored procedures', 'JIRA', 'Data warehousing', 'SDLC', 'Analytics', 'Data extraction']",2025-06-14 06:03:21
Senior .NET Backend Developer - Azure Data Engineering Experience,Suzva Software Technologies,5 - 10 years,Not Disclosed,['Hyderabad'],"We Are Hiring: Senior .NET Backend Developer with Azure Data Engineering Experience\nJob Location: Hyderabad, India\nWork Mode: Onsite Only\nExperience: Minimum 6+ Years\nQualification: B.Tech, B.E, MCA, M.Tech\n\nRole Overview\nWe are seeking an experienced .NET Backend Developer with strong Azure Data Engineering skills to join our growing team in Hyderabad. You will work closely with cross-functional teams to build scalable backend systems, modern APIs, and data pipelines using cutting-edge tools like Azure Databricks and MS Fabric.\n\nTechnical Skills (Must-Have)\nStrong hands-on experience in C, SQL Server, and OOP Concepts\n\nProficiency with .NET Core, ASP.NET Core, Web API, Entity Framework (v6 or above)\n\nStrong understanding of Microservices Architecture\n\nExperience with Azure Cloud technologies including Data Engineering, Azure Databricks, MS Fabric, Azure SQL, Blob Storage, etc.\n\nExperience with Snowflake or similar cloud data platforms\n\nExperience working with NoSQL databases\n\nSkilled in Database Performance Tuning and Design Patterns\n\nWorking knowledge of Agile methodologies\n\nAbility to write reusable libraries and modular, maintainable code\n\nExcellent verbal and written communication skills (especially with US counterparts)\n\nStrong troubleshooting and debugging skills\n\nNice to Have Skills\nExperience with Angular, MongoDB, NPM\n\nFamiliarity with Azure DevOps CI/CD pipelines for build and release configuration\n\nSelf-starter attitude with strong analytical and problem-solving abilities\n\nWillingness to work extra hours when needed to meet tight deadlines\n\nWhy Join Us\nWork with a passionate, high-performing team\n\nOpportunity to grow your technical and leadership skills in a dynamic environment\n\nBe part of global digital transformation initiatives with top-tier clients\n\nExposure to real-world enterprise data systems\n\nOpportunity to work on cutting-edge Azure and cloud technologies\n\nPerformance-based growth & internal mobility opportunities\n\nTags\nDotNetDeveloper BackendDeveloper AzureDataEngineering Databricks MSFabric Snowflake Microservices CSharpJobs HyderabadJobs FullTimeJob HiringNow EntityFramework ASPNetCore CloudEngineering SQLJobs DevOps DotNetCore BackendJobs SuzvaCareers DataPlatformDeveloper SoftwareJobsIndia",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['.Net', 'Web API', 'C', 'Data Engineering', 'Azure Databricks', 'MS Fabric', 'SQL Server', 'Azure SQL', 'Backend', 'Azure Data Engineering', 'ASP.NET Core', 'Blob Storage', 'Entity Framework', 'OOP Concepts', '.NET Core']",2025-06-14 06:03:24
Data Analyst / Data Analyst Manager (Immediate Joiners),Millennium Organisation,1 - 4 years,3-6 Lacs P.A.,['Mumbai (All Areas)'],"Analyze data to find trends and insights\nCreate easy-to-understand reports and dashboards\nHelp teams make better decisions using data\nProficient in Advance Excel, Power BI and SQL\nEnsure data is accurate and up to data\nCall HR Drashti - 8169887699\n\nRequired Candidate profile\nExperience in data analysis, reporting and insights\n1+ years of relevant experience\nStrong leadership skills\nMumbai candidates preferred\nImmediate joiners required",Industry Type: Advertising & Marketing,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Power Bi', 'Advanced Excel', 'Data Analysis', 'Business Strategy', 'Sales Analysis', 'Team Handling', 'Team Coordination', 'Data Management', 'Formulas', 'Powerpoint', 'MIS Reporting', 'Leadership Skills', 'SQL', 'Business Planning', 'Marketing Operations', 'Team Leading', 'Dashboards', 'Operations Management']",2025-06-14 06:03:26
Software Data Science Engineer,Johnson Controls,2 - 5 years,Not Disclosed,['Pune'],"Job Title Software and Data Science Engineer\nJob Summary We are seeking a highly motivated and analytical Data Scientist to join our team. The ideal candidate will possess a strong engineering background, a passion for solving technical problems, and the ability to work collaboratively with both technical and non-technical stakeholders. You will play a key role in developing and maintaining our platform, utilizing large-scale data to derive insights and drive business value, while working on both front-end and back-end components.\nWhat We Value",,,,"['Computer science', 'C++', 'Backend', 'Front end', 'data science', 'Data management', 'Analytical', 'Javascript', 'Data structures', 'Python']",2025-06-14 06:03:28
Senior Data Engineer,Tanasvi Technologies,9 - 14 years,9.6-24 Lacs P.A.,['Visakhapatnam'],"Responsibilities:\n* Design, develop & maintain data pipelines using PySpark, SQL & DBs.\n* Collaborate with cross-functional teams on project\ndelivery.\n*Strong in Databricks, PySpark, SQL\n* Databricks certification is mandatory\n*Location: Remote",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'Data Bricks', 'SQL']",2025-06-14 06:03:30
Azure Data Engineer-ADF,Ltimindtree,6 - 11 years,Not Disclosed,"['Hyderabad', 'Pune', 'Bengaluru']","Hello,\nGreetings from LTIMindtree!\nWe are Hiring for Azure Data Engineer-ADF (PAN India)!\n\nJob Description\nNotice Period:- 0 to 30 Days only\nExperience:- 5 to 12 Years\nInterview Mode:- 2 rounds (One round is F2F)\nHybrid (2-3 WFO)\nBrief Description of Role\nJob Summary:\nDetailed JD\nAzure Data Factory ADF\nExtensive experience in designing developing and maintaining data pipelines using Azure Data Factory\nProficiency in creating and managing data flows activities and triggers in ADF\nAzure Cloud Services\nStrong knowledge of Azure services such as Azure Data Lake Storage ADLS Azure Synapse Analytics and Azure SQL Database\nExperience with Azure DevOps for CICD pipeline implementation\nData Integration and ETL\nExpertise in data integration and ETL processes including data ingestion transformation and loading\nFamiliarity with various data sources and sinks including SQL and NoSQL databases",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Data Factory', 'Datafactory', 'ADF']",2025-06-14 06:03:33
PySpark Data Engineer (Only Immediate Joiner),Adecco,5 - 9 years,12-22 Lacs P.A.,"['Hyderabad', 'Bengaluru']","Position : PySpark Data Engineer\nLocation : Bangalore / Hyderabad\nExperience : 5 to 9 Yrs\nJob Type : On Role\n\nJob Description: PySpark Data Engineer:-\n1. API Development: Design, develop, and maintain robust APIs using FastAPI and RESTful principles for scalable backend systems.\n2. Big Data Processing: Leverage PySpark to process and analyze large datasets efficiently, ensuring optimal performance in big data environments.\n3. Full-Stack Integration: Develop seamless backend-to-frontend feature integrations, collaborating with front-end developers for cohesive user experiences.\n4. CI/CD Pipelines: Implement and manage CI/CD pipelines using GitHub Actions and Azure DevOps to streamline deployments and ensure system reliability.\n5. Containerization: Utilize Docker for building and deploying containerized applications in development and production environments.\n6. Team Leadership: Lead and mentor a team of developers, providing guidance, code reviews, and support to junior team members to ensure high-quality deliverables.\n7. Code Optimization: Write clean, maintainable, and efficient Python code, with a focus on scalability, reusability, and performance.\n8. Cloud Deployment: Deploy and manage applications on cloud platforms like Azure, ensuring high availability and fault tolerance.\n9. Collaboration: Work closely with cross-functional teams, including product managers and designers, to translate business requirements into technical solutions.\n10. Documentation: Maintain thorough documentation for APIs, processes, and systems to ensure transparency and ease of maintenance.\nHighlighted Skillset:-\nBig Data: Strong PySpark skills for processing large datasets.\nDevOps: Proficiency in GitHub Actions, CI/CD pipelines, Azure DevOps, and Docker.\nIntegration: Experience in backend-to-frontend feature connectivity.\nLeadership: Proven ability to lead and mentor development teams.\nCloud: Knowledge of deploying and managing applications in Azure or other cloud environments.\nTeam Collaboration: Strong interpersonal and communication skills for working in cross-functional teams.\nBest Practices: Emphasis on clean code, performance optimization, and robust documentation.\n\nInterested candidates kindly share your CV and below details to usha.sundar@adecco.com\n1) Present CTC (Fixed + VP) -\n2) Expected CTC -\n3) No. of years experience -\n4) Notice Period -\n5) Offer-in hand -\n6) Reason of Change -\n7) Present Location -",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Pyspark', 'Cicd Methodology', 'FastAPI', 'API', 'Python', 'Cloud Deployment', 'Data Engineering', 'Docker', 'Big Data', 'Github Actions', 'Performance Tuning', 'Azure Devops']",2025-06-14 06:03:35
Remote Data Platform Engineer 57LakhsCTC|| Srinivasa Reddy Kandi,Integra Technologies,12 - 15 years,55-60 Lacs P.A.,"['Ahmedabad', 'Chennai', 'Bengaluru']","Dear Candidate,\nWe are hiring a Data Platform Engineer to build and maintain scalable, secure, and reliable data infrastructure for analytics and real-time processing.\n\nKey Responsibilities:\nDesign and manage data pipelines, storage layers, and ingestion frameworks.\nBuild platforms for batch and streaming data processing (Spark, Kafka, Flink).\nOptimize data systems for scalability, fault tolerance, and performance.\nCollaborate with data engineers, analysts, and DevOps to enable data access.\nEnforce data governance, access controls, and compliance standards.\nRequired Skills & Qualifications:\nProficiency with distributed data systems (Hadoop, Spark, Kafka, Airflow).\nStrong SQL and experience with cloud data platforms (Snowflake, BigQuery, Redshift).\nKnowledge of data warehousing, lakehouse, and ETL/ELT pipelines.\nExperience with infrastructure as code and automation.\nFamiliarity with data quality, security, and metadata management.\nSoft Skills:\nStrong troubleshooting and problem-solving skills.\nAbility to work independently and in a team.\nExcellent communication and documentation skills.\nNote: If interested, please share your updated resume and preferred time for a discussion. If shortlisted, our HR team will contact you.\n\nSrinivasa Reddy Kandi\nDelivery Manager\nIntegra Technologies",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Airflow', 'Data Engineering', 'Spark', 'Python', 'Streaming', 'Apache Flink', 'Kafka', 'Apache', 'Spark Streaming', 'Apache Nifi', 'Stream Processing', 'Apache Storm', 'Apache Pulsar', 'Data Pipeline', 'Streams']",2025-06-14 06:03:38
AWS Data Engineer,Redserv Global Solutions,3 - 6 years,10-15 Lacs P.A.,[],"Roles and Responsibilities:\n\nWe are looking for an experienced AWS Cloud Data Engineer to join our Data Science & Analytics team to build, optimize, and maintain cloud-based data solutions. The ideal candidate will possess strong technical knowledge in data engineering on AWS, expertise in data integration, pipeline creation, performance optimization, and a strong understanding of DevOps methodologies.",,,,"['Data Engineer', 'AWS', 'Python']",2025-06-14 06:03:40
PySpark Azure Data Engineer,Our Client is American multinational inf...,5 - 10 years,16-25 Lacs P.A.,"['Hyderabad', 'Bengaluru']","PySpark Data Engineer:-\n\nJob Description:\n\n1. API Development: Design, develop, and maintain robust APIs using FastAPI and RESTful principles for scalable backend systems.\n2. Big Data Processing: Leverage PySpark to process and analyze large datasets efficiently, ensuring optimal performance in big data environments.\n3. Full-Stack Integration: Develop seamless backend-to-frontend feature integrations, collaborating with front-end developers for cohesive user experiences.\n4. CI/CD Pipelines: Implement and manage CI/CD pipelines using GitHub Actions and Azure DevOps to streamline deployments and ensure system reliability.\n5. Containerization: Utilize Docker for building and deploying containerized applications in development and production environments.\n6. Team Leadership: Lead and mentor a team of developers, providing guidance, code reviews, and support to junior team members to ensure high-quality deliverables.\n7. Code Optimization: Write clean, maintainable, and efficient Python code, with a focus on scalability, reusability, and performance.\n8. Cloud Deployment: Deploy and manage applications on cloud platforms like Azure, ensuring high availability and fault tolerance.\n9. Collaboration: Work closely with cross-functional teams, including product managers and designers, to translate business requirements into technical solutions.\n10. Documentation: Maintain thorough documentation for APIs, processes, and systems to ensure transparency and ease of maintenance\n\n\nHighlighted Skillset:-\nBig Data: Strong PySpark skills for processing large datasets.\nDevOps: Proficiency in GitHub Actions, CI/CD pipelines, Azure DevOps, and Docker.\nIntegration: Experience in backend-to-frontend feature connectivity.\nLeadership: Proven ability to lead and mentor development teams.\nCloud: Knowledge of deploying and managing applications in Azure or other cloud environments.\nTeam Collaboration: Strong interpersonal and communication skills for working in cross-functional teams.\nBest Practices: Emphasis on clean code, performance optimization, and robust documentation\n\n\nShare updated resume at siddhi.pandey@adecco.com or whatsapp at 6366783349",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'CI/CD pipelines', 'GitHub Actions', 'Docker', 'Azure Devops', 'FastAPI', 'RESTful principles']",2025-06-14 06:03:42
AWS Data Engineer,Redserv Global Solutions,3 - 6 years,10-15 Lacs P.A.,[],"Roles and Responsibilities:\n\nWe are looking for an experienced AWS Cloud Data Engineer to join our Data Science & Analytics team to build, optimize, and maintain cloud-based data solutions. The ideal candidate will possess strong technical knowledge in data engineering on AWS, expertise in data integration, pipeline creation, performance optimization, and a strong understanding of DevOps methodologies.",,,,"['Data Engineer', 'AWS', 'Python']",2025-06-14 06:03:44
AWS Data Engineer,Redserv Global Solutions,3 - 6 years,10-15 Lacs P.A.,[],"Roles and Responsibilities:\n\nWe are looking for an experienced AWS Cloud Data Engineer to join our Data Science & Analytics team to build, optimize, and maintain cloud-based data solutions. The ideal candidate will possess strong technical knowledge in data engineering on AWS, expertise in data integration, pipeline creation, performance optimization, and a strong understanding of DevOps methodologies.",,,,"['Data Engineer', 'AWS', 'Python']",2025-06-14 06:03:46
AWS Data Engineer,Redserv Global Solutions,3 - 6 years,10-15 Lacs P.A.,[],"Roles and Responsibilities:\n\nWe are looking for an experienced AWS Cloud Data Engineer to join our Data Science & Analytics team to build, optimize, and maintain cloud-based data solutions. The ideal candidate will possess strong technical knowledge in data engineering on AWS, expertise in data integration, pipeline creation, performance optimization, and a strong understanding of DevOps methodologies.",,,,"['Data Engineer', 'AWS', 'Python']",2025-06-14 06:03:48
PySpark Azure Data Engineer,American multinational information techn...,5 - 10 years,16-25 Lacs P.A.,"['Hyderabad', 'Bengaluru']","Urgent Hiring for PySpark Data Engineer:-\n\nJob Location- Bangalore and Hyderabad\n\nExp- 5yrs-9yrs\n\nShare CV Mohini.sharma@adecco.com OR Call 9740521948\n\n\nJob Description:\n\n1. API Development: Design, develop, and maintain robust APIs using FastAPI and RESTful principles for scalable backend systems.\n2. Big Data Processing: Leverage PySpark to process and analyze large datasets efficiently, ensuring optimal performance in big data environments.\n3. Full-Stack Integration: Develop seamless backend-to-frontend feature integrations, collaborating with front-end developers for cohesive user experiences.\n4. CI/CD Pipelines: Implement and manage CI/CD pipelines using GitHub Actions and Azure DevOps to streamline deployments and ensure system reliability.\n5. Containerization: Utilize Docker for building and deploying containerized applications in development and production environments.\n6. Team Leadership: Lead and mentor a team of developers, providing guidance, code reviews, and support to junior team members to ensure high-quality deliverables.\n7. Code Optimization: Write clean, maintainable, and efficient Python code, with a focus on scalability, reusability, and performance.\n8. Cloud Deployment: Deploy and manage applications on cloud platforms like Azure, ensuring high availability and fault tolerance.\n9. Collaboration: Work closely with cross-functional teams, including product managers and designers, to translate business requirements into technical solutions.\n10. Documentation: Maintain thorough documentation for APIs, processes, and systems to ensure transparency and ease of maintenance\n\n\nHighlighted Skillset:-\nBig Data: Strong PySpark skills for processing large datasets.\nDevOps: Proficiency in GitHub Actions, CI/CD pipelines, Azure DevOps, and Docker.\nIntegration: Experience in backend-to-frontend feature connectivity.\nLeadership: Proven ability to lead and mentor development teams.\nCloud: Knowledge of deploying and managing applications in Azure or other cloud environments.\nTeam Collaboration: Strong interpersonal and communication skills for working in cross-functional teams.\nBest Practices: Emphasis on clean code, performance optimization, and robust documentation",Industry Type: Financial Services (Asset Management),Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'CI/CD pipelines', 'GitHub Actions', 'Docker', 'Azure Devops', 'FastAPI', 'RESTful principles']",2025-06-14 06:03:50
Lead Data Engineer,Transunion,5 - 10 years,Not Disclosed,['Chennai'],"TransUnions Job Applicant Privacy Notice\nWhat Well Bring:\nData Pipeline Engineer at Orion project are embedded within our engineering teams and support the development and operation.\nWhat Youll Bring:\nLead Data Engineer\nWhat We Offer\nWe are looking for an individual to be part of an autonomous, cross-functional agile/scrum team where everyone shares responsibility for all aspects of the work.\nThis is a hybrid position and involves regular performance of job responsibilities virtually as well as in-person at an assigned TU office location for a minimum of two days a week.\nLead Developer, Software Development",,,,"['orchestration', 'Data management', 'Coding', 'GCP', 'Postgresql', 'Agile', 'Data processing', 'big data', 'Analytics', 'SQL']",2025-06-14 06:03:53
Azure Data Engineer,Fortune India 500 IT Services Firm,4 - 8 years,5-12 Lacs P.A.,"['Hyderabad', 'Chennai', 'Bengaluru']",Hiring for Azure Data Engineer and also the client is looking for Immediate joiners who can join in 30 days.,Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Temporary/Contractual","['Azure Data Factory', 'Pyspark', 'Azure Databricks', 'Python', 'Azure Synapse', 'Azure Data Lake', 'SQL']",2025-06-14 06:03:54
Snowflake Data Engineer,Tredence,6 - 10 years,Not Disclosed,"['Gurugram', 'Chennai', 'Bengaluru']",Role & responsibilities\n\nCollaborate with DW/BI leads to understand new ETL pipeline development requirements\nTriage issues to find gaps in existing pipelines and fix the issues\nWork with business to understand the need in reporting layer and develop data model to fulfill reporting needs\nHelp joiner team members to resolve issues and technical challenges.\nDrive technical discussion with client architect and team members,,,,"['Snowflake', 'Snowpipe', 'AWS', 'DBT']",2025-06-14 06:03:57
Data Visualization Engineer,Maimsd Technology,4 - 7 years,Not Disclosed,['Bengaluru'],"Role : Data Visualization Engineer\n\nLocation : Bangalore, Pune\n\nExperience : 4 - 7 Yrs\n\nEmployment Type : Full Time, Permanent\n\nWorking mode : Regular\n\nNotice Period : Immediate - 15 Day\n\nAbout the Role :\n\nWe are seeking a skilled Data Visualization Engineer to join our team and transform raw data into actionable insights. You will play a crucial role in designing, developing, and maintaining interactive dashboards and reports using Power BI, Power Apps, Power Query, and Power Automate.\n\nResponsibilities :\n\n- Data Visualization : Create compelling and informative dashboards and reports using Power BI, effectively communicating complex data to stakeholders.\n\n- Data Integration : Import data from various sources (e.g., databases, spreadsheets, APIs) using Power Query and ensure data quality and consistency.\n\n- Data Modeling : Develop robust data models in Power BI to support complex analysis and reporting requirements.\n\n- Power Apps Development : Create custom applications using Power Apps to enable data-driven decision-making and automate workflows.\n\n- Power Automate Integration : Automate repetitive tasks and workflows using Power Automate to improve efficiency and reduce errors.\n\n- Cloud Data Analytics : Leverage cloud-based data analytics platforms to process and analyze large datasets.\n\n- Collaboration : Work closely with data analysts, data scientists, and business users to understand their requirements and deliver effective visualizations.\n\nQualifications :\n\nExperience : 4-7 years of experience in data visualization and business intelligence.\n\nTechnical Skills :\n\n- Proficiency in Power BI, Power Apps, Power Query, and Power Automate.\n\n- Strong understanding of data modeling and ETL processes.\n\n- Experience working with cloud-based data analytics platforms.\n\n- Familiarity with SQL and other data query languages.\n\nSoft Skills :\n\n- Excellent communication and interpersonal skills.\n\n- Strong problem-solving and analytical abilities.\n\n- Attention to detail and ability to deliver high-quality work",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Visualization Engineer', 'Reporting Analytics', 'Power BI', 'Dashboard Design', 'Data Visualization', 'Power Automate', 'Data Modeling', 'ETL', 'Power Query M', 'Data Integration', 'SQL']",2025-06-14 06:03:59
Data Consultant-GCP Data Engineer,Kyndryl,10 - 15 years,Not Disclosed,['Bengaluru'],"Who We Are\nAt Kyndryl, we design, build, manage and modernize the mission-critical technology systems that the world depends on every day. So why work at Kyndryl? We are always moving forward – always pushing ourselves to go further in our efforts to build a more equitable, inclusive world for our employees, our customers and our communities.\n\nThe Role\nAt Kyndryl, we design, build, manage and modernize the mission-critical technology systems that the world depends on every day. So why work at Kyndryl? We are always moving forward – always pushing ourselves to go further in our efforts to build a more equitable, inclusive world for our employees, our customers and our communities.",,,,"['kubernetes', 'components', 'analytical', 'data', 'metadata management', 'pyspark', 'master data management', 'sql', 'docker', 'cloud', 'gcp', 'looker', 'bigquery', 'etl', 'programming', 'snowflake', 'python', 'data analytics', 'airflow', 'machine learning', 'data engineering', 'dataproc', 'nosql', 'data bricks', 'data quality', 'mdm', 'data flow']",2025-06-14 06:04:02
Azure Data Engineer,Arcadis,3 - 8 years,Not Disclosed,['Bengaluru'],"ARCADISis looking for Azure Data Engineer with a passion to drive and execute Digital to the core of everything we do. We firmly believe in Everything Digital, Digital Everything. We are transforming, we are reimagining the industry and we are reimagining how communities and nations can help becoming more sustainable places to live for today and future generations.\nTechnology is the core and integral part of what we do, all the way for empowering Arcadians to harnessing power of data and AI/ML for sensors, IIOT and Advanced Drones, the technology teams are Dreaming Big and Delivering on future. As part of our Technology drive, we are looking for on-board talented and passionate Azure data engineers across multiple locations in North America.\nRole accountabilities:\nPossess excellent design and coding skills and a zeal for owning the complete SDLC of building applications in a DevOps environment\nYou are excited about working with Azure Data Platform\nchallenges while building the next wave of software engineering solutions\nCollaborating with and across Agile teams to design, develop, test, implement, and support technical solutions in Microsoft Azure Data Platform\nLeading the craftsmanship, security, availability, resilience, and scalability of your solutions\nVery strong on database concepts, data modelling, stored procedures, complex query writing, performance optimization of SQL queries.\nStrong experience in\nT-SQL, SSIS, SSAS, SSRS\nAzure Data Factory\nAzure Data Lake Store\nAzure Data Lake Analytics (Good to have, not mandatory)\nAzure SQL DB\nAzure SQL DW\nAzure Analysis Services, DAX\nAzure Data Bricks with Python/Scala\nExperience in building end to end solution using Azure data analytics platform.\nExperience in building generic framework solution which can be reused for upcoming similar use cases.\nExperience in building Azure data analytics solutions with DevOps (CI/CD) approach.\nExperience in using TFS, Azure Repos.\nMentor peers to gain expertise on Azure data platform solutions skills.\nExperience in developing, maintaining, publishing, and supporting dashboards using Power BI.\nStrong experience in publishing dashboards to Power BI service, using Power BI gateways, Power BI Report Server & Power BI Embedded\nQualifications & Experience:\nBasic Qualifications:\nBachelor in Engineering/Math/Statistics/Econometrics or related discipline\nShould have 3-8 years of experience in MSBI with relevant hands-on experience in Azure Data Platform (must) for a minimum of 3 years.\nPreferred Qualifications:\nMasters or Minor in Computer Science\n3+ years of experience developing Data Engineering solutions\nArchitecture, design experience with good knowledge of data model design & their implementation.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure', 'T-SQL', 'Azure Data Factory', 'SSAS', 'SSRS', 'Azure SQL DB', 'SSIS', 'Azure SQL DW']",2025-06-14 06:04:04
Aws Data Engineer,Agilisium,4 - 8 years,Not Disclosed,"['Chennai', 'Bengaluru']",Years of experience: 4 to 8 years (with minimum 4 years of relevant experience)\n\nTech Stack :\nAWS\nPython\nSQL\nPyspark,,,,"['Pyspark', 'AWS', 'Python', 'SQL']",2025-06-14 06:04:06
Azure Data Engineer,Hiring for Leading MNC Company!!,5 - 8 years,Not Disclosed,"['Gurugram', 'Bengaluru']","Warm Greetings from SP Staffing!!\n\nRole:Azure Data Engineer\nExperience Required :5 to 8 yrs\nWork Location :Bangalore/Gurgaon\n\nRequired Skills,\nAzure Databricks, ADF, Pyspark/SQL\n\nInterested candidates can send resumes to nandhini.spstaffing@gmail.com",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Data Factory', 'Azure Databricks', 'Pyspark', 'Datafactory', 'ADF', 'Azure Data Lake', 'Data Bricks', 'Python', 'SQL', 'ADB']",2025-06-14 06:04:08
Data Analyst,2070Health,2 - 3 years,10-12 Lacs P.A.,['Mumbai (All Areas)'],"Data Analyst to drive data-led decisions in healthcare. Build dashboards, manage CRM and billing data, generate insights, and support business reviews. Ideal for data-driven problem solvers with 2–3 yrs of experience.",Industry Type: Medical Services / Hospital (Diagnostics),Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Power Bi', 'Bi Tools', 'Advanced Excel', 'SQL', 'Critical Thinking', 'Communication Skills']",2025-06-14 06:04:10
Data Science Engineer,ACL Digital,3 - 7 years,12-16 Lacs P.A.,['Chennai'],"Work Location: Chennai\n\nPlease share your updated profile to sugantha.krishnan@acldigital.com\n\nRole & responsibilities\nResponsibilities:\n- Collecting, cleaning, and preprocessing data from various sources\n- Developing and applying machine learning algorithms and statistical models to solve business problems\n- Conducting exploratory data analysis and data visualization\n- Collaborating with cross-functional teams to identify business opportunities and develop data-driven solutions\n- Developing and deploying data pipelines and ETL processes\n- Communicating insights and findings to technical and non-technical stakeholders\n- Evaluating the effectiveness and accuracy of models and recommending improvements\n- Keeping up-to-date with the latest trends and technologies in the field of data science\n\nQualifications\n- Minimum of 3 years' Relevant experience with Bachelors' or Masters' degree in STEM (Science, Technology, Engineering, and Mathematics)\n- Work experience where data science, data engineering and application development was a major part of your work in automating software deployments and following a continuous delivery and deployment model\n- Experience with mathematical & statistical modeling, Cloud environment, APIs, Kubernetes\n- Solid experience in program coding with Python, SQL,or R Programming\n- Experience in visualization tools like Power BI\n- Experience in data science, machine learning or artificial intelligence technologies",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['R', 'Dash', 'Python', 'R shiny']",2025-06-14 06:04:13
Data Engineer,Hunch,2 - 7 years,Not Disclosed,['New Delhi'],"What is Hunch\nHunch is a dating app that helps you land a date without swiping like a junkie. Designed for people tired of mindless swiping and commodified matchmaking, Hunch leverages a powerful AI-engine to help users find meaningful connections by focusing on personality over just looks. With 2M+ downloads and a 4.4-star rating, Hunch is going viral in the US by challenging the swipe-left/right norm of traditional apps. Hunch is a Series A funded ($23 Million) startup building the future of social discovery in a post-AI world.\n\nLink to our fundraising announcement\n\nKey offerings of Hunch:\n\n\nSwipe Less, Vibe More: Curated profiles, cutting the clutter of endless swiping.\n\nPersonality Matters: Opinion-based, belief-based, and thought-based compatibility rather than just focusing on looks.\n\nEvery Match, Verified: No bots, no catfishing just real, trustworthy connections\n\nMatch Scores: Our AI shows compatibility percentages, helping users identify their 100% vibe match.\n\n\nWere looking for a highly motivated and skilled Data Engineer. Youll design, build, and optimize our robust data infrastructure. Youll also develop scalable data pipelines, ensure data quality, and collaborate closely with our machine learning teams. Were looking for someone passionate about data who thrives in a dynamic environment. If you enjoy tackling complex challenges with cutting-edge technologies, we encourage you to apply.\n\nWhat Youll Do:\n\nArchitect Optimize Data Infrastructure: Design, implement, and maintain highly scalable data infrastructure. This includes processes for auto-scaling and easy maintainability of our data pipelines.\n\nDevelop Deploy Data Pipelines: Lead the design, implementation, testing, and deployment of resilient data pipelines. These pipelines will ingest, transform, and process large datasets efficiently.\n\nEmpower ML Workflows: Partner with Machine Learning Engineers to understand their specific data needs. This includes providing high-quality data for model training and ensuring low-latency data delivery for real-time inference. Ensure seamless data flow and efficient integration with ML models.\n\nEnsure Data Integrity: Establish and enforce robust systems and processes. These will ensure comprehensive data quality assurance, validation, and reliability across the entire data lifecycle.\n\n\nWhat Youll Bring:\n\nExperience: A minimum of 2+ years of professional experience in data engineering. You should have a proven track record of delivering solutions in a production environment.\n\nData Storage Expertise: Hands-on experience with relational databases (e.g., PostgreSQL, MySQL, Redshift) and cloud object storage (e.g., S3) is required. Experience with distributed file systems (e.g., HDFS) and NoSQL databases is a plus.\n\nBig Data Processing: Demonstrated proficiency with big data processing platforms and frameworks. Examples include Hadoop, Spark, Hive, Presto, and Trino.\n\nPipeline Orchestration Messaging: Practical experience with key data pipeline tools. This includes message queues (e.g., Kafka, Kinesis), workflow orchestrators (e.g., dbt, Airflow), change data capture (e.g., Debezium), and ETL services (e.g., AWS Glue ETL).\n\nProgramming Prowess: Strong programming skills in Python and SQL are essential. Proficiency in at least one JVM-based language (e.g., Java, Scala) is also required.\n\nML Acumen: A solid understanding of machine learning workflows. This includes data preparation and feature engineering concepts.\n\nInnovation Agility: You should be a creative problem-solver. Youll need a proactive approach to experimenting with new technologies.\n\n\nWhat we have to offer\n\nCompetitive financial rewards + annual PLI (Performance Linked Incentives).\n\nMeritocracy-driven, candid, and diverse culture.\n\nEmployee benefits like Medical Insurance\n\nOne annual all expenses paid by company trip for all employees to bond\n\nAlthough we work from our office in New Delhi, we are flexible in our style and approach\n\n\nLife @Hunch\n\nWork Culture: At Hunch we take our work seriously but don t take ourselves too seriously. Everyone is encouraged to think as owners and not renters, and we prefer to let builders build, empowering people to pursue independent ideas.\n\nImpact: Your work will shape the future of social engagement and connect people around the world.\n\nCollaboration: Join a diverse team of creative minds and be part of a supportive community.\n\nGrowth: We invest in your development and provide opportunities for continuous learning.\n\nBacked by Global Investors: Hunch is a Series A funded startup, backed by Hashed, AlphaWave, Brevan Howard and Polygon Studios\n\nExperienced Leadership: Hunch is founded by a trio of industry veterans - Ish Goel (CEO), Nitika Goel (CTO), and Kartic Rakhra (CMO) - serial entrepreneurs with the last exit from Nexus Mutual, a web3 consumer-tech startup.",Industry Type: Internet,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Manager Quality Assurance', 'NoSQL', 'Postgresql', 'MySQL', 'Machine learning', 'Data processing', 'Data quality', 'SQL', 'Python']",2025-06-14 06:04:15
Data Engineer,Tek Ninjas,8 - 13 years,Not Disclosed,['Pune'],"Must have:-\n\n• Strong communicator, from making presentations to technical writing\n• Experience in financial domain preferably in the areas of Financial accounting and Risk.\n• Strong data analysis and validation experience with attention to detail and understanding patterns\n• Good understanding of Credit Risk, workflow and reporting solutions\n• Have a flair for technology and are adept at leveraging the latest tools and technologies to increase team productivity and collaboration across dispersed teams\n• Experience in Python, SQL and PL/SQL (Oracle) and writing queries, scripts.\n• A real passion for and experience of Agile working practices, with a strong desire to work with baked in quality subject areas such as TDD, BDD, test automation and DevOps principles\n• Experience using DevOps toolsets like GitLab\n• Has prior experience of working on databases and usage of SQL to perform data analysis, preferably cloud technologies\n• Familiar with Azure Native Cloud services, software design and enterprise integration patterns.\n• Experience working with / exposure to NLP, Gen AI, ML and data modelling projects, a plus\n• Has prior experience of working on IT projects and has knowledge and experience of software development life cycle using Agile methodology\n• Is well structured, very reliable and dedicated; has high attention to detail, has ability to handle a significant number of dependencies and issues ensuring nothing is missed out.\n• Has excellent written and verbal communication skills, inter-personal and negotiation skills\n• Ability to work as part of a global team with multiple delivery teams and engage with stakeholders at various levels\n• Able to challenge the status quo and if required have the ability to push-back demands from stakeholders with right justification\n• Someone with a general ability to pick up information quickly and turn around deliverables under pressure\n• A team player with excellent people management skills\n• Takes ownership of tasks assigned to ultimate resolution\n• Accuracy and timeliness of delivering solutions using the best IT standards and practices",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Gen Ai', 'AWS', 'Machine Learning', 'Python', 'SQL', 'Generative Ai', 'Data Engineering', 'Ml']",2025-06-14 06:04:17
DBA DATA Engineer || 12 Lakhs CTC,Robotics Technologies,5 - 10 years,12 Lacs P.A.,['Hyderabad( Banjara hills )'],"Dear Candidate,\nWe are seeking a skilled and experienced DBA Data Engineer to join our growing data team. The ideal candidate will play a key role in designing, implementing, and maintaining our databases and data pipeline architecture. You will collaborate with software engineers, data analysts, and DevOps teams to ensure efficient data flow, data integrity, and optimal database performance across all systems. This role requires a strong foundation in database administration, SQL performance tuning, data modeling, and experience with both on-prem and cloud-based environments.\n\nRequirements:\nBachelors degree in Computer Science, Information Systems, or a related field.\n5+ years of experience in database administration and data engineering.\nProven expertise in RDBMS (Oracle, MySQL, and PostgreSQL) and NoSQL systems (MongoDB, Cassandra).\nExperience managing databases in cloud environments (AWS, Azure, or GCP).\nProficiency in ETL processes and tools (e.g., Apache NiFi, Talend, Informatica, AWS Glue).\nStrong experience with scripting languages such as Python, Bash, or PowerShell.\n\nDBA Data Engineer Roles & Responsibilities:\nDesign and maintain scalable and high-performance database architectures.\nMonitor and optimize database performance using tools like CloudWatch, Oracle Enterprise Manager, pgAdmin, Mongo Compass, or Dynatrace.\nDevelop and manage ETL/ELT pipelines to support business intelligence and analytics.\nEnsure data integrity and security through best practices in backup, recovery, and encryption.\nAutomate regular database maintenance tasks using scripting and scheduled jobs.\nImplement high availability, failover, and disaster recovery strategies.\nConduct performance tuning of queries, stored procedures, indexes, and table structures.\nCollaborate with DevOps to automate database deployments using CI/CD and IaC tools (e.g., Terraform, AWS CloudFormation).\nDesign and implement data models, including star/snowflake schemas for data warehousing.\nDocument data flows, data dictionaries, and database configurations.\nManage user access and security policies using IAM roles or database-native permissions.\nAnalyze existing data systems and propose modernization or migration plans (on-prem to cloud, SQL to NoSQL, etc.).\nUse AWS RDS, Amazon Redshift, Azure SQL Database, or Google BigQuery as needed.\nStay up-to-date with emerging database technologies and make recommendations.\n\nMust-Have Skills:\nDeep knowledge of SQL and database performance tuning.\nHands-on experience with database migrations and replication strategies.\nFamiliarity with data governance, data quality, and compliance frameworks (GDPR, HIPAA, etc.).\nStrong problem-solving and troubleshooting skills.\nExperience with data streaming platforms such as Apache Kafka, AWS Kinesis, or Apache Flink is a plus.\nExperience with data lake and data warehouse architectures.\nExcellent communication and documentation skills.\n\nSoft Skills:\nProblem-Solving: Ability to analyze complex problems and develop effective solutions.\nCommunication Skills: Strong verbal and written communication skills to effectively collaborate with cross-functional teams.\nAnalytical Thinking: Ability to think critically and analytically to solve technical challenges.\nTime Management: Capable of managing multiple tasks and deadlines in a fast-paced environment.\nAdaptability: Ability to quickly learn and adapt to new technologies and methodologies.\n\nInterview Mode : F2F for who are residing in Hyderabad / Zoom for other states\nLocation : 43/A, MLA Colony,Road no 12, Banjara Hills, 500034\nTime : 2 - 4pm",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Database Administration', 'Database Migration', 'Dba Skills', 'High Availability', 'Db Administration', 'Hadr', 'Database Security', 'Disaster Recovery', 'Dr Testing', 'DR', 'Luw', 'Db Upgrade', 'Brtools', 'UDDI', 'Os Migration', 'Dbase', 'DBMS', 'IBM DB2', 'Db Migration', 'Disaster Recovery Planning', 'Db Dba', 'Backup And Recovery']",2025-06-14 06:04:19
Data Engineer,ESolutioninc,4 - 9 years,Not Disclosed,"['Navi Mumbai', 'Pune', 'Mumbai (All Areas)']","Job Description :\n\nJob Overview:\nWe are seeking a highly skilled Data Engineer with expertise in SQL, Python, Data Warehousing, AWS, Airflow, ETL, and Data Modeling. The ideal candidate will be responsible for designing, developing, and maintaining robust data pipelines, ensuring efficient data processing and integration across various platforms. This role requires strong problem-solving skills, an analytical mindset, and a deep understanding of modern data engineering frameworks.Key Responsibilities:\nDesign, develop, and optimize scalable data pipelines and ETL processes to support business intelligence, analytics, and operational data needs.\nBuild and maintain data models (conceptual, logical, and physical) to enhance data storage, retrieval, and transformation efficiency.\nDevelop, test, and optimize complex SQL queries for efficient data extraction, transformation, and loading (ETL).\nImplement and manage data warehousing solutions (e.g., Snowflake, Redshift, BigQuery) for structured and unstructured data storage.\nWork with AWS, Azure, and cloud-based data solutions to build high-performance data ecosystems.\nUtilize Apache Airflow for orchestrating workflows and automating data pipeline execution.\nCollaborate with cross-functional teams to understand business data requirements and ensure alignment with data strategies.\nEnsure data integrity, security, and compliance with governance policies and best practices.\nMonitor, troubleshoot, and improve the performance of existing data systems for scalability and reliability.\nStay updated with emerging data engineering technologies, frameworks, and best practices to drive continuous improvement.\nRequired Skills & Qualifications:\nProficiency in SQL for query development, performance tuning, and optimization.\nStrong Python programming skills for data processing, automation, and scripting.\nHands-on experience with ETL development, data integration, and transformation workflows.\nExpertise in data modeling for efficient database and data warehouse design.\nExperience with cloud platforms such as AWS (S3, Redshift, Lambda), Azure, or GCP.\nWorking knowledge of Airflow or similar workflow orchestration tools.\nFamiliarity with Big Data frameworks like Hadoop or Spark (preferred but not mandatory).\nStrong problem-solving skills and ability to work in a fast-paced, dynamic environment.Role & responsibilities\n\n\nPreferred candidate profile",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Advanced Python', 'Pyspark', 'Microsoft Azure', 'Data Modeling', 'Data Warehousing', 'ETL', 'Elt', 'Advanced SQl', 'AWS', 'SQL']",2025-06-14 06:04:22
Data Engineer,Dslr Technologies,4 - 6 years,Not Disclosed,['Gurugram'],"Job Title: Data Engineer\nLocation: Gurugram (WFO)\nExperience: 46 years\nDepartment: Engineering / Data & Analytics\n\nAbout Aramya\nAt Aramya, were redefining fashion for Indias underserved Gen X/Y women, offering size-inclusive, comfortable, and stylish ethnic wear at affordable prices. Launched in 2024, weve already achieved 40 Cr in revenue in our first year, driven by a unique blend of data-driven design, in-house manufacturing, and a proprietary supply chain.\n\nToday, with an ARR of 100 Cr, were scaling rapidly with ambitious growth plans for the future.\nOur vision is bold to build the most loved fashion and lifestyle brands across the world while empowering individuals to express themselves effortlessly. Backed by marquee investors like Accel and Z47, we’re on a mission to make high-quality ethnic wear accessible to every woman.\nWe’ve built a community of loyal customers who love our weekly design launches, impeccable quality, and value-for-money offerings. With a fast-moving team driven by creativity, technology, and customer obsession, Aramya is more than a fashion brand—it’s a movement to celebrate every woman’s unique journey.\n\nRole Overview\nWe’re looking for a results-driven Data Engineer who will play a key role in building and scaling our data infrastructure. This individual will own our end-to-end data pipelines, backend services for analytics, and infrastructure automation—powering real-time decision-making across our business.\nThis is a high-impact role for someone passionate about data architecture, cloud engineering, and creating a foundation for scalable insights in a fast-paced D2C environment.\nKey Responsibilities\nDesign, build, and manage scalable ETL/ELT pipelines using tools like Apache Airflow, Databricks, or Spark.\nOwn and optimize data lakes and data warehouses on AWS Redshift (or Snowflake/BigQuery).\nDevelop robust and scalable backend APIs using Python (FastAPI/Django/Flask) or Node.js.\nIntegrate third-party data sources (APIs, SFTP, flat files) and ensure data validation and consistency.\nEnsure high availability, observability, and fault-tolerance of data systems via logging, monitoring, and alerting.\nCollaborate with analysts, product managers, and business stakeholders to gather requirements and define data contracts.\nImplement Infrastructure-as-Code using tools like Terraform or AWS CDK to automate data workflows and provisioning.\n\nMust-Have Skills\nProficiency in SQL and data modeling for both OLTP and OLAP systems.\nStrong Python skills, with demonstrated experience in both backend and data engineering use cases.\nHands-on experience with Databricks, Apache Spark, and AWS Redshift.\nExperience in Airflow, dbt, or other workflow orchestration tools.\nWorking knowledge of REST APIs, backend architectures, and microservices.\nFamiliarity with Docker, Git, and CI/CD pipelines.\nExperience working on AWS cloud (S3, Lambda, ECS/Fargate, CloudWatch, etc.).\nNice-to-Have Skills\nExperience with streaming platforms like Kafka, Flink, or Kinesis.\nExposure to Snowflake, BigQuery, or Delta Lake.\nUnderstanding of data governance and PII handling best practices.\nExperience with GraphQL, gRPC, or event-driven architectures.\nFamiliarity with data observability tools like Monte Carlo, Great Expectations, or Datafold.\nPrior experience in D2C, e-commerce, or high-growth startup environments.\nQualifications\nBachelor’s degree in Computer Science, Data Engineering, or related technical discipline.\n4–6 years of experience in data engineering roles with strong backend and cloud integration exposure.",Industry Type: Textile & Apparel (Fashion),Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Engineering', 'ETL', 'Python', 'Rest Api Development', 'Spark', 'Apache', 'AWS']",2025-06-14 06:04:24
Data Engineer,Invitusdata,1 - 4 years,Not Disclosed,['Chennai'],"Strong understanding of Data Warehousing concepts and data modeling techniques and columnar databases\n\nStrong Hands on experience in Data Analysis, preprocessing and validation of the engineered dataset in SQL/NoSQL databases\n\nHands-on Experience in data cleansing,processing and loading using Big-data tools mainly (Py)Spark and Hive/Presto\n\nExposure to any workflow scheduling & Orchestration tools Airflow/Prefect/SQS would be helpful\n\nComfortable working with any programming/scripting language like Python(Preferred)/Scala/Shell scripting\n\nStrong data engineering skills on any Cloud Platform is essential(Preferred: AWS)\n\nExposure to data lake and data lake house would be an added advantage\n\nSolid Understanding of data structures and algorithms\n\nKnowledge of distributed/MPP systems pertaining to data storage and computing\n\nAbility to effectively communicate with both business and technical teams\n\nGood interpersonal skills and positive attitude\n\nExperience in working with agile methodology in a fast paced environment\n\n""] , keyResponsibilities:Design , Develop and implement ETL data pipelines that load data into an information product that helps the organization in reaching strategic goals\n\nWork on ingesting, storing, cleansing, processing and analyzing large data sets from heterogeneous data sources\n\nFinalizing the scope of the system and delivering Big Data solutions\n\nTranslate complex technical and functional requirements into detailed designs\n\nSchedule, orchestrate and Implement data pipelines and processes that scale with increase in data volume\n\nInvestigate and analyze alternative solutions to data storing, processing etc\n\nto ensure most streamlined approaches are implemented\n\nCollaborate with business consultants, data scientists, and application developers to develop analytics solutions\n\nHelp define data governance policies and support data versioning processes\n\nMaintain security & data privacy working closely with the Data Protection Officer internally\n\nAnalyze a vast number of data stores and uncover insights\n\nCreate data tools for business team members that assist them in analysis that gives them the competitive edge\n\nMonitor data application performance for potential bottlenecks and resolve performance issues\n\nIdentify and implement cost-saving strategies to reduce ongoing Big Data and Cloud costs/expenses",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data analysis', 'Data modeling', 'Shell scripting', 'Data structures', 'Workflow', 'Scheduling', 'Agile methodology', 'Analytics', 'SQL', 'Python']",2025-06-14 06:04:26
Data Engineer,Konrad Group,3 - 8 years,Not Disclosed,['Gurugram'],"About The Role\nAs a Data Engineer you ll be tasked with designing, building, and maintaining scalable data platforms and pipelines. Your deep knowledge of data platforms such as Azure Fabric, Databricks, and Snowflake will be essential as you collaborate closely with data analysts, scientists, and other engineers to ensure reliable, secure, and efficient data solutions.\nWhat You ll Do\nDesign, build, and manage robust data pipelines and data architectures.\nImplement solutions leveraging platforms such as Azure Fabric, Databricks, and Snowflake.\nOptimize data workflows, ensuring reliability, scalability, and performance.\nCollaborate with internal stakeholders to understand data needs and deliver tailored solutions.\nEnsure data security and compliance with industry standards and best practices.\nPerform data modelling, data extraction, transformation, and loading (ETL/ELT).\nIdentify and recommend innovative solutions to enhance data quality and analytics capabilities.\nQualifications\nBachelor s degree or higher in Computer Science, Data Engineering, Information Technology, or a related field.\nAt least 3 years of professional experience as a Data Engineer or similar role.\nProficiency in data platforms such as Azure Fabric, Databricks, and Snowflake.\nHands-on experience with data pipeline tools, cloud services, and storage solutions.\nStrong programming skills in SQL, Python, or related languages.\nExperience with big data technologies and concepts (Spark, Hadoop, Kafka).\nExcellent analytical, troubleshooting, and problem-solving skills.\nAbility to effectively communicate technical concepts clearly to non-technical stakeholders.\nNice to have\nCertifications related to Azure Data Engineering, Databricks, or Snowflake.\nFamiliarity with DevOps practices and CI/CD pipelines.\nPerks and Benefits\nComprehensive Health Wellness Benefits Package\nSocials, Outings Retreats\nRetirement Planning\nParental Leave Program\nCulture of Learning Development\nFlexible Working Hours\nWork from Home Flexibility\nService Recognition Programs",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Analytical', 'Genetics', 'Data quality', 'Troubleshooting', 'Information technology', 'Analytics', 'SQL', 'Python', 'Data extraction']",2025-06-14 06:04:28
Python Data Engineer,MNC IT,12 - 14 years,15-18 Lacs P.A.,"['Pune', 'Mumbai (All Areas)']","Bachelor of Engineering or equivalent.\n• Interested in learning new technologies and practices, reuse strategic platforms and standards, evaluate options, and make decisions with long-term sustainability in mind\nMust have\n• Strong communicator, from making presentations to technical writing\n• Experience in financial domain preferably in the areas of Financial accounting and Risk.\n• Strong data analysis and validation experience with attention to detail and understanding patterns\n• Good understanding of Credit Risk, workflow and reporting solutions\n• Have a flair for technology and are adept at leveraging the latest tools and technologies to increase team productivity and collaboration across dispersed teams\n• Experience in Python, SQL and PL/SQL (Oracle) and writing queries, scripts.\n• A real passion for and experience of Agile working practices, with a strong desire to work with baked in quality subject areas such as TDD, BDD, test automation and DevOps principles\n• Experience using DevOps toolsets like GitLab\n• Has prior experience of working on databases and usage of SQL to perform data analysis, preferably cloud technologies\n• Familiar with Azure Native Cloud services, software design and enterprise integration patterns.\n• Experience working with / exposure to NLP, Gen AI, ML and data modelling projects, a plus\n• Has prior experience of working on IT projects and has knowledge and experience of software development life cycle using Agile methodology\n• Is well structured, very reliable and dedicated; has high attention to detail, has ability to handle a significant number of dependencies and issues ensuring nothing is missed out.\n• Has excellent written and verbal communication skills, inter-personal and negotiation skills\n• Ability to work as part of a global team with multiple delivery teams and engage with stakeholders at various levels\n• Able to challenge the status quo and if required have the ability to push-back demands from stakeholders with right justification\n• Someone with a general ability to pick up information quickly and turn around deliverables under pressure\n• A team player with excellent people management skills\n• Takes ownership of tasks assigned to ultimate resolution\n• Accuracy and timeliness of delivering solutions using the best IT standards and practices",Industry Type: IT Services & Consulting,Department: Project & Program Management,"Employment Type: Full Time, Permanent","['PLSQL', 'Devops', 'SQL', 'Azure Native Cloud']",2025-06-14 06:04:30
Data Engineer,Agilisium,4 - 6 years,Not Disclosed,"['Hyderabad', 'Gurugram']","We are looking for a skilled Data Engineer with strong expertise in Python, PySpark, SQL, and AWS to join our data engineering team. The ideal candidate will be responsible for building scalable data pipelines, transforming large datasets, and enabling data-driven decision-making across the organization.\n\nRole & responsibilities",,,,"['Pyspark', 'AWS', 'Python', 'SQL', 'Delta Lake', 'Data Bricks']",2025-06-14 06:04:32
Data Engineer - Python/PySpark,Ekloud Inc,6 - 8 years,Not Disclosed,['Kolkata'],"Job Summary :\nWe are seeking an experienced Data Engineer with strong expertise in Databricks, Python, PySpark, and Power BI, along with a solid background in data integration and the modern Azure ecosystem. The ideal candidate will play a critical role in designing, developing, and implementing scalable data engineering\nsolutions and pipelines.\nKey Responsibilities :\n- Design, develop, and implement robust data solutions using Azure Data Factory, Databricks, and related data engineering tools.\n- Build and maintain scalable ETL/ELT pipelines with a focus on performance and reliability.\n- Write efficient and reusable code using Python and PySpark.\n- Perform data cleansing, transformation, and migration across various platforms.\n- Work hands-on with Azure Data Factory (ADF) for at least 1.5 to 2 years.\n- Develop and optimize SQL queries, stored procedures, and manage large data sets using SQL Server, T-SQL, PL/SQL, etc.\n- Collaborate with cross-functional teams to understand business requirements and provide data-driven solutions.\n- Engage directly with clients and business stakeholders to gather requirements, suggest optimal solutions, and ensure successful delivery.\n- Work with Power BI for basic reporting and data visualization tasks.\n- Apply strong knowledge of data warehousing concepts, modern data platforms, and cloud-based analytics.\n- Adhere to coding standards and best practices, including thorough documentation and testing (unit, integration, performance).\n- Support the operations, maintenance, and enhancement of existing data pipelines and architecture.\n- Estimate tasks and plan release cycles effectively.\nRequired Technical Skills :\n- Languages & Frameworks : Python, PySpark\n- Cloud & Tools : Azure Data Factory, Databricks, Azure ecosystem\n- Databases : SQL Server, T-SQL, PL/SQL\n- Reporting & BI Tools : Power BI (PBI)\n- Data Concepts : Data Warehousing, ETL/ELT, Data Cleansing, Data Migration\n- Other : Version control, Agile methodologies, good problem-solving skills\nPreferred Qualifications :\n- Experience with coding in Pysense within Databricks (added advantage)\n- Solid understanding of cloud data architecture and analytics processes\n- Ability to independently initiate and lead conversations with business stakeholders",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Azure Data Factory', 'Power BI', 'Data Pipeline', 'PySpark', 'Azure Databricks', 'Data Visualization', 'Data Warehousing', 'ETL', 'Python', 'SQL']",2025-06-14 06:04:35
Devops AWS DATA Engineeer|| Technical Analyst || 12Lakhs CTC,Robotics Technologies,5 - 7 years,11-12 Lacs P.A.,['Hyderabad( Banjara hills )'],"We are seeking a highly skilled Devops Engineer to join our dynamic development team. In this role, you will be responsible for designing, developing, and maintaining both frontend and backend components of our applications using Devops and associated technologies.\nYou will collaborate with cross-functional teams to deliver robust, scalable, and high-performing software solutions that meet our business needs. The ideal candidate will have a strong background in devops, experience with modern frontend frameworks, and a passion for full-stack development.\n\nRequirements:\nBachelor's degree in Computer Science Engineering, or a related field.\n5 to 7+ years of experience in full-stack development, with a strong focus on DevOps.\n\nDevOps with AWS Data Engineer - Roles & Responsibilities:\nUse AWS services like EC2, VPC, S3, IAM, RDS, and Route 53.\nAutomate infrastructure using Infrastructure as Code (IaC) tools like Terraform or AWS CloudFormation.\nBuild and maintain CI/CD pipelines using tools AWS CodePipeline, Jenkins,GitLab CI/CD.\nCross-Functional Collaboration\nAutomate build, test, and deployment processes for Java applications.\nUse Ansible, Chef, or AWS Systems Manager for managing configurations across environments.\nContainerize Java apps using Docker.\nDeploy and manage containers using Amazon ECS, EKS (Kubernetes), or Fargate.\nMonitoring & Logging using Amazon CloudWatch,Prometheus + Grafana,E\nStack (Elasticsearch, Logstash, Kibana),AWS X-Ray for distributed tracing manage access with IAM roles/policies.\nUse AWS Secrets Manager / Parameter Store for managing credentials.\nEnforce security best practices, encryption, and audits.\nAutomate backups for databases and services using AWS Backup, RDS Snapshots, and S3 lifecycle rules.\nImplement Disaster Recovery (DR) strategies.\nWork closely with development teams to integrate DevOps practices.\nDocument pipelines, architecture, and troubleshooting runbooks.\nMonitor and optimize AWS resource usage.\nUse AWS Cost Explorer, Budgets, and Savings Plans.\n\nMust-Have Skills:\nExperience working on Linux-based infrastructure.\nExcellent understanding of Ruby, Python, Perl, and Java.\nConfiguration and managing databases such as MySQL, Mongo.\nExcellent troubleshooting.\nSelecting and deploying appropriate CI/CD tools\nWorking knowledge of various tools, open-source technologies, and cloud services.\nAwareness of critical concepts in DevOps and Agile principles.\nManaging stakeholders and external interfaces.\nSetting up tools and required infrastructure.\nDefining and setting development, testing, release, update, and support processes for DevOps operation.\nHave the technical skills to review, verify, and validate the software code developed in the project.\nInterview Mode : F2F for who are residing in Hyderabad / Zoom for other states\nLocation : 43/A, MLA Colony,Road no 12, Banjara Hills, 500034\nTime : 2 - 4pm",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Iac', 'Devops', 'Jenkins', 'AWS', 'Kubernetes', 'RDS', 'Aws Cloudformation', 'Amazon Cloudwatch', 'Prometheus', 'Ci/Cd', 'Grafana', 'DR', 'Cloud Trail', 'Docker', 'IAM', 'Ansible / Chef', 'fargate', 'Gitlab', 'Monitoring', 'Python']",2025-06-14 06:04:37
MDM Associate Data Engineer,Amgen Inc,2 - 5 years,Not Disclosed,['Hyderabad'],"We are seeking an MDM Associate Data Engineerwith 25 years of experience to support and enhance our enterprise MDM (Master Data Management) platforms using Informatica/Reltio. This role is critical in delivering high-quality master data solutions across the organization, utilizing modern tools like Databricks and AWS to drive insights and ensure data reliability. The ideal candidate will have strong SQL, data profiling, and experience working with cross-functional teams in a pharma environment.To succeed in this role, the candidate must have strong data engineering experience along with MDM knowledge, hence the candidates having only MDM experience are not eligible for this role. Candidate must have data engineering experience on technologies like (SQL, Python,",,,,"['MDM', 'Informatica MDM', 'SQL queries', 'AWS cloud services', 'Reltio MDM platforms', 'data modeling', 'PySpark', 'IDQ', 'Databricks', 'data stewardship processes', 'JIRA', 'Python']",2025-06-14 06:04:40
MDM Associate Data Engineer,Amgen Inc,1 - 6 years,Not Disclosed,['Hyderabad'],"We are seeking an MDM Associate Data Engineer with 2-- 5 years of experience to support and enhance our enterprise MDM (Master Data Management) platforms using Informatica/Reltio. This role is critical in delivering high-quality master data solutions across the organization, utilizing modern tools like Databricks and AWS to drive insights and ensure data reliability. The ideal candidate will have strong SQL, data profiling, and experience working with cross-functional teams in a pharma environment. To succeed in this role, the candidate must have strong data engineering experience along with MDM knowledge, hence the candidates having only MDM experience are not eligible for this role. Candidate must have data engineering experience on technologies like (SQL, Python, PySpark , Databricks, AWS etc ), along with knowledge of MDM (Master Data Management)",,,,"['Python', 'AZURE', 'data modeling', 'Confluence', 'PySpark', 'MDM', 'IDQ', 'data governance', 'Databricks', 'JIRA', 'AWS', 'SQL']",2025-06-14 06:04:42
DATA ENGINEER,Svitla Systems,4 - 9 years,Not Disclosed,[],"Svitla Systems Inc. is looking for a Data Engineer (with ML/AI Experience) for a full-time position (40 hours per week) in India . Our client is the world s largest travel guidance platform, helping hundreds of millions each month become better travelers, from planning to booking to taking a trip. Travelers across the globe use the site and app to discover where to stay, what to do, and where to eat based on guidance from those who have been there before.\nWith more than 1 billion reviews and opinions from nearly 8 million businesses, travelers turn to clients to find deals on accommodations, book experiences, and reserve tables at delicious restaurants. They discover great places nearby as a travel guide company, available in 43 markets and 22 languages.\nAs a member of the Data Platform Enterprise Services Team, you will collaborate with engineering and business stakeholders to build, optimize, maintain, and secure the full data vertical, including tracking instrumentation, information architecture, ETL pipelines, and tooling that provide key analytics insights for business-critical decisions at the highest levels of product, finance, sales, CRM, marketing, data science, and more, all in a dynamic environment of continuously modernizing tech stack including highly scalable architecture, cloud-based infrastructure, and real-time responsiveness.\nRequirements:\nBS/MS in Computer Science or related field.\n4+ years of experience in data engineering or software development.\nProven data design and modeling with large datasets ( star/snowflake schema , SCDs, etc.).\nStrong SQL skills and ability to query large datasets.\nExperience with modern cloud data warehouses : Snowflake , BigQuery , etc.\nETL development experience: SLA , performance , and monitoring .\nFamiliarity with BI tools and semantic layer principles (e.g., Looker , Tableau ).\nUnderstanding of CI/CD , testing , documentation practices.\nComfortable in a fast-paced , dynamic environment .\nAbility to collaborate cross-functionally and communicate with technical/non-technical peers.\nStrong data investigation and problem-solving abilities.\nComfortable with ambiguity and focused on clean, maintainable data architecture .\nDetail-oriented with a strong sense of ownership .\nNice to Have\nExperience with data governance , data transformation tools .\nPrior work with e-commerce platforms .\nExperience with Airflow , Dagster , Monte Carlo , or Knowledge Graphs .\nResponsibilities:\nCollaborate with stakeholders from multiple teams to collect business requirements and translate them into technical data model solutions .\nDesign, build, and maintain efficient, scalable, and reusable data models in cloud data warehouses (e.g., Snowflake , BigQuery ).\nTransform data from many sources into clean, curated, standardized, and trustworthy data products .\nBuild data pipelines and ETL processes handling terabytes of data .\nAnalyze data using SQL and dashboards; ensure models align with business needs.\nEnsure data quality through testing, observability tools , and monitoring .\nTroubleshoot complex data issues , validate assumptions, and trace anomalies.\nParticipate in code reviews and help improve data development standards .\nWe offer\nUS and EU projects based on advanced technologies.\nCompetitive compensation based on skills and experience.\nAnnual performance appraisals.\nRemote-friendly culture and no micromanagement.\nPersonalized learning program tailored to your interests and skill development.\nBonuses for article writing, public talks, other activities.\n15 PTO days, 10 national holidays.\nFree webinars, meetups and conferences organized by Svitla.\nFun corporate celebrations and activities.\nAwesome team, friendly and supportive community!",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Article writing', 'Cloud', 'Schema', 'Instrumentation', 'Data quality', 'Analytics', 'Monitoring', 'CRM', 'SQL']",2025-06-14 06:04:44
Data Engineer,Digital Aptech,3 - 8 years,Not Disclosed,['Kolkata'],"Data Engineer || Product Based MNC (Direct Payroll) || Kolkata Location\nRole & responsibilities :\nUnderstands requirements and is involved in the discussions relating to technical and functional design of the sprint/ module/project\nDesign and implement end-to-end data solutions (storage, integration, processing, and visualization) in Azure.\nUsed various sources to ingest data into Azure Data Factory ,Azure Data Lake Storage (ADLS) such as SQL Server, Excel, Oracle, SQL Azure etc.\nExtract data from one database and load it into another\nBuild data architecture for ingestion, processing, and surfacing of data for large-scale applications\nUse many different scripting languages, understanding the nuances and benefits of each, to combine systems\nResearch and discover new methods to acquire data, and new applications for existing data\nWork with other members of the data team, including data architects, data analysts, and data scientists\nPrepare data sets for analysis and interpretation\nPerform statistical analysis and fine-tuning using test results\nCreate libraries and extend the existing frameworks\nCreate design documents basis discussions and assists in providing technical solutions for the business process\nPreferred candidate profile\nIn-depth understanding of database management systems, online analytical processing (OLAP) and ETL (Extract, transform, load) framework\n3+ years of overall experience with Azure, Data Factory and .Net\nStrong in Data factory and should be able to create manual and auto trigger pipelines\nShould be able to create, update, edit and delete ETL jobs in Azure Synapse Analytics\nRecreate existing application logic and functionality in the Azure Data Lake, Data Factory, SQL Database and SQL data warehouse environment.\nKnowledge of SQL queries, SQL Server Reporting Services (SSRS) and SQL Server Integration Services (SSIS)\nProven abilities to take initiative and be innovative\nAnalytical mind with a problem-solving aptitude\n10 LPA - 15 LPA Apply for this position Allowed Type(s): .pdf, .doc, .docx By using this form you agree with the storage and handling of your data by this website. * Your Next Step Towards Success Starts Here Why Choose Us\nFree Expert Consultation\nHave an idea but unsure how to execute it? Our industry experts offer free feasibility checks, expert advice, and actionable strategies tailored to your goals at no cost!\nComplimentary Technical Project Manager\nEvery project comes with a Complimentary Technical Project Manager to ensure smooth project management, offer valuable development guidance and keep everything on track.",Industry Type: Internet,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Business process', 'Payroll', 'Project management', 'Consulting', 'SSRS', 'OLAP', 'HTML', 'SSIS', 'Analytics', 'Data architecture']",2025-06-14 06:04:46
Data Engineer,Konrad Group,5 - 10 years,Not Disclosed,['Gurugram'],"About The Role\nAs a Senior Data Engineer you ll be tasked with designing, building, and maintaining scalable data platforms and pipelines. Your deep knowledge of data platforms such as Azure Fabric, Databricks, and Snowflake will be essential as you collaborate closely with data analysts, scientists, and other engineers to ensure reliable, secure, and efficient data solutions.\nWhat You ll Do\nDesign, build, and manage robust data pipelines and data architectures.\nImplement solutions leveraging platforms such as Azure Fabric, Databricks, and Snowflake.\nOptimize data workflows, ensuring reliability, scalability, and performance.\nCollaborate with internal stakeholders to understand data needs and deliver tailored solutions.\nEnsure data security and compliance with industry standards and best practices.\nPerform data modelling, data extraction, transformation, and loading (ETL/ELT).\nIdentify and recommend innovative solutions to enhance data quality and analytics capabilities.\nQualifications\nBachelor s degree or higher in Computer Science, Data Engineering, Information Technology, or a related field.\n5+ years of professional experience as a Data Engineer or similar role.\nProficiency in data platforms such as Azure Fabric, Databricks, and Snowflake.\nHands-on experience with data pipeline tools, cloud services, and storage solutions.\nStrong programming skills in SQL, Python, or related languages.\nExperience with big data technologies and concepts (Spark, Hadoop, Kafka).\nExcellent analytical, troubleshooting, and problem-solving skills.\nAbility to effectively communicate technical concepts clearly to non-technical stakeholders.\nNice to have\nCertifications related to Azure Data Engineering, Databricks, or Snowflake.\nFamiliarity with DevOps practices and CI/CD pipelines",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Analytical', 'Genetics', 'Data quality', 'Troubleshooting', 'Information technology', 'Analytics', 'SQL', 'Python', 'Data extraction']",2025-06-14 06:04:48
MDM Data Engineer,Amgen Inc,4 - 9 years,Not Disclosed,['Hyderabad'],"ABOUT THE ROLE\nRole Description:\nWe are seeking an experienced MDM Senior Data Engineer with 6-9 years of experience and expertise in backend engineering to work closely with business on development and operations of our Master Data Management (MDM) platforms, with hands-on experience in Informatica or Reltio and data engineering experience.\nThis role will also involve guiding junior data engineers/analysts, and quality experts to deliver high-performance, scalable, and governed MDM solutions that align with enterprise data strategy.",,,,"['Data Engineering', 'Unix', 'data modeling', 'PySpark', 'Informatica', 'AWS cloud', 'Reltio', 'Python', 'SQL']",2025-06-14 06:04:50
Data Engineer with API Development,Ntrix Innovations,7 - 12 years,Not Disclosed,[],"Job Title -Data Engineer with API Development\nExp - 7+ years\nLocation- Remote\nShift timing- 11am to 8:30pm\nWork type: Contract\n\n\n7+ Years of Overall IT experience.\n3+ Years of experience - Azure Architecture / Engineering (one or more of: Azure Functions, App Services, API Development)\n3+ Years of experience - Development (ex. Python, C#, Go or other)\n1+ CI/CD Experience (GitHub preferred)\n2+ Years of API Development experience (Creating APIs and Consuming APIs)\n\nNice to Have:\n\n- Service Bus\n- Terraform\n- ADF\n- Data Bricks\n- Spark\n- Scala",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Temporary/Contractual","['C#', 'Data Engineering', 'API Development', 'Python', 'github', 'Azure Architecture', 'Azure engineer', 'Terraform', 'Azure Functions', 'Azure App Service', 'cicd', 'azure data factory']",2025-06-14 06:04:52
Software Data Operations Engineer (MS+0),MAQ Software,2 - 4 years,Not Disclosed,"['Noida', 'Mumbai', 'Hyderabad']","MAQ LLC d.b.a MAQ Software has multiple openings at Redmond, WA for:\n\nSoftware Data Operations Engineer (MS+0)\n\nWill support data management projects to include architecting, programming, testing and modifying software to meet customer specifications. Deploy, configure, implement, test reports, analyze databases for errors and fix them. Automate user test scenarios, configure, debug and fix errors in cloud-based infrastructure and dashboards to meet customer needs. Must be able to travel temporarily to client sites and or relocate throughout the United States.\n\nRequirements: Masters Degree or foreign equivalent in Computer Science, Computer Applications, Computer Information Systems, Information Technology or related field.\n\nBenefits: Standard Employee Benefits.\n\nThe position qualifies for Employee Referral program.\n\nSend resume to 2027 152nd Avenue NE, Redmond, WA 98052, Attn: H.R. Manager.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Operations', 'Data Engineering', 'test scenarios', 'configuration', 'Data Management']",2025-06-14 06:04:54
"Hiring Alert:- Customer Data Platform & Integration, Google Cloud Data",Fortune Global 500 IT Services Firm,12 - 22 years,Not Disclosed,"['Bengaluru', 'Delhi / NCR', 'Mumbai (All Areas)']","Professional & Technical Skills:\n- Must To Have Skills: Proficiency in Customer Data Platform & Integration, Google Cloud Data Services.\n- Strong understanding of data integration and data management principles.\n- Experience in architecting and implementing scalable and secure data solutions.\n- Knowledge of cloud-based data services and technologies.\n- Hands-on experience with data modeling and database design.",Industry Type: Analytics / KPO / Research,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Integration', 'Data Engineering', 'implementation', 'Customer data platform', 'Cdp']",2025-06-14 06:04:56
Engineer III,AMERICAN EXPRESS,0 - 4 years,Not Disclosed,['Chennai'],"Here, your voice and ideas matter, your work makes an impact, and together, you will help us define the future of American Express.\nHow will you make an impact in this role?\nFunction Description:\nAmerican Express is embarking on an exciting transformation driven by an energetic new team of high performers. This group is nimble and creative with the power to shape our technology and product roadmap. If you have the talent and desire to deliver innovative digital and servicing products at a rapid pace, serving our customers seamlessly across physical, digital, mobile, and social media, join our transformation team! You will be part of a fast-paced, entrepreneurial team responsible for delivering projects platform supporting our global customer base.",,,,"['Unix', 'Computer science', 'Career development', 'Linux', 'Analytical', 'Social media', 'HTTP', 'Application development', 'Continuous improvement', 'Apache']",2025-06-14 06:04:59
Data Modeler - SQL / Erwin,Leading Client,7 - 10 years,Not Disclosed,[],"Employment Type : Contract (Remote).\n\nJob Summary :\n\nWe are looking for a highly skilled Data Engineer / Data Modeler with strong experience in Snowflake, DBT, and GCP to support our data infrastructure and modeling initiatives.\n\nThe ideal candidate should possess excellent SQL skills, hands-on experience with Erwin Data Modeler, and a strong background in modern data architectures and data modeling techniques.\n\nKey Responsibilities :\n\n- Design and implement scalable data models using Snowflake and Erwin Data Modeler.\n\n- Create, maintain, and enhance data pipelines using DBT and GCP (BigQuery, Cloud Storage, Dataflow).\n\n- Perform reverse engineering on existing systems (e.g., Sailfish/DDMS) using DBeaver or similar tools to understand and rebuild data models.\n\n- Develop efficient SQL queries and stored procedures for data transformation, quality, and validation.\n\n- Collaborate with business analysts and stakeholders to gather data requirements and convert them into physical and logical models.\n\n- Ensure performance tuning, security, and optimization of the Snowflake data warehouse.\n\n- Document metadata, data lineage, and business logic behind data structures and flows.\n\n- Participate in code reviews, enforce coding standards, and provide best practices for data modeling and governance.\n\nMust-Have Skills :\n\n- Snowflake architecture, schema design, and data warehouse experience.\n\n- DBT (Data Build Tool) for data transformation and pipeline development.\n\n- Strong expertise in SQL (query optimization, complex joins, window functions, etc.)\n\n- Hands-on experience with Erwin Data Modeler (logical and physical modeling).\n\n- Experience with GCP (BigQuery, Cloud Composer, Cloud Storage).\n\n- Experience in reverse engineering legacy systems like Sailfish or DDMS using DBeaver.\n\nGood To Have :\n\n- Experience with CI/CD tools and DevOps for data environments.\n\n- Familiarity with data governance, security, and privacy practices.\n\n- Exposure to Agile methodologies and working in distributed teams.\n\n- Knowledge of Python for data engineering tasks and orchestration scripts.\n\nSoft Skills :\n\n- Excellent problem-solving and analytical skills.\n\n- Strong communication and stakeholder management.\n\n- Self-driven with the ability to work independently in a remote setup.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Modeling', 'Data Quality', 'GCP', 'Snowflake architecture', 'Data Security', 'Data Warehousing', 'Data Governance', 'Erwin', 'SQL']",2025-06-14 06:05:01
Data Modeler Lead,Genzeon Corporation,10 - 20 years,25-40 Lacs P.A.,"['Hyderabad', 'Pune']","Data Modeler / Lead - Healthcare Data Systems\nPosition Overview\nWe are seeking an experienced Data Modeler/Lead with deep expertise in health plan data\nmodels and enterprise data warehousing to drive our healthcare analytics and reporting\ninitiatives. The candidate should have hands-on experience with modern data platforms\nand a strong understanding of healthcare industry data standards.\nKey Responsibilities\nData Architecture & Modeling\n•\nDesign and implement comprehensive data models for health plan operations,\nincluding member enrollment, claims processing, provider networks, and medical\nmanagement\n•\n•\n•\nDevelop logical and physical data models that support analytical and regulatory\nreporting requirements (HEDIS, Stars, MLR, risk adjustment)\nCreate and maintain data lineage documentation and data dictionaries for\nhealthcare datasets\nEstablish data modeling standards and best practices across the organization\nTechnical Leadership\n•\n•\n•\nLead data warehousing initiatives using modern platforms like Databricks or\ntraditional ETL tools like Informatica\nArchitect scalable data solutions that handle large volumes of healthcare\ntransactional data\nCollaborate with data engineers to optimize data pipelines and ensure data quality\nHealthcare Domain Expertise\n•\nApply deep knowledge of health plan operations, medical coding (ICD-10, CPT,\nHCPCS), and healthcare data standards (HL7, FHIR, X12 EDI)\n•\n•\nDesign data models that support analytical, reporting and AI/ML needs\nEnsure compliance with healthcare regulations including HIPAA/PHI, and state\ninsurance regulations\n\n\n•\nPartner with business stakeholders to translate healthcare business requirements\ninto technical data solutions\nData Governance & Quality\n•\n•\n•\nImplement data governance frameworks specic to healthcare data privacy and\nsecurity requirements\nEstablish data quality monitoring and validation processes for critical health plan\nmetrics\nLead eorts to standardize healthcare data denitions across multiple systems and\ndata sources\nRequired Qualications\nTechnical Skills\n•\n•\n•\n•\n•\n10+ years of experience in data modeling with at least 4 years focused on\nhealthcare/health plan data\nExpert-level prociency in dimensional modeling, data vault methodology, or other\nenterprise data modeling approaches\nHands-on experience with Informatica PowerCenter/IICS or Databricks platform for\nlarge-scale data processing\nStrong SQL skills and experience with Oracle Exadata and cloud data warehouses\n(Databricks)\nProciency with data modeling tools (Hackolade, ERwin, or similar)\nHealthcare Industry Knowledge\n•\nDeep understanding of health plan data structures including claims, eligibility,\nprovider data, and pharmacy data\n•\n•\nExperience with healthcare data standards and medical coding systems\nKnowledge of regulatory reporting requirements (HEDIS, Medicare Stars, MLR\nreporting, risk adjustment)\n•\nFamiliarity with healthcare interoperability standards (HL7 FHIR, X12 EDI)\nLeadership & Communication\n\n\n•\n•\n•\n•\nProven track record of leading data modeling projects in complex healthcare\nenvironments\nStrong analytical and problem-solving skills with ability to work with ambiguous\nrequirements\nExcellent communication skills with ability to explain technical concepts to\nbusiness stakeholders\nExperience mentoring team members and establishing technical standards\nPreferred Qualications\n•\nExperience with Medicare Advantage, Medicaid, or Commercial health plan\noperations\n•\n•\n•\n•\nCloud platform certications (AWS, Azure, or GCP)\nExperience with real-time data streaming and modern data lake architectures\nKnowledge of machine learning applications in healthcare analytics\nPrevious experience in a lead or architect role within healthcare organizations",Industry Type: IT Services & Consulting,Department: Other,"Employment Type: Full Time, Permanent","['Healthcare Domain', 'Relational Databases', 'Dimensional Modeling', 'Data Modeling', 'Data Architecture', 'Data Warehousing', 'Informatica', 'Oracle', 'Data Bricks']",2025-06-14 06:05:03
Principal Data Solution Architect,Goodyear,12 - 16 years,Not Disclosed,['Hyderabad'],"A\nRoles & Responsibilities:\nResponsible for designing/building data products, logical data layers, data streams, algorithms and reporting systems (i.e. dashboards, front ends). Secure correct designed solution, performance and scalability Considering appropriate cost control. Link data product design with DevOps and Infrastructure. Act as a reference within and outside DEA team.\nBe a technical partner to Data Engineer(s) regarding digital product implementation. Responsible for developing visualizations for complex data sets. Provide guidance to internal DEA associates, IT and business users on data solutions available and related guidelines/standards.\nFamiliarity with neuro-linguistic programming (NLP) and other advanced techniques to simplify interfaces. Work across data analytics projects to provide support in data analytics methodology, processes and standards. Create/Deliver user training.\nResponsible for user acceptance testing (in collaboration with business/demand owners). Work closely with internal team members, on/off-shore contractors and strategic business unit (SBU) IT/business associates to develop guiding principles/best practices for determining solution architecture that will be needed for a particular information requirement.\nDevelop an in-depth understanding of DEA data for communication and support of business/SBU support. Work with the SBU counterparts to develop/manage roles and processes for on-going user support and solution architecture administration. Participate/Assist in conducting user group meetings\nSkills Required:\nProven success interfacing with the business community and identifying business requirements\nMinimum 5+ years of experience in developing advanced data pipelines in Python & SQL.\nStrong experience of configuring and deploying AWS infrastructure using Terraform, etc.\nStrong experience of developing DevOps pipelines using GitHub / GitHub Actions.\nPrior experience of working with Snowflake and/or other Data Warehouses.\nSome exposure to AI/ML based applications is good to have.\nExcellent analytical and problem-solving skill\nExpertise in data warehouse concepts, methodology and technology\nExcellent interpersonal/communication skills - Ability to work in a dynamic team environment and be comfortable/credible interacting with both technical/business organizations and executive management\nThe highest-level of personal ethics - Ability to keep sensitive information confidential - Unquestionable integrity and character\nKnowledgeable in current and possible future policies, practices, trends, technologies and information\nUnderstands and works with the organization's mission, operations, structure and goals\nWorking knowledge of Goodyear operational systems (e.g. SAP/R3) - Desired",Industry Type: Auto Components (Tyre),Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Snowflake', 'AWS', 'Python', 'Terraform', 'Github', 'Artificial Intelligence', 'AI', 'Machine Learning', 'SQL', 'ML']",2025-06-14 06:05:06
Data Modeler - SQL/Erwin,Leading Client,7 - 10 years,Not Disclosed,['Jaipur'],"Job Title : Data Engineer / Data Modeler.\n\nLocation : Remote (India).\n\nEmployment Type : Contract (Remote).\n\nExperience Required : 7+ Years.\n\nJob Summary :\n\nWe are looking for a highly skilled Data Engineer / Data Modeler with strong experience in Snowflake, DBT, and GCP to support our data infrastructure and modeling initiatives.\n\nThe ideal candidate should possess excellent SQL skills, hands-on experience with Erwin Data Modeler, and a strong background in modern data architectures and data modeling techniques.\n\nKey Responsibilities :\n\n- Design and implement scalable data models using Snowflake and Erwin Data Modeler.\n\n- Create, maintain, and enhance data pipelines using DBT and GCP (BigQuery, Cloud Storage, Dataflow).\n\n- Perform reverse engineering on existing systems (e.g., Sailfish/DDMS) using DBeaver or similar tools to understand and rebuild data models.\n\n- Develop efficient SQL queries and stored procedures for data transformation, quality, and validation.\n\n- Collaborate with business analysts and stakeholders to gather data requirements and convert them into physical and logical models.\n\n- Ensure performance tuning, security, and optimization of the Snowflake data warehouse.\n\n- Document metadata, data lineage, and business logic behind data structures and flows.\n\n- Participate in code reviews, enforce coding standards, and provide best practices for data modeling and governance.\n\nMust-Have Skills :\n\n- Snowflake architecture, schema design, and data warehouse experience.\n\n- DBT (Data Build Tool) for data transformation and pipeline development.\n\n- Strong expertise in SQL (query optimization, complex joins, window functions, etc.)\n\n- Hands-on experience with Erwin Data Modeler (logical and physical modeling).\n\n- Experience with GCP (BigQuery, Cloud Composer, Cloud Storage).\n\n- Experience in reverse engineering legacy systems like Sailfish or DDMS using DBeaver.\n\nGood To Have :\n\n- Experience with CI/CD tools and DevOps for data environments.\n\n- Familiarity with data governance, security, and privacy practices.\n\n- Exposure to Agile methodologies and working in distributed teams.\n\n- Knowledge of Python for data engineering tasks and orchestration scripts.\n\nSoft Skills :\n\n- Excellent problem-solving and analytical skills.\n\n- Strong communication and stakeholder management.\n\n- Self-driven with the ability to work independently in a remote setup.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Modeling', 'Data Quality', 'Data Build Tool', 'Data Security', 'Snowflake DB', 'Data Warehousing', 'Data Analytics', 'Data Governance', 'Erwin', 'SQL']",2025-06-14 06:05:08
Data Modeler - SQL / Erwin,Leading Client,7 - 10 years,Not Disclosed,[],"Employment Type : Contract (Remote).\n\nJob Summary :\n\nWe are looking for a highly skilled Data Engineer / Data Modeler with strong experience in Snowflake, DBT, and GCP to support our data infrastructure and modeling initiatives.\n\nThe ideal candidate should possess excellent SQL skills, hands-on experience with Erwin Data Modeler, and a strong background in modern data architectures and data modeling techniques.\n\nKey Responsibilities :\n\n- Design and implement scalable data models using Snowflake and Erwin Data Modeler.\n\n- Create, maintain, and enhance data pipelines using DBT and GCP (BigQuery, Cloud Storage, Dataflow).\n\n- Perform reverse engineering on existing systems (e.g., Sailfish/DDMS) using DBeaver or similar tools to understand and rebuild data models.\n\n- Develop efficient SQL queries and stored procedures for data transformation, quality, and validation.\n\n- Collaborate with business analysts and stakeholders to gather data requirements and convert them into physical and logical models.\n\n- Ensure performance tuning, security, and optimization of the Snowflake data warehouse.\n\n- Document metadata, data lineage, and business logic behind data structures and flows.\n\n- Participate in code reviews, enforce coding standards, and provide best practices for data modeling and governance.\n\nMust-Have Skills :\n\n- Snowflake architecture, schema design, and data warehouse experience.\n\n- DBT (Data Build Tool) for data transformation and pipeline development.\n\n- Strong expertise in SQL (query optimization, complex joins, window functions, etc.)\n\n- Hands-on experience with Erwin Data Modeler (logical and physical modeling).\n\n- Experience with GCP (BigQuery, Cloud Composer, Cloud Storage).\n\n- Experience in reverse engineering legacy systems like Sailfish or DDMS using DBeaver.\n\nGood To Have :\n\n- Experience with CI/CD tools and DevOps for data environments.\n\n- Familiarity with data governance, security, and privacy practices.\n\n- Exposure to Agile methodologies and working in distributed teams.\n\n- Knowledge of Python for data engineering tasks and orchestration scripts.\n\nSoft Skills :\n\n- Excellent problem-solving and analytical skills.\n\n- Strong communication and stakeholder management.\n\n- Self-driven with the ability to work independently in a remote setup.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Modeling', 'Data Quality', 'GCP', 'Snowflake architecture', 'Data Security', 'Data Warehousing', 'Data Governance', 'Erwin', 'SQL']",2025-06-14 06:05:10
Data Modeler - SQL/Erwin,Leading Client,7 - 10 years,Not Disclosed,['Kolkata'],"Job Title : Data Engineer / Data Modeler.\n\nLocation : Remote (India).\n\nEmployment Type : Contract (Remote).\n\nExperience Required : 7+ Years.\n\nJob Summary :\n\nWe are looking for a highly skilled Data Engineer / Data Modeler with strong experience in Snowflake, DBT, and GCP to support our data infrastructure and modeling initiatives.\n\nThe ideal candidate should possess excellent SQL skills, hands-on experience with Erwin Data Modeler, and a strong background in modern data architectures and data modeling techniques.\n\nKey Responsibilities :\n\n- Design and implement scalable data models using Snowflake and Erwin Data Modeler.\n\n- Create, maintain, and enhance data pipelines using DBT and GCP (BigQuery, Cloud Storage, Dataflow).\n\n- Perform reverse engineering on existing systems (e.g., Sailfish/DDMS) using DBeaver or similar tools to understand and rebuild data models.\n\n- Develop efficient SQL queries and stored procedures for data transformation, quality, and validation.\n\n- Collaborate with business analysts and stakeholders to gather data requirements and convert them into physical and logical models.\n\n- Ensure performance tuning, security, and optimization of the Snowflake data warehouse.\n\n- Document metadata, data lineage, and business logic behind data structures and flows.\n\n- Participate in code reviews, enforce coding standards, and provide best practices for data modeling and governance.\n\nMust-Have Skills :\n\n- Snowflake architecture, schema design, and data warehouse experience.\n\n- DBT (Data Build Tool) for data transformation and pipeline development.\n\n- Strong expertise in SQL (query optimization, complex joins, window functions, etc.)\n\n- Hands-on experience with Erwin Data Modeler (logical and physical modeling).\n\n- Experience with GCP (BigQuery, Cloud Composer, Cloud Storage).\n\n- Experience in reverse engineering legacy systems like Sailfish or DDMS using DBeaver.\n\nGood To Have :\n\n- Experience with CI/CD tools and DevOps for data environments.\n\n- Familiarity with data governance, security, and privacy practices.\n\n- Exposure to Agile methodologies and working in distributed teams.\n\n- Knowledge of Python for data engineering tasks and orchestration scripts.\n\nSoft Skills :\n\n- Excellent problem-solving and analytical skills.\n\n- Strong communication and stakeholder management.\n\n- Self-driven with the ability to work independently in a remote setup.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SQL', 'Data Quality', 'Data Build Tool', 'Data Security', 'Snowflake DB', 'Data Modeling', 'Data Warehousing', 'Data Analytics', 'Data Governance', 'Erwin']",2025-06-14 06:05:13
Data Modeler - SQL/Erwin,Leading Client,7 - 10 years,Not Disclosed,['Surat'],"Job Title : Data Engineer / Data Modeler.\n\nLocation : Remote (India).\n\nEmployment Type : Contract (Remote).\n\nExperience Required : 7+ Years.\n\nJob Summary :\n\nWe are looking for a highly skilled Data Engineer / Data Modeler with strong experience in Snowflake, DBT, and GCP to support our data infrastructure and modeling initiatives.\n\nThe ideal candidate should possess excellent SQL skills, hands-on experience with Erwin Data Modeler, and a strong background in modern data architectures and data modeling techniques.\n\nKey Responsibilities :\n\n- Design and implement scalable data models using Snowflake and Erwin Data Modeler.\n\n- Create, maintain, and enhance data pipelines using DBT and GCP (BigQuery, Cloud Storage, Dataflow).\n\n- Perform reverse engineering on existing systems (e.g., Sailfish/DDMS) using DBeaver or similar tools to understand and rebuild data models.\n\n- Develop efficient SQL queries and stored procedures for data transformation, quality, and validation.\n\n- Collaborate with business analysts and stakeholders to gather data requirements and convert them into physical and logical models.\n\n- Ensure performance tuning, security, and optimization of the Snowflake data warehouse.\n\n- Document metadata, data lineage, and business logic behind data structures and flows.\n\n- Participate in code reviews, enforce coding standards, and provide best practices for data modeling and governance.\n\nMust-Have Skills :\n\n- Snowflake architecture, schema design, and data warehouse experience.\n\n- DBT (Data Build Tool) for data transformation and pipeline development.\n\n- Strong expertise in SQL (query optimization, complex joins, window functions, etc.)\n\n- Hands-on experience with Erwin Data Modeler (logical and physical modeling).\n\n- Experience with GCP (BigQuery, Cloud Composer, Cloud Storage).\n\n- Experience in reverse engineering legacy systems like Sailfish or DDMS using DBeaver.\n\nGood To Have :\n\n- Experience with CI/CD tools and DevOps for data environments.\n\n- Familiarity with data governance, security, and privacy practices.\n\n- Exposure to Agile methodologies and working in distributed teams.\n\n- Knowledge of Python for data engineering tasks and orchestration scripts.\n\nSoft Skills :\n\n- Excellent problem-solving and analytical skills.\n\n- Strong communication and stakeholder management.\n\n- Self-driven with the ability to work independently in a remote setup.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SQL/Erwin', 'Data Quality', 'Data Build Tool', 'Data Security', 'Snowflake DB', 'Data Modeling', 'Data Warehousing', 'Data Analytics', 'Data Governance', 'Erwin', 'SQL']",2025-06-14 06:05:15
Data Consultant-Elastic Developer,Kyndryl,10 - 15 years,Not Disclosed,['Bengaluru'],"Who We Are\nAt Kyndryl, we design, build, manage and modernize the mission-critical technology systems that the world depends on every day. So why work at Kyndryl? We are always moving forward – always pushing ourselves to go further in our efforts to build a more equitable, inclusive world for our employees, our customers and our communities.\n\nThe Role\nAs an ELK (Elastic, Logstash & Kibana) Data Engineer, you would be responsible for developing, implementing, and maintaining the ELK stack-based solutions for Kyndryl’s clients. This role would be responsible to develop efficient and effective, data & log ingestion, processing, indexing, and visualization for monitoring, troubleshooting, and analysis purposes.",,,,"['kubernetes', 'analytical', 'logstash', 'elastic search', 'scripting', 'aix', 'automation', 'gcp', 'devops', 'linux', 'writing', 'aix environment', 'debugging', 'shell scripting', 'mysql', 'prometheus', 'kibana', 'python', 'elk', 'beats', 'microsoft azure', 'nosql', 'servicenow', 'grafana', 'microsoft windows', 'troubleshooting', 'splunk', 'aws']",2025-06-14 06:05:17
Azure Data Engineer,Randomtrees,2 - 6 years,Not Disclosed,['Chennai'],"We are seeking a highly skilled and motivated Azure Data Engineer to join our growing data team. In this role, you will be responsible for designing, developing, and maintaining scalable and robust data pipelines and data solutions within the Microsoft Azure ecosystem. You will work closely with data scientists, analysts, and business stakeholders to understand data requirements and translate them into effective data architectures. The ideal candidate will have a strong background in data warehousing, ETL/ELT processes, and a deep understanding of Azure data services.\nResponsibilities:",,,,"['Azure Data Factory', 'Pyspark', 'Azure Databricks', 'SQL']",2025-06-14 06:05:20
Data Modeler - SQL/Erwin,Leading Client,7 - 10 years,Not Disclosed,['Noida'],"Job Title : Data Engineer / Data Modeler.\n\nLocation : Remote (India).\n\nEmployment Type : Contract (Remote).\n\nExperience Required : 7+ Years.\n\nJob Summary :\n\nWe are looking for a highly skilled Data Engineer / Data Modeler with strong experience in Snowflake, DBT, and GCP to support our data infrastructure and modeling initiatives.\n\nThe ideal candidate should possess excellent SQL skills, hands-on experience with Erwin Data Modeler, and a strong background in modern data architectures and data modeling techniques.\n\nKey Responsibilities :\n\n- Design and implement scalable data models using Snowflake and Erwin Data Modeler.\n\n- Create, maintain, and enhance data pipelines using DBT and GCP (BigQuery, Cloud Storage, Dataflow).\n\n- Perform reverse engineering on existing systems (e.g., Sailfish/DDMS) using DBeaver or similar tools to understand and rebuild data models.\n\n- Develop efficient SQL queries and stored procedures for data transformation, quality, and validation.\n\n- Collaborate with business analysts and stakeholders to gather data requirements and convert them into physical and logical models.\n\n- Ensure performance tuning, security, and optimization of the Snowflake data warehouse.\n\n- Document metadata, data lineage, and business logic behind data structures and flows.\n\n- Participate in code reviews, enforce coding standards, and provide best practices for data modeling and governance.\n\nMust-Have Skills :\n\n- Snowflake architecture, schema design, and data warehouse experience.\n\n- DBT (Data Build Tool) for data transformation and pipeline development.\n\n- Strong expertise in SQL (query optimization, complex joins, window functions, etc.)\n\n- Hands-on experience with Erwin Data Modeler (logical and physical modeling).\n\n- Experience with GCP (BigQuery, Cloud Composer, Cloud Storage).\n\n- Experience in reverse engineering legacy systems like Sailfish or DDMS using DBeaver.\n\nGood To Have :\n\n- Experience with CI/CD tools and DevOps for data environments.\n\n- Familiarity with data governance, security, and privacy practices.\n\n- Exposure to Agile methodologies and working in distributed teams.\n\n- Knowledge of Python for data engineering tasks and orchestration scripts.\n\nSoft Skills :\n\n- Excellent problem-solving and analytical skills.\n\n- Strong communication and stakeholder management.\n\n- Self-driven with the ability to work independently in a remote setup.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SQL/Erwin', 'Data Quality', 'Data Build Tool', 'Data Security', 'Snowflake DB', 'Data Modeling', 'Data Warehousing', 'Data Analytics', 'Data Governance', 'Erwin', 'SQL']",2025-06-14 06:05:22
Data Modeler - SQL/Erwin,Leading Client,7 - 10 years,Not Disclosed,[],"Employment Type : Contract (Remote).\n\nExperience Required : 7+ Years.\n\nJob Summary :\n\nWe are looking for a highly skilled Data Engineer / Data Modeler with strong experience in Snowflake, DBT, and GCP to support our data infrastructure and modeling initiatives.\n\nThe ideal candidate should possess excellent SQL skills, hands-on experience with Erwin Data Modeler, and a strong background in modern data architectures and data modeling techniques.\n\nKey Responsibilities :\n\n- Design and implement scalable data models using Snowflake and Erwin Data Modeler.\n\n- Create, maintain, and enhance data pipelines using DBT and GCP (BigQuery, Cloud Storage, Dataflow).\n\n- Perform reverse engineering on existing systems (e.g., Sailfish/DDMS) using DBeaver or similar tools to understand and rebuild data models.\n\n- Develop efficient SQL queries and stored procedures for data transformation, quality, and validation.\n\n- Collaborate with business analysts and stakeholders to gather data requirements and convert them into physical and logical models.\n\n- Ensure performance tuning, security, and optimization of the Snowflake data warehouse.\n\n- Document metadata, data lineage, and business logic behind data structures and flows.\n\n- Participate in code reviews, enforce coding standards, and provide best practices for data modeling and governance.\n\nMust-Have Skills :\n\n- Snowflake architecture, schema design, and data warehouse experience.\n\n- DBT (Data Build Tool) for data transformation and pipeline development.\n\n- Strong expertise in SQL (query optimization, complex joins, window functions, etc.)\n\n- Hands-on experience with Erwin Data Modeler (logical and physical modeling).\n\n- Experience with GCP (BigQuery, Cloud Composer, Cloud Storage).\n\n- Experience in reverse engineering legacy systems like Sailfish or DDMS using DBeaver.\n\nGood To Have :\n\n- Experience with CI/CD tools and DevOps for data environments.\n\n- Familiarity with data governance, security, and privacy practices.\n\n- Exposure to Agile methodologies and working in distributed teams.\n\n- Knowledge of Python for data engineering tasks and orchestration scripts.\n\nSoft Skills :\n\n- Excellent problem-solving and analytical skills.\n\n- Strong communication and stakeholder management.\n\n- Self-driven with the ability to work independently in a remote setup.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SQL', 'Data Quality', 'Data Build Tool', 'Data Security', 'Snowflake DB', 'Data Modeling', 'Data Warehousing', 'Data Analytics', 'Data Governance', 'Erwin']",2025-06-14 06:05:24
Data Modeler - SQL/Erwin,Leading Client,7 - 10 years,Not Disclosed,[],"Employment Type : Contract (Remote).\n\nExperience Required : 7+ Years.\n\nJob Summary :\n\nWe are looking for a highly skilled Data Engineer / Data Modeler with strong experience in Snowflake, DBT, and GCP to support our data infrastructure and modeling initiatives.\n\nThe ideal candidate should possess excellent SQL skills, hands-on experience with Erwin Data Modeler, and a strong background in modern data architectures and data modeling techniques.\n\nKey Responsibilities :\n\n- Design and implement scalable data models using Snowflake and Erwin Data Modeler.\n\n- Create, maintain, and enhance data pipelines using DBT and GCP (BigQuery, Cloud Storage, Dataflow).\n\n- Perform reverse engineering on existing systems (e.g., Sailfish/DDMS) using DBeaver or similar tools to understand and rebuild data models.\n\n- Develop efficient SQL queries and stored procedures for data transformation, quality, and validation.\n\n- Collaborate with business analysts and stakeholders to gather data requirements and convert them into physical and logical models.\n\n- Ensure performance tuning, security, and optimization of the Snowflake data warehouse.\n\n- Document metadata, data lineage, and business logic behind data structures and flows.\n\n- Participate in code reviews, enforce coding standards, and provide best practices for data modeling and governance.\n\nMust-Have Skills :\n\n- Snowflake architecture, schema design, and data warehouse experience.\n\n- DBT (Data Build Tool) for data transformation and pipeline development.\n\n- Strong expertise in SQL (query optimization, complex joins, window functions, etc.)\n\n- Hands-on experience with Erwin Data Modeler (logical and physical modeling).\n\n- Experience with GCP (BigQuery, Cloud Composer, Cloud Storage).\n\n- Experience in reverse engineering legacy systems like Sailfish or DDMS using DBeaver.\n\nGood To Have :\n\n- Experience with CI/CD tools and DevOps for data environments.\n\n- Familiarity with data governance, security, and privacy practices.\n\n- Exposure to Agile methodologies and working in distributed teams.\n\n- Knowledge of Python for data engineering tasks and orchestration scripts.\n\nSoft Skills :\n\n- Excellent problem-solving and analytical skills.\n\n- Strong communication and stakeholder management.\n\n- Self-driven with the ability to work independently in a remote setup.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SQL/Erwin', 'Data Quality', 'Data Build Tool', 'Data Security', 'Snowflake DB', 'Data Modeling', 'Data Warehousing', 'Data Analytics', 'Data Governance', 'Erwin', 'SQL']",2025-06-14 06:05:27
Databricks Unified Data Analytics Platform- Pan India,Fortune Global 500 IT Services Firm,8 - 13 years,Not Disclosed,['Bhubaneswar'],"Project Role : Application Developer\nProject Role Description : Design, build and configure applications to meet business process and application requirements.\nMust have skills : Databricks Unified Data Analytics Platform\nGood to have skills : NA\nMinimum 7.5 year(s) of experience is required\nEducational Qualification : 15 years of fulltime education or above\nSummary:\nAs an Application Developer, you will be responsible for designing, building, and configuring applications to meet business process and application requirements using Databricks Unified Data Analytics Platform. Your typical day will involve working with the development team, analyzing business requirements, and developing solutions to meet those requirements.\n\nRoles & Responsibilities:\n- Design, develop, and maintain applications using Databricks Unified Data Analytics Platform.\n- Collaborate with cross-functional teams to analyze business requirements and develop solutions to meet those requirements.\n- Develop and maintain technical documentation related to the application development process.\n- Ensure that all applications are developed according to industry standards and best practices.\n- Provide technical support and troubleshooting for applications developed using Databricks Unified Data Analytics Platform.\n\nProfessional & Technical Skills:\n- Must To Have Skills: Experience with Databricks Unified Data Analytics Platform.\n- Good To Have Skills: Experience with other big data technologies such as Hadoop, Spark, and Hive.\n- Strong understanding of software development principles and methodologies.\n- Experience with programming languages such as Python, Java, or Scala.\n- Experience with database technologies such as SQL and NoSQL.\n- Experience with version control systems such as Git or SVN.\n\nAdditional Information:\n- The candidate should have a minimum of 3 years of experience in Databricks Unified Data Analytics Platform.\n- The ideal candidate will possess a strong educational background in computer science or a related field, along with a proven track record of delivering impactful data-driven solutions.\n- This position is based at our Bengaluru office.\n15 years of fulltime education or above",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Databricks Unified Data Analytics Platform', 'Java', 'Hive', 'Hadoop', 'big data', 'Python']",2025-06-14 06:05:29
Azure Data Engineer,SoulPage IT,3 - 8 years,Not Disclosed,['Hyderabad'],"Azure Data Engineer - Soulpage IT Solutions\nHome\nAzure Data Engineer\nMarch 6, 2025\nPosition: Azure Data Engineer\nSkill set: Azure Databricks and Data Lake implementation\nExperience: 3+ years\nNotice Period: Immediate Immediate to 15 days\nLocation: WFO, Hyderabad\nJob Type : Full-Time\nPositions: 2\nJob Summary:\nWe are looking for a highly skilled Azure Data Engineer with expertise in Azure Databricks and Data Lake implementation to design, develop, and optimize our data pipelines. The engineer will be responsible for integrating data from multiple sources, ensuring data is cleaned, standardized, and normalized for ML model building. This role involves working closely with stakeholders to understand data requirements and ensuring seamless data flow across different platforms.\nKey Responsibilities: Data Lake & Pipeline Development\nDesign, develop, and implement scalable Azure Data Lake solutions .\nBuild robust ETL/ELT pipelines using Azure Databricks, Data Factory, and Synapse Analytics .\nOptimize data ingestion and processing from multiple structured and unstructured sources.\nImplement data cleaning, standardization, and normalization processes to ensure high data quality.\nImplement best practices for data governance, security, and compliance .\nOptimize data storage and retrieval for performance and cost-efficiency.\nMonitor and troubleshoot data pipelines, ensuring minimal downtime.\nWork closely with data scientists, analysts, and business stakeholders to define data needs.\nMaintain thorough documentation for data pipelines, transformations, and integrations.\nAssist in developing ML-ready datasets by ensuring consistency across integrated data sources.\nRequired Skills & Qualifications:\n3+ years of experience in data engineering, with a focus on Azure cloud technologies .\nExpertise in Azure Databricks, Data Factory, Data Lake\nStrong proficiency in Python, SQL, and PySpark for data processing and transformations.\nUnderstanding of ML data preparation workflows , including feature engineering and data normalization.\nKnowledge of data security and governance principles .\nExperience in optimizing ETL pipelines for scalability and performance.\nStrong analytical and problem-solving skills.\nExcellent written and verbal communication skills.\nPreferred Qualifications:\nAzure Certifications Azure Data Engineer Associate, Azure Solutions Architect .\nWhy Join Us?\nWork on cutting-edge Azure cloud and data technologies .\nCollaborate with a dynamic and innovative team solving complex data challenges.\nCompetitive compensation and career growth opportunities.\nApplication Process:\nInterested candidates can send their resumes to [email protected] with the subject line:\nApplication for Azure Data Engineer\nWe look forward to welcoming passionate individuals to our team!",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data security', 'Analytical', 'Cloud', 'data governance', 'Data processing', 'Data quality', 'Analytics', 'Solution Architect', 'SQL', 'Python']",2025-06-14 06:05:31
Data QA Lead,Codvo,8 - 13 years,Not Disclosed,[],"Role: Data QA Lead\nExperience Required- 8+ Years\nLocation- India/Remote\n\nCompany Overview\nAt Codvo.ai, software and people transformations go hand-in-hand. We are a global empathy-led technology services company. Product innovation and mature software engineering are part of our core DNA. Respect, Fairness, Growth, Agility, and Inclusiveness are the core values that we aspire to live by each day.\nWe continue to expand our digital strategy, design, architecture, and product management capabilities to offer expertise, outside-the-box thinking, and measurable results.\nThe Data Quality Analyst is responsible for ensuring the quality, accuracy, and consistency of data within the Customer and Loan Master Data API solution. This role will work closely with data owners, data modelers, and developers to identify and resolve data quality issues.\nKey Responsibilities\nLead and manage end-to-end ETL/data validation activities.\nDesign test strategy, plans, and scenarios for source-to-target validation.\nBuild automated data validation frameworks (SQL/Python/Great Expectations).\nIntegrate tests with CI/CD pipelines (Jenkins, Azure DevOps).\nPerform data integrity, transformation logic, and reconciliation checks.\nCollaborate with Data Engineering, Product, and DevOps teams.\nDrive test metrics reporting, defect triage, and root cause analysis.\nMentor QA team members and ensure process adherence.\nMust-Have Skills\n8+ years in QA with 4+ years in ETL testing.\nStrong SQL and database testing experience.\nProficiency with ETL tools (Airbyte, DBT, Informatica, etc.).\nAutomation using Python or similar scripting language.\nSolid understanding of data warehousing, SCD, deduplication.\nExperience with large datasets and structured/unstructured formats.\nPreferred Skills\nKnowledge of data orchestration tools (Prefect, Airflow).\nFamiliarity with data quality/observability tools.\nExperience with big data systems (Spark, Hive).\nHands-on with test data generation (Faker, Mockaroo).\nSub-Department:\nTesting",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Product management', 'Automation', 'Data validation', 'orchestration', 'Test strategy', 'Reconciliation', 'Data quality', 'Informatica', 'SQL', 'Python']",2025-06-14 06:05:33
Data Architect,Arcadis,7 - 12 years,Not Disclosed,['Bengaluru'],"Individual Accountabilities\nCollaboration\nCollaborates with domain architects in the DSS, OEA, EUS, and HaN towers and if appropriate, the respective business stakeholders in architecting data solutions for their data service needs.\nCollaborates with the Data Engineering and Data Software Engineering teams to effectively communicate the data architecture to be implemented. Contributes to prototype or proof of concept efforts.\nCollaborates with InfoSec organization to understand corporate security policies and how they apply to data solutions.\nCollaborates with the Legal and Data Privacy organization to understand the latest policies so they may be incorporated into every data architecture solution.\nSuggest architecture design with Ontologies, MDM team.\nTechnical skills & design\nSignificant experience working with structured and unstructured data at scale and comfort with a variety of different stores (key-value, document, columnar, etc.) as well as traditional RDBMS and data warehouses.\nDeep understanding of modern data services in leading cloud environments, and able to select and assemble data services with maximum cost efficiency while meeting business requirements of speed, continuity, and data integrity.\nCreates data architecture artifacts such as architecture diagrams, data models, design documents, etc.\nGuides domain architect on the value of a modern data and analytics platform.\nResearch, design, test, and evaluate new technologies, platforms and third-party products.\nWorking experience with Azure Cloud, Data Mesh, MS Fabric, Ontologies, MDM, IoT, BI solution and AI would be greater assets.\nExpert troubleshoot skills and experience.\nLeadership\nMentors aspiring data architects typically operating in data engineering and software engineering roles.\nKey shared accountabilities\nLeads medium to large data services projects.\nProvides technical partnership to product owners\nShared stewardship, with domains architects, of the Arcadis data ecosystem.\nActively participates in Arcadis Tech Architect community.\nKey profile requirements\nMinimum of 7 years of experience in designing and implementing modern solutions as part of variety of data ingestion and transformation pipelines\nMinimum of 5 years of experience with best practice design principles and approaches for a range of application styles and technologies to help guide and steer decisions.\nExperience working in large scale development and cloud environment.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Architecture', 'data modeling', 'Azure Cloud', 'RDBMS', 'data ingestion', 'MS Fabric', 'data transformation']",2025-06-14 06:05:36
Data Engineer - AWS & Python,Agilisium,6 - 10 years,Not Disclosed,['Saidapet'],"Key Responsibilities:\nDesign, develop, and optimizedata pipelinesandETL workflowsusingAWS Glue, AWS Lambda, and Apache Spark.\nImplement big data processingsolutions leveraging AWS EMR and AWS Redshift.\nDevelop and maintain data lakes and data warehouses on AWS (S3, Redshift, RDS).\nEnsure data quality, integrity, and governance usingAWS Glue Data Catalog and AWS Lake Formation.",,,,"['Python', 'continuous integration', 'cloud services', 'performance tuning', 'amazon redshift', 'ci/cd', 'kinesis', 'aws lambda', 'sql', 'aws glue', 'database design', 'git', 'data modeling', 'aws cloud', 'spark', 'kafka', 'aws']",2025-06-14 06:05:39
Data Engineer - R&D Precision Medicine,Amgen Inc,1 - 3 years,Not Disclosed,['Hyderabad'],"What you will do\nIn this vital role, you will be responsible for the end-to-end development of an enterprise analytics and data mastering solution using Databricks and Power BI. This role requires expertise in both data architecture and analytics, with the ability to create scalable, reliable, and impactful enterprise solutions that research cohort-building and advanced research pipeline. The ideal candidate will have experience creating and surfacing large unified repositories of human data, based on integrations from multiple repositories and solutions, and be extraordinarily skilled with data analysis and profiling.",,,,"['Data Engineering', 'CDC', 'Power BI', 'Data Analysis', 'ETL', 'ITIL', 'AWS', 'data warehouses']",2025-06-14 06:05:41
Analyst-10,Aegis Media,0 - 3 years,Not Disclosed,['Bengaluru'],"The purpose of this role is to provide support for the collection, analysis, and dissemination of insights to our clients\nJob Description:\nKey responsibilities:\nIntegrates disparate datasets, conducts data preparation for analyses\nApplies data science methods to provide insights and recommendations to clients\nDelivers analytic outcomes based on project timelines and key milestones\nMaintains knowledge of new trends in the data science industry\nDevelops and manages code used for analytics purposes\nCommunicates findings and insights\nKnowledge on SQL, Tableau\nLocation:\nBangalore\nBrand:\nIprospect\nTime Type:\nFull time\nContract Type:\nPermanent",Industry Type: Advertising & Marketing,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['tableau', 'Usage', 'data science', 'Senior Analyst', 'Analytics', 'SQL']",2025-06-14 06:05:43
Senior ML Scientist,Wayfair,9 - 14 years,Not Disclosed,['Bengaluru'],"Candidates for this position are preferred to be based in Bangalore, India and will be expected to comply with their teams hybrid work schedule requirements.\n\n\nThe Advertising Optimization & Automation Science team is central to this effort. We leverage machine learning and generative AI to streamline campaign workflows, delivering impactful recommendations on budget allocation, target Return on Ad Spend (tROAS), and SKU selection. Additionally, we are developing intelligent systems for creative optimization and exploring agentic frameworks to further simplify and enhance advertiser interactions.\nWe are looking for an experienced Senior Machine Learning Scientist to join the Advertising Optimization & Automation Science team. In this role, you will be responsible for building intelligent, ML-powered systems that drive personalized recommendations and campaign automation within Wayfair s advertising platform. You will work closely with other scientists, as well as members of our internal Product and Engineering teams, to apply your ML expertise to define and deliver 0-to-1 capabilities that unlock substantial commercial value and directly enhance advertiser outcomes.\nWhat You ll do:\nDesign and build intelligent budget, tROAS, and SKU recommendations, and simulation-driven decisioning that extends beyond the current advertising platform capabilities.\nLead the next phase of GenAI-powered creative optimization and automation to drive significant incremental ad revenue and improve supplier outcomes.\nRaise technical standards across the team by promoting best practices in ML system design and development.\nPartner cross-functionally with Product, Engineering, and Sales to deliver scalable ML solutions that improve supplier campaign performance. Ensure systems are designed for reuse, extensibility, and long-term impact across multiple advertising workflows.\nResearch and apply best practices in advertising science, GenAI applications in creative personalization, and auction modeling. Keep Wayfair at the forefront of innovation in supplier marketing optimization.\nCollaborate with Engineering teams (AdTech, ML Platform, Campaign Management) to build and scale the infrastructure needed for automated, intelligent advertising decisioning.\nWe Are a Match Because You Have:\nBachelors or Master s degree in Computer Science, Mathematics, Statistics, or related field.\n9+ years of experience in building large scale machine learning algorithms.\n4+ years of experience working in an architect or technical leadership position.\nStrong theoretical understanding of statistical models such as regression, clustering and ML algorithms such as decision trees, neural networks, transformers and NLP techniques.\nProficiency in programming languages such as Python and relevant ML libraries (e.g., TensorFlow, PyTorch) to develop production-grade products.\nStrategic thinker with a customer-centric mindset and a desire for creative problem solving, looking to make a big impact in a growing organization.\nDemonstrated success influencing senior level stakeholders on strategic direction based on recommendations backed by in-depth analysis; Excellent written and verbal communication.\nAbility to partner cross-functionally to own and shape technical roadmaps\nIntellectual curiosity and a desire to always be learning!\nNice to have:\nExperience with GCP, Airflow, and containerization (Docker).\nExperience building scalable data processing pipelines with big data tools such as Hadoop, Hive, SQL, Spark, etc.\nFamiliarity with Generative AI and agentic workflows.\nExperience in Bayesian Learning, Multi-armed Bandits, or Reinforcement Learning.\nYour personal data is processed in accordance with our Candidate Privacy Notice ( https: / / www.wayfair.com / careers / privacy ).",Industry Type: Retail,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Automation', 'Product engineering', 'Simulation', 'Machine learning', 'System design', 'Data processing', 'SQL', 'Python', 'Logistics']",2025-06-14 06:05:45
Data Engineering Trainee,Lanxess,1 - 3 years,Not Disclosed,['Thane'],"Contract Type: Regular 12 months\nIf the chemistry is right, we can make a difference at LANXESS: speed up sports, make beverages last longer, add more color to leisure time and much more.\nAs a leading specialty chemicals group, we develop and produce chemical intermediates, additives, specialty chemicals and high-tech plastics. With more than 13,000 employees. Be part of it!\nJob Highlights\nPlease shortly describe the most important, appealing 5 to 8 tasks of this position:",,,,"['Training', 'Engineer Trainee', 'SAP', 'Business reporting', 'data science', 'Finance', 'Javascript', 'Analytics', 'SQL', 'Python']",2025-06-14 06:05:48
Cloud Data Engineer,Luxoft,7 - 10 years,20-25 Lacs P.A.,"['Hyderabad', 'Pune']",Role & responsibilities\n\n\n• Azure Cloud\n• FinOps / Cloud cost efficiencies\n• Azure CosmosDB / SQL\n• Terraform / IaC\n• PowerShell / Bash\n• Linux\n• DevOps skills CI/CD\n• Automation\n• UBS processes / tooling\n• Grid DataSynpase\n\n\nPreferred candidate profile,Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['PowerShell', 'Terraform', 'Linux', 'Azure Cloud', 'Azure CosmosDB', 'DevOps', 'Grid DataSynpase']",2025-06-14 06:05:50
Data Engineering Trainee,Lanxess,1 - 3 years,Not Disclosed,['Thane'],"Contract Type: Regular 12 months\nIf the chemistry is right, we can make a difference at LANXESS: speed up sports, make beverages last longer, add more color to leisure time and much more.\nAs a leading specialty chemicals group, we develop and produce chemical intermediates, additives, specialty chemicals and high-tech plastics. With more than 13,000 employees. Be part of it!\nJob Highlights",,,,"['Training', 'Engineer Trainee', 'SAP', 'Business reporting', 'data science', 'Javascript', 'Analytics', 'SQL', 'Python']",2025-06-14 06:05:52
Data Engineering Specialist,Investec Global Services,4 - 6 years,Not Disclosed,['Mumbai'],"The Offshore Data Engineering Lead will be responsible for overseeing the data and application development efforts which support our Microsoft Data Mesh Platform. Working as a part of the Investec Central Data Team, the candidate will be responsible for leading development on solutions and applications that support our data domain teams with creation of data products. This role involves driving technical initiatives, exploring new technologies, and enhancing engineering practices within the data teams in-line with the group engineering strategy. The Data Engineering Lead will be a key driver for Investecs move to Microsoft Fabric and other enablement data quality, data management and data orchestration technologies.\nKey Roles and Responsibilities:\nLead the development and implementation of data and custom application solutions that support the creation of data products across various data domains.\nDesign, build, and maintain data pipelines using Microsoft Azure Data Platform, Microsoft Fabric and Databricks technologies.\nEnsure data quality, integrity, and security within the data mesh architecture.\nShare group engineering context with the CIO and engineers within the business unit continuously.\nDrive engineering efficiency and enable teams to deliver high-quality software quickly within the business unit\nCultivate a culture focused on security, risk management, and best practices in engineering\nActively engage with the data domain teams, business units and wider engineering community to promote knowledge sharing\nSpearhead technical projects and innovation within the business units engineering teams and contribute to group engineering initiatives\nAdvance the technical skills of the engineering community and mentor engineers within the business unit\nEnhance the stability, performance, and security of the business units systems.\nDevelop and promote exceptional engineering documentation and practices\nBuild a culture of development and mentorship within the central data team\nProvide guidance on technology and engineering practices\nActively encourages creating Investec open-source software where appropriate within the business unit\nActively encourages team members within the business unit to speak at technical conferences based on the work being done\nCore Skills and Knowledge:\nProven experience in data engineering, with a strong focus on Microsoft Data Platform technologies, including Azure Data Factory, Azure SQL Database, and Databricks\nProficiency in programming languages such as C# and/or Python, with experience in application development being a plus\nExperience with CI/CD pipelines, Azure, and Azure DevOps\nStrong experience and knowledge with PySpark and SQL with the ability to create solutions using Microsoft Fabric\nAbility to create solutions that query and work with web APIs\nIn-depth knowledge of Azure, containerisation, and Kubernetes\nStrong understanding of data architecture concepts, particularly data mesh principles\nExcellent problem-solving skills and the ability to work independently as a self-starter\nStrong communication and collaboration skills, with the ability to work effectively in a remote team environment\nRelevant degree in Computer Science, Data Engineering, or a related field is preferred",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Data management', 'Agile', 'Engineering Manager', 'Application development', 'microsoft', 'Open source', 'Risk management', 'SQL', 'Python']",2025-06-14 06:05:54
Principal Data Engineer,Horizon Therapeutics,5 - 18 years,Not Disclosed,['Hyderabad'],"Career Category Engineering Job Description\nJoin Amgen s Mission of Serving Patients\nAt Amgen, if you feel like you re part of something bigger, it s because you are. Our shared mission to serve patients living with serious illnesses drives all that we do.\nSince 1980, we ve helped pioneer the world of biotech in our fight against the world s toughest diseases. With our focus on four therapeutic areas -Oncology, Inflammation, General Medicine, and Rare Disease- we reach millions of patients each year. As a member of the Amgen team, you ll help make a lasting impact on the lives of patients as we research, manufacture, and deliver innovative medicines to help people live longer, fuller happier lives.\nOur award-winning culture is collaborative, innovative, and science based. If you have a passion for challenges and the opportunities that lay within them, you ll thrive as part of the Amgen team. Join us and transform the lives of patients while transforming your career.\nWhat you will do\nLet s do this. Let s change the world. In this vital role you will manage and oversee the development of robust Data Architectures, Frameworks, Data product Solutions, while mentoring and guiding a small team of data engineers. You will be responsible for leading the development, implementation, and management of enterprise-level data data engineering frameworks and solutions that support the organizations data-driven strategic initiatives. You will continuously strive for innovation in the technologies and practices used for data engineering and build enterprise scale data frameworks and expert data engineers. This role will closely collaborate with counterparts in US and EU. You will collaborate with cross-functional teams, including platform, functional IT, and business stakeholders, to ensure that the solutions that are built align with business goals and are scalable, secure, and efficient.\nRoles Responsibilities:\nArchitect Implement of scalable, high-performance Modern Data Engineering solutions (applications) that include data analysis, data ingestion, storage, data transformation (data pipelines), and analytics.\nEvaluate the new trends in data engineering area and build rapid prototypes\nBuild Data Solution Architectures and Frameworks to accelerate the Data Engineering processes\nBuild frameworks to improve the re-usability, reduce the development time and cost of data management governance\nIntegrate AI into data engineering practices to bring efficiency through automation\nBuild best practices in Data Engineering capability and ensure their adoption across the product teams\nBuild and nurture strong relationships with stakeholders, emphasizing value-focused engagement and partnership to align data initiatives with broader business goals.\nLead and motivate a high-performing data engineering team to deliver exceptional results.\nProvide expert guidance and mentorship to the data engineering team, fostering a culture of innovation and best practices.\nCollaborate with counterparts in US and EU and work with business functions, functional IT teams, and others to understand their data needs and ensure the solutions meet the requirements.\nEngage with business stakeholders to understand their needs and priorities, ensuring that data and analytics solutions built deliver real value and meet business objectives.\nDrive adoption of the data and analytics solutions by partnering with the business stakeholders and functional IT teams in rolling out change management, trainings, communications, etc.\nTalent Growth People Leadership: Lead, mentor, and manage a high-performing team of engineers, fostering an environment that encourages learning, collaboration, and innovation. Focus on nurturing future leaders and providing growth opportunities through coaching, training, and mentorship.\nRecruitment Team Expansion: Develop a comprehensive talent strategy that includes recruitment, retention, onboarding, and career development and build a diverse and inclusive team that drives innovation, aligns with Amgens culture and values, and delivers business priorities\nOrganizational Leadership: Work closely with senior leaders within the function and across the Amgen India site to align engineering goals with broader organizational objectives and demonstrate leadership by contributing to strategic discussions\nWhat we expect of you\nWe are all different, yet we all use our unique contributions to serve patients. The [vital attribute] professional we seek is a [type of person] with these qualifications.\nBasic Qualifications:\nMaster s degree and 8 to 10 years of computer science and engineering preferred, other Engineering fields will be considered OR\nBachelor s degree and 12 to 14 years of computer science and engineering preferred, other Engineering fields will be considered OR\nDiploma and 16 to 18 years of computer science and engineering preferred, other Engineering fields will be considered\n10+ years of experience in Data Engineering, working in COE development or product building\n5+ years of experience in leading enterprise scale data engineering solution development.\nExperience building enterprise scale data lake, data fabric solutions on cloud leveraging modern approaches like Data Mesh\nDemonstrated proficiency in leveraging cloud platforms (AWS, Azure, GCP) for data engineering solutions. Strong understanding of cloud architecture principles and cost optimization strategies.\nHands-on experience using Databricks, Snowflake, PySpark, Python, SQL\nProven ability to lead and develop high-performing data engineering teams.\nStrong problem-solving, analytical, and critical thinking skills to address complex data challenges.\nPreferred Qualifications:\nExperience in Integrating AI with Data Engineering and building AI ready data lakes\nPrior experience in data modeling especially star-schema modeling concepts.\nFamiliarity with ontologies, information modeling, and graph databases.\nExperience working with agile development methodologies such as Scaled Agile.\nExperienced with software engineering best-practices, including but not limited to version control (Git, Subversion, etc.), CI/CD (Jenkins, Maven etc.), automated unit testing, and Dev Ops.\nEducation and Professional Certifications\nSAFe for Teams certification (preferred)\nDatabricks certifications\nAWS cloud certification\nSoft Skills:\nExcellent analytical and troubleshooting skills.\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nTeam-oriented, with a focus on achieving team goals\nStrong presentation and public speaking skills.\nWhat you can expect of us\nAs we work to develop treatments that take care of others, we also work to care for your professional and personal growth and well-being. From our competitive benefits to our collaborative culture, we ll support your journey every step of the way.\nIn addition to the base salary, Amgen offers competitive and comprehensive Total Rewards Plans that are aligned with local industry standards.\nApply now\nfor a career that defies imagination\nObjects in your future are closer than they appear. Join us.\ncareers.amgen.com\nAs an organization dedicated to improving the quality of life for people around the world, Amgen fosters an inclusive environment of diverse, ethical, committed and highly accomplished people who respect each other and live the Amgen values to continue advancing science to serve patients. Together, we compete in the fight against serious disease.\nAmgen is an Equal Opportunity employer and will consider all qualified applicants for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, disability status, or any other basis protected by applicable law.\nWe will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation.\n.",Industry Type: Biotechnology,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Automation', 'Data analysis', 'Data management', 'Analytical', 'Troubleshooting', 'Analytics', 'SQL', 'Python', 'Recruitment']",2025-06-14 06:05:56
Data Engineer (With Testing & Validation Experience),Mcsquared Ai,8 - 13 years,25-30 Lacs P.A.,[],"Job Title: Data Engineer (With Testing & Validation Experience)\nJob Summary:\nWe are looking for a Data Engineer with strong testing experience to join our team. You will design, build, and test data pipelines while ensuring data quality and reliability. The role involves managing the full development lifecycle from design to testing and deployment. Experience with cloud platforms is required; knowledge of Palantir Foundry (Contour, Code Repo, and Data Lineage) is a strong advantage.\nKey Responsibilities:\nBuild and maintain data pipelines using Python, PySpark, and SQL.\nDesign and implement unit tests, regression tests, and validations to ensure data accuracy.\nManage test plans, automation, and execution across the data lifecycle.\nWork closely with analysts, engineers, and business teams to gather requirements and deliver solutions.\nMaintain data quality and consistency across platforms.\nUse AWS for data processing, storage, and integrations.\nDocument work using tools like Confluence and Jira.\nCommunicate testing outcomes and data quality metrics to stakeholders.\nTechnical Skills:\nMust Have:\nPython, PySpark, SQL.\nTesting experience (unit testing, regression testing, QA practices).\nAWS cloud experience.\nStrong problem-solving and analytical skills.\nGood communication and teamwork.\nNice to Have:\nExperience with Palantir Foundry (Contour, Code Repo, Data Lineage).\nFamiliarity with Confluence, Jira, and CI/CD tools.\nExposure to data orchestration and monitoring tools.\nSoft Skills:\nStrong analytical and problem-solving abilities.\nEffective verbal and written communication skills.\nCollaborative mindset with a proactive approach.\nAdaptability to fast-paced and evolving project environments.\nSelf-starter with a high level of accountability and ownership.\nQualifications & Certifications:\nPreferred certifications:\nAWS Certified Data Analytics or Solutions Architect\nPalantir Foundry Certification\nISTQB or equivalent testing certifications",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Pyspark', 'python', 'Data Testing', 'Data Pipeline Developement', 'SQl', 'Data Validation', 'Regression Testing', 'Unit Testing', 'Test Automation', 'Data Quality Management', 'ETL', 'AWS']",2025-06-14 06:05:58
Data Engineering Specialist,Sanofi,7 - 11 years,Not Disclosed,['Hyderabad'],"About the job\nWe are seeking an experienced Data Engineering Specialist interested in challenging the status quo to ensure the seamless creation and operation of the data pipelines that are needed by Sanofis advanced analytic, AI and ML initiatives for the betterment of our global patients and customers.\nSanofi has recently embarked into a vast and ambitious digital transformation program. A cornerstone of this roadmap is the acceleration of its data transformation and of the adoption of artificial intelligence (AI) and machine learning (ML) solutions, to accelerate R&D, manufacturing and commercial performance and bring better drugs and vaccines to patients faster, to improve health and save lives\nMain Responsibilities:\nEstablish technical designs to meet Sanofi requirements aligned with the architectural and Data standards\nOwnership of the entire back end of the application, including the design, implementation, test, and troubleshooting of the core application logic, databases, data ingestion and transformation, data processing and orchestration of pipelines, APIs, CI/CD integration and other processes\nFine-tune and optimize queries using Snowflake platform and database techniques\nOptimize ETL/data pipelines to balance performance, functionality, and other operational requirements.\nAssess and resolve data pipeline issues to ensure performance and timeliness of execution\nAssist with technical solution discovery to ensure technical feasibility.\nAssist in setting up and managing CI/CD pipelines and development of automated tests\nDeveloping and managing microservices using python\nConduct peer reviews for quality, consistency, and rigor for production level solution\nDesign application architecture for efficient concurrent user handling, ensuring optimal performance during high usage periods\nOwn all areas of the product lifecycle: design, development, test, deployment, operation, and support\nAbout you\nQualifications:\n5+ years of relevant experience developing backend, integration, data pipelining, and infrastructure\nExpertise in database optimization and performance improvement\nExpertise in Python, PySpark, and Snowpark\nExperience data warehousing and object-relational database (Snowflake and PostgreSQL) and writing efficient SQL queries\nExperience in cloud-based data platforms (Snowflake, AWS)\nProficiency in developing robust, reliable APIs using Python and FastAPI Framework\nExpert in ELT and ETL & experience working with large data sets and performance and query optimization. IICS is a plus\nUnderstanding of data structures and algorithms\nUnderstanding of DBT is a plus\nExperience in modern testing framework (SonarQube, K6 is a plus)\nStrong collaboration skills, willingness to work with others to ensure seamless integration of the server-side and client-side\nKnowledge of DevOps best practices and associated tools is a plus, especially in the setup, configuration, maintenance, and troubleshooting of associated tools:\nContainers and containerization technologies (Kubernetes, Argo, Red Hat OpenShift)\nInfrastructure as code (Terraform)\nMonitoring and Logging (CloudWatch, Grafana)\nCI/CD Pipelines (JFrog Artifactory)\nScripting and automation (Python, GitHub, Github actions)\nExperience with JIRA & Confluence\nWorkflow orchestration (Airflow)\nMessage brokers (RabbitMQ)\nEducation: Bachelor’s degree in computer science, engineering, or similar quantitative field of study\nWhy choose us?\nBring the miracles of science to life alongside a supportive, future-focused team.\nDiscover endless opportunities to grow your talent and drive your career, whether it’s through a promotion or lateral move, at home or internationally.\nEnjoy a thoughtful, well-crafted rewards package that recognizes your contribution and amplifies your impact.\nTake good care of yourself and your family, with a wide range of health and wellbeing benefits including high-quality healthcare, prevention and wellness programs and at least 14 weeks’ gender-neutral parental leave.\nOpportunity to work in an international environment, collaborating with diverse business teams and vendors, working in a dynamic team, and fully empowered to propose and implement innovative ideas.\nPursue Progress. Discover Extraordinary.\nProgress doesn’t happen without people – people from different backgrounds, in different locations, doing different roles, all united by one thing: a desire to make miracles happen. You can be one of those people. Chasing change, embracing new ideas and exploring all the opportunities we have to offer. Let’s pursue progress. And let’s discover extraordinary together.\nAt Sanofi, we provide equal opportunities to all regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, or gender identity.\nWatch our ALL IN video and check out our Diversity Equity and Inclusion actions at sanofi.com!\nLanguages: English is a must",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Engineering', 'Snowflake', 'AWS', 'Python', 'Fast Api']",2025-06-14 06:06:00
Big Data Developer,Nitor,6 - 11 years,15-30 Lacs P.A.,['Pune'],"Hi ,\n\nGreetings from Nitor infotech!\n\nPlease find below the Company  Job Description with this mail\n\n\n6+ years candidates only apply\nImmediate joiners Preferred\nPython ,Hadoop,hive,spark and SQL are mandatory\n\n\n\nJob Description:\n\nPrimary Skills:\n•6+ years of Data Engineering experience.\n•4+ years of experience in Big Data technologies (e.g. Spark, Hive, Hadoop, etc.).\n•Strong Experience designing and implementing data pipelines.\n•Excellent knowledge of Data engineering concepts and best practices.\n•Proven ability to lead, mentor, inspire and support more junior team members.\n•Able to lead technical deliverables autonomously and lead more junior data engineers.\n•Strong attention to detail and working according to best practices.\n•Experience in designing solution using batch data processing methods, real-time streams, ETL processes and Business Intelligence tools.\n•Experience designing Logical Data Model and Physical Data Models including data warehouse and data mart designs.\n•Strong SQL knowledge & experience (T-SQL, working with SQL Server, SSMS)\n•Apache Spark: Advanced proficiency with Spark, including PySpark and SparkSQL, for distributed data processing.\n•Working knowledge of Apache Hive\n•Proficiency in Python, Pandas, PySpark (Scala knowledge is a plus).\n•Knowledge of Delta Lake concepts and common data formats, Lakehouse architecture.\n•Source control with Git.\n•Expertise in designing and implementing scalable data pipelines and ETL processes using the GCP data stack including: BigQuery, Dataflow, Pub/Sub, Cloud Storage, Cloud Composer, Cloud Functions, Dataproc (spark)\n•Apache Airflow - Expertise in building and managing ETL workflows using Airflow, including DAG creation, scheduling, and error handling.\n•Knowledge of CI/CD concepts, experience designing CI/CD for data pipelines.\nSecondary Skills:\n•Experience with Streaming services such as Kafka is a plus.\n•R & Sparklyr experience is a plus\n•Knowledge of MLOps concepts, AI/ML life-cycle management, Mlflow\n•Expertise in writing complex, highly optimized queries across large data sets to write data pipelines and data processing layers.\n•Jenkins\n\nCandidate Profile:\nDesign, build, test and deploy innovative Big Data solutions at scale.\nExtract, Clean, transform, and analyse vast amounts of raw data from various Data Sources.\nBuild data pipelines and API integrations with various internal systems.\nWork Across all stages of Data Lifecycle\nImplement best practices across all Data Analytics Processes\nEstimate effort, identify risks, and plan execution.\nProactively monitor, identify, and escalate issues or root causes of systemic issues.\nEnable data scientists, business, and product partners to fully leverage our platform.\nEngage with business stakeholders to understand client requirements and build technical solutions and delivery plans.\nEvaluate and communicate technical risks effectively and ensure assignments delivery in scheduled time with desired quality.\nProvide end to end big data solution and design details to data engineering teams.\nExcellent analytical & problem-solving skills\nExcellent communication skills, experience communicating with Snr. Business stakeholders\nLeading technical delivery on use-cases, able to plan and delegate tasks to more junior team members, oversee the work from inception to final product. \n\nKey Required Skills:\nApache Airflow, Kafka, SQL, Data Engineering, CI CD pipelines , Big Data, Apache Spark, Logical Data Model, Physical Data Model\n\nWish you all the best!\nThanks & Regards,\nVIGNESH\n6379146150",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Hive', 'Hadoop', 'Spark', 'Python', 'SQL']",2025-06-14 06:06:02
Data & Analytics Engineering Manager,Avalara Technologies,10 - 15 years,Not Disclosed,['Pune'],"What You'll Do\n\nThe Global Analytics & Insights (GAI) team is seeking a Data & Analytics Engineering Manager to lead our team in designing, developing, and maintaining data pipelines and analytics infrastructure. As a Data & Analytics Engineering Manager, you will play a pivotal role in empowering a team of engineers to build and enhance analytics applications and a modern data platform using Snowflake, dbt (Data Build Tool), Python, Terraform, and Airflow. You will become an expert in Avalaras financial, marketing, sales, and operations data. The ideal candidate will have deep SQL experience, an understanding of modern data stacks and technology, demonstrated leadership and mentoring experience, and an ability to drive innovation and manage complex projects. This position will report to Senior Manager.\n\nWhat Your Responsibilities Will Be\nMentor a team of data engineers, providing guidance and support to ensure a high level of quality and career growth\nLead a team of data engineers in the development and maintenance of data pipelines, data modelling, code reviews and data products\nCollaborate with cross-functional teams to understand requirements and translate them into scalable data solutions\nDrive innovation and continuous improvements within the data engineering team\nBuild maintainable and scalable processes and playbooks to ensure consistent delivery and quality across projects\nDrive adoption of best practices in data engineering and data modelling\nBe the visible lead of the team- coordinate communication, releases, and status to various stakeholders\nWhat You'll Need to be Successful\nBachelor's degree in Computer Science, Engineering, or related field\n10+ years experience in data engineering field, with deep SQL knowledge\n2+ years management experience, including direct technical reports\n5+ years experience with data warehousing concepts and technologies\n4+ years of working with Git, and demonstrated experience using these tools to facilitate growth of engineers\n4+ years working with Snowflake\n3+ years working with dbt (dbt core preferred)\nPreferred Qualifications:\nSnowflake, Dbt, AWS Certified\n3+ years working with Infrastructure as Code, preferably Terraform\n2+ years working with CI CD, and demonstrated ability to build and operate pipelines\nExperience and understanding of Snowflake administration and security principles\nDemonstrated experience with Airflow",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Terraform', 'Snowflake administration', 'Snowflake', 'Dbt', 'AWS']",2025-06-14 06:06:05
Azure Data Engineer,Black and white Business Solution,4 - 7 years,10-17 Lacs P.A.,['Pune'],"Hi\nGreeting for the Day!\nWe found your profile suitable for the below opening, kindly go through the JD and reach out to us if you are interested.\nAbout Us\nIncorporated in 2006, We are an 18 year old recruitment and staffing company, we are a provider of manpower for some of the fortune 500 companies for junior/ Middle/ Executive talent.\nAbout Client\nHiring for One of the Most Prestigious Multinational Corporations!\nJob Description\nJob Title :\nAzure Data Engineer\n\nQualification :\nAny Graduate or Above\n\nRelevant Experience :\n4+Yrs\n\nRequired skills:\n\nAzure Databricks\nPython\nPySpark\nSQL\nAZURE Cloud\nPowerBI {Basic+Debug}\n\nLocation :\nPune\n\nCTC Range :\n10-17 (Lakhs Per Annum)\n\nMode of Work :\nHybrid\nJoel.\nIT Staff.\nBlack and White Business solutions PVT Ltd\nBangalore, Karnataka, INDIA.\n8067432416 I joel.manivasan@blackwhite.in I www.blackwhite.in",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'Power Bi', 'Azure Databricks', 'Python']",2025-06-14 06:06:07
Gcp Data Engineer,Cloudxtreme,8 - 13 years,16-31 Lacs P.A.,['Gurugram'],"Role & responsibilities\n\nGCP; Big Data; ETL - Big Data / Data Warehousing\nBig query, data proc, data flow, composer\nLooking forGCP Developerwith below mandatory skills and requirementsMandatory Skills::\nBigQuery,Cloud Storage, Cloud Pub/Sub, Dataflow, Dataproc,Composer• 6+ years in cloud infrastructure and designing data pipeline, specifically in GCP•\nProficiency in programming languages Python, SQL•Proven experience in designing and implementing cloud-native applications and microservices on GCP.\nHands-on experience with CI/CD tools like Jenkins and Github Action•In-depth understanding of GCP networking, IAM policies, and security best practices.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Temporary/Contractual","['GCP', 'Big Data', 'ETL']",2025-06-14 06:06:09
Software Data Operations Engineer,MAQ Software,5 - 7 years,Not Disclosed,"['Noida', 'Mumbai', 'Hyderabad']","Software Data Operations Engineer (BS+5)\n\nResponsible for analyzing, developing and managing cloud computing architecture for data-oriented applications. Work with cross functional team of business analysts, software programmers, engineers to define and own solution architecture. Work with clients to design a full-stack solution using software that fully integrates with customers existing cloud infrastructure from data flow, security, DevOps & other standpoints. Evaluate technical risks, costs and map out mitigation strategies. Create well-informed cloud strategy and administer the adaption process. Develop & organize cloud systems & work closely with IT security to monitor the companys cloud privacy. Develop new specs, documentation; & partake in the development of technical procedures & user support guides. Direct infrastructure movement procedure, including bulk application transfers into cloud environment. Design & manage server systems locally & on Amazon Web Services & Microsoft Azure cloud-based Data operations. Evaluate & monitor cloud applications, hardware & software at regular intervals. Carry out debugging, troubleshooting, modifications & unit testing of custom solutions built on client platform. Must be able to travel temporarily to client sites & or relocate throughout the United States.\n\nRequirements: Bachelors Degree or foreign equivalent in Computer Science, Computer Engineering, Information Technology or related field with 5 years of work experience in job offered, Software Engineer, Development Operations Engineer, Data Engineer, Programmer Analyst, Business Analytics or related job.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data operations', 'data analysis', 'data analytics', 'advanced excel', 'pivot table', 'sql']",2025-06-14 06:06:11
Data Architect,Thinkapps Solutions,10 - 20 years,Not Disclosed,"['Kolkata', 'Hyderabad', 'Pune', 'Ahmedabad', 'Chennai', 'Bengaluru', 'Delhi / NCR', 'Mumbai (All Areas)']","• Proficient in Databricks Data Engg.\n• Knowledge in ETL concepts.\n• Proficient in writing SQL queries, performance tuning.\n• Knowledge in python script writing.\n• Good Communication skill.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Databricks', 'ETL', 'SQL', 'Python']",2025-06-14 06:06:14
AWS Data Engineer,SoulPage IT,4 - 9 years,Not Disclosed,['Hyderabad'],"About Us:\nSoulpage IT Solutions is an AI/ML technology company based in Hyderabad, specializing in cutting-edge solutions for data engineering, AI, and cloud-based analytics. We are looking for an experienced Data Engineer with expertise in Big Data technologies, Python, and PySpark to join our team.\nKey Responsibilities:\nDevelop and optimize PySpark applications using Spark DataFrames in Python. Work on large-scale data processing, ensuring performance optimization for high-volume data pipelines .\nImplement best practices for version control using Git.\nWork with Amazon Analytics services such as Amazon EMR, Amazon Athena, and AWS Glue .\nUtilize Amazon Compute services including AWS Lambda, EC2 , and storage solutions like S3, SNS .\nWork with columnar storage formats like Parquet, Avro, ORC , and apply compression techniques ( Snappy, Gzip ).\nGood to have experience with data warehousing concepts (dimensions, facts, schemas - Star, Snowflake ).\nGood to have experience with AWS databases such as Aurora, RDS, Redshift, ElastiCache, or DynamoDB .\nRequirements:\n4+ years of experience in IT, with strong hands-on expertise in Big Data technologies .\nMandatory: Hands-on experience in Python and PySpark .\nExperience working on AWS ecosystem , including EMR, Athena, Glue, Lambda, EC2, S3, and SNS .\nStrong understanding of data modeling and warehousing concepts .\nProficiency in working with structured and semi-structured data\nAbility to optimize Spark jobs for better performance and efficiency .\nWhy Join Us?\nWork on cutting-edge AI & ML projects in a fast-growing technology company.\nBe part of an innovative and collaborative work environment.\nCareer growth opportunities in Big Data, AI, and Cloud technologies .\nCompetitive salary and benefits.\nApplication Process:\nInterested candidates can send their resumes to [email protected] with the subject line:\nApplication for AWS Data Engineer\nWe look forward to welcoming passionate individuals to our team!",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Warehouse', 'GIT', 'Data modeling', 'spark', 'Cloud', 'Manager Technology', 'big data', 'AWS', 'Data warehousing', 'Python']",2025-06-14 06:06:16
Data Engineering Professional,Agilisium,4 - 6 years,Not Disclosed,['Chennai'],"Experience - 4+ years\n\nProgramming Side:\nStrong programming skill on Python\nStrong skill on SQL and any one of the Database/Data warehouses\nKnowledge on Spark coding and Architecture\nSound knowledge on ETL flow, Good Knowledge on Databricks.\nExperience on any cloud(AWS/Azure/GCP)",,,,"['Data Engineering', 'python', 'Kinesis', 'Kafka', 'data warehousing', 'dbms', 'etl', 'sql']",2025-06-14 06:06:19
Azure Data Engineer,Addnectar Solutions,6 - 10 years,Not Disclosed,"['Chennai', 'Coimbatore', 'Mumbai (All Areas)']","We have an urgent requirement for\nRole: (Senior Azure Data Engineer)\n\nExperience: 6 years.\n\nNotice Period: 0-15 days Max\n\nPosition: C2H\n\nShould be able to work in Flexible timing.\n\nCommunication should be excellent.\n\nMust Have:\nStrong understanding of ADF, Azure, Databricks, PySpark,\nStrong understanding of SQL, ADO, PowerBI, Unity Catalog is mandatory",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Part Time, Temporary/Contractual","['Pyspark', 'Azure', 'ADF', 'PowerBI', 'SQL', 'Azure Data Factory', 'Databricks']",2025-06-14 06:06:21
Azure Data Engineer (6 plus years) @ chennai,Changepond,6 - 11 years,Not Disclosed,['Chennai'],"Role & responsibilities\nData Engineer, with working on data migration projects.\nExperience with Azure data stack, including Data Lake Storage, Synapse Analytics, ADF, Azure Databricks, and Azure ML.\nSolid knowledge of Python, PySpark and other Python packages\nFamiliarity with ML workflows and collaboration with data science teams.\nStrong understanding of data governance, security, and compliance in financial domains.",,,,"['Azure Data Factory', 'Azure Databricks', 'Python', 'Pyspark', 'Data Engineering', 'Migration', 'Synapse Analytics', 'Machine Learning']",2025-06-14 06:06:23
Fresher Business Analyst,Smartconnect Technologies,0 - 2 years,1-2 Lacs P.A.,['Thane'],"Translate business requirements into comprehensive functional specifications, user stories & technical documentation for Devt. teams.\n\nCollaborate with the Product & Business teams to implement data-driven strategies & improve overall effectiveness.\n\nRequired Candidate profile\nStrong analytical skills, with a problem-solving attitude.\nExcellent written and verbal communication skills.\nBachelor's degree in Engineering, IT, Marketing, Business, or a related field",Industry Type: Software Product,Department: Product Management,"Employment Type: Full Time, Permanent","['Fresher BA', 'Trainee BA', 'Business analyst', 'BA', 'Business Analysis']",2025-06-14 06:06:25
Full Stack AI & ML Engineer (Trainee),Inzint,0 years,"70,000-3 Lacs P.A.",['Noida( Sector-65 Noida )'],"Full Stack AI & ML Engineer Trainee\n\nInzint invites job applications from the graduating batch of 2025 with a bachelor's or master's degree in computer science, Information Technology, Data Science, or related fields are eligible to apply.\n\nCompany Name- Inzint Pvt. Ltd\nLocation: [B111, B Block, Sector 65, Noida Uttar Pradesh]\n\nAbout the Role:\nWe, at Inzint are looking for enthusiastic and self-driven Full Stack AI & ML Engineer – Trainees to join our dynamic team. This is a unique opportunity for 2025 graduates who are passionate about Artificial Intelligence, Machine Learning, and Full Stack Development to work on cutting-edge technologies and real-world AI applications.\nDuring your first six months with us, you will be given extensive training in various technologies including Data Science, AI/Ml, DevOps, and AWS among others. In this JD, we will refer to this period as the ""Training Period"".\n\nNumber of positions: 08\nCommencement Date: Aug 01, 2025\nJob Type: Full-Time | Trainee Program\nWork Mode- No remote or WFH during the training period\n\nPrimary Responsibilities:\n\nWriting and Maintaining Code: Develop, test, and maintain high-quality software solutions in alignment with project requirements and coding standards.\nClient Engagement: Participate in client meetings to gather requirements, provide technical insights, and ensure alignment between development and client expectations.\nProject Reporting: Prepare and deliver regular progress reports, document development activities, and communicate updates to internal teams and stakeholders.\nCollaboration: Work closely with cross-functional teams including designers, QA, and project managers to ensure smooth project execution.\n\nEducation Requirement:         bachelor's or master's degree in computer science, Information Technology, Data Science, or related fields are eligible to apply. The course must be full-time and regular.\n\nAdditional Requirements:\n\n1) You must have scored at least 60% marks throughout your academics.\n2) There should be no active backlogs on the day of joining (i.e. on Aug 01, 2025).\n3) You must have studied Mathematics in your 10th, 12th, and Graduation for at least two semesters.\n4)You must have done projects in a Full Stack Web App, Mobile App or an AI/ML project.  \n\nRequired Skills:\n\nCore ML Concepts:\nSupervised & Unsupervised Learning, Model Selection & Evaluation, Mathematical Foundations, Common ML Algorithms\nProgramming:  Python and JavaScript\nFrontend: ReactJS\nBackend Development: REST API development using Flask, Django, FastAPI, or Node.js\nDatabases: SQL and NoSQL\nAI/ML Libraries: scikit-learn, pandas, numpy, TensorFlow, etc.\nVersion Control: GitHub\nDeployment Platforms: Vercel, Render, Railway\n\nGood-to-Have Skills:\n\nAdvanced ML Concepts: Feature Engineering, Bias-Variance Tradeoff, Pipeline Thinking, Overfitting & Generalization, Advanced Algorithms\nCloud Services: AWS (EC2, Lambda, Amplify)\nBig Data Technologies: Apache Spark, Hadoop, AWS S3\nData Engineering: ETL processes, data pipelines, vector databases\n\nCompensation-  \n\nCTC of 3 LPA after the training period, of which 2.4 LPA will be fixed CTC and 0.6 will be the variable CTC.\nDuring the training, you will receive a stipend of 12,000 per month.\n\nBond: The company will invest a significant amount of time, resources, and finances into your training and development. This investment is aimed at upskilling you and bringing you up to the company's standards. A bond ensures that you do not take advantage of this investment and leave immediately after the training, which could lead to a loss for the company. Hence, you will be required to sign an 18-month service bond and submit an undated signed cheque of 1.5 lakh as security. The cheque will be returned on completion of 18 months with us. The bond period will be Aug 2025 - Feb 2026.\n\nExemptions or waivers can be made in the following cases:\n\nUnsatisfactory Performance: If the employee's performance is consistently unsatisfactory during the training period, the company might exempt them from the bond.\nLong-Term Disability that Hinders the Employee's Ability to Work: If an employee faces a long-term disability that significantly hampers their capacity to perform the job, they might be exempt from the bond.\nChange in Job Role: In situations where the employee is transferred to a different department within the company that significantly changes their roles & responsibilities, the bond terms might be revisited or waived.\nAdmission to a Globally Ranked Institution\nThe signatory secures confirmed admission into a degree program (undergraduate, postgraduate, or doctoral) at any university ranked within the top 200 globally, as per the latest edition of the QS World University Rankings.\nEmployment with a Recognized Global Corporation - A Global Fortune 500 company, as listed in the most recent Fortune magazine global ranking, or\nEmployment with a Unicorn startup, defined as a privately held startup company with a valuation of USD 1 billion or more at the time of employment offer.\nGovernment Employment\nThe signatory is appointed to a permanent and full-time position with any Central or State Government department or agency in India, under direct payroll employment (i.e., not through third-party contract or outsourcing).\n\nHR Contact Person:  You can also mail your resume to adarshmishra@inzint.com,  twinkle@inzint.com,   Feel free to connect directly at +91 9289704058 for any queries.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Java', 'Github', 'React.Js', 'Machine Learning', 'Python', 'NoSQL', 'Rest Api Development', 'SQL']",2025-06-14 06:06:27
Analyst,Ford,1 - 8 years,Not Disclosed,['Chennai'],"The role demands knowledge on accounting and month end GL close activities.\nIndia related advance and retention payment procedure.\nClearence of supplier debit balance through Debit Memo / Credit Memo processing.\nGood Communication, analytical and problem-solving skills.\nPrior Payable operations experience will be preferred.\nBachelor s degree in commerce with good accounting, analytical and communication skills.\nProcessing of Debit Memo / Credit Memo to offset supplier debit balance within the same group of suppliers.\nAnalysis of Debit balance and supplier follow up for clearance.\nPrepare and submit Journal Entry, Sub-ledger before the timeline.\nHandle BCP activity.\nHandle India advance and retention payments.",Industry Type: Automobile,Department: Finance & Accounting,"Employment Type: Full Time, Permanent","['Analyst', 'Accounting', 'Analytical', 'Commerce', 'debit']",2025-06-14 06:06:29
Analyst-10,Merkle Science,1 - 2 years,Not Disclosed,['Bengaluru'],"The purpose of this role is to provide support for the collection, analysis, and dissemination of insights to our clients\nJob Description:\nKey responsibilities:\nIntegrates disparate datasets, conducts data preparation for analyses\nApplies data science methods to provide insights and recommendations to clients\nDelivers analytic outcomes based on project timelines and key milestones\nMaintains knowledge of new trends in the data science industry\nDevelops and manages code used for analytics purposes\nCommunicates findings and insights\nKnowledge on SQL, Tableau\nLocation:\nBangalore\nBrand:\nIprospect\nTime Type:\nFull time\nContract Type:\nPermanent",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Analyst', 'tableau', 'data science', 'Analytics', 'SQL']",2025-06-14 06:06:31
Business Analyst,Gieom Business Solutions,0 - 2 years,4.8-6 Lacs P.A.,['Mumbai (All Areas)'],"Working on GRC and process management projects with global banks, gathering and documenting requirements, configuring systems, supporting pre-sales, learning products quickly & contributing insights via research and content creation.\n\n\n\nHealth insurance\nProvident fund",Industry Type: Software Product,Department: Other,"Employment Type: Full Time, Permanent","['Requirement Gathering', 'Strong fundamentals on Financial/Operations domain', 'Communication Skills', 'Requirement Analysis']",2025-06-14 06:06:33
"Business Analyst, INSLP",Amazon,1 - 6 years,Not Disclosed,['Bengaluru'],"The ideal candidate is an experienced analyst who has demonstrated proficiency in analytics driven business solutions. The person would be a Data resource for the team and would work to generate actionable intelligence and insights for the team through rigorous data analysis and structured reporting, ensuring their efforts are focused in the appropriate areas. The person would deep-dive and bring out pointers that will help bring in continuous improvement/changes in processes from the Loss Prevention standpoint, thereby helping in reducing the losses across Amazon network. They are comfortable in analyzing data from multiple sources to create strategic recommendations in a thoughtful, concise manner and obtaining organizational buy-in at senior levels. They are well-organized, can manage multiple analyses/projects simultaneously, and is intellectually curious. Successful candidates will be expected to demonstrate our leadership principles: a bias for action, deep-dive, ownership and customer-obsession.\n\n\nKey Responsibilities includes\n1.Converting data into digestible business intelligence and actionable information\n2.Writing high quality SQL codes to retrieve and analyze data.\n3.Working with large data sets, automate data extraction, and build monitoring/reporting dashboard\n4.Interacting with internal stakeholders to deep-dive outlier events\n5.Analyze and solve business problems with focus on understanding root causes and driving forward-looking opportunities\n6.Communicating complex analysis and insights to our stakeholders and business leaders, both verbally and in writing.\n7.Enable effective decision making by retrieving and aggregating data from multiple sources and compiling it into a digestible and actionable format 1+ years of tax, finance or a related analytical field experience\n2+ years of complex Excel VBA macros writing experience\nBachelors degree or equivalent\nExperience defining requirements and using data and metrics to draw business insights\nExperience with SQL or ETL Experience working with Tableau\nExperience using very large datasets",,,,"['Data analysis', 'Analytical', 'Business intelligence', 'Continuous improvement', 'Business solutions', 'Macros', 'Analytics', 'Monitoring', 'SQL', 'Data extraction']",2025-06-14 06:06:36
Sr Manager,Visa,11 - 16 years,Not Disclosed,['Bengaluru'],"Visa s Technology Organization is a community of problem solvers and innovators reshaping the future of commerce. We operate the world s most sophisticated processing networks capable of handling more than 65k secure transactions a second across 80M merchants, 15k Financial Institutions, and billions of everyday people. While working with us you ll get to work on complex distributed systems and solve massive scale problems centered on new payment flows, business and data solutions, cyber security, and B2C platforms.\nThe Opportunity:\nWe are looking for Versatile, curious, and energetic Software Engineers who embrace solving complex challenges on a global scale. As a Visa Software Engineer, you will be an integral part of a multi-functional development team inventing, designing, building, and testing software products that reach a truly global customer base. While building components of powerful payment technology, you will get to see your efforts shaping the digital future of monetary transactions.\nRoles and Responsibilities:\nResponsible for building a strong B2B Payables team for commercial products and solutions.\nInspire and lead your team to deliver strategic and novel approaches that help optimize Visa s approach to growth, the client-facing products, and streaming payment processing and guide ongoing product decisions.\nPartner with business for market research across the B2B transaction eco system to understand the key impediments to Visa s acceptance for commercial transactions.\nGrow and expand your expert team with an eye toward diverse and forward-pushing skills that span data engineering, experimentation, and building of resilient solutions.\nEnhance B2B adoption of AI and Agentic frameworks\nBuild an inclusive team culture that is innovative and collaborative.\nLead strong partnerships key stakeholders including product management, engineering, design, and operations.\nEngage in technical and functional discussions with all engineering teams with strong interpersonal and organisational skills to effectively manage multiple work streams.\nEnjoy having a strategic impact at all levels, including interacting with other leaders on tangly issues and applying strong judgment and analysis toward important business decisions.\nArticulate with both technical and business partners to create effective frameworks to aid the discussion of complicated topics.\nThe Work itself:\nDesign code and systems that touch 40% of the world population while influencing Visa s internal standards for scalability, security, and reusability\nCollaborate multi-functionally to create design artifacts and develop best-in-class software solutions for multiple Visa technical offerings\nActively contribute to product quality improvements, valuable service technology, and new business flows in diverse agile squads\nDevelop robust and scalable products intended for a myriad of customers including end-user merchants, b2b, and business to government solutions.\nLeverage innovative technologies to build the next generation of Payment Services, Transaction Platforms, Real-Time Payments, and Buy Now Pay Later Technology\nOpportunities to make a difference on a global or local scale through mentorship and continued learning opportunities\nEssential Functions:\nWorks with product owners to gather and refine requirements for one product, adding and taking into account existing tools and solutions across departments.\nDevelops and designs moderately advanced architect solutions that are robust and scalable, considering integrations with other solutions across the internal technical ecosystem.\nProvides domain expertise on the development of user documentation of solutions and implements standard processes in user documentation.\nPlays a significant role in the development and delivery of new features within a product from end-to-end.\nThe Skills You Bring:\nEnergy and Experience: A growth mindset that is curious and passionate about technologies and enjoys challenging projects on a global scale\nChallenge the Status Quo: Comfort in pushing the boundaries, hacking beyond traditional solutions\nLanguage Expertise: Expertise in one or more general development languages (e.g., Java, C#, C++)\nBuilder: Experience building and deploying modern services and web applications with quality and scalability\nLearner: Constant drive to learn new technologies such as Angular, React, Kubernetes, Docker, etc.\nPartnership: Experience collaborating with Product, Test, Dev-ops, and Agile/Scrum teams\n\n\nBasic Qualifications\no12+ years of work experience with a M.S/Ph.D. in an engineering/quantitative\nfield such as Mathematics, Computer Science, Statistics, Artificial Intelligence,\nMachine Learning, etc.\n\nPreferred Qualificat",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Product management', 'Computer science', 'C++', 'Backend', 'Machine learning', 'Agile', 'Problem management', 'Market research', 'Application development', 'Distribution system']",2025-06-14 06:06:38
"Business Analyst, JWO CXQO",Amazon,1 - 6 years,Not Disclosed,['Hyderabad'],"As part of the AWS Solutions organization, we have a vision to provide business applications, leveraging Amazon s unique experience and expertise, that are used by millions of companies worldwide to manage day-to-day operations. We will accomplish this by accelerating our customers businesses through delivery of intuitive and differentiated technology solutions that solve enduring business challenges. We blend vision with curiosity and Amazon s real-world experience to build opinionated, turnkey solutions. Where customers prefer to buy over build, we become their trusted partner with solutions that are no-brainers to buy and easy to use.\n\nAmazon Physical Stores is more start-up than big-company, a group of entrepreneurial, analytical, and creative leaders with innovation at our core. We re pushing the state of the art in helping customers shop in engaging, fast, and safe ways. To date we ve created stores that let you use an app to enter, take what you want from our fresh selection, and go (Amazon Go); a smart shopping cart that uses computer vision algorithms and sensor fusion to let you skip the checkout line (Amazon Dash Cart); and contactless services that let you pay, enter or identify yourself (Amazon One). Note: The specific team is internally and tech focused to delivery scalable solutions.\n\nOur checkout-free shopping experience is made possible by our Just Walk Out Technology, which automatically detects when products are taken from or returned to the shelves and keeps track of them in a cart. When you re done shopping, you can just leave the store. Shortly after, we ll charge your Amazon account and send you a receipt. Check it out at amazon.com/go . Designed and custom-built by Amazonians, our Just Walk Out Technology uses a variety of technologies including computer vision, sensor fusion, and advanced machine learning. Innovation is part of our DNA! Our goal is to be Earths most customer centric company and we are just getting started. We need people who want to join an ambitious program that continues to push the state of the art in computer vision, machine learning, distributed systems and hardware design.\n\nWe are looking for a Business Analyst with a passion for using data to discover and solve real world problems. You will enjoy working with one of the richest data sets in the world, latest technology, and the ability to see your insights drive the creation of JWO stores.\n\nThe perfect candidate will have passion and experience analyzing data and using that analysis to drive key insights and recommendations. As a business analyst you will also build reports and metrics, drive ad hoc analysis and communicate insights to key stakeholders. You will not only execute on the required skills but also be able to influence, educate, and drive results in a fast-paced, ambiguous environment.\n\n\nContinually improve ongoing reporting and analysis processes, automating or simplifying self-service support for customers\nInterface with business customers, gathering requirements and delivering complete reporting solutions\nInterface with other technology teams to extract, transform, and load (ETL) data from a wide variety of data sources\n\nA day in the life\nThis position will closely work with CXQO operations team to develop dashboards to analyze data for determining root cause, building business insights and dive deep.\nIt involves complex analysis and diving deep into key metrics based on requests, building and maintaining multiple databases.\n\nAbout the team\nDiverse Experiences\nAmazon values diverse experiences. Even if you do not meet all of the preferred qualifications and skills listed in the job description, we encourage candidates to apply. If your career is just starting, hasn t followed a traditional path, or includes alternative experiences, don t let it stop you from applying.\n\nWhy AWS\nAmazon Web Services (AWS) is the world s most comprehensive and broadly adopted cloud platform. We pioneered cloud computing and never stopped innovating that s why customers from the most successful startups to Global 500 companies trust our robust suite of products and services to power their businesses.\nWork/Life Balance\nWe value work-life harmony. Achieving success at work should never come at the expense of sacrifices at home, which is why we strive for flexibility as part of our working culture. When we feel supported in the workplace and at home, there s nothing we can t achieve.\nInclusive Team Culture\nAWS values curiosity and connection. Our employee-led and company-sponsored affinity groups promote inclusion and empower our people to take pride in what makes us unique. Our inclusion events foster stronger, more collaborative teams. Our continual innovation is fueled by the bold ideas, fresh perspectives, and passionate voices our teams bring to everything we do.\n\nMentorship and Career Growth\nWe re continuously raising our performance bar as we strive to become Earth s Best Employer. That s why you ll find endless knowledge-sharing, mentorship and other career-advancing resources here to help you develop into a better-rounded professional.\n\nWe have BIEs and DEs in our team to support analytics,data and reporting needs for DS, SDE and PMs across JWO team 1+ years of tax, finance or a related analytical field experience\n2+ years of complex Excel VBA macros writing experience\nBachelors degree or equivalent\nExperience defining requirements and using data and metrics to draw business insights\nExperience with SQL or ETL Experience working with Tableau\nExperience using very large datasets",,,,"['Cloud computing', 'Analytical', 'Machine learning', 'Hardware design', 'Technology solutions', 'Business applications', 'Macros', 'Distribution system', 'Analytics', 'SQL']",2025-06-14 06:06:40
"Mgr II, Data Science",Reuters,10 - 15 years,Not Disclosed,"['Mumbai', 'Hyderabad']","The Data Scientist organization within the Data and Analytics division is responsible for designing and implementing a unified data strategy that enables the efficient, secure, and governed use of data across the organization. We aim to create a trusted and customer-centric data ecosystem, built on a foundation of data quality, security, and openness, and guided by the Thomson Reuters Trust Principles. Our team is dedicated to developing innovative data solutions that drive business value while upholding the highest standards of data management and ethics.\nAbout the role:\nWork with low to minimum supervision to solve business problems using data and analytics.\nWork in multiple business domain areas including Customer Experience and Service, Operations, Finance, Sales and Marketing.\nWork with various business stakeholders, to understand and document requirements.\nDesign an analytical framework to provide insights into a business problem.\nExplore and visualize multiple data sets to understand data available for problem solving.\nBuild end to end data pipelines to handle and process data at scale.\nBuild machine learning models and/or statistical solutions.\nBuild predictive models.\nUse Natural Language Processing to extract insight from text.\nDesign database models (if a data mart or operational data store is required to aggregate data for modeling).\nDesign visualizations and build dashboards in Tableau and/or PowerBI\nExtract business insights from the data and models.\nPresent results to stakeholders (and tell stories using data) using power point and/or dashboards.\nWork collaboratively with other team members.\nAbout you:\nOverall 10+ years experience in technology roles.\nMust have a minimum of 5 years of experience working in the data science domain.\nHas used frameworks/libraries such as Scikit-learn, PyTorch, Keras, NLTK.\nHighly proficient in Python.\nHighly proficient in SQL.\nExperience with Tableau and/or PowerBI.\nHas worked with Amazon Web Services and Sagemaker.\nAbility to build data pipelines for data movement using tools such as Alteryx, GLUE, Informatica.\nProficient in machine learning, statistical modelling, and data science techniques.\nExperience with one or more of the following types of business analytics applications:\nPredictive analytics for customer retention, cross sales and new customer acquisition.\nPricing optimization models.\nSegmentation.\nRecommendation engines.\nExperience in one or more of the following business domains\nCustomer Experience and Service.\nFinance.\nOperations.\nGood presentation skills and the ability to tell stories using data and PowerPoint/Dashboard Visualizations.\nExcellent organizational, analytical and problem-solving skills.\nAbility to communicate complex results in a simple and concise manner at all levels within the organization.\nAbility to excel in a fast-paced, startup-like environment.\n#LI-SS5\nWhat s in it For You?\nHybrid Work Model: We ve adopted a flexible hybrid working environment (2-3 days a week in the office depending on the role) for our office-based roles while delivering a seamless experience that is digitally and physically connected.\nFlexibility & Work-Life Balance: Flex My Way is a set of supportive workplace policies designed to help manage personal and professional responsibilities, whether caring for family, giving back to the community, or finding time to refresh and reset. This builds upon our flexible work arrangements, including work from anywhere for up to 8 weeks per year, empowering employees to achieve a better work-life balance.\nCareer Development and Growth: By fostering a culture of continuous learning and skill development, we prepare our talent to tackle tomorrow s challenges and deliver real-world solutions. Our Grow My Way programming and skills-first approach ensures you have the tools and knowledge to grow, lead, and thrive in an AI-enabled future.\nIndustry Competitive Benefits: We offer comprehensive benefit plans to include flexible vacation, two company-wide Mental Health Days off, access to the Headspace app, retirement savings, tuition reimbursement, employee incentive programs, and resources for mental, physical, and financial wellbeing.\nCulture: Globally recognized, award-winning reputation for inclusion and belonging, flexibility, work-life balance, and more. We live by our values: Obsess over our Customers, Compete to Win, Challenge (Y)our Thinking, Act Fast / Learn Fast, and Stronger Together.\nSocial Impact: Make an impact in your community with our Social Impact Institute. We offer employees two paid volunteer days off annually and opportunities to get involved with pro-bono consulting projects and Environmental, Social, and Governance (ESG) initiatives.\nMaking a Real-World Impact: We are one of the few companies globally that helps its customers pursue justice, truth, and transparency. Together, with the professionals and institutions we serve, we help uphold the rule of law, turn the wheels of commerce, catch bad actors, report the facts, and provide trusted, unbiased information to people all over the world.\nAbout Us\nThomson Reuters informs the way forward by bringing together the trusted content and technology that people and organizations need to make the right decisions. We serve professionals across legal, tax, accounting, compliance, government, and media. Our products combine highly specialized software and insights to empower professionals with the data, intelligence, and solutions needed to make informed decisions, and to help institutions in their pursuit of justice, truth, and transparency. Reuters, part of Thomson Reuters, is a world leading provider of trusted journalism and news.\nWe are powered by the talents of 26,000 employees across more than 70 countries, where everyone has a chance to contribute and grow professionally in flexible work environments. At a time when objectivity, accuracy, fairness, and transparency are under attack, we consider it our duty to pursue them. Sound exciting? Join us and help shape the industries that move society forward.\nAs a global business, we rely on the unique backgrounds, perspectives, and experiences of all employees to deliver on our business goals. To ensure we can do that, we seek talented, qualified employees in all our operations around the world regardless of race, color, sex/gender, including pregnancy, gender identity and expression, national origin, religion, sexual orientation, disability, age, marital status, citizen status, veteran status, or any other protected classification under applicable law. Thomson Reuters is proud to be an Equal Employment Opportunity Employer providing a drug-free workplace.\nWe also make reasonable accommodations for qualified individuals with disabilities and for sincerely held religious beliefs in accordance with applicable law. More information on requesting an accommodation here .\nLearn more on how to protect yourself from fraudulent job postings here .\nMore information about Thomson Reuters can be found on thomsonreuters.com.",Industry Type: Internet,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Customer acquisition', 'Data management', 'Business analytics', 'Analytical', 'Consulting', 'Customer retention', 'Data quality', 'Operations', 'SQL', 'Service operations']",2025-06-14 06:06:43
Backend Developer Lead,Tata Elxsi,10 - 20 years,Not Disclosed,"['Chennai', 'Bengaluru', 'Thiruvananthapuram']","For Lead: 10+ YRS\nDesign and develop scalable microservices for content ingestion, metadata management,\nuser authentication, and video playback.\nBuild and maintain APIs for mobile, web, and smart TV clients.\nOptimize backend systems for performance, scalability, and fault tolerance.\nImplement caching strategies and data pipelines for real-time analytics and personalization.\nCommunicate with Customer/Stakeholders and Lead Team in Development and solving\nProduction Issues\nCollaborate with cross-functional teams including frontend, DevOps, data engineering, and\nproduct management.\nCollaborate with DevOps to ensure CI/CD, observability, and infrastructure automation.\nIntegrate with third-party services such as DRM providers, CDNs and recommendation\nengines.\nMentor junior developers and contribute to architectural decisions.\nFor Architect Role: 14+ YRS\nArchitect and design scalable, resilient, and secure backend systems for OTT services including\nvideo streaming, content delivery, user management, and recommendation engines.\nLead the technical roadmap, ensuring alignment with business goals and product vision.\nCollaborate with cross-functional teams including frontend, DevOps, data engineering, and\nproduct management.\nDefine and enforce best practices in software architecture, coding standards, and system\ndesign.\nEvaluate and integrate third-party services such as CDNs, DRM providers, and analytics\nplatforms.\nDrive performance optimization, scalability improvements, and cost-efficiency across backend\nservices.\nMentor and guide backend engineers, conducting code reviews and architectural\nassessments.\nStay updated with emerging technologies in cloud computing, microservices, and media\nstreaming.\nEnsure code quality, proper documentation, and smooth deployment using CI/CD pipelines.\nTroubleshoot and resolve production issues efficiently",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Ott', 'Backend Development', 'RESTful API', 'Java', 'VOD', 'Node.Js', 'Live Streaming', 'Microservices', 'Docker', 'Azure Cloud', 'Video', 'MySQL', 'MongoDB', 'Python']",2025-06-14 06:06:45
Analyst,Visa,5 - 6 years,Not Disclosed,['Bengaluru'],"Visa is seeking a Controls Monitoring & Testing Analyst within its Technology Risk Management program to review and assess Cybersecurity and Technology risks. The candidate will perform Risk Assessments, Design Effectiveness Assessments, and Operational Effectiveness Testing for key technology threat vectors such as security configuration management, firewall configuration, application, user access management, and availability & reliability. Responsibilities include managing stakeholder engagement plans, participating in process walkthroughs, tracking/reporting deliverables, and producing high-quality work papers for all lines of defense and risk stakeholders. Additionally, the candidate will interpret data from source systems to perform statistical sampling and aggregate assessment across various risk management levers, collaborate with technology partners, and distill information into management and executive-level reporting.\nKey Responsibilities:\nTechnology & Cybersecurity Controls Testing:\nPerform independent technology and cybersecurity controls testing.\nDocument testing results in detailed workpapers.\nPrepare management reports based on testing outcomes.\nCommunicate findings with stakeholders.\nAutomation for Continuous Monitoring:\nDevelop automation for continuous controls monitoring/auditing for technology and cybersecurity.\nMonitor the results of automated controls, perform investigation and follow-ups as needed.\nRisk & Control Self-Assessment (RCSA):\nExecute RCSA Risk Business Partner (RBP) controls quality review and sample-based testing.\nConduct Key Risk Indicator (KRI) testing.\nTraining, Metrics Alignment & Reporting:\nDevelop and track risk management training.\nAlign metrics with reporting dashboards.\nDevelop reporting and stakeholder communication.\n\n\nBachelor s degree with 5 years of work experience in cyber, risk controls, or equivalent.\nExperience with technology and cyber processes and functions (e.g., Vulnerability, Availability & Reliability Risk, Cyber Defense, Third Par",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Cism', 'Automation', 'Cisa', 'Access management', 'Configuration management', 'Analytical', 'Vulnerability', 'Risk management', 'Operations', 'Testing']",2025-06-14 06:06:47
"Director, Enterprise Data Architecture",Amgen Inc,10 - 15 years,Not Disclosed,['Hyderabad'],"Role Description:\nThe Director for Data Architecture and Solutions will lead Amgens enterprise data architecture and solutions strategy, overseeing the design, integration, and deployment of scalable, secure, and future-ready data systems. This leader will define the architectural vision and guide a high-performing team of architects and technical experts to implement data and analytics solutions that drive business value and innovation.\nThis role demands a strong blend of business acumen, deep technical expertise, and strategic thinking to align data capabilities with the company's mission and growth. The Director will also serve as a key liaison with executive leadership, influencing technology investment and enterprise data direction",,,,"['Data Architecture', 'DevOps', 'Azure', 'Zachman', 'GCP', 'TOGAF', 'CI/CD', 'ETL', 'ELT', 'AWS']",2025-06-14 06:06:50
Data Visualization Manager,Avalara Technologies,8 - 13 years,Not Disclosed,['Pune'],"What You'll Do\n\nThe Global Analytics and Insights (GAI) team is seeking an experienced and experienced Data Visualization Manager to lead our data-driven decision-making initiatives.\nThe ideal candidate will have a background in Power BI, expert-level SQL proficiency, to drive actionable insights and demonstrated leadership and mentoring experience, and an ability to drive innovation and manage complex projects.\nYou will become an expert in Avalara's financial, marketing, sales, and operations data.\n\nWhat Your Responsibilities Will Be\n\nWhat You'll Need to be Successful",,,,"['Data Visualization', 'Business Intelligence', 'Power BI', 'performance tuning', 'AWS Redshift', 'BI strategy', 'Power Query', 'SQL', 'Azure Synapse', 'data modeling', 'Snowflake', 'DAX', 'Data Analytics', 'ETL']",2025-06-14 06:06:52
Reservoir Modeling Digital Solutions Earth Scientist,Chevron India,2 - 4 years,Not Disclosed,['Bengaluru'],"Total Number of Openings\n1\nAbout the position:\n\nChevron ENGINE is looking for high-performing candidates to join our Reservoir Modeling team with a focus on digital solutions development. The Reservoir Modeling team focuses on field development/application as well as technology development. This is a mid-career position in the Chevron ENGINE Earth Science Chapter for a knowledgeable earth scientist practitioner with digital solutions development (SLB Ocean and Petrel) applied to the subsurface domain. This role performs Reservoir Modeling related digital solutions development to support Chevron operations. Will be involved in enhancing Chevron s Reservoir Characterization and Modeling workflows to provide differentiating capability and integration of a mix of proprietary and best-in-class vendor products. The Reservoir Modeling and Digital Solutions Earth Scientist should have sufficient experience and expertise to deliver products with support from the Digital Platform.\n\nKey responsibilities:\nResponsible for the development and/or deployment of existing and new technology and workflows to solve key business challenges across Chevron\nThe job involves integration of geological, petrophysical, geophysical and engineering data to create reservoir models used for resource evaluation and production forecasting\nProvides support to the Digital Platform, and is responsible for running, maintaining, supporting users and improving the Geology suite of Petrel-plugins\nContributes to the development of new Geological plug-ins in Petrel (e.g. coding in Ocean, development of user interfaces and algorithms)\nTargeted user-experience and data management development (e.g., GUI, Power-BI, data engineering, Machine Learning engineering, and mature digital minimum viable products)\nTeaming with US-based R&D groups focusing on developing and deploying reservoir modeling and geology products and workflows\nApplication testing, pipeline, build and release in cloud platform\nDevelopment of technical digital solutions and training documentation\n\nRequired Qualifications:\nMSc/PHD degree in Earth Science or Engineering from deemed/recognized (AICTE) university\nAt least five years of experience in Earth Science or Engineering or related industry\nProficiency with Petrel or equivalent Earth Modeling suite\nDemonstrated fluency in digital solutions development, including python, C#, C++, Azure ecosystems developments (pipelines, dashboards, visualization, data management systems)\nUnderstanding of physical processes associated with earth science, reservoir modeling and subsurface.\nExperience with customer support and understanding of subsurface business needs and utilizing Scaled Agile Framework (SAFe) processes to implement highly prioritized features\nGood communication skills and ability to work effectively in a team environment\nFundamental knowledge of geological workflows applied to subsurface\nFamiliarity with SLB Ocean development framework is preferred but not a must\nSkills of using Machine Learning/AI to accelerate performance, and accuracy of reservoir characterization and modeling is preferred but not a must\nDirect experience with discretization of reservoir models is preferred but not a must\n\nChevron ENGINE supports global operations, supporting business requirements across the world. Accordingly, the work hours for employees will be aligned to support business requirements. The standard work week will be Monday to Friday. Working hours are 8:00am to 5:00pm or 1.30pm to 10.30pm.\nChevron participates in E-Verify in certain locations as required by law.",Industry Type: Oil & Gas,Department: Research & Development,"Employment Type: Full Time, Permanent","['global operations', 'C++', 'Data management', 'Coding', 'Management systems', 'Machine learning', 'Agile', 'Customer support', 'Forecasting', 'Python']",2025-06-14 06:06:55
Data Architect (Data Snowflake),Diacto Technologies,5 - 9 years,Not Disclosed,['Pune( Baner )'],"Job Overview:\nDiacto is looking for a highly capable Data Architect with 5 to 9 years of experience to lead cloud data platform initiatives with a primary focus on Snowflake and Azure Data Hub. This individual will play a key role in defining the data architecture strategy, implementing robust data pipelines, and enabling enterprise-grade analytics solutions. This is an on-site role based in our Baner, Pune office.\n\nQualifications:\nB.E./B.Tech in Computer Science, IT, or related discipline\nMCS/MCA or equivalent preferred\n\nKey Responsibilities:\nDesign and implement enterprise-level data architecture with a strong focus on Snowflake and Azure Data Hub\nDefine standards and best practices for data ingestion, transformation, and storage\nCollaborate with cross-functional teams to develop scalable, secure, and high-performance data pipelines\nLead Snowflake environment setup, configuration, performance tuning, and optimization\nIntegrate Azure Data Services with Snowflake to support diverse business use cases\nImplement governance, metadata management, and security policies\nMentor junior developers and data engineers on cloud data technologies and best practices\n\nExperience and Skills Required:\n5 to 9 years of overall experience in data architecture or data engineering roles\nStrong, hands-on expertise in Snowflake, including design, development, and performance tuning\nSolid experience with Azure Data Hub and Azure Data Services (Data Lake, Synapse, etc.)\nUnderstanding of cloud data integration techniques and ELT/ETL frameworks\nFamiliarity with data orchestration tools such as DBT, Airflow, or Azure Data Factory\nProven ability to handle structured, semi-structured, and unstructured data\nStrong analytical, problem-solving, and communication skills\n\nNice to Have:\nCertifications in Snowflake and/or Microsoft Azure\nExperience with CI/CD tools like GitHub for code versioning and deployment\nFamiliarity with real-time or near-real-time data ingestion\n\nWhy Join Diacto Technologies?\nWork with a cutting-edge tech stack and cloud-native architectures\nBe part of a data-driven culture with opportunities for continuous learning\nCollaborate with industry experts and build transformative data solutions\nCompetitive salary and benefits with a collaborative work environment in Baner, Pune\n\nHow to Apply:\n\nOption 1 (Preferred)\nCopy and paste the following link on your browser and submit your application for automated interview process: -\n\nhttps://app.candidhr.ai/app/candidate/gAAAAABoRrcIhRQqJKDXiCEfrQG8Rtsk46Etg4-K8eiwqJ_GELL6ewSC9vl4BjaTwUAHzXZTE3nOtgaiQLCso_vWzieLkoV9Nw==/\n\nOption 2\n\n1. Please visit our website's career section at https://www.diacto.com/careers/\n2. Scroll down to the ""Who are we looking for?"" section\n3. Find the listing for "" Data Architect (Snowflake)"" and\n4. Proceed with the virtual interview by clicking on ""Apply Now.""",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Airflow', 'Azure Data Factory', 'Snowflake', 'DBT', 'Etl Pipelines', 'Team Collaboration', 'Problem Solving', 'Azure Databricks', 'Elt', 'Apache', 'Analytical', 'Data Security', 'Data Lake', 'Data Modeling', 'Data Governance', 'Synopsys']",2025-06-14 06:06:57
Senior Markit EDM Developer/Consultant Specialist,Hsbc,4 - 7 years,Not Disclosed,['Bengaluru'],"Some careers shine brighter than others.\nIf you re looking for a career that will help you stand out, join HSBC and fulfil your potential. Whether you want a career that could take you to the top, or simply take you in an exciting new direction, HSBC offers opportunities, support and rewards that will take you further.\nHSBC is one of the largest banking and financial services organisations in the world, with operations in 64 countries and territories. We aim to be where the growth is, enabling businesses to thrive and economies to prosper, and, ultimately, helping people to fulfil their hopes and realise their ambitions.\nBackground:\nHSBC Securities Services (SSv) provides comprehensive global, regional and domestic Custody, Fund Administration, Transfer Agency, Middle Office, Treasury and Fiduciary services to large institutional clients. The SSv Data, Digital and Innovation team (DDI) is responsible for delivering strategic data and digital solutions using open source, big data cloud technologies as well as by collaborating with various fintech partners.\nThe Opportunity:\nThe DDI technology team is a cross functional capability that focuses on building, maintaining and supporting platforms for clients as well as core business teams like Custody, Fund Services, Middle Office etc. Our multi-disciplined teams include Application Data Engineers, Architects, DevOps Engineers, Infra Specialists Programme Managers. The successful candidate will have an opportunity to play a major role in our strategic multi-year, multi-million dollar Digital and Data Transformation Programme to support the global business.\nThe role holder is expected to work in partnership with Platform Technology Leads DDI, Architects and technology delivery teams to deliver on the agenda for the DDI function.\nWe are currently seeking an experienced professional to join our team in the role of a Consultant Specialist\nIn this role, you will:\nResponsible for ensuring the effective delivery of technology services through support and maintenance of the Digital and Data technology estate, and the design, build and delivery of new services as well as management of existing services.\nCreating Data pipelines for Consolation of Data from multiple streams and further use for reporting\nResponsible for the delivery and rollout of strategic Digital and Data Technology platform globally, in line with the roadmap agreed with the business.\nBuild excellent relationships with Product, Product Delivery and Operations teams to help create a one team approach to planning and delivery.\nIdentifies and removes barriers to success. Always acts in the HSBC s best interests and willing to openly discuss potentially difficult topics.\nPromotes agile transformation and deployment methodologies, and helps with adoption of moving from Project to Product , along with Product, Product Delivery and Operations peers.\nPromotes and takes accountability for quality; identifies opportunities to improve delivery and oversees the governance process to reduce risk and ensure effective and timely decisions\nPromote a positive work culture based on respect for people, integrity of actions, creativity and collaboration leading to increased productivity\nInspire confidence in the future vision and generate excitement, enthusiasm and commitment.\nDemonstrable ability to gather requirements, analyse, question and produce technical documentation\nProvide technical development expertise to convert business requirements into appropriate technical solutions\n\n\n\n\n\n\n\n\n\n\nRequirements\n\n\n\nTo be successful in this role, you should meet the following requirements:\nProficiency in writing SQL Queries, Stored Procedures, functions, views, analytical functions for data extraction, transformation, aggregation, analytics and reporting\nHands-on Expertize in Markit EDM\nAbility to debug, analyse and optimize SQL queries\nExposure to build Management tools like G3, Code repository GitHub and in continuous integration and deployment tools\nProblem solving and critical thinking",Industry Type: Consumer Electronics & Appliances,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SQL queries', 'Analytical', 'Agile', 'Fund administration', 'Stored procedures', 'Open source', 'Analytics', 'Financial services', 'Data extraction', 'Technical documentation']",2025-06-14 06:07:00
Data Governance Consultant,mncs,3 - 8 years,Not Disclosed,['Ahmedabad'],"Project Role : Data Engineer\nProject Role Description : Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\nMust have skills : Data Governance\nMinimum 3 year(s) of experience is required\nEducational Qualification : 15 years full time education\nJob Summary:\nWe are seeking a highly skilled and motivated Governance Tool Specialist to join our team with 4 years of experience.\nThe candidate will be responsible for the implementation, configuration, and management of our governance tools.\nThis role requires a deep understanding of data governance principles, excellent technical skills, and the ability to work collaboratively with various stakeholders.\nOptional - Experienced Data Quality Specialist with extensive expertise in using Alex Solutions tools to ensure data accuracy, consistency, and reliability.\nProficiency in data profiling, cleansing, validation, and governance.\nKey Responsibilities:\nData Governance:\n• Implement and configure Alex Solutions governance tools to meet client requirements.\n• Collaborate with clients to understand their data governance needs and provide tailored solutions.\n• Provide technical support and troubleshooting for governance tool issues.\n• Conduct training sessions and workshops to educate clients on the use of governance tools.\n• Develop and maintain documentation for governance tool configurations and processes.\n• Monitor and report on the performance and usage of governance tools.\n• Stay up-to-date with the latest developments in data governance and related technologies.\n• Work closely with the product development team to provide feedback and suggestions for tool enhancements.\nData Quality:\n• Utilized Alex Solutions' data quality tools to develop and implement processes, standards, and guidelines that ensure data accuracy and reliability.\n• Conducted comprehensive data profiling using Alex Solutions, identifying and rectifying data anomalies and inconsistencies.\n• Monitored data quality metrics through Alex Solutions, providing regular reports on data quality issues and improvements to stakeholders.\n• Collaborated with clients to understand their data quality needs and provided tailored solutions using Alex Solutions.\n• Implemented data cleansing, validation, and enrichment processes within the Alex Solutions platform to maintain high data quality standards.\n• Developed and maintained detailed documentation for data quality processes and best practices using Alex Solutions' tools.\nPreferred Skills:\nMust Have Skills: Alex Solutions\nGood to Have: Unity Catalog, Microsoft Purview, Data Quality tool\nSecondary Skills: Informatica, Collibra\nExperience with data cataloging, data lineage, data quality and metadata management.\n• Knowledge of regulatory requirements related to data governance (e.g., GDPR, CCPA).\n• Familiarity with cloud platforms and services (e.g., AWS, Azure, Google Cloud).\n• Certification in data governance or related fields.\n• Proven experience with data governance, data quality tools and technologies.\n• Strong understanding of data governance principles and best practices.\n• Proficiency in SQL, data modeling, and database management.\n• Excellent problem-solving and analytical skills.\n• Strong communication and interpersonal skills.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Microsoft Purview', 'Alex Solutions', 'Data Governance', 'Unity Catalog']",2025-06-14 06:07:02
Microsoft Azure Modern Data Platform,mncs,5 - 10 years,Not Disclosed,['Bhubaneswar'],"Project Role : Data Platform Engineer\nProject Role Description : Assists with the data platform blueprint and design, encompassing the relevant data platform components. Collaborates with the Integration Architects and Data Architects to ensure cohesive integration between systems and data models.\nMust have skills : Microsoft Azure Modern Data Platform\nMinimum 5 year(s) of experience is required\nEducational Qualification : 15 years full time education\nJob Description:\n• Development, customize and manage integration tools, databases, warehouses and analytical systems with the use of data related instruments/instances\n• Create and run complex queries and automation scripts for operational data processing.\n• Test the reliability and performance of each part of a system and cooperate with the testing team\n• Deploying data models into production environments. This entails providing the model with data stored in a warehouse or coming directly from sources, configuring data attributes, managing computing resources, setting up monitoring tools, etc.\n• Responsible for setting up tools to view data, generate reports, and create visuals\n• Monitoring the overall performance and stability of the system. Adjust and adapt the automated pipeline as data/models/requirements change.\n• Mentor and train colleagues where necessary by helping them learn and improve their skills, as well as innovate and iterate on best practices.\n• Solve complex issues with minimal supervision\n• Make improvement and process recommendations that have an impact on the business\nTechnology Stack Used & Required Knowledge:\n• Mastery of Azure Data Services with minimum of 4 years of such as Azure Data Factory, Azure Synapse, Azure Databricks, Azure SQL Database, Azure Cosmos DB, Azure Blob Storage/Data Lake Storage, and Azure Stream Analytics.\n• 5+ years Experience in using of Python/ PySpark for data engineering.\n• Understanding of data types/ handling of different data models.\n• Excellent understanding of ETL cycle\n• Expertise in designing scalable and efficient data architectures on Azure is a plus\n• Understanding of descriptive and exploratory statistics, predictive modelling, evaluation metrics, decision trees, machine learning algorithms is a plus.\n• Good scripting and programming skills.\n• Expertise in implementing data governance frameworks and ensuring compliance using Azure Purview. Familiarity with Azure Data Catalog for managing and discovering data assets.\n• Understanding of Azure Key Vault for secure key management.\n• Azure scripting with PowerShell/Azure CLI",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Data Factory', 'Microsoft Azure Modern Data Platform', 'Azure Databricks', 'Azure Data Lake', 'Azure SQL Database', 'Azure Synapse']",2025-06-14 06:07:04
Data Architect - Adobe Experience Platform,Ekloud Inc,10 - 12 years,Not Disclosed,['New Delhi'],"About the Role :\n\nWe are seeking a highly experienced and results-driven AEP Data Architect to join our team. The ideal candidate will possess deep expertise in data architecture, transformation, and modeling across large, complex datasets, particularly within customer-centric domains such as CRM, marketing, and sales systems.\n\nThis role requires hands-on experience with Adobe Experience Platform (AEP), advanced ETL processes, and the ability to translate business needs into scalable data solutions.\n\nKey Responsibilities :\n\n- Design, develop, and implement robust data architecture solutions within the Adobe Experience Platform (AEP).\n\n- Transform and model large-scale datasets, ensuring they are customer-centric and optimized for business intelligence and marketing use cases.\n\n- Define and manage business requirements, process designs, and use cases in collaboration with stakeholders.\n\n- Develop and maintain data pipelines using industry-standard ETL tools (e.g., Informatica, Unifi).\n\n- Create and manage relational, dimensional, columnar, and big data models.\n\n- Write complex SQL or NoSQL queries for data extraction, transformation, and analysis.\n\n- Collaborate with business teams to deliver actionable insights using reporting tools like Tableau and Power BI.\n\n- Lead data-related discussions with cross-functional teams including Sales, Marketing, and Engineering.\n\n- Provide strategic guidance and support for data governance, data quality, and integration best practices.\n\n- Manage multiple projects simultaneously with a strong focus on delivery, accuracy, and customer satisfaction.\n\nRequired Qualifications :\n\n- 10+ years of experience in data transformation and ETL processes on large datasets.\n\n- 5+ years of hands-on data modeling experience across various paradigms (relational, dimensional, big data, etc.).\n\n- Proven expertise in writing complex SQL or NoSQL queries.\n\n- In-depth knowledge of advanced data warehousing concepts.\n\n- Strong understanding of customer-centric data domains including CRM, Call Center, Marketing, POS, and Offline data.\n\n- Proficiency in ETL tools such as Informatica or Unifi.\n\n- Experience with data visualization tools such as Tableau and Power BI.\n\n- Strong analytical and problem-solving skills with a detail-oriented mindset.\n\n- Excellent verbal and written communication skills; ability to interface with both technical and business teams.\n\n- Demonstrated ability to work independently, proactively, and in a customer-focused manner.\n\n- Bachelors or Masters degree in Computer Science, Information Systems, Data Science, or a related field.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Adobe Experience Platform', 'Data Engineering', 'Data Pipeline', 'Data Architect', 'Big Data', 'Informatica', 'Data Modeling', 'ETL', 'SQL']",2025-06-14 06:07:06
"Senior Specialist, Java/Angular JS Engineer",Merck Sharp & Dohme (MSD),5 - 15 years,Not Disclosed,['Hyderabad'],"Based in Hyderabad, join a global healthcare biopharma company and be part of a 130- year legacy of success backed by ethical integrity, forward momentum, and an inspiring mission to achieve new milestones in global healthcare.\nBe part of an organization driven by digital technology and data-backed approaches that support a diversified portfolio of prescription medicines, vaccines, and animal health products.\nDrive innovation and execution excellence. Be a part of a team with passion for using data, analytics, and insights to drive decision-making, and which creates custom software, allowing us to tackle some of the worlds greatest health threats.\nOur Technology Centers focus on creating a space where teams can come together to deliver business solutions that save and improve lives. An integral part of our company s IT operating model, Tech Centers are globally distributed locations where each IT division has employees to enable our digital transformation journey and drive business outcomes. These locations, in addition to the other sites, are essential to supporting our business and strategy.\nA focused group of leaders in each Tech Center helps to ensure we can manage and improve each location, from investing in growth, success, and well-being of our people, to making sure colleagues from each IT division feel a sense of belonging to managing critical emergencies. And together, we must leverage the strength of our team to collaborate globally to optimize connections and share best practices across the Tech Centers.\nRole Overview\nWe are seeking a highly skilled and experienced Senior Multistack Java Developer to join our team and technically lead a group of talented developers. This role will involve close collaboration with a large team, focusing on enhancing, developing, and maintaining enterprise-grade applications. The ideal candidate will have 8-15 years of strong hands-on experience in hardcore development, expertise in Java,and front-end frameworks like React and Angular, in a continuous release environment. This position also requires proactive leadership, and expertise in managing production outages and troubleshooting.\nAs a Software Engineer focusing on Java/Angular JS you will design, develop, and maintain software systems. This role involves both creative and analytical skills to solve complex problems and create efficient, reliable software. You will use your expertise in requirements analysis, programming languages, software development methodologies, and tools to build and deliver software products that meet the needs of businesses, organizations, or end- users. You will work with other engineers, product managers and delivery leads, to design systems, determine functional and non-functional needs and implement solutions accordingly. You should be ready to work independently as well as in a team.\nWhat will you do in this role\nDesign, code, verify, test, document, amend and refactor moderately complex applications and software configurations for deployment in collaboration with cross-disciplinary teams across various regions worldwide.\nDesign test cases and test scripts under own direction, mapping back to pre-determined criteria, recording and reporting test outcomes. Participate in requirement, design and specification reviews. Perform manual and automation testing.\nElicit requirements for systems and software life cycle working practices and automation. Prepare design options for the working environment of methods, procedures, techniques, tools, and people. Utilize systems and software life cycle working practices for software components and micro-services. Deploy automation to achieve well-engineered and secure outcome.\nWork within a matrix organizational structure, reporting to both the functional manager and the Product manager.\nParticipate in Product planning, execution, and delivery, ensuring alignment with both functional and Product goals.\nLead and mentor a team of Java Microservices and front-end developers, providing technical guidance, code reviews, and fostering a culture of continuous learning.\nStay actively involved in development tasks, ensuring the delivery of high-quality, maintainable code.\nDesign and develop scalable systems using Java, Spring Framework, and Spring Boot for backend development.\nDevelop rich, responsive user interfaces using React, Angular to deliver seamless user experiences.\nEnsure optimal integration of front-end and back-end components for customer-facing applications\nDevelop and consume Microservices, REST/SOAP APIs, and XML web services.\nUtilize tools like Splunk, ELK, and database queries to monitor, troubleshoot, and resolve system issues.\nLeverage tools like Jenkins, Ansible, and Terraform for automated deployments.\nWork on cloud platforms like AWS, implementing and supporting cloud-based solutions.\nWhat should you have\nBachelor s degree in information technology, Computer Science or any Technology stream.\n~ 8-15 years of experience. 5+ years of experience with Java and Java frameworks and libraries (such as Spring Framework, Spring Boot, Hibernate, JEE, JDBC, JMS, JMX), relational databases, Kafka/RabbitMQ, and deployment to application servers.\nDevelop and maintain microservices-based applications using Spring Boot.\nDesign and implement RESTful APIs for seamless communication between services.\nExperience with enterprise-level databases both SQL and No-SQL such as Oracle, PostgreSQL, MongoDB etc\nAutomated testing frameworks (JUnit, TestNG, Selenium, etc.)\nMonitoring and log tools (Splunk, ELK, Prometheus, Grafana)\nFamiliarity with AWS Cloud, CI/CD pipelines (Jenkins, GitLab CI, etc.), and automation tools such as Jenkins, Ansible, and Terraform.\nProficiency in cloud platforms (e.g., AWS, Azure, Google Cloud) and containerization technologies (e.g., Docker, Kubernetes)..\nExcellent debugging, troubleshooting, and analytical skills.\nEffective verbal and written communication skills.\nFamiliarity with modern product development practices - Agile, Scrum, test driven development, UX, design thinking.\nHands-on experience with DevOps practices (Git, infrastructure as code, observability, continuous integration/continuous deployment - CI/CD).\nCloud-native, ideally AWS certified.\nProduct and customer-centric approach\nGood to Have Skills:\nExperience with any front-end technologies like React, Angular for building responsive user interfaces.\nUnderstanding of security best practices and data protection methodologies.\nExperience with Agile methodologies and tools like Jira or Trello.\nCertification of any of public cloud (AWS, GCP, Azure, OCI)\nA passionate commitment to learning about business domains and emerging technologies.\nOur technology teams operate as business partners, proposing ideas and innovative solutions that enable new organizational capabilities. We collaborate internationally to deliver services and solutions that help everyone be more productive and enable innovation.\nWho we are\nFor more than a century, we have been inventing for life, bringing forward medicines and vaccines for many of the worlds most challenging diseases. Today, our company continues to be at the forefront of research to deliver innovative health solutions and advance the prevention and treatment of diseases that threaten people and animals around the world.\nWhat we look for\nImagine getting up in the morning for a job as important as helping to save and improve lives around the world. Here, you have that opportunity. You can put your empathy, creativity, digital mastery, or scientific genius to work in collaboration with a diverse group of colleagues who pursue and bring hope to countless people who are battling some of the most challenging diseases of our time. Our team is constantly evolving, so if you are among the intellectually curious, join us and start making your impact today\nCurrent Employees apply HERE\nCurrent Contingent Workers apply HERE\nSearch Firm Representatives Please Read Carefully\nEmployee Status:\nRegular\nRelocation:\nVISA Sponsorship:\nTravel Requirements:\nFlexible Work Arrangements:\nHybrid\nShift:\nValid Driving License:\nHazardous Material(s):\n\nRequired Skills:\nAnimal Vaccination, Computer Science, Data Engineering, Data Visualization, Design, Design Applications, DevOps, Digital Technology, Digital Transformation, JavaScript, PostgreSQL, Requirements Analysis, Social Collaboration, Software Configurations, Software Development, Software Development Life Cycle (SDLC), Solution Architecture, Spring Framework, System Designs, Systems Integration, Systems Troubleshooting, Technical Consulting, Test Automation, Testing, Web Development",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Solution architecture', 'JMS', 'Hibernate', 'Front end', 'XML', 'Consulting', 'Healthcare', 'data visualization', 'Oracle', 'SQL']",2025-06-14 06:07:09
Intern - Knowledge Management,Oliver Wyman,3 months duration,Unpaid,['Mumbai'],"Company: Marsh\nDescription:\nMarsh McLennan Global Services India Private Limited (MMGS) is seeking a Knowledge Management professional for the following position based in the Mumbai, India office:\nSenior Analyst\nMarsh is a global leader in insurance broking and risk management. In more than 130 countries, our experts in every facet of risk and across industries help clients to anticipate, quantify, and more fully understand the range of risks they face.\nMarsh Advisory is the consultative branch of Marsh, provides solutions in the increasing needs of our clients to implement risk management programs within their organization. Marsh Advisory helps companies to change their risk profiles so they can improve resiliency, reduce claims, and minimize the total cost of risk. Businesses today regularly tackle multiple challenges, whether facing property and casualty, cyber, pandemic, ERM / BCP / BCM, climate change, supply chain, reputational, or other risks, Marsh Advisory can help.\nMMGS is a global knowledge center for Marsh McLennan and houses teams, which work closely with the colleagues across various operating units and locations. The Knowledge Services function under MMGS aims to provide specialized services in the domain of Research, Consulting, Data Analytics, Data Science, Actuarial and Design.\nKnowledge Management (KM) is a key service offering provided by Knowledge Services to global Marsh McLennan businesses. The Internship program at Knowledge Services aims to provide students with an early exposure towards key risks faced by businesses and compliment their academic learning with real world scenarios. Combining specialized expertise and advanced analytics, the interns will learn to identify various risks faced by businesses and understand how various strategies help reduce risk exposure, improve profitability and strengthen organizational resilience\nWhat can you expect?\nAn opportunity to work with Senior practice leaders from the Advisory business to understand their unique needs and accordingly conceptualise and deliver value-add solutions for their business.\nExposure to a very dynamic team of subject-matter-experts across various domains.\nAn environment that fosters collaboration with Knowledge Services colleagues across the Research, Data Science, SharePoint / portal management and Design teams to be able to build a scalable, solutions-oriented knowledge platform.\nWhat is in it for you?\nAs a global leader in insurance broking and risk management, we are devoted to finding diverse individuals who are committed to the success of our clients and our organization. You will join a team of talented professionals from across the globe which is dedicated to helping clients manage some of the worlds most challenging and complex risks. We can promise you extraordinary challenges, extraordinary colleagues, and the opportunity to make a difference.\nOur rich history has created a client service culture that we believe is second to none. Our commitments to Diversity and Inclusion , Corporate Social Responsibility , and sustainability demonstrate our commitment to stand for what is right.\nWe will count on you to:\nAssist in the ongoing project plan for the timely roll-out of the Knowledge Management (KM) portal to business teams. This includes:\nSupporting the compilation of case studies, people bios, marketing collaterals, and other relevant content.\nHelping with content management on the platform, including metadata tagging and document structuring.\nAssisting in the sanitization of uploaded documents by anonymizing client names and other confidential information while retaining relevant sections.\nParticipating in quality checks and final inspections before documents are uploaded to the KM platform.\nEngage with team members and stakeholders to understand the KM portals features and articulate its potential value to their work.\nSupport the creation of a plan for the phased roll-out of the portal and assist in tracking project milestones.\nAfter the launch of the portal, help promote its use and identify ways to encourage consistent adoption by business teams. This may include:\nAssisting in the creation of newsletters highlighting new content and developments on the platform.\nHelping to gather and report on utilization statistics for the portal.\nParticipating in engagement activities such as town halls and team meetings to promote the KM portal.\nAssist in the ongoing maintenance of repositories to ensure easy access to current information. This includes collaborating with marketing and regional business teams to update and curate new content.\nGather feedback from project sponsors and team members on areas where expanded support could enhance client interactions.\nCollaborate with team members, including Data Scientists, to explore automation and features that can improve the user experience of the KM portal.\nContribute to the design and documentation of KM processes to support the expansion of the portal and its capabilities.\nWhat you need to have:\nStrong organizational skills and attention to detail to assist in project management and content organization.\nBasic understanding of content management systems and document structuring.\nFamiliarity with Microsoft Office Suite, particularly PowerPoint, Word, and Excel, for creating presentations and managing data.\nGood communication skills to articulate ideas and collaborate effectively with team members and stakeholders.\nA willingness to learn about knowledge management practices and tools.\nWhat makes you stand out?\nUnderstating of insurance and risk management Ability to incorporate basic business understanding while curating content will make you a strong ally for business leaders.\nPrior experience utilizing GenAI tools to introduce operational efficiencies through automation.\nAbility to analyze utilization metrics and translate insight into actionable SEO strategies.\nWorking knowledge of Adobe Photoshop or other image editing software.\nBasic understanding of HTML, XML and CSS",Industry Type: Management Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Supply chain', 'Content management', 'Claims', 'Project management', 'XML', 'Consulting', 'HTML', 'Actuarial', 'SEO', 'Risk management']",2025-06-14 06:07:11
Analyst,Ford,1 - 8 years,Not Disclosed,['Chennai'],"The role demands knowledge on accounting and month end GL close activities.\n\nIndia related advance and retention payment procedure.\n\nClearence of supplier debit balance through Debit Memo / Credit Memo processing.\n\nGood Communication, analytical and problem-solving skills.\n\nPrior Payable operations experience will be preferred.\nBachelor s degree in commerce with good accounting, analytical and communication skills.\nProcessing of Debit Memo / Credit Memo to offset supplier debit balance within the same group of suppliers.\nAnalysis of Debit balance and supplier follow up for clearance.\nPrepare and submit Journal Entry, Sub-ledger before the timeline.\nHandle BCP activity.\nHandle India advance and retention payments.",Industry Type: Auto Components,Department: Finance & Accounting,"Employment Type: Full Time, Permanent","['SUB', 'Accounting', 'Senior Analyst', 'Analytical', 'Commerce', 'debit']",2025-06-14 06:07:13
Spark Developer,Infosys,2 - 3 years,Not Disclosed,['Bengaluru'],"Responsibilities\nA day in the life of an Infoscion\nAs part of the Infosys consulting team, your primary role would be to actively aid the consulting team in different phases of the project including problem definition, effort estimation, diagnosis, solution generation and design and deployment\nYou will explore the alternatives to the recommended solutions based on research that includes literature surveys, information available in public domains, vendor evaluation information, etc. and build POCs\nYou will create requirement specifications from the business needs, define the to-be-processes and detailed functional designs based on requirements.\nYou will support configuring solution requirements on the products; understand if any issues, diagnose the root-cause of such issues, seek clarifications, and then identify and shortlist solution alternatives\nYou will also contribute to unit-level and organizational initiatives with an objective of providing high quality value adding solutions to customers. If you think you fit right in to help our clients navigate their next in their digital transformation journey, this is the place for you!\nAdditional Responsibilities:\nAbility to work with clients to identify business challenges and contribute to client deliverables by refining, analyzing, and structuring relevant data\nAwareness of latest technologies and trends\nLogical thinking and problem solving skills along with an ability to collaborate\nAbility to assess the current processes, identify improvement areas and suggest the technology solutions\nOne or two industry domain knowledge\nTechnical and Professional Requirements:\nPrimary skills:Technology->Big Data - Data Processing->Spark\nPreferred Skills:\nTechnology->Big Data - Data Processing->Spark\nEducational Requirements\nMCA,MSc,Bachelor of Engineering,BBA,BCom\nService Line\nData & Analytics Unit",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Spark Development', 'Spark Developer', 'spark', 'data processing', 'big data']",2025-06-14 06:07:16
HLS Engagement Manager,Fractal Analytics,1 - 6 years,Not Disclosed,['Bengaluru'],"Its fun to work in a company where people truly BELIEVE in what they are doing!\nWere committed to bringing passion and customer focus to the business.\nLocation: Start offshore then work out of Vietnam for 1 year.\n\nCore Skill - Pharma SME, SQL, Powerpoint, Excel, Storyboarding, Client Management, Data Science\n\nLead the engagement overseeing the training, roadmap development, and implementation of AI projects\nCollaborate closely with cross-functional teams, including data scientists, engineers, and business stakeholders to ensure alignment of project goals with business objectives.\nDrive the development and execution of project plans, including timelines, resource allocation, and risk management to ensure successful project delivery within scope, budget, and timeline constraints\nServe as the primary point of contact for the client, fostering strong relationships and managing expectations to ensure a high level of customer satisfaction throughout the engagement.\nMonitor project progress and performance, proactively identifying and addressing any issues or obstacles that may arise, and implementing corrective actions as needed to keep the project on track.",,,,"['Storyboarding', 'data science', 'Customer satisfaction', 'Pharma', 'Resource allocation', 'Risk management', 'Project delivery', 'Powerpoint', 'Client management', 'SQL']",2025-06-14 06:07:18
Snowflake Developer / Lead,Zensar,6 - 11 years,17-30 Lacs P.A.,"['Pune', 'Bengaluru']","Job Title: Snowflake Developer\nExperience: 5+ years\nLocation: Pune and Hyderabad (Hybrid)\n\nJob Type: Full-timeAbout Us:We're seeking an experienced Snowflake Developer to join our team in Pune and Hyderabad. As a Snowflake Developer, you will be responsible for designing, developing, and implementing data warehousing solutions using Snowflake. You will work closely with cross-functional teams to ensure seamless data integration and analytics.\n\nKey Responsibilities:\nDesign, develop, and deploy Snowflake-based data warehousing solutions\nCollaborate with stakeholders to understand data requirements and develop data models\nOptimize Snowflake performance, scalability, and security\nDevelop and maintain Snowflake SQL scripts, stored procedures, and user-defined functions\nTroubleshoot data integration and analytics issues\nEnsure data quality, integrity, and compliance with organizational standards\nWork with data engineers, analysts, and scientists to ensure seamless data integration and analytics\nStay up-to-date with Snowflake features and best practices\n\nRequirements:\n5+ years of experience in Snowflake development and administration\nStrong expertise in Snowflake architecture, data modeling, and SQL\nExperience with data integration tools (e.g., Informatica, Talend, Informatica PowerCenter)\nProficiency in Snowflake security features and access control\nStrong analytical and problem-solving skills\nExcellent communication and collaboration skills\nExperience working in hybrid or remote teams\nBachelor's degree in Computer Science, Engineering, or related field\n\nNice to Have:\nExperience with cloud platforms (AWS, Azure, GCP)\nKnowledge of data governance and data quality frameworks\nExperience with ETL/ELT tools (e.g., Informatica PowerCenter, Talend, Microsoft SSIS)\nFamiliarity with data visualization tools (e.g., Tableau, Power BI)\nExperience working with agile methodologies",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Snowsql', 'Snowflake', 'Snowpipe']",2025-06-14 06:07:20
Site Reliability Engineer Lead,Optum,9 - 14 years,30-37.5 Lacs P.A.,['Hyderabad'],"Primary Responsibilities:\nDesign, implement, and maintain scalable, reliable, and secure infrastructure on AWS and EKS\nDevelop and manage observability and monitoring solutions using Datadog, Splunk, and Kibana\nCollaborate with development teams to ensure high availability and performance of microservices-based applications\nAutomate infrastructure provisioning, deployment, and monitoring using Infrastructure as Code (IaC) and CI/CD pipelines",,,,"['AWS services', 'EKS', 'Splunk', 'microservices', 'Python', 'Terraform', 'Ansible', 'Bash', 'Datadog']",2025-06-14 06:07:22
"Associate Staff Engineer, Machine Learning",Nagarro,5 - 7 years,Not Disclosed,['India'],"We're Nagarro.\n\nWe are a Digital Product Engineering company that is scaling in a big way! We build products, services, and experiences that inspire, excite, and delight. We work at scale across all devices and digital mediums, and our people exist everywhere in the world (18000+ experts across 38 countries, to be exact). Our work culture is dynamic and non-hierarchical. We are looking for great new colleagues. That's where you come in!\n\nREQUIREMENTS:\nExperience: 5+ Years\nStrong working experience in machine learning, with a proven track record of delivering impactful solutions in NLP, machine vision, and AI.\nProficiency in programming languages such as Python or R, and experience with data manipulation libraries (e.g., Pandas, NumPy).\nStrong understanding of statistical concepts and techniques, and experience applying them to real-world problems.\nStrong programming skills in Python, and proficiency in deep learning frameworks such as TensorFlow, PyTorch, or JAX, as well as machine learning libraries such as scikit-learn.\nPractical experience with Generative AI frameworks such as GANs, VAEs, prompt engineering, and retrieval-augmented generation (RAG), and the ability to apply them to real-world problems.\nExcellent problem-solving skills, with a creative and analytical mindset.\nStrong communication and teamwork skills, with the ability to work effectively in a team environment and interact with stakeholders at all levels.\nExperience with AI ethics and responsible AI practices.\n\nRESPONSIBILITIES:\nUnderstanding the clients business use cases and technical requirements and be able to convert them into technical design which elegantly meets the requirements.\nMapping decisions with requirements and be able to translate the same to developers.\nIdentifying different solutions and being able to narrow down the best option that meets the clients requirements.\nDefining guidelines and benchmarks for NFR considerations during project implementation\nWriting and reviewing design document explaining overall architecture, framework, and high-level design of the application for the developers\nReviewing architecture and design on various aspects like extensibility, scalability, security, design patterns, user experience, NFRs, etc., and ensure that all relevant best practices are followed.\nDeveloping and designing the overall solution for defined functional and non-functional requirements; and defining technologies, patterns, and frameworks to materialize it\nUnderstanding and relating technology integration scenarios and applying these learnings in projects\nResolving issues that are raised during code/review, through exhaustive systematic analysis of the root cause, and being able to justify the decision taken.\nCarrying out POCs to make sure that suggested design/technologies meet the requirements.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Tensorflow', 'Machine Learning', 'Python', 'Pytorch', 'R']",2025-06-14 06:07:25
Senior Business Analytics Analyst,"Godaddy Operating Company, Llc",5 - 10 years,Not Disclosed,"['Kolkata', 'Mumbai', 'New Delhi', 'Hyderabad', 'Pune', 'Chennai', 'Bengaluru']","At GoDaddy the future of work looks different for each team. Some teams work in the office full-time, others have a hybrid arrangement (they work remotely some days and in the office some days) and some work entirely remotely.\nThis is a remote position, so you ll be working remotely from your home. You may occasionally visit a GoDaddy office to meet with your team for events or meetings.\nJoin Our Team\nAs a data-driven company, GoDaddy is looking for a quick learner and result oriented Senior Analytics Engineer to join our Strategic Finance team. Strategic Finance is a part of GoDaddy s Finance Data, Analytics, and Technology team, and our overall mission is to optimize the power of data insights & automation solutions by enabling capabilities around technology, data, and people for improved efficiency & scalability. In Strategic Finance, we leverage business intelligence and financial models to derive data-driven insights that drive top-line, strategic growth. As part of this team, you will enable efficiency and velocity of insight discovery through data product development- velocity is key but be a good citizen!",,,,"['Automation', 'tableau', 'Business analytics', 'data governance', 'Data structures', 'Teradata', 'Business intelligence', 'Analytics', 'SQL', 'Python']",2025-06-14 06:07:27
Business Analyst,Mindteck,5 - 8 years,Not Disclosed,['Bengaluru'],"Key Responsibilities:\n\n\n\nCollaborate with Product Owners, Scrum Masters, and Development teams to gather and analyze requirements.\n\nTranslate high-level business needs and stakeholder inputs into detailed user stories and acceptance criteria.\n\nParticipate actively in sprint planning, backlog grooming, reviews, retrospectives.\n\nMaintain and groom the product backlog in collaboration with the Product Owner.\n\nAssist in risk assessments, impact analysis, and traceability matrix creation.\n\nCollaborate with UI/UX, test engineers, and developers to ensure user-centric design and verification traceability.\n\n\n\nRequired Skills Experience:\n\n\n\n5-8 years of experience as a Business Analyst, with at least 2-3 years in medical device or healthcare IT.\n\nStrong understanding of Agile frameworks (Scrum, SAFe).\n\nExperience working with tools Azure DevOps.\n\nProven ability to work with cross-functional teams in a regulated software development lifecycle.\n\nStrong understanding of software requirement specifications, use cases and traceability.\n\nExcellent verbal and written communication skills.\n\n\n\nPreferred Qualifications:\n\n\n\nBachelor s or master s degree in computer science, or related field.\n\n3+ years of experience as a Business Analyst in Agile software development, preferably in the medical device or healthcare domain\n\n\n\nSoft Skills:\n\n\n\nDetail-oriented with strong analytical and problem-solving skills.\n\nAbility to negotiate priorities and facilitate consensus across stakeholder groups.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Business Analyst', 'Analytical', 'devops', 'Agile', 'Software development life cycle', 'Healthcare', 'Scrum', 'Testing']",2025-06-14 06:07:29
Senior Lead Engineer - Artificial Intelligence,Konverge.AI,4 - 6 years,Not Disclosed,['Nagpur'],"Key Responsibilities:\n\nDemonstrates proficiency in at least one of the four areas: Computer Vision, Natural Language Processing, Machine Learning, and Forecasting. Expected to deepen this expertise further.\nTranslates data-related client requirements into project tasks and deliverables, with an understanding of complex problems and solutions.\nDrives data solutions within projects, with a focus on innovation and value creation.\nProvides technical guidance and support to team members on data science methodologies, tools, and algorithms.\nActs as a point of escalation for data science-related issues within the project, working towards efficient resolution.\nReviews and ensures adherence to performance parameters for data science tasks, with a focus on data quality and model performance.\nBalances data science workload within the team, considering the skills and capacities of team members.\nChampions learning initiatives for upskilling and cross-skilling in data science within the team, including knowledge sharing sessions and promoting best practices.\nContributes to the development of new data science solution offerings, leveraging insights from project work.\nEncourages team members to build technical expertise and innovate within the realm of data science.\nBegins to build and nurture client relationships with a focus on identifying opportunities to add value through data science.\nStarts to develop proficiency in at least one of data engineering, MLOps, product engineering, and cloud services, and applies this knowledge within projects.\nBegins to contribute to the organizations thought leadership in data science, such as writing articles, giving presentations, or contributing to internal forums.\nActively identifies and communicates opportunities for the organization to enhance its data science capabilities, including new tools, methodologies, or service offerings.\nKeeps an eye on industry trends in data science to ensure personal growth and the competitive advantage of the organization",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer vision', 'Article writing', 'Product engineering', 'data science', 'Artificial Intelligence', 'Machine learning', 'Data quality', 'Natural language processing', 'Forecasting']",2025-06-14 06:07:32
IN_-Senior Associate_Palantir Foundary_Data and Analytics,PwC Service Delivery Center,3 - 8 years,Not Disclosed,['Gurugram'],"Not Applicable\nSpecialism\nData, Analytics & AI\nManagement Level\nSenior Associate\n& Summary\n.\n\nIn data analysis at PwC, you will focus on utilising advanced analytical techniques to extract insights from large datasets and drive datadriven decisionmaking. You will leverage skills in data manipulation, visualisation, and statistical modelling to support clients in solving complex business problems.\nAt PwC, you will be part of a vibrant community of solvers that leads with trust and creates distinctive outcomes for our clients and communities. This purposeled and valuesdriven work, powered by technology in an environment that drives innovation, will enable you to make a tangible impact in the real world. We reward your contributions, support your wellbeing, and offer inclusive benefits, flexibility programmes and mentorship that will help you thrive in work and life. Together, we grow, learn, care, collaborate, and create a future of infinite experiences for each other. Learn more about us .\n& Summary A career within Data and Analytics services will provide you with the opportunity to help organisations uncover enterprise insights and drive business results using smarter data analytics. We focus on a collection of organisational technology capabilities, including business intelligence, data management, and data assurance that help our clients drive innovation, growth, and change within their organisations in order to keep up with the changing nature of customers and technology. We make impactful decisions by mixing mind and machine to leverage data, understand and navigate risk, and help our clients gain a competitive edge.\nResponsibilities\nJD Palantir Foundry Clientfrontline Solution Architect 3+ years of experience in implementing analytical solutions using Palantir Foundry. preferably in PySpark and hyperscaler platforms (cloud services like AWS, GCP and Azure) with focus on building data transformation pipelines at scale. Team management Must have experience in mentoring and managing large teams (20 to 30 people) for complex engineering programs. Candidate should have experience in hiring and nurturing talent in Palantir Foundry. Training candidate should have experience in creating training programs in Foundry and delivering the same in a handson format either offline or virtually. At least 3 years of handson experience of building and managing Ontologies on Palantir Foundry. At least 3 years of experience with Foundry services Data Engineering with Contour and Fusion Dashboarding, and report development using Quiver (or Reports) Application development using Workshop. Exposure to Map and Vertex is a plus Palantir AIP experience will be a plus A Palantir Foundry Certification (Solution Architect, Data Engineer) is a plus. Certificate should be valid at the time of Interview.\nMandatory skill sets\nMust have knowledge, skills and experiences Handson experience in data engineering and building data pipelines (Code/No Code) for ELT/ETL data migration, data refinement and data quality checks on Palantir Foundry. Handson experience of managing data life cycle on at least one hyperscaler platform (AWS, GCP, Azure) using managed services or containerized deployments for data pipelines is necessary. Handson experience in working & building on Ontology (esp. demonstrable experience in building Semantic relationships). Proficiency in SQL, Python and PySpark. Demonstrable ability to write & optimize SQL and spark jobs. Some experience in Apache Kafka and Airflow is a prerequisite as well. Handson experience on DevOps on hyperscaler platforms and Palantir Foundry is necessary. Experience in MLOps is a plus. Experience in developing and managing scalable architecture & working experience in managing large data sets.\nOpensource contributions (or own repositories highlighting work) on GitHub or Kaggle is a plus. Experience with Graph data and graph analysis libraries (like Spark GraphX, Python NetworkX etc.) is a plus.\nPreferred skill sets\nGood to have knowledge, skills and experiences Experience in developing GenAI application is a plus\nYears of experience required\n3 to 8\nEducation qualification\no BE, B.Tech, ME, M,Tech, MBA, MCA (60% above) o Minimum Education Bachelors degree in computer science, data science or any other Engineering discipline. Master s degree is a plus.\nEducation\nDegrees/Field of Study required Master of Business Administration, Bachelor of Engineering, Bachelor of Technology\nDegrees/Field of Study preferred\nRequired Skills\nExtract Transform Load (ETL)\nGenerative AI\nNo",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Data analysis', 'Data migration', 'Team management', 'Data management', 'Application development', 'Apache', 'Business intelligence', 'SQL', 'Python']",2025-06-14 06:07:34
S&C Global Network - AI - CDP - Marketing Analytics - Manager,Accenture,3 - 5 years,Not Disclosed,['Pune'],"Job Title -\n\nS&C Global Network - AI - CDP - Marketing Analytics - Analyst\n\nManagement Level:\n\n11-Analyst\n\nLocation:\n\nBengaluru, BDC7C\n\nMust-have skills:Data Analytics\n\n\nGood to have skills:Ability to leverage design thinking, business process optimization, and stakeholder management skills.\n\nJob\n\nSummary:\n\nThis role involves driving strategic initiatives, managing business transformations, and leveraging industry expertise to create value-driven solutions.\n\n\nRoles & Responsibilities:\n\nProvide strategic advisory services, conduct market research, and develop data-driven recommendations to enhance business performance.\n\nWHATS IN IT FOR YOU\n\nAs part of our Analytics practice, you will join a worldwide network of over 20k+ smart and driven colleagues experienced in leading AI/ML/Statistical tools, methods and applications. From data to analytics and insights to actions, our forward-thinking consultants provide analytically-informed, issue-based insights at scale to help our clients improve outcomes and achieve high performance.\n\nWhat you would do in this role\n\nA Consultant/Manager for Customer Data Platforms serves as the day-to-day marketing technology point of contact and helps our clients get value out of their investment into a Customer Data Platform (CDP) by developing a strategic roadmap focused on personalized activation. You will be working with a multidisciplinary team of Solution Architects, Data Engineers, Data Scientists, and Digital Marketers.\n\nKey Duties and Responsibilities:\nBe a platform expert in one or more leading CDP solutions. Developer level expertise on Lytics, Segment, Adobe Experience Platform, Amperity, Tealium, Treasure Data etc. Including custom build CDPs\nDeep developer level expertise for real time even tracking for web analytics e.g., Google Tag Manager, Adobe Launch etc.\nProvide deep domain expertise in our clients business and broad knowledge of digital marketing together with a Marketing Strategist industry\nDeep expert level knowledge of GA360/GA4, Adobe Analytics, Google Ads, DV360, Campaign Manager, Facebook Ads Manager, The Trading desk etc.\nAssess and audit the current state of a clients marketing technology stack (MarTech) including data infrastructure, ad platforms and data security policies together with a solutions architect.\nConduct stakeholder interviews and gather business requirements\nTranslate business requirements into BRDs, CDP customer analytics use cases, structure technical solution\nPrioritize CDP use cases together with the client.\nCreate a strategic CDP roadmap focused on data driven marketing activation.\nWork with the Solution Architect to strategize, architect, and document a scalable CDP implementation, tailored to the clients needs.\nProvide hands-on support and platform training for our clients.\nData processing, data engineer and data schema/models expertise for CDPs to work on data models, unification logic etc.\nWork with Business Analysts, Data Architects, Technical Architects, DBAs to achieve project objectives - delivery dates, quality objectives etc.\nBusiness intelligence expertise for insights, actionable recommendations.\nProject management expertise for sprint planning\n\nProfessional & Technical\n\nSkills:\n- Relevant experience in the required domain.- Strong analytical, problem-solving, and communication skills.- Ability to work in a fast-paced, dynamic environment.\nStrong understanding of data governance and compliance (i.e. PII, PHI, GDPR, CCPA)\nExperience with analytics tools like Google Analytics or Adobe Analytics is a plus.\nExperience with A/B testing tools is a plus.\nMust have programming experience in PySpark, Python, Shell Scripts.\nRDBMS, TSQL, NoSQL experience is must.\nManage large volumes of structured and unstructured data, extract & clean data to make it amenable for analysis.\nExperience in deployment and operationalizing the code is an added advantage.\nExperience with source control systems such as Git, Bitbucket, and Jenkins build and continuous integration tools.\nProficient in Excel, MS word, PowerPoint, etc\nTechnical\n\nSkills:\nAny CDP platforms experience e.g., Lytics CDP platform developer, or/and\nSegment CDP platform developer, or/and\nAdobe Experience Platform (Real time CDP) developer, or/and\nCustom CDP developer on any cloud\nGA4/GA360, or/and Adobe Analytics\nGoogle Tag Manager, and/or Adobe Launch, and/or any Tag Manager Tool\nGoogle Ads, DV360, Campaign Manager, Facebook Ads Manager, The Trading desk etc.\nDeep Cloud experiecne (GCP, AWS, Azure)\nAdvance level Python, SQL, Shell Scripting experience\nData Migration, DevOps, MLOps, Terraform Script programmer\nSoft\n\nSkills:\nStrong problem solving skills\nGood team player\nAttention to details\nGood communication skills\n\nAdditional Information:\n\n- Opportunity to work on innovative projects.\n\n- Career growth and leadership exposure.\n\nQualification\n\nExperience:\n\n3-5Years\n\n\nEducational Qualification:\n\nAny Degree",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['digital marketing', 'python', 'data analytics', 'pyspark', 'campaign management', 'trading', 'adobe analytics', 'rdbms', 'adobe', 'display video', 't-sql', 'google ads', 'nosql', 'sql', 'devops', 'data governance', 'jenkins', 'facebook ads manager', 'segmentation', 'shell scripting', 'aws']",2025-06-14 06:07:37
"Senior Specialist Product Owner, Actimize",NICE,12 - 15 years,Not Disclosed,['Pune'],"So, what’s the role all about?\nAt Actimize, we stop bad actors from doing bad things. We are a leading Financial Crime & Compliance (FCC) organization dedicated to protecting institutions and their customers from evolving financial crime threats. We’re seeking a Specialist Product Owner to drive innovation and lead the development of cutting-edge solutions that address compliance challenges, improve risk management, and enable seamless operations for our clients.\nThis role offers the opportunity to shape products that combat financial crime while ensuring adherence to complex regulatory requirements. The ideal candidate will combine domain expertise, strategic thinking, and technical acumen to deliver impactful solutions.",,,,"['risk management', 'requirements', 'data analysis', 'confluence', 'analytical', 'verbal communication', 'analysis', 'financial services', 'nice', 'product management', 'data modeling', 'written communication', 'compliance', 'flex', 'regulations', 'product vision', 'metrics', 'agile', 'api', 'communication skills', 'jira']",2025-06-14 06:07:39
S&C Global Network - AI - CDP - Marketing Analytics - Manager,Accenture,5 - 10 years,Not Disclosed,['Bengaluru'],"Job Title -\n\nS&C Global Network - AI - CDP - Marketing Analytics - Manager\n\nManagement Level:\n\n7-Manager\n\nLocation:\n\nBengaluru, BDC7C\n\nMust-have skills:Web Analytics Tools\n\n\nGood to have skills:Ability to leverage design thinking, business process optimization, and stakeholder management skills.\n\nJob\n\nSummary:\n\nThis role involves driving strategic initiatives, managing business transformations, and leveraging industry expertise to create value-driven solutions.\n\n\nRoles & Responsibilities:\n\nProvide strategic advisory services, conduct market research, and develop data-driven recommendations to enhance business performance.\n\n\nProfessional & Technical\n\nSkills:\n- Relevant experience in the required domain.- Strong analytical, problem-solving, and communication skills.- Ability to work in a fast-paced, dynamic environment.Technical Skills:\nMust have:\nGA4/GA360, or/and Adobe Analytics/Customer Journey Analytics\nGoogle Tag Manager, and/or Adobe Launch/AEP Web SDK, and/or Tealium iQ and/or any Tag Manager Tool\nTableau, and/or Power BI, and/or Looker, and/or any Visualization tools\nAdvance level SQL\nAdvance level JavaScript and jQuery\nBasic HTML\nExperience on debuggers like Adobe Launch debugger, Omnibug, Charles, Browser stack, etc.\nGood to have:\nAEP-RTCDP and/or Tealium and/or Segment and/or Lytics and/or any Customer Data Platform (CDPs)\nAdobe Target and/or Optimizely or any personalization or A/B testing tool\nAny Marketing destinations like Google Ads, DV360, Adobe Audience Manager, etc.\nSoft\n\nSkills:\nStrong problem-solving skills\nGood team player\nAttention to details\nEffective communication skills\n\nAdditional Information:\n\n- Opportunity to work on innovative projects.\n\n- Career growth and leadership exposure.\n\nWHATS IN IT FOR YOU\n\nAs part of our Analytics practice, you will join a worldwide network of over 20k+ smart and driven colleagues experienced in leading AI/ML/Statistical tools, methods, and applications for digital marketing, web/mobile analytics, customer data platform, personalization and now GEN AI. From data to analytics and insights to actions, our forward-thinking consultants provide analytically informed, issue-based insights at scale to help our clients improve outcomes and achieve high performance.\n\nWhat you would do in this role\n\nAn Analyst/Consultant/Manager for Web Analytics serves as the day-to-day marketing technology point of contact and helps our clients get measure value out of their investment into their digital assets by developing a strategic roadmap focused on personalized activation. You will be working with a multidisciplinary team of Solution Architects, Data Engineers, Data Scientists, and Digital Marketers.\n\nKey Duties and Responsibilities:\nBe a platform expert in one or more leading Tag Management platforms. Deep developer level expertise on Adobe Launch/AEP Web SDK, Google Tag Manager, Tealium iQ.\nDeveloper level expertise for Visualization tools e.g. Tableau, Power BI, Looker, etc. Dashboard development using any of these tools.\nProvide deep domain expertise in our clients business and broad knowledge of digital marketing together with a Marketing Strategist industry.\nDeep expert level knowledge of GA360/GA4, Adobe Analytics/Customer Journey Analytics. End-to end configuration of these tools for digital assets.\nAssess and audit the current state of a clients marketing technology stack (MarTech) together with a solutions architect.\nPerform technical analysis & design for analytics-related initiatives. Define requirements for web or app digital analytics implementation/reporting dashboard.\nConduct stakeholder interviews and gather business requirements.\nTranslate business requirements into BRDs, technical specifications, tagging guide, data layer design documents, SDR.\nCoordinate with website developers for Analytics integration, datalayer implementation.\nImplement tags for websites/apps using tag managers inclusive of any custom implementations, custom JavaScript, web scraping, tag manager configuration from start to end.\nDiagnose and troubleshoot analytics implementation and platform configuration issues. Perform end-to-end QA testing forintegrations.\nManage ad pixels, third-party tags for digital campaigns.\nManage large volumes of structured and unstructured data, extract & clean data to make it amenable for analysis.\nRoot cause analysis, creating actionable insights using Web Analytics Reporting tools or Visualization tools\nWork with/as the Solution Architect to strategize, architect, and document a scalable Web Analytics implementation (websites/apps), tailored to the clients needs. Able to integrate multiple Martech tools.\nEnsure analytics are consistently implemented across digital properties.\nProvide hands-on support and platform training for our clients.\nWork with Business Analysts, Data Architects, Technical Architects, DBAs to achieve project objectives - delivery dates, quality objectives etc.\nProject management expertise for sprint planning.\nWho we are looking for\nB Tech/M Tech from reputed engineering colleges\nMasters/M Tech in Computer Science\nAt least 4+ years of relevant work experience of reporting and tag management across web/mobile analytics platforms inclusive of aspects like technical analysis, design and development\nAt least 4+ years of relevant work experience in marketing, consulting, or analytics\nAt least 5+ years of experience working in an agency environment\nSubject matter expert in any one multi-channel marketing hubs by Adobe Experience Platform, Google, Tealium.\nAt least 1 years of dashboard development experience with visualization tools like Tableau, Power BI, Looker, etc.\nStrong understanding of implementation of consent management platforms e.g. Sourcepoint, OneTrust on digital assets\nMust have developer experience with tag management platforms like Adobe Launch/AEP Web SDK, Google Tag Manager, Tealium iQ. Certification in any one is must\nMust have developer and reporting experience with web analytics tools like Google Analytics or Adobe Analytics/Customer Journey Analytics inclusive of building custom reports. Certification in Adobe Analytics/Google Analytics is a must\nExperience in requirement gathering, solution design, tag implementation and audits, Martech tools integration\nExperience in defining measurement frameworks for websites/app, suggesting KPIs to track, driving strategy for customer engagement\nBusiness intelligence expertise for insights, actionable recommendations.\nStrong hands-on with advanced JavaScript and jQuery. Able to develop custom tags/scripts using tag managers\nBasic understanding of HTML is a must.\nExperience in Postman APIs is a plus.\nStrong experience in ETL, data aggregation and transformation using SQL is a must\nExperience debugging with browser developer tools like Adobe Launch debugger, Omnibug, Charles, Browser stack, etc.\nExperience in mobile analytics implementations is a nice to have\nExperience with A/B testing tools is a plus. Good to have knowledge on personalization tools like Adobe Target, Optimizely\nGood to have knowledge on CDPs like AEP-RTCDP, Tealium, Segment, Lytics\nGood to have exposure to Gen AI\nGood to have knowledge of any marketing destinations like Google Ads, DV360, Campaign Manager, Facebook Ads Manager, The Trading desk etc.\nProficient in Excel, MS word, PowerPoint, etc.\nQualification\n\nExperience:\n\n12-14Years\n\n\nEducational Qualification:\n\nAny Degree",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Root cause analysis', 'technical analysis', 'Marketing Analytics', 'Gen AI', 'Power BI', 'Dashboard development', 'HTML', 'Tableau', 'Looker', 'Postman APIs']",2025-06-14 06:07:41
"Manager, Finance",Walmart,6 - 10 years,Not Disclosed,['Bengaluru'],"Position Summary...\nWhat youll do...\nSkillset: FP&A\nLocation: Bangalore\nExperience: 6 to 10 years\nTeam and Position Summary:\nThis position is part of the Global Tech FP&A Team which is based out of Walmart Global Tech Bengaluru office. This team primarily provides operational support to Walmart s technology division and drive continuous improvement/enhancements for budgeting and planning processes\nFinance professional having 6 - 10 years of experience in FP&A processes (various planning models including driver-based planning) with robust understanding of FP&A tools and technology who can support transformations & ongoing improvements/ operations of different Walmart segments FP&A processes\nWhat you will do:\nAct as a liaison between business stakeholders and partner teams (product, engineering) to manage & continuously improve product processes and enabling technologies\nAbility to work in a matrix environment and collaborate across business and fin-Tech teams\nSound understanding of forecasting and budgeting (AOP) activities with experience of end to end cycle (Variance analysis, flash reports, etc)\nHigh proficiency in excel based modelling, developing & maintaining clean, scalable, and flexible models\nStrong experience in Excel modelling required\nCollaborate with Fin-tech & business teams in product UAT and sign off\nAbility to identify execution risks and roadblocks in timely manner thereby ensuring noiseless delivery and efficient stakeholder management\nProven ability to accurately document existing processes, change requests and business requirements and maintaining functional design models\nSupport in maintaining training documentation to capture FP&A models and system\nExperience of tech implementation/enhancements along with hyper-care support\nWhat you will bring:\nCA / CPA / MBA or equivalent qualification\nMinimum 6-10 years of post-qualification experience in FP&A processes\nExperience of working with Fin-Tech and product teams in matrix environment\nUnderstanding of excel based financial data modelling\nAdditional Qualifications:\nKnowledge of PowerBI, Tableau will prove to be an asset\nExposure to OneStream/ HFM application will be added advantage\nExperience in finance/FP&A transformation role in an agile methodology\nAbout Walmart Global Tech\n\nImagine working in an environment where one line of code can make life easier for hundreds of millions of people. That s what we do at Walmart Global Tech. We re a team of software engineers, data scientists, cybersecurity experts and service professionals within the world s leading retailer who make an epic impact and are at the forefront of the next retail disruption. People are why we innovate, and people power our innovations. We are people-led and tech-empowered.\n.\nFlexible, hybrid work\n\n.\nBenefits\n.\nEqual Opportunity Employer:\nMinimum Qualifications...\nMinimum Qualifications:Bachelors degree in Finance, Accounting, or related field and 2 years experience in accounting, finance, or relevant area OR 4 years experience in accounting, finance, or related area.\nPreferred Qualifications...\nData Analysis and Insights, Microsoft Office, Supervising Associates\nPrimary Location...",Industry Type: Retail,Department: Finance & Accounting,"Employment Type: Full Time, Permanent","['Data analysis', 'operational support', 'Product engineering', 'Excel', 'Networking', 'SEZ', 'Agile methodology', 'Budgeting', 'Forecasting', 'Variance analysis']",2025-06-14 06:07:44
Lead Software Quality Engineer,Mastercard,5 - 10 years,Not Disclosed,['Pune'],"Lead Software Quality Engineer\nJob Description\nIn addition to our traditional payments portfolio, Mastercard is aggressively expanding into new payment flow and leveraging the digital and connected world of people and economies to deliver faster, reliable and more efficient propositions to make payments - send or receive money.\nThe Cross Border program is part of the wider Disbursement & Remittances business that we are looking to expand at a compounded rate of 50% per year for the next 5 years. We are building solutions to enable payments in an any-to-any format - anyone, anywhere can send or receive money using any channel - money exchanges, banks, digital wallets, cards or fintech apps - providing a seamless and unified experience for both individuals and business to move money.\n\n?\nWe are the global technology company behind the world s fastest payments processing network. We are a vehicle for commerce, a connection to financial systems for the previously excluded, a technology innovation lab, and the home of Priceless . We ensure every employee has the opportunity to be a part of something bigger and to change lives. We believe as our company grows, so should you. We believe in connecting everyone to endless, priceless possibilities.\n\nLead Software Quality Engineer\nAs a Lead Software Quality Engineer, you will have the chance to tap into your expertise and knowledge to champion quality by crafting and deploying testing at scale. This role requires a deep understanding of software quality principles, quality assurance methods, and the ability to collaborate with cross-functional teams to identify and resolve issues. Someone in this role is expected to implement proactive software quality strategies which are both scalable and maintainable.\nResponsibilities:\nParticipate in requirements discussion, test planning, test data creation and execution of testing Plan in adherence with MasterCard standards, processes and best practices.\nWork with project teams to meet scheduled due dates, while identifying emerging issues and recommending solutions for problems and independently perform assigned tasks.\nDesign and develop test automation frameworks to validate system to system interfaces and complete software solutions (for Database/ETL, API and UI tests)\nInteract with business and development stakeholders to define test plans and schedules\nTranslate complex system requirements into test requirements and testing methods\nIdentify and implement complex automation efforts, including refactoring of automation code where needed\nDevelop test scripts and perform automated and manual exploratory testing to ensure software meets business and security requirements and established practices.\nDesign and develop test data management for defined test cases, recognize test environment preparation needs, and execute existing test plans and report results\nOwn responsibility for defect management and oversight and escalation of issues discovered during the testing phase\nDocument as per Software Development Best Practices and follow MasterCard Quality Assurance and Quality Control processes.\nDocument performance test strategies and test plans, and execute performance validation\nCollect quality metric data and communicate test status/risks to stakeholders\nAct as first-review for project-level reviews, walkthroughs and inspections\nProvide technical support and mentoring to junior team members\nPerform demos of new product functionality to stakeholders\nDevelop business and product knowledge over time.\nIdentify opportunities to improve effectiveness and time-to-market\nProvide training and guidance to team members on quality best practices and principles\nFacilitate knowledge sharing sessions to promote a culture of quality awareness\nBe a strong individual contributor to the implementation efforts of product solutions\n\nAll About You:\nBachelors degree in Information Technology, Computer Science or Management Information Systems or equivalent work experience\n8+ years of experience in the Software Engineering with a focus on Quality Engineering methodologies\nTechnical skills in Java, Selenium, Cucumber, Soap UI, Spring framework, REST, JSON, Eclipse, GIT, Jmeter/Blazemeter\nExcellent SQL skills to work on large and complex data sources and capability of comprehending and writing complex queries\nExperience testing APIs (REST and SOAP), web user interface, and/or reports\nExperience in implementing CI/CD build pipelines with tools like Git/Bit Bucket, Jenkins and Maven\nSuccessfully validated one or more application codebases via automation, for new feature functionality and regression testing\nExperience working in Agile teams and conversant with Agile/SAFe tenets and ceremonies. Strong analytical and problem-solving abilities, with quick adaptation to new technologies, methodologies, and systems\nExcellent English communication skills (both written and verbal) to effectively interact with multiple technical teams and other stakeholders\nHigh-energy, detail-oriented and proactive, with ability to function under pressure in an independent environment along with a high degree of initiative and self-motivation to drive results\nEager to experiment with new team processes and innovate on testing approach\nPrior experience with Data Analysis and Data Engineering is a plus\nStrong collaboration skills and ability to work effectively in a cross-functional, interdependent team environment",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Maven', 'Manager Quality Assurance', 'Eclipse', 'Information security', 'Agile', 'JSON', 'Selenium', 'Information technology', 'Technical support', 'SQL']",2025-06-14 06:07:47
Lead Software Quality Engineer,Dynamic Yield,5 - 10 years,Not Disclosed,['Pune'],"Our Purpose\nMastercard powers economies and empowers people in 200+ countries and territories worldwide. Together with our customers, we re helping build a sustainable economy where everyone can prosper. We support a wide range of digital payments choices, making transactions secure, simple, smart and accessible. Our technology and innovation, partnerships and networks combine to deliver a unique set of products and services that help people, businesses and governments realize their greatest potential.\nTitle and Summary\nLead Software Quality Engineer\nJob Description\nIn addition to our traditional payments portfolio, Mastercard is aggressively expanding into new payment flow and leveraging the digital and connected world of people and economies to deliver faster, reliable and more efficient propositions to make payments - send or receive money.\nThe Cross Border program is part of the wider Disbursement Remittances business that we are looking to expand at a compounded rate of 50% per year for the next 5 years. We are building solutions to enable payments in an any-to-any format - anyone, anywhere can send or receive money using any channel - money exchanges, banks, digital wallets, cards or fintech apps - providing a seamless and unified experience for both individuals and business to move money.\n\nWho is Mastercard\nWe are the global technology company behind the world s fastest payments processing network. We are a vehicle for commerce, a connection to financial systems for the previously excluded, a technology innovation lab, and the home of Priceless . We ensure every employee has the opportunity to be a part of something bigger and to change lives. We believe as our company grows, so should you. We believe in connecting everyone to endless, priceless possibilities.\n\nLead Software Quality Engineer\nAs a Lead Software Quality Engineer, you will have the chance to tap into your expertise and knowledge to champion quality by crafting and deploying testing at scale. This role requires a deep understanding of software quality principles, quality assurance methods, and the ability to collaborate with cross-functional teams to identify and resolve issues. Someone in this role is expected to implement proactive software quality strategies which are both scalable and maintainable.\nResponsibilities:\nParticipate in requirements discussion, test planning, test data creation and execution of testing Plan in adherence with MasterCard standards, processes and best practices.\nWork with project teams to meet scheduled due dates, while identifying emerging issues and recommending solutions for problems and independently perform assigned tasks.\nDesign and develop test automation frameworks to validate system to system interfaces and complete software solutions (for Database/ETL, API and UI tests)\nInteract with business and development stakeholders to define test plans and schedules\nTranslate complex system requirements into test requirements and testing methods\nIdentify and implement complex automation efforts, including refactoring of automation code where needed\nDevelop test scripts and perform automated and manual exploratory testing to ensure software meets business and security requirements and established practices.\nDesign and develop test data management for defined test cases, recognize test environment preparation needs, and execute existing test plans and report results\nOwn responsibility for defect management and oversight and escalation of issues discovered during the testing phase\nDocument as per Software Development Best Practices and follow MasterCard Quality Assurance and Quality Control processes.\nDocument performance test strategies and test plans, and execute performance validation\nCollect quality metric data and communicate test status/risks to stakeholders\nAct as first-review for project-level reviews, walkthroughs and inspections\nProvide technical support and mentoring to junior team members\nPerform demos of new product functionality to stakeholders\nDevelop business and product knowledge over time.\nIdentify opportunities to improve effectiveness and time-to-market\nProvide training and guidance to team members on quality best practices and principles\nFacilitate knowledge sharing sessions to promote a culture of quality awareness\nBe a strong individual contributor to the implementation efforts of product solutions\n\nAll About You:\nBachelors degree in Information Technology, Computer Science or Management Information Systems or equivalent work experience\n8+ years of experience in the Software Engineering with a focus on Quality Engineering methodologies\nTechnical skills in Java, Selenium, Cucumber, Soap UI, Spring framework, REST, JSON, Eclipse, GIT, Jmeter/Blazemeter\nExcellent SQL skills to work on large and complex data sources and capability of comprehending and writing complex queries\nExperience testing APIs (REST and SOAP), web user interface, and/or reports\nExperience in implementing CI/CD build pipelines with tools like Git/Bit Bucket, Jenkins and Maven\nSuccessfully validated one or more application codebases via automation, for new feature functionality and regression testing\nExperience working in Agile teams and conversant with Agile/SAFe tenets and ceremonies. Strong analytical and problem-solving abilities, with quick adaptation to new technologies, methodologies, and systems\nExcellent English communication skills (both written and verbal) to effectively interact with multiple technical teams and other stakeholders\nHigh-energy, detail-oriented and proactive, with ability to function under pressure in an independent environment along with a high degree of initiative and self-motivation to drive results\nEager to experiment with new team processes and innovate on testing approach\nPrior experience with Data Analysis and Data Engineering is a plus\nStrong collaboration skills and ability to work effectively in a cross-functional, interdependent team environment\nCorporate Security Responsibility\n\nAll activities involving access to Mastercard assets, information, and networks comes with an inherent risk to the organization and, therefore, it is expected that every person working for, or on behalf of, Mastercard is responsible for information security and must:\nAbide by Mastercard s security policies and practices;\nEnsure the confidentiality and integrity of the information being accessed;\nReport any suspected information security violation or breach, and\nComplete all periodic mandatory security trainings in accordance with Mastercard s guidelines.",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Maven', 'Manager Quality Assurance', 'Eclipse', 'Information security', 'Agile', 'JSON', 'Selenium', 'Information technology', 'Technical support', 'SQL']",2025-06-14 06:07:49
Snowflake Databricks Engineer,Oracle,4 - 7 years,Not Disclosed,['Bengaluru'],"Snowflake Development:\nDesign, build, and optimize ETL pipelines within and outside of Snowflake.\nDevelop scripts using languages like Python, Unix, or other relevant technologies for data loading, extraction, and transformation.\nCreate and maintain views, stored procedures, and other database objects in Snowflake.\nOptimize query performance and troubleshoot data-related issues.\nDatabricks Integration:\nCollaborate with data scientists and analysts to integrate Snowflake with Databricks.\nBuild and maintain highly scalable data pipelines using Databricks, Azure, AWS, or other cloud platforms.\nDesign and implement ETL/ELT data pipelines to extract, process, and transform data from various sources.\nWork on data insights, machine learning models, and fraud detection within the Databricks environment.\nData Quality and Security:\nImplement extensive data quality checks to ensure high-quality data.\nDefine and enforce data security measures within both Snowflake and Databricks.\nMonitor and manage access control for data assets.\nCollaboration and Communication:\nCollaborate with business leaders to understand organizational goals and align data engineering efforts.\nCommunicate data trends, insights, and recommendations to business executives.\nQualifications:\nBachelor s degree in Computer Science, Information Systems, or a related field.\nExp Level- 4-7 Years\nProven experience as a Data Engineer, with a focus on Snowflake and Databricks.\nStrong proficiency in SQL and Snowflake s Snow SQL.\nFamiliarity with cloud platforms (Azure, AWS, or Google Cloud).\nExperience with ETL/ELT processes, data modeling, and performance optimization.\nExcellent problem-solving skills and attention to detail.\nEffective communication and collaboration abilities.\nAdditional Notes:\nThis role offers an exciting opportunity to work at the intersection of Snowflake and Databricks, leveraging the strengths of both platforms.\nYou ll contribute to building scalable, reliable, and secure data solutions that drive business insights and innovation.\nIdeal to have some background knowledge around Finance / Investment Banking / Fixed Income / OCIO Business\nCareer Level - IC2",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Unix', 'Computer science', 'Data modeling', 'data security', 'Fixed income', 'Data quality', 'Investment banking', 'Stored procedures', 'SQL', 'Python']",2025-06-14 06:07:52
Senior Databricks Engineer - Python Programming,Leading Client,8 - 13 years,Not Disclosed,"['Chennai', 'Bengaluru', 'Delhi / NCR']","About the job :\n\nRole : Senior Databricks Engineer / Databricks Technical Lead/ Data Architect\n\nExperience : 8-15 years\n\nLocation : Bangalore, Chennai, Delhi, Pune\n\nPrimary Roles And Responsibilities :\n\n- Developing Modern Data Warehouse solutions using Databricks and AWS/ Azure Stack\n\n- Ability to provide solutions that are forward-thinking in data engineering and analytics space\n\n- Collaborate with DW/BI leads to understand new ETL pipeline development requirements.\n\n- Triage issues to find gaps in existing pipelines and fix the issues\n\n- Work with business to understand the need in reporting layer and develop data model to fulfill reporting needs\n\n- Help joiner team members to resolve issues and technical challenges.\n\n- Drive technical discussion with client architect and team members\n\n- Orchestrate the data pipelines in scheduler via Airflow\n\nSkills And Qualifications :\n\n- Bachelor's and/or masters degree in computer science or equivalent experience.\n\n- Must have total 6+ yrs. of IT experience and 3+ years' experience in Data warehouse/ETL projects.\n\n- Deep understanding of Star and Snowflake dimensional modelling.\n\n- Strong knowledge of Data Management principles\n\n- Good understanding of Databricks Data & AI platform and Databricks Delta Lake Architecture\n\n- Should have hands-on experience in SQL, Python and Spark (PySpark)\n\n- Candidate must have experience in AWS/ Azure stack\n\n- Desirable to have ETL with batch and streaming (Kinesis).\n\n- Experience in building ETL / data warehouse transformation processes\n\n- Experience with Apache Kafka for use with streaming data / event-based data\n\n- Experience with other Open-Source big data products Hadoop (incl. Hive, Pig, Impala)\n\n- Experience with Open Source non-relational / NoSQL data repositories (incl. MongoDB, Cassandra, Neo4J)\n\n- Experience working with structured and unstructured data including imaging & geospatial data.\n\n- Experience working in a Dev/Ops environment with tools such as Terraform, CircleCI, GIT.\n\n- Proficiency in RDBMS, complex SQL, PL/SQL, Unix Shell Scripting, performance tuning and troubleshoot\n\n- Databricks Certified Data Engineer Associate/Professional Certification (Desirable).\n\n- Comfortable working in a dynamic, fast-paced, innovative environment with several ongoing concurrent projects\n\n- Should have experience working in Agile methodology\n\n- Strong verbal and written communication skills.\n\n- Strong analytical and problem-solving skills with a high attention to detail.\n\nLocation - Bangalore, Chennai, Delhi / NCR, Pune",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Python Programming', 'RDBMS', 'PySpark', 'Azure Databricks', 'Snowflake DB', 'Data Modeling', 'ETL', 'Data Analytics', 'Data Integration', 'Python', 'SQL']",2025-06-14 06:07:55
S&C Global Network - AI - Telecom Analytics - Manager,Accenture,10 - 15 years,Not Disclosed,"['Hyderabad', 'Gurugram', 'Bengaluru']","Location:Bangalore/Gurgaon/Hyderabad/Mumbai\n\nMust have skills:Must have skills:Data Scientist / Transformation Leader & at least 5 years in Telecom Analytics\n\n\nGood to have skills:GEN AI, Agentic AI,\n\nJob Summary: About Global Network Data & AI:- Accenture Strategy & Consulting Global Network - Data & AI practice help our clients grow their business in entirely new ways. Analytics enables our clients to achieve high performance through insights from data - insights that inform better decisions and strengthen customer relationships. From strategy to execution, Accenture works with organizations to develop analytic capabilities - from accessing and reporting on data to predictive modelling - to outperform the competition\n\nAbout Comms & Media practice:\n\nComms & Media (C&M) is one of the Industry Practices within Accentures S&C Global Network team. It focuses in serving clients across specific Industries Communications, Media & Entertainment.\n\nCommunications Focuses primarily on industries related with telecommunications and information & communication technology (ICT). This team serves most of the worlds leading wireline, wireless, cable and satellite communications and service providers\n\nMedia & Entertainment Focuses on industries like broadcast, entertainment, print and publishing\n\nGlobally, Accenture Comms & Media practice works to develop value growth strategies for its clients and infuse AI & GenAI to help deliver top their business imperatives i.e., revenue growth & cost reduction.\n\nFrom multi-year Data & AI transformation projects to shorter more agile engagements, we have a rapidly expanding portfolio of hyper-growth clients and an increasing footprint with next-gen solutions and industry practices.\n\n\nRoles & Responsibilities: A Telco domain experienced and data science consultant is responsible to help the clients with designing & delivering AI solutions. He/she should be strong in Telco domain, AI fundamentals and should have good hands-on experience working with the following:\n\nAbility to work with large data sets and present conclusions to key stakeholders; Data management using SQL.\n\nPropose solutions to the client based on gap analysis for the existing Telco platforms that can generate long term & sustainable value to the client.\n\nGather business requirements from client stakeholders via interactions like interviews and workshops with all stakeholders Track down and read all previous information on the problem or issue in question. Explore obvious and known avenues thoroughly. Ask a series of probing questions to get to the root of a problem.\n\nAbility to understand the as-is process; understand issues with the processes which can be resolved either through Data & AI or process solutions and design detail level to-be state\n\nUnderstand customer needs and identify/translate them to business requirements (business requirement definition), business process flows and functional requirements and be able to inform the best approach to the problem.\n\nAdopt a clear and systematic approach to complex issues (i.e. A leads to B leads to C). Analyze relationships between several parts of a problem or situation. Anticipate obstacles and identify a critical path for a project.\n\nIndependently able to deliver products and services that empower clients to implement effective solutions. Makes specific changes and improvements to processes or own work to achieve more. Work with other team members and make deliberate efforts to keep others up to date.\n\nEstablish a consistent and collaborative presence with clients and act as the primary point of contact for assigned clients; escalate, track, and solve client issues.\n\nPartner with clients to understand end clients business goals, marketing objectives, and competitive constraints.\n\nStorytelling Crunch the data & numbers to craft a story to be presented to senior client stakeholders.\n\n\nProfessional & Technical\n\nSkills:\nOverall 10+ years of experience in Data Science & at least 5 years in Telecom Analytics\nMasters (MBA/MSc/MTech) from a Tier 1/Tier 2 and Engineering from Tier 1 school\nDemonstrated experience in solving real-world data problems through Data & AI\nDirect onsite experience (i.e., experience of facing client inside client offices in India or abroad) is mandatory. Please note we are looking for client facing roles.\nProficiency with data mining, mathematics, and statistical analysis\nAdvanced pattern recognition and predictive modeling experience; knowledge of Advanced analytical fields in text mining, Image recognition, video analytics, IoT etc.\nExecution level understanding of econometric/statistical modeling packages\nTraditional techniques like Linear/logistic regression, multivariate statistical analysis, time series techniques, fixed/Random effect modelling.\nMachine learning techniques like - Random Forest, Gradient Boosting, XG boost, decision trees, clustering etc.\nKnowledge of Deep learning modeling techniques like RNN, CNN etc.\nExperience using digital & statistical modeling software (one or more) Python, R, PySpark, SQL, BigQuery, Vertex AI\nProficient in Excel, MS word, Power point, and corporate soft skills\nKnowledge of Dashboard creation platforms Excel, tableau, Power BI etc.\nExcellent written and oral communication skills with ability to clearly communicate ideas and results to non-technical stakeholders.\nStrong analytical, problem-solving skills and good communication skills\nSelf-Starter with ability to work independently across multiple projects and set priorities\nStrong team player Proactive and solution oriented, able to guide junior team members.\nExecution knowledge of optimization techniques is a good-to-have\nExact optimization Linear, Non-linear optimization techniques\nEvolutionary optimization Both population and search-based algorithms\nCloud platform Certification, experience in Computer Vision are good-to-haves\nQualification\n\nExperience:Overall 10+ years of experience in Data Science & at least 5 years in Telecom\n\n\nEducational Qualification:Masters (MBA/MSc/MTech) from a Tier 1/Tier 2 and Engineering from Tier 1 school",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Telecom Analytics', 'GEN AI', 'Agentic AI', 'predictive modelling', 'BigQuery', 'Data management', 'Gradient Boosting', 'XG boost', 'PySpark', 'Vertex AI', 'SQL', 'Random Forest']",2025-06-14 06:07:57
Lead Analytics Specialist,Razorpay,5 - 7 years,Not Disclosed,['Bengaluru'],"Lead Analytics Specialist will work with the central analytics team at Razorpay. This will give you an opportunity to work in a fast-paced environment aimed at creating a very high impact and to work with a diverse team of smart and hardworking professionals from various backgrounds. Some of the responsibilities include working with large, complex data sets, developing strong business and product understanding and closely being involved in the product life cycle. Setting high standards in project management and mentoring analysts.\nRoles and Responsibilities:\nYou will work with large, complex data sets to solve open-ended, high impact business problems using data mining, experimentation, statistical analysis and related techniques, machine learning as needed\nYou would have/develop a strong understanding of the business & product and conduct analysis to derive insights, develop hypothesis and validate with sound rigorous methodologies or formulate the problems for modeling with ML\nYou would apply excellent problem solving skills and independently scope, deconstruct and formulate solutions from first-principles that bring outside-in and state of the art view\nYou would be closely involved with the product life cycle working on ideation, reviewing Product Requirement Documents, defining success criteria, instrumenting for product features, Impact assessment and identifying and recommending improvements to further enhance the Product features\nYou would expedite root cause analyses/insight generation against a given recurring use case through automation/self-serve platforms\nYou will develop compelling stories with business insights, focusing on strategic goals of the organization\nYou will work with Business, Product and Data engineering teams for continuous improvement of data accuracy through feedback and scoping on instrumentation quality and completeness\nSet high standards in project management; own scope and timelines for the team\nTake responsibility for skill-building within the organization (training, process definition, research of new tools and techniques etc.)\nMentoring analysts within the team to deliver effectively, incubating new ideas.\nMandatory Qualifications:\nBachelors/Master s degree in Engineering, Economics, Finance, Mathematics, Statistics, Business Administration or a related quantitative field\n5-7 years of high quality hands-on experience in analytics and data science\nHands on experience in SQL and Python\nDefine the business and product metrics to be evaluated, work with engg on data instrumentation, create and automate self-serve dashboards to present to relevant stakeholders leveraging tools such as Tableau, Qlikview, Looker etc.\nAbility to structure and analyze data leveraging techniques like EDA, Cohort analysis, Funnel analysis and transform them into understandable and actionable recommendations and then communicate them effectively across the organization.\nHands on experience in working with large scale structured, semi structured and unstructured data and various approach to preprocess/cleanse data, dimensionality reduction\nWork experience in Consumer-tech organizations would be a plus\nDeveloped a clear understanding of the qualitative and quantitative aspects of the product/strategic initiative and leverage it to identify and act upon existing Gaps and Opportunities\nHands on experience of A/B testing, Significance testing, supervised and unsupervised ML, Web Analytics and Statistical Learning",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Loans', 'Automation', 'Web analytics', 'Project management', 'Instrumentation', 'QlikView', 'Data mining', 'Continuous improvement', 'Financial services', 'SQL']",2025-06-14 06:07:59
Name List Screening and Transaction Screening Model Strats,Deutsche Bank,4 - 9 years,Not Disclosed,['Bengaluru'],"Job Title- Name List Screening and Transaction Screening Model Strats, AS\nLocation- Bangalore, India\n\nRole Description\nGroup Strategic Analytics (GSA) is part of Group Chief Operation Office (COO) which acts as the bridge between the Banks business and infrastructure functions to help deliver the efficiency, control, and transformation goals of the Bank.\nYou will work within the Global Strategic Analytics Team as part of a global model strategy and deployment of Name List Screening and Transaction Screening. To be successful in that role, you will be familiar with the most recent data science methodologies and have a delivery-centric attitude, strong analytical skills, and a detail-oriented approach to breaking down complex matters into more understandable details.\nThe purpose of Name List Screening and Transaction Screening is to identify and investigate unusual customer names and transactions and behavior, to understand if that activity is considered suspicious from a financial crime perspective, and to report that activity to the government. You will be responsible for helping to implement and maintain the models for Name List Screening and Transaction Screening to ensure that all relevant criminal risks, typologies, products, and services are properly monitored.\nWe are looking for a high-performing Associate in financial crime model development, tuning, and analytics to support the global strategy for screening systems across Name List Screening (NLS) and Transaction Screening (TS). This role offers the opportunity to work on key model initiatives within a cross-regional team and contribute directly to the banks risk mitigation efforts against financial crime.\nYou will support model tuning and development efforts, support regulatory deliverables, and help collaborate with cross-functional teams including Compliance, Data Engineering, and Technology.\n\nWhat well offer you\nAs part of our flexible scheme, here are just some of the benefits that youll enjoy\nBest in class leave policy\nGender neutral parental leaves\n100% reimbursement under childcare assistance benefit (gender neutral)\nSponsorship for Industry relevant certifications and education\nEmployee Assistance Program for you and your family members\nComprehensive Hospitalization Insurance for you and your dependents\nAccident and Term life Insurance\nComplementary Health screening for 35 yrs. and above\n\nYour key responsibilities\nSupport the design and implementation of the model framework for name and transaction screening including coverage, data, model development and optimisation.\nSupport key data initiatives, including but not limited to, data lineage, data quality controls, and data quality issues management.\nDocument model logic and liaise with Compliance and Model Risk Management teams to ensure screening systems and scenarios adhere to all model governance standards\nParticipate in research projects on innovative solutions to make detection models more pro-active\nAssist in model testing, calibration and performance monitoring. Ensure detailed metrics & reporting are developed to provide transparency and maintain effectiveness of name and transaction screening models.\nSupport all examinations and reviews performed by regulators, monitors, and internal audit\n\nYour skills and experience\nAdvanced degree (Masters or PhD) in a quantitative discipline (Mathematics, Computer Science, Data Science, Physics or Statistics)\n1–3 years experience in data analytics or model development (internships included).\nProficiency in designing, implementing (python, spark, cloud environments) and deploying quantitative models in a large financial institution, preferably in Front Office. Hands-on approach needed.\nExperience utilizing Machine Learning and Artificial Intelligence\nExperience with data and the ability to clearly articulate data requirements as they relate to NLS and TS, including comprehensiveness, quality, accuracy and integrity \nKnowledge of the bank’s products and services, including those related to corporate banking, investment banking, private banking, and asset management\n\nHow we’ll support you\nTraining and development to help you excel in your career\nCoaching and support from experts in your team\nA culture of continuous learning to aid progression\nA range of flexible benefits that you can tailor to suit your needs\n\nAbout us and our teams\nPlease visit our company website for further information\nhttps://www.db.com/company/company.htm",Industry Type: Banking,"Department: BFSI, Investments & Trading","Employment Type: Full Time, Permanent","['Data Analysis', 'Model Risk', 'Artificial Intelligence', 'Data Modeling', 'Machine Learning', 'Model Development', 'Python']",2025-06-14 06:08:02
"Business Intelligence Engineer II, Amazon FinAuto - GREF Tech",Amazon,3 - 8 years,Not Disclosed,['Hyderabad'],"We are seeking a passionate Business Intelligence Engineer II to create the next generation of real estate systems and tools. You will play a crucial role in driving the development and implementation of advanced business intelligence and analytics solutions. You will leverage your strong technical expertise and analytical skills to empower GREFs cross-functional teams with actionable data insights that support strategic decision-making.\n\nThe ideal candidate is a self-starter, comfortable with ambiguity, and excels at building highly scalable solutions. They will enjoy learning, implementing new technologies, and will be innovative in implementing them in suitable situations. A Business Intelligence Engineer II at Amazon works on real world problems on a global scale, owns their systems end to end and influences the direction of our technology that impacts hundreds of millions of customers around the world. Coming to Amazon gives you the opportunity to work on development teams in one of our many rapidly growing organizations. Come join us in making history!\n\n\nCollaborate across a team of BI engineers, sharing domain expertise and best practices to elevate the overall\nBI capabilities",,,,"['Automation', 'MIS', 'Machine learning', 'corporate real estate', 'Predictive modeling', 'Business intelligence', 'Distribution system', 'SQL', 'Python']",2025-06-14 06:08:04
Spark Developer,Infosys,3 - 5 years,Not Disclosed,['Bengaluru'],"Educational Requirements\nMCA,MSc,Bachelor of Engineering,BBA,BCom,BSc\nService Line\nData & Analytics Unit\nResponsibilities\nSpark Expertise\nExpert proficiency in Spark\nAbility to design and implement efficient data processing workflows\nExperience with Spark SQL and DataFrames\nGood exposure to Big Data architectures and good understanding of Big Data eco system\nExperience with some framework building experience on Hadoop\nGood with DB knowledge with SQL tuning experience.\nGood to have experience with Python, APIs and exposure to Kafka.\nAdditional Responsibilities:\nAbility to work with clients to identify business challenges and contribute to client deliverables by refining, analyzing, and structuring relevant data\nAwareness of latest technologies and trends\nLogical thinking and problem solving skills along with an ability to collaborate\nAbility to assess the current processes, identify improvement areas and suggest the technology solutions\nOne or two industry domain knowledge\nTechnical and Professional Requirements:\nPrimary skills:Technology->Big Data - Data Processing->Spark\nPreferred Skills:\nTechnology->Big Data - Data Processing->Spark",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Spark', 'Big Data', 'Kafka', 'Data architectures', 'API', 'DataFrames', 'SQL', 'Python']",2025-06-14 06:08:06
Senior BI Developer,ZS,3 - 8 years,Not Disclosed,['Pune'],"ZS   is a place where passion changes lives. As a management consulting and technology firm focused on improving life and how we live it , our most valuable asset is our people. Here you ll work side-by-side with a powerful collective of thinkers and experts shaping life-changing solutions for patients, caregivers and consumers, worldwide. ZSers drive impact by bringing a client first mentality to each and every engagement. We partner collaboratively with our clients to develop custom solutions and technology products that create value and deliver company results across critical areas of their business. Bring your curiosity for learning; bold ideas; courage an d passion to drive life-changing impact to ZS.\n\n\n\n\n\n  .\n\nAt   we honor the visible and invisible elements of our identities, personal experiences and belief systems the ones that comprise us as individuals, shape who we are and\n\nmake us unique. We believe your personal interests, identities, and desire to learn are part of your success here. Learn more about our diversity, equity, and inclusion efforts and the networks ZS supports to assist our ZSers in cultivating community spaces, obtaining the resources they need to thrive, and sharing the messages they are passionate about.\n\n \n\n\n\nWe are looking for an experienced   to join our Data Analytics team. The ideal candidate will have a strong background in designing, developing, and deploying Power BI reports and dashboards, with in-depth expertise in data modeling, DAX, SQL, and ETL processes. You will work closely with business stakeholders to translate data into actionable insights and enable data-driven decision-making.\n\n\n\n \n\n\n\n\n\n \n\n\n\n\n\n \n\nZS offers a comprehensive total rewards package including health and well-being, financial planning, annual leave, personal growth and professional development. Our robust skills development programs, multiple career progression options and internal mobility paths and collaborative culture empowers you to thrive as an individual and global team member.\n\nWe are committed to giving our employees a flexible and connected way of working. A flexible and connected ZS allows us to combine work from home and on-site presence at clients/ZS offices for the majority of our week. The magic of ZS culture and innovation thrives in both planned and spontaneous face-to-face connections.\n\n\n\n \n\nTravel is a requirement at ZS for client facing ZSers; business needs of your project and client are the priority. While some projects may be local, all client-facing ZSers should be prepared to travel as needed. Travel provides opportunities to strengthen client relationships, gain diverse experiences, and enhance professional growth by working in different environments and cultures.\n\n\n\n\n\n \n\nAt ZS, we're building a diverse and inclusive company where people bring their passions to inspire life-changing impact and deliver better outcomes for all. We are most interested in finding the best candidate for the job and recognize the value that candidates with all backgrounds, including non-traditional ones, bring. If you are interested in joining us, we encourage you to apply even if you don't meet 100% of the requirements listed above.\n\n\n\n\n\n\n\n \n\nCandidates must possess or be able to obtain work authorization for their intended country of employment.An on-line application, including a full set of transcripts (official or unofficial), is required to be considered.",,,,"['power bi', 'sql', 'data modeling', 'etl tool', 'dax', 'python', 'm-query', 'microsoft azure', 'sharepoint', 'azure data factory', 'relational databases', 'sql server', 'power query', 'bi development', 'powerapps', 'stakeholder management', 'row', 'data visualization', 'api', 'etl', 'etl process']",2025-06-14 06:08:09
"Business Intelligence Engineer, III, Amazon FinAuto - GREF Tech",Amazon,8 - 13 years,Not Disclosed,['Hyderabad'],"The Global Real Estate and Facilities (GREF) Technology team, part of Finance Automation, is the software development and data and analytics team for GREF. Our mission is to build technology solutions that simplify the processes Amazon employs to manage its corporate real estate and improve workplace experience. We build services and products that support various GREF domains, including space and occupancy planning, design and construction, employee health and safety, sustainability, facilities maintenance, help desk, and building management systems requiring IoT expertise. If you possess the skills to build, operate, and scale the next generation of distributed systems supporting Amazons growth, this role may be well-suited for you.\n\nWe are seeking a passionate Business Intelligence Engineer, III to create the next generation of real estate systems and tools. You will play a crucial role in driving the development and implementation of advanced business intelligence and analytics solutions. You will leverage your strong technical expertise and analytical skills to empower GREFs cross-functional teams with actionable data insights that support strategic decision-making.\n\nThe ideal candidate is a self-starter, comfortable with ambiguity, and excels at building highly scalable solutions. They will enjoy learning, implementing new technologies, and will be innovative in implementing them in suitable situations. A Business Intelligence Engineer, III at Amazon works on real world problems on a global scale, owns their systems end to end and influences the direction of our technology that impacts hundreds of millions of customers around the world. Coming to Amazon gives you the opportunity to work on development teams in one of our many rapidly growing organizations. Come join us in making history!\n\n\nLead a team of BI engineers, sharing domain expertise and best practices to elevate the overall\nBI capabilities\nBuild visually stunning, interactive dashboards and reports that provide actionable insights to\nGREF customers using Amazon QuickSight\nLead the design and implementation of highly complex, scalable, and high-performance BI\narchitectures and data models to support the organizations strategic business objectives\nUse advanced analytics techniques, such as statistical analysis, predictive modeling, and\nmachine learning, to uncover hidden patterns and trends in data\nCollaborate cross-functionally with data engineers, data scientists, and business partners to\ndefine and deliver impactful BI solutions that drive business value\nLead the identification, evaluation, and implementation of new BI tools and technologies to continuously enhance the organizations analytical capabilities\nRepresent the BI function in strategic business discussions and provide data-driven\nrecommendations to senior leadership Bachelors degree in Computer Science, Data Science, Statistics, Management Information Systems (MIS), or a related field\n8+ years of experience as a Business Intelligence Engineer or similar senior-level role\nProven expertise in designing and implementing complex, large-scale BI architectures and data models\n5+ years experience with leading BI tools and technologies (e.g., Tableau, Amazon QuickSight, Power BI, Looker, AWS Athena, Snowflake)\nDeep understanding of data engineering concepts and proficiency in SQL, Python, or other data-oriented programming languages\nAbility to effectively communicate technical concepts to both technical and non-technical stakeholders\nStrong problem-solving, critical thinking, and analytical skills\nExperience leading cross-functional teams and driving the successful delivery of BI initiative Bachelors degree in Computer Science, Data Science, Statistics, Management Information Systems (MIS), or a related field\n8+ years of experience as a Business Intelligence Engineer or similar senior-level role\nProven expertise in designing and implementing complex, large-scale BI architectures and data models\n5+ years experience with leading BI tools and technologies (e.g., Tableau, Amazon QuickSight, Power BI, Looker, AWS Athena, Snowflake)\nDeep understanding of data engineering concepts and proficiency in SQL, Python, or other data-oriented programming languages",,,,"['Computer science', 'Automation', 'MIS', 'Machine learning', 'corporate real estate', 'Predictive modeling', 'Business intelligence', 'Distribution system', 'SQL', 'Python']",2025-06-14 06:08:11
"Manager, Business Intelligence Engineer, III, Amazon FinAuto",Amazon,8 - 13 years,Not Disclosed,['Hyderabad'],"The Global Real Estate and Facilities (GREF) Technology team, part of Finance Automation, is the software development and data and analytics team for GREF. Our mission is to build technology solutions that simplify the processes Amazon employs to manage its corporate real estate and improve workplace experience. We build services and products that support various GREF domains, including space and occupancy planning, design and construction, employee health and safety, sustainability, facilities maintenance, help desk, and building management systems requiring IoT expertise. If you possess the skills to build, operate, and scale the next generation of distributed systems supporting Amazons growth, this role may be well-suited for you.\n\nWe are seeking a passionate Manager Business Intelligence Engineer, III to create the next generation of real estate systems and tools. You will play a crucial role in driving the development and implementation of advanced business intelligence and analytics solutions. You will leverage your strong technical expertise and analytical skills to empower GREFs cross-functional teams with actionable data insights that support strategic decision-making.\n\nThe ideal candidate is a self-starter, comfortable with ambiguity, and excels at building highly scalable solutions. They will enjoy learning, implementing new technologies, and will be innovative in implementing them in suitable situations. A Manager Business Intelligence Engineer, III at Amazon works on real world problems on a global scale, owns their systems end to end and influences the direction of our technology that impacts hundreds of millions of customers around the world. Coming to Amazon gives you the opportunity to work on development teams in one of our many rapidly growing organizations. Come join us in making history!\n\n\nLead a team of BI engineers, sharing domain expertise and best practices to elevate the overall\nBI capabilities\nBuild visually stunning, interactive dashboards and reports that provide actionable insights to\nGREF customers using Amazon QuickSight\nLead the design and implementation of highly complex, scalable, and high-performance BI\narchitectures and data models to support the organizations strategic business objectives\nUse advanced analytics techniques, such as statistical analysis, predictive modeling, and\nmachine learning, to uncover hidden patterns and trends in data\nCollaborate cross-functionally with data engineers, data scientists, and business partners to\ndefine and deliver impactful BI solutions that drive business value\nLead the identification, evaluation, and implementation of new BI tools and technologies to continuously enhance the organizations analytical capabilities\nRepresent the BI function in strategic business discussions and provide data-driven\nrecommendations to senior leadership Bachelors degree in Computer Science, Data Science, Statistics, Management Information Systems (MIS), or a related field\n8+ years of experience as a Business Intelligence Engineer or similar senior-level role\nProven expertise in designing and implementing complex, large-scale BI architectures and data models\n5+ years experience with leading BI tools and technologies (e.g., Tableau, Amazon QuickSight, Power BI, Looker, AWS Athena, Snowflake)\nDeep understanding of data engineering concepts and proficiency in SQL, Python, or other data-oriented programming languages\nAbility to effectively communicate technical concepts to both technical and non-technical stakeholders\nStrong problem-solving, critical thinking, and analytical skills\nExperience leading cross-functional teams and driving the successful delivery of BI initiative Bachelors degree in Computer Science, Data Science, Statistics, Management Information Systems (MIS), or a related field\n8+ years of experience as a Business Intelligence Engineer or similar senior-level role\nProven expertise in designing and implementing complex, large-scale BI architectures and data models\n5+ years experience with leading BI tools and technologies (e.g., Tableau, Amazon QuickSight, Power BI, Looker, AWS Athena, Snowflake)\nDeep understanding of data engineering concepts and proficiency in SQL, Python, or other data-oriented programming languages",,,,"['Computer science', 'Automation', 'MIS', 'Machine learning', 'corporate real estate', 'Predictive modeling', 'Distribution system', 'Manager Business Intelligence', 'SQL', 'Python']",2025-06-14 06:08:14
Scala Developer,Infosys,3 - 5 years,Not Disclosed,['Bengaluru'],"Educational Requirements\nBachelor of Engineering\nService Line\nData & Analytics Unit\nResponsibilities\nA day in the life of an Infoscion\nAs part of the Infosys delivery team, your primary role would be to interface with the client for quality assurance, issue resolution and ensuring high customer satisfaction.\nYou will understand requirements, create and review designs, validate the architecture and ensure high levels of service offerings to clients in the technology domain.\nYou will participate in project estimation, provide inputs for solution delivery, conduct technical risk planning, perform code reviews and unit test plan reviews.\nYou will lead and guide your teams towards developing optimized high quality code deliverables, continual knowledge management and adherence to the organizational guidelines and processes.\nYou would be a key contributor to building efficient programs/ systems and if you think you fit right in to help our clients navigate their next in their digital transformation journey, this is the place for you!If you think you fit right in to help our clients navigate their next in their digital transformation journey, this is the place for you!\nAdditional Responsibilities:\nKnowledge of more than one technology\nBasics of Architecture and Design fundamentals\nKnowledge of Testing tools\nKnowledge of agile methodologies\nUnderstanding of Project life cycle activities on development and maintenance projects\nUnderstanding of one or more Estimation methodologies, Knowledge of Quality processes\nBasics of business domain to understand the business requirements\nAnalytical abilities, Strong Technical Skills, Good communication skills\nGood understanding of the technology and domain\nAbility to demonstrate a sound understanding of software quality assurance principles, SOLID design principles and modelling methods\nAwareness of latest technologies and trends\nExcellent problem solving, analytical and debugging skills\nTechnical and Professional Requirements:\nTechnology->Big Data - Data Processing->Spark\nPreferred Skills:\nTechnology->Java->Apache->Scala\nTechnology->Functional Programming->Scala",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Scala', 'Project Estimation', 'quality assurance', 'Big Data', 'agile methodologies', 'SOLID']",2025-06-14 06:08:16
Job opportunity! Senior Kafka Engineer! Technogen! Work from office!,TechnoGen INC,8 - 13 years,35-70 Lacs P.A.,['Bengaluru'],"Job Title: Senior Kafka Engineer\nLocation: Bangalore (Work from Office)\nShift: Evening Shift (Ends by 11 PM IST)\nExperience: 8+ Years\nInterview : Virtual Drive, Saturday 11 AM to 5 PM\nAbout the Role\nWere seeking a highly experienced Senior Kafka Engineer to join our fast-paced engineering team in Bangalore. This role requires deep expertise in Apache Kafka, Confluent Platform, and cloud-native tooling to support our real-time data streaming infrastructure. If you are passionate about building scalable, fault-tolerant data pipelines and mentoring others, wed love to talk to you!\nKey Responsibilities\nManage and enhance existing Apache Kafka and Confluent Platform on AWS.\nReview existing implementations and recommend architectural or performance improvements.\nCollaborate with engineering and product teams to integrate new use cases and define scalable streaming patterns.\nImplement and maintain Kafka producers/consumers, Kafka Connectors, and Kafka Streams applications.\nEnforce governance policies around topic design, schema evolution, partitioning strategies, and data retention.\nMonitor, troubleshoot, and optimize Kafka clusters using Confluent Control Center, Prometheus, and Grafana.\nUse Kubernetes and Terraform to automate infrastructure provisioning, deployment, and scaling.\nEnsure high availability, security, and disaster recovery for Kafka environments.\nMentor junior engineers and lead Kafka-related initiatives across teams.\nRequired Skills & Qualifications\n8+ years of hands-on experience in backend/data engineering with at least 4 years focused on Apache Kafka.\nDeep understanding of Kafka internals, brokers, zookeepers, producers, consumers, and message delivery semantics.\nExperience with Confluent Platform and schema management via Schema Registry.\nStrong background in cloud platforms (preferably AWS).\nProficient with Kubernetes, Terraform, and CI/CD pipelines.\nWorking knowledge of observability stacks like Grafana, Prometheus, and ELK.\nSolid understanding of distributed systems, fault tolerance, and data streaming patterns.\nWork Arrangement\nLocation: Bangalore Work from Office\nShift Timing: Ends by 11 PM IST (Evening Shift)\nEmployment Type: Full-Time, Permanent\nApply Now by reaching out to [sasidhar.m@technogenindia.com] with your resume.\nPlease share availability for tomorrow",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Kafka', 'Confluent', 'Implementation And Maintenance']",2025-06-14 06:08:18
Scala Developer,Infosys,3 - 8 years,Not Disclosed,['Bengaluru'],"Educational Requirements\nBachelor of Engineering\nService Line\nData & Analytics Unit\nResponsibilities\nA day in the life of an Infoscion\nAs part of the Infosys consulting team, your primary role would be to actively aid the consulting team in different phases of the project including problem definition, effort estimation, diagnosis, solution generation and design and deployment\nYou will explore the alternatives to the recommended solutions based on research that includes literature surveys, information available in public domains, vendor evaluation information, etc. and build POCs\nYou will create requirement specifications from the business needs, define the to-be-processes and detailed functional designs based on requirements.\nYou will support configuring solution requirements on the products; understand if any issues, diagnose the root-cause of such issues, seek clarifications, and then identify and shortlist solution alternatives\nYou will also contribute to unit-level and organizational initiatives with an objective of providing high quality value adding solutions to customers. If you think you fit right in to help our clients navigate their next in their digital transformation journey, this is the place for you!\nAdditional Responsibilities:\nAbility to work with clients to identify business challenges and contribute to client deliverables by refining, analyzing, and structuring relevant data\nAwareness of latest technologies and trends\nLogical thinking and problem solving skills along with an ability to collaborate\nAbility to assess the current processes, identify improvement areas and suggest the technology solutions\nOne or two industry domain knowledge\nTechnical and Professional Requirements:\nPrimary skills:Bigdata->Scala,Bigdata->Spark,Technology->Java->Play Framework,Technology->Reactive Programming->Akka\nPreferred Skills:\nBigdata->Spark\nBigdata->Scala\nTechnology->Reactive Programming->Akka\nTechnology->Java->Play Framework",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Scala', 'Reactive Programming', 'Bigdata', 'Play Framework', 'Akka']",2025-06-14 06:08:21
Senior Machine Learning Engineer,Harness,5 - 10 years,Not Disclosed,['Bengaluru'],"Job Summary:\n\nAs a Senior Machine Learning Engineer at Traceable, you will be instrumental in transforming\nML models from prototype to production at scale. You will work closely with data scientists,\nMLOps engineers, and product teams to design, develop and deploy critical, high-performing\nML solutions. This role requires a blend of engineering, MLOps, and data science skills to\nstreamline model deployment and ensure continuous, reliable operations in the production\nenvironments.\n\nResponsibilities :\nModel Productionization: Convert ML models from prototypes to scalable, production-\nready solutions. Optimize models for performance, scalability, and resource efficiency.\n\nIntegration and Deployment: Develop and maintain enablement pipelines for continuous\nintegration and deployment of ML models, ensuring smooth transitions from development\nto production.\n\nScalability and Optimization: Implement distributed systems and leverage cloud-based\narchitectures (e.g., AWS, GCP) to scale ML models and optimize for low latency and high\navailability.\nModel Monitoring and Maintenance: Set up monitoring systems to track model\nperformance in production, detect data drift, and trigger automated retraining when\nneeded\n. Innovation and Tooling: Evaluate and integrate new tools, frameworks, and libraries that\ncan improve model deployment speed and robustness and keep Traceable.ai at the\ncutting edge of ML infrastructure.\n\nDocumentation and Knowledge Sharing: Document processes, maintain well-structured\ncodebases, promote best practices in ML engineering, and lead internal knowledge-\nsharing sessions to foster a culture of continuous improvement and technical excellence.\n\nRequirements :\nEducation: Bachelor s or master s degree in computer science, Machine Learning,\nEngineering, or a related field.\n\nExperience: 5+ years in machine learning engineering or software engineering with\nsignificant ML focus, including experience in deploying ML models in production.\nTechnical Skills:\nProgramming: Proficiency in Python and familiarity with ML libraries (e.g.,\nTensorFlow, PyTorch, Scikit-Learn).\n\nMLOps Tools: Experience with CI/CD for ML, containerization (Docker,\nKubernetes), and workflow orchestration tools (e.g., Airflow, MLflow).\n\nCloud Infrastructure: Strong knowledge of cloud platforms (AWS or GCP),\nincluding managed ML services (SageMaker, Vertex AI).\n\nData Processing: Familiarity with distributed computing frameworks (e.g., Spark,\nDask) and data pipelines. Experience with relational databases like MySQL,\nPostgreSQL and experience with SQL query tuning, performance optimizations is\na plus.\n\nProblem-Solving: Proven ability to troubleshoot and optimize ML systems in production.\n\nCollaboration: Excellent communication and teamwork skills, with experience working in.\nAdaptability: Ability to thrive in a fast-paced, evolving environment and rapidly adopt new\ntools and technologies.\nWork Location\nBangalore. The successful candidate will be expected to be in the Bangalore office 3x/ week.\nWhat You Will Have at Harness\nExperience building a transformative product\nEnd-to-end ownership of your projects\nCompetitive salary\nComprehensive healthcare benefit\nFlexible work schedule\nQuarterly Harness TGIF-Off / 4 days\nPaid Time Off and Parental Leave\nMonthly, quarterly, and annual social and team building events\nMonthly internet reimbursement\n. You can also find additional information about this type of scam and report any fraudulent employment offers via the Federal Trade Commission s website ( https: / / consumer.ftc.gov / articles / job-scams) , or you can contact your local law enforcement agency.",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Prototype', 'GCP', 'MySQL', 'Machine learning', 'Healthcare', 'Continuous improvement', 'Monitoring', 'SQL', 'Python']",2025-06-14 06:08:23
Senior Machine Learning Engineer - R&D,Quantiphi Analytics Solutions,4 - 9 years,Not Disclosed,"['Mumbai', 'Bengaluru', 'Thiruvananthapuram']","While technology is the heart of our business, a global and diverse culture is the heart of our success. We love our people and we take pride in catering them to a culture built on transparency, diversity, integrity, learning and growth.\n\n\nIf working in an environment that encourages you to innovate and excel, not just in professional but personal life, interests you- you would enjoy your career with Quantiphi!\nAbout Quantiphi\nQuantiphi is a cutting-edge AI and data science company focused on delivering transformative solutions through innovation and technology. Phi Labs(RD) spearheads research and development in advanced machine learning, tackling complex real-world challenges through cutting-edge algorithmic innovation.\nRole Overview\nWe are seeking a highly skilled Senior Machine Learning Engineer to join Phi Labs(RD), specializing in cutting-edge AI technologies including agentic AI, multimodal learning, and generative AI. You will design and develop innovative models that enable autonomous, context-aware systems, integrate multiple data modalities (text, vision, audio, etc.), and create state-of-the-art generative models. This role offers a unique opportunity to push the boundaries of AI capabilities in a research-driven environment.\nKey Responsibilities\nDesign and develop advanced machine learning models focusing on agentic AI enabling autonomous agents that perceive, reason, and act effectively in complex environments.\nDevelop multimodal AI systems that seamlessly integrate and reason across diverse data types such as images, text, video, and audio.\nprototype and implement generative AI models (e.g., GANs, diffusion models, large language models) for innovative applications.\nDemonstrate deep expertise in core NLP concepts including tokenization, embeddings, attention mechanisms, and transformer architectures (e.g., BERT, GPT, T5, and their variants).\nStay current with cutting-edge research by actively reading, analyzing, and synthesizing insights from recent ML/AI papers to identify novel techniques and breakthrough applications for product innovation.\nCollaborate with data scientists, engineers, and product teams to build scalable ML pipelines and deploy models into production.\nOptimize and fine-tune large-scale models to improve performance, efficiency, and interpretability.\nLead exploratory research efforts to identify novel AI techniques and applications, contributing to publications, patents, and technology roadmaps.\nMentor junior team members and promote best practices in AI research and engineering.\nDesign and implement state-of-the-art machine learning models for complex, large-scale problems.\nProven ability to translate research papers into practical implementations and production-ready solutions\n\nQualifications\nBachelor s degree in Computer Science, Machine Learning, AI, or related fields.\nwith 4+ years of industry experience in Machine Learning engineering, preferably with a strong focus on research and development (RD).\nMaster s degree in Computer Science, Machine Learning, AI, or related fields.\nwith 3+ years of industry experience in Machine Learning engineering, preferably with a strong focus on research and development (RD)\nDeep expertise in developing and deploying agentic AI systems, multimodal learning models, and generative AI architectures.\nProficiency with Python and ML frameworks like TensorFlow, Pytorch, Hugging Face Transformers, or similar.\nFamiliarity with cloud platforms (AWS, GCP, Azure) and container orchestration (Docker, Kubernetes).\nStrong problem-solving, communication, and collaboration skills.\nPreferred Skills\nHands-on experience with reinforcement learning, large language models, diffusion models, or vision-language models.\nKnowledge of MLOps and model interpretability/explain ability frameworks.\nExperience with real-time AI systems or human-AI interaction models.\nIf you like wild growth and working with happy, enthusiastic over-achievers, youll enjoy your career with us !",Industry Type: IT Services & Consulting,Department: Research & Development,"Employment Type: Full Time, Permanent","['Prototype', 'orchestration', 'data science', 'GCP', 'Product innovation', 'Machine learning', 'Manager Technology', 'Research', 'Python']",2025-06-14 06:08:25
"Software Development Engineer, Digital Acceleration",Amazon,3 - 8 years,Not Disclosed,['Chennai'],"Amazon.com is looking for a talented and enthusiastic SDE to join the Digital Acceleration team. You will be involved in enabling AI, data, metrics and analytics at scale to several Amazon digital teams.\n\nWe specialize in building AI, analytics and data engineering related products specifically around metadata management, privacy compliance, data quality and customer lifecycle management analytics. We are seeking a SDE with a strong knowledge of distributed systems to build enterprise-scale, mission-critical, multi-tiered applications using tools that are well out in front on the technology wave. You must enjoy working on complex software systems in a customer-centric environment and be passionate not only about building good software but also ensuring that the same software achieves its goals in operational reality.\n\n3+ years of non-internship professional software development experience\n2+ years of non-internship design or architecture (design patterns, reliability and scaling) of new and existing systems experience\nExperience programming with at least one software programming language 3+ years of full software development life cycle, including coding standards, code reviews, source control management, build processes, testing, and operations experience\nBachelors degree in computer science or equivalent",,,,"['Computer science', 'Customer life cycle management', 'metadata', 'Coding', 'Software development life cycle', 'Data quality', 'Internship', 'Distribution system', 'Operations', 'Analytics']",2025-06-14 06:08:27
Senior AI Machine Learning Engineer,Chevron India,5 - 10 years,Not Disclosed,['Bengaluru'],"Total Number of Openings\n2\nAbout the position:\n\nChevron invites applications for the role of AI/ML Engineer within our Enterprise AI team in India. This position is integral to designing and developing AI/ML models that significantly accelerate the delivery of business value. We are looking for a Machine Learning Engineer with the ability to bring their expertise, innovative attitude, and excitement for solving complex problems with modern technologies and approaches. We are looking for those few individuals with a passion for exploring, innovating, and delivering innovative Data Science solutions that provide immense value to our business. The expectation for this role is 5-10 years of relevant experience.\n\nKey responsibilities:\n\nTransform data science prototypes into appropriate scale solutions in a production environment\nOrchestrate and configure infrastructure that assists Data Scientists and analysts in building low latency, scalable and resilient machine learning, and optimization workloads into an enterprise software product\nCombine expertise in mathematics, statistics, computer science, and domain knowledge to create advanced AI/ML models.\nCollaborate closely with the AI Technical Manager and GCC Petro-technical professionals and data engineers to integrate and scale models into the business framework.\nIdentify data, appropriate technology, and architectural design patterns to solve business challenges using Chevron approved standard analytical tools and AI design patterns and architectures\nPartner with Data Scientists and Chevron IT Foundational services to implement complex algorithms and models into enterprise scale machine learning pipelines\nRun machine learning experiments and fine-tune algorithms to ensure optimal performance\nConsistently deliver complex, innovative, and complete solutions, driving them through design, planning, development, and deployment that simplify business processes and workflows to drive business value\nWork collaboratively with a large variety of different teams, including data scientists, data engineers, and solution architects from various organizations within business units and IT\n\nRequired Qualifications:\n\nMinimum 5 years experience in Object Oriented Design and/or Functional Programming in Python. 5 - 10 years of experience\nMature software engineering skills, such as source control versioning, requirement spec, architecture, and design review, testing methodologies, CI/CD, etc.\nMust have a disciplined, methodical, minimalist approach to designing and constructing layered software components that can be embedded within larger frameworks or applications.\nExperience implementing machine learning frameworks and libraries such as MLflow\nExperience with containers and container managements (docker, Kubernetes)\nExperience developing cloud first solutions using Microsoft Azure Services including building machine learning pipelines in Azure Machine Learning and/or Fabric, Hands-on experience in deploying machine learning pipelines with Azure Machine Learning SDK\nWorking knowledge of mathematics (primarily linear algebra, probability, statistics), and algorithms.\nProficient at orchestrating large-scale ML/DL jobs, leveraging big data tooling and modern container orchestration infrastructure, to tackle distributed training and massive parallel model executions on cloud infrastructure.\nExperience designing custom APIs for machine learning models for training and inference processes and designing, implementing, and delivering frameworks for MLOps.\nExperience with model lifecycle management and automation to support retraining and model monitoring\nExperience implementing and incorporating ML models on unstructured data using cognitive services and/or computer vision as part of AI solutions and workflows.\nHistory of working with large scale model optimization and hyperparameter tuning, applied to ML/DL models.\nKnowledge of enterprise SaaS complexities including security/access control, scalability, high availability, concurrency, online diagnoses, deployment, upgrade/migration, internationalization, and production support.\nKnowledge of data engineering and transformation tools and patterns such as Databricks, Spark, Azure Data Factory\nAbility to engage other technical experts at all organizational levels and assess opportunities to apply machine learning and analytics to improve business workflows and deliver information and insight to support business decisions.\nAbility to communicate in a clear, concise, and understandable manner both orally and in writing.\n\nChevron ENGINE supports global operations, supporting business requirements across the world. Accordingly, the work hours for employees will be aligned to support business requirements. The standard work week will be Monday to Friday. Working hours are 8:00am to 5:00pm or 1.30pm to 10.30pm.\nChevron participates in E-Verify in certain locations as required by law.",Industry Type: Oil & Gas,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Computer vision', 'Automation', 'Design review', 'Analytical', 'Architectural design', 'Machine learning', 'Monitoring', 'Analytics', 'Python']",2025-06-14 06:08:30
Analytics Engineer,Merck Sharp & Dohme (MSD),5 - 10 years,Not Disclosed,['Pune'],"Analytics Engineer\nWe are seeking a talented, motivated and self-driven professional to join the HH Digital, Data & Analytics (HHDDA) organization and play an active role in Human Health transformation journey to become the premier Data First commercial biopharma organization.\nAs a Analytics Engineer, you will be part of the HHDDA Commercial Data Solutions team, providing technical/data expertise development of analytical data products to enable data science & analytics use cases. In this role, you will create and maintain data assets/domains used in the commercial/marketing analytics space - to develop best-in-class data pipelines and products, working closely with data product owners to translate data product requirements and user stories into development activities throughout all phases of design, planning, execution, testing, deployment and delivery.\nYour specific responsibilities will include:\nHands-on development of last-mile data products using the most up-to-date technologies and software / data / DevOps engineering practices\nEnable data science & analytics teams to drive data modeling and feature engineering activities aligned with business questions and utilizing datasets in an optimal way\nDevelop deep domain expertise and business acumen to ensure that all specificalities and pitfalls of data sources are accounted for\nBuild data products based on automated data models, aligned with use case requirements, and advise data scientists, analysts and visualization developers on how to use these data models\nDevelop analytical data products for reusability, governance and compliance by design\nAlign with organization strategy and implement semantic layer for analytics data products\nSupport data stewards and other engineers in maintaining data catalogs, data quality measures and governance frameworks\nEducation:\nB.Tech / B.S., M.Tech / M.S. or PhD in Engineering, Computer Science, Engineering, Pharmaceuticals, Healthcare, Data Science, Business, or related field\nRequired experience:\n5+ years of relevant work experience in the pharmaceutical/life sciences industry, with demonstrated hands-on experience in analyzing, modeling and extracting insights from commercial/marketing analytics datasets (specifically, real-world datasets)\nHigh proficiency in SQL, Python and AWS\nGood understanding and comprehension of the requirements provided by Data Product Owner and Lead Analytics Engineer\nExperience creating / adopting data models to meet requirements from Marketing, Data Science, Visualization stakeholders\nExperience with including feature engineering\nExperience with cloud-based (AWS / GCP / Azure) data management platforms and typical storage/compute services (Databricks, Snowflake, Redshift, etc.)\nExperience with modern data stack tools such as Matillion, Starburst, ThoughtSpot and low-code tools (e.g. Dataiku)\nExcellent interpersonal and communication skills, with the ability to quickly establish productive working relationships with a variety of stakeholders\nExperience in analytics use cases of pharmaceutical products and vaccines\nExperience in market analytics and related use cases\nPreferred experience:\nExperience in analytics use cases focused on informing marketing strategies and commercial execution of pharmaceutical products and vaccines\nExperience with Agile ways of working, leading or working as part of scrum teams\nCertifications in AWS and/or modern data technologies\nKnowledge of the commercial/marketing analytics data landscape and key data sources/vendors\nExperience in building data models for data science and visualization/reporting products, in collaboration with data scientists, report developers and business stakeholders\nExperience with data visualization technologies (e.g, PowerBI)\nCurrent Employees apply HERE\nCurrent Contingent Workers apply HERE\nSearch Firm Representatives Please Read Carefully\nEmployee Status:\nRegular\nRelocation:\nVISA Sponsorship:\nTravel Requirements:\nFlexible Work Arrangements:\nHybrid\nShift:\nValid Driving License:\nHazardous Material(s):\n\nRequired Skills:\nBusiness Intelligence (BI), Data Management, Data Modeling, Data Visualization, Measurement Analysis, Stakeholder Relationship Management, Waterfall Model\n\nPreferred Skills:\nJob Posting End Date:\n07/31/2025\n*A job posting is effective until 11:59:59PM on the day BEFORE the listed job posting end date. Please ensure you apply to a job posting no later than the day BEFORE the job posting end date.",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Relationship management', 'Computer science', 'Data management', 'Data modeling', 'Analytical', 'Pharma', 'Healthcare', 'Business intelligence', 'SQL', 'Python']",2025-06-14 06:08:32
Business Analyst - MDM,Leading Client,5 - 7 years,Not Disclosed,['Chennai'],"Business Analyst focused on designing and implementing a Data Governance strategy and Master Data Management (MDM) framework.\n\nThis role will support the high-level design and detailed design phases of a transformative project involving systems such as 3DS PLM, SAP, Team Centre, and Blue Yonder.\n\nThe ideal candidate will bring a blend of business analysis expertise, data governance knowledge, and automotive/manufacturing domain experience to drive workshops, map processes, and deliver actionable recommendations.\n\nWorking closely with the GM of Master Data and MDM technical resources, you will play a pivotal role in aligning people, processes, and technology to achieve M&M's data governance and MDM objectives.\n\nKey Responsibilities :\n\n- Requirements Gathering & Workshops: Lead and facilitate workshops with business and IT stakeholders to elicit requirements, define data governance policies, and establish MDM strategies for automotive specific data domains (e.g. , parts, engineering data, bill of material, service parts, supplier and dealer master data).\n\n- Process Mapping & Design: Document and design master data-related processes, including data flows between systems such as 3DS, SAP, Talend, and Blue Yonder, ensuring alignment with business needs and technical feasibility.\n\n- Analysis & Recommendations: Analyse existing data structures, processes, and system integrations to identify gaps and opportunities; provide clear, actionable recommendations to support Data Governance and MDM strategy.\n\n- Stakeholder Collaboration: Act as a bridge between business units, IT teams, and technical resources (e.g. , 3DS specialists) to ensure cohesive delivery of the project objectives.\n\n- Documentation & Communication: Create high-quality deliverables, including process maps, requirement specifications, governance frameworks, and summary reports, tailored to both technical and non-technical audiences.\n\n- Support Detailed Design: Collaborate with the 3DS/Talend technical resource to translate high-level designs into detailed MDM solutions, ensuring consistency across people, process, and technology components.\n\n- Project Support: Assist the MDM Leadership in planning, tracking, and executing project milestones, adapting to evolving client needs.\n\nExperience :\n\nRequired Skills & Qualifications :\n\n- 5+ years of experience as a Business Analyst, with a focus on data governance, master data management (MDM) such as Talend, Informatica, Reltio etc.\n\n- Proven track record of working on auto/manufacturing industry projects, ideally with exposure to systems like 3DS, Team Centre, SAP S/4HANA, MDG, or Blue Yonder.\n\nTechnical Knowledge :\n\n- Strong understanding of MDM concepts, data flows, and governance frameworks.\n\n- Familiarity with auto-specific data domains (e.g. , ECCMA/E-Class Schema).\n\n- Experience with process modelling tools (e.g. , Visio, Lucid chart, or BPMN) and documentation standards.\n\nSoft Skills :\n\n- Exceptional communication and facilitation skills, with the ability to engage diverse stakeholders and drive consensus in workshops.\n\n- Methodical and structured approach to problem-solving and project delivery.\n\n- Ability to summarize complex information into clear, concise recommendations.\n\n- Education: Bachelor's degree in business, Information Systems, or a related field (or equivalent experience).\n\n- Certifications: Relevant certifications (e.g. , CBAP, PMP, or MDM-specific credentials) are a plus but not required.\n\nPreferred Qualifications :\n\n- Prior consulting experience in a client-facing role.\n\n- Hands-on experience with MDG, Talend, Informatica, Reltio etc. or similar MDM platforms.\n\n- Exposure to data quality analysis or profiling (not required to be at a Data Analyst level).",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['MDM', 'PLM', 'BPMN', 'S/4 HANA', 'Project Management', 'SAP MDG', 'Informatica', 'Talend', 'Data Governance', 'Master Data Services', 'Business Analysis']",2025-06-14 06:08:35
Senior Machine Learning Engineer - NLP / Python,Maimsd Technology,4 - 7 years,Not Disclosed,"['Pune', 'Bengaluru']","Notice Period : Immediate - 15 Days\n\nAbout the Role :\n\nWe are seeking a highly skilled Senior Data Scientist to join our team and contribute to cutting-edge projects. As a key member of our data science team, you will leverage your expertise in machine learning, natural language processing, and data engineering to develop innovative solutions that drive business value.\n\nResponsibilities :\n\n- Machine Learning Model Development : Design, develop, and deploy advanced machine learning models, focusing on natural language processing tasks such as text classification, sentiment analysis, and language generation.\n\n- Data Engineering : Extract, transform, and load (ETL) data from various sources, ensuring data quality and consistency.\n\n- NLP Techniques : Apply state-of-the-art NLP techniques, including language models and text processing, to solve complex problems.\n\n- Python and ML Frameworks : Utilize Python programming language and popular ML frameworks like PyTorch or TensorFlow to build efficient and scalable models.\n\n- Cloud and Containerization : Leverage cloud platforms (GCP, AWS) and containerization technologies (Docker) for efficient deployment and management of ML models.\n\n- Database Management : Work with relational databases (Postgres, MySQL) to store, manage, and query large datasets.\n\n- Knowledge Graphs and ML Publications : Contribute to the development of knowledge graphs and stay updated with the latest advancements in the field through research and publications.\n\nQualifications :\n\nExperience : 4-7 years of hands-on experience in data science, with a strong focus on machine learning and natural language processing.\n\nTechnical Skills :\n\n- Proficiency in Python programming language.\n\n- Deep understanding of machine learning algorithms and techniques.\n\n- Expertise in NLP, including language models and text processing.\n\n- Familiarity with ML frameworks like PyTorch or TensorFlow.\n\n- Experience with cloud platforms (GCP, AWS) and containerization (Docker).\n\n- Knowledge of relational databases (Postgres, MySQL).\n\nSoft Skills :\n\n- Strong problem-solving and analytical skills.\n\n- Excellent communication and collaboration abilities.\n\n- Ability to work independently and as part of a team.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Machine Learning', 'Data Science', 'Azure', 'NLP', 'PyTorch', 'GCP', 'MySQL', 'Postgres', 'AWS', 'TensorFlow', 'Python']",2025-06-14 06:08:37
Test Engineer,Blend360 India,5 - 8 years,Not Disclosed,['Hyderabad'],"Job Description\nWe are looking for a skilled Test Engineer with experience in automated testing, rollback testing, and continuous integration environments. You will be responsible for ensuring the quality and reliability of our software products through automated testing strategies and robust test frameworks\nDesign and execute end-to-end test strategies for data pipelines, ETL/ELT jobs, and database systems.\nValidate data quality, completeness, transformation logic, and integrity across distributed data systems (e.g., Hadoop, Spark, Hive).\nDevelop Python-based automated test scripts to validate data flows, schema validations, and business rules.\nPerform complex SQL queries to verify large datasets across staging and production environments.\nIdentify data issues and work closely with data engineers to resolve discrepancies.\nContribute to test data management, environment setup, and regression testing processes.\nWork collaboratively with data engineers, business analysts, and QA leads to ensure accurate and timely data delivery.\nParticipate in sprint planning, reviews, and defect triaging as part of the Agile process.\n\n\nQualifications\n4+ of experience in Data Testing, Big Data Testing, and/or Database Testing.\nStrong programming skills in Python for automation and scripting.\nExpertise in",Industry Type: Advertising & Marketing,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Automation', 'Version control', 'Data management', 'Test scripts', 'Agile', 'Regression testing', 'Data quality', 'JIRA', 'SQL', 'Python']",2025-06-14 06:08:39
"Specialist , Digital Supply Chain AWS DevOps Engineer",Merck Sharp & Dohme (MSD),3 - 8 years,Not Disclosed,['Hyderabad'],"The Opportunity\nBased in Hyderabad, join a global healthcare biopharma company and be part of a 130- year legacy of success backed by ethical integrity, forward momentum, and an inspiring mission to achieve new milestones in global healthcare.\nBe part of an organization driven by digital technology and data-backed approaches that support a diversified portfolio of prescription medicines, vaccines, and animal health products. Drive innovation and execution excellence.\nBe a part of a team with passion for using data, analytics, and insights to drive decision-making, and which creates custom software, allowing us to tackle some of the worlds greatest health threats.\nOur Technology Centers focus on creating a space where teams can come together to deliver business solutions that save and improve lives. An integral part of our company s IT operating model, Tech Centers are globally distributed locations where each IT division has employees to enable our digital transformation journey and drive business outcomes. These locations, in addition to the other sites, are essential to supporting our business and strategy.\nA focused group of leaders in each Tech Center helps to ensure we can manage and improve each location, from investing in growth, success, and well-being of our people, to making sure colleagues from each IT division feel a sense of belonging to managing critical emergencies. And together, we must leverage the strength of our team to collaborate globally to optimize connections and share best practices across the Tech Centers.\nRole Overview\nAs a Software Engineer you will design, develop, and maintain software systems. This role involves both creative and analytical skills to solve complex problems and create efficient, reliable software. You will use your expertise in requirements analysis, programming languages, software development methodologies, and tools to build and deliver software products that meet the needs of businesses, organizations, or end- users. You will work with other engineers, product managers and delivery leads, to design systems, determine functional and non-functional needs and implement solutions accordingly. You should be ready to work independently as well as in a team.\nWhat will you do in this role\nDesign, code, verify, test, document, amend and refactor moderately complex applications and software configurations for deployment in collaboration with cross-disciplinary teams across various regions worldwide\nDesign test cases and test scripts under own direction, mapping back to pre-determined criteria, recording and reporting test outcomes. Participate in requirement, design and specification reviews. Perform manual and automation testing.\nElicit requirements for systems and software life cycle working practices and automation. Prepare design options for the working environment of methods, procedures, techniques, tools, and people. Utilize systems and software life cycle working practices for software components and micro-services. Deploy automation to achieve well-engineered and secure outcome.\nWork within a matrix organizational structure, reporting to both the functional manager and the Product manager.\nParticipate in Product planning, execution, and delivery, ensuring alignment with both functional and Product goals.\nWhat should you have\nBachelor s degree in information technology, Computer Science or any Technology stream.\n3+ years of hands-on experience working with technologies - Terraform, Ansible and at least one programming language from our supported stack (TypeScript / Node)\nFamiliarity with modern product development practices - Agile, Scrum, test driven development, UX, design thinking.\nHands-on experience with DevOps practices (Git, infrastructure as code, observability, continuous integration/continuous deployment - CI/CD).\nPossesses both theoretical and practical knowledge, with the ability to autonomously implement given tasks, including producing and deploying pieces of code.\nCloud-native, ideally AWS certified.\nRelease Manager Experience\nProduct and customer-centric approach.\nOur technology teams operate as business partners, proposing ideas and innovative solutions that enable new organizational capabilities. We collaborate internationally to deliver services and solutions that help everyone be more productive and enable innovation.\nCurrent Employees apply HERE\nCurrent Contingent Workers apply HERE\nSearch Firm Representatives Please Read Carefully\nEmployee Status:\nRegular\nRelocation:\nVISA Sponsorship:\nTravel Requirements:\nFlexible Work Arrangements:\nHybrid\nShift:\nValid Driving License:\nHazardous Material(s):\n\nRequired Skills:\nAnimal Vaccination, Business, Business Management, CI/CD, Computer Science, Continuous Deployment, Continuous Integrations, Data Engineering, Data Visualization, Design Applications, DevOps, Digital Supply Chain Management, Digital Transformation, Information Technology Operations, Requirements Analysis, Software Configurations, Software Development, Software Development Life Cycle (SDLC), Solution Architecture, Supply Chain Optimization, Supply Management, System Designs, Systems Integration, Test Automation, Testing",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Solution architecture', 'Computer science', 'CVS', 'Supply chain management', 'devops', 'Software development life cycle', 'Healthcare', 'data visualization', 'Information technology', 'digital transformation']",2025-06-14 06:08:41
"Specialist, Java/Angular JS Engineer",Merck Sharp & Dohme (MSD),5 - 10 years,Not Disclosed,['Hyderabad'],"The Opportunity\nBased in Hyderabad, join a global healthcare biopharma company and be part of a 130- year legacy of success backed by ethical integrity, forward momentum, and an inspiring mission to achieve new milestones in global healthcare.\nBe part of an organization driven by digital technology and data-backed approaches that support a diversified portfolio of prescription medicines, vaccines, and animal health products.\nDrive innovation and execution excellence. Be a part of a team with passion for using data, analytics, and insights to drive decision-making, and which creates custom software, allowing us to tackle some of the worlds greatest health threats.\nOur Technology Centers focus on creating a space where teams can come together to deliver business solutions that save and improve lives. An integral part of our company s IT operating model, Tech Centers are globally distributed locations where each IT division has employees to enable our digital transformation journey and drive business outcomes. These locations, in addition to the other sites, are essential to supporting our business and strategy.\nA focused group of leaders in each Tech Center helps to ensure we can manage and improve each location, from investing in growth, success, and well-being of our people, to making sure colleagues from each IT division feel a sense of belonging to managing critical emergencies. And together, we must leverage the strength of our team to collaborate globally to optimize connections and share best practices across the Tech Centers.\nRole Overview\nWe are seeking a highly skilled and experienced Senior Multistack Java Developer to join our team and technically lead a group of talented developers. This role will involve close collaboration with a large team, focusing on enhancing, developing, and maintaining enterprise-grade applications. The ideal candidate will have 8-15 years of strong hands-on experience in hardcore development, expertise in Java,and front-end frameworks like React and Angular, in a continuous release environment. This position also requires proactive leadership, and expertise in managing production outages and troubleshooting.\nAs a Software Engineer focusing on Java/Angular JS you will design, develop, and maintain software systems. This role involves both creative and analytical skills to solve complex problems and create efficient, reliable software. You will use your expertise in requirements analysis, programming languages, software development methodologies, and tools to build and deliver software products that meet the needs of businesses, organizations, or end- users. You will work with other engineers, product managers and delivery leads, to design systems, determine functional and non-functional needs and implement solutions accordingly. You should be ready to work independently as well as in a team.\nWhat will you do in this role\nDesign, code, verify, test, document, amend and refactor moderately complex applications and software configurations for deployment in collaboration with cross-disciplinary teams across various regions worldwide.\nDesign test cases and test scripts under own direction, mapping back to pre-determined criteria, recording and reporting test outcomes. Participate in requirement, design and specification reviews. Perform manual and automation testing.\nElicit requirements for systems and software life cycle working practices and automation. Prepare design options for the working environment of methods, procedures, techniques, tools, and people. Utilize systems and software life cycle working practices for software components and micro-services. Deploy automation to achieve well-engineered and secure outcome.\nWork within a matrix organizational structure, reporting to both the functional manager and the Product manager.\nParticipate in Product planning, execution, and delivery, ensuring alignment with both functional and Product goals.\nLead and mentor a team of Java Microservices and front-end developers, providing technical guidance, code reviews, and fostering a culture of continuous learning.\nStay actively involved in development tasks, ensuring the delivery of high-quality, maintainable code.\nDesign and develop scalable systems using Java, Spring Framework, and Spring Boot for backend development.\nDevelop rich, responsive user interfaces using React, Angular to deliver seamless user experiences.\nEnsure optimal integration of front-end and back-end components for customer-facing applications.\nDevelop and consume Microservices, REST/SOAP APIs, and XML web services.\nUtilize tools like Splunk, ELK, and database queries to monitor, troubleshoot, and resolve system issues.\nLeverage tools like Jenkins, Ansible, and Terraform for automated deployments.\nWork on cloud platforms like AWS, implementing and supporting cloud-based solutions.\nWhat should you have\nBachelor s degree in information technology, Computer Science or any Technology stream.\n~5-10 years of experience. 3+ years of experience with Java and Java frameworks and libraries (such as Spring Framework, Spring Boot, Hibernate, JEE, JDBC, JMS, JMX), relational databases, Kafka/RabbitMQ, and deployment to application servers.\nDevelop and maintain microservices-based applications using Spring Boot.\nDesign and implement RESTful APIs for seamless communication between services.\nExperience with enterprise-level databases both SQL and No-SQL such as Oracle, PostgreSQL, MongoDB etc\nAutomated testing frameworks (JUnit, TestNG, Selenium, etc.)\nMonitoring and log tools (Splunk, ELK, Prometheus, Grafana)\nFamiliarity with AWS Cloud, CI/CD pipelines(Jenkins, GitLab CI, etc.), and automation tools such as Jenkins, Ansible, and Terraform.\nProficiency in cloud platforms (e.g., AWS, Azure, Google Cloud) and containerization technologies (e.g., Docker, Kubernetes)..\nExcellent debugging, troubleshooting, and analytical skills.\nEffective verbal and written communication skills.\nFamiliarity with modern product development practices - Agile, Scrum, test driven development, UX, design thinking.\nHands-on experience with DevOps practices (Git, infrastructure as code, observability, continuous integration/continuous deployment - CI/CD).\nCloud-native, ideally AWS certified.\nProduct and customer-centric approach\nGood to Have Skills:\nExperience with any front-end technologies like React, Angular for building responsive user interfaces.\nUnderstanding of security best practices and data protection methodologies.\nExperience with Agile methodologies and tools like Jira or Trello.\nCertification of any of public cloud (AWS, GCP, Azure, OCI)\nA passionate commitment to learning about business domains and emerging technologies.\n\nOur technology teams operate as business partners, proposing ideas and innovative solutions that enable new organizational capabilities. We collaborate internationally to deliver services and solutions that help everyone be more productive and enable innovation.\nWho we are\nFor more than a century, we have been inventing for life, bringing forward medicines and vaccines for many of the worlds most challenging diseases. Today, our company continues to be at the forefront of research to deliver innovative health solutions and advance the prevention and treatment of diseases that threaten people and animals around the world.\nWhat we look for\nImagine getting up in the morning for a job as important as helping to save and improve lives around the world. Here, you have that opportunity. You can put your empathy, creativity, digital mastery, or scientific genius to work in collaboration with a diverse group of colleagues who pursue and bring hope to countless people who are battling some of the most challenging diseases of our time. Our team is constantly evolving, so if you are among the intellectually curious, join us and start making your impact today\nCurrent Employees apply HERE\nCurrent Contingent Workers apply HERE\nSearch Firm Representatives Please Read Carefully\nEmployee Status:\nRegular\nRelocation:\nVISA Sponsorship:\nTravel Requirements:\nFlexible Work Arrangements:\nHybrid\nShift:\nValid Driving License:\nHazardous Material(s):\n\nRequired Skills:\nAnimal Vaccination, Computer Science, Data Engineering, Data Visualization, Design, Design Applications, DevOps, Digital Technology, Digital Transformation, JavaScript, PostgreSQL, Requirements Analysis, Social Collaboration, Software Configurations, Software Development, Software Development Life Cycle (SDLC), Solution Architecture, Spring Framework, System Designs, Systems Integration, Systems Troubleshooting, Technical Consulting, Test Automation, Testing, Web Development",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Solution architecture', 'JMS', 'Hibernate', 'Front end', 'XML', 'Consulting', 'Healthcare', 'data visualization', 'Oracle', 'SQL']",2025-06-14 06:08:43
COE Analyst,Career Guideline,1 - 6 years,9-18 Lacs P.A.,['Bengaluru'],Designation - COE Analyst\nJob Function - FP&A / Portfolio Management / Valuation Reporting .\nExperience - 1-3 years\nSalary - 9 - 13 Lpa\nLocation -Bengaluru\nNotice - Immediate Joiner,Industry Type: Financial Services,Department: Finance & Accounting,"Employment Type: Full Time, Permanent","['Excel', 'Dcf Valuation', 'Financial Modelling', 'MIS Reporting', 'Portfolio Analyst', 'Discounted Cash Flow', 'FPA', 'Data Analysis', 'Coe', 'Investment Analyst', 'Reporting And Analysis', 'Financial Analyst', 'Dashboard Development', 'Valuation Analysis']",2025-06-14 06:08:46
ML Engineer,Squarepeg Hires,3 - 8 years,Not Disclosed,['Bengaluru'],"Voicecare AI is a Healthcare Administration General Intelligence (HAGI) company for the back-office and the RCM industry. We are building a safety focused large and small conversational language model for the healthcare industry. Our mission is to dramatically improve access, adherence, and outcomes for the patients and the healthcare workforce through the application of generative AI. We are a venture-backed company partnering with the top healthcare stakeholders in the country.\n\nWe are seeking a Machine Learning NLP Engineer to drive the development of large language models (LLMs), deep learning architectures, and speech processing systems. This role is critical for AI-driven healthcare solutions while ensuring high performance, accuracy, and compliance with healthcare standards.\n\nResponsibilities:\nMachine Learning Deep Learning Development\nDesign and implement machine learning algorithms for healthcare AI applications.\nBuild and optimize deep learning models to enhance AI-driven decision-making processes.\nLLM Fine-Tuning Training\nDevelop and fine-tune and train large and small language models to meet domain-specific requirements.\nDesign robust data preparation and training pipelines for efficient model performance.\nLLM Architecture Expertise\nWork with state-of-the-art LLM architectures such as Transformers, GPT models, LLaMA, and others\nResearch and implement novel enhancements to existing language models for better contextual understanding.\nSpeech NLP Project Execution\nImplement speech-to-text and text-to-speech models for healthcare applications.\nDevelop NLP solutions for intent prediction, sentiment analysis, and medical language translation.\nProgramming Development\nWrite and maintain high-performance Python code for ML/NLP applications.\nUtilize frameworks like PyTorch and TensorFlow to train and deploy AI models.\nAdditional Contributions (Optional but Preferred)\nApply advanced prompt engineering techniques for optimizing LLM interactions.\nDeploy ML models on cloud platforms (AWS, GCP, Azure) for scalability.\nImplement MLOps best practices, including CI/CD pipelines for ML and model monitoring.\nSkills and Experience:\nMachine Learning Deep Learning: Strong experience in designing, training, and deploying ML/DL models.\nLLM Expertise: Hands-on experience in fine-tuning and optimizing LLMs.\nNLP Speech Processing: Experience in developing NLP solutions and speech models.\nProgramming: Advanced Python skills with proficiency in PyTorch and TensorFlow.\nLLM Architecture: Deep Knowledge of modern transformer-based models.\nMLOps Cloud: Demonstrated experience with cloud deployment, CI/CD for ML, and model monitoring.\nQualifications:\nBachelor s or Master s degree in Computer Science, Artificial Intelligence, Data Science, or a related field.\n3+ years of experience in ML, NLP, or AI-driven model development.\nProven track record of deploying LLM-based solutions in real-world applications.\nExperience working in healthcare AI, health tech startups, or regulated industries is a plus.\nRelevant certifications preferred (e.g., Google Professional ML Engineer, AWS Certified Machine Learning - Specialty).\nCandidates located in Bangalore is a plus\nCandidates from top universities is a plus",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['deep learning', 'GCP', 'Artificial Intelligence', 'Machine learning', 'Back office', 'Healthcare', 'AWS', 'Monitoring', 'Python']",2025-06-14 06:08:48
ML Engineer,Acenet,4 - 6 years,Not Disclosed,"['Pune', 'Gurugram', 'Bengaluru']","About Us:\nAceNet Consulting is a fast-growing global business and technology consulting firm specializing in business strategy, digital transformation, technology consulting, product development, start-up advisory and fund-raising services to our global clients across banking financial services, healthcare, supply chain logistics, consumer retail, manufacturing, eGovernance and other industry sectors.\nWe are looking for hungry, highly skilled and motivated individuals to join our dynamic team. If you re passionate about technology and thrive in a fast-paced environment, we want to hear from you.\n\n\nJob Summary :\nWe are seeking a skilled and motivated Machine Learning Engineer with 4 years of experience to join our growing team. You will play a key role in designing, building, and deploying ML models at scale, with a strong focus on cloud-native tools and production readiness.\n\n\nKey Responsibilities :\n*Design, develop, and deploy robust machine learning models using Python.\n*Write and optimize SQL queries for data extraction, transformation, and analysis.\n*Experience with AWS SageMaker or at least an understanding of containerised deployment and Lambda.\n*Collaborate with data scientists, software engineers, and DevOps teams to productionize ML solutions.\n\n\nRole Requirements and Qualifications :\n*Experience with Generative AI tools and frameworks (e.g., LangChain, RAG pipelines, vector databases).\n*Familiarity with DevOps/MLOps practices including Infrastructure as Code (IaC) using cdk.\n*Experience of deployments on ECS (Fargate).\n*Strong problem-solving skills and the ability to work independently.\n\n\nWhy Join Us:\n*Opportunities to work on transformative projects, cutting-edge technology and innovative solutions with leading global firms across industry sectors.\n*Continuous investment in employee growth and professional development with a strong focus on up re-skilling.\n*Competitive compensation benefits, ESOPs and international assignments.\n*Supportive environment with healthy work-life balance and a focus on employee well-being.\n*Open culture that values diverse perspectives, encourages transparent communication and rewards contributions.\n\n\nHow to Apply:\nIf you are interested in joining our team and meet the qualifications listed above, please apply and submit your resume highlighting why you are the ideal candidate for this position.",Industry Type: Management Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Supply chain', 'Machine learning', 'Manager Technology', 'Healthcare', 'Business strategy', 'Fund raising', 'Financial services', 'Python', 'Logistics', 'Data extraction']",2025-06-14 06:08:50
Business Analyst,Product Based Company,5 - 10 years,Not Disclosed,['Bengaluru'],"We are hiring a Business Analyst for the Bangalore location in a Hybrid mode.\n\nRole Overview\nThe Business Analyst role is focused on the clinical domain, requiring strong knowledge of clinical trials, clinical research associates, investigators, and subjects (patients). The candidate should be familiar with Electronic Data Capture (EDC) systems and Clinical Research Forms (CRFs).\n\n\nKey Requirements and Skills:\n\nSQL Knowledge: The candidate should have a basic understanding of SQL queries.\nClinical Domain Expertise: Strong knowledge in clinical trials and related processes.\nBusiness Analyst Skills: Understanding of business goals, Agile methodology, and waterfall methodology.\nTechnical Skills: Experience with JIRA, TopTeam, SynapseRT, and proficiency in Microsoft Office applications (Word, Excel, PowerPoint, Project, and Visio).\nWriting Skills: Strong technical and business writing skills, including knowledge of technical and grammatical editing.\nCommunication Skills: Ability to communicate technical concepts to non-technical audiences and business concepts to technical audiences.\nModelling Languages: Ability to learn and use modelling languages when required.\nNegotiation and Customer Management: Skills in negotiation and customer management.\n\nIf interested, drop your profile at nusrath.begum@priglobal.com along with the following details:\nTotal Experience:\nCurrent CTC:\nExpected CTC:\nNotice Period:",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Temporary/Contractual","['Requirement Gathering', 'Clinical', 'JIRA', 'Business Analysis', 'SQL', 'Use Cases', 'Clinical Research', 'User Stories', 'Clinical Trials', 'Clinical Data']",2025-06-14 06:08:52
Business Analyst,Pinnacle Group Inc,2 - 5 years,Not Disclosed,"['Kolkata', 'Mumbai', 'New Delhi', 'Hyderabad', 'Pune', 'Chennai', 'Bengaluru']","Pinnacle Group exists to connect people with opportunity. For the last 25 years, weve done exactly that by living our core values of putting people first, delivering excellence in all we do, and giving back to the communities in which we live and work. We are a leading workforce solutions company supporting the talent needs of global leaders in financial services, technology, communications, utilities, and transportation and we are one of the largest women and minority-owned companies in our industry. Our team of service-driven, energetic, and diverse professionals is well-respected in our industry and our leadership team is aligned and focused on taking the company to the next level. If youre looking for a new opportunity where you can truly make a difference, we hope youll apply for a position with us.\nJob Summary\nJob Description\nPinnacle Group is seeking a strong analytical resource who will be responsible for maintaining and creating analysis and data visuals that will enable leaders of the program to make strategic decisions. The ideal candidate will champion change and effectively manage the implementation of new ideas. Proactively communicate and collaborate with external and internal stakeholders to analyze information needs and functional requirements. Participate in planning, needs analysis and risk assessment; consult with stakeholders on how to best support their data use through the effective use of technology. Ability to take large data sets and present to leadership opportunities and strategies in a condensed manner. Candidate will also be responsible for presenting actionable insights at monthly and quarterly program review sessions.\nQualifications\nBachelor s degree in MIS, Information Technology, Computer Science, or other quantitative major. An understanding of basic accounting/finance is a plus\nData management and analysis experience (2+ years working with databases and creating data visualizations for fortune 500 or top tier consulting company)\nExperienced in developing analytics visualizations. Strong knowledge of Tableau and intermediate to advanced Database and T-SQL skills\nExperience in creating ETL processes\nExperience in Python, C#, PowerShell, SSIS packages, and Visual Studio is a plus but not required\nExperience in creating and presenting presentations decks to external audiences\nStrong skillset to develop insightful dashboards and reports that drive business decision making and outcomes\nStrong attention to detail is a must\nDemonstrated ability to communicate with and work well with all levels within an organization\nTechnical expertise regarding data models, database design development, data mining and segmentation techniques\nStrong written and verbal communication skills",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'Data management', 'Database design', 'MIS', 'Visual Studio', 'SSIS', 'Data mining', 'Information technology', 'Analytics', 'Python']",2025-06-14 06:08:54
Senior Business Analyst - Immediate Joiner,Service based Top B2B MNC in IT Services...,9 - 14 years,30-35 Lacs P.A.,"['Noida', 'Gurugram', 'Delhi / NCR']","*ONLY IMMEDIATE JOINERS*\n\nRequirement : Senior Business Analyst (Data Application & Integration)\nExperience: 9+ Years\nLocation: Gurgaon (Hybrid)\n\nJob Summary:\nWe are seeking an experienced Senior Business Analyst (Data Application & Integration) to drive key data and integration initiatives. The ideal candidate will have a strong business analysis background and a deep understanding of data applications, API integrations, and cloud-based platforms like Salesforce, Informatica, DBT, IICS, and Snowflake.\n\nKey Responsibilities:\nGather, document, and analyse business requirements for data application and integration projects.\nWork closely with business stakeholders to translate business needs into technical solutions.\nDesign and oversee API integrations to ensure seamless data flow across platforms.\nCollaborate with cross-functional teams including developers, data engineers, and architects.\nDefine and maintain data integration strategies, ensuring high availability and security.\nWork on Salesforce, Informatica, and Snowflake to streamline data management and analytics.\nDevelop use cases, process flows, and documentation to support business and technical teams.\nEnsure compliance with data governance and security best practices.\nAct as a liaison between business teams and technical teams, providing insights and recommendations.\n\nKey Skills & Requirements:\nStrong expertise in business analysis methodologies and data-driven decision-making.\nHands-on experience with API integration and data application management.\nProficiency in Salesforce, Informatica, DBT, IICS, and Snowflake.\nStrong analytical and problem-solving skills.\nAbility to work in an Agile environment and collaborate with multi-functional teams.\nExcellent communication and stakeholder management skills.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Snowflake', 'Informatica', 'Data Integration', 'Salesforce']",2025-06-14 06:08:57
Staff Machine Learning Engineer,Xoom,10 - 15 years,Not Disclosed,['Bengaluru'],"Job Summary\nWhat you need to know about the role\nThis role leads the design, development, and implementation of highly scalable and advanced Generative AI solutions that can be leveraged across the organization. You will collaborate closely with the Architect, Product Managers, and development teams to deliver top-quality products with high security through innovative AI/ML solutions.\n\nMeet our team\nYou ll be part of a team committed to solving challenges to deliver exceptional results. You will get an opportunity to work with latest AI technologies and frameworks that can make a wider impact. We encourage innovation and creative problem-solving approaches to deliver. If you are a passionate engineer who likes to get on to the ground with the roll-up-the-sleeves attitude, this would be your right place\nJob Description\nYour way to impact\nWe are seeking a highly skilled Machine Learning Engineering Lead with expertise in building scalable AI enterprise solutions and a solid understanding of Generative AI and Prompt Engineering techniques. In this role, you will take ownership of architecting, designing, and leading development teams to deliver advanced Generative AI solutions that focus on technology improvements and intelligent systems, such as Technology Assistants, AI Agents, fine-tuned models with domain expertise. You will play a pivotal role in shaping the AI solutions architecture while collaborating with multidisciplinary teams.\nYour day to day\nLead the development and delivery of Generative AI solutions.\nKeep the team ahead with research and implementation innovative AI solutions that can make an organization-wide impact.\nCollaborate with cross-functional teams to ensure deliveries are done on with high-quality and great standards.\nWhat do you need to bring\nA Bachelors, Masters, or Ph.D. degree in Computer Science, Computer Engineering, or a related field.\nA minimum of 10 years of professional experience, including 5+ years of experience with ML frameworks, model training and deployment.\nMinimum 2 years of hands-on experience with Generative AI technologies, large language models (LLMs), prompt engineering techniques, and vector databases (e.g., Pinecone, Weaviate, Milvus, or similar).\nStrong proficiency with Python, and cloud infrastructure (e.g., AWS, Azure, GCP) is a must.\nExperience in architecting and building enterprise-scale systems, such as Knowledge Graphs or distributed AI platforms.\nExperience in fine-tuning LLM models to inject domain expertise and enhance ethical standards.\nExcellent communication skills to interface with both technical and non-technical stakeholders, enabling clear articulation of project requirements and outcomes.",Industry Type: Internet,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'GCP', 'Diversity and Inclusion', 'Machine learning', 'Design development', 'Manager Technology', 'Wellness', 'Architecting', 'Engineering Lead', 'Python']",2025-06-14 06:08:59
"Associate Manager, D&AI BIOps (Business Intelligence Operations)",Pepsico,7 - 12 years,Not Disclosed,['Hyderabad'],"Overview \n\nWe are seeking a detail-oriented and proactive Associate Manager BIOps Program Management to support and optimize Business Intelligence Operations (BIOps) programs. This role requires a hands-on professional with a solid foundation in BI governance, data analytics, cloud-based BI platforms, automation, and operational processes.\n\n Responsibilities \n\nThe ideal candidate will assist in implementing scalable BIOps strategies, improving BI platform performance, and ensuring the availability, reliability, and efficiency of enterprise analytics solutions.\n\n",,,,"['data management', 'power bi', 'sap bo', 'microstrategy', 'tableau', 'catalog', 'bi', 'microsoft azure', 'networking', 'strong interpersonal skills', 'team coordination', 'data acquisition', 'cpg', 'operational excellence', 'business operations', 'microsoft azure security', 'customer experience']",2025-06-14 06:09:02
Analyst/ Associate- Syndication,Fairdeal Corporate Advisors,2 - 5 years,15-20 Lacs P.A.,['Mumbai (All Areas)'],"Reporting to: Senior Vice President (SVP)\nAbout the role:\nEnd to end execution of Syndication process with respect to sell-down transactions in Primary/\nSecondary Syndication deals in the infrastructure and allied infrastructure sectors.\nResponsibilities:\n1. Preparation of Information Memorandum, teasers, pitch decks, mandate letters for Syndication proposals (Primary/ Secondary deals);\n2. Work on financial models for viability analysis and other case scenarios;\n3. Review of termsheets of potential Syndication proposals;\n4. Liaison with potential investors across hierarchy; Effective and timely addressal of credit queries of the investors;\n5. Coordinate and collaborate with internal and external stakeholders during the syndication process (including LIE, LLC, etc.) till the documentation/ disbursement stage;\n6. Maintain and deepen relationship with existing investors; Develop relationships with new\ninvestors;\n7. Maintain the Syndication database;\n8. Collate market intelligence and maintain database for the same;\n9. Preparation of periodic Syndication presentations to the management.\nRequirements:\nExperience 2-5 years, preferably from prior experience in bank or NBFC in Wholesale/\nInfrastructure financing\nEducational qualification: Postgraduate in Business management / CA\nFunctional Competencies\na) Strong finance and accounting fundamentals\nb) Strong analytical skills\nc) Strong modelling skills\nd) Excellent written and verbal communication skills\ne) Good Interpersonal skills\nf) Demonstrated academic excellence\nBehavioral Competencies:\na) Ability to handle execution (credit evaluation, financial modelling, Legal documentation)\nend to end, with minimal supervision\nb) Pleasant and balanced personality with relationship building mindset.",Industry Type: Real Estate,Department: Other,"Employment Type: Full Time, Permanent","['Maintain databse', 'internal and external stakeholders', 'Syndication deals in the infrastructure', 'Pitch Books', 'Syndication process', 'Syndication proposals', 'existing investors relation', 'Preparation of Information Memorandum', 'teasers', 'Financial Modelling']",2025-06-14 06:09:04
Business Analyst,Toae Security,2 - 5 years,Not Disclosed,"['Noida', 'Kanpur', 'Bengaluru']","TOAE Security is looking for Business Analyst to join our dynamic team and embark on a rewarding career journey\nEvaluating business processes, anticipating requirements, uncovering areas for improvement, and developing and implementing solutions.\nLeading ongoing reviews of business processes and developing optimization strategies.\nStaying up-to-date on the latest process and IT advancements to automate and modernize systems.\nConducting meetings and presentations to share ideas and findings.\nPerforming requirements analysis.\nDocumenting and communicating the results of your efforts.\nEffectively communicating your insights and plans to cross-functional team members and management.\nGathering critical information from meetings with various stakeholders and producing useful reports.\nWorking closely with clients, technicians, and managerial staff.\nProviding leadership, training, coaching, and guidance to junior staff.\nAllocating resources and maintaining cost efficiency.\nEnsuring solutions meet business needs and requirements.\nPerforming user acceptance testing.\nManaging projects, developing project plans, and monitoring performance.\nUpdating, implementing, and maintaining procedures.\nPrioritizing initiatives based on business needs and requirements.\nServing as a liaison between stakeholders and users.\nManaging competing resources and priorities.\nMonitoring deliverables and ensuring timely completion of projects.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent",['Business Analyst'],2025-06-14 06:09:06
ML-Ops - Engineer,Datametica,3 - 7 years,Not Disclosed,"['Pune', 'Bengaluru']","Job Summary:\nWe are looking for a talented MLOps Engineer with 4-7yrs to help design, build, and manage scalable infrastructure for deploying AI/ML and Generative AI models into production. You will be responsible for implementing and maintaining robust ML pipelines, CI/CD workfl ows, containerized deployments, and model monitoring systems. The ideal candidate will have strong experience in cloud-native MLOps practices, especially on GCP (preferred), and a solid understanding of modern machine learning workfl ows and tools.\n\n\nRoles & Responsibilities:\nML Pipelines & Automation\nDevelop and manage end-to-end ML pipelines for data processing, model training, testing, and deployment.\nAutomate model lifecycle using tools like MLfl ow, Kubefl ow, Airfl ow, or Vertex AI Pipelines.\nModel Deployment & Infrastructure\nPackage and deploy models using Docker, Kubernetes, and cloud-native platforms like GCP Vertex AI, Cloud Run, or SageMaker.\nImplement CI/CD pipelines using tools such as GitHub Actions, Cloud Build, or Jenkins for continuous model integration and delivery.\nMonitoring & Performance Optimization\nSet up monitoring systems for model drift, latency, accuracy, and resource utilization.\nImplement logging, alerting, and observability using tools like Prometheus, Grafana, or Cloud Logging.\nCollaboration & Support\nWork closely with AI/ML Architects, Data Scientists, and Software Engineers to ensure reproducibility, scalability, and reliability of AI solutions.\nSupport deployment of GenAI models and components (e.g., RAG pipelines, LLMs, embedding services).\nSecurity, Governance & Compliance\nEnsure secure and compliant handling of data and model artifacts.\nManage model versioning, lineage tracking, and audit logging in accordance with internal policies.\nRequired Skills & Qualifications:\n4-7 years of experience in MLOps, DevOps, or ML Engineering roles.\nStrong programming skills in Python and scripting for automation.\nHands-on with MLOps tools: MLfl ow, DVC, TFX, or Kubefl ow.\nExperience with cloud platforms: GCP (Vertex AI, Cloud Build, Artifact Registry), AWS (SageMaker, Lambda), or Azure ML.\nProfi ciency with containerization (Docker) and orchestration platforms (Kubernetes, Cloud Run).\nSolid experience in setting up and managing CI/CD pipelines for machine learning workflows.\nFamiliarity with data pipelines, ETL tools (Airfl ow, Datafl ow), and data validation tools.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['MLOps', 'GCP', 'Ci/Cd', 'Machine Learning', 'Operations', 'Python', 'Ml']",2025-06-14 06:09:09
TPRC Analyst,Vestas,2 - 7 years,Not Disclosed,['Chennai'],"Turbine Performance Reporting Centre (TPRC) major task is to deliver to customers across the globe reports in superior formats - related to wind turbine electrical power production.\nOn a daily basis, we are in cross-border dialogue with different cultures which makes our work even more interesting but also more complex.\nResponsibilities\nTurbine data preparation and verification for the purpose of performance reporting",,,,"['Service delivery', 'Business administration', 'SAP', 'Excel', 'Social media', 'Turbine', 'Database', 'wind energy', 'Powerpoint', 'Recruitment']",2025-06-14 06:09:11
Business Analyst - MDM,Vichara Technologies,5 - 10 years,25-40 Lacs P.A.,"['Hyderabad', 'Bengaluru', 'Delhi / NCR']","Write specifications for Master Data Management builds\nCreate requirements, including rules of survivorship, for migrating data to Markit EDM\nimplementation of data governance\nSupport testing\nDevelop data quality reports for the data warehouse\n\nRequired Candidate profile\n3+ years of experience documenting data management requirements\nExperience writing technical specifications for MDM builds\nFamiliarity with enterprise data warehouse\nKnowledge of data governance",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Lineage', 'Investment Banking', 'SQL', 'Master Data Management', 'Data Governance', 'Data Dictionary', 'Markit EDM', 'Data Quality Management', 'Semarchy', 'Master Data', 'Reltio', 'Data Quality', 'Data Stewardship', 'MDM', 'Data Warehousing']",2025-06-14 06:09:14
Team Lead/Senior Specialist,"BuzzBoard, Inc.",3 - 6 years,Not Disclosed,[],"Join our pioneering team at BuzzBoard , a recognized first-mover in enterprise generative AI, as a Team Lead of Generative AI and LLM. Weve already deployed production GenAI systems generating thousands of posts and content pieces monthly across our ecosystem. Youll lead the charge in scaling our mature AI infrastructure while architecting next-generation applications. As a key leader in our Product Team, youll orchestrate collaboration between Data Engineering, Software Engineering, and AI Operations teams while expanding our proven generative AI innovations.\nKey Responsibilities:\nStrategic Leadership & Vision\nLead and mentor a team of GenAI engineers and researchers within our mature AI ecosystem\nScale our proven production AI systems generating thousands of content pieces monthly across multiple business verticals\nDefine generative AI strategy and roadmap building upon our established first-mover advantage\nDrive adoption of multimodal AI systems incorporating vision, audio, and text capabilities\nContribute and own the GenAI governance frameworks and best practices\nAdvanced AI Development\nArchitect and scale sophisticated generative AI systems building upon our established multi-LLM infrastructure (GPT-4o, Claude Sonnet, Gemini, O1)\nOptimize our proven tech stack including LangChain, CrewAI, LangGraph, and vector databases (Chroma)\nImplement advanced RAG, fine-tuning, and prompt engineering across our existing model inventory\nLead development of next-generation agentic AI systems with tool use, reasoning capabilities, and autonomous decision-making\nEnhance our multi-agent orchestration platforms and complex workflow automation\nTechnology Leadership\nDrive adoption of agentic AI frameworks including LangGraph, CrewAI, AutoGen, and Microsoft Semantic Kernel\nLead implementation of multi-agent orchestration platforms and autonomous decision-making systems\nOversee vector databases (Pinecone, Weaviate, Chroma) and semantic search systems\nLead integration of AI observability and monitoring tools (LangSmith, Weights & Biases, MLflow)\nChampion AI development platforms and low-code/no-code solutions\nEnterprise Integration & Scaling\nScale our mature AI infrastructure supporting thousands of monthly content generations across multiple business verticals\nEnhance our proven MLOps and LLMOps practices for continuous model deployment and performance monitoring\nDrive technical collaboration for seamless AI integration into established production systems\nOptimize edge AI capabilities and multi-environment deployment strategies\nEnhance our performance tracking framework including regression testing, edit ratio tracking, and analytics integration\nSkills and Qualifications:\nCore Technical Expertise\nDeep expertise in generative AI systems, large language models, and transformer architectures with proven production experience\nExpert proficiency in our established tech stack : LangChain, CrewAI, LangGraph, AutoGen, and multi-LLM orchestration\nHands-on experience with our model ecosystem : OpenAI GPT, Anthropic, Google, and vector databases\nExtensive experience with agentic AI frameworks and autonomous workflow orchestration in production environments\nHands-on with programming skills in Python with experience in FastAPI, Streamlit, and modern web frameworks\nExpert-level understanding of prompt engineering, fine-tuning techniques, and model optimization at scale\nAI Architecture & Operations\nExperience with vector databases, embedding models, and semantic search implementations\nProficiency in containerization (Docker, Kubernetes) and cloud-native AI deployments\nKnowledge of AI model serving platforms (vLLM, TensorRT-LLM, Ollama) and inference optimization\nUnderstanding of AI safety, alignment, and responsible AI development practices\nTechnical Leadership (Individual Contributor Focus)\nExperience providing technical guidance to engineering teams in fast-paced environments\nExperience with AI product development lifecycle and technical go-to-market strategies\nStrong technical communication and ability to explain AI concepts to technical and non-technical audiences\nKnowledge of AI regulation landscape and compliance requirements\nModern AI Ecosystem\nFamiliarity with AI agent frameworks (LangGraph, CrewAI, Microsoft Semantic Kernel) - REQUIRED\nExperience with compound AI systems and multi-step reasoning architectures - REQUIRED\nExperience with multimodal AI systems and computer vision integration\nUnderstanding of federated learning and privacy-preserving AI techniques\nKnowledge of AI model evaluation frameworks and benchmarking methodologies\nAdvanced Qualifications:\nAgentic AI Mastery (Required)\nExtensive hands-on experience building and deploying agentic AI systems in production environments\nDeep understanding of tool-calling, function-calling, and API integration within agent workflows\nProven track record with multi-agent collaboration patterns and complex reasoning chains\nExpertise in agent memory systems, context management, and state persistence across interactions\nIndustry Integration\nExperience scaling production GenAI systems with proven track record of generating thousands of content pieces monthly\nKnowledge of multi-LLM orchestration and model switching strategies for optimal performance and cost efficiency\nUnderstanding of content generation workflows across social media, marketing, and business communications\nFamiliarity with performance monitoring frameworks including regression testing and analytics dashboard integration\nResearch & Innovation\nExperience with AI model interpretability and explainable AI techniques\nKnowledge of quantum-classical hybrid AI approaches and emerging paradigms\nTechnical Excellence\nAdvanced degree in Computer Science, AI, or related field (preferred)\nExperience building and scaling AI teams in fast-paced environments (preferred)\nExperience with AI ethics committees and responsible AI governance (preferred)\nProven ability to drive digital transformation through AI adoption (preferred)",Industry Type: Internet,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Social media marketing', 'Computer science', 'Automation', 'orchestration', 'Infrastructure', 'Regression testing', 'microsoft', 'Performance monitoring', 'digital transformation', 'Python']",2025-06-14 06:09:16
Sr. GTM Analytics Analyst,Slintel,3 - 8 years,Not Disclosed,[],"Our Mission:\n6sense is on a mission to revolutionize how B2B organizations create revenue by predicting customers most likely to buy and recommending the best course of action to engage anonymous buying teams. 6sense Revenue AI is the only sales and marketing platform to unlock the ability to create, manage and convert high-quality pipeline to revenue.\nOur People:\nPeople are the heart and soul of 6sense. We serve with passion and purpose. We live by our Being 6sense values of Accountability, Growth Mindset, Integrity, Fun and One Team. Every 6sensor plays a part in de ning the future of our industry-leading technology. 6sense is a place where difference-makers roll up their sleeves, take risks, act with integrity, and measure success by the value we create for our customers.\nWe want 6sense to be the best chapter of your career.\nPosition Overview:\nWe are seeking a detail-oriented and strategic Sr. Analyst to focus on data storytelling, dashboarding, and insight generation. This role will partner closely with business stakeholders to develop data products that guide decisions across the customer lifecycle. In addition to analysis and visualization work, this person will support data governance efforts to ensure the consistency and accuracy of reporting outputs.\nKey Responsibilities:\nPartner with GTM and cross-functional teams to translate business needs into clear metrics, dashboards, and reports.\nCreate data visualizations that bring clarity to trends, performance, and strategic opportunities.\nPerform deep-dive analyses to inform go-to-market strategies, pipeline health, churn, and revenue performance.\nSupport data governance efforts by defining and documenting standard metrics, KPIs, and data sources.\nEnsure analytic outputs are built on clean, governed data and communicate data integrity issues when necessary.\nCollaborate with backend and data engineering teams to iterate on models and datasets that power business-facing reports.\nProactively identify data gaps and opportunities to improve self-serve access and decision-making.\nQualifications:\nBachelor s degree in Business Analytics, Statistics, Economics, or a related field. Master s preferred.\n3+ years of experience in a data analyst or insights role, preferably supporting GTM functions (Sales, Marketing, CS).\nProficiency in SQL and experience working with modern BI tools (Looker, Tableau, Power BI, Sigma).\nStrong communication skills; able to tailor insights to technical and non-technical audiences.\nExperience with or exposure to data governance and metric standardization.\nFamiliarity with customer journey metrics and revenue performance reporting is a plus.\nOur Benefits:\nFull-time employees can take advantage of health coverage, paid parental leave, generous paid time-off and holidays, quarterly self-care days off, and stock options. We ll make sure you have the equipment and support you need to work and connect with your teams, at home or in one of our o ces.\nWe have a growth mindset culture that is represented in all that we do, from onboarding through to numerous learning and development initiatives including access to our LinkedIn Learning platform. Employee well-being is also top of mind for us. We host quarterly wellness education sessions to encourage self care and personal growth. From wellness days to ERG-hosted events, we celebrate and energize all 6sense employees and their backgrounds.\nEqual Opportunity Employer:\n6sense is an Equal Employment Opportunity and Affirmative Action Employers. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender perception or identity, national origin, age, marital status, protected veteran status, or disability status. .\nWe are aware of recruiting impersonation attempts that are not affiliated with 6sense in any way. A ll email communications from 6sense will originate from the @6sense.com domain . We will not initially contact you via text message and will never request payments . If you are uncertain whether you have been contacted by an official 6sense employee, reach out to jobs@ 6sense.com",Industry Type: Internet,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Selection process', 'Backend', 'Sales', 'Business analytics', 'data governance', 'Wellness', 'power bi', 'data integrity', 'Data Analyst', 'SQL']",2025-06-14 06:09:19
Machine Learning Engineer - Python,Qcentrio,6 - 10 years,Not Disclosed,['Bengaluru'],"Mandatory Skills : Python, Machine Learning Models, Flask, Data Visualization Tools.\n\nGood to have : Gen AI, NLP, Cloud (AWS) etc\n\nNotice Period : Immediate - 30 days\n\nRelevant Experience : 5+ Years in Machine Learning\n\nResponsibilities :\n\n- Design, develop, and deploy machine learning models and algorithms.\n\n- Perform data analysis to extract insights and identify patterns.\n\n- Develop and implement feature engineering processes to enhance model performance.\n\n- Collaborate with data scientists and cross-functional teams to understand business requirements and translate them into technical solutions.\n\n- Build and maintain scalable data pipelines and infrastructure.\n\n- Develop and deploy machine learning applications using Flask, FastAPI, or Django.\n\n- Conduct experiments, model tuning, and evaluate model performance using appropriate metrics.\n\nMust-Have Skills :\n\n- Python and its libraries (NumPy, pandas, scikit-learn, TensorFlow, PyTorch).\n\n- Experience with web frameworks such as Flask, FastAPI, or Django.\n\n- Strong understanding of machine learning algorithms and techniques.\n\n- Experience with data visualization tools (e.g., Matplotlib, Seaborn).\n\n- Strong problem-solving skills and ability to work independently as well as in a team.\n\n- Excellent communication skills to effectively convey technical concepts to non-technical stakeholders.\n\nGood-to-Have Skills :\n\n- Experience with cloud platforms, particularly AWS.\n\n- Knowledge of MLOps practices and tools.\n\n- Experience with Natural Language Processing (NLP) techniques.\n\n- Exposure to Generative AI (GenAI) technologies.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Python', 'Tensorflow', 'GenAI', 'Generative AI', 'NLP', 'MLOps', 'Artificial Intelligence', 'Natural Language Processing', 'Machine Learning']",2025-06-14 06:09:21
Sr Associate Software Engineer,Amgen Inc,6 - 8 years,Not Disclosed,['Hyderabad'],"What you will do\nIn this vital role We are looking for a creative and technically skilled Senior Software Engineer/AI Engineer Search to help design and build cutting-edge AI-powered search solutions for the pharmaceutical industry. In this role, you'll develop intelligent systems that surface the most relevant insights from clinical trials, scientific literature, regulatory documents, and internal knowledge assets. Your work will empower researchers, clinicians, and decision-makers with faster, smarter access to the right information.\n.",,,,"['NLP', 'NoSQL', 'Java Script', 'Data engineering', 'Java Crawlers', 'Databricks', 'Python', 'SQL']",2025-06-14 06:09:23
IND_Senior Analyst FDS&A,Lowes Services India Private limited,4 - 5 years,Not Disclosed,['Bengaluru'],"Lowe s Companies, Inc. (NYSE: LOW) is a FORTUNE 50 home improvement company serving approximately 16 million customer transactions a week in the United States. With total fiscal year 2023 sales of more than $86 billion, Lowe s operates over 1,700 home improvement stores and employs approximately 300,000 associates. Based in Bengaluru, Lowe s India develops innovative technology products and solutions and delivers business capabilities to provide the best omnichannel experience for Lowe s customers. Lowe s India employs over 4,200 associates across technology, analytics, merchandising, supply chain, marketing, finance and accounting, product management and shared services. Lowe s India actively supports the communities it serves through programs focused on skill-building, sustainability and safe homes. For more information, visit, www.lowes.co.in .\nThis includes, among other things, responsibility for financial metrics, reporting, forecasting and analysis. The successful candidate will be analytical, and focus on providing data driven business and financial insights to the relevant stakeholders that will enable sound decision making.\nPerform financial reporting, ad-hoc business analyses, and financial modeling\nFocus to ensure 100% accuracy and timely analysis\nContribute towards monthly business / financial review meetings and work with HQ Stores Finance managers to ensure effective finance and cost management\nDeliver projects that provide answers to strategic and tactical business questions - sometimes with very basic problem statement (dealing with ambiguity)\nQuickly grasp the business problem and translates into tangible, usable actionable outputs\nWork with business partners and teams in SSC-B and SSC-M through effective collaboration\nKnowledge of working scenario based analysis / multiple approaches to tackle problems and recommending a suitable approach based on facts, figures, data. Ability to story-tell (why/what/how) enabling decision making\nDevelop ad hoc business analysis (scenario analysis) and present recommendations to senior management on strategic decisions, and planned future initiatives.\nFinancial modelling - Ability to build accurate and situational financial models to solve business problems\nManage the financial planning and analysis effort, forecasting using the key input levers to the business - pricing, promotion, weather, business landscape and such that play a key role in retail dynamics\nEstablish and maintain operating and business metrics.\nWork with accounting team to drive the monthly, quarterly, and annual close process, as required\nBasic Qualifications\nBachelors/Master s degree in finance, accounting, business and/or Chartered Accountant, MBA\n4-5+ years of experience with at least 3+ years in Financial Planning and Analysis\nStrong Data collation, compilation, visualization skills\nAnalytical skills- querying using SQL, SAS, Python will be a strong plus\nAdvanced knowledge of Excel, VBA, Automation\nAbility to work independently in a fast-paced and rapidly changing environment\nPreferred Qualifications\nA work ethic based on a strong desire to exceed expectations\nStrong interpersonal skills emphasizing written and oral communication\nDemonstrated ability to build and manage financial models for business forecasting, variance analysis, and problem solving\nExperience working in FMCG / similar industry / retail will be a plus",Industry Type: Retail,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Supply chain', 'Automation', 'Financial reporting', 'SAS', 'Business analysis', 'Financial planning', 'FMCG', 'Variance analysis', 'Analytics', 'SQL']",2025-06-14 06:09:25
Unity Catalog ADB Job opening with Hexaware !!!,Hexaware Technologies,6 - 11 years,Not Disclosed,[],"Role & responsibilities\n\nMandatory skills: ADB AND UNITY CATALOG\n\nJob Summary: We are looking for a skilled StData Engineer /with expertise in Databricks and Unity Catalog to design, implement, and manage scalable data solutions. Key Responsibilities: • Design and implement scalable data pipelines and ETL workflows using Databricks. • Implement Unity Catalog for data governance, access control, and metadata management across multiple workspaces. • Develop Delta Lake architectures for optimized data storage and retrieval. • Establish best practices for data security, compliance, and lineage tracking in Unity Catalog. • Optimize data lakehouse architecture for performance and cost efficiency. • Collaborate with data scientists, engineers, and business teams to support analytical workloads. • Monitor and troubleshoot Databricks clusters, performance tuning, and cost management. • Implement data quality frameworks and observability solutions to maintain high data integrity. • Work with Azure/AWS/GCP cloud environments to deploy and manage data solutions. Required Skills & Qualifications: • 8-19 years of experience in data engineering, data architecture, or cloud data solutions. • Strong hands-on experience with Databricks and Unity Catalog. • Expertise in PySpark, Scala, or SQL for data processing. • Deep understanding of Delta Lake, Lakehouse architecture, and data partitioning strategies. • Experience with RBAC, ABAC, and access control mechanisms within Unity Catalog. • Knowledge of data governance, compliance standards (GDPR, HIPAA, etc.), and audit logging. • Familiarity with cloud platforms (Azure, AWS, or GCP) and their respective data services. • Strong understanding of CI/CD pipelines, DevOps, and Infrastructure as Code (IaC). • Experience integrating BI tools (Tableau, Power BI, Looker) and ML frameworks is a plus. • Excellent problem-solving, communication, and collaboration skills.",,,,"['Azure Databricks', 'Unity Catalog', 'ADB']",2025-06-14 06:09:27
Senior Full Stack Developer - .Net / AngularJS,Maimsd Technology,5 - 8 years,Not Disclosed,['Bengaluru'],"Your Role :\n\nYou- will work with our development and production support team on efforts to support Arcus Power's growth initiatives.\n\nThe Full Stack Developer will play a pivotal role in developing leading SaaS products for energy power market intelligence. Arcus' products provide insights and data-driven solutions to stakeholders in the energy industry.\n\nWith your expertise in microservices, APIs, AI/ML integration, and cloud technologies, you will be instrumental in driving the evolution of our software applications.\n\nYour ability to work across the stack, from frontend to backend, and integrate cutting-edge technologies will be essential in delivering innovative and high-quality solutions.\n\nCollaborating with cross-functional teams, including data scientists, UX designers, and DevOps engineers, you will play a key role in revamping legacy applications, building microservices, and infusing AI/ML capabilities into web user interfaces.\n\nResponsibilities will include :\n\n- Design, develop, and maintain scalable and responsive web applications using modern frontend and backend technologies.\n\n- Lead the revamping of legacy applications, ensuring modernization, improved performance, and UX.\n\n- Create and manage microservices architecture, including API design, development, and integration.\n\n- Collaborate with data scientists to integrate AI/ML capabilities into web user interfaces for predictive analytics and data-driven insights.\n\n- Integrate SQL and NoSQL databases, optimizing data storage and retrieval for efficient application performance.\n\n- Work with cloud platforms such as Azure or other relevant providers to deploy, monitor, and manage applications.\n\n- Implement DevOps practices for CI/CD, and automated testing.\n\n- Collaborate with UX/UI designers to create visually appealing and user-friendly interfaces.\n\n- Troubleshoot and debug issues, identifying root causes and implementing effective solutions.\n\n- Keep up-to-date with industry trends and emerging technologies to drive innovation in application development.\n\nTeams onboarding and training junior development staff\n\nOther duties as assigned\n\nYour Qualifications\n\n- Bachelor's or Master's degree in Computer Science, Software Engineering, or a related field\n\n- Proven experience as a Full Stack Developer with expertise in either frontend or backend development, or both\n\n- Strong proficiency in web development technologies, including HTML, CSS, JavaScript, and modern frontend frameworks (e.g., React, Angular, Vue). Deep knowledge of .NET languages and servers\n\n- Experience in designing and implementing microservices architecture, RESTful APIs, and integration patterns\n\n- Proficiency in both SQL and NoSQL databases and their integration into applications.\n\n- Experience administrating and integrating with cloud platforms such as Azure, AWS, or Google Cloud Platform.\n\n- Previous experience in successfully revamping and modernizing legacy applications.\n\n- Experience with Agile methodologies and participation in sprint planning and review meetings.\n\n- Familiarity with integrating AI/ML capabilities into web user interfaces for data visualization and insights.\n\n- Knowledge of DevOps practices, CI/CD pipelines, and automated testing.\n\n- Familiarity with MLOps methodologies and best practices.\n\nExperience considered assets :\n\n- Integrating with Snowflake, Databricks, or other data lake technologies\n\n- Utilizing, extending, and integrating business intelligence tools like Grafana\n\n- Integrating with iPaaS (Integration Platforms as a Service)\n\n- Leveraging graphs for modeling data and organizing metadata, semantics.\n\nKey Competencies & Attributes :\n\n- Delivering in self-organizing teams with high levels of trust and transparency\n\n- Strong problem-solving skills and ability to troubleshoot and debug complex issues\n\n- Strong organizational skills and ability to manage multiple projects and priorities\n\n- Adaptability to evolving technology landscapes and industry trends\n\n- Excellent collaboration and verbal/written communication skills, with the ability to work effectively in cross-functional teams\n\nYour Metrics :\n\nSuccess in this position will be measured against the following groups of Metrics :\n\nDevelopment Performance: Sustain and maintain an acceptable pace of development according to sprint plans, and backlog items, as per direction by product manager and owner.\n\nDeliverables Quality: For us, quality of deliverables will be critical to our success and to scale. The deliverables include proper documentation to handover to other groups.\n\nApplication Performance: Measure its responsiveness and efficiency in real-world usage.\n\nLegacy Application Modernization: Track the progress of legacy application revamping and performance improvements.\n\nAI/ML Integration Success: Monitor the successful integration of AI/ML capabilities into web interfaces.\n\nMicroservices Architecture: Measure the efficiency and scalability of the microservices architecture.\n\nDevOps Efficiency: Measure the effectiveness of CI/CD pipelines and automated testing in the development process.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['.Net', 'Full Stack development', 'AngularJS', 'Vue.js', 'CSS', 'Microservices Architecture', 'JavaScript', 'HTML', 'React.js', 'SQL']",2025-06-14 06:09:30
Business Analyst,Manektech,4 - 8 years,Not Disclosed,['Ahmedabad'],"ManekTech is looking for Business Analyst to join our dynamic team and embark on a rewarding career journey\n\nAnalyze business processes and identify areas for improvement using data-driven insights\n\n\nDevelop functional requirements, process flows, and documentation for tech implementation\n\n\nCollaborate with stakeholders to align solutions with business goals\n\n\nMonitor KPIs post-implementation to evaluate effectiveness",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent",['Business Analyst'],2025-06-14 06:09:32
"Risk Analyst, E-commerce - Top Indian Conglomerate",Top Indian Conglomerate,3 - 5 years,20-25 Lacs P.A.,['Bengaluru'],"Job Title: Risk Analyst - Commerce\n\nReports To: Lead Fraud Risk Commerce\n\n""LOOKING FOR CANDIDATES ONLY FROM E-COMMERCE COMPANIES""\n\nAbout the Role:\nWe are seeking a skilled and detail-oriented Risk Analyst to join our Commerce Risk team. This role is critical in driving risk intelligence through data-backed insights and timely analysis. The ideal candidate will not only work with large datasets and analytical models but will also be responsible for translating technical findings into clear, actionable summaries for leadership and crossfunctional stakeholders.\nThe analyst is expected to identify risks, uncover patterns in user behavior, and support strategic risk mitigation decisions across the platform, particularly in the areas of customer abuse, promotional misuse, and trust violations.\n\nKey Responsibilities:\nPrepare and analyze complex datasets to identify risk patterns, surface anomalies, and validate business hypotheses.\nBuild concise summaries and visualizations that simplify technical data for diverse audiences, enabling informed decision-making across Risk, Product, and Business teams.\nCommunicate insights through clear narratives, providing actionable recommendations and highlighting associated risks and trade-offs.\nDesign and execute BI and reporting frameworks to monitor key metrics, rule performance, and customer impact across commerce brands.\nCollaborate with data engineering teams to maintain robust data pipelines and models for risk signal generation.\nContribute to data modeling efforts and manage risk-centric data structures within the data warehouse.\nSupport development and refinement of risk rules and machine learning models to strengthen detection of fraudulent or abusive behaviors.\nMaintain documentation and continuously evolve analysis SOPs based on the latest business rules, thresholds, and ecosystem learnings.\n\nRole Requirements:\nBachelors degree in a quantitative discipline such as Statistics, Mathematics, Economics, Computer Science, or a related field.\n3 - 5 years of hands-on experience in risk analytics, fraud detection, or business intelligence, preferably within eCommerce or fintech domains.\nProficient in SQL and Python for data extraction, transformation, and visualization; exposure to tools like R, Tableau, Power BI is a plus.\nStrong understanding of data warehousing, data modeling, and performance monitoring techniques.\nExperience working with rule-based and predictive risk frameworks is preferred.\nExcellent communication and storytelling skills, with the ability to present insights to nontechnical audiences in a structured and impactful manner.\nProven experience working in cross-functional environments, balancing business priorities and analytical rigor.\n\nBasic Qualifications:\nBachelor's degree in a quantitative discipline such as Statistics, Mathematics, Economics, Computer Science, or a related field.",Industry Type: Internet (E-Commerce),Department: Risk Management & Compliance,"Employment Type: Full Time, Permanent","['Risk Intelligence', 'Risk Analytics', 'Predictive Modeling', 'Cross Functional Coordination', 'Power Bi', 'E-commerce', 'Tableau', 'SQL', 'Anomaly Detection', 'Risk Mitigation', 'Communication Skills', 'Data Analysis', 'Data Visualization', 'Data Warehousing', 'Data Modeling', 'Fraud Detection', 'Python']",2025-06-14 06:09:34
Business Analyst,Technocrats Horizons Compusoft,1 - 3 years,Not Disclosed,['Hyderabad'],"Education Required\nGraduate: B.Tech/B.E. in Computers, BCA in Any Specialization.\nPG: MBA, MCA in Computers, MS/M.Sc in Any Specialization.\nBehavior & Character Attributes Required:\nUser-Focused Empathy: You naturally understand our customers' needs and view things from their perspective, always striving to create impactful solutions.\nTeam Player & Relationship Builder: You're great at building strong, trusting relationships with both colleagues and clients, fostering open communication.\nPositive Problem Solver: You maintain a positive attitude and always look for solutions, even when faced with difficulties.\nProactive & Forward-Thinking: You anticipate client needs and potential issues, always thinking a step ahead.\nAnalytical & Creative Thinker: You're skilled at breaking down problems and finding innovative, effective solutions.\nAttentive Listener: You actively listen to truly understand others, which is vital for effective collaboration and problem-solving.\nOrganized & Detail-Oriented: You work systematically and keep your documentation clear and tidy, showing attention to detail.\nEager to Learn & Adapt: You actively research global tech trends, always seeking new knowledge to keep us competitive.\nResponsibilities:\nClient & Requirement Management: Conduct sessions with clients and sales teams to gather requirements, define user stories, and propose tailored solutions aligned with business goals.\nProject & Process Oversight: Manage project scope, timelines, and sprints; coordinate stand-ups, retrospectives, and ensure quality deliverables.\nSystem & Workflow Documentation: Help design, document, and maintain system processes while facilitating smooth implementation of changes.\nTeam Coordination & Conflict Resolution: Guide development teams, support effective internal communication, resolve conflicts, and remove project obstacles.\nClient Relationship & Communication: Build and maintain strong client relationships, serving as the key contact to understand goals, challenges, and deliver consistent value.\nAccount & Performance Management: Oversee client accounts, ensure satisfaction and retention through regular reviews and collaboration with internal teams.\n\nGrowth & Business Development: Identify upsell/cross-sell opportunities, drive new business through referrals, and support backlog grooming with product owners.\nWhat you need to Bring:\nExperience: You've worked before as a Business Analyst or in a similar position.\nTech Know-How: You understand software products, their features, and related services.\nCommunication Skills: You can explain complex ideas clearly, both when speaking and writing (including technical documents).\nAgile & Scrum Familiarity: You're comfortable with Agile and Scrum ways of working.\nTool Proficiency: You know how to use project management tools.\nDocumentation Expertise: Proficient in creating and managing BRDs, FRDs, user stories, use cases, and process flow diagrams.\nOrganizational Strength: Strong organizational, time management, and multitasking skills to manage competing priorities efficiently.\nGood to Have:\n\nCertified in Business Analysis: Holding a certification like CBAP or CCBA is a plus.\nIndustry Software Knowledge: You're familiar with software and tools specific to our industry.\nData Analysis Skills: You can analyze data to find useful business insights and are good with data visualization tools.\nClient Interaction Experience: You have experience working directly with clients.\nSQL Knowledge: You know how to use SQL or other data querying languages.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Business Analysis', 'CCBA', 'Client Management', 'CBAP', 'Data Analysis', 'Performance Management', 'Requirement Management', 'SQL']",2025-06-14 06:09:36
Scrum Master - BI Products,Pepsico,8 - 12 years,Not Disclosed,['Hyderabad'],"Overview \n\nThe Scrum Master serves as a servant leader and delivery enabler for the BI Reporting team, ensuring Agile practices are followed, blockers are removed, and delivery of high-impact Power BI dashboards is achieved with quality, speed, and technical robustness.\n\n Responsibilities \n\n\n \n\nEducation & Certifications\nBachelors or Masters degree in Computer Science, Information Systems, Business Analytics, or a related field.\nCertified Scrum Master (CSM), Certified SAFe Scrum Master (SSM), or equivalent Agile certification is mandatory.\nAdditional certifications in Agile Coaching, Product Ownership, or Project Management (e.g., PMI-ACP, PSM II, ICP-ACC) are a strong plus.\n\nExperience\n8-12 years of total experience, with at least 6 years as a dedicated Scrum Master in data, analytics, or BI-focused environments.\nProven experience supporting Business Intelligence and Analytics teams, preferably in an enterprise environment.\nHands-on understanding of Power BI, data modeling, ETL processes, and report lifecycle is a strong advantage.\nDemonstrated success facilitating Scrum/Agile delivery in teams working on dashboard/report development, data pipelines, or data warehousing.\nExperience working in multi-stakeholder environments with business, IT, and data governance teams.\n\nTechnical & Functional",,,,"['data security', 'access management', 'facilitation', 'coaching', 'agile methodology', 'project management', 'version control', 'bi', 'power bi', 'pmi acp', 'psm', 'user stories', 'pmi', 'azure devops', 'data modeling', 'sprint planning', 'kanban', 'scrum', 'agile', 'etl', 'csm', 'agile coaching', 'etl process']",2025-06-14 06:09:40
"Senior Manager, ML Engineering",Avalara Technologies,10 - 15 years,Not Disclosed,[],"What You'll Do\n\nWe are seeking an experienced Senior AI & Machine Learning Manager to lead our dynamic team in developing cutting-edge AI & ML solutions. This role is perfect for someone passionate about leveraging AI and ML, inspire creativity, and create impactful products. You will be responsible for our Generative AI systems and how we apply them at Avalara to simplify and scale tax compliance across our entire portfolio of products. Major initiatives currently involve creating conversational agents. As an integral part of our leadership team, you will have the opportunity to shape the future of our AI & ML initiatives, manage a talented team of AI professionals, and collaborate with cross-functional teams to implement strategic projects.\n\nWhat Your Responsibilities Will Be\n\n\nWhat You'll Need to be Successful",,,,"['ML Engineering', 'Java', 'R', 'C++', 'Azure', 'Artificial Intelligence', 'Scala', 'AWS', 'Python', 'Google Cloud']",2025-06-14 06:09:42
"Product Manager III, Flights",Expedia Group,5 - 10 years,Not Disclosed,['Gurugram'],"Why Join Us?\nTo shape the future of travel, people must come first. Guided by our Values and Leadership Agreements, we foster an open culture where everyone belongs, differences are celebrated and know that when one of us wins, we all win.\nWe provide a full benefits package, including exciting travel perks, generous time-off, parental leave, a flexible work model (with some pretty cool offices), and career development resources, all to fuel our employees passion for travel and ensure a rewarding career journey. We re building a more open world. Join us.\nProduct Manager III, Flights\nExpedia s Flights team is looking for a Product Manager to help us reimagine and deliver the best Flight search and discovery customer experience on the web and mobile app. Do you want to help people go places? Come fly with us!\nIn this role you will develop, prioritize and deliver a portfolio of features and A/B tests to improve the search and shopping experience for millions of travelers, across our various sites (e.g. Expedia, Orbitz, Travelocity) and mobile apps all around the world. As a part of the team, you will work with local and global teammates to build a delightful product, develop a compelling strategy, and drive business goals. Our team embraces a bold, customer-centric approach and the technical and analytical know-how needed to deliver results. We are looking for an effective relationship builder to drive support across a broad range of fields and personalities. Most importantly, you are a creative problem solver with an eye for details and a keen focus on results\n\nWhat you ll do:\nCraft Flight search and booking experiences with your innovative ideas and deep understanding of customer experience and industry / e-commerce trends\nEstablish the vision, business goals, roadmap, and A/B testing strategy for your product features\nDefine product success metrics and provide detailed analysis of business impact, and lead executive communication on results\nUse data to address customer and partner needs\nCollaborate with designers, researchers, developers, data scientists, finance, legal, support and other product teams to deliver outstanding product improvements\nLead trade-off decisions between cost, schedule, and business benefit, and ensure delivery of features to production that meet customer and business needs\nEncourage and influence by articulating and continuously reinforcing the product vision\nWho you are:\nBachelor s degree in Computer Science or related field; or equivalent related professional experience\n5+ years of product management or related experience\nShould include several product/service launches using software development methodologies, gathering business requirements and turning them into detailed specifications\nExperience in a fast-paced eCommerce environment or Air/Travel experience preferred\nVery strong interpersonal and communication skills (externally and internally at all levels)\nProven ability to work with a diverse set of team members and positively influence a large, complex organization\nClear record of driving key analyses that generate measurable insights with minimum\nAccommodation requests\nIf you need assistance with any part of the application or recruiting process due to a disability, or other physical or mental health conditions, please reach out to our Recruiting Accommodations Team through the Accommodation Request .\nWe are proud to be named as a Best Place to Work on Glassdoor in 2024 and be recognized for award-winning culture by organizations like Forbes, TIME, Disability:IN, and others.\nExpedia Groups family of brands includes: Brand Expedia , Hotels.com , Expedia Partner Solutions, Vrbo , trivago , Orbitz , Travelocity , Hotwire , Wotif , ebookers , CheapTickets , Expedia Group Media Solutions, Expedia Local Expert , CarRentals.com , and Expedia Cruises . 2024 Expedia, Inc. All rights reserved. Trademarks and logos are the property of their respective owners. . Never provide sensitive, personal information to someone unless you re confident who the recipient is. Expedia Group does not extend job offers via email or any other messaging tools to individuals with whom we have not made prior contact. Our email domain is @expediagroup.com. The official website to find and apply for job openings at Expedia Group is careers.expediagroup.com/jobs .\nExpedia is committed to creating an inclusive work environment with a diverse workforce. All qualified applicants will receive consideration for employment without regard to race, religion, gender, sexual orientation, national origin, disability or age.",Industry Type: IT Services & Consulting,Department: Product Management,"Employment Type: Full Time, Permanent","['Product management', 'Computer science', 'Career development', 'Software development methodologies', 'Talent acquisition', 'Product service', 'Analytical', 'E-commerce', 'Customer experience', 'Lead Executive']",2025-06-14 06:09:44
"Product Manager III, Flights",Expedia Group,5 - 10 years,Not Disclosed,['Gurugram'],"Why Join Us?\nTo shape the future of travel, people must come first. Guided by our Values and Leadership Agreements, we foster an open culture where everyone belongs, differences are celebrated and know that when one of us wins, we all win.\nWe provide a full benefits package, including exciting travel perks, generous time-off, parental leave, a flexible work model (with some pretty cool offices), and career development resources, all to fuel our employees passion for travel and ensure a rewarding career journey. We re building a more open world. Join us.\nProduct Manager III, Flights\nExpedia s Flights team is looking for a Product Manager to help us reimagine and deliver the best Flight search and discovery customer experience on the web and mobile app. Do you want to help people go places? Come fly with us!\nIn this role you will develop, prioritize and deliver a portfolio of features and A/B tests to improve the search and shopping experience for millions of travelers, across our various sites (e.g. Expedia, Orbitz, Travelocity) and mobile apps all around the world. As a part of the team, you will work with local and global teammates to build a delightful product, develop a compelling strategy, and drive business goals. Our team embraces a bold, customer-centric approach and the technical and analytical know-how needed to deliver results. We are looking for an effective relationship builder to drive support across a broad range of fields and personalities. Most importantly, you are a creative problem solver with an eye for details and a keen focus on results\n\nWhat you ll do:\nCraft engaging travel shopping experiences with your innovative ideas and deep understanding of customer experience and industry / e-commerce trends\nEstablish the vision, business goals, roadmap, and A/B testing strategy for your product features\nDefine product success metrics and provide detailed analysis of business impact, and lead executive communication on results\nUse machine learning to address customer and partner needs\nCollaborate with designers, researchers, developers, data scientists, finance, legal, support and other product teams to deliver outstanding product improvements\nLead trade-off decisions between cost, schedule, and business benefit, and ensure delivery of features to production that meet customer and business needs\nEncourage and influence by articulating and continuously reinforcing the product vision\nWho you are:\nBachelor s degree in Computer Science or related field; or equivalent related professional experience\n5+ years of product management or related experience\nShould include several product/service launches using software development methodologies, gathering business requirements and turning them into detailed specifications\nExperience in a fast-paced eCommerce environment or Air/Travel experience preferred\nVery strong interpersonal and communication skills (externally and internally at all levels)\nProven ability to work with a diverse set of team members and positively influence a large, complex organization\nClear record of driving key analyses that generate measurable insights with minimum\nAccommodation requests\nIf you need assistance with any part of the application or recruiting process due to a disability, or other physical or mental health conditions, please reach out to our Recruiting Accommodations Team through the Accommodation Request .\nWe are proud to be named as a Best Place to Work on Glassdoor in 2024 and be recognized for award-winning culture by organizations like Forbes, TIME, Disability:IN, and others.\nExpedia Groups family of brands includes: Brand Expedia , Hotels.com , Expedia Partner Solutions, Vrbo , trivago , Orbitz , Travelocity , Hotwire , Wotif , ebookers , CheapTickets , Expedia Group Media Solutions, Expedia Local Expert , CarRentals.com , and Expedia Cruises . 2024 Expedia, Inc. All rights reserved. Trademarks and logos are the property of their respective owners. . Never provide sensitive, personal information to someone unless you re confident who the recipient is. Expedia Group does not extend job offers via email or any other messaging tools to individuals with whom we have not made prior contact. Our email domain is @expediagroup.com. The official website to find and apply for job openings at Expedia Group is careers.expediagroup.com/jobs .\nExpedia is committed to creating an inclusive work environment with a diverse workforce. All qualified applicants will receive consideration for employment without regard to race, religion, gender, sexual orientation, national origin, disability or age.",Industry Type: IT Services & Consulting,Department: Product Management,"Employment Type: Full Time, Permanent","['Product management', 'Computer science', 'Career development', 'Software development methodologies', 'Talent acquisition', 'Product service', 'Analytical', 'Machine learning', 'E-commerce', 'Customer experience']",2025-06-14 06:09:46
Bigdata AWS- Team Lead,Legato,2 - 7 years,Not Disclosed,['Bengaluru'],"Position Title:\nBigdata AWS- Team Lead - BF - 23751- 28909 - JR155190\nJob Family:\nIFT > Engineering /Dev\nShift:\nJob Description:\nJOB DESCRIPTION\nJob Title\nTeam Lead I (IND)\nRequirement Type\nFull-Time Employee\nJob Location\nBangalore\nRequirement Level\nI10\nHiring Manager\nAssociate Delivery Manager\nPrimary Skill\nData Engineer (Bigdata, Scala and AWS) and Agile Methodology, Data Analytics\nBusiness\nHealth Services Platform\nSkill Category\nNiche\nABOUT ELEVANCE HEALTH\nElevance Health is a leading health company in America dedicated to improving lives and communities and making healthcare simpler. It is the largest managed health care company in the Blue Cross Blue Shield (BCBS) Association serving more than 45 million lives across 14 states.\nA regular in Fortune 500 list, Elevance Health ranked 20 in 2022. Gail Boudreaux , President and CEO of Elevance Health has been a consistent name in the Fortune list of most powerful women and currently holds 4th rank on this list.\nABOUT CARELON\nCarelon Global Solutions (CGS) is a healthcare solutions company that is simplifying complex operational processes to improve the health of the healthcare system.\nPreviously known as Legato Health Technologies, Carelon Global Solutions (hereinafter, CGS) underwent a name change and joined the Carelon family of brands in January 2023, as a fully owned subsidiary of Elevance Health (Previously Anthem Inc.). CGS brings together a global team of like-minded innovators who manage and optimize operational processes for health plans as well as providers .\nOur brightest minds housed across our global headquarters in Indianapolis as well as Bengaluru, Hyderabad and Gurugram in India, Manila in the Philippines, Limerick in Ireland and San Juan in Puerto Rico bring with them innovative capabilities and an unmatched depth of experience. This global team uniquely positions CGS to enable scalable, next-generation platforms and specialized digital tools that make healthcare operations more practical, effective and efficient.\nOUR MISSION & VALUES\nOur Mission: Improving Lives and Communities. Simplifying Healthcare. Expecting More.\nOur Values: Leadership | Community | Integrity | Agility | Diversity\nJOB POSITION\nCarelon Global Solutions India is seeking a highly creative and meticulous Team Lead, who will be responsible for all design requirements for our leader communications. The incumbent will report to Manager - Communications and must understand the design process, liaising with multiple stakeholders to understand the brief and deliver all in-house design requirements, apart from coordinating with external agencies to ensure brand guidelines are followed.\nJOB RESPONSIBILITY\nGood experience and understanding of various core AWS services such as IAM, Cloud Formation, EC2, S3, EMR, Glue, Lambda, Athena and Redshift.\nGood experience and understanding of Bigdata technologies such as Spark, Scala, Hive, Hadoop.\nExperience playing the Scrum Master role for at least 2 years for a software development team that was diligently applying Scrum principles, practices, and theory.\nGood skills and knowledge of servant leadership, facilitation, situational awareness, conflict resolution, continual improvement, empowerment, and increasing transparency\nKnowledge of numerous well documented patterns and techniques for filling in the intentional gaps left in the Scrum approach (example: numerous Burndown techniques, numerous Retrospective formats, handling bugs, etc)\nExcellent communication and mentoring skills\nProviding all support to the team using a servant leadership style whenever possible and leading by example.\nLead and coordinate the activities of the production support team to ensure efficient operations.\nManage incidents and problems effectively, ensuring minimal impact on production.\nCommunicate the status and health of applications to business lines and management.\nWhen issues are reported by business, production support engineers must act quickly to analyze the available data and find the root cause of the problem, all the while providing users with progress updates.\nPerform advanced troubleshooting, analysis, and resolution of production issues using programming and query skills.\nQUALIFICATION\nB.Tech /B.E /MCA\nEXPERIENCE\n5+ Years experience in conduct root cause analysis to identify and resolve underlying issues.\n6+ year s experience SQL and NoSQL databases like MySQL, Postgres\n6+ year s experience with Strong analytical skills and advanced SQL knowledge, indexing, query optimization techniques.\nSKILLS AND COMPETENCIES\nProfound understanding of Big Data core concepts and technologies - Apache Spark, Kafka, spark, Scala , Hive and AWS etc.\nGood understanding of business and operational processes.\nCapable of Problem / issue resolution, capable of thinking out of the box.\nTHE CARELON PROMISE\nAligning with our brand belief of limitless minds are our biggest asset , we offer a world of limitless opportunities to our associates.\nIt is our strong belief that one is committed to a role when it is not just what the role entails, but also what lies in its periphery that completes the value circle for an associate.\nThis world of limitless opportunities thrives in an environment that fosters growth and well-being, and gives you purpose and the feeling of belonging.\nLIFE @ CARELON\nExtensive focus on learning and development\nAn inspiring culture built on innovation, creativity, and freedom\nHolistic well-being\nComprehensive range of rewards and recognitions\nCompetitive health and medical insurance coverage\nBest-in-class amenities and workspaces\nPolicies designed with associates at the center\nEQUAL OPPORTUNITY EMPLOYER\nReasonable Accommodation\nOur inclusive culture empowers Carelon to deliver the best results for our customers. We not only celebrate the diversity of our workforce, but we also celebrate the diverse ways we work.\nIf you have a disability and need accommodation such as an interpreter or a different interview format, please ask for the Reasonable Accommodation Request Form.\n* Disclaimer: Offered designation titles differs *\nJob Type:\nFull time",Industry Type: Medical Services / Hospital,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SAN', 'Production support', 'MySQL', 'Genetics', 'Scrum', 'Agile methodology', 'Troubleshooting', 'Operations', 'AWS', 'SQL']",2025-06-14 06:09:49
Direct WalkIn at Tcs Hyderabad on 21 June For .Net/Azure/AWS & more,Tata Consultancy Services,5 - 10 years,Not Disclosed,['Hyderabad'],We are organizing a direct walk-in drive at Hyderabad location.\nPlease find below details and skills for which we have a walk-in at TCS - Hyderabad on 21stJune 2025\n\nExperience: 5- 10 years\n\nSkill Name :-\n(1) Dot Net (All Cloud),,,,"['AWS data engineer', '.Net', 'Azure Devops', 'Microservices', 'SQL', 'Azure Data Engineer', 'Java Spring Boot', 'Mainframe Developer', 'Node.Js', 'Java Fullstack', 'React.Js', 'Teradata']",2025-06-14 06:09:51
"Tech Manager, Actimize",NICE,13 - 18 years,Not Disclosed,['Pune'],"So, what’s the role all about?\nThe Tech Manager of Engineering plans and directs all aspects of engineering activities and projects within the organization and manages a team of engineers. Ensures all engineering projects, initiatives, and processes are in conformance with organization's established policies and objectives. The ideal candidate should have strong technical skills and be able to provide expert technical guidance for engineering initiatives. He should also be good with project management skills and. Guides the team to follow the best software development and testing processes and tools to ensure best quality of products.",,,,"['artifactory', 'cd', 'project management', 'software testing', 'ci/cd', 'machine learning', 'docker', 'cloud', 'analytics', 'technical architecture', 'people management skills', 'saas', 'flex', 'safe', 'jenkins', 'scrum', 'software development methodologies', 'software engineering', 'agile', 'aws', 'big data', 'vms', 'architecture']",2025-06-14 06:09:53
Technical Lead,IT service based company,8 - 12 years,25-35 Lacs P.A.,"['Hyderabad', 'Bengaluru', 'Delhi / NCR']","Hi candidates,\n\n\nWe have an opportunities with leading brand for the Lead Data Engineer interested candidates can mail their CV's at Abhishek.saxena@mounttalent.com\n\nLocation- Location: Delhi NCR, Hyderabad, Bangalore, Pune, Chennai, this is a hybrid work opportunity.\n\n\nJob Description-\n\n\n8+ years of experience in data engineering with 3+ years hands on Databricks (DB) experience.(Must)\nStrong expertise in Microsoft Azure cloud platform and services, particularly Azure Databricks, Azure Data Factory, Azure Fabric, Azure SQL Database, and Power BI\n3+ years of hands-on experience with event-driven streaming data engineering, preferably with Apache Flink and/or Confluent. (Pref- Both skills)\nExtensive experience working with large data sets with hands-on technology skills to design and build robust data architecture.\nExtensive experience in data modeling and database design (Good to have)\nStrong programming skills in SQL, Python and Pyspark\nExtensive experience of Medallion architecture (3 layer/ specific to Databricks) and delta lake (or lakehouse) (Databricks)\nAgile development environment experience applying DEVOPS along with data quality and governance principles.\nGood leadership skills to guide and mentor the work of less experienced personnel\nAbility to contribute to continual improvement by suggesting improvements to Architecture or new technologies and mentoring junior employees and being ready to shoulder ad-hoc.\nGood to Have:\n- Experience with Change Data Capture (CDC) frameworks, with Debezium experience being a bonus.\n- Familiarity with containerized environments for event-driven streaming\n- Experience in Unity catalog, DBT and data governance knowledge\n- Experience in Snowflake",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Data Warehousing', 'ETL', 'azure']",2025-06-14 06:09:55
Business Technology Solutions Associate - Health Plan & Provider,ZS,4 - 8 years,Not Disclosed,"['Pune', 'Gurugram']","ZS   is a place where passion changes lives. As a management consulting and technology firm focused on improving life and how we live it , our most valuable asset is our people. Here you ll work side-by-side with a powerful collective of thinkers and experts shaping life-changing solutions for patients, caregivers and consumers, worldwide. ZSers drive impact by bringing a client first mentality to each and every engagement. We partner collaboratively with our clients to develop custom solutions and technology products that create value and deliver company results across critical areas of their business. Bring your curiosity for learning; bold ideas; courage an d passion to drive life-changing impact to ZS.\n\n\n\n\n\n  .\n\nAt   we honor the visible and invisible elements of our identities, personal experiences and belief systems the ones that comprise us as individuals, shape who we are and\n\nmake us unique. We believe your personal interests, identities, and desire to learn are part of your success here. Learn more about our diversity, equity, and inclusion efforts and the networks ZS supports to assist our ZSers in cultivating community spaces, obtaining the resources they need to thrive, and sharing the messages they are passionate about.\n\n \n\nZS s Technology group focuses on scalable strategies, assets and accelerators that deliver to our clients enterprise-wide transformation via cutting-edge technology. We leverage digital and technology solutions to optimize business processes, enhance decision-making, and drive innovation. Our services include, but are not limited to, Digital and Technology advisory, Product and Platform development and Data, Analytics and AI implementation.\n\n \n\n\n\n   \n\n\n\n   \n\n \n\nZS offers a comprehensive total rewards package including health and well-being, financial planning, annual leave, personal growth and professional development. Our robust skills development programs, multiple career progression options and internal mobility paths and collaborative culture empowers you to thrive as an individual and global team member.\n\nWe are committed to giving our employees a flexible and connected way of working. A flexible and connected ZS allows us to combine work from home and on-site presence at clients/ZS offices for the majority of our week. The magic of ZS culture and innovation thrives in both planned and spontaneous face-to-face connections.\n\n\n\n \n\nTravel is a requirement at ZS for client facing ZSers; business needs of your project and client are the priority. While some projects may be local, all client-facing ZSers should be prepared to travel as needed. Travel provides opportunities to strengthen client relationships, gain diverse experiences, and enhance professional growth by working in different environments and cultures.\n\n\n\n\n\n \n\nAt ZS, we're building a diverse and inclusive company where people bring their passions to inspire life-changing impact and deliver better outcomes for all. We are most interested in finding the best candidate for the job and recognize the value that candidates with all backgrounds, including non-traditional ones, bring. If you are interested in joining us, we encourage you to apply even if you don't meet 100% of the requirements listed above.\n\n\n\n\n\n\n\n \n\nCandidates must possess or be able to obtain work authorization for their intended country of employment.An on-line application, including a full set of transcripts (official or unofficial), is required to be considered.",,,,"['azure databricks', 'python', 'pyspark', 'azure data factory', 'sql', 'continuous integration', 'snowflake', 'css', 'microsoft azure', 'relational databases', 'data engineering', 'javascript', 'aws glue', 'data bricks', 'angular', 'git', 'django', 'postgresql', 'html', 'mysql', 'aws', 'flask', 'etl', 'etl process']",2025-06-14 06:09:57
Scala Developer,Infosys,3 - 8 years,Not Disclosed,['Hyderabad'],"Educational Requirements\nBachelor of Engineering\nService Line\nData & Analytics Unit\nResponsibilities\nA day in the life of an Infoscion\nAs part of the Infosys consulting team, your primary role would be to actively aid the consulting team in different phases of the project including problem definition, effort estimation, diagnosis, solution generation and design and deployment\nYou will explore the alternatives to the recommended solutions based on research that includes literature surveys, information available in public domains, vendor evaluation information, etc. and build POCs\nYou will create requirement specifications from the business needs, define the to-be-processes and detailed functional designs based on requirements.\nYou will support configuring solution requirements on the products; understand if any issues, diagnose the root-cause of such issues, seek clarifications, and then identify and shortlist solution alternatives\nYou will also contribute to unit-level and organizational initiatives with an objective of providing high quality value adding solutions to customers. If you think you fit right in to help our clients navigate their next in their digital transformation journey, this is the place for you!\nAdditional Responsibilities:\nAbility to work with clients to identify business challenges and contribute to client deliverables by refining, analyzing, and structuring relevant data\nAwareness of latest technologies and trends\nLogical thinking and problem solving skills along with an ability to collaborate\nAbility to assess the current processes, identify improvement areas and suggest the technology solutions\nOne or two industry domain knowledge\nTechnical and Professional Requirements:\nPrimary skills:Bigdata->Scala,Bigdata->Spark,Technology->Java->Play Framework,Technology->Reactive Programming->Akka\nPreferred Skills:\nBigdata->Spark\nBigdata->Scala\nTechnology->Reactive Programming->Akka\nTechnology->Java->Play Framework",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Scala', 'Java', 'Reactive Programming', 'Bigdata', 'Spark', 'Play Framework', 'Akka']",2025-06-14 06:09:59
Analyst,Avalara Technologies,3 - 8 years,Not Disclosed,['Pune'],"What You'll Do\n\nWe are looking for an Associate Analyst/ Analyst who would involve in understanding the transactions on the Sales & use form and file tax returns for US states Must be a supporting member of the Professional Services team, ensuring client returns are filed. Work with team and all other tax personnel and deadlines, observe processes to determine ways that can help output of the team\nAlso, would have to provide customer support in addition to producing and accurate results by way of preparing and filing the Sales & use tax returns. This role report to Manager-PS.\n\nWhat Your Responsibilities Will Be\n\nWhat You'll Need to be Successful",,,,"['Tax Summary Report', 'US Taxation', 'Tax Compliance', 'Sales Tax', 'Tax Returns Filing', 'Use Tax', 'State Tax Laws']",2025-06-14 06:10:02
Business Analyst,Centricity Wealth Tech,2 - 7 years,4-5 Lacs P.A.,"['Lucknow', 'Gurugram']","Job Description:\nWe are seeking an experienced MIS Executive with advanced Excel skills to join our team. The ideal candidate will have a minimum of 3 years of experience working with large datasets, creating complex reports, and developing interactive dashboards. This role is crucial in providing insights to management through detailed analysis and data-driven decision-making.\nKey Responsibilities:\nCollect, organize, and analyze large datasets to generate MIS reports.\nCreate and maintain advanced Excel reports, utilizing functions such as VLOOKUP, INDEX-MATCH, PivotTables, and Macros.\nDesign interactive dashboards and visualizations for easy data interpretation.\nTrack key performance indicators (KPIs) and provide actionable insights to stakeholders.\nAutomate reporting processes using Excel to improve efficiency.\nEnsure data integrity and consistency across reports and dashboards.\nCollaborate with various departments to understand their data requirements and deliver customized reports.\nConduct ad-hoc data analysis and assist with strategic decision-making by presenting data clearly.\nMonitor and analyze trends to help senior management make informed business decisions.\nQualifications:\nMinimum 3 years of experience in an MIS Executive or similar data-related role.\nAdvanced proficiency in Microsoft Excel, including PivotTables, VLOOKUP, INDEX-MATCH, advanced formulas, and Macros.\nStrong experience in data analysis, reporting, and dashboard creation.\nFamiliarity with other data visualization tools (e.g., Power BI, Tableau) is a plus.\nExcellent analytical, problem-solving, and organizational skills.\nStrong attention to detail and the ability to work with large datasets efficiently.\nEffective communication skills and ability to present complex data clearly.",Industry Type: Insurance,"Department: BFSI, Investments & Trading","Employment Type: Full Time, Permanent","['MIS Preparation', 'Advanced Excel', 'MIS Reporting']",2025-06-14 06:10:04
Business Analyst (Shipping domain exp is Mandatory) 5-7 yrs Chennai!!,Kumaran Systems,5 - 7 years,Not Disclosed,['Chennai'],"We are hiring for Senior Business Analyst 6-8 years (Preferably to join immediate/less than 15 days) !!\n\nLocation :Chennai (Sirsueri- work from office - 5 days)\n\nAbout us ,We are a rapidly growing NASSCOM 2009-listed IT-enabling company with footprint in four countries. For two decades now, we have provided our global clientele with a range of high-end IT services including migration support, system integration, and infrastructure management solutions. Equipped with two development centres located at Chennai, Kumaran Systems can be that one-stop-shop for every IT solution that your business demands. Kumaran has been ranked among the Top 500 global software companies (16th Annual Ranking by Software Magazine) and among the Top 200 Indian software companies (3rd Annual Ranking by Data quest).",,,,"['Cross Functional Coordination', 'Business Analyst', 'Requirement Gathering', 'Shipping', 'Cross Functional Teams', 'Gap Analysis', 'Use Cases', 'FRS', 'User Stories', 'Uat Scenarios', 'User Acceptance Testing', 'Business Requirement Analysis', 'Wireframing', 'Functional Analysis', 'Brd', 'FRD']",2025-06-14 06:10:06
Manager-Technical Support,Congruent,8 - 12 years,Not Disclosed,['Chennai'],"Manager-Technical Support\nAbout The Role\nAs a member of the Customer Support department and reporting to the Director, Technical Support, you will play a pivotal role as an internal support escalation subject matter expert. Our technical support services provide our customers with resolving technical challenges involving platform issues, data flows, networking, communication failures, and KPI calculations for the customer assets.\nThis role is responsible for overseeing the technical support team to ensure efficient operations and exceptional customer service aligned with our 24/7 Global Support Team. Additionally, it involves managing support processes, enhancing team performance, documentation preparation and maintaining a high standard of technical issue resolution. Finally, it involves managing a small team of technical support specialists.\n\nWhat You Will Be Doing\nLead and manage a team of Technical Support specialists. Provide guidance, training, and mentorship to ensure excellent customer service and technical expertise as well as cultivate a positive work environment, strengthen teamwork, and knowledge sharing to ensure their professional development and function.\nCollaborate with your team to troubleshoot and resolve complex technical issues related to CLIENTs products and other services within. Analyze problems, identify root causes, and implement solutions and making sure to incorporate best practices that are aligned with the Product roadmap and technology considerations Interact with customers as well as the technical support team to understand their challenges and requirements. Provide clear and concise explanations of technical concepts in writing and verbally to our customers on the resolution of tickets via email, meetings or conference calls.\nManage and escalate high-priority and critical customer issues to appropriate levels within the organization.\nPerform standard personnel management responsibilities to support team member development, goal setting, annual performance reviews as well as vacation and paid time off approvals.\nCoordinate the day-to-day work of a technical support team focused on successfully maintaining the customer assets on CLIENTs products.\nEnsure accurate documentation of technical issues, solutions, and best practices. Maintain a knowledge base to aid in efficient issue resolution and to facilitate continuous learning.\nContinuously evaluate and improve technical support processes to enhance efficiency and customer satisfaction. Identify trends in support requests and work with relevant teams to address recurring issues.\nCo-operate with the rest Technical Support Managers and your Director to improve technical support processes and documentation\nServe as a technical expert on CLIENTs platform with our customers for data-related issues, such as data flow issues, KPI calculations, and software logic.\n\nWhat You Will Need To` Be Successful\nBachelor's degree in engineering (Renewable /Communication/Data Engineering preferred).\nMinimum of 5 years of experience in a technical support role in the renewable industry or operations & maintenance (O&M).\nExtensive knowledge of Freshdesk as well as Technical Support ticketing workflows.\nPrevious working experience in project management, schedule management, risk management, and issue resolution.\nGood understanding of renewable energy sources: Such as Wind, solar, and energy storage systems\nKnowledge of data acquisition systems, SCADA. Ability to interpret networking, data flows, and electrical diagrams.\nKnowledge of common industrial communications protocols like Modbus RTU/TCP, OPC DA/UA, and DNP3.\nUnderstanding of OSI SoftsPI system and SQL Database.\n\nPreferred Experience\nMinimum of 2 years' experience as a lead or manager of a technically focus team\nKnowledge of SQL, Python, or other scripting tools to perform data analysis.\nKnowledge of Linux\n\nCompetencies -Technical Skills\nTechnical Knowledge and Expertise. A deep understanding of the products, services, or systems being supported is essential.\nTroubleshooting. This includes researching problems, following procedures, and using relevant tools.\nSoftware Proficiency. Support team members should be proficient in using relevant software and tools, such as CRM systems, ticketing systems, diagnostic tools, and communication platforms.\nProduct Knowledge Management. Managing and updating knowledge bases is critical for providing consistent and accurate information to customers or colleagues.\nInnovation. Innovation is a valuable competency for support team members, enabling them to find creative solutions to customer issues and improve internal processes.\n\nLeadership Skills\nCommunication. Effective communication fosters understanding, trust, and collaboration within a team.\nEmotional Intelligence. It's vital for building strong relationships, resolving conflicts, and making empathetic decisions.\nDecision-Making. Leaders need strong decision-making skills, including analyzing situations, weighing pros and cons, and making informed choices.\nAdaptability. Leaders should be open to new ideas, pivot when needed, and help their teams navigate change effectively.\nInspiring and Motivating. Leaders set a compelling vision, set clear goals, and provide encouragement and recognition.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Technical Support', 'SCADA', 'Operation And Maintanance', 'RTU/TCP']",2025-06-14 06:10:09
Senior Full Stack Developer,Maimsd Technology,6 - 11 years,Not Disclosed,['Bengaluru'],"Job Title : Senior Full Stack Developer\n\nExperience : 6 to 13 Yrs\n\nMandatory Skills :\n\n- JavaScript, React & Redux\n- Node, Express & .net\n\n- DB - SQL\n\n- CI/CD\n\n- MobX, RTK (Redux Tool Kit), Zustand\n\nAbout the Role :\n\nYou will work with our development and production support team on efforts to support Arcus Power's growth initiatives. The Full Stack Developer will play a pivotal role in developing leading SaaS products for energy power market intelligence. Arcus products provide insights and data-driven solutions to stakeholders in the energy industry.\n\nWith your expertise in microservices, APIs, AI/ML integration, and cloud technologies, you will be instrumental in driving the evolution of our software applications. Your ability to work across the stack, from frontend to backend, and integrate cutting-edge technologies will be essential in delivering innovative and high-quality solutions. Collaborating with cross-functional teams, including data scientists, UX designers, and DevOps engineers, you will play a key role in revamping legacy applications, building microservices, and infusing AI/ML capabilities into web user interfaces.\n\nResponsibilities will include :\n\n- Design, develop, and maintain scalable and responsive web applications using modern frontend and backend technologies.\n\n- Lead the revamping of legacy applications, ensuring modernization, improved performance, and UX.\n\n- Create and manage microservices architecture, including API design, development, and integration.\n\n- Collaborate with data scientists to integrate AI/ML capabilities into web user interfaces for predictive analytics and data-driven insights.\n\n- Integrate SQL and NoSQL databases, optimizing data storage and retrieval for efficient application performance.\n\n- Work with cloud platforms such as Azure or other relevant providers to deploy, monitor, and manage applications.\n\n- Implement DevOps practices for CI/CD, and automated testing.\n\n- Collaborate with UX/UI designers to create visually appealing and user-friendly interfaces.\n\n- Troubleshoot and debug issues, identifying root causes and implementing effective solutions.\n\n- Keep up-to-date with industry trends and emerging technologies to drive innovation in application development.\n\n- Teams onboarding and training junior development staff\n\n- Other duties as assigned\n\nYour Qualifications :\n\n- Bachelor's or Master's degree in Computer Science, Software Engineering, or a related field\n\n- Proven experience as a Full Stack Developer with expertise in either frontend or backend development, or both\n\n- Strong proficiency in web development technologies, including HTML, CSS, JavaScript, and modern frontend frameworks (e.g., React, Angular, Vue). Deep knowledge of .NET languages and servers\n\n- Experience in designing and implementing microservices architecture, RESTful APIs, and integration patterns\n\n- Proficiency in both SQL and NoSQL databases and their integration into applications.\n\n- Experience administrating and integrating with cloud platforms such as Azure, AWS, or Google Cloud Platform.\n\n- Previous experience in successfully revamping and modernizing legacy applications.\n\n- Experience with Agile methodologies and participation in sprint planning and review meetings.\n\n- Familiarity with integrating AI/ML capabilities into web user interfaces for data visualization and insights.\n\n- Knowledge of DevOps practices, CI/CD pipelines, and automated testing.\n\n- Familiarity with MLOps methodologies and best practices.\n\nExperience considered assets :\n\n- Integrating with Snowflake, Databricks, or other data lake technologies\n\n- Utilizing, extending, and integrating business intelligence tools like Grafana\n\n- Integrating with iPaaS (Integration Platforms as a Service)\n\n- Leveraging graphs for modeling data and organizing metadata, semantics.\n\nKey Competencies & Attributes :\n\n- Delivering in self-organizing teams with high levels of trust and transparency\n\n- Strong problem-solving skills and ability to troubleshoot and debug complex issues\n\n- Strong organizational skills and ability to manage multiple projects and priorities\n\n- Adaptability to evolving technology landscapes and industry trends\n\n- Excellent collaboration and verbal/written communication skills, with the ability to work effectively in cross functional teams\n\nYour Metrics :\n\nSuccess in this position will be measured against the following groups of Metrics :\n\n- Development Performance : Sustain and maintain an acceptable pace of development according to sprint plans, and backlog items, as per direction by product manager and owner.\n\n- Deliverables Quality : For us, quality of deliverables will be critical to our success and to scale. The deliverables include proper documentation to handover to other groups.\n\n- Application Performance : Measure its responsiveness and efficiency in real-world usage.\n\n- Legacy Application Modernization : Track the progress of legacy application revamping and performance improvements.\n\n- AI/ML Integration Success : Monitor the successful integration of AI/ML capabilities into web interfaces.\n\n- Microservices Architecture : Measure the efficiency and scalability of the microservices architecture.\n\n- DevOps Efficiency : Measure the effectiveness of CI/CD pipelines and automated testing in the development process.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Full Stack', 'Microservices Architecture', 'Javascript', '.Net', 'Node.js', 'Express.js', 'React.js', 'SQL']",2025-06-14 06:10:11
Cyber Security Expert // 7-10 years // Mumbai,2coms,7 - 12 years,Not Disclosed,['Mumbai'],"SUMMARY\nOur client is IT MNC part of one of the major insurance groups based out of Germany and Europe. The Group is represented in around 30 countries worldwide, with Over 40,000 people worldwide, focusing mainly on Europe and Asia. Our client offers a comprehensive range of insurances, pensions, investments and services by focusing on all cutting edge technologies majorly on Could, Digital, Robotics Automation, IoT, Voice Recognition, Big Data science, advanced mobile solutions and much more to accommodate the customers future needs around the globe thru supporting millions of internal and external customers with state of-the-art IT solutions to everyday problems & dedicated to bringing digital innovations to every aspect of the landscape of insurance.\n \nJob Location: Hiranandani Gardens, Powai, Mumbai\n \nMode: Work from Office\n\n\nRequirements\nKey Responsibilities: :\n         Business-Cybersecurity Alignment:\no     Work closely with business stakeholders, IT security teams, and cross-functional teams to ensure cybersecurity initiatives align with the organization’s broader business goals.\no     Translate business needs into technical security requirements that can be effectively executed by the security and IT teams.\n           Risk Analysis & Security Assessments:\no     Conduct risk assessments in the context of hybrid IT environments (cloud, on-premises, and edge) to identify security gaps and vulnerabilities.\no     Collaborate with security teams to evaluate existing security controls and recommend solutions to mitigate identified risks, balancing business needs with security requirements.\n         Cybersecurity Frameworks & Compliance:\no     Ensure that all business and technical security requirements comply with relevant regulatory compliance frameworks (e.g., NIST CSF, ISO 27001, GDPR, HIPAA).\no     Support audits and compliance assessments, identifying any gaps between current practices and regulatory standards. (must have)\n         Security Process Improvement:\no     Identify opportunities for process improvements within the cybersecurity function, including streamlining security incident response, access management processes, and threat detection workflows.\no     Develop business cases for proposed security improvements, including cost-benefit analyses and risk assessments.\n         The Business Analyst will have comprehensive responsibilities spanning multiple cybersecurity domains, and should have expertise in at least 5 of the following areas\no     SIEM Sentinel & Security Operations:\n  Manage and optimize SIEM solutions, particularly Sentinel, for effective monitoring, incident detection, and security event correlation across hybrid environments.\n  Collaborate with security operations teams to ensure proper configuration, tuning, and reporting within SIEM platforms to support proactive threat management.\no     Security Tools & Technology Integration:\n  Work with security teams to implement and optimize security tools such as SIEM (e.g., Splunk, Microsoft Sentinel), EDR (e.g., CrowdStrike, MS Purview/Defender), SOAR platforms, CASB (Cloud Access Security Broker), and Threat Intelligence systems.\n  Help define and document requirements for the integration of cybersecurity tools into the broader security ecosystem.\no     User Access Management (UAM) & RBAC:\n  Work closely with identity and access management teams to ensure the implementation of UAM and RBACsystems that align with the organization's security policy and business requirements.\n  Support the development of processes for managing user roles, privileges, and access rights across enterprise systems.\no     Cloud & Encryption Security:\n  Ensure that security policies and controls are applied across both on-premises and cloud environments(AWS, Azure, Google Cloud), addressing challenges related to cloud security, data encryption, and access management.\n  Collaborate with technical teams to implement strong encryption methods for data - in - transit, data-at-rest, and data-in-use in line with organizational security policies.\no     AI & ML in Cybersecurity: (Good to have)\n  Contribute to the use of AI/ML technologies to enhance threat detection, anomaly identification, and predictive analytics within the organization’s security operations.\n  Collaborate with data scientists and security teams to define requirements for AI/ML-based security models and incident response automation.\no     SOAR Integration & Incident Response:\n  Assist with the integration of Security Orchestration, Automation, and Response (SOAR) solutions into the incident response lifecycle to streamline response times and automate repetitive tasks.\n  Support the continuous improvement of incident response procedures and playbooks, ensuring a consistent, rapid, and efficient approach to security incidents.\n        \n\n\nBenefits\n",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['security operations', 'access', 'iso', 'hipaa', 'soc', 'siem', 'microsoft', 'artificial intelligence', 'incident response', 'cloud', 'security', 'encryption', 'gcp', 'cmbs', 'sentinel', 'it security', 'cloud security', 'cyber security', 'information security', 'microsoft azure', 'broker', 'nist', 'cyber', 'soar', 'threat intelligence', 'splunk', 'aws']",2025-06-14 06:10:14
"Manager, SAP ABAP",Merck Sharp & Dohme (MSD),5 - 9 years,Not Disclosed,['Hyderabad'],"Based in Hyderabad, join a global healthcare biopharma company and be part of a 130- year legacy of success backed by ethical integrity, forward momentum, and an inspiring mission to achieve new milestones in global healthcare.\nBe part of an organisation driven by digital technology and data-backed approaches that support a diversified portfolio of prescription medicines, vaccines, and animal health products.\nDrive innovation and execution excellence. Be a part of a team with passion for using data, analytics, and insights to drive decision-making, and which creates custom software, allowing us to tackle some of the worlds greatest health threats.\nOur Technology Centers focus on creating a space where teams can come together to deliver business solutions that save and improve lives. An integral part of our company s IT operating model, Tech Centers are globally distributed locations where each IT division has employees to enable our digital transformation journey and drive business outcomes. These locations, in addition to the other sites, are essential to supporting our business and strategy.\nA focused group of leaders in each Tech Center helps to ensure we can manage and improve each location, from investing in growth, success, and well-being of our people, to making sure colleagues from each IT division feel a sense of belonging to managing critical emergencies. And together, we must leverage the strength of our team to collaborate globally to optimize connections and share best practices across the Tech Centers.\nRole Overview\nAs a SAP ABAP Engineer, you will be responsible for the design, implementation, customization, and ongoing maintenance of SAP systems within an organization. This role combines both technical and functional expertise to ensure that SAP solutions are effectively integrated and aligned with business processes. SAP ABAP Engineers work closely with both IT teams and business stakeholders to optimize the performance and functionality of SAP applications, ensuring they support business objectives and drive operational efficiency.\nWhat will you do in this role:\nCollaborate with business users and functional teams to gather requirements, design tailored SAP solutions and provide work estimates. Ensure SAP systems are integrated with other business applications and third-party systems.\nDevelop and implement custom applications, enhancements, interfaces, forms and workflows within the SAP environment, often using programming languages like ABAP (Advanced Business Application Programming). Build code that is flexible, easy to maintain, easy to understand, performs well, is secure and robust, in accordance with quality standards.\nPerform technical unit testing and support system integration testing, and user acceptance testing with defect resolution to ensure solutions are working as expected. Troubleshoot and resolve issues related to SAP systems, applications, and integrations.\nMonitor the performance of SAP processes and implement enhancements to improve system efficiency and speed. Provide post-implementation support, including maintenance, troubleshooting, and applying system patches and upgrades.\nWork closely with cross-functional teams (e.g., functional consultants, business analysts, security, infrastructure, middleware and change management) to ensure solutions align with business goals and IT standards. Document system configurations, customizations, and procedures to facilitate knowledge transfer and ensure compliance with best practices.\nWork within a matrix organizational structure, reporting to the technical manager and the project manager.\nParticipate in project planning, execution, and delivery, ensuring alignment with both functional and project goals.\nWhat should you have:\nBachelor s degree in information technology, Computer Science or any Technology stream.\n4+ years of hands-on experience working with SAP technologies like ABAP objects, Fiori/SAPUI5, Workflow, IDocs, Forms, ALV, BRF+, SAP S/4HANA, CDS, OData, Git, BTP, Web IDE / BAS.\nExperience in development at least in two of the SAP Modules (FI, CO, StS, OtC, SCM, MM, QM, PM, SD, MDG, etc.) and integration/implementation with third-party applications. Experience with SAP Gateway, Solution Manager / ALM and SAP upgrade projects\nGood interpersonal and communication skills (verbal and written).\nRelevant SAP certifications (e.g., SAP Certified Technology Associate) are a plus.\nProven record of delivering high-quality results.\nProduct and customer-centric approach.\nInnovative thinking, experimental mindset.\nOur technology teams operate as business partners, proposing ideas and innovative solutions that enable new organizational capabilities. We collaborate internationally to deliver services and solutions that help everyone be more productive and enable innovation.\nOur technology teams operate as business partners, proposing ideas and innovative solutions that enable new organizational capabilities. We collaborate internationally to deliver services and solutions that help everyone be more productive and enable innovation.\nWho we are:\nWhat we look for:\n#HYDIT2025\nCurrent Employees apply HERE\nCurrent Contingent Workers apply HERE\nSearch Firm Representatives Please Read Carefully\nEmployee Status:\nRegular\nRelocation:\nVISA Sponsorship:\nTravel Requirements:\nFlexible Work Arrangements:\nHybrid\nShift:\nValid Driving License:\nHazardous Material(s):\n\nRequired Skills:\nBusiness, Data Engineering, Data Visualization, Design Applications, Program Implementation, SAP ABAP HR, Software Configurations, Software Development, Software Development Life Cycle (SDLC), Solution Architecture, System Designs, Systems Integration, Test and Evaluation (T&E), Testing",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['CVS', 'Change management', 'Workflow', 'Healthcare', 'Project planning', 'Unit testing', 'SCM', 'Middleware', 'Information technology', 'SDLC']",2025-06-14 06:10:16
Business Analyst,Codvo,4 - 6 years,Not Disclosed,['Pune'],"Company Overview:\n\nAt Codvo, software and people transformations go hand-in-hand. We are a global empathy-led technology services company. Product innovation and mature software engineering are part of our core DNA. Respect, Fairness, Growth, Agility, and Inclusiveness are the core values that we aspire to live by each day.\nWe continue to expand our digital strategy, design, architecture, and product management capabilities to offer expertise, outside-the-box thinking, and measurable results.\nTechnical or Functional Responsibilities:\nSuccessfully gather and analyze technical requirements for projects through client interactions requirement gather process such as, workshops, questionnaire, survey etc.\nCreate detailed business requirement documents, functional specifications documents, user journey, process flows and wireframes.\nCreate presentations, documents collaterals such as client pitch decks, governance meeting deck, demo outline document, UAT sign-off document, go-live presentation etc.\nCollaborate with development teams to ensure clear understanding and implementation plan of all functional non-functional requirements.\nTranslate business requirements into technical specifications - maintain azure DevOps board, define features and user stories with detailed descriptions acceptance criteria.\nEnsure alignment with project goals and objectives being met within defined timeline.\nDomain/ Practice Ownership:\nConduct regular research and analysis of industry trends and best practices for one or more dedicated practice areas assigned and report to team/ management on the evolving practices in market in form of practice area specific Solution Catalogue, Market research deck.\nPropose innovative ideas, plans thought leaderships for growth of the practice areas.\nGather in-depth domain know-how techno-functional skills to be champion of the assigned practice areas. Stay updated on emerging trends, and best practices.\nCreate various product solution related collaterals such as, whitepaper, blog, Solution/ Practice pitch deck, case studies etc.\nKnowledge transfer to other team members on domain know-how, to enhance overall team competency.\nPlanning, Estimation, and Execution:\nDevelop project plans and timelines for all pre-sales product-solutioning related activities, ensuring alignment with overall project goals.\nEnsure accurate effort estimations along with development team, contributing to project planning and resource allocation.\nActive participation in project scrum meetings provide necessary inputs or coordinate with other stakeholders for seamless execution without any roadblocks.\nMonitor and report progress on project deliverables, identifying/ addressing potential roadblocks proactively.\nProactive Communication Ownership Responsibilities:\nCommunicate effectively with stakeholders to gather requirements and provide updates throughout the project lifecycle.\nTake ownership of presales solutioning deliverables, ensuring quality, accuracy, and adherence to timeline.\nProactively identify and communicate risks, issues, and dependencies, proposing mitigation strategies.\n\nExperience: 4-6 Years\nLocation: Remote",Industry Type: Software Product,Department: Other,"Employment Type: Full Time, Permanent","['Product management', 'Team management', 'Techno functional', 'Resource allocation', 'Presales', 'Market research', 'digital strategy', 'Product sales', 'Project planning', 'Scrum']",2025-06-14 06:10:19
AI Prompt Engineer,Squarepeg Hires,2 - 7 years,Not Disclosed,['Bengaluru'],"Voicecare AI is a Healthcare Administration General Intelligence (HAGI) company for the back-office and the RCM industry. We are building a safety focused large and small conversational language model for the healthcare industry. Our mission is to dramatically improve access, adherence, and outcomes for the patients and the healthcare workforce through the application of generative AI. We are a venture-backed company partnering with the top healthcare stakeholders in the country.\n\nWe are seeking an AI Prompt Engineer with expertise in advanced prompt engineering techniques to optimize interactions with large language models for healthcare applications. This role will focus on designing, refining, and testing highly effective prompts that enhance the accuracy, relevance, and safety of AI-generated responses while ensuring compliance with healthcare regulations.\n\nResponsibilities:\nPrompt Engineering Optimization\nDesign and develop advanced prompt engineering techniques to improve LLM-generated responses for healthcare applications.\nFine-tune contextual prompts for improved accuracy, coherence, and latency\nConduct rigorous A/B testing and evaluation of prompt strategies to optimize performance.\nLLM Training and Customization\nWork closely with ML and NLP engineers to fine-tune and train SLM and LLMs for domain-specific tasks.\nCollaborate on data curation, augmentation, and preprocessing to improve model outputs.\nGenerative AI Conversational AI Development\nCreate prompts optimized for healthcare-related interactions, summarization, and decision support.\nPerformance Monitoring Model Improvement\nDevelop advanced metrics and evaluation frameworks to assess prompt effectiveness.\nImplement continuous learning mechanisms to refine AI responses based on user feedback.\nCollaboration Research\nStay ahead of emerging trends in generative AI, LLM advancements, and NLP techniques via research papers\nCollaborate with healthcare professionals, data engineers, and software engineers to integrate AI solutions into real-world workflows.\nSkills and Experience:\nAdvanced Prompt Engineering: Strong demonstrated experience in crafting effective, structured prompts for AI models.\nGenerative AI LLMs: Hands-on experience working with GPT models, LLaMA, or other state-of-the-art open and closed sourced architectures.\nNLP Expertise: Experience in natural language processing techniques such as semantic search, intent recognition, and contextual AI.\nProgramming: Strong proficiency in Python, with experience using LangChain,,Transformers\nA/B Testing Model Evaluation: Expertise in designing experiments to assess prompt effectiveness and model accuracy.\nHealthcare Industry Knowledge (Preferred): Familiarity with healthcare workflows, clinical documentation, and medical terminology.\nQualifications:\nBachelor s or Master s degree in Computer Science, AI, NLP, Data Science, or a related field.\n2+ years of experience in prompt engineering, NLP, or generative AI.\nExperience working with LLM-based applications in regulated industries (healthcare preferred).\nStrong portfolio of AI-driven projects, showcasing prompt engineering and model interaction improvements.\nRelevant certifications (e.g., Prompt Engineering, AWS/GCP AI Certifications) are a plus.\nCandidates located in Bangalore is a plus\nCandidates from top universities is a plus",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data science', 'GCP', 'Back office', 'Healthcare', 'Natural language processing', 'Research', 'Performance monitoring', 'Python', 'Testing']",2025-06-14 06:10:21
Business Analyst,Mytech Consulting Services,4 - 7 years,5-12 Lacs P.A.,['Hyderabad'],"Job Description\nJob Summary:\nWe are looking for a business analyst who can act as the project manager/Lead on several exciting projects.\nWe are looking for an energetic, enthusiastic, and passionate professional, who can work with minimal supervision.\nExcellent verbal and written communication skills are essential including the ability to author comprehensive documentation.\nKey Responsibilities:\nIdentify, analyse, and elicit business requirements\nFacilitate workshops with business and technical stakeholders\nEngage stakeholders throughout the life cycle\nWork in multidiscipline teams in an agile development environment\nSpecify business and functional requirements via user stories, use cases, workshops, process models in a 'just in time' collaborative environment\nLiaise with business and technical staff to ensure the documented solution is fit for purpose.\nElicit and define requirements and functionality through Agile techniques and methods such as epics, stories, tasks, and acceptance criteria\nEnsure requirements and user stories are clear, concise, comprehensible, and that can be turned into code and tested by the delivery team\nPerform ad-hoc analysis using a range of techniques to clarify and remove impediments to delivery and improve delivery speed\nProvide functional expertise to testers for test cases and to developers for user story details\nCreate and/or review test plans for each narrative to ensure test coverage\nPerform hands on testing tasks and activities as required\nPerform hands on change analysis and change management tasks\nKey Requirements:\n4 years experience as Business and/or System Analyst\nExperience with use cases / user stories / wireframes / user navigation\nExperience implementing web-based applications\nExcellent documentation - business and functional requirements\nExcellent stakeholder management skills and ability to resolve problems with conflicting needs / requirements from a diverse group\nExperience in running requirements workshops / interviews\nAbility to deal with complex data requirements\nExperience managing projects and development activities.\nPreferred:\nScrum master experience\nAny Project Management certificationRole & responsibilities\n\n\nPreferred candidate profile",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Business Analyst', 'System Analyst', 'Use Cases', 'User Stories', 'Business Fuctional Requirements', 'wireframes', 'user navigation', 'web-based applications']",2025-06-14 06:10:24
AI/ML Platform Engineer,Johnson Controls,5 - 6 years,Not Disclosed,['Pune'],"Job Title ML Platform Engineer - AI Data Platforms\nML Platform Engineering MLOps (Azure-Focused)\nBuild and manage end-to-end ML/LLM pipelines on Azure ML using Azure DevOps for CI/CD, testing, and release automation.\nOperationalize LLMs and generative AI solutions (e. g. , GPT, LLaMA, Claude) with a focus on automation, security, and scalability.\nDevelop and manage infrastructure as code using Terraform , including provisioning compute clusters (e. g. , Azure Kubernetes Service, Azure Machine Learning compute), storage, and networking.",,,,"['Computer science', 'Automation', 'orchestration', 'Networking', 'data science', 'devops', 'Machine learning', 'Monitoring', 'Auditing', 'Python']",2025-06-14 06:10:26
Business Analyst,Visions,3 - 7 years,Not Disclosed,['Chandigarh'],"Roles and Responsibilities\n\nProposal Writing for RFx (RFI/RFQ/RFP) and requirement gathering documentation(SOW).\nSolution Preparation as a well-structured and well-designed proposal to present a solution based on Customers Requirements.\nAssist Proposal Lead/Manager to create and manage a full cycle of IT Services and Solutions proposal development schedule from RFI/RFP release to proposal submission, ensuring all deadlines are met and action items are closed and completed in a timely manner.\nUtilize proposal templates; follow proposal-writing standards including readability, consistency, and tone; keep proposal support databases up to date.\nEnsure submissions are accurate, complete, and compliant with RFP requirements and editorial specifications, as well as corporate quality and branding standards.\nWrite technology specific content for sales collateral, presentations, case studies, etc.\nWork closely with Technical team members for preparing estimates & commercial proposals.\nSolution Preparation as a well-structured and well-designed proposal to present a solution based on Customers Requirements.\nAssess the impact and feasibility of the solution from a business perspective while collaborating with the business and technical team.\n\nDesired Candidate Profile\n\nThe skill to independently define, schedule, and manage the proposal preparation process, including the ability to understand and analyze the details of an RFP.\nMicrosoft Word, Microsoft Excel, and Microsoft PowerPoint.\nHighly organized, team-oriented, enthusiastic, independent thinker, and collaborative.\nOutstanding written and oral communication are critical.",Industry Type: IT Services & Consulting,Department: Sales & Business Development,"Employment Type: Full Time, Permanent","['proposal preparation', 'business analysis', 'proposal writing', '2-4 years in proposal management', 'and the direct management of proposal operational organizations']",2025-06-14 06:10:28
AI / ML Engineer – Computer Vision & Generative AI,Lericon Informatics,2 - 6 years,Not Disclosed,"['Hyderabad', 'Bengaluru', 'Delhi / NCR']","Job Summary:\nWe are seeking a passionate and technically skilled AI / ML Engineer with 2+ years of hands-on experience in Computer Vision and Generative AI (GenAI). This role involves building and optimizing advanced visual intelligence systems leveraging PyTorch, OpenCV, transformers, and diffusion models. The ideal candidate will have experience in areas such as image segmentation, pose estimation, video inpainting, and synthetic data generation, along with exposure to LLM-prompt integration and datasets like DeepFashion2 and COCO.\n\nKey Responsibilities:\n\nDevelop and deploy advanced computer vision models for tasks such as image segmentation, pose estimation, and video manipulation.\nImplement and experiment with Generative AI techniques, including diffusion models, GANs, and video inpainting.\nLeverage PyTorch, OpenCV, and modern deep learning frameworks to build scalable vision pipelines.\nIntegrate LLM-based prompting into visual workflows and multimodal applications.\nUtilize datasets such as DeepFashion2, COCO, and custom synthetic datasets for model training and validation.\nOptimize model inference for performance, latency, and resource efficiency in production environments.\nCollaborate with data scientists, product engineers, and designers to deliver intelligent visual features in end-user applications.\n\nQualifications:\n\n2+ years of experience in machine learning and computer vision.\nProficiency with PyTorch, OpenCV, and relevant deep learning libraries.\nHands-on experience with image segmentation, pose estimation, or video-based vision tasks.\nUnderstanding of diffusion models, GANs, and transformer-based architectres.\nKnowledge of LLM prompt engineering and multimodal integration.\nExperience working with structured datasets like COCO and DeepFashion2.\nStrong debugging, analytical, and performance optimization skills.\n\nPreferred Qualifications :\n\nExperience in synthetic data generation and domain-specific data augmentation.\nFamiliarity with model quantization, pruning, or other inference optimization techniques.\nExposure to MLOps tools and cloud-based model deployment (e.g., AWS Sagemaker, GCP AI Platform).\nContributions to open-source projects or published research in computer vision or generative models.\nLocation: Pan- Bengaluru,Hyderabad,Delhi / NCR,Chennai,Pune,Kolkata,Ahmedabad,Mumbai",Industry Type: Banking,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer Vision', 'Diffusion Models', 'Image Segmentation', 'COCO', 'Pose Estimation', 'DeepFashion2', 'LLM Integration', 'Synthetic Data', 'PyTorch', 'Video Inpainting', 'OpenCV', 'Transformers', 'GANs']",2025-06-14 06:10:30
MLOps Engineer,Lericon Informatics,2 - 7 years,Not Disclosed,"['Mumbai', 'Delhi / NCR', 'Bengaluru']","Job Summary:\nWe are looking for a highly capable and automation-driven MLOps Engineer with 2+ years of experience in building and managing end-to-end ML infrastructure. This role focuses on operationalizing ML pipelines using tools like DVC, MLflow, Kubeflow, and Airflow, while ensuring efficient deployment, versioning, and monitoring of machine learning and Generative AI models across GPU-based cloud infrastructure (AWS/GCP). The ideal candidate will also have experience in multi-modal orchestration, model drift detection, and CI/CD for ML systems.\n\nKey Responsibilities:\nDevelop, automate, and maintain scalable ML pipelines using tools such as Kubeflow, MLflow, Airflow, and DVC.\nSet up and manage CI/CD pipelines tailored to ML workflows, ensuring reliable model training, testing, and deployment.\nContainerize ML services using Docker and orchestrate them using Kubernetes in both development and production environments.\nManage GPU infrastructure and cloud-based deployments (AWS, GCP) for high-performance training and inference.\nIntegrate Hugging Face models and multi-modal AI systems into robust deployment frameworks.\nMonitor deployed models for drift, performance degradation, and inference bottlenecks, enabling continuous feedback and retraining.\nEnsure proper model versioning, lineage, and reproducibility for audit and compliance.\nCollaborate with data scientists, ML engineers, and DevOps teams to build reliable and efficient MLOps systems.\nSupport Generative AI model deployment with scalable architecture and automation-first practices.\n\nQualifications:\n2+ years of experience in MLOps, DevOps for ML, or Machine Learning Engineering.\nHands-on experience with MLflow, DVC, Kubeflow, Airflow, and CI/CD tools for ML.\nProficiency in containerization and orchestration using Docker and Kubernetes.\nExperience with GPU infrastructure, including setup, scaling, and cost optimization on AWS or GCP.\nFamiliarity with model monitoring, drift detection, and production-grade deployment pipelines.\nGood understanding of model lifecycle management, reproducibility, and compliance.\n\nPreferred Qualifications :\nExperience deploying Generative AI or multi-modal models in production.\nKnowledge of Hugging Face Transformers, model quantization, and resource-efficient inference.\nFamiliarity with MLOps frameworks and observability stacks.\nExperience with security, governance, and compliance in ML environments.\nLocation-Delhi NCR,Bangalore,Chennai,Pune,Kolkata,Ahmedabad,Mumbai,Hyderabad",Industry Type: Banking,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['MLOps', 'Airflow', 'MLflow', 'Hugging Face', 'GenAI Deployment', 'DVC', 'Kubeflow', 'Model Drift', 'Docker', 'GPU Infrastructure', 'CI/CD', 'Model Versioning', 'Kubernetes']",2025-06-14 06:10:32
Software Engineer,Blackline,3 - 8 years,Not Disclosed,['Bengaluru'],"Get to Know Us:\n\nIts fun to work in a company where people truly believe in what theyre doing!\nAt BlackLine, were committed to bringing passion and customer focus to the business of enterprise applications.\nSince being founded in 2001, BlackLine has become a leading provider of cloud software that automates and controls the entire financial close process. Our vision is to modernize the finance and accounting function to enable greater operational effectiveness and agility, and we are committed to delivering innovative solutions and services to empower accounting and finance leaders around the world to achieve Modern Finance.\nBeing a best-in-class SaaS Company, we understand that bringing in new ideas and innovative technology is mission critical. At BlackLine we are always working with new, cutting edge technology that encourages our teams to learn something new and expand their creativity and technical skillset that will accelerate their careers.\nWork, Play and Grow at BlackLine!\n\nMake Your Mark:\n\nAs a member of the Data BI Engineering team you will primarily focus on advancing our Enterprise Data Platform to allow the organization to make data-driven decisions. The successful candidate will work closely with cross-functional teams to identify business requirements, design, and develop data models, data warehouses, and data visualization solutions that help support the organizations strategic goals.\nThe Data Engineer will work in a dynamic environment and will be required to stay current with the latest trends and technologies in the business intelligence field. The ideal candidate will be able to pick up business domain and internal process knowledge and leverage that knowledge to think strategically, communicate effectively, and manage multiple projects simultaneously.\nThe team is also responsible for administering tools and platforms around reporting, analytics, and data visualization while promoting best practices. The role requires a strong combination of technical expertise, leadership skills, and a deep understanding of data engineering principles and best practices. We are looking for a driven, detail-oriented, and passionate engineer to come to join our team.\n\nYoull Get To:\n\nProvide technical expertise and leadership in technology direction, road-mapping, architecture definition, design, development, and delivery of enterprise-class solutions while adhering to timelines, coding standards, requirements, and quality.\nArchitect, design, develop, test, troubleshoot, debug, optimize, scale, perform the capacity planning, deploy, maintain, and improve software applications, driving the delivery of high-quality value and features to Blackline s customers.\nWork collaboratively across the company to design, communicate and further assist with adoption of best practices in architecture and implementation.\nDeliver robust architectural solutions for complex design problems.\nImplement, refine, and enforce data engineering best practices to ensure that delivered features meet performance, security, and maintainability expectations.\nResearch, test, benchmark, and evaluate new tools and technologies, and recommend ways to implement them in data platform. Identify and create solutions that are likely to contribute to the development of new company concepts while keeping in mind the business strategy, short- and long-term roadmap, and architectural considerations to support them in a highly scalable and easy extensible manner.\nActively participate in research, development, support, management, and other company initiatives designing solutions to optimally address current and future business requirements and infrastructure plans.\nInspire a forward-thinking team of developers, acting as an agent of change and evangelist for a quality-first culture within the organization. Mentor and coach key technical staff and guide them to solutions on complex design issues.\nAct as a conduit for questions and information flow when those outside of Engineering have ideas for new technology applications.\nSpeak in terms relevant to audience, translating technical concepts into non-technical language and vice versa. Facilitate consensus building while striving for win/win scenarios and elicit value-add contributions from all team members in group settings.\nMaintain a strong sense of business value and return on investment in planning, design, and communication.\nProactively identify issues, bottlenecks, gaps, or other areas of concern or opportunity and work to either directly affect change, or advocate for that change by working with peers and leadership to build consensus and act.\nPerform critical maintenance, deployment, and release support activities, including occasional off-hours support.\n\nWhat Youll Bring:\n\nBachelors or Masters degree in Computer Science, Data Science, or a related field. 3+ years as a data engineer. 3+ years of experience using RDBMS, SQL, NoSQL, Python, Java, or other programming languages is a plus. 3+ years of experience designing, developing, testing, and implementing Extract, Transform and Load (ELT/ETL) solutions using enterprise ELT/ETL tools and Open source. 2+ years working experience with SQL and familiarity with Snowflake data warehouse, strong working knowledge in stored procedures, CTEs, and UDFs, RBAC Knowledge of data integration and data quality best practices Familiarity with data security and privacy regulations. Experience in working in a startup-type environment, good team player, and can work independently with minimal supervision Experience with cloud-native architecture and data solutions. Strong working knowledge in data modeling, data partitioning, and query optimization Demonstrated knowledge of development processes and agile methodologies. Strong analytical and interpersonal skills, comfortable presenting complex ideas in simple terms. Proficient in managing large volumes of data. Strong analytical and interpersonal skills, comfortable presenting complex ideas in simple terms. Strong communication and collaboration skills, with the ability to work effectively with cross-functional teams. Experience in providing technical support and troubleshooting for data-related issues. Expertise with at least one cloud environment and building cloud native data services. Prior experience driving data governance, quality, security initiatives.\n\nWe re Even More Excited If You Have:\n\nExperience with Google Cloud or similar cloud provider\nSignificant experience with open source platforms and technologies.\nExperience with data science and machine learning tools and technologies is a plus.\n\nThrive at BlackLine Because You Are Joining:\n\nA technology-based company with a sense of adventure and a vision for the future. Every door at BlackLine is open. Just bring your brains, your problem-solving skills, and be part of a winning team at the worlds most trusted name in Finance Automation!\nA culture that is kind, open, and accepting. Its a place where people can embrace what makes them unique, and the mix of cultural backgrounds and varying interests cultivates diverse thought and perspectives.\nA culture where BlackLiners continued growth and learning is empowered. BlackLine offers a wide variety of professional development seminars and inclusive affinity groups to celebrate and support our diversity.\nBlackLine is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to sex, gender identity or expression, race, ethnicity, age, religious creed, national origin, physical or mental disability, ancestry, color, marital status, sexual orientation, military or veteran status, status as a victim of domestic violence, sexual assault or stalking, medical condition, genetic information, or any other protected class or category recognized by applicable equal employment opportunity or other similar laws.\nBlackLine recognizes that the ways we work and the workplace itself have shifted. We innovate in a workplace that optimizes a combination of virtual and in-person interactions to maximize collaboration and nurture our culture. Candidates who live within a reasonable commute to one of our offices will work in the office at least 2 days a week.\nBachelors or Masters degree in Computer Science, Data Science, or a related field. 3+ years as a data engineer. 3+ years of experience using RDBMS, SQL, NoSQL, Python, Java, or other programming languages is a plus. 3+ years of experience designing, developing, testing, and implementing Extract, Transform and Load (ELT/ETL) solutions using enterprise ELT/ETL tools and Open source. 2+ years working experience with SQL and familiarity with Snowflake data warehouse, strong working knowledge in stored procedures, CTEs, and UDFs, RBAC Knowledge of data integration and data quality best practices Familiarity with data security and privacy regulations. Experience in working in a startup-type environment, good team player, and can work independently with minimal supervision Experience with cloud-native architecture and data solutions. Strong working knowledge in data modeling, data partitioning, and query optimization Demonstrated knowledge of development processes and agile methodologies. Strong analytical and interpersonal skills, comfortable presenting complex ideas in simple terms. Proficient in managing large volumes of data. Strong analytical and interpersonal skills, comfortable presenting complex ideas in simple terms. Strong communication and collaboration skills, with the ability to work effectively with cross-functional teams. Experience in providing technical support and troubleshooting for data-related issues. Expertise with at least one cloud environment and building cloud native data services. Prior experience driving data governance, quality, security initiatives.",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Automation', 'Data modeling', 'RDBMS', 'Coding', 'Agile', 'Business intelligence', 'Open source', 'Technical support', 'SQL', 'Python']",2025-06-14 06:10:34
Ai Ml Engineer,Fortune India 500 IT Services Company,5 - 9 years,12-22 Lacs P.A.,"['Hyderabad', 'Pune', 'Bengaluru']","5+yrs experience\nProficiency in ML frameworks (e.g., TensorFlow, Py Torch).\nExperience with natural language processing (NLP) and large language models (LLMs).\nUnderstanding of generative models and their applications.\nProficiency in programming languages such as Python, Go, or Java.\nExperience in developing APIs and integrating AI models into existing systems.\nFamiliarity with containerization tools (e.g., Docker, Kubernetes).\nExperience with (CI/CD) pipelines and databases or data lakes, and/or real-time data processing frameworks.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['AI', 'Ml', 'Natural Language Processing', 'Aiml', 'Machine Learning', 'Deep Learning', 'Python']",2025-06-14 06:10:37
Facilities Information Management Engineer,Chevron India,1 - 4 years,Not Disclosed,['Bengaluru'],"Total Number of Openings\n1\nAbout the position:\n\nThe Facilities Engineering Information Management Engineer is part of the Technical Services Team within the Chevron ENGINE Center and supports the process, guidelines, and best practices for managing enterprise facilities information. This role involves ensuring data integrity, security, and compliance with engineering functional IM requirements, as well as supporting the development and maintenance of engineering information systems.\n\nKey responsibilities:\n\n1. Functional Data Governance: Establish and support the delivery of data governance policies and procedures to ensure data integrity, security, and compliance with regulatory requirements.\n2. Information Systems Management: Support the development, implementation, and maintenance of engineering information systems, ensuring they meet the organization's needs and support business objectives.\n3. Collaboration: Work closely with various departments, including IT, digital engineering, and business units, to understand their information needs and provide guidance about solutions.\n4. Training and Support: Provide training and support to staff on information management best practices and the use of information systems.\n5. Continuous Improvement: Identify opportunities for improving facilities information management processes and systems, and implement changes to enhance efficiency and effectiveness.\n\nRequired Qualifications:\n\nBachelor s degree in a related engineering discipline (mechanical, chemical, etc.) (B.E./B.Tech.) from a deemed/recognized (AICTE) university\nPrior capital projects on the engineering management and or information management teams.\n\nPreferred Qualifications:\n\nData engineering or data science certification\nExperience with data management tools and software.\n5+ years of Information Management experience, knowledgeable with several Engineering IM systems and is able to provide guidance to users of these systems, including training, sharing best practices and driving adoption of IM practices.\n\nChevron ENGINE supports global operations, supporting business requirements across the world. Accordingly, the work hours for employees will be aligned to support business requirements. The standard work week will be Monday to Friday. Working hours are 8:00am to 5:00pm or 1:30pm to 10:30pm.\nChevron participates in E-Verify in certain locations as required by law.",Industry Type: Oil & Gas,Department: Administration & Facilities,"Employment Type: Full Time, Permanent","['global operations', 'Technical services', 'Data management', 'Engineering management', 'data science', 'Information systems management', 'data governance', 'data integrity', 'Information management', 'Continuous improvement']",2025-06-14 06:10:39
MI Reporting Engineer,Barclays,2 - 6 years,Not Disclosed,['Pune'],"Join us as a MI Reporting Engineer at Barclays, where youll spearhead the evolution of our digital landscape, driving innovation and excellence. Youll harness cutting-edge technology to revolutionise our digital offerings, ensuring unparalleled customer experiences. As a part of team of developers, you will deliver technology stack, using strong analytical and problem solving skills to understand the business requirements and deliver quality solutions.\n\nTo be successful as a MI Reporting Engineer you should have experience with:\nHands on experience in developing complex/medium/easy reports in Tableau, QlikView SAP BO reports.\nComfortable with Extracting, transforming and loading data from multiple sources such as Teradata and Hive into BI tools.\nExperience in Snowflake / AWS Quicksight preferrable.\nCreate performance efficient data models and dashboards.\nSolid working knowledge of writing SQL queries in Teradata and Hive/Impala\nExperience in writing PySpark queries and exposure to AWS Athena\nAttention to details with strong analytical and problem solving skills.\nExceptional communication and interpersonal skills\nComfortable working in a corporate environment, someone who has business acumen and an innovative mind-set\nSome other highly valued skills includes:\nHigh level understanding of ETL processes\nBanking domain experience\nQuantitative mind set, with a desire to work in a data-intensive environment\nFamiliarity with Agile delivery methodologies and project management techniques\nYou may be assessed on key critical skills relevant for success in role, such as risk and controls, change and transformation, business acumen, strategic thinking and digital and technology, as well as job-specific technical skills.\nThis role is based out of Pune.\nPurpose of the role\nTo design and develop compelling visualizations that effectively communicate data insights to stakeholders across the bank, influencing decision-making and improving business outcomes.\nAccountabilities\nPerforming exploratory data analysis and data cleansing to prepare data for visualization.\nTranslation of complex data into clear, concise, and visually appealing charts, graphs, maps, and other data storytelling formats.\nUtilisation of best practices in data visualization principles and design aesthetics to ensure clarity, accuracy, and accessibility.\nDocumentation of visualization methodologies and findings in clear and concise reports.\nPresentation of data insights and visualizations to stakeholders at all levels, including executives, business users, and data analysts.\nAnalyst Expectations\nTo perform prescribed activities in a timely manner and to a high standard consistently driving continuous improvement.\nRequires in-depth technical knowledge and experience in their assigned area of expertise\nThorough understanding of the underlying principles and concepts within the area of expertise\nThey lead and supervise a team, guiding and supporting professional development, allocating work requirements and coordinating team resources.\nIf the position has leadership responsibilities, People Leaders are expected to demonstrate a clear set of leadership behaviours to create an environment for colleagues to thrive and deliver to a consistently excellent standard. The four LEAD behaviours are: L - Listen and be authentic, E - Energise and inspire, A - Align across the enterprise, D - Develop others.\nOR for an individual contributor, they develop technical expertise in work area, acting as an advisor where appropriate.\nWill have an impact on the work of related teams within the area.\nPartner with other functions and business areas.\nTakes responsibility for end results of a team s operational processing and activities.\nEscalate breaches of policies / procedure appropriately.\nTake responsibility for embedding new policies/ procedures adopted due to risk mitigation.\nAdvise and influence decision making within own area of expertise.\nTake ownership for managing risk and strengthening controls in relation to the work you own or contribute to. Deliver your work and areas of responsibility in line with relevant rules, regulation and codes of conduct.\nMaintain and continually build an understanding of how own sub-function integrates with function, alongside knowledge of the organisations products, services and processes within the function.\nDemonstrate understanding of how areas coordinate and contribute to the achievement of the objectives of the organisation sub-function.\nMake evaluative judgements based on the analysis of factual information, paying attention to detail.\nResolve problems by identifying and selecting solutions through the application of acquired technical experience and will be guided by precedents.\nGuide and persuade team members and communicate complex / sensitive information.\nAct as contact point for stakeholders outside of the immediate function, while building a network of contacts outside team and external to the organisation.",Industry Type: Financial Services,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data cleansing', 'Data analysis', 'Senior Analyst', 'Project management', 'Analytical', 'Agile', 'QlikView', 'Continuous improvement', 'Teradata', 'Operations']",2025-06-14 06:10:41
Business Analyst,Freight and Package Transportation,4 - 8 years,15-18 Lacs P.A.,['Kolkata'],"We are seeking an experienced and forward-thinking Senior Business Analyst with a deep understanding of courier logistics and intra-network operations. This role will drive automation  of our intra-branch payment system, support logistics network design, and build scalable  processes integrated with our IT backbone. \n\nKey Responsibilities  \nProcess Automation & Payment Systems  :-\nDesign, develop, and implement an automated intra-branch payment and settlement  system.  \nWork closely with finance, operations, and IT teams to streamline cash and digital  transaction workflows.  \nCreate audit-friendly digital trails and dashboards for real-time reconciliation.  \nLogistics Network Design & Optimization  :-\nCollaborate with leadership to design, redesign, and optimize branch-to-branch movement  and connectivity.  \nPropose cost-effective and time-efficient routing models for shipments across the national  network.  \nLeverage data to suggest new branch locations or closures based on traffic and profitability  analysis.  \nSystems Thinking & IT Collaboration  :-\nPartner with internal tech teams to ensure all operational processes are linked with IT  systems (ERP, CRMs, tracking platforms).  \nDraft user requirements, wireframes, and test cases for tech teams.  \nAdvocate for tech adoption across operations and help with change management.  \nOperational Excellence  :-\nConduct end-to-end mapping of booking, transit, and delivery operations and define  measurable KPIs.  \nIdentify process bottlenecks and recommend/implement solutions.  \nEstablish SOPs and training materials for new systems and processes.  \nData Analysis & Reporting  :-\nBuild and maintain dashboards for key operational and financial metrics.  \nUse insights to influence strategic decisions and operational changes. \nQualifications  \nBachelor's or Masters degree in Engineering, Logistics, Business, or related fields.  \n48 years of experience in business analysis, preferably in logistics/courier/e-commerce  operations.  \nStrong domain understanding of courier network dynamics, cash flow between branches,  and delivery routing.  \nProven experience in process automation, ERP/CRM system integration, and workflow  optimization.  \nAdvanced proficiency in Excel, SQL, Power BI/Tableau and familiarity with tech product  lifecycle.  \nExcellent communication and cross-functional collaboration skills.  \nPreferred Traits  \nEntrepreneurial mindset, thrives in ambiguity.  \nHands-on attitude, willing to visit hubs and branches for on-ground insights.  \nKeen interest in transforming legacy businesses with systems and data.  \nAbility to independently drive initiatives from concept to execution.\nWhy Join Us?  \nBe at the forefront of transforming a legacy logistics business.  \nWork directly with leadership to shape the future of courier logistics in India.  \nGet ownership of high-impact projects that touch operations across the country.",Industry Type: Courier / Logistics (Logistics Tech),Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Power BI/Tableau', 'Excel', 'SQL', 'Process Automation', 'ERP/CRM system']",2025-06-14 06:10:43
Software Engineer / Jetpac / Gurgaon,Circles,1 - 4 years,Not Disclosed,"['Kolkata', 'Mumbai', 'New Delhi', 'Hyderabad', 'Pune', 'Chennai', 'Bengaluru']","Founded in 2014, Circles is a global technology company reimagining the telco industry with its innovative SaaS platform, empowering telco operators worldwide to effortlessly launch innovative digital brands or refresh existing ones, accelerating their transformation into techcos.\nToday, Circles partners with leading telco operators across multiple countries and continents, including KDDI Corporation, Etisalat Group (e&), AT&T, and Telkomsel, creating blueprints for future telco and digital experiences enjoyed by millions of consumers globally.\nBesides its SaaS business, Circles operates three other distinct businesses:\nCircles.Life : A wholly-owned digital lifestyle telco brand based in Singapore, Circles.Life is powered by Circles SaaS platform and pioneering go-to-market strategies. It is the digital market leader in Singapore and has won numerous awards for marketing, customer service, and innovative product offerings beyond connectivity.\nCircles Aspire : A global provider of Communications Platform-as-a-Service (CPaaS) solutions. Its cloud-based Experience Cloud platform enables enterprises, service providers and developers to deliver and scale mobile, messaging, IoT, and connectivity services worldwide.\nJetpac : Specializing in travel tech solutions, Jetpac provides seamless eSIM roaming for over 200 destinations and innovative travel lifestyle products, redefining connectivity for digital travelers. Jetpac was awarded Travel eSIM of the Year.\nCircles is backed by renowned global investors, including Peak XV Partners (formerly Sequoia), Warburg Pincus, Founders Fund, and EDBI (the investment arm of the Singapore Economic Development Board), with a track record of backing industry challengers.\nJetpac is a revolutionary product launched Q4 2022 solving the ever dynamic problem of travel convenience. Starting from connectivity, it is now moving towards the holistic experience by including insurance coverage, concierge services as well as other AI-powered features.\nAs we re growing to become the biggest and most loved brand, we are looking for: Jetpac Software Engineer\nMission\nWork with the team to learn about Practical Software Development and deliver Solutions based upon Python, Node.js, GoLang and React.js.\nAs a key member of our team, your responsibilities include:\nWorking closely with engineers across the company to develop the best technical design and strategy\nOwning the delivery of various timelines, ensuring that key milestones are met and deliveries are of the highest quality\nAdapt to changing priorities and ambiguities which may arise from complex, cutting-edge and industry disrupting initiatives\nCollaborate with other non-technical stakeholders of the business which includes Product Managers, Designers, Marketing etc.\nWorking with an excellent team of talented engineers to solve the travel convenience problem\nWe will provide you..\nAn insight into inner working of a growing and continuously evolving organisation\nExcellent environment to learn and grow your development skills\nRegular feedback to improve both development and soft skills.\nRequirements:\nDegree / Masters in Computer Science, Software Engineering or equivalent\nStrong fundamentals on software development\nHaving actively worked on open source projects would be a plus\nExperience on at least any one of the following:\nGolang\nNode.js\nReact.js\nReact Native (Preferred)\nPython (optional)\nFamiliarity on RESTful web applications\nReact frontend development is a plus\nData engineering fundamentals\nPassionate about delivering quality work ahead of time\nExcellent interpersonal skills\nTeam player\nCircles is committed to a diverse and inclusive workplace. We are an equal opportunity employer and do not discriminate on the basis of race, national origin, gender, disability or age.\nTo all recruitment agencies: Circles will only acknowledge resumes shared by recruitment agencies if selected in our preferred supplier partnership program.\nPlease do not forward resumes to our jobs alias, Circles, employees or any other company location. Circles will not be held accountable for any fees related to unsolicited resumes not uploaded via our ATS.",Industry Type: Medical Devices & Equipment,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Interpersonal skills', 'telco', 'SAAS', 'Javascript', 'Customer service', 'Roaming', 'Open source', 'Recruitment', 'Python']",2025-06-14 06:10:45
Principal Engineer,Skyhigh Security,10 - 15 years,Not Disclosed,['Bengaluru'],"Our Engineering team is driving the future of cloud security developing one of the world s largest, most resilient cloud-native data platforms. At Skyhigh Security, we re enabling enterprises to protect their data with deep intelligence and dynamic enforcement across hybrid and multi-cloud environments. As we continue to grow, we re looking for a Principal Data Engineer to help us scale our platform, integrate advanced AI/ML workflows, and lead the evolution of our secure data infrastructure.\nResponsibilities:\nAs a Principal Data Engineer, you will be responsible for:\nLeading the design and implementation of high-scale, cloud-native data pipelines for real-time and batch workloads.\nCollaborating with product managers, architects, and backend teams to translate business needs into secure and scalable data solutions.\nIntegrating big data frameworks (like Spark, Kafka, Flink) with cloud-native services (AWS/GCP/Azure) to support security analytics use cases.\nDriving CI/CD best practices, infrastructure automation, and performance tuning across distributed environments.\nEvaluating and piloting the use of AI/LLM technologies in data pipelines (e.g., anomaly detection, metadata enrichment, automation).\nEvaluate and integrate LLM-based automation and AI-enhanced observability into engineering workflows.\nEnsure data security and privacy compliance.\nMentoring engineers, ensuring high engineering standards, and promoting technical excellence across teams.\nWhat We re Looking For (Minimum Qualifications)\n10+ years of experience in big data architecture and engineering, including deep proficiency with the AWS cloud platform.\nExpertise in distributed systems and frameworks such as Apache Spark, Scala, Kafka, Flink, and Elasticsearch, with experience building production-grade data pipelines.\nStrong programming skills in Java for building scalable data applications.\nHands-on experience with ETL tools and orchestration systems.\nSolid understanding of data modeling across both relational (PostgreSQL, MySQL) and NoSQL (HBase) databases and performance tuning.\nWhat Will Make You Stand Out (Preferred Qualifications )\nExperience integrating AI/ML or LLM frameworks (e.g., LangChain, LlamaIndex) into data workflows.\nExperience implementing CI/CD pipelines with Kubernetes, Docker, and Terraform.\nKnowledge of modern data warehousing (e.g., BigQuery, Snowflake) and data governance principles (GDPR, HIPAA).\nStrong ability to translate business goals into technical architecture and mentor teams through delivery.\nFamiliarity with visualization tools (Tableau, Power BI) to communicate data insights, even if not a primary responsibility.",Industry Type: Management Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Performance tuning', 'Automation', 'Backend', 'metadata', 'Data modeling', 'GCP', 'Postgresql', 'MySQL', 'Customer support', 'Analytics']",2025-06-14 06:10:48
Reservoir Engineering and Simulation Development Engineer,Chevron India,2 - 6 years,Not Disclosed,['Bengaluru'],"\nChevron ENGINE is looking for high-performing candidates to join our Reservoir Engineering and Simulation team with a focus on software development. The Reservoir Engineering and Simulation team focuses on field development/application as well as technology development. This role performs Reservoir Engineering and Simulation related software development to support Chevron operations. Will be involved in enhancing Chevron s Reservoir Simulation and Optimization workflows to provide differentiating capability and integration of a mix of proprietary and best-in-class vendor products. The senior simulation development engineer should have sufficient experience and expertise to coach and mentor others in the team.\n\nKey responsibilities:\nTeaming with US-based R&D groups focusing on developing and deploying reservoir simulation and optimization products and workflows\nTargeted user-experience and data management development, e.g., GUI, Power-BI, data engineering, machine learning engineering, and mature digital MVPs (Minimal Viable Products)\nApplication testing, pipeline, build and release in cloud platform\nApplication run, maintain and user support\nDevelopment of technical software and training documentation\nCoaching and mentoring\n\nRequired Qualifications:\nBSc/MSc degree in Petroleum Engineering or related areas\nAt least three years of experience in Petroleum Engineering or related industry\nDemonstrated fluency in engineering software development, including python, C#, C++, HPC (MPI, GPU, threading), cloud computing frameworks, Azure ecosystems developments (pipelines, dashboards, visualization, data management systems)\nUnderstanding of physical process associated with subsurface flow and the relationship to the governing model equations required for subsurface flow simulations\nUnderstanding of subsurface uncertainty analysis and optimization technologies and workflows, such as design of experiments and history matching\nAbility to integrate reservoir simulation & optimization workflows & engines with cloud platforms & services (DELFI, OSDU, RESQML)\nExperience of customer support and understanding of subsurface business needs and utilizing SAFe agile processes to implement highly prioritized features\nGood communication skills and work effectively in a team environment\nFamiliarity with reservoir simulation, uncertainty & optimization (U&O) workflows, Intersect, Petrel-RE, and Petroleum Experts IPM is preferred but not a must\nSkills of using ML/AI to accelerate performance, and accuracy of reservoir simulation & optimization is preferred but not a must\nDirect experience with flow-geomechanics coupling, surface-subsurface coupling, unconventional reservoirs, uncertainty analysis, optimization, computational geosciences, fracture modeling, discretization, gridding, improved/enhanced/chemical oil recovery is a plus but not a must",Industry Type: Oil & Gas,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Cloud computing', 'C++', 'Simulation', 'Data management', 'Machine learning', 'Cloud', 'Agile', 'Customer support', 'Petroleum', 'Python']",2025-06-14 06:10:50
AI / ML Engineer,a leading global professional services c...,12 - 22 years,Not Disclosed,"['Hyderabad', 'Chennai', 'Bengaluru']","Project Role : AI / ML Engineer\n\nProject Role Description : Develops applications and systems that utilize AI tools, Cloud AI services, with proper cloud or on-prem application pipeline with production ready quality. Be able to apply GenAI models as part of the solution. Could also include but not limited to deep learning, neural networks, chatbots, image processing.\n\nSummary:\nAs an AI/ML Engineer, you will develop applications and systems utilizing AI tools, Cloud AI services, and GenAI models. Your role involves creating cloud or on-prem application pipelines with production-ready quality, incorporating deep learning, neural networks, chatbots, and image processing.\n\nLocation: Pan India",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['nlp', 'Tensorflow', 'Object Detection', 'AI tools', 'Artificial Intelligence', 'Natural Language Processing', 'Neural Networks', 'Image Recognition', 'Cloud AI services', 'Machine Learning', 'Deep Learning', 'Pytorch', 'Nltk', 'Pattern Recognition', 'Image Processing', 'Computer Vision', 'Large Language Models', 'OCR']",2025-06-14 06:10:53
Murex FO Business Analyst,One of The MNC company,8 - 11 years,8-18 Lacs P.A.,"['Hyderabad', 'Pune', 'Mumbai (All Areas)']","Role & responsibilities\n\nManage and analyze business requirements into a solution design, managing user requirements workshops and formulation of an overall solution design, modeling transactions through the system to ensure that the business requirements are met.\n• Hands-on business analysis role in analyzing and proposing solutions for business issues, process changes, and functional requirements.\n• Work with different IT teams across infrastructure, and other divisions to deliver system solutions for the business.\n• Collaborate with stakeholders on their priorities, needs as well as system improvements.\n• Build a strong relationship and manage expectations with users and stakeholders.\n\n\n\n\nPreferred candidate profile\n\nMust have Overall 8+ years of IT Experience\n• Knowledge in Murex - Versions 3.1\n• FO Modules of Murex - E-Tradepad, Simulation, Viewers, Pre-Trade Workflow, Market Data, Dynamic Tables, P L Notepad, FDI Templates, Blotters, Risk Metrics (atleast 3 to 4 Modules)\n• Static data / Reference data - Instruments/Counterparties/ SSIs etc.\n• Strong analytical and problem-solving skills accompanied by excellent communication\nDomain knowledge :\nFunctional Knowledge should encompass any two or all of the following asset classes:\nCredit Derivatives, Interest Rate Derivatives, Equity Derivatives, Fixed Income, FX Cash, FX Derivatives",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['P L Notepad', 'E-Tradepad', 'Dynamic Tables', 'Viewers', 'Market Data', 'Simulation', 'Pre-Trade Workflow']",2025-06-14 06:10:55
Business Analyst,Comunus Technologies,8 - 10 years,1-6 Lacs P.A.,"['Navi Mumbai', 'Pune', 'Mumbai (All Areas)']","ComUnus is #hiring Business Analyst NBFC !!\nExperience: 8+ Years\nLocation: Mumbai (Vikhroli)\nMax NP : Immediate Joiners are preferred\n\nPreferred Candidate : Mumbai\nKindly Note : Immediate Joiners are preferred !!\n\nInterested Candidates Share there CV on : vidhi.bhatti@comunus.in | 8591723284\n\nJob Description:\n\nAs a Business Analyst, you will play a crucial role in analyzing business requirements, optimizing financial processes, and supporting technology-driven solutions within the NBFC sector. You will collaborate with stakeholders, product teams, and IT professionals to enhance operational efficiency and drive business growth.\n\nKey Responsibilities:\n1. Requirement Gathering & Analysis: Work closely with stakeholders to understand business needs and translate them into functional specifications.\n2. Process Optimization: Identify inefficiencies in NBFC operations and propose solutions to improve workflow and financial services.\n3. Regulatory Compliance: Ensure adherence to RBI guidelines and other financial regulations applicable to NBFCs.\n4. Data Analysis & Reporting: Utilize data analytics to generate insights, track KPIs, and support decision-making.\n5. Technology Implementation: Collaborate with IT teams to implement digital solutions, including loan management systems, risk assessment tools, and customer portals.\n6. Stakeholder Communication: Act as a bridge between business teams and technical teams, ensuring smooth execution of projects.\n7. Product Development Support: Assist in designing and launching new financial products tailored for NBFC customers.\n\nRequired Skills:\n1. Strong knowledge of NBFC operations, lending, and financial products.\n2. Expertise in business analysis methodologies and process reengineering.\n3. Experience with data analytics tools (Excel, SQL, Power BI, etc.).\n4. Familiarity with loan management systems and financial software.\n5. Understanding of RBI regulations and compliance requirements.\n6. Excellent communication and stakeholder management skills.\n\nExperience Required:\n1. Prior experience in NBFCs, Banks, or FinTech companies.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['business analyst', 'excel', 'Power Bi', 'Brd', 'NBFC', 'FRD', 'SQL']",2025-06-14 06:10:58
Privacy Analyst,Icims,3 - 8 years,Not Disclosed,['Hyderabad'],"Overview\nWith guidance from the Sr. Manager, Privacy Operations, the Privacy Analyst supports the iCIMS Legal team through preparation and review of privacy documents across the organization. This role works with the Legal team to enable execution of the privacy strategy, and with local and international business partners to support data protection and privacy compliance. The person who fills this role will seek ways to improve processes and help the department function more efficiently and effectively. They will also provide support in managing privacy impact assessments, coordinating responses to data subject requests, performing vendor due diligence, and responding to customer questions.",,,,"['project management', 'due diligence', 'law', 'administration', 'verbal communication', 'policies', 'mergers and acquisitions', 'technology', 'mergers', 'operations', 'continuous improvement', 'written communication', 'compliance', 'vendor', 'ms office suite', 'acquisition', 'finance', 'communication skills']",2025-06-14 06:11:00
FP&A O&G Decomissioning Analyst,BP INCORPORATE INTERNATIONAL.,5 - 10 years,Not Disclosed,['Pune'],"Grade H - Office/ CoreResponsible for delivering financial forecasting, analysis and insights and other financial performance and planning activities, using sound technical capabilities to ensure the business transactions and results of the reporting entity are recorded, reported and forecasted with integrity, in a timely manner and in conformance Group accounting policies and control standards.\nEntity:\nFinance\n\nFinance Group\n\nJob Description:\nAbout bp\nOur purpose is to bring together people, energy and markets to power and navigate a changing world. In a time of constant change and possibility we need talent to pursue opportunities, motivated by elite insight and expertise. We re always aspiring for more digital solutions, balanced outcomes and closer collaboration across our company and beyond, and you could be part of that too. Together we continue to grow as the world s leading energy company!\nThe role will be the part of The Finance Business & Technology (FBT) organization at bp is modernizing and digitizing finance activities. Within FBT, the Financial Planning and Analysis (FP&A) team plays a critical role in driving end-to-end process control and compliance. The FP&A team is instrumental in delivering best-in-class financial insights and analysis to support business decisions, while also chipping in to the standardization of our processes and driving operational perfection. By joining the FP&A team, you will be part of a group that brings additional value to bp through innovative financial strategies and solutions.\nBusiness Entity:\nThis role will sit within the Finance FP&A organization and will be accountable for delivering business planning and performance management and business collaboration support for specific Business/Functions.\nLet me tell you about the role\nA short, sharp and descriptive role summary - need to concisely tell the candidate what the role is, an outline of its objectives and how these objectives link in with bp s overall strategy\nThe FP&A O&G Decommissioning Analyst role involves delivering asset decommissioning accounting, planning, and performance management activity. This position requires partnering with onsite technical and finance teams across different geographies to gain a deep understanding of the business context. The key objective is to develop and deliver insightful financial products that enable effective business financial performance management and support decision-making.\nThe role demands good engagement skills and the ability to manage conflicting priorities while meeting collaborator expectations. It also involves contributing to the cost performance management agenda, supporting core FP&A processes, and maintaining a proactive business partnering relationship between finance and business teams.\nThe incumbent would be potentially required to work under alternative shift arrangements to ensure appropriate overlap hours.\nWhat you will deliver\nArticulate key outcomes and accountabilities that this person will deliver to be successful in this role. Ensure you describe the deliverables the results you want the job holder to deliver and not the process you expect them to apply to get to these outcomes\nAccounting/Control: Account for decommissioning provision; recognise liability or change of estimate. Manage annual decommissioning financial reporting (SOx) controls. Be the FP&A SME on decommissioning cost estimation for provisioning. Provide input to assessment of reversion risk and securitization. Identify impairment triggers associated with decommissioning. Support the internal governance process for decommissioning. Track internal performance targets and understand their impact on accounting provision\nDecommissioning compliance: includes supporting annual decommissioning security process and review of Section 29 notices (Regulator notifications of decom liability)\nPerformance Management : Deliver timely, accurate, and insightful financial and management information to empower strategic business decisions. Conduct in depth analysis of monthly and quarterly actuals, including trend analysis and variance explanations, to provide actionable insights. Identify and highlight areas where performance exceeds or falls short of key targets, driving continuous improvement.\nBusiness Planning : Contribute to the development of the long-term strategic plan for the region and support the quarterly planning process, including capital and cash cost forecasts, and key financial outcomes. Collaborate with team members to refine activity sets, input assumptions, and generate accurate forecasts.\nInsights : Use SAP-based systems to submit plan data and leverage PowerBI for detailed analysis and reporting. Build engaging presentation materials, provide insightful commentary, and address follow-up questions from leadership and central finance teams.\nContinuous Improvement : Promote the use of new and/or standardized systems and continuously improve supply related MI systems and processes to increase automation and move towards growing the self-service model.\nWhat you will need to be successful:\nMust have educational qualifications : Business/Finance or Engineering Discipline Degree level or equivalent\nPreferred education/certifications : Master s Degree or other qualification in a finance discipline e.g. MBA, CA, ICWA/Cost Accountants\nMinimum years of relevant experience : 5 years of relevant post degree experience in financial reporting, budgeting and forecasting\nPreferred experience : Experience within global, sophisticated and matrix organizations, preferably within an Oil & Gas business or experience in industries such as Retail, Asset-based business, Manufacturing, or Logistics.\nMust have experiences/skills :\nStrong accounting and reporting skills including asset and obligation measurement, provisioning and impairment\nStrong understanding of internal control over financial reporting including risks and controls\nPerformance management and tracking of spend\nStrong analytical and insight capabilities, with a focus on continuous improvement in performance management and MI.\nExcellent teammate with strong communication skills, capable of translating complex requirements into simple outcomes\nAbility to gain trust from finance and business senior collaborator\nDeliver operational improvements, share standard methodologies, and drive performance\nExperience in working with financial systems such as SAP, Microsoft products and visualization tools such as Power BI, Tableau\nYou will work with:\nYou will be working with a team of finance professionals as part of the Financial Planning and Analysis (FP&A) organization. The FP&A organization is an exciting new team being established by bp to create a centre of expertise in the areas of business planning, budgeting and forecasting, financial analysis and economic evaluation.\nThe role will regularly interact and be the main contact point for Business/Functions senior leadership.\nIn addition to the FP&A team, you will also be partnering with the local finance team and various technical and leadership teams in onsite locations.\nWhy join our team?\nAt bp, we provide the following environment & benefits to you:\nLife & health insurance, medical care package\nFlexible working schedule\nOpportunity to build up long term career path and develop your skills with wide range of learning options\nFamily friendly workplace e.g.: parental leave, bereavement and compassionate leave\nEmployees wellbeing programs e.g.: Employee Assistance Program, Company Recognition Program\nPossibility to join our social communities and networks\nIf this role attracts you, apply now!\nA company culture where we respect our diverse and unified teams, where we are proud of our achievements and where fun and the attitude of giving back to our environment are highly valued! Possibility to join our social communities and networks - Learning opportunities and other development opportunities to craft your career path. Life and health insurance, medical care package and many other benefits. We are an equal opportunity employer and value diversity at our company. We do not discriminate based on race, religion, colour, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.\nWe will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, perform crucial job functions, and receive other benefits and privileges of employment.\n\nTravel Requirement\nUp to 10% travel should be expected with this role\n\nRelocation Assistance:\nThis role is eligible for relocation within country\n\nRemote Type:\nThis position is not available for remote working\n\nSkills:\nAnalysis and modelling, Analysis and modelling, Analytics, Benchmarking, Business Performance, Business process improvement, Commercial Acumen, Commercial performance management, Communication, Cost Management, Data visualization and interpretation, Decision Making, Economic evaluation methodology, Economic modelling, Financial Analysis, Group Problem Solving, Integrated Planning, Investment appraisal, Long Term Planning, Management Reporting, Managing change, Measurement and metrics, Organizational knowledge, Performance and planning, Performance management {+ 4 more}",Industry Type: Oil & Gas,Department: Finance & Accounting,"Employment Type: Full Time, Permanent","['SAP', 'Financial reporting', 'Financial analysis', 'Financial planning', 'Business planning', 'Cost estimation', 'Forecasting', 'Operations', 'Analytics', 'Logistics']",2025-06-14 06:11:02
Business Analyst,Opash Software,2 - 4 years,4.8-8.4 Lacs P.A.,['Surat( Adajan )'],Responsibilities:\n* Collaborate with cross-functional teams on JIRA integration\n* Ensure compliance with industry standards and best practices\n* Analyze business needs through requirement gathering,Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Requirement Gathering', 'Use Cases', 'Brd', 'Srs', 'FSD', 'FRS', 'FRD', 'JIRA', 'Jira Integration']",2025-06-14 06:11:05
Machine Learning Engineer,Randomtrees,2 - 6 years,Not Disclosed,['Chennai'],"Job Summary: We are seeking a talented and driven Machine Learning Engineer with 2-5 years of experience to join our dynamic team in Chennai. The ideal candidate will have a strong foundation in machine learning principles and extensive hands-on experience in building, deploying, and managing ML models in production environments. A key focus of this role will be on MLOps practices and orchestration, ensuring our ML pipelines are robust, scalable, and automated.\nKey Responsibilities:\nML Model Deployment & Management: Design, develop, and implement end-to-end MLOps pipelines for deploying, monitoring, and managing machine learning models in production.",,,,"['Mlops', 'Machine Learning', 'Python', 'SQL']",2025-06-14 06:11:07
Analyst/Chemist,Bee Pharmo Labs,3 - 6 years,2.4-3.6 Lacs P.A.,['Thane'],HPLC Operator from pharmaceutical industry preferably from formulation background\n\n\nProvident fund\nAnnual bonus,Industry Type: Pharmaceutical & Life Sciences,Department: Research & Development,"Employment Type: Full Time, Permanent","['Hplc Analysis', 'QC', 'pharmaceutical', 'Quality Control', 'hplc']",2025-06-14 06:11:09
Finite Element (FE) Analyst- Pune,India International Technical Recruiters,6 - 11 years,7-12 Lacs P.A.,['Pune'],"Supporting Product Design & Development by performing advanced simulations & structural analyses on complex systems such as industrial cranes, military vehicles, and construction equipment. Send your CV to chinju@hireindians.com\nWhatsApp: 79821 34859",Industry Type: Engineering & Construction,"Department: Production, Manufacturing & Engineering","Employment Type: Full Time, Permanent","['MSC Nastran', 'En Codes', 'ASME', 'Construction Equipment', 'heavy machinery', 'Analysis', 'construction', 'Heavy Equipment', 'Apex', 'Cae Tools']",2025-06-14 06:11:11
Business Analyst,Harjai Computers,3 - 5 years,8-10 Lacs P.A.,['Mumbai (All Areas)'],Key Responsibilities:\n--Requirements Gathering and Analysis\n--Work with the Products team and enhance operational efficiency\n--Stakeholder Management and collaboration\n--Must have prior *LIFE Insurance domain*,Industry Type: Analytics / KPO / Research,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Life Insurance', 'Business Analysis', 'brd', 'frd']",2025-06-14 06:11:13
Business Analyst,reycruit,8 - 13 years,20-25 Lacs P.A.,['Hyderabad'],"looking for 8+ years of exp\nsupply chain, logistics,IT\nAgile & Scrum\nSDLC (Software Development Life Cycle)\nWaterfall Methodology\nChange Management\nBusiness Process Reengineering (BPR)\nLean & Six Sigma\nDesign Thinking",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Procurement Planning', 'Supply Chain Management', 'Logistics Management', 'Procurement Management']",2025-06-14 06:11:15
Machine Learning Engineer - Python/Tensorflow,Vayuz Technologies,4 - 5 years,Not Disclosed,['Hyderabad'],"Key Responsibilities :\n\n- Conduct feature engineering, data analysis, and data exploration to extract valuable insights.\n\n- Develop and optimize Machine Learning models to achieve high accuracy and performance.\n\n- Design and implement Deep Learning models, including Artificial Neural Networks (ANN), Convolutional Neural Networks (CNN), and Reinforcement Learning techniques.\n\n- Handle real-time imbalanced datasets and apply appropriate techniques to improve model fairness and robustness.\n\n- Deploy models in production environments and ensure continuous monitoring, improvement, and updates based on feedback.\n\n- Collaborate with cross-functional teams to align ML solutions with business goals.\n\n- Utilize fundamental statistical knowledge and mathematical principles to ensure the reliability of models.\n\n- Bring in the latest advancements in ML and AI to drive innovation.\n\nRequirements :\n\n- 4-5 years of hands-on experience in Machine Learning and Deep Learning.\n\n- Strong expertise in feature engineering, data exploration, and data preprocessing.\n\n- Experience with imbalanced datasets and techniques to improve model generalization.\n\n- Proficiency in Python, TensorFlow, Scikit-learn, and other ML frameworks.\n\n- Strong mathematical and statistical knowledge with problem-solving skills.\n\n- Ability to optimize models for high accuracy and performance in real-world scenarios.\n\nPreferred Qualifications :\n\n- Experience with Big Data technologies (Hadoop, Spark, etc.)\n\n- Familiarity with containerization and orchestration tools (Docker, Kubernetes).\n\n- Experience in automating ML pipelines with MLOps practices.\n\n- Experience in model deployment using cloud platforms (AWS, GCP, Azure) or MLOps tools.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Machine Learning', 'Tensorflow', 'MLOps', 'Big Data', 'Neural Networks', 'Scikit-Learn', 'Deep Learning', 'Python']",2025-06-14 06:11:17
Machine Learning Engineer - Python/Tensorflow,Vayuz Technologies,4 - 5 years,Not Disclosed,['Jaipur'],"Key Responsibilities :\n\n- Conduct feature engineering, data analysis, and data exploration to extract valuable insights.\n\n- Develop and optimize Machine Learning models to achieve high accuracy and performance.\n\n- Design and implement Deep Learning models, including Artificial Neural Networks (ANN), Convolutional Neural Networks (CNN), and Reinforcement Learning techniques.\n\n- Handle real-time imbalanced datasets and apply appropriate techniques to improve model fairness and robustness.\n\n- Deploy models in production environments and ensure continuous monitoring, improvement, and updates based on feedback.\n\n- Collaborate with cross-functional teams to align ML solutions with business goals.\n\n- Utilize fundamental statistical knowledge and mathematical principles to ensure the reliability of models.\n\n- Bring in the latest advancements in ML and AI to drive innovation.\n\nRequirements :\n\n- 4-5 years of hands-on experience in Machine Learning and Deep Learning.\n\n- Strong expertise in feature engineering, data exploration, and data preprocessing.\n\n- Experience with imbalanced datasets and techniques to improve model generalization.\n\n- Proficiency in Python, TensorFlow, Scikit-learn, and other ML frameworks.\n\n- Strong mathematical and statistical knowledge with problem-solving skills.\n\n- Ability to optimize models for high accuracy and performance in real-world scenarios.\n\nPreferred Qualifications :\n\n- Experience with Big Data technologies (Hadoop, Spark, etc.)\n\n- Familiarity with containerization and orchestration tools (Docker, Kubernetes).\n\n- Experience in automating ML pipelines with MLOps practices.\n\n- Experience in model deployment using cloud platforms (AWS, GCP, Azure) or MLOps tools.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Machine Learning', 'Tensorflow', 'MLOps', 'Big Data', 'Neural Networks', 'Scikit-Learn', 'Deep Learning', 'Python']",2025-06-14 06:11:20
Machine Learning Engineer - Python / TensorFlow,Vayuz Technologies,4 - 5 years,Not Disclosed,['Pune'],"Key Responsibilities :\n\n- Conduct feature engineering, data analysis, and data exploration to extract valuable insights.\n\n- Develop and optimize Machine Learning models to achieve high accuracy and performance.\n\n- Design and implement Deep Learning models, including Artificial Neural Networks (ANN), Convolutional Neural Networks (CNN), and Reinforcement Learning techniques.\n\n- Handle real-time imbalanced datasets and apply appropriate techniques to improve model fairness and robustness.\n\n- Deploy models in production environments and ensure continuous monitoring, improvement, and updates based on feedback.\n\n- Collaborate with cross-functional teams to align ML solutions with business goals.\n\n- Utilize fundamental statistical knowledge and mathematical principles to ensure the reliability of models.\n\n- Bring in the latest advancements in ML and AI to drive innovation.\n\nRequirements :\n\n- 4-5 years of hands-on experience in Machine Learning and Deep Learning.\n\n- Strong expertise in feature engineering, data exploration, and data preprocessing.\n\n- Experience with imbalanced datasets and techniques to improve model generalization.\n\n- Proficiency in Python, TensorFlow, Scikit-learn, and other ML frameworks.\n\n- Strong mathematical and statistical knowledge with problem-solving skills.\n\n- Ability to optimize models for high accuracy and performance in real-world scenarios.\n\nPreferred Qualifications :\n\n- Experience with Big Data technologies (Hadoop, Spark, etc.)\n\n- Familiarity with containerization and orchestration tools (Docker, Kubernetes).\n\n- Experience in automating ML pipelines with MLOps practices.\n\n- Experience in model deployment using cloud platforms (AWS, GCP, Azure) or MLOps tools.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Machine Learning', 'MLOps', 'Hadoop', 'Big Data', 'Neural Networks', 'Spark', 'Scikit-Learn', 'Deep Learning', 'Python', 'TensorFlow']",2025-06-14 06:11:22
Machine Learning Engineer - Python/Tensorflow,Vayuz Technologies,4 - 5 years,Not Disclosed,['Kolkata'],"Key Responsibilities :\n\n- Conduct feature engineering, data analysis, and data exploration to extract valuable insights.\n\n- Develop and optimize Machine Learning models to achieve high accuracy and performance.\n\n- Design and implement Deep Learning models, including Artificial Neural Networks (ANN), Convolutional Neural Networks (CNN), and Reinforcement Learning techniques.\n\n- Handle real-time imbalanced datasets and apply appropriate techniques to improve model fairness and robustness.\n\n- Deploy models in production environments and ensure continuous monitoring, improvement, and updates based on feedback.\n\n- Collaborate with cross-functional teams to align ML solutions with business goals.\n\n- Utilize fundamental statistical knowledge and mathematical principles to ensure the reliability of models.\n\n- Bring in the latest advancements in ML and AI to drive innovation.\n\nRequirements :\n\n- 4-5 years of hands-on experience in Machine Learning and Deep Learning.\n\n- Strong expertise in feature engineering, data exploration, and data preprocessing.\n\n- Experience with imbalanced datasets and techniques to improve model generalization.\n\n- Proficiency in Python, TensorFlow, Scikit-learn, and other ML frameworks.\n\n- Strong mathematical and statistical knowledge with problem-solving skills.\n\n- Ability to optimize models for high accuracy and performance in real-world scenarios.\n\nPreferred Qualifications :\n\n- Experience with Big Data technologies (Hadoop, Spark, etc.)\n\n- Familiarity with containerization and orchestration tools (Docker, Kubernetes).\n\n- Experience in automating ML pipelines with MLOps practices.\n\n- Experience in model deployment using cloud platforms (AWS, GCP, Azure) or MLOps tools.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Machine Learning Engineering', 'Tensorflow', 'MLOps', 'Big Data', 'Neural Networks', 'Machine Learning', 'Scikit-Learn', 'Deep Learning', 'Python']",2025-06-14 06:11:25
Machine Learning Engineer - Python/Tensorflow,Vayuz Technologies,4 - 5 years,Not Disclosed,['Bhopal'],"Key Responsibilities :\n\n- Conduct feature engineering, data analysis, and data exploration to extract valuable insights.\n\n- Develop and optimize Machine Learning models to achieve high accuracy and performance.\n\n- Design and implement Deep Learning models, including Artificial Neural Networks (ANN), Convolutional Neural Networks (CNN), and Reinforcement Learning techniques.\n\n- Handle real-time imbalanced datasets and apply appropriate techniques to improve model fairness and robustness.\n\n- Deploy models in production environments and ensure continuous monitoring, improvement, and updates based on feedback.\n\n- Collaborate with cross-functional teams to align ML solutions with business goals.\n\n- Utilize fundamental statistical knowledge and mathematical principles to ensure the reliability of models.\n\n- Bring in the latest advancements in ML and AI to drive innovation.\n\nRequirements :\n\n- 4-5 years of hands-on experience in Machine Learning and Deep Learning.\n\n- Strong expertise in feature engineering, data exploration, and data preprocessing.\n\n- Experience with imbalanced datasets and techniques to improve model generalization.\n\n- Proficiency in Python, TensorFlow, Scikit-learn, and other ML frameworks.\n\n- Strong mathematical and statistical knowledge with problem-solving skills.\n\n- Ability to optimize models for high accuracy and performance in real-world scenarios.\n\nPreferred Qualifications :\n\n- Experience with Big Data technologies (Hadoop, Spark, etc.)\n\n- Familiarity with containerization and orchestration tools (Docker, Kubernetes).\n\n- Experience in automating ML pipelines with MLOps practices.\n\n- Experience in model deployment using cloud platforms (AWS, GCP, Azure) or MLOps tools.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Machine Learning', 'Tensorflow', 'MLOps', 'Big Data', 'Neural Networks', 'Scikit-Learn', 'Deep Learning', 'Python']",2025-06-14 06:11:27
Machine Learning Engineer - Python/Tensorflow,Vayuz Technologies,4 - 5 years,Not Disclosed,['Nagpur'],"Key Responsibilities :\n\n- Conduct feature engineering, data analysis, and data exploration to extract valuable insights.\n\n- Develop and optimize Machine Learning models to achieve high accuracy and performance.\n\n- Design and implement Deep Learning models, including Artificial Neural Networks (ANN), Convolutional Neural Networks (CNN), and Reinforcement Learning techniques.\n\n- Handle real-time imbalanced datasets and apply appropriate techniques to improve model fairness and robustness.\n\n- Deploy models in production environments and ensure continuous monitoring, improvement, and updates based on feedback.\n\n- Collaborate with cross-functional teams to align ML solutions with business goals.\n\n- Utilize fundamental statistical knowledge and mathematical principles to ensure the reliability of models.\n\n- Bring in the latest advancements in ML and AI to drive innovation.\n\nRequirements :\n\n- 4-5 years of hands-on experience in Machine Learning and Deep Learning.\n\n- Strong expertise in feature engineering, data exploration, and data preprocessing.\n\n- Experience with imbalanced datasets and techniques to improve model generalization.\n\n- Proficiency in Python, TensorFlow, Scikit-learn, and other ML frameworks.\n\n- Strong mathematical and statistical knowledge with problem-solving skills.\n\n- Ability to optimize models for high accuracy and performance in real-world scenarios.\n\nPreferred Qualifications :\n\n- Experience with Big Data technologies (Hadoop, Spark, etc.)\n\n- Familiarity with containerization and orchestration tools (Docker, Kubernetes).\n\n- Experience in automating ML pipelines with MLOps practices.\n\n- Experience in model deployment using cloud platforms (AWS, GCP, Azure) or MLOps tools.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Machine Learning', 'MLOps', 'Hadoop', 'Big Data', 'Neural Networks', 'Spark', 'Scikit-Learn', 'Deep Learning', 'Python', 'TensorFlow']",2025-06-14 06:11:29
Machine Learning Engineer - Python/Tensorflow,Vayuz Technologies,4 - 5 years,Not Disclosed,['Mumbai'],"Key Responsibilities :\n\n- Conduct feature engineering, data analysis, and data exploration to extract valuable insights.\n\n- Develop and optimize Machine Learning models to achieve high accuracy and performance.\n\n- Design and implement Deep Learning models, including Artificial Neural Networks (ANN), Convolutional Neural Networks (CNN), and Reinforcement Learning techniques.\n\n- Handle real-time imbalanced datasets and apply appropriate techniques to improve model fairness and robustness.\n\n- Deploy models in production environments and ensure continuous monitoring, improvement, and updates based on feedback.\n\n- Collaborate with cross-functional teams to align ML solutions with business goals.\n\n- Utilize fundamental statistical knowledge and mathematical principles to ensure the reliability of models.\n\n- Bring in the latest advancements in ML and AI to drive innovation.\n\nRequirements :\n\n- 4-5 years of hands-on experience in Machine Learning and Deep Learning.\n\n- Strong expertise in feature engineering, data exploration, and data preprocessing.\n\n- Experience with imbalanced datasets and techniques to improve model generalization.\n\n- Proficiency in Python, TensorFlow, Scikit-learn, and other ML frameworks.\n\n- Strong mathematical and statistical knowledge with problem-solving skills.\n\n- Ability to optimize models for high accuracy and performance in real-world scenarios.\n\nPreferred Qualifications :\n\n- Experience with Big Data technologies (Hadoop, Spark, etc.)\n\n- Familiarity with containerization and orchestration tools (Docker, Kubernetes).\n\n- Experience in automating ML pipelines with MLOps practices.\n\n- Experience in model deployment using cloud platforms (AWS, GCP, Azure) or MLOps tools.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Machine Learning', 'MLOps', 'Hadoop', 'Big Data', 'Neural Networks', 'Spark', 'Scikit-Learn', 'Deep Learning', 'Python', 'TensorFlow']",2025-06-14 06:11:31
Machine Learning Engineer - Python/Tensorflow,Vayuz Technologies,4 - 5 years,Not Disclosed,['Ludhiana'],"Key Responsibilities :\n\n- Conduct feature engineering, data analysis, and data exploration to extract valuable insights.\n\n- Develop and optimize Machine Learning models to achieve high accuracy and performance.\n\n- Design and implement Deep Learning models, including Artificial Neural Networks (ANN), Convolutional Neural Networks (CNN), and Reinforcement Learning techniques.\n\n- Handle real-time imbalanced datasets and apply appropriate techniques to improve model fairness and robustness.\n\n- Deploy models in production environments and ensure continuous monitoring, improvement, and updates based on feedback.\n\n- Collaborate with cross-functional teams to align ML solutions with business goals.\n\n- Utilize fundamental statistical knowledge and mathematical principles to ensure the reliability of models.\n\n- Bring in the latest advancements in ML and AI to drive innovation.\n\nRequirements :\n\n- 4-5 years of hands-on experience in Machine Learning and Deep Learning.\n\n- Strong expertise in feature engineering, data exploration, and data preprocessing.\n\n- Experience with imbalanced datasets and techniques to improve model generalization.\n\n- Proficiency in Python, TensorFlow, Scikit-learn, and other ML frameworks.\n\n- Strong mathematical and statistical knowledge with problem-solving skills.\n\n- Ability to optimize models for high accuracy and performance in real-world scenarios.\n\nPreferred Qualifications :\n\n- Experience with Big Data technologies (Hadoop, Spark, etc.)\n\n- Familiarity with containerization and orchestration tools (Docker, Kubernetes).\n\n- Experience in automating ML pipelines with MLOps practices.\n\n- Experience in model deployment using cloud platforms (AWS, GCP, Azure) or MLOps tools.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Machine Learning Engineering', 'Tensorflow', 'MLOps', 'Big Data', 'Neural Networks', 'Machine Learning', 'Scikit-Learn', 'Deep Learning', 'Python']",2025-06-14 06:11:34
Machine Learning Engineer - Python/Tensorflow,Vayuz Technologies,4 - 5 years,Not Disclosed,['Kanpur'],"Key Responsibilities :\n\n- Conduct feature engineering, data analysis, and data exploration to extract valuable insights.\n\n- Develop and optimize Machine Learning models to achieve high accuracy and performance.\n\n- Design and implement Deep Learning models, including Artificial Neural Networks (ANN), Convolutional Neural Networks (CNN), and Reinforcement Learning techniques.\n\n- Handle real-time imbalanced datasets and apply appropriate techniques to improve model fairness and robustness.\n\n- Deploy models in production environments and ensure continuous monitoring, improvement, and updates based on feedback.\n\n- Collaborate with cross-functional teams to align ML solutions with business goals.\n\n- Utilize fundamental statistical knowledge and mathematical principles to ensure the reliability of models.\n\n- Bring in the latest advancements in ML and AI to drive innovation.\n\nRequirements :\n\n- 4-5 years of hands-on experience in Machine Learning and Deep Learning.\n\n- Strong expertise in feature engineering, data exploration, and data preprocessing.\n\n- Experience with imbalanced datasets and techniques to improve model generalization.\n\n- Proficiency in Python, TensorFlow, Scikit-learn, and other ML frameworks.\n\n- Strong mathematical and statistical knowledge with problem-solving skills.\n\n- Ability to optimize models for high accuracy and performance in real-world scenarios.\n\nPreferred Qualifications :\n\n- Experience with Big Data technologies (Hadoop, Spark, etc.)\n\n- Familiarity with containerization and orchestration tools (Docker, Kubernetes).\n\n- Experience in automating ML pipelines with MLOps practices.\n\n- Experience in model deployment using cloud platforms (AWS, GCP, Azure) or MLOps tools.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Machine Learning Engineering', 'Tensorflow', 'MLOps', 'Big Data', 'Neural Networks', 'Machine Learning', 'Scikit-Learn', 'Deep Learning', 'Python']",2025-06-14 06:11:37
Machine Learning Engineer - Python/Tensorflow,Vayuz Technologies,4 - 5 years,Not Disclosed,['Agra'],"Key Responsibilities :\n\n- Conduct feature engineering, data analysis, and data exploration to extract valuable insights.\n\n- Develop and optimize Machine Learning models to achieve high accuracy and performance.\n\n- Design and implement Deep Learning models, including Artificial Neural Networks (ANN), Convolutional Neural Networks (CNN), and Reinforcement Learning techniques.\n\n- Handle real-time imbalanced datasets and apply appropriate techniques to improve model fairness and robustness.\n\n- Deploy models in production environments and ensure continuous monitoring, improvement, and updates based on feedback.\n\n- Collaborate with cross-functional teams to align ML solutions with business goals.\n\n- Utilize fundamental statistical knowledge and mathematical principles to ensure the reliability of models.\n\n- Bring in the latest advancements in ML and AI to drive innovation.\n\nRequirements :\n\n- 4-5 years of hands-on experience in Machine Learning and Deep Learning.\n\n- Strong expertise in feature engineering, data exploration, and data preprocessing.\n\n- Experience with imbalanced datasets and techniques to improve model generalization.\n\n- Proficiency in Python, TensorFlow, Scikit-learn, and other ML frameworks.\n\n- Strong mathematical and statistical knowledge with problem-solving skills.\n\n- Ability to optimize models for high accuracy and performance in real-world scenarios.\n\nPreferred Qualifications :\n\n- Experience with Big Data technologies (Hadoop, Spark, etc.)\n\n- Familiarity with containerization and orchestration tools (Docker, Kubernetes).\n\n- Experience in automating ML pipelines with MLOps practices.\n\n- Experience in model deployment using cloud platforms (AWS, GCP, Azure) or MLOps tools.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Machine Learning', 'MLOps', 'Hadoop', 'Big Data', 'Neural Networks', 'Spark', 'Scikit-Learn', 'Deep Learning', 'Python', 'TensorFlow']",2025-06-14 06:11:39
MLOps/LLMOps Manager,Crisil,8 - 13 years,Not Disclosed,"['Mumbai', 'Pune']","Key Responsibilities:\nDesign, build, and maintain CI/CD pipelines for ML model training, validation, and deployment\nAutomate and optimize ML workflows, including data ingestion, feature engineering, model training, and monitoring\nDeploy, monitor, and manage LLMs and other ML models in production (on-premises and/or cloud)\nImplement model versioning, reproducibility, and governance best practices\nCollaborate with data scientists, ML engineers, and software engineers to streamline end-to-end ML lifecycle\nEnsure security, compliance, and scalability of ML/LLM infrastructure\nTroubleshoot and resolve issues related to ML model deployment and serving\nEvaluate and integrate new MLOps/LLMOps tools and technologies\nMentor junior engineers and contribute to best practices documentation\nRequired Skills & Qualifications:\n8+ years of experience in DevOps, with at least 3 years in MLOps/LLMOps\nStrong experience with cloud platforms (AWS, Azure, GCP) and container orchestration (Kubernetes, Docker)\nProficient in CI/CD tools (Jenkins, GitHub Actions, GitLab CI, etc.)\nHands-on experience deploying and managing different types of AI models (e.g., OpenAI, HuggingFace, custom models) to be used for developing solutions.\nExperience with model serving tools such as TGI, vLLM, BentoML, etc.\nSolid scripting and programming skills (Python, Bash, etc.)\nFamiliarity with monitoring/logging tools (Prometheus, Grafana, ELK stack)\nStrong understanding of security and compliance in ML environments\nPreferred Skills:\nKnowledge of model explainability, drift detection, and model monitoring\nFamiliarity with data engineering tools (Spark, Kafka, etc.\nKnowledge of data privacy, security, and compliance in AI systems.\nStrong communication skills to effectively collaborate with various stakeholders\nCritical thinking and problem-solving skills are essential\nProven ability to lead and manage projects with cross-functional teams",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Training', 'security compliance', 'Compliance', 'spark', 'Cloud', 'Programming', 'data privacy', 'Management', 'Monitoring', 'Python']",2025-06-14 06:11:41
Sr. Project Manager,Useready,15 - 20 years,Not Disclosed,['Mohali'],"Job Summary:\n\n\nWe are seeking an experienced and detail-oriented Technical Project Manager, with strong interpersonal skills to lead and manage Data, Business Intelligence (BI), and Analytics initiatives across single and multiple client engagements. The ideal candidate will have a solid background in data project delivery, knowledge of modern cloud platforms, and familiarity with tools like Snowflake, Tableau, and Power BI. Understanding of AI and machine learning projects is a strong plus.\n\n\nThis role requires strong communication and leadership skills, with the ability to translate complex technical requirements into actionable plans and ensure successful, timely, and high-quality delivery with attention to details.\n\n\n\n\n\nKey Responsibilities:\n\n\nProject Program Delivery\n\n\n\nManage end-to-end, the full lifecycle of data engineering and analytics, projects including data platform migrations, dashboard/report development, and advanced analytics initiatives.\n\nDefine project scope, timelines, milestones, resource needs, and deliverables in alignment with stakeholder objectives.\n\nManage budgets, resource allocation, and risk mitigation strategies to ensure successful program delivery.\n\nUse Agile, Scrum, or hybrid methodologies to ensure iterative delivery and continuous improvement.\n\nMonitor performance, track KPIs, and adjust plans to maintain scope, schedule, and quality.\n\nExcellence in execution and ensure client satisfaction\n\n\n\nClient Stakeholder Engagement\n\n\n\nServe as the primary point of contact for clients and internal teams across all data initiatives.\n\nTranslate business needs into actionable technical requirements and facilitate alignment across teams.\n\nConduct regular status meetings, monthly and quarterly reviews, executive updates, and retrospectives.\n\n\n\nManage Large teams\n\n\n\nAbility to manage up to 50+ resources working on different projects for different clients.\n\nWork with practice and talent acquisition teams for resourcing needs\n\n\n\nManage P L\n\n\n\nManage allocation, gross margin, utilization etc effectively\n\n\n\nTeam Coordination\n\n\n\nLead and coordinate cross-functional teams including data engineers, BI developers, analysts, and QA testers.\n\nEnsure appropriate allocation of resources across concurrent projects and clients.\n\nFoster collaboration, accountability, and a results-oriented team culture.\n\n\n\n\n\nData, AI and BI Technology Oversight\n\n\n\nManage project delivery using modern cloud data platforms\n\nOversee BI development using Tableau and/or Power BI, ensuring dashboards meet user needs and follow visualization best practices. Conduct UATs\n\nManage initiatives involving ETL/ELT processes, data modeling, and real-time analytics pipelines.\n\nEnsure compatibility with data governance, security, and privacy requirements.\n\nManage AL ML projects\n\n\n\nData Cloud Understanding\n\n\n\nOversee delivery of solutions involving cloud data platforms (e.g., Azure, AWS, GCP), data lakes, and modern data stacks.\n\nSupport planning for data migrations, ETL processes, data modeling, and analytics pipelines.\n\nBe conversant in tools such as Power BI, Tableau, Snowflake, Databricks, Azure Synapse, or BigQuery.\n\n\n\nRisk, Quality Governance\n\n\n\nIdentify and mitigate risks related to data quality, project timelines, and resource availability.\n\nEnsure adherence to governance, compliance, and data privacy standards (e.g., GDPR, HIPAA).\n\nMaintain thorough project documentation including charters, RACI matrices, RAID logs, and retrospectives.\n\n\n\nQualifications:\n\n\n\n\n\nBachelor s degree in Computer Science, Information Systems, Business, or a related field.\n\n\n\n\n\n\nCertifications (Preferred):\n\n\n\nPMP, PRINCE2, or Certified ScrumMaster (CSM)\n\nCloud certifications (e.g., AWS Cloud Practitioner, Azure Fundamentals, Google Cloud Certified)\n\nBI/analytics certifications (e.g., Tableau Desktop Specialist, Power BI Data Analyst Associate, DA-100)\n\n\n\n\n\n\nMust Have Skills:\n\n\n\nStrong communication skills\n\nStrong interpersonal\n\nAbility to work collaboratively\n\nExcellent Organizing skills\n\nStakeholder Management\n\nCustomer Management\n\nPeople Management\n\nContract Management\n\nRisk Compliance Management\n\nC-suite reporting\n\nTeam Management\n\nResourcing\n\nExperience using tools like JIRA, MS Plan etc.\n\n\n\nDesirable Skills:\n\n\n\n15 years of IT experience with 8+ years of proven project management experience, in delivering data, AI Ml, BI / analytics-focused environments.\n\nExperience delivering projects with cloud platforms (e.g., Azure, AWS, GCP) and data platforms like Snowflake.\n\nProficiency in managing BI projects preferably Tableau and/or Power BI.\n\nKnowledge or hands on experience on legacy tools is a plus.\n\nSolid understanding of the data lifecycle including ingestion, transformation, visualization, and reporting.\n\nComfortable using PM tools like Jira, Azure DevOps, Monday.com, or Smartsheet.\n\n\n\n\nExperience managing projects involving data governance, metadata management, or master data management (MDM).",Industry Type: IT Services & Consulting,Department: Project & Program Management,"Employment Type: Full Time, Permanent","['PMP', 'Team management', 'Contract management', 'Data modeling', 'Talent acquisition', 'Project management', 'Data quality', 'Business intelligence', 'Continuous improvement', 'Analytics']",2025-06-14 06:11:44
AI Solution architect,Crisil,8 - 13 years,Not Disclosed,"['Mumbai', 'Pune']","Key Responsibilities:\nLead the design and implementation of AI/ML solutions across various business domains.\nArchitect end-to-end GenAI solutions, ensuring scalability, reliability, and security.\nCollaborate with data scientists, engineers, and product managers to translate business requirements into technical solutions.\nEvaluate and select appropriate AI frameworks, tools, and build core AI framework.\nEnsure best practices in software engineering, code quality, and system security.\nOversee integration of AI models into existing software products and cloud environments.\nMentor and guide development teams on AI/ML best practices and software architecture.\nStay updated with the latest advancements in AI, ML, and software engineering.\nRequired Qualifications:\n8+ years of experience in software development, with at least 3 years in AI/ML solution architecture.\nStrong programming skills in Python, Java, or similar languages.\nHands-on experience with AI/ML frameworks (TensorFlow, PyTorch, Scikit-learn, etc.).\nSolid understanding of software architecture patterns, APIs, and microservices.\nProven track record of delivering complex AI projects from concept to production.\nPreferred Skills:\nExperience with MLOps, CI/CD for ML, and containerization (Docker, Kubernetes).\nFamiliarity with data engineering tools (Spark, Kafka, Airflow).\nKnowledge of data privacy, security, and compliance in AI systems.\nStrong communication skills to effectively collaborate with various stakeholders\nCritical thinking and problem-solving skills are essential\nProven ability to lead and manage projects with cross-functional teams",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Solution architecture', 'software architecture', 'Architect', 'Compliance', 'spark', 'Programming', 'data privacy', 'Management', 'Solution Architect', 'Python']",2025-06-14 06:11:46
Senior Program Officer (Education),Wadhwani Ai,5 - 10 years,Not Disclosed,['Mumbai'],"SUMMARY\nAt Wadhwani AI, we use the power of modern artificial intelligence to help solve some of the most challenging problems in the world. We are looking for a candidate to join our team in a full-time role, who can represent Wadhwani AI in various states as our resident expert and program manager in the field of Education.\nABOUT US - https://www.wadhwaniai.org/\nWadhwani AI is a nonprofit institute building and deploying applied AI solutions to solve critical issues in public health, agriculture, education, and urban development in underserved communities in the global south. We collaborate with governments, social sector organizations, academic and research institutions, and domain experts to identify real-world problems, and develop practical AI solutions to tackle these issues with the aim of making a substantial positive impact.\nWe have over 30+ AI projects supported by leading philanthropies such as Bill & Melinda Gates Foundation, USAID and Google.org. With a team of over 200 professionals, our expertise encompasses AI/ML research and innovation, software engineering, domain knowledge, design and user research.\nIn the Press:\nOur Founder Donors are among the Top 100 AI Influencers\nG20 India s Presidency: AI Healthcare, Agriculture, & Education Solutions Showcased Globally.\nUnlocking the potentials of AI in Public Health\nWadhwani AI Takes an Impact-First Approach to Applying Artificial Intelligence - data.org\nWinner of the H&M Foundation Global Change Award 2022\nIndian Winners of the 2019 Google AI Impact Challenge, and the first in the Asia Pacific to host Google Fellows\nCultures page of Wadhwani AI - https: / / www.wadhwaniai.org / culture /\nROLES AND RESPONSIBILITIES:\nBring in the advanced level of domain expertise, engagement and field experience for technicalities, pedagogy, governance for early and primary education field experience etc.\nParticipate in and contribute to project planning, coordination, implementation, monitoring and reporting.\nImplement project activities to ensure the delivery of objectives in a timely and quality fashion.\nLiaise with key stakeholders including project staff, external partners, Directorate of Primary and Secondary Education, State Council for Education Research and Training and other relevant government education functionaries at field, district and state levels.\nServe as the liaison between the various departments of Directorate of Primary and Secondary Education, State Council for Education Research and Training, other key departments, staff and partners in selected geographies.\nAssist core product, deployment and other X-functional teams in identifying critical issues, mitigation strategies, and potential impact across various deployments.\nWork with staff from selected geographies to document activities, success stories, and other materials to share achievements.\nLead the on-field implementation. AI-based research and solution deployment in the geographies of interest through field experiments, pilots and scale-ups.\nCoordinate with and provide support to the internal team of AI researchers, data scientists, engineers etc for acquiring data as needed, liaising with government stakeholders, and conducting field research and field experimentations.\nConduct supervision and monitoring of project activities to ensure cadence and delivery as per project work plans, timelines, resource allocations, and deliverable schedules through regular contact and coordination with relevant personnel.\nPerform any other duties as assigned by the supervisor from time to time.\nIDEAL BACKGROUND:\nAn expert-level understanding of ground realities in the ECCE and primary education ecosystem, its priorities, and relevant national and state-specific policies, enabling awareness of the current status and implementation challenges, along with the competence to manage field-level challenges in the deployment of AI solutions.\nThorough understanding of the technicalities of frameworks like the NEP, CCE, standardized assessments like NAS (National Achievement Survey) and CBA (Competency Based Assessments)\n5+ years of experience working with state education departments in supporting implementing various programs and interventions (either via non-profit organisations, multilateral organisations or non-profit organisations) is a must.\nDemonstrated ability for engagement and advocacy with a variety of stakeholders and managing large-scale operations effectively.\nExperience as a teacher/educator will be a plus.\nA graduate or post-graduate degree specializing in Education will be a plus.\nA team player and able to manage teams effectively.\nDemonstrated ability to communicate (orally and in writing) complex issues in a concise, compelling, and easily understandable manner (English and Hindi).\nSkilled in successfully working with geographically dispersed teams and working in a multicultural environment.\nProficient computer skills in Microsoft Office Suite applications, including Word, Excel, PowerPoint, and Outlook.\nA quick and comprehensive learner. An analytical mindset is a big plus.\nStrong interpersonal and organizational skills, with the ability to prioritize deadlines and work in fast-moving environments and teams, while maintaining accuracy.\nAbility to travel frequently.\nPossess native-language proficiency in Hindi and English (Read, Write, Listen, Speak)\nSPECIAL NOTE\nCandidates who have played the role of a consultant or an engagement lead with state education departments/directorates and with prior experience as a teacher either in the private or public education system, or through fellowships (Teach for India, Gandhi Fellowship, Azim Premji Foundation fellowship etc.), are strongly encouraged to apply.",Industry Type: Education / Training,Department: Teaching & Training,"Employment Type: Full Time, Permanent","['Teacher Educator', 'Analytical', 'Artificial Intelligence', 'Engagement Lead', 'Healthcare', 'Project planning', 'Research', 'MS Office', 'Monitoring', 'Public health']",2025-06-14 06:11:49
Excellent opportunity For Analyst Reporting and Analytics Role,Zenith Algorithms,4 - 9 years,4.75-9.75 Lacs P.A.,['Pune( Magarpatta )'],"Role & responsibilities\n\nAnalyst, Reporting & Analytics role:\nCollaborate with business stakeholders to gather reporting and analytics requirements and translate them into actionable insights.\nDesign, develop, and maintain dashboards and reports using BI tools to support business decision-making.\nPerform deep-dive data analysis to uncover trends, patterns, and opportunities that drive strategic initiatives.\nProvide ad-hoc reporting and data extracts to business teams, ensuring data accuracy and timeliness.\nWork closely with data engineering teams to ensure data availability, quality, and alignment with reporting needs.\nSupport testing and validation of data/reporting logic to maintain consistency and integrity across reports.""\n\nPreferred candidate profile",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Business Intelligence Reporting', 'Management Reporting', 'Business Reporting', 'Business Stakeholder', 'Bi Tools', 'Dashboarding', 'Reporting And Analytics', 'Reporting Tools', 'Analytics Reporting']",2025-06-14 06:11:52
Scrum Lead,Metlife,8 - 12 years,Not Disclosed,['Hyderabad'],"Position Summary\nMetLife established a Global capability center (MGCC) in India to scale and mature Data & Analytics, technology capabilities in a cost-effective manner and make MetLife future ready. The center is integral to Global Technology and Operations with a with a focus to protect & build MetLife IP, promote reusability and drive experimentation and innovation. The Data & Analytics team in India mirrors the Global D&A team with an objective to drive business value through trusted data, scaled capabilities, and actionable insights. The operating models consists of business aligned data officers- US, Japan and LatAm & Corporate functions enabled by enterprise COEs- data engineering, data governance and data science\nRole Value Proposition:\nDriven by passion and a zeal to succeed, we are looking for accomplished Program Manager to structure, plan and handle multiple projects with minimum supervision and will be responsible for successful completion of projects supporting MGCC and US D&A leadership with various strategic initiatives in development and successful implementation of governance and process excellence practices.\nThis position would be responsible for complete adherence of the projects and its objectives and support all aspects of project management. This role will support development of best practices, processes and framework to achieve standardization and streamlining across various initiatives.\nJob Responsibilities\nServe as analytics program manager on data, analytics projects and POCs working with data engineers, business analysts, data scientists, IT teams, vendors, executive leaders, and business stakeholders\nDrive transparency leveraging tech stack and data, own progress reporting and proactively communicate status\nDrive delivery of projects using Agile methodology for data and analytics programs\nFacilitate scrum ceremonies including Sprint planning, Daily stand ups, sprint reviews and retrospectives\nResponsible for defining relevant program metrics, status reports and continuous measurement of program portfolio best practices\nLead, coach, support and mentor junior team members\nInteract with senior leadership teams across Data and analytics, IT and business teams\nKnowledge, Skills and Abilities\nEducation\nBachelor s degree. Technology /IT specialization is preferred.\nMBA is a preferred qualification\nExperience\n8-12+ years of progressive experience in project/program management role with proven people influencing experience including with virtual and global teams\nAgile project management/delivery experience is a must preferably with Data and Analytics background\nProficient in MS Office suite: Excel, PowerPoint, Project.\nUnderstanding of analytical tool stack, Azure Devboards, Jira, SharePoint is a plus\nCSM, SAFe Agilist certifications are preferred\nAbility to identify risks to project success and recommend course of action to prevent risk from negatively impacting the project; Effectively recognize when to escalate issues and options to senior management for resolution\nSuperior solutioning techniques, organizational skills and ability to manage multiple ongoing projects.\nExcellent collaboration and communication skills, both written and verbal\nDemonstrated competency with cross-group collaboration, organizational agility, and analytical planning\nStrong leadership & negotiation skills",Industry Type: Insurance,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['MS Office suite', 'Senior management', 'agile project management', 'Analytical', 'data governance', 'Scrum', 'Agile methodology', 'Data analytics', 'JIRA', 'Analytics']",2025-06-14 06:11:55
HLS Engagement Manager,Neal Analytics,1 - 6 years,Not Disclosed,['Bengaluru'],"Its fun to work in a company where people truly BELIEVE in what they are doing!\nWere committed to bringing passion and customer focus to the business.\nLocation: Start offshore then work out of Vietnam for 1 year.\n\nCore Skill - Pharma SME, SQL, Powerpoint, Excel, Storyboarding, Client Management, Data Science\n\nLead the engagement overseeing the training, roadmap development, and implementation of AI projects\nCollaborate closely with cross-functional teams, including data scientists, engineers, and business stakeholders to ensure alignment of project goals with business objectives.\nDrive the development and execution of project plans, including timelines, resource allocation, and risk management to ensure successful project delivery within scope, budget, and timeline constraints\nServe as the primary point of contact for the client, fostering strong relationships and managing expectations to ensure a high level of customer satisfaction throughout the engagement.\nMonitor project progress and performance, proactively identifying and addressing any issues or obstacles that may arise, and implementing corrective actions as needed to keep the project on track.\nIf you like wild growth and working with happy, enthusiastic over-achievers, youll enjoy your career with us!\nNot the right fit? Let us know youre interested in a future opportunity by clicking Introduce Yourself in the top-right corner of the page or create an account to set up email alerts as new job postings become available that meet your interest!",Industry Type: IT Services & Consulting,Department: Project & Program Management,"Employment Type: Full Time, Permanent","['Storyboarding', 'data science', 'Customer satisfaction', 'Pharma', 'Resource allocation', 'Risk management', 'Project delivery', 'Powerpoint', 'Client management', 'SQL']",2025-06-14 06:11:59
Remote AI Software Developer 57LakhsCTC|| Srinivasa Reddy Kandi,Integra Technologies,12 - 15 years,55-60 Lacs P.A.,"['Ahmedabad', 'Chennai', 'Bengaluru']","Dear Candidate,\nWe are seeking an AI Software Developer to design and implement intelligent applications using machine learning, deep learning, and natural language processing.\n\nKey Responsibilities:\nBuild, train, and deploy AI/ML models for real-world applications.\nCollaborate with data scientists to convert models into production-ready APIs.\nOptimize models for performance, scalability, and latency.\nIntegrate AI capabilities into enterprise systems or consumer products.\nMonitor model performance and update based on new data.\nRequired Skills & Qualifications:\nProficiency in Python and frameworks like TensorFlow, PyTorch, or scikit-learn.\nExperience with AI/ML deployment (e.g., Docker, FastAPI, AWS SageMaker).\nKnowledge of computer vision, NLP, or recommendation systems.\nFamiliarity with MLOps practices and version control for models.\nStrong problem-solving and data structure knowledge.\nSoft Skills:\nStrong troubleshooting and problem-solving skills.\nAbility to work independently and in a team.\nExcellent communication and documentation skills.\nNote: If interested, please share your updated resume and preferred time for a discussion. If shortlisted, our HR team will contact you.\n\nSrinivasa Reddy Kandi\nDelivery Manager\nIntegra Technologies",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Ml Algorithms', 'Machine Learning', 'Ai Algorithms', 'Python', 'Tensorflow', 'Algorithm Development', 'Object Detection', 'Cnn', 'Neural Networks', 'MATLAB', 'Signal Processing', 'Pattern Recognition', 'Opencv', 'Image Processing', 'Algorithm Design', 'Machine Learning Algorithms', 'Computer Vision']",2025-06-14 06:12:03
Machine Learning Professional,Nihilent,5 - 8 years,Not Disclosed,"['Kolkata', 'Pune', 'Chennai']","Nihilent Technologies Pvt. Ltd. is looking for Machine Learning Professional to join our dynamic team and embark on a rewarding career journey\nDevelop and train machine learning models using data sets to address specific business problems\nWork with stakeholders to identify problems and opportunities where machine learning can be applied to improve processes or create new business opportunities\nDesign and implement data pipelines to ingest, clean, transform, and prepare data for analysis\nApply various techniques such as clustering, classification, regression, neural networks, deep learning, and reinforcement learning to build models\nOptimize and fine-tune machine learning models to improve performance and increase accuracy\nPerform statistical analyses and hypothesis testing to evaluate model performance\nCollaborate with cross-functional teams, including data engineers, data analysts, and business stakeholders, to ensure successful model deployment and integration with existing systems\nDevelop and maintain documentation of model development, testing, and deployment processes\nIdentify opportunities for automation and process improvement using machine learning algorithms",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent",['Machine Learning Professional'],2025-06-14 06:12:06
Technical Recruiter Team Lead,DIVERSE,2 - 6 years,2-5.5 Lacs P.A.,['Noida'],"Job Title: Technical Recruiter / Sr. Technical Recruiter IT HiringLocation: Noida (Onsite) Experience Required: 1 to 5 Years Shift: Domestic Shift Industry: IT Consulting / IT Services\nAbout Diverse Lynx:Diverse Lynx is a leading IT staffing and solutions company delivering high-quality talent to top-tier clients, including Big Four consulting firms and Fortune 500 companies. We specialize exclusively in IT hiring and are known for our speed, precision, and delivery excellence.\nRole Overview:We are looking for passionate and result-oriented Technical Recruiters (Junior to Mid-level) who will be responsible for sourcing and hiring top IT talent for our Big 4 and MNC clients. This is an onsite, full-cycle recruitment role in a high-paced delivery environment.Key Responsibilities:\nSourcing & Screening\nSource relevant IT profiles from job portals (Naukri, LinkedIn, etc.), internal database, and referrals.\nScreen and assess candidates based on technical skillsets like Java, .Net, DevOps, Data Engineering, Cloud, Testing, Full Stack, etc.\nConduct initial HR screening and coordinate technical interviews.\nCandidate Management\nEngage with candidates throughout the recruitment lifecycle to ensure a strong candidate experience.\nHandle interview scheduling, follow-ups, offer negotiations, and onboarding.\nClient Coordination (as required)\nFor senior recruiters: Coordinate with client POCs to understand job requirements and update on hiring status.\nMaintain internal trackers and ensure data accuracy for reporting.\nRecruitment Metrics\nMeet daily, weekly, and monthly recruitment targets.\nTrack interview-to-offer and offer-to-join ratios; continuously improve turnaround time and quality of delivery.\nRequirements:\n1 to 5 years of experience in domestic IT recruitment.\nHands-on experience in hiring for technologies like Java, .Net, DevOps, Cloud, QA, Data Science, etc.\nExperience using job portals (Naukri, LinkedIn), Excel trackers, and ATS systems.\nExcellent communication, coordination, and candidate engagement skills.\nStrong understanding of IT roles and hiring trends.\nExperience with client-facing or SPOC responsibilities will be an added advantage for Sr. Recruiters.\nPerks & Benefits:\nCompetitive salary + attractive incentive structure\n5-day work week\nHealth insurance benefits\nExposure to Big 4 and Fortune 500 clients\nFast-track career growth within recruitment leadership\nContact Person : Rashi Yadav\nInterested candidates can share the cv at rashi.yadav@diverselynx.in\nImportant Notes:\nThis role is strictly IT-focused. Non-IT recruiters, or candidates with BFSI, BPO, Healthcare hiring backgrounds will not be considered.\nOnly candidates with hands-on experience in IT hiring should apply.",Industry Type: IT Services & Consulting,Department: Human Resources,"Employment Type: Full Time, Permanent","['Domestic Staffing', 'Contract To Hire', 'C2H', 'Permanent Staffing']",2025-06-14 06:12:10
Project Marketing Executive,Rainfield Technologies,2 - 4 years,3.5-4.75 Lacs P.A.,['Bengaluru'],"Project Marketing Executive Rainfield Technologies\n(Full-Time | On-Site | Immediate Joiner Preferred)\n\nLocation:\nBangalore (Field-based role, 6 days/week)\n\nAbout Rainfield Technologies:\nRainfield Technologies is a project-based company specializing in water management systems including irrigation, rainwater harvesting, swimming pools, and water bodies. Were also expanding into landscaping solutions.\n\nOur work sits at the intersection of civil execution, sustainable infrastructure, and long-term environmental utility. As we grow, were building a strong project marketing function to own B2B relationship-building, site-level outreach, and early-stage technical sales.\n\nRole Summary:\nWe are looking to hire a Project Marketing Executive to independently lead field outreach, technical awareness-building, and lead nurturing for our Bangalore operations. You will be responsible for identifying relevant partners (landscapers, architects, contractors, and FM companies), establishing first contact, tracking their project requirements, and supporting the quoting and pre-sales process. This is a mid-level position that reports directly to the General Manager.\n\nTop Priority Fit What We Value Most:\n\n2–4 years of B2B project marketing or technical field sales experience\nStrong command of English (mandatory) and Kannada (preferred)\nEngineering background (Agricultural, Mechanical, Civil, or Water Systems) strongly preferred\nWillingness to visit client sites, understand project status, and participate in solutioning\nAbility to report independently and grow into handling high-ticket project leads\n\n1. Client Identification & Targeting\n\nIdentify B2B companies and professionals relevant to Rainfield’s offerings.\nBuild a qualified database of landscape architects, landscaping contractors, civil contractors, and facility management firms.\nTarget firms that consistently require irrigation systems, rainwater harvesting, and other water-related solutions.\nKeep this prospect database organized, updated, and location-wise segmented.\n\n2. First Contact & Field Outreach\n\nVisit offices and sites to make direct, face-to-face introductions with potential clients.\nIntroduce Rainfield Technologies as a project partner with end-to-end water management capabilities.\nEstablish the first layer of trust, interest, and brand awareness.\nEnsure these visits are logged, followed up, and pushed forward consistently.\n\n3. Project Site Visits & Coordination with Engineers\n\nAccompany internal engineering teams to active project sites, when needed.\nUnderstand the ground-level work status and verify measurements or specifications alongside engineers.\nUse this information to validate leads and ensure technical alignment before quoting.\n\n4. Lead Nurturing & Requirement Tracking\n\nFollow up regularly with warm leads to maintain relationships over time.\nTrack each lead’s evolving project pipeline and expected timelines.\nUpdate internal tracking sheets or CRM with current lead status and next steps.\nEnsure no lead is dropped or forgotten once the relationship begins.\n\n5. Quotation Preparation Support\n\nCollect all client-side information needed for quote preparation.\nUnderstand basic pricing structures and be able to discuss high-level estimates.\nCoordinate with the internal team to ensure timely delivery of commercial proposals.\nClarify any doubts or changes requested by the client before finalization.\n\n6. Technical Understanding of Products & Solutions\n\nLearn how Rainfield’s systems work — especially for irrigation, pumping, and water-saving applications.\nUnderstand how to match product use cases to client needs based on site/project data.\nEngineering candidates (especially from agriculture, mechanical, or water) will have an edge here.\nNon-engineers can also apply but must commit to learning and passing internal training checkpoints.\n\n7. Client Segmentation & Upskilling Path\n\nBegin with easier clients like landscapers and contractors.\nGradually progress to more advanced stakeholders like landscape architects and real estate developers.\nYour outreach depth and lead complexity will grow as your technical understanding improves.\nUpskilling and growth will be performance-based and supported by mentoring.\n\n8. Reporting & CRM Management\n\nMaintain clear, up-to-date lead tracking and status logs.\nSubmit a weekly report directly to the General Manager, covering site visits, leads touched, follow-up status, and next actions.\nHighlight any bottlenecks, competitor insights, or client feedback from the field.\nMake sure internal communication is smooth and client-facing touchpoints are well-documented.\n\n9. Training & Onboarding\n\nStart with initial training in Bangalore, led by the General Manager.\nCover Rainfield’s products, target profiles, pricing logic, and site pitch techniques.\nOnce ready, attend advanced product and systems training in Nashik or Mumbai.\nTraining will be phased, not overwhelming — you will be tested gradually based on readiness and output.\n\nYou’ll Thrive in This Role If You:\n\nHave 2–4 years of client-facing, field-based B2B sales experience\nCome from an engineering background, or are technically curious and eager to learn\nAre confident walking into new offices or project sites and making first contact\nCan take ownership of your own lead pipeline, follow-up rhythm, and quote handover\nAre energized by field work, client interaction, and technical solutioning\n\nRole Logistics\n\nRole Type: Full-time\nWork Location: Bangalore (on-ground and client-facing)\nWork Days: Monday to Saturday (Sunday off)\nOffice Hours: 9:30 AM – 6:00 PM\nTravel: Daily local travel within Bangalore and surrounding areas\nTwo-wheeler: Preferred (can be arranged for strong candidates)\n\nSalary & Benefits\nMonthly Salary: 30,000 – 40,000 (Negotiable based on experience and fit)\n\nBenefits include:\nProvident Fund (PF)\nEmployee State Insurance (ESI)\nPaid Leaves\nPetrol Reimbursement for all field-related travel",Industry Type: Miscellaneous,Department: Other,"Employment Type: Full Time, Permanent","['B2B Marketing', 'Field Sales', 'Technical Sales', 'B2B Sales']",2025-06-14 06:12:14
Python (Programming Language) - Hyderabad or Bangalore,Fortune Global 500 IT Services Firm,3 - 8 years,Not Disclosed,"['Hyderabad', 'Bengaluru']","Project Role : Application Developer\nProject Role Description : Design, build and configure applications to meet business process and application requirements.\nMust have skills : Python (Programming Language)\nGood to have skills : NA\nMinimum 3 year(s) of experience is required\nEducational Qualification : 15 years full time education\nSummary:\nAs an Application Developer, you will design, build, and configure applications to meet business process and application requirements. You will be responsible for ensuring the functionality and efficiency of the applications, as well as collaborating with the team to provide solutions to work-related problems. A typical day in this role involves designing and implementing application features, troubleshooting issues, and actively participating in team discussions to contribute to the development process.\n\nRoles & Responsibilities:\n- Expected to perform independently and become an SME.\n- Required active participation/contribution in team discussions.\n- Contribute in providing solutions to work-related problems.\n- Design and implement application features based on business requirements.\n- Troubleshoot and resolve issues in the applications.\n- Collaborate with the team to provide solutions to work-related problems.\n- Participate in code reviews to ensure code quality and adherence to coding standards.\n- Conduct unit testing and debugging of applications.\n- Stay updated with the latest technologies and industry trends.\n- Assist in the documentation of application design and functionality.\n\nProfessional & Technical Skills:\n- Must To Have Skills: Proficiency in Python (Programming Language).\n- Strong understanding of software development principles and methodologies.\n- Experience in designing and building applications based on business requirements.\n- Knowledge of database management systems and SQL.\n- Experience with version control systems such as Git.\n- Good To Have Skills: Experience with web development frameworks such as Django or Flask.\n- Familiarity with front-end technologies such as HTML, CSS, and JavaScript.\n- Experience with cloud platforms such as AWS or Azure.\n\nAdditional Information:\n- The candidate should have a minimum of 3 years of experience in Python (Programming Language).\n- This position is based at our Hyderabad office.\n- A 15 years full-time education is required.\n15 years full time education",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Python Development', 'Dash', 'Django', 'Pandas', 'Numpy', 'Flask', 'Python']",2025-06-14 06:12:18
Fullstack Developer,Immersive Infotech,3 - 5 years,6-16 Lacs P.A.,['Bengaluru'],"Job Title: Full Stack Developer\nLocation: Bangalore (Onsite)\nExperience: 3 to 4 Years\n\nJoining: Immediate to 15 Days\nAbout the Role\nWe are seeking a highly motivated Full Stack Developer to join our team in Bangalore. The ideal candidate will have a solid foundation in web development, a passion for integrating ML/AI solutions, and experience working in cloud-based DevOps environments. This is an on-site contractual opportunity for individuals who enjoy working in fast-paced, cross-functional teams.\nKey Responsibilities\nDesign, develop, and maintain scalable front-end and back-end web applications.\nCollaborate with data scientists and MLOps engineers to integrate ML models and data pipelines into production systems.\nImplement and manage CI/CD pipelines, automate deployments, and support DevOps processes.\nDesign, maintain, and optimise relational and non-relational databases.\nBuild and maintain robust RESTful APIs.\nMonitor application performance and implement optimisation strategies.\nEnsure security best practices are followed throughout the development lifecycle.\nMaintain clean, well-documented, and testable codebases.\nKeep up to date with the latest industry trends, tools, and technologies.\nRequired Qualifications\nBachelor's degree in Computer Science, Information Technology, or a related field.\n3 to 4 years of professional experience as a full-stack developer.\nDemonstrated portfolio or examples of previous web application projects.\nTechnical Skills\nFront-end:\nReact.js, Angular\nHTML5, CSS3, JavaScript/TypeScript\nBack-end:\nNode.js (Express/Nest.js)\nFlask, Django, or FastAPI\nCloud Platforms:\nAWS or Azure\nDevOps Tools:\nDocker, Kubernetes\nExperience with CI/CD tools and pipelines\nDatabases:\nSQL (PostgreSQL, MySQL)\nNoSQL (MongoDB, DynamoDB)\nNice to Have\nExperience integrating ML/AI models into production.\nFamiliarity with GraphQL and WebSockets.\nUnderstanding of application security standards (OWASP).",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Temporary/Contractual","['Angularjs', 'Microsoft Azure', 'Node.Js', 'React.Js', 'AWS']",2025-06-14 06:12:21
Power BI Consultant,Microexcel,3 - 6 years,Not Disclosed,"['Kolkata', 'Mumbai', 'New Delhi', 'Hyderabad', 'Pune', 'Chennai', 'Bengaluru']","Join New Era Technology, where\nPeople First\nis at the heart of everything we do. With a global team of over 4,500 professionals, we re committed to creating a workplace where everyone feels valued, empowered, and inspired to grow. Our mission is to securely connect people, places, and information with end-to-end technology solutions at scale.\n\nAt New Era, you ll join a team-oriented culture that prioritizes your personal and professional development. Work alongside industry-certified experts, access continuous training, and enjoy competitive benefits. Driven by values like Community, Integrity, Agility, and Commitment, we nurture our people to deliver exceptional customer service.\n\nIf you want to make an impact in a supportive, growth-oriented environment, New Era is the place for you. Apply today and help us shape the future of work together.\nJob Title: Power BI Developer\nLocation: Remote (India)\nEmployment Type: Full-Time - Permanent\nCompany: New Era Technology - www.neweratech.com\nAbout Us\nJoin New Era Technology , a global IT solutions provider and Microsoft Gold Partner , with a presence in over 80 locations worldwide. Our team of 4,500+ professionals is dedicated to delivering innovative, scalable, and secure digital transformation solutions across industries including healthcare, manufacturing, retail, education, and more.\nWe are currently seeking a Power BI Developer to join our growing analytics team. The selected candidate will be on the permanent rolls of New Era Technology and will have the flexibility to work remotely.\nKey Responsibilities\nDesign, develop, and deploy interactive and visually compelling Power BI dashboards and reports\nTranslate business needs into technical specifications using Power BI and related tools\nDevelop data models and perform DAX queries to optimize dashboard performance\nConnect to various data sources (SQL Server, Excel, APIs, etc.) and transform data using Power Query (M language)\nEnsure accuracy and data integrity in all reporting and visualizations\nWork closely with cross-functional teams (BI, Data Engineering, and Business Analysts) to gather requirements\nDeploy and maintain reports in Power BI Service and manage access permissions\nStay current with Power BI updates and recommend enhancements\nKey Requirements\n3 to 6 years of hands-on experience in Power BI development\nStrong proficiency in DAX , Power Query (M), and data modeling\nSound knowledge of SQL and experience working with relational databases\nExperience with Power BI Service - publishing, scheduling refreshes, gateway management, etc.\nFamiliarity with Power Apps, Power Automate is a plus\nAbility to translate business problems into actionable insights\nStrong communication skills and ability to collaborate in a remote work environment\nNice to Have\nExperience working in Agile environments\nMicrosoft Power BI Certification\nExposure to cloud platforms like Azure (especially Azure SQL, Data Lake, Synapse)\nWhat We Offer\nWork-from-home flexibility\nOpportunity to be part of a fast-growing Microsoft Gold Partner\nAccess to global projects and emerging technologies\nCollaborative and inclusive work culture\nContinuous learning and development opportunities\nReady to join a team that s transforming data into insights?\nApply today and be part of our digital innovation journey at New Era Technology.\n\n\nView our Privacy Policy here https: / / www.neweratech.com / us / privacy-policy /",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Publishing', 'Data modeling', 'Agile', 'power bi', 'Healthcare', 'Scheduling', 'Customer service', 'microsoft', 'Analytics', 'SQL']",2025-06-14 06:12:24
Apache Spark - Pan India,Fortune Global 500 IT Services Firm,5 - 10 years,Not Disclosed,"['Bhubaneswar', 'Bengaluru', 'Delhi / NCR']","Project Role : Application Developer\nProject Role Description : Design, build and configure applications to meet business process and application requirements.\nMust have skills : Apache Spark\nGood to have skills : Oracle Procedural Language Extensions to SQL (PLSQL)\nMinimum 5 year(s) of experience is required\nEducational Qualification : 15 years full time education\nSummary:\nAs an Application Developer, you will design, build, and configure applications to meet business process and application requirements. You will be responsible for ensuring that the applications are developed and implemented efficiently and effectively, while meeting the needs of the organization. Your typical day will involve collaborating with the team, making team decisions, engaging with multiple teams, and providing solutions to problems for your immediate team and across multiple teams. You will also contribute to key decisions and provide expertise in application development.\n\nRoles & Responsibilities:\n- Expected to be an SME\n- Collaborate and manage the team to perform\n- Responsible for team decisions\n- Engage with multiple teams and contribute on key decisions\n- Provide solutions to problems for their immediate team and across multiple teams\n- Design, build, and configure applications to meet business process and application requirements\n- Ensure that applications are developed and implemented efficiently and effectively\n- Contribute expertise in application development\n\nProfessional & Technical Skills:\n- Must To Have Skills: Proficiency in Apache Spark\n- Good To Have Skills: Experience with Oracle Procedural Language Extensions to SQL (PLSQL), Google BigQuery\n- Strong understanding of statistical analysis and machine learning algorithms\n- Experience with data visualization tools such as Tableau or Power BI\n- Hands-on implementing various machine learning algorithms such as linear regression, logistic regression, decision trees, and clustering algorithms\n- Solid grasp of data munging techniques, including data cleaning, transformation, and normalization to ensure data quality and integrity\n\nAdditional Information:\n- The candidate should have a minimum of 5 years of experience in Apache Spark\n- This position is based at our Gurugram office\n- A 15 years full time education is required",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Apache Spark', 'Streaming', 'Google BigQuery', 'Sql Plsql', 'Spark Streaming']",2025-06-14 06:12:26
"Database Administrator (DBA) - SQL Server, Azure",Vertiv Group Corp,4 - 5 years,Not Disclosed,['Pune'],"Senior Database Administrator\n\n\nJob Summary\n\nWe are looking for a seasoned Senior Database Administrator (DBA) with strong expertise in SQL Server to join our IoT Software team . In this role, you will manage, optimize, and scale databases that support massive volumes of machine-generated and telemetry data from connected devices. The ideal candidate is passionate about data performance, experienced in high-ingestion environments, and comfortable working in fast-paced, innovation-driven teams.",,,,"['Computer science', 'Performance tuning', 'Automation', 'Disaster recovery', 'Data structures', 'Data quality', 'Stored procedures', 'microsoft', 'Analytics', 'SQL']",2025-06-14 06:12:28
"Machine Learning Engineer (AWS, Databricks, RAG & LLMs)",V3-empower-solutions,6 - 8 years,8-12 Lacs P.A.,['Hyderabad'],"ML Engineer | RAG, LLM, AWS, Databricks | 6–8 Yrs Exp | Build scalable ML systems with GenAI, pipelines & cloud integration",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Cloud Integration', 'Pyspark', 'AWS', 'Data Bricks', 'Python', 'code commit']",2025-06-14 06:12:30
AWS BigData,mncs,5 - 10 years,Not Disclosed,"['Gurugram', 'Bengaluru', 'Mumbai (All Areas)']","Project Role : Software Development Lead\nProject Role Description : Develop and configure software systems either end-to-end or for a specific stage of product lifecycle. Apply knowledge of technologies, applications, methodologies, processes and tools to support a client, project or entity.\nMust have skills : AWS BigData\nMinimum 5 year(s) of experience is required\nEducational Qualification : 15 years full time education\nSummary:\nAs a Software Development Lead, you will be responsible for developing and configuring software systems either end-to-end or for a specific stage of the product lifecycle. You will apply your knowledge of technologies, applications, methodologies, processes, and tools to support clients, projects, or entities. Your typical day will involve collaborating with the team, making team decisions, engaging with multiple teams, and providing solutions to problems for your immediate team and across multiple teams. You will also contribute to key decisions and ensure the successful execution of projects.\n\nRoles & Responsibilities:\n- AWS EMR, Glue, S3, Python/PySpark\n- Resource must have SQL experience\n- Expected to be an SME\n- Collaborate and manage the team to perform\n- Responsible for team decisions\n- Engage with multiple teams and contribute on key decisions\n- Provide solutions to problems for their immediate team and across multiple teams\n- Lead the development and configuration of software systems\n- Ensure the successful execution of projects\n- Contribute to the improvement of processes and methodologies\n\nProfessional & Technical Skills:\n- Must To Have Skills: Proficiency in AWS BigData\n- Strong understanding of statistical analysis and machine learning algorithms\n- Experience with data visualization tools such as Tableau or Power BI\n- Hands-on implementing various machine learning algorithms such as linear regression, logistic regression, decision trees, and clustering algorithms\n- Solid grasp of data munging techniques, including data cleaning, transformation, and normalization to ensure data quality and integrity",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Glue', 'AWS BigData', 'EMR', 'Python']",2025-06-14 06:12:32
Tableau Cloud,Useready,8 - 12 years,Not Disclosed,"['Mohali', 'Gurugram', 'Bengaluru']","Key Responsibilities:\nLead Tableau Cloud implementation and architecture design, including site structure, user provisioning, data connections, and security models.\nDevelop and enforce Tableau governance policies, including naming conventions, folder structure, usage monitoring, and content lifecycle management.\nGuide and mentor Tableau developers and analysts in best practices for visualization design, performance tuning, and metadata management.\nCollaborate with Data Engineering and BI teams to design scalable, secure, and high-performing data models.\nMonitor Tableau Cloud usage, site health, and performance, proactively resolving issues and managing upgrades or configuration changes.\nWork closely with stakeholders to understand business requirements and translate them into high-impact Tableau dashboards and reports.\nDevelop technical documentation, training materials, and user guides for Tableau Cloud users and developers.\nLead or support migration efforts from Tableau Server or on-prem environments to Tableau Cloud.\nLead Tableau Cloud implementation and architecture design, including site structure, user provisioning, data connections, and security models.\nDevelop and enforce Tableau governance policies, including naming conventions, folder structure, usage monitoring, and content lifecycle management.\nGuide and mentor Tableau developers and analysts in best practices for visualization design, performance tuning, and metadata management.\nCollaborate with Data Engineering and BI teams to design scalable, secure, and high-performing data models.\nMonitor Tableau Cloud usage, site health, and performance, proactively resolving issues and managing upgrades or configuration changes.\nWork closely with stakeholders to understand business requirements and translate them into high-impact Tableau dashboards and reports.\nDevelop technical documentation, training materials, and user guides for Tableau Cloud users and developers.\nLead or support migration efforts from Tableau Server or on-prem environments to Tableau Cloud.\nQualifications\nRequired:\nBachelor’s or Master’s degree in Computer Science, Information Systems, or a related field.\n5+ years of experience with Tableau, with at least 1–2 years in Tableau Cloud (SaaS) specifically.\nStrong experience with dashboard design, Tableau calculations (LOD, table calcs), parameters, and actions.\nSolid understanding of data visualization best practices and UX principles.\nHands-on experience managing Tableau Cloud environments, including content governance, access control, and site administration.\nPreferred:\nTableau Certified Associate or Tableau Certified Professional certification.\nExperience with Tableau Prep, REST API, or other automation tools for Tableau administration.\nFamiliarity with DevOps, CI/CD, or version control for BI assets (e.g., Git).\nExperience migrating from Tableau Server to Tableau Cloud.\nKnowledge of data security, privacy, and compliance standards in cloud environments.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['calculation', 'architectural design', 'cd', 'continuous integration', 'rest', 'dashboard development', 'bi', 'ci/cd', 'monitoring', 'dashboards', 'cloud', 'tableau', 'git', 'automation tools', 'automation', 'saas', 'devops', 'compliance', 'migration', 'cloud infrastructure', 'access control', 'data visualization', 'reporting', 'architecture']",2025-06-14 06:12:35
MongoDB Developer,Suzva Software Technologies,5 - 7 years,Not Disclosed,"['Mumbai', 'Delhi / NCR', 'Bengaluru']","Locations : Mumbai, Delhi / NCR, Bengaluru , Kolkata, Chennai, Hyderabad, Ahmedabad, Pune, Remote\nEmployment Type: Full-Time / Contract (Specify as needed)\nNotice Period: Immediate to 15 days preferred\n\nWe are actively looking for a MongoDB Developer to join our growing technology team. This role involves working on data modeling, ingestion, and integration with various systems while ensuring high performance, scalability, and reliability of data-driven applications.\n\nKey Responsibilities:\nDesign, develop, and manage scalable and optimized MongoDB-based database solutions\n\nWork on data modeling for performance and storage efficiency\n\nDevelop effective and scalable queries and operations using MongoDB\n\nIntegrate third-party services, APIs, and tools with MongoDB for streamlined data management\n\nWork collaboratively with developers, data engineers, and business teams to ensure seamless application integration\n\nWrite and execute unit, integration, and performance tests for MongoDB implementations\n\nConduct code reviews and database optimization, ensuring best practices in data security and architecture\n\nDocument and maintain schema changes and performance improvements\n\nPreferred Skills:\nExperience with Snowflake or similar cloud-based data warehouses is an advantage\n\nExposure to Agile methodologies\n\nFamiliarity with CI/CD pipelines for data workflows\n\nPrimary Skills Required:\nStrong experience with MongoDB (querying, schema design, ingestion)\n\nFamiliarity with JavaScript/Node.js or Python (for integrations, if applicable)\n\nAPI Integration\n\nPerformance tuning and data management\n\nMongoDB Atlas (preferred).",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['MongoDB', 'JavaScript', 'CI/CD', 'Agile', 'data warehousing', 'MongoDB Developer', 'Python']",2025-06-14 06:12:37
ASP.Net MVC Developer | Work from office | MNC,Mount Talent Consulting Private Limited,4 - 8 years,6-12 Lacs P.A.,"['Chennai', 'Bengaluru', 'Mumbai (All Areas)']","Hiring for- Services based MNC Client\nWork location- Bangalore, Chennai, Hyderabad, Kolkata, Gurugram, Pune, Mumbai (as preferred)\nOffered salary- 30-35% hike on current CTC (Non negotiable, no hike will be given on a counter offer)\nRole- ASP.Net MVC Developer\n\n\nResponsibilities :\nDesign, build, test, debug, and document software to meet the business process and application requirements.\nDevelop, maintain, and support programs tools for internal and external clients. Analyze, diagnose, and resolve errors related to their applications.\n\n\nTechnical Experience :\nASP.NET MVC development across the technology stack eg Front end, web APIs and database.\nDesign, mock-up and implement web application prototypes before actual implementation\nCollaborate with teams of experienced data scientists, analysts, developers and business experts to build high-performing web applications\nAssist in support and enhancements of existing applications, if required.",Industry Type: Management Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['ASP.Net', 'ASP.Net MVC', 'Web Api', 'Asp.Net Core', 'Asp.Net Web Api', 'Asp.net development']",2025-06-14 06:12:39
Tiger Graph or Neo4J-,Rarr Technologies,6 - 8 years,Not Disclosed,['Bengaluru'],"Flashed JC 72927, enable some profiles today.\nExperience: 6-8years\nBand: B3\nLocation: Bangalore , Chennai & Hyderabad\nBelow is the JD:\n6+ years of hands-on experience in software development or data science\nSupport the company s commitment to protect the integrity and confidentiality of systems and data.\nExperience building E2E analytics platform using Streaming, Graph and Big Data platform\nExperience with Graph-based data workflows and working with Graph Analytics\nExtensive hands on experience in designing, developing, and maintaining software frameworks using Kafka, Spark, Neo4J, Tiger Graph DB,\nHands one experience on Java, Scala or Python\nDesign and build and deploy streaming and batch data pipelines capable of processing and storing large datasets quickly and reliably using Kafka, Spark and YARN\nExperience managing and leading small development teams in an Agile environment\nDrive and maintain a culture of quality, innovation and experimentation\nCollaborate with product teams, data analysts and data scientists to design and build data-forward solutions\nProvide the prescriptive point-solution architectures and guide the descriptive architectures within assigned modules\nOwn technical decisions for the solution and application developers in the creation of architectural decisions and artifacts\nManage day-to-day technology architectural decision for limited number of specified assigned modules including making decision on best path to achieve requirements and schedules.\nOwn the quality of modules being delivered, insure proper testing and validation processes are followed.\nEnsure the point-solution architectures are in line with the enterprise strategies and principles\nReviews technical designs to ensure that they are consistent with defined architecture principles, standards and best practices.\nAccountable for the availability, stability, scalability, security, and recoverability enabled by the designs\nAbility to clearly communicate with team & stakeholders\nNeo4j, Tigergraph Db",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['neo4j', 'Architecture', 'data science', 'spark', 'SCALA', 'Agile', 'big data', 'YARN', 'Analytics', 'Python']",2025-06-14 06:12:41
Databricks with AWS cloud service,Clifyx Technology,8 - 10 years,Not Disclosed,['Bengaluru'],"ECMS Req # /Demand ID\n529266\nPU\nDNA\nRole\nTechnology Lead\nNumber of Openings\n1\nDuration of project\n1-2 years\nNo of years experience\n8-10 years\nDetailed job description - Skill Set:\n6+ years of industry experience including cloud technologies.\nVery strong hands-on experience in Databricks with AWS cloud service in Data engineering/Data processing hands-on.\nHands on experience in AWS C loud-based development and integration\nProficiency in programming languages Scala and Spark Data frame for data processing and application development\nPractical experience with Data Engineering, Data Ingestion/Orchestration with Apache Airflow and the accompanying DevOps with CI-CD tools\nStrong knowledge on Spark, Databricks SQL - Data engineering pipeline\nExperience in offshore/onshore model and Ability and agile methodology.\nGathering requirements, understand the business need and regular discussion with tech on design, development activities.\nShould have good experience working with client architect/design team to understand the architecture, requirement and work on the development.\nExperience working in a Financial Industry.\nCertification on Databricks and AWS will be added advantage",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['orchestration', 'spark', 'Billing', 'Cloud', 'SCALA', 'Data processing', 'Application development', 'Apache', 'AWS', 'SQL']",2025-06-14 06:12:43
SAP HCM On Premise ABAP,MNCS,5 - 10 years,Not Disclosed,['Bengaluru'],"Project Role : Data Engineer\nProject Role Description : Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\nMust have skills : SAP HCM On Premise ABAP\nGood to have skills : SAP HCM Organizational Management\nMinimum 5 year(s) of experience is required\nEducational Qualification : 15 years full time education\nSummary:\nAs a Data Engineer, you will design, develop, and maintain data solutions for data generation, collection, and processing. Your typical day will involve creating data pipelines, ensuring data quality, and implementing ETL processes to migrate and deploy data across systems. You will play a crucial role in managing and analyzing data to support business decisions and drive data-driven insights.\n\nRoles & Responsibilities:\n- Expected to be an SME, collaborate and manage the team to perform.\n- Responsible for team decisions.\n- Engage with multiple teams and contribute on key decisions.\n- Provide solutions to problems for their immediate team and across multiple teams.\n- Design, develop, and maintain data solutions for data generation, collection, and processing.\n- Create data pipelines to extract, transform, and load data across systems.\n- Ensure data quality and integrity throughout the data lifecycle.\n- Implement ETL processes to migrate and deploy data across systems.\n\nProfessional & Technical Skills:\n- Must To Have Skills: Proficiency in SAP HCM On Premise ABAP.\n- Good To Have Skills: Experience with SAP HCM Organizational Management.\n- Strong understanding of data engineering principles and best practices.\n- Experience with data modeling and database design.\n- Hands-on experience with ETL tools and processes.\n- Proficient in programming languages such as ABAP, SQL, and Python.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SAP HCM', 'SAP Premise ABAP', 'SAP HCM Organizational Management', 'Sap Hcm On Premise Abap', 'ABAP', 'PYTHON', 'SQL']",2025-06-14 06:12:46
Software Engineer,MAN Truck & Bus,3 - 6 years,Not Disclosed,['Pune'],"Role & responsibilities\nFacilitate Agile ceremonies and lead Scrum practices.\nSupport the Product Owner in backlog management and\nteam organization.\nPromote Agile best practices (Scrum, SAFe) and",,,,"['Python', 'ETL', 'CSM', 'SQL']",2025-06-14 06:12:48
AI/ML Engineer,Ahurasense Technologies PVT LTD,3 - 8 years,9-19 Lacs P.A.,['Ahmedabad( Prahlad Nagar )'],"We're seeking an experienced AI/ML Engineer who thrives on curiosity and innovation to join our dynamic engineering team. This role offers an exciting chance to tackle impactful real-world machine learning challenges and play a key part in developing advanced, state-of-the-art AI solutions.\n\nRole & responsibilities\nArchitect, train, fine-tune, and distill LLMs and deep-learning models for NLP, generative AI, and agentic AI applications.\nAssist in designing, developing, and training machine learning models using structured and unstructured data\nConduct thorough testing, evaluation, benchmarking, and validation to ensure performance and robustness\nDeploy models into production using MLOps pipelines (CI/CD, versioning, monitoring)\nOptimize models and inference pipelines for latency, throughput, and compute efficiency\nMentor junior engineers and collaborate cross-functionally with data engineers, DevOps, product managers\nCurate, cleanse, and transform extensive datasets to prepare them for robust model development.\nConduct insightful exploratory data analysis and apply advanced statistical modeling techniques.\nExecute systematic experiments, optimize hyperparameters, and rigorously assess models using established performance metrics.\nMaintain comprehensive documentation detailing model architectures, data pipelines, and experimental outcomes.\nDevelop and integrate AI-enhanced tools including web-search functionalities, intelligent image editing, and related applications.\nSupport in integrating AI/ML models into production environments\n\nSkill Sets We Require-\nFrameworks: PyTorch (latest, including TorchDynamo), TensorFlow 2.x\nFinetuning & Distillation: Implement teacherstudent training, supervised finetuning, responsebased distillation and Transorfermers.\nModel Lifecycle\nHands-on experience with Python and ML libraries such as scikit-learn, pandas, NumPy.\nUnderstanding of probability, statistics, and linear algebra\nAPIs & Storage: RESTful APIs, ONNX, SQL/NoSQL databases, S3 Buckets.\nCI/CD: Git, GitHub Actions, Docker Registry, automated testing & deployment (Good to have)\n\nRequired Qualifications-\nBtech/BE in Computer Science or Equivalent Field\n4+ years of hands-on experience in training, tuning, and deploying LLMs and deep-learning models.\nProficient with PyTorch, Transformer frameworks",Industry Type: Emerging Technologies (AI/ML),Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Natural Language Processing', 'Neural Networks', 'Agentic Ai', 'Transformers', 'Generative Artificial Intelligence', 'Tensorflow', 'Distillation', 'Artificial Intelligence', 'Mathematics', 'Calculus', 'Deep Learning', 'Pytorch', 'Fine Tuning', 'Image Processing', 'Pandas', 'Computer Vision', 'Python']",2025-06-14 06:12:50
Analytics Engineer,Mitratech India,5 - 10 years,Not Disclosed,[],"At Mitratech, we are a team of technocrats focused on building world-class products that simplify operations in the Legal, Risk, Compliance, and HR functions of Fortune 100 companies. We are a close-knit, globally dispersed team that thrives in an ecosystem that supports individual excellence and takes pride in its diverse and inclusive work culture centered around great people practices, learning opportunities, and having fun! Our culture is the ideal blend of entrepreneurial spirit and enterprise investment, enabling the chance to move at a rapid pace with some of the most complex, leading-edge technologies available.\n\nGiven our continued growth, we always have room for more intellect, energy, and enthusiasm - join our global team and see why it's so special to be a part of Mitratech!\n\nJob Description\nWe are seeking a highly motivated and skilled Analytics Engineer to join our dynamic data team. The ideal candidate will possess a strong background in data engineering and analytics, with hands-on experience in modern analytics tools such as Airbyte, Fivetran, dbt, Snowflake, Airflow, etc. This role will be pivotal in transforming raw data into valuable insights, ensuring data integrity, and optimizing our data infrastructure to support the organization's data platform.\n\nEssential Duties & Responsibilities\nData Integration and ETL Processes:\nDesign, implement, and manage ETL pipelines using tools like Airbyte and Fivetran to\nensure efficient and accurate data flow from various sources into our Snowflake data\nwarehouse.\nMaintain and optimize existing data integration workflows to improve performance and\nscalability.\nData Modeling and Transformation:\nDevelop and maintain data models using dbt / dbt Cloud to transform raw data into\nstructured, high-quality datasets that meet business requirements.\nEnsure data consistency and integrity across various datasets and implement data\nquality checks.\nData Warehousing:\nManage and optimize our Redshift / Snowflake data warehouses, ensuring it meets\nperformance, storage, and security requirements.\nImplement best practices for data warehouse management, including partitioning,\nclustering, and indexing.\nCollaboration and Communication:\nWork closely with data analysts, data scientists, and business stakeholders to\nunderstand data requirements and deliver solutions that meet their needs.\nCommunicate complex technical concepts to non-technical stakeholders in a clear and\nconcise manner.\nContinuous Improvement:\nStay updated with the latest developments in data engineering and analytics tools, and\nevaluate their potential to enhance our data infrastructure.\nIdentify and implement opportunities for process improvements, automation, and\noptimization within the data pipeline.\n\nRequirements & Skills:\nEducation and Experience:\nBachelor's degree in Computer Science, Information Systems, Data Science, or a related field.\n3-5 years of experience in data engineering or analytics engineering roles.\nExperience in AWS and DevOps is a plus.\nTechnical Skills:\nProficiency with modern ETL tools such as Airbyte and Fivetran.\nMust have experience with dbt for data modeling and transformation.\nExtensive experience working with Snowflake or similar cloud data warehouses.\nSolid understanding of SQL and experience writing complex queries for data extraction and\nmanipulation.\nFamiliarity with Python or other programming languages used for data engineering tasks.\nAnalytical Skills:\nStrong problem-solving skills and the ability to troubleshoot data-related issues.\nAbility to understand business requirements and translate them into technical specifications.\nSoft Skills:\nExcellent communication and collaboration skills.\nStrong organizational skills and the ability to manage multiple projects simultaneously.\nDetail-oriented with a focus on data quality and accuracy.\n\nWe are an equal-opportunity employer that values diversity at all levels. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, national origin, age, sexual orientation, gender identity, disability, or veteran status.",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Snowflake', 'Data Build Tool', 'Airbyte', 'Fivetran']",2025-06-14 06:12:53
AI Engineer,Royal Cyber,8 - 10 years,Not Disclosed,['Chennai'],"We are seeking a highly skilled AI Data Scientist to join our growing data science team.\nThe ideal candidate will possess strong analytical capabilities, machine learning expertise, and practical experience deploying AI solutions.\nYou will work on cutting-edge projects involving deep learning, NLP, and predictive analytics to solve complex business challenges.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['deep learning', 'data science', 'Analytical', 'Machine learning', 'Deployment', 'Predictive analytics']",2025-06-14 06:12:55
AI/ML Engineer - Generative AI,Maimsd Technology,10 - 12 years,Not Disclosed,['Hyderabad'],"We are seeking a highly skilled and experienced AI/ML Lead to spearhead the development and execution of our AI build plan and investment strategy. The ideal candidate will possess a deep understanding of AI/ML technologies, strong leadership abilities, and a proven track record of delivering successful AI initiatives.\n\nResponsibilities :\n\nAI Build Plan Development :\n\n- Conduct in-depth analysis of business requirements and identify AI/ML opportunities to drive growth and efficiency.\n\n- Develop a comprehensive AI build plan outlining strategic objectives, technology roadmap, and resource allocation.\n\n- Prioritize AI initiatives based on business impact, feasibility, and ROI.\n\n- Collaborate with cross-functional teams to align AI strategy with overall business goals.\n\nAI Investment Strategy :\n\n- Define AI investment criteria and evaluate potential AI projects for funding.\n\n- Develop financial models to assess the ROI of AI initiatives and justify investments.\n\n- Manage AI budget allocation and track spending against the approved plan.\n\nAI Team Leadership :\n\n- Build and lead a high-performing AI team of data scientists, machine learning engineers, and other AI professionals.\n\n- Provide technical guidance and mentorship to team members.\n\n- Foster a culture of innovation and experimentation within the AI team.\n\nAI Model Development and Deployment :\n\n- Oversee the development and deployment of AI models across various business domains.\n\n- Ensure the quality, accuracy, and scalability of AI models.\n\n- Monitor model performance and implement necessary improvements.\n\nTechnology Evaluation and Adoption :\n\n- Stay up-to-date with the latest advancements in AI/ML technologies.\n\n- Evaluate new AI tools and platforms to identify potential benefits for the organization.\n\n- Drive the adoption of AI technologies across the enterprise.\n\nRequired Qualifications :\n\n- Strong proficiency in Python, C++, and deep learning frameworks (Keras, TensorFlow, PyTorch, etc.)\n\n- Extensive experience in machine learning algorithms (CNNs, RNNs, LSTMs, Seq2seq) and neural network architectures.\n\n- Proven ability to create and optimize AI models for various use cases.\n\n- Deep understanding of performance and accuracy metrics for different AI models.\n\n- Experience with AI model optimization for CPUs and GPUs.\n\n- Knowledge of machine learning accelerators and related software.\n\n- Excellent communication and interpersonal skills.\n\n- Strong analytical and problem-solving abilities.\n\n- Experience with open-source tools for AI model optimization (TF/ONNX/PyTorch).\n\n- Exposure to GenAI technologies and their potential applications.\n\nPreferred Qualifications :\n\n- Experience in leading AI projects from inception to deployment.\n\n- Knowledge of cloud platforms (AWS, Azure, GCP).\n\n- Experience with big data technologies and tools.\n\n- A Master's or Ph.D. degree in computer science, statistics, or a related field.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Generative AI', 'Artificial Intelligence', 'Google Cloud Platform', 'AIML', 'LLM', 'Machine Learning']",2025-06-14 06:12:57
Software Engineer / Jetpac/ Gurgaon,Circles,1 - 4 years,Not Disclosed,['Gurugram'],"Founded in 2014, Circles is a global technology company reimagining the telco industry with its innovative SaaS platform, empowering telco operators worldwide to effortlessly launch innovative digital brands or refresh existing ones, accelerating their transformation into techcos.\nToday, Circles partners with leading telco operators across multiple countries and continents, including KDDI Corporation, Etisalat Group (e), ATT, and Telkomsel, creating blueprints for future telco and digital experiences enjoyed by millions of consumers globally.\nBesides its SaaS business, Circles operates three other distinct businesses:\nCircles.Life : A wholly-owned digital lifestyle telco brand based in Singapore, Circles.Life is powered by Circles SaaS platform and pioneering go-to-market strategies. It is the digital market leader in Singapore and has won numerous awards for marketing, customer service, and innovative product offerings beyond connectivity.\nCircles Aspire : A global provider of Communications Platform-as-a-Service (CPaaS) solutions. Its cloud-based Experience Cloud platform enables enterprises, service providers and developers to deliver and scale mobile, messaging, IoT, and connectivity services worldwide.\nJetpac : Specializing in travel tech solutions, Jetpac provides seamless eSIM roaming for over 200 destinations and innovative travel lifestyle products, redefining connectivity for digital travelers. Jetpac was awarded Travel eSIM of the Year.\nCircles is backed by renowned global investors, including Peak XV Partners (formerly Sequoia), Warburg Pincus, Founders Fund, and EDBI (the investment arm of the Singapore Economic Development Board), with a track record of backing industry challengers.\nJetpac is a revolutionary product launched Q4 2022 solving the ever dynamic problem of travel convenience. Starting from connectivity, it is now moving towards the holistic experience by including insurance coverage, concierge services as well as other AI-powered features.\nAs we re growing to become the biggest and most loved brand, we are looking for: Jetpac Software Engineer\nMission\nWork with the team to learn about Practical Software Development and deliver Solutions based upon Python, Node.js, GoLang and React.js.\nAs a key member of our team, your responsibilities include:\nWorking closely with engineers across the company to develop the best technical design and strategy\nOwning the delivery of various timelines, ensuring that key milestones are met and deliveries are of the highest quality\nAdapt to changing priorities and ambiguities which may arise from complex, cutting-edge and industry disrupting initiatives\nCollaborate with other non-technical stakeholders of the business which includes Product Managers, Designers, Marketing etc.\nWorking with an excellent team of talented engineers to solve the travel convenience problem\nWe will provide you..\nAn insight into inner working of a growing and continuously evolving organisation\nExcellent environment to learn and grow your development skills\nRegular feedback to improve both development and soft skills.\nRequirements:\nDegree / Masters in Computer Science, Software Engineering or equivalent\nStrong fundamentals on software development\nHaving actively worked on open source projects would be a plus\nExperience on at least any one of the following:\nGolang\nNode.js\nReact.js\nReact Native (Preferred)\nPython (optional)\nFamiliarity on RESTful web applications\nReact frontend development is a plus\nData engineering fundamentals\nPassionate about delivering quality work ahead of time\nExcellent interpersonal skills\nTeam player\nCircles is committed to a diverse and inclusive workplace. We are an equal opportunity employer and do not discriminate on the basis of race, national origin, gender, disability or age.\nTo all recruitment agencies: Circles will only acknowledge resumes shared by recruitment agencies if selected in our preferred supplier partnership program.\nPlease do not forward resumes to our jobs alias, Circles, employees or any other company location. Circles will not be held accountable for any fees related to unsolicited resumes not uploaded via our ATS.",Industry Type: Medical Devices & Equipment,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Interpersonal skills', 'telco', 'SAAS', 'Javascript', 'Customer service', 'Roaming', 'Open source', 'Recruitment', 'Python']",2025-06-14 06:12:59
Database Engineer,TechStar Group,8 - 13 years,25-37.5 Lacs P.A.,['Hyderabad'],"SQL & Database Management:\n\nDeep knowledge of relational databases (PostgreSQL), cloud-hosted data platforms (AWS, Azure, GCP), and data warehouses like Snowflake.\n\nETL/ELT Tools:\n\nExperience with SnapLogic, StreamSets, or DBT for building and maintaining data pipelines. / ETL Tools Extensive Experience on data Pipelines\n\nData Modeling & Optimization:\n\nStrong understanding of data modeling, OLAP systems, query optimization, and performance tuning.\n\nCloud & Security:\n\nFamiliarity with cloud platforms and SQL security techniques (e.g., data encryption, TDE).\nData Warehousing: Experience managing large datasets, data marts, and optimizing databases for performance.\n\nAgile & CI/CD: Knowledge of Agile methodologies and CI/CD automation tools.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Snowflake', 'DBT', 'Data Warehousing', 'ETL', 'SQL', 'Star Schema', 'Postgresql', 'Data Modeling']",2025-06-14 06:13:01
Cloud Architect,"""Exciting Opportunity with a Leading IT ...",10 - 13 years,8-17 Lacs P.A.,"['Hyderabad', 'Chennai', 'Bengaluru']","Detailed job description - Skill Set:\nLooking for 10+ Y / highly experienced and deeply hands-on Data Architect to lead the design, build, and optimization of our data platforms on AWS and Databricks. This role requires a strong blend of architectural vision and direct implementation expertise, ensuring scalable, secure, and performant data solutions from concept to production.\nStrong hand on exp in data engineering/architecture, hands-on architectural and implementation experience on AWS and Databricks, Schema modeling .\nAWS: Deep hands-on expertise with key AWS data services and infrastructure.\nDatabricks: Expert-level hands-on development with Databricks (Spark SQL, PySpark), Delta Lake, and Unity Catalog.\nCoding: Exceptional proficiency in Python , Pyspark , Spark , AWS Services and SQL.\nArchitectural: Strong data modeling and architectural design skills with a focus on practical implementation.\nPreferred: AWS/Databricks certifications, experience with streaming technologies, and other data tools.\nDesign & Build: Lead and personally execute the design, development, and deployment of complex data architectures and pipelines on AWS (S3, Glue, Lambda, Redshift, etc.) and Databricks (PySpark/Spark SQL, Delta Lake, Unity Catalog).\nDatabricks Expertise: Own the hands-on development, optimization, and performance tuning of Databricks jobs, clusters, and notebooks.\nMandatory Skills\nAWS, Databricks",Industry Type: IT Services & Consulting,Department: Other,"Employment Type: Full Time, Permanent","['Glue', 'Pyspark', 'Databricks', 'Spark', 'AWS', 'SQL', 'Python']",2025-06-14 06:13:03
Principal Engineer - Applications Development,Marsh McLennan,3 - 6 years,Not Disclosed,"['Pune', 'Mumbai (All Areas)']","Company:\nMarsh McLennan Agency\n\nDescription:\nMarsh McLennan is seeking candidates for the following position based in the Pune/Mumbai office.\n\nSenior Engineer/Principal Engineer\n\nWhat can you expect?\nWe are seeking a skilled Data Engineer with 3 to 5 years of hands-on experience in building and optimizing data pipelines and architectures. The ideal candidate will have expertise in Spark, AWS Glue, AWS S3, Python, complex SQL, and AWS EMR.\n\nWhat is in it for you?\nHolidays (As Per the location)\nMedical & Insurance benefits (As Per the location)\nShared Transport (Provided the address falls in service zone)\nHybrid way of working\nDiversify your experience and learn new skills\nOpportunity to work with stakeholders globally to learn and grow\n\nWe will count on you to:\nDesign and implement scalable data solutions that support our data-driven decision-making processes.\n\nWhat you need to have: \nSQL and RDBMS knowledge - 5/5.\nPostgres.\nShould have extensive hands-on Database systems carrying tables, schema, views, materialized views.\nAWS Knowledge.\nCore and Data engineering services.\nGlue/ Lambda/ EMR/ DMS/ S3 - services in focus.\nETL data:dge :-\nAny ETL tool preferably Informatica.\nData warehousing.\nBig data:-\nHadoop - Concepts.\nSpark - 3/5\nHive - 5/5\nPython/ Java.\nInterpersonal skills:-\nExcellent communication skills and Team lead capabilities.\nUnderstanding of data systems well in big organizations setup.\nPassion deep diving and working with data and delivering value out of it.\n\nWhat makes you stand out?\nDatabricks knowledge.\nAny Reporting tool experience.\nPreferred MicroStrategy.\n\nMarsh McLennan (NYSE: MMC) is the worlds leading professional services firm in the areas of risk, strategy and people. The Companys more than 85,000 colleagues advise clients in over 130 countries.  With annual revenue of $23 billion, Marsh McLennan helps clients navigate an increasingly dynamic and complex environment through four market-leading businesses. Marsh provides data-driven risk advisory services and insurance solutions to commercial and consumer clients. Guy Carpenter  develops advanced risk, reinsurance and capital strategies that help clients grow profitably and pursue emerging opportunities. Mercer  delivers advice and technology-driven solutions that help organizations redefine the world of work, reshape retirement and investment outcomes, and unlock health and well being for a changing workforce. Oliver Wyman serves as a critical strategic, economic and brand advisor to private sector and governmental clients. For more information, visit marshmclennan.com, or follow us on LinkedIn and X.\nMarsh McLennan is committed to embracing a diverse, inclusive and flexible work environment. We aim to attract and retain the best people regardless of their sex/gender, marital or parental status, ethnic origin, nationality, age, background, disability, sexual orientation, caste, gender identity or any other characteristic protected by applicable law.\nMarsh McLennan is committed to hybrid work, which includes the flexibility of working remotely and the collaboration, connections and professional development benefits of working together in the office. All Marsh McLennan colleagues are expected to be in their local office or working onsite with clients at least three days per week. Office-based teams will identify at least one anchor day per week on which their full team will be together in person.\nMarsh McLennan (NYSE: MMC) is a global leader in risk, strategy and people, advising clients in 130 countries across four businesses: Marsh, Guy Carpenter, Mercer and Oliver Wyman. With annual revenue of $24 billion and more than 90,000 colleagues, Marsh McLennan helps build the confidence to thrive through the power of perspective. For more information, visit marshmclennan.com, or follow on LinkedIn and X.\nMarsh McLennan is committed to embracing a diverse, inclusive and flexible work environment. We aim to attract and retain the best people and embrace diversity of age, background, caste, disability, ethnic origin, family duties, gender orientation or expression, gender reassignment, marital status, nationality, parental status, personal or social status, political affiliation, race, religion and beliefs, sex/gender, sexual orientation or expression, skin color, or any other characteristic protected by applicable law.\nMarsh McLennan is committed to hybrid work, which includes the flexibility of working remotely and the collaboration, connections and professional development benefits of working together in the office. All Marsh McLennan colleagues are expected to be in their local office or working onsite with clients at least three days per week. Office-based teams will identify at least one anchor day” per week on which their full team will be together in person.",Industry Type: Management Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Transformation', 'AWS', 'Python', 'SQL', 'Data Ingestion', 'Data Engineering', 'ETL']",2025-06-14 06:13:06
Software Engineer / Jetpac,Circles.Life,4 - 6 years,Not Disclosed,['Gurugram'],"About Us\nFounded in 2014, Circles is a global technology company reimagining the telco industry with its innovative SaaS platform, empowering telco operators worldwide to effortlessly launch innovative digital brands or refresh existing ones, accelerating their transformation into techcos.\nToday, Circles partners with leading telco operators across multiple countries and continents, including KDDI Corporation, Etisalat Group (e&), AT&T, and Telkomsel, creating blueprints for future telco and digital experiences enjoyed by millions of consumers globally.\nBesides its SaaS business, Circles operates three other distinct businesses:\nCircles.Life : A wholly-owned digital lifestyle telco brand based in Singapore, Circles.Life is powered by Circles SaaS platform and pioneering go-to-market strategies. It is the digital market leader in Singapore and has won numerous awards for marketing, customer service, and innovative product offerings beyond connectivity.\nCircles Aspire : A global provider of Communications Platform-as-a-Service (CPaaS) solutions. Its cloud-based Experience Cloud platform enables enterprises, service providers and developers to deliver and scale mobile, messaging, IoT, and connectivity services worldwide.\nJetpac : Specializing in travel tech solutions, Jetpac provides seamless eSIM roaming for over 200 destinations and innovative travel lifestyle products, redefining connectivity for digital travelers. Jetpac was awarded Travel eSIM of the Year.\nCircles is backed by renowned global investors, including Peak XV Partners (formerly Sequoia), Warburg Pincus, Founders Fund, and EDBI (the investment arm of the Singapore Economic Development Board), with a track record of backing industry challengers.\nJetpac is a revolutionary product launched Q4 2022 solving the ever dynamic problem of travel convenience. Starting from connectivity, it is now moving towards the holistic experience by including insurance coverage, concierge services as well as other AI-powered features.\nAs we re growing to become the biggest and most loved brand, we are looking for: Jetpac Software Engineer\nMission\nWork with the team to learn about Practical Software Development and deliver Solutions based upon Python, Node.js, GoLang and React.js.\nAs a key member of our team, your responsibilities include:\nWorking closely with engineers across the company to develop the best technical design and strategy\nOwning the delivery of various timelines, ensuring that key milestones are met and deliveries are of the highest quality\nAdapt to changing priorities and ambiguities which may arise from complex, cutting-edge and industry disrupting initiatives\nCollaborate with other non-technical stakeholders of the business which includes Product Managers, Designers, Marketing etc.\nWorking with an excellent team of talented engineers to solve the travel convenience problem\nWe will provide you..\nAn insight into inner working of a growing and continuously evolving organisation\nExcellent environment to learn and grow your development skills\nRegular feedback to improve both development and soft skills.\nRequirements:\nDegree / Masters in Computer Science, Software Engineering or equivalent\nStrong fundamentals on software development\nHaving actively worked on open source projects would be a plus\nExperience on at least any one of the following:\nGolang\nNode.js\nReact.js\nReact Native (Preferred)\nPython (optional)\nFamiliarity on RESTful web applications\nReact frontend development is a plus\nData engineering fundamentals\nPassionate about delivering quality work ahead of time\nExcellent interpersonal skills\nTeam player",Industry Type: Advertising & Marketing,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Interpersonal skills', 'telco', 'SAAS', 'Javascript', 'Customer service', 'Roaming', 'Open source', 'Recruitment', 'Python']",2025-06-14 06:13:08
Software Engineer / Jetpac/ Gurgaon,Circles.Life,4 - 5 years,Not Disclosed,['Gurugram'],"About Us\nFounded in 2014, Circles is a global technology company reimagining the telco industry with its innovative SaaS platform, empowering telco operators worldwide to effortlessly launch innovative digital brands or refresh existing ones, accelerating their transformation into techcos.\nToday, Circles partners with leading telco operators across multiple countries and continents, including KDDI Corporation, Etisalat Group (e), ATT, and Telkomsel, creating blueprints for future telco and digital experiences enjoyed by millions of consumers globally.\nBesides its SaaS business, Circles operates three other distinct businesses:\nCircles.Life : A wholly-owned digital lifestyle telco brand based in Singapore, Circles.Life is powered by Circles SaaS platform and pioneering go-to-market strategies. It is the digital market leader in Singapore and has won numerous awards for marketing, customer service, and innovative product offerings beyond connectivity.\nCircles Aspire : A global provider of Communications Platform-as-a-Service (CPaaS) solutions. Its cloud-based Experience Cloud platform enables enterprises, service providers and developers to deliver and scale mobile, messaging, IoT, and connectivity services worldwide.\nJetpac : Specializing in travel tech solutions, Jetpac provides seamless eSIM roaming for over 200 destinations and innovative travel lifestyle products, redefining connectivity for digital travelers. Jetpac was awarded Travel eSIM of the Year.\nCircles is backed by renowned global investors, including Peak XV Partners (formerly Sequoia), Warburg Pincus, Founders Fund, and EDBI (the investment arm of the Singapore Economic Development Board), with a track record of backing industry challengers.\nJetpac is a revolutionary product launched Q4 2022 solving the ever dynamic problem of travel convenience. Starting from connectivity, it is now moving towards the holistic experience by including insurance coverage, concierge services as well as other AI-powered features.\nAs we re growing to become the biggest and most loved brand, we are looking for: Jetpac Software Engineer\nMission\nWork with the team to learn about Practical Software Development and deliver Solutions based upon Python, Node.js, GoLang and React.js.\nAs a key member of our team, your responsibilities include:\nWorking closely with engineers across the company to develop the best technical design and strategy\nOwning the delivery of various timelines, ensuring that key milestones are met and deliveries are of the highest quality\nAdapt to changing priorities and ambiguities which may arise from complex, cutting-edge and industry disrupting initiatives\nCollaborate with other non-technical stakeholders of the business which includes Product Managers, Designers, Marketing etc.\nWorking with an excellent team of talented engineers to solve the travel convenience problem\nWe will provide you..\nAn insight into inner working of a growing and continuously evolving organisation\nExcellent environment to learn and grow your development skills\nRegular feedback to improve both development and soft skills.\nRequirements:\nDegree / Masters in Computer Science, Software Engineering or equivalent\nStrong fundamentals on software development\nHaving actively worked on open source projects would be a plus\nExperience on at least any one of the following:\nGolang\nNode.js\nReact.js\nReact Native (Preferred)\nPython (optional)\nFamiliarity on RESTful web applications\nReact frontend development is a plus\nData engineering fundamentals\nPassionate about delivering quality work ahead of time\nExcellent interpersonal skills\nTeam player\nCircles is committed to a diverse and inclusive workplace. We are an equal opportunity employer and do not discriminate on the basis of race, national origin, gender, disability or age.\nTo all recruitment agencies: Circles will only acknowledge resumes shared by recruitment agencies if selected in our preferred supplier partnership program.\nPlease do not forward resumes to our jobs alias, Circles, employees or any other company location. Circles will not be held accountable for any fees related to unsolicited resumes not uploaded via our ATS.",Industry Type: Advertising & Marketing,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Interpersonal skills', 'telco', 'SAAS', 'Javascript', 'Customer service', 'Roaming', 'Open source', 'Recruitment', 'Python']",2025-06-14 06:13:10
Software Engineer,Webmd Guru,3 - 8 years,Not Disclosed,['Mumbai'],"Description\nPosition at WebMD\nAbout Company:\nWebMD Health Corp., an Internet Brands Company, is the leading provider of health information services, servingpatients, physicians, health care professionals, employers, and health plans through our public and private onlineportals, mobile platforms, and health-focused publications. The WebMD Health Network includes WebMD Health, Medscape, Jobson Healthcare Information, prIME Oncology, MediQuality, Frontline, QxMD, Vitals Consumer Services, MedicineNet, eMedicineHealth, RxList, OnHealth, Medscape Education, and other owned WebMD sites. WebMD , Medscape , CME Circle , Medpulse , eMedicine , MedicineNet , theheart.org , and RxList are among the trademarks of WebMD Health Corp. or its subsidiaries.\n\nAll qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or veteran status.\n\nFor Company details, visit our website: www.webmd.com and www.nolo.com\nEducation: B.E. Computer Science/IT degree (or any other engineering discipline)\nExperience: 3+ years\nWork Timings: 2pm to 11pm\nPosition Requirement:\n2+ years of experience using Data Integration Tools - Pentaho or any other ETL / ELT tools\n2+ years of experience using traditional databases like Postgress, MSSQL, Oracle\n1+ years of experience using Columnar databases like Vertica, Google, BigQuery, Amazon Redshift\n1+ years of experience in Scheduler/ Orchestration Tools like Control-M, Autosys, Airflow, JAMS\nGood conceptual knowledge on ETL/ ELT strategies\nGood conceptual knowledge in any code Versioning Tools\nGood collaboration, communication and documentation skills\nExperience of working in Agile Delivery model\nRequires minimal or no direct supervision\nGood knowledge in Data Visualization Tools like Tableau, Pentaho BA tools\nDigital marketing/ Web Analytics or BI is a plus\nKnowledge of scripting languages such as Python is good to have\nExperience in Linux environment is preferred but not mandatory\nRole & Responsibilities:\nDevelop and support multiple Data Engineering projects with heterogenous data sources, produce/ consume data to/from messaging queues like Kafka, push/ pull data to/from REST APIs\nSupport in-house build Data integration Framework, Data Replication Framework, Data Profiling & Reconciliation Framework\nDevelop data Pipelines with good coding standards, unit testing with detailed test cases\nWillingness to learn new technologies.",Industry Type: Internet,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Linux', 'Web analytics', 'Coding', 'Reconciliation', 'Healthcare', 'Oracle', 'Digital marketing', 'Automotive', 'Software services']",2025-06-14 06:13:12
Cloud Architect - AWS + Databricks,MNC IT,12 - 14 years,14-18 Lacs P.A.,"['Hyderabad', 'Bengaluru']","Looking for 10+ Y / highly experienced and deeply hands-on Data Architect to lead the design, build, and optimization of our data platforms on AWS and Databricks. This role requires a strong blend of architectural vision and direct implementation expertise, ensuring scalable, secure, and performant data solutions from concept to production.\nStrong hand on exp in data engineering/architecture, hands-on architectural and implementation experience on AWS and Databricks, Schema modeling .\nAWS: Deep hands-on expertise with key AWS data services and infrastructure.\nDatabricks: Expert-level hands-on development with Databricks (Spark SQL, PySpark), Delta Lake, and Unity Catalog.\nCoding: Exceptional proficiency in Python , Pyspark , Spark , AWS Services and SQL.\nArchitectural: Strong data modeling and architectural design skills with a focus on practical implementation.\nPreferred: AWS/Databricks certifications, experience with streaming technologies, and other data tools.\nDesign & Build: Lead and personally execute the design, development, and deployment of complex data architectures and pipelines on AWS (S3, Glue, Lambda, Redshift, etc.) and Databricks (PySpark/Spark SQL, Delta Lake, Unity Catalog).\nDatabricks Expertise: Own the hands-on development, optimization, and performance tuning of Databricks jobs, clusters, and notebooks.",Industry Type: IT Services & Consulting,Department: Project & Program Management,"Employment Type: Full Time, Permanent","['Architect', 'databricks', 'AWS']",2025-06-14 06:13:14
Scala Developer,Forbes Global 2000 IT Services Firm,8 - 13 years,Not Disclosed,"['Bengaluru', 'Delhi / NCR', 'Mumbai (All Areas)']","Key Responsibilities:\nDevelop and maintain data processing applications using Spark and Scala.\nCollaborate with cross-functional teams to understand data requirements and design efficient solutions.\nImplement test-driven deployment practices to enhance the reliability of application.\nDeploy artifacts from lower to higher environment ensuring smooth transition\nTroubleshoot and debug Spark performance issues to ensure optimal data processing.\nWork in an agile environment, contributing to sprint planning, development and delivering high quality solutions on time\nProvide essential support for production batches, addressing issues and providing fix to meet critical business needs\n\nSkills/Competencies:\nStrong knowledge of Scala programming language\nExcellent problem-solving and analytical skills.\nProficiency in Spark, including the development and optimization of Spark applications.\nAbility to troubleshoot and debug performance issues in Spark.\nUnderstanding of design patterns and data structure for efficient data processing\nFamiliarity with database concepts and SQL\nUnderstanding of DevOps practices (Good to have).",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Temporary/Contractual","['Scala', 'Big Data', 'Spark']",2025-06-14 06:13:16
Rosemallow Technologies - Artificial Intelligence Engineer,Rosemallow Technologies,3 - 5 years,Not Disclosed,['Pune'],"Key Responsibilities : We are seeking a highly skilled and motivated AI Engineer to join our team.\n\nIn this role, you will leverage your expertise in AI technologies and the Microsoft ecosystem to design, build, and deploy intelligent agents and automation solutions that enhance business processes and deliver value to our clients.\n\nCandidates must have extensive experience in the Microsoft environment.\n\nYou will collaborate with cross-functional teams to create innovative solutions using Microsoft tools and platforms.\n\nResponsibilities :\n\nAgent Development :\n\n- Design, implement, and optimize AI agents using Microsoft Azure Framework and related technologies.\n\n- Develop custom AI solutions leveraging Power Automate, Azure OpenAI, and other Microsoft tools.\n\nSolution Integration :\n\n- Deploy AI solutions within client environments, ensuring scalability and seamless integration with existing systems.\n\n- Work with stakeholders to identify automation opportunities and tailor solutions to business needs.\n\nAI Algorithm and Model Implementation :\n\n- Design and implement machine learning algorithms, focusing on natural language processing (NLP) and conversational AI.\n\n- Perform data preprocessing, feature engineering, and model training to create high-performing solutions.\n\nCollaboration and Support :\n\n- Collaborate with cross-functional teams, including software engineers, data scientists, and product managers, to deliver integrated solutions.\n\n- Provide technical guidance and support to ensure the successful adoption and use of AI-driven tools.\n\nContinuous Improvement :\n\n- Stay updated on advancements in AI, machine learning, and Microsofts AI technologies.\n\n- Contribute to knowledge sharing by conducting training sessions and documenting best practices.\n\nPreferred Skills :\n\n- Strong knowledge of Azure OpenAI, Azure AI Search Index and open source libraries such as LangChain and LlamaIndex.\n\n- Proficiency in Python and its AI/ML libraries (e.g., TensorFlow, PyTorch, scikit-learn).\n\n- Familiarity with building and managing cloud-based solutions, preferably on Microsoft Azure.\n\n- Understanding of conversational AI technologies and chatbot frameworks.\n\n- Experience with data analysis tools and techniques to uncover insights and optimize models.\n\nRequirements :\n\n- Bachelors or Masters degree in Computer Science, Data Science, or a related field.\n\n- Proven experience in developing and deploying AI/ML models in real-world applications.\n\n- Strong programming skills, especially in Python, and familiarity with version control systems like Git.\n\n- Extensive experience in the Microsoft environment and related technologies.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Artificial Intelligence Engineer', 'Azure', 'NLP', 'Artificial Intelligence', 'LLama', 'LangChain', 'Power Automate', 'Conversational AI', 'Chatbot', 'OpenAI', 'Machine Learning', 'Python']",2025-06-14 06:13:19
Python Developer,Element Technologies,4 - 6 years,8-11 Lacs P.A.,['Bengaluru'],"Job Title: Python Developer MDM Integration (MongoDB & Neo4j)\nLocation: Bengaluru\nExperience Level: 4+\nJob Summary:\nWe are seeking a skilled Python Developer to work on our Master Data Management (MDM) initiative. The ideal candidate will have hands-on experience with MongoDB, Neo4j, and data integration from MySQL using Debezium and Kafka.\nKey Responsibilities:\nDesign and develop MDM solutions using MongoDB and Neo4j\nBuild data ingestion pipelines from MySQL via Debezium and Apache Kafka\nWrite clean, modular, and scalable Python code for data processing and transformation\nCollaborate with architects and data engineers to maintain data consistency across systems\nOptimize database queries and data models for performance and scalability\nSupport data governance, lineage, and traceability efforts\nRequired Skills & Qualifications:\n4+ years of professional experience in Python development\nStrong knowledge of MongoDB and NoSQL data modelling\nExperience with Neo4j and graph database concepts\nHands-on experience with Debezium and Kafka for real-time data streaming\nSolid understanding of MySQL and relational-to-NoSQL data mapping\nExperience working with REST APIs and microservices architecture\nGood problem-solving and communication skills\nPreferred Qualifications:\nExperience with data governance or MDM frameworks\nKnowledge of data lineage and data quality principles\nFamiliarity with Docker and containerized environments\nExposure to cloud platforms (e.g., AWS, Azure, or GCP)",Industry Type: Automobile,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Neo4J', 'Kafka', 'Python', 'Graph Databases', 'MongoDB']",2025-06-14 06:13:21
Azure Databricks Engineer,Tredence,4 - 9 years,Not Disclosed,"['Kolkata', 'Pune', 'Chennai']",Role & responsibilities:\nDeveloping Modern Data Warehouse solutions using Databricks and AWS/ Azure Stack\nAbility to provide solutions that are forward-thinking in data engineering and analytics space\nCollaborate with DW/BI leads to understand new ETL pipeline development requirements.\nTriage issues to find gaps in existing pipelines and fix the issues\nWork with business to understand the need in reporting layer and develop data model to fulfill reporting needs,,,,"['Azure Data Factory', 'Pyspark', 'Azure Databricks', 'SQL', 'Python', 'Data Bricks']",2025-06-14 06:13:23
Engineer III - Software Engineering (IN),World Courier,8 - 13 years,Not Disclosed,['Pune'],"Our team members are at the heart of everything we do. At Cencora, we are united in our responsibility to create healthier futures, and every person here is essential to us being able to deliver on that purpose. If you want to make a difference at the center of health, come join our innovative company and help us improve the lives of people and animals everywhere. Apply today!\nJob Details\nPrimary Duties & Responsibilities:\nDesigns, implements, unit tests, documents and deploys applications and APIs based on functional requirements\nDesigns and develops database applications using SQL Server Integration Services, TSQL and Stored Procedures\nConsults with the business to determine logical design for new business solutions according to existing data architecture\nPerforms code reviews, analyze execution plans, and re-factor inefficient code\nProvides technical support & guidance to Software Engineers I/II in development of desired software/applications\nFollows data standards, resolves data issues, completes unit testing and completes system documentation for ETL processes\nAssists Managers with project development, including documentation of features, recording of progress, and creation of the testing plan\nCollaborates with business analysts, source system experts, and other team members to determine data extraction and transformation requirements\nCollaborates with IT operations and testing organizations to ensure timely releases of projects and database environments are sustainable\nSupports IT staff and business users in the development, usage and troubleshooting of database-driven applications\nCreates detailed designs, performs analyses, creates prototypes, and documents completed design\nActs as a senior member of the team; represents the organization on project teams and may perform technical project leadership roles while fostering teamwork and collaboration within and across work groups\nIntermediates knowledge on client requests, features, best practices, project scopes, and budgets\nContributes to the growth of the company by advancing personal skills, working with the development team to improve the codebase, and seeking opportunities to improve the company s tools and processes\nDesigns, develops, and automates scalable data engineering solutions by leveraging cloud infrastructure\nExtends or migrate existing data pipelines to new cloud environment\nCrafts data models of program processes and data entities and develops technical design & documentation of solutions\nParticipates in optimization of data asset performance\nTransfers knowledge of data access/consumption mechanisms to business stakeholders, data visualization specialists and/or data scientists\nDevelops solutions and recommendations for improving data integrity issues\n\nWhat your background should look like (minimum qualifications)\nBachelors degree or related experience and 8+ years of development experience. Requires some training in fields such as business administration, accountancy, sales, marketing, computer sciences, or similar vocations, generally obtained through completion of a four-year bachelor s degree program, technical vocational training, or equivalent combination of experience and education.\n\n1. 8+ years of professional experience in designing, developing, and maintaining Java applications, with a strong understanding of core Java concepts, data structures, and algorithms.\n2. 5+ years of React or Angular, Typescript, JavaScript, Node.js, HTML 5, CSS, and Bootstrap preferred.\n3. Proven experience in designing, developing, and deploying scalable and resilient microservices-based applications.\n4. In-depth knowledge and hands-on experience with either Spring Boot or Quarkus for building enterprise-level, cloud-native Java applications and RESTful APIs.\n5. Solid understanding of RESTful API design principles and experience in building and consuming RESTful web services. Familiarity with tools like Swagger/OpenAPI is a plus.\n6. Experience working with SQL (e.g.,MySQL, Oracle) databases, including data modeling and query optimization.\n7. Proficiency with Git for version control and experience with CI/CD pipelines and tools (e.g., Jenkins, Azure Repos, Azure Pipelines).\n8. Strong understanding of software testing principles and experience with unit testing (e.g., JUnit, Mockito), integration testing.\n9. Familiarity with cloud platforms (e.g., AWS, Azure) and containerization technologies like Docker and orchestration tools like Kubernetes is desirable.\n10. Excellent analytical and problem-solving abilities, with strong communication and collaboration skills to work effectively within an agile team environment.\n11. Good interpersonal skills\n12. Ability to prioritize workload and consistently meet deadlines.\n13. Ability to use good judgment in conveying project status and problem escalation.\n.\nExperience & Educational Requirements:\nBachelor s Degree in Computer Science, Information Technology or any other related discipline or equivalent related experience. 4+ years of directly-related or relevant experience, preferably in software designing and development.\n\nPreferred Certifications:\nAndroid Development Certification\nMicrosoft Asp.Net Certification\nMicrosoft Certified Engineer\nApplication / Infrastructure / Enterprise Architect Training and Certification, e.g. TOGAF\nCertified Scrum Master\nSAFe Agile Certification\nDevOps Certifications like AWS Certified DevOps Engineer\n\nSkills & Knowledge:\nBehavioral Skills:\nCritical Thinking\nDetail Oriented\nImpact and Influencing\nInterpersonal Communication\nMultitasking\nProblem Solving\nTime Management\n\nTechnical Skills:\nAPI Design\nCloud Computing Methodologies\nIntegration Testing & Validation\nProgramming/Coding\nDatabase Management\nSoftware Development Life Cycle (SDLC)\nTechnical Documentation\nWeb Application Infrastructure\nWeb Development Frameworks\n\nTools Knowledge:\nCloud Computing Tools like AWS, Azure, Google cloud\nContainer Management and Orchestration Tools\nBig Data Frameworks like Hadoop\nJava Frameworks like JDBC, Spring, ORM Solutions, JPA, JEE, JMS, Gradle, Object Oriented Design\nMicrosoft Office Suite\nNoSQL Database Platforms like MongoDB, BigTable, Redis, RavenDB Cassandra, HBase, Neo4j, and CouchDB\nProgramming Languages like JavaScript, HTML/CSS, Python, SQL\nOperating Systems & Servers like Windows, Linux, Citrix, IBM, Oracle, SQL\nWhat Cencora offers\nBenefit offerings outside the US may vary by country and will be aligned to local market practice. The eligibility and effective date may differ for some benefits and for team members covered under collective bargaining agreements.\nFull time\nAffiliated Companies Affiliated Companies: CENCORA BUSINESS SERVICES INDIA PRIVATE LIMITED\nEqual Employment Opportunity\nCencora is committed to providing equal employment opportunity without regard to race, color, religion, sex, sexual orientation, gender identity, genetic information, national origin, age, disability, veteran status or membership in any other class protected by federal, state or local law.\nCencora is committed to providing reasonable accommodations to individuals with disabilities during the employment process which are consistent with legal requirements. If you wish to request an accommodation while seeking employment, please call 888.692.2272 or email hrsc@cencora.com . We will make accommodation determinations on a request-by-request basis. Messages and emails regarding anything other than accommodations requests will not be returned",Industry Type: Medical Devices & Equipment,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['JMS', 'Core Java', 'Linux', 'MySQL', 'JDBC', 'HTML', 'Windows', 'Troubleshooting', 'Technical support', 'Python']",2025-06-14 06:13:26
Test Engineer,Blend360 India,4 - 8 years,Not Disclosed,['Hyderabad'],"We are looking for a skilled Test Engineer with experience in automated testing, rollback testing, and continuous integration environments. You will be responsible for ensuring the quality and reliability of our software products through automated testing strategies and robust test frameworks\nDesign and execute end-to-end test strategies for data pipelines, ETL/ELT jobs, and database systems.\nValidate data quality, completeness, transformation logic, and integrity across distributed data systems (e.g., Hadoop, Spark, Hive).\nDevelop Python-based automated test scripts to validate data flows, schema validations, and business rules.\nPerform complex SQL queries to verify large datasets across staging and production environments.\nIdentify data issues and work closely with data engineers to resolve discrepancies.\nContribute to test data management, environment setup, and regression testing processes.\nWork collaboratively with data engineers, business analysts, and QA leads to ensure accurate and timely data delivery.\nParticipate in sprint planning, reviews, and defect triaging as part of the Agile process.\n\n\n4+ of experience in Data Testing, Big Data Testing, and/or Database Testing.\nStrong programming skills in Python for automation and scripting.\nExpertise in SQL for writing complex queries a",Industry Type: Industrial Automation,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Automation', 'Version control', 'Data management', 'Test scripts', 'Agile', 'Regression testing', 'Data quality', 'JIRA', 'SQL', 'Python']",2025-06-14 06:13:28
Manager- SAP BW4HANA + Azure Developer,Sun Pharma,5 - 7 years,Not Disclosed,['Mumbai'],"Hi,\nWe are having an opening for Manager- SAP BW4HANA + Azure Developer at our Mumbai location.\n\n\nJob Summary :\nWe are searching for skilled SAP BW / BW4HANA Developer and Data engineer to join our dynamic team. The role involves working closely with business stakeholders to understand business requirement and translating them into technical specifications and ensure successful deployment.",,,,"['Azure Cloud', 'SAP data Warehouse', 'Azure developer', 'Bw 4Hana', 'SAP BW4HANA', 'ETL']",2025-06-14 06:13:30
Product / Project Manager (Immediate joining preferred),Delta Hr Consultancy,3 - 7 years,15-22.5 Lacs P.A.,['Pune'],"We are seeking to hire an individual to play the dual role of Product and Project Manager. This individual should be experienced in managing enterprise software implementation projects, building requirements and have strong learning skills and exceptional analytical skills.\n\nYou will be playing a critical role in project implementation right from requirements gathering, design & specification, development & testing, deployment and post implementation and BAU/application enhancement/defect maintenance. You will work hand in hand with client business, project, tech teams and internal team of engineers, data scientists and business teams to build solutions that address unique challenges related to unstructured data, knowledge representation and data visualization. You will have the opportunity to lead high impact, technically challenging projects that will influence the technical strategy.\n\nResponsibilities:\n\nOwn and manage the project plan planning, managing and tracking tasks, ensure timely and quality delivery within scope and budget.\nLiaising with all stakeholders to understand business objectives, user requirements, scope, deliverables, sign offs, etc\nInvolved in a combination of Business Analyst activities such as requirement gathering, analyzing process as is & to be mapping, preparing BRDs, wireframe prototypes, process models. flowcharts etc.\nUser story writing with well-defined acceptance criteria\nAssigning, managing, coordinating and monitoring work in completing project tasks during each SDLC phase to ensure quality delivery within approved project scope, time and budget\nMaintaining RAID Log\nTrack project/product deliverables using appropriate tools and conducting regular review\nReporting regular status updates with all stakeholders highlighting track, progress, timelines, risks, mitigations, etc.\nCommunicating with tech team the requirements for development & testing and ensure deliverables adhere to the requirements\nAnalyzing scope creep and project change requests\nClient communication and demos\nQuery handling and issue resolution\nEnsure all required documentations and sign off are maintained\nProject walkthroughs: Design Walkthrough, functional test plans, etc.\n\nQualifications:\n\nTech Skills:\nPost-Graduation degree in Finance or related field\nExperience in project management of software product implementation\nShould have hands-on experience with project management tools - Jira, Atlas, and Confluence\nStrong business and IT SDLC fundamentals\nKnowledge in Agile methodologies\nA PMP or Agile certification is preferred\nFinance background is preferred\n\nSoft Skills:\nStrong verbal and written communication skills with the ability to work well in a team\nExperience in working directly with customers\nExperience working in an agile software development organization\nStrong analytical skills\nStrong customer focus, ownership, urgency and drive\nAbility to work with cross functional teams\nAbility to handle multiple, competing priorities in a fast-paced environment\nAbility to inform about progress via accurate reporting.\nExcited about technology, and have a strong interest in learning about and building compelling web AI applications\n\nEducation Qualifications - MBA or equivalent\nEmployment Type - Full Time\nLocation – Pune, Maharashtra",Industry Type: Emerging Technologies (AI/ML),Department: Project & Program Management,"Employment Type: Full Time, Permanent","['Product Management', 'Project Management', 'Requirement Gathering', 'Agile', 'Project Delivery', 'JIRA', 'Project Planning', 'SDLC', 'Stakeholder Management', 'Project Life Cycle']",2025-06-14 06:13:32
Hiring For AI Developer,Acceltree Softwares,2 - 6 years,Not Disclosed,['Pune'],"We are seeking a Data Scientist with 2 to 5 years of experience to join our AI/ML team. The ideal candidate will work under the guidance of a team lead to develop and implement machine learning solutions, with a focus on Natural Language Processing (NLP) and generative AI applications.\n\nKey Responsibilities:\nDevelop and maintain NLP models and chatbot solutions under the direction of the team lead\nImplement machine learning algorithms and deep learning models using Python\nProcess, clean, and validate data to ensure high-quality model inputs\nCreate and optimize generative AI solutions for various business applications\nCollaborate with team members to improve existing models and develop new solutions\nDocument technical processes and maintain codebase\nParticipate in code reviews and team discussions\nDebug and troubleshoot models to improve performance\nMonitor model performance and implement necessary updates\nPrepare and briefing for client presentation\nCandidate Specification:\nBachelor's degree in Computer Science, Data Science, or related field\n2+ years of professional experience in data science or machine learning\nExperience with large language models and transformer architectures\nFamiliarity with chatbot development using RASA (preferable) or other platform\nKnowledge of MLOps practices and tools\nFamiliarity with cloud platforms (AWS/Azure/GCP)\nKnowledge on REST APIs\nBackground in implementing production-grade ML solutions\nStrong proficiency in Python programming\nHands-on experience with NLP libraries and frameworks (e.g., NLTK, spaCy, Transformers)\nExperience with machine learning frameworks (e.g., TensorFlow, PyTorch, scikit-learn)\nKnowledge of chatbot development and conversational AI\nUnderstanding of generative AI concepts and applications\nFamiliarity with version control systems (e.g., Git)\nStrong analytical and problem-solving skills\nTechnical Skills:\nProgramming Languages:Python (required)\nML/AI Frameworks: TensorFlow, PyTorch, scikit-learn\nNLP Tools: NLTK, spaCy,Hugging Face Transformers\nVersion Control: Git\nData Processing: Pandas, NumPy\nDevelopment Tools: VS Code\nSoft Skills:\nExcellent oral and written communication skills\nGood at client interaction and client communication\nPresentation skill\nGood comprehension and articulator\nStrong team player with ability to take direction from lead\nDetail-oriented approach to problem-solving\nAbility to work in a fast-paced environment\nGood time management skills\nEagerness to learn and adapt to new technologies\nPerks and Benefits:\n5 days working\nGood work environment\nGreat learning opportunity\nWork life balance",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Generative AI', 'Chatbot', 'Machine Learning', 'Rasa', 'Python', 'Tensorflow', 'Artificial Intelligence', 'Natural Language Processing', 'Scikit-Learn', 'Deep Learning', 'Pytorch', 'Presentation Skills', 'Client Communication', 'GIT', 'Client Presentations', 'AWS']",2025-06-14 06:13:34
Python Developer,Automationedge,2 - 7 years,Not Disclosed,['Pune'],"We are looking for a passionate and skilled Python Developer with 1 2 years of experience to join our team. This role involves designing and developing scalable backend systems with integration of Large Language Models (LLMs). The ideal candidate will have hands-on experience with modern Python frameworks and a solid understanding of backend architecture and APIs.\nKey Responsibilities\nDesign, develop, test, and maintain backend services using Python.\nBuild and optimize RESTful APIs.\nWork on integrating and utilizing LLMs within backend services.\nCollaborate with frontend developers, data scientists, and DevOps teams to deliver robust solutions.\nWrite clean, maintainable, and efficient code.\nTroubleshoot and debug applications and production issues.\nStay updated with the latest trends in AI and backend technologies.\nRequired Skills\n1 2 years of professional experience in Python development.\nProficiency in at least one Python web framework: Django, Flask, or FastAPI.\nStrong understanding of REST APIs and backend development best practices.\nFamiliarity with SQL databases (e.g., PostgreSQL, MySQL).\nExperience working with Large Language Models (LLMs) and integrating them into backend services.\nNice to Have (Preferred)\nExposure to Agentic frameworks (e.g., LangChain, AutoGen, CrewAI, or similar).\nKnowledge of NoSQL databases or message queues.\nExperience working in Agile/Scrum teams.\nOpportunity to work on cutting-edge technologies including Generative AI and LLMs.\nA collaborative and innovative work environment.\nGrowth and learning opportunities across domains.\nApply for this position Apply for this position Allowed Type(s): .pdf, .doc, .docx By using this form you agree with the storage and handling of your data by this website. *",Industry Type: Film / Music / Entertainment,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Backend', 'NoSQL', 'Web technologies', 'Agile scrum', 'Postgresql', 'Django', 'MySQL', 'SQL', 'Python']",2025-06-14 06:13:37
ML Engineer - Radar,IT Bridge Consultants,2 - 6 years,18-30 Lacs P.A.,"['Kochi', 'Coimbatore', 'Thiruvananthapuram']","Radar Machine Learning Engineer with 3-5 years of experience in machine learning, systems engineering , Sensor development & digital signal processing. M. Tech / Ph. D in Computer Science or Electrical or Computer Engineering preferred.\n\nRequired Candidate profile\nResearch, design, develop radar sensor prototypes, Knowledge about microwave circuits and antenna, Prototype & evaluate algorithms with MATLAB/ Python. Hands-on coding experience in C or C++.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Radar Signal Processing', 'DSP', 'Mathlab', 'Python', 'Ml', 'C++', 'C', 'Algorithms']",2025-06-14 06:13:39
Immediate Hiring - IT & NON-IT Recruitment Specialist,.,2 - 6 years,3-5 Lacs P.A.,['Noida'],"IT Recruitment Specialist Job Description\n\nAn IT Recruitment Specialist is responsible for sourcing, screening, and hiring IT& Non - IT professionals For Product & Service based clients to meet the organization's technical hiring needs. This role involves collaborating with hiring managers to understand job requirements, identifying top talent through various channels, conducting interviews, and managing the candidate experience throughout the recruitment process. The ideal candidate will have a strong understanding of the tech industry and experience recruiting for technical roles like developers, system administrators, and data scientists.\n\nKey Responsibilities:\n\nSource and attract candidates through job boards, social media, and networking.\nScreen resumes, conduct interviews, and assess technical skills.\nCollaborate with hiring managers to define role requirements and candidate profiles.\nManage candidate communications and offer negotiations.\nTrack recruitment metrics and continuously improve processes.\n\nQualifications:\n\nExperience recruiting for IT & Non - IT roles.\nStrong understanding of the tech industry and technical skills.\nExcellent communication and organizational skills.\nFamiliarity with ATS and recruitment tools.",Industry Type: Recruitment / Staffing,Department: Human Resources,"Employment Type: Full Time, Permanent","['IT Recruitment', 'Non IT Recruitment', 'Product Hiring', 'End To End Recruitment', 'Technical Recruitment']",2025-06-14 06:13:41
Principal Talent acquisition Partner,Avalara Technologies,15 - 20 years,Not Disclosed,['Pune'],"What You'll Do\n\nAs a Principal Talent Acquisition Partner , you will help build world-class technology and business teams. You will be the trusted advisor to hiring managers, guiding end-to-end hiring strategies, across India and global markets.\nThe role will report to Director TA\nJob Location: Remote\n\nWhat Your Responsibilities Will Be\n\nWhat You'll Need to be Successful",,,,"['Talent acquisition', 'cloud', 'ATS platforms', 'Generative AI', 'LinkedIn Recruiter', 'SaaS', 'product security']",2025-06-14 06:13:43
Power BI Developer,Vayuz Technologies,12 - 15 years,Not Disclosed,['Visakhapatnam'],"Location : Remote.\n\nExperience : 12+ years.\n\nRole Expectations :\n\n- Design, develop, and maintain interactive dashboards and reports using Power BI.\n\n- Create and optimize data models, including star/snowflake schemas.\n\n- Develop complex DAX calculations and KPIs to drive business insights.\n\n- Integrate data from various sources (SQL Server, Excel, SharePoint, APIs, etc.)\n\n- Collaborate with business stakeholders to gather requirements and translate them into technical solutions.\n\n- Perform data validation and troubleshooting to ensure accuracy and performance.\n\n- Implement row-level security and user-based data access strategies.\n\n- Provide guidance on Power BI governance, best practices, and self-service analytics models.\n\n- Maintain data refresh schedules and monitor report performance.\n\n- Work with cross-functional teams including data engineers, analysts, and business users.\n\nQualifications :\n\n- 12+ years of hands-on Power BI development.\n\n- Advanced DAX and data modeling expertise.\n\n- Experience with Power Query/M Language.\n\n- Strong dashboard and visualization design skills.\n\n- Power BI Service and Power BI Report Server knowledge.\n\n- Row-level security and gateway configurations.\n\n- Familiarity with Power BI REST API and embedding techniques.\n\n- Proficient in writing complex T-SQL queries, stored procedures, views, and functions.\n\n- Data extraction, transformation, and integration using SQL.\n\n- Experience with large datasets and performance tuning.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Power BI', 'Data Validation', 'T-SQL', 'BI Reporting', 'DAX Power BI', 'Dashboard Design', 'Data Visualization', 'Data Modeling', 'Power Query M']",2025-06-14 06:13:45
Power BI Developer,Vayuz Technologies,12 - 15 years,Not Disclosed,['Kanpur'],"Location : Remote.\n\nExperience : 12+ years.\n\nRole Expectations :\n\n- Design, develop, and maintain interactive dashboards and reports using Power BI.\n\n- Create and optimize data models, including star/snowflake schemas.\n\n- Develop complex DAX calculations and KPIs to drive business insights.\n\n- Integrate data from various sources (SQL Server, Excel, SharePoint, APIs, etc.)\n\n- Collaborate with business stakeholders to gather requirements and translate them into technical solutions.\n\n- Perform data validation and troubleshooting to ensure accuracy and performance.\n\n- Implement row-level security and user-based data access strategies.\n\n- Provide guidance on Power BI governance, best practices, and self-service analytics models.\n\n- Maintain data refresh schedules and monitor report performance.\n\n- Work with cross-functional teams including data engineers, analysts, and business users.\n\nQualifications :\n\n- 12+ years of hands-on Power BI development.\n\n- Advanced DAX and data modeling expertise.\n\n- Experience with Power Query/M Language.\n\n- Strong dashboard and visualization design skills.\n\n- Power BI Service and Power BI Report Server knowledge.\n\n- Row-level security and gateway configurations.\n\n- Familiarity with Power BI REST API and embedding techniques.\n\n- Proficient in writing complex T-SQL queries, stored procedures, views, and functions.\n\n- Data extraction, transformation, and integration using SQL.\n\n- Experience with large datasets and performance tuning.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Power BI', 'Data Validation', 'T-SQL', 'BI Reporting', 'DAX Power BI', 'Dashboard Design', 'Data Visualization', 'Data Modeling', 'Power Query M']",2025-06-14 06:13:47
Talent Acquisition Specialist,Mount Talent Consulting pvt. ltd.,1 - 4 years,1-3.75 Lacs P.A.,['Noida'],"Job Title: IT & Non-IT Recruiter\nLocation: Noida Sec 16\nCompany: Mount talent Consultancy\n\nJob Description:\nWe are seeking a versatile IT & Non-IT Recruiter to support our recruitment efforts in both the tech and non-tech sectors. In this hybrid role, you will work on sourcing and hiring top talent for both IT (software developers, system engineers, cybersecurity experts) and Non-IT roles (sales, marketing, HR, finance, operations).\n\nKey Responsibilities:\nIT Recruiting:\nSource, screen, and recruit qualified candidates for various technical roles, including software developers, network engineers, data scientists, and IT support staff.\nCollaborate with hiring managers to understand the technical requirements for each role and create tailored sourcing strategies.\nConduct interviews to assess technical skills and cultural fit for IT positions.\n\nNon-IT Recruiting:\nManage recruitment for non-technical roles such as marketing, sales, HR, finance, operations, and customer support.\nCollaborate with department heads to understand hiring needs and develop effective sourcing plans.\nSource candidates through various channels, including job boards, LinkedIn, referrals, and networking events.\n\nInterested candidate share your cv on himanshi@mounttalent.com or 8470009971",Industry Type: Recruitment / Staffing,Department: Human Resources,"Employment Type: Full Time, Permanent","['Recruiter', 'Screening', 'IT Recruitment', 'Non IT Recruitment', 'Hiring', 'Sourcing', 'Head Hunting', 'Recruitment', 'Talent Acquisition', 'Recruitment Consulting']",2025-06-14 06:13:49
Power BI Developer,Vayuz Technologies,12 - 15 years,Not Disclosed,['Ludhiana'],"Location : Remote.\n\nExperience : 12+ years.\n\nRole Expectations :\n\n- Design, develop, and maintain interactive dashboards and reports using Power BI.\n\n- Create and optimize data models, including star/snowflake schemas.\n\n- Develop complex DAX calculations and KPIs to drive business insights.\n\n- Integrate data from various sources (SQL Server, Excel, SharePoint, APIs, etc.)\n\n- Collaborate with business stakeholders to gather requirements and translate them into technical solutions.\n\n- Perform data validation and troubleshooting to ensure accuracy and performance.\n\n- Implement row-level security and user-based data access strategies.\n\n- Provide guidance on Power BI governance, best practices, and self-service analytics models.\n\n- Maintain data refresh schedules and monitor report performance.\n\n- Work with cross-functional teams including data engineers, analysts, and business users.\n\nQualifications :\n\n- 12+ years of hands-on Power BI development.\n\n- Advanced DAX and data modeling expertise.\n\n- Experience with Power Query/M Language.\n\n- Strong dashboard and visualization design skills.\n\n- Power BI Service and Power BI Report Server knowledge.\n\n- Row-level security and gateway configurations.\n\n- Familiarity with Power BI REST API and embedding techniques.\n\n- Proficient in writing complex T-SQL queries, stored procedures, views, and functions.\n\n- Data extraction, transformation, and integration using SQL.\n\n- Experience with large datasets and performance tuning.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Power BI', 'Data Validation', 'T-SQL', 'BI Reporting', 'DAX Power BI', 'Dashboard Design', 'Data Visualization', 'Data Modeling', 'Power Query M']",2025-06-14 06:13:51
Software Development Manager - Machine Learning,ClearDemand,10 - 15 years,Not Disclosed,['Chennai'],"**Key Responsibilities**\nPeople management - Lead a team of software engineers, DS, DE, MLE, in the design, development, and delivery of software solutions.\nProgram management - Strong program leader that has run program management functions to efficiently deliver ML projects to production and manage its operations.\nWork with Business stakeholders & customers in the Retail Business domain to execute the product vision using the power of AI/ML.\nScope out the business requirements by performing necessary data-driven statistical analysis.\nSet goals and, objectives using proper business metrics and constraints.\nConduct exploratory analysis on large volumes of data, understand the statistical shape, and use the right visuals to drive & present the analysis.\nAnalyse and extract relevant information from large amounts of data and derive useful insights on a big-data scale.\nCreate labelling manuals and work with labellers to manage ground truth data and perform feature engineering as needed.\nWork with software engineering teams, data engineers and ML operations team (Data Labellers, Auditors) to deliver production systems with your deep learning models.\nSelect the right model, train, validate, test, optimise neural net models and keep improving our image and text processing models.\nArchitecturally optimize the deep learning models for efficient inference, reduce latency, improve throughput, reduce memory footprint without sacrificing model accuracy.\nEstablish scalable, efficient, automated processes for large scale data analyses, model development, model validation and model implementation.\nCreate and enhance model monitoring system that could measure data distribution shifts, alert when model performance degrades in production.\nStreamline ML operations by envisioning human in the loop kind of workflows, collect necessary labels/audit information from these workflows/processes, that can feed into improved training and algorithm development process.\nMaintain multiple versions of the model and ensure the controlled release of models.\nManage and mentor junior data scientists, providing guidance on best practices in data science methodologies and project execution.\nLead cross-functional teams in the delivery of data-driven projects, ensuring alignment with business goals and timelines.\nCollaborate with stakeholders to define project objectives, deliverables, and timelines.\n**Skills required: **\nMS/PhD from reputed institution with a delivery focus.\n5+ years of experience in data science, with a proven track record of delivering impactful data-driven solutions.\nDelivered AI/ML products/features to production.\nSeen the complete cycle from Scoping & analysis, Data Ops, Modelling, MLOps, Post deployment analysis.\nExperts in Supervised and Semi-Supervised learning techniques. Hands-on in ML Frameworks - Pytorch or TensorFlow.\nHands-on in Deep learning models. Developed and fine-tuned Transformer based models. (Input output metric, Sampling technique)\nDeep understanding of Transformers, GNN models and its related math & internals.\nExhibit high coding standards and create production quality code with maximum efficiency.\nHands-on in Data analysis & Data engineering skills involving Sqls, PySpark etc.\nExposure to ML & Data services on the cloud AWS, Azure, GCP Understanding internals of compute hardware - CPU, GPU, TPU is a plus.\nCan leverage the power of hardware accel to optimize the model execution —PyTorch Glow, cuDNN, is a plus.",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['ML operations', 'Data Science', 'Artificial Intelligence', 'Statistical Modeling', 'Deep Learning', 'Predictive Analytics']",2025-06-14 06:13:54
Power BI Developer,Vayuz Technologies,12 - 15 years,Not Disclosed,['Lucknow'],"Location : Remote.\n\nExperience : 12+ years.\n\nRole Expectations :\n\n- Design, develop, and maintain interactive dashboards and reports using Power BI.\n\n- Create and optimize data models, including star/snowflake schemas.\n\n- Develop complex DAX calculations and KPIs to drive business insights.\n\n- Integrate data from various sources (SQL Server, Excel, SharePoint, APIs, etc.)\n\n- Collaborate with business stakeholders to gather requirements and translate them into technical solutions.\n\n- Perform data validation and troubleshooting to ensure accuracy and performance.\n\n- Implement row-level security and user-based data access strategies.\n\n- Provide guidance on Power BI governance, best practices, and self-service analytics models.\n\n- Maintain data refresh schedules and monitor report performance.\n\n- Work with cross-functional teams including data engineers, analysts, and business users.\n\nQualifications :\n\n- 12+ years of hands-on Power BI development.\n\n- Advanced DAX and data modeling expertise.\n\n- Experience with Power Query/M Language.\n\n- Strong dashboard and visualization design skills.\n\n- Power BI Service and Power BI Report Server knowledge.\n\n- Row-level security and gateway configurations.\n\n- Familiarity with Power BI REST API and embedding techniques.\n\n- Proficient in writing complex T-SQL queries, stored procedures, views, and functions.\n\n- Data extraction, transformation, and integration using SQL.\n\n- Experience with large datasets and performance tuning.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Power BI', 'Data Validation', 'T-SQL', 'BI Reporting', 'DAX Power BI', 'Dashboard Design', 'Data Visualization', 'Data Modeling', 'Power Query M']",2025-06-14 06:13:56
Power BI Developer,Vayuz Technologies,12 - 15 years,Not Disclosed,['Chennai'],"Location : Remote.\n\nExperience : 12+ years.\n\nRole Expectations :\n\n- Design, develop, and maintain interactive dashboards and reports using Power BI.\n\n- Create and optimize data models, including star/snowflake schemas.\n\n- Develop complex DAX calculations and KPIs to drive business insights.\n\n- Integrate data from various sources (SQL Server, Excel, SharePoint, APIs, etc.)\n\n- Collaborate with business stakeholders to gather requirements and translate them into technical solutions.\n\n- Perform data validation and troubleshooting to ensure accuracy and performance.\n\n- Implement row-level security and user-based data access strategies.\n\n- Provide guidance on Power BI governance, best practices, and self-service analytics models.\n\n- Maintain data refresh schedules and monitor report performance.\n\n- Work with cross-functional teams including data engineers, analysts, and business users.\n\nQualifications :\n\n- 12+ years of hands-on Power BI development.\n\n- Advanced DAX and data modeling expertise.\n\n- Experience with Power Query/M Language.\n\n- Strong dashboard and visualization design skills.\n\n- Power BI Service and Power BI Report Server knowledge.\n\n- Row-level security and gateway configurations.\n\n- Familiarity with Power BI REST API and embedding techniques.\n\n- Proficient in writing complex T-SQL queries, stored procedures, views, and functions.\n\n- Data extraction, transformation, and integration using SQL.\n\n- Experience with large datasets and performance tuning.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Power BI', 'Data Validation', 'T-SQL', 'BI Reporting', 'DAX Power BI', 'Dashboard Design', 'Data Visualization', 'Data Modeling', 'Power Query M']",2025-06-14 06:13:58
Google BigQuery,MNCS,3 - 6 years,Not Disclosed,"['Noida', 'Mumbai (All Areas)']","Project Role : Data Engineer\nProject Role Description : Design, develop and maintain data solutions for data generation, collection, and processing. Create data pipelines, ensure data quality, and implement ETL (extract, transform and load) processes to migrate and deploy data across systems.\nMust have skills : Google BigQuery\nMinimum 3 year(s) of experience is required\nEducational Qualification : 15 years fulltime education\nSummary:\nAs a Data Engineer, you will be responsible for designing, developing, and maintaining data solutions for data generation, collection, and processing. Your typical day will involve creating data pipelines, ensuring data quality, and implementing ETL processes to migrate and deploy data across systems using Google BigQuery.\n\nRoles & Responsibilities:\n- Design, develop, and maintain data solutions for data generation, collection, and processing using Google BigQuery.\n- Create data pipelines, ensuring data quality, and implementing ETL processes to migrate and deploy data across systems.\n- Collaborate with cross-functional teams to understand data requirements and design solutions that meet business needs.\n- Develop and maintain data models, data dictionaries, and other documentation to ensure data accuracy and consistency.\n- Optimize data storage and retrieval processes to ensure efficient and effective use of resources.\n\nProfessional & Technical Skills:\n- Must To Have Skills: Experience with Google BigQuery.\n- Good To Have Skills: Experience with ETL tools such as Apache NiFi or Talend.\n- Strong understanding of data modeling and database design principles.\n- Experience with SQL and NoSQL databases.\n- Experience with data warehousing and data integration technologies.\n- Familiarity with cloud computing platforms such as AWS or Google Cloud Platform.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Google BigQuery', 'ETL', 'Apache NiFi']",2025-06-14 06:14:00
Microsoft Azure Analytics Services - Coimbatore,Fortune Global 500 IT Services Firm,5 - 10 years,Not Disclosed,['Coimbatore'],"Project Role : Application Lead\nProject Role Description : Lead the effort to design, build and configure applications, acting as the primary point of contact.\nMust have skills : Microsoft Azure Analytics Services\nGood to have skills : NA\nMinimum 5 year(s) of experience is required\nEducational Qualification : BEBTECHMTECH\nSummary:\nAs an Application Lead, you will be responsible for leading the effort to design, build and configure applications, acting as the primary point of contact. Your typical day will involve working with Microsoft Azure Analytics Services and collaborating with cross-functional teams to deliver high-quality solutions.\n\nRoles & Responsibilities:\n- Lead the design, development, and deployment of applications using Microsoft Azure Analytics Services.\n- Collaborate with cross-functional teams to ensure the timely delivery of high-quality solutions.\n- Act as the primary point of contact for all application-related issues, providing technical guidance and support to team members.\n- Ensure adherence to best practices and standards for application development, testing, and deployment.\n- Identify and mitigate risks and issues related to application development and deployment.\n\nProfessional & Technical Skills:\n- Must To Have Skills: Strong experience in Microsoft Azure Analytics Services.\n- Good To Have Skills: Experience in other Microsoft Azure services such as Azure Functions, Azure Logic Apps, and Azure Event Grid.\n- Experience in designing, developing, and deploying applications using Microsoft Azure Analytics Services.\n- Strong understanding of cloud computing concepts and principles.\n- Experience in working with Agile methodologies.\n- Excellent problem-solving and analytical skills.\n\nAdditional Information:\n- The candidate should have a minimum of 5 years of experience in Microsoft Azure Analytics Services.\n- The ideal candidate will possess a strong educational background in computer science or a related field, along with a proven track record of delivering high-quality solutions.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Microsoft Azure Analytics Services', 'Azure Data Factory', 'Azure Data Engineer', 'Azure Data Lake', 'Azure Databricks', 'Azure Analysis Services', 'Azure Data Services', 'Data Bricks']",2025-06-14 06:14:02
Pyspark Developer,Fortune Global 500 IT Services Firm,3 - 8 years,Not Disclosed,['Hyderabad'],Technical Experience :\n\nExperience 5 yrs of Hands on Exp of Pyspark\nAWS Glue\nPython SQL\nExperience in Leading a team managing Delivery\nUnderstanding of Development LifeCycle CICD Pipelines\nExperience around Working with Agile Practices JIRA\nExperience in Designing Data Driven Solutions,Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'Aws Glue', 'SQL']",2025-06-14 06:14:04
C2H IT Recruiter,.,1 - 6 years,3-4.5 Lacs P.A.,['Noida'],"C2H IT Recruitment Specialist\nJob Description\nAn IT Recruitment Specialist is responsible for sourcing, screening, and hiring IT& Non - IT professionals For Product & Service based clients to meet the organization's technical hiring needs. This role involves collaborating with hiring managers to understand job requirements, identifying top talent through various channels, conducting interviews, and managing the candidate experience throughout the recruitment process. The ideal candidate will have a strong understanding of the tech industry and experience recruiting for technical roles like developers, system administrators, and data scientists.\n\nKey Responsibilities:\nSource and attract candidates through job boards, social media, and networking.\nScreen resumes, conduct interviews, and assess technical skills.\nCollaborate with hiring managers to define role requirements and candidate profiles.\nManage candidate communications and offer negotiations.\nTrack recruitment metrics and continuously improve processes.\n\nQualifications:\nExperience recruiting for C2H IT & Non - IT roles.\nStrong understanding of the tech industry and technical skills.\nExcellent communication and organizational skills.\nFamiliarity with ATS and recruitment tools.",Industry Type: Recruitment / Staffing,Department: Human Resources,"Employment Type: Full Time, Permanent","['Contract Hiring', 'Technical Hiring', 'Contract Staffing', 'C2H', 'Technical Recruitment']",2025-06-14 06:14:06
OCI Cloud AI Architect,Tekskills India private Limited (CMMI Le...,10 - 15 years,25-30 Lacs P.A.,['Noida'],"Job Title: OCI Cloud AI Architect\nLocation: Preferable- Noida (PAN India)-WFO\nShift timings : 2PM to 11PM IST\nTotal Experience : 10-15 years\n\nPrimary Skill\n1015 years in Oracle Cloud and Artificial Intelliegence, with 5+ years of proven experience in designing, architecting and deploying full stack AI/ML & Gen AI Solution over OCI AI stack\nStrong Python programming development experience, streamlit, XML, JSON etc.\nDeep hands-on knowledge of LLMs (e.g. Cohere, GPT etc.) & prompt engineering techniques (e.g. zero-shot, few-shot, CoT, and ReAct) especially for Cohere, & OpenAI Models.\nStrong knowledge of AI governance, security, guardrails, and responsible AI.\nMust have knowledge of data ingestion, feature engineering, model training, evaluation, deployment, and monitoring.\nProficient in AI/ML/Gen AI frameworks (e.g., TensorFlow, PyTorch, Hugging Face, LangChain) & Vector DBs (e.g., Pinecone, Milvus).\nProficient in implementing Agentic AI Frameworks (e.g. CrewAI, AutoGen and multi-agent orchestration workflow)\nDeep knowledge of OCI services including: OCI Data Science, OCI AI Services, OCI Gen AI Service, OCI Gen AI Agents, Oracle ATP, Data Flow (Apache Spark), OCI Functions, API Gateway, and Monitoring\nStrong understanding and practical application of: Prompt engineering, Fine-tuning and parameter-efficient tuning, Agentic orchestration workflows\nExperience with front end languages such as React, Angular or Javascript etc.\nExperience in Oracle ATP, 23ai Databases and vector queries\n\nSecondary Skill\nWorking experience of implementing semantic matching, fuzzy logic, and similarity scoring algorithms (e.g., cosine similarity, Jaccard, Levenshtein distance) to drive intelligent entry matching in noisy or inconsistent data scenarios.\nExperience with Finance domain solutions, particularly around reconciliations, journal entry matching, or financial close processes.\nUnderstanding of Oracle Cloud Deployment, architecture, networking, Security and other essential components\nGood to have knowledge of Analytics and Data Science.\nWe are looking for an experienced OCI AI Architect to lead the design and deployment of Gen AI, Agentic AI, and traditional AI/ML solutions on Oracle Cloud. This role requires a deep understanding of Oracle Cloud Architecture, Gen AI, Agentic and AI/ML frameworks, data engineering, and OCI-native services. The ideal candidate combines deep technical expertise in AI/ML and Gen AI over OCI with domain knowledge in Finance and Accounting.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Generative Ai', 'Oracle Cloud Infrastructure', 'Artificial Intelligence', 'Cloud Architecture', 'Aiml', 'data science', 'Oracle Cloud', 'Python']",2025-06-14 06:14:08
Power BI Developer,Vayuz Technologies,12 - 15 years,Not Disclosed,[],"Role Expectations :\n\n- Design, develop, and maintain interactive dashboards and reports using Power BI.\n\n- Create and optimize data models, including star/snowflake schemas.\n\n- Develop complex DAX calculations and KPIs to drive business insights.\n\n- Integrate data from various sources (SQL Server, Excel, SharePoint, APIs, etc.)\n\n- Collaborate with business stakeholders to gather requirements and translate them into technical solutions.\n\n- Perform data validation and troubleshooting to ensure accuracy and performance.\n\n- Implement row-level security and user-based data access strategies.\n\n- Provide guidance on Power BI governance, best practices, and self-service analytics models.\n\n- Maintain data refresh schedules and monitor report performance.\n\n- Work with cross-functional teams including data engineers, analysts, and business users.\n\nQualifications :\n\n- 12+ years of hands-on Power BI development.\n\n- Advanced DAX and data modeling expertise.\n\n- Experience with Power Query/M Language.\n\n- Strong dashboard and visualization design skills.\n\n- Power BI Service and Power BI Report Server knowledge.\n\n- Row-level security and gateway configurations.\n\n- Familiarity with Power BI REST API and embedding techniques.\n\n- Proficient in writing complex T-SQL queries, stored procedures, views, and functions.\n\n- Data extraction, transformation, and integration using SQL.\n\n- Experience with large datasets and performance tuning.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Power BI Development', 'Data Validation', 'T-SQL', 'BI Reporting', 'DAX Power BI', 'Power BI', 'Dashboard Design', 'Data Visualization', 'Data Modeling', 'Power Query M']",2025-06-14 06:14:10
Snowflake with DBT,Randomtrees,2 - 6 years,Not Disclosed,['Chennai'],"About the Role:\nWe are looking for a highly skilled and passionate Data Engineer to join our dynamic data team. In this role, you will be instrumental in designing, building, and optimizing our data infrastructure, with a strong emphasis on leveraging Snowflake for data warehousing and dbt (data build tool) for data transformation. You will work across various cloud environments (AWS, Azure, GCP), ensuring our data solutions are scalable, reliable, and efficient. This position requires a deep understanding of data warehousing principles, ETL/ELT methodologies, and a commitment to data quality and governance.",,,,"['Snowflake', 'DBT', 'SQL', 'Azure Cloud', 'GCP', 'aws']",2025-06-14 06:14:12
Full Stack Developer II,Bristol Myers Squibb,3 - 5 years,Not Disclosed,['Hyderabad'],"Challenging. Meaningful. Life-changing. Those aren t words that are usually associated with a job. But working at Bristol Myers Squibb is anything but usual. Here, uniquely interesting work happens every day, in every department. From optimizing a production line to the latest breakthroughs in cell therapy, this is work that transforms the lives of patients, and the careers of those who do it. You ll get the chance to grow and thrive through opportunities uncommon in scale and scope, alongside high-achieving teams. Take your career farther than you thought possible.\nBristol Myers Squibb recognizes the importance of balance and flexibility in our work environment. We offer a wide variety of competitive benefits, services and programs that provide our employees with the resources to pursue their goals, both at work and in their personal lives. Read more: careers. bms. com/working-with-us .\nPosition: Full Stack Developer II Location : Hyderabad, India\nPosition Summary\nAt BMS, digital innovation and Information Technology are central to our vision of transforming patients lives through science. To accelerate our ability to serve patients around the world, we must unleash the power of technology. We are committed to being at the forefront of transforming the way medicine is made and delivered by harnessing the power of computer and data science, artificial intelligence, and other technologies to promote scientific discovery, faster decision making, and enhanced patient care.\nIf you want an exciting and rewarding career that is meaningful, consider joining our diverse team!\nAs a Full Stack Developer II based out of our BMS Hyderabad you are part of the GDD Business Insights & Technology team that delivers data and analytics capabilities for RWD, Portfolio & Trial Operations functions. The ideal candidate will have a strong background in building, designing and improving the user experience of data products, platforms and services. The role involves a combination of creative and analytical skills, as well as an understanding of user behavior, and technology.\nKey Responsibilities\nThe Full Stack Developer II will be responsible for creating user and web applications using Python, SQL and React JS\nWrite clean, efficient, and maintainable code while following best practices.\nDevelop project and portfolio user interfaces like report visualizations and dashboards as a key capability and value driver\nUser research: Conducting user research to understand user needs, behaviors, and pain points. This may involve surveys, interviews, and usability testing.\nAccountability and involvement in wireframing, prototyping, and creating high-fidelity designs to enhance user experience.\nCollaboration with stakeholders: Collaborating with product managers, Senior Data Visualization Engineer, Data Engineers, and other stakeholders to ensure that the user experience aligns with business objectives and technical requirements.\nAccessibility: Ensuring that digital products and services are accessible to users with disabilities and comply with accessibility guidelines.\nHelp analyze data multiple sources of spectrum-related information, recommend and develop reports\nStaying up-to-date with industry trends: Keeping up-to-date with the latest trends and advancements in Engineering, UX design and technology, and applying this knowledge to enhance the organizations data pipeline and UX design capabilities.\nServes as the Subject Matter Expert on Data pipelines and BI reporting\nComfortable working in a fast-paced environment with minimal oversight\nPrior experience working in an Agile/Product based environment.\nProvides strategic feedback to vendors on service delivery and balances workload with vendor teams.\nQualifications & Experience\nDegree in Computer Science, Design & Engineering, Biotechnology, or a related field\n3-5 years proven working experience of front-end and back-end technologies, including - Python, flask, SQL alchemy, sql, React. js, HTML, CSS, JavaScript, and RESTful APIs.\nHands on experience on AWS Glue, Azure and Tableau is desired\nExperience with rapid wireframing, prototyping and storyboarding tools like Miro boards, and Adobe Creative Cloud Data Visualization Engineer tool set\nKnowledgeable in designing for both responsive and fixed size applications.\nAbility to wireframe, prototype, apply Design thinking principles and validate data products.\nAt least 3 years technical expertise in product design and development, data visualization techniques\nFamiliarity with database and ETL processes from Information management and consumption standpoint is needed is a plus\nStrong analytical and problem-solving skills\nExcellent communication and collaboration skills Functional knowledge or prior experience in Lifesciences Research and Development domain is a plus.\nExperience and expertise in establishing agile and product-oriented teams that work effectively with teams in US and other global BMS site.\nInitiates challenging opportunities that build strong capabilities for self and team.\nAround the world, we are passionate about making an impact on the lives of patients with serious diseases. Empowered to apply our individual talents and diverse perspectives in an inclusive culture, our shared values of passion, innovation, urgency, accountability, inclusion, and integrity bring out the highest potential of each of our colleagues.\nBristol Myers Squibb recognizes the importance of balance and flexibility in our work environment. We offer a wide variety of competitive benefits, services and programs that provide our employees with the resources to pursue their goals, both at work and in their personal lives.\nIf you come across a role that intrigues you but doesn t perfectly line up with your resume, we encourage you to apply anyway. You could be one step away from work that will transform your life and career.\nWith a single vision as inspiring as Transforming patients lives through science , every BMS employee plays an integral role in work that goes far beyond ordinary. Each of us is empowered to apply our individual talents and unique perspectives in a supportive culture, promoting global participation in clinical trials, while our shared values of passion, innovation, urgency, accountability, inclusion and integrity bring out the highest potential of each of our colleagues.\nBMS has an occupancy structure that determines where an employee is required to conduct their work. This structure includes site-essential, site-by-design, field-based and remote-by-design jobs. The occupancy type that you are assigned is determined by the nature and responsibilities of your role:\nSite-essential roles require 100% of shifts onsite at your assigned facility. Site-by-design roles may be eligible for a hybrid work model with at least 50% onsite at your assigned facility. For these roles, onsite presence is considered an essential job function and is critical to collaboration, innovation, productivity, and a positive Company culture. For field-based and remote-by-design roles the ability to physically travel to visit customers, patients or business partners and to attend meetings on behalf of BMS as directed is an essential job function.\nBMS is dedicated to ensuring that people with disabilities can excel through a transparent recruitment process, reasonable workplace accommodations/adjustments and ongoing support in their roles. Applicants can request a reasonable workplace accommodation/adjustment prior to accepting a job offer. If you require reasonable accommodations/adjustments in completing this application, or in any part of the recruitment process, direct your inquiries to adastaffingsupport@bms. com . Visit careers. bms. com/ eeo -accessibility to access our complete Equal Employment Opportunity statement.\nBMS cares about your well-being and the well-being of our staff, customers, patients, and communities. As a result, the Company strongly recommends that all employees be fully vaccinated for Covid-19 and keep up to date with Covid-19 boosters.\nBMS will consider for employment qualified applicants with arrest and conviction records, pursuant to applicable laws in your area.\nIf you live in or expect to work from Los Angeles County if hired for this position, please visit this page for important additional information: https://careers. bms. com/california-residents/\nAny data processed in connection with role applications will be treated in accordance with applicable data privacy policies and regulations.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Prototype', 'Analytical', 'Clinical trials', 'Oncology', 'Product design', 'HTML', 'Analytics', 'SQL', 'Python']",2025-06-14 06:14:14
"Customer Technical Support, SaaS",Congruent,5 - 10 years,Not Disclosed,['Chennai'],"Customer Technical Support, SaaS\n\nAbout The Role\nAs a member of the Global Customer Support department and reporting to the Manager of Technical Support, you will play a pivotal role as an internal support escalation subject matter expert. Our technical support services provide our customers with resolving technical challenges involving multiple platform issues, data flows, networking, communication failures, and KPI calculations for customer assets.\nThe Technical Support Specialist is the 1st line of support for incoming issues, requests and phone calls, This position involves prioritizing incoming requests, assigning tasks to appropriate support teams, and ensuring timely and efficient responses to customer concerns while maintaining a high level of customer In this role, you will support our customers by resolving technical challenges involving platform issues, data flows, networking, communication failures and more..\nThis is part of the shifting 24x7 schedule that includes weekends.\n\nWhat You Will Be Doing\nReceive, triage, and prioritize incoming technical support calls from customers.\nReceive incoming support requests through various channels such as phone and ticketing systems. Assess and prioritize cases based on urgency, impact, and customer Service Level Agreements (SLA)\nGather detailed information from customers to accurately identify and document issues, including relevant symptoms, error messages, and troubleshooting steps already taken. Update the client as support tickets progress and communicate issue resolution.\nTroubleshoot and diagnose data flow issues from remote sites into the Cloud. Perform diagnostic tests to resolve straight-forward issues or escalate complex cases to appropriate support tiers or subject matter experts.\nIdentify and escalate priority issues or service disruptions to appropriate personnel, such as support managers or senior support specialists, for resolution. Ensure timely resolution of escalated issues to minimize the impact on customers.\nMaintain accurate and up-to-date records of support requests, including details of the issue, troubleshooting steps taken, resolution provided, and any follow-up actions required.\nServe as the face of CLIENT with our Customers. Strive to meet or exceed customer satisfaction targets by providing professional, empathetic, and effective support experiences. Gather customer feedback and insights for continuous improvement.\nProvide expert technical support for SaaS Solutions used in renewable energy applications, particularly for solar, wind, storage and hybrid sites.\nAssist clients with troubleshooting, and resolving technical issues related to SCADA system operations, data collection, and control processes.\nTroubleshoot, diagnose, SaaS-related issues in real-time, ensuring minimal downtime for clients.\nRespond to customer inquiries through various channels (phone, ticketing system) with clear, concise, and actionable solutions.\nCreate and maintain detailed documentation for the resolution provided in the support ticketing system.\nCreate knowledge articles of recurring issues solutions, best practices and configuration guidelines in the knowledge base system.\nCollaborate with internal teams, including engineering, product development to escalate and resolve more complex issues.\nOwn Customer issue resolution and communication from start to end including engaging cross-functional teams for assistance in the background.\nParticipate in weekly on-call rotation.\n\nWhat You Will Need to Be Successful\nWe believe you enjoy working in a purpose-driven organization and thrive in an environment where you need to find creative solutions to challenging problems in a fast-changing context.\nThis also means that you effectively manage multiple tasks of varying complexities, work well in a fast-paced environment, and are driven by continuous learning and growth. Specifically, we are looking for someone with the following toolbox:\nBachelor's degree in engineering (Renewable /Communication/Data Engineering preferred).\nMinimum of 3 years of experience in a technical support role. Renewable industry or operations & maintenance (O&M) preferred).\nKnowledge of CRM / Ticketing system as well as ticketing workflows.\n\nCompetencies And Qualifications\nSoftware Proficiency. Proficient in using relevant software and support tools, CRM platforms like Freshdesk as well as Technical Support ticketing workflows. Familiarity with SaaS (Software as a Service) products and technologies is a plus.\nDatabase Knowledge. Familiar with SQL and relational databases.\nCloud Computing. Good understanding of cloud computing platforms and databases such as Azure and AWS (Amazon Web Services).\nComputer Networking Expertise. Experience working with Linux and networking fundamentals. Good understanding of networked devices and communication protocols.\nTroubleshooting. The ability to diagnose and resolve technical issues efficiently is fundamental. This includes researching problems, following procedures, and using relevant tools.\n\nSoft Skills\nCommunication\nEmpathy and Customer Focus\nTime Management and Prioritization\nAdaptability\nTeam Player",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Customer Support', 'SAAS', 'Freshdesk', 'customer Technical support']",2025-06-14 06:14:16
Power BI Developer,Vayuz Technologies,12 - 15 years,Not Disclosed,['Kolkata'],"Location : Remote.\n\nExperience : 12+ years.\n\nRole Expectations :\n\n- Design, develop, and maintain interactive dashboards and reports using Power BI.\n\n- Create and optimize data models, including star/snowflake schemas.\n\n- Develop complex DAX calculations and KPIs to drive business insights.\n\n- Integrate data from various sources (SQL Server, Excel, SharePoint, APIs, etc.)\n\n- Collaborate with business stakeholders to gather requirements and translate them into technical solutions.\n\n- Perform data validation and troubleshooting to ensure accuracy and performance.\n\n- Implement row-level security and user-based data access strategies.\n\n- Provide guidance on Power BI governance, best practices, and self-service analytics models.\n\n- Maintain data refresh schedules and monitor report performance.\n\n- Work with cross-functional teams including data engineers, analysts, and business users.\n\nQualifications :\n\n- 12+ years of hands-on Power BI development.\n\n- Advanced DAX and data modeling expertise.\n\n- Experience with Power Query/M Language.\n\n- Strong dashboard and visualization design skills.\n\n- Power BI Service and Power BI Report Server knowledge.\n\n- Row-level security and gateway configurations.\n\n- Familiarity with Power BI REST API and embedding techniques.\n\n- Proficient in writing complex T-SQL queries, stored procedures, views, and functions.\n\n- Data extraction, transformation, and integration using SQL.\n\n- Experience with large datasets and performance tuning.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Power BI', 'Data Validation', 'T-SQL', 'BI Reporting', 'DAX Power BI', 'Dashboard Design', 'Data Visualization', 'Data Modeling', 'Power Query M']",2025-06-14 06:14:18
Principal Architect,Horizon Therapeutics,5 - 14 years,Not Disclosed,['Hyderabad'],"Career Category Engineering Job Description\nABOUT AMGEN\nAmgen harnesses the best of biology and technology to fight the world s toughest diseases, and make people s lives easier, fuller and longer. We discover, develop, manufacture and deliver innovative medicines to help millions of patients. Amgen helped establish the biotechnology industry more than 40 years ago and remains on the cutting-edge of innovation, using technology and human genetic data to push beyond what s known today.\nABOUT THE ROLE\nRole Description:\nWe are seeking a seasoned and passionate Principal Architect (Enterprise Architect - Data Platform Engineering ) in our Data Architecture Engineer ing group to drive the architecture, development and implementation of our strategy spanning across Data Fabric, Data Management, and Data Analytics Platform stack . The ideal candidate possesses a deep technical expertise and understanding of data and analytics landscape, current tools and technology trends, and data engineering principles, coupled with strong leadership and data-driven problem-solving skills . As a Principal Architect , you will play a crucial role in building the strategy and driving the implementation of best practices across data and analytics platforms .\nRoles Responsibilities:\nMust be passionate about Data, Content and AI technologies - with ability to evaluate and assess new technology and trends in the market quickly - with enterprise architecture in mind\nDrive the strategy and implementation of enterprise data platform and technical roadmaps that align with the Amgen Data strategy\nMaintain the pulse of current market trends in data AI space and be able to quickly perform hands-on experimentation and evaluations\nProvide expert guidance and influence the management and peers from functional groups with Enterprise mindset and goals\nR esponsible for design, develop , optimize , delivery and support of Enterprise Data platform on AWS and Databricks architecture\nLeverage cloud platforms (AWS preferred) to build scalable and efficient data solutions.\nAdvice and support Application teams (product managers, architects, business analysts, and developers) on tools, technology, and methodology related to the design and development of applications that have large data volume and variety of data types\nCollaborate and align with EARB, Cloud Infrastructure , Security and other technology leaders on Enterprise Data Architecture changes\nEnsure scalability, reliability, and performance of data platforms by implementing best practices for architecture, cloud resource optimization, and system tuning.\nCollaboration with RunOps engineers to continuously increase our ability to push changes into production with as little manual overhead and as much speed as possible.\nBasic Qualifications and Experience:\nMaster s degree with 8 - 10 years of experience in Computer Science, IT or related field OR\nBachelor s degree with 10 - 14 years of experience in Computer Science, IT or related field\nFunctional Skills:\nMust-Have Skills:\n8 + years of experience in data architecture and engineering or related roles with hands-on experience building enterprise data platforms in a cloud environment (AWS, Azure, GCP).\n5+ years of experience in leading enterprise scale data platforms and solutions\nExpert-level proficiency with Databricks and experience in optimizing data pipelines and workflows in Databricks environments.\nDeep understanding of distributed computing, data architecture, and performance optimization in cloud-based environments.\nExperience with Enterprise mindset / certifications like TOGAF etc. are a plus.\nHighly preferred to have Big Tech or Big Consulting experience.\nSolid knowledge of data security, governance, and compliance practices in cloud environments.\nMust have exceptional communication to engage and influence architects and leaders in the organization\nGood-to-Have Skills:\nExperience with Gen AI tools in databricks\nExperience with unstructured data architecture and pip elines\nExperience working with agile development methodologies such as Scaled Agile.\nProfessional Certifications\nAWS Certified Data Engineer preferred\nDatabricks Certifi cate preferred\nSoft Skills:\nExcellent analytical and troubleshooting skills.\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nTeam-oriented, with a focus on achieving team goals\nStrong presentation and public speaking skills.\nEQUAL OPPORTUNITY STATEMENT\nAmgen is an Equal Opportunity employer and will consider you without regard to your race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, or disability status.\nWe will ensure that individuals with disabilities are provided with reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request an accommodation .\n.",Industry Type: Biotechnology,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Principal Architect', 'Data management', 'Enterprise architecture', 'TOGAF', 'data security', 'Consulting', 'Cloud', 'Manager Technology', 'AWS', 'Data architecture']",2025-06-14 06:14:21
Principal Solution Architect,Horizon Therapeutics,8 - 17 years,Not Disclosed,['Hyderabad'],"Career Category Engineering Job Description\nABOUT AMGEN\nAmgen harnesses the best of biology and technology to fight the world s toughest diseases, and make people s lives easier, fuller and longer. We discover, develop, manufacture and deliver innovative medicines to help millions of patients. Amgen helped establish the biotechnology industry more than 40 years ago and remains on the cutting-edge of innovation, using technology and human genetic data to push beyond what s known today.\nABOUT THE ROLE\nRole Description:\nWe are seeking a seasoned Principal Architect - Solutions to drive the architecture, development and implementation of data solutions to Amgen functional groups. The ideal candidate able to work in large scale Data Analytic initiatives, engage and work along with Business, Program Management, Data Engineering and Analytic Engineering teams. Be champions of enterprise data analytic strategy, data architecture blueprints and architectural guidelines. As a Principal Architect, you will play a crucial role in designing, building, and optimizing data solutions to Amgen functional groups such as RD, Operations and GCO.\nRoles Responsibilities:\nImplement and manage large scale data analytic solutions to Amgen functional groups that align with the Amgen Data strategy\nCollaborate with Business, Program Management, Data Engineering and Analytic Engineering teams to deliver data solutions\nResponsible for design, develop, optimize , delivery and support of Data solutions on AWS and Databricks architecture\nLeverage cloud platforms (AWS preferred) to build scalable and efficient data solutions.\nProvide expert guidance and mentorship to the team members, fostering a culture of innovation and best practices.\nBe passionate and hands-on to quickly experiment with new data related technologies\nDefine guidelines, standards, strategies, security policies and change management policies to support the Enterprise Data platform.\nCollaborate and align with EARB, Cloud Infrastructure, Security and other technology leaders on Enterprise Data Architecture changes\nWork with different project and application groups to drive growth of the Enterprise Data Platform using effective written/verbal communication skills, and lead demos at different roadmap sessions\nOverall management of the Enterprise Data Platform on AWS environment to ensure that the service delivery is cost effective and business SLAs around uptime, performance and capacity are met\nEnsure scalability, reliability, and performance of data platforms by implementing best practices for architecture, cloud resource optimization, and system tuning.\nCollaboration with RunOps engineers to continuously increase our ability to push changes into production with as little manual overhead and as much speed as possible.\nMaintain knowledge of market trends and developments in data integration, data management and analytics software/tools\nWork as part of team in a SAFe Agile/Scrum model\nBasic Qualifications and Experience:\nMaster s degree with 12 - 15 years of experience in Computer Science, IT or related field OR\nBachelor s degree with 14 - 17 years of experience in Computer Science, IT or related field\nFunctional Skills:\nMust-Have Skills:\n8+ years of hands-on experience in Data integrations, Data Management and BI technology stack.\nStrong experience with one or more Data Management tools such as AWS data lake, Snowflake or Azure Data Fabric\nExpert-level proficiency with Databricks and experience in optimizing data pipelines and workflows in Databricks environments.\nStrong experience with Python, PySpark , and SQL for building scalable data workflows and pipelines.\nExperience with Apache Spark, Delta Lake, and other relevant technologies for large-scale data processing.\nFamiliarity with BI tools including Tableau and PowerBI\nDemonstrated ability to enhance cost-efficiency, scalability, and performance for data solutions\nStrong analytical and problem-solving skills to address complex data solutions\nGood-to-Have Skills:\nPreferred to have experience in life science or tech or consultative solution architecture roles\nExperience working with agile development methodologies such as Scaled Agile.\nProfessional Certifications\nAWS Certified Data Engineer preferred\nDatabricks Certificate preferred\nSoft Skills:\nExcellent analytical and troubleshooting skills.\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nTeam-oriented, with a focus on achieving team goals\nStrong presentation and public speaking skills.\nEQUAL OPPORTUNITY STATEMENT\nAmgen is an Equal Opportunity employer and will consider you without regard to your race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, or disability status.\nWe will ensure that individuals with disabilities are provided with reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request an accommodation .\n.",Industry Type: Biotechnology,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Solution architecture', 'Architecture', 'Data management', 'Analytical', 'Manager Technology', 'Data analytics', 'AWS', 'SQL', 'Data architecture', 'Python']",2025-06-14 06:14:23
Principal Solution Architect,Horizon Therapeutics,8 - 17 years,Not Disclosed,['Hyderabad'],"Career Category Engineering Job Description\nABOUT AMGEN\nAmgen harnesses the best of biology and technology to fight the world s toughest diseases, and make people s lives easier, fuller and longer. We discover, develop, manufacture and deliver innovative medicines to help millions of patients. Amgen helped establish the biotechnology industry more than 40 years ago and remains on the cutting-edge of innovation, using technology and human genetic data to push beyond what s known today.\nABOUT THE ROLE\nRole Description:\nWe are seeking a seasoned Principal Architect - Solutions to drive the architecture, development and implementation of data solutions to Amgen functional groups. The ideal candidate able to work in large scale Data Analytic initiatives, engage and work along with Business, Program Management, Data Engineering and Analytic Engineering teams. Be champions of enterprise data analytic strategy, data architecture blueprints and architectural guidelines. As a Principal Architect, you will play a crucial role in designing, building, and optimizing data solutions to Amgen functional groups such as RD, Operations and GCO.\nRoles Responsibilities:\nImplement and manage large scale data analytic solutions to Amgen functional groups that align with the Amgen Data strategy\nCollaborate with Business, Program Management, Data Engineering and Analytic Engineering teams to deliver data solutions\nResponsible for design, develop, optimize, delivery and support of Data solutions on AWS and Databricks architecture\nLeverage cloud platforms (AWS preferred) to build scalable and efficient data solutions.\nProvide expert guidance and mentorship to the team members, fostering a culture of innovation and best practices.\nBe passionate and hands-on to quickly experiment with new data related technologies\nDefine guidelines, standards, strategies, security policies and change management policies to support the Enterprise Data platform.\nCollaborate and align with EARB, Cloud Infrastructure, Security and other technology leaders on Enterprise Data Architecture changes\nWork with different project and application groups to drive growth of the Enterprise Data Platform using effective written/verbal communication skills, and lead demos at different roadmap sessions\nOverall management of the Enterprise Data Platform on AWS environment to ensure that the service delivery is cost effective and business SLAs around uptime, performance and capacity are met\nEnsure scalability, reliability, and performance of data platforms by implementing best practices for architecture, cloud resource optimization, and system tuning.\nCollaboration with RunOps engineers to continuously increase our ability to push changes into production with as little manual overhead and as much speed as possible.\nMaintain knowledge of market trends and developments in data integration, data management and analytics software/tools\nWork as part of team in a SAFe Agile/Scrum model\nBasic Qualifications and Experience:\nMaster s degree with 12 - 15 years of experience in Computer Science, IT or related field OR\nBachelor s degree with 14 - 17 years of experience in Computer Science, IT or related field\nFunctional Skills:\nMust-Have Skills:\n8+ years of hands-on experience in Data integrations, Data Management and BI technology stack.\nStrong experience with one or more Data Management tools such as AWS data lake, Snowflake or Azure Data Fabric\nExpert-level proficiency with Databricks and experience in optimizing data pipelines and workflows in Databricks environments.\nStrong experience with Python, PySpark, and SQL for building scalable data workflows and pipelines.\nExperience with Apache Spark, Delta Lake, and other relevant technologies for large-scale data processing.\nFamiliarity with BI tools including Tableau and PowerBI\nDemonstrated ability to enhance cost-efficiency, scalability, and performance for data solutions\nStrong analytical and problem-solving skills to address complex data solutions\nGood-to-Have Skills:\nPreferred to have experience in life science or tech or consultative solution architecture roles\nExperience working with agile development methodologies such as Scaled Agile.\nProfessional Certifications\nAWS Certified Data Engineer preferred\nDatabricks Certificate preferred\nSoft Skills:\nExcellent analytical and troubleshooting skills.\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nTeam-oriented, with a focus on achieving team goals\nStrong presentation and public speaking skills.\nEQUAL OPPORTUNITY STATEMENT\nAmgen is an Equal Opportunity employer and will consider you without regard to your race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, or disability status.\nWe will ensure that individuals with disabilities are provided with reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request an accommodation.\n.",Industry Type: Biotechnology,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Computer science', 'Solution architecture', 'Change management', 'Data management', 'Analytical', 'Troubleshooting', 'infrastructure security', 'SQL', 'Python', 'Data architecture']",2025-06-14 06:14:25
Python Developer,Wits Innovation Lab,3 - 5 years,Not Disclosed,['Pune'],"Primary Job Responsibilities:\nCollaborate with team members to maintain, monitor, and improve data ingestion pipelines on the Data & AI platform.\nAttend the office 3 times a week for collaborative sessions and team alignment.\nDrive innovation in ingestion and analytics domains to enhance performance and scalability.\nWork closely with the domain architect to implement and evolve data engineering strategies\nRequired Skills:\nMinimum 5 years of experience in Python development focused on Data Engineering.\nHands-on experience with Databricks and Delta Lake format.\nStrong proficiency in SQL, data structures, and robust coding practices.\nSolid understanding of scalable data pipelines and performance optimization.\nPreferred / Nice to Have:\nFamiliarity with monitoring tools like Prometheus and Grafana.\nExperience using Copilot or AI-based tools for code enhancement and efficiency.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Databrick', 'Delta lake', 'Prometheus', 'Python', 'Robust Coding', 'Data Engineering', 'Data Structures', 'Python development', 'Copilot', 'SQL']",2025-06-14 06:14:27
Python Developer,Wits Innovation Lab,3 - 5 years,Not Disclosed,['Pune'],"Primary Job Responsibilities:\nCollaborate with team members to maintain, monitor, and improve data ingestion pipelines on the Data & AI platform.\n\nAttend the office 3 times a week for collaborative sessions and team alignment.\n\nDrive innovation in ingestion and analytics domains to enhance performance and scalability.\n\nWork closely with the domain architect to implement and evolve data engineering strategies.\n\nRequired Skills:\nMinimum 5 years of experience in Python development focused on Data Engineering.\n\nHands-on experience with Databricks and Delta Lake format.\n\nStrong proficiency in SQL, data structures, and robust coding practices.\n\nSolid understanding of scalable data pipelines and performance optimization.\n\nPreferred / Nice to Have:\nFamiliarity with monitoring tools like Prometheus and Grafana.\n\nExperience using Copilot or AI-based tools for code enhancement and efficiency.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Python', 'coding', 'Databrick', 'Delta lake', 'Prometheus', 'data structures', 'performance optimization', 'Grafana', 'SQL', 'analytics domains']",2025-06-14 06:14:30
Power BI Developer,Vayuz Technologies,12 - 15 years,Not Disclosed,[],"Role Expectations :\n\n- Design, develop, and maintain interactive dashboards and reports using Power BI.\n\n- Create and optimize data models, including star/snowflake schemas.\n\n- Develop complex DAX calculations and KPIs to drive business insights.\n\n- Integrate data from various sources (SQL Server, Excel, SharePoint, APIs, etc.)\n\n- Collaborate with business stakeholders to gather requirements and translate them into technical solutions.\n\n- Perform data validation and troubleshooting to ensure accuracy and performance.\n\n- Implement row-level security and user-based data access strategies.\n\n- Provide guidance on Power BI governance, best practices, and self-service analytics models.\n\n- Maintain data refresh schedules and monitor report performance.\n\n- Work with cross-functional teams including data engineers, analysts, and business users.\n\nQualifications :\n\n- 12+ years of hands-on Power BI development.\n\n- Advanced DAX and data modeling expertise.\n\n- Experience with Power Query/M Language.\n\n- Strong dashboard and visualization design skills.\n\n- Power BI Service and Power BI Report Server knowledge.\n\n- Row-level security and gateway configurations.\n\n- Familiarity with Power BI REST API and embedding techniques.\n\n- Proficient in writing complex T-SQL queries, stored procedures, views, and functions.\n\n- Data extraction, transformation, and integration using SQL.\n\n- Experience with large datasets and performance tuning.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Power BI', 'Data Validation', 'T-SQL', 'Dashboard Design', 'Data Visualization', 'Data Modeling', 'Power Query M', 'Data extraction']",2025-06-14 06:14:32
Azure Synapse,InfoCepts,3 - 7 years,Not Disclosed,['Mumbai'],Senior Azure Data Engineer ? L1 Support,Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['azure databricks', 'python', 'azure data lake', 'azure synapse', 'microsoft azure', 'power bi', 'data warehousing', 'dbms', 'azure data factory', 'azure logic apps', 'sql server', 'azure devops', 'sql', 'data bricks', 'azure functions', 'sql azure', 'spark', 'oracle adf', 'azure cosmosdb', 'ssis', 'etl', 'azure active directory', 'msbi']",2025-06-14 06:14:34
Power BI Developer,Vayuz Technologies,12 - 15 years,Not Disclosed,[],"Role Expectations :\n\n- Design, develop, and maintain interactive dashboards and reports using Power BI.\n\n- Create and optimize data models, including star/snowflake schemas.\n\n- Develop complex DAX calculations and KPIs to drive business insights.\n\n- Integrate data from various sources (SQL Server, Excel, SharePoint, APIs, etc.)\n\n- Collaborate with business stakeholders to gather requirements and translate them into technical solutions.\n\n- Perform data validation and troubleshooting to ensure accuracy and performance.\n\n- Implement row-level security and user-based data access strategies.\n\n- Provide guidance on Power BI governance, best practices, and self-service analytics models.\n\n- Maintain data refresh schedules and monitor report performance.\n\n- Work with cross-functional teams including data engineers, analysts, and business users.\n\nQualifications :\n\n- 12+ years of hands-on Power BI development.\n\n- Advanced DAX and data modeling expertise.\n\n- Experience with Power Query/M Language.\n\n- Strong dashboard and visualization design skills.\n\n- Power BI Service and Power BI Report Server knowledge.\n\n- Row-level security and gateway configurations.\n\n- Familiarity with Power BI REST API and embedding techniques.\n\n- Proficient in writing complex T-SQL queries, stored procedures, views, and functions.\n\n- Data extraction, transformation, and integration using SQL.\n\n- Experience with large datasets and performance tuning.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Power BI Development', 'Data Validation', 'T-SQL', 'BI Reporting', 'DAX Power BI', 'Power BI', 'Dashboard Design', 'Data Visualization', 'Data Modeling', 'Power Query M']",2025-06-14 06:14:37
Power BI Developer,Vayuz Technologies,12 - 15 years,Not Disclosed,[],"Role Expectations :\n\n- Design, develop, and maintain interactive dashboards and reports using Power BI.\n\n- Create and optimize data models, including star/snowflake schemas.\n\n- Develop complex DAX calculations and KPIs to drive business insights.\n\n- Integrate data from various sources (SQL Server, Excel, SharePoint, APIs, etc.)\n\n- Collaborate with business stakeholders to gather requirements and translate them into technical solutions.\n\n- Perform data validation and troubleshooting to ensure accuracy and performance.\n\n- Implement row-level security and user-based data access strategies.\n\n- Provide guidance on Power BI governance, best practices, and self-service analytics models.\n\n- Maintain data refresh schedules and monitor report performance.\n\n- Work with cross-functional teams including data engineers, analysts, and business users.\n\nQualifications :\n\n- 12+ years of hands-on Power BI development.\n\n- Advanced DAX and data modeling expertise.\n\n- Experience with Power Query/M Language.\n\n- Strong dashboard and visualization design skills.\n\n- Power BI Service and Power BI Report Server knowledge.\n\n- Row-level security and gateway configurations.\n\n- Familiarity with Power BI REST API and embedding techniques.\n\n- Proficient in writing complex T-SQL queries, stored procedures, views, and functions.\n\n- Data extraction, transformation, and integration using SQL.\n\n- Experience with large datasets and performance tuning.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Power BI', 'Data Validation', 'T-SQL', 'Dashboard Design', 'Data Visualization', 'Data Modeling', 'Power Query M', 'Data extraction']",2025-06-14 06:14:39
Power BI Developer,Vayuz Technologies,12 - 15 years,Not Disclosed,[],"Role Expectations :\n\n- Design, develop, and maintain interactive dashboards and reports using Power BI.\n\n- Create and optimize data models, including star/snowflake schemas.\n\n- Develop complex DAX calculations and KPIs to drive business insights.\n\n- Integrate data from various sources (SQL Server, Excel, SharePoint, APIs, etc.)\n\n- Collaborate with business stakeholders to gather requirements and translate them into technical solutions.\n\n- Perform data validation and troubleshooting to ensure accuracy and performance.\n\n- Implement row-level security and user-based data access strategies.\n\n- Provide guidance on Power BI governance, best practices, and self-service analytics models.\n\n- Maintain data refresh schedules and monitor report performance.\n\n- Work with cross-functional teams including data engineers, analysts, and business users.\n\nQualifications :\n\n- 12+ years of hands-on Power BI development.\n\n- Advanced DAX and data modeling expertise.\n\n- Experience with Power Query/M Language.\n\n- Strong dashboard and visualization design skills.\n\n- Power BI Service and Power BI Report Server knowledge.\n\n- Row-level security and gateway configurations.\n\n- Familiarity with Power BI REST API and embedding techniques.\n\n- Proficient in writing complex T-SQL queries, stored procedures, views, and functions.\n\n- Data extraction, transformation, and integration using SQL.\n\n- Experience with large datasets and performance tuning.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Power BI', 'Data Validation', 'T-SQL', 'Dashboard Design', 'Data Visualization', 'Data Modeling', 'Power Query M', 'Data extraction']",2025-06-14 06:14:41
Oracle Database Administrator,Top Rated Firm in Banking Domain,3 - 8 years,1-2.75 Lacs P.A.,"['Mumbai', 'Navi Mumbai']","Drop cvs -Mansi.kapoor@walkingtree.in\n3 to 6 years of Oracle database experience on Linux and windows\nPlatform\nHands on knowledge on SQL and PL/SQL.\nExperience with RMAN backup and recovery.\nGood Knowledge on Oracle RAC and Data guard concepts.\nGood to have goldengate\nStrong knowledge on Data Engineering activities. (Data Loading into\nOracle Database, Data Validation and verification), Experience on\ndatabase refresh tasks.\nExperience or conceptual knowledge of at least two opensource\ndatabases like MySQL, Hadoop, Postgre, MongoDB, Cassandra.\nExperience on backup, HA & DR of database.\nFamiliar with Linux & Windows OS.\nFamiliar with best practices of Storage, networking & TCP/IP port will\nbe added advantage.\nGood understanding of ITIL process & compliance, certification will\nbe added advantage.\nExcellent oral and written communication skills, excellent\ninterpersonal skills and the ability to work calmly and effectively in\npressure situations.\nIndustry Information technology\n\nResponsibilities\n\nInvolve in all DBA related activities and maintain DB uptime\nUnderstand the application flow and support to troubleshoot\ndatabase performance related issues.\nImplement database best practices to secure database.\nFollow change management & organization process.\nCollaborating with other members, to ensure that our services meet\noperations requirements for both internal and external clients and\nensuring that standards for consistent\ndocumentation are met. The successful candidate will need to work\nproductively as a team member and participate in a collaborative\ndecision-making process.\nActively participate in execution of activities, Collaborating with the\nteam on future direction and opportunities for new technology.\n\nIndustry\nCertifications Oracle certifications",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Oracle Administration', 'Oracle admin', 'Oracle Database', 'RAC', 'Datapump', 'Golden Gate', 'RMAN']",2025-06-14 06:14:43
Scrum Master,Syren Cloud Inc,4 - 10 years,Not Disclosed,['Hyderabad'],"Company Overview:\nSyrenCloud Inc. is a leading Data Engineering company that specializes in solving complex challenges in the Supply Chain Management industry. With a growing team of 350+ professionals and a solid revenue of $25M+, our mission is to empower organizations with cutting-edge software engineering solutions that optimize operations, harness supply chain intelligence, and drive sustainable growth.\nWe prioritize both professional growth and employee well-being, maintaining a positive work culture while offering opportunities for continuous learning and advancement.",,,,"['Supply chain', 'Stakeholder Engagement', 'PMI ACP', 'Supply chain management', 'Conflict resolution', 'Agile', 'Engineering projects', 'Scrum', 'PSM', 'Continuous improvement']",2025-06-14 06:14:46
Power BI Developer,Vayuz Technologies,12 - 15 years,Not Disclosed,[],"Role Expectations :\n\n- Design, develop, and maintain interactive dashboards and reports using Power BI.\n\n- Create and optimize data models, including star/snowflake schemas.\n\n- Develop complex DAX calculations and KPIs to drive business insights.\n\n- Integrate data from various sources (SQL Server, Excel, SharePoint, APIs, etc.)\n\n- Collaborate with business stakeholders to gather requirements and translate them into technical solutions.\n\n- Perform data validation and troubleshooting to ensure accuracy and performance.\n\n- Implement row-level security and user-based data access strategies.\n\n- Provide guidance on Power BI governance, best practices, and self-service analytics models.\n\n- Maintain data refresh schedules and monitor report performance.\n\n- Work with cross-functional teams including data engineers, analysts, and business users.\n\nQualifications :\n\n- 12+ years of hands-on Power BI development.\n\n- Advanced DAX and data modeling expertise.\n\n- Experience with Power Query/M Language.\n\n- Strong dashboard and visualization design skills.\n\n- Power BI Service and Power BI Report Server knowledge.\n\n- Row-level security and gateway configurations.\n\n- Familiarity with Power BI REST API and embedding techniques.\n\n- Proficient in writing complex T-SQL queries, stored procedures, views, and functions.\n\n- Data extraction, transformation, and integration using SQL.\n\n- Experience with large datasets and performance tuning.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Power BI', 'Data Validation', 'T-SQL', 'Dashboard Design', 'Data Visualization', 'Data Modeling', 'Power Query M', 'Data extraction']",2025-06-14 06:14:48
Power BI Developer,Vayuz Technologies,12 - 15 years,Not Disclosed,[],"Role Expectations :\n\n- Design, develop, and maintain interactive dashboards and reports using Power BI.\n\n- Create and optimize data models, including star/snowflake schemas.\n\n- Develop complex DAX calculations and KPIs to drive business insights.\n\n- Integrate data from various sources (SQL Server, Excel, SharePoint, APIs, etc.)\n\n- Collaborate with business stakeholders to gather requirements and translate them into technical solutions.\n\n- Perform data validation and troubleshooting to ensure accuracy and performance.\n\n- Implement row-level security and user-based data access strategies.\n\n- Provide guidance on Power BI governance, best practices, and self-service analytics models.\n\n- Maintain data refresh schedules and monitor report performance.\n\n- Work with cross-functional teams including data engineers, analysts, and business users.\n\nQualifications :\n\n- 12+ years of hands-on Power BI development.\n\n- Advanced DAX and data modeling expertise.\n\n- Experience with Power Query/M Language.\n\n- Strong dashboard and visualization design skills.\n\n- Power BI Service and Power BI Report Server knowledge.\n\n- Row-level security and gateway configurations.\n\n- Familiarity with Power BI REST API and embedding techniques.\n\n- Proficient in writing complex T-SQL queries, stored procedures, views, and functions.\n\n- Data extraction, transformation, and integration using SQL.\n\n- Experience with large datasets and performance tuning.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Power BI Development', 'Data Validation', 'T-SQL', 'BI Reporting', 'DAX Power BI', 'Power BI', 'Dashboard Design', 'Data Visualization', 'Data Modeling', 'Power Query M']",2025-06-14 06:14:50
Power BI Developer,Vayuz Technologies,12 - 15 years,Not Disclosed,[],"Role Expectations :\n\n- Design, develop, and maintain interactive dashboards and reports using Power BI.\n\n- Create and optimize data models, including star/snowflake schemas.\n\n- Develop complex DAX calculations and KPIs to drive business insights.\n\n- Integrate data from various sources (SQL Server, Excel, SharePoint, APIs, etc.)\n\n- Collaborate with business stakeholders to gather requirements and translate them into technical solutions.\n\n- Perform data validation and troubleshooting to ensure accuracy and performance.\n\n- Implement row-level security and user-based data access strategies.\n\n- Provide guidance on Power BI governance, best practices, and self-service analytics models.\n\n- Maintain data refresh schedules and monitor report performance.\n\n- Work with cross-functional teams including data engineers, analysts, and business users.\n\nQualifications :\n\n- 12+ years of hands-on Power BI development.\n\n- Advanced DAX and data modeling expertise.\n\n- Experience with Power Query/M Language.\n\n- Strong dashboard and visualization design skills.\n\n- Power BI Service and Power BI Report Server knowledge.\n\n- Row-level security and gateway configurations.\n\n- Familiarity with Power BI REST API and embedding techniques.\n\n- Proficient in writing complex T-SQL queries, stored procedures, views, and functions.\n\n- Data extraction, transformation, and integration using SQL.\n\n- Experience with large datasets and performance tuning.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Power BI Development', 'Data Validation', 'T-SQL', 'BI Reporting', 'DAX Power BI', 'Power BI', 'Dashboard Design', 'Data Visualization', 'Data Modeling', 'Power Query M']",2025-06-14 06:14:53
MDM Specialist,Amgen Inc,10 - 14 years,Not Disclosed,['Hyderabad'],"ABOUT THE ROLE\nRole Description:\nWe are seeking an experienced MDM Manager with 1014 years of experience to lead strategic development and operations of our Master Data Management (MDM) platforms, with hands-on experience in Informatica or Reltio. This role will involve managing a team of data engineers, architects, and quality experts to deliver high-performance, scalable, and governed MDM solutions that align with enterprise data strategy.To succeed in this role, the candidate must have strong MDM experience along with Data Governance, DQ, Data Cataloging implementation knowledge, hence the candidates",,,,"['MDM', 'Unix', 'Informatica', 'AWS', 'Reltio', 'Python', 'SQL']",2025-06-14 06:14:55
MDM Platform Administrator,Horizon Therapeutics,1 - 8 years,Not Disclosed,['Hyderabad'],"Career Category Information Systems Job Description\nABOUT AMGEN\nAmgen harnesses the best of biology and technology to fight the world s toughest diseases, and make people s lives easier, fuller and longer. We discover, develop, manufacture and deliver innovative medicines to help millions of patients. Amgen helped establish the biotechnology industry more than 40 years ago and remains on the cutting-edge of innovation, using technology and human genetic data to push beyond what s known today.\nABOUT THE ROLE\nRole Description:\nWe are seeking an MDM Admin/Infrastructure Resource with 2-5 years of experience to support and maintain our enterprise MDM (Master Data Management) platforms using Informatica MDM and IDQ. This role is critical in ensuring the reliability, availability, and performance of master data solutions across the organization, utilizing modern tools like Databricks and AWS for automation, backup, recovery, and preventive maintenance. The ideal candidate will have strong experience in server maintenance, data recovery, data backup, and MDM software support.\nRoles Responsibilities:\nAdminister and maintain customer, product, study master data using Informatica MDM and IDQ solutions.\nPerform data recovery and data backup processes to ensure master data integrity.\nConduct server maintenance and preventive maintenance activities to ensure system reliability.\nLeverage Unix/Linux, Python, and Databricks for scalable data processing and automation.\nCollaborate with business and data engineering teams for continuous improvement in MDM solutions.\nImplement automation processes for data backup, recovery, and preventive maintenance.\nUtilize AWS cloud services for data storage and compute processes related to MDM.\nSupport MDM software maintenance and upgrades.\nTrack and manage data issues using tools such as JIRA and document processes in Confluence.\nApply Life Sciences/Pharma industry context to ensure data standards and compliance.\nFunctional Skills:\nMust-Have Skills:\nStrong experience with Informatica MDM, IDQ platforms in administering and maintaining configurations\nStrong experience in data recovery and data backup processes\nStrong experience in server maintenance and preventive maintenance activities (Linux/Unix strong hands on and server upgrade experience)\nExpertise in handling data backups, server backups, MDM products upgrades, server upgrades\nGood understanding and hands on experience of access control\nExperience with IDQ, data modeling, and approval workflow/DCR\nAdvanced SQL expertise and data wrangling\nKnowledge of MDM, data governance, stewardship, and profiling practices\nGood-to-Have Skills:\nFamiliarity with Databricks and AWS architecture\nBackground in Life Sciences/Pharma industries\nFamiliarity with project tools like JIRA and Confluence\nBasics of data engineering concepts\nBasic Qualifications and Experience:\nMaster s degree with 1 - 3 years of experience in Business, Engineering, IT or related field OR\nBachelor s degree with 2 - 5 years of experience in Business, Engineering, IT or related field OR\nDiploma with 6 - 8 years of experience in Business, Engineering, IT or related field\nProfessional Certifications (preferred):\nAny ETL certification (e. g. , Informatica)\nAny Data Analysis certification (SQL)\nAny cloud certification (AWS or Azure)\nSoft Skills:\nExcellent written and verbal communications skills (English) in translating technology content into business-language at various levels\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation\nAbility to manage multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals\nStrong problem-solving and analytical skills.\nStrong time and task management skills to estimate and successfully meet project timeline with ability to bring consistency and quality assurance across various projects.\nEQUAL OPPORTUNITY STATEMENT\nAmgen is an Equal Opportunity employer and will consider you without regard to your race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, or disability status.\nWe will ensure that individuals with disabilities are provided with reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. Please contact us to request accommodation.\n.",Industry Type: Biotechnology,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Unix', 'Automation', 'Data analysis', 'Manager Quality Assurance', 'Linux', 'Data modeling', 'Workflow', 'Informatica', 'SQL', 'Python']",2025-06-14 06:14:57
Data Analyst,Ornella Enterprises,0 - 2 years,2-4.25 Lacs P.A.,['Mumbai (All Areas)'],"A data analyst's primary role is to transform raw data into actionable insights that inform business decisions. They gather, clean, analyze, visualize, and present data to help businesses understand trends, identify patterns, and make informed choice",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Transformation', 'Data Manipulation', 'Data Analysis', 'Data Crunching', 'Data Reporting', 'Data Presentation', 'Data Research', 'Data Enrichment', 'Data Interpretation', 'Data Management', 'Data Extraction', 'Data Cleansing', 'Data Mining', 'Data Visualization', 'Data Processing', 'Data Analytics']",2025-06-14 06:14:59
Python Project Manager,Flynaut Saas,2 - 5 years,3.5-6 Lacs P.A.,['Pune( Hinjewadi )'],"Job Summary:\nWe are seeking an experienced and dynamic Python Project Manager to lead and deliver complex software projects while guiding and mentoring a high-performing technical team. This role blends project leadership with technical expertise, making it ideal for someone with a solid background in Python development who also excels at managing people, timelines, and client expectations.\nKey Responsibilities:\nLead end-to-end project execution with a focus on Python-based development solutions.\nManage the planning, coordination, and delivery of software projects, ensuring high quality and timely execution.\nProvide hands-on technical guidance in designing, developing, and deploying Python applications (e.g., using Flask, Django, FastAPI).\nDefine project scope, schedules, and resource plans in collaboration with stakeholders.\nConduct code reviews, set development standards, and support best practices in coding and architecture.\nServe as a technical and strategic advisor to internal teams and clients.\nFacilitate Agile ceremonies including sprint planning, daily standups, and retrospectives.\nProactively identify project risks and issues, and implement mitigation strategies.\nCollaborate with DevOps, QA, UI/UX, and data teams to ensure integration, testing, and deployment are seamless.\nDeliver regular updates to senior leadership and stakeholders on project status and team performance.\nRequired Skills and Qualifications:\nMinimum of 2 years of experience in a technical leadership or project management role, demonstrating the ability to guide teams and deliver successful outcomes.\nStrong proficiency in Python and frameworks such as Django, Flask, or FastAPI.\nExperience designing RESTful APIs and working with databases (SQL/NoSQL).\nProven ability to lead software engineering teams and manage project life cycles.\nStrong understanding of Agile/Scrum methodologies and project management tools (e.g., Jira, Trello).\nHands-on experience with DevOps tools, CI/CD pipelines, and cloud platforms (AWS, GCP, or Azure).\nExceptional communication and stakeholder management skills.\nBachelors or Masters degree in Computer Science, Engineering, or related field.\nPMP, PMI-ACP, or Scrum certifications are a plus.\nPreferred Qualifications:\nExperience in data engineering, machine learning, or AI projects.\nFamiliarity with front-end technologies (e.g., JavaScript, React) is a plus.\nPrior experience working in a client-facing or consulting environment.\n\n\nCompany Website: https://flynautsaas.com/",Industry Type: IT Services & Consulting,Department: Project & Program Management,"Employment Type: Full Time, Permanent","['Django', 'Python', 'Flask', 'Trello', 'Fast Api', 'JIRA', 'PMI', 'SQL', 'Rest', 'PMP', 'Agile', 'Cicd Pipeline', 'Scrum', 'AWS']",2025-06-14 06:15:01
Technical Architect - Python,Career Foresight Hr Solution,8 - 13 years,35-45 Lacs P.A.,['Kochi'],"Exp in Python programming for data tasks.\nExp in AWS data technologies, including AWS Data Lakehouse, S3, Redshift, Athena etc and exposure to equivalent Azure technologies\nExp in working with applications built on microservices architecture\n\nRequired Candidate profile\nExtensive experience with ETL and Data Engineering tools and processes, particularly within the AWS ecosystem including AWS Glue, Athena, AWS Data Pipeline, AWS Lambda etc.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Python', 'Java', 'Glue', 'Redshift Aws', 'Lamda', 'AWS', 'Athena']",2025-06-14 06:15:03
F2F interview 9+ yrs exp in Architect Oracle DBA For Mumbai (Juinagar),Reserve Bank Information Technology,9 - 14 years,7-17 Lacs P.A.,"['Navi Mumbai', 'Mumbai (All Areas)']","Role & responsibilities\n\n9+ years of Oracle database experience on Linux and windows Platform\nHands on knowledge on SQL and PL/SQL.\nExperience with RMAN backup and recovery.\nGood Knowledge on Oracle RAC and Data guard concepts.\nGoldengate is must have\nStrong knowledge on Data Engineering activities. (Data Loading into Oracle Database, Data Validation and verification), Experience on database refresh tasks.\nExperience or conceptual knowledge of at least two opensource databases like MySQL, Hadoop, Postgre, MongoDB, Cassandra.\nExperience on backup, HA & DR of database.\nFamiliar with Linux & Windows OS.\nFamiliar with best practices of Storage, networking & TCP/IP port will be added advantage.\nGood understanding of ITIL process & compliance, certification will be added advantage.\nExcellent oral and written communication skills, excellent interpersonal skills and the ability to work calmly and effectively in pressure situations.\n\nIndustry Certifications Oracle certifications\nMandatory F2F interview\nLocation: Navi Mumbai (Juinagar)\n\nInterested candidate can share me there updated resume in recruiter.wtr26@walkingtree.in",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Oracle Database Architecture', 'Golden Gate', 'Backup', 'Hadoop', 'Cassandra', 'Oracle certifications', 'Rman', 'plsql', 'sql', 'MySQL', 'HA', 'MongoDB', 'DA', 'Postgre']",2025-06-14 06:15:06
F2F interview For Oracle DBA For Navi Mumbai (Juinagar),Reserve Bank Information Technology,3 - 8 years,4-9 Lacs P.A.,"['Navi Mumbai', 'Mumbai (All Areas)']","Role & responsibilities\n\n3+ years of Oracle database experience on Linux and windows Platform\nHands on knowledge on SQL and PL/SQL.\nExperience with RMAN backup and recovery.\nGood Knowledge on Oracle RAC and Data guard concepts.\nGoldengate is must have\nStrong knowledge on Data Engineering activities. (Data Loading into Oracle Database, Data Validation and verification), Experience on database refresh tasks.\nExperience or conceptual knowledge of at least two opensource databases like MySQL, Hadoop, Postgre, MongoDB, Cassandra.\nExperience on backup, HA & DR of database.\nFamiliar with Linux & Windows OS.\nFamiliar with best practices of Storage, networking & TCP/IP port will be added advantage.\nGood understanding of ITIL process & compliance, certification will be added advantage.\nExcellent oral and written communication skills, excellent interpersonal skills and the ability to work calmly and effectively in pressure situations.\n\nIndustry Certifications Oracle certifications\n\nMandatory F2F interview\n\nLocation: Navi Mumbai (Juinagar)\n\nInterested candidate can share me there updated resume in recruiter.wtr26@walkingtree.in",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Oracle Database', 'Golden Gate', 'backup', 'Hadoop', 'Cassandra', 'recovery', 'windows', 'RMAN', 'sql', 'plsql', 'Linux', 'RAC', 'MySQL', 'HA', 'MongoDB', 'Oracle RAC', 'DA', 'Postgre']",2025-06-14 06:15:08
Power BI Developer,Ignitho,3 - 5 years,Not Disclosed,['Chennai( Sholinganallur )'],"Position: Senior Power BI Developer\nLocation: Chennai, Tamil Nadu\nExperience Required: 3 to 5 Years\nAbout the Company:\nIgnitho Inc. is a leading AI and data engineering company with a global presence, including US, UK, India, and Costa Rica offices.\nVisit our website to learn more about our work and culture: www.ignitho.com.\nIgnitho is a portfolio company of Nuivio Ventures Inc., a venture builder dedicated to developing Enterprise AI product companies across various domains, including AI, Data Engineering, and IoT.\nLearn more about Nuivio at: www.nuivio.com.\nJob Overview:\nWe are seeking a highly skilled and experienced Senior Power BI Developer to join our dynamic and growing team. In this role, you will be responsible for designing, developing, and maintaining interactive reports and dashboards that empower business users to make informed, data-driven decisions. The ideal candidate will possess a deep understanding of Power BI, data modelling, and business intelligence best practices.\nKey Responsibilities:\nDesign and develop robust Power BI reports and dashboards aligned with business objectives.\nBuild complex semantic models, including composite models.\nUtilise DAX and Power Query to create high-performance BI solutions.\nIntegrate data from multiple on-premises and cloud-based databases.\nDevelop reports with advanced custom visuals and interactive elements.\nImplement data loading through XMLA Endpoints.\nDesign and manage Power Automate flows for process automation.\nCreate and maintain Paginated Reports.\nIntegrate advanced analytics tools (e.g., Python, R) within Power BI.\nApply strong SQL skills and ETL processes for data transformation and warehousing.\nFollow Agile methodologies for BI solution delivery.\nManage deployment pipelines and version control.\nAdminister Power BI environments, including workspace management, security, and content sharing.\nLeverage Power BI Embedded or REST API for advanced integration and automation.\nRequired Qualifications:\nBachelors degree in computer science, Information Systems, Data Science, or a related field (or equivalent professional experience).\n3 to 5 years of hands-on experience in developing Power BI reports and dashboards.\nProven expertise in DAX, Power Query, and data modelling.\nPreferred Skills:\nExperience with Databricks.\nFamiliarity with Python, R, or other data analysis tools.\nPower BI certification is a strong advantage.",Industry Type: Emerging Technologies (AI/ML),Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Power Bi', 'Databricks Unified Data Analytics Platform', 'Power Automate', 'Dax', 'Python', 'Xmla', 'Data Modeling', 'ETL', 'Power Query', 'SQL']",2025-06-14 06:15:10
Solution Architect,Arges Global,8 - 12 years,30-45 Lacs P.A.,['Pune( Baner )'],"Role Summary:\nWe are looking for a dynamic and experienced Solution Architect RPA & Intelligent Automation to lead automation-driven digital transformation initiatives. The ideal candidate will have deep hands-on experience with RPA platforms like Power Automate, UiPath, Automation Anywhere, along with a working understanding of data architecture to design intelligent, end-to-end process automation solutions.\nYou will be responsible for defining the automation architecture, overseeing bot design and orchestration, and driving adoption of intelligent technologies like AI-OCR. Process Automation tools and Data Management across enterprise systems.\n\nKey Responsibilities:\nAutomation & RPA Architecture\nLead the architecture, design, and implementation of enterprise-wide automation solutions using Automation Anywhere (preferred) or equivalent RPA platforms.\nDesign reusable bots, components, and templates for scalable automation across departments like Finance, HR, Supply Chain, and Customer Support.\nDefine automation standards, best practices, and frameworks including BOT lifecycle management and exception handling.\nAI & Intelligent Document Processing\nArchitect solutions that leverage AI-OCR, document classification, entity extraction, and LLMs to automate unstructured document workflows (e.g., invoices, forms, contracts).\nIntegrate OCR tools (e.g., ABBYY, Tesseract, or Automation Anywhere IQ Bot) with RPA to deliver end-to-end document automation.\nProcess Mining & Optimization\nUtilize process mining and task mining tools to identify automation opportunities and baseline performance metrics.\nCollaborate with business stakeholders to re-engineer processes for maximum automation ROI.\nIntegration & Orchestration\nDesign secure, scalable integrations between RPA bots and enterprise applications (ERP, CRM, custom systems) via APIs, Front-end or file-based methods.\nImplement orchestration strategies for attended, unattended, and hybrid bots, including version control and scheduling.\nData Awareness & Automation\nCollaborate with data engineering teams to ensure RPA and automation platforms can integrate effectively with data lakes and BI tools.\nLeverage structured and semi-structured data from platforms like Cloudera or Azure Data Lake for enhanced automation logic and analytics.\n\nRequired Experience & Skills:\n8–12 years in IT, with 5+ years in RPA and Intelligent Automation, and at least 2 years in an architecture role.\nProven hands-on experience with Automation Anywhere (A360) including bot development, control room setup, and IQ Bot/Document Automation.\nExposure to AI technologies including OCR, NLP, and LLMs (OpenAI, Azure Cognitive Services, Google AI).\nExperience in implementing end-to-end automation across business functions— preferably as part of a Digital Transformation initiative.\nStrong understanding of process design, business rules, workflow optimization, and exception handling.\nAbility to lead cross-functional automation teams and manage vendor relationships.\n\nGood to Have (Not Mandatory):\nKnowledge of Cloudera, Snowflake, or Azure Data Lake for data handling within automation workflows.\nFamiliarity with Python, SQL, API/Webhooks, or basic scripting to support advanced automation logic.\nExperience with cloud-native RPA deployments and DevOps automation pipelines.\nCertifications:\no Automation Anywhere Certified Master Architect\nAI/ML or OCR-related certifications\nCloud certifications (AWS, Azure, GCP) are a bonus\n\nSoft Skills:\nStrategic mindset with hands-on delivery capabilities\nStrong communication and stakeholder engagement\nDetail-oriented with the ability to simplify complex solutions for business users\nLeadership experience in cross-functional teams",Industry Type: Financial Services (Asset Management),Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Power Automate', 'Automation Anywhere', 'Rpa Automation', 'Uipath']",2025-06-14 06:15:12
