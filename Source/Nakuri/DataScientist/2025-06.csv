job_title,company,experience,salary,locations,description,industry,department,employment_type,skills,scraped_at
Data Scientist-Artificial Intelligence,IBM,3 - 7 years,Not Disclosed,['Pune'],"As an Associate Data Scientist at IBM, you will work to solve business problems using leading edge and open-source tools such as Python, R, and TensorFlow, combined with IBM tools and our AI application suites. You will prepare, analyze, and understand data to deliver insight, predict emerging trends, and provide recommendations to stakeholders.\n\nIn your role, you may be responsible for:\nImplementing and validating predictive and prescriptive models and creating and maintaining statistical models with a focus on big data & incorporating machine learning. techniques in your projects\nWriting programs to cleanse and integrate data in an efficient and reusable manner\nWorking in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors\nCommunicating with internal and external clients to understand and define business needs and appropriate modelling techniques to provide analytical solutions.\nEvaluating modelling results and communicating the results to technical and non-technical audiences\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nProof of Concept (POC) DevelopmentDevelop POCs to validate and showcase the feasibility and effectiveness of the proposed AI solutions.\nCollaborate with development teams to implement and iterate on POCs, ensuring alignment with customer requirements and expectations.\nHelp in showcasing the ability of Gen AI code assistant to refactor/rewrite and document code from one language to another, particularly COBOL to JAVA through rapid prototypes/ PoC\nDocument solution architectures, design decisions, implementation details, and lessons learned.\nCreate technical documentation, white papers, and best practice guides\n\n\nPreferred technical and professional experience\nStrong programming skills, with proficiency in Python and experience with AI frameworks such as TensorFlow, PyTorch, Keras or Hugging Face.\nUnderstanding in the usage of libraries such as SciKit Learn, Pandas, Matplotlib, etc. Familiarity with cloud platforms\nExperience and working knowledge in COBOL & JAVA would be preferred",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'scikit-learn', 'tensorflow', 'pytorch', 'keras', 'natural language processing', 'neural networks', 'predictive', 'huggingface', 'machine learning', 'prototype', 'artificial intelligence', 'sql', 'pandas', 'deep learning', 'r', 'java', 'cobol', 'data science', 'matplotlib', 'big data', 'statistics']",2025-06-12 06:13:54
Data Scientist,Ford,3 - 7 years,Not Disclosed,['Chennai'],"As a Data Scientist, you will be part of a high performing team working on exciting opportunities in AI/ML within Ford Credit. We are looking for a seasoned Data Scientist with proven expertise in implementing Machine Learning/Optimization solutions with familiarity in Generative AI and a good grasp of Statistics.\nDevelop Machine Learning (Supervised/Unsupervised learning), Neural Networks (ANN, CNN, RNN, LSTM, Decision tree, Encoder, Decoder), Natural Language Processing, Generative AI (LLMs, Lang Chain, RAG, Vector Database) .\nHands-on Expertise in Python programming (OOPs concepts), SQL (relational/non-relational databases), experience in handling various data science libraries (Pandas, NumPy, SciPy, Sklearn, TensorFlow, Keras, Pytorch, etc. ) would be a necessary requirement.\nExposure to Cloud technologies (e. g. , Google Cloud/AWS/Azure), including executing Machine Learning algorithms on Cloud is necessary.\nExposure to Generative AI technologies.\nProfessional Qualification:\nPotential candidates should possess 3 to 7 years of working experience as a Data Scientist.\nBE/MSc/ MTech /ME/PhD (Computer Science/Maths, Statistics).\nPossess a strong analytical mindset and be very comfortable with data.\nExperience with handling both relational and non-relational data.\nHands-on with analytics methods (descriptive / predictive / prescriptive) , Statistical Analysis, Probability and Data Visualization tools (Python-Matplotlib, Seaborn).\nBackground of Computer Science with excellent Data Science working experience.\nTechnical Experience:\nDevelop Machine Learning (Supervised/Unsupervised learning), Neural Networks (ANN, CNN, RNN, LSTM, Decision tree, Encoder, Decoder), Natural Language Processing, Generative AI (LLMs, Lang Chain, RAG, Vector Database) .\nHands-on Expertise in Python programming (OOPs concepts), SQL (relational/non-relational databases), experience in handling various data science libraries (Pandas, NumPy, SciPy, Sklearn, TensorFlow, Keras, Pytorch, etc. ) would be a necessary requirement.\nExposure to Cloud technologies (e. g. , Google Cloud/AWS/Azure), including executing Machine Learning algorithms on Cloud is necessary.\nExposure to Generative AI technologies.\nAbility to scope the problem statement, data preparation, training and making the AI/ML model production ready.\nWork with business partners to understand the problem statement, translate the same into analytical problem.\nAbility to manipulate structured and unstructured data.\nDevelop, test and improve existing machine learning models.\nAnalyse large and complex data sets to derive valuable insights.\nResearch and implement best practices to enhance existing machine learning infrastructure. Develop prototypes for future exploration.\nDesign and evaluate approaches for handling large volume of real data streams.\nAbility to determine appropriate analytical methods to be used.\nCollaborate with data engineers, solutions architects, application engineers, and product teams across time zones to develop data and model pipelines",Industry Type: Auto Components,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'data science', 'Neural networks', 'Analytical', 'Machine learning', 'Natural language processing', 'data visualization', 'Analytics', 'SQL', 'Python']",2025-06-12 06:13:56
Data Scientist - L3,Wipro,3 - 5 years,Not Disclosed,['Bengaluru'],"Role Purpose\nThe purpose of the role is to define, architect and lead delivery of machine learning and AI solutions.\n\n\n\nDo\n1. Demand generation through support in Solution development\na. Support Go-To-Market strategy\ni. Contribute to development solutions, proof of concepts aligned to key offerings to enable solution led sales\nb. Collaborate with different colleges and institutes for research initiatives and provide data science courses\n2. Revenue generation through Building & operationalizing Machine Learning, Deep Learning solutions\na. Develop Machine Learning / Deep learning models for decision augmentation or for automation solutions\nb. Collaborate with ML Engineers, Data engineers and IT to evaluate ML deployment options\n3. Team Management\na. Talent Management\ni. Support on boarding and training to enhance capability & effectiveness\n\n\n\nDeliver\n\nNo.Performance ParameterMeasure\n1.Demand generation# PoC supported\n2.Revenue generation through deliveryTimeliness, customer success stories, customer use cases\n3.Capability Building & Team Management# Skills acquired\n\n\n\n\n\n\nMandatory Skills: Data Analysis.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Analysis', 'ML deployment', 'Deep learning models', 'Solution development', 'Talent Management', 'Machine Learning']",2025-06-12 06:13:59
Data Scientist - L3,Wipro,3 - 5 years,Not Disclosed,['Ramdurg'],"Role Purpose\nThe purpose of the role is to define, architect and lead delivery of machine learning and AI solutions.\nDo\n1. Demand generation through support in Solution development\na. Support Go-To-Market strategy\ni. Contribute to development solutions, proof of concepts aligned to key offerings to enable solution led sales\nb. Collaborate with different colleges and institutes for research initiatives and provide data science courses\n2. Revenue generation through Building & operationalizing Machine Learning, Deep Learning solutions\na. Develop Machine Learning / Deep learning models for decision augmentation or for automation solutions\nb. Collaborate with ML Engineers, Data engineers and IT to evaluate ML deployment options\n3. Team Management\na. Talent Management\ni. Support on boarding and training to enhance capability & effectiveness\nDeliver\n\nNo.Performance ParameterMeasure\n1.Demand generation# PoC supported\n2.Revenue generation through deliveryTimeliness, customer success stories, customer use cases\n3.Capability Building & Team Management# Skills acquired\n\n\nMandatory Skills: Data Analysis.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Analysis', 'python', 'team management', 'natural language processing', 'scikit-learn', 'ml deployment', 'machine learning', 'data engineering', 'artificial intelligence', 'sql', 'deep learning', 'tensorflow', 'data science', 'predictive modeling', 'statistical modeling', 'ml']",2025-06-12 06:14:01
Data Scientist,Cars24,2 - 5 years,15-30 Lacs P.A.,['Gurugram'],"About the Role:\nWere looking for an innovative and execution-focused Machine Learning Engineer with deep expertise in AI/ML model development, deployment, and scaling. This role goes beyond research — we’re looking for someone eager to take models from notebooks to real-world systems and own the full ML lifecycle.\n\nKey Responsibilities:\nDesign, develop, and deploy machine learning models for large-scale applications.\nApply GenAI, NLP, and deep learning techniques to real business problems.\nOwn the entire model pipeline — from ideation and training to testing and deployment.\nWork closely with cross-functional teams to drive AI-powered product features.\nEnsure scalability, reliability, and robustness in model performance.\nCore Requirements:\nStrong foundation in AI/ML research and model deployment.\nHands-on experience with GenAI, NLP, deep learning, and large-scale ML systems.\nProficiency in Python (Pandas, NumPy, Matplotlib, Seaborn, etc.).\nExperience with ML frameworks: TensorFlow / PyTorch.\nWorking knowledge of ML algorithms – Regression, Classification, Clustering, Random Forest, Boosting, etc.\nSolid grasp of Probability, Statistics, and Mathematics.\nStrong command of SQL for data extraction.\nUnderstanding of production ML architecture and deployment pipelines.\nBonus Skills:\nExperience deploying models to production environments.\nFamiliarity with Linux commands and scripting.\nExposure to cloud platforms (e.g., GCP, AWS, Azure).\nUnderstanding of A/B Testing frameworks and experimentation.",Industry Type: Automobile (Automobile Dealers),Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Tensorflow', 'Artificial Intelligence', 'Natural Language Processing', 'Machine Learning', 'Deep Learning', 'Python']",2025-06-12 06:14:03
Data Scientist,Wipro,2 - 7 years,Not Disclosed,['Bengaluru'],"Role Purpose\nThe purpose of this role is to design, test and maintain software programs for operating systems or applications which needs to be deployed at a client end and ensure its meet 100% quality assurance parameters\nDo\nInstrumental in understanding the requirements and design of the product/ software\nDevelop software solutions by studying information needs, studying systems flow, data usage and work processes\nInvestigating problem areas followed by the software development life cycle\nFacilitate root cause analysis of the system issues and problem statement\nIdentify ideas to improve system performance and impact availability\nAnalyze client requirements and convert requirements to feasible design\nCollaborate with functional teams or systems analysts who carry out the detailed investigation into software requirements\nConferring with project managers to obtain information on software capabilities\nPerform coding and ensure optimal software/ module development\nDetermine operational feasibility by evaluating analysis, problem definition, requirements, software development and proposed software\nDevelop and automate processes for software validation by setting up and designing test cases/scenarios/usage cases, and executing these cases\nModifying software to fix errors, adapt it to new hardware, improve its performance, or upgrade interfaces.\nAnalyzing information to recommend and plan the installation of new systems or modifications of an existing system\nEnsuring that code is error free or has no bugs and test failure\nPreparing reports on programming project specifications, activities and status\nEnsure all the codes are raised as per the norm defined for project / program / account with clear description and replication patterns\nCompile timely, comprehensive and accurate documentation and reports as requested\nCoordinating with the team on daily project status and progress and documenting it\nProviding feedback on usability and serviceability, trace the result to quality risk and report it to concerned stakeholders\nStatus Reporting and Customer Focus on an ongoing basis with respect to project and its execution\nCapturing all the requirements and clarifications from the client for better quality work\nTaking feedback on the regular basis to ensure smooth and on time delivery\nParticipating in continuing education and training to remain current on best practices, learn new programming languages, and better assist other team members.\nConsulting with engineering staff to evaluate software-hardware interfaces and develop specifications and performance requirements\nDocument and demonstrate solutions by developing documentation, flowcharts, layouts, diagrams, charts, code comments and clear code\nDocumenting very necessary details and reports in a formal way for proper understanding of software from client proposal to implementation\nEnsure good quality of interaction with customer w.r.t. e-mail content, fault report tracking, voice calls, business etiquette etc\nTimely Response to customer requests and no instances of complaints either internally or externally\nStakeholder Interaction\n\nStakeholder Type\nStakeholder Identification\nPurpose of Interaction\nInternal\nLead Software Developer and Project Manager\nRegular reporting & updates\nSoftware Developers\nFor work coordination and support in providing testing solutions\nExternal\nClients\nProvide apt solutions and support as per the requirement\nDisplay\nLists the competencies required to perform this role effectively:\nFunctional Competencies/ Skill\nLeveraging Technology Knowledge of current and upcoming technology along with expertise in programming (automation, tools and systems) to build efficiencies and effectiveness in own function/ Client organization Competent\nProcess Excellence - Ability to follow the standards and norms to produce consistent results, provide effective control and reduction of risk Expert\nTechnical knowledge knowledge of various programming languages, tools, quality management standards and processes - Expert\n\nCompetency Levels\nFoundation\nKnowledgeable about the competency requirements. Demonstrates (in parts) frequently with minimal support and guidance.\nCompetent\nConsistently demonstrates the full range of the competency without guidance. Extends the competency to difficult and unknown situations as well.\nExpert\nApplies the competency in all situations and is serves as a guide to others as well.\nMaster\nCoaches others and builds organizational capability in the competency area. Serves as a key resource for that competency and is recognised within the entire organization.\nBehavioral Competencies\nFormulation & Prioritization\nInnovation\nManaging Complexity\nExecution Excellence\nPassion for Results\nDeliver / No. / Performance Parameter / Measure -\n1. Continuous Integration, Deployment & Monitoring of Software\n100% error free on boarding & implementation, throughput %, Adherence to the schedule/ release plan\n2. Quality & CSAT\nOn-Time Delivery, Manage software, Troubleshoot queries\nCustomer experience, completion of assigned certifications for skill upgradation\n3. MIS & Reporting\n100% on time MIS & report generation\nMandatory Skills: Python, GenAI, AWS\nPreferred Skills: NLP , AI/ML , LLM\nArchitect and implement Al solutions utilizing cutting-edge technologies like LLM, Langchain, and Machine Learning.\nAIML solution development in Azure using Python\nAbility to build and finetune the model to improve the performance\nCreate own technology if off-the-shelf technology is not solving the problem. E.g changes to traditional RAG approaches, finetune LLM, create architectures.\nUse experience and advise leadership and team of data scientists best approaches, architectures for complex ML use cases.\nLead from the front, responsible for coding, designing, and ensuring best practices & frameworks are adhered by the team.\nCreate end to end AI systems with responsible AI principles\nDevelop data pipelines using SQL to extract and transform data from Snowflake for Al model training and inference.\nPossess expertise in Natural Language Processing (NLP) & GenAI to integrate text-based data sources into the Al architecture.\nCollaborate with data scientists and engineers to ensure seamless integration of Al components into existing systems.\nResponsible for continuous communication about the team progress to keystakeholder.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'GenAI', 'NLP', 'Artificial Intelligence', 'LLM', 'AWS', 'Machine Learning', 'Python']",2025-06-12 06:14:05
Data Scientist-Artificial Intelligence,IBM,3 - 7 years,Not Disclosed,['Bengaluru'],"As an Data Engineer at IBM you will harness the power of data to unveil captivating stories and intricate patterns. You'll contribute to data gathering, storage, and both batch and real-time processing.\n\nCollaborating closely with diverse teams, you'll play an important role in deciding the most suitable data management systems and identifying the crucial data required for insightful analysis. As a Data Engineer, you'll tackle obstacles related to database integration and untangle complex, unstructured data sets.\n\n In this role, your responsibilities may include: \nImplementing and validating predictive models as well as creating and maintain statistical models with a focus on big data, incorporating a variety of statistical and machine learning techniques\nDesigning and implementing various enterprise search applications such as Elasticsearch and Splunk for client requirements\nWork in an Agile, collaborative environment, partnering with other scientists, engineers, consultants and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviours’.\nBuild teams or writing programs to cleanse and integrate data in an efficient and reusable manner, developing predictive or prescriptive models, and evaluating modelling results\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nProof of Concept (POC) DevelopmentDevelop POCs to validate and showcase the feasibility and effectiveness of the proposed AI solutions.\nHelp in showcasing the ability of Gen AI code assistant to refactor/rewrite and document code from one language to another\nDocument solution architectures, design decisions, implementation details, and lessons learned.\nStay up to date with the latest trends and advancements in AI, foundation models, and large language models.\nEvaluate emerging technologies, tools, and frameworks to assess their potential impact on solution design and implementation\n\n\nPreferred technical and professional experience\nExperience and working knowledge in COBOL & JAVA would be preferred\nHaving experience in Code generation, code matching & code translation leveraging LLM capabilities would be a Big plus\nDemonstrate a growth mindset to understand clients' business processes and challenges",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['elastic search', 'java', 'proof of concept', 'cobol', 'splunk', 'targetlink', 'simulink', 'data management', 'stateflow', 'sil', 'big data', 'can bus', 'matlab', 'python', 'c', 'predictive', 'machine learning', 'presales', 'autosar', 'code generation', 'rtw', 'rfi', 'embedded c', 'model based development', 'rfp']",2025-06-12 06:14:08
Data Scientist,Capgemini,4 - 7 years,Not Disclosed,['Pune'],"\n\nWorks in the area of Software Engineering, which encompasses the development, maintenance and optimization of software solutions/applications.\n1. Applies scientific methods to analyse and solve software engineering problems.\n2. He/she is responsible for the development and application of software engineering practice and knowledge, in research, design, development and maintenance.\n3. His/her work requires the exercise of original thought and judgement and the ability to supervise the technical and administrative work of other software engineers.\n4. The software engineer builds skills and expertise of his/her software engineering discipline to reach standard software engineer skills expectations for the applicable role, as defined in Professional Communities.\n5. The software engineer collaborates and acts as team player with other software engineers and stakeholders.\nWorks in the area of Software Engineering, which encompasses the development, maintenance and optimization of software solutions/applications.1. Applies scientific methods to analyse and solve software engineering problems.2. He/she is responsible for the development and application of software engineering practice and knowledge, in research, design, development and maintenance.3. His/her work requires the exercise of original thought and judgement and the ability to supervise the technical and administrative work of other software engineers.4. The software engineer builds skills and expertise of his/her software engineering discipline to reach standard software engineer skills expectations for the applicable role, as defined in Professional Communities.5. The software engineer collaborates and acts as team player with other software engineers and stakeholders.\n\n - Grade Specific \nIs highly respected, experienced and trusted. Masters all phases of the software development lifecycle and applies innovation and industrialization. Shows a clear dedication and commitment to business objectives and responsibilities and to the group as a whole. Operates with no supervision in highly complex environments and takes responsibility for a substantial aspect of Capgeminis activity. Is able to manage difficult and complex situations calmly and professionally. Considers the bigger picture when making decisions and demonstrates a clear understanding of commercial and negotiating principles in less-easy situations. Focuses on developing long term partnerships with clients. Demonstrates leadership that balances business, technical and people objectives. Plays a significant part in the recruitment and development of people.\n\n Skills (competencies) \n\nVerbal Communication",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'natural language processing', 'machine learning', 'deep learning', 'data science', 'gsm', 'rf engineering', 'data analysis', 'artificial intelligence', 'r', 'rf optimization', '3g', 'predictive modeling', 'lte', 'rf planning', 'statistics', 'drive test']",2025-06-12 06:14:10
Data Scientist - L3,Wipro,3 - 6 years,Not Disclosed,['Bengaluru'],"Wipro Limited (NYSEWIT, BSE507685, NSEWIPRO) is a leading technology services and consulting company focused on building innovative solutions that address clients’ most complex digital transformation needs. Leveraging our holistic portfolio of capabilities in consulting, design, engineering, and operations, we help clients realize their boldest ambitions and build future-ready, sustainable businesses. With over 230,000 employees and business partners across 65 countries, we deliver on the promise of helping our customers, colleagues, and communities thrive in an ever-changing world. For additional information, visit us at www.wipro.com.\n About The Role  \n\nRole Purpose\n\nThe purpose of the role is to define, architect and lead delivery of machine learning and AI solutions.\n\n ? \n\nDo\n\n1. Demand generation through support in Solution development\n\na. Support Go-To-Market strategy\n\ni. Contribute to development solutions, proof of concepts aligned to key offerings to enable solution led sales\n\nb. Collaborate with different colleges and institutes for research initiatives and provide data science courses\n\n2. Revenue generation through Building & operationalizing Machine Learning, Deep Learning solutions\n\na. Develop Machine Learning / Deep learning models for decision augmentation or for automation solutions\n\nb. Collaborate with ML Engineers, Data engineers and IT to evaluate ML deployment options\n\n3. Team Management\n\na. Talent Management\n\ni. Support on boarding and training to enhance capability & effectiveness\n\n ? \n\nDeliver\n\n\nNo.\n\nPerformance Parameter\n\nMeasure 1. Demand generation # PoC supported 2. Revenue generation through delivery Timeliness, customer success stories, customer use cases 3. Capability Building & Team Management # Skills acquired\n\n\n ? \n\n ? \nMandatory\n\nSkills:\nData Analysis.\n\nExperience3-5 Years.\n\nReinvent your world. We are building a modern Wipro. We are an end-to-end digital transformation partner with the boldest ambitions. To realize them, we need people inspired by reinvention. Of yourself, your career, and your skills. We want to see the constant evolution of our business and our industry. It has always been in our DNA - as the world around us changes, so do we. Join a business powered by purpose and a place that empowers you to design your own reinvention. Come to Wipro. Realize your ambitions. Applications from people with disabilities are explicitly welcome.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data analysis', 'machine learning', 'deep learning', 'data science', 'ml', 'python', 'natural language processing', 'scikit-learn', 'neural networks', 'ml deployment', 'data engineering', 'artificial intelligence', 'sql', 'tensorflow', 'r', 'predictive modeling', 'statistical modeling', 'statistics']",2025-06-12 06:14:13
Data Scientist,Ford,3 - 7 years,Not Disclosed,['Chennai'],"As a Data Scientist, you will be part of a high performing team working on exciting opportunities in AI/ML within Ford Credit. We are looking for a seasoned Data Scientist with proven expertise in implementing Machine Learning/Optimization solutions with familiarity in Generative AI and a good grasp of Statistics.\nDevelop Machine Learning (Supervised/Unsupervised learning), Neural Networks (ANN, CNN, RNN, LSTM, Decision tree, Encoder, Decoder), Natural Language Processing, Generative AI (LLMs, Lang Chain, RAG, Vector Database) .\nHands-on Expertise in Python programming (OOPs concepts), SQL (relational/non-relational databases), experience in handling various data science libraries (Pandas, NumPy, SciPy, Sklearn, TensorFlow, Keras, Pytorch, etc.) would be a necessary requirement.\nExposure to Cloud technologies (e.g., Google Cloud/AWS/Azure), including executing Machine Learning algorithms on Cloud is necessary.\nExposure to Generative AI technologies.\nProfessional Qualification:\nPotential candidates should possess 3 to 7 years of working experience as a Data Scientist.\nBE/MSc/ MTech /ME/PhD (Computer Science/Maths, Statistics).\nPossess a strong analytical mindset and be very comfortable with data.\nExperience with handling both relational and non-relational data.\nHands-on with analytics methods (descriptive / predictive / prescriptive) , Statistical Analysis, Probability and Data Visualization tools (Python-Matplotlib, Seaborn).\nBackground of Computer Science with excellent Data Science working experience.\nTechnical Experience:\nDevelop Machine Learning (Supervised/Unsupervised learning), Neural Networks (ANN, CNN, RNN, LSTM, Decision tree, Encoder, Decoder), Natural Language Processing, Generative AI (LLMs, Lang Chain, RAG, Vector Database) .\nHands-on Expertise in Python programming (OOPs concepts), SQL (relational/non-relational databases), experience in handling various data science libraries (Pandas, NumPy, SciPy, Sklearn, TensorFlow, Keras, Pytorch, etc.) would be a necessary requirement.\nExposure to Cloud technologies (e.g., Google Cloud/AWS/Azure), including executing Machine Learning algorithms on Cloud is necessary.\nExposure to Generative AI technologies.\nAbility to scope the problem statement, data preparation, training and making the AI/ML model production ready.\nWork with business partners to understand the problem statement, translate the same into analytical problem.\nAbility to manipulate structured and unstructured data.\nDevelop, test and improve existing machine learning models.\nAnalyse large and complex data sets to derive valuable insights.\nResearch and implement best practices to enhance existing machine learning infrastructure. Develop prototypes for future exploration.\nDesign and evaluate approaches for handling large volume of real data streams.\nAbility to determine appropriate analytical methods to be used.\nCollaborate with data engineers, solutions architects, application engineers, and product teams across time zones to develop data and model pipelines",Industry Type: Automobile,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Computer science', 'data science', 'Neural networks', 'Analytical', 'Machine learning', 'Natural language processing', 'data visualization', 'Analytics', 'SQL', 'Python']",2025-06-12 06:14:15
Data Scientist-Artificial Intelligence,IBM,3 - 7 years,Not Disclosed,['Bengaluru'],"An AI Data Scientist at IBM is not just a job title - it’s a mindset. You’ll leverage the watsonx,AWS Sagemaker,Azure Open AI platform to co-create AI value with clients, focusing on technology patterns to enhance repeatability and delight clients.\n\nWe are seeking an experienced and innovative AI Data Scientist to be specialized in foundation models and large language models. In this role, you will be responsible for architecting and delivering AI solutions using cutting-edge technologies, with a strong focus on foundation models and large language models. You will work closely with customers, product managers, and development teams to understand business requirements and design custom AI solutions that address complex challenges. Experience with tools like Github Copilot, Amazon Code Whisperer etc. is desirable.\n\nSuccess is our passion, and your accomplishments will reflect this, driving your career forward, propelling your team to success, and helping our clients to thrive.\n\nDay-to-Day Duties:\nProof of Concept (POC) DevelopmentDevelop POCs to validate and showcase the feasibility and effectiveness of the proposed AI solutions. Collaborate with development teams to implement and iterate on POCs, ensuring alignment with customer requirements and expectations.\nHelp in showcasing the ability of Gen AI code assistant to refactor/rewrite and document code from one language to another, particularly COBOL to JAVA through rapid prototypes/ PoC\nDocumentation and Knowledge SharingDocument solution architectures, design decisions, implementation details, and lessons learned. Create technical documentation, white papers, and best practice guides. Contribute to internal knowledge sharing initiatives and mentor new team members.\nIndustry Trends and InnovationStay up to date with the latest trends and advancements in AI, foundation models, and large language models. Evaluate emerging technologies, tools, and frameworks to assess their potential impact on solution design and implementation\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nStrong programming skills, with proficiency in Python and experience with AI frameworks such as TensorFlow, PyTorch, Keras or Hugging Face. Understanding in the usage of libraries such as SciKit Learn, Pandas, Matplotlib, etc. Familiarity with cloud platforms (e.g. Kubernetes, AWS, Azure, GCP) and related services is a plus.\nExperience and working knowledge in COBOL & JAVA would be preferred\no Having experience in Code generation, code matching & code translation leveraging LLM capabilities would be a Big plus (e.g. Amazon Code Whisperer, Github Copilot etc.) Soft\n\nSkills:\nExcellent interpersonal and communication skills. Engage with stakeholders for analysis and implementation. Commitment to continuous learning and staying updated with advancements in the field of AI.\nGrowth mindsetDemonstrate a growth mindset to understand clients' business processes and challenges.\nExperience in python and pyspark will be added advantage\n\n\nPreferred technical and professional experience\nExperienceProven experience in designing and delivering AI solutions, with a focus on foundation models, large language models, exposure to open source, or similar technologies. Experience in natural language processing (NLP) and text analytics is highly desirable. Understanding of machine learning and deep learning algorithms.\nStrong track record in scientific publications or open-source communities\nExperience in full AI project lifecycle, from research and prototyping to deployment in production environments",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'machine learning', 'tensorflow', 'pytorch', 'keras', 'kubernetes', 'github', 'natural language processing', 'scikit-learn', 'pyspark', 'microsoft azure', 'artificial intelligence', 'text analytics', 'pandas', 'deep learning', 'java', 'code generation', 'cobol', 'gcp', 'matplotlib', 'aws']",2025-06-12 06:14:18
Hdfc Bank - Digital Banking - Data Scientist - Generative AI,Hdfc Bank,4 - 9 years,Not Disclosed,['Mumbai (All Areas)'],"Role - Digital Banking-Data Scientist-Digital Experience Analytics\n\nLocation - Mumbai\nGrade - Deputy Manager to Senior Manager\nMinimum 4 years experience\n\nJob Purpose:\nThe Data Scientist will design, develop and deploy advanced AI models to drive digital transformation, enhance customer experience, optimize operations and mitigate risks. This role requires a sound understanding of AI/ML techniques and their application to challenges such as customer engagement and process automation.",,,,"['Data Science', 'Artificial Intelligence', 'Machine Learning', 'Generative AI', 'Risk Analytics', 'Data Analytics']",2025-06-12 06:14:20
Data Scientist,"NTT DATA, Inc.",2 - 5 years,Not Disclosed,['Bengaluru'],"Your day at NTT DATA\nThe Senior Data Scientist is an advanced subject matter expert, tasked with taking accountability in the adoption of data science and analytics within the organization.\n\nThe primary responsibility of this role is to participate in the creation and delivery of data-driven solutions that add business value using statistical models, machine learning algorithms, data mining, and visualization techniques.\n\nWhat youll be doing\n\nKey Responsibilities:\nDesigns, develops, and programs methods, processes, and systems to consolidate and analyze unstructured, diverse big data sources to generate actionable insights and solutions for client services and product enhancement.\nDesigns and enhances data collection procedures to include information that is relevant for building analytic systems.\nResponsible for ensuring that data used for analysis is processed, cleaned and, integrally verified and build algorithms necessary to find meaningful answers.\nDesigns and codes software programs, algorithms, and automated processes to cleanse, integrate and evaluate large datasets from multiple disparate sources\nProvides meaningful insights from large data and metadata sources; interprets and communicates insights and findings from analysis and experiments to product, service, and business managers.\nDirects scalable and highly available applications leveraging the latest tools and technologies.\nAccountable for creatively visualizing and effectively communicating results of data analysis, insights, and ideas in a variety of formats to key decision-makers within the business.\nCreates SQL queries for the analysis of data and visualizes the output of the models.\nResponsible for ensuring that industry standards best practices are applied to development activities.\nKnowledge and Attributes:\nAdvanced understanding of data modelling, statistical methods and machine learning techniques.\nStrong ability to thrive in a dynamic, fast-paced environment.\nStrong quantitative and qualitative analysis skills.\nDesire to acquire more knowledge to keep up to speed with the ever-evolving field of data science.\nCuriosity to sift through data to find answers and more insights.\nAdvanced understanding of the information technology industry within a matrixed organization and the typical business problems such organizations face.\nStrong ability to translate technical findings clearly and fluently to non-technical team business stakeholders to enable informed decision-making.\nStrong ability to create a storyline around the data to make it easy to interpret and understand.\nSelf-driven and able to work independently yet acts as a team player.\nAcademic Qualifications and Certifications:\nBachelors degree or equivalent in Data Science, Business Analytics, Mathematics, Economics, Engineering, Computer Science or a related field.\nRelevant programming certification preferred.\nAgile certification preferred.\nRequired Experience:\nAdvanced demonstrated experience in a data science position in a corporate environment and/or related industry.\nAdvanced demonstrated experience in statistical modelling and data modelling, machine learning, data mining, unstructured data analytics, natural language processing.\nAdvanced demonstrated experience in programming languages (R, Python, etc.).\nAdvanced demonstrated experience working with and creating data architectures.\nAdvanced demonstrated experience with extracting, cleaning, and transforming data and working with data owners to understand the data.\nAdvanced demonstrated experience visualizing and/or presenting data for stakeholder use and reuse across the business.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data science', 'R', 'data modelling', 'data mining', 'statistical modelling', 'machine learning', 'Python', 'SQL']",2025-06-12 06:14:22
Data Scientist-MLOps/LLMOps,IBM,3 - 6 years,Not Disclosed,['Mumbai'],"Role Overview :\nHiring an ML Engineer with experience in Cloudera ML to support end-to-end model development, deployment, and monitoring on the CDP platform.\n\n Key Responsibilities :\nDevelop and deploy models using CML workspaces\nBuild CI/CD pipelines for ML lifecycle\nIntegrate with governance and monitoring tools\nEnable secure model serving via REST APIs\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\n\n Skills Required :\nExperience in Cloudera ML, Spark MLlib, or scikit-learn\nML pipeline automation (MLflow, Airflow, or equivalent)\nModel governance, lineage, and versioning\nAPI exposure for real-time inference",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['continuous integration', 'rest', 'scikit-learn', 'ci/cd', 'ml', 'cloudera', 'python', 'deploying models', 'natural language processing', 'airflow', 'neural networks', 'machine learning', 'sql', 'deep learning', 'tensorflow', 'data science', 'spark', 'spark mllib', 'model development', 'keras']",2025-06-12 06:14:24
Data Scientist,THERMAX,2 - 3 years,Not Disclosed,['Pune'],Job Details:\nWe are seeking a highly motivated and enthusiastic Junior Data Scientist with 2-3 years of experience to join our data science team. This role offers an exciting opportunity to contribute to both traditional Machine Learning projects for our commercial IoT platform and cutting-edge Generative AI initiatives.\n\nExperience,,,,"['Tensorflow', 'Manufacturing', 'Machine Learning', 'IOT', 'Python', 'Pytorch', 'Data Science', 'Aiml', 'Scikit-Learn', 'Ml']",2025-06-12 06:14:28
Data Scientist-Artificial Intelligence,IBM,5 - 7 years,Not Disclosed,['Mumbai'],"Work with broader team to build, analyze and improve the AI solutions.\nYou will also work with our software developers in consuming different enterprise applications\n\n\nRequired education\nBachelor's Degree\n\nPreferred education\nMaster's Degree\n\nRequired technical and professional expertise\nResource should have 5-7 years of experience. Sound knowledge of Python and should know how to use the ML related services.\nProficient in Python with focus on Data Analytics Packages.\nStrategy Analyse large, complex data sets and provide actionable insights to inform business decisions. Strategy Design and implementing data models that help in identifying patterns and trends. Collaboration Work with data engineers to optimize and maintain data pipelines.\nPerform quantitative analyses that translate data into actionable insights and provide analytical, data-driven decision-making. Identify and recommend process improvements to enhance the efficiency of the data platform. Develop and maintain data models, algorithms, and statistical models\n\n\nPreferred technical and professional experience\nExperience with conversation analytics. Experience with cloud technologies\nExperience with data exploration tools such as Tableu",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['algorithms', 'python', 'data analytics', 'tableau', 'ml', 'hive', 'data analysis', 'natural language processing', 'pyspark', 'data warehousing', 'machine learning', 'artificial intelligence', 'sql', 'pandas', 'deep learning', 'java', 'data science', 'spark', 'kafka', 'hadoop', 'big data', 'aws', 'etl']",2025-06-12 06:14:31
Specialist Data Scientist,NICE,8 - 11 years,Not Disclosed,['Pune'],"So, what’s the role all about?\nNICE provides state-of-the-art enterprise level AI and analytics for all forms of business communications between speech and digital.   We are a world class research team developing new algorithms and approaches to help companies with solving critical issues such as identifying their best performing agents, preventing fraud, categorizing customer issues, and determining overall customer satisfaction.  If you have interacted with a major contact center in the last decade, it is very likely we have processed your call. \nThe research group partners with all areas of NICE’s business to scale out the delivery of new technology and AI models to customers around the world that are tailored to their company, industry, and language needs.",,,,"['python', 'confluence', 'natural language processing', 'presentation skills', 'big data technologies', 'pyspark', 'microsoft azure', 'bert', 'machine learning', 'sql', 'tensorflow', 'data science', 'gcp', 'pytorch', 'machine learning algorithms', 'aws', 'big data', 'communication skills', 'statistics', 'jira']",2025-06-12 06:14:33
Data Scientist - SCB,Wipro,8 - 10 years,Not Disclosed,['Chennai'],"About The Role  \n\nRole Purpose\n\nThe purpose of the role is to create exceptional architectural solution design and thought leadership and enable delivery teams to provide exceptional client engagement and satisfaction.\n\n\n ? \n\nMandatory Skills\n\nData Science, ML, DL, NLP or Computer Vision, Python, Tensorflow, Pytorch, Django, PostgreSQL\n\nPreferred Skills\n\nGen AI, LLM, RAG, Lanchain, Vector DB, Azure Cloud, MLOps, Banking exposure\n\n\n ? \n\n \n\n3.Competency Building and Branding  \nEnsure completion of necessary trainings and certifications\nDevelop Proof of Concepts (POCs),case studies, demos etc. for new growth areas based on market and customer research\nDevelop and present a point of view of Wipro on solution design and architect by writing white papers, blogs etc.\nAttain market referencability and recognition through highest analyst rankings, client testimonials and partner credits\nBe the voice of Wipro’s Thought Leadership by speaking in forums (internal and external)\nMentor developers, designers and Junior architects in the project for their further career development and enhancement\nContribute to the architecture practice by conducting selection interviews etc\n\n\n ? \n\nMandatory\nStrong understanding of Data Science, machine learning and deep learning principles and algorithms.\nProficiency in programming languages such as Python, TensorFlow, and PyTorch.\nAbility to work with large datasets and knowledge of data preprocessing techniques.\nStrong Backend Python developer\nExperience in applying machine learning techniques,\n\nNatural Language Processing or Computer Vision using TensorFlow, Pytorch\nBuild and deploy end to end ML models and leverage metrics to support predictions, recommendations, search, and growth strategies\nExpert in applying ML techniques such asclassification, clustering, deep learning, optimization methods, supervised and unsupervised techniques\nOptimize model performance and scalability for real-time inference and deployment.\nExperiment with different hyperparameters and model configurations to improve AI model quality.\nEnsure AI ML solutions are developed, and validations are performed in accordance with Responsible AI guidelines.\n\n\n ? \n\n \n\n4.Team Management  \n\n Resourcing  \nAnticipating new talent requirements as per the market/ industry trends or client requirements\nHire adequate and right resources for the team\nTalent Management\nEnsure adequate onboarding and training for the team members to enhance capability & effectiveness\nBuild an internal talent pool and ensure their career progression within the organization\nManage team attrition\nDrive diversity in leadership positions\nPerformance Management\nSet goals for the team, conduct timely performance reviews and provide constructive feedback to own direct reports\nEnsure that the Performance Nxt is followed for the entire team\nEmployee Satisfaction and Engagement\nLead and drive engagement initiatives for the team\nTrack team satisfaction scores and identify initiatives to build engagement within the team\nMandatory\n\nSkills:\nGenerative AI.\n\nExperience8-10 Years.\n\nReinvent your world. We are building a modern Wipro. We are an end-to-end digital transformation partner with the boldest ambitions. To realize them, we need people inspired by reinvention. Of yourself, your career, and your skills. We want to see the constant evolution of our business and our industry. It has always been in our DNA - as the world around us changes, so do we. Join a business powered by purpose and a place that empowers you to design your own reinvention. Come to Wipro. Realize your ambitions. Applications from people with disabilities are explicitly welcome.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'natural language processing', 'machine learning', 'deep learning', 'tensorflow', 'algorithms', 'dl', 'azure cloud', 'sql', 'gen', 'django', 'data science', 'postgresql', 'predictive modeling', 'computer vision', 'pytorch', 'statistical modeling', 'vector', 'clustering', 'db', 'ml', 'statistics']",2025-06-12 06:14:35
"Data Scientist with EDA, Python, SQL and Business Analysis",Cognizant,8 - 11 years,Not Disclosed,['Hyderabad'],"Job Summary\nWe are seeking a highly skilled Data Scientist with Business analysis with good hands on in Python , SQL , EDA Data Visualization with 3 to 12+ years of experience\nKey Responsibilities:\nPerform exploratory data analysis (EDA) to uncover trends, patterns, and insights.\nDevelop and maintain dashboards and reports to visualize key business metrics.\nCollaborate with cross-functional teams to gather requirements and deliver data-driven solutions.",,,,"['python', 'eda', 'data analysis', 'data management', 'modeling', 'analytical', 'data manipulation', 'query', 'predictive analytics', 'microsoft azure', 'business analysis', 'cloud platforms', 'business analytics', 'machine learning', 'dashboards', 'sql', 'analytics', 'gcp', 'statistical modeling', 'data visualization', 'aws']",2025-06-12 06:14:38
MLOps Engineer / Data Scientist,CEVA Logistics,3 - 7 years,Not Disclosed,['Mumbai'],"CEVA Logistics provides global supply chain solutions to connect people, products, and providers all around the world\nPresent in 170+ countries and with more than 110,000 employees spread over 1,500 sites, we are proud to be a Top 5 global 3PL\nWe believe that our employees are the key to our success\nWe want to engage and empower our diverse, global team to co-create value with our customers through our solutions in contract logistics and air, ocean, ground, and finished vehicle transport\nThat is why CEVA Logistics offers a dynamic and exceptional work environment that fosters personal growth, innovation, and continuous improvement\nDARE TO GROW! Join CEVA Logistics, and you will be part of a team that values imagination and continued learning and is committed to excellence in everything we do\nJoin us in our mission to shape the future of global logistics\nAs we continue growing at a fast pace, will you Dare to Grow with us\nJoin the forefront of AI-driven logistics innovation as a MLOps Engineer/Data Scientist at CEVA Logistics, a global leader in supply chain solutions\nYOUR ROLE\nAs a MLOps Engineer/Data scientist, you will play a pivotal role in CEVA LogisticsGlobal IT Data & Digital organization, reporting directly to the Global IT Data & Digital BI & Advanced Analytics AI & Data Science Manager\nThis hybrid role combines the expertise of data science with operational practices focused on the industrialization and deployment of AI and machine learning solutions at scale\nYou will collaborate with cross-functional teams in IT, Corporate Support Functions, Business Development, and Operations to drive the development and industrialization of machine learning models\nYour work will ensure that AI-driven insights and use cases are seamlessly integrated, scalable, and continuously optimized to provide ongoing value to CEVA Logistics and its customers\nThis position requires expertise not only in data science and machine learning but also in the processes that turn innovative solutions into robust, enterprise-grade systems\nThis position is open in Spain (Madrid / Barcelona) or Poland (Warsaw) or India (Mumbai)\nWHAT ARE YOU GOING TO DO\nExpertise in data science and machine learning techniques with experience in designing, building, and deploying models to solve business challenges\nProficiency in industrializing use cases, focusing on turning proof-of-concept models into full-scale, production-ready AI solutions that are both scalable and sustainable\nHands-on experience with MLOps tools and technologies (such as Kubernetes, Docker, Jenkins, MLflow, TensorFlow, etc-) for automating the deployment, monitoring, and maintenance of machine learning models\nStrong experience with cloud platforms (AWS, Google Cloud, Azure) and modern deployment architectures for AI/ML workloads\nAdvanced proficiency in Python, SQL, and experience with big data technologies to work with large datasets\nStrong understanding of model governance, model monitoring, and model retraining to ensure AI solutions deliver consistent and ongoing business value\nAbility to work collaboratively across cross-functional teams to ensure smooth transitions from development to deployment and operationalization\nExcellent communication skills to explain complex technical concepts to non-technical stakeholders and influence business strategies through AI-driven insights\nWHAT ARE WE LOOKING FOR\nA minimum of 5 years of professional experience with proven results in AI, Data Science & MLOps\nMasters degree in computer science, Data Science, Engineering, Mathematics, or a related field\nAdvanced certifications in AI, Data Science, Machine Learning, or MLOps would be plus\nProven ability to collaborate effectively with cross-functional teams, including business stakeholders, IT, and operations, to translate business needs into technical solutions\nStrong written and verbal communication skills, with the ability to explain complex technical concepts to non-technical audiences\nTechnical Expertise:\nData Science:\nSolid understanding of machine learning algorithms, statistical methods, and predictive modeling\nProficiency in Python and libraries such as TensorFlow, Keras, scikit-learn, XGBoost, and PyTorch\nMLOps:\nHands-on experience implementing MLOps practices for model deployment, automation, and continuous integration/continuous delivery (CI/CD)\nFamiliarity with tools like MLflow, Kubeflow, Jenkins, Docker, Kubernetes, and Terraform for automating machine learning pipelines\nData Integration & Management:\nProficient in using Snowflake for data integration, storage, and processing at scale\nExperience with Dataiku for creating and managing end-to-end data science workflows\nBig Data Technologies:\nFamiliarity with big data platforms and tools to process and analyze large datasets\nData Visualization & Reporting:\nExpertise in using Qlik Sense, Streamlite, or other BI, webapp & data visualization tools for developing interactive and insightful dashboards and reports to communicate complex results to non-technical stakeholders\nModel Development & Industrialization:\nExperience in transitioning machine learning models from prototype to production at scale, ensuring robustness, scalability, and reliability in real-world applications\nFamiliarity with best practices in model versioning, testing, monitoring, and retraining to maintain model accuracy and performance over time\nCloud Platforms:\nStrong experience with cloud-based platforms like AWS, Google Cloud and Azure for deploying, managing, and scaling AI/ML models\nCollaboration & Communication:\nProven ability to collaborate effectively with cross-functional teams, including business stakeholders, IT, and operations, to translate business needs into technical solutions\nStrong written and verbal communication skills, with the ability to explain complex technical concepts to non-technical audiences\nProblem Solving & Innovation:\nStrong analytical and problem-solving skills, with a creative mindset for leveraging AI and machine learning to solve business problems and drive innovation\nAbility to stay up to date with the latest developments in AI, machine learning, and MLOps, and apply cutting-edge techniques to business challenges\nSoft Skills:\nSelf-motivated, adaptable, and able to thrive in a fast-paced and dynamic work environment\nStrong attention to detail with a focus on quality, accuracy, and reliability in all work\nEffective time management and organizational skills, with the ability to handle multiple projects simultaneously and meet deadlines\nAdditional Information:\nThis position offers the opportunity to work with both existing tools at CEVA and new, modern tools available in the Cloud, leveraging our newly developed CEVA Data Platform\nThe role requires a proactive individual with a strong commitment to driving data-driven strategies and solutions within the organization\nWHAT DO WE HAVE TO OFFER\nWith a genuine culture of recognition, we want our employees to grow, develop and be part of our journey\nYou have access to the CEVA academy for training\nYou receive healthcare benefits, reimbursement of the transportation card (50%) and meal vouchers for each working day\nWe are a team in every sense, and we support each other and work collaboratively to achieve our goals together\nIt is our goal that you will be compensated for your hard work and commitment, so if youd like to work for one of the top Logistics providers in the world then lets work together to help you find your new role",Industry Type: Courier / Logistics,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['model monitoring', 'snowflake', 'python', 'predictive modeling', 'cloud platforms', 'machine learning algorithms', 'sql', 'communication skills']",2025-06-12 06:14:40
"Data Analyst, Staff",Qualcomm,4 - 7 years,Not Disclosed,['Bengaluru'],"Job Area: Miscellaneous Group, Miscellaneous Group > Data Analyst\n \n\nQualcomm Overview: \nQualcomm is a company of inventors that unlocked 5G ushering in an age of rapid acceleration in connectivity and new possibilities that will transform industries, create jobs, and enrich lives. But this is just the beginning. It takes inventive minds with diverse skills, backgrounds, and cultures to transform 5Gs potential into world-changing technologies and products. This is the Invention Age - and this is where you come in.\n\nGeneral Summary:\n\nAbout the Team\n\nQualcomm's People Analytics team plays a crucial role in transforming data into strategic workforce insights that drive HR and business decisions. As part of this lean but high-impact team, you will have the opportunity to analyze workforce trends, ensure data accuracy, and collaborate with key stakeholders to enhance our data ecosystem. This role is ideal for a generalist who thrives in a fast-paced, evolving environment""”someone who can independently conduct data analyses, communicate insights effectively, and work cross-functionally to enhance our People Analytics infrastructure.\n\nWhy Join Us\n\n\nEnd-to-End ImpactWork on the full analytics cycle""”from data extraction to insight generation""”driving meaningful HR and business decisions.\n\n\nCollaboration at ScalePartner with HR leaders, IT, and other analysts to ensure seamless data integration and analytics excellence.\n\n\nData-Driven CultureBe a key player in refining our data lake, ensuring data integrity, and influencing data governance efforts.\n\n\nProfessional GrowthGain exposure to multiple areas of people analytics, including analytics, storytelling, and stakeholder engagement.\n\n\nKey Responsibilities\n\n\nPeople Analytics & Insights\nAnalyze HR and workforce data to identify trends, generate insights, and provide recommendations to business and HR leaders.\nDevelop thoughtful insights to support ongoing HR and business decision-making.\nPresent findings in a clear and compelling way to stakeholders at various levels, including senior leadership.\n\n\nData Quality & Governance\nEnsure accuracy, consistency, and completeness of data when pulling from the data lake and other sources.\nIdentify and troubleshoot data inconsistencies, collaborating with IT and other teams to resolve issues.\nDocument and maintain data definitions, sources, and reporting standards to drive consistency across analytics initiatives.\n\n\nCollaboration & Stakeholder Management\nWork closely with other analysts on the team to align methodologies, share best practices, and enhance analytical capabilities.\nAct as a bridge between People Analytics, HR, and IT teams to define and communicate data requirements.\nPartner with IT and data engineering teams to improve data infrastructure and expand available datasets.\n\n\nQualifications\n\nRequired4-7 years experience in a People Analytics focused role\n\n\nAnalytical & Technical Skills\nStrong ability to analyze, interpret, and visualize HR and workforce data to drive insights.\nExperience working with large datasets and ensuring data integrity.\nProficiency in Excel and at least one data visualization tool (e.g., Tableau, Power BI).\n\n\nCommunication & Stakeholder Management\nAbility to communicate data insights effectively to both technical and non-technical audiences.\nStrong documentation skills to define and communicate data requirements clearly.\nExperience collaborating with cross-functional teams, including HR, IT, and business stakeholders.\n\n\nPreferred:\n\n\nTechnical Proficiency\nExperience with SQL, Python, or R for data manipulation and analysis.\nFamiliarity with HR systems (e.g., Workday) and cloud-based data platforms.\n\n\nPeople Analytics Expertise\nPrior experience in HR analytics, workforce planning, or related fields.\nUnderstanding of key HR metrics and workforce trends (e.g., turnover, engagement, diversity analytics).\n\n\nAdditional Information\nThis is an office-based position (4 days a week onsite) with possible locations that may include India and Mexico",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['sql', 'people analytics', 'documentation', 'tableau', 'data integration tools', 'hiring', 'data warehousing', 'data architecture', 'sourcing', 'jquery', 'staffing', 'plsql', 'oracle 10g', 'java', 'etl tool', 'html', 'etl', 'mongodb', 'python', 'oracle', 'power bi', 'hrsd', 'r', 'node.js', 'hr analytics', 'angularjs']",2025-06-12 06:14:43
Associate Data Engineer,Amgen Inc,0 - 3 years,Not Disclosed,['Hyderabad'],"What you will do\nIn this vital role We are seeking a Associate Data Engineer to design, build, and maintain scalable data solutions that drive business insights. You will work with large datasets, cloud platforms (AWS preferred), and big data technologies to develop ETL pipelines, ensure data quality, and support data governance initiatives.\nDevelop and maintain data pipelines, ETL/ELT processes, and data integration solutions.\nDesign and implement data models, data dictionaries, and documentation for accuracy and consistency.\nEnsure data security, privacy, and governance standard processes.\nUse Databricks, Apache Spark (PySpark, SparkSQL), AWS, Redshift, for scalable data processing.\nCollaborate with cross-functional teams to understand data needs and deliver actionable insights.\nOptimize data pipeline performance and explore new tools for efficiency.\nFollow best practices in coding, testing, and infrastructure-as-code (CI/CD, version control, automated testing).\n\n\nWhat we expect of you\nWe are all different, yet we all use our unique contributions to serve patients. Strong problem-solving, critical thinking, and communication skills.\nAbility to collaborate effectively in a team setting.\nProficiency in SQL, data analysis tools, and data visualization.\nHands-on experience with big data technologies (Databricks, Apache Spark, AWS, Redshift ).\nExperience with ETL tools, workflow orchestration, and performance tuning for big data.\nBasic Qualifications:\nBachelors degree and 0 to 3 years of experience OR Diploma and 4 to 7 years of experience in Computer science, IT or related field.\nPreferred Qualifications:\nKnowledge of data modeling, warehousing, and graph databases\nExperience with Python, SageMaker, and cloud data platforms.\nAWS Certified Data Engineer or Databricks certification preferred.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'data analysis', 'data modeling', 'data warehousing', 'data visualization', 'Databricks', 'ETL', 'AWS', 'SQL', 'Apache Spark', 'Python']",2025-06-12 06:14:45
Data Analyst - L3,Wipro,3 - 5 years,Not Disclosed,['Hyderabad'],"Role Purpose\nThe purpose of this role is to interpret data and turn into information (reports, dashboards, interactive visualizations etc) which can offer ways to improve a business, thus affecting business decisions.\nDo\n1. Managing the technical scope of the project in line with the requirements at all stages\na. Gather information from various sources (data warehouses, database, data integration and modelling) and interpret patterns and trends\nb. Develop record management process and policies\nc. Build and maintain relationships at all levels within the client base and understand their requirements.\nd. Providing sales data, proposals, data insights and account reviews to the client base\ne. Identify areas to increase efficiency and automation of processes\nf. Set up and maintain automated data processes\ng. Identify, evaluate and implement external services and tools to support data validation and cleansing.\nh. Produce and track key performance indicators\n2. Analyze the data sets and provide adequate information\na. Liaise with internal and external clients to fully understand data content\nb. Design and carry out surveys and analyze survey data as per the customer requirement\nc. Analyze and interpret complex data sets relating to customers business and prepare reports for internal and external audiences using business analytics reporting tools\nd. Create data dashboards, graphs and visualization to showcase business performance and also provide sector and competitor benchmarking\ne. Mine and analyze large datasets, draw valid inferences and present them successfully to management using a reporting tool\nf. Develop predictive models and share insights with the clients as per their requirement\n\nDeliver\n\nNoPerformance ParameterMeasure1.Analyses data sets and provide relevant information to the clientNo. Of automation done, On-Time Delivery, CSAT score, Zero customer escalation, data accuracy\n\nMandatory Skills: Business Analyst/ Data Analyst(Media). Experience: 3-5 Years.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data analysis', 'data validation', 'data mining', 'business analysis', 'data warehousing', 'business analytics', 'dbms', 'dashboards', 'sales', 'analytics reporting', 'reporting tools', 'data integration', 'digital transformation']",2025-06-12 06:14:48
Senior Associate Data Scientist,Amgen Inc,1 - 3 years,Not Disclosed,['Hyderabad'],"What you will do\nIn this vital role you will identify trends, root causes, and potential improvements in our products and processes, ensuring that patient voices are heard and addressed with utmost precision.\nAs the Sr Associate Data Scientist at Amgen, you will be responsible for developing and deploying basic machine learning, operational research, semantic analysis, and statistical methods to uncover structure in large data sets. This role involves creating analytics solutions to address customer needs and opportunities.\nCollect, clean, and manage large datasets related to product performance and patient complaints.\nEnsure data integrity, accuracy, and accessibility for further analysis.\nDevelop and maintain databases and data systems for storing patient complaints and product feedback.\nAnalyze data to identify patterns, trends, and correlations in patient complaints and product issues.\nUse advanced statistical methods and machine learning techniques to uncover insights and root causes.\nDevelop analytics or predictive models to foresee potential product issues and patient concerns to address customer needs and opportunities.\nPrepare comprehensive reports and visualizations to communicate findings to key collaborators.\nPresent insights and recommendations to cross-functional teams, including product development, quality assurance, and customer service.\nCollaborate with regulatory and compliance teams to ensure adherence to healthcare standards and regulations.\nFind opportunities for product enhancements and process improvements based on data analysis.\nWork with product complaint teams to implement changes and monitor their impact.\nStay abreast of industry trends, emerging technologies, and standard methodologies in data science and healthcare analytics.\nEvaluate data to support product complaints.\nWork alongside software developers and software engineers to translate algorithms into commercially viable products and services.\nWork in technical teams in development, deployment, and application of applied analytics, predictive analytics, and prescriptive analytics.\nPerform exploratory and targeted data analyses using descriptive statistics and other methods.\nWork with data engineers on data quality assessment, data cleansing and data analytics\nGenerate reports, annotated code, and other projects artifacts to document, archive, and communicate your work and outcomes.\n\nBasic Qualifications:\nMasters degree and 1 to 3 years of Data Science and with one or more analytic software tools or languages, and data visualization tools experience OR\nBachelors degree and 3 to 5 years of Data Science and with one or more analytic software tools or languages, and data visualization tools experience OR\nDiploma and 7 to 9 years of Data Science and with one or more analytic software tools or languages, and data visualization tools experience\nPreferred Qualifications:\nDemonstrated skill in the use of applied analytics, descriptive statistics, feature extraction and predictive analytics on industrial datasets.\nExperience in statistical techniques and hypothesis testing, experience with regression analysis, clustering and classification.\nExperience in analyzing time-series data for forecasting and trend analysis.\nExperience with Data Bricks platform for data analytics.\nExperience working with healthcare data, including patient complaints, product feedback, and regulatory requirements.",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'data bricks', 'hypothesis testing', 'predictive analytics', 'data visualization', 'machine learning', 'statistics']",2025-06-12 06:14:50
Data Analyst - Gurugram,Infosys,5 - 10 years,Not Disclosed,"['Chennai', 'Bengaluru', 'PAN INDIA']","Responsibilities:\nUnderstand architecture requirements and ensure effective design, development, validation, and support activities.\nAnalyze user requirements, envisioning system features and functionality.\nIdentify bottlenecks and bugs, and recommend system solutions by comparing advantages and disadvantages of custom development.\nContribute to team meetings, troubleshooting development and production problems across multiple environments and operating platforms.\nEnsure effective design, development, validation, and support activities for Big Data solutions.\nTechnical and Professional Requirements:\nSkills:\nProficiency in Scala, Spark, Hive, and Kafka.\nIn-depth knowledge of design issues and best practices.\nSolid understanding of object-oriented programming.\nFamiliarity with various design, architectural patterns, and software development processes.\nExperience with both external and embedded databases.\nCreating database schemas that represent and support business processes.\nImplementing automated testing platforms and unit tests.\nPreferred Skills:\nTechnology -> Big Data -> Scala, Spark, Hive, Kafka\nAdditional Responsibilities:\nCompetencies:\nGood verbal and written communication skills.\nAbility to communicate with remote teams effectively.\nHigh flexibility to travel.\nEducational Requirements:Master of Computer Applications, Master of Technology, Master of Engineering, MSc, Bachelor of Technology, Bachelor of Computer Applications, Bachelor of Computer Science, Bachelor of Engineering",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Spark', 'Hive', 'Hadoop', 'Big Data', 'Kafka']",2025-06-12 06:14:52
Lead Data Scientist,Bizopp Management Consultants,11 - 18 years,25-35 Lacs P.A.,['Chennai'],"• Proficiency in Python and SQL for data extraction, manipulation, and analysis\n\n• Exploratory data analysis (EDA), developing, and deploying machine learning models\n\n• Expertise in deploying ML models on cloud platforms such as AWS or Azure",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['EDA', 'Data Scientist', 'Python', 'SQL', 'Azure', 'Exploratory data analysis', 'Machine Learning', 'AWS', 'ML']",2025-06-12 06:14:55
Lead Data Scientist,Trion Consultancy Services,10 - 18 years,20-35 Lacs P.A.,['Chennai'],"LD Scientist with 12 yrs of industry exp, including at least 5 yrs of hands-on exp in data science & a proven track record of delivering impactful data science solutions.\nData Analysis &Exploration\nTime Series Analysis\nModel Deployment & Integration\n\nRequired Candidate profile\n12+ yrs/including 5+ yrs in data science\nExp in Python and SQL for data extraction, manipulation & analysis\nDS & Model Development: Demonstrated exp in performing exploratory data analysis (EDA)",Industry Type: Emerging Technologies (AI/ML),Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Lead Data science', 'Machine Learning', 'Python']",2025-06-12 06:14:57
Associate Data Engineer,Amgen Inc,0 - 2 years,Not Disclosed,['Hyderabad'],"Role Description:\nWe are looking for an Associate Data Engineer with deep expertise in writing data pipelines to build scalable, high-performance data solutions. The ideal candidate will be responsible for developing, optimizing and maintaining complex data pipelines, integration frameworks, and metadata-driven architectures that enable seamless access and analytics. This role prefers deep understanding of the big data processing, distributed computing, data modeling, and governance frameworks to support self-service analytics, AI-driven insights, and enterprise-wide data management.\nRoles & Responsibilities:\nData Engineer who owns development of complex ETL/ELT data pipelines to process large-scale datasets\nContribute to the design, development, and implementation of data pipelines, ETL/ELT processes, and data integration solutions\nEnsuring data integrity, accuracy, and consistency through rigorous quality checks and monitoring\nExploring and implementing new tools and technologies to enhance ETL platform and performance of the pipelines\nProactively identify and implement opportunities to automate tasks and develop reusable frameworks\nEager to understand the biotech/pharma domains & build highly efficient data pipelines to migrate and deploy complex data across systems\nWork in an Agile and Scaled Agile (SAFe) environment, collaborating with cross-functional teams, product owners, and Scrum Masters to deliver incremental value\nUse JIRA, Confluence, and Agile DevOps tools to manage sprints, backlogs, and user stories.\nSupport continuous improvement, test automation, and DevOps practices in the data engineering lifecycle\nCollaborate and communicate effectively with the product teams, with cross-functional teams to understand business requirements and translate them into technical solutions\nMust-Have Skills:\nExperience in Data Engineering with a focus on Databricks, AWS, Python, SQL, and Scaled Agile methodologies\nProficiency & Strong understanding of data processing and transformation of big data frameworks (Databricks, Apache Spark, Delta Lake, and distributed computing concepts)\nStrong understanding of AWS services and can demonstrate the same\nAbility to quickly learn, adapt and apply new technologies\nStrong problem-solving and analytical skills\nExcellent communication and teamwork skills\nExperience with Scaled Agile Framework (SAFe), Agile delivery, and DevOps practices\nGood-to-Have Skills:\nData Engineering experience in Biotechnology or pharma industry\nExposure to APIs, full stack development\nExperienced with SQL/NOSQL database, vector database for large language models\nExperienced with data modeling and performance tuning for both OLAP and OLTP databases\nExperienced with software engineering best-practices, including but not limited to version control (Git, Subversion, etc.), CI/CD (Jenkins, Maven etc.), automated unit testing, and Dev Ops\nEducation and Professional Certifications\nBachelors degree and 2 to 5 + years of Computer Science, IT or related field experience\nOR\nMasters degree and 1 to 4 + years of Computer Science, IT or related field experience\nAWS Certified Data Engineer preferred\nDatabricks Certificate preferred\nScaled Agile SAFe certification preferred\nSoft Skills:\nExcellent analytical and troubleshooting skills.\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nTeam-oriented, with a focus on achieving team goals.\nAbility to learn quickly, be organized and detail oriented.\nStrong presentation and public speaking skills.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Maven', 'test automation', 'data engineering lifecycle', 'Scaled Agile methodologies', 'JIRA', 'SQL', 'Apache Spark', 'Jenkins', 'Agile DevOps tools', 'ETL platform', 'Confluence', 'Scaled Agile', 'Delta Lake', 'Agile', 'Databricks', 'AWS', 'Python']",2025-06-12 06:14:59
Senior Cloud Data Engineer,PwC India,10 - 15 years,12-22 Lacs P.A.,['Bengaluru'],"Role & responsibilities\n\nExperienced Senior Data Engineer utilizing Big Data & Google Cloud technologies to develop large scale, on-cloud data processing pipelines and data warehouses with Overall 12 to 15 years of experience\nHave 3 to 4 years of experience of leading Data Engineer teams developing enterprise grade data processing pipelines on multi Clouds like GCP and AWS",,,,"['Pyspark', 'Python', 'SQL', 'Datafactory', 'GCP', 'Microsoft Azure', 'ETL', 'AWS', 'Data Bricks']",2025-06-12 06:15:02
Big Data Lead,IQVIA,8 - 13 years,25-40 Lacs P.A.,['Bengaluru'],"Job Title / Primary Skill: Big Data Developer (Lead/Associate Manager)\nManagement Level: G150\nYears of Experience: 8 to 13 years\nJob Location: Bangalore (Hybrid)\nMust Have Skills: Big data, Spark, Scala, SQL, Hadoop Ecosystem.\nEducational Qualification: BE/BTech/ MTech/ MCA, Bachelor or masters degree in Computer Science,\n\nJob Overview\nOverall Experience 8+ years in IT, Software Engineering or relevant discipline.\nDesigns, develops, implements, and updates software systems in accordance with the needs of the organization.\nEvaluates, schedules, and resources development projects; investigates user needs; and documents, tests, and maintains computer programs.\nJob Description:\nWe look for developers to have good knowledge of Scala programming skills and Knowledge of SQL\nTechnical Skills:\nScala, Python -> Scala is often used for Hadoop-based projects, while Python and Scala are choices for Apache Spark-based projects.\nSQL -> Knowledge of SQL (Structured Query Language) is important for querying and manipulating data\nShell Script -> Shell scripts are used for batch processing of data, it can be used for scheduling the jobs and shell scripts are often used for deploying applications\nSpark Scala -> Spark Scala allows you to write Spark applications using the Spark API in Scala\nSpark SQL -> It allows to work with structured data using SQL-like queries and Data Frame APIs.\nWe can execute SQL queries against Data Frames, enabling easy data exploration, transformation, and analysis.\n\nThe typical tasks and responsibilities of a Big Data Developer include:\n1. Data Ingestion: Collecting and importing data from various sources, such as databases, logs, APIs into the Big Data infrastructure.\n2. Data Processing: Designing data pipelines to clean, transform, and prepare raw data for analysis. This often involves using technologies like Apache Hadoop, Apache Spark.\n3. Data Storage: Selecting appropriate data storage technologies like Hadoop Distributed File System (HDFS), HIVE, IMPALA, or cloud-based storage solutions (Snowflake, Databricks).",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SCALA', 'Big Data', 'Spark', 'Hive', 'Apache Pig', 'Hadoop', 'Hadoop Development', 'Mapreduce', 'Hdfs', 'Impala', 'YARN']",2025-06-12 06:15:05
Senior Data Scientist,Epsilon,6 - 9 years,Not Disclosed,['Bengaluru'],"Responsibilities: -\nContribute and build an internal product library that is focused on solving business problems related to prediction & recommendation.\nResearch unfamiliar methodologies, techniques to fine tune existing models in the product suite and, recommend better solutions and/or technologies.\nImprove features of the product to include newer machine learning algorithms in the likes of product recommendation, real time predictions, fraud detection, offer personalization etc\nCollaborate with client teams to on-board data, build models and score predictions.\nParticipate in building automations and standalone applications around machine learning algorithms to enable a One Click solution to getting predictions and recommendations.\nAnalyze large datasets, perform data wrangling operations, apply statistical treatments to filter and fine tune input data, engineer new features and eventually aid the process of building machine learning models.\nRun test cases to tune existing models for performance, check criteria and define thresholds for success by scaling the input data to multifold.\nDemonstrate a basic understanding of different machine learning concepts such as Regression, Classification, Matrix Factorization, K-fold Validations and different algorithms such as Decision Trees, Random Forrest, K-means clustering.\nDemonstrate working knowledge and contribute to building models using deep learning techniques, ensuring robust, scalable and high-performance solutions\nMinimum Qualifications:\nEducation: Master's or PhD in a quantitative discipline (Statistics, Economics, Mathematics, Computer Science) is highly preferred.\nDeep Learning Mastery: Extensive experience with deep learning frameworks (TensorFlow, PyTorch, or Keras) and advanced deep learning projects across various domains, with a focus on multimodal data applications.\nGenerative AI Expertise: Proven experience with generative AI models and techniques, such as RAG, VAEs, Transformers, and applications at scale in content creation or data augmentation.\nProgramming and Big Data: Expert-level proficiency in Python and big data/cloud technologies (Databricks and Spark) with a minimum of 4-5 years of experience.\nRecommender Systems and Real-time Predictions: Expertise in developing sophisticated recommender systems, including the application of real-time prediction frameworks.\nMachine Learning Algorithms: In-depth experience with complex algorithms such as logistic regression, random forest, XGBoost, advanced neural networks, and ensemble methods.\nExperienced with machine learning algorithms such as logistic regression, random forest, XG boost, KNN, SVM, neural network, linear regression, lasso regression and k-means.\nDesirable Qualifications:\nGenerative AI Tools Knowledge: Proficiency with tools and platforms for generative AI (such as OpenAI, Hugging Face Transformers).\nDatabricks and Unity Catalog: Experience leveraging Databricks and Unity Catalog for robust data management, model deployment, and tracking.\nWorking experience in CI/CD tools such as GIT & BitBucket",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Data Engineering', 'Pyspark', 'Azure Aws', 'Generative AI', 'Big Data', 'AWS', 'Data Bricks', 'Deep Learning', 'Python', 'SQL']",2025-06-12 06:15:07
Sr. Data Scientist-Stratup-Mid-Size companies Exp.@ Bangalore_Urgent,"A leader in this space, we deliver world...",8 - 13 years,Not Disclosed,['Bengaluru'],"Senior Data Scientist\n\nLocation: Onsite Bangalore\nExperience: 8+ years\n\nRole Overview\n\nWe are seeking a Senior Data Scientist with a strong foundation in machine learning, deep learning, and statistical modeling, with the ability to translate complex operational problems into scalable AI/ML solutions. In addition to core data science responsibilities, the role involves building production-ready backends in Python and contributing to end-to-end model lifecycle management. Exposure to computer vision is a plus, especially for industrial use cases like identification, intrusion detection, and anomaly detection.\n\nKey Responsibilities\n\nDevelop, validate, and deploy machine learning and deep learning models for forecasting, classification, anomaly detection, and operational optimization\nBuild backend APIs using Python (FastAPI, Flask) to serve ML/DL models in production environments\nApply advanced computer vision models (e.g., YOLO, Faster R-CNN) to object detection, intrusion detection, and visual monitoring tasks\nTranslate business problems into analytical frameworks and data science solutions\nWork with data engineering and DevOps teams to operationalize and monitor models at scale\nCollaborate with product, domain experts, and engineering teams to iterate on solution design\nContribute to technical documentation, model explainability, and reproducibility practices\n\n\nRequired Skills\n\nStrong proficiency in Python for data science and backend development\nExperience with ML/DL libraries such as scikit-learn, TensorFlow, or PyTorch\nSolid knowledge of time-series modeling, forecasting techniques, and anomaly detection\nExperience building and deploying APIs for model serving (FastAPI, Flask)\nFamiliarity with real-time data pipelines using Kafka, Spark, or similar tools\nStrong understanding of model validation, feature engineering, and performance tuning\nAbility to work with SQL and NoSQL databases, and large-scale datasets\nGood communication skills and stakeholder engagement experience\n\n\nGood to Have\n\nExperience with ML model deployment tools (MLflow, Docker, Airflow)\nUnderstanding of MLOps and continuous model delivery practices\nBackground in aviation, logistics, manufacturing, or other industrial domains\nFamiliarity with edge deployment and optimization of vision models\n\n\nQualifications\n\nMasters or PhD in Data Science, Computer Science, Applied Mathematics, or related field\n7+ years of experience in machine learning and data science, including end-to-end deployment of models in production",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['scikit-learn', 'time-series modeling', 'ML/DL libraries', 'data science', 'Python', 'Airflow', 'Kafka', 'MLflow', 'logistics', 'anomaly detection', 'aviation', 'SQL', 'PyTorch', 'NoSQL', 'MLOps', 'forecasting techniques', 'Docker', 'manufacturing', 'FastAPI', 'Spark', 'TensorFlow', 'Flask']",2025-06-12 06:15:09
Senior Big Data Engineer,Qualcomm,2 - 7 years,Not Disclosed,['Hyderabad'],"Job Area: Engineering Group, Engineering Group > Software Engineering\n\nGeneral Summary:\n\nAs a leading technology innovator, Qualcomm pushes the boundaries of what's possible to enable next-generation experiences and drives digital transformation to help create a smarter, connected future for all. As a Qualcomm Software Engineer, you will design, develop, create, modify, and validate embedded and cloud edge software, applications, and/or specialized utility programs that launch cutting-edge, world class products that meet and exceed customer needs. Qualcomm Software Engineers collaborate with systems, hardware, architecture, test engineers, and other teams to design system-level software solutions and obtain information on performance requirements and interfaces.\n\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 2+ years of Software Engineering or related work experience.\nOR\nMaster's degree in Engineering, Information Systems, Computer Science, or related field and 1+ year of Software Engineering or related work experience.\nOR\nPhD in Engineering, Information Systems, Computer Science, or related field.\n\n2+ years of academic or work experience with Programming Language such as C, C++, Java, Python, etc.\n\nGeneral Summary\n\nPreferred Qualifications\n\n3+ years of experience as a Data Engineer or in a similar role\n\nExperience with\n\ndata modeling, data warehousing, and building ETL pipelines\n\nSolid working experience with\n\nPython, AWS analytical technologies and related resources (Glue, Athena, QuickSight, SageMaker, etc.,)\n\nExperience with\n\nBig Data tools, platforms and architecture with solid working experience with SQL\n\nExperience working in a very large data warehousing environment,\n\nDistributed System.\n\nSolid understanding on various data exchange formats and complexities\n\nIndustry experience in software development, data engineering, business intelligence, data science, or related field with a track record of manipulating, processing, and extracting value from large datasets\n\nStrong data visualization skills\n\nBasic understanding of Machine Learning; Prior experience in ML Engineering a plus\n\nAbility to manage on-premises data and make it inter-operate with AWS based pipelines\n\nAbility to interface with Wireless Systems/SW engineers and understand the Wireless ML domain; Prior experience in Wireless (5G) domain a plus\n\n\nEducation\n\nBachelor's degree in computer science, engineering, mathematics, or a related technical discipline\n\nPreferred QualificationsMasters in CS/ECE with a Data Science / ML Specialization\n\n\nMinimum Qualifications:\n\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 3+ years of Software Engineering or related work experience.\n\nOR\n\nMaster's degree in Engineering, Information Systems, Computer Science, or related field\n\nOR\n\nPhD in Engineering, Information Systems, Computer Science, or related field.\n\n3+ years of experience with Programming Language such as C, C++, Java, Python, etc.\n\n\nDevelops, creates, and modifies general computer applications software or specialized utility programs. Analyzes user needs and develops software solutions. Designs software or customizes software for client use with the aim of optimizing operational efficiency. May analyze and design databases within an application area, working individually or coordinating database development as part of a team. Modifies existing software to correct errors, allow it to adapt to new hardware, or to improve its performance. Analyzes user needs and software requirements to determine feasibility of design within time and cost constraints. Confers with systems analysts, engineers, programmers and others to design system and to obtain information on project limitations and capabilities, performance requirements and interfaces. Stores, retrieves, and manipulates data for analysis of system capabilities and requirements. Designs, develops, and modifies software systems, using scientific analysis and mathematical models to predict and measure outcome and consequences of design.\n\nPrincipal Duties and Responsibilities:\n\nCompletes assigned coding tasks to specifications on time without significant errors or bugs.\n\nAdapts to changes and setbacks in order to manage pressure and meet deadlines.\n\nCollaborates with others inside project team to accomplish project objectives.\n\nCommunicates with project lead to provide status and information about impending obstacles.\n\nQuickly resolves complex software issues and bugs.\n\nGathers, integrates, and interprets information specific to a module or sub-block of code from a variety of sources in order to troubleshoot issues and find solutions.\n\nSeeks others' opinions and shares own opinions with others about ways in which a problem can be addressed differently.\n\nParticipates in technical conversations with tech leads/managers.\n\nAnticipates and communicates issues with project team to maintain open communication.\n\nMakes decisions based on incomplete or changing specifications and obtains adequate resources needed to complete assigned tasks.\n\nPrioritizes project deadlines and deliverables with minimal supervision.\n\nResolves straightforward technical issues and escalates more complex technical issues to an appropriate party (e.g., project lead, colleagues).\n\nWrites readable code for large features or significant bug fixes to support collaboration with other engineers.\n\nDetermines which work tasks are most important for self and junior engineers, stays focused, and deals with setbacks in a timely manner.\n\nUnit tests own code to verify the stability and functionality of a feature.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'sql', 'software engineering', 'data visualization', 'aws', 'quicksight', 'c', 'software development', 'glue', 'aws sagemaker', 'data warehousing', 'machine learning', 'business intelligence', 'data engineering', 'java', 'data science', 'data modeling', 'athena', 'wireless', 'big data', 'etl', 'ml']",2025-06-12 06:15:11
"4 To 8 years of exp. as a Data Analyst @ Banglore, Hyderabad , Chennai",A Client of Career Focus Consultancy,4 - 8 years,5-10 Lacs P.A.,"['Hyderabad', 'Chennai', 'Bengaluru']","Strong proficiency in Advanced SQL with experience in writing optimized queries for large datasets.\nMandatory skill : Data Analyst, Python ,SQL, Power BI\n\n\nExposure in, including predictive modeling and machine learning techniques.\n\nRequired Candidate profile\nHands-on experience with Python, R, or similar analytical tools is a plus.\nFamiliarity with cloud platforms such as AWS, Azure, or GCP for data processing and analytics.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['R', 'Power BI', 'Data Analyst', 'Python', 'SQL', 'Azure', 'GCP', 'AWS']",2025-06-12 06:15:14
Data Engineer _Technology Lead,Broadridge,6 - 10 years,Not Disclosed,['Bengaluru'],"Key Responsibilities:\nAnalyzes and solve problems using technical experience, judgment and precedents\nProvides informal guidance to new team members\nExplains complex information to others in straightforward situations\n1. Data Engineering and Modelling:\nDesign & Develop Scalable Data Pipelines: Leverage AWS technologies to design, develop, and manage end-to-end data pipelines with services like .",,,,"['Star Schema', 'Snowflake', 'AWS', 'Apache Airflow']",2025-06-12 06:15:16
Senior Data Scientist - AI/ML,Inumellas Consultancy Services,9 - 14 years,Not Disclosed,"['Hyderabad', 'Pune', 'Bengaluru']","Role - Senior Data Scientist / Senior Gen AI Engineer\nExp Range - 8 to 18 yrs\nPosition - Permanent Fulltime\nCompany - Data Analytics & AIML MNC\nLocation - Hyderabad, Pune, Bangalore (Relocation accepted)\nAbout the Role:\n\nWe are seeking a Software Engineer with expertise in Generative AI and Microsoft technologies to design, develop, and deploy AI-powered solutions using the Microsoft ecosystem. You will work with cross-functional teams to build scalable applications leveraging generative AI models and Azure services.\n\nSkills Required:\n\nExperience with Large Language Models (LLMs) like GPT, LLaMA, Claude, etc.\nProficiency in Python for building and fine-tuning AI/ML models\nFamiliarity with LangChain, LLMOps, or RAG (Retrieval-Augmented Generation) pipelines\nExperience with Vector Databases (e.g. FAISS, Pinecone, Weaviate)\nKnowledge of Prompt Engineering and model evaluation techniques\nExposure to cloud platforms (Azure, AWS or GCP) for deploying GenAI solutions\n\nPreferred Skills:\n\nExperience with Azure OpenAI, Databricks or Microsoft Fabric\nHands-on with Hugging Face Transformers, OpenAI APIs or custom model training",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Deep Learning', 'Prompt Engineering', 'Large Language Model', 'Vector Database', 'Retrieval Augmented Generation', 'GenAI', 'Langchain', 'Artificial Intelligence', 'LLMOps', 'LLaMa', 'GPT', 'Azure OpenAI', 'Machine Learning', 'ML Models', 'Model Evaluation', 'Huggingface', 'Aiml', 'OpenAI', 'Azure Machine Learning', 'Python']",2025-06-12 06:15:18
Senior Data Scientist,Straive,5 - 10 years,Not Disclosed,"['Hyderabad', 'Gurugram', 'Bengaluru']","Role & responsibilities\nRequires 5-8 years of proven experience in banking/payments/other domains\nStrong experience in developing Machine Learning models, Python & SQL\nExperience working with pre-trained models, awareness of state-of-art in embeddings and applicability for use cases\nDetailed oriented with a proactive mindset towards problem-solving\nExcellent communication and presentation skills with the ability to convey complex information clearly and concisely",,,,"['Machine Learning', 'Python', 'SQL', 'Xgboost', 'Neural Networks', 'Random Forest']",2025-06-12 06:15:20
Lead Data Engineer,Conduent,8 - 13 years,Not Disclosed,['Noida'],"Job Overview \n\nWe are looking for a Data Engineer who will be part of our Analytics Practice and will be expected to actively work in a multi-disciplinary fast paced environment. This role requires a broad range of skills and the ability to step into different roles depending on the size and scope of the project; its primary responsibility is the acquisition, transformation, loading and processing of data from a multitude of disparate data sources, including structured and unstructured data for advanced analytics and machine learning in a big data environment.\n\n\n Responsibilities: \nEngineer a modern data pipeline to collect, organize, and process data from disparate sources.\nPerforms data management tasks, such as conduct data profiling, assess data quality, and write SQL queries to extract and integrate data\nDevelop efficient data collection systems and sound strategies for getting quality data from different sources\nConsume and analyze data from the data pool to support inference, prediction and recommendation of actionable insights to support business growth.\nDesign and develop ETL processes using tools and scripting. Troubleshoot and debug ETL processes. Performance tuning and opitimization of the ETL processes.\nProvide support to new of existing applications while recommending best practices and leading projects to implement new functionality.\nCollaborate in design reviews and code reviews to ensure standards are met. Recommend new standards for visualizations.\nLearn and develop new ETL techniques as required to keep up with the contemporary technologies.\nReviews the solution requirements and architecture to ensure selection of appropriate technology, efficient use of resources and integration of multiple systems and technology.\nSupport presentations to Customers and Partners\nAdvising on new technology trends and possible adoption to maintain competitive advantage\n\n\n Experience Needed: \n8+ years of related experience is required.\nA BS or Masters degree in Computer Science or related technical discipline is required\nETL experience with data integration to support data marts, extracts and reporting\nExperience connecting to varied data sources\nExcellent SQL coding experience with performance optimization for data queries.\nUnderstands different data models like normalized, de-normalied, stars, and snowflake models. Worked with transactional, temporarl, time series, and structured and unstructured data.\nExperience on Azure Data Factory and Azure Synapse Analytics\nWorked in big data environments, cloud data stores, different RDBMS and OLAP solutions.\nExperience in cloud-based ETL development processes.\nExperience in deployment and maintenance of ETL Jobs.\nIs familiar with the principles and practices involved in development and maintenance of software solutions and architectures and in service delivery.\nHas strong technical background and remains evergreen with technology and industry developments.\nAt least 3 years of demonstrated success in software engineering, release engineering, and/or configuration management.\nHighly skilled in scripting languages like PowerShell.\nSubstantial experience in the implementation and exectuion fo CI/CD processes.\n\n\n Additional  \nDemonstrated ability to have successfully completed multiple, complex technical projects\nPrior experience with application delivery using an Onshore/Offshore model\nExperience with business processes across multiple Master data domains in a services based company\nDemonstrates a rational and organized approach to the tasks undertaken and an awareness of the need to achieve quality.\nDemonstrates high standards of professional behavior in dealings with clients, colleagues and staff.\nIs able to make sound and far reaching decisions alone on major issues and to take full responsibility for them on a technical basis.\nStrong written communication skills. Is effective and persuasive in both written and oral communication.\nExperience with gathering end user requirements and writing technical documentation\nTime management and multitasking skills to effectively meet deadlines under time-to-market pressure",Industry Type: BPM / BPO,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sql coding', 'sql', 'configuration management', 'software engineering', 'release engineering', 'continuous integration', 'rdbms', 'sql queries', 'performance tuning', 'azure synapse', 'ci/cd', 'azure data factory', 'machine learning', 'data engineering', 'powershell', 'olap', 'etl', 'big data']",2025-06-12 06:15:22
Senior Data Engineer,Qualcomm,5 - 10 years,Not Disclosed,['Hyderabad'],"Job Area: Information Technology Group, Information Technology Group > IT Data Engineer\n\nGeneral Summary:\n\nDeveloper will play an integral role in the PTEIT Machine Learning Data Engineering team. Design, develop and support data pipelines in a hybrid cloud environment to enable advanced analytics. Design, develop and support CI/CD of data pipelines and services. - 5+ years of experience with Python or equivalent programming using OOPS, Data Structures and Algorithms - Develop new services in AWS using server-less and container-based services. - 3+ years of hands-on experience with AWS Suite of services (EC2, IAM, S3, CDK, Glue, Athena, Lambda, RedShift, Snowflake, RDS) - 3+ years of expertise in scheduling data flows using Apache Airflow - 3+ years of strong data modelling (Functional, Logical and Physical) and data architecture experience in Data Lake and/or Data Warehouse - 3+ years of experience with SQL databases - 3+ years of experience with CI/CD and DevOps using Jenkins - 3+ years of experience with Event driven architecture specially on Change Data Capture - 3+ years of Experience in Apache Spark, SQL, Redshift (or) Big Query (or) Snowflake, Databricks - Deep understanding building the efficient data pipelines with data observability, data quality, schema drift, alerting and monitoring. - Good understanding of the Data Catalogs, Data Governance, Compliance, Security, Data sharing - Experience in building the reusable services across the data processing systems. - Should have the ability to work and contribute beyond defined responsibilities - Excellent communication and inter-personal skills with deep problem-solving skills.\n\nMinimum Qualifications:\n3+ years of IT-related work experience with a Bachelor's degree in Computer Engineering, Computer Science, Information Systems or a related field.\nOR\n5+ years of IT-related work experience without a Bachelors degree.\n\n2+ years of any combination of academic or work experience with programming (e.g., Java, Python).\n1+ year of any combination of academic or work experience with SQL or NoSQL Databases.\n1+ year of any combination of academic or work experience with Data Structures and algorithms.\n5 years of Industry experience and minimum 3 years experience in Data Engineering development with highly reputed organizations- Proficiency in Python and AWS- Excellent problem-solving skills- Deep understanding of data structures and algorithms- Proven experience in building cloud native software preferably with AWS suit of services- Proven experience in design and develop data models using RDBMS (Oracle, MySQL, etc.)\n\nDesirable - Exposure or experience in other cloud platforms (Azure and GCP) - Experience working on internals of large-scale distributed systems and databases such as Hadoop, Spark - Working experience on Data Lakehouse platforms (One House, Databricks Lakehouse) - Working experience on Data Lakehouse File Formats (Delta Lake, Iceberg, Hudi)\n\nBachelor's or Master's degree in Computer Science, Software Engineering, or a related field.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['algorithms', 'python', 'data quality', 'data structures', 'aws', 'schema', 'continuous integration', 'glue', 'amazon redshift', 'event driven architecture', 'ci/cd', 'data engineering', 'sql', 'alerts', 'java', 'data modeling', 'spark', 'devops', 'data flow', 'nosql databases', 'sql database']",2025-06-12 06:15:25
Lead Engineer - Data Science,Sasken Technologies,5 - 8 years,Not Disclosed,['Bengaluru'],"Job Summary\nPerson at this position takes ownership of a module and associated quality and delivery. Person at this position provides instructions, guidance and advice to team members to ensure quality and on time delivery.\nPerson at this position is expected to be able to instruct and review the quality of work done by technical staff.\nPerson at this position should be able to identify key issues and challenges by themselves, prioritize the tasks and deliver results with minimal direction and supervision.\nPerson at this position has the ability to investigate the root cause of the problem and come up alternatives/ solutions based on sound technical foundation gained through in-depth knowledge of technology, standards, tools and processes.\nPerson has the ability to organize and draw connections among ideas and distinguish between those which are implementable.\nPerson demonstrates a degree of flexibility in resolving problems/ issues that atleast to in-depth command of all techniques, processes, tools and standards within the relevant field of specialisation.\n\n\nRoles & Responsibilities\nResponsible for requirement analysis and feasibility study including system level work estimation while considering risk identification and mitigation.\nResponsible for design, coding, testing, bug fixing, documentation and technical support in the assigned area. Responsible for on time delivery while adhering to quality and productivity goals.\nResponsible for traceability of the requirements from design to delivery Code optimization and coverage.\nResponsible for conducting reviews, identifying risks and ownership of quality of deliverables.\nResponsible for identifying training needs of the team.\nExpected to enhance technical capabilities by attending trainings, self-study and periodic technical assessments.\nExpected to participate in technical initiatives related to project and organization and deliver training as per plan and quality.\nExpected to be a technical mentor for junior members.\nPerson may be given additional responsibility of managing people based on discretion of Project Manager.\n\nEducation and Experience Required\nEngineering graduate, MCA, etc Experience: 5-8 years\n\n\nCompetencies Description\nData Science TCB is applicable to one who\n1) Analyses data to arrive at patterns/Insights/models\n2) Come up with models based on the data to provide recommendations, predictive analytics etc\n3) Provides implementation of the models in R, Matlab etc\n4) Can understand and apply machine learning/AI techniques\nPlatforms-\nUnix\nTools-\nR, Matlab, Spark Machine Learning, Python-ML, SPSS, SAS\nLanguages-\nR, Perl, Python, Scala\nSpecialization-\nCOGNITIVE ANALYTICS INCLUDING COMPUTER VISION, AI and ML, STATISTICS",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Unix', 'R', 'SAS', 'Scala', 'Perl', 'SPSS', 'Machine Learning', 'Python']",2025-06-12 06:15:27
Data Engineer,AMERICAN EXPRESS,2 - 4 years,13-17 Lacs P.A.,"['Gurugram', 'Delhi / NCR']","Role & responsibilities\nUnderstanding business use cases and be able to convert to technical design\nPart of a cross-disciplinary team, working closely with other data engineers, software engineers, data scientists, data managers and business partners.\nYou will be designing scalable, testable and maintainable data pipelines\nIdentify areas for data governance improvements and help to resolve data quality problems through the appropriate choice of error detection and correction, process control and improvement, or process design changes",,,,"['Spark', 'SQL', 'Python', 'Hadoop', 'Big Data']",2025-06-12 06:15:30
Cloud Data Engineer,PwC India,5 - 8 years,10-20 Lacs P.A.,['Bengaluru'],"Role & responsibilities\nStrong hands-on experience with multi cloud (AWS, Azure, GCP)  services such as GCP BigQuery, Dataform AWS Redshift, \nProficient in PySpark and SQL for building scalable data processing pipelines\nKnowledge of utilizing serverless technologies like AWS Lambda, and Google Cloud Functions \nExperience in orchestration frameworks like Apache Airflow, Kubernetes, and Jenkins to manage and orchestrate data pipelines",,,,"['Pyspark', 'Python', 'SQL', 'Datafactory', 'GCP', 'Microsoft Azure', 'AWS', 'Data Bricks']",2025-06-12 06:15:33
Cloud Data Engineer,PwC India,3 - 8 years,Not Disclosed,"['Gurugram', 'Bengaluru']","Job Description:\n\nWe are seeking skilled and dynamic Cloud Data Engineers specializing in AWS, Azure, Databricks. The ideal candidate will have a strong background in data engineering, with a focus on data ingestion, transformation, and warehousing. They should also possess excellent knowledge of PySpark or Spark, and a proven ability to optimize performance in Spark job executions.\n\nKey Responsibilities:",,,,"['AWS OR Azure', 'Azure Data Engineer OR AWS Data Engineer', 'Azure', 'AWS']",2025-06-12 06:15:35
Data Engineer,AMERICAN EXPRESS,3 - 8 years,Not Disclosed,['Chennai'],"You Lead the Way. We've Got Your Back.\n\nWith the right backing, people and businesses have the power to progress in incredible ways. When you join Team Amex, you become part of a global and diverse community of colleagues with an unwavering commitment to back our customers, communities and each other. Here, youll learn and grow as we help you create a career journey thats unique and meaningful to you with benefits, programs, and flexibility that support you personally and professionally.\nAt American Express, you’ll be recognized for your contributions, leadership, and impact—every colleague has the opportunity to share in the company’s success. Together, we’ll win as a team, striving to uphold our company values and powerful backing promise to provide the world’s best customer experience every day. And we’ll do it with the utmost integrity, and in an environment where everyone is seen, heard and feels like they belong. As part of our diverse tech team, you can architect, code and ship software that makes us an essential part of our customers’ digital lives. Here, you can work alongside talented engineers in an open, supportive, inclusive environment where your voice is valued, and you make your own decisions on what tech to use to solve challenging problems. Amex offers a range of opportunities to work with the latest technologies and encourages you to back the broader engineering community through open source. And because we understand the importance of keeping your skills fresh and relevant, we give you dedicated time to invest in your professional development. Find your place in technology on #TeamAmex.",,,,"['Data Engineering', 'GCP', 'Airflow', 'Pyspark', 'Bigquery', 'Hadoop', 'Big Data', 'SQL', 'Python', 'Backend Development']",2025-06-12 06:15:38
Data Engineer,Bajaj Financial Securities,2 - 5 years,Not Disclosed,['Pune'],"We're Hiring: Data Engineer | 25 Years Experience | AWS + Real-time Focus\nJoin our fast-moving team as a Data Engineer where you'll build scalable, real-time data pipelines, own cloud infrastructure, and collaborate across teams to drive data-first decisions.\nIf you're strong in Python, experienced with streaming platforms like Kafka/Kinesis, and have shipped cloud-native data pipelines (preferably AWS) — we want to hear from you.\nMust-Haves:\n2–5 years of experience in Data Engineering\nPython (Pandas, PySpark, async), SQL, ETL/ELT\nStreaming experience (Kafka/Kinesis)\nAWS cloud stack (Glue, Lambda, S3, Athena)\nExperience in APIs, data warehousing, and data modelling\nBonus if you know: Docker, Kubernetes, Airflow/dbt, or have a background in MLOps.Role & responsibilities\n\n\nPreferred candidate profile",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Aws Lambda', 'Docker', 'Cloud Platform', 'Data Warehousing', 'Python', 'Pyspark', 'Api Gateway', 'Kinesis', 'Kafka', 'ETL', 'SQL', 'Kubernetes']",2025-06-12 06:15:40
Data Techology Senior Associate,MSCI Services,4 - 7 years,Not Disclosed,['Pune'],"Overview\nThe Data Technology team at MSCI is responsible for meeting the data requirements across various business areas, including Index, Analytics, and Sustainability. Our team collates data from multiple sources such as vendors (e.g., Bloomberg, Reuters), website acquisitions, and web scraping (e.g., financial news sites, company websites, exchange websites, filings). This data can be in structured or semi-structured formats. We normalize the data, perform quality checks, assign internal identifiers, and release it to downstream applications.\nResponsibilities\nAs data engineers, we build scalable systems to process data in various formats and volumes, ranging from megabytes to terabytes. Our systems perform quality checks, match data across various sources, and release it in multiple formats. We leverage the latest technologies, sources, and tools to process the data. Some of the exciting technologies we work with include Snowflake, Databricks, and Apache Spark.\nQualifications\nCore Java, Spring Boot, Apache Spark, Spring Batch, Python. Exposure to sql databases like Oracle, Mysql, Microsoft Sql is a must. Any experience/knowledge/certification on Cloud technology preferrably Microsoft Azure or Google cloud platform is good to have. Exposures to non sql databases like Neo4j or Document database is again good to have.\n What we offer you\nTransparent compensation schemes and comprehensive employee benefits, tailored to your location, ensuring your financial security, health, and overall wellbeing.\nFlexible working arrangements, advanced technology, and collaborative workspaces.\nA culture of high performance and innovation where we experiment with new ideas and take responsibility for achieving results.\nA global network of talented colleagues, who inspire, support, and share their expertise to innovate and deliver for our clients.\nGlobal Orientation program to kickstart your journey, followed by access to our Learning@MSCI platform, LinkedIn Learning Pro and tailored learning opportunities for ongoing skills development.\nMulti-directional career paths that offer professional growth and development through new challenges, internal mobility and expanded roles.\nWe actively nurture an environment that builds a sense of inclusion belonging and connection, including eight Employee Resource Groups. All Abilities, Asian Support Network, Black Leadership Network, Climate Action Network, Hola! MSCI, Pride & Allies, Women in Tech, and Women’s Leadership Forum.\nAt MSCI we are passionate about what we do, and we are inspired by our purpose – to power better investment decisions. You’ll be part of an industry-leading network of creative, curious, and entrepreneurial pioneers. This is a space where you can challenge yourself, set new standards and perform beyond expectations for yourself, our clients, and our industry.\nMSCI is a leading provider of critical decision support tools and services for the global investment community. With over 50 years of expertise in research, data, and technology, we power better investment decisions by enabling clients to understand and analyze key drivers of risk and return and confidently build more effective portfolios. We create industry-leading research-enhanced solutions that clients use to gain insight into and improve transparency across the investment process.\nMSCI Inc. is an equal opportunity employer. It is the policy of the firm to ensure equal employment opportunity without discrimination or harassment on the basis of race, color, religion, creed, age, sex, gender, gender identity, sexual orientation, national origin, citizenship, disability, marital and civil partnership/union status, pregnancy (including unlawful discrimination on the basis of a legally protected parental leave), veteran status, or any other characteristic protected by law. MSCI is also committed to working with and providing reasonable accommodations to individuals with disabilities. If you are an individual with a disability and would like to request a reasonable accommodation for any part of the application process, please email Disability.Assistance@msci.com and indicate the specifics of the assistance needed. Please note, this e-mail is intended only for individuals who are requesting a reasonable workplace accommodation; it is not intended for other inquiries.\n To all recruitment agencies\nMSCI does not accept unsolicited CVs/Resumes. Please do not forward CVs/Resumes to any MSCI employee, location, or website. MSCI is not responsible for any fees related to unsolicited CVs/Resumes.\n Note on recruitment scams\nWe are aware of recruitment scams where fraudsters impersonating MSCI personnel may try and elicit personal information from job seekers. Read our full note on careers.msci.com",Industry Type: Financial Services,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['hive', 'access', 'scala', 'pyspark', 'data warehousing', 'hibernate', 'research', 'sql', 'analytics', 'spring', 'java', 'spring batch', 'spark', 'gcp', 'mysql', 'html', 'hadoop', 'big data', 'etl', 'snowflake', 'python', 'oracle', 'data analysis', 'microsoft azure', 'power bi', 'sql server', 'javascript', 'data bricks', 'spring boot', 'tableau', 'neo4j', 'aws', 'sql database']",2025-06-12 06:15:42
Data loss prevention Analyst,Forvis Mazars,2 - 5 years,Not Disclosed,['Mumbai (All Areas)'],"Role Overview:\nWe are looking for dedicated professionals to join our DLP Operations Desk in Mumbai. The candidate should have practical experience in managing Data Loss Prevention (DLP) technologies and operational workflows.\nProduct Expertise:\nZscalar\nKey Responsibilities:\nMonitor and analyze DLP alerts and incidents as per established processes.\nInvestigate security incidents, coordinate with stakeholders, and drive closure.\nPrepare and share executive reports of DLP incidents/alerts on a defined schedule.\nContinuously fine-tune and optimize DLP policies based on operational insights, emerging threats, and best practices.\nStay updated with the latest trends in DLP and recommend enhancements to current policies.\n\nRequirements:\n2-5 years experience in security operations with a focus on DLP.\nPrior exposure to DLP tools and incident management processes.\nAnalytical mindset with strong documentation and reporting skills.\nAbility to research and apply best practices to policy management.\nExcellent communication and collaboration skills.",Industry Type: Miscellaneous,Department: Other,"Employment Type: Full Time, Permanent","['Dlp', 'Data Loss Prevention', 'Zscaler', 'Forcepoint']",2025-06-12 06:15:45
AWS Data Engineer,Infosys,5 - 9 years,Not Disclosed,"['Hyderabad', 'Pune', 'Bengaluru']","Significant 5 to 9 years of experience in designing and implementing scalable data engineering solutions on AWS.\nStrong proficiency in Python programming language.\nExpertise in serverless architecture and AWS services such as Lambda, Glue, Redshift, Kinesis, SNS, SQS, and CloudFormation.\nExperience with Infrastructure as Code (IaC) using AWS CDK for defining and provisioning AWS resources.\nProven leadership skills with the ability to mentor and guide junior team members.\nExcellent understanding of data modeling concepts and experience with tools like ERStudio.\nStrong communication and collaboration skills, with the ability to work effectively in a cross-functional team environment.\nExperience with Apache Airflow for orchestrating data pipelines is a plus.\nKnowledge of Data Lakehouse, dbt, or Apache Hudi data format is a plus.\nRoles and Responsibilities\nDesign, develop, test, deploy, and maintain large-scale data pipelines using AWS services such as S3, Glue, Lambda, Redshift.\nCollaborate with cross-functional teams to gather requirements and design solutions that meet business needs.\nDesired Candidate Profile\n5-9 years of experience in an IT industry setting with expertise in Python programming language (Pyspark).\nStrong understanding of AWS ecosystem including S3, Glue, Lambda, Redshift.\nBachelor's degree in Any Specialization (B.Tech/B.E.).",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Aws Glue', 'Pyspark', 'Aws Lambda', 'Amazon Redshift', 'Athena', 'Python']",2025-06-12 06:15:47
Azure Data Engineer,Infosys,5 - 9 years,Not Disclosed,"['Hyderabad', 'Pune', 'Bengaluru']","Job description\nWe are looking for Azure Data Engineer's resources having minimum 5 to 9 years of Experience.\n\nRole & responsibilities\nBlend of technical expertise with 5 to 9 year of experience, analytical problem-solving, and collaboration with cross-functional teams. Design and implement Azure data engineering solutions (Ingestion & Curation)\nCreate and maintain Azure data solutions including Azure SQL Database, Azure Data Lake, and Azure Blob Storage.\nDesign, implement, and maintain data pipelines for data ingestion, processing, and transformation in Azure.\nUtilizing Azure Data Factory or comparable technologies, create and maintain ETL (Extract, Transform, Load) operations\nUse Azure Data Factory and Databricks to assemble large, complex data sets\nImplementing data validation and cleansing procedures will ensure the quality, integrity, and dependability of the data.\nEnsure data quality / security and compliance.\nOptimize Azure SQL databases for efficient query performance.\nCollaborate with data engineers, and other stakeholders to understand requirements and translate them into scalable and reliable data platform architectures.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Databricks', 'Azure Data Factory', 'Azure Synapse', 'Pyspark', 'Azure Data Lake']",2025-06-12 06:15:50
Senior Engineer - Data Science,Sasken Technologies,2 - 5 years,Not Disclosed,['Bengaluru'],"Job Summary\nPerson at this position has gained significant work experience to be able to apply their knowledge effectively and deliver results. Person at this position is also able to demonstrate the ability to analyse and interpret complex problems and improve change or adapt existing methods to solve the problem.\nPerson at this position regularly interacts with interfacing groups / customer on technical issue clarification and resolves the issues. Also participates actively in important project/ work related activities and contributes towards identifying important issues and risks. Reaches out for guidance and advice to ensure high quality of deliverables.\nPerson at this position consistently seek opportunities to enhance their existing skills, acquire more complex skills and work towards enhancing their proficiency level in their field of specialisation.\nWorks under limited supervision of Team Lead/ Project Manager.\n\n\nRoles & Responsibilities\nResponsible for design, coding, testing, bug fixing, documentation and technical support in the assigned area. Responsible for on time delivery while adhering to quality and productivity goals. Responsible for adhering to guidelines and checklists for all deliverable reviews, sending status report to team lead and following relevant organizational processes. Responsible for customer collaboration and interactions and support to customer queries. Expected to enhance technical capabilities by attending trainings, self-study and periodic technical assessments. Expected to participate in technical initiatives related to project and organization and deliver training as per plan and quality.\n\n\nEducation and Experience Required\nEngineering graduate, MCA, etc Experience: 2-5 years\n\nCompetencies Description\nData Science TCB is applicable to one who\n1) Analyses data to arrive at patterns/Insights/models\n2) Come up with models based on the data to provide recommendations, predictive analytics etc\n3) Provides implementation of the models in R, Matlab etc\n4) Can understand and apply machine learning/AI techniques\nPlatforms-\nUnix\nTechnology Standard-\nNA\nTools-\nR, Matlab, Spark Machine Learning, Python-ML, SPSS, SAS\nLanguages-\nR, Perl, Python, Scala\nSpecialization-\nCOGNITIVE ANALYTICS INCLUDING COMPUTER VISION, AI and ML, STATISTICS.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'R', 'SAS', 'Scala', 'Perl', 'SPSS', 'Spark', 'machine learning', 'Python']",2025-06-12 06:15:52
MDM Associate Data Steward,Amgen Inc,0 - 3 years,Not Disclosed,['Hyderabad'],"ABOUT THE ROLE\n\nRole Description\n\nWe are seeking an MDM Associate Data Steward who will be responsible for ensuring the accuracy, completeness, and reliability of master data across critical business domains such as Customer, Product, Affiliations, and Payer. This role involves actively managing and curating master data through robust data stewardship processes, comprehensive data cataloging, and data governance frameworks utilizing Informatica or Reltio MDM platforms. Additionally, the incumbent will perform advanced data analysis, data validation, and data transformation tasks through SQL queries and Python scripts to enable informed, data-driven business decisions. The role emphasizes cross-functional collaboration with various teams, including Data Engineering, Commercial, Medical, Compliance, and IT, to align data management activities with organizational goals and compliance standards.\n\nRoles & Responsibilities\nResponsible for master data stewardship, ensuring data accuracy and integrity across key master data domains (e.g., Customer, Product, Affiliations).\nConduct advanced data profiling, cataloging, and reconciliation activities using Informatica or Reltio MDM platforms.\nManage the reconciliation of potential matches, ensuring accurate resolution of data discrepancies and preventing duplicate data entries.\nEffectively manage Data Change Request (DCR) processes, including reviewing, approving, and documenting data updates in compliance with established procedures and SLAs.\nExecute and optimize SQL queries for validation and analysis of master data.\nPerform basic Python for data transformation, quality checks, and automation.\nCollaborate effectively with cross-functional teams including Data Engineering, Commercial, Medical, Compliance, and IT to fulfill data requirements.\nSupport user acceptance testing (UAT) and system integration tests for MDM related system updates.\nImplement data governance processes ensuring compliance with enterprise standards, policies, and frameworks.\nDocument and maintain accurate SOPs, Data Catalogs, Playbooks, and SLAs.\nIdentify and implement process improvements to enhance data stewardship and analytic capabilities.\nPerform regular audits and monitoring to maintain high data quality and integrity.\nBasic Qualifications and Experience\nMasters degree with 1 - 3 years of experience in Business, Engineering, IT or related fieldOR\nBachelors degree with 2 - 5 years of experience in Business, Engineering, IT or related fieldOR\nDiploma with 6 - 8 years of experience in Business, Engineering, IT or related field\nFunctional\n\nSkills:\nMust-Have Skills:\nDirect experience in data stewardship, data profiling, and master data management.\nHands-on experience with Informatica or Reltio MDM platforms.\nProficiency in SQL for data analysis and querying.\nKnowledge of data cataloging techniques and tools.\nBasic proficiency in Python scripting for data processing.\nGood-to-Have\n\nSkills:\nExperience with PySpark and Databricks for large-scale data processing.\nBackground in the pharmaceutical, healthcare, or life sciences industries.\nFamiliarity with AWS or other cloud-based data solutions.\nStrong project management and agile workflow familiarity (e.g., using Jira, Confluence).\nUnderstanding of regulatory compliance related to data protection (GDPR, CCPA).\nProfessional Certifications\nAny ETL certification ( e.g. Informatica)\nAny Data Analysis certification (SQL)\nAny cloud certification (AWS or AZURE)\nSoft\n\nSkills:\nStrong analytical abilities to assess and improve master data processes and solutions.\nExcellent verbal and written communication skills, with the ability to convey complex data concepts clearly to technical and non-technical stakeholders.\nEffective problem-solving skills to address data-related issues and implement scalable solutions.\nAbility to work effectively with global, virtual teams",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data management', 'python', 'project management', 'data analysis', 'data stewardship', 'agile database', 'data processing', 'sql', 'data profiling']",2025-06-12 06:15:54
Big Data Engineer - Hadoop,Info Origin Technologies Pvt Ltd,3 - 7 years,Not Disclosed,"['Hyderabad', 'Gurugram']","Role: Hadoop Data Engineer\nLocation: Gurgaon / Hyderabad\nWork Mode: Hybrid\nEmployment Type: Full-Time\nInterview Mode: First Video then In Person\nJob Description\nJob Overview:\nWe are looking for experienced Data Engineers proficient in Hadoop, Hive, Python, SQL, and Pyspark/Spark to join our dynamic team. Candidates will be responsible for designing, developing, and maintaining scalable big data solutions.\nKey Responsibilities:\nDevelop and optimize data pipelines for large-scale data processing.\nWork with structured and unstructured datasets to derive actionable insights.\nCollaborate with cross-functional teams to enhance data-driven decision-making.\nEnsure the performance, scalability, and reliability of data architectures.\nImplement best practices for data security and governance.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Hive', 'Hadoop', 'Pyspark', 'Big Data', 'Python', 'SQL']",2025-06-12 06:15:57
Senior Associate - Data Science,Axtria,3 - 8 years,Not Disclosed,['Noida'],"Job Summary-\nData Scientist with good hands-on experience of 3+ years in developing state of the art and scalable Machine Learning models and their operationalization, leveraging off-the-shelf workbench production.\n\nJob Responsibilities-\n\n1. Hands on experience in Python data-science and math packages such as NumPy, Pandas, Sklearn, Seaborn, PyCaret, Matplotlib\n2. Proficiency in Python and common Machine Learning frameworks (TensorFlow, NLTK, Stanford NLP, PyTorch, Ling Pipe, Caffe, Keras, SparkML and OpenAI etc.)\n3. Experience of working in large teams and using collaboration tools like GIT, Jira and Confluence\n4. Good understanding of any of the cloud platform - AWS, Azure or GCP\n5. Understanding of Commercial Pharma landscape and Patient Data / Analytics would be a huge plus\n6. Should have an attitude of willingness to learn, accepting the challenging environment and confidence in delivering the results within timelines. Should be inclined towards self motivation and self-driven to find solutions for problems.\n7. Should be able to mentor and guide mid to large sized teams under him/her\n\nJob -\n1. Strong experience on Spark with Scala/Python/Java\n2. Strong proficiency in building/training/evaluating state of the art machine learning models and its deployment\n3. Proficiency in Statistical and Probabilistic methods such as SVM, Decision-Trees, Bagging and Boosting Techniques, Clustering\n4. Proficiency in Core NLP techniques like Text Classification, Named Entity Recognition (NER), Topic Modeling, Sentiment Analysis, etc. Understanding of Generative AI / Large Language Models / Transformers would be a plus",Industry Type: Analytics / KPO / Research,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['scala', 'java', 'spark', 'machine learning algorithms', 'python', 'confluence', 'scikit-learn', 'nltk', 'training', 'numpy', 'tensorflow', 'git', 'seaborn', 'gcp', 'pytorch', 'keras', 'spark mllib', 'jira', 'sentiment analysis', 'lingpipe', 'caffe', 'microsoft azure', 'pandas', 'matplotlib', 'aws', 'statistics']",2025-06-12 06:15:59
Azure Data Engineer,Wipro,5 - 8 years,Not Disclosed,['Bengaluru'],"Role Purpose\nThe purpose of the role is to support process delivery by ensuring daily performance of the Production Specialists, resolve technical escalations and develop technical capability within the Production Specialists.\n\nDo\nOversee and support process by reviewing daily transactions on performance parameters\nReview performance dashboard and the scores for the team\nSupport the team in improving performance parameters by providing technical support and process guidance\nRecord, track, and document all queries received, problem-solving steps taken and total successful and unsuccessful resolutions\nEnsure standard processes and procedures are followed to resolve all client queries\nResolve client queries as per the SLAs defined in the contract\nDevelop understanding of process/ product for the team members to facilitate better client interaction and troubleshooting\nDocument and analyze call logs to spot most occurring trends to prevent future problems\nIdentify red flags and escalate serious client issues to Team leader in cases of untimely resolution\nEnsure all product information and disclosures are given to clients before and after the call/email requests\nAvoids legal challenges by monitoring compliance with service agreements\n\nHandle technical escalations through effective diagnosis and troubleshooting of client queries\nManage and resolve technical roadblocks/ escalations as per SLA and quality requirements\nIf unable to resolve the issues, timely escalate the issues to TA & SES\nProvide product support and resolution to clients by performing a question diagnosis while guiding users through step-by-step solutions\nTroubleshoot all client queries in a user-friendly, courteous and professional manner\nOffer alternative solutions to clients (where appropriate) with the objective of retaining customers and clients business\nOrganize ideas and effectively communicate oral messages appropriate to listeners and situations\nFollow up and make scheduled call backs to customers to record feedback and ensure compliance to contract SLAs\nBuild people capability to ensure operational excellence and maintain superior customer service levels of the existing account/client\nMentor and guide Production Specialists on improving technical knowledge\nCollate trainings to be conducted as triage to bridge the skill gaps identified through interviews with the Production Specialist\nDevelop and conduct trainings (Triages) within products for production specialist as per target\nInform client about the triages being conducted\nUndertake product trainings to stay current with product features, changes and updates\nEnroll in product specific and any other trainings per client requirements/recommendations\nIdentify and document most common problems and recommend appropriate resolutions to the team\nUpdate job knowledge by participating in self learning opportunities and maintaining personal networks\n\nDeliver\n\nNoPerformance ParameterMeasure1ProcessNo. of cases resolved per day, compliance to process and quality standards, meeting process level SLAs, Pulse score, Customer feedback, NSAT/ ESAT2Team ManagementProductivity, efficiency, absenteeism3Capability developmentTriages completed, Technical Test performance\nMandatory Skills: Azure Data Factory. Experience: 5-8 Years.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure', 'azure databricks', 'azure data lake', 'ssas', 'ssrs', 'microsoft azure', 'azure data factory', 'ssis', 'msbi', 'sql server', 'sql']",2025-06-12 06:16:02
AWS Data Engineer,Infosys,5 - 9 years,Not Disclosed,"['Hyderabad', 'Pune', 'Bengaluru']","AWS Data Engineer\n\nTo Apply, use the below link:\nhttps://career.infosys.com/jobdesc?jobReferenceCode=INFSYS-EXTERNAL-210775&rc=0\n\nJOB Profile:\nSignificant 5 to 9 years of experience in designing and implementing scalable data engineering solutions on AWS.\nStrong proficiency in Python programming language.\nExpertise in serverless architecture and AWS services such as Lambda, Glue, Redshift, Kinesis, SNS, SQS, and CloudFormation.\nExperience with Infrastructure as Code (IaC) using AWS CDK for defining and provisioning AWS resources.\nProven leadership skills with the ability to mentor and guide junior team members.\nExcellent understanding of data modeling concepts and experience with tools like ERStudio.\nStrong communication and collaboration skills, with the ability to work effectively in a cross-functional team environment.\nExperience with Apache Airflow for orchestrating data pipelines is a plus.\nKnowledge of Data Lakehouse, dbt, or Apache Hudi data format is a plus.\n\n\nRoles and Responsibilities\nDesign, develop, test, deploy, and maintain large-scale data pipelines using AWS services such as S3, Glue, Lambda, Redshift.\nCollaborate with cross-functional teams to gather requirements and design solutions that meet business needs.\nDesired Candidate Profile\n5-9 years of experience in an IT industry setting with expertise in Python programming language (Pyspark).\nStrong understanding of AWS ecosystem including S3, Glue, Lambda, Redshift.\nBachelor's degree in Any Specialization (B.Tech/B.E.).",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Aws Glue', 'AWS Data Engineer', 'Pyspark', 'Aws Lambda', 'Redshift Aws', 'Python']",2025-06-12 06:16:05
"Sr. Data Analyst – Tableau, SQL, Snowflake",Int9 Solutions,5 - 7 years,Not Disclosed,['Bengaluru'],"We are looking for a skilled Data Analyst with excellent communication skills and deep expertise in SQL, Tableau, and modern data warehousing technologies. This role involves designing data models, building insightful dashboards, ensuring data quality, and extracting meaningful insights from large datasets to support strategic business decisions.\n\nKey Responsibilities:\nWrite advanced SQL queries to retrieve and manipulate data from cloud data warehouses such as Snowflake, Redshift, or BigQuery.\nDesign and develop data models that support analytics and reporting needs.\nBuild dynamic, interactive dashboards and reports using tools like Tableau, Looker, or Domo.\nPerform advanced analytics techniques including cohort analysis, time series analysis, scenario analysis, and predictive analytics.\nValidate data accuracy and perform thorough data QA to ensure high-quality output.\nInvestigate and troubleshoot data issues; perform root cause analysis in collaboration with BI or data engineering teams.\nCommunicate analytical insights clearly and effectively to stakeholders.\n\nRequired Skills & Qualifications:\nExcellent communication skills are mandatory for this role.\n5+ years of experience in data analytics, BI analytics, or BI engineering roles.\nExpert-level skills in SQL, with experience writing complex queries and building views.\nProven experience using data visualization tools like Tableau, Looker, or Domo.\nStrong understanding of data modeling principles and best practices.\nHands-on experience working with cloud data warehouses such as Snowflake, Redshift, BigQuery, SQL Server, or Oracle.\nIntermediate-level proficiency with spreadsheet tools like Excel, Google Sheets, or Power BI, including functions, pivots, and lookups.\nBachelor's or advanced degree in a relevant field such as Data Science, Computer Science, Statistics, Mathematics, or Information Systems.\nAbility to collaborate with cross-functional teams, including BI engineers, to optimize reporting solutions.\nExperience in handling large-scale enterprise data environments.\nFamiliarity with data governance, data cataloging, and metadata management tools (a plus but not required).",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Snowflake', 'Tableau', 'Data Warehousing', 'Data Analytics', 'SQL', 'Scenario Analysis', 'Cohort Analysis', 'Data Modeling', 'Predictive Analysis', 'Redshift']",2025-06-12 06:16:07
Azure Data Engineer,Hexaware Technologies,6 - 10 years,Not Disclosed,"['Chennai', 'Bengaluru']","Hi\n\nWork Location : Chennai AND Bangalore\nWork location : Imm - 30 days\n\nPrimary: Azure Databricks,ADF, Pyspark SQL",,,,"['Pyspark', 'Azure Databricks', 'SQL', 'Azure Data Factory']",2025-06-12 06:16:10
MDM Data Analyst / Steward Lead,Gallagher Service Center (GSC),3 - 7 years,Not Disclosed,['Bengaluru'],"Role & responsibilities\n\nThe MDM Analyst / Data Steward works closely with business stakeholders to understand and gather data requirements, develop data models and database designs, and define and implement data standards, policies, and procedures. This role also implements any rules inside of the MDM tool to improve the data, performs deduplication projects to develop golden records, and overall works towards improving the quality of data in the domain assigned.\n\nRequired skills :\nTechnical Skills: Proficiency in MDM tools and technologies such as Informatica MDM, CluedIn, or similar platforms is essential. Familiarity with data modeling, data integration, and data quality control techniques is also important. Experience with data governance platforms like Collibra and Alation can be beneficial1.\nAnalytical Skills: Strong analytical and problem-solving skills are crucial for interpreting and working with large volumes of data. The ability to translate complex business requirements into practical MDM solutions is also necessary.\nData Management: Experience in designing, implementing, and maintaining master data management systems and solutions. This includes conducting data cleansing, data auditing, and data validation activities.\nCommunication and Collaboration: Excellent communication and interpersonal skills to effectively collaborate with business stakeholders, IT teams, and other departments.\nData Governance: In-depth knowledge of data governance, data quality, and data integration principles. The ability to develop and implement data management processes and policies is essential.\nEducational Background: A Bachelor's or Master's degree in Computer Science, Information Systems, Data Science, or a related field is typically required1.\nCertifications: Certification in the MDM domain (e.g., Certified MDM Professional) can be a plus\n\nKey Skills:\nBecome the expert at the assigned domain of data\nUnderstand all source systems feeding into the MDM\nWrite documentation of stewardship for the domain\nDevelop rules and standards for the domain of data\nGenerate measures of improvement to demonstrate to the business the quality of the data\n\nWe are seeking candidates who can join immediately or within a maximum of 30 days' notice.\nMinimum of 3+ years of relevant experience is required.\nCandidates who are willing to relocate to Bangalore or are already based in Bangalore.\nCandidates should be flexible with working UK/US shifts.",Industry Type: Analytics / KPO / Research,Department: Other,"Employment Type: Full Time, Permanent","['Informatica Mdm', 'Data Modeling', 'Data Integration']",2025-06-12 06:16:12
"Sr. Data Analyst – Tableau, SQL, Snowflake",Int9 Solutions,5 - 10 years,Not Disclosed,"['Mumbai', 'Delhi / NCR', 'Bengaluru']","We are looking for a skilled Data Analyst with excellent communication skills and deep expertise in SQL, Tableau, and modern data warehousing technologies. This role involves designing data models, building insightful dashboards, ensuring data quality, and extracting meaningful insights from large datasets to support strategic business decisions.\n\nKey Responsibilities:\nWrite advanced SQL queries to retrieve and manipulate data from cloud data warehouses such as Snowflake, Redshift, or BigQuery.\nDesign and develop data models that support analytics and reporting needs.\nBuild dynamic, interactive dashboards and reports using tools like Tableau, Looker, or Domo.\nPerform advanced analytics techniques including cohort analysis, time series analysis, scenario analysis, and predictive analytics.\nValidate data accuracy and perform thorough data QA to ensure high-quality output.\nInvestigate and troubleshoot data issues; perform root cause analysis in collaboration with BI or data engineering teams.\nCommunicate analytical insights clearly and effectively to stakeholders.\n\nRequired Skills & Qualifications:\nExcellent communication skills are mandatory for this role.\n5+ years of experience in data analytics, BI analytics, or BI engineering roles.\nExpert-level skills in SQL, with experience writing complex queries and building views.\nProven experience using data visualization tools like Tableau, Looker, or Domo.\nStrong understanding of data modeling principles and best practices.\nHands-on experience working with cloud data warehouses such as Snowflake, Redshift, BigQuery, SQL Server, or Oracle.\nIntermediate-level proficiency with spreadsheet tools like Excel, Google Sheets, or Power BI, including functions, pivots, and lookups.\nBachelor's or advanced degree in a relevant field such as Data Science, Computer Science, Statistics, Mathematics, or Information Systems.\nAbility to collaborate with cross-functional teams, including BI engineers, to optimize reporting solutions.\nExperience in handling large-scale enterprise data environments.\nFamiliarity with data governance, data cataloging, and metadata management tools (a plus but not required).\nLocation : - Mumbai, Delhi / NCR, Bengaluru , Kolkata, Chennai, Hyderabad, Ahmedabad, Pune, Remote",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Snowflake', 'Tableau', 'SQL', 'BI Tools', 'Scenario Analysis', 'Cohort Analysis', 'Data Warehousing', 'SQL Server', 'Data Modeling', 'Data Analytics', 'Predictive Analysis', 'Redshift']",2025-06-12 06:16:16
"Senior Data Engineer (Snowflake, DBT)",Allegis Global Solutions (AGS),5 - 10 years,Not Disclosed,[],"Senior Data Engineer (Snowflake, DBT, Azure)\nJob Location: Hyderabad / Bangalore / Chennai / Kolkata / Noida/ Gurgaon / Pune / Indore / Mumbai\n\nJob Details\nTechnical Expertise:\nStrong proficiency in Snowflake architecture, including data sharing, partitioning, clustering, and materialized views.\nAdvanced experience with DBT for data transformations and workflow management.\nExpertise in Azure services, including Azure Data Factory, Azure Data Lake, Azure Synapse, and Azure Functions.\nData Engineering:\nProficiency in SQL, Python, or other relevant programming languages.\nStrong understanding of data modeling concepts, including star schema and normalization.\nHands-on experience with ETL/ELT pipelines and data integration tools.\n\nPreferred Qualifications:\nCertifications in Snowflake, DBT, or Azure Data Engineering.\nFamiliar with data visualization tools like Power BI or Tableau.\nKnowledge of CI/CD pipelines and DevOps practices for data workflows.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Snowflake', 'Data Build Tool', 'Azure']",2025-06-12 06:16:18
Data Annotation hiring For Fresher || Excellent communication skills,Multinational Company,0 - 4 years,1-3 Lacs P.A.,"['Gurugram', 'Delhi / NCR']","Lead data annotation and collection projects.\nDevelop and implement data annotation guidelines and processes.\nTrain and manage data annotation teams.\nCollaborate with data scientists and engineers to understand data requirements.\n\nHR - 63980 09438\n\nRequired Candidate profile\nQualification - Graduate\nSalary :-\nCTC\n25,000 / experience\n20,000 / fresher\nExperience - Data Annotation only\nTransport:- Both Side\n\n5 Day working / Rotation shift / 2 day Rotation week off",Industry Type: BPM / BPO,"Department: Customer Success, Service & Operations","Employment Type: Full Time, Permanent","['Object Detection', 'Data Annotation', 'Business Intelligence', 'Digital Image Processing', 'Data Management', 'Image Recognition', 'Image Analysis', 'Annotation', 'Deep Learning', 'Pattern Recognition', 'Image Processing', 'Imaging', 'Content Moderation', 'Data Warehousing', 'Data Analytics']",2025-06-12 06:16:20
Data Analyst,Primary Healthtech,1 - 5 years,2.5-4.5 Lacs P.A.,['Noida'],"Roles and Responsibilities\nCollect data from various sources, clean it, and analyze it using statistical tools.\nCreate reports based on analysis findings to present insights to stakeholders.\nDevelop dashboards and visualizations to effectively communicate results.\nManage databases by designing schema, writing queries, and optimizing performance.\nEnsure accuracy of data through quality control measures.\nDesired Candidate Profile\n1-5 years of experience in Data Analysis or related field (Data Analytics).\nB.Tech/B.E. degree in Any Specialization.\nProficiency in SQL programming language with knowledge of database management systems like MySQL or PostgreSQL.\nStrong understanding of statistics, data interpretation, and data visualization techniques.",Industry Type: Medical Devices & Equipment,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Interpretation', 'Data Analysis', 'Data Analytics', 'Data Management', 'Data Visualization', 'Data Reporting']",2025-06-12 06:16:22
Data Bricks,PwC India,7 - 12 years,Not Disclosed,['Bengaluru'],"Job Summary:\n\nWe are seeking a talented Data Engineer with strong expertise in Databricks, specifically in Unity Catalog, PySpark, and SQL, to join our data team. Youll play a key role in building secure, scalable data pipelines and implementing robust data governance strategies using Unity Catalog.\n\nKey Responsibilities:",,,,"['DataBricks', 'Data Bricks', 'Pyspark', 'Delta Lake', 'Databricks Engineer', 'Unity Catalog', 'SQL']",2025-06-12 06:16:25
"AI/ML Engineer (Specializing in NLP/ML, Large Data Processing,",Synechron,8 - 10 years,Not Disclosed,"['Pune', 'Hinjewadi']","job requisition idJR1027361\n\nJob Summary\nSynechron seeks a highly skilled AI/ML Engineer specializing in Natural Language Processing (NLP), Large Language Models (LLMs), Foundation Models (FMs), and Generative AI (GenAI). The successful candidate will design, develop, and deploy advanced AI solutions, contributing to innovative projects that transform monolithic systems into scalable microservices integrated with leading cloud platforms such as Azure, Amazon Bedrock, and Google Gemini. This role plays a critical part in advancing Synechrons capabilities in cutting-edge AI technologies, enabling impactful business insights and product innovations.\n\nSoftware\n\nRequired Proficiency:\nPython (core librariesTensorFlow, PyTorch, Hugging Face transformers, etc.)\nCloud platformsAzure, AWS, Google Cloud (familiarity with AI/ML services)\nContainerizationDocker, Kubernetes\nVersion controlGit\nData management toolsSQL, NoSQL databases (e.g., MongoDB)\nModel deployment and MLOps toolsMLflow, CI/CD pipelines, monitoring tools\nPreferred\n\nSkills:\nExperience with cloud-native AI frameworks and SDKs\nFamiliarity with AutoML tools\nAdditional programming languages (e.g., Java, Scala)\nOverall Responsibilities\nDesign, develop, and optimize NLP models, including advanced LLMs and Foundation Models, for diverse business use cases.\nLead the development of large data pipelines for training, fine-tuning, and deploying models on big data platforms.\nArchitect, implement, and maintain scalable AI solutions in line with MLOps best practices.\nTransition legacy monolithic AI systems into modular, microservices-based architectures for scalability and maintainability.\nBuild end-to-end AI applications from scratch, including data ingestion, model training, deployment, and integration.\nImplement retrieval-augmented generation techniques for enhanced context understanding and response accuracy.\nConduct thorough testing, validation, and debugging of AI/ML models and pipelines.\nCollaborate with cross-functional teams to embed AI capabilities into customer-facing and enterprise products.\nSupport ongoing maintenance, monitoring, and scaling of deployed AI systems.\nDocument system designs, workflows, and deployment procedures for compliance and knowledge sharing.\nPerformance Outcomes:\nProduction-ready AI solutions delivering high accuracy and efficiency.\nRobust data pipelines supporting training and inference at scale.\nSeamless integration of AI models with cloud infrastructure.\nEffective collaboration leading to innovative AI product deployment.\nTechnical Skills (By Category)\n\nProgramming Languages:\nEssential: Python (TensorFlow, PyTorch, Hugging Face, etc.)\nPreferred: Java, Scala\nDatabases/Data Management:\nSQL (PostgreSQL, MySQL), NoSQL (MongoDB, DynamoDB)\nCloud Technologies:\nAzure AI, AWS SageMaker, Bedrock, Google Cloud Vertex AI, Gemini\nFrameworks and Libraries:\nTransformers, Keras, scikit-learn, XGBoost, Hugging Face engines\nDevelopment Tools & Methodologies:\nDocker, Kubernetes, Git, CI/CD pipelines (Jenkins, Azure DevOps)\nSecurity & Compliance:\nKnowledge of data security standards and privacy policies (GDPR, HIPAA as applicable)\nExperience\n8 to 10 years of hands-on experience in AI/ML development, especially NLP and Generative AI.\nDemonstrated expertise in designing, fine-tuning, and deploying LLMs, FMs, and GenAI solutions.\nProven ability to develop end-to-end AI applications within cloud environments.\nExperience transforming monolithic architectures into scalable microservices.\nStrong background with big data processing pipelines.\nPrior experience working with cloud-native AI tools and frameworks.\nIndustry experience in finance, healthcare, or technology sectors is advantageous.\nAlternative Experience:\nCandidates with extensive research or academic experience in AI/ML, especially in NLP and large-scale data processing, are eligible if they have practical deployment experience.\n\nDay-to-Day Activities\nDevelop and optimize sophisticated NLP/GenAI models fulfilling business requirements.\nLead data pipeline construction for training and inference workflows.\nCollaborate with data engineers, architects, and product teams to ensure scalable deployment.\nConduct model testing, validation, and performance tuning.\nImplement and monitor model deployment pipelines, troubleshoot issues, and improve system robustness.\nDocument models, pipelines, and deployment procedures for audit and knowledge sharing.\nStay updated with emerging AI/ML trends, integrating best practices into projects.\nPresent findings, progress updates, and technical guidance to stakeholders.\nQualifications\nBachelors degree in Computer Science, Data Science, or related field; Masters or PhD preferred.\nCertifications in AI/ML, Cloud (e.g., AWS, Azure, Google Cloud), or Data Engineering are a plus.\nProven professional experience with advanced NLP and Generative AI solutions.\nCommitment to continuous learning to keep pace with rapidly evolving AI technologies.\nProfessional Competencies\nStrong analytical and problem-solving capabilities.\nExcellent communication skills, capable of translating complex technical concepts.\nCollaborative team player with experience working across global teams.\nAdaptability to rapidly changing project scopes and emerging AI trends.\nInnovation-driven mindset with a focus on delivering impactful solutions.\nTime management skills to prioritize and manage multiple projects effectively.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'data management', 'data processing', 'pipeline', 'big data', 'continuous integration', 'kubernetes', 'deploying models', 'natural language processing', 'ci/cd', 'fms', 'artificial intelligence', 'docker', 'sql', 'microservices', 'tensorflow', 'java', 'pytorch', 'jenkins', 'keras', 'aws']",2025-06-12 06:16:27
Big Data Developer - N,Infosys,5 - 10 years,Not Disclosed,"['Hyderabad', 'Pune', 'Bengaluru']","Job description\n\nHiring for Bigdata Developer with experience range 5 to 15 years.\n\nMandatory Skills: Bigdata, Scala, Spark, Hive, Kafka\n\nEducation: BE/B.Tech/MCA/M.Tech/MSc./MSts",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Hive', 'SCALA', 'Big Data', 'Kafka', 'Spark', 'Bigdata Technologies']",2025-06-12 06:16:29
DataBricks - Data Engineering Professional,Wipro,3 - 5 years,Not Disclosed,['Pune'],"Role Purpose\nThe purpose of this role is to design, test and maintain software programs for operating systems or applications which needs to be deployed at a client end and ensure its meet 100% quality assurance parameters\n\nDo\n1. Instrumental in understanding the requirements and design of the product/ software\nDevelop software solutions by studying information needs, studying systems flow, data usage and work processes\nInvestigating problem areas followed by the software development life cycle\nFacilitate root cause analysis of the system issues and problem statement\nIdentify ideas to improve system performance and impact availability\nAnalyze client requirements and convert requirements to feasible design\nCollaborate with functional teams or systems analysts who carry out the detailed investigation into software requirements\nConferring with project managers to obtain information on software capabilities\n\n\n\n2. Perform coding and ensure optimal software/ module development\nDetermine operational feasibility by evaluating analysis, problem definition, requirements, software development and proposed software\nDevelop and automate processes for software validation by setting up and designing test cases/scenarios/usage cases, and executing these cases\nModifying software to fix errors, adapt it to new hardware, improve its performance, or upgrade interfaces.\nAnalyzing information to recommend and plan the installation of new systems or modifications of an existing system\nEnsuring that code is error free or has no bugs and test failure\nPreparing reports on programming project specifications, activities and status\nEnsure all the codes are raised as per the norm defined for project / program / account with clear description and replication patterns\nCompile timely, comprehensive and accurate documentation and reports as requested\nCoordinating with the team on daily project status and progress and documenting it\nProviding feedback on usability and serviceability, trace the result to quality risk and report it to concerned stakeholders\n\n\n\n3. Status Reporting and Customer Focus on an ongoing basis with respect to project and its execution\nCapturing all the requirements and clarifications from the client for better quality work\nTaking feedback on the regular basis to ensure smooth and on time delivery\nParticipating in continuing education and training to remain current on best practices, learn new programming languages, and better assist other team members.\nConsulting with engineering staff to evaluate software-hardware interfaces and develop specifications and performance requirements\nDocument and demonstrate solutions by developing documentation, flowcharts, layouts, diagrams, charts, code comments and clear code\nDocumenting very necessary details and reports in a formal way for proper understanding of software from client proposal to implementation\nEnsure good quality of interaction with customer w.r.t. e-mail content, fault report tracking, voice calls, business etiquette etc\nTimely Response to customer requests and no instances of complaints either internally or externally\n\n\n\nDeliver\n\nNo.Performance ParameterMeasure1.Continuous Integration, Deployment & Monitoring of Software100% error free on boarding & implementation, throughput %, Adherence to the schedule/ release plan2.Quality & CSATOn-Time Delivery, Manage software, Troubleshoot queries,Customer experience, completion of assigned certifications for skill upgradation3.MIS & Reporting100% on time MIS & report generation\nMandatory Skills: DataBricks - Data Engineering. Experience: 3-5 Years.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'data bricks', 'software development life cycle', 'continuous integration', 'software development', 'mis', 'software management', 'root cause analysis']",2025-06-12 06:16:32
Senior Data Engineer,Epsilon,5 - 9 years,Not Disclosed,['Bengaluru'],"This position in the Engineering team under the Digital Experience organization. We drive the first mile of the customer experience through personalization of offers and content. We are currently on the lookout for a smart, highly driven engineer.\nYou will be part of a team that is focused on building & managing solutions, pipelines using marketing technology stacks. You will also be expected to Identify and implement improvements including for optimizing data delivery and automate processes/pipelines.\nThe incumbent is also expected to partner with various stakeholders, bring scientific rigor to design and develop high quality solutions.\nCandidate must have excellent verbal and written communication skills and be comfortable working in an entrepreneurial, startup environment within a larger company.\nClick here to view how Epsilon transforms marketing with 1 View, 1 Vision and 1 Voice.\n\nBrief Description of Role:\nExperience with both structured and unstructured data\nExperience working on AdTech or MarTech technologies.\nExperience in relational and non-relational databases and SQL (NoSQL is a plus).\nUnderstanding of Data Modeling, Data Catalog concepts and tools\nAbility to deal with data imperfections such as missing values, outliers, inconsistent formatting, etc.\nCollaborate with other members of the team to ensure high quality deliverables\nLearning and implementing the latest design patterns in data engineering\n\nData Management\nExperience with both structured and unstructured data\nExperience building Data and CI/CD pipelines\nExperience working on AdTech or MarTech technologies is added advantage\nExperience in relational and non-relational databases and SQL (NoSQL is a plus).\nHands on experience building ETL workflows/pipelines on large volumes of data\nGood understanding of Data Modeling, Data Warehouse, Data Catalog concepts and tools\nAble to identify, join, explore, and examine data from multiple disparate sources and formats\nAbility to reduce large quantities of unstructured or formless data and get it into a form in which it can be analyzed\nAbility to deal with data imperfections such as missing values, outliers, inconsistent formatting, etc.\nDevelopment\nAbility to write code in programming languages such as Python and shell script on Linux\nFamiliarity with development methodology such as Agile/Scrum\nLove to learn new technologies, keep abreast of the latest technologies within the cloud architecture, and drive your organization to adapt to emerging best practices\nGood knowledge of working in UNIX/LINUX systems\nQualifications\nBachelors degree in computer science with 5+ years of similar experience\nTech Stack: Python, SQL, Scripting language (preferably JavaScript)\nExperience or knowledge on Adobe Experience Platform (RT-CDP/AEP)\nExperience working in Cloud Platforms (GCP or AWS)\nFamiliarity with automated unit/integration test frameworks\nGood written and spoken communication skills, team player.\nStrong analytic thought process and ability to interpret findings",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'Data Engineering', 'Data Bricks', 'Python', 'SQL', 'Azure Aws', 'AWS']",2025-06-12 06:16:34
Senior Data Engineer,Wavicle Data Solutions,6 - 11 years,15-25 Lacs P.A.,"['Chennai', 'Coimbatore', 'Bengaluru']","Hi Professionals,\n\nWe are looking for Senior Data Engineer for Permanent Role\n\nWork Location: Hybrid Chennai, Coimbatore or Bangalore\n\nExperience: 6 to 12 Years\n\nNotice Period: 0 TO 15 Days or Immediate Joiner.\n\nSkills:\n1. Python\n2. Pyspark\n3. SQL\n4. AWS\n5. GCP\n6. MLOps\n\nInterested can send your resume to gowtham.veerasamy@wavicledata.com.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'GCP', 'AWS', 'Ml']",2025-06-12 06:16:37
Senior Data Engineer,SPAN.IO,5 - 10 years,Not Disclosed,['Bengaluru( Indira Nagar )'],"Senior Data Engineer\n\nOur Mission\n\nSPAN is enabling electrification for all\nWe are a mission-driven company designing, building, and deploying products that electrify the built environment, reduce carbon emissions, and slow the effects of climate change.\nDecarbonization is the process to reduce or remove greenhouse gas emissions, especially carbon dioxide, from entering our atmosphere.\nElectrification is the process of replacing fossil fuel appliances that run on gas or oil with all-electric upgrades for a cleaner way to power our lives.\n\nAt SPAN, we believe in:\nEnabling homes and vehicles powered by clean energy\nMaking electrification upgrades possible\nBuilding more resilient homes with reliable backup\nDesigning a flexible and distributed electrical grid\n\nThe Role\nAs a Data Engineer you would be working to design, build, test and create infrastructure necessary for real time analytics and batch analytics pipelines. You will work with multiple teams within the org to provide analysis, insights on the data. You will also be involved in writing ETL processes that support data ingestion. You will also guide and enforce best practices for data management, governance and security. You will build infrastructure to monitor these data pipelines / ETL jobs / tasks and create tooling/infrastructure for providing visibility into these.\n\nResponsibilities\nWe are looking for a Data Engineer with passion for building data pipelines, working with product, data science and business intelligence teams and delivering great solutions. As a part of the team you:-\nAcquire deep business understanding on how SPAN data flows from IoT device to cloud through the system and build scalable and optimized data solutions that impact many stakeholders.\nBe an advocate for data quality and excellence of our platform.\nBuild tools that help streamline the management and operation of our data ecosystem.\nEnsure best practices and standards in our data ecosystem are shared across teams.\nWork with teams within the company to build close relationships with our partners to understand the value our platform can bring and how we can make it better.\nImprove data discovery by creating data exploration processes and promoting adoption of data sources across the company.\nHave a desire to write tools and applications to automate work rather than do everything by hand.\nAssist internal teams in building out data logging, alerting and monitoring for their applications\nAre passionate about CI/CD process.\nDesign, develop and establish KPIs to monitor analysis and provide strategic insights to drive growth and performance.\n\nAbout You\n\nRequired Qualifications\nBachelor's Degree in a quantitative discipline: computer science, statistics, operations research, informatics, engineering, applied mathematics, economics, etc.\n5+ years of relevant work experience in data engineering, business intelligence, research or related fields.\nExpert level production-grade, programming experience in at least one of these languages (Python, Kotlin, or other JVM based languages)\nExperience in writing clean, concise and well structured code in one of the above languages.\nExperience working with Infrastructure-as-code tools: Pulumi, Terraform, etc.\nExperience working with CI/CD systems: Circle-CI, Github Actions, Argo-CD, etc.\nExperience managing data engineering infrastructure through Docker and Kubernetes\nExperience working with latency data processing solutions like Flink, Prefect, AWS Kinesis, Kafka, Spark Stream processing etc.\nExperience with SQL/Relational databases, OLAP databases like Snowflake.\nExperience working in AWS: S3, Glue, Athena, MSK, EMR, ECR etc.\n\nBonus Qualifications\nExperience with the Energy industry\nExperience with building IoT and/or hardware products\nUnderstanding of electrical systems and residential loads\nExperience with data visualization using Tableau.\nExperience in Data loading tools like FiveTran as well as data debugging tools such as DataDog\n\nLife at SPAN\nOur Bengaluru team plays a pivotal role in SPANs continued growth and expansion. Together, were driving engineering, product development, and operational excellence to shape the future of home energy solutions.\nAs part of our team in India, youll have the opportunity to collaborate closely with our teams in the US and across the globe. This international collaboration fosters innovation, learning, and growth, while helping us achieve our bold mission of electrifying homes and advancing clean energy solutions worldwide.\nOur in-office culture offers the chance for dynamic interactions and hands-on teamwork, making SPAN a truly collaborative environment where every team members contribution matters.\nOur climate-focused culture is driven by a team of forward-thinkers, engineers, and problem-solvers who push boundaries every day.\nDo mission-driven work: Every role at SPAN directly advances clean energy adoption.\nBring powerful ideas to life: We encourage diverse ideas and perspectives to drive stronger products.\nNurture an innovation-first mindset: We encourage big thinking and bold action.\nDeliver exceptional customer value: We value hard work, and the ability to deliver exceptional customer value.\n\nBenefits at SPAN India\nGenerous paid leave\nComprehensive Insurance & Health Benefits\nCentrally located office in Bengaluru with easy access to public transit, dining, and city amenities\n\nInterested in joining our team? Apply today and well be in touch with the next steps!",Industry Type: Electronics Manufacturing,"Department: Production, Manufacturing & Engineering","Employment Type: Full Time, Permanent","['Terraform', 'Snowflake', 'AWS', 'Python', 'SQL', 'Java', 'Apache Flink', 'Kotlin']",2025-06-12 06:16:40
Data Engineer,Dun & Bradstreet,5 - 9 years,Not Disclosed,['Hyderabad'],"Key Responsibilities:\n1. Design, build, and deploy new data pipelines within our Big Data Eco-Systems using Streamsets/Talend/Informatica BDM etc. Document new/existing pipelines, Datasets.\n2. Design ETL/ELT data pipelines using StreamSets, Informatica or any other ETL processing engine. Familiarity with Data Pipelines, Data Lakes and modern Data Warehousing practices (virtual data warehouse, push down analytics etc.)\n3. Expert level programming skills on Python\n4. Expert level programming skills on Spark\n5. Cloud Based Infrastructure: GCP\n6. Experience with one of the ETL Informatica, StreamSets in creation of complex parallel loads, Cluster Batch Execution and dependency creation using Jobs/Topologies/Workflows etc.,\n7. Experience in SQL and conversion of SQL stored procedures into Informatica/StreamSets, Strong exposure working with web service origins/targets/processors/executors, XML/JSON Sources and Restful APIs.\n8. Strong exposure working with relation databases DB2, Oracle & SQL Server including complex SQL constructs and DDL generation.\n9. Exposure to Apache Airflow for scheduling jobs\n10. Strong knowledge of Big data Architecture (HDFS), Cluster installation, configuration, monitoring, cluster security, cluster resources management, maintenance, and performance tuning\n11. Create POCs to enable new workloads and technical capabilities on the Platform.\n12. Work with the platform and infrastructure engineers to implement these capabilities in production.\n13. Manage workloads and enable workload optimization including managing resource allocation and scheduling across multiple tenants to fulfill SLAs.\n14. Participate in planning activities, Data Science and perform activities to increase platform skills\n\nKey Requirements:\n1. Minimum 6 years of experience in ETL/ELT Technologies, preferably StreamSets/Informatica/Talend etc.,\n2. Minimum of 6 years hands-on experience with Big Data technologies e.g. Hadoop, Spark, Hive.\n3. Minimum 3+ years of experience on Spark\n4. Minimum 3 years of experience in Cloud environments, preferably GCP\n5. Minimum of 2 years working in a Big Data service delivery (or equivalent) roles focusing on the following disciplines:\n6. Any experience with NoSQL and Graph databases\n7. Informatica or StreamSets Data integration (ETL/ELT)\n8. Exposure to role and attribute based access controls\n9. Hands on experience with managing solutions deployed in the Cloud, preferably on GCP\n10. Experience working in a Global company, working in a DevOps model is a plus",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Airflow', 'GCP', 'Data engineer', 'Spark', 'ETL']",2025-06-12 06:16:42
Data Engineer,Infiniti Research,3 - 7 years,22.5-25 Lacs P.A.,['Bengaluru'],"Role & responsibilities\n3-6 years of experience in Data Engineering Pipeline Ownership and Quality Assurance, with hands-on expertise in building, testing, and maintaining data pipelines.\nProficiency with Azure Data Factory (ADF), Azure Databricks (ADB), and PySpark for data pipeline orchestration and processing large-scale datasets.\nStrong experience in writing SQL queries and performing data validation, data profiling, and schema checks.\nExperience with big data validation, including schema enforcement, data integrity checks, and automated anomaly detection.\nAbility to design, develop, and implement automated test cases to monitor and improve data pipeline efficiency.\nDeep understanding of Medallion Architecture (Raw, Bronze, Silver, Gold) for structured data flow management.\nHands-on experience with Apache Airflow for scheduling, monitoring, and managing workflows.\nStrong knowledge of Python for developing data quality scripts, test automation, and ETL validations.\nFamiliarity with CI/CD pipelines for deploying and automating data engineering workflows.\nSolid data governance and data security practices within the Azure ecosystem.\n\nAdditional Requirements:\nOwnership of data pipelines ensuring end-to-end execution, monitoring, and troubleshooting failures proactively.\nStrong stakeholder management skills, including follow-ups with business teams across multiple regions to gather requirements, address issues, and optimize processes.\nTime flexibility to align with global teams for efficient communication and collaboration.\nExcellent problem-solving skills with the ability to simulate and test edge cases in data processing environments.\nStrong communication skills to document and articulate pipeline issues, troubleshooting steps, and solutions effectively.\nExperience with Unity Catalog or willingness to learn.\n\nPreferred candidate profile\nImmediate Joiner's",Industry Type: Analytics / KPO / Research,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['ADF', 'pyspark', 'Unity Catalog', 'ADB', 'SQL', 'Medallion Architecture']",2025-06-12 06:16:45
Senior Associate - Data Science,Axtria,2 - 5 years,Not Disclosed,['Noida'],"Be an Individual Contributor in the Analytics and Development team and solve real-world problems using cutting-edge capabilities and emerging technologies based on LLM/GenAI/GPT\nSoftware development experience in python is needed as backend for UI based applications\nBe a part of large delivery teams working on advanced projects when expert assistance is required.\nDeliver advanced Data Science capabilities to businesses in a meaningful manner through successful proof-of-concept solutions, and later smoothly transition the proof-of-concept into production.\nCreate Technical documents, develop, test, and deploy data analytics processes using Python, SQL on Azure/AWS platforms\nCan interact with client on GenAI related capabilities and use cases\n\n\nMust have\n\nSkills:\n\n\nMinimum of 3-5years develop, test, and deploy Python based applications on Azure/AWS platforms\nMust have basic knowledge on concepts of Generative AI / LLMs / GPT\nDeep understanding of architecture and work experience on Web Technologies\nPython, SQL hands-on experience\nExpertise in any popular python web frameworks e.g. flask, Django etc.\nFamiliarity with frontend technologies like HTML, JavaScript, REACT",Industry Type: Analytics / KPO / Research,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'software testing', 'gpm', 'microsoft azure', 'python web framework', 'data analytics', 'neural networks', 'aws stack', 'machine learning', 'javascript', 'artificial intelligence', 'sql', 'react.js', 'deep learning', 'django', 'data science', 'html', 'flask', 'aws']",2025-06-12 06:16:47
"Data Engineering : Sr Software Engineer, Tech Lead & Sr Tech Lead",Reflion Tech,7 - 12 years,22.5-37.5 Lacs P.A.,"['Mumbai( Ghansoli )', 'Navi Mumbai', 'Mumbai (All Areas)']","Hiring: Data Engineering Senior Software Engineer / Tech Lead / Senior Tech Lead\n\n- Hybrid (3 Days from office) | Shift: 2 PM 11 PM IST\n- Experience: 5 to 12+ years (based on role & grade)\n\nOpen Grades/Roles:\nSenior Software Engineer: 58 Years\nTech Lead: 7–10 Years\nSenior Tech Lead: 10–12+ Years\n\nJob Description – Data Engineering Team\n\nCore Responsibilities (Common to All Levels):\n\nDesign, build and optimize ETL/ELT pipelines using tools like Pentaho, Talend, or similar\nWork on traditional databases (PostgreSQL, MSSQL, Oracle) and MPP/modern systems (Vertica, Redshift, BigQuery, MongoDB)\nCollaborate cross-functionally with BI, Finance, Sales, and Marketing teams to define data needs\nParticipate in data modeling (ER/DW/Star schema), data quality checks, and data integration\nImplement solutions involving messaging systems (Kafka), REST APIs, and scheduler tools (Airflow, Autosys, Control-M)\nEnsure code versioning and documentation standards are followed (Git/Bitbucket)\n\nAdditional Responsibilities by Grade\n\nSenior Software Engineer (5–8 Yrs):\nFocus on hands-on development of ETL pipelines, data models, and data inventory\nAssist in architecture discussions and POCs\nGood to have: Tableau/Cognos, Python/Perl scripting, GCP exposure\n\nTech Lead (7–10 Yrs):\nLead mid-sized data projects and small teams\nDecide on ETL strategy (Push Down/Push Up) and performance tuning\nStrong working knowledge of orchestration tools, resource management, and agile delivery\n\nSenior Tech Lead (10–12+ Yrs):\nDrive data architecture, infrastructure decisions, and internal framework enhancements\nOversee large-scale data ingestion, profiling, and reconciliation across systems\nMentoring junior leads and owning stakeholder delivery end-to-end\nAdvantageous: Experience with AdTech/Marketing data, Hadoop ecosystem (Hive, Spark, Sqoop)\n\n- Must-Have Skills (All Levels):\n\nETL Tools: Pentaho / Talend / SSIS / Informatica\nDatabases: PostgreSQL, Oracle, MSSQL, Vertica / Redshift / BigQuery\nOrchestration: Airflow / Autosys / Control-M / JAMS\nModeling: Dimensional Modeling, ER Diagrams\nScripting: Python or Perl (Preferred)\nAgile Environment, Git-based Version Control\nStrong Communication and Documentation",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'SQL', 'ETL', 'Orchestration', 'Postgresql', 'Peri', 'Informatica', 'ETL Tool', 'SSIS', 'Elt', 'Modeling', 'MongoDB', 'Data Architecture', 'Talend', 'Pentaho', 'Python']",2025-06-12 06:16:50
Lead AWS Glue Data Engineer,Allegis Group,8 - 13 years,Not Disclosed,[],"Lead AWS Glue Data Engineer\nJob Location : Hyderabad / Bangalore / Chennai / Noida/ Gurgaon / Pune / Indore / Mumbai/ Kolkata\n\nWe are seeking a skilled Lead AWS Data Engineer with 8+ years of strong programming and SQL skills to join our team. The ideal candidate will have hands-on experience with AWS Data Analytics services and a basic understanding of general AWS services. Additionally, prior experience with Oracle and Postgres databases and secondary skills in Python and Azure DevOps will be an advantage.\n\nKey Responsibilities:\nDesign, develop, and optimize data pipelines using AWS Data Analytics services such as RDS, DMS, Glue, Lambda, Redshift, and Athena.\nImplement data migration and transformation processes using AWS DMS and Glue.\nWork with SQL (Oracle & Postgres) to query, manipulate, and analyse large datasets.\nDevelop and maintain ETL/ELT workflows for data ingestion and transformation.\nUtilize AWS services like S3, IAM, CloudWatch, and VPC to ensure secure and efficient data operations.\nWrite clean and efficient Python scripts for automation and data processing.\nCollaborate with DevOps teams using Azure DevOps for CI/CD pipelines and infrastructure management.\nMonitor and troubleshoot data workflows to ensure high availability and performance.\n\nPreferred Qualifications:\nAWS certifications in Data Analytics, Solutions Architect, or DevOps.\nExperience with data warehousing concepts and data lake implementations.\nHands-on experience with Infrastructure as Code (IaC) tools like Terraform or CloudFormation.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['RDS', 'Glue', 'DMS', 'Lambda', 'Redshift', 'Athena']",2025-06-12 06:16:52
Big Data Developer/Data Engineer,Grid Dynamics,5 - 10 years,Not Disclosed,['Bengaluru'],"Role & responsibilities\nExperience: 5 - 8 years\nEmployment Type: Full-Time\n\nJob Summary:\nWe are looking for a highly skilled Scala and Spark Developer to join our data engineering team. The ideal candidate will have strong experience in building scalable data processing solutions using Apache Spark and writing robust, high-performance applications in Scala. You will work closely with data scientists, data analysts, and product teams to design, develop, and optimize large-scale data pipelines and ETL workflows.\n\nKey Responsibilities:\nDevelop and maintain scalable data processing pipelines using Apache Spark and Scala.\nWork on batch and real-time data processing using Spark (RDD/DataFrame/Dataset).\nWrite efficient and maintainable code following best practices and coding standards.\nCollaborate with cross-functional teams to understand data requirements and implement solutions.\nOptimize performance of Spark jobs and troubleshoot data-related issues.\nIntegrate data from multiple sources and ensure data quality and consistency.\nParticipate in design reviews, code reviews, and provide technical leadership when needed.\nContribute to data modeling, schema design, and architecture discussions.\nRequired Skills:\nStrong programming skills in Scala.\nExpertise in Apache Spark (Core, SQL, Streaming).\nHands-on experience with distributed computing and large-scale data processing.\nExperience with data formats like Parquet, Avro, ORC, and JSON.\nGood understanding of functional programming concepts.\nFamiliarity with data ingestion tools (Kafka, Flume, Sqoop, etc.).\nExperience working with Hadoop ecosystem (HDFS, Hive, YARN, etc.) is a plus.\nStrong SQL skills and experience working with relational and NoSQL databases.\nExperience with version control tools like Git.\nPreferred Qualifications:\nBachelor's or Masters degree in Computer Science, Engineering, or related field.\nExperience with cloud platforms like AWS, Azure, or GCP (especially EMR, Databricks, etc.).\nKnowledge of containerization (Docker, Kubernetes) is a plus.\nFamiliarity with CI/CD tools and DevOps practices.ndidate profile",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Scala', 'Pyspark', 'Spark']",2025-06-12 06:16:54
Senior Data Engineer,Talentien Global Solutions,4 - 8 years,12-18 Lacs P.A.,"['Hyderabad', 'Chennai', 'Coimbatore']","We are seeking a skilled and motivated Data Engineer to join our dynamic team. The ideal candidate will have experience in designing, developing, and maintaining scalable data pipelines and architectures using Hadoop, PySpark, ETL processes, and Cloud technologies.\n\nResponsibilities:\nDesign, develop, and maintain data pipelines for processing large-scale datasets.\nBuild efficient ETL workflows to transform and integrate data from multiple sources.\nDevelop and optimize Hadoop and PySpark applications for data processing.\nEnsure data quality, governance, and security standards are met across systems.\nImplement and manage Cloud-based data solutions (AWS, Azure, or GCP).\nCollaborate with data scientists and analysts to support business intelligence initiatives.\nTroubleshoot performance issues and optimize query executions in big data environments.\nStay updated with industry trends and advancements in big data and cloud technologies.\nRequired Skills:\nStrong programming skills in Python, Scala, or Java.\nHands-on experience with Hadoop ecosystem (HDFS, Hive, Spark, etc.).\nExpertise in PySpark for distributed data processing.\nProficiency in ETL tools and workflows (SSIS, Apache Nifi, or custom pipelines).\nExperience with Cloud platforms (AWS, Azure, GCP) and their data-related services.\nKnowledge of SQL and NoSQL databases.\nFamiliarity with data warehousing concepts and data modeling techniques.\nStrong analytical and problem-solving skills.\n\nInterested can reach us at +91 7305206696/ saranyadevib@talentien.com",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'Data Engineering', 'Hadoop', 'Spark', 'ETL', 'Airflow', 'Etl Pipelines', 'Big Data', 'EMR', 'Gcp Cloud', 'Data Bricks', 'Azure Cloud', 'Data Pipeline', 'SCALA', 'Snowflake', 'Data Lake', 'Data Warehousing', 'Data Modeling', 'AWS', 'Python']",2025-06-12 06:16:57
Collibra Data Governance Engineer,Allegis Group,6 - 11 years,Not Disclosed,[],"Collibra Data Governance Engineer\nJob Location : Hyderabad / Bangalore / Chennai / Noida/ Gurgaon / Pune / Indore / Mumbai/ Kolkata\nRequired Skills\n5+ years of experience in data governance and/or metadata management.\nHands-on experience with Collibra Data Governance Center (Collibra DGC), including workflow configuration, cataloging, and operating model customization.\nStrong knowledge of metadata management, data lineage, and data quality principles.\nHands-on experience with Snowflake\nFamiliarity with data integration tools and AWS cloud platform\nExperience with SQL and working knowledge of relational databases.\nUnderstanding of data privacy regulations (e.g., GDPR, CCPA) and compliance frameworks.\nPreferred Skills\nCertifications such as Collibra Certified Solution Architect.\nExperience integrating Collibra with tools like Snowflake, Tableau or other BI/analytics platforms.\nExposure to DataOps, MDM (Master Data Management), and data governance frameworks like DAMA-DMBOK.\nStrong communication and stakeholder management skills.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Collibra', 'Metadata', 'Data Governance']",2025-06-12 06:16:59
Azure Data Engineer,HTC Global Services,4 - 8 years,Not Disclosed,['Bengaluru( Murugeshpalya )'],"Job Summary:\nWe are looking for a highly skilled Azure Data Engineer with experience in building and managing scalable data pipelines using Azure Data Factory, Synapse, and Databricks. The ideal candidate should be proficient in big data tools and Azure services, with strong programming knowledge and a solid understanding of data architecture and cloud platforms.\n\nKey Responsibilities:",,,,"['Power Bi', 'Azure Databricks', 'Azure Data Factory', 'Synapse', 'Python', 'Java', 'Scala', 'Kafka', 'big data tools', 'SQL', 'EventHub', 'Azure cloud services', 'Spark']",2025-06-12 06:17:01
"Senior Data Engineer - Airflow, PLSQL",Relanto Global,5 - 10 years,Not Disclosed,['Bengaluru'],"PositionSenior Data Engineer - Airflow, PLSQL \n\n Experience5+ Years \n\n LocationBangalore/Hyderabad/Pune \n\n\n\nSeeking a Senior Data Engineer with strong expertise in Apache Airflow and Oracle PL/SQL, along with working experience in Snowflake and Agile methodologies. The ideal candidate will also take up Scrum Master responsibilities and lead a data engineering scrum team to deliver robust, scalable data solutions.\n\n\n Key Responsibilities: \nDesign, develop, and maintain scalable data pipelines using Apache Airflow.\nWrite and optimize complex PL/SQL queries, procedures, and packages on Oracle databases.\nCollaborate with cross-functional teams to design efficient data models and integration workflows.\nWork with Snowflake for data warehousing and analytics use cases.\nOwn the delivery of sprint goals, backlog grooming, and facilitation of agile ceremonies as the Scrum Master.\nMonitor pipeline health and troubleshoot production data issues proactively.\nEnsure code quality, documentation, and best practices across the team.\nMentor junior data engineers and promote a culture of continuous improvement.\n\n\n Required Skills and Qualifications: \n5+ years of experience as a Data Engineer in enterprise environments.\nStrong expertise in  Apache Airflow  for orchestrating workflows.\nExpert in  Oracle PL/SQL  - stored procedures, performance tuning, debugging.\nHands-on experience with  Snowflake  - data modeling, SQL, optimization.\nWorking knowledge of version control (Git) and CI/CD practices.\nPrior experience or certification as a  Scrum Master  is highly desirable.\nStrong analytical and problem-solving skills with attention to detail.\nExcellent communication and leadership skills.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sql', 'plsql', 'stored procedures', 'oracle pl', 'performance tuning', 'hive', 'continuous integration', 'ci/cd', 'data warehousing', 'pyspark', 'git', 'apache', 'data modeling', 'spark', 'debugging', 'hadoop', 'big data', 'snowflake', 'python', 'oracle', 'sql queries', 'airflow', 'data engineering', 'agile', 'sqoop']",2025-06-12 06:17:04
"Senior Data Engineer Databricks, ADF, PySpark",Suzva Software Technologies,6 - 11 years,Not Disclosed,"['Mumbai', 'Delhi / NCR', 'Bengaluru']","Senior Data Engineer (Remote, Contract 6 Months) Databricks, ADF, and PySpark.\nWe are hiring a Senior Data Engineer for a 6-month remote contract position. The ideal candidate is highly skilled in building scalable data pipelines and working within the Azure cloud ecosystem, especially Databricks, ADF, and PySpark. You'll work closely with cross-functional teams to deliver enterprise-level data engineering solutions.\n\nKeyResponsibilities\nBuild scalable ETL pipelines and implement robust data solutions in Azure.\n\nManage and orchestrate workflows using ADF, Databricks, ADLS Gen2, and Key Vaults.\n\nDesign and maintain secure and efficient data lake architecture.\n\nWork with stakeholders to gather data requirements and translate them into technical specs.\n\nImplement CI/CD pipelines for seamless data deployment using Azure DevOps.\n\nMonitor data quality, performance bottlenecks, and scalability issues.\n\nWrite clean, organized, reusable PySpark code in an Agile environment.\n\nDocument pipelines, architectures, and best practices for reuse.\n\nMustHaveSkills\nExperience: 6+ years in Data Engineering\n\nTech Stack: SQL, Python, PySpark, Spark, Azure Databricks, ADF, ADLS Gen2, Azure DevOps, Key Vaults\n\nCore Expertise: Data Warehousing, ETL, Data Pipelines, Data Modelling, Data Governance\n\nAgile, SDLC, Containerization (Docker), Clean coding practices\n\nGoodToHaveSkills\nEvent Hubs, Logic Apps\n\nPower BI\n\nStrong logic building and competitive programming background\n\nLocation : - Remote,Mumbai, Delhi / NCR, Bengaluru , Kolkata, Chennai, Hyderabad, Ahmedabad, Pune",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Databricks', 'ADF', 'PySpark', 'ADLS Gen2', 'Azure Databricks', 'Key Vaults', 'Spark', 'Azure DevOps', 'SQL', 'Python']",2025-06-12 06:17:07
Senior ML Compiler Engineer,Qualcomm,0 - 5 years,Not Disclosed,['Hyderabad'],"Job Area: Engineering Group, Engineering Group > Software Engineering\n\nGeneral Summary:\n\nInterested in accelerating machine learning and artificial intelligence on mobile devices for millions of usersCome join our team. We are building software platforms that enable users of Qualcomms silicon to construct optimized neural networks and machine learning algorithms. We are looking for software engineers with a machine learning or compiler background who will help us build these software platforms. In this role, you will construct and tune machine learning frameworks, build compilers and tools, and collaborate with Qualcomm hardware and software engineers to enable efficient usage of Qualcomms silicon for machine learning applications.\n\nMinimum qualifications:\nBachelors degree in Engineering, Information Systems, Computer Science, or related field.\nProgramming in C/C++\n0 to 10 years of software engineering or related work experience\n\n\nPreferred qualifications:\nExperience in machine learning frameworks such as MxNet/NNVM/TVM, Pytorch, Tensorflow, Caffe\n\nOR experience in compilers with an interest in machine learning\nDeep knowledge of software engineering\n\n\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 2+ years of Software Engineering or related work experience.\nOR\nMaster's degree in Engineering, Information Systems, Computer Science, or related field and 1+ year of Software Engineering or related work experience.\nOR\nPhD in Engineering, Information Systems, Computer Science, or related field.\n\n2+ years of academic or work experience with Programming Language such as C, C++, Java, Python, etc.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'machine learning', 'tensorflow', 'pytorch', 'software engineering', 'algorithms', 'c++', 'natural language processing', 'caffe', 'neural networks', 'mxnet', 'artificial intelligence', 'sql', 'deep learning', 'r', 'java', 'data science', 'computer vision', 'machine learning algorithms', 'ml']",2025-06-12 06:17:09
Data & Gen AI Specialist,Altimetrik,1 - 4 years,Not Disclosed,['Bengaluru'],"Job Title: Data & GenAI AWS Specialist\nExperience: 1-4 Years\nLocation: Bangalore\nMandatory Qualification: B.E./ B.Tech/ M.Tech/ MS from IIT or IISc ONLY\nJob Overview:\nWe are seeking a seasoned Data & GenAI Specialist with deep expertise in AWS Managed Services (PaaS) to join our innovative team. The ideal candidate will have extensive experience in designing sophisticated, scalable architectures for data pipelines and Generative AI (GenAI) solutions leveraging cloud services.",,,,"['Generative Ai', 'Cloud', 'Data Science', 'Open Source', 'Data Pipeline', 'GCP', 'Azure Cloud', 'Snowflake', 'Machine Learning', 'AWS']",2025-06-12 06:17:12
Sr. Data Analyst,Icims,4 - 9 years,Not Disclosed,['Hyderabad'],"Overview\nThe Senior Data Analyst is responsible for serving as a subject matter expert who can lead efforts to analyze data with the goal of delivering insights that will influence our products and customers. This position will report into the Data Analytics Manager, and will work closely with members of our product and marketing teams, data engineers, and members of our Customer Success organization supporting client outreach efforts. The chief functions of this role will be finding and sharing data-driven insights to deliver value to less technical audiences, and instilling best practices for analytics in the rest of the team.",,,,"['server', 'data', 'vlookup', 'market data', 'data mapping', 'dashboards', 'research', 'sql', 'analytics', 'tables', 'prep', 'pivot', 'data visualization', 'communication skills', 'python', 'data analytics', 'data analysis', 'insights', 'pivot table', 'data engineering', 'graph', 'excel', 'data quality', 'tableau', 'data governance', 'root cause']",2025-06-12 06:17:15
Data Analyst,B.M. House India limited,2 - 4 years,3.5-4.5 Lacs P.A.,['Bengaluru( HSR Layout )'],"Microsoft Excel (including PivotTables, VLOOKUP/XLOOKUP, Power Query, Macros, and Charts) to analyze and present data. CANVA and Photoshop experience and digital marketing ensure data accuracy and integrity at all times.",Industry Type: Import & Export,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Canva', 'Advanced Excel', 'Power Bi', 'VLOOKUP', 'Data Management', 'Data Analysis', 'Pivot', 'Digital Marketing', 'Data Reporting']",2025-06-12 06:17:17
Senior Data Manager/ Lead,Codeforce 360,6 - 8 years,Not Disclosed,['Hyderabad'],"Job Description:\nWe are looking for a highly experienced and dynamic Senior Data Manager / Lead to oversee a team of Data Engineers and Data Scientists. This role demands a strong background in data platforms such as Snowflake and proficiency in Python, combined with excellent people management and project leadership skills. While hands-on experience in the technologies is beneficial, the primary focus of this role is on team leadership, strategic planning, and project delivery .\n\nJob Title : Senior Data Manager / Lead\nLocation: Hyderabad (Work From Office)\nShift Timing: 10AM-7PM\nKey Responsibilities:\nLead, mentor, and manage a team of Data Engineers and Data Scientists.\nOversee the design and implementation of data pipelines and analytics solutions using Snowflake and Python.\nCollaborate with cross-functional teams (product, business, engineering) to align data solutions with business goals.\nEnsure timely delivery of projects, with high quality and performance.\nConduct performance reviews, training plans, and support career development for the team.\nSet priorities, allocate resources, and manage workloads within the data team.\nDrive adoption of best practices in data management, governance, and documentation.\nEvaluate new tools and technologies relevant to data engineering and data science.\n\nRequired Skills & Qualifications:\n6+ years of experience in data-related roles, with at least 23 years in a leadership or management position.\nStrong understanding of Snowflake architecture, performance tuning, data sharing, security, etc.\nSolid knowledge of Python for data engineering or data science tasks.\nExperience in leading data migration, ETL/ELT, and analytics projects.\nAbility to translate business requirements into technical solutions.\nExcellent leadership, communication, and stakeholder management skills.\nExposure to tools like Databricks, Dataiku, Airflow, or similar platforms is a plus.\nBachelors or Master’s degree in Computer Science, Engineering, Mathematics, or a related field.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Snowflake', 'Data Bricks', 'Python', 'Airflow', 'Data Migration', 'Dataiku', 'Data Warehousing', 'ETL', 'ELT', 'SQL']",2025-06-12 06:17:19
Business Data Analyst,CGI,5 - 8 years,Not Disclosed,['Hyderabad'],"Business Data Analyst - HealthCare\n\nJob Summary\nWe are seeking an experienced and results-driven Business Data Analyst with 5+ years of hands-on experience in data analytics, visualization, and business insight generation. This role is ideal for someone who thrives at the intersection of business and datatranslating complex data sets into compelling insights, dashboards, and strategies that support decision-making across the organization.\nYou will collaborate closely with stakeholders across departments to identify business needs, design and build analytical solutions, and tell compelling data stories using advanced visualization tools.\nKey Responsibilities\nData Analytics & Insights Analyze large and complex data sets to identify trends, anomalies, and opportunities that help drive business strategy and operational efficiency.\n• Dashboard Development & Data Visualization Design, develop, and maintain interactive dashboards and visual reports using tools like Power BI, Tableau, or Looker to enable data-driven decisions.\n• Business Stakeholder Engagement Collaborate with cross-functional teams to understand business goals, define metrics, and convert ambiguous requirements into concrete analytical deliverables.\n• KPI Definition & Performance Monitoring Define, track, and report key performance indicators (KPIs), ensuring alignment with business objectives and consistent measurement across teams.\n• Data Modeling & Reporting Automation Work with data engineering and BI teams to create scalable, reusable data models and automate recurring reports and analysis processes.\n• Storytelling with Data Communicate findings through clear narratives supported by data visualizations and actionable recommendations to both technical and non-technical audiences.\n• Data Quality & Governance Ensure accuracy, consistency, and integrity of data through validation, testing, and documentation practices.\nRequired Qualifications\nBachelor’s or Master’s degree in Business, Economics, Statistics, Computer Science, Information Systems, or a related field.\n• 5+ years of professional experience in a data analyst or business analyst role with a focus on data visualization and analytics.\n• Proficiency in data visualization tools: Power BI, Tableau, Looker (at least one).\n• Strong experience in SQL and working with relational databases to extract, manipulate, and analyze data.\n• Deep understanding of business processes, KPIs, and analytical methods.\n• Excellent problem-solving skills with attention to detail and accuracy.\n• Strong communication and stakeholder management skills with the ability to explain technical concepts in a clear and business-friendly manner.\n• Experience working in Agile or fast-paced environments.\nPreferred Qualifications\nExperience working with cloud data platforms (e.g., Snowflake, BigQuery, Redshift).\n• Exposure to Python or R for data manipulation and statistical analysis.\n• Knowledge of data warehousing, dimensional modeling, or ELT/ETL processes.\n• Domain experience in Healthcare is a plus.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Bigquery', 'Snowflake', 'Data Warehousing', 'Redshift', 'Python', 'ETL']",2025-06-12 06:17:21
Data Analyst-Having Stratup-Mid-Size companies Exp.@ Bangalore_Urgent,"A leader in this space, we deliver world...",8 - 13 years,Not Disclosed,['Bengaluru'],"Data Analyst\n\nLocation: Bangalore\nExperience: 8 - 15 Yrs\nType: Full-time\n\nRole Overview\n\nWe are seeking a skilled Data Analyst to support our platform powering operational intelligence across airports and similar sectors. The ideal candidate will have experience working with time-series datasets and operational information to uncover trends, anomalies, and actionable insights. This role will work closely with data engineers, ML teams, and domain experts to turn raw data into meaningful intelligence for business and operations stakeholders.\n\nKey Responsibilities\n\nAnalyze time-series and sensor data from various sources\nDevelop and maintain dashboards, reports, and visualizations to communicate key metrics and trends.\nCorrelate data from multiple systems (vision, weather, flight schedules, etc) to provide holistic insights.\nCollaborate with AI/ML teams to support model validation and interpret AI-driven alerts (e.g., anomalies, intrusion detection).\nPrepare and clean datasets for analysis and modeling; ensure data quality and consistency.\nWork with stakeholders to understand reporting needs and deliver business-oriented outputs.\n\n\nQualifications & Required Skills\n\nBachelors or Masters degree in Data Science, Statistics, Computer Science, Engineering, or a related field.\n5+ years of experience in a data analyst role, ideally in a technical/industrial domain.\nStrong SQL skills and proficiency with BI/reporting tools (e.g., Power BI, Tableau, Grafana).\nHands-on experience analyzing structured and semi-structured data (JSON, CSV, time-series).\nProficiency in Python or R for data manipulation and exploratory analysis.\nUnderstanding of time-series databases or streaming data (e.g., InfluxDB, Kafka, Kinesis).\nSolid grasp of statistical analysis and anomaly detection methods.\nExperience working with data from industrial systems or large-scale physical infrastructure.\n\n\nGood-to-Have Skills\n\nDomain experience in airports, smart infrastructure, transportation, or logistics.\nFamiliarity with data platforms (Snowflake, BigQuery, Custom-built using open-source).\nExposure to tools like Airflow, Jupyter Notebooks and data quality frameworks.\nBasic understanding of AI/ML workflows and data preparation requirements.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Kafka', 'SQL', 'airports', 'InfluxDB', 'Airflow', 'structured Data', 'time-series', 'JSON', 'Tableau', 'Grafana', 'R', 'AI/ML', 'Kinesis', 'Snowflake', 'time-series databases', 'Data Preparation', 'Python', 'smart infrastructure', 'BigQuery', 'streaming data', 'Power BI', 'CSV', 'transportation', 'logistic', 'reporting tools']",2025-06-12 06:17:24
Data Engineer,Aqilea Softech,5 - 9 years,13-20 Lacs P.A.,"['Bangalore Rural', 'Bengaluru']","Job Title: Data Engineer\nCompany : Aqilea India(Client : H&M India)\nEmployment Type: Full Time\nLocation: Bangalore(Hybrid)\nExperience: 4.5 to 9 years\nClient : H&M India\n\nAt H&M, we welcome you to be yourself and feel like you truly belong. Help us reimagine the future of an entire industry by making everyone look, feel, and do good. We take pride in our history of making fashion accessible to everyone and led by our values we strive to build a more welcoming, inclusive, and sustainable industry. We are privileged to have more than 120,000 colleagues, in over 75 countries across the world. Thats 120 000 individuals with unique experiences, skills, and passions. At H&M, we believe everyone can make an impact, we believe in giving people responsibility and a strong sense of ownership. Our business is your business, and when you grow, we grow.\nWebsite : https://career.hm.com/\n\nWe are seeking a skilled and forward-thinking Data Engineer to join our Emerging Tech team. This role is designed for someone passionate about working with cutting-edge technologies such as AI, machine learning, IoT, and big data to turn complex data sets into actionable insights.\nAs the Data Engineer in Emerging Tech, you will be responsible for designing, implementing, and optimizing data architectures and processes that support the integration of next-generation technologies. Your role will involve working with large-scale datasets, building predictive models, and utilizing emerging tools to enable data-driven decision-making across the business. You ll collaborate with technical and business teams to uncover insights, streamline data pipelines, and ensure the best use of advanced analytics technologies.\n\nKey Responsibilities:\nDesign and build scalable data architectures and pipelines that support machine learning, analytics, and IoT initiatives.\nDevelop and optimize data models and algorithms to process and analyse large-scale, complex data sets.\nImplement data governance, security, and compliance measures to ensure high-quality\nCollaborate with cross-functional teams (engineering, product, and business) to translate business requirements into data-driven solutions.\nEvaluate, integrate, and optimize new data technologies to enhance analytics capabilities and drive business outcomes.\nApply statistical methods, machine learning models, and data visualization techniques to deliver actionable insights.\nEstablish best practices for data management, including data quality, consistency, and scalability.\nConduct analysis to identify trends, patterns, and correlations within data to support strategic business initiatives.\nStay updated on the latest trends and innovations in data technologies and emerging data management practices.\n\nSkills Required :\nBachelors or masters degree in data science, Computer Science, Engineering, Statistics, or a related field.\n4.5-9 years of experience in data engineering, data science, or a similar analytical role, with a focus on emerging technologies.\nProficiency with big data frameworks (e.g., Hadoop, Spark, Kafka) and experience with modern cloud platforms (AWS, Azure, or GCP).\nSolid skills in Python, SQL, and optionally R, along with experience using machine learning libraries such as Scikit-learn, TensorFlow, or PyTorch.\nExperience with data visualization tools (e.g., Tableau or Power BI or D3.js) to communicate insights effectively.\nFamiliarity with IoT and edge computing data architectures is a plus.\nUnderstanding of data governance, compliance, and privacy standards.\nAbility to work with both structured and unstructured data.\nExcellent problem-solving, communication, and collaboration skills, with the ability to work in a fast-paced, cross-functional team environment.\nA passion for emerging technologies and a continuous desire to learn and innovate.\nInterested Candidates can share your Resumes to mail id karthik.prakadish@aqilea.com",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Powerbi', 'Hadoop', 'Kafka', 'Tableau', 'Azure', 'GCP', 'Data Engineer', 'Spark', 'AWS', 'Python', 'SQL']",2025-06-12 06:17:26
Data Engineer-Having Stratup-Mid-Size company Exp.@ Bangalore_Urgent,"As a leader in this space, we deliver wo...",8 - 13 years,Not Disclosed,['Bengaluru'],"Data Engineer\n\nLocation: Bangalore - Onsite\nExperience: 8 - 15 years\nType: Full-time\n\nRole Overview\n\nWe are seeking an experienced Data Engineer to build and maintain scalable, high-performance data pipelines and infrastructure for our next-generation data platform. The platform ingests and processes real-time and historical data from diverse industrial sources such as airport systems, sensors, cameras, and APIs. You will work closely with AI/ML engineers, data scientists, and DevOps to enable reliable analytics, forecasting, and anomaly detection use cases.\nKey Responsibilities\nDesign and implement real-time (Kafka, Spark/Flink) and batch (Airflow, Spark) pipelines for high-throughput data ingestion, processing, and transformation.\nDevelop data models and manage data lakes and warehouses (Delta Lake, Iceberg, etc) to support both analytical and ML workloads.\nIntegrate data from diverse sources: IoT sensors, databases (SQL/NoSQL), REST APIs, and flat files.\nEnsure pipeline scalability, observability, and data quality through monitoring, alerting, validation, and lineage tracking.\nCollaborate with AI/ML teams to provision clean and ML-ready datasets for training and inference.\nDeploy, optimize, and manage pipelines and data infrastructure across on-premise and hybrid environments.\nParticipate in architectural decisions to ensure resilient, cost-effective, and secure data flows.\nContribute to infrastructure-as-code and automation for data deployment using Terraform, Ansible, or similar tools.\n\n\nQualifications & Required Skills\n\nBachelors or Master’s in Computer Science, Engineering, or related field.\n6+ years in data engineering roles, with at least 2 years handling real-time or streaming pipelines.\nStrong programming skills in Python/Java and SQL.\nExperience with Apache Kafka, Apache Spark, or Apache Flink for real-time and batch processing.\nHands-on with Airflow, dbt, or other orchestration tools.\nFamiliarity with data modeling (OLAP/OLTP), schema evolution, and format handling (Parquet, Avro, ORC).\nExperience with hybrid/on-prem and cloud platforms (AWS/GCP/Azure) deployments.\nProficient in working with data lakes/warehouses like Snowflake, BigQuery, Redshift, or Delta Lake.\nKnowledge of DevOps practices, Docker/Kubernetes, Terraform or Ansible.\nExposure to data observability, data cataloging, and quality tools (e.g., Great Expectations, OpenMetadata).\nGood-to-Have\nExperience with time-series databases (e.g., InfluxDB, TimescaleDB) and sensor data.\nPrior experience in domains such as aviation, manufacturing, or logistics is a plus.\n\nRole & responsibilities\n\n\nPreferred candidate profile",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['aviation', 'Data Modeling', 'Python', 'OLAP', 'Cloud', 'ORC', 'logistics', 'Avro', 'Terraform', 'Snowflake', 'manufacturing', 'AWS', 'Parquet', 'Java', 'Azure', 'BigQuery', 'Data', 'Redshift', 'SQL', 'TimescaleDB', 'GCP', 'InfluxDB', 'dbt', 'Ansible', 'OLTP', 'Kubernetes']",2025-06-12 06:17:29
Big Data Engineer,Rarr Technologies,6 - 8 years,Not Disclosed,['Bengaluru'],Job description\n\nProven experience working with data pipelines ETL BI regardless of the technology\nProven experience working with AWS including at least 3 of RedShift S3 EMR Cloud Formation DynamoDB RDS lambda\nBig Data technologies and distributed systems one of Spark Presto or Hive\nPython language scripting and object oriented\nFluency in SQL for data warehousing RedShift in particular is a plus\nGood understanding on data warehousing and Data modelling concepts\nFamiliar with GIT Linux CICD pipelines is a plus\nStrong systems process orientation with demonstrated analytical thinking organization skills and problem solving skills\nAbility to self manage prioritize and execute tasks in a demanding environment\nStrong consultancy orientation and experience with the ability to form collaborative\nproductive working relationships across diverse teams and cultures is a must\nWillingness and ability to train and teach others\nAbility to facilitate meetings and follow up with resulting action items,Industry Type: IT Services & Consulting,Department: Other,"Employment Type: Full Time, Permanent","['python scripting', 'Big Data Technologies', 'ETL', 'AWS']",2025-06-12 06:17:31
Data Engineer,LTIMindtree,5 - 8 years,Not Disclosed,"['Noida', 'Chennai', 'Bengaluru']","1.Big Data Engineer:\n\nCompany: LTIMINDTREE\nMandotory skills - Python,Pyspark & AWS\nLocation : Noida & Pan India\nExperience : 5-8Years\nSalary: 19LPA\n\nShare your cv at Muktai.S@alphacom.in",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'AWS', 'Python']",2025-06-12 06:17:33
Big Data Engineer,Client of Hiresquad Resources,5 - 8 years,22.5-30 Lacs P.A.,"['Noida', 'Hyderabad', 'Bengaluru']","Role: Data Engineer\nExp: 5 to 8 Years\nLocation: Bangalore, Noida, and Hyderabad (Hybrid, weekly 2 Days office must)\nNP: Immediate to 15 Days (Try to find only immediate joiners)\n\n\nNote:\nCandidate must have experience in Python, Kafka Streams, Pyspark, and Azure Databricks.\nNot looking for candidates who have only Exp in Pyspark and not in Python.\n\n\nJob Title: SSE Kafka, Python, and Azure Databricks (Healthcare Data Project)\nExperience:  5 to 8 years\n\nRole Overview:\nWe are looking for a highly skilled with expertise in Kafka, Python, and Azure Databricks (preferred) to drive our healthcare data engineering projects. The ideal candidate will have deep experience in real-time data streaming, cloud-based data platforms, and large-scale data processing. This role requires strong technical leadership, problem-solving abilities, and the ability to collaborate with cross-functional teams.\n\nKey Responsibilities:\nLead the design, development, and implementation of real-time data pipelines using Kafka, Python, and Azure Databricks.\nArchitect scalable data streaming and processing solutions to support healthcare data workflows.\nDevelop, optimize, and maintain ETL/ELT pipelines for structured and unstructured healthcare data.\nEnsure data integrity, security, and compliance with healthcare regulations (HIPAA, HITRUST, etc.).\nCollaborate with data engineers, analysts, and business stakeholders to understand requirements and translate them into technical solutions.\nTroubleshoot and optimize Kafka streaming applications, Python scripts, and Databricks workflows.\nMentor junior engineers, conduct code reviews, and ensure best practices in data engineering.\nStay updated with the latest cloud technologies, big data frameworks, and industry trends.\n\n\nRequired Skills & Qualifications:\n4+ years of experience in data engineering, with strong proficiency in Kafka and Python.\nExpertise in Kafka Streams, Kafka Connect, and Schema Registry for real-time data processing.\nExperience with Azure Databricks (or willingness to learn and adopt it quickly).\nHands-on experience with cloud platforms (Azure preferred, AWS or GCP is a plus).\nProficiency in SQL, NoSQL databases, and data modeling for big data processing.\nKnowledge of containerization (Docker, Kubernetes) and CI/CD pipelines for data applications.\nExperience working with healthcare data (EHR, claims, HL7, FHIR, etc.) is a plus.\nStrong analytical skills, problem-solving mindset, and ability to lead complex data projects.\nExcellent communication and stakeholder management skills.\n\n\n\nEmail: Sam@hiresquad.in",Industry Type: Medical Services / Hospital,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Kafka', 'Azure Databricks', 'Python', 'Etl Pipelines', 'Pyspark', 'spark architecture', 'Data Engineering', 'opps concepts', 'Data Streaming', 'Medallion Architecture', 'python scripts', 'schema registry', 'SQL Database', 'Nosql Databases', 'spark tuning', 'Kafka Streams', 'kafka connect']",2025-06-12 06:17:36
Data Analyst - Python/Hadoop,Sadup Soft,3 - 6 years,Not Disclosed,['Bengaluru'],"- Minimum of 3 years of hands-on experience.\n\n- Python/ML, Hadoop, Spark : Minimum of 2 years of experience.\n\n- At least 3 years of prior experience as a Data Analyst.\n\n- Detail-oriented with a structured thinking and analytical mindset.\n\n- Proven analytic skills, including data analysis, data validation, and technical writing.\n\n- Strong proficiency in SQL and Excel.\n\n- Experience with Big Query is mandatory.\n\n- Knowledge of Python and machine learning algorithms is a plus.\n\n- Excellent communication skills with the ability to be precise and clear.\n\n- Learning Ability : Ability to quickly learn and adapt to new analytic tools and technologies.\n\nKey Responsibilities :\n\nData Analysis :\n\n- Perform comprehensive data analysis using SQL, Excel, and Big Query.\n\n- Validate data integrity and ensure accuracy across datasets.\n\n- Develop detailed reports and dashboards that provide actionable insights.\n\n- Create and deliver presentations to stakeholders with clear and concise findings.\n\n- Document queries, reports, and analytical processes clearly and accurately.\n\n- Leverage Python/ML for advanced data analysis and model development.\n\n- Utilize Hadoop and Spark for handling and processing large datasets.\n\n- Work closely with cross-functional teams to understand data requirements and provide analytical support.\n\n- Communicate findings effectively and offer recommendations based on data analysis.\n\nEducation : Bachelor's degree in Computer Science, Data Science, Statistics, or a related field.\n\nExperience : Minimum of 3 years of experience as a Data Analyst with a strong focus on SQL, Excel, and Big Query.\n\nTechnical Skills : Proficiency in SQL, Excel, and Big Query; experience with Python, ML, Hadoop, and Spark is preferred.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Analysis', 'Data Validation', 'Big Query', 'Data Integrity', 'Hadoop', 'Spark', 'Python', 'SQL']",2025-06-12 06:17:38
Data Analyst III,Sadup Soft,5 - 8 years,Not Disclosed,['Bengaluru'],"- Minimum of 5 years of experience in data analysis with a strong SQL background.\n\n- Solid experience in creating and extracting metrics, and writing complex SQL scripts.\n\n- Hands-on experience with Tableau, Looker, or any equivalent data visualization tools.\n\n- Strong skills in SQL and Excel, with the ability to quickly learn other analytic tools.\n\n- Knowledge of Python and ML algorithms is a plus.\n\nResponsibilities :\n\n- Perform detailed data analysis and validation to ensure data integrity and accuracy.\n\n- Extract and create meaningful metrics to support business decisions\n\n- Design, develop, and maintain interactive dashboards using Tableau, Looker, or equivalent tools.\n\n- Translate complex data into visually appealing and actionable insights.\n\n- Document queries, reports, and analytical processes clearly and accurately.\n\n- Create detailed reports and presentations to communicate findings to stakeholders.\n\n- Work closely with cross-functional teams to understand data requirements and provide analytical support.\n\n- Communicate findings effectively and provide actionable recommendations.\n\n- Utilize SQL and Excel extensively for data analysis and reporting.\n\n- Apply Python and ML algorithms as needed for advanced analytics (preferred but not mandatory)",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Analysis', 'Data Integrity', 'Data Analyst', 'Data Visualization Tools', 'Tableau', 'Data Analytics', 'Looker']",2025-06-12 06:17:40
Data Engineering - Senior Developer with Salesforce,Job Delights,5 - 10 years,25-27.5 Lacs P.A.,"['Hyderabad', 'Chennai', 'Bengaluru']","Data Engineering with SQL, Python, ETL & SalesForce Marketing Cloud (Must)",Industry Type: Software Product,Department: Other,"Employment Type: Full Time, Permanent","['Salesforce Marketing Cloud', 'SQL', 'Marketing Cloud', 'Salesforce', 'Python']",2025-06-12 06:17:43
Senior Data Engineer,The Main Stage Productions,4 - 6 years,Not Disclosed,['Bengaluru'],"Design and implement cloud-native data architectures on AWS, including data lakes, data warehouses, and streaming pipelines using services like S3, Glue, Redshift, Athena, EMR, Lake Formation, and Kinesis.\nDevelop and orchestrate ETL/ELT pipelines\n\nRequired Candidate profile\nParticipate in pre-sales and consulting activities such as:\nEngaging with clients to gather requirements and propose AWS-based data engineering solutions.\nSupporting RFPs/RFIs, technical proposals",Industry Type: Advertising & Marketing,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['AWS Glue', 'GitHub Actions', 'PySpark', 'Scala', 'CodePipeline', 'Step Functions', 'data engineering']",2025-06-12 06:17:45
Data Engineer,Nemetschek,5 - 10 years,Not Disclosed,"['Hyderabad', 'Bengaluru', 'Mumbai (All Areas)']","Role & responsibilities\n5+ years in software development, with a focus on data-intensive applications, cloud solutions, and scalable data architectures.\nDevelopment experience in GoLang for building scalable and efficient data applications.\nExperience with Snowflake, Redshift, or similar data platforms including architecture, data modeling, performance optimization, and integrations.\nExperience designing and building data lakes and data warehouses, ensuring data integrity, scalability, and performance.\nProficient in developing and managing ETL pipelines, using modern tools and techniques to transform, load, and integrate data efficiently.\nExperience with high-volume event streams (such as Kafka, Kinesis) and near real-time data processing solutions for fast and accurate reporting.\nHands-on experience with Terraform for automating infrastructure deployment and configuration management in cloud environments.\nExperience with containerization technologies (Docker, Kubernetes) and orchestration.\nSolid grasp of database fundamentals (SQL, NoSQL, data modeling, performance tuning)\nExperience with CI/CD pipelines and automation tools for testing, deployment, and continuous improvement.\nExperience working in AWS cloud environments, specifically with big data solutions and serverless architectures\nAbility to mentor and guide junior engineers, fostering a culture of learning and innovation\nStrong communication skills to articulate technical concepts clearly to non-technical stakeholders.\nWHAT WE OFFER\nA young, dynamic, and innovation-oriented environment\nA wide variety of projects within different industries\nA very open and informal culture where knowledge sharing, and employee development are key.\nRoom for personal initiative, development, and growth\nRealistic career opportunities\nCompetitive package and fringe benefits.\n\nPreferred candidate profile",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Golang', 'Snowflake', 'Javascript', 'ETL', 'AWS']",2025-06-12 06:17:47
Azure Data Engineer/Lead/Architect (5 - 20 Years) (Pan India Location),Allegis Group,5 - 10 years,Not Disclosed,[],"Azure Data Engineer/Lead/Architect (5 - 20 Years) (Pan India Location)\nJob Location : Hyderabad / Bangalore / Chennai / Kolkata / Noida/ Gurgaon / Pune / Indore / Mumbai\n\n\n5 -20 years of relevant hands on development experience. And 4+ years as Azure Data Engineering role\nProficient in Azure technologies like ADB, ADF, SQL(capability of writing complex SQL queries), ADB, PySpark, Python, Synapse, Delta Tables, Unity Catalog\nHands on in Python, PySpark or Spark SQL\nHands on in Azure Analytics and DevOps\nTaking part in Proof of Concepts (POCs) and pilot solutions preparation\nAbility to conduct data profiling, cataloguing, and mapping for technical design and construction of technical data flows\nExperience in business processing mapping of data and analytics solutions",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Analytics', 'Azure Data Engineering', 'Azure Databricks', 'Devops', 'Python', 'Azure Data Factory', 'Pyspark', 'Azure', 'Adb']",2025-06-12 06:17:49
Consultant - Lead Data Engineer,Affine Analytics,5 - 8 years,Not Disclosed,['Bengaluru'],"Strong experience with Python, SQL, pySpark, AWS Glue. Good to have - Shell Scripting, Kafka\nGood knowledge of DevOps pipeline usage (Jenkins, Bitbucket, EKS, Lightspeed)\nExperience of AWS tools (AWS S3, EC2, Athena, Redshift, Glue, EMR, Lambda, RDS, Kinesis, DynamoDB, QuickSight etc.).\nOrchestration using Airflow\nGood to have - Streaming technologies and processing engines, Kinesis, Kafka, Pub/Sub and Spark Streaming\nGood debugging skills",,,,"['Python', 'RDS', 'Shell Scripting', 'Kafka', 'AWS Glue', 'DynamoDB', 'Lightspeed', 'EMR', 'EKS', 'pySpark', 'Redshift', 'SQL', 'Jenkins', 'QuickSight', 'Glue', 'EC2', 'Kinesis', 'AWS S3', 'Bitbucket', 'Athena', 'Lambda']",2025-06-12 06:17:52
Consultant - Data Engineer (with Fabric),Affine Analytics,5 - 8 years,Not Disclosed,['Bengaluru'],"We are looking for a highly skilled API & Pixel Tracking Integration Engineer to lead the development and deployment of server-side tracking and attribution solutions across multiple platforms. The ideal candidate brings deep expertise in CAPI integrations (Meta, Google, and other platforms), secure data handling using cryptographic techniques, and experience working within privacy-first environments like Azure Clean Rooms.\nThis role requires strong hands-on experience in C# development, Azure cloud services, OCI (Oracle Cloud Infrastructure), and marketing technology stacks including Adobe Tag Management and Pixel Management. You will work closely with engineering, analytics, and marketing teams to deliver scalable, compliant, and secure data tracking solutions that drive business insights and performance.",,,,"['Adobe Tag Management', 'Data Engineering', 'Azure Data Factory', 'Azure security', 'ADF', 'ADLS', 'Azure Functions', 'Meta CAPI', 'Google Enhanced Conversions', 'Key Vault', 'Cosmos DB']",2025-06-12 06:17:54
Data Engineer,Careerzgraph,2 - 4 years,4.8-7.2 Lacs P.A.,['Bengaluru( Electronic City )'],"Hands On* Experience Required (Code Level) *\nSQL Query Writing - Query Optimization -\nPython Scripting - Programming\n* Design, develop & maintain data pipelines using Python, Azure & Power BI\n* Collaborate with cross-functional teams on ETL projects\n\n\nAssistive technologies\nProvident fund\nHealth insurance",Industry Type: Recruitment / Staffing,"Department: Production, Manufacturing & Engineering","Employment Type: Full Time, Permanent","['SQL Queries', 'Optimization', 'Python', 'Azure', 'Power Bi', 'Stored Procedures', 'Optimization Techniques', 'Programming', 'ETL', 'SQL Scripting']",2025-06-12 06:17:56
Data Architect,Neo Aid,12 - 20 years,48-60 Lacs P.A.,['Bengaluru'],"Data Architect\nBangalore (Pune option).\nHybrid, 2-3 days WFO,\nUp to 60 LPA. Needs GCP, Data Engineering, Analytics, Visualization, Modeling. 1\n5-20 yrs exp, end-to-end data pipeline.\nNotice: 60 days.\nArchitecture - recent 1-2 years\n\n\nProvident fund",Industry Type: Recruitment / Staffing,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Visualizing', 'Data Architecture', 'Data Modeling', 'Data Analytics', 'Gcp Cloud', 'ETL']",2025-06-12 06:17:58
Data Engineer,Lenskart,1 - 4 years,Not Disclosed,['Bengaluru'],"Key Responsibilities\nBuild and maintain scalable ETL/ELT data pipelines using Python and cloud-native tools.\nDesign and optimize data models and queries on Google BigQuery for analytical workloads.\nDevelop, schedule, and monitor workflows using orchestration tools like Apache Airflow or Cloud Composer.\nIngest and integrate data from multiple structured and semi-structured sources, including MySQL, MongoDB, APIs, and cloud storage.",,,,"['GCP', 'Bigquery', 'MySQL', 'MongoDB', 'Python']",2025-06-12 06:18:00
Senior Data Engineer,Conversehr Business Solutions,7 - 12 years,30-45 Lacs P.A.,['Hyderabad'],"What is the Data team responsible for?\nAs a Senior Data Engineer, youll be a key member of the Data & AI team. This team is responsible for designing and delivering data engineering, analytics, and generative AI solutions that drive meaningful business impact. Were looking for a pragmatic, results-driven problem solver who thrives in a fast-paced environment and is passionate about building solutions on a scale #MID_SENIOR_LEVEL\nThe ideal candidate has a strong technical foundation, a collaborative mindset, and the ability to navigate complex challenges. You should be comfortable working in a fast-moving, startup-like environment within an established enterprise, and should bring strong skill sets to adapt to new solutions fast. You will play a pivotal role in optimizing data infrastructure, enabling data-driven decision-making and integrating AI across the organization.\nWhat is the Lead Software Engineer (Senior Data Engineer) responsible for?\nServe as a hands-on technical lead, driving project execution and delivery in our growing data team based in the Hyderabad office.\nCollaborate closely with the U.S.-based team and cross-functional stakeholders to understand business needs and deliver scalable solutions.\nLead the initiative to build firmwide data models and master data management solutions for structured data (in Snowflake) and manage unstructured data using vector embeddings.\nBuild, maintain, and optimize robust data pipelines and frameworks to support business intelligence and operational workflows.\nDevelop dashboards and data visualizations that support strategic business decisions.\nStay current with emerging trends in data engineering and help implement best practices within the team.\nMentor and support junior engineers, fostering a culture of learning and technical excellence.\nWhat ideal qualifications, skills & experience would help someone to be successful?\nBachelors or master’s degree in computer science, data science, engineering, or a related field.\n7+ years of experience in data engineering including 3+ years in a technical leadership role.\nStrong SQL skills and hands-on experience with modern data pipeline technologies (e.g., Spark, Flink).\nDeep expertise in the Snowflake ecosystem, including data modeling, data warehousing, and master data management.\nProficiency in at least one programming language - Python preferred.\nExperience with Tableau and Alteryx is a plus.\nSelf-starter with a passion for learning new tools and technologies.\nStrong communication skills and a collaborative, ownership-driven mindset.\nWork Shift Timings - 2:00 PM - 11:00 PM IST",Industry Type: Investment Banking / Venture Capital / Private Equity,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Engineering', 'Snowflake', 'Python', 'flink', 'Data Pipeline', 'Spark', 'SQL']",2025-06-12 06:18:02
Enterprise Data Operations Manager,Pepsico,12 - 17 years,Not Disclosed,['Hyderabad'],"Overview\n\nDeputy Director - Data Engineering\n\nPepsiCo operates in an environment undergoing immense and rapid change. Big-data and digital technologies are driving business transformation that is unlocking new capabilities and business innovations in areas like eCommerce, mobile experiences and IoT. The key to winning in these areas is being able to leverage enterprise data foundations built on PepsiCos global business scale to enable business insights, advanced analytics, and new product development. PepsiCos Data Management and Operations team is tasked with the responsibility of developing quality data collection processes, maintaining the integrity of our data foundations, and enabling business leaders and data scientists across the company to have rapid access to the data they need for decision-making and innovation.\nIncrease awareness about available data and democratize access to it across the company.\nAs a data engineering lead, you will be the key technical expert overseeing PepsiCo's data product build & operations and drive a strong vision for how data engineering can proactively create a positive impact on the business. You'll be empowered to create & lead a strong team of data engineers who build data pipelines into various source systems, rest data on the PepsiCo Data Lake, and enable exploration and access for analytics, visualization, machine learning, and product development efforts across the company. As a member of the data engineering team, you will help lead the development of very large and complex data applications into public cloud environments directly impacting the design, architecture, and implementation of PepsiCo's flagship data products around topics like revenue management, supply chain, manufacturing, and logistics. You will work closely with process owners, product owners and business users. You'll be working in a hybrid environment with in-house, on-premises data sources as well as cloud and remote systems.\nResponsibilities\n\nData engineering lead role for D&Ai data modernization (MDIP)\n\nIdeally Candidate must be flexible to work an alternative schedule either on tradition work week from Monday to Friday; or Tuesday to Saturday or Sunday to Thursday depending upon coverage requirements of the job. The candidate can work with immediate supervisor to change the work schedule on rotational basis depending on the product and project requirements.\nResponsibilities\nManage a team of data engineers and data analysts by delegating project responsibilities and managing their flow of work as well as empowering them to realize their full potential.\nDesign, structure and store data into unified data models and link them together to make the data reusable for downstream products.\nManage and scale data pipelines from internal and external data sources to support new product launches and drive data quality across data products.\nCreate reusable accelerators and solutions to migrate data from legacy data warehouse platforms such as Teradata to Azure Databricks and Azure SQL.\nEnable and accelerate standards-based development prioritizing reuse of code, adopt test-driven development, unit testing and test automation with end-to-end observability of data\nBuild and own the automation and monitoring frameworks that captures metrics and operational KPIs for data pipeline quality, performance and cost.\nCollaborate with internal clients (product teams, sector leads, data science teams) and external partners (SI partners/data providers) to drive solutioning and clarify solution requirements.\nEvolve the architectural capabilities and maturity of the data platform by engaging with enterprise architects to build and support the right domain architecture for each application following well-architected design standards.\nDefine and manage SLAs for data products and processes running in production.\nCreate documentation for learnings and knowledge transfer to internal associates.\nQualifications\n\n12+ years of engineering and data management experience\n\nQualifications\n12+ years of overall technology experience that includes at least 5+ years of hands-on software development, data engineering, and systems architecture.\n8+ years of experience with Data Lakehouse, Data Warehousing, and Data Analytics tools.\n6+ years of experience in SQL optimization and performance tuning on MS SQL Server, Azure SQL or any other popular RDBMS\n6+ years of experience in Python/Pyspark/Scala programming on big data platforms like Databricks\n4+ years in cloud data engineering experience in Azure or AWS.\nFluent with Azure cloud services. Azure Data Engineering certification is a plus.\nExperience with integration of multi cloud services with on-premises technologies.\nExperience with data modelling, data warehousing, and building high-volume ETL/ELT pipelines.\nExperience with data profiling and data quality tools like Great Expectations.\nExperience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets.\nExperience with at least one business intelligence tool such as Power BI or Tableau\nExperience with running and scaling applications on the cloud infrastructure and containerized services like Kubernetes.\nExperience with version control systems like ADO, Github and CI/CD tools for DevOps automation and deployments.\nExperience with Azure Data Factory, Azure Databricks and Azure Machine learning tools.\nExperience with Statistical/ML techniques is a plus.\nExperience with building solutions in the retail or in the supply chain space is a plus.\nUnderstanding of metadata management, data lineage, and data glossaries is a plus.\nBA/BS in Computer Science, Math, Physics, or other technical fields.\nCandidate must be flexible to work an alternative work schedule either on tradition work week from Monday to Friday; or Tuesday to Saturday or Sunday to Thursday depending upon product and project coverage requirements of the job.\nCandidates are expected to be in the office at the assigned location at least 3 days a week and the days at work needs to be coordinated with immediate supervisor\nSkills, Abilities, Knowledge:\nExcellent communication skills, both verbal and written, along with the ability to influence and demonstrate confidence in communications with senior level management.\nProven track record of leading, mentoring data teams.\nStrong change manager. Comfortable with change, especially that which arises through company growth.\nAbility to understand and translate business requirements into data and technical requirements.\nHigh degree of organization and ability to manage multiple, competing projects and priorities simultaneously.\nPositive and flexible attitude to enable adjusting to different needs in an ever-changing environment.\nStrong leadership, organizational and interpersonal skills; comfortable managing trade-offs.\nFoster a team culture of accountability, communication, and self-management.\nProactively drives impact and engagement while bringing others along.\nConsistently attain/exceed individual and team goals.\nAbility to lead others without direct authority in a matrixed environment.\nComfortable working in a hybrid environment with teams consisting of contractors as well as FTEs spread across multiple PepsiCo locations.\nDomain Knowledge in CPG industry with Supply chain/GTM background is preferred.",Industry Type: Beverage,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Pyspark', 'Azure', 'Power BI', 'Github', 'Azure Databricks', 'Tableau', 'ADO', 'Scala programming', 'SQL', 'Azure Data Factory', 'Azure Machine learning', 'Data Lakehouse', 'Azure Data Engineering', 'CI/CD', 'Data Warehousing', 'Data Analytics', 'AWS', 'Python']",2025-06-12 06:18:05
Associate- Referral - Decision Science / Data Science,Axtria,3 - 5 years,Not Disclosed,['Gurugram'],"Position Summary \n\nThis Requisition is for the Employee Referral Campaign.\n\nWe are seeking high-energy, driven, and innovative Data Scientists to join our Data Science Practice to develop new, specialized capabilities for Axtria, and to accelerate the company’s growth by supporting our clients’ commercial & clinical strategies.\n\n Job Responsibilities \n\nBe an Individual Contributor tothe Data Science team and solve real-world problems using cutting-edge capabilities and emerging technologies.\n\nHelp clients translate the business use cases they are trying to crack into data science solutions. Provide genuine assistance to users by advising them on how to leverage Dataiku DSS to implement data science projects, from design to production.\n\nData Source Configuration, Maintenance, Document and maintain work-instructions.\n\nDeep working onmachine learning frameworks such as TensorFlow, Caffe, Keras, SparkML\n\nExpert knowledge in Statistical and Probabilistic methods such as SVM, Decision-Trees, Clustering\n\nExpert knowledge of python data-science and math packages such as NumPy , Pandas, Sklearn\n\nProficiency in object-oriented languages (Java and/or Kotlin),Python and common machine learning frameworks(TensorFlow, NLTK, Stanford NLP, Ling Pipe etc\n\n\n Education \n\nBachelor Equivalent - Engineering\nMaster's Equivalent - Engineering\n\n Work Experience \n\nData Scientist 3-5 years of relevant experience in advanced statistical and mathematical models and predictive modeling using Python. Experience in the data science space prior relevant experience in Artificial intelligence and machine Learning algorithms for developing scalable models supervised and unsupervised techniques likeNLP and deep Learning Algorithms. Ability to build scalable models using Python, R-Studio, R Shiny, PySpark, Keras, and TensorFlow. Experience in delivering data science projects leveraging cloud infrastructure. Familiarity with cloud technology such as AWS / Azure and knowledge of AWS tools such as S3, EMR, EC2, Redshift, and Glue; viz tools like Tableau and Power BI. Relevant experience in Feature Engineering, Feature Selection, and Model Validation on Big Data. Knowledge of self-service analytics platforms such as Dataiku/ KNIME/ Alteryx will be an added advantage.\n\nML Ops Engineering 3-5 years of experience with MLOps Frameworks like Kubeflow, MLFlow, Data Robot, Airflow, etc., experience with Docker and Kubernetes, OpenShift. Prior experience in end-to-end automated ecosystems including, but not limited to, building data pipelines, developing & deploying scalable models, orchestration, scheduling, automation, and ML operations. Ability to design and implement cloud solutions and ability to build MLOps pipelines on cloud solutions (AWS, MS Azure, or GCP). Programming languages like Python, Go, Ruby, or Bash, a good understanding of Linux, knowledge of frameworks such as Keras, PyTorch, TensorFlow, etc. Ability to understand tools used by data scientists and experience with software development and test automation. Good understanding of advanced AI/ML algorithms & their applications.\n\nGen AI :Minimum of 4-6 years develop, test, and deploy Python based applications on Azure/AWS platforms.Must have basic knowledge on concepts of Generative AI / LLMs / GPT.Deep understanding of architecture and work experience on Web Technologies.Python, SQL hands-on experience.Expertise in any popular python web frameworks e.g. flask, Django etc. Familiarity with frontend technologies like HTML, JavaScript, REACT.Be an Individual Contributor in the Analytics and Development team and solve real-world problems using cutting-edge capabilities and emerging technologies based on LLM/GenAI/GPT.Can interact with client on GenAI related capabilities and use cases.",Industry Type: Analytics / KPO / Research,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'gpm', 'machine learning', 'python data', 'statistics', 'kubernetes', 'microsoft azure', 'numpy', 'javascript', 'sql', 'docker', 'pandas', 'tensorflow', 'java', 'django', 'predictive modeling', 'python web framework', 'mathematical modeling', 'pytorch', 'keras', 'aws', 'flask', 'advanced statistical']",2025-06-12 06:18:07
Data Engineer,Prohr Strategies,9 - 11 years,Not Disclosed,['Bengaluru'],"Hands-on Data Engineer with strong Databricks expertise in Git/DevOps integration, Unity Catalog governance, and performance tuning of data transformation workloads. Skilled in optimizing pipelines and ensuring secure, efficient data operations.",Industry Type: Emerging Technologies (AI/ML),Department: Data Science & Analytics,"Employment Type: Full Time, Temporary/Contractual","['Data Transformation', 'GIT', 'Azure Databricks', 'Databricks', 'Devops', 'Data Engineering', 'Governance', 'Catalog', 'Code Versioning Tools']",2025-06-12 06:18:09
Consultant - Data Engineer,Affine Analytics,5 - 8 years,Not Disclosed,['Bengaluru'],"We are looking for a highly skilled API & Pixel Tracking Integration Engineer to lead the development and deployment of server-side tracking and attribution solutions across multiple platforms. The ideal candidate brings deep expertise in CAPI integrations (Meta, Google, and other platforms), secure data handling using cryptographic techniques, and experience working within privacy-first environments like Azure Clean Rooms.\nThis role requires strong hands-on experience in Azure cloud services, OCI (Oracle Cloud Infrastructure), and marketing technology stacks including Adobe Tag Management and Pixel Management. You will work closely with engineering, analytics, and marketing teams to deliver scalable, compliant, and secure data tracking solutions that drive business insights and performance.",,,,"['Python', 'Azure Cloud technologies', 'Azure Data Factory', 'Adobe Tag Management', 'Azure security', 'ADF', 'ADLS', 'Azure Functions', 'Key Vault']",2025-06-12 06:18:12
"Sr. Data Engineer, R&D Data Catalyst Team",Amgen Inc,7 - 9 years,Not Disclosed,['Hyderabad'],"The R&D Data Catalyst Team is responsible for buildingData Searching, Cohort Building, and Knowledge Management tools that provide the Amgen scientific community with visibility to Amgens wealth of human datasets, projects and study histories, and knowledge over various scientific findings. These solutions are pivotal tools in Amgens goal to accelerate the speed of discovery, and speed to market of advanced precision medications.\nThe Data Engineer will be responsible for the end-to-end development of an enterprise analytics and data mastering solution leveraging Databricks and Power BI. This role requiresexpertise in both data architecture and analytics, with the ability to create scalable, reliable, and high-performing enterprise solutions that research cohort-building and advanced research pipeline.The ideal candidate will have experience creating and surfacing large unifiedrepositories of human data, based on integrations from multiple repositories and solutions, and be exceptionally skilled with data analysis and profiling.\nYou will collaborate closely with stakeholders, product team members, and related IT teams, to design and implement data models, integrate data from various sources, and ensure best practices for data governance and security. The ideal candidate will have a strong background in data warehousing, ETL, Databricks, Power BI, and enterprise data mastering.\nRoles & Responsibilities:\nDesign and build scalable enterprise analytics solutions using Databricks, Power BI, and other modern data tools.\nLeverage data virtualization, ETL, and semantic layers to balance need for unification, performance, and data transformation with goal to reduce data proliferation\nBreak down features into work that aligns with the architectural direction runway\nParticipate hands-on in pilots and proofs-of-concept for new patterns\nCreate robust documentation from data analysis and profiling, and proposed designs and data logic\nDevelop advanced sql queries to profile, and unify data\nDevelop data processing code in sql, along with semantic views to prepare data for reporting\nDevelop PowerBI Models and reporting packages\nDesign robust data models, and processing layers, that support both analytical processing and operational reporting needs.\nDesign and develop solutions based on best practices for data governance, security, and compliance within Databricks and Power BI environments.\nEnsure the integration of data systems with other enterprise applications, creating seamless data flows across platforms.\nDevelop and maintain Power BI solutions, ensuring data models and reports are optimized for performance and scalability.\nCollaborate with stakeholders to define data requirements, functional specifications, and project goals.\nContinuously evaluate and adopt new technologies and methodologies to enhance the architecture and performance of data solutions.\nBasic Qualifications and Experience:\nMasters degree with 1 to 3years of experience in Data Engineering OR\nBachelors degree with 4 to 5 years of experience in Data Engineering\nDiploma and 7 to 9 years of experience in Data Engineering.\nFunctional Skills:\nMust-Have Skills:\nMinimum of 3 years of hands-on experience with BI solutions (Preferrable Power BI or Business Objects) including report development, dashboard creation, and optimization.\nMinimum of 3years of hands-on experience building Change-data-capture (CDC) ETL pipelines, data warehouse design and build, and enterprise-level data management.\nHands-on experience with Databricks, including data engineering, optimization, and analytics workloads.\nDeep understanding of Power BI, including model design, DAX, and Power Query.\nProven experience designing and implementing data mastering solutions and data governance frameworks.\nExpertise in cloud platforms (AWS), data lakes, and data warehouses.\nStrong knowledge of ETL processes, data pipelines, and integration technologies.\nStrong communication and collaboration skills to work with cross-functional teams and senior leadership.\nAbility to assess business needs and design solutions that align with organizational goals.\nExceptional hands-on capabilities with data profiling, data transformation, data mastering\nSuccess in mentoring and training team members\nGood-to-Have Skills:\nExperience in developing differentiated and deliverable solutions\nExperience with human data, ideally human healthcare data\nFamiliarity with laboratory testing, patient data from clinical care, HL7, FHIR, and/or clinical trial data management\nProfessional Certifications (please mention if the certification is preferred or mandatory for the role):\nITIL Foundation or other relevant certifications (preferred)\nSAFe Agile Practitioner (6.0)\nMicrosoft Certified: Data Analyst Associate (Power BI) or related certification.\nDatabricks Certified Professional or similar certification.\nSoft Skills:\nExcellent analytical and troubleshooting skills\nDeep intellectual curiosity\nHighest degree of initiative and self-motivation\nStrong verbal and written communication skills, including presentation to varied audiences of complex technical/business topics\nConfidence technical leader\nAbility to work effectively with global, virtual teams, specifically including leveraging of tools and artifacts to assure clear and efficient collaboration across time zones\nAbility to manage multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals\nStrong problem solving, analytical skills;\nAbility to learn quickly and retain and synthesize complex information from diverse sources",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Engineering', 'data analysis', 'ETL processes', 'DAX', 'Business Objects', 'data warehouse design', 'ETL', 'PowerBI Models', 'AWS', 'Power Query']",2025-06-12 06:18:14
Data Lineage Engineers,Altimetrik,5 - 10 years,15-30 Lacs P.A.,"['Pune', 'Chennai', 'Bengaluru']","Role & responsibilities\nSkill need :  Data Lineage with Ab-initio - Metadata hub\n\nSkills & Experience:\nExpertise in mHub or similar tools, data pipelines, and cloud platforms.\nProficiency in Python, Oracle, SQL, Java, and ETL tools.\n5-10 years of experience in data engineering and governance.",,,,"['Ab-initio', 'Metadata hub', 'Python', 'mhub', 'ETL', 'Oracle', 'SQL']",2025-06-12 06:18:17
Sr Data Engineering Manager,Amgen Inc,12 - 15 years,Not Disclosed,['Hyderabad'],"ABOUT THE ROLE\nRole Description:\nWe are seeking a Senior Data Engineering Manager with a strong background in Regulatory or Integrated Product Teams within the Biotech or Pharmaceutical domain. This role will lead the end-to-end data strategy and execution for regulatory product submissions, lifecycle management, and compliance reporting, ensuring timely and accurate delivery of regulatory data assets across global markets.You will be embedded in a cross-functional Regulatory Integrated Product Team (IPT) and serve as the data and technology lead, driving integration between scientific, regulatory, and engineering functions to support submission-ready data and regulatory intelligence solutions.\nRoles & Responsibilities:\nFunctional Skills:\nLead the engineering strategy and implementation for end-to-end regulatory operations, including data ingestion, transformation, integration, and delivery across regulatory systems.\nServe as the data engineering SME in the Integrated Product Team (IPT) to support regulatory submissions, agency interactions, and lifecycle updates.\nCollaborate with global regulatory affairs, clinical, CMC, quality, safety, and IT teams to gather submission data requirements and translate them into data engineering solutions.\nManage and oversee the development of data pipelines, data models, and metadata frameworks that support submission data standards (e.g., eCTD, IDMP, SPL, xEVMPD).\nEnable integration and reporting across regulatory information management systems (RIMS), EDMS, clinical trial systems, and lab data platforms.\nImplement data governance, lineage, validation, and audit trails for regulatory data workflows, ensuring GxP and regulatory compliance.\nGuide the development of automation solutions, dashboards, and analytics that improve visibility into submission timelines, data quality, and regulatory KPIs.\nEnsure interoperability between regulatory data platforms and enterprise data lakes or lakehouses for cross-functional reporting and insights.\nCollaborate with IT, data governance, and enterprise architecture teams to ensure alignment with overall data strategy and compliance frameworks.\nDrive innovation by evaluating emerging technologies in data engineering, graph data, knowledge management, and AI for regulatory intelligence.\nLead, mentor, and coach a small team of data engineers and analysts, fostering a culture of excellence, innovation, and delivery.\nDrive Agile and Scaled Agile (SAFe) methodologies, managing sprint backlogs, prioritization, and iterative improvements to enhance team velocity and project delivery.\nStay up-to-date with emerging data technologies, industry trends, and best practices, ensuring the organization leverages the latest innovations in data engineering and architecture.\nMust-Have Skills:\n812 years of experience in data engineering or data architecture, with 3+ years in a senior or managerial capacity, preferably within the biotech or pharmaceutical industry.\nProven experience supporting regulatory functions, including submissions, tracking, and reporting for FDA, EMA, and other global authorities.\nExperience with ETL/ELT tools, data pipelines, and cloud-based data platforms (e.g., Databricks, AWS, Azure, or GCP).\nFamiliarity with regulatory standards and data models such as eCTD, IDMP, HL7, CDISC, and xEVMPD.\nDeep understanding of GxP data compliance, audit requirements, and regulatory submission processes.\nExperience with tools like Power BI, Tableau, or Qlik for regulatory dashboarding and visualization is a plus.\nStrong project management, stakeholder communication, and leadership skills, especially in matrixed, cross-functional environments.\nAbility to translate technical capabilities into regulatory and business outcomes.Prepare team members for stakeholder discussions by helping assess data costs, access requirements, dependencies, and availability for business scenarios.\nGood-to-Have Skills:\nPrior experience working on integrated product teams or regulatory transformation programs.\nKnowledge of Regulatory Information Management Systems (RIMS), Veeva Vault RIM, or Master Data Management (MDM) in regulated environments.\nFamiliarity with Agile/SAFe methodologies and DevOps/DataOps best practices.\nEducation and Professional Certifications\n12 to 15 years of experience in Computer Science, IT or related field\nScaled Agile SAFe certification preferred\nProject Management certifications preferred\nSoft Skills:\nExcellent analytical and troubleshooting skills\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation\nAbility to manage multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals\nStrong presentation and public speaking skills",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'engineering strategy', 'DevOps', 'Project Management', 'DataOps', 'Agile', 'data strategy']",2025-06-12 06:18:19
Senior Data Engineer (Data Architect),Adastra Corp,8 - 13 years,Not Disclosed,[],"Join our innovative team and architect the future of data solutions on Azure, Synapse, and Databricks!\nSenior Data Engineer (Data Architect)\nAdditional Details:\nNotice Period: 30 days (maximum)\nLocation: Remote\nAbout the Role\nDesign and implement scalable data pipelines, data warehouses, and data lakes that drive business growth. Collaborate with stakeholders to deliver data-driven insights and shape the data landscape.\nRequirements\n8+ years of experience in data engineering and data architecture\nStrong expertise in Azure services (Synapse Analytics, Databricks, Storage, Active Directory)\nProven experience in designing and implementing data pipelines, data warehouses, and data lakes\nStrong understanding of data governance, data quality, and data security\nExperience with infrastructure design and implementation, including DevOps practices and tools",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Azure', 'DESIGN', 'Architecture', 'Synapse Analytics']",2025-06-12 06:18:21
Senior Engineering Manager - Data Operations,Amgen Inc,12 - 15 years,Not Disclosed,['Hyderabad'],"ABOUT THE ROLE\nRole Description:\nWe are seeking a seasoned Senior Engineering Manager(Data Engineering) to lead the end-to-end management of enterprise data assets and operational data workflows. This role is critical in ensuring the availability, quality, consistency, and timeliness of data across platforms and functions, supporting analytics, reporting, compliance, and digital transformation initiatives.As a senior leader in the data organization, you will oversee the day-to-day data operations, manage a team of data professionals, and drive process excellence in data intake, transformation, validation, and delivery. You will work closely with cross-functional teams including data engineering, analytics, IT, governance, and business stakeholders to align operational data capabilities with enterprise needs.\nRoles & Responsibilities:\nLead and manage the enterprise data operations team, responsible for data ingestion, processing, validation, quality control, and publishing to various downstream systems.\nDefine and implement standard operating procedures for data lifecycle management, ensuring availability, accuracy, completeness, and integrity of critical data assets.\nOversee and continuously improve daily operational workflows, including scheduling, monitoring, and troubleshooting data jobs across cloud and on-premise environments.\nEstablish and track key data operations metrics (SLAs, throughput, latency, data quality, incident resolution) and drive continuous improvements.\nPartner with data engineering and platform teams to optimize pipelines, support new data integrations, and ensure scalability and resilience of operational data flows.\nCollaborate with data governance, compliance, and security teams to maintain regulatory compliance, data privacy, and access controls.\nServe as the primary escalation point for data incidents and outages, ensuring rapid response and root cause analysis.\nBuild strong relationships with business and analytics teams to understand data consumption patterns, prioritize operational needs, and align with business objectives.\nDrive adoption of best practices for documentation, metadata, lineage, and change management across data operations processes.\nMentor and develop a high-performing team of data operations analysts and leads.\nFunctional Skills:\nMust-Have Skills:\nExperience managing a team of data engineers in biotech/pharmadomain companies.\nExperience in designing and maintainingdata pipelines and analytics solutions that extract, transform, and load data from multiple source systems.\nDemonstrated hands-on experience with cloud platforms (AWS) and the ability to architect cost-effective and scalable data solutions.\nExperience managing data workflows on Databricks in cloud environments such as AWS, Azure, or GCP.\nStrong problem-solving skills with the ability to analyze complex data flow issues and implement sustainable solutions.\nWorking knowledge of SQL, Python, PySparkor scripting languages for process monitoring and automation.\nExperience collaborating with data engineering, analytics, IT operations, and business teams in a matrixed organization.\nFamiliarity with data governance, metadata management, access control, and regulatory requirements (e.g., GDPR, HIPAA, SOX).\nExcellent leadership, communication, and stakeholder engagement skills.\nWell versed with full stack development& DataOps automation, logging & observability frameworks, and pipeline orchestration tools.\nStrong analytical and problem-solving skills to address complex data challenges.\nEffective communication and interpersonal skills to collaborate with cross-functional teams.\nGood-to-Have Skills:\nData Engineering Management experience in Biotech/Life Sciences/Pharma\nExperience using graph databases such as Stardog or Marklogic or Neo4J or Allegrograph, etc.\nEducation and Professional Certifications\n12 to 15 years of experience in Computer Science, IT or related field\nDatabricks Certificate preferred\nScaled Agile SAFe certification preferred\nExperience in life sciences, healthcare, or other regulated industries with large-scale operational data environments.\nFamiliarity with incident and change management processes (e.g., ITIL).\nSoft Skills:\nExcellent analytical and troubleshooting skills\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation\nAbility to manage multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals\nStrong presentation and public speaking skills.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Operations', 'Azure', 'Data Engineering', 'Neo4J', 'GCP', 'Engineering Management', 'troubleshooting', 'Stardog', 'Marklogic', 'AWS']",2025-06-12 06:18:23
"Walk-In Senior Data Engineer - DataStage, Azure & Power BI",Net Connect,6 - 10 years,5-11 Lacs P.A.,['Hyderabad( Madhapur )'],"Greetings from NCG!\n\nWe have a opening for Snowflake Developer role in Hyderabad office!\nBelow JD for your reference\n\nJob Description:\n\nWe are hiring an experienced Senior Data Engineer with strong expertise in IBM DataStage, , and . The ideal candidate will be responsible for end-to-end data integration, transformation, and reporting solutions that drive business decisions.",,,,"['Azure Data Factory', 'Datastage', 'Etl Datastage']",2025-06-12 06:18:26
Senior Data Engineer,Jeavio,5 - 10 years,Not Disclosed,[],"We are seeking an experienced Senior Data Engineer to join our team. The ideal candidate will have a strong background in data engineering and AWS infrastructure, with hands-on experience in building and maintaining data pipelines and the necessary infrastructure components. The role will involve using a mix of data engineering tools and AWS services to design, build, and optimize data architecture.\n\nKey Responsibilities:\nDesign, develop, and maintain data pipelines using Airflow and AWS services.\nImplement and manage data warehousing solutions with Databricks and PostgreSQL.\nAutomate tasks using GIT / Jenkins.\nDevelop and optimize ETL processes, leveraging AWS services like S3, Lambda, AppFlow, and DMS.\nCreate and maintain visual dashboards and reports using Looker.\nCollaborate with cross-functional teams to ensure smooth integration of infrastructure components.\nEnsure the scalability, reliability, and performance of data platforms.\nWork with Jenkins for infrastructure automation.\n\nTechnical and functional areas of expertise:\nWorking as a senior individual contributor on a data intensive project\nStrong experience in building high performance, resilient & secure data processing pipelines preferably using Python based stack.\nExtensive experience in building data intensive applications with a deep understanding of querying and modeling with relational databases preferably on time-series data.\nIntermediate proficiency in AWS services (S3, Airflow)\nProficiency in Python and PySpark\nProficiency with ThoughtSpot or Databricks.\nIntermediate proficiency in database scripting (SQL)\nBasic experience with Jenkins for task automation\n\nNice to Have :\nIntermediate proficiency in data analytics tools (Power BI / Tableau / Looker / ThoughSpot)\nExperience working with AWS Lambda, Glue, AppFlow, and other AWS transfer services.\nExposure to PySpark and data automation tools like Jenkins or CircleCI.\nFamiliarity with Terraform for infrastructure-as-code.\nExperience in data quality testing to ensure the accuracy and reliability of data pipelines.\nProven experience working directly with U.S. client stakeholders.\nAbility to work independently and take the lead on tasks.\n\nEducation and experience:\nBachelors or masters in computer science or related fields.\n5+ years of experience\n\nStack/Skills needed:\nDatabricks\nPostgreSQL\nPython & Pyspark\nAWS Stack\nPower BI / Tableau / Looker / ThoughSpot\nFamiliarity with GIT and/or CI/CD tools",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Pyspark', 'Data Engineering', 'AWS', 'Data Bricks', 'Python', 'Etl Pipelines', 'Airflow', 'Database Scripting', 'Postgresql', 'Looker', 'SQL']",2025-06-12 06:18:28
Senior Data Engineer : 7+ Years,Jayam Solutions Pvt Ltd - CMMI Level III Company,5 - 9 years,Not Disclosed,['Hyderabad( Madhapur )'],"Job Description:\nPosition: Sr.Data Engineer\nExperience: Minimum 7 years\nLocation: Hyderabad\nJob Summary:\n\nWhat Youll Do\n\nDesign and build efficient, reusable, and reliable data architecture leveraging technologies like Apache Flink, Spark, Beam and Redis to support large-scale, real-time, and batch data processing.\nParticipate in architecture and system design discussions, ensuring alignment with business objectives and technology strategy, and advocating for best practices in distributed data systems.\nIndependently perform hands-on development and coding of data applications and pipelines using Java, Scala, and Python, including unit testing and code reviews.\nMonitor key product and data pipeline metrics, identify root causes of anomalies, and provide actionable insights to senior management on data and business health.\nMaintain and optimize existing datalake infrastructure, lead migrations to lakehouse architectures, and automate deployment of data pipelines and machine learning feature engineering requests.\nAcquire and integrate data from primary and secondary sources, maintaining robust databases and data systems to support operational and exploratory analytics.\nEngage with internal stakeholders (business teams, product owners, data scientists) to define priorities, refine processes, and act as a point of contact for resolving stakeholder issues.\nDrive continuous improvement by establishing and promoting technical standards, enhancing productivity, monitoring, tooling, and adopting industry best practices.\n\nWhat Youll Bring\n\nBachelors degree or higher in Computer Science, Engineering, or a quantitative discipline, or equivalent professional experience demonstrating exceptional ability.\n7+ years of work experience in data engineering and platform engineering, with a proven track record in designing and building scalable data architectures.\nExtensive hands-on experience with modern data stacks, including datalake, lakehouse, streaming data (Flink, Spark), and AWS or equivalent cloud platforms.\nCloud - AWS\nApache Flink/Spark , Redis\nDatabase platform- Databricks.\nProficiency in programming languages such as Java, Scala, and Python(Good to have) for data engineering and pipeline development.\nExpertise in distributed data processing and caching technologies, including Apache Flink, Spark, and Redis.\nExperience with workflow orchestration, automation, and DevOps tools (Kubernetes,git,Terraform, CI/CD).\nAbility to perform under pressure, managing competing demands and tight deadlines while maintaining high-quality deliverables.\nStrong passion and curiosity for data, with a commitment to data-driven decision making and continuous learning.\nExceptional attention to detail and professionalism in report and dashboard creation.\nExcellent team player, able to collaborate across diverse functional groups and communicate complex technical concepts clearly.\nOutstanding verbal and written communication skills to effectively manage and articulate the health and integrity of data and systems to stakeholders.\n\nPlease feel free to contact us: 9440806850\nEmail ID : careers@jayamsolutions.com",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Apache Flink', 'Redis', 'Spark', 'Python', 'SCALA', 'Ci/Cd', 'Devops', 'AWS']",2025-06-12 06:18:31
Senior Data Engineer,Amgen Inc,3 - 7 years,Not Disclosed,['Hyderabad'],"Role Description:\nWe are looking for highly motivated expert Senior Data Engineer who can own the design & development of complex data pipelines, solutions and frameworks. The ideal candidate will be responsible to design, develop, and optimize data pipelines, data integration frameworks, and metadata-driven architectures that enable seamless data access and analytics. This role prefers deep expertise in big data processing, distributed computing, data modeling, and governance frameworks to support self-service analytics, AI-driven insights, and enterprise-wide data management.\nRoles & Responsibilities:\nDesign, develop, and maintain scalable ETL/ELT pipelines to support structured, semi-structured, and unstructured data processing across the Enterprise Data Fabric.\nImplement real-time and batch data processing solutions, integrating data from multiple sources into a unified, governed data fabric architecture.\nOptimize big data processing frameworks using Apache Spark, Hadoop, or similar distributed computing technologies to ensure high availability and cost efficiency.\nWork with metadata management and data lineage tracking tools to enable enterprise-wide data discovery and governance.\nEnsure data security, compliance, and role-based access control (RBAC) across data environments.\nOptimize query performance, indexing strategies, partitioning, and caching for large-scale data sets.\nDevelop CI/CD pipelines for automated data pipeline deployments, version control, and monitoring.\nImplement data virtualization techniques to provide seamless access to data across multiple storage systems.\nCollaborate with cross-functional teams, including data architects, business analysts, and DevOps teams, to align data engineering strategies with enterprise goals.\nStay up to date with emerging data technologies and best practices, ensuring continuous improvement of Enterprise Data Fabric architectures.\nMust-Have Skills:\nHands-on experience in data engineering technologies such as Databricks, PySpark, SparkSQL Apache Spark, AWS, Python, SQL, and Scaled Agile methodologies.\nProficiency in workflow orchestration, performance tuning on big data processing.\nStrong understanding of AWS services\nExperience with Data Fabric, Data Mesh, or similar enterprise-wide data architectures.\nAbility to quickly learn, adapt and apply new technologies\nStrong problem-solving and analytical skills\nExcellent communication and teamwork skills\nExperience with Scaled Agile Framework (SAFe), Agile delivery practices, and DevOps practices.\nGood-to-Have Skills:\nGood to have deep expertise in Biotech & Pharma industries\nExperience in writing APIs to make the data available to the consumers\nExperienced with SQL/NOSQL database, vector database for large language models\nExperienced with data modeling and performance tuning for both OLAP and OLTP databases\nExperienced with software engineering best-practices, including but not limited to version control (Git, Subversion, etc.), CI/CD (Jenkins, Maven etc.), automated unit testing, and Dev Ops\nEducation and Professional Certifications\n9 to 12 years of Computer Science, IT or related field experience\nAWS Certified Data Engineer preferred\nDatabricks Certificate preferred\nScaled Agile SAFe certification preferred\nSoft Skills:\nExcellent analytical and troubleshooting skills.\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nTeam-oriented, with a focus on achieving team goals.\nAbility to learn quickly, be organized and detail oriented.\nStrong presentation and public speaking skills.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Maven', 'SparkSQL Apache Spark', 'PySpark', 'Subversion', 'OLAP', 'Scaled Agile methodologies', 'SQL', 'Scaled Agile Framework', 'Jenkins', 'NOSQL database', 'Git', 'Databricks', 'Data Fabric', 'Data Mesh', 'AWS', 'Python']",2025-06-12 06:18:33
Senior Data Engineer,Amgen Inc,3 - 8 years,Not Disclosed,['Hyderabad'],"Role Description:\nWe are looking for highly motivated expert Senior Data Engineer who can own the design & development of complex data pipelines, solutions and frameworks. The ideal candidate will be responsible to design, develop, and optimize data pipelines, data integration frameworks, and metadata-driven architectures that enable seamless data access and analytics. This role prefers deep expertise in big data processing, distributed computing, data modeling, and governance frameworks to support self-service analytics, AI-driven insights, and enterprise-wide data management.\nRoles & Responsibilities:\nDesign, develop, and maintain scalable ETL/ELT pipelines to support structured, semi-structured, and unstructured data processing across the Enterprise Data Fabric.\nImplement real-time and batch data processing solutions, integrating data from multiple sources into a unified, governed data fabric architecture.\nOptimize big data processing frameworks using Apache Spark, Hadoop, or similar distributed computing technologies to ensure high availability and cost efficiency.\nWork with metadata management and data lineage tracking tools to enable enterprise-wide data discovery and governance.\nEnsure data security, compliance, and role-based access control (RBAC) across data environments.\nOptimize query performance, indexing strategies, partitioning, and caching for large-scale data sets.\nDevelop CI/CD pipelines for automated data pipeline deployments, version control, and monitoring.\nImplement data virtualization techniques to provide seamless access to data across multiple storage systems.\nCollaborate with cross-functional teams, including data architects, business analysts, and DevOps teams, to align data engineering strategies with enterprise goals.\nStay up to date with emerging data technologies and best practices, ensuring continuous improvement of Enterprise Data Fabric architectures.\nMust-Have Skills:\nHands-on experience in data engineering technologies such as Databricks, PySpark, SparkSQL Apache Spark, AWS, Python, SQL, and Scaled Agile methodologies.\nProficiency in workflow orchestration, performance tuning on big data processing.\nStrong understanding of AWS services\nExperience with Data Fabric, Data Mesh, or similar enterprise-wide data architectures.\nAbility to quickly learn, adapt and apply new technologies\nStrong problem-solving and analytical skills\nExcellent communication and teamwork skills\nExperience with Scaled Agile Framework (SAFe), Agile delivery practices, and DevOps practices.\nGood-to-Have Skills:\nGood to have deep expertise in Biotech & Pharma industries\nExperience in writing APIs to make the data available to the consumers\nExperienced with SQL/NOSQL database, vector database for large language models\nExperienced with data modeling and performance tuning for both OLAP and OLTP databases\nExperienced with software engineering best-practices, including but not limited to version control (Git, Subversion, etc.), CI/CD (Jenkins, Maven etc.), automated unit testing, and DevOps\nEducation and Professional Certifications\nMasters degree and 3 to 4 + years of Computer Science, IT or related field experience\nOR\nBachelors degree and 5 to 8 + years of Computer Science, IT or related field experience\nAWS Certified Data Engineer preferred\nDatabricks Certificate preferred\nScaled Agile SAFe certification preferred\nSoft Skills:\nExcellent analytical and troubleshooting skills.\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nTeam-oriented, with a focus on achieving team goals.\nAbility to learn quickly, be organized and detail oriented.\nStrong presentation and public speaking skills.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Git', 'PySpark', 'CI/CD', 'Databricks', 'ETL', 'NOSQL', 'AWS', 'data integration', 'SQL', 'Apache Spark', 'Python']",2025-06-12 06:18:36
Senior Data Engineer,Amgen Inc,9 - 12 years,Not Disclosed,['Hyderabad'],"Role Description:\nWe are looking for highly motivated expert Senior Data Engineer who can own the design & development of complex data pipelines, solutions and frameworks. The ideal candidate will be responsible to design, develop, and optimize data pipelines, data integration frameworks, and metadata-driven architectures that enable seamless data access and analytics. This role prefers deep expertise in big data processing, distributed computing, data modeling, and governance frameworks to support self-service analytics, AI-driven insights, and enterprise-wide data management.\nRoles & Responsibilities:\nDesign, develop, and maintain scalable ETL/ELT pipelines to support structured, semi-structured, and unstructured data processing across the Enterprise Data Fabric.\nImplement real-time and batch data processing solutions, integrating data from multiple sources into a unified, governed data fabric architecture.\nOptimize big data processing frameworks using Apache Spark, Hadoop, or similar distributed computing technologies to ensure high availability and cost efficiency.\nWork with metadata management and data lineage tracking tools to enable enterprise-wide data discovery and governance.\nEnsure data security, compliance, and role-based access control (RBAC) across data environments.\nOptimize query performance, indexing strategies, partitioning, and caching for large-scale data sets.\nDevelop CI/CD pipelines for automated data pipeline deployments, version control, and monitoring.\nImplement data virtualization techniques to provide seamless access to data across multiple storage systems.\nCollaborate with cross-functional teams, including data architects, business analysts, and DevOps teams, to align data engineering strategies with enterprise goals.\nStay up to date with emerging data technologies and best practices, ensuring continuous improvement of Enterprise Data Fabric architectures.\nMust-Have Skills:\nHands-on experience in data engineering technologies such as Databricks, PySpark, SparkSQL Apache Spark, AWS, Python, SQL, and Scaled Agile methodologies.\nProficiency in workflow orchestration, performance tuning on big data processing.\nStrong understanding of AWS services\nExperience with Data Fabric, Data Mesh, or similar enterprise-wide data architectures.\nAbility to quickly learn, adapt and apply new technologies\nStrong problem-solving and analytical skills\nExcellent communication and teamwork skills\nExperience with Scaled Agile Framework (SAFe), Agile delivery practices, and DevOps practices.\nGood-to-Have Skills:\nGood to have deep expertise in Biotech & Pharma industries\nExperience in writing APIs to make the data available to the consumers\nExperienced with SQL/NOSQL database, vector database for large language models\nExperienced with data modeling and performance tuning for both OLAP and OLTP databases\nExperienced with software engineering best-practices, including but not limited to version control (Git, Subversion, etc.), CI/CD (Jenkins, Maven etc.), automated unit testing, and Dev Ops\nEducation and Professional Certifications\n9 to 12 years of Computer Science, IT or related field experience\nAWS Certified Data Engineer preferred\nDatabricks Certificate preferred\nScaled Agile SAFe certification preferred\nSoft Skills:\nExcellent analytical and troubleshooting skills.\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nTeam-oriented, with a focus on achieving team goals.\nAbility to learn quickly, be organized and detail oriented.\nStrong presentation and public speaking skills.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data engineering', 'performance tuning', 'data security', 'data processing', 'Hadoop', 'Apache Spark', 'SQL', 'CI/CD', 'troubleshooting', 'big data', 'aws', 'ETL', 'Python']",2025-06-12 06:18:38
Senior Data Engineer,Amgen Inc,3 - 7 years,Not Disclosed,['Hyderabad'],"What you will do\nRole Description:\nWe are seeking a Senior Data Engineer with expertise in Graph Data technologies to join our data engineering team and contribute to the development of scalable, high-performance data pipelines and advanced data models that power next-generation applications and analytics. This role combines core data engineering skills with specialized knowledge in graph data structures, graph databases, and relationship-centric data modeling, enabling the organization to leverage connected data for deep insights, pattern detection, and advanced analytics use cases. The ideal candidate will have a strong background in data architecture, big data processing, and Graph technologies and will work closely with data scientists, analysts, architects, and business stakeholders to design and deliver graph-based data engineering solutions.\nRoles & Responsibilities:\nDesign, build, and maintain robust data pipelines using Databricks (Spark, Delta Lake, PySpark) for complex graph data processing workflows.\nOwn the implementation of graph-based data models, capturing complex relationships and hierarchies across domains.\nBuild and optimize Graph Databases such as Stardog, Neo4j, Marklogic or similar to support query performance, scalability, and reliability.\nImplement graph query logic using SPARQL, Cypher, Gremlin, or GSQL, depending on platform requirements.\nCollaborate with data architects to integrate graph data with existing data lakes, warehouses, and lakehouse architectures.\nWork closely with data scientists and analysts to enable graph analytics, link analysis, recommendation systems, and fraud detection use cases.\nDevelop metadata-driven pipelines and lineage tracking for graph and relational data processing.\nEnsure data quality, governance, and security standards are met across all graph data initiatives.\nMentor junior engineers and contribute to data engineering best practices, especially around graph-centric patterns and technologies.\nStay up to date with the latest developments in graph technology, graph ML, and network analytics.\nWhat we expect of you\nMust-Have Skills:\nHands-on experience in Databricks, including PySpark, Delta Lake, and notebook-based development.\nHands-on experience with graph database platforms such as Stardog, Neo4j, Marklogic etc.\nStrong understanding of graph theory, graph modeling, and traversal algorithms\nProficiency in workflow orchestration, performance tuning on big data processing\nStrong understanding of AWS services\nAbility to quickly learn, adapt and apply new technologies with strong problem-solving and analytical skills\nExcellent collaboration and communication skills, with experience working with Scaled Agile Framework (SAFe), Agile delivery practices, and DevOps practices.\nGood-to-Have Skills:\nGood to have deep expertise in Biotech & Pharma industries\nExperience in writing APIs to make the data available to the consumers\nExperienced with SQL/NOSQL database, vector database for large language models\nExperienced with data modeling and performance tuning for both OLAP and OLTP databases\nExperienced with software engineering best-practices, including but not limited to version control (Git, Subversion, etc.), CI/CD (Jenkins, Maven etc.), automated unit testing, and Dev Ops\nEducation and Professional Certifications\nMasters degree and 3 to 4 + years of Computer Science, IT or related field experience\nBachelors degree and 5 to 8 + years of Computer Science, IT or related field experience\nAWS Certified Data Engineer preferred\nDatabricks Certificate preferred\nScaled Agile SAFe certification preferred\nSoft Skills:\nExcellent analytical and troubleshooting skills.\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nTeam-oriented, with a focus on achieving team goals.\nAbility to learn quickly, be organized and detail oriented.\nStrong presentation and public speaking skills.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'SPARQL', 'Maven', 'PySpark', 'GSQL', 'Subversion', 'AWS services', 'Stardog', 'Cypher', 'SAFe', 'Jenkins', 'DevOps', 'Git', 'Neo4j', 'Delta Lake', 'Graph Databases', 'Spark', 'Marklogic', 'Gremlin']",2025-06-12 06:18:41
Immediate Joiner- Data Engineer,Healthedge,1 - 4 years,Not Disclosed,['Bengaluru'],"Data Engineer\nYou will be working with agile cross functional software development teams developing cutting age software to solve a significant problem in the Provider Data Management space. This hire will have experience building large scale complex data systems involving multiple cross functional data sets and teams. The ideal candidate will be excited about working on new product development, is comfortable pushing the envelope and challenging the status quo, sets high standards for him/herself and the team, and works well with ambiguity.\nWhat you will do:\nBuild data pipelines to assemble large, complex sets of data that meet non-functional and functional business requirements.\nWork closely with data architect, SMEs and other technology partners to develop & execute data architecture and product roadmap.\nBuild analytical tools to utilize the data pipeline, providing actionable insight into key business performance including operational efficiency and business metrics.\nWork with stakeholders including the leadership, product, customer teams to support their data infrastructure needs while assisting with data-related technical issues.\nAct as a subject matter expert to other team members for technical guidance, solution design and best practices within the customer organization.\nKeep current on big data and data visualization technology trends, evaluate, work on proof-of-concept and make recommendations on cloud technologies.\nWhat you bring:\n2+ years of data engineering experience working in partnership with large data sets (preferably terabyte scale)\nExperience in building data pipelines using any of the ETL tools such as Glue, ADF, Notebooks, Stored Procedures, SQL/Python constructs or similar.\nDeep experience working with industry standard RDBMS such Postgres, SQL Server, Oracle, MySQL etc. and any of the analytical cloud databases such as Big Query, Redshift, Snowflake or similar\nAdvanced SQL expertise and solid programming experience with Python and/or Spark\nExperience working with orchestration tools such as Airflow and building complex dependency workflows.\nExperience, developing and implementing Data Warehouse or Data Lake Architectures, OLAP technologies, data modeling with star/snowflake-schemas to enable analytics & reporting.\nGreat problem-solving capabilities, troubleshooting data issues and experience in stabilizing big data systems.\nExcellent communication and presentation skills as youll be regularly interacting with stakeholders and engineering leadership.\nBachelors or master's in quantitative disciplines such as Computer Science, Computer Engineering, Analytics, Mathematics, Statistics, Information Systems, or other scientific fields.\nBonus points:\nHands-on deep experience with cloud data migration, and experience working with analytic platforms like Fabric, Databricks on the cloud.\nCertification in one of the cloud platforms (AWS/GCP/Azure)\nExperience or demonstrated understanding with real-time data streaming tools like Kafka, Kinesis or any similar tools.",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['ETL', 'SQL', 'Pyspark', 'Cloud', 'Python']",2025-06-12 06:18:43
Data Analyst,Skywings Advisors,4 - 9 years,10-17 Lacs P.A.,['Mumbai'],"Actively work with latest technologies&leading practices specific to analytics,data visualization,AI/ML&RPA to drive strategic benefits in the area of audit quality,efficiency&value creation leading audit related data extractions,enablement,analytics\n\nRequired Candidate profile\nMin 4 yrs relevant data analytics experience,EDW concepts,good understanding of data mining and programming languages such as python, oracle sql\nworking exp with visualization tools power BI,SAP BO",Industry Type: Banking,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Analysis', 'Data Anlalytics', 'Power Bi', 'Oracle SQL', 'Data Visualization', 'Data Extraction', 'SQL']",2025-06-12 06:18:45
Data Analyst,Flywings Hr Services,10 - 17 years,20-22.5 Lacs P.A.,['Pune'],"we are looking Data analyst who has experienced in Data transformation, Etl, Data modeling at least 5 years experienced & No Sql & Complex database at least 5 Years Experienced",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Transformation', 'NoSQL', 'Data Modeling', 'ETL']",2025-06-12 06:18:47
Data Engineering Lead,Yotta Techports,10 - 15 years,30-35 Lacs P.A.,['Hyderabad'],"Responsibilities:\nLead and manage an offshore team of data engineers, providing strategic guidance, mentorship, and support to ensure the successful delivery of projects and the development of team members.\nCollaborate closely with onshore stakeholders to understand project requirements, allocate resources efficiently, and ensure alignment with client expectations and project timelines.\nDrive the technical design, implementation, and optimization of data pipelines, ETL processes, and data warehouses, ensuring scalability, performance, and reliability.\nDefine and enforce engineering best practices, coding standards, and data quality standards to maintain high-quality deliverables and mitigate project risks.\nStay abreast of emerging technologies and industry trends in data engineering, and provide recommendations for tooling, process improvements, and skill development.\nAssume a data architect role as needed, leading the design and implementation of data architecture solutions, data modeling, and optimization strategies.\nDemonstrate proficiency in AWS services such as:\nExpertise in cloud data services, including AWS services like Amazon Redshift, Amazon EMR, and AWS Glue, to design and implement scalable data solutions.\nExperience with cloud infrastructure services such as AWS EC2, AWS S3, to optimize data processing and storage.\nKnowledge of cloud security best practices, IAM roles, and encryption mechanisms to ensure data privacy and compliance.\nProficiency in managing or implementing cloud data warehouse solutions, including data modeling, schema design, performance tuning, and optimization techniques.\nDemonstrate proficiency in modern data platforms such as Snowflake and Databricks, including:\nDeep understanding of Snowflake's architecture, capabilities, and best practices for designing and implementing data warehouse solutions.\nHands-on experience with Databricks for data engineering, data processing, and machine learning tasks, leveraging Spark clusters for scalable data processing.\nAbility to optimize Snowflake and Databricks configurations for performance, scalability, and cost-effectiveness.\nManage the offshore team's performance, including resource allocation, performance evaluations, and professional development, to maximize team productivity and morale.\n\nQualifications:\nBachelor's degree in Computer Science, Engineering, or a related field; advanced degree preferred.\n10+ years of experience in data engineering, with a proven track record of leadership and technical expertise in managing complex data projects.\nProficiency in programming languages such as Python, Java, or Scala, as well as expertise in SQL and relational databases (e.g., PostgreSQL, MySQL).\nStrong understanding of distributed computing, cloud technologies (e.g., AWS), and big data frameworks (e.g., Hadoop, Spark).\nExperience with data architecture design, data modeling, and optimization techniques.\nExcellent communication, collaboration, and leadership skills, with the ability to effectively manage remote teams and engage with onshore stakeholders.\nProven ability to adapt to evolving project requirements and effectively prioritize tasks in a fast-paced environment.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Team Handling', 'Snowflake', 'Data Services', 'Cloud Infrastructure', 'Data Bricks']",2025-06-12 06:18:50
Urgent hiring For Cloud Data Engineer,Wowjobs,7 - 10 years,30-45 Lacs P.A.,"['Hyderabad', 'Chennai', 'Bengaluru']","Role & responsibilities\n*  Design, Build, and Maintain ETL Pipelines: Develop robust, scalable, and efficient ETL workflows to ingest, transform, and load data into distributed data products within the Data Mesh architecture.\n*   Data Transformation with dbt: Use dbt to build modular, reusable transformation workflows that align with the principles of Data Products.\n*   Cloud Expertise: Leverage Google Cloud Platform (GCP) services such as BigQuery, Cloud Storage, Pub/Sub, and Dataflow to implement highly scalable data solutions.\n*   Data Quality & Governance: Enforce strict data quality standards by implementing validation checks, anomaly detection mechanisms, and monitoring frameworks.\n*   Performance Optimization: Continuously optimize ETL pipelines for speed, scalability, and cost efficiency.\n*   Collaboration & Ownership: Work closely with data product owners, BI developers, and stakeholders to understand requirements and deliver on expectations. Take full ownership of your deliverables.\n*   Documentation & Standards: Maintain detailed documentation of ETL workflows, enforce coding standards, and adhere to best practices in data engineering.\n*   Troubleshooting & Issue Resolution: Proactively identify bottlenecks or issues in pipelines and resolve them quickly with minimal disruption.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Python coding', 'Cloud data engineer', 'ETL Workflow']",2025-06-12 06:18:52
Snowflake Data Engineer,Tredence,3 - 8 years,Not Disclosed,['Bengaluru'],"Role & responsibilities\n\nDesign, build, and maintain scalable data pipelines using DBT and Airflow.\nDevelop and optimize SQL queries and data models in Snowflake.\nImplement ETL/ELT workflows, ensuring data quality, performance, and reliability.\nWork with Python for data processing, automation, and integration tasks.\nHandle JSON data structures for data ingestion, transformation, and APIs.\nLeverage AWS services (e.g., S3, Lambda, Glue, Redshift) for cloud-based data solutions. Collaborate with data analysts, engineers, and business teams to deliver high-quality data products.",,,,"['Snowflake', 'DBT', 'SQL']",2025-06-12 06:18:54
Tech. PM - Data Engineering-Data Analytics@ Gurgaon/Blore_Urgent,A global leader in delivering innovative...,5 - 10 years,Not Disclosed,"['Gurugram', 'Bengaluru']","Job Title - Technical Project Manager\n\nLocation - Gurgaon/ Bangalore\n\nNature of Job - Permanent\n\nDepartment - data analytics\n\nWhat you will be doing\n\n\nDemonstrated client servicing and business analytics skills with at least 5 - 9 years of experience as data engineer, BI developer, data analyst, technical project manager, program manager etc.\nTechnical project management- drive BRD, project scope, resource allocation, team\ncoordination, stakeholder communication, UAT, Prod fix, change requests, project governance\nSound knowledge of banking industry (payments, retail operations, fraud etc.)\nStrong ETL experience or experienced Teradata developer\nManaging team of business analysts, BI developers, ETL developers to ensure that projects are completed on time\nResponsible for providing thought leadership and technical advice on business issues\nDesign methodological frameworks and solutions.\n\n\nWhat were looking for\n\n\nBachelors/masters degree in computer science/data science/AI/statistics, Certification in Gen AI. Masters degree Preferred.\nManage multiple projects, at a time, from inception to delivery\nSuperior problem-solving, analytical, and quantitative skills\nEntrepreneurial mindset, coupled with a “can do” attitude\nDemonstrated ability to collaborate with cross-functional, cross-border teams and coach / mentor colleagues.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Technical Project Manager', 'Data Engineering', 'multiple projects', 'Technical project management', 'Data Analytics', 'project scope', 'ETL Pipeline', 'team coordination', 'resource allocation', 'Prod fix', 'drive BRD', 'program manager', 'Big data']",2025-06-12 06:18:56
Sr Data Engineer - Fully Remote & Immediate Opportunity,Zealogics.com,10 - 15 years,Not Disclosed,[],"10 yrs of exp working in cloud-native data (Azure Preferred),Databricks, SQL,PySpark, migrating from Hive Metastore to Unity Catalog, Unity Catalog, implementing Row-Level Security (RLS), metadata-driven ETL design patterns,Databricks certifications",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'Azure', 'Metadata', 'Data Bricks', 'Unity Catalog', 'ETL design', 'SQL']",2025-06-12 06:18:59
Data Science_ Lead,Rishabh Software,8 - 13 years,Not Disclosed,"['Ahmedabad', 'Bengaluru', 'Vadodara']","Job Description\n\nWith excellent analytical and problem-solving skills, you should understand business problems of the customers, translate them into scope of work and technical specifications for developing into Data Science projects. Efficiently utilize cutting edge technologies in AI, Generative AI areas and implement solutions for business problems. Good exposure technology platforms for Data Science, AI, Gen AI, cloud with implementation experience. Ability to provide end to end technical solutions leveraging latest AI, Gen AI tools, frameworks for the business problems. This Job requires the following:",,,,"['Data Science', 'gen ai', 'Computer Vision', 'Machine Learning', 'Deep Learning', 'Tensorflow', 'NLP', 'Artificial Intelligence', 'Dl', 'Python']",2025-06-12 06:19:01
Director - Data Science,Axtria,12 - 17 years,Not Disclosed,['Noida'],"Minimum 12+ years of relevant experience in building software applications in data and analytics field\nEnhance the go-to-market strategy by designing new and relevant solution frameworks to accelerate our clients’ journeys for impacting patient outcomes. Pitch for these opportunities and craft winning proposals to grow the Data Science Practice.\nBuild and lead a team of data scientists and analysts, fostering a collaborative and innovative environment.\nOversee the design and delivery of the models, ensuring projects are completed on time and meet business objectives.\nEngaging in consultative selling with clients to grow/deliver business.\nDevelop and operationalize scalable processes to deliver on large & complex client engagements.\nExtensive hands-on experience with Python, R, or Julia, focusing on data science and generative AI frameworks.\nExpertise in working with generative models such as GPT, DALL-E, Stable Diffusion, Codex, and MidJourney for various applications.\nProficiency in fine-tuning and deploying generative models using libraries like Hugging Face Transformers, Diffusers, or PyTorch Lightning.\nStrong understanding of generative techniques, including GANs, VAEs, diffusion models, and autoregressive models.\nExperience in prompt engineering, zero-shot, and few-shot learning for optimizing generative AI outputs across different use cases.\nExpertise in managing generative AI data pipelines, including preprocessing large-scale multimodal datasets for text, image, or code generation.",Industry Type: Analytics / KPO / Research,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['application software', 'python', 'artificial intelligence', 'r', 'julia', 'hive', 'natural language processing', 'neural networks', 'predictive analytics', 'machine learning', 'sql', 'deep learning', 'java', 'data science', 'spark', 'predictive modeling', 'pytorch', 'hadoop', 'statistics']",2025-06-12 06:19:03
Data Engineer II - Marketplace (Experimentation Track),Booking Holdings,5 - 10 years,Not Disclosed,['Bengaluru'],"We are looking for a Data Engineer to join our team and help us to improve the platform that supports one of the best experimentation tools in the world.\nYou will work side by side with other data engineers and site reliability engineers to improve the reliability, scalability, maintenance and operations of all the data products that are part of the experimentation tool at Booking.com.\nYour day to day work includes but is not limited to: maintenance and operations of data pipelines and products that handles data at big scale; the development of capabilities for monitoring, alerting, testing and troubleshooting of the data ecosystem of the experiment platform; and the delivery of data products that produce metrics for experimentation at scale. You will collaborate with colleagues in Amsterdam to achieve results the right way. This will include engineering managers, product managers, engineers and data scientists.\nKey Responsibilities and Duties\nTake ownership of multiple data pipelines and products and provide innovative solutions to reduce the operational workload required to maintain them\nRapidly developing next-generation scalable, flexible, and high-performance data pipelines.\nContribute to the development of data platform capabilities such as testing, monitoring, debugging and alerting to improve the development environment of data products\nSolve issues with data and data pipelines, prioritizing based on customer impact.\nEnd-to-end ownership of data quality in complex datasets and data pipelines.\nExperiment with new tools and technologies, driving innovative engineering solutions to meet business requirements regarding performance, scaling, and data quality.\nProvide self-organizing tools that help the analytics community discover data, assess quality, explore usage, and find peers with relevant expertise.\nServe as the main point of contact for technical and business stakeholders regarding data engineering issues, such as pipeline failures and data quality concerns\nRole requirements\nMinimum 5 years of hands-on experience in data engineering as a Data Engineer or as a Software Engineer developing data pipelines and products.\nBachelors degree in Computer Science, Computer or Electrical Engineering, Mathematics, or a related field or 5 years of progressively responsible experience in the specialty as equivalent\nSolid experience in at least one programming language. We use Java and Python\nExperience building production data pipelines in the cloud, setting up data-lakes and server-less solutions\nHands-on experience with schema design and data modeling\nExperience designing systems E2E and knowledge of basic concepts (lb, db, caching, NoSQL, etc)\nKnowledge of Flink, CDC, Kafka, Airflow, Snowflake, DBT or equivalent tools\nPractical experience building data platform capabilities like testing, alerting, monitoring, debugging, security\nExperience working with big data.\nExperience working with teams located in different timezones is a plus\nExperience with experimentation, statistics and A/B testing is a plus",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Airflow', 'Java', 'CDC', 'NoSQL', 'Snowflake', 'DBT', 'Kafka', 'Python']",2025-06-12 06:19:06
Manager - Data Science,Axtria,6 - 11 years,Not Disclosed,['Noida'],"Job Summary-\n\nData Scientist with good hands-on experience of 6+ years in developing state of the art and scalable Machine Learning models and their operationalization, leveraging off-the-shelf workbench production.\n\nJob Responsibilities-\n\nHands on experience in Python data-science and math packages such as NumPy, Pandas, Sklearn, Seaborn, PyCaret, Matplotlib\nProficiency in Python and common Machine Learning frameworks (TensorFlow, NLTK, Stanford NLP, PyTorch, Ling Pipe, Caffe, Keras, SparkML and OpenAI etc.)\nExperience of working in large teams and using collaboration tools like GIT, Jira and Confluence\nGood understanding of any of the cloud platform – AWS, Azure or GCP\nUnderstanding of Commercial Pharma landscape and Patient Data / Analytics would be a huge plus\nShould have an attitude of willingness to learn, accepting the challenging environment and confidence in delivering the results within timelines. Should be inclined towards self motivation and self-driven to find solutions for problems.\nShould be able to mentor and guide mid to large sized teams under him/her\n\n\nJob -\nStrong experience on Spark with Scala/Python/Java\nStrong proficiency in building/training/evaluating state of the art machine learning models and its deployment\nProficiency in Statistical and Probabilistic methods such as SVM, Decision-Trees, Bagging and Boosting Techniques, Clustering\nProficiency in Core NLP techniques like Text Classification, Named Entity Recognition (NER), Topic Modeling, Sentiment Analysis, etc. Understanding of Generative AI / Large Language Models / Transformers would be a plus",Industry Type: Analytics / KPO / Research,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['scala', 'java', 'spark', 'machine learning algorithms', 'python', 'confluence', 'scikit-learn', 'nltk', 'training', 'numpy', 'tensorflow', 'git', 'seaborn', 'gcp', 'pytorch', 'keras', 'spark mllib', 'jira', 'sentiment analysis', 'lingpipe', 'caffe', 'microsoft azure', 'pandas', 'matplotlib', 'aws', 'statistics']",2025-06-12 06:19:09
Data Analyst,Bhavani Shipping services,1 - 3 years,1-2.75 Lacs P.A.,"['Thane', 'Navi Mumbai', 'Mumbai (All Areas)']","Principal Duties and Responsibilities\nInterpreting data, analyzing results using statistical techniques.\nDeveloping and implementing data analyses, data collection systems and other strategies that optimize statistical efficiency and quality.\nAcquiring data from primary or secondary data sources and maintaining databases.\n\nKey Responsibilities:\n\nData Collection and Processing:\nGather data from various sources, ensuring accuracy and completeness.\nCleanse and preprocess data to remove errors and inconsistencies.\nStatistical Analysis and Interpretation:\nUtilize statistical methods to analyze data and identify trends, patterns, and correlations.\nPresent findings through reports, visualizations, and presentations to stakeholders.\nData Visualization and Reporting:\nCreate visualizations and dashboards to effectively communicate insights.\nPrepare regular reports and ad-hoc analyses to support strategic decision-making.\nProblem-Solving and Recommendations:\nCollaborate with cross-functional teams to address business challenges using data-driven insights.",Industry Type: Courier / Logistics,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Reporting', 'Data Analysis', 'Excel Powerpoint', 'Data Extraction', 'Charts', 'Data Collection', 'Advanced Excel', 'Power Point Presentation', 'Powerpoint', 'Excel Report Preparation', 'Presentation Skills', 'Report Generation', 'MS Office Tools', 'Dashboards', 'MS Office Word']",2025-06-12 06:19:11
Data Engineering Lead Microsoft Power BI,Client of Techs To Suit,8 - 12 years,25-32.5 Lacs P.A.,"['Indore', 'Hyderabad', 'Ahmedabad']","Poistion - Data Engineering Lead\nExp - 8 to 12 Years\nJob Location: Hyderabad, Ahmedabad, Indore, India.\n\nMust be Able to join in 30 days\nJob Summary:\nAs a Data Engineering Lead, your role will involve designing, developing, and implementing\ninteractive dashboards and reports using data engineering tools. You will work closely with\nstakeholders to gather requirements and translate them into effective data visualizations that\nprovide valuable insights. Additionally, you will be responsible for extracting, transforming, and\nloading data from multiple sources into Power BI, ensuring its accuracy and integrity. Your\nexpertise in Power BI and data analytics will contribute to informed decision-making and\nsupport the organization in driving data-centric strategies and initiatives.\nWe are looking for you!\nAs an ideal candidate for the Data Engineering Lead position, you embody the qualities of a\nteam player with a relentless get-it-done attitude. Your intellectual curiosity and customer\nfocus drive you to continuously seek new ways to add value to your job accomplishments. You\nthrive under pressure, maintaining a positive attitude and understanding that your career is a\njourney. You are willing to make the right choices to support your growth. In addition to your\nexcellent communication skills, both written and verbal, you have a proven ability to create\nvisually compelling designs using tools like Power BI and Tableau that effectively\ncommunicate our core values.\nYou build high-performing, scalable, enterprise-grade applications and teams. Your creativity\nand proactive nature enable you to think differently, find innovative solutions, deliver high-\nquality outputs, and ensure customers remain referenceable. With over eight years of\nexperience in data engineering, you possess a strong sense of self-motivation and take\nownership of your responsibilities. You prefer to work independently with little to no\nsupervision.\nYou are process-oriented, adopt a methodical approach, and demonstrate a quality-first\nmindset. You have led mid to large-size teams and accounts, consistently using constructive\nfeedback mechanisms to improve productivity, accountability, and performance within the\nteam. Your track record showcases your results-driven approach, as you have consistently\ndelivered successful projects with customer case studies published on public platforms.\nOverall, you possess a unique combination of skills, qualities, and experiences that make you\nan ideal fit to lead our data engineering team(s). You value inclusivity and want to join a culture\nthat empowers you to show up as your authentic self.\nYou know that success hinges on commitment, our differences make us stronger, and the\nfinish line is always sweeter when the whole team crosses together. In your role, you shouldbe driving the team using data, data, and more data. You will manage multiple teams, oversee\nagile stories and their statuses, handle escalations and mitigations, plan ahead, identify hiring\nneeds, collaborate with recruitment teams for hiring, enable sales with pre-sales teams, and\nwork closely with development managers/leads for solutioning and delivery statuses, as well\nas architects for technology research and solutions.\nWhat You Will Do:\nAnalyze Business Requirements.\nAnalyze the Data Model and do GAP analysis with Business Requirements and Power\nBI. Design and Model Power BI schema.\nTransformation of Data in Power BI/SQL/ETL Tool.\nCreate DAX Formula, Reports, and Dashboards. Able to write DAX formulas.\nExperience writing SQL Queries and stored procedures.\nDesign effective Power BI solutions based on business requirements.\nManage a team of Power BI developers and guide their work.\nIntegrate data from various sources into Power BI for analysis.\nOptimize performance of reports and dashboards for smooth usage.\nCollaborate with stakeholders to align Power BI projects with goals.\nKnowledge of Data Warehousing(must), Data Engineering is a plus",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Power Bi Dashboards', 'Microsoft Power Bi', 'SQL Queries', 'Azure Databricks', 'Dax', 'Azure Data Factory']",2025-06-12 06:19:14
Azure Data Engineer,Big4,7 - 12 years,18-30 Lacs P.A.,['Bengaluru'],"Urgently Hiring for Senior Azure Data Engineer\n\nJob Location- Bangalore\nMinimum exp - Total 7+yrs with min 4 years relevant exp\n\nKeywords Databricks, Pyspark, SCALA, SQL, Live / Streaming data, batch processing data\n\nShare CV siddhi.pandey@adecco.com\nOR Call 6366783349\n\nRoles and Responsibilities:\nThe Data Engineer will work on data engineering projects for various business units, focusing on delivery of complex data management solutions by leveraging industry best practices. They work with the project team to build the most efficient data pipelines and data management solutions that make data easily available for consuming applications and analytical solutions. A Data engineer is expected to possess strong technical skills\n\nKey Characteristics\nTechnology champion who constantly pursues skill enhancement and has inherent curiosity to understand work from multiple dimensions\nInterest and passion in Big Data technologies and appreciates the value that can be brought in with an effective data management solution\nHas worked on real data challenges and handled high volume, velocity, and variety of data.\nExcellent analytical & problem-solving skills, willingness to take ownership and resolve technical challenges.\nContributes to community building initiatives like CoE, CoP.\nMandatory skills:\nAzure - Master\nELT - Skill\nData Modeling - Skill\nData Integration & Ingestion - Skill\nData Manipulation and Processing - Skill\nGITHUB, Action, Azure DevOps - Skill\nData factory, Databricks, SQL DB, Synapse, Stream Analytics, Glue, Airflow, Kinesis, Redshift, SonarQube, PyTest - Skill\nOptional skills:\nExperience in project management, running a scrum team.\nExperience working with BPC, Planning.\nExposure to working with external technical ecosystem.\nMKDocs documentation\n\nShare CV siddhi.pandey@adecco.com\nOR Call 6366783349",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['databricks', 'Azure Synapse', 'Pyspark', 'Stream Analytics', 'SCALA', 'SQL Azure', 'Data Bricks', 'SQL']",2025-06-12 06:19:16
Senior Data Engineer,Conviction HR,8 - 10 years,Not Disclosed,"['Kolkata', 'Hyderabad', 'Pune( Malad )']","Must have -Azure Data Factory (Mandatory). Azure Databricks, Pyspark and Python and advance SQL Azure eco-system. 1) Advanced SQL Skills. 2)Data Analysis. 3) Data Models. 4) Python (Desired). 5) Automation - Experience required : 8 to 10 years.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Data Factory', 'Data Engineering', 'Python', 'Azure Databricks', 'Data Modeling', 'Data Bricks', 'SQL']",2025-06-12 06:19:19
Gcp Data Engineer,Royal Cyber,9 - 14 years,Not Disclosed,[],"Minimum 7+ years in data engineering with 5+ years of hands-on experience on GCP.\nProven track record with tools and services like BigQuery, Cloud Composer (Apache Airflow), Cloud Functions, Pub/Sub, Cloud Storage, Dataflow, and IAM/VPC.\nDemonstrated expertise in Apache Spark (batch and streaming), PySpark, and building scalable API integrations.\nAdvanced Airflow skills including custom operators, dynamic DAGs, and workflow performance tuning.\nCertifications\nGoogle Cloud Professional Data Engineer certification preferred.\nKey Skills\nMandatory Technical Skills\nAdvanced Python (PySpark, Pandas, pytest) for automation and data pipelines.\nStrong SQL with experience in window functions, CTEs, partitioning, and optimization.\nProficiency in GCP services including BigQuery, Dataflow, Cloud Composer, Cloud Functions, and Cloud Storage.\nHands-on with Apache Airflow, including dynamic DAGs, retries, and SLA enforcement.\nExpertise in API data ingestion, Postman collections, and REST/GraphQL integration workflows.\nFamiliarity with CI/CD workflows using Git, Jenkins, or Bitbucket.\nExperience with infrastructure security and governance using IAM and VPC.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Part Time, Temporary/Contractual","['GCP', 'Bigquery', 'Google Cloud Platforms', 'Cloud Storage', 'Data Flow']",2025-06-12 06:19:21
Data Engineer - AWS,Happiest Minds Technologies,6 - 10 years,Not Disclosed,"['Pune', 'Bengaluru']","Role & responsibilities\nEssential Skills: Experience: 6 to 10 yrs\n- Technical Expertise: Proficiency in AWS services such as Amazon S3, Redshift, EMR, Glue, Lambda, and Kinesis. Strong skills in SQL and experience with scripting languages like Python or Java.\n- Data Engineering Experience: Hands on experience in building and maintaining data pipelines, data modeling, and working with big data technologies.\n- Problem-Solving Skills: Ability to analyze complex data issues and develop effective solutions to optimize data processing and storage.",,,,"['Data Engineering', 'Pyspark', 'Aws Lambda', 'Amazon Redshift', 'Aws Glue', 'Athena', 'AWS', 'Python', 'SQL']",2025-06-12 06:19:24
"Data Engineer( Python, AWS, Databricks, EKS, Airflow)",Banking,5 - 9 years,Not Disclosed,['Bengaluru'],"Exprence 5-8 Years\nLocation - Bangalore\nMode C2H\n\nHands on data engineering experience.\nHands on experience with Python programming\nHands-on Experience with AWS & EKS\nWorking knowledge of Unix, Databases, SQL\nWorking Knowledge on Databricks\nWorking Knowledge on Airflow and DBT",Industry Type: Investment Banking / Venture Capital / Private Equity,Department: Engineering - Software & QA,"Employment Type: Full Time, Temporary/Contractual","['Airflow', 'Data Engineering', 'AWS', 'Python', 'SQL', 'Databricks', 'Eks']",2025-06-12 06:19:26
Aws Data Engineer,Hiring for Leading MNC Company!!,8 - 13 years,Not Disclosed,['Bengaluru'],"Warm Greetings from SP Staffing!!\n\nRole:AWS Data Engineer\nExperience Required :8 to 15 yrs\nWork Location :Bangalore\n\nRequired Skills,\n\nTechnical knowledge of data engineering solutions and practices. Implementation of data pipelines using tools like EMR, AWS Glue, AWS Lambda, AWS Step Functions, API Gateway, Athena\nProficient in Python and Spark, with a focus on ETL data processing and data engineering practices.\n\nInterested candidates can send resumes to nandhini.spstaffing@gmail.com",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['ETL', 'AWS', 'Pyspark', 'python', 'EMR', 'Aws Glue', 'Aws Emr', 'AWS Data Engineer', 'Aws Lambda', 'Lakehouse', 'spark', 'Data Engineer', 'Athena', 'gateway']",2025-06-12 06:19:28
Data Engineer,IT Services Company,2 - 3 years,6-7 Lacs P.A.,['Pune'],"Data Engineer\nJob Description :\nJash Data Sciences: Letting Data Speak!\nDo you love solving real-world data problems with the latest and best techniques? And having fun while solving them in a team! Then come and join our high-energy team of passionate data people. Jash Data Sciences is the right place for you.\nWe are a cutting-edge Data Sciences and Data Engineering startup based in Pune, India. We believe in continuous learning and evolving together. And we let the data speak!\nWhat will you be doing?\nYou will be discovering trends in the data sets and developing algorithms to transform\nraw data for further analytics\nCreate Data Pipelines to bring in data from various sources, with different formats,\ntransform it, and finally load it to the target database.\nImplement ETL/ ELT processes in the cloud using tools like AirFlow, Glue, Stitch, Cloud\nData Fusion, and DataFlow.\nDesign and implement Data Lake, Data Warehouse, and Data Marts in AWS, GCP, or\nAzure using Redshift, BigQuery, PostgreSQL, etc.\nCreating efficient SQL queries and understanding query execution plans for tuning\nqueries on engines like PostgreSQL.\nPerformance tuning of OLAP/ OLTP databases by creating indices, tables, and views.\nWrite Python scripts for the orchestration of data pipelines\nHave thoughtful discussions with customers to understand their data engineering\nrequirements. Break complex requirements into smaller tasks for execution.\nWhat do we need from you?\nStrong Python coding skills with basic knowledge of algorithms/data structures and\ntheir application.\nStrong understanding of Data Engineering concepts including ETL, ELT, Data Lake, Data\nWarehousing, and Data Pipelines.\nExperience designing and implementing Data Lakes, Data Warehouses, and Data Marts\nthat support terabytes of scale data.\nA track record of implementing Data Pipelines on public cloud environments\n(AWS/GCP/Azure) is highly desirable\nA clear understanding of Database concepts like indexing, query performance\noptimization, views, and various types of schemas.\nHands-on SQL programming experience with knowledge of windowing functions,\nsubqueries, and various types of joins.\nExperience working with Big Data technologies like PySpark/ Hadoop\nA good team player with the ability to communicate with clarity\nShow us your git repo/ blog!\nQualification\n1-2 years of experience working on Data Engineering projects for Data Engineer I\n2-5 years of experience working on Data Engineering projects for Data Engineer II\n1-5 years of Hands-on Python programming experience\nBachelors/Masters' degree in Computer Science is good to have\nCourses or Certifications in the area of Data Engineering will be given a higher preference.\nCandidates who have demonstrated a drive for learning and keeping up to date with technology by continuing to do various courses/self-learning will be given high preference.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Airflow', 'Elt', 'Data Mart', 'Data Pipeline', 'ETL', 'Pyspark', 'Hadoop', 'Data Bricks', 'SQL', 'Data Fusion', 'Glue', 'GCP', 'Data Flow', 'Data Warehousing', 'Azzure', 'AWS']",2025-06-12 06:19:31
Big Data Engineer_ Info Edge _Noida,Info Edge,1 - 4 years,14-19 Lacs P.A.,['Noida'],"About Info Edge India Ltd.\nInfo Edge: Info Edge (India) Limited (NSE: NAUKRI) is among the leading internet companies in India. Info Edge, Indias premier online classifieds company is fundamentally in the matching business. With a network of 62 offices located in 43 cities throughout India, Info Edge has 5000 plus employees engaged in innovation, product development, integration with mobile and social media, technology and technology updation, research and development, quality assurance, sales, marketing and payment collection.\nThe umbrella brand has an online recruitment classifieds, www.naukri.com– India’s No. 1 Jobsite with over 75% traffic share, a matrimony classifieds, www.jeevansathi.com, a real estate classifieds, www.99acres.com– India’s largest property marketplace and an education classifieds, www.shiksha.com. Find out more about the Company at",,,,"['Data Modeling', 'Python', 'SCALA']",2025-06-12 06:19:33
Data Engineer,Amgen Inc,1 - 6 years,Not Disclosed,['Hyderabad'],"Role Description:\nAs part of the cybersecurity organization, In this vital role you will be responsible for designing, building, and maintaining data infrastructure to support data-driven decision-making. This role involves working with large datasets, developing reports, executing data governance initiatives, and ensuring data is accessible, reliable, and efficiently managed. The role sits at the intersection of data infrastructure and business insight delivery, requiring the Data Engineer to design and build robust data pipelines while also translating data into meaningful visualizations for stakeholders across the organization. The ideal candidate has strong technical skills, experience with big data technologies, and a deep understanding of data architecture, ETL processes, and cybersecurity data frameworks.\nRoles & Responsibilities:\nDesign, develop, and maintain data solutions for data generation, collection, and processing.\nBe a key team member that assists in design and development of the data pipeline.\nBuild data pipelines and ensure data quality by implementing ETL processes to migrate and deploy data across systems.\nDevelop and maintain interactive dashboards and reports using tools like Tableau, ensuring data accuracy and usability\nSchedule and manage workflows the ensure pipelines run on schedule and are monitored for failures.\nCollaborate with multi-functional teams to understand data requirements and design solutions that meet business needs.\nDevelop and maintain data models, data dictionaries, and other documentation to ensure data accuracy and consistency.\nImplement data security and privacy measures to protect sensitive data.\nLeverage cloud platforms (AWS preferred) to build scalable and efficient data solutions.\nCollaborate and communicate effectively with product teams.\nCollaborate with data scientists to develop pipelines that meet dynamic business needs.\nShare and discuss findings with team members practicing SAFe Agile delivery model.\n\n\nBasic Qualifications:\nMasters degree and 1 to 3 years of experience of Computer Science, IT or related field experience OR\nBachelors degree and 3 to 5 years of Computer Science, IT or related field experience OR\nDiploma and 7 to 9 years of Computer Science, IT or related field experience\nPreferred Qualifications:\nHands on experience with data practices, technologies, and platforms, such as Databricks, Python, GitLab, LucidChart, etc.\nHands-on experience with data visualization and dashboarding toolsTableau, Power BI, or similar is a plus\nProficiency in data analysis tools (e.g. SQL) and experience with data sourcing tools\nExcellent problem-solving skills and the ability to work with large, complex datasets\nUnderstanding of data governance frameworks, tools, and best practices\nKnowledge of and experience with data standards (FAIR) and protection regulations and compliance requirements (e.g., GDPR, CCPA)\n\nGood-to-Have Skills:\nExperience with ETL tools and various Python packages related to data processing, machine learning model development\nStrong understanding of data modeling, data warehousing, and data integration concepts\nKnowledge of Python/R, Databricks, cloud data platforms\nExperience working in Product team's environment\nExperience working in an Agile environment\n\nProfessional Certifications:\nAWS Certified Data Engineer preferred\nDatabricks Certificate preferred\n\nSoft Skills:\nInitiative to explore alternate technology and approaches to solving problems\nSkilled in breaking down problems, documenting problem statements, and estimating efforts\nExcellent analytical and troubleshooting skills\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation\nAbility to handle multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['data engineering', 'data analysis', 'data modeling', 'analysis tools', 'data warehousing', 'troubleshooting', 'data architecture', 'data integration', 'etl process']",2025-06-12 06:19:35
Data Engineer,Xenonstack,2 - 5 years,Not Disclosed,['Mohali( Phase 8B Mohali )'],"At XenonStack, We committed to become the Most Value Driven Cloud Native, Platform Engineering and Decision Driven Analytics Company. Our Consulting Services and Solutions towards the Neural Company and its Key Drivers.\nXenonStacks DataOps team is looking for a Data Engineer who will be responsible for employing techniques to create and sustain structures that allow for the analysis of data while remaining familiar with dominant programming and deployment strategies in the field.\nYou should demonstrate flexibility, creativity, and the capacity to receive and utilize constructive criticism. The ideal candidate should be highly skilled in all aspects of Python, Java/Scala, SQL and analytical skills.\nJob Responsibilities:\nDevelop, construct, test and maintain Data Platform Architectures\nAlign Data Architecture with business requirements\nLiaising with co-workers and clients to elucidate the requirements for each task.\nScalable and High Performant Data Platform Infrastructure that allows big data to be accessed and analysed quickly by BI & AI Teams.\nReformulating existing frameworks to optimize their functioning.\nTransforming Raw Data into InSights for manipulation by Data Scientists.\nEnsuring that your work remains backed up and readily accessible to relevant co-workers.\nRemaining up-to-date with industry standards and technological advancements that will improve the quality of your outputs.\nRequirements:\nTechnical Requirements\nExperience of Python, Java/Scala\nGreat Statistical / SQL based Analytical Skills\nExperience of Data Analytics Architectural Design Patterns for Batch, Event Driven and Real-Time Analytics Use Cases\nUnderstanding of Data warehousing, ETL tools, machine learning, Data EPIs\nExcellent in Algorithms and Data Systems\nUnderstanding of Distributed System for Data Processing and Analytics\nFamiliarity with Popular Data Analytics Framework like Hadoop , Spark , Delta Lake , Time Series / Analytical Stores Stores.\nProfessional Attributes:\nExcellent communication skills & Attention to detail.\nAnalytical mind and problem-solving Aptitude with Strong Organizational skills & Visual Thinking.\nBenefits:\nDiscover the benefits of joining our team:\nDynamic and purposeful work culture in a people-oriented organization contributing to multi-million-dollar projects with guaranteed job security.\nOpen, authentic, and transparent communication fostering a warm work environment.\nRegular constructive feedback and exposure to diverse technologies.\nRecognition and rewards for exceptional performance achievements.\nAccess to certification courses & Skill Sessions to develop continually and refine your skills.\nAdditional allowances for team members assigned to specific projects.\nSpecial skill allowances to acknowledge and compensate for unique expertise.\nComprehensive medical insurance policy for your health and well-being.\nTo Learn more about the company -\nWebsite - http://www.xenonstack.com/",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Hadoop', 'Spark', 'ETL', 'Python', 'SQL', 'Java', 'Data Processing', 'Machine Learning']",2025-06-12 06:19:38
Data Governance Engineers,Meritus Management Service,4 - 9 years,14-17 Lacs P.A.,"['Pune', 'Gurugram']","Define, implement, & enforce data governance policies & standards to ensure data quality, consistency, & compliance across the organization\nCollaborate with data stewards, business users, & IT teams to maintain metadata, lineage, & data catalog tools",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Stewardship', 'Metadata', 'Data Governance', 'Metadata Management', 'Data Lineage', 'Data Modeling', 'SQL']",2025-06-12 06:19:40
DBA DATA Engineer || 12 Lakhs CTC,Robotics Technologies,8 - 9 years,12 Lacs P.A.,['Hyderabad( Banjara hills )'],"Dear Candidate,\nWe are seeking a skilled and experienced DBA Data Engineer to join our growing data team. The ideal candidate will play a key role in designing, implementing, and maintaining our databases and data pipeline architecture. You will collaborate with software engineers, data analysts, and DevOps teams to ensure efficient data flow, data integrity, and optimal database performance across all systems. This role requires a strong foundation in database administration, SQL performance tuning, data modeling, and experience with both on-prem and cloud-based environments.\n\nRequirements:\nBachelors degree in Computer Science, Information Systems, or a related field.\n8+ years of experience in database administration and data engineering.\nProven expertise in RDBMS (Oracle, MySQL, and PostgreSQL) and NoSQL systems (MongoDB, Cassandra).\nExperience managing databases in cloud environments (AWS, Azure, or GCP).\nProficiency in ETL processes and tools (e.g., Apache NiFi, Talend, Informatica, AWS Glue).\nStrong experience with scripting languages such as Python, Bash, or PowerShell.\n\nDBA Data Engineer Roles & Responsibilities:\nDesign and maintain scalable and high-performance database architectures.\nMonitor and optimize database performance using tools like CloudWatch, Oracle Enterprise Manager, pgAdmin, Mongo Compass, or Dynatrace.\nDevelop and manage ETL/ELT pipelines to support business intelligence and analytics.\nEnsure data integrity and security through best practices in backup, recovery, and encryption.\nAutomate regular database maintenance tasks using scripting and scheduled jobs.\nImplement high availability, failover, and disaster recovery strategies.\nConduct performance tuning of queries, stored procedures, indexes, and table structures.\nCollaborate with DevOps to automate database deployments using CI/CD and IaC tools (e.g., Terraform, AWS CloudFormation).\nDesign and implement data models, including star/snowflake schemas for data warehousing.\nDocument data flows, data dictionaries, and database configurations.\nManage user access and security policies using IAM roles or database-native permissions.\nAnalyze existing data systems and propose modernization or migration plans (on-prem to cloud, SQL to NoSQL, etc.).\nUse AWS RDS, Amazon Redshift, Azure SQL Database, or Google BigQuery as needed.\nStay up-to-date with emerging database technologies and make recommendations.\n\nMust-Have Skills:\nDeep knowledge of SQL and database performance tuning.\nHands-on experience with database migrations and replication strategies.\nFamiliarity with data governance, data quality, and compliance frameworks (GDPR, HIPAA, etc.).\nStrong problem-solving and troubleshooting skills.\nExperience with data streaming platforms such as Apache Kafka, AWS Kinesis, or Apache Flink is a plus.\nExperience with data lake and data warehouse architectures.\nExcellent communication and documentation skills.\n\nSoft Skills:\nProblem-Solving: Ability to analyze complex problems and develop effective solutions.\nCommunication Skills: Strong verbal and written communication skills to effectively collaborate with cross-functional teams.\nAnalytical Thinking: Ability to think critically and analytically to solve technical challenges.\nTime Management: Capable of managing multiple tasks and deadlines in a fast-paced environment.\nAdaptability: Ability to quickly learn and adapt to new technologies and methodologies.\n\nInterview Mode : F2F for who are residing in Hyderabad / Zoom for other states\nLocation : 43/A, MLA Colony,Road no 12, Banjara Hills, 500034\nTime : 2 - 4pm",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Database Administration', 'Database Migration', 'Dba Skills', 'High Availability', 'Db Administration', 'Hadr', 'Database Security', 'Disaster Recovery', 'Dr Testing', 'DR', 'Luw', 'Db Upgrade', 'Brtools', 'UDDI', 'Os Migration', 'Dbase', 'DBMS', 'IBM DB2', 'Db Migration', 'Disaster Recovery Planning', 'Db Dba', 'Backup And Recovery']",2025-06-12 06:19:42
"Data Engineer Openings at Advantum Health, Hyderabad",Advantum Health,3 - 5 years,Not Disclosed,['Hyderabad'],"Data Engineer openings at Advantum Health Pvt Ltd, Hyderabad.\nOverview:\nWe are looking for a Data Engineer to build and optimize robust data pipelines that support AI and RCM analytics. This role involves integrating structured and unstructured data from diverse healthcare systems into scalable, AI-ready datasets.\nKey Responsibilities:\nDesign, implement, and optimize data pipelines for ingesting and transforming healthcare and RCM data.\nBuild data marts and warehouses to support analytics and machine learning.\nEnsure data quality, lineage, and governance across AI use cases.\nIntegrate data from EMRs, billing platforms, claims databases, and third-party APIs.\nSupport data infrastructure in a HIPAA-compliant cloud environment.\nQualifications:\nBachelors in Computer Science, Data Engineering, or related field.\n3+ years of experience with ETL/ELT pipelines using tools like Apache Airflow, dbt, or Azure Data Factory.\nStrong SQL and Python skills.\nExperience with healthcare data standards (HL7, FHIR, X12) preferred.\nFamiliarity with data lake house architectures and AI integration best practices\nPh: 9177078628\nEmail id: jobs@advantumhealth.com\nAddress: Advantum Health Private Limited, Cyber gateway, Block C, 4th floor Hitech City, Hyderabad.\nDo follow us on LinkedIn, Facebook, Instagram, YouTube and Threads\nAdvantum Health LinkedIn Page:\nhttps://lnkd.in/gVcQAXK3\n\nAdvantum Health Facebook Page:\nhttps://lnkd.in/g7ARQ378\n\nAdvantum Health Instagram Page:\nhttps://lnkd.in/gtQnB_Gc\n\nAdvantum Health India YouTube link:\nhttps://lnkd.in/g_AxPaPp\n\nAdvantum Health Threads link:\nhttps://lnkd.in/gyq73iQ6",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Data Factory', 'SQL', 'Python', 'Airflow', 'ETL', 'Elt']",2025-06-12 06:19:45
Data Engineer,Reputed Client,5 - 10 years,18-25 Lacs P.A.,[],"Data Engineer\n(Python, PySpark, SQL and Spark SQL)\n\nExperience - 5-10 Years\nMandate Skills: Python, PySpark, SQL and SparkSQL\nWorking Hours: 11:00 am to 8 pm\n\n(Candidate has to be flexible. 4-hour overlap with US business hours)\n\nSalary : 1.50 LPM to 2 LPM + Tax (Rate is not fixed, negotiable depending upon the candidate feedback)\n\nRemote / Hybrid (3 Days in a week WFO) (Pune, Bangalore, Noida, Mumbai, Hyderabad)\n\nNOTE: Need candidates within these cities, they have to collect assets from the office / need to be available for meetings - if they are working remotely)\n\nIt's a 6 months (C2H role).",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Temporary/Contractual","['Python', 'PySpark', 'Spark', 'SQL']",2025-06-12 06:19:47
Data Engineer,Amgen Inc,1 - 6 years,Not Disclosed,['Hyderabad'],"ABOUT THE ROLE\nRole Description:\nAs part of the cybersecurity organization, the Data Engineer is responsible for designing, building, and maintaining data infrastructure to support data-driven decision-making. This role involves working with large datasets, developing reports, executing data governance initiatives, and ensuring data is accessible, reliable, and efficiently managed. The ideal candidate has strong technical skills, experience with big data technologies, and a deep understanding of data architecture, ETL processes, and cybersecurity data frameworks.\nRoles & Responsibilities:\nDesign, develop, and maintain data solutions for data generation, collection, and processing.\nBe a key team member that assists in design and development of the data pipeline.\nCreate data pipelines and ensure data quality by implementing ETL processes to migrate and deploy data across systems.\nSchedule and manage workflows the ensure pipelines run on schedule and are monitored for failures.\nCollaborate with cross-functional teams to understand data requirements and design solutions that meet business needs.\nDevelop and maintain data models, data dictionaries, and other documentation to ensure data accuracy and consistency.\nImplement data security and privacy measures to protect sensitive data.\nLeverage cloud platforms (AWS preferred) to build scalable and efficient data solutions.\nCollaborate and communicate effectively with product teams.\nCollaborate with data scientists to develop pipelines that meet dynamic business needs.\nShare and discuss findings with team members practicing SAFe Agile delivery model.\nFunctional Skills:\nBasic Qualifications:\nMasters degree and 1 to 3 years of Computer Science, IT or related field experience OR\nBachelors degree and 3 to 5 years of Computer Science, IT or related field experience OR\nDiploma and 7 to 9 years of Computer Science, IT or related field experience\nPreferred Qualifications:\nHands on experience with data practices, technologies, and platforms, such as Databricks, Python, Gitlab, LucidChart,etc.\nProficiency in data analysis tools (e.g. SQL) and experience with data sourcing tools\nExcellent problem-solving skills and the ability to work with large, complex datasets\nUnderstanding of data governance frameworks, tools, and best practices\nKnowledge of and experience with data standards (FAIR) and protection regulations and compliance requirements (e.g., GDPR, CCPA)\nGood-to-Have Skills:\nExperience with ETL tools and various Python packages related to data processing, machine learning model development\nStrong understanding of data modeling, data warehousing, and data integration concepts\nKnowledge of Python/R, Databricks, cloud data platforms\nExperience working in Product team's environment\nExperience working in an Agile environment\nProfessional Certifications:\nAWS Certified Data Engineer preferred\nDatabricks Certificate preferred\nSoft Skills:\nInitiative to explore alternate technology and approaches to solving problems\nSkilled in breaking down problems, documenting problem statements, and estimating efforts\nExcellent analytical and troubleshooting skills\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation\nAbility to manage multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data engineering', 'data security', 'Agile', 'cloud data platforms', 'Databricks', 'data governance frameworks', 'ETL', 'AWS', 'SQL', 'Python']",2025-06-12 06:19:49
Data Engineer,Conversehr Business Solutions,4 - 7 years,15-30 Lacs P.A.,['Hyderabad'],"What are the ongoing responsibilities of Data Engineer responsible for?\nWe are building a growing Data and AI team. You will play a critical role in the efforts to centralize structured and unstructured data for the firm. We seek a candidate with skills in data modeling, data management and data governance, and can contribute first-hand towards firms data strategy. The ideal candidate is a self-starter with a strong technical foundation, a collaborative mindset, and the ability to navigate complex data challenges #ASSOCIATE\nWhat ideal qualifications, skills & experience would help someone to be successful?\nBachelors degree in computer science or computer applications; or equivalent experience in lieu of degree with 3 years of industry experience.\nStrong expertise in data modeling and data management concepts. Experience in implementing master data management is preferred.\nSound knowledge on Snowflake and data warehousing techniques.\nExperience in building, optimizing, and maintaining data pipelines and data management frameworks to support business needs.\nProficiency in at least one programming language, preferably python.\nCollaborate with cross-functional teams to translate business needs into scalable data and AI-driven solutions.\nTake ownership of projects from ideation to production, operating in a startup-like culture within an enterprise environment. Excellent communication, collaboration, and ownership mindset.\nFoundational Knowledge of API development and integration.\nKnowledge of Tableau, Alteryx is good-to-have.\nWork Shift Timings - 2:00 PM - 11:00 PM IST",Industry Type: Financial Services (Asset Management),Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Snowflake', 'Master Data Management', 'Python', 'Etl Pipelines', 'Alteryx', 'ai', 'Data Modeling', 'Tableau', 'ETL']",2025-06-12 06:19:52
Data Engineer,Meritus Management Service,4 - 9 years,9-18 Lacs P.A.,"['Pune', 'Gurugram']","The first Data Engineer specializes in traditional ETL with SAS DI and Big Data (Hadoop, Hive). The second is more versatile, skilled in modern data engineering with Python, MongoDB, and real-time processing.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Etl Pipelines', 'Big Data', 'Informatica', 'SAS DI', 'SQL', 'Hive', 'Hadoop', 'Talend', 'ETL Tool', 'Python']",2025-06-12 06:19:54
Data Engineer,Society Managers,3 - 5 years,Not Disclosed,['Mumbai (All Areas)'],"We are seeking a skilled and driven SDE-II (Data Engineering) to join our dynamic team. In this role, you will design, develop, and maintain scalable data pipelines, working with large, complex datasets. Youll collaborate closely with cross-functional teams to gather data requirements and contribute to the architecture of our data systems, leveraging your expertise in tools like Databricks, Spark, and SQL.\n\nRoles and responsibilities\nData Pipeline Development: Design,build, and maintain scalable data pipelines using Databricks, Python,and Spark.\nData Processing & Transformation: Handle large, complex datasets to ensure efficient data processing and transformations.\nCollaboration: Work with cross-functional teams to gather, understand, and implement data requirements.\nSQL & ETL: Write and optimize SQL queries for data extraction, transformation, and loading (ETL) processes.\nData Quality & Security: Ensure data accuracy, integrity, and security across all stages of the data lifecycle.\nSystem Design & Architecture: Contribute to the design and architecture of scalable data systems and solutions.\nRequired Skills and Qualification\nExperience: 3+ years of experience in data engineering or a related field.\nDatabricks & Spark: Strong expertise in Databricks and distributed data processing with Spark.\nProgramming: Proficiency in Python for data engineering tasks.\nSQL Optimization: Solid experience in writing and optimizing complex SQL queries.\nData Systems Knowledge: Hands-on experience with large-scale data systems and tools.\nDomain Knowledge: Familiarity with Capital Market/Private Equity is a plus (relaxation may apply).\nData Visualization: Experience with Tableau for creating insightful data visualizations and reports.\nPreferred skills\nCloud Platforms: Familiarity with cloud services like AWS, Azure, or GCP.\nData Warehousing & ETL:Experience with data warehousing concepts and ETL processes.\nAnalytical Skills: Strong problem-solving and analytical capabilities Analytics Tools: Hands-on experience with tools like Amplitude, PostHog, Google Analytics, or Mixpanel.\nAdditional Tools: Knowledge of Python for web scraping and frameworks like Django (good to have).",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'Microsoft Azure', 'Data Bricks', 'Spark', 'ETL', 'Python', 'SQL']",2025-06-12 06:19:57
Data Engineer,Trantor,5 - 10 years,Not Disclosed,[],"We are looking for a skilled and motivated Data Engineer with deep expertise in GCP,\nBigQuery, Apache Airflow to join our data platform team. The ideal candidate should have hands-on experience building scalable data pipelines, automating workflows, migrating large-scale datasets, and optimizing distributed systems. The candidate should have experience with building Web APIs using Python. This role will play a key part in designing and maintaining robust data engineering solutions across cloud and on-prem environments.\nKey Responsibilities\nBigQuery & Cloud Data Pipelines:\nDesign and implement scalable ETL pipelines for ingesting large-scale datasets.\nBuild solutions for efficient querying of tables in BigQuery.\nAutomated scheduled data ingestion using Google Cloud services and scheduled\nApache Airflow DAGs",,,,"['Airflow', 'Etl Pipelines', 'GCP', 'Bigquery', 'Python', 'SFTP', 'ETL', 'SQL']",2025-06-12 06:20:00
Devops AWS DATA Engineeer|| Technical Analyst || 12Lakhs CTC,Robotics Technologies,7 - 9 years,11-12 Lacs P.A.,['Hyderabad( Banjara hills )'],"We are seeking a highly skilled Devops Engineer to join our dynamic development team. In this role, you will be responsible for designing, developing, and maintaining both frontend and backend components of our applications using Devops and associated technologies.\nYou will collaborate with cross-functional teams to deliver robust, scalable, and high-performing software solutions that meet our business needs. The ideal candidate will have a strong background in devops, experience with modern frontend frameworks, and a passion for full-stack development.\n\nRequirements:\nBachelor's degree in Computer Science Engineering, or a related field.\n7 to 9+ years of experience in full-stack development, with a strong focus on DevOps.\n\nDevOps with AWS Data Engineer - Roles & Responsibilities:\nUse AWS services like EC2, VPC, S3, IAM, RDS, and Route 53.\nAutomate infrastructure using Infrastructure as Code (IaC) tools like Terraform or AWS CloudFormation.\nBuild and maintain CI/CD pipelines using tools AWS CodePipeline, Jenkins,GitLab CI/CD.\nCross-Functional Collaboration\nAutomate build, test, and deployment processes for Java applications.\nUse Ansible, Chef, or AWS Systems Manager for managing configurations across environments.\nContainerize Java apps using Docker.\nDeploy and manage containers using Amazon ECS, EKS (Kubernetes), or Fargate.\nMonitoring & Logging using Amazon CloudWatch,Prometheus + Grafana,E\nStack (Elasticsearch, Logstash, Kibana),AWS X-Ray for distributed tracing manage access with IAM roles/policies.\nUse AWS Secrets Manager / Parameter Store for managing credentials.\nEnforce security best practices, encryption, and audits.\nAutomate backups for databases and services using AWS Backup, RDS Snapshots, and S3 lifecycle rules.\nImplement Disaster Recovery (DR) strategies.\nWork closely with development teams to integrate DevOps practices.\nDocument pipelines, architecture, and troubleshooting runbooks.\nMonitor and optimize AWS resource usage.\nUse AWS Cost Explorer, Budgets, and Savings Plans.\n\nMust-Have Skills:\nExperience working on Linux-based infrastructure.\nExcellent understanding of Ruby, Python, Perl, and Java.\nConfiguration and managing databases such as MySQL, Mongo.\nExcellent troubleshooting.\nSelecting and deploying appropriate CI/CD tools\nWorking knowledge of various tools, open-source technologies, and cloud services.\nAwareness of critical concepts in DevOps and Agile principles.\nManaging stakeholders and external interfaces.\nSetting up tools and required infrastructure.\nDefining and setting development, testing, release, update, and support processes for DevOps operation.\nHave the technical skills to review, verify, and validate the software code developed in the project.\nInterview Mode : F2F for who are residing in Hyderabad / Zoom for other states\nLocation : 43/A, MLA Colony,Road no 12, Banjara Hills, 500034\nTime : 2 - 4pm",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Iac', 'Devops', 'Jenkins', 'AWS', 'Kubernetes', 'RDS', 'Aws Cloudformation', 'Amazon Cloudwatch', 'Prometheus', 'Ci/Cd', 'Grafana', 'DR', 'Cloud Trail', 'Docker', 'IAM', 'Ansible / Chef', 'fargate', 'Gitlab', 'Monitoring', 'Python']",2025-06-12 06:20:02
Data Engineer,Cloud Angles Digital Transformation,3 - 5 years,Not Disclosed,['Noida'],"Essential Functions/Responsibilities/Duties\n•       Work closely with Senior Business Intelligence engineer and BI architect to understand the schema objects and build BI reports and Dashboards\n•       Participation in sprint refinement, planning, and kick-off to understand the Agile process and Sprint priorities\n•       Develop necessary transformations and aggregate tables required for the reporting\\Dashboard needs\n•       Understand the Schema layer in MicroStrategy and business requirements\n•       Develop complex reports and Dashboards in MicroStrategy\n•       Investigate and troubleshoot issues with Dashboard and reports\n•       Proactively researching new technologies and proposing improvements to processes and tech stack\n•       Create test cases and scenarios to validate the dashboards and maintain data accuracy\nEducation and Experience\n•       3 years of experience in Business Intelligence and Data warehousing\n•       3+ years of experience in MicroStrategy Reports and Dashboard development\n•       2 years of experience in SQL\n•       Bachelors or masters degree in IT or Computer Science or ECE.\n•       Nice to have – Any MicroStrategy certifications\nRequired Knowledge, Skills, and Abilities\n•       Good in writing complex SQL, including aggregate functions, subqueries and complex date calculations and able to teach these concepts to others.\n•       Detail oriented and able to examine data and code for quality and accuracy.\n•       Self-Starter – taking initiative when inefficiencies or opportunities are seen.\n•       Good understanding of modern relational and non-relational models and differences between them\n•       Good understanding of Datawarehouse concepts, snowflake & star schema architecture and SCD concepts\n•       Good understanding of MicroStrategy Schema objects\n•       Develop Public objects such as metrics, filters, prompts, derived objects, custom groups and consolidations in MicroStrategy\n•       Develop complex reports and dashboards using OLAP and MTDI cubes\n•       Create complex dashboards with data blending\n•       Understand VLDB settings and report optimization\n•       Understand security filters and connection mappings in MSTR\nWork Environment\nAt Personify Health, we value and celebrate diversity and are committed to creating an inclusive environment for all employees. We believe in creating teams made up of individuals with various backgrounds, experiences, and perspectives. Diversity inspires innovation and collaboration and challenges us to produce better solutions. But more than this, diversity is our strength and a catalyst in our ability to change lives for the good. \nPhysical Requirements\n•       Constantly operates a computer and other office productivity machinery, such as copy machine, computer printer, calculator, etc.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Microstrategy', 'SQL', 'Dashboards']",2025-06-12 06:20:05
Data Engineer,Amgen Inc,1 - 3 years,Not Disclosed,['Hyderabad'],"What you will do\nIn this vital role you will be responsible to design, develop, and optimize data pipelines, data integration frameworks, and metadata-driven architectures that enable seamless data access and analytics. This role prefers deep expertise in big data processing, distributed computing, data modeling, and governance frameworks to support self-service analytics, AI-driven insights, and enterprise-wide data management.\nDesign, develop, and maintain complex ETL/ELT data pipelines in Databricks using PySpark, Scala, and SQL to process large-scale datasets\nUnderstand the biotech/pharma or related domains & build highly efficient data pipelines to migrate and deploy complex data across systems\nDesign and Implement solutions to enable unified data access, governance, and interoperability across hybrid cloud environments\nIngest and transform structured and unstructured data from databases (PostgreSQL, MySQL, SQL Server, MongoDB etc.), APIs, logs, event streams, images, pdf, and third-party platforms\nEnsuring data integrity, accuracy, and consistency through rigorous quality checks and monitoring\nExpert in data quality, data validation and verification frameworks\nInnovate, explore and implement new tools and technologies to enhance efficient data processing\nProactively identify and implement opportunities to automate tasks and develop reusable frameworks\nWork in an Agile and Scaled Agile (SAFe) environment, collaborating with cross-functional teams, product owners, and Scrum Masters to deliver incremental value\nUse JIRA, Confluence, and Agile DevOps tools to manage sprints, backlogs, and user stories\nSupport continuous improvement, test automation, and DevOps practices in the data engineering lifecycle\nCollaborate and communicate effectively with the product teams, with cross-functional teams to understand business requirements and translate them into technical solutions\nWhat we expect of you\nWe are all different, yet we all use our unique contributions to serve patients. We are looking for highly motivated expert Data Engineer who can own the design & development of complex data pipelines, solutions and frameworks.\nBasic Qualifications:\nMasters degree and 1 to 3 years of Computer Science, IT or related field experience OR\nBachelors degree and 3 to 5 years of Computer Science, IT or related field experience OR\nDiploma and 7 to 9 years of Computer Science, IT or related field experience\nHands-on experience in data engineering technologies such as Databricks, PySpark, SparkSQL Apache Spark, AWS, Python, SQL, and Scaled Agile methodologies\nProficiency in workflow orchestration, performance tuning on big data processing\nStrong understanding of AWS services\nAbility to quickly learn, adapt and apply new technologies\nStrong problem-solving and analytical skills\nExcellent communication and teamwork skills\nExperience with Scaled Agile Framework (SAFe), Agile delivery practices, and DevOps practices\nPreferred Qualifications:\nAWS Certified Data Engineer preferred\nDatabricks Certificate preferred\nScaled Agile SAFe certification preferred\nData Engineering experience in Biotechnology or pharma industry\nExperience in writing APIs to make the data available to the consumers\nExperienced with SQL/NOSQL database, vector database for large language models\nExperienced with data modeling and performance tuning for both OLAP and OLTP databases\nExperienced with software engineering best-practices, including but not limited to version control (Git, Subversion, etc.), CI/CD (Jenkins, Maven etc.), automated unit testing, and DevOps\nSoft Skills:\nExcellent analytical and troubleshooting skills.\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nTeam-oriented, with a focus on achieving team goals.\nAbility to learn quickly, be organized and detail oriented.\nStrong presentation and public speaking skills.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data engineering', 'Maven', 'data validation', 'PySpark', 'Scala', 'APIs', 'SQL Server', 'SQL', 'Jenkins', 'Git', 'MySQL', 'troubleshooting', 'MongoDB', 'ETL']",2025-06-12 06:20:07
MDM Associate Data Engineer,Amgen Inc,1 - 4 years,Not Disclosed,['Hyderabad'],"We are seeking an MDM Associate Data Engineerwith 25 years of experience to support and enhance our enterprise MDM (Master Data Management) platforms using Informatica/Reltio. This role is critical in delivering high-quality master data solutions across the organization, utilizing modern tools like Databricks and AWS to drive insights and ensure data reliability. The ideal candidate will have strong SQL, data profiling, and experience working with cross-functional teams in a pharma environment.To succeed in this role, the candidate must have strong data engineering experience along with MDM knowledge, hence the candidates having only MDM experience are not eligible for this role. Candidate must have data engineering experience on technologies like (SQL, Python, PySpark, Databricks, AWS etc), along with knowledge of MDM (Master Data Management)\nRoles & Responsibilities:\nAnalyze and manage customer master data using Reltio or Informatica MDM solutions.\nPerform advanced SQL queries and data analysis to validate and ensure master data integrity.\nLeverage Python, PySpark, and Databricks for scalable data processing and automation.\nCollaborate with business and data engineering teams for continuous improvement in MDM solutions.\nImplement data stewardship processes and workflows, including approval and DCR mechanisms.\nUtilize AWS cloud services for data storage and compute processes related to MDM.\nContribute to metadata and data modeling activities.\nTrack and manage data issues using tools such as JIRA and document processes in Confluence.\nApply Life Sciences/Pharma industry context to ensure data standards and compliance.\nBasic Qualifications and Experience:\nMasters degree with 1 - 3 years of experience in Business, Engineering, IT or related field OR\nBachelors degree with 2 - 5 years of experience in Business, Engineering, IT or related field OR\nDiploma with 6 - 8 years of experience in Business, Engineering, IT or related field\nFunctional Skills:\nMust-Have Skills:\nAdvanced SQL expertise and data wrangling.\nStrong experience in Python and PySpark for data transformation workflows.\nStrong experience with Databricks and AWS architecture.\nMust have knowledge of MDM, data governance, stewardship, and profiling practices.\nIn addition to above, candidates having experience with Informatica or Reltio MDM platforms will be preferred.\nGood-to-Have Skills:\nExperience with IDQ, data modeling and approval workflow/DCR.\nBackground in Life Sciences/Pharma industries.\nFamiliarity with project tools like JIRA and Confluence.\nStrong grip on data engineering concepts.\nProfessional Certifications:\nAny ETL certification (e.g. Informatica)\nAny Data Analysis certification (SQL, Python, Databricks)\nAny cloud certification (AWS or AZURE)\nSoft Skills:\nStrong analytical abilities to assess and improve master data processes and solutions.\nExcellent verbal and written communication skills, with the ability to convey complex data concepts clearly to technical and non-technical stakeholders.\nEffective problem-solving skills to address data-related issues and implement scalable solutions.\nAbility to work effectively with global, virtual teams",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['MDM', 'PySpark', 'AWS architecture', 'Jira', 'Reltio', 'SQL', 'Informatica MDM', 'data modeling', 'Confluence', 'IDQ', 'Databricks', 'data stewardship processes', 'Python']",2025-06-12 06:20:10
Data Engineer,Databeat,3 - 7 years,Not Disclosed,['Hyderabad( Rai Durg )'],"Experience Required: 3+ years\n\nTechnical knowledge: AWS, Python, SQL, S3, EC2, Glue, Athena, Lambda, DynamoDB, RedShift, Step Functions, Cloud Formation, CI/CD Pipelines, Github, EMR, RDS,AWS Lake Formation, GitLab, Jenkins and AWS CodePipeline.\n\n\n\nRole Summary: As a Senior Data Engineer,with over 3 years of expertise in Python, PySpark, SQL to design, develop and optimize complex data pipelines, support data modeling, and contribute to the architecture that supports big data processing and analytics to cutting-edge cloud solutions that drive business growth. You will lead the design and implementation of scalable, high-performance data solutions on AWS and mentor junior team members.This role demands a deep understanding of AWS services, big data tools, and complex architectures to support large-scale data processing and advanced analytics.\nKey Responsibilities:\nDesign and develop robust, scalable data pipelines using AWS services, Python, PySpark, and SQL that integrate seamlessly with the broader data and product ecosystem.\nLead the migration of legacy data warehouses and data marts to AWS cloud-based data lake and data warehouse solutions.\nOptimize data processing and storage for performance and cost.\nImplement data security and compliance best practices, in collaboration with the IT security team.\nBuild flexible and scalable systems to handle the growing demands of real-time analytics and big data processing.\nWork closely with data scientists and analysts to support their data needs and assist in building complex queries and data analysis pipelines.\nCollaborate with cross-functional teams to understand their data needs and translate them into technical requirements.\nContinuously evaluate new technologies and AWS services to enhance data capabilities and performance.\nCreate and maintain comprehensive documentation of data pipelines, architectures, and workflows.\nParticipate in code reviews and ensure that all solutions are aligned to pre-defined architectural specifications.\nPresent findings to executive leadership and recommend data-driven strategies for business growth.\nCommunicate effectively with different levels of management to gather use cases/requirements and provide designs that cater to those stakeholders.\nHandle clients in multiple industries at the same time, balancing their unique needs.\nProvide mentoring and guidance to junior data engineers and team members.\n\n\n\nRequirements:\n3+ years of experience in a data engineering role with a strong focus on AWS, Python, PySpark, Hive, and SQL.\nProven experience in designing and delivering large-scale data warehousing and data processing solutions.\nLead the design and implementation of complex, scalable data pipelines using AWS services such as S3, EC2, EMR, RDS, Redshift, Glue, Lambda, Athena, and AWS Lake Formation.\nBachelor's or Masters degree in Computer Science, Engineering, or a related technical field.\nDeep knowledge of big data technologies and ETL tools, such as Apache Spark, PySpark, Hadoop, Kafka, and Spark Streaming.\nImplement data architecture patterns, including event-driven pipelines, Lambda architectures, and data lakes.\nIncorporate modern tools like Databricks, Airflow, and Terraform for orchestration and infrastructure as code.\nImplement CI/CD using GitLab, Jenkins, and AWS CodePipeline.\nEnsure data security, governance, and compliance by leveraging tools such as IAM, KMS, and AWS CloudTrail.\nMentor junior engineers, fostering a culture of continuous learning and improvement.\nExcellent problem-solving and analytical skills, with a strategic mindset.\nStrong communication and leadership skills, with the ability to influence stakeholders at all levels.\nAbility to work independently as well as part of a team in a fast-paced environment.\nAdvanced data visualization skills and the ability to present complex data in a clear and concise manner.\nExcellent communication skills, both written and verbal, to collaborate effectively across teams and levels.\n\nPreferred Skills:\nExperience with Databricks, Snowflake, and machine learning pipelines.\nExposure to real-time data streaming technologies and architectures.\nFamiliarity with containerization and serverless computing (Docker, Kubernetes, AWS Lambda).",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'Aws Glue', 'SQL', 'Data Pipeline', 'Python', 'Amazon Ec2', 'Data Engineering', 'Data Bricks', 'Aws Lambda', 'Amazon Redshift', 'Azure Cloud', 'Data Lake', 'Data Modeling', 'Athena']",2025-06-12 06:20:13
Data Engineer IV - Big Data / Spark,Sadup Soft,5 - 7 years,Not Disclosed,['Chennai'],"Must have skills :\n\n- Minimum of 5-7 years of experience in software development, with a focus on Java and infrastructure tools.\n\n- Min 6+ years of experience as a Data Engineer.\n\n- Good Experience in handling Big Data Spark, Hive SQL, BigQuery, SQL.\n\n- Candidate worked on cloud platforms and GCP would be an added advantage.\n\n- Good understanding of Hadoop based ecosystem including hard sequel, HDFS would be very essential.\n\n- Very good professional knowledge of PySpark or using Scala\n\nResponsibilities :\n\n- Collaborate with cross-functional teams such as Data Scientists, Product Partners and Partner Team Developers to identify opportunities for Big Data, Query ( Spark, Hive SQL, BigQuery, SQL ) tuning opportunities that can be solved using machine learning and generative AI.\n\n- Write clean, high-performance, high-quality, maintainable code.\n\n- Design and develop Big Data Engineering Solutions Applications for above ensuring scalability, efficiency, and maintainability of such solutions.\n\nRequirements :\n\n- A Bachelor or Master's degree in Computer Science or a related field.\n\n- Proven experience working as a Big Data & MLOps Engineer, with a focus on Spark, Scala Spark or PySpark, Spark SQL, BigQuery, Python, Google Cloud,.\n\n- Deep understanding and experience in tuning Dataproc, BigQuery, Spark Applications.\n\n- Solid knowledge of software engineering best practices, including version control systems (e.g Git), code reviews, and testing methodologies.\n\n- Strong communication skills to effectively collaborate and present findings to both technical and non-technical stakeholders.\n\n- Proven ability to adapt and learn new technologies and frameworks quickly.\n\n- A proactive mindset with a passion for continuous learning and research.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Big Data', 'Data Engineering', 'BigQuery', 'GCP', 'Spark', 'Machine Learning', 'Python', 'SQL']",2025-06-12 06:20:15
Data Engineer- MS Fabric,InfoCepts,5 - 9 years,Not Disclosed,['India'],"Position: Data Engineer – MS Fabric\n  Purpose of the Position: As an MS Fabric Data engineer you will be responsible for designing, implementing, and managing scalable data pipelines. Strong experience in implementation and management of lake House using MS Fabric Azure Tech stack (ADLS Gen2, ADF, Azure SQL) .\nProficiency in data integration techniques, ETL processes and data pipeline architectures. Well versed in Data Quality rules, principles and implementation.\n",,,,"['components', 'data', 'scala', 'delta', 'pyspark', 'data warehousing', 'rules', 'azure data factory', 'sql', 'parquet', 'analytics', 'sql azure', 'spark', 'oracle adf', 'data pipeline architecture', 'etl', 'python', 'azure synapse', 'microsoft azure', 'power bi', 'data bricks', 'data quality', 'system', 't', 'fabric', 'data integration', 'etl process']",2025-06-12 06:20:18
Data Engineer - ETL/ Python,Meritus Management Service,5 - 7 years,10-14 Lacs P.A.,['Indore'],"Focus on Python, you'll play a crucial role in designing, developing, and maintaining data pipelines and ETL processes. Python to manage large datasets, automate data workflows, and ensure data accuracy and efficiency across our organization.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pandas', 'MySQL', 'Sqlalchemy', 'Numpy', 'Python', 'Azure Synapse', 'Postgresql', 'Etl Process', 'SQL']",2025-06-12 06:20:20
It Recruiter,IonIdea,0 - 3 years,Not Disclosed,['Bengaluru'],"Key Responsibilities:\nTalent Sourcing: Utilize various channels such as job boards, social media, LinkedIn, networking events, and internal databases to source and attract high-quality candidates for a variety of technical positions (software developers, systems engineers, data scientists, etc.).\nCandidate Screening: Review resumes, conduct initial phone screenings, and assess candidates technical skills, experience, and cultural fit.\nInterview Coordination: Schedule and facilitate interviews with hiring managers, ensuring a smooth and efficient process for all parties involved.\nCandidate Engagement: Build relationships with both active and passive candidates to maintain a strong pipeline of qualified talent. Keep candidates informed throughout the hiring process.\nOffer Management: Work with HR and hiring managers to present offers, negotiate terms, and ensure a positive candidate experience during the offer process.\n\nQualifications:\nExperience: Fresher-3years\n\nTechnical Knowledge: A solid understanding of IT roles, including knowledge of programming languages, software development frameworks, network infrastructure, cloud technologies, and emerging IT trends.\nRecruitment Tools: Proficient in using Applicant Tracking Systems (ATS), job boards (e.g., LinkedIn, Indeed), and social media platforms for sourcing candidates.\nCommunication Skills: Excellent written and verbal communication skills with the ability to engage with both technical and non-technical stakeholders.",Industry Type: IT Services & Consulting,Department: Human Resources,"Employment Type: Full Time, Permanent","['IT Recruitment', 'C2H', 'Contract Hiring']",2025-06-12 06:20:22
Data Engineer (Australian Shift 5 AM To 1 PM IST),Fascave It Solutions,4 - 9 years,12-15 Lacs P.A.,['Pune'],Position: Data Engineer\nLocation: Remote\nDuration: 10-12 Months (Contract)\nExperience: 4-10 Years\nShift Time: Australian Shift (5AM TO 1 PM IST)\n\nKey Requirements\nStrong SQL skills\nSnowflake\nAzure Data Factory\nPower BI\nSSIS (Nice to have),Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Azure Data Factory', 'Power Bi', 'Snowflake', 'SQL', 'English', 'SSIS']",2025-06-12 06:20:24
Data Engineer - R&D Data Catalyst Team,Amgen Inc,1 - 3 years,Not Disclosed,['Hyderabad'],"What you will do\nIn this vital role, you will be responsible for the end-to-end development of an enterprise analytics and data mastering solution using Databricks and Power BI. This role requires expertise in both data architecture and analytics, with the ability to create scalable, reliable, and impactful enterprise solutions that research cohort-building and advanced research pipeline. The ideal candidate will have experience creating and surfacing large unified repositories of human data, based on integrations from multiple repositories and solutions, and be extraordinarily skilled with data analysis and profiling.\nYou will collaborate closely with key customers, product team members, and related IT teams, to design and implement data models, integrate data from various sources, and ensure best practices for data governance and security. The ideal candidate will have a good background in data warehousing, ETL, Databricks, Power BI, and enterprise data mastering.\nDesign and build scalable enterprise analytics solutions using Databricks, Power BI, and other modern data tools.\nLeverage data virtualization, ETL, and semantic layers to balance need for unification, performance, and data transformation with goal to reduce data proliferation\nBreak down features into work that aligns with the architectural direction runway\nParticipate hands-on in pilots and proofs-of-concept for new patterns\nCreate robust documentation from data analysis and profiling, and proposed designs and data logic\nDevelop advanced sql queries to profile, and unify data\nDevelop data processing code in sql, along with semantic views to prepare data for reporting\nDevelop PowerBI Models and reporting packages\nDesign robust data models, and processing layers, that support both analytical processing and operational reporting needs.\nDesign and develop solutions based on best practices for data governance, security, and compliance within Databricks and Power BI environments.\nEnsure the integration of data systems with other enterprise applications, creating seamless data flows across platforms.\nDevelop and maintain Power BI solutions, ensuring data models and reports are optimized for performance and scalability.\nCollaborate with key customers to define data requirements, functional specifications, and project goals.\nContinuously evaluate and adopt new technologies and methodologies to enhance the architecture and performance of data solutions.\n\n\nWhat we expect of you\nWe are all different, yet we all use our unique contributions to serve patients. The R&D Data Catalyst Team is responsible for building Data Searching, Cohort Building, and Knowledge Management tools that provide the Amgen scientific community with visibility to Amgens wealth of human datasets, projects and study histories, and knowledge over various scientific findings. These solutions are pivotal tools in Amgens goal to accelerate the speed of discovery, and speed to market of advanced precision medications.\nBasic Qualifications:\nMasters degree and 1 to 3 years of Data Engineering experience OR\nBachelors degree and 3 to 5 years of Data Engineering experience OR\nDiploma and 7 to 9 years of Data Engineering experience\nMust Have Skills:\nMinimum of 3 years of hands-on experience with BI solutions (Preferable Power BI or Business Objects) including report development, dashboard creation, and optimization.\nMinimum of 3 years of hands-on experience building Change-data-capture (CDC) ETL pipelines, data warehouse design and build, and enterprise-level data management.\nHands-on experience with Databricks, including data engineering, optimization, and analytics workloads.\nDeep understanding of Power BI, including model design, DAX, and Power Query.\nProven experience designing and implementing data mastering solutions and data governance frameworks.\nExpertise in cloud platforms (AWS), data lakes, and data warehouses.\nStrong knowledge of ETL processes, data pipelines, and integration technologies.\nGood communication and collaboration skills to work with cross-functional teams and senior leadership.\nAbility to assess business needs and design solutions that align with organizational goals.\nExceptional hands-on capabilities with data profiling, data transformation, data mastering\nSuccess in mentoring and training team members\nGood to Have Skills:\nITIL Foundation or other relevant certifications (preferred)\nSAFe Agile Practitioner (6.0)\nMicrosoft Certified: Data Analyst Associate (Power BI) or related certification.\nDatabricks Certified Professional or similar certification.\nSoft Skills:\nExcellent analytical and troubleshooting skills\nDeep intellectual curiosity\nThe highest degree of initiative and self-motivation\nStrong verbal and written communication skills, including presentation to varied audiences of complex technical/business topics\nConfidence technical leader\nAbility to work effectively with global, remote teams, specifically including using of tools and artifacts to assure clear and efficient collaboration across time zones\nAbility to handle multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals\nStrong problem solving, analytical skills;\nAbility to learn quickly and retain and synthesize complex information from diverse sources.",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Engineering', 'data management', 'Power BI', 'data governance', 'data warehousing', 'Databricks', 'ETL', 'AWS']",2025-06-12 06:20:26
Data Analytics Mgr,Amgen Inc,4 - 6 years,Not Disclosed,['Hyderabad'],"What you will do\n\n\nIn this vital role you will report to the Organizational Planning Analytics & Insights Procurement & Sourcing Lead, you will support Amgens Tech & Workforce Strategy by applying business analytics and change leadership skills to drive insights that impact resource allocation and sourcing strategy.\n\nYour responsibilities include dashboard development, ad-hoc reporting, business partnering & engagement, and financial baselining. This role supports organizational change and enables the development of an integrated approach to global sourcing and financial planning.\n\nReporting to the Organizational Planning Analytics & Insights Procurement & Sourcing Lead, you will support Amgens Tech & Workforce Strategy by applying business analytics and change management skills to drive insights that impact resource allocation and sourcing strategy.\n\nYour responsibilities include dashboard development, ad-hoc reporting, business partnering & engagement, and financial baselining. This role supports change management and enables the development of an integrated approach to global sourcing and financial planning.\n\nRoles & Responsibilities:\nAddressing business challenges through process evaluation and insight generation.\nDevelop insights with a strong focus on Tableau and Power BI dashboard creation, as well as PowerPoint presentations.\nGuide data analysts and data engineers on standard methodologies for building data pipelines to support dashboards and other business objectives.\nConduct ad hoc analyses of FP&A and sourcing/procurement data.\nAddressing business challenges through process evaluation and insight generation.\nDevelop insights with a strong focus on Tableau and Power BI dashboard creation, as well as PowerPoint presentations.\nGuide data analysts and data engineers on standard methodologies for building data pipelines to support dashboards and other business objectives.\nConduct ad hoc analyses of FP&A and sourcing/procurement data.\nWhat we expect of you\n\nWe are all different, yet we all use our unique contributions to serve patients.\n\nBasic Qualifications:\nMasters degree and 4 to 6 years of applicable experience in business analysis (finance analysis, data analysis, sourcing analysis, or similar experience OR\nBachelors degree and 6 to 8 years of applicable experience in business analysis (finance analysis, data analysis, sourcing analysis, or similar experience OR\nDiploma and 10 to 12 years of applicable experience in business analysis (finance analysis, data analysis, sourcing analysis, or similar experience\nPreferred Qualifications:\nMasters degree in data science, business, statistics, data mining, applied mathematics, business analytics, engineering, computer science, or a related field\n4 years of relevant experience in data science, data analytics, consulting, and/or financial planning & analysis.\nA keen eye for design, with the ability to craft engaging PowerPoint decks and develop compelling Power BI and Tableau dashboards.\nProven expertise in statistical/mathematical modeling and working with structured/unstructured data.\nExperience with procurement, sourcing, and/or financial planning data.\nSkilled in automating data workflows using tools like Tableau, Python, R, Alteryx, and PowerApps.\nKnowledge of global finance systems, Procurement, and sourcing operations.\nExperience with data analysis, budgeting, forecasting, and strategic planning in the Bio-Pharmaceutical or biotech industry.\nGrowing in a start-up environment, building a data-driven transformation capability.\nUnderstanding of the Bio-Pharmaceutical and biotech industry trends and operations.\nProven ability to engage with cross-functional business leaders to align data strategies with corporate objectives, redefining complex data insights into actionable strategies.\nFlexible work models, including remote work arrangements, where possible\n\nAs we work to develop treatments that deal with others, we also work to care for your professional and personal growth and well-being. From our competitive benefits to our collaborative culture, well support your journey every step of the way.",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Analysis', 'data analytics', 'data science', 'mathematical modeling', 'financial planning', 'financial planning and analysis', 'statistics']",2025-06-12 06:20:29
Assoc. Data Engineer - R&D Precision Medicine Team,Amgen Inc,1 - 3 years,Not Disclosed,['Hyderabad'],"What you will do\n\n\nThe R&D Precision Medicine team is responsible for Data Standardization, Data Searching, Cohort Building, and Knowledge Management tools that provide the Amgen scientific community with access to Amgens wealth of human datasets, projects and study histories, and knowledge over various scientific findings. These data include clinical data, omics, and images. These solutions are pivotal tools in Amgens goal to accelerate the speed of discovery, and speed to market of advanced precision medications.\n\nThe Data Engineer will be responsible for full stack development of enterprise analytics and data mastering solutions leveraging Databricks and Power BI. This role requires expertise in both data architecture and analytics, with the ability to create scalable, reliable, and high-performing enterprise solutions that support research cohort-building and advanced AI pipelines. The ideal candidate will have experience creating and surfacing large unified repositories of human data, based on integrations from multiple repositories and solutions, and be exceptionally skilled with data analysis and profiling.\n\nYou will collaborate closely with partners, product team members, and related IT teams, to design and implement data models, integrate data from various sources, and ensure best practices for data governance and security. The ideal candidate will have a solid background in data warehousing, ETL, Databricks, Power BI, and enterprise data mastering.\n\nRoles & Responsibilities\nDesign and build scalable enterprise analytics solutions using Databricks, Power BI, and other modern data management tools.\nLeverage data virtualization, ETL, and semantic layers to balance need for unification, performance, and data transformation with goal to reduce data proliferation\nBreak down features into work that aligns with the architectural direction runway\nParticipate hands-on in pilots and proofs-of-concept for new patterns\nCreate robust documentation from data analysis and profiling, and proposed designs and data logic\nDevelop advanced sql queries to profile, and unify data\nDevelop data processing code in sql, along with semantic views to prepare data for reporting\nDevelop PowerBI Models and reporting packages\nDesign robust data models, and processing layers, that support both analytical processing and operational reporting needs.\nDesign and develop solutions based on best practices for data governance, security, and compliance within Databricks and Power BI environments.\nEnsure the integration of data systems with other enterprise applications, creating seamless data flows across platforms.\nDevelop and maintain Power BI solutions, ensuring data models and reports are optimized for performance and scalability.\nCollaborate with partners to define data requirements, functional specifications, and project goals.\nContinuously evaluate and adopt new technologies and methodologies to enhance the architecture and performance of data solutions.\n\n\n\nWhat we expect of you\nWe are all different, yet we all use our unique contributions to serve patients. The professional we seek is someone with these qualifications.\n\nBasic Qualifications:\nMasters degree with 1 to 3 years of experience in Data Engineering OR\nBachelors degree with 1 to 3 years of experience in Data Engineering\nMust-Have\n\nSkills:\nMinimum of 1 year of hands-on experience with BI solutions (Preferrable Power BI or Business Objects) including report development, dashboard creation, and optimization.\nMinimum of 1 year of hands-on experience building Change-data-capture (CDC) ETL pipelines, data warehouse design and build, and enterprise-level data management.\nHands-on experience with Databricks, including data engineering, optimization, and analytics workloads.\nExperience using cloud platforms (AWS), data lakes, and data warehouses.\nWorking knowledge of ETL processes, data pipelines, and integration technologies.\nGood communication and collaboration skills to work with cross-functional teams and senior leadership.\nAbility to assess business needs and design solutions that align with organizational goals.\nExceptional hands-on capabilities with data profiling and data anlysis\nGood-to-Have\n\nSkills:\nExperience with human data, ideally human healthcare data\nFamiliarity with laboratory testing, patient data from clinical care, HL7, FHIR, and/or clinical trial data management\nProfessional Certifications:\nITIL Foundation or other relevant certifications (preferred)\nSAFe Agile Practitioner (6.0)\nMicrosoft CertifiedData Analyst Associate (Power BI) or related certification.\nDatabricks Certified Professional or similar certification.\nSoft\n\nSkills:\nExcellent analytical and troubleshooting skills\nDeep intellectual curiosity\nHighest degree of initiative and self-motivation\nStrong verbal and written communication skills, including presentation to varied audiences of complex technical/business topics\nConfidence technical leader\nAbility to work effectively with global, virtual teams, specifically including using of tools and artifacts to assure clear and efficient collaboration across time zones\nAbility to manage multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals\nStrong problem solving, analytical skills;\nAbility to learn quickly and retain and synthesize complex information from diverse sources",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Engineering', 'data lakes', 'data pipelines', 'ETL processes', 'AWS', 'data warehouses', 'BI solutions']",2025-06-12 06:20:31
Immediate Opening For Data Science,Happiest Minds Technologies,8 - 13 years,Not Disclosed,['Bengaluru( Madiwala )'],"Machine Learning, Deep Learning models, Data Science. (Important);-R / python programming (mandatory) ;- Fast API development ;- deployment of models experience ; - any cloud Azure (good to have - for this requirement); - basics of Generative AI , NLP (optional - Good to have)\n\nGIS data, Geospatial data, Google Maps, ArcGIS, Demand pattern analysis\n\n5 to 15 Yrs",,,,"['Data Science', 'Machine Learning', 'Deep Learning', 'Python', 'GenAi', 'Natural Language Processing']",2025-06-12 06:20:34
Architect (Data Engineering),Amgen Inc,9 - 12 years,Not Disclosed,['Hyderabad'],"ABOUT THE ROLE\n\nRole Description:\n\nWe are seeking a Data Solutions Architect with deep expertise in Biotech/Pharma to design, implement, and optimize scalable and high-performance data solutions that support enterprise analytics, AI-driven insights, and digital transformation initiatives. This role will focus on data strategy, architecture, governance, security, and operational efficiency, ensuring seamless data integration across modern cloud platforms. The ideal candidate will work closely with engineering teams, business stakeholders, and leadership to establish a future-ready data ecosystem, balancing performance, cost-efficiency, security, and usability. This position requires expertise in modern cloud-based data architectures, data engineering best practices, and Scaled Agile methodologies.\n\nRoles & Responsibilities:\nDesign and implement scalable, modular, and future-proof data architectures that initiatives in enterprise.\nDevelop enterprise-wide data frameworks that enable governed, secure, and accessible data across various business domains.\nDefine data modeling strategies to support structured and unstructured data, ensuring efficiency, consistency, and usability across analytical platforms.\nLead the development of high-performance data pipelines for batch and real-time data processing, integrating APIs, streaming sources, transactional systems, and external data platforms.\nOptimize query performance, indexing, caching, and storage strategies to enhance scalability, cost efficiency, and analytical capabilities.\nEstablish data interoperability frameworks that enable seamless integration across multiple data sources and platforms.\nDrive data governance strategies, ensuring security, compliance, access controls, and lineage tracking are embedded into enterprise data solutions.\nImplement DataOps best practices, including CI/CD for data pipelines, automated monitoring, and proactive issue resolution, to improve operational efficiency.\nLead Scaled Agile (SAFe) practices, facilitating Program Increment (PI) Planning, Sprint Planning, and Agile ceremonies, ensuring iterative delivery of enterprise data capabilities.\nCollaborate with business stakeholders, product teams, and technology leaders to align data architecture strategies with organizational goals.\nAct as a trusted advisor on emerging data technologies and trends, ensuring that the enterprise adopts cutting-edge data solutions that provide competitive advantage and long-term scalability.\nMust-Have\n\nSkills:\nExperience in data architecture, enterprise data management, and cloud-based analytics solutions.\nWell versed in domain of Biotech/Pharma industry and has been instrumental in solving complex problems for them using data strategy.\nExpertise in Databricks, cloud-native data platforms, and distributed computing frameworks.\nStrong proficiency in modern data modeling techniques, including dimensional modeling, NoSQL, and data virtualization.\nExperience designing high-performance ETL/ELT pipelines and real-time data processing solutions.\nDeep understanding of data governance, security, metadata management, and access control frameworks.\nHands-on experience with CI/CD for data solutions, DataOps automation, and infrastructure as code (IaC).\nProven ability to collaborate with cross-functional teams, including business executives, data engineers, and analytics teams, to drive successful data initiatives.\nStrong problem-solving, strategic thinking, and technical leadership skills.\nExperienced with SQL/NOSQL database, vector database for large language models\nExperienced with data modeling and performance tuning for both OLAP and OLTP databases\nExperienced with Apache Spark, Apache Airflow\nExperienced with software engineering best-practices, including but not limited to version control (Git, Subversion, etc.), CI/CD (Jenkins, Maven etc.), automated unit testing, and Dev Ops\nGood-to-Have\n\nSkills:\nExperience with Data Mesh architectures and federated data governance models.\nCertification in cloud data platforms or enterprise architecture frameworks.\nKnowledge of AI/ML pipeline integration within enterprise data architectures.\nFamiliarity with BI & analytics platforms for enabling self-service analytics and enterprise reporting.\nEducation and Professional Certifications\n9 to 12 years of experience in Computer Science, IT or related field\nAWS Certified Data Engineer preferred\nDatabricks Certificate preferred\nSoft\n\nSkills:\nExcellent analytical and troubleshooting skills.\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nTeam-oriented, with a focus on achieving team goals.\nAbility to learn quickly, be organized and detail oriented.\nStrong presentation and public speaking skills.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'continuous integration', 'technical leadership', 'metadata management', 'presentation skills', 'ci/cd', 'distributed computing', 'sql', 'data bricks', 'git', 'data modeling', 'spark', 'devops', 'data governance', 'jenkins', 'troubleshooting', 'access control', 'etl']",2025-06-12 06:20:36
Data Solution Architect,Maveric,13 - 20 years,Not Disclosed,"['Chennai', 'Bengaluru']","Position Overview\nWe are looking for a highly experienced and versatile Solution Architect Data to lead the solution design and delivery of next-generation data solutions for our BFS clients. The ideal candidate will have a strong background in data architecture and engineering, deep domain expertise in financial services, and hands-on experience with cloud-native data platforms and modern data analytics tools. The role will require architecting solutions across Retail, Corporate, Wealth, and Capital Markets, as well as Payments, Lending, and Onboarding journeys. Possession of Data Analytics and Exposure to Data regulatory domain will be of distinct advantage. Hands on experience of AI & Gen AI enabling data related solution will be a distinct advantage for the position.",,,,"['Data Quality', 'Data Engineering', 'Data Governance', 'GenAI']",2025-06-12 06:20:39
Manager Data Engineer – Research Data and Analytics,Amgen Inc,4 - 6 years,Not Disclosed,['Hyderabad'],"What you will do\nIn this vital role you will create and develop data lake solutions for scientific data that drive business decisions for Research. You will build scalable and high-performance data engineering solutions for large scientific datasets and collaborate with Research collaborators. You will also provide technical leadership to junior team members. The ideal candidate possesses experience in the pharmaceutical or biotech industry, demonstrates deep technical skills, is proficient with big data technologies, and has a deep understanding of data architecture and ETL processes.\nRoles & Responsibilities:\nLead, manage, and mentor a high-performing team of data engineers\nDesign, develop, and implement data pipelines, ETL processes, and data integration solutions\nTake ownership of data pipeline projects from inception to deployment, manage scope, timelines, and risks\nDevelop and maintain data models for biopharma scientific data, data dictionaries, and other documentation to ensure data accuracy and consistency\nOptimize large datasets for query performance\nCollaborate with global multi-functional teams including research scientists to understand data requirements and design solutions that meet business needs\nImplement data security and privacy measures to protect sensitive data\nLeverage cloud platforms (AWS preferred) to build scalable and efficient data solutions\nCollaborate with Data Architects, Business SMEs, Software Engineers and Data Scientists to design and develop end-to-end data pipelines to meet fast paced business needs across geographic regions\nIdentify and resolve data-related challenges\nAdhere to best practices for coding, testing, and designing reusable code/component\nExplore new tools and technologies that will help to improve ETL platform performance\nParticipate in sprint planning meetings and provide estimations on technical implementation\n\n\nWhat we expect of you\nWe are all different, yet we all use our unique contributions to serve patients. The [vital attribute] professional we seek is a [type of person] with these qualifications.\nBasic Qualifications:\nDoctorate Degree OR\nMasters degree with 4 - 6 years of experience in Computer Science, IT, Computational Chemistry, Computational Biology/Bioinformatics or related field OR\nBachelors degree with 6 - 8 years of experience in Computer Science, IT, Computational Chemistry, Computational Biology/Bioinformatics or related field OR\nDiploma with 10 - 12 years of experience in Computer Science, IT, Computational Chemistry, Computational Biology/Bioinformatics or related field\nPreferred Qualifications:\n3+ years of experience in implementing and supporting biopharma scientific research data analytics (software platforms)\n\n\nFunctional Skills:\nMust-Have Skills:\nProficiency in SQL and Python for data engineering, test automation frameworks (pytest), and scripting tasks\nHands on experience with big data technologies and platforms, such as Databricks, Apache Spark (PySpark, SparkSQL), workflow orchestration, performance tuning on big data processing\nExcellent problem-solving skills and the ability to work with large, complex datasets\nAble to engage with business collaborators and mentor team to develop data pipelines and data models\n\n\nGood-to-Have Skills:\nA passion for tackling complex challenges in drug discovery with technology and data\nGood understanding of data modeling, data warehousing, and data integration concepts\nGood experience using RDBMS (e.g. Oracle, MySQL, SQL server, PostgreSQL)\nKnowledge of cloud data platforms (AWS preferred)\nExperience with data visualization tools (e.g. Dash, Plotly, Spotfire)\nExperience with diagramming and collaboration tools such as Miro, Lucidchart or similar tools for process mapping and brainstorming\nExperience writing and maintaining technical documentation in Confluence\nUnderstanding of data governance frameworks, tools, and best practices\n\n\nProfessional Certifications:\nDatabricks Certified Data Engineer Professional preferred\n\n\nSoft Skills:\nExcellent critical-thinking and problem-solving skills\nGood communication and collaboration skills\nDemonstrated awareness of how to function in a team setting\nDemonstrated presentation skills",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'Spotfire', 'PySpark', 'PostgreSQL', 'Plotly', 'SparkSQL', 'SQL server', 'SQL', 'process mapping', 'Dash', 'MySQL', 'ETL', 'Oracle', 'data governance frameworks', 'Python']",2025-06-12 06:20:41
Data Engineering Manager,Amgen Inc,8 - 12 years,Not Disclosed,['Hyderabad'],"ABOUT THE ROLE\n\n\nRole Description:\n\nWe are seeking a seasoned Engineering Manager (Data Engineering) to lead the end-to-end management of enterprise data assets and operational data workflows. This role is critical in ensuring the availability, quality, consistency, and timeliness of data across platforms and functions, supporting analytics, reporting, compliance, and digital transformation initiatives. You will be responsible for the day-to-day data operations, manage a team of data professionals, and drive process excellence in data intake, transformation, validation, and delivery. You will work closely with cross-functional teams including data engineering, analytics, IT, governance, and business stakeholders to align operational data capabilities with enterprise needs.\n\nRoles & Responsibilities:\nLead and manage the enterprise data operations team, responsible for data ingestion, processing, validation, quality control, and publishing to various downstream systems.\nDefine and implement standard operating procedures for data lifecycle management, ensuring accuracy, completeness, and integrity of critical data assets.\nOversee and continuously improve daily operational workflows, including scheduling, monitoring, and troubleshooting data jobs across cloud and on-premise environments.\nEstablish and track key data operations metrics (SLAs, throughput, latency, data quality, incident resolution) and drive continuous improvements.\nPartner with data engineering and platform teams to optimize pipelines, support new data integrations, and ensure scalability and resilience of operational data flows.\nCollaborate with data governance, compliance, and security teams to maintain regulatory compliance, data privacy, and access controls.\nServe as the primary escalation point for data incidents and outages, ensuring rapid response and root cause analysis.\nBuild strong relationships with business and analytics teams to understand data consumption patterns, prioritize operational needs, and align with business objectives.\nDrive adoption of best practices for documentation, metadata, lineage, and change management across data operations processes.\nMentor and develop a high-performing team of data operations analysts and leads.\nFunctional\n\nSkills:\nMust-Have Skills:\nExperience managing a team of data engineers in biotech/pharma domain companies.\nExperience in designing and maintaining data pipelines and analytics solutions that extract, transform, and load data from multiple source systems.\nDemonstrated hands-on experience with cloud platforms (AWS) and the ability to architect cost-effective and scalable data solutions.\nExperience managing data workflows in cloud environments such as AWS, Azure, or GCP.\nStrong problem-solving skills with the ability to analyze complex data flow issues and implement sustainable solutions.\nWorking knowledge of SQL, Python, or scripting languages for process monitoring and automation.\nExperience collaborating with data engineering, analytics, IT operations, and business teams in a matrixed organization.\nFamiliarity with data governance, metadata management, access control, and regulatory requirements (e.g., GDPR, HIPAA, SOX).\nExcellent leadership, communication, and stakeholder engagement skills.\nWell versed with full stack development & DataOps automation, logging frameworks, and pipeline orchestration tools.\nStrong analytical and problem-solving skills to address complex data challenges.\nEffective communication and interpersonal skills to collaborate with cross-functional teams.\nGood-to-Have\n\nSkills:\nData Engineering Management experience in Biotech/Life Sciences/Pharma\nExperience using graph databases such as Stardog or Marklogic or Neo4J or Allegrograph, etc.\nEducation and Professional Certifications\nDoctorate Degree with 3-5 + years of experience in Computer Science, IT or related field\nOR\nMasters degree with 6 - 8 + years of experience in Computer Science, IT or related field\nOR\nBachelors degree with 10 - 12 + years of experience in Computer Science, IT or related field\nAWS Certified Data Engineer preferred\nDatabricks Certificate preferred\nScaled Agile SAFe certification preferred\nSoft\n\nSkills:\nExcellent analytical and troubleshooting skills\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation\nAbility to manage multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals\nStrong presentation and public speaking skills",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Data Engineering', 'fullstack development', 'logging framework', 'stakeholder engagement', 'troubleshooting', 'cloud platforms']",2025-06-12 06:20:43
Azure Data Architect,Syren Technologies,10 - 18 years,Not Disclosed,[],"About Syren Cloud\n\nSyren Cloud Technologies is a cutting-edge company specializing in supply chain solutions and data engineering. Their intelligent insights, powered by technologies like AI and NLP, empower organizations with real-time visibility and proactive decision-making. From control towers to agile inventory management, Syren unlocks unparalleled success in supply chain management.\n\nRole Summary",,,,"['Pyspark', 'Azure', 'Architecture', 'Data Bricks']",2025-06-12 06:20:46
Data Architect - AWS,Happiest Minds Technologies,10 - 15 years,Not Disclosed,"['Noida', 'Pune', 'Bengaluru']","Roles and responsibilities\nWork closely with the Product Owners and stake holders to design the Technical Architecture for data platform to meet the requirements of the proposed solution.\nWork with the leadership to set the standards for software engineering practices within the machine learning engineering team and support across other disciplines\nPlay an active role in leading team meetings and workshops with clients.\nChoose and use the right analytical libraries, programming languages, and frameworks for each task.",,,,"['SQL', 'data architect', 'Python', 'Pyspark', 'Apache Airflow', 'GLUE', 'Kinesis', 'Amazon Redshift', 'Data Architecture Principles', 'Data Modeling', 'Data Warehousing', 'Athena', 'Lambda', 'AWS']",2025-06-12 06:20:48
Data Architecture,Top B2B MNC in Management Consulting Dom...,5 - 8 years,Not Disclosed,['Bengaluru'],"About the Company\nGreetings from Teamware Solutions a division of Quantum Leap Consulting Pvt. Ltd\n\nAbout the Role\nWe are hiring a Data Architecture\n\nLocation: Bangalore\nWork Model: Hybrid\nExperience: 5-9 Years\nNotice Period: Immediate to 15 Days\n\nJob Description:\nData Architecture, Data Governance, Data Modeling\n\nAdditional Information:\nMandatory Skills: Data Architecture, Data Governance, Data Modeling\nNice to have skills Certification in Data Engineering\nInterview Mode Virtual Interview\nminimum 5 yrs relevant experience and maximum 9 yrs for this requirement. Someone with more experience in building PySpark data streaming jobs on Azure Databricks\nwho have done real projects, have expertise, and hands-on experience also\nAlso, Data governance and data modeling experience with a minimum of 4 years is mandatory\nCommunication should be excellent\n\n\nPlease let me know if you are interested in this position and send me your resumes to netra.s@twsol.com",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Data Architecture', 'Data Modeling', 'Data Governance', 'Data Engineering']",2025-06-12 06:20:50
Snowflake Data Engineer,Epam Systems,5 - 10 years,Not Disclosed,['Chennai'],"Key Skills:\nSnowflake (Snow SQL, Snow PLSQL and Snowpark)\nStrong Python\nAirflow/DBT\nAny DevOps tools\nAWS/Azure Cloud Skills\n\nRequirements:\nLooking for engineer for information warehouse\nWarehouse is based on AWS/Azure, DBT, Snowflake.\nStrong programming experience with Python.\nExperience with workflow management tools like Argo/Oozie/Airflow.\nExperience in Snowflake modelling - roles, schema, databases\nExperience in data Modeling (Data Vault).\nExperience in design and development of data transformation pipelines using the DBT framework.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Snowflake', 'Python', 'Azure Cloud', 'AWS', 'SQL']",2025-06-12 06:20:52
Snowflake Developer with Azure Data Factory,Net Connect,6 - 10 years,6-11 Lacs P.A.,['Hyderabad'],Greetings from NCG!\n\nWe have a opening for Snowflake Developer role in Hyderabad office!\n\nBelow JD for your reference\n\nJob Description:,,,,"['Azure Data Factory', 'Snowflake', 'SQL']",2025-06-12 06:20:55
Job opening For Data Warehouse + ADF + ETL,bct,3 - 6 years,Not Disclosed,['Pune'],"Greetings of the Day !!!\n\nWe have job opening for Data Warehouse + ADF + ETL with one of our Client .If you are interested for this role , kindly share update resume along with below details in this email id : shaswati.m@bct-consulting.com\n\nJob Description:\nSenior Data Engineer\nAs a Senior Data Engineer, you will support the European World Area using the Windows & Azure suite of Analytics & Data platforms. The focus of the role is on the technical aspects and implementation of data gathering, integration and database design.\nWe look forward to seeing your application!\nIn This Role, Your Responsibilities Will Be:\nData Ingestion and Integration: Collaborate with Product Owners and analysts to understand data requirements & design, develop, and maintain data pipelines for ingesting, transforming, and integrating data from various sources into Azure Data Services.\nMigration of existing ETL packages: Migrate existing SSIS packages to Synapse pipelines\nData Modelling: Assist in designing and implementing data models, data warehouses, and databases in Azure Synapse Analytics, Azure Data Lake Storage, and other Azure services.\nData Transformation: Develop ETL (Extract, Transform, Load) processes using SQL Server Integration Services (SSIS), Azure Synapse Pipelines, or other relevant tools to prepare data for analysis and reporting.\nData Quality and Governance: Implement data quality checks and data governance practices to ensure the accuracy, consistency, and security of data assets.\nMonitoring and Optimization: Monitor and optimize data pipelines and workflows for performance, scalability, and cost efficiency.\nDocumentation: Maintain comprehensive documentation of processes, including data lineage, data dictionaries, and pipeline schedules.\nCollaboration: Work closely with cross-functional teams, including data analysts, data scientists, and business stakeholders, to understand their data needs and deliver solutions accordingly.\nAzure Services: Stay updated on Azure data services and best practices to recommend and implement improvements in our data architecture and processes\nFor This Role, You Will Need:\n3-5 years of experience in Data Warehousing with On-Premises or Cloud technologies\nStrong practical experience of Synapse pipelines / ADF.\nStrong practical experience of developing ETL packages using SSIS.\nStrong practical experience with T-SQL or any variant from other RDBMS.\nGraduate degree educated in computer science or a relevant subject.\nStrong analytical and problem-solving skills.\nStrong communication skills in dealing with internal customers from a range of functional areas.\nWillingness to work flexible working hours according to project requirements.\nTechnical documentation skills.\nFluent in English.\nPreferred Qualifications that Set You Apart:\nOracle PL/SQL.\nExperience in working on Azure Services like Azure Synapse Analytics, Azure Data Lake.\nWorking experience with Azure DevOps paired with knowledge of Agile and/or Scrum methods of delivery.\nLanguages: French, Italian, or Spanish would be an advantage.\nAgile certification.\nThanks,\nShaswati",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['ADF', 'ETL', 'SSIS', 'Data ware house']",2025-06-12 06:20:57
Azure Data Bricks (4-15 Yrs) - Bangalore,Happiest Minds Technologies,4 - 9 years,Not Disclosed,['Bengaluru'],"Hi,\n\nGreetings from Happiest Minds Technologies\n\nCurrently we are hiring for below positions and looking for immediate joiners.\n1. Azure Databricks Bangalore 5 to 10 Yrs - Bangalore\nAs a Senior Azure Data Engineer, you will leverage Azure technologies to drive data transformation, analytics, and machine learning. You will design scalable Databricks data pipelines using PySpark, transforming raw data into actionable insights. Your role includes building, deploying, and maintaining machine learning models using MLlib or TensorFlow while optimizing cloud data integration from Azure Blob Storage, Data Lake, and SQL/NoSQL sources. You will execute large-scale data processing using Spark Pools, fine-tuning configurations for efficiency. The ideal candidate holds a Bachelors or Masters in Computer Science, Data Science, or a related field, with 7+ years in data engineering and 3+ years specializing in Azure Databricks, PySpark, and Spark Pools. Proficiency in Python PySpark, Pandas, NumPy, SciPy, Spark SQL, DataFrames, RDDs, Delta Lake, Databricks Notebooks, and MLflow is required, along with hands-on experience in Azure Data Lake, Blob Storage, and Synapse Analytics.",,,,"['Pyspark', 'Azure', 'Data Bricks', 'sql', 'ETL']",2025-06-12 06:20:59
Azure Data Engineer (Azure Databricks must),Fortune India 500 IT Services Firm,5 - 8 years,Not Disclosed,['Hyderabad'],"We are looking for an experienced Azure Data Engineer with strong expertise in Azure Databricks to join our data engineering team.\n\nMandatory skill- Azure Databricks\nExperience- 5 to 8 years\nLocation- Hyderabad\nKey Responsibilities:\nDesign and build data pipelines and ETL/ELT workflows using Azure Databricks and Azure Data Factory\nIngest, clean, transform, and process large datasets from diverse sources (structured and unstructured)\nImplement Delta Lake solutions and optimize Spark jobs for performance and reliability\nIntegrate Azure Databricks with other Azure services including Data Lake Storage, Synapse Analytics, and Event Hubs\n\n\n\nInterested candidates share your CV at himani.girnar@alikethoughts.com with below details\n\nCandidate's name-\nEmail and Alternate Email ID-\nContact and Alternate Contact no-\nTotal exp-\nRelevant experience-\nCurrent Org-\nNotice period-\nCCTC-\nECTC-\nCurrent Location-\nPreferred Location-\nPancard No-",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Azure Databricks', 'Azure Data Factory', 'Pyspark', 'Azure Data Lake', 'SQL']",2025-06-12 06:21:01
Master Data Management Architect,K-logix Partnering Solutions,8 - 13 years,Not Disclosed,[],"Bachelor'sMaster's\nOverview:\n\nWe are seeking a highly skilled and experienced Celonis MDM Data Architect to lead the design, implementation, and optimization of our Master Data Management (MDM) solutions in alignment with Celonis Process Mining and Execution Management System (EMS) capabilities.\nThe ideal candidate will play a key role in bridging data architecture and business process insights, ensuring data quality, consistency, and governance across the enterprise.\n\nKey Responsibilities:\nDesign and implement MDM architecture and data models aligned with enterprise standards and best practices.\n• Lead the integration of Celonis with MDM platforms to drive intelligent process automation, data governance, and operational efficiencies.\n• Collaborate with business stakeholders and data stewards to define MDM policies, rules, and processes.\n• Support data profiling, data cleansing, and data harmonization efforts to improve master data quality.\n• Work closely with Celonis analysts, data engineers, and process owners to deliver actionable insights based on MDM-aligned process data.\n• Develop and maintain scalable, secure, and high-performance data pipelines and integration architectures.\n• Translate business requirements into technical solutions, ensuring alignment with both MDM and Celonis data models.\n• Create and maintain data architecture documentation, data dictionaries, and metadata repositories.\n• Monitor and optimize the performance of MDM systems and Celonis EMS integrations.\nQualifications:\nBachelors or Masters degree in Computer Science, Information Systems, Data Engineering, or a related field.\n• 7+ years of experience in data architecture, MDM, or enterprise data management.\n• 2+ years of hands-on experience with Celonis and process mining tools.\n• Proficient in MDM platforms (e.g., Informatica MDM, SAP MDG, Oracle MDM, etc.).\n• Strong knowledge of data modeling, data governance, and metadata management.\n• Proficiency in SQL, data integration tools (e.g., ETL/ELT platforms), and APIs.\n• Deep understanding of business process management and data-driven transformation initiatives.",Industry Type: IT Services & Consulting,Department: Consulting,"Employment Type: Full Time, Permanent","['Celonis', 'MDM', 'Master Data Management', 'ETL', 'Elt']",2025-06-12 06:21:03
Business Analyst,Amdocs,2 - 4 years,Not Disclosed,['Pune'],"Amdocs helps those who build the future to make it amazing. With our market-leading portfolio of software products and services, we unlock our customers innovative potential, empowering them to provide next-generation communication and media experiences for both the individual end user and enterprise customers. Our employees around the globe are here to accelerate service providers migration to the cloud, enable them to differentiate in the 5G era, and digitalize and automate their operations. Listed on the NASDAQ Global Select Market, Amdocs had revenue of $5.00 billion in fiscal 2024. For more information, visit www.amdocs.com\n\n\nIn one sentence\n\nUnderstand the business needs of the customer and assess the impact of those needs in order to communicate and implement the recommended efficient solutions.\n\n\nWhat will your job look like\n""Lead domain-specific solutioning activities across solution and delivery engagements.\nAct as a trusted advisor to customers, providing deep expertise in your domain (e.g., Charging & Billing, CRM, Ordering, Catalog, Network Provisioning).\nDefine end-to-end domain solution and ensure alignment with customer business goals and operational strategies.\nCollaborate with Solution Architects, Business Analysts, and Product Managers for requirement feasibility and solution scope.\nProvide functional and technical support during design, integration, migration, and testing phases.\nIdentify domain risks, dependencies, and business impacts; recommend best practices and innovative approaches.""\n\n\nAll you need is...\nDegree in Computer Science or Industrial Engineering & Management - Information System.\nCustomer-facing experience - ability to communicate the Amdocs solution using various methods (presentations, demos, and so on).\nWide knowledge of relevant products and E2E Business process.\nKnowledge of the telecom industry and Amdocs business processes (ETOM, ASOM).\nExperience in managing a team in cross-Amdocs domain solutions.\n\n\nWhy you will love this job:\nUse your outstanding business analysis skills to make a significant impact on leading solutions that produce the most efficient product solutions.\nBe a key member of a global, dynamic and highly collaborative team with various possibilities for personal and professional development!\nYou will have the opportunity to work in multinational environment for the global market leader in its field.\nWe offer a wide range of stellar benefits including health, dental, vision, and life insurance as well as paid time off, sick time, and parental leave!\n\n",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['business analysis', 'demo', 'end user', 'operations strategy', 'acquisition', 'project management', 'program management', 'presentation skills', 'business analytics', 'business development', 'business planning', 'corporate planning', 'business consulting', 'telecom', 'crm']",2025-06-12 06:21:05
Business Analyst,Barbeque Nation,6 - 11 years,8-12 Lacs P.A.,['Bengaluru( Sarjapur Road )'],"Corporate Finance and Investor relations role:\n\nKey Responsibilities:\n\nFinancial Analysis and Strategy:\nProvide strategic financial analysis to senior management to support decision-making, with a focus on profitability, growth, and market expansion.\nFinancial Modeling: Develop and maintain financial models to support business planning and decision-making\nReporting & Dashboards: Prepare monthly/quarterly financial reports, dashboards, and presentations for senior leadership\nCost Management: Analyze costs, profitability, and business performance to optimize financial outcomes\nScenario Analysis: Evaluate various business scenarios and their financial impact, providing data-driven recommendations\nWork closely with the finance team to track key performance indicators (KPIs), develop financial models, and assist in budget forecasting and financial planning\nAnalyze market trends and competitive positioning to inform strategic decisions and communicate findings to both internal stakeholders and investors\nInvestor Relations:\nLead the creation of investor presentations, quarterly earnings releases, investor briefings, annual report and ESG/BRSR reporting\nManage the flow of financial and strategic information to investors and analysts, ensuring transparency, accuracy, and timeliness\nRespond to investor inquiries and provide updates on corporate performance, strategy, and market developments\nBuild and maintain relationships with institutional investors, analysts, and stakeholders to effectively communicate the company's vision, strategy, and financial performance\nCorporate development and M&A Support:\nLead financial due diligence for potential mergers, acquisitions, and partnerships, collaborating with legal, finance, and business development teams\nAssist in assessing the financial health of potential targets, analyzing synergies, and preparing financial reports and recommendations\nConduct valuations and financial modeling to support the evaluation of potential investment opportunities",Industry Type: Hotels & Restaurants,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Business Analysis', 'Financial Analytics', 'Business Analytics', 'Business Modeling', 'Business Insights', 'Financial Modelling']",2025-06-12 06:21:07
Machine Learning Engineer,Panacorp Software Solutions,0 - 5 years,Not Disclosed,"['Nagercoil', 'Kanyakumari']","Research Programmer (Python/ MATLAB) Fresher & Experienced\nAbout Panacorp Software Solutions\nPanacorp Software Solutions is a research-driven organization specializing in providing technical assistance for PhD research projects. Our focus is on supporting research scholars with programming, simulations, and computational analysis in various domains, including AI, Machine Learning, and numerical computing.\n\nJob Role & Responsibilities\nAssist in research-based projects related to PhD studies.\nPerform simulations, numerical computing, and data analysis using Python, MATLAB, and Simulink.\nSupport research scholars in implementing Machine Learning (ML) and Deep Learning (DL) models.\nAutomate processes and optimize research workflows through scripting.\nDocument research methodologies, findings, and technical reports.\nWork closely with scholars to analyze and interpret computational results.\nEligibility Criteria\nQualification: BE/B.Tech/MCA\nExperience: 0 5+ years (Freshers with strong academic knowledge can apply).\nStrong understanding of research methodologies and computational tools.\nPreferred Skills\nProficiency in Python, MATLAB, and Simulink.\nKnowledge of data analysis, AI/ML techniques, and numerical simulations.\nAbility to interpret and validate research outcomes.\nStrong analytical and problem-solving skills.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Machine Learning', 'matlab', 'simulink', 'python', 'data analysis', 'research methodology', 'artificial intelligence']",2025-06-12 06:21:10
Python Senior Developer,Infosys,3 - 5 years,Not Disclosed,['Bengaluru'],"Job Title\nPython Senior Developer\n\nResponsibilities\nSolid development experience in Data Science Arch.\nExperience in Application Architecture & Design of Java Based Applications\nGood Knowledge of Architecture and related technologies\nExperience in Integration Technologies and Architecture\nWorking knowledge of frontend and database technologies\nExcellent Analytical and Debugging Skills\nFamiliarity with Agile & DevSecOps, Log Analytics, APM\nExperience in leading the teams technically\nExperience in requirements gathering, analysis & design and estimation\nGood communication and articulation skills Technical and Professional :\nWe are seeking a skilled Python and SQL Developer to join our dynamic team. The ideal candidate will have a strong background in Python programming and SQL database management.\nDevelop and maintain Python-based applications and scripts.\nWrite efficient SQL queries for data extraction and manipulation.\nCollaborate with cross-functional teams to gather requirements and deliver solutions.\nFamiliarity with Linux operating systems.\nBasic understanding of cloud platforms (e.g., AWS, Azure, Google Cloud).\nKnowledge of Model Quantization and Pruning\nExperience playing a Data Scientist role Preferred Skills:\nPython Technology-Open System-Open System- ALL-Python Technology-Full stack-Java Full stack-Frontend(Vue.js)+Enterprise layer(Python)+DB Additional Responsibilities:\nIn-depth knowledge of design issues and best practices\nSolid understanding of object-oriented programming\nFamiliar with various design, architectural patterns and software development process.\nExperience with both external and embedded databases\nCreating database schemas that represent and support business processes\nImplementing automated testing platforms and unit tests\nGood verbal and written communication skills\nAbility to communicate with remote teams in effective manner\nHigh flexibility to travelSoft Skills\nGood verbal & written communication skills articulate value of AI to business, project managers & other team members\nAbility to break complex problem into smaller problems and create hypothesis\nInnovation and experimentation Educational Master of Computer Science,Master Of Science,Master Of Technology,MCA,Bachelor Of Comp. Applications,Bachelor Of Computer Science,Bachelor of Engineering,Bachelor Of Technology Service LineApplication Development and Maintenance* Location of posting is subject to business requirements",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Python', 'Enterprise layer', 'software development', 'report generation', 'MIS', 'CI/CD', 'Java Full stack-Frontend', 'SDLC']",2025-06-12 06:21:12
Business Analyst,Sapiens,4 - 9 years,Not Disclosed,['Bengaluru'],"Sapiens is on the lookout for a Business Analyst to become a key player in our Bangalore team. If you're a seasoned BA pro and ready to take your career to new heights with an established, globally successful company, this role could be the perfect fit.\nWorking Model: Our flexible work arrangement combines both remote and in-office work, optimizing flexibility and productivity.\nWhat you ll do:\nWork closely with customer to identify, analyse, validate and document business processes and functional requirements. Oversee proper implementation by providing functional specifications and acceptance criteria. Act as a liaison between the customer business users and the project development and testing team.\nUnderstand and document customer s functional and technical requirements, user stories, and acceptance scenarios.\nSpecialize in Sapiens ALIS application; Understand limitations and possibilities of the system and their implications on the business processes and functionality.\nInitiate and oversee project solution design\nProvide presentations and demonstrations on Product Features\nFunctional support to development teams in design processes\nFunctional support to the testing teams by preparing test scenarios and participate in system testing before releases to the customers\nWrite new requirements / User Stories documents, for new functionalities (CRs, new features, etc.).\nAssist Sapiens RI 2nd and 3rd line support representatives in analysing and reproducing incidents reported by the customer.\nParticipate in training activities of employees and customers.\nSupport the TW with updating product documentation.\nWhat to Have for this position.\nMust have Skills.\nEducation : BE and MBA - MUST\nExperience required is minimum 4+ years.\nExcellent analytical skills\nExperience with information systems (such as ERP) at a super user level\nExperience with Financial services / Systems. Excellent command of the insurance business (experience in the actuarial or insurance fields)- must, Reinsurance is advantage\nAbility to match between customer functional requirements and application system options/functionalities in an efficient way.\nInsurance business knowledge - an advantage\nExcellent communications skills English (mother tongue level) - must\nRepresentative\nWilling to travel extensively\nExperience with overseas customers.\nExcellent analytical skills\nExperience with core organizational product implementations\nAbility to match between customer functional requirements and application system options/functionalities in an efficient way.\nHaving experience in Insurance background and worked on insurance products are added advantage.",Industry Type: BPM / BPO,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Business Analysis', 'product documentation', 'ERP', 'product implementation', 'business process analysis']",2025-06-12 06:21:14
Senior ML Compiler Engineer,Qualcomm,2 - 4 years,Not Disclosed,['Hyderabad'],"Job Area: Engineering Group, Engineering Group > Software Engineering\n\nGeneral Summary:\n\nInterested in accelerating machine learning and artificial intelligence on mobile devices for millions of usersCome join our team. We are building software platforms that enable users of Qualcomms silicon to construct optimized neural networks and machine learning algorithms. We are looking for software engineers with a machine learning or compiler background who will help us build these software platforms. In this role, you will construct and tune machine learning frameworks, build compilers and tools, and collaborate with Qualcomm hardware and software engineers to enable efficient usage of Qualcomms silicon for machine learning applications.\n\nMinimum qualifications:\nBachelors degree in Engineering, Information Systems, Computer Science, or related field.\nProgramming in C/C++\n2 to 4 years of software engineering or related work experience\n\n\nPreferred qualifications:\nExperience in machine learning frameworks such as MxNet/NNVM/TVM, Pytorch, Tensorflow, Caffe\n\nOR experience in compilers with an interest in machine learning\nDeep knowledge of software engineering\n\n\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 2+ years of Software Engineering or related work experience.\nOR\nMaster's degree in Engineering, Information Systems, Computer Science, or related field and 1+ year of Software Engineering or related work experience.\nOR\nPhD in Engineering, Information Systems, Computer Science, or related field.\n\n2+ years of academic or work experience with Programming Language such as C, C++, Java, Python, etc.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'machine learning', 'tensorflow', 'pytorch', 'software engineering', 'algorithms', 'c++', 'natural language processing', 'caffe', 'neural networks', 'mxnet', 'artificial intelligence', 'sql', 'deep learning', 'r', 'java', 'data science', 'computer vision', 'machine learning algorithms', 'ml']",2025-06-12 06:21:17
Data Science Consultant,Techf Solutions,8 - 13 years,22.5-30 Lacs P.A.,['Indore'],"As a Senior AI Developer/ AI Architect in the AI team, you will work and mentor a team of developers, working on the Fusion AI Team and its AI engine AI Talos alongside research in the space, such as large language models, simulations, & agentic AI.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['RAG architecture', 'Python', 'full-stack developer', 'GitHub', 'Hugging Face', 'Jira', 'Pytorch', 'Gen AI', 'Llama2', 'Docker', 'Pandas', 'Pydantic', 'Pyarrow', 'Scikit', 'Mistral AI']",2025-06-12 06:21:20
Senior Copy Writer- WFH,Aegis Softtech,6 - 7 years,6.5-9 Lacs P.A.,"['Hyderabad', 'Chennai', 'Bengaluru']","Immediate Openings for Senior Copy Writer (Permanent WFH)\n\nCopywriter - Job Description\n\nAbout Aegis :\nAegis Softtech is a global technology services firm delivering customized software solutions in AI, ML, Cloud, Data Engineering, CRM Consulting, and more. We work with tech leaders, enterprises, and fast-scaling startups to help them solve business problems with scalable, future-ready software.\n\nWe are building a lean, quality-first content team.\nIf you're a copywriter who knows how to write for humans, doesn't hide behind jargon, and loves shaping complex tech stories into simple, compelling narratives, lets work together.\n\nWhat Youll Do:\n1.Craft compelling, conversion-driven content across formats: website pages, landing pages, email copy, social posts, and ad creatives\n2. Own copy development for key service areas like AI/ML, Data & Cloud, and CRM solutions (Microsoft Dynamics, Salesforce, etc.)\n3. Translate technical inputs into benefit-focused, client-first messaging that aligns with our authoritative yet approachable voice\n4.Collaborate with the Content Lead, designers, and developers to align messaging across touchpoints.\n5. Edit and refine content for clarity, brevity, tone, and SEO, without diluting meaning.\n6.Bring consistency to brand tone across different regions and verticals.\n\nWhat Were Looking For\n> 5–8 years of proven experience as a copywriter, ideally in the B2B tech or SaaS industry.\n> Comfort working across multiple formats and content lengths—from short CTAs to full-service pages.\n> A storytelling mindset with a keen understanding of buyer psychology and content structure.\n> Strong grasp of SEO principles and how to write for both humans and search engines.\n> Ability to simplify complex tech ideas without dumbing them down.\n> Self-driven, collaborative, and comfortable with remote async work.\n\nWhy Join Us\n> Flexible remote work with a global team of tech thinkers and builders\n> A chance to influence messaging at a strategic level, not just execute briefs\n> Open and transparent communication culture\n> Opportunity to work closely with a Lead who values content quality as much as delivery speed.\n\nTo Apply:\nSend your resume, a short note about why this role speaks to you, and 2–3 samples that show your ability to:\nTranslate tech to value.\nBuild momentum with words.\nWrite with clarity and character.\nEmail your application to hr@aegissofttech.com with the subject line: Copywriter Application – [Your Name].",Industry Type: IT Services & Consulting,"Department: Content, Editorial & Journalism","Employment Type: Full Time, Permanent","['SEO Writing', 'technical content', 'copy writing', 'Content Writing', 'Content Strategy']",2025-06-12 06:21:23
Senior/Lead MLops Engineer,Tiger Analytics,7 - 10 years,Not Disclosed,"['Hyderabad', 'Chennai', 'Bengaluru']","JOB DESCRIPTION\n\nSenior MLE / Architect MLE (ML Ops) Chennai / Bangalore / Hyderabad (Hybrid)\n\nWho we are Tiger Analytics is a global leader in AI and analytics, helping Fortune 1000 companies solve their toughest challenges. We offer fullstack AI and analytics services & solutions to empower businesses to achieve real outcomes and value at scale. We are on a mission to push the boundaries of what AI and analytics can do to help enterprises navigate uncertainty and move forward decisively. Our purpose is to provide certainty to shape a better tomorrow. Our team of 4000+ technologists and consultants are based in the US, Canada, the UK, India, Singapore and Australia, working closely with clients across CPG, Retail, Insurance, BFS, Manufacturing, Life Sciences, and Healthcare. Many of our team leaders rank in Top 10 and 40 Under 40 lists, exemplifying our dedication to innovation and excellence. We are a Great Place to Work-Certified (2022-24), recognized by analyst firms such as Forrester, Gartner, HFS, Everest, ISG and others. We have been ranked among the Best and Fastest Growing analytics firms lists by Inc., Financial Times, Economic Times and Analytics India Magazine.",,,,"['MLops', 'Azure', 'Snowflake', 'Deployment', 'Ci/Cd', 'Machine Learning']",2025-06-12 06:21:25
Business Analyst,Global Banking Organization,3 - 8 years,Not Disclosed,['Bengaluru'],"Key Skills: Marketing Analytics, Analytics, SQL, Python, Business Analysis, Predictive Analysis, Statistical Analysis.\nRoles and Responsibilities:\nGathers operational data from various cross-functional stakeholders to examine past business performance.\nIdentifies data patterns and trends, and provides insights to enhance business decision-making capability in business planning, process improvement, solution assessment, etc.\nRecommends actions for future developments and strategic business opportunities, as well as enhancements to operational policies.\nMay be involved in exploratory data analysis, confirmatory data analysis, and/or qualitative analysis.\nTranslates data into consumer or customer behavioral insights to drive targeting and segmentation strategies, and communicates clearly and effectively to business partners and senior leaders all findings.\nContinuously improves processes and strategies by exploring and evaluating new data sources, tools, and capabilities.\nWorks closely with internal and external business partners in building, implementing, tracking, and improving decision strategies.\nAppropriately assesses risk when business decisions are made, demonstrating particular consideration for the firm's reputation and safeguarding its clients and assets, by driving compliance with applicable laws, rules and regulations, adhering to policy, applying sound ethical judgment regarding personal behavior, conduct and business practices, and escalating, managing, and reporting control issues with transparency.\nExperience Requirement:\n3-8 years of relevant experience in business analytics, data analysis, or business intelligence roles.\nProven experience in using analytical tools such as SQL, Excel, Python, or R to extract and analyze data.\nHands-on experience with data visualization tools such as Tableau, Power BI, or similar platforms.\nExperience working in cross-functional teams and supporting decision-making through data-driven insights.\nStrong track record of identifying business trends and providing actionable recommendations based on data analysis.\nDemonstrated ability to handle multiple projects simultaneously with a strong attention to detail.\nEducation: B.Tech M.Tech (Dual), MCA, B.Tech.",Industry Type: Financial Services,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Marketing Analytics', 'Analytics', 'SQL', 'Python', 'Business Analysis', 'Statistical Analysis.', 'Predictive Analysis']",2025-06-12 06:21:27
Cloud Machine Learning LLM Serving Engineer,Qualcomm,2 - 7 years,Not Disclosed,['Bengaluru'],"Job Area: Engineering Group, Engineering Group > Software Engineering\n\nGeneral Summary:\n\nJD for Cloud Machine Learning LLM Serving engineer\n\nJob Overview:\n\nThe Qualcomm Cloud Computing team is developing hardware and software for Machine Learning solutions spanning the data center, edge, infrastructure, automotive market. We are seeking ambitious, bright, and innovative engineers with experience in machine learning framework development. Job activities span the whole product life cycle from early design to commercial deployment. The environment is fast-paced and requires cross-functional interaction daily so good communication, planning and execution skills are a must.\n\nKey Responsibilities\nImprove and optimize key Deep Learning models on Qualcomm AI 100.\nBuild deep learning framework extensions for Qualcomm AI 100 in upstream open-source repositories.\nImplement Kernels for AI workloads\nCollaborate and interact with internal teams to analyze and optimize training and inference for deep learning.\nBuild software tools and ecosystem around AI SW Stack.\nWork on vLLM, Triton, ExecuTorch, Inductor, TorchDynamo to build abstraction layers for inference accelerator.\nOptimize workloads for both scale-up (multi-SoC) and scale-out (multi-card) systems.\nOptimize the entire deep learning pipeline including graph compiler integration.\nApply knowledge of software engineering best practices.\n\n\nDesirable Skills and Aptitudes\nDeep Learning experience or knowledge- LLMs, Natural Language Processing, Vision, Audio, Recommendation systems.\nKnowledge of the structure and function of different components of Pytorch, TensorFlow software stacks.\nExcellent C/C++/Python programming and software design skills, including debugging, performance analysis, and test design.\nAbility to work independently, define requirements and scope, and lead your own development effort.\nWell versed with open-source development practices.\nStrong developer with a research mindset- strives to innovate.\nAvid problem solver- should be able to find solutions to key engineering and domain problems.\n\n\nKnowledge of tiling and scheduling a Machine learning operator is a plus.\nExperience in using C++ 14 (advanced features)\nExperience of profiling software and optimization techniques\nHands on experience writing SIMD and/or multi-threaded high-performance code is a plus.\nExperience of ML compiler, Auto-code generation (using MLIR) is a plus.\nExperiences to run workloads on large scale heterogeneous clusters is a plus.\nHands-on experience with CUDA, CUDNN is a plus.\n\n\nQualifications:\nBachelor's / Masters/ PHD degree in Engineering, Machine learning/ AI, Information Systems, Computer Science, or related field.\n2+ years Software Engineering or related work experience.\n2+ years experience with Programming Language such as C++, Python.\n\n\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'c++', 'c', 'software design', 'software engineering', 'cuda', 'natural language processing', 'scale', 'machine learning', 'artificial intelligence', 'deep learning', 'tensorflow', 'code generation', 'computer science', 'pytorch', 'debugging', 'machine learning algorithms', 'ml']",2025-06-12 06:21:30
"Senior Staff Engineer, Frontend React",Nagarro,10 - 13 years,Not Disclosed,['India'],"We're Nagarro.\n\nWe are a Digital Product Engineering company that is scaling in a big way! We build products, services, and experiences that inspire, excite, and delight. We work at scale across all devices and digital mediums, and our people exist everywhere in the world (18000+ experts across 38 countries, to be exact). Our work culture is dynamic and non-hierarchical. We're looking for great new colleagues. That's where you come in!\n\nREQUIREMENTS:\nTotal Experience 10+ years.\nHands on working experience in front-end or full-stack development experience, with building production apps in React.js and Next.js.\nHands-on expertise writing unit/integration tests for React components (Jest, React Testing Library, etc.)\nSolid grasp of state-management patterns and libraries (Redux, React Context, Zustand, etc.).\nStrong understanding of RESTful APIs, asynchronous programming (Promises, async/await), and modern build tools (Webpack, Vite, or Turbopack).\nPractical experience with Git, pull-request workflows, and collaborative development tools (GitHub, GitLab, Bitbucket).\nAdvanced proficiency in JavaScript (ES6+) and TypeScript.\nProblem-solving mindset with the ability to tackle complex data engineering challenges. \nStrong communication and teamwork skills, with the ability to mentor and collaborate effectively.\n\nRESPONSIBILITIES:\nWriting and reviewing great quality code\nUnderstanding the clients business use cases and technical requirements and be able to convert them in to technical design which elegantly meets the requirements\nMapping decisions with requirements and be able to translate the same to developers.\nIdentifying different solutions and being able to narrow down the best option that meets the clients requirements.\nDefining guidelines and benchmarks for NFR considerations during project implementation\nWriting and reviewing design document explaining overall architecture, framework, and high-level design of the application for the developers.\nReviewing architecture and design on various aspects like extensibility, scalability, security, design patterns, user experience, NFRs, etc., and ensure that all relevant best practices are followed.\nDeveloping and designing the overall solution for defined functional and non-functional requirements; and defining technologies, patterns, and frameworks to materialize it\nUnderstanding and relating technology integration scenarios and applying these learnings in projects\nResolving issues that are raised during code/review, through exhaustive systematic analysis of the root cause, and being able to justify the decision taken.\nCarrying out POCs to make sure that suggested design/technologies meet the requirements",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Javascript', 'React.Js', 'Nextjs', 'Typescript']",2025-06-12 06:21:32
"Senior Staff Engineer, QA Automation",Nagarro,10 - 13 years,Not Disclosed,['India'],"We're Nagarro.\nWe are a Digital Product Engineering company that is scaling in a big way! We build products, services, and experiences that inspire, excite, and delight. We work at scale across all devices and digital mediums, and our people exist everywhere in the world (18000+ experts across 38 countries, to be exact). Our work culture is dynamic and non-hierarchical. We're looking for great new colleagues. That's where you come in!\n\nREQUIREMENTS:\nTotal Experience 10+ years.\nStrong working experience in QA with a strong background in both manual and automation testing.\nHands-on experience with Selenium WebDriver, Appium, and Postman.\nSolid understanding of REST API testing and automation using tools like RestAssured.\nProficient in testing frameworks such as TestNG, JUnit, or Cypress.\nStrong experience in automation of mobile and web applications.\nFamiliarity with CI/CD tools like Jenkins, GitLab CI, or equivalent.\nWorking knowledge of bug tracking and test management tools (e.g., JIRA, TestRail).\nExperience with BDD frameworks like Cucumber.\nGood command of scripting or programming in Java, Python, or similar languages is a plus.\nProblem-solving mindset with the ability to tackle complex data engineering challenges.\nStrong communication and teamwork skills, with the ability to mentor and collaborate effectively.\nRESPONSIBILITIES:\nUnderstanding the clients business use cases and technical requirements and be able to convert them in to technical design which elegantly meets the requirements\nMapping decisions with requirements and be able to translate the same to developers.\nIdentifying different solutions and being able to narrow down the best option that meets the clients requirements.\nDefining guidelines and benchmarks for NFR considerations during project implementation\nWriting and reviewing design document explaining overall architecture, framework, and high-level design of the application for the developers.\nReviewing architecture and design on various aspects like extensibility, scalability, security, design patterns, user experience, NFRs, etc., and ensure that all relevant best practices are followed.\nDeveloping and designing the overall solution for defined functional and non-functional requirements; and defining technologies, patterns, and frameworks to materialize it\nUnderstanding and relating technology integration scenarios and applying these learnings in projects\nResolving issues that are raised during code/review, through exhaustive systematic analysis of the root cause, and being able to justify the decision taken.\nCarrying out POCs to make sure that suggested design/technologies meet the requirements",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['API Testing', 'Appium', 'QA Automation', 'Selenium', 'Postman']",2025-06-12 06:21:35
Senior Lead business execution consultant,Wells Fargo,7 - 12 years,Not Disclosed,['Bengaluru'],"About this role:\nWells Fargo is seeking a Senior Lead business execution consultant\n\nIn this role, you will:\nAct as a Business Execution advisor to leadership to drive performance and initiatives, and develop and implement information delivery or presentations to key stakeholders and senior management",,,,"['Business execution', 'Business Implementation', 'Data Engineering', 'NLP', 'generative AI', 'Data Mining', 'machine learning', 'Strategic Planning', 'agentic AI']",2025-06-12 06:21:37
"Sr Software Eng: Generative AI, Go/Python, AWS, Kubernetes 7-12 Yrs",Cisco,7 - 12 years,Not Disclosed,['Bengaluru'],"Meet The Team\nThe Cisco AI Software & Platform Group drives the development of groundbreaking generative AI applications. We empower Cisco's diverse product portfolio, spanning networking and security, with intelligent assistants and agents. We work on pioneering technologies that proactively defend against threats, safeguard critical business assets, and simplify security operations. Fueled by a passion for AI/ML, we strive to create a secure future for businesses. Our collaborative and passionate team thrives with tackling sophisticated challenges and delivering innovative solutions.",,,,"['Golang', 'Generative Ai', 'AWS', 'Python', 'Kubernetes', 'Java']",2025-06-12 06:21:39
"Senior Staff Engineer, Mobile -Flutter",Nagarro,10 - 13 years,Not Disclosed,['India'],"We're Nagarro.\n\nWe are a Digital Product Engineering company that is scaling in a big way! We build products, services, and experiences that inspire, excite, and delight. We work at scale across all devices and digital mediums, and our people exist everywhere in the world (18000+ experts across 38 countries, to be exact). Our work culture is dynamic and non-hierarchical. We're looking for great new colleagues. That's where you come in!\n\nREQUIREMENTS:\nTotal Experience 10+ years.\nStrong working experience in Dart and Flutter.\nSolid understanding of Mobile App Architecture (MVVM, BLoC, Provider, GetX).\nExperience with local databases like Sqflite or alternatives.\nHands-on experience in unit testing and test automation for Flutter apps.\nProven experience in building and deploying apps to the App Store and Google Play Store.\nFamiliarity with Git, GitHub/GitLab, CI/CD tools (e.g., Jenkins, Bitrise, GitHub Actions).\nDeep knowledge of mobile app lifecycle, design principles, and clean architecture patterns (MVVM, MVC, etc.)\nExpertise in mobile app performance optimization and security best practices.\nExperience in API integration, RESTful services, and handling JSON data.\nProficient in Version Control Systems like Git.\nProblem-solving mindset with the ability to tackle complex data engineering challenges.\nStrong communication and teamwork skills, with the ability to mentor and collaborate effectively.\nRESPONSIBILITIES:\nWriting and reviewing great quality code\nUnderstanding the clients business use cases and technical requirements and be able to convert them in to technical design which elegantly meets the requirements\nMapping decisions with requirements and be able to translate the same to developers.\nIdentifying different solutions and being able to narrow down the best option that meets the clients requirements.\nDefining guidelines and benchmarks for NFR considerations during project implementation\nWriting and reviewing design document explaining overall architecture, framework, and high-level design of the application for the developers.\nReviewing architecture and design on various aspects like extensibility, scalability, security, design patterns, user experience, NFRs, etc., and ensure that all relevant best practices are followed.\nDeveloping and designing the overall solution for defined functional and non-functional requirements; and defining technologies, patterns, and frameworks to materialize it\nUnderstanding and relating technology integration scenarios and applying these learnings in projects\nResolving issues that are raised during code/review, through exhaustive systematic analysis of the root cause, and being able to justify the decision taken.\nCarrying out POCs to make sure that suggested design/technologies meet the requirements",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['GIT', 'Flutter', 'Dart', 'Swift', 'IOS', 'Android']",2025-06-12 06:21:42
Lead Pyspark Developer,Synechron,5 - 10 years,Not Disclosed,"['Chennai', 'Bengaluru']","job requisition idJR1027452\n\n\n\nOverall Responsibilities:\nData Pipeline Development:Design, develop, and maintain highly scalable and optimized ETL pipelines using PySpark on the Cloudera Data Platform, ensuring data integrity and accuracy.\nData Ingestion:Implement and manage data ingestion processes from a variety of sources (e.g., relational databases, APIs, file systems) to the data lake or data warehouse on CDP.\nData Transformation and Processing:Use PySpark to process, cleanse, and transform large datasets into meaningful formats that support analytical needs and business requirements.\nPerformance Optimization:Conduct performance tuning of PySpark code and Cloudera components, optimizing resource utilization and reducing runtime of ETL processes.\nData Quality and Validation:Implement data quality checks, monitoring, and validation routines to ensure data accuracy and reliability throughout the pipeline.\nAutomation and Orchestration:Automate data workflows using tools like Apache Oozie, Airflow, or similar orchestration tools within the Cloudera ecosystem.\nMonitoring and Maintenance:Monitor pipeline performance, troubleshoot issues, and perform routine maintenance on the Cloudera Data Platform and associated data processes.\nCollaboration:Work closely with other data engineers, analysts, product managers, and other stakeholders to understand data requirements and support various data-driven initiatives.\nDocumentation:Maintain thorough documentation of data engineering processes, code, and pipeline configurations.\n\n\n\nSoftware :\nAdvanced proficiency in PySpark, including working with RDDs, DataFrames, and optimization techniques.\nStrong experience with Cloudera Data Platform (CDP) components, including Cloudera Manager, Hive, Impala, HDFS, and HBase.\nKnowledge of data warehousing concepts, ETL best practices, and experience with SQL-based tools (e.g., Hive, Impala).\nFamiliarity with Hadoop, Kafka, and other distributed computing tools.\nExperience with Apache Oozie, Airflow, or similar orchestration frameworks.\nStrong scripting skills in Linux.\n\n\n\nCategory-wise Technical\n\nSkills:\nPySpark:Advanced proficiency in PySpark, including working with RDDs, DataFrames, and optimization techniques.\nCloudera Data Platform:Strong experience with Cloudera Data Platform (CDP) components, including Cloudera Manager, Hive, Impala, HDFS, and HBase.\nData Warehousing:Knowledge of data warehousing concepts, ETL best practices, and experience with SQL-based tools (e.g., Hive, Impala).\nBig Data Technologies:Familiarity with Hadoop, Kafka, and other distributed computing tools.\nOrchestration and Scheduling:Experience with Apache Oozie, Airflow, or similar orchestration frameworks.\nScripting and Automation:Strong scripting skills in Linux.\n\n\n\nExperience:\n5-12 years of experience as a Data Engineer, with a strong focus on PySpark and the Cloudera Data Platform.\nProven track record of implementing data engineering best practices.\nExperience in data ingestion, transformation, and optimization on the Cloudera Data Platform.\n\n\n\nDay-to-Day Activities:\nDesign, develop, and maintain ETL pipelines using PySpark on CDP.\nImplement and manage data ingestion processes from various sources.\nProcess, cleanse, and transform large datasets using PySpark.\nConduct performance tuning and optimization of ETL processes.\nImplement data quality checks and validation routines.\nAutomate data workflows using orchestration tools.\nMonitor pipeline performance and troubleshoot issues.\nCollaborate with team members to understand data requirements.\nMaintain documentation of data engineering processes and configurations.\n\n\n\nQualifications:\nBachelors or Masters degree in Computer Science, Data Engineering, Information Systems, or a related field.\nRelevant certifications in PySpark and Cloudera technologies are a plus.\n\n\n\nSoft\n\nSkills:\nStrong analytical and problem-solving skills.\nExcellent verbal and written communication abilities.\nAbility to work independently and collaboratively in a team environment.\nAttention to detail and commitment to data quality.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['cloudera', 'hive', 'pyspark', 'linux', 'hadoop', 'scala', 'amazon redshift', 'data warehousing', 'emr', 'sql', 'docker', 'apache', 'java', 'spark', 'gcp', 'etl', 'big data', 'hbase', 'data lake', 'python', 'oozie', 'airflow', 'microsoft azure', 'impala', 'data engineering', 'nosql', 'amazon ec2', 'mapreduce', 'kafka', 'sqoop', 'aws']",2025-06-12 06:21:45
Principal - Data Architect,Affine Analytics,8 - 12 years,Not Disclosed,['Chennai'],"We are seeking a highly skilled Data Architect to design and implement robust, scalable, and secure data solutions on AWS Cloud. The ideal candidate should have expertise in AWS services, data modeling, ETL processes, and big data technologies, with hands-on experience in Glue, DMS, Python, PySpark, and MPP databases like Snowflake, Redshift, or Databricks.\nKey Responsibilities:\nArchitect and implement data solutions leveraging AWS services such as EC2, S3, IAM, Glue (Mandatory), and DMS for efficient data processing and storage.",,,,"['Python', 'S3', 'AWS Glue', 'DMS', 'SQL Server', 'Redshift', 'Glue', 'IAM', 'EC2', 'Snowflake', 'Databricks', 'Oracle', 'Lambda']",2025-06-12 06:21:47
CPU Full Stack Python Developer (Staff/Sr. Staff),Qualcomm,4 - 9 years,Not Disclosed,['Bengaluru'],"Job Area: Engineering Group, Engineering Group > Hardware Engineering\n\nGeneral Summary:\n\nWe are seeking a highly skilled Full Stack Python Developer to join our dynamic team. The ideal candidate should have a strong background in tool development, data science, and automation of complex tasks. You will be responsible for developing high volume regression dashboard, parametric and power tools and contributing to both front-end and back-end development.\n\nMinimum Qualifications:\nBachelor's degree in Computer Science, Electrical/Electronics Engineering, Engineering, or related field and 4+ years of Hardware Engineering or related work experience.\nOR\nMaster's degree in Computer Science, Electrical/Electronics Engineering, Engineering, or related field and 3+ years of Hardware Engineering or related work experience.\nOR\nPhD in Computer Science, Electrical/Electronics Engineering, Engineering, or related field and 2+ years of Hardware Engineering or related work experience.\n\nTechnical\n\nSkills:\n\n\n\nPythonProficiency in Python programming, including libraries like Pandas, NumPy, and SciPy for data science.\n\n\nFull Stack DevelopmentExperience with both front-end (HTML, CSS, JavaScript, React, Vue.js) and back-end (Django, Flask) technologies.\n\n\nTool DevelopmentAbility to develop parametric and power tools, possibly using frameworks like Vue.js , PyQt or Tkinter for GUI development.\n\n\nData ScienceStrong understanding of data analysis, machine learning (using libraries like scikit-learn, TensorFlow), and data visualization (using Matplotlib, Seaborn).\n\n\nAutomationExperience in automating complex tasks using scripting and tools like Selenium, Airflow, or custom automation scripts.\n\n\nSoft\n\nSkills:\n\n\n\nProblem-SolvingAbility to tackle complex problems and develop innovative solutions.\n\n\nCommunicationStrong communication skills to effectively collaborate with team members and stakeholders.\n\n\nAdaptabilityFlexibility to adapt to new technologies and methodologies.\n\n\nExperience:\n\n\nProjectsPrevious experience in developing tools and automation solutions.\n\n\nIndustry KnowledgeFamiliarity with the specific industry or domain you're working in can be a plus.\n\n\nKey Responsibilities:\n\nDevelop and maintain parametric and power tools using Python.\n\nDesign and implement automation solutions for complex tasks.\n\nCollaborate with data scientists to analyze and visualize data.\n\nBuild and maintain web applications using Django or Flask.\n\nDevelop front-end components using HTML, CSS, JavaScript, and React.\n\nIntegrate third-party APIs and services.\n\nOptimize applications for maximum speed and scalability.\n\nWrite clean, maintainable, and efficient code.\n\nTroubleshoot and debug applications.\n\nStay updated with the latest industry trends and technologies.\n\n\nPreferred Qualifications:\n\nBachelor's degree in Computer Science, Engineering, or related field.\n\nPrevious experience in tool development and automation.\n\nFamiliarity with industry-specific tools and technologies.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['tool development', 'data science', 'python', 'data analysis', 'machine learning', 'css', 'hiring', 'scikit-learn', 'vue.js', 'numpy', 'staffing', 'react.js', 'tensorflow', 'seaborn', 'selenium', 'pyqt', 'html', 'data visualization', 'scipy', 'hardware engineering', 'javascript', 'pandas', 'django', 'matplotlib', 'flask']",2025-06-12 06:21:50
Data Architect (Data Bricks),Diacto Technologies Pvt Ltd,5 - 9 years,Not Disclosed,['Pune( Baner )'],"Job Overview:\nDiacto is seeking an experienced and highly skilled Data Architect to lead the design and development of scalable and efficient data solutions. The ideal candidate will have strong expertise in Azure Databricks, Snowflake (with DBT, GitHub, Airflow), and Google BigQuery. This is a full-time, on-site role based out of our Baner, Pune office.\n\nQualifications:\nB.E./B.Tech in Computer Science, IT, or related discipline\nMCS/MCA or equivalent preferred\n\nKey Responsibilities:\nDesign, build, and optimize robust data architecture frameworks for large-scale enterprise solutions\nArchitect and manage cloud-based data platforms using Azure Databricks, Snowflake, and BigQuery\nDefine and implement best practices for data modeling, integration, governance, and security\nCollaborate with engineering and analytics teams to ensure data solutions meet business needs\nLead development using tools such as DBT, Airflow, and GitHub for orchestration and version control\nTroubleshoot data issues and ensure system performance, reliability, and scalability\nGuide and mentor junior data engineers and developers\n\nExperience and Skills Required:\n5 to12 years of experience in data architecture, engineering, or analytics roles\nHands-on expertise in Databricks, especially Azure Databricks\nProficient in Snowflake, with working knowledge of DBT, Airflow, and GitHub\nExperience with Google BigQuery and cloud-native data processing workflows\nStrong knowledge of modern data architecture, data lakes, warehousing, and ETL pipelines\nExcellent problem-solving, communication, and analytical skills\n\nNice to Have:\nCertifications in Azure, Snowflake, or GCP\nExperience with containerization (Docker/Kubernetes)\nExposure to real-time data streaming and event-driven architecture\n\nWhy Join Diacto Technologies?\nCollaborate with experienced data professionals and work on high-impact projects\nExposure to a variety of industries and enterprise data ecosystems\nCompetitive compensation, learning opportunities, and an innovation-driven culture\nWork from our collaborative office space in Baner, Pune\nHow to Apply:\nOption 1 (Preferred)\n\nCopy and paste the following link on your browser and submit your application for the automated interview process: -\n\nhttps://app.candidhr.ai/app/candidate/gAAAAABoRrTQoMsfqaoNwTxsE_qwWYcpcRyYJk7NzSUmO3LKb6rM-8FcU58CUPYQKc65n66feHor-TGdCEfyouj0NmKdgYcNbA==/\n\nOption 2\n\n1. Please visit our website's career section at https://www.diacto.com/careers/\n2. Scroll down to the ""Who are we looking for?"" section\n3. Find the listing for "" Data Architect (Data Bricks)"" and\n4. Proceed with the virtual interview by clicking on ""Apply Now.""",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Snowflake', 'Azure Databricks', 'Airflow', 'Etl Pipelines', 'Github', 'google BigQuery', 'DBT', 'Data Security', 'Data Modeling', 'Elt', 'Data Governance']",2025-06-12 06:21:52
Artificial Intelligence Intern,Kumaran Systems,0 - 1 years,4.5-5 Lacs P.A.,['Chennai( Siruseri Sipcot IT Park )'],"We are looking for a passionate and motivated AI Developer Fresher to join our growing AI team. This role will focus on Generative AI (GenAI) technologies such as large language models (LLMs), diffusion models, and other cutting-edge machine learning techniques.\n\nAs a fresher, youll work closely with senior AI engineers and data scientists to build and fine-tune generative models, contribute to prompt engineering, and support model integration into real-world applications.",,,,"['Data Science', 'Mechine Learning', 'Artificial Intelligence', 'GEN AI', 'Python']",2025-06-12 06:21:54
Machine Learning Scientist,Glynac,2 - 7 years,6-12 Lacs P.A.,['Bengaluru'],Responsibilities:\n* Develop machine learning models using PyTorch.\n* Optimize model performance through data analysis and experimentation.\n* Collaborate with cross-functional teams on product development.\n\n\nWork from home,Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Machine Learning', 'Pytorch', 'Artificial Intelligence', 'Natural Language Processing']",2025-06-12 06:21:56
Analyst,Wipro,1 - 3 years,Not Disclosed,['Kolkata'],"Role Purpose\nThe purpose of the role is to resolve, maintain and manage clients software/ hardware/ network based on the service requests raised from the end-user as per the defined SLAs ensuring client satisfaction\n\n\n\nDo\nEnsure timely response of all the tickets raised by the client end user\nService requests solutioning by maintaining quality parameters\nAct as a custodian of clients network/ server/ system/ storage/ platform/ infrastructure and other equipments to keep track of each of their proper functioning and upkeep\nKeep a check on the number of tickets raised (dial home/ email/ chat/ IMS), ensuring right solutioning as per the defined resolution timeframe\nPerform root cause analysis of the tickets raised and create an action plan to resolve the problem to ensure right client satisfaction\nProvide an acceptance and immediate resolution to the high priority tickets/ service\nInstalling and configuring software/ hardware requirements based on service requests\n100% adherence to timeliness as per the priority of each issue, to manage client expectations and ensure zero escalations\nProvide application/ user access as per client requirements and requests to ensure timely solutioning\nTrack all the tickets from acceptance to resolution stage as per the resolution time defined by the customer\nMaintain timely backup of important data/ logs and management resources to ensure the solution is of acceptable quality to maintain client satisfaction\nCoordinate with on-site team for complex problem resolution and ensure timely client servicing\nReview the log which Chat BOTS gather and ensure all the service requests/ issues are resolved in a timely manner\n\n\n\nDeliver\n\nNoPerformance ParameterMeasure1.100% adherence to SLA/ timelines\nMultiple cases of red time\nZero customer escalation\nClient appreciation emails\n\n\nMandatory Skills: L&P Policy Acquisition & Servicing. Experience: 1-3 Years.",Industry Type: IT Services & Consulting,"Department: Customer Success, Service & Operations","Employment Type: Full Time, Permanent","['root cause analysis', 'itsm', 'incident management', 'acquisition', 'problem management', 'digital transformation', 'itil']",2025-06-12 06:21:59
Business Analyst - L4,Wipro,5 - 8 years,Not Disclosed,['Hyderabad'],"Role Purpose\nThe purpose of the role is to liaison and bridging the gap between customer and Wipro delivery team to comprehend and analyze customer requirements and articulating aptly to delivery teams thereby, ensuring right solutioning to the customer.\n\nDo\n1. Customer requirements gathering and engagement\nInterface and coordinate with client engagement partners to understand the RFP/ RFI requirements\nDetail out scope documents, functional & non-functional requirements, features etc ensuring all stated and unstated customer needs are captured\nConstruct workflow charts and diagrams, studying system capabilities, writing specification after thorough research and analysis of customer requirements\nEngage and interact with internal team - project managers, pre-sales team, tech leads, architects to design and formulate accurate and timely response to RFP/RFIs\nUnderstand and communicate the financial and operational impact of any changes\nPeriodic cadence with customers to seek clarifications and feedback wrt solution proposed for a particular RFP/ RFI and accordingly instructing delivery team to make changes in the design\nEmpower the customers through demonstration and presentation of the proposed solution/ prototype\nMaintain relationships with customers to optimize business integration and lead generation\nEnsure ongoing reviews and feedback from customers to improve and deliver better value (services/ products) to the customers\n\n2.Engage with delivery team to ensure right solution is proposed to the customer\na.Periodic cadence with delivery team to:\nProvide them with customer feedback/ inputs on the proposed solution\nReview the test cases to check 100% coverage of customer requirements\nConduct root cause analysis to understand the proposed solution/ demo/ prototype before sharing it with the customer\nDeploy and facilitate new change requests to cater to customer needs and requirements\nSupport QA team with periodic testing to ensure solutions meet the needs of businesses by giving timely inputs/feedback\nConduct Integration Testing and User Acceptance demos testing to validate implemented solutions and ensure 100% success rate\nUse data modelling practices to analyze the findings and design, develop improvements and changes\nEnsure 100% utilization by studying systems capabilities and understanding business specifications\nStitch the entire response/ solution proposed to the RFP/ RFI before its presented to the customer\nb.Support Project Manager/ Delivery Team in delivering the solution to the customer\nDefine and plan project milestones, phases and different elements involved in the project along with the principal consultant\nDrive and challenge the presumptions of delivery teams on how will they successfully execute their plans\nEnsure Customer Satisfaction through quality deliverable on time\n\n\n\n3.Build domain expertise and contribute to knowledge repository\nEngage and interact with other BAs to share expertise and increase domain knowledge across the vertical\nWrite whitepapers/ research papers, point of views and share with the consulting community at large\nIdentify and create used cases for a different project/ account that can be brought at Wipro level for business enhancements\nConduct market research for content and development to provide latest inputs into the projects thereby ensuring customer delight\n\nDeliver\n\nNo.Performance ParameterMeasure1.Customer Engagement and Delivery ManagementPCSAT, utilization % achievement, no. of leads generated from the business interaction, no. of errors/ gaps in documenting customer requirements, feedback from project manager, process flow diagrams (quality and timeliness), % of deal solutioning completed within timeline, velocity generated.2.Knowledge ManagementNo. of whitepapers/ research papers written, no. of user stories created, % of proposal documentation completed and uploaded into knowledge repository, No of reusable components developed for proposal during quarter\nMandatory Skills: ServiceNow - Platform Core. Experience: 5-8 Years.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Business analysis', 'client engagement', 'Business Analyst', 'test cases', 'Integration Testing']",2025-06-12 06:22:01
Software Engineer Gen AI,Wells Fargo,2 - 7 years,Not Disclosed,['Bengaluru'],"locationsBengaluru, India\nposted onPosted 4 Days Ago\njob requisition idR-462134\nAbout this role:\nWells Fargo is seeking a Software Engineer.\n\nIn this role, you will:\nParticipate in low to moderately complex initiatives and projects associated with the technology domain, including installation, upgrades, and deployment efforts\nIdentify opportunities for service quality and availability improvements within the technology domain environment\nDesign, code, test, debug, and document for low to moderately complex projects and programs associated with technology domain, including upgrades and deployments\nReview and analyze technical assignments or challenges that are related to low to medium risk deliverables and that require research, evaluation, and selection of alternative technology domains\nPresent recommendations for resolving issues or may escalate issues as needed to meet established service level agreements\nExercise some independent judgment while also developing understanding of given technology domain in reference to security and compliance requirements\nProvide information to technology colleagues, internal partners, and stakeholders\n\nRequired Qualifications:\n2+ years of software engineering experience, or equivalent demonstrated through one or a combination of the following: work experience, training, military experience, education\n\nDesired Qualifications:\nWork as a Generative AI engineer developing enterprise-scale AI applications\nDesign, implement, and optimize LLM-based solutions using state-of-the-art frameworks\nLead Gen AI initiatives focused on developing intelligent agents and conversational systems\nDesign and build robust LLM interfaces and orchestration pipelines\nDevelop evaluation frameworks to measure and improve model performance\nImplement prompt engineering techniques to optimize model outputs\nIntegrate Gen AI capabilities with existing enterprise applications\nBuild and maintain frontend interfaces for AI applications\nStrong proficiency in Python/Java and LLM orchestration frameworks (LangChain, LangGraph)\nBasic Knowledge of model context protocols, RAG architectures, and embedding techniques\nExperience with model evaluation frameworks and metrics for LLM performance\nProficiency in frontend development with React.js for AI applications\nExperience with UI/UX design patterns specific to AI interfaces\nExperience with vector databases and efficient retrieval methods\nKnowledge of prompt engineering techniques and best practices\nExperience with containerization and microservices architecture\nStrong understanding of semantic search and document retrieval systems\nWorking knowledge of both structured and unstructured data processing\nExperience with version control using GitHub and CI/CD pipelines\nExperience working with globally distributed teams in Agile scrums\n\nJob Expectations:\nUnderstanding of enterprise use cases for Generative AI\nKnowledge of responsible AI practices and ethical considerations\nAbility to optimize AI solutions for performance and cost\nWell versed in MLOps concepts for LLM applications\nStaying current with rapidly evolving Gen AI technologies and best practices\nExperience implementing security best practices for AI applications",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Gen AI', 'Java', 'UI design', 'UX design', 'LLM orchestration', 'React.js', 'Python', 'MLOps concepts']",2025-06-12 06:22:03
Business Analyst - L5,Wipro,8 - 10 years,Not Disclosed,['Pune'],"Role Purpose\nThe purpose of the role is to liaison and bridging the gap between customer and Wipro delivery team to comprehend and analyze customer requirements and articulating aptly to delivery teams thereby, ensuring right solutioning to the customer.\n\n\n\nDo\n1. Customer requirements gathering and engagement\nInterface and coordinate with client engagement partners to understand the RFP/ RFI requirements\nDetail out scope documents, functional & non-functional requirements, features etc ensuring all stated and unstated customer needs are captured\nConstruct workflow charts and diagrams, studying system capabilities, writing specification after thorough research and analysis of customer requirements\nEngage and interact with internal team - project managers, pre-sales team, tech leads, architects to design and formulate accurate and timely response to RFP/RFIs\nUnderstand and communicate the financial and operational impact of any changes\nPeriodic cadence with customers to seek clarifications and feedback wrt solution proposed for a particular RFP/ RFI and accordingly instructing delivery team to make changes in the design\nEmpower the customers through demonstration and presentation of the proposed solution/ prototype\nMaintain relationships with customers to optimize business integration and lead generation\nEnsure ongoing reviews and feedback from customers to improve and deliver better value (services/ products) to the customers\n\n\n\n2.Engage with delivery team to ensure right solution is proposed to the customer\na.Periodic cadence with delivery team to:\nProvide them with customer feedback/ inputs on the proposed solution\nReview the test cases to check 100% coverage of customer requirements\nConduct root cause analysis to understand the proposed solution/ demo/ prototype before sharing it with the customer\nDeploy and facilitate new change requests to cater to customer needs and requirements\nSupport QA team with periodic testing to ensure solutions meet the needs of businesses by giving timely inputs/feedback\nConduct Integration Testing and User Acceptance demos testing to validate implemented solutions and ensure 100% success rate\nUse data modelling practices to analyze the findings and design, develop improvements and changes\nEnsure 100% utilization by studying systems capabilities and understanding business specifications\nStitch the entire response/ solution proposed to the RFP/ RFI before its presented to the customer\nb.Support Project Manager/ Delivery Team in delivering the solution to the customer\nDefine and plan project milestones, phases and different elements involved in the project along with the principal consultant\nDrive and challenge the presumptions of delivery teams on how will they successfully execute their plans\nEnsure Customer Satisfaction through quality deliverable on time\n\n\n\n3.Build domain expertise and contribute to knowledge repository\nEngage and interact with other BAs to share expertise and increase domain knowledge across the vertical\nWrite whitepapers/ research papers, point of views and share with the consulting community at large\nIdentify and create used cases for a different project/ account that can be brought at Wipro level for business enhancements\nConduct market research for content and development to provide latest inputs into the projects thereby ensuring customer delight\n\n\n\nDeliver\n\nNo.Performance ParameterMeasure1.Customer Engagement and Delivery ManagementPCSAT, utilization % achievement, no. of leads generated from the business interaction, no. of errors/ gaps in documenting customer requirements, feedback from project manager, process flow diagrams (quality and timeliness), % of deal solutioning completed within timeline, velocity generated.2.Knowledge ManagementNo. of whitepapers/ research papers written, no. of user stories created, % of proposal documentation completed and uploaded into knowledge repository, No of reusable components developed for proposal during quarter\nMandatory Skills: Business Analysis. Experience: 8-10 Years.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Business analysis', 'business integration', 'client engagement', 'data modelling', 'Customer Engagement']",2025-06-12 06:22:05
Teamcenter Analyst,Tata Technologies,2 - 5 years,Not Disclosed,['Bengaluru'],"Sustainability Tool Implementation (Siemens TcPCM), Carbon footprint, Recycled content, Know-How on Scope 1, 2 and 3 emissions, Water and Waste ManagementConsulting awarenessCost and Carbon balanceCompliance management exposureSustainability Tool Implementatio n (Siemens TcPCM), Carbon footprint, Recycled content, Know-How on Scope 1, 2 and 3 emissions, Water and Waste ManagementConsulting awarenessCost and Carbon balanceCompliance management exposureSustainability Tool Implementation (Siemens TcPCM), Carbon footpri nt, Recycled content, Know-How on Scope 1, 2 and 3 emissions, Water and Waste ManagementConsulting awarenessCost and Carbon balanc eCompliance management exposure",Industry Type: Building Material (Cement),Department: Research & Development,"Employment Type: Full Time, Permanent","['Siemens TcPCM', 'Waste Management', 'Compliance management', 'Recycled content', 'Sustainability Tool Implementation', 'Carbon footprint']",2025-06-12 06:22:08
Lead Analytics Consultant,Wells Fargo,5 - 10 years,Not Disclosed,['Hyderabad'],"locationsHyderabad, India\nposted onPosted Yesterday\njob requisition idR-446112\nAbout this role:\nWells Fargo is seeking a Lead Analytics Consultant People Analytics. As a consultant, you will work as analytics professional in HR People Analytics and Business Insights delivery team and will be responsible for effective delivery of projects as per the business priority. The incumbent is expected to be an expert into executive summary, people strategy, HR consulting, HR advisory, advanced analytics & data science and value addition to the projects.",,,,"['Data Analytics', 'Data science tools', 'product lifecycle', 'SAS programming', 'ETL development', 'Alteryx', 'Data Management', 'Tableau Prep', 'SQL']",2025-06-12 06:22:11
Senior Analytics Consultant,Wells Fargo,4 - 9 years,Not Disclosed,['Hyderabad'],"About this role:\nWells Fargo is seeking a Senior Analytics Consultant with a proven track record of success preferably in the banking industry.\n\nIn this role, you will:\nConsult, review and research moderately complex business, operational, and technical challenges that require an in-depth evaluation of variable data factors",,,,"['data manipulation', 'Data Engineering', 'data analysis', 'data management', 'SQL']",2025-06-12 06:22:13
Business Analyst,CGI,6 - 11 years,Not Disclosed,['Hyderabad'],"Business Data Analyst - HealthCare\nPosition Description\nJob Summary\nWe are seeking an experienced and results-driven Business Data Analyst with 5+ years of hands-on experience in data analytics, visualization, and business insight generation. This role is ideal for someone who thrives at the intersection of business and datatranslating complex data sets into compelling insights, dashboards, and strategies that support decision-making across the organization.\nYou will collaborate closely with stakeholders across departments to identify business needs, design and build analytical solutions, and tell compelling data stories using advanced visualization tools.\nKey Responsibilities\nData Analytics & Insights Analyze large and complex data sets to identify trends, anomalies, and opportunities that help drive business strategy and operational efficiency.\n• Dashboard Development & Data Visualization Design, develop, and maintain interactive dashboards and visual reports using tools like Power BI, Tableau, or Looker to enable data-driven decisions.\n• Business Stakeholder Engagement Collaborate with cross-functional teams to understand business goals, define metrics, and convert ambiguous requirements into concrete analytical deliverables.\n• KPI Definition & Performance Monitoring Define, track, and report key performance indicators (KPIs), ensuring alignment with business objectives and consistent measurement across teams.\n• Data Modeling & Reporting Automation Work with data engineering and BI teams to create scalable, reusable data models and automate recurring reports and analysis processes.\n• Storytelling with Data Communicate findings through clear narratives supported by data visualizations and actionable recommendations to both technical and non-technical audiences.\n• Data Quality & Governance Ensure accuracy, consistency, and integrity of data through validation, testing, and documentation practices.\nRequired Qualifications\nBachelor’s or Master’s degree in Business, Economics, Statistics, Computer Science, Information Systems, or a related field.\n• 5+ years of professional experience in a data analyst or business analyst role with a focus on data visualization and analytics.\n• Proficiency in data visualization tools: Power BI, Tableau, Looker (at least one).\n• Strong experience in SQL and working with relational databases to extract, manipulate, and analyze data.\n• Deep understanding of business processes, KPIs, and analytical methods.\n• Excellent problem-solving skills with attention to detail and accuracy.\n• Strong communication and stakeholder management skills with the ability to explain technical concepts in a clear and business-friendly manner.\n• Experience working in Agile or fast-paced environments.\nPreferred Qualifications\nExperience working with cloud data platforms (e.g., Snowflake, BigQuery, Redshift).\n• Exposure to Python or R for data manipulation and statistical analysis.\n• Knowledge of data warehousing, dimensional modeling, or ELT/ETL processes.\n• Domain experience in Healthcare is a plus.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Healthcare Domain', 'Bigquery', 'Redshift Aws', 'Snowflake', 'Data Analytics', 'Data Visualization', 'Python']",2025-06-12 06:22:16
ETL Developer,Wipro,5 - 8 years,Not Disclosed,['Bengaluru'],"Role Purpose\nThe purpose of this role is to design, test and maintain software programs for operating systems or applications which needs to be deployed at a client end and ensure its meet 100% quality assurance parameters\n\nResponsibilities:\nDesign and implement the data modeling, data ingestion and data processing for various datasets\nDesign, develop and maintain ETL Framework for various new data source\nDevelop data ingestion using AWS Glue/ EMR, data pipeline using PySpark, Python and Databricks.\nBuild orchestration workflow using Airflow & databricks Job workflow\nDevelop and execute adhoc data ingestion to support business analytics.\nProactively interact with vendors for any questions and report the status accordingly\nExplore and evaluate the tools/service to support business requirement\nAbility to learn to create a data-driven culture and impactful data strategies.\nAptitude towards learning new technologies and solving complex problem.\nQualifications:\nMinimum of bachelors degree. Preferably in Computer Science, Information system, Information technology.\nMinimum 5 years of experience on cloud platforms such as AWS, Azure, GCP.\nMinimum 5 year of experience in Amazon Web Services like VPC, S3, EC2, Redshift, RDS, EMR, Athena, IAM, Glue, DMS, Data pipeline & API, Lambda, etc.\nMinimum of 5 years of experience in ETL and data engineering using Python, AWS Glue, AWS EMR /PySpark and Airflow for orchestration.\nMinimum 2 years of experience in Databricks including unity catalog, data engineering Job workflow orchestration and dashboard generation based on business requirements\nMinimum 5 years of experience in SQL, Python, and source control such as Bitbucket, CICD for code deployment.\nExperience in PostgreSQL, SQL Server, MySQL & Oracle databases.\nExperience in MPP such as AWS Redshift, AWS EMR, Databricks SQL warehouse & compute cluster.\nExperience in distributed programming with Python, Unix Scripting, MPP, RDBMS databases for data integration\nExperience building distributed high-performance systems using Spark/PySpark, AWS Glue and developing applications for loading/streaming data into Databricks SQL warehouse & Redshift.\nExperience in Agile methodology\nProven skills to write technical specifications for data extraction and good quality code.\nExperience with big data processing techniques using Sqoop, Spark, hive is additional plus\nExperience in data visualization tools including PowerBI, Tableau.\nNice to have experience in UI using Python Flask framework anglular\n\n\nMandatory Skills: Python for Insights. Experience: 5-8 Years.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['ETL', 'data bricks', 'aws glue', 'amazon ec2', 'python', 'spark', 'glue', 'amazon redshift', 'cloud platforms', 'aws', 'data engineering', 'sql']",2025-06-12 06:22:18
AI/ML framework Staff Engineer,Qualcomm,4 - 9 years,Not Disclosed,['Bengaluru'],"Job Area: Engineering Group, Engineering Group > Systems Engineering\n\nGeneral Summary:\n\nLooking for ""ML framework and AI compiler Engineer"" responsible for\nDesigning, implementing, and deploying machine learning models using PyTorch\nFocusing on backend infrastructure and system architecture.\nResponsibilities often include developing framework, integrating with other AI tools, and ensuring scalability and reliability.\n\nHere's a more detailed breakdown of what you might see in such a job description:\n\nKey Responsibilities:\n\n\nModel Development and DeploymentDesigning, building, and deploying AI models, particularly those leveraging PyTorch for deep learning.\n\n\nBackend InfrastructureDeveloping and maintaining the backend systems that power AI applications, including data ingestion, processing, and storage.\n\n\nSystem ArchitectureDesigning scalable and high-performance backend architectures to handle AI workloads.\n\n\nModel OptimizationOptimizing model performance for speed, accuracy, and resource efficiency.\n\n\nIntegrationIntegrating AI models with other systems and applications.\n\n\nAPI DevelopmentCreating and maintaining APIs for communication between frontend and backend components.\n\n\nData HandlingManaging data ingestion, preprocessing, and storage for AI training and inference.\n\n\nCollaborationWorking with data scientists, product managers, and other engineers to bring AI solutions to life.\n\nTools, Technologies, Skills and Programming:\n\n\nC, C++: Strong programming capability using advanced techniques to design and develop AI compilers and backends.\n\n\nScripting: Strong expertise in Python with design, develop, release and maintain projects.\n\n\nAI Frameworks: Familiarity with other AI frameworks like PyTorch, TensorFlow, Hugging Face, etc.\n\n\nMachine Learning Knowledge: Understanding of machine learning principles and algorithms starting Computer vision to large language models and continuously update to new trends.\nExpertise to deep learning accelerator programming (GPU, NPU). Any parallel programming experience (Like CUDA, OpenCL, MKLDNN ..etc) is a plus.\nExperience with deep leaning compilers like Glow, TVM ""etc is a plus.\n\n\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 4+ years of Systems Engineering or related work experience.\nOR\nMaster's degree in Engineering, Information Systems, Computer Science, or related field and 3+ years of Systems Engineering or related work experience.\nOR\nPhD in Engineering, Information Systems, Computer Science, or related field and 2+ years of Systems Engineering or related work experience.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'machine learning', 'tensorflow', 'pytorch', 'system engineering', 'c#', 'cuda', 'algorithms', 'c++', 'parallel programming', 'artificial intelligence', 'opencl', 'deep learning', 'java', 'product management', 'computer vision', 'asp.net', 'multithreading', 'mvc', 'ml']",2025-06-12 06:22:20
Python Engineer - ML/Big Query - Hyd/Chennai/Bangalore,People staffing Solutions,5 - 10 years,12-20 Lacs P.A.,"['Hyderabad', 'Chennai', 'Bengaluru']","Key Responsibilities:\nDesign, develop, and maintain scalable and optimized ETL pipelines using Python and SQL.\nWork with Google BigQuery and other cloud-based platforms to build data warehousing solutions.\nDevelop and deploy ML models; collaborate with Data Scientists for productionizing models.\nWrite efficient and optimized SQL queries for large-scale data processing.\nBuild APIs using Flask/Django for machine learning and data applications.\nWork with both SQL and NoSQL databases including Elasticsearch.\nImplement data ingestion using batch and streaming technologies.\nEnsure data quality, integrity, and governance across the data lifecycle.\nAutomate and optimize CI/CD pipelines for data solutions.\nCollaborate with cross-functional teams to gather data requirements and deliver solutions.\nTroubleshoot and monitor data pipelines for seamless operations.\nRequired Skills & Qualifications:\nBachelor's or Master's degree in Computer Science, Engineering, or related field.\n5+ years of experience with Python in a data engineering and/or ML context.\nStrong hands-on experience with SQL, BigQuery, and cloud data platforms (preferably GCP).\nPractical knowledge of ML concepts and experience developing ML models.\nProficiency in frameworks such as Flask and Django.\nExperience with NoSQL databases and data streaming technologies.\nSolid understanding of data modeling, warehousing, and ETL frameworks.\nFamiliarity with CI/CD tools and automation best practices.\nExcellent communication, problem-solving, and collaboration skills.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Django', 'Machine Learning', 'Python', 'SQL', 'Pandas', 'Numpy', 'Ml', 'Flask']",2025-06-12 06:22:22
AI Test Lead,Naukri,8 - 13 years,20-32.5 Lacs P.A.,['Bengaluru'],"Role & responsibilities\nKey Responsibilities:\nAI Testing Strategy and Planning\nCollaborate with cross-functional teams to develop comprehensive AI testing strategies and plans for AI-powered applications.\nWork closely with product managers, data scientists, and developers to understand AI model requirements, use cases, and project goals.\nDefine the scope and objectives of AI testing efforts, including performance, accuracy, bias detection, and robustness of AI models. Test Execution for AI Models and Algorithms\nDesign, develop, and execute test cases for AI systems and models (including machine learning and deep learning algorithms).\nTest and validate AI solutions across various stages of the development lifecycle, including model training, testing, and deployment.\nEnsure that AI models meet business requirements and perform accurately under various real-world conditions.\nEvaluate the performance of AI models by assessing speed, efficiency, scalability, and resource utilization.\nPerform manual and automated testing on AI-based applications, platforms, and solutions.\nAI Model Accuracy and Validation\nTest AI models for accuracy, precision, recall, F1 score, and other performance metrics.\nEnsure AI models' fairness by conducting tests for potential bias in decisionmaking processes, especially in clinical or medical applications.\nValidate AI model predictions against real-world data, ensuring that results are consistent, reliable, and actionable. Also, need to look the test results from a business perspective and help evaluate the balance between risks and benefits.\nCollaboration and Knowledge Sharing\nWork with data scientists, AI engineers and Test Manager to improve testing methodologies and continuously optimize AI model testing processes.\nProvide feedback on AI models, pointing out any potential improvements in testing coverage or areas for model retraining.\nCommunicate findings, bugs, and issues related to AI models to technical teams, ensuring prompt resolution.\nHelp the team set up AI Testing standards, make informed decisions, and build knowledge across projects\nHelp the team in decision-making processes, such as whether to continue or stop investments based on testing results. Test Automation for AI Projects\nDevelop and implement automated testing scripts and frameworks specifically designed for AI applications.\nUtilize AI testing tools and frameworks (RAGAS etc.) to automate the validation of AI models and algorithms.\nIntegrate automated AI testing within continuous integration and continuous deployment (CI/CD) pipelines.\nCompliance and Regulatory Testing\nEnsure that AI applications comply with industry-specific regulations, especially in the pharma and healthcare sectors (e.g., FDA regulations, HIPAA compliance).\nVerify that all AI-driven processes adhere to ethical standards and data privacy laws.\nContinuous Improvement and Research\nStay up-to-date with the latest trends, tools, and techniques in AI testing and apply these advancements to optimize the testing process.\nParticipate in AI testing forums and workshops, contributing insights to improve best practices within the team. Reporting and Documentation\nDocument test results, methodologies, and issues clearly, providing insights into test coverage, risk analysis, and performance benchmarks.\nPrepare detailed reports for both technical and non-technical stakeholders, summarizing testing outcomes and potential risks associated with AI implementations.\nAssist in the creation and maintenance of knowledge-sharing platforms related to AI testing best practices.\nKey Skills and Qualifications:\nTechnical Expertise\nStrong knowledge of AI/ML testing methodologies and best practices.\nExperience with any AI development frameworks and libraries such as TensorFlow, Keras, PyTorch, scikit-learn, RAGAS and MLlib.\nExperience in testing tools and environments for AI-based systems (e.g., Jupyter Notebooks, Apache Spark, and DataRobot).\nExperience with performance testing tools like Grafana K6 and JMeter for AI solutions.\nKnowledge of Python (Must to have), R, JavaScript or other programming languages frequently used in AI/ML.\nKnowledge of cloud technologies like Microsoft Azure / AWS.\nUnderstanding of test automation frameworks and experience in tools like Cypress, Playwright and Pytest for automating AI tests. AI Model Evaluation\nSolid understanding of machine learning and deep learning models, including supervised and unsupervised learning techniques.\nFamiliarity with evaluating AI models on metrics such as accuracy, precision, recall, F1 score, confusion matrices, and AUC.\nAbility to identify and test for model biases, fairness, and ethical implications, especially in sensitive applications like healthcare and pharma. Analytical and Problem-Solving Skills\nStrong problem-solving abilities and keen attention to detail, with a systematic approach to diagnosing and resolving AI-related issues.\nAbility to perform root cause analysis of issues in AI algorithms and suggest actionable fixes.\nCollaboration and Communication\nExcellent teamwork and communication skills, with the ability to collaborate with cross-functional teams, including data scientists, engineers, and product managers.\nStrong verbal and written communication skills to convey technical information clearly and concisely to both technical and non-technical stakeholders.\nExperience\nMinimum of 8+ years experience in software testing, with at least 2 years focused on testing AI/ML models or AI-based applications.\nProven experience in testing AI/ML algorithms in production or staging environments.\nExperience in testing Visual AI Assistant Applications is good to have.\nExperience working in a regulated industry (such as pharmaceuticals or healthcare) is a plus.\nPreferred Qualifications:\nExperience with cloud platforms (e.g., AWS, Azure) for deploying AI applications and models. Certification in AWS/Azure will be good to have.\nFamiliarity with DevOps practices and integrating AI testing into CI/CD pipelines.\nCertification in AI/ML or related testing frameworks (e.g. ISTQB AI Tester)\nThis AI Tester role is a unique opportunity to shape the future of AI in the pharmaceutical industry. If youre passionate about AI, testing, and making a difference in healthcare, we encourage you to apply.\n\nPreferred candidate profile",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Automation Testing', 'AI/ ML', 'Python', 'Performance Testing', 'Automation Strategy', 'AI Framework', 'AI testing']",2025-06-12 06:22:25
RAG Architect,Qualcomm,13 - 18 years,Not Disclosed,['Bengaluru'],"Job Area: Engineering Group, Engineering Group > Software Test Engineering\n\nGeneral Summary:\n\nJob description\n\nWe are seeking an experienced AI Architect to design, develop, and deploy Retrieval-Augmented Generation (RAG) solutions for Qualcomm Cloud AI Platforms.\n\nRoles and Responsibilities\nLead the design and development of applications for RAG AI models and provide APIs for frontend consumption. Manage the interaction between retrieval-augmented techniques and generative models.\nBuild services that connect AI models (e.g., transformers, embeddings, and vector search) to handle tasks such as query retrieval, model inference, and generating responses. Leverage frameworks like Flask, FastAPI, or Django for API development.\nDesign pipelines to preprocess, clean, and prepare data for AI model training, as well as for serving the models in production environments. Optimize these pipelines to support both batch and real-time data processing. Implement RESTful APIs or GraphQL endpoints for seamless frontend-backend interaction.\nImplement cloud solutions to host Python-based services, ensuring that AI models are scalable and that the infrastructure can handle high traffic. Leverage containerization (Docker) and orchestration (Kubernetes) for model deployment and management.\nSet up monitoring, logging, and alerting for Python backend services, ensuring smooth operation of AI features. Use tools like Prometheus, Grafana, and ELK stack for real-time performance tracking.\nContinuously optimize model performance by fine-tuning and adapting Python-based AI models for real-time use cases. Manage trade-offs between computation load, response time, and quality of generated content.\nPartner with data scientists, machine learning engineers, and mobile/web developers to ensure tight integration between AI models, mobile/web front-end, and backend infrastructure.\n\n- Experience:\n13+ years of overall SW development experience\n10+ years Strong experience in working with technologies (e.g., React, React Native, Flutter, Django, Flask, FastAPI).\n5+ years of experience in building AI applications with a focus on NLP, machine learning, generative models, and retrieval-augmented systems.\nProven experience in designing and deploying AI systems that integrate retrieval-based techniques (e.g., FAISS, Weaviate) and generative models (e.g., GPT, BERT). - Expertise in cloud platforms (e.g., AWS, GCP, Azure) and deployment of Python-based microservices.\nBuilding RESTful APIs or GraphQL services (using frameworks like Flask, FastAPI, or Django).\nHandling AI model inference and data processing (using libraries like NumPy, Pandas, TensorFlow, PyTorch, and Hugging Face Transformers).\nIntegrating vector search solutions (e.g., FAISS, Pinecone, Weaviate) with the AI models for efficient retrieval-augmented generation. - Experience with containerization (Docker) and Kubernetes for deploying scalable Python-based services.\nProficient in cloud infrastructure management, with a focus on managing Python services in the cloud.\nExperience in End-to-End product development and Software Lifecycle\n\n\nKey\n\nSkills:\n\nAdvanced proficiency in Python for building backend services and data processing pipelines. Familiarity with frameworks like Flask, Django, and FastAPI. Experience with AI libraries and frameworks (TensorFlow, PyTorch, Hugging Face Transformers).\nFamiliarity with vector databases (e.g., Pinecone, FAISS, Weaviate) and integration with retrieval-augmented systems.\nStrong knowledge of RESTful API design, GraphQL, and API security best practices (e.g., OAuth, JWT).\nExcellent problem-solving abilities and a strong focus on creating highly scalable and performant solutions.\nStrong communication skills, with the ability to collaborate across different teams and geography\nAbility to mentor junior team members and lead technical discussions.\n\n\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 6+ years of Software Test Engineering or related work experience.\nOR\nMaster's degree in Engineering, Information Systems, Computer Science, or related field and 5+ years of Software Test Engineering or related work experience.\nOR\nPhD in Engineering, Information Systems, Computer Science, or related field and 4+ years of Software Test Engineering or related work experience.\n\n2+ year of work experience with Software Test or System Test, developing and automating test plans, and/or tools (e.g., Source Code Control Systems, Continuous Integration Tools, and Bug Tracking Tools).",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'data processing', 'cloud platforms', 'api', 'graphql', 'natural language processing', 's w development', 'rest api design', 'system testing', 'react native', 'machine learning', 'pipeline', 'react.js', 'flutter', 'test engineering', 'django', 'cloud infrastructure management', 'flask']",2025-06-12 06:22:27
AutoIT Solutioning Engineer-Staff,Qualcomm,3 - 7 years,Not Disclosed,['Bengaluru'],"Job Area: Engineering Group, Engineering Group > Software Engineering\n \n\nQualcomm Overview: \nQualcomm is a company of inventors that unlocked 5G ushering in an age of rapid acceleration in connectivity and new possibilities that will transform industries, create jobs, and enrich lives. But this is just the beginning. It takes inventive minds with diverse skills, backgrounds, and cultures to transform 5Gs potential into world-changing technologies and products. This is the Invention Age - and this is where you come in.\n\nGeneral Summary:\n\nAs a Site Reliability Engineer (SRE), youll be part of a highly collaborative team focused on provisioning and maintaining infrastructure and services with stability, sustainability, and security always on your mind. You will work in a self-guided, cross-functional team responsible for everything from modernizing traditional services and applications to deploying new technology. You'll collaborate closely with software engineers, data scientists, and product managers to maintain and optimize our systems. If you're passionate about automotive technology, software reliability, and continuous improvement, this role is perfect for you.\n\nYour Guiding Principles:\n\n\nAutomationYou understand the power of automation and ""infrastructure as code"" concepts. Automation is your primary consideration in problem-solving.\n\n\nCollaboration: You share a common language with fellow engineers, understand their needs, and thrive working in a high trust collaborate culture in which people are rewarded for taking risks.\n\n\nData-drivenYou understand why decisions are supported by facts and not opinions. You have experience applying logical approach to decision making. Skilled at metric collection and using that data to drive change.\n\n\nDebuggingYou understand debugging principles and are adept at applying them routinely and successfully.\n\n\nDevSecOps: You understand that DevSecOps is a culture which needs to be cultivated and you can help nurture those philosophies.\n\n\nSecurityYou know how to layer appropriate security within solutions across the lifecycle. You understand the security implications and consequences of any deployment.\n\n\nSelf-Driven: You understand how to prioritize work and time allocation at a personal and team level.\n\n\nStability: You know what it means to deliver a service with a high degree of reliability and are intimately familiar with how disruptions impact consumers.\n\n\nSustainability: You avoid one off solutions which are challenging to support. Instead, your solutions are aligned with team goals and strategic vision. You routinely dedicate cycles to reducing technical debt.\n\n\nWhat you have:\nExtensive Linux experience with servers and workstations. You can easily navigate the CLI, knowledgeable with typical Linux troubleshooting tools, and have a broad understanding of Ubuntu and RedHat.\nThe ability to automate through scripting languages such as Python, Bash, Go, etc.\nThe skill to provide sufficient automated test coverage of various implementations.\nYou have familiarity with Jenkins, Puppet, Splunk, JIRA, Vault, Docker, AWS, Cloud services, etc.\nAbility to respond rapidly to changing landscapes while providing stable, reliable, and secure services to customers.\nYou have a passion for continuous learning and leverage the scientific method to ensure nothing is taken for granted.\n\n\nResponsibilities:\nSystem Monitoring and Incident Response:\nMonitor system health, detect anomalies, and respond promptly to incidents.\nInvestigate and troubleshoot issues related to services.\nImplement proactive measures to prevent service disruptions.\nInfrastructure Automation:\nDevelop and maintain infrastructure-as-code (IaC) scripts for deployment and scaling.\nAutomate routine tasks to improve efficiency and reduce manual intervention.\nPerformance Optimization:\nCollaborate with development teams to optimize software performance.\nIdentify bottlenecks and implement solutions to enhance system speed and reliability.\nCapacity Planning:\nForecast resource requirements based on traffic patterns and business growth.\nScale infrastructure to accommodate increasing demand.\nSecurity and Compliance:\nEnsure compliance with industry standards and best practices.\nImplement security controls and participate in security audits.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['docker', 'linux', 'python', 'puppet', 'aws', 'kubernetes', 'owasp', 'golang', 'redhat linux', 'vulnerability assessment', 'ansible', 'microservices', 'java', 'devops', 'jenkins', 'debugging', 'penetration testing', 'vault', 'jira', 'cloud services', 'ubuntu', 'microsoft azure', 'splunk', 'bash', 'devsecops', 'terraform']",2025-06-12 06:22:30
"Senior Engineer, Application Development",S&P Global Market Intelligence,5 - 8 years,Not Disclosed,['Hyderabad'],"Grade Level (for internal use):\n10\nMarket Intelligence\nThe Role: Senior Full Stack Developer\nGrade level :10\nThe Team: You will work with a team of intelligent, ambitious, and hard-working software professionals. The team is responsible for the architecture, design, development, quality, and maintenance of the next-generation financial data web platform. Other responsibilities include transforming product requirements into technical design and implementation. You will be expected to participate in the design review process, write high-quality code, and work with a dedicated team of QA Analysts, and Infrastructure Teams\nThe Impact: Market Intelligence is seeking a Software Developer to create software design, development, and maintenance for data processing applications. This person would be part of a development team that manages and supports the internal & external applications that is supporting the business portfolio. This role expects a candidate to handle any data processing, big data application development. We have teams made up of people that learn how to work effectively together while working with the larger group of developers on our\nplatform.\nWhats in it for you:\nOpportunity to contribute to the development of a world-class Platform Engineering team .\nEngage in a highly technical, hands-on role designed to elevate team capabilities and foster continuous skill enhancement.\nBe part of a fast-paced, agile environment that processes massive volumes of dataideal for advancing your software development and data engineering expertise while working with a modern tech stack.\nContribute to the development and support of Tier-1, business-critical applications that are central to operations.\nGain exposure to and work with cutting-edge technologies including AWS Cloud , EMR and Apache NiFi .\nGrow your career within a globally distributed team , with clear opportunities for advancement and skill development.\nResponsibilities:\nDesign and develop applications, components, and common services based on development models, languages and tools, including unit testing, performance testing and monitoring and implementation\nSupport business and technology teams as necessary during design, development and delivery to ensure scalable and robust solutions\nBuild data-intensive applications and services to support and enhance fundamental financials in appropriate technologies.( C#, .Net Core, Databricsk, Spark ,Python, Scala, NIFI , SQL)\nBuild data modeling, achieve performance tuning and apply data architecture concepts\nDevelop applications adhering to secure coding practices and industry-standard coding guidelines, ensuring compliance with security best practices (e.g., OWASP) and internal governance policies.\nImplement and maintain CI/CD pipelines to streamline build, test, and deployment processes; develop comprehensive unit test cases and ensure code quality\nProvide operations support to resolve issues proactively and with utmost urgency\nEffectively manage time and multiple tasks\nCommunicate effectively, especially written with the business and other technical groups\nWhat Were Looking For:\nBasic Qualifications:\nBachelorsMasters Degree in Computer Science, Information Systems or equivalent.\nMinimum 5 to 8 years of strong hand-development experience in C#, .Net Core, Cloud Native, MS SQL Server backend development. Proficiency with Object Oriented Programming.\nAdvance SQL programming skills\nPreferred experience or familiarity with tools and technologies such as Odata, Grafana, Kibana, Big Data platforms, Apache Kafka, GitHub, AWS EMR, Terraform, and emerging areas like AI/ML and GitHub Copilot.\nHighly recommended skillset in Databricks, SPARK, Scalatechnologies.\nUnderstanding of database performance tuning in large datasets\nAbility to manage multiple priorities efficiently and effectively within specific timeframes\nExcellent logical, analytical and communication skills are essential, with strong verbal and writing proficiencies\nKnowledge of Fundamentals, or financial industry highly preferred.\nExperience in conducting application design and code reviews\nProficiency with following technologies:\nObject-oriented programming\nPrograming Languages (C#, .Net Core)\nCloud Computing\nDatabase systems (SQL, MS SQL)\nNice to have: No-SQL (Databricks, Spark, Scala, python), Scripting (Bash, Scala, Perl, Powershell)\nPreferred Qualifications:\nHands-on experience with cloud computing platforms including AWS , Azure , or Google Cloud Platform (GCP) .\nProficient in working with Snowflake and Databricks for cloud-based data analytics and processing.\nBenefits:\nHealth & Wellness: Health care coverage designed for the mind and body.\nContinuous Learning: Access a wealth of resources to grow your career and learn valuable new skills.\nInvest in Your Future: Secure your financial future through competitive pay, retirement planning, a continuing education program with a company-matched student loan contribution, and financial wellness programs.\nFamily Friendly Perks: Its not just about you. S&P Global has perks for your partners and little ones, too, with some best-in class benefits for families.\nBeyond the Basics: From retail discounts to referral incentive awardssmall perks can make a big difference.",Industry Type: Banking,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['GitHub Copilot', 'AI/ML', 'Kibana', 'python', 'GitHub', 'Scala', 'AWS EMR', 'Grafana', 'Odata', 'Big Data platforms', 'Terraform', 'Apache Kafka', 'Databricks', 'Spark']",2025-06-12 06:22:32
Manager-Business Analyst,Jubilant FoodWorks (JFL),8 - 12 years,Not Disclosed,['Noida'],"The IT Business Analyst is responsible for bridging business needs and IT capabilities, ensuring that technology solutions align with strategic objectives. This role involves analysing business processes including SAP, gathering requirements, and collaborating with IT teams to develop effective solutions. The ideal candidate has strong analytical skills, deep understanding of both business processes preferably Finance background along with IT systems including SAP, and experience in project management.\n\nKey Responsibilities:\n\nBusiness Requirements Gathering\nWork with business stakeholders to identify needs, pain points, and opportunities for process improvement.\nDocument business requirements in clear, detailed formats for technical teams.\n\nSolution Design and Analysis\nAnalyze and evaluate technology solutions that best align with business requirements.\nCreate functional specifications, use cases, and workflow diagrams to communicate solutions effectively.\n\nProject Support and Coordination\nCollaborate with project managers to ensure that projects meet business goals and timelines.\nTrack progress and provide updates on requirements, ensuring adherence to project scope and budget.\n\nTesting and Quality Assurance\nDevelop test cases and participate in system testing to validate that requirements are met.\nAssist in User Acceptance Testing (UAT) and ensure successful project delivery.\n\nContinuous Improvement and Documentation\nRecommend improvements to business processes based on data analysis.\nMaintain and update documentation for requirements, processes, and system changes.\n\nPreferred qualification & skills\n\nBachelors degree preferably in Finance Business, IT, or a related field.\n8+ years of experience in business analysis or a similar role. Experience of Finance & HR domain is preferred under QSR Industry.\nStrong analytical and problem-solving skills, with experience in requirements gathering and process improvement.\nExcellent communication skills, with the ability to work cross-functionally.\nFamiliarity with project management methodologies (Agile, Waterfall).\nKnowledge of data analysis and ERP preferably SAP, CRM, or other business applications.",Industry Type: Hotels & Restaurants,Department: Project & Program Management,"Employment Type: Full Time, Permanent","['User Acceptance Testing', 'SAP', 'Digital Transformation', 'Business Transformation', 'Digitization', 'Process Improvement']",2025-06-12 06:22:35
Staff System Test Engineer,Qualcomm,8 - 13 years,Not Disclosed,['Bengaluru'],"Job Area: Engineering Group, Engineering Group > Systems Test Engineering\n\nGeneral Summary:\n\n\n\nWe are seeking a Senior Staff AI System-Level Test Engineer to lead end-to-end testing of Retrieval-Augmented Generation (RAG) AI systems for Hybrid, Edge-AI Inference solutions. This role will focus on designing, developing, and executing comprehensive test strategies for evaluating the reliability, accuracy, usability and scalability of large-scale AI models integrated with external knowledge retrieval systems.\n\nThe ideal candidate needs to have deep expertise in AI testing methodologies, experience with large language models (LLMs), expertise in building test solutions for AI Inference stacks, RAG, search/retrieval architecture, and a strong background in automation frameworks, performance validation, and building E2E automation architecture.\n\nExperience testing large-scale generative AI applications, familiarity with LangChain, LlamaIndex, or other RAG-specific frameworks, and knowledge of adversarial testing techniques for AI robustness are preferred qualifications\n\nKey Responsibilities:\n\nTest Strategy & Planning\nDefine end-to-end test strategies for RAG, retrieval, generation, response coherence, and knowledge correctness\nDevelop test plans & automation frameworks to validate system performance across real-world scenarios.\nHands-on experience in benchmarking and optimizing Deep Learning Models on AI Accelerators/GPUs\nImplement E2E solutions to integrate Inference systems with customer software workflows\nIdentify and implement metrics to measure retrieval accuracy, LLM response quality\n\n\nTest Automation\nBuild automated pipelines for regression, integration, and adversarial testing of RAG workflows.\nValidate search relevance, document ranking, and context injection into LLMs using rigorous test cases.\nCollaborate with ML engineers and data scientists to debug model failures and identify areas for improvement.\nConduct scalability and latency tests for retrieval-heavy applications. Analyze failure patterns, drift detection, and robustness against hallucinations and misinformation.\n\n\nCollaboration\nWork closely with AI research, engineering teams & customer teams to align testing with business requirements.\nGenerate test reports, dashboards, and insights to drive model improvements.\nStay up to date with the latest AI testing frameworks, LLM evaluation benchmarks, and retrieval models.\n\n\nRequired Qualifications:\n8+ years of experience in AI/ML system testing, software quality engineering, or related fields.\nBachelors or masters degree in computer science engineering/ data science / AI/ML\nHands-on experience with test automation frameworks (e.g., PyTest, Robot Framework, JMeter).\nProficiency in Python, SQL, API testing, vector databases (e.g., FAISS, Weaviate, Pinecone) and retrieval pipelines.\nExperience with ML model validation metrics (e.g., BLEU, ROUGE, MRR, NDCG).\nExpertise in CI/CD pipelines, cloud platforms (AWS/GCP/Azure), and containerization (Docker, Kubernetes).\n\n\nWhy Join Us\nWork on cutting-edge AI retrieval-augmented generation technologies\nCollaborate with world-class AI researchers and engineers.\n\nIf you are passionate about AI system testing and ensuring the reliability of next-generation generative models, apply now!\n\nMinimum Qualifications:\nBachelor's degree in Engineering, Information Systems, Computer Science, or related field and 6+ years of Systems Test Engineering or related work experience.\nOR\nMaster's degree in Engineering, Information Systems, Computer Science, or related field and 5+ years of Systems Test Engineering or related work experience.\nOR\nPhD in Engineering, Information Systems, Computer Science, or related field and 4+ years of Systems Test Engineering or related work experience.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['automation framework', 'continuous integration', 'python', 'sql', 'ci cd pipeline', 'kubernetes', 'ci/cd', 'cloud platforms', 'software quality', 'artificial intelligence', 'docker', 'test engineering', 'quality engineering', 'e2e', 'testing methodologies', 'vector', 'aws', 'api testing']",2025-06-12 06:22:38
Senior MSBI Developer (SQL & SSRS/SSIS Expert),Synechron,8 - 10 years,Not Disclosed,"['Pune', 'Hinjewadi']","job requisition idJR1027513\n\nJob Summary\nSynechron is seeking an experienced and detail-oriented Senior MSBI Developerexpertise in MSBI (Microsoft Business Intelligence) to join our data and analytics team. In this role, you will contribute to designing, developing, and maintaining robust reporting and data integration solutions that support our business objectives. Your expertise will help deliver actionable insights, improve decision-making processes, and enhance overall data management efficiency within the organization.\n\nSoftware\n\nRequired\n\nSkills:\nMSBI Suite (including SSIS, SSRS, SSAS)\nSQL Server (including SQL Server Management Studio and Query Performance Tuning)\nVersionsRecent versions of SQL Server (2016 or later preferred)\nProven experience in creating complex reports, data transformation, and integration workflows\nPreferred\n\nSkills:\nPower BI or other visualization tools\nExperience with cloud-based data solutions (e.g., Azure SQL, Synapse Analytics)\nOverall Responsibilities\nDevelop, implement, and maintain MSBI solutions such as SSIS packages, SSRS reports, and data models to meet business requirements\nCollaborate with business stakeholders and data teams to gather reporting needs and translate them into scalable solutions\nOptimize and troubleshoot existing reports and data pipelines to improve performance and reliability\nEnsure data accuracy, security, and compliance within reporting processes\nDocument solution architectures, workflows, and processes for ongoing support and knowledge sharing\nParticipate in team initiatives to enhance data governance and best practices\nContribute to strategic planning for data platform evolution and modernization\nTechnical Skills (By Category)\n\nProgramming Languages:\nRequiredSQL (Advanced proficiency in query writing, stored procedures, and performance tuning)\nPreferredT-SQL scripting for data transformations and automation\nDatabases / Data Management:\nRequiredDeep knowledge of relational database concepts with extensive experience in SQL Server databases\nPreferredFamiliarity with data warehouse concepts, OLAP cubes, and data mart design\nCloud Technologies:\nDesiredBasic understanding of cloud-based data platforms like Azure Data Factory, Azure Synapse\nFrameworks and Libraries:\nNot directly applicable, focus on MSBI tools\nDevelopment Tools and Methodologies:\nExperience working within Agile development environments\nData pipeline development and testing best practices\nSecurity Protocols:\nImplement data security measures, role-based access controls, and ensure compliance with data privacy policies\nExperience\n8 to 10 years of professional experience in software development with substantial hands-on MSBI expertise\nDemonstrated experience in designing and deploying enterprise-level BI solutions\nDomain experience in finance, healthcare, retail, or similar industries is preferred\nAlternative candidacyExtensive prior experience with BI tools and proven success in similar roles may be considered in lieu of exact industry background\nDay-to-Day Activities\nDesign and develop SSIS data integration workflows to automate data loading processes\nCreate and optimize SSRS reports and dashboards for various organizational units\nEngage in troubleshooting and resolving technical issues in existing BI solutions\nCollaborate with data architects, developers, and business analysts to align data solutions with business needs\nConduct code reviews, testing, and validation of reports and data pipelines\nParticipate in scrum meetings, planning sessions, and stakeholder discussions\nEnsure documentation of solutions, processes, and workflows for ease of maintenance and scalability\nQualifications\nBachelors degree or equivalent in Computer Science, Information Technology, or related field\nRelevant certifications in Microsoft BI or SQL Server (e.g., Microsoft Certified Data Engineer Associate) preferred\nOngoing engagement in professional development related to BI, data management, and analytics tools\nProfessional Competencies\nAnalytical mindset with strong problem-solving abilities in data solution development\nCapable of working collaboratively across diverse teams and communicating technical concepts effectively\nStakeholder management skills to interpret and prioritize reporting needs\nAdaptability to evolving technologies and continuous learning mindset\nFocus on delivering high-quality, sustainable data solutions with attention to detail\nEffective time management, prioritizing tasks to meet project deadlines",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['performance tuning', 'stored procedures', 'sql scripting', 'query writing', 'msbi', 'sql server database', 'software development', 'ssas', 'power bi', 'microsoft azure', 'data mart design', 'olap cubes', 'sql server', 'sql azure', 'ssrs', 'data warehousing concepts', 'data transformation', 'ssis']",2025-06-12 06:22:40
Ai Ml Engineer,Optum,5 - 10 years,Not Disclosed,['Noida'],"Optum is a global organization that delivers care, aided by technology to help millions of people live healthier lives. The work you do with our team will directly improve health outcomes by connecting people with the care, pharmacy benefits, data and resources they need to feel their best. Here, you will find a culture guided by inclusion, talented peers, comprehensive benefits and career development opportunities. Come make an impact on the communities we serve as you help us advance health optimization on a global scale. Join us to start Caring. Connecting. Growing together.  \nAI Engineer is tasked with the design, development, and deployment of advanced generative AI models and systems. This position requires close collaboration with data scientists, product managers, and other stakeholders to integrate generative AI solutions into existing products and develop new innovative features. Proficiency in the Agentic AI framework is vital for coordinating multiple autonomous AI agents to accomplish complex tasks.\n\nPrimary Responsibilities:\nImplement Generative AI Models: Develop sophisticated generative AI algorithms and models to create new data samples, patterns, or content based on existing data or inputs\nData Processing: Collaborate with stakeholders to preprocess, analyze, and interpret extensive datasets\nModel Deployment: Deploy generative AI models into production environments, ensuring scalability and robustness\nOptimization: Conduct model testing, validation, and optimization to enhance performance\nIntegration: Work with cross-functional teams to seamlessly integrate generative AI solutions into products\nResearch: Stay current with the latest advancements in generative AI technologies and practices\nAgentic AI Framework: Utilize the Agentic AI framework to coordinate multiple AI agents for the completion of complex tasks\nMentorship: Provide mentorship to junior team members and offer technical guidance\nComply with the terms and conditions of the employment contract, company policies and procedures, and any and all directives (such as, but not limited to, transfer and/or re-assignment to different work locations, change in teams and/or work shifts, policies in regards to flexibility of work benefits and/or work environment, alternative work arrangements, and other decisions that may arise due to the changing business environment). The Company may adopt, vary or rescind these policies and directives in its absolute discretion and without any limitation (implied or otherwise) on its ability to do so\n\nRequired Qualifications:\nBachelor's or Master's degree in Computer Science, Engineering, or a related field\n5+ years of experience in software engineering with a focus on AI/ML\nExperience with data preprocessing and analysis\nKnowledge of the Agentic AI framework and its application in AI systems\nProficiency in machine learning frameworks such as TensorFlow and PyTorch\nSolid programming skills in Python, Java, or C++\nFamiliarity with cloud platforms (e.g., AWS, Google Cloud, Azure)\nProven excellent problem-solving abilities and algorithmic thinking\nProven solid communication and teamwork skills\n\nPreferred Qualifications:\nExperience with data processing\nKnowledge of version control systems like Git\nUnderstanding of Generative AI, associated technologies and frameworks like RAG, agents etc.",Industry Type: Software Product,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Agentic Ai', 'Gen AI', 'Cloud', 'RAG', 'LLM']",2025-06-12 06:22:42
Business Analyst,Karur Vysya Bank,2 - 5 years,4-7 Lacs P.A.,['Karur'],"Role & responsibilities\nDrive awareness of requirements across business units and identify substandard systems/ processes through evaluation of real-time data.\nServe as thought leader for technical business processes, developing systems prototypes that promote increased efficiency and productivity on multiple levels.\nCreate and implement precise management plans for every project, with attention to transparent communication at all levels",,,,"['Power Bi', 'Tableau', 'Business Analysis', 'SQL']",2025-06-12 06:22:45
Business Analyst,Netcracker,10 - 16 years,Not Disclosed,"['Hyderabad', 'Pune', 'Gurugram']","Job Description\nGeneral purpose of position:\nEssentially, the work will be around conducting requirement analysis, preparing and tracking delivery of high level and detailed solution designs. It will require candidate to understand and analyze client business operations and work on impact analysis on to-be or already deployed NC product stack. The candidate is expected to provide solution options that deliver value to the client and will be responsible for high quality assurance of his work products.\nSkills and abilities: The incumbent must have the ability and orientation to work on simple to complex solutions with high degree of quality and accuracy.\n       Strong expertise in requirements management processes, including standard tools and techniques such as facilitated meetings, use cases, mapping documents, models and other requirements management tools\n       Applied knowledge of different data interchange formats and Telecom APIs is a must, especially covering the following:\n       Should be able to read JSON, XML structure/documentation.\n       Hands on experience on UML Diagrams like (Sequence, Component Architecture, and ER etc.)\n       Experience in File based, Web-Service based, and Messaging Based Integration.\n       Basic Knowledge of FTP, REST/SOAP, Queue Concepts.\n       Excellent written and verbal communication skills.\n       Process oriented. Ability to learn new processes and willingness to apply the learning.\n       Ability to meet occasionally tight deadlines, proactive and committed to excellence in providing results\n       Highly Self-motivated and able to work independently with limited supervision\n       Basic knowledge and understanding of SQL, object data model\nMinimum Education and Experience:\n       People with experience in Business Analysis in Telecom BSS processes and in IT and not just limited to documentation of functional aspects\n       Graduates and Post graduates with excellent academics.\n       Prior telecom experience is mandatory.\nSkills: E2E Solutioning, eTOM, TMF Open Api, Digital Transformation\n \nRoles and Responsibilities\nMajor duties and responsibilities:\n       Understand end to end telecom processes around Customer acquisition, CIM, POC, Ordering etc (End to End Telecom BSS experience/knowledge required)\n       Understand catalog configuration and Integration processes involved in telecom around Customer Data, Billing and Payments Data, Resource Inventory and provisioning systems.\n       Analyze the integration data sources including the Interface agreements involved in the integration with third-party/external systems.\n       Conduct detailed business analysis and prepare designs for various stages for development and implementation projects as telecom subject matter expert;\n       Consolidate various inputs and data from multiple sources to create and modify solution options.\n       Perform through quality assurance on documents to ensure consistency and quality.\n       Keep in touch with the newest technologies and trends in the telecommunications industry\n       Manage client engagement through written and verbal communication throughout the scoping, analysis and design delivery stages.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['bss', 'api integration', 'data', 'tmf', 'sql', 'requirement gathering', 'uml', 'xml', 'open api', 'writing', 'json', 'e2e', 'telecom', 'api', 'requirement analysis', 'communication skills', 'rest', 'process', 'etom', 'ftp', 'solutioning', 'verbal communication', 'business analysis', 'telecom bss', 'requirements management', 'integration', 'soap', 'object']",2025-06-12 06:22:47
Business Analyst - Demo Engineering,Etech Global Services,2 - 6 years,Not Disclosed,[],"About Etech\nEtech is a leading provider of customer engagement solutions and services utilising inbound and outbound voice as well as web chat. For over a decade, we have been helping companies cost-effectively acquire new customers and maximise profits by servicing and growing existing customers. We are a Tier One preferred provider for Fortune 500 companies and employ roughly 3000+ team members operating in 9 Global Centres (6 in the US, 2 in India, and 1 in Jamaica)\nWe are hiring for one of the top Saas-based reputed clients based in the US which is associated with Etech. What is the client software?\n\nThe client product is a SaaS (Software as a Service) platform to help companies learn about their customers feedback about their products and services. This domain is called CEM (Customer Experience Management).\n\nJob Description:\nAs an analyst, you will first learn a lot about the client platform and how to configure/create a solution for its customers through extensive product training. After that, you will work with client project managers and other team members to build and support our product implementations for large companies. You will utilise your knowledge of technology to think of creative solutions on the client platform.\n\nThe Role:\nAs an Analyst, you will be assigned to multiple projects depending on our needs, your interests, and expertise. The projects can vary from assisting your team in implementing, maintaining, and testing our software for new customers to managing customer accounts post-launch.\n\nRESPONSIBILITIES:\n\nProduct Implementation:\nWork with senior team members to carry out customer implementations and program enhancements.\nParticipate in the implementation design, setup, and review processes\nIdentify improvements to our feedback products and processes\nUtilize Client software knowledge for testing customized software solutions\nClient Management:\nBuild long-standing customer relationships by improving customer feedback programs\nProvide support to client meetings by leveraging in-depth Client system capabilities\nWork with client teams in resolving technical/system-related inquiries\nProvide quality assurance support when providing features to clients\nProvide client support when analysing large sets of data\nQualification Required:\nExcellent analytical skills (including Microsoft Excel) and attention to detail\nStrong written, oral communication and presentation skills\nBachelor's / Master's degree in Computer Application (B.Sc. IT, M.Sc. IT, BCA, MCA, B.Tech Computer Engineering, B.Tech Computer Science etc.)\nExperience working on JavaScript, HTML, CSS, and XML.\nGUI would be an added advantage\nETL Experience / MEC Pre-processor building\nExperience working with customer IT teams for setting up data transfer API calls / SFTP\ntransfers\nAny experience in working with Audio / Speech / Speech to text (in or outside Medallia)\nImplementation experience (not just servicing), if you have Medallia experience\nCustomer facing Experience\nPreferred Candidates:\nExperience in management consulting, IT consulting, market research, and/or enterprise software client management either in college or at work to facilitate teamwork in remote settings will be preferred.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['CSS', 'Javascript', 'HTML', 'API', 'ETL', 'XML']",2025-06-12 06:22:50
Software Engineer II,Chegg,3 - 8 years,Not Disclosed,['New Delhi'],"About the Team\nChegg's engineering team is a group of passionate engineers who, in close collaboration with data scientists, product managers, designers, and other backend developers, build the future of the online education industry. We develop our products to scale and to last, we dont take shortcuts (hello unit tests and documentation), and we take pride in delivering high-quality solutions on time. We are cloud native.\nRole\nWe are looking for software engineers passionate about solving real-world problems for students in online education using technology. The ideal candidate can think outside the box, is passionate about technology, is adaptable, thinks big, and is passionate about making an impact. Chegg is evolving very fast, and we are constantly redefining our offerings to match the requirements of our student community; the candidate should have the appetite to pivot fast and be interested in continuous improvement and learning. Chegg has a very open and vibrant engineering culture where the candidate will get the opportunity to work with the best in the industry; the role demands ideating and sharing creative ideas as you never know the next big thing Chegg works on can come from you !! If you have dreamt of leveraging your skills and knowledge to impact something big enough to matter, Chegg provides those opportunities, and the candidate should make the best use of them.\nResponsibilities\nDetermine operational feasibility by evaluating analysis, problem definition, requirements, solution development, and proposed solutions;\nCross-team collaboration in driving the end-to-end delivery of SDN on Edge;\nParticipating in the code reviews and design discussions of other engineers;\nHave a strong sense of end-to-end ownership;\nAdhere to key principles: Code and design for best performance, scalability, and resiliency;\nParticipate in daily SCRUM meetings;\nParticipates in the testing process through test review and analysis, test witnessing, and certification of software;\nBe a self-starter, capable of solving ambiguous and challenging technical problems with wide scope;\nFull stack development of new features/tools, including design, documentation, implementation, and testing;\nWork alongside other engineers on the team to elevate technology and consistently apply best practices.\nSkills and Qualifications [Must Have]\nB.E., B.Tech, . degree in Computer Science or a related technical field\n3+ years of product lifecycle experience (from customer requirements -> functional spec -> design -> development/testing -> deployment and monitoring);\nStrong interpersonal and communication skills;\nStrong hands-on development/scripting experience with Python and shell.\nUse tools and methodologies to create representations of workflows, user interfaces, data schemas, etc;\nSolid understanding of software design and development;\nExperience with third-party libraries and APIs;\nExcellent design and problem-solving skills.\nStrong experience with Cloud technologies such as AWS\nExperience with Unit testing frameworks for TDD (Test Driven Development) methodology\nSkills and Qualifications [Good To Have]\nSolid understanding of Agile methodologies and experience working in Agile teams.\nHands-on experience with CI/CD pipelines, preferably using GitLab.\nDevelopment knowledge of mobile apps (android/iOS)",Industry Type: E-Learning / EdTech,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Python', 'schema', 'continuous integration', 'software testing', 'software design', 'unit testing', 'android', 'ci/cd', 'solution development', 'ios', 'cloud technologies', 'tdd', 'full stack', 'scrum', 'gitlab', 'shell scripting', 'software engineering', 'code review', 'agile', 'api', 'agile methodology']",2025-06-12 06:22:52
Business Analyst (LTS),Relanto Global,5 - 10 years,Not Disclosed,['Bengaluru'],"Client - Cisco \n\n Experience - 5+Years \n\n Primary Job Responsibilities: \nManage customer involvement for tool and legal platform design, testing, and program roll-out.\nConduct business process analysis, scope assessments, and preliminary cost/benefits/gap analyses for business initiatives.\nHighly experienced in identifying policy violations, analyzing, and proposing risk mitigating design solutions.\nAbility to translate work processes into precise business requirements, epics, and user stories.\nSuccessfully plan, implement, test, and enable new features or enhancements for new or existing applications.\nServe as a liaison between the business, cross-functional teams, and IT to provide technical and business solutions that meet user needs.\nConduct training and knowledge transfer sessions to worldwide business teams.\nAbility to efficiently handle work that crosses across inter-related business teams and tools.\nAbility to access data from repositories using SQL queries to analyze large datasets and determine trends, opportunities, alarming issues, and hidden patterns.\nManage customer involvement for tool and legal platform design, testing, and program roll-out.\nConduct business process analysis, scope assessments, and preliminary cost/benefits/gap analyses for business initiatives.\nHighly experienced in identifying policy violations, analyzing, and proposing risk mitigating design solutions.\nAbility to translate work processes into precise business requirements, epics, and user stories.\nSuccessfully plan, implement, test, and enable new features or enhancements for new or existing applications.\nServe as a liaison between the business, cross-functional teams, and IT to provide technical and business solutions that meet user needs.\nConduct training and knowledge transfer sessions to worldwide business teams.\nAbility to efficiently handle work that crosses across inter-related business teams and tools.\nAbility to access data from repositories using SQL queries to analyze large datasets and determine trends, opportunities, alarming issues, and hidden patterns.\n\n General expectations for this role: \nUnderstand contracting tools and contract repositories\nHave awareness of business needs and practical application for business needs\nBuild, depict, and optimize contract process flows for different levels of complexities\nRespond to requests; gathers necessary information to resolve cases\nDelivers quality work product on assigned tasks; aids in troubleshooting and drives creative solutions; implements solutions and/or fixes\n\n Desired Skills and Characteristics: \nOutstanding ability to discuss complex issues in a clear and simple manner both in writing and orally;\nFocused attention to detail and ability to work independently;\nStrong organizational and social skills and desire to make legal processes more efficient;\nExcellent business judgment, advocating for acceptable legal risk to enable business outcomes;\nCustomer and client-centric approach, demonstrating respect and a positive attitude under all circumstances;\nStrong leadership skills, wanting to chip in to a team culture focused on shared success\nCritical skills include problem solving, ability to envision the implications of decisions\nProcess and systems background\nKnowledge and experience in agile project methodology\n\n Minimum Qualifications: \n\nBachelor's degree in computer science or related field\n\n5+ years proven experience in related field\n\nExperience with configuring and updating custom objects, fields, conditional logic, user access permission, validation rules, workflows, and approvals\n\nKnowledge of end-to-end legal contracting processes including contract generation, contract negotiations, and contract repositories\n\nExperience in contract lifecycle management tools such as Ariba, Conga, Ironclad, Apttus, Icertis, or others",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['sql queries', 'business process analysis', 'agile', 'conga', 'ariba', 'visualforce', 'soql', 'software testing', 'sfdc', 'business solutions', 'business analysis', 'user stories', 'triggers', 'process flow', 'sql', 'apex', 'salesforce', 'sales force development', 'workflow analysis']",2025-06-12 06:22:55
AI Engineer,HCLTech,10 - 14 years,Not Disclosed,['Noida'],"Seniority: Senior\nDescription & Requirements\nPosition Summary\nThe Senior AI Engineer with GenAI expertise is responsible for developing advanced technical solutions, integrating cutting-edge generative AI technologies. This role requires a deep understanding of modern technical and cloud-native practices, AI, DevOps, and machine learning technologies, particularly in generative models. You will support a wide range of customers through the Ideation to MVP journey, showcasing leadership and decision-making abilities while tackling complex challenges.",,,,"['AI engineering', 'VMware', 'Java', 'Azure', 'Data engineering', 'AI models', 'Node.js', 'NLP', 'Azure AKS', 'Machine Learning Operations', 'AWS', 'Kubernetes', 'Python']",2025-06-12 06:22:57
Sr. Database Engineer For US shift (Eastern Time),TEOCO,5 - 10 years,Not Disclosed,"['Kolkata', 'Bengaluru']","Position: Sr. Database Engineer US shift (Eastern Time)\nLocation: Kolkata or Bangalore\nFull time permanent position\n\nUS shift (Eastern Time) : 4.30PM to 12.30AM IST (complete work from home)\n\nExperience required: 6-8+ years\n\n\nMajor skills required: SQL, C# or Python, any ETL tool\n\n\nProduct Development:\n\nWork with the business analysts to understand the high level business need and requirements;\nWork with operation team to understand issues and provide step-by-step solutions;\nImplement business logic using SQL;\nMonitor the system for any issues and quickly respond to emergencies;\nDeep detailed understanding of internal ETL tool;\nPrepare product for the release and drive the process;\nProficiency in query optimization (understanding query plans, improving execution time etc.);\nWrite scripts using internal code language (C# based) in order to optimize the process;\nUnderstand the overall process and data flows;\nPerform detailed analysis of the code and do research to help analysts understand current situation and make a decision.\n\nExperience and required skills:\n\nStrong understanding of relational databases;\nAdvanced SQL knowledge is required;\nWorking experience with MPP (MPP Massively Parallel Processing) Databases, understanding database design (data distribution, partitioning etc.);\nMedium Linux knowledge is required;\nC# or Python mid-level;\nExperience with analytic reporting tools such as SAP Business Object is preferred;\nAbility to work in a multi-cultured team environment;\nStrong oral and written communication skills.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SQL', 'C#', 'python', 'ETL']",2025-06-12 06:22:59
Business Analyst,Honeybee Tech Solutions,3 - 6 years,2-6 Lacs P.A.,['Pune'],"raja.a@honeybeetechsolutions.com RESUME SHARE TO Position Name Business Analyst Job No : BALI-13937 Position type: Contractual Total Exp: 3 to 5 Years HBTS Budget: 6.5 LPA No of Position: 1 Notice Period: Immedidate joiner Asset: Laptop Mandatory for interview Work Location: Pune Work Type: WFO CVR Type: Internal CVR CVR Panel Name: NA Interview Rounds: 2 Rounds Interview Mode: Virtual in Teams POSITION GRADE : GB4 DEPARTMENT : Product Validation Team (Information Technology) SUB DEPARTMENT Product Validation Team Job Description Must have: KEY RESPONSIBILITIES : Must Have 1. Collaborate with business stakeholders to gather and document detailed requirements for insurance products, processes, and systems. Must Have 2. Analyse current insurance processes, identify bottlenecks, and propose solutions to enhance efficiency and effectiveness. Must Have 3. Create comprehensive business requirement documents (BRDs) and functional requirement documents (FRDs) to guide development teams Must Have 4. Work with data to identify trends, patterns, and insights that can inform decision-making within the insurance domain. Must Have 5. Assist in the integration of new insurance technologies, ensuring they align with business needs and regulatory requirements. Must Have 6. Collaborate with quality assurance teams to develop test cases and validate that solutions meet business requirements. Must Have 7. Maintain open communication with stakeholders to provide project updates, address concerns, and ensure alignment with business goals Must Have 8. Stay up-to-date with insurance industry regulations and ensure that systems and processes comply with legal requirements Must Have 9. Identify and assess potential risks associated with insurance operations and recommend mitigation strategies. Must Have 10. Should be able to evaluate business processes, anticipate requirements, uncover areas for improvement, and developing and implementing solutions. Must Have 11. Should be able to communicate the documented business requirements with the technical team. Reviewing FSD prepared by IT and ensuring all client requirements are met Must Have 12. Should be able to prepare and review manual test scenarios and test cases Must Have 13. Should be able to liase with IT / vendors for their project deliveries/bugs etc. Must Have 14. Leading UAT team for project closure ensuring that complete functional and regression testing is done before go live Must Have 15. Ensuring smooth deployment and monitoring in production end ensuring post production signoff Must Have 16. Maintaining project tracker INTERACTIONS Must Have • Internal Relations: Must Have - Product Development, Other Projects Team, IT Team Must Have • External Relations: Must Have - IT vendors - Development & Testing REQUIRED QUALIFICATION AND SKILLS Must Have - Educational Qualifications: Post Graduate/Graduate (BE) Work Experience: Must Have - 3 to 5 years + work experience as Business Analyst preferably in the BFSI domain (Experience in Insurance domain would be added advantage) Must Have • Certifications: CBAP/ISTQ/III/Agile/PMP/Scrum Master/Prince2 Certificate Must Have • Other skill set: Must Have 1. Team management capability Must Have 2. Should be aggressive and take responsibility of completing multiple projects within defined timelines Must Have 3. Should have good oral and written communication skills Must Have 4. Should be able to analyse all outcomes of a situation and take steps accordingly",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Temporary/Contractual","['Business Analytics', 'Requirement Gathering', 'Business Analysis']",2025-06-12 06:23:01
Python/Pyspark developer,Zensar,4 - 5 years,Not Disclosed,"['Pune', 'Bengaluru']","Job Description:\nWe are seeking a highly skilled and motivated Python/PySpark Developer to join our growing team. In this role, you will be responsible for designing, developing, and maintaining high-performance data processing pipelines using Python and the PySpark framework. You will work closely with data engineers, data scientists, and other stakeholders to deliver impactful data-driven solutions.\nResponsibilities:\n- Design, develop, and implement scalable and efficient data pipelines using PySpark.\n- Write clean, well-documented, and maintainable Python code.\n- Optimize data processing performance and resource utilization.\n- Implement ETL (Extract, Transform, Load) processes to migrate and transform data across various systems.\n- Collaborate with data scientists and analysts to understand data requirements and translate them into technical solutions.\n- Troubleshoot and debug data processing issues.\n- Stay up-to-date with the latest advancements in big data technologies and best practices.\nQualifications:\n- Bachelor's degree in Computer Science, Engineering, or a related field.\n- 3+ years of experience in Python development.\n- 2+ years of experience with PySpark and Spark ecosystem.\n- Strong understanding of data structures, algorithms, and object-oriented programming.\n- Experience with SQL and relational databases.\n- Familiarity with cloud platforms such as AWS, Azure, or GCP (preferred).\n- Excellent problem-solving and analytical skills.\n- Strong communication and teamwork skills.\nBonus Points:\n- Experience with data visualization tools (e.g., Tableau, Power BI).\n- Knowledge of machine learning and data science concepts.\n- Experience with containerization technologies (e.g., Docker, Kubernetes).\n- Contributions to open-source projects.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pyspark', 'Cloud Technologies', 'SQL', 'Python']",2025-06-12 06:23:03
Analyst,Essen Vision Software,4 - 5 years,Not Disclosed,['Bengaluru'],"Key Responsibilities:\nMonitor Checkpoint firewall logs and alerts for anomalies or security threats.\nPerform initial triage and basic troubleshooting of firewall-related issues.\nHandle incidents and service requests related to firewall policies, NAT rules, and VPN access.\nEscalate complex issues to L2/L3 teams with detailed documentation.\nMaintain and update tickets in accordance with SLA requirements.\nAssist with periodic firewall health checks and performance monitoring.\nWork on predefined Standard Operating Procedures (SOPs).\nCoordinate with internal teams and vendors for issue resolution.\nEnsure compliance with security policies and procedures.\nRequired Skills:\nBasic understanding of firewall concepts, IP routing, and network protocols (TCP/IP, DNS, DHCP, etc.).\nExposure to Checkpoint firewall (R80.x preferred) or equivalent security solutions.\nFamiliarity with security incident management tools and ticketing systems.\nGood communication and documentation skills.\nWillingness to work in 24x7 environments or rotational shifts.\nPreferred Qualifications:\nBachelors degree in Engineering , Information Technology, or related field.\nCheckpoint CCSA (or in progress) is a plus.",Industry Type: IT Services & Consulting,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['TCP', 'Ccsa', 'Checkpoint Firewall', 'Configuration', 'VPN', 'IP', 'Troubleshooting']",2025-06-12 06:23:06
Programmatic - Analyst,Omnicom Media Group,1 - 3 years,Not Disclosed,['Bengaluru'],"Overview\nWe are an integral part of Annalect Global and Omnicom Group, one of the largest media and advertising agency holding companies in the world. Omnicom’s branded networks and numerous specialty firms provide advertising, strategic media planning and buying, digital and interactive marketing, direct and promotional marketing, public relations, and other specialty communications services. Our agency brands are consistently recognized as being among the world’s creative best. Annalect India plays a key role for our group companies and global agencies by providing stellar products and services in areas of Creative Services, Technology, Marketing Science (data & analytics), Market Research, Business Support Services, Media Services, Consulting & Advisory Services. We are growing rapidly and looking for talented professionals like you to be part of this journey. Let us build this, together\nResponsibilities\nManage campaigns on Demand Side Platforms (DSPs) through optimization strategies, campaign insights, monitoring and adjusting pacing, controlling quality of inventory, troubleshooting ads, and tracking tags, etc. to ensure all KPIs are met (delivery, performance, revenue goals, etc.)\nDeliver actionable audience and optimization-based insights at agreed-upon cadence.\nWork closely with agency teams to provide insights and recommendations that align with, or enhance, strategy/business goals\nUnderstand performance objectives and KPIs to develop successful optimization recommendations\nProactively learn advanced trading and optimization techniques across all preferred platforms\nStrictly adhere to agreed quality and delivery timelines.\nQualifications\n• A full-time graduate degree (Mandatory)\n2 to 3 years of experience in managing programmatic campaigns (e.g. Amazon DSP, DV360 or TTD)\nHands-on experience using DSPs to build and optimize campaigns across various inventory. Preferred experience – On Dv360/TTD/Amazon DSP\nGeneral troubleshooting skills and strong attention to detail\nWorking knowledge of digital marketing: display, video, social and marketing analytics\nA Team player & creative thinker with proven technical and analytical aptitude.\nGood with MS Excel and PowerPoint, who can demonstrate his/her ability to organize and consolidate multiple data sources for analysis.",Industry Type: Film / Music / Entertainment,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['digital marketing', 'dsp', 'trading', 'side', 'advisory services', 'analytical', 'consulting', 'ad operations', 'display video', 'market research', 'monitoring', 'navision', 'research', 'cal', 'excel', 'analytics', 'campaigns', 'marketing analytics', 'troubleshooting', 'media planning', 'powerpoint', 'support services']",2025-06-12 06:23:08
Remote Cybersecurity Analyst 57LakhsCTC|| Srinivasa Reddy Kandi,Integra Technologies,12 - 15 years,55-60 Lacs P.A.,"['Ahmedabad', 'Chennai', 'Bengaluru']","Dear Candidate,\nWe are seeking a Cybersecurity Analyst to detect, investigate, and prevent security threats across digital assets and systems.\n\nKey Responsibilities:\nMonitor and analyze security alerts, logs, and events.\nPerform threat intelligence, malware analysis, and incident response.\nConduct vulnerability assessments and patch management.\nSupport compliance and audit activities (ISO, NIST, GDPR).\nEducate staff on cybersecurity best practices and awareness.\nRequired Skills & Qualifications:\nExperience with SIEM tools (Splunk, AlienVault, QRadar).\nKnowledge of firewalls, IDS/IPS, endpoint protection, and antivirus.\nFamiliarity with scripting for automation and reporting.\nStrong analytical, investigative, and communication skills.\nSecurity certifications preferred (e.g., CompTIA Security+, SOC Analyst, CISSP).\nSoft Skills:\nStrong troubleshooting and problem-solving skills.\nAbility to work independently and in a team.\nExcellent communication and documentation skills.\nNote: If interested, please share your updated resume and preferred time for a discussion. If shortlisted, our HR team will contact you.\n\nSrinivasa Reddy Kandi\nDelivery Manager\nIntegra Technologies",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Cyber Security', 'Threat Analysis', 'Incident Response', 'Malware Analysis', 'Encase', 'Chfi', 'Forensic', 'Forensic Investigation', 'Cyber Forensics', 'Nuix', 'Forensic Investigations', 'Brainspace', 'Digital Forensics', 'Computer Forensics', 'E-discovery', 'Incident Handling', 'Ftk']",2025-06-12 06:23:11
Business Analyst,TechStar Group,7 - 12 years,15-25 Lacs P.A.,"['Bengaluru', 'Mumbai (All Areas)']","Business Analyst - 7 to 10years' experience in Capital markets or global treasury.\nKnowledge of capital markets & financial instruments Equities, Derivatives, Fixed Income, FX.\nUnderstanding of Investment Banking and Asset Management  Front, Middle & Back Office functions.\nAsset management experience of Trading Compliance rule coding & testing is strongly preferred, but other experience within Asset Management industry is acceptable Financial industry knowledge within investments and distribution highly desired (i.e., understanding of investment process, capital markets, fixed income, equities Sound knowledge on Database SQL querying.\nShould be a strong team player Excellent communication skills - written & verbal.\nShould be able to create good documentation and correspond on functionalities & issues concisely.\nShould be able to articulate well during discussions.\nShould be able to work with tight deadlines Confident of interacting with business users and various stakeholders.\nResponsibility: Should be able to work with tight deadlines • Confident of interacting with business users and various stakeholders. • Skilled at using MS Excel, Word, PowerPoint & Visio.",Industry Type: IT Services & Consulting,"Department: BFSI, Investments & Trading","Employment Type: Full Time, Permanent","['Capital Market', 'Investment Banking', 'Derivatives', 'Global Treasury', 'FX', 'Equity', 'Derivative Market', 'Fixed Income']",2025-06-12 06:23:13
MDM Business Analyst,Gallagher Service Center (GSC),3 - 5 years,Not Disclosed,['Bengaluru'],"Role & responsibilities :\nThis role involves understanding and analyzing business requirements, designing and implementing MDM solutions, and ensuring the successful integration and management of master data across various systems and platforms. The MDM Business Analyst works closely with business stakeholders, IT teams, and data governance teams to align MDM initiatives with overall business objectives.\n\nKey responsibilities include conducting thorough analysis of existing data management processes and systems to identify areas for improvement and optimization. The MDM Business Analyst is also responsible for developing and documenting business requirements, creating data models, and defining data standards and policies. Additionally, they collaborate with various teams to ensure data integrity, accuracy, and consistency, and provide support for data governance and data quality initiatives\n\nRequired Skillsets:\nA Bachelor's or Master's degree in Computer Science, Information Systems, or a related field.\nProven experience, usually around 5 years, working as an MDM Domain Expert or in a similar role.\nIn-depth knowledge of MDM concepts, methodologies, and best practices.\nStrong understanding of data governance, data quality, and data integration principles.\nProficiency in MDM tools and technologies, such as Informatica MDM, CluedIn, or similar platforms.\nExcellent analytical and problem-solving skills to translate complex business requirements into practical MDM solutions.\nStrong communication and interpersonal skills to effectively collaborate with clients and internal teams.\nAbility to work independently and manage multiple projects simultaneously.\nCertification in the MDM domain (e.g., Certified MDM Professional) can be a plus.\n\nKey Responsibilities:\nDocument all existing rules in the MDM system\nDevelop the framework for stewards to create golden records\nWork directly with the business to identify the measures and areas of improvement for the data\n\n\nWe are seeking candidates who can join immediately or within a maximum of 30 days' notice\n3+ years of relevant experience is required\nCandidates who are willing to relocate to Bangalore or are already based in Bangalore\nCandidates should be flexible with working UK/US shifts.\n\nInterested candidates can share the profiles to the below mentioned email id's.\nmohammedjaveed_shaikbabu@ajg.com\nAbhishekKumar_Priyadarshi@ajg.com",Industry Type: Analytics / KPO / Research,Department: Other,"Employment Type: Full Time, Permanent","['Informatica Mdm', 'methodologies', 'MDM concepts', 'CluedIn', 'Informatica Master Data Management', 'MDM', 'Master Data Management']",2025-06-12 06:23:15
"Staff Engineer, Nodejs",Nagarro,7 - 10 years,Not Disclosed,['India'],"We're Nagarro.\n\nWe are a Digital Product Engineering company that is scaling in a big way! We build products, services, and experiences that inspire, excite, and delight. We work at scale across all devices and digital mediums, and our people exist everywhere in the world (18000+ experts across 38 countries, to be exact). Our work culture is dynamic and non-hierarchical. We're looking for great new colleagues. That's where you come in!\n\nREQUIREMENTS:\nTotal Experience 7+ years.\nExcellent knowledge developing scalable and highly available Restful APIs using NodeJS technologies.\nThorough understanding of React.js and its core principles and experience with popular React.js workflows (such as Flux or Redux or Context API or Data Structures).\nFamiliarity with common programming tools such as RESTful APIs, TypeScript, version control software, and remote deployment tools, CI/CD tools.\nUnderstanding of linter libraries (TSLINT, Prettier etc) and Unit testing using Jest, Enzyme, Jasmine or equivalent framework.\nStrong proficiency in JavaScript, including DOM manipulation and the JavaScript object model. Proficient with the latest versions of ECMAScript (JavaScript or TypeScript).\nUnderstanding of containerization, experienced in Dockers, Kubernetes.\nExposed to API gateway integrations like 3Scale.\nUnderstanding of Single-Sign-on or token-based authentication (Rest, JWT, OAuth).\nPossess expert knowledge of task/message queues include but not limited to: AWS, Microsoft Azure, Pushpin. and Kafka.\nPractical experience with GraphQL is good to have.\nWriting tested, idiomatic, and documented JavaScript, HTML and CSS.\nExperiencing in Developing responsive web-based UI.\nHave experience on Styled Components, Tailwind CSS, Material UI and other CSS-in-JS techniques.\nProblem-solving mindset with the ability to tackle complex data engineering challenges.\nStrong communication and teamwork skills, with the ability to mentor and collaborate effectively.\nRESPONSIBILITIES:\nWriting and reviewing great quality code\nUnderstanding the clients business use cases and technical requirements and be able to convert them into technical design which elegantly meets the requirements\nMapping decisions with requirements and be able to translate the same to developers.\nIdentifying different solutions and being able to narrow down the best option that meets the clients requirements.\nDefining guidelines and benchmarks for NFR considerations during project implementation\nWriting and reviewing design document explaining overall architecture, framework, and high-level design of the application for the developers.\nReviewing architecture and design on various aspects like extensibility, scalability, security, design patterns, user experience, NFRs, etc., and ensure that all relevant best practices are followed.\nDeveloping and designing the overall solution for defined functional and non-functional requirements; and defining technologies, patterns, and frameworks to materialize it\nUnderstanding and relating technology integration scenarios and applying these learnings in projects\nResolving issues that are raised during code/review, through exhaustive systematic analysis of the root cause, and being able to justify the decision taken.\nCarrying out POCs to make sure that suggested design/technologies meet the requirements",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure', 'Typescript', 'Node.Js', 'Docker', 'Microservices', 'Kubernetes']",2025-06-12 06:23:17
Senior Generative AI Engineer - Python Programming,Zettamine Labs,7 - 8 years,Not Disclosed,['Bengaluru'],"We are looking for a Senior Generative AI Engineer who is passionate about cutting-edge AI innovation and has significant hands-on experience in building and deploying Generative AI models. In this role, you will be responsible for designing, fine-tuning, and optimizing large language models (LLMs), implementing innovative GenAI solutions, and contributing to the architecture of AI-driven platforms that deliver real business value.\n\nYou will collaborate with cross-functional teams including data scientists, machine learning engineers, product managers, and cloud infrastructure teams to build scalable, reliable, and secure AI systems. This is a high-impact position where you will directly influence the AI roadmap and innovation strategy.\n\nKey Responsibilities :\n\n- Design, develop, and fine-tune state-of-the-art Generative AI and LLM models tailored for various business use cases.\n\n- Build, integrate, and optimize solutions using transformer-based architectures (e.g., GPT, BERT, T5, LLaMA, Mistral).\n\n- Apply techniques such as fine-tuning, prompt engineering, RLHF (Reinforcement Learning from Human Feedback), and knowledge distillation to improve model performance.\n\n- Work with vector databases (e.g., FAISS, Pinecone, Weaviate) for implementing retrieval-augmented generation (RAG) pipelines.\n\n- Develop and deploy embedding models and integrate them into LLM pipelines.\n\n- Collaborate with engineering and product teams to deploy scalable AI systems using MLOps practices and CI/CD pipelines.\n\n- Leverage LangChain, Hugging Face Transformers, OpenAI APIs, and similar frameworks/tools to accelerate development.\n\n- Optimize model performance across different environments (cloud/on-premise).\n\n- Develop end-to-end pipelines, from data preprocessing to real-time inference and monitoring.\n\n- Ensure high standards of software quality, including testing, version control, code reviews, and documentation.\n\n- Stay up to date with the latest research in Generative AI and translate breakthroughs into production-ready solutions.\n\nRequired Skills & Qualifications :\n\n- Experience : 7+ years in AI/ML, data science, or software engineering; at least 3 - 4 years in Generative AI/LLMs.\n\n- Advanced Python programming skills, including familiarity with object-oriented design and software engineering best practices.\n\n- Deep expertise in PyTorch, TensorFlow, Transformers (Hugging Face), LangChain, and OpenAI or Anthropic APIs.\n\n- Experience in LLM fine-tuning, parameter-efficient tuning methods (LoRA, PEFT), RLHF, and model evaluation.\n\n- Experience with embeddings, vector stores (FAISS, Pinecone), semantic search, and RAG systems.\n\n- Hands-on experience with AWS, GCP, or Azure; knowledge of MLOps tools (SageMaker, Vertex AI, MLflow, Kubeflow) for training, deploying, and monitoring models.\n\n- Familiarity with structured/unstructured data handling and integrating AI systems with SQL/NoSQL databases.\n\n- Strong analytical thinking, problem-solving ability, and a keen interest in research and innovation.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Python Programming', 'Tensorflow', 'PyTorch', 'Generative AI', 'MLOps', 'NoSQL', 'ChatGPT', 'Artificial Intelligence', 'Data Modeling', 'LLM', 'Python', 'SQL']",2025-06-12 06:23:20
Sr. Associate Full Stack Software Engineer,Amgen Inc,1 - 3 years,Not Disclosed,['Hyderabad'],"What you will do\n\n\nIn this vital role you will be responsible for designing, developing, and maintaining software applications and solutions that meet business needs and ensuring the availability and performance of critical systems and applications. This role involves working closely with product managers, designers, data engineers, and other engineers to create high-quality, scalable software solutions and automating operations, monitoring system health, and responding to incidents to minimize downtime.\n\nYou will play a key role in a regulatory submission content automation initiative which will modernize and digitize the regulatory submission process, positioning Amgen as a leader in regulatory innovation. The initiative demonstrates innovative technologies, including Generative AI, Structured Content Management, and integrated data to automate the creation, and management of regulatory content.\n\n\n\nRoles & Responsibilities:\nPossesses strong rapid prototyping skills and can quickly translate concepts into working code\nContribute to both front-end and back-end development using cloud technology\nDevelop innovative solution using generative AI technologies\nEnsure code quality and consistency to standard methodologies\nCreate and maintain documentation on software architecture, design, deployment, disaster recovery, and operations\nIdentify and resolve technical challenges effectively\nStay updated with the latest trends and advancements\nWork closely with product team, business team, and other collaborators\nDesign, develop, and implement applications and modules, including custom reports, interfaces, and enhancements\nAnalyze and understand the functional and technical requirements of applications, solutions and systems and translate them into software architecture and design specifications\nDevelop and implement unit tests, integration tests, and other testing strategies to ensure the quality of the software\nIdentify and resolve software bugs and performance issues\nWork closely with multi-functional teams, including product management, design, and QA, to deliver high-quality software on time\nCustomize modules to meet specific business requirements\nWork on integrating with other systems and platforms to ensure seamless data flow and functionality\nProvide ongoing support and maintenance for applications, ensuring that they operate smoothly and efficiently\n\n\n\nWhat we expect of you\nWe are all different, yet we all use our unique contributions to serve patients.\nMasters degree and 1 to 3 years of experience in Computer Science, IT or related field OR\nBachelors degree and 3 to 5 years of experience in Computer Science, IT or related field OR\nDiploma and 7 to 9 years of experience in Computer Science, IT or related field\nPreferred Qualifications:\n\n\n\nFunctional\n\nSkills:\nMust-Have Skills:\nProficiency in Python/PySpark development, Fast API, PostgreSQL, Databricks, DevOps Tools, CI/CD, Data Ingestion.\nCandidates should be able to write clean, efficient, and maintainable code.\nKnowledge of HTML, CSS, and JavaScript, along with popular front-end frameworks like React or Angular, is required to build interactive and responsive web applications\nIn-depth knowledge of data engineering concepts, ETL processes, and data architecture principles. Solid understanding of cloud computing principles, particularly within the AWS ecosystem\nSolid understanding of software development methodologies, including Agile and Scrum\nExperience with version control systems like Git\nHands on experience with various cloud services, understand pros and cons of various cloud service in well architected cloud design principles\nStrong problem solving, analytical skills; Ability to learn quickly; Good communication and interpersonal skills\nExperienced with API integration, serverless, microservices architecture.\nExperience in SQL/NOSQL database, vector database for large language models\n\n\n\nGood-to-Have\n\nSkills:\nSolid understanding of cloud platforms (e.g., AWS, GCP, Azure) and containerization technologies (e.g., Docker, Kubernetes)\nExperience with monitoring and logging tools (e.g., Prometheus, Grafana, Splunk)\nExperience with data processing tools like Hadoop, Spark, or similar\n\n\n\nSoft\n\nSkills:\nExcellent analytical and solving skills\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation\nAbility to manage multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals\nStrong presentation and public speaking skills",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Python', 'PySpark development', 'Data Ingestion', 'PostgreSQL', 'Fast API', 'CI/CD', 'DevOps Tools', 'Databricks']",2025-06-12 06:23:22
Manager - BIM,Axtria,10 - 15 years,Not Disclosed,['Bengaluru'],"Position Summary \n\nLooking for a Salesforce Data Cloud Engineer to design, implement, and manage data integrations and solutions using Salesforce Data Cloud (formerly Salesforce CDP). This role is essential for building a unified, 360-degree view of the customer by integrating and harmonizing data across platforms.\n\n Job Responsibilities \n\nConsolidate the Customer data to create a Unified Customer profile\nDesign and implement data ingestion pipelines into Salesforce Data Cloud from internal and third-party systems .\nWork with stakeholders to define Customer 360 data model requirements, identity resolution rules, and calculated insights.\nConfigure and manage the Data Cloud environment, including data streams, data bundles, and harmonization.\nImplement identity resolution, micro segmentation, and activation strategies.\nCollaborate with Salesforce Marketing Cloud, to enable real-time personalization and journey orchestration.\nEnsure data governance, and platform security.\nMonitor data quality, ingestion jobs, and overall platform performance.\n\n\n Education \n\nBE/B.Tech\nMaster of Computer Application\n\n Work Experience \nOverall experience of minimum 10 years in Data Management and Data Engineering role, with a minimum experience of 3 years as Salesforce Data Cloud Data Engineer\nHands-on experience with Salesforce Data Cloud (CDP), including data ingestion, harmonization, and segmentation.\nProficient in working with large datasets, data modeling, and ETL/ELT processes.\nUnderstanding of Salesforce core clouds (Sales, Service, Marketing) and how they integrate with Data Cloud.\nExperience with Salesforce tools such as Marketing Cloud.\nStrong knowledge of SQL, JSON, Apache Iceberg and data transformation logic.\nFamiliarity with identity resolution and customer 360 data unification concepts.\nSalesforce certifications (e.g., Salesforce Data Cloud Accredited Professional, Salesforce Administrator, Platform App Builder).\nExperience with CDP platforms other than Salesforce (e.g., Segment, Adobe Experience Platform (Good to have)).\nExperience with cloud data storage and processing tools (Azure, Snowflake, etc.).\n\n\n Behavioural Competencies \n\nTeamwork & Leadership\nMotivation to Learn and Grow\nOwnership\nCultural Fit\nTalent Management\n\n Technical Competencies \n\nProblem Solving\nAzure Data Factory\nAzure DevOps\nAzure SQL",Industry Type: Analytics / KPO / Research,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sql', 'apache', 'data modeling', 'data transformation', 'etl', 'snowflake', 'navisworks', 'data management', 'bim', 'revit architecture', 'microsoft azure', 'revit mep', 'azure data factory', 'autocad', 'azure devops', 'salesforce', 'talent management', 'sql azure', 'revit', 'json', 'salesforce core']",2025-06-12 06:23:25
Business Analyst,Genisys Information Systems,10 - 14 years,Not Disclosed,['Bengaluru'],"Role & responsibilities :\nCollaborate with Business, finance, technical & Security team /stakeholders to capture and document business requirements.\nAnalyze existing Business & financial processes and propose improvements or automation opportunities with respect to ESOP Plan Management\nAct as the liaison between Business, finance and technical teams for NetSuite configuration and customization.",,,,"['NetSuite', 'Client Management', 'ESOP Plan Management', 'Business Analysis', 'Stakeholder Management', 'Requirement Gathering', 'Process Improvement', 'System Integration']",2025-06-12 06:23:27
Business Analyst,Amgen Inc,1 - 3 years,Not Disclosed,['Hyderabad'],"What you will do\nIn this vital role we are seeking a Business Systems Analyst with a good background in data and analytics to define and manage product requirements for AI-driven applications.\nPartner with Data Scientists, ML Engineers, and Product Managers to define business processes, product needs, and AI solution requirements.\nCapture and document epics, user stories, acceptance criteria, and data process flows for AI-powered analytics applications.\nWork closely with partners to define scope, priorities, and impact of new AI and data initiatives.\nEnsure non-functional requirements, such as data security, model interpretability, and system performance, are included in product backlogs.\nFacilitate the breakdown of Epics into Features and Sprint-Sized User Stories and lead backlog grooming sessions.\nEnsure alignment of technical requirements and UX for AI-based applications and interactive dashboards.\nCollaborate with engineers to define data ingestion, transformation, and model deployment processes.\nDevelop and implement product demonstrations showcasing AI-driven insights and analytics.\nMaintain detailed documentation of data pipelines, model lifecycle management, and system integrations.\nStay engaged throughout software development, providing proactive feedback to ensure business needs are met\nWhat we expect of you\nWe are all different, yet we all use our unique contributions to serve patients. This role bridges the gap between business needs and technical execution, ensuring the development of high-quality, scalable AI solutions. You will collaborate with data scientists, engineers, and product managers to shape product roadmaps, refine requirements, and drive alignment between business objectives and technical capabilities.\nBasic Qualifications:\nMasters degree and 1 to 3 years expereince in Computer Science, Data Science, Information Systems, or related field OR\nBachelors degree and 3 to 5 years of in Computer Science, Data Science, Information Systems, or related field OR\nDiploma and 7 to 9 years of in Computer Science, Data Science, Information Systems, or related field\nPreferred Qualifications:\nExperience defining requirements for AI/ML models, data pipelines, or analytics dashboards.\nFamiliarity with cloud platforms (AWS, Azure, GCP) for AI and data applications.\nUnderstanding of data security, governance, and compliance in AI solutions.\nAbility to communicate complex AI concepts and technical constraints to non-technical partners.\nKnowledge of MLOps, model monitoring, and CI/CD for AI applications.",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Business analysis', 'continuous integration', 'data science', 'gcp', 'ci/cd', 'microsoft azure', 'information systems', 'aws', 'artificial intelligence']",2025-06-12 06:23:29
Technical Illustrator(Automotive),Cyient,4 - 9 years,Not Disclosed,"['Pune', 'Bengaluru']","Create and revise Parts Lists in the Parts Catalogs for automotive, agriculture and construction equipment.\nCreating exploded view artworks associated with parts catalogs as per given standards.\nProduce complete, clear and accurate parts catalogs content as per customer standards and guidelines.\nProcessing of information and data (engineering documents, changes, etc.) from\nEngineering, Manufacturing, Parts Marketing, Customer Service, Parts Warehouse and Suppliers.",,,,"['Iso Draw', 'Technical Illustration']",2025-06-12 06:23:32
Business Analyst,Exaltare Technologies,4 - 7 years,Not Disclosed,['Pune'],"Please Note:\nThis position is on third-party payroll and will involve collaboration with international teams (India, US, and Mexico).\n\nRole Summary:\nWe are seeking an experienced Business Analyst with Project Coordination expertise to manage and support the execution of multiple business-critical technology projects. The role involves coordinating project activities, facilitating cross-functional team communication, ensuring alignment with business objectives, and driving efficient execution. The ideal candidate will have a strong grasp of Agile methodologies, project planning, stakeholder management, and business process analysis.\nKey Responsibilities:\n1. Business Analysis & Planning:\nEngage with stakeholders to gather and analyze business requirements aligned with organizational goals.\nTranslate business needs into functional specifications and project deliverables.\nDevelop project charters, roadmaps, and requirement documentation.\nDefine project objectives and success criteria in collaboration with leadership and technical teams.\nSupport creation of detailed project plans including scope, milestones, timelines, and resource allocation.\n2. Project Coordination & Execution:\nCoordinate and track project execution across multiple teams (technical, UX, QA, etc.).\nSchedule and lead daily/weekly meetings to ensure milestones are met and blockers are resolved.\nSupport sprint planning, review, and retrospective sessions with Agile teams.\nMonitor task progress, manage risks, and escalate issues to leadership when necessary.\nCollaborate with product and technology teams to ensure solutions are aligned with the overall business vision.\n3. Communication & Stakeholder Management:\nAct as the primary point of contact between project stakeholders, technical teams, and leadership.\nMaintain clear, consistent communication across all levels of the organization.\nProvide regular status updates, dashboards, and reports for leadership and executive sponsors.\nPrepare and deliver project documentation, presentations, and reports.\nFacilitate international team collaboration across India, the US, and Mexico time zones.\nRequired Skills & Experience:\n5+ years in business analysis and project coordination roles.\nStrong knowledge of Agile/Scrum methodologies; experience with sprint planning and retrospectives.\nPrior experience working with SaaS or enterprise-level solutions preferred.\nHands-on experience with project management tools like JIRA, Confluence, Trello, etc.\nFamiliarity with data and cloud platforms; ability to understand technical architectures is a plus.\nExcellent written and verbal communication skills; ability to influence stakeholders at all levels.\nStrong analytical thinking, problem-solving, and organizational skills.\nComfortable working in a global team environment and managing cross-time-zone communication.\nPreferred Qualifications:\nBachelors or Masters degree in Business, Computer Science, or a related field.\nCertification in Business Analysis (e.g., CBAP, CCBA) or Agile/Scrum (e.g., CSM, PMI-ACP) is a plus.\nExperience in customer engagement or workplace solutions domain preferred.\nFamiliarity with DevOps, CI/CD, and testing concepts (e.g., TDD, automated testing).\nAdditional Notes:\nThis role requires flexibility in working hours to accommodate meetings with international teams (Mexico, US, India).\nStrong interpersonal and stakeholder management skills are critical for success.)",Industry Type: IT Services & Consulting,Department: Project & Program Management,"Employment Type: Full Time, Permanent","['Technical Project Management', 'Sprint Planning', 'Kanban', 'ESP', 'Waterfall', 'esp Data and cloud platforms', 'Agile Lean', 'Cloud Architecture', 'Saas Product Development', 'Cloud Platform', 'Scrum', 'Customer Engagement']",2025-06-12 06:23:34
Specialist Business Analyst,Amgen Inc,4 - 6 years,Not Disclosed,['Hyderabad'],"What you will do\nIn this vital role you will be responsible for business process expertise to detail product requirements as epics and user stories, along with supporting artifacts like business process maps, use cases, and test plans for the software development teams.\nThis role involves working closely with Veeva - Site Collaboration and Veeva Vault Study Training business partners, Veeva engineers, data engineers, AI/ML engineers to ensure that the technical requirements for upcoming development are thoroughly elaborated. This enables the delivery team to estimate, plan, and commit to delivery with high confidence and identify test cases and scenarios to ensure the quality and performance of IT Systems.\nYou will analyze business requirements and design information systems solutions. You will collaborate with multi-functional teams to understand business needs, identify system enhancements, and drive system implementation projects. Your solid experience in business analysis, system design, and project management will enable you to deliver innovative and effective technology products. You will collaborate with Product owners and developers to maintain an efficient and consistent process, ensuring quality work from the team.\nRoles & Responsibilities:\nCollaborate with System Architects and Product Owners to manage business analysis activities for Veeva - Site Collaboration and Veeva Vault Study Training systems, ensuring alignment with engineering and product goals.\nCapture the voice of the customer to define business processes and product needs.\nCollaborate with Veeva - Site Collaboration and Veeva Vault Study Training business partners, Amgen Engineering teams and Veeva consultants to prioritize release scopes and refine the Product backlog .\nSupport the implementation and integrations of Veeva - Site Collaboration and Veeva Vault Study Training systems with other Amgen systems.\nEnsure non-functional requirements are included and prioritized in the Product and Release Backlogs.\nFacilitate the breakdown of Epics into Features and Sprint-Sized User Stories and participate in backlog reviews with the development team.\nClearly express features in User Stories/requirements so all team members and collaborators understand how they fit into the product backlog .\nEnsure Acceptance Criteria and Definition of Done are well-defined.\nStay focused on software development to ensure it meets requirements, providing proactive feedback to customers.\nDevelop and implement effective product demonstrations for internal and external partners .\nHelp develop and maintain a product roadmap that clearly outlines the planned features and enhancements, timelines, and achievements.\nIdentify and manage risks associated with the systems, requirement validation, and user acceptance.\nDevelop & maintain documentations of configurations, processes, changes, communication plans and training plans for end users.\nEnsure operational excellence, cybersecurity, and compliance.\nCollaborate with geographically dispersed teams, including those in the US and other international locations.\nFoster a culture of collaboration, innovation, and continuous improvement .\n\nBasic Qualifications:\nMasters degree with 4 - 6 years of experience in Computer Science/Information Systems experience with Agile Software Development methodologies OR\nBachelors degree with 6 - 8 years of experience in Computer Science/Information Systems experience with Agile Software Development methodologies OR\nDiploma with 10 - 12 years of experience in Computer Science/Information Systems experience with Agile Software Development methodologies\nPreferred Qualifications:\nExperience with Agile software development methodologies (Scrum).\nGood communication skills and the ability to collaborate with senior leadership with confidence and clarity.\nStrong knowledge of clinical trial processes especially Site Collaboration and Study Training process.\nFamiliarity with regulatory requirements for Clinical Trials (e.g. 21 CFR Part11, ICH ).\nHas experience with writing user requirements and acceptance criteria in agile project management systems such as JIRA.\n\n\nGood-to-Have Skills:\nFamiliarity with Veeva Clinical Platform, especially Veeva - Site Collaboration and Veeva Vault Study Training systems .\nExperience in managing product features for PI planning and developing product roadmaps and user journeys.\nExperience maintaining SaaS (software as a system) solutions and COTS (Commercial off the shelf) solutions.\nTechnical thought leadership.\nAble to communicate technical or complex subject matters in business terms.\nJira Align experience.\nExperience with AWS Services (like EC2, S3), Salesforce, Jira, and API gateway, etc.\nSAFe for Teams certification (preferred).\nCertifications in Veeva products (Preferred).\nCertified Business Analysis Professional (Preferred).\n\n\nSoft Skills:\nAble to work under minimal supervision .\nSkilled in providing oversight and mentoring team members. Demonstrated ability in effectively delegating work.\nExcellent analytical and gap/fit assessment skills.\nStrong verbal and written communication skills.\nAbility to work effectively with global, virtual teams .\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nTeam-oriented, with a focus on achieving team goals.\nStrong presentation and public speaking skills.",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Business Analysis', 'API gateway', 'Agile', 'AWS', 'Jira', 'Salesforce']",2025-06-12 06:23:37
"Sr. Product Manager, AI",Conga,3 - 5 years,Not Disclosed,['Bengaluru( Kadubeesanahalli )'],"Job Title: Sr. Product Manager\nLocation: Bangalore\nReports to: Manager, Product Management\n\nA quick snapshot\n\nAs a Product Manager on the Conga Discovery AI team, you will help define, build, and launch AI-driven metadata extraction solutions on top of the Conga platform. Youll bring your experience in AIand ideally Contract Lifecycle Management (CLM)to shape products that help enterprises discover, process, and analyze critical business data. This role is hands-on, requiring you to be comfortable demonstrating features, collaborating on implementations, and working closely with scrum teams to drive value for our customers.\n\nWhy it’s a big deal\n\nMetadata extraction is at the core of how businesses understand their documents and processes. By leveraging Discovery AI, you’ll help enterprises transform manual, time-consuming tasks into automated workflows that reduce errors, improve compliance, and deliver actionable insights. Your role will be central to creating and refining solutions that can scale to handle the most complex enterprise needs.\n\nAre you the person we’re looking for?\n\nRelevant Experience. You should have more than 5 years of experience in Product Management in B2B SaaS, preferably with data extraction, AI, or document automation products.\n\nDemonstrate. A success in conceptualizing and launching new product features from initial idea to market adoption.\n\nAI or machine learning. Knowledge of fundamentals and an interest in applying them to solve real-world business challenges.\n\nCLM. Exposure or understanding in CLM is a strong plus, as it ties closely into many metadata extraction use cases.\n\nResearch and Creativity. Conduct market and user research to identify new opportunities for AI-driven features.\n\nCustomer feedback. You will gather continuous customer feedback to iterate and prioritize feature development that delivers tangible customer value.\n\nMaintain strong partnerships. With professional services and support teams to address implementation details and customer escalations.\n\nDemo. You should confidently demo features to internal stakeholders, customers, and prospects to showcase Discovery AI capabilities.\n\nAnalyze and prioritize. You will use data analytics, user feedback, and market insights to guide product decisions and roadmap priorities.\n\nBalance. customer requirements, technical feasibility, and time-to-market considerations in a fast-paced environment.\n\nCustomer experience. You will regularly engage with customers to understand their needs and pain points, ensuring Discovery AI addresses real-world challenges.\n\nDocument. New workflows, provide training materials or guidelines, and gather post-launch feedback.\n\nEducation. Bachelor’s degree in Engineering or equivalent; a higher degree is a plus.\n\nHere’s what will give you an edge\n\nChampion the Customer. You appreciate that customers are the heart of the business, and you’re dedicated to delivering solutions that solve their problems while meeting them where they are.\n\nNatural collaborator. You thrive in an Agile, cross-functional setting, seeking input from engineers, designers, data scientists, and peers to make well-rounded product decisions.\n\nPassion. Your genuine enthusiasm for AI and data-driven solutions is evident in your work. You love delving into technical details, exploring new possibilities, and shaping products that redefine how companies operate.\n\nStrong Communication. You will communicate releases, risks, and timelines effectively to leadership and cross-functional stakeholders.\n\nCollaborate. With marketing to position and promote new features that drive adoption and user satisfaction.\n\nLeadership skills. You will work closely with scrum teams to ensure clear backlog priorities, smooth sprint planning, and timely delivery.",Industry Type: Software Product,Department: Product Management,"Employment Type: Full Time, Permanent","['Product Strategy', 'Product Management', 'Product Portfolio', 'Product Life Cycle Management', 'Product Planning']",2025-06-12 06:23:39
Analytics & Visualization Developer,Qualcomm,7 - 10 years,Not Disclosed,['Hyderabad'],"Job Area: Information Technology Group, Information Technology Group > IT Programmer Analyst\n\nGeneral Summary:\n\nQualcomms Engineering IT EDAAP team is looking for an independent contributor experienced in development and sustaining enterprise level software applications. Experience:7-10 years of experience developing dashboard with reporting tools- Tableau (Tableau API), Power BI, OBIEE. SkillsMust:\nExpert in developing visualizations/dashboards with Tableau\nStrong knowledge with SQL\nFundamentals in object-oriented design, data structures, algorithms and problem solving.\nTest, debug and performance tuning of dashboards/reports\nExperience working in an agile development environment.\nTranslate ad hoc report requests into common dashboards and application requirements\nKnowledge of different types of enterprise systems, their interaction, boundaries within an enterprise.\nUnderstanding of complex data models.\nExperience working with Oracle/MySQl/Postgres\nWILLINGNESS to multi task and work in a fast paced environment.\nMust be willing to take ownership and drive tasks to completion. Desirable:\nPython programming experience.\nExperience developing dashboards with Power BI.\nExposure to Qlikview, OBIEE and ThoughtSpot\nExperience with semiconductor industry.\nExperience working with NoSQL Databases (MongoDB) as well as relational DBs (MySQL/Oracle) Education\nBachelor's degree in technical discipline or equivalent experience required.\n\nQualifications\n5 or more years of experience in applying AI and machine learning techniques to practical and comprehensive technology solutions.\nA strong background in machine learning, deep learning, and natural language processing.\nExpertise in ML, deep learning, Py Torch, Python, NLP and Transformer architecture.\nExperience in deploying LLMs, embedding model/sentence transformers in production use cases.\nThorough knowledge in basic algorithms, object-oriented and functional design principles, and best-practice patterns\nStrong expertise in programming (Rust/Python)\nExperience in fine-tuning a large language model using custom content (documents, data, code).\nExperience in developing Generative AI applications, Agentic Systems and Retrieval Augmented Generation.\nExperience working with large-scale datasets, preprocess them, and create appropriate data representations.\nSolid understanding of statistics, linear algebra, and probability theory.\n\nPreferred Qualifications\nBachelors/masters degree in computer science, Artificial Intelligence, Data Science, or a related field.\nExperience in implementing projects involving end to end ML/NLP systems from development to deployment.\nExperience with transformer-based models (e.g., BERT, GPT, T5, Llama).\nExperience working in a distributed team.\nExperience with cloud environments (GCP/AWS).\nWorking knowledge of Rust is a plus.\n\nMinimum Qualifications:\n4+ years of work experience in programming, scripting, and/or automation or IT-relevant work experience with a Bachelor's degree.\nOR\n6+ years of work experience in programming, scripting, and/or automation or IT-relevant work experience without a Bachelors degree.\n\n2+ years experience with Database Design structures such as Mongo DB, MySQL.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'natural language processing', 'machine learning', 'deep learning', 'pytorch', 'algorithms', 'functional design', 'dashboards', 'artificial intelligence', 'sql', 'database design', 'tableau', 'data visualization', 'design principles', 'linear algebra', 'ml', 'statistics']",2025-06-12 06:23:41
Senior Software Engineer-7916,WebMD,5 - 10 years,Not Disclosed,['Navi Mumbai'],"Position: Senior Software Engineer (Data Engineer)\nNo. of Positions: 1\nAbout WebMD:\nHeadquartered in El Segundo, Calif., Internet Brands is a fully integrated online media and software services\norganization focused on four high-value vertical categories: Health, Automotive, Legal, and Home/Travel. The\ncompanys award-winning consumer websites lead their categories and serve more than 250 million monthly\nvisitors, while a full range of web presence offerings has established deep, long-term relationships with SMB and\nenterprise clients. Internet Brands,powerful, proprietary operating platform provides the flexibility and\nscalability to fuel the companys continued growth. Internet Brands is a portfolio company of KKR and Temasek.\nWebMD Health Corp., an Internet Brands Company, is the leading provider of health information services, serving\npatients, physicians, health care professionals, employers, and health plans through our public and private online\nportals, mobile platforms, and health-focused publications. The WebMD Health Network includes WebMD\nHealth, Medscape, Jobson Healthcare Information, prIME Oncology, MediQuality, Frontline, QxMD, Vitals\nConsumer Services, MedicineNet, eMedicineHealth, RxList, OnHealth, Medscape Education, and other owned\nWebMD sites. WebMD, Medscape, CME Circle, Medpulse, eMedicine®, MedicineNet®, theheart.org®, and\nRxList® are among the trademarks of WebMD Health Corp. or its subsidiaries.\nFor Company details, visit our website: www.webmd.com / www.internetbrands.com\nAll qualified applicants will receive consideration for employment without regard to race, color, religion, sex,\nsexual orientation, gender identity, national origin, disability, or veteran status\nEducation: B.E. Computer Science/IT degree (or any other engineering discipline)\nExperience: 5+ years\nWork timings: 2:00 PM to 11:00 PM IST.\nDescription:\nWe are seeking an experienced and passionate Senior Software Developer to join our team. In this role,\nyou will work closely with cross-functional teams, developers, stakeholders, and business units to\ngather and analyze business requirements, design, build and implement ETL Solutions, and maintain\nthe infrastructure. The ideal candidate will have a strong background in business analysis, SQL, Unix,\nPython, ETL Tools to ensure the successful execution of projects.\nResponsibilities:\nLead requirements gathering sessions with key stakeholders to understand business needs and\nobjectives.\nCollaborate with and across Agile teams to design, develop, test, implement and support ETL\nprocesses for data transformation and preparation.\nManage data pipelines for analytics and operational use.\nEnsure data quality, data accuracy and integrity across multiple sources and systems.\nPerform unit tests and conduct reviews with other team members to make sure your code is\nrigorously designed, elegantly coded, and effectively tuned for performance.\nShould be able to come up with multiple approaches to any ETL\nproblem statement/solution/technical challenge and take well informed decision to pick the\nbest solution.\nAutomate ETL Processes using Cron and/or using Job Scheduler tools like AirFlow.\nAdhere to company standards and Serve as a key contributor to the design and development of\nexception handling, code/data standardization procedures, resolution steps and Quality Assurance\ncontrols.\nMaintain a version repository and ensure version control.\nCreate visual aids such as diagrams, charts, and screenshots to enhance documentation.\nWork with Infrastructure/systems team and developers to ensure all modules are up-to- date and\nare compatible with the code.",,,,"['ETL', 'SQL', 'UNIX', 'Python']",2025-06-12 06:23:43
Business Analyst,Randomtrees,5 - 10 years,Not Disclosed,[],"5+ years of Commercial Analytics experience in Pharma/Healthcare industry (must have).\nExcellent communication skills.\nStrong stakeholder and project management skills.\nGood proficiency in SQL(must have). Working Knowledge of Snowflake, good to have.\nKnowledge of at least one BI tool, Microstrategy preferred.\nShould have worked on Commercial and Call Activity data. Exposure to pharma datasets from IMS, IQVIA or other similar vendors",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Business Analysis', 'project management', 'Commercial Analytics', 'MicroStrategy', 'Snowflake', 'SQL']",2025-06-12 06:23:45
Test Lead,Infosys,5 - 10 years,10-20 Lacs P.A.,[],"Role & responsibilities\n\nThe Test Lead oversees the testing strategy and execution for the Microsoft Fabric migration and Power BI reporting solutions. This offshore role ensures quality, reliability, and client satisfaction through rigorous validation.\nThe successful candidate will have a strong testing background and coordination skills.\nResponsibilities\nDevelop and execute the testing strategy for Microsoft Fabric and Power BI deliverables.\nValidate data migration, pipeline functionality, and report accuracy against requirements.\nCoordinate with the Offshore Project Manager to align testing with development milestones.\nCollaborate with onsite technical leads to validate results and resolve defects. • Oversee offshore testers, ensuring comprehensive coverage and quality standards.\nProactively identify risks and articulate solutions to minimize delivery issues.\nSkills\nBachelors degree in IT, computer science, or a related field.\n5+ years of experience in test leadership for data platforms and BI solutions.\nKnowledge of Microsoft Fabric, Power BI, and data migration testing.\nProficiency with testing tools (e.g., Azure DevOps, Selenium) and SQL.\nStrong communication and stakeholder management skills.\nDetail-oriented with a focus on quality and continuous improvement\n1. JD for Data Modeler\nThe Data Modeler designs and implements data models for Microsoft Fabric and Power BI, supporting the migration from Oracle/Informatica. This offshore role ensures optimized data structures for performance and reporting needs. The successful candidate will bring expertise in data modeling and a collaborative approach.\nResponsibilities\nDevelop conceptual, logical, and physical data models for Microsoft Fabric and Power BI solutions.\nImplement data models for relational, dimensional, and data lake environments on target platforms.\nCollaborate with the Offshore Data Engineer and Onsite Data Modernization Architect to ensure model alignment.\nDefine and govern data modeling standards, tools, and best practices.\nOptimize data structures for query performance and scalability.\nProvide updates on modeling progress and dependencies to the Offshore Project Manager.\nSkills\nBachelor’s or master’s degree in computer science, data science, or a related field.\n5+ years of data modeling experience with relational and NoSQL platforms.\nProficiency with modeling tools (e.g., Erwin, ER/Studio) and SQL.\nExperience with Microsoft Fabric, data lakes, and BI data structures.\nStrong analytical and communication skills for team collaboration.\nAttention to detail with a focus on performance and consistency.\nmanagement, communication, and presentation",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Test lead', 'Migration', 'power bi', 'microsoft fabric']",2025-06-12 06:23:48
Consultant,Amdocs,4 - 9 years,Not Disclosed,['Pune'],"Amdocs helps those who build the future to make it amazing. With our market-leading portfolio of software products and services, we unlock our customers innovative potential, empowering them to provide next-generation communication and media experiences for both the individual end user and enterprise customers. Our employees around the globe are here to accelerate service providers migration to the cloud, enable them to differentiate in the 5G era, and digitalize and automate their operations. Listed on the NASDAQ Global Select Market, Amdocs had revenue of $5.00 billion in fiscal 2024. For more information, visit www.amdocs.com\n\n\nIn one sentence\n\nWe are seeking a Data Engineer with advanced expertise in Databricks SQL, PySpark, Spark SQL, and workflow orchestration using Airflow. The successful candidate will lead critical projects, including migrating SQL Server Stored Procedures to Databricks Notebooks, designing incremental data pipelines, and orchestrating workflows in Azure Databricks\n\n\nWhat will your job look like\n\nMigrate SQL Server Stored Procedures to Databricks Notebooks, leveraging PySpark and Spark SQL for complex transformations.\nDesign, build, and maintain incremental data load pipelines to handle dynamic updates from various sources, ensuring scalability and efficiency.\nDevelop robust data ingestion pipelines to load data into the Databricks Bronze layer from relational databases, APIs, and file systems.\nImplement incremental data transformation workflows to update silver and gold layer datasets in near real-time, adhering to Delta Lake best practices.\nIntegrate Airflow with Databricks to orchestrate end-to-end workflows, including dependency management, error handling, and scheduling.\nUnderstand business and technical requirements, translating them into scalable Databricks solutions.\nOptimize Spark jobs and queries for performance, scalability, and cost-efficiency in a distributed environment.\nImplement robust data quality checks, monitoring solutions, and governance frameworks within Databricks.\nCollaborate with team members on Databricks best practices, reusable solutions, and incremental loading strategies\n\n\nAll you need is...\n\nBachelor s degree in computer science, Information Systems, or a related discipline.\n4+ years of hands-on experience with Databricks, including expertise in Databricks SQL, PySpark, and Spark SQL.\nProven experience in incremental data loading techniques into Databricks, leveraging Delta Lake's features (e.g., time travel, MERGE INTO).\nStrong understanding of data warehousing concepts, including data partitioning, and indexing for efficient querying.\nProficiency in T-SQL and experience in migrating SQL Server Stored Procedures to Databricks.\nSolid knowledge of Azure Cloud Services, particularly Azure Databricks and Azure Data Lake Storage.\nExpertise in Airflow integration for workflow orchestration, including designing and managing DAGs.\nFamiliarity with version control systems (e.g., Git) and CI/CD pipelines for data engineering workflows.\nExcellent analytical and problem-solving skills with a focus on detail-oriented development.\n  Preferred Qualifications  \nAdvanced knowledge of Delta Lake optimizations, such as compaction, Z-ordering, and vacuuming.\nExperience with real-time streaming data pipelines using tools like Kafka or Azure Event Hubs.\nFamiliarity with advanced Airflow features, such as SLA monitoring and external task dependencies.\nCertifications such as Databricks Certified Associate Developer for Apache Spark or equivalent.\nExperience in Agile development methodologie\n\n\nWhy you will love this job:\nYou will be able to use your specific insights to lead business change on a large scale and drive transformation within our organization.\nYou will be a key member of a global, dynamic and highly collaborative team with various possibilities for personal and professional development.\nYou will have the opportunity to work in multinational environment for the global market leader in its field!\nWe offer a wide range of stellar benefits including health, dental, vision, and life insurance as well as paid time off, sick time, and parental leave!",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['azure databricks', 'airflow', 'pyspark', 'sql', 'spark', 'azure cloud services', 'continuous integration', 'azure data lake', 'workflow orchestration', 'ci/cd', 'warehouse', 't-sql', 'sql server', 'stored procedures', 'data bricks', 'git', 'kafka', 'data warehousing concepts', 'agile']",2025-06-12 06:23:50
Associate - Business Information Management,Axtria,5 - 10 years,Not Disclosed,['Gurugram'],"Position Summary \n\nThis is the Requisition for Employee Referrals Campaign and JD is Generic.\n\nWe are looking for Associates with 5+ years of experience in delivering solutions around Data Engineering, Big data analytics and data lakes, MDM, BI, and data visualization. Experienced to Integrate and standardize structured and unstructured data to enable faster insights using cloud technology. Enabling data-driven insights across the enterprise.\n\n Job Responsibilities \n\n\nHe/she should be able to design implement and deliver complex Data Warehousing/Data Lake, Cloud Data Management, and Data Integration project assignments.\n\nTechnical Design and Development – Expertise in any of the following skills.\n\nAny ETL tools (Informatica, Talend, Matillion, Data Stage), andhosting technologies like the AWS stack (Redshift, EC2) is mandatory.\n\nAny BI toolsamong Tablau, Qlik & Power BI and MSTR.\n\nInformatica MDM, Customer Data Management.\n\nExpert knowledge of SQL with the capability to performance tune complex SQL queries in tradition and distributed RDDMS systems is must.\n\nExperience across Python, PySpark and Unix/Linux Shell Scripting.\n\nProject Managementis\n\nmust to have. Should be able create simple to complex project plans in Microsoft Project Plan and think in advance about potential risks and mitigation plans as per project plan.\n\nTask Management – Should be able to onboard team on the project plan and delegate tasks to accomplish milestones as per plan. Should be comfortable in discussing and prioritizing work items with team members in an onshore-offshore model.\n\nHandle Client Relationship – Manage client communication and client expectations independently or with support of reporting manager. Should be able to deliver results back to the Client as per plan. Should have excellent communication skills.\n\n\n Education \n\nBachelor of Technology\nMaster's Equivalent - Engineering\n\n Work Experience \n\nOverall, 5- 7years of relevant experience inData Warehousing, Data management projects with some experience in the Pharma domain.\n\nWe are hiring for following roles across Data management tech stacks -\n\nETL toolsamong Informatica, IICS/Snowflake,Python& Matillion and other Cloud ETL.\n\nBI toolsamong Power BI and Tableau.\n\nMDM - Informatica/ Raltio, Customer Data Management.\n\nAzure cloud Developer using Data Factory and Databricks\n\nData Modeler-Modelling of data - understanding source data, creating data models for landing, integration.\n\nPython/PySpark -Spark/ PySpark Design, Development, and Deployment",Industry Type: Analytics / KPO / Research,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['aws stack', 'sql', 'etl tool', 'data visualization', 'sql queries', 'data management', 'amazon redshift', 'bi', 'data warehousing', 'pyspark', 'spark', 'etl', 'data lake', 'snowflake', 'python', 'big data analytics', 'datastage', 'talend', 'power bi', 'data engineering', 'tableau', 'mdm', 'aws', 'informatica', 'unix']",2025-06-12 06:23:52
Senior DevSecOps (Bangalore) - 8+ Years - Hybrid,Databuzz Ltd,8 - 10 years,5-15 Lacs P.A.,['Bengaluru'],"Databuzz is Hiring for Senior DevSecOps Engineer Dynamics 365 (Bangalore) - 8+ Years - Hybrid\n\nPlease mail your profile to jagadish.raju@databuzzltd.com with the below details, If you are Interested.\n\nAbout DatabuzzLTD: Databuzz is One stop shop for data analytics specialized in Data Science, Big Data, Data Engineering, AI & ML, Cloud Infrastructure and Devops. We are an MNC based in both UK and INDIA. We are a ISO 27001 & GDPR complaint company.\n\nCTC -\nECTC -\nNotice Period/LWD - (Candidate serving notice period will be preferred)\nDOB-\n\nPosition: Senior DevSecOps Engineer Dynamics 365 (Bangalore) - 8+ Years - Hybrid\n\nMandatory Skills:\n\nShould have experience in Azure DevOps\nShould have experience Dynamics - 365\nStrong experience with Python and PowerShell for scripting and automation tasks\nExperienced in working with Kubernetes, Terraform,\nGood to have Service Now, YAML\n\nRegards,\nJagadish Raju - Talent Acquisition Specialist\njagadish.raju@databuzzltd.com",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Dynamics 365', 'Powershell', 'Azure Devops', 'Python', 'Terraform', 'Docker', 'Servicenow', 'Yaml', 'Kubernetes']",2025-06-12 06:23:55
SR. Databricks Developer,Labcorp,7 - 12 years,Not Disclosed,['Bengaluru'],"Labcorp is hiring a Senior Data engineer.  This person will be an integrated member of Labcorp Data and Analytics team and work within the IT team.   Play a crucial role in designing, developing and maintaining data solutions using Databricks, Fabric, Spark, PySpark and Python.  Responsible to review business requests and translate them into technical solution and technical specification.  In addition, work with team members to mentor fellow developers to grow their knowledge and expertise.  Work in a fast paced and high-volume processing environment, where quality and attention to detail are vital.\n\nRESPONSIBILITIES:\nDesign and implement end-to-end data engineering solutions by leveraging the full suite of Databricks, Fabric tools, including data ingestion, transformation, and modeling.\nDesign, develop and maintain end-to-end data pipelines by using spark, ensuring scalability, reliability, and cost optimized solutions.\nConduct performance tuning and troubleshooting to identify and resolve any issues.\nImplement data governance and security best practices, including role-based access control, encryption, and auditing.\nWork in fast-paced environment and perform effectively in an agile development environment.\n\nREQUIREMENTS:\n8+ years of experience in designing and implementing data solutions with at least 4+ years of experience in data engineering.\nExtensive experience with Databricks, Fabric, including a deep understanding of its architecture, data modeling, and real-time analytics.\nMinimum 6+ years of experience in Spark, PySpark and Python.\nMust have strong experience in SQL, Spark SQL, data modeling & RDBMS concepts.\nStrong knowledge of Data Fabric services, particularly Data engineering, Data warehouse, Data factory, and Real- time intelligence.\nStrong problem-solving skills, with ability to perform multi-tasking.\nFamiliarity with security best practices in cloud environments, Active Directory, encryption, and data privacy compliance.\nCommunicate effectively in both oral and written.\nExperience in AGILE development, SCRUM and Application Lifecycle Management (ALM).\nPreference given to current or former Labcorp employees.\n\nEDUCATION:\nBachelors in engineering, MCA.",Industry Type: Medical Services / Hospital (Diagnostics),Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Pyspark', 'Data Bricks', 'Python', 'Parquet', 'UDP', 'Shell Scripting', 'Microsoft SQL Server', 'DW BI project', 'Kafka', 'Mapreduce', 'EMR', 'Redshift', 'Hive', 'MySQL', 'Spark', 'Aws Databricks', 'Oracle', 'Redshift spectrum', 'Fabric', 'Lambda', 'Athena']",2025-06-12 06:23:57
"Spark, Java, Kafka- Hyderabad",Cognizant,12 - 15 years,Not Disclosed,['Hyderabad'],"Skill: Java, Spark, Kafka\nExperience: 10 to 16 years\nLocation: Hyderabad\n As Data Engineer, you will :\n       Support in designing and rolling out the data architecture and infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources\n       Identify data source, design  and implement data schema/models and integrate data that meet the requirements of the business stakeholders",,,,"['hive', 'cloudera', 'modeling', 'scala', 'data warehousing', 'apache pig', 'data pipeline', 'data architecture', 'scalability', 'sql', 'java', 'data modeling', 'spark', 'mysql', 'hadoop', 'etl', 'big data', 'hbase', 'python', 'oozie', 'data processing', 'airflow', 'elt', 'data engineering', 'nosql', 'mapreduce', 'kafka', 'feasibility analysis', 'hdfs', 'sqoop', 'aws']",2025-06-12 06:24:00
Media AdTech Specialist,Capgemini,5 - 10 years,Not Disclosed,['Kolkata'],"Provide ad operations and/or AdTech operations expertise\nExecute and help implement an AdTech compliance program\nStrong understanding of Programmatic Ad Eco system and Retail Media bidding advertisement.\nYouTube, Connected TV and Video Ads advertisement and hands on ad set up experience.\nDesign and implement advertising solutions tailored for retail media needs.\nBuild operational systems that enhances the productivity of our Ad Operations team\nCollaborate with other departments and stakeholders to identify and solve complex problems\nContinuously testing and improving software solutions to ensure optimal performance and user experience\nCreate and manage strong relationships with DSPs and other relevant players in the ad tech space that can help our clients achieve their objectives.\nSpearhead initiatives to refine and expand digital media services.\nCollaborate with a diverse team of experts to drive innovation. (data scientists, developers, engineers, clients and stakeholders).\nEnsure seamless integration and service delivery.\nApply the latest industry trends and best practices to achieve our clients outcomes\nStay abreast of media regulations and trends affecting digital advertising.\nBuild highly performant AdTech platforms that will support our future growth in the Ads Space\n\nQualifications:\n5 years experience in advertising operations (AdTech ops) and/or revenue operations (Revops).\nStrong understanding of DSPs, digital advertising ecosystems, ad networks, and/or advertising exchanges.\nDemonstrated excellence in client relationship management.\nDemonstrated ability to build and work across teams.\nExperience in Technical Solutions Architecture and design leadership.\nManage multiple projects and prioritize tasks effectively\nBroad knowledge across multiple technology areas Marketing Operation, Ecommerce Domain and Retail Media.\nStrong organizational skills and attention to detail.\nAbility to work independently and as part of a team.\n\nTools:\nFacebook Ads Manager, Pinterest Ad Manager, Instagram ads, DV360, Programmatic, Campaign Manager 360, Google Ad Manager, TTD\nPower-Bi, Excel, PowerPoint will be a plus point.\nYouTube, Videos, CTV related ad platforms.",Industry Type: IT Services & Consulting,Department: Marketing & Communication,"Employment Type: Full Time, Permanent","['DSP', 'Programmatic Buying', 'DV360', 'Facebook Ads Manager', 'Bidding', 'Display Video', 'Google Ads', 'Media Buying', 'Atl', 'Media Planning', 'Pinterest', 'Campaign Management', 'Btl']",2025-06-12 06:24:03
Senior JavaScript Software Engineer,Ciklum,6 - 10 years,Not Disclosed,"['Pune', 'Chennai']","Ciklum is looking for a Senior JavaScript Software Engineer to join our team full-time in India.\nWe are a custom product engineering company that supports both multinational organizations and scaling startups to solve their most complex business challenges. With a global team of over 4,000 highly skilled developers, consultants, analysts and product owners, we engineer technology that redefines industries and shapes the way people live.\n\nAbout the role:\nAs a Senior JavaScript Software Engineer, become a part of a cross-functional development team engineering experiences of tomorrow.\nClient for this project is a leading global provider of audit and assurance, consulting, financial advisory, risk advisory, tax, and related services. They are launching a digital transformation project to evaluate existing technology across the tax lifecycle and determine the best future state for that technology. This will include decomposing existing assets to determine functionality, assessment of those functionalities to determine the appropriate end state and building of new technologies to replace those functionalities.\n\nResponsibilities:\nParticipate in requirements analysis\nCollaborate with US and Vendors teams to produce software design and architecture\nWrite clean, scalable code using Angular with Typescript, HTML, CSS, and NET programming languages\nParticipate in pull request code review process\nTest and deploy applications and systems\nRevise, update, refactor and debug code\nDevelop, support and maintain applications and technology solutions\nEnsure that all development efforts meet or exceed client expectations. Applications should meet requirements of scope, functionality, and time and adhere to all defined and agreed upon standards\nBecome familiar with all development tools, testing tools, methodologies and processes\nBecome familiar with the project management methodology and processes\nEncourage collaborative efforts and camaraderie with on-shore and off-shore team members\nDemonstrate a strong working understanding of the best industry standards in software development and version controlling\nEnsure the quality and low bug rates of code released into production\nWork on agile projects, participate in daily SCRUM calls and provide task updates\nDuring design and key development phases, we might need to work a staggered shift as applicable to ensure appropriate overlap with US teams and project deliveries\n\nRequirements:\n  We know that sometimes, you cant tick every box. We would still love to hear from you if you think you will be a good fit\n6+ years of strong hands-on experience with JavaScript (ES6/ES2015+), HTML5, CSS3\n2+ years with hands-on experience with Typescript\n2+ years of hands-on experience with Angular 11+ component architecture, applying design patterns\nExperience with Angular 11+ and migrating to newer versions\nExperience with Angular State management or NgXs\nExperience with RxJS operators\nHands on experience with Kendo UI or Angular material or SpreadJS libraries\nExperience with Nx – Nrwl/Nx library for monorepos\nSkill for writing reusable components, Angular services, directives and pipes\nHands-on experience on C#, SQL Server, OOPS Concepts, Micro Services Architecture\nAt least two-year hands-on experience on .NET Core, ASP.NET Core Web API, SQL, NoSQL, Entity Framework 6 or above, Azure, Database performance tuning, Applying Design Patterns, Agile\n.Net back-end development with data engineering expertise. Experience with MS Fabric as a data platform/ Snowflake or similar tools would be a plus, but not a must need\nSkill for writing reusable libraries\nComfortable with Git & Git hooks using PowerShell, Terminal or a variation thereof\nFamiliarity with agile development methodologies\nExcellent Communication skills both oral & written\nExcellent troubleshooting and communication skills, ability to communicate clearly with US counterparts\n\nDesirable:\nExposure to micro-frontend architecture\nKnowledge on Yarn, Webpack, Mongo DB, NPM, Azure Devops Build/Release configuration\nSignalR, ASP.NET Core and WebSockets\nThis is an experienced level position, and we will train the qualified candidate in the required applications\nWillingness to work extra hours to meet deliverables\nExposure to Application Insights & Adobe Analytics\nUnderstanding of cloud infrastructure design and implementation\nExperience in CI/CD configuration\nGood knowledge of data analysis in enterprises\nExperience with Databricks, Snowflake\nExposure to Docker and its configurations, Experience with Kubernetes\n\nWhat's in it for you?\nCare: your mental and physical health is our priority. We ensure comprehensive company-paid medical insurance, as well as financial and legal consultation\nTailored education path: boost your skills and knowledge with our regular internal events (meetups, conferences, workshops), Udemy licence, language courses and company-paid certifications\nGrowth environment: share your experience and level up your expertise with a community of skilled professionals, locally and globally\nFlexibility: hybrid work mode at Chennai or Pune \nOpportunities: we value our specialists and always find the best options for them. Our Resourcing Team helps change a project if needed to help you grow, excel professionally and fulfil your potential\nGlobal impact: work on large-scale projects that redefine industries with international and fast-growing clients\nWelcoming environment: feel empowered with a friendly team, open-door policy, informal atmosphere within the company and regular team-building events\n\nAbout us:\nAt Ciklum, we are always exploring innovations, empowering each other to achieve more, and engineering solutions that matter. With us, you’ll work with cutting-edge technologies, contribute to impactful projects, and be part of a One Team culture that values collaboration and progress.\nIndia is a strategic innovation hub for Ciklum, with growing teams in Chennai and Pune leading advancements in EdgeTech, AR/VR, IoT, and beyond. Join us to collaborate on game-changing solutions and take your career to the next level.\nWant to learn more about us? Follow us on Instagram, Facebook, LinkedIn\n\nExplore, empower, engineer with Ciklum!\nExperiences of tomorrow. Engineered together\nInterested already?\nWe would love to get to know you! Submit your application. Can’t wait to see you at Ciklum.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Typescript', 'Angular', 'CSS', '.Net', 'HTML']",2025-06-12 06:24:05
Senior High Performance Computing Engineer,Amgen Inc,4 - 6 years,Not Disclosed,['Hyderabad'],"What you will do\nIn this vital role you will.\nRole Description:\nThe role is responsible for the design, integration, and management of high performance computing (HPC) systems that encompass both hardware and software components into the organizations network infrastructure. This individual will be responsible for all activities related to handling and supporting the Business and platforms including system administration, as well as incorporating new technologies under the challenge of a sophisticated and constantly evolving technology landscape. This role involves ensuring that all parts of a system work together seamlessly to meet the organizations requirements.\nRoles & Responsibilities:\nImplement, and manage cloud-based infrastructure that supports HPC environments that support data science (e.g. AI/ML workflows, Image Analysis).\nCollaborate with data scientists and ML engineers to deploy scalable machine learning models into production.\nEnsure the security, scalability, and reliability of HPC systems in the cloud.\nOptimize cloud resources for cost-effective and efficient use.\nKeep abreast of the latest in cloud services and industry standard processes.\nProvide technical leadership and guidance in cloud and HPC systems management.\nDevelop and maintain CI/CD pipelines for deploying resources to multi-cloud environments.\nMonitor and fix cluster operations/applications and cloud environments.\nDocument system design and operational procedures.\nBasic Qualifications:\nMasters degree with a 4 - 6 years of experience in Computer Science, IT or related field with hands-on HPC administration OR\nBachelors degree with 6 - 8 years of experience in Computer Science, IT or related field with hands-on HPC administration OR\nDiploma with 10-12 years of experience in Computer Science, IT or related field with hands-on HPC administration\nDemonstrable experience in cloud computing (preferably AWS) and cloud architecture.\nExperience with containerization technologies (Singularity, Docker) and cloud-based HPC solutions.\nExperience with infrastructure-as-code (IaC) tools such as Terraform, CloudFormation, Packer, Ansible and Git.\nExpert with scripting (Python or Bash) and Linux/Unix system administration (preferably Red Hat or Ubuntu).\nProficiency with job scheduling and resource management tools (SLURM, PBS, LSF, etc.).\nKnowledge of storage architectures and distributed file systems (Lustre, GPFS, Ceph).\nUnderstanding of networking architecture and security best practices.\nPreferred Qualifications:\nExperience supporting research in healthcare life sciences.\nExperience with Kubernetes (EKS) and service mesh architectures.\nKnowledge of AWS Lambda and event-driven architectures.\nExposure to multi-cloud environments (Azure, GCP).\nFamiliarity with machine learning frameworks (TensorFlow, PyTorch) and data pipelines.\nCertifications in cloud architecture (AWS Certified Solutions Architect, Google Cloud Professional Cloud Architect, etc.).\nExperience in an Agile development environment.\nPrior work with distributed computing and big data technologies (Hadoop, Spark).\nProfessional Certifications (please mention if the certification is preferred or mandatory for the role):\nRed Hat Certified Engineer (RHCE) or Linux Professional Institute Certification (LPIC)\nAWS Certified Solutions Architect Associate or Professional\nSoft Skills:\nStrong analytical and problem-solving skills.\nAbility to work effectively with global, virtual teams\nEffective communication and collaboration with cross-functional teams.\nAbility to work in a fast-paced, cloud-first environment.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['cloud computing', 'resource management', 'Ubuntu', 'Unix system administration', 'linux', 'unix production support', 'Python']",2025-06-12 06:24:08
Sr Software Engineer: Integration Engineer,HMH,5 - 7 years,Not Disclosed,['Pune'],"The Data Integration Engineer will play a key role in designing, building, and maintaining data integrations between core business systems such as Salesforce and SAP and our enterprise data warehouse on Snowflake. This position is ideal for an early-career professional (1 to 4 years of experience) eager to contribute to transformative data integration initiatives and learn in a collaborative, fast-paced environment.\n\nDuties & Responsibilities:\nCollaborate with cross-functional teams to understand business requirements and translate them into data integration solutions.\nDevelop and maintain ETL/ELT pipelines using modern tools like Informatica IDMC to connect source systems to Snowflake.\nEnsure data accuracy, consistency, and security in all integration workflows.\nMonitor, troubleshoot, and optimize data integration processes to meet performance and scalability goals.\nSupport ongoing integration projects, including Salesforce and SAP data pipelines, while adhering to best practices in data governance.\nDocument integration designs, workflows, and operational processes for effective knowledge sharing.\nAssist in implementing and improving data quality controls at the start of processes to ensure reliable outcomes.\nStay informed about the latest developments in integration technologies and contribute to team learning and improvement.",,,,"['GCP', 'Azure', 'IDMC', 'XML', 'CSV', 'JSON', 'SQL Server', 'AWS', 'data integration', 'data engineering']",2025-06-12 06:24:10
Senior High Performance Computing Engineer,Amgen Inc,6 - 8 years,Not Disclosed,['Hyderabad'],"What you will do\n\n\nIn this vital role you will be responsible for deploying, maintaining and supporting HPC infrastructure in a multi-cloud environment. Hands-on engineering which requires\n\ndeep technical expertise in HPC technology and standard methodologies.\nImplement, and manage cloud-based infrastructure that supports HPC environments that support data science (e.g. AI/ML workflows, Image Analysis).\nCollaborate with data scientists and ML engineers to deploy scalable machine learning models into production.\nEnsure the security, scalability, and reliability of HPC systems in the cloud.\nOptimize cloud resources for cost-effective and efficient use.\nStay ahead of with the latest in cloud services and industry standard processes.\nProvide technical leadership and guidance in cloud and HPC systems management.\nDevelop and maintain CI/CD pipelines for deploying resources to multi-cloud environments.\nMonitor and fix cluster operations/applications and cloud environments.\nDocument system design and operational procedures.\n\n\n\nMust-Have\n\nSkills:\nExpert with Linux/Unix system administration (RHEL, CentOS, Ubuntu, etc.).\nProficiency with job scheduling and resource management tools (SLURM, PBS, LSF, etc.).\nGood understanding of parallel computing, MPI, OpenMP, and GPU acceleration (CUDA, ROCm).\nKnowledge of storage architectures and distributed file systems (Lustre, GPFS, Ceph).\nExperience with containerization technologies (Singularity, Docker) and cloud-based HPC solutions.\nExpert in scripting languages (Python, Bash) and containerization technologies (Docker, Kubernetes).\nFamiliarity with automation tools (Ansible, Puppet, Chef) for system provisioning and maintenance.\nUnderstanding of networking protocols, high-speed interconnects, and security best practices.\nDemonstrable experience in cloud computing (AWS, Azure, GCP) and cloud architecture.\nExperience with infrastructure as code (IaC) tools like Terraform or CloudFormation and Git.\n\n\n\nWhat we expect of you\nWe are all different, yet we all use our unique contributions to serve patients. Expert knowledge in\n\nlarge Linux environments, networking, storage, and cloud related technologies. Also, the candidate will have\n\nexpertise in root-cause analysis and fix while working with a team and stakeholders.\n\nTop-level communication and documentation skills are required.\n\nExpertise in coding in\n\nPython, Bash, YAML is expected.\n\n\n\nGood-to-Have\n\nSkills:\nExperience with Kubernetes (EKS) and service mesh architectures.\nKnowledge of AWS Lambda and event-driven architectures.\nFamiliarity with AWS CDK, Ansible, or Packer for cloud automation.\nExposure to multi-cloud environments (Azure, GCP).\nBasic Qualifications:\nBachelors degree in computer science, IT, or related field with 6-8 years of hands-on HPC administration or a related field.\n\n\n\nProfessional Certifications (preferred):\nRed Hat Certified Engineer (RHCE) or Linux Professional Institute Certification (LPIC)\nAWS Certified Solutions Architect Associate or Professional\nPreferred Qualifications:\n\n\n\nSoft\n\nSkills:\nStrong analytical and problem-solving skills.\nAbility to work effectively with global, virtual teams\nEffective communication and collaboration with cross-functional teams.\nAbility to work in a fast-paced, cloud-first environment.\nShift Information: This position is required to be onsite and participate in 24/5 and weekend on call in rotation fashion and may require you to work a later shift. Candidates must be willing and able to work off hours, as required based on business requirements.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Performance Computing', 'python', 'cloud architecture', 'linux', 'bash', 'networking', 'linux internals', 'cloud computing', 'scripting languages']",2025-06-12 06:24:12
Sr ETL/SSIS developer,Sagility India,6 - 11 years,Not Disclosed,['Bengaluru'],"Job Summary\nWe are seeking a highly skilled and self-driven SSIS with strong communication and client-facing skills to join our healthcare analytics team. This role requires a combination of deep technical expertise in SSIS and data integration along with the ability to consult and collaborate directly with clients to understand and address their data needs.\nThe ideal candidate will be experienced in building and maintaining scalable data pipelines, working with diverse healthcare data sources, and ensuring data quality and availability for downstream analytics. You will play a key role in delivering clean, trusted, and timely data for insights and reporting.\nKey Responsibilities\nDesign, develop, and maintain robust and scalable SSIS to support healthcare analytics and reporting platforms.\nEngage directly with clients to gather requirements, provide consultation, and translate business needs into technical solutions.\nIntegrate and normalize data from diverse healthcare data sources, including claims, EMR, lab, pharmacy, and eligibility systems.\nEnsure data accuracy, completeness, and consistency throughout ingestion and transformation processes.\nOptimize and tune data workflows for performance and scalability in a cloud or on-premise data platform.\nTroubleshoot and resolve data issues in a timely and proactive manner to support high data availability.\nCollaborate with analysts, data scientists, and business stakeholders to ensure data pipelines meet analytical needs.\nCreate and maintain comprehensive technical documentation for data pipelines, data dictionaries, and workflows.\nStay informed on healthcare compliance requirements (e.g., HIPAA), and ensure data handling practices follow regulatory standards.\nRequired Skills and Qualifications\n6+ years of experience in SSIS development and data engineering\nProven ability to interact directly with clients and translate business problems into data solutions\nStrong experience with SQL, SSIS, or PySpark for data processing\nDeep understanding of data warehousing concepts and dimensional modeling\nExperience working with healthcare datasets (e.g., claims, eligibility, clinical data)\nFamiliarity with cloud platforms (Azure, AWS, or GCP) and data lakes\nStrong troubleshooting, problem-solving, and performance tuning skills\nExcellent verbal and written communication skills\nBachelor's or Masters degree in Computer Science, Engineering, Information Systems, or a related field\nPreferred Qualifications\nProficiency in building data pipelines using tools such as Azure Data Factory, Informatica, Databricks, or equivalent\nExperience with FHIR, HL7, or other healthcare data standards\nFamiliarity with HIPAA and healthcare compliance requirements\nKnowledge of reporting tools like Power BI or Tableau\nExposure to CI/CD and data pipeline automation\nWhy Join Us?\nWork on high-impact healthcare projects with meaningful outcomes\nEngage directly with clients and make a tangible difference in their data strategy\nCollaborative team culture and continuous learning opportunities\nFlexible work arrangements and competitive compensation\n\nLocation - Bangalore\nShit Timing - 2 Pm to 11 PM\nWork - Hybrid\n\nRegards,\nnaveen.vediyappan@sagility.com",Industry Type: BPM / BPO,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['SSIS', 'SQL', 'ETL']",2025-06-12 06:24:14
Business Analyst,Lions Workforce Solutions,3 - 5 years,27.5-37.5 Lacs P.A.,"['Dubai', 'United Arab Emirates']","Hiring Now: Business Analyst Systems (ERP/PLM)\n Location: Dubai, UAE (Local candidates only)\n Contract Duration: 6 Months\n Industry: Systems Implementation (Not a finance/tax role)\nWe are looking for an experienced Business Analyst – Systems to support the implementation of a Product Lifecycle Management (PLM) system, integrated with ERP (preferably Dynamics). The ideal candidate will bring a strong background in systems analysis and implementation support within the manufacturing or related industries.\n Key Responsibilities:\nGather and document business and functional requirements\nConduct AS-IS process analysis and develop To-Be process designs\nDevelop detailed System Requirements documentation\nDocument ERP integration requirements\nCollaborate with vendors on solution design and implementation\nReview Solution Design Documents (SDD) and system configurations\nDevelop and execute test cases\nSupport system testing and validation",Industry Type: IT Services & Consulting,Department: Product Management,"Employment Type: Full Time, Temporary/Contractual","['ERP', 'Product Life Cycle Management', 'Product Implementation', 'Product Management', 'ERP Implementation', 'ERP Functional', 'Product Life Cycle', 'Product Planning']",2025-06-12 06:24:17
Associate Systems Analyst,Amgen Inc,1 - 3 years,Not Disclosed,['Hyderabad'],"The role leverages domain and business process expertise to detail product requirements as epics and user stories, along with supporting artifacts like business process maps, use cases, and test plans for the software development teams.\nThis role involves working closely with business stakeholders, Data engineers,AI/ML engineers to ensure that the technical requirements for upcoming development are thoroughly elaborated. This enables the delivery team to estimate, plan, and commit to delivery with high confidence and identify test cases and scenarios to ensure the quality and performance of IT Systems. You will collaborate with Product Owner and developers to maintain an efficient and consistent process, ensuring quality deliverables from the team.\nRoles & Responsibilities:\nCollaborate with System Architects and Product owners to manage business analysis activities, ensuring alignment with engineering and product goals.\nCapture the voice of the customer to define business processes and product needs.\nWork with Product Owners and customers to define scope and value for new developments.\nCollaborate with Engineering, testing teams as well as Product Management to prioritize release scopes and groom the Product backlog.\nFacilitate the breakdown of Epics into Features and Sprint-Sized User Stories and participate in backlog reviews with the development team.\nClearly express features in User Stories/requirements so all team members and collaborators understand how they fit into the product backlog.\nMaintain and ensure the quality of documented user stories/requirements in tools such as Jira.\nEnsure Acceptance Criteria and Definition of Done are well-defined.\nWork closely with UX to align technical requirements, scenarios, and business process maps with User Experience designs.\nValidate that test scenarios meet feature acceptance criteria and customer expectations.\nMaintain and ensure the quality of documented user stories/requirements in tools like Jira.\nStay focused on software development to ensure it meets requirements, providing proactive feedback to collaborators.\nDevelop and implement effective product demonstrations for internal and external collaborators.\nBasic Qualifications and Experience:\nMasters degree and 1 to 3 years of Life Science/Biotechnology/Pharmacology/Information Systems experience OR\nBachelors degree and 3 to 5 years of Life Science/Biotechnology/Pharmacology/Information Systems experience OR\nDiploma and 7 to 9 years of Life Science/Biotechnology/Pharmacology/Information Systems experience\nFunctional Skills:\nMust-Have Skills:\nExperience with Agile software development methodologies (Scrum)\nExcellent communication skills and the ability to collaborate with senior leadership with confidence and clarity\nExperience in writing requirements for development of modern web application\nExperience of DevOps, Continuous Integration and Continuous Delivery methodology\nHas experience with writing user requirements and acceptance criteria in agile project management systems like JIRA.\nGood-to-Have Skills:\nFamiliarity with GxP, CFR 21 Part 11 and systems validation\nExperience in creating and executing validation protocols (e.g., Installation Qualification (IQ), Operational Qualification (OQ))\nExperience with testing and validation tools, and testing frameworks\nExperience as a business analyst, with command of business analysis techniques & tools, as well as SDLC & iterative systems development methodologies\nProfessional Certifications:\nSAFe for Teams certification (preferred)\nCertified Business Analysis Professional (CBAP) (preferred)\nSoft Skills:\nAble to work under minimal supervision\nSkilled in providing oversight and mentoring team members. Demonstrated ability in effectively delegating work\nExcellent analytical and gap/fit assessment skills\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation\nAbility to manage multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals\nStrong presentation and public speaking skills\nShift Information:\nThis position operates on the second shift, from 2:00 PM to 10:00 PM IST. Candidates must be willing and able to work during these hours.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Systems Analysis', 'DevOps', 'iterative systems development methodologies', 'testing frameworks', 'Agile software development methodologies', 'CI/CD', 'Jira', 'SDLC']",2025-06-12 06:24:19
AI Engineer,Lericon Informatics,5 - 7 years,Not Disclosed,"['Mumbai', 'Delhi / NCR', 'Bengaluru']","Job Summary:\nWe are seeking a passionate and skilled AI Engineer to design, develop, and deploy cutting-edge AI solutions across domains such as large language models (LLMs), computer vision, and autonomous agent workflows. You will collaborate with data scientists, researchers, and engineering teams to build intelligent systems that solve real-world problems using deep learning, transformer-based architectures, and multi-modal AI models.\n\nKey Responsibilities:\n\nDesign and implement AI/ML models, especially transformer-based LLMs (e.g., BERT, GPT, LLaMA) and vision models (e.g., ViT, YOLO, Detectron2).\nDevelop and deploy computer vision pipelines for object detection, segmentation, OCR, and image classification tasks.\nBuild and orchestrate intelligent agent workflows using prompt engineering, memory systems, retrieval-augmented generation (RAG), and multi-agent coordination.\nFine-tune and optimize pre-trained models on domain-specific datasets using frameworks like PyTorch or TensorFlow.\nCollaborate with cross-functional teams to understand problem requirements and translate them into scalable AI solutions.\nImplement inference pipelines and APIs to serve AI models efficiently using tools such as FastAPI, ONNX, or Triton Inference Server.\nConduct model evaluation, benchmarking, A/B testing, and performance tuning.\nStay updated with state-of-the-art research in deep learning, generative AI, and multi-modal learning.\nEnsure reproducibility, versioning, and documentation of all experiments and production models.\n\nQualifications:\n\nBachelors or Masters degree in Computer Science, Artificial Intelligence, Data Science, or a related field.\n35 years of hands-on experience in designing and deploying deep learning models.\nStrong knowledge of LLMs (e.g., GPT, BERT, T5), Vision Models (e.g., CNNs, Vision Transformers), and Computer Vision techniques.\nExperience building intelligent agents or using frameworks like LangChain, Haystack, AutoGPT, or similar.\nProficiency in Python, with expertise in libraries such as PyTorch, TensorFlow, Hugging Face Transformers, OpenCV, and Scikit-learn.\nFamiliarity with MLOps concepts and deployment tools (Docker, Kubernetes, MLflow).\nStrong understanding of NLP, image processing, model fine-tuning, and optimization.\nExperience with cloud platforms (AWS, GCP, Azure) and GPU environments.\nExcellent problem-solving, communication, and teamwork skills.\n\nPreferred Qualifications:\n\nExperience in building multi-modal AI systems (e.g., combining vision + language models).\nExposure to real-time inference systems and low-latency model deployment.\nContributions to open-source AI projects or research publications.\nFamiliarity with vector databases (e.g., FAISS, Pinecone, Weaviate) and RAG pipelines.\n\nLocations : Mumbai, Delhi / NCR, Bengaluru , Kolkata, Chennai, Hyderabad, Ahmedabad, Pune, India",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['AI Engineering', 'Object Detection', 'Vision Models', 'LLMs', 'AI Agents', 'LangChain', 'Hugging Face', 'Deep Learning', 'PyTorch', 'NLP', 'Transformer Models', 'Model Deployment', 'RAG', 'Computer Vision', 'TensorFlow', 'OCR']",2025-06-12 06:24:21
Business Analyst,Abad Fisheries,3 - 5 years,4-7 Lacs P.A.,['Kochi( Thoppumpady )'],"Collaborate with department heads to define data needs and ensure consistent metric capture. Apply statistical tools to analyze data, identify trends & create clear visualizations using Power BI. Identify bottlenecks and improvement opportunities,\n\nRequired Candidate profile\nWe are looking for candidates from FMCG background with advanced Exel Knowledge and other data analysis tools like power BI, tableau etc..\n\nPerks and benefits\nBest in the industry",Industry Type: Food Processing,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Power Bi', 'VLOOKUP', 'Data Analysis', 'Advanced Excel', 'MIS Reporting', 'Data Visualizations', 'Excel', 'Dashboards']",2025-06-12 06:24:24
HPLC & GC Analyst,Choksi Laboratories,2 - 5 years,2.75-5.5 Lacs P.A.,['Indore'],"Experience in Quality Control\nExperience with HPLC and GC analysis\nFamiliarity with laboratory equipment and instruments\nAbility to work independently and in a team\nExcellent organizational and time management skills\nAMV, AMD",Industry Type: Pharmaceutical & Life Sciences,Department: Other,"Employment Type: Full Time, Permanent","['Method Validation', 'GC', 'Method Development', 'HPLC']",2025-06-12 06:24:27
Business Analyst,Allied Globetech,3 - 6 years,4-7.5 Lacs P.A.,"['Navi Mumbai', 'Mumbai (All Areas)']","Key Responsibilities:\nCollaborate with stakeholders to understand business needs and objectives.\nGather, document, and prioritize functional and non-functional requirements.\nAnalyze existing processes and identify opportunities for improvement.\nCreate detailed business requirement documents (BRDs), use cases, and process flows.\nAct as a liaison between business users and technical teams.\nSupport user acceptance testing (UAT) and ensure solutions meet business expectations.\nAssist in change management and training efforts related to new implementations.\n\nPreferred Skills\nProficiency in tools like JIRA, Confluence, or similar.\nKnowledge of languages like .NET or Java\nWell versed with Oracle Database\nStrong communication, documentation, and stakeholder management skills.\nKnowledge of industry-specific domains (e.g., Banking, Finance, Healthcare, e-Commerce) is a plus.\n\nPreferred candidate profile\nBachelors degree in Business, Information Systems, or a related field.\n36 years of experience as a Business Analyst or in a similar role.\nStrong understanding of business process modeling and requirement-gathering techniques.\nFamiliarity with project management and agile methodologies.\n\nIMMEDIATE JOINERS ARE HIGHLY PREFERRED.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['It Business Analysis', 'Change Management', 'Oracle Database', 'Requirement Gathering', 'UAT', 'Business Process Modeling', 'JIRA', 'User Acceptance Testing', 'Agile Methodology', 'Stakeholder Engagement', 'Requirements Management', 'Business Requirement Analysis', 'Project Management', 'Brd', 'Confluence']",2025-06-12 06:24:30
Business Analyst,Shaip Ai Data (india),3 - 6 years,Not Disclosed,['Ahmedabad'],"Role Summary As a Business Analyst, you will be the liaison between the Product Management team and the Engineering team. You will be responsible for translating product strategy into actionable user stories, ensuring that product features are delivered on time and meeting business and quality expectations. Key Responsibilities\nManage and prioritize product and release backlogs in alignment with the product roadmap.\nTranslate high-level product strategies into detailed, actionable user stories and acceptance criteria.\nIdentify dependencies and risks, proactively addressing them to ensure smooth delivery.",,,,"['Product Strategy', 'Sprint Planning', 'Certified Scrum Product Owner', 'Agile', 'US Healthcare', 'Grooming', 'sprint process']",2025-06-12 06:24:32
Business Analyst (FTC),IQEQ,5 - 8 years,Not Disclosed,['Hyderabad'],"As a BA working within the Change Delivery Department, you will work with colleagues and other stakeholders to investigate operational issues, problems and new opportunities, seeking effective business solutions through improvements in aspects of business areas or systems. \nYou'll also assist in the analysis of underlying issues and their root causes, identifying available options for consideration. \nAppropriate use of diagrams, graphs and other mechanisms to communicate effectively with diverse stakeholder groups, across and outside of the organisation, including senior management. \nPreparation of various documentation as required by the project to agreed standards, of consistent quality and to agreed timelines \nTasks \nSupport Change Delivery Department in the delivery of Programmes and Projects by working with colleagues and other stakeholders to investigate and model business functions, business processes, information flows and data structures; \nInvestigating operational issues, problems and new opportunities; seeking effective business solutions through improvements in aspects of business areas or systems of interest. Assisting in the analysis of underlying issues and their root causes, and identifying available options; \nSpecifying data, data objects and information flows that align with the needs of the business; \nProducing business analysis deliverables using relevant documentation styles in line with organisational standards using appropriate tools, for example; \nBusiness Requirements Documentation \nSpecification Documentation  \nDevelopment of test plan and support scripts and KPI reporting \nFacilitating stakeholder meetings and workshops, and presenting findings and actions both verbally and in writing to the business; \nAssisting in defining user acceptance tests for new or improved business processes and IT systems; \nAssisting in defining holistic solutions that address organisational, people, processes, information and technology aspects; justifying the solutions when supporting the development of a business case for a business change initiative. \nconsider opportunities and potential risks attached to suggestions you make \ncommunicate the benefits of your recommendations across departments and help to address any uncertainty and concern \nsupport staff and teams in making your recommended changes, including helping to resolve any issues \nensure plans are made and processes are created to evaluate the impact of the changes made, including taking responsibility for overseeing and reporting on this evaluation. \nKey competencies for position and level \nCommunicates Effectively   \nOrganisation Savvy \nBeing Resilient  \nPlans and Aligns \nDemonstrates Self-Awareness \nKey behaviours we expect to see \nIn addition to demonstrating our Group Values (Authentic, Bold, and Collaborative), the role holder will be expected to demonstrate the following: \nAttention to detail \nTaking Ownership \nCuriosity \nPositivity",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['senior management', 'business requirements', 'kpi', 'documentation', 'business solutions', 'business analysis', 'user stories', 'requirement gathering', 'business process improvement', 'brd', 'options', 'frd', 'scrum', 'data structures', 'risk', 'root cause', 'agile', 'user acceptance', 'business case', 'reporting']",2025-06-12 06:24:35
Business Analyst,WP Translation And Testing Services,1 - 4 years,5-7 Lacs P.A.,['Navi Mumbai'],"Role - Business Analyst Software Testing Products\nExperience 1-3 Years\nLocation Ghansoli, Navi Mumbai\nNotice period – Immediate – 30 days\nJob Type – Contract (6-9 Months)\n\nWP is looking for a results-oriented Business Analyst with 1-2 years of experience in software product development, specializing in quality assurance (QA) and testing solutions. The candidate should have expertise in gathering and analyzing business requirements by coordinating with relevant stakeholders, act as an interface between stakeholders and development teams, and contribute to the development of scalable, user-friendly test management and testing automation products.\n\nResponsibilities:\n• Collaborate with development team, UI/UX engineering and QA, assist on continuous basis to shape product features aligned with product testing workflows.\n• Research and document requirements from stakeholders and product owners.\n• Document detailed functional specifications and write user stories. Prioritize tasks using internal task management tool.\n• Worked closely with developers to validate logic and alignment with product requirements & functionalities.\n• Perform UAT, validate test cases, and coordinate with QA for release signoffs.\n\nKey Skills:\n• Business analysis expertise with software testing experience will be an added advantage\n• Agile Software Development Lifecycle (SDLC) understanding\n• Exposure to various tools like TestRail, Jira, Bugzila, etc.\n• Workflow Design & User Story Creation (Agile/Scrum)\n• Background of API & Integration Requirement Analysis\n• SQL & Basic Data Analysi",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Temporary/Contractual","['Agile', 'JIRA', 'SDLC Life Cycle', 'SQL', 'Testrail', 'Scrum', 'API', 'Bugzilla']",2025-06-12 06:24:37
MDM Associate Analyst,Amgen Inc,1 - 6 years,Not Disclosed,['Hyderabad'],"Role Description:\nWe are seeking an MDM Associate Analystwith 25 years of development experience to support and enhance our enterprise MDM (Master Data Management) platforms using Informatica/Reltio. This role is critical in delivering high-quality master data solutions across the organization, utilizing modern tools like Databricks and AWS to drive insights and ensure data reliability. The ideal candidate will have strong SQL, data profiling, and experience working with cross-functional teams in a pharma environment.To succeed in this role, the candidate must have strong experience on MDM (Master Data Management) on configuration (L3 Configuration, Assets creation, Data modeling etc), ETL and data mappings (CAI, CDI) , data mastering (Match/Merge and Survivorship rules), source and target integrations (RestAPI, Batch integration, Integration with Databricks tables etc)\nRoles & Responsibilities:\nAnalyze and manage customer master data using Reltio or Informatica MDM solutions.\nPerform advanced SQL queries and data analysis to validate and ensure master data integrity.\nLeverage Python, PySpark, and Databricks for scalable data processing and automation.\nCollaborate with business and data engineering teams for continuous improvement in MDM solutions.\nImplement data stewardship processes and workflows, including approval and DCR mechanisms.\nUtilize AWS cloud services for data storage and compute processes related to MDM.\nContribute to metadata and data modeling activities.\nTrack and manage data issues using tools such as JIRA and document processes in Confluence.\nApply Life Sciences/Pharma industry context to ensure data standards and compliance.\nBasic Qualifications and Experience:\nMasters degree with 1 - 3 years of experience in Business, Engineering, IT or related field OR\nBachelors degree with 2 - 5 years of experience in Business, Engineering, IT or related field OR\nDiploma with 6 - 8 years of experience in Business, Engineering, IT or related field\nFunctional Skills:\nMust-Have Skills:\nStrong experience with Informatica or Reltio MDM platforms in building configurations from scratch (Like L3 configuration or Data modeling, Assets creations, Setting up API integrations, Orchestration)\nStrong experience in building data mappings, data profiling, creating and implementation business rules for data quality and data transformation\nStrong experience in implementing match and merge rules and survivorship of golden records\nExpertise in integrating master data records with downstream systems\nVery good understanding of DWH basics and good knowledge on data modeling\nExperience with IDQ, data modeling and approval workflow/DCR.\nAdvanced SQL expertise and data wrangling.\nExposure to Python and PySpark for data transformation workflows.\nKnowledge of MDM, data governance, stewardship, and profiling practices.\nGood-to-Have Skills:\nFamiliarity with Databricks and AWS architecture.\nBackground in Life Sciences/Pharma industries.\nFamiliarity with project tools like JIRA and Confluence.\nBasics of data engineering concepts.\nProfessional Certifications:\nAny ETL certification (e.g. Informatica)\nAny Data Analysis certification (SQL, Python, Databricks)\nAny cloud certification (AWS or AZURE)\nSoft Skills:\nStrong analytical abilities to assess and improve master data processes and solutions.\nExcellent verbal and written communication skills, with the ability to convey complex data concepts clearly to technical and non-technical stakeholders.\nEffective problem-solving skills to address data-related issues and implement scalable solutions.\nAbility to work effectively with global, virtual teams",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['MDM', 'configuration', 'Data modeling', 'data governance', 'API integrations', 'Databricks', 'data mappings']",2025-06-12 06:24:39
MDM Associate Analyst,Amgen Inc,1 - 3 years,Not Disclosed,['Hyderabad'],"ABOUT THE ROLE\n\nRole Description\n\nWe are seeking an MDM Associate Analyst with 2 5 years of development experience to support and enhance our enterprise MDM (Master Data Management) platforms using Informatica/Reltio. This role is critical in delivering high-quality master data solutions across the organization, utilizing modern tools like Databricks and AWS to drive insights and ensure data reliability. The ideal candidate will have strong SQL, data profiling, and experience working with cross-functional teams in a pharma environment.\n\nTo succeed in this role, the candidate must have strong experience on MDM (Master Data Management) on configuration (L3 Configuration, Assets creati on, Data modeling etc ) , ETL and data mappings (CAI, CDI ) , data mastering (Match/Merge and Survivorship rules) , source and target integrations ( RestAPI , Batch integration, Integration with Databricks tables etc )\n\nRoles & Responsibilities\nAnalyze and manage customer master data using Reltio or Informatica MDM solutions.\nPerform advanced SQL queries and data analysis to validate and ensure master data integrity.\nLeverage Python, PySpark , and Databricks for scalable data processing and automation.\nCollaborate with business and data engineering teams for continuous improvement in MDM solutions.\nImplement data stewardship processes and workflows, including approval and DCR mechanisms.\nUtilize AWS cloud services for data storage and compute processes related to MDM.\nContribute to metadata and data modeling activities.\nTrack and manage data issues using tools such as JIRA and document processes in Confluence.\nApply Life Sciences/Pharma industry context to ensure data standards and compliance.\nBasic Qualifications and Experience\nMasters degree with 1 - 3 years of experience in Business, Engineering, IT or related field OR\nBachelors degree with 2 - 5 years of experience in Business, Engineering, IT or related field OR\nDiploma with 6 - 8 years of experience in Business, Engineering, IT or related field\nFunctional\n\nSkills:\nMust-Have Skills:\nStrong experience with Informatica or Reltio MDM platforms in building configurations from scratch (Like L3 configuration or Data modeling, Assets creations, Setting up API integrations, Orchestration)\nStrong experience in building data mappings, data profiling, creating and implementation business rules for data quality and data transformation\nStrong experience in implementing match and merge rules and survivorship of golden records\nExpertise in integrating master data records with downstream systems\nVery good understanding of DWH basics and good knowledge on data modeling\nExperience with IDQ, data modeling and approval workflow/DCR.\nAdvanced SQL expertise and data wrangling.\nExposure to Python and PySpark for data transformation workflows.\nKnowledge of MDM, data governance, stewardship, and profiling practices.\nGood-to-Have\n\nSkills:\nFamiliarity with Databricks and AWS architecture.\nBackground in Life Sciences/Pharma industries.\nFamiliarity with project tools like JIRA and Confluence.\nBasics of data engineering concepts.\nProfessional Certifications\nAny ETL certification ( e.g. Informatica)\nAny Data Analysis certification (SQL , Python, Databricks )\nAny cloud certification (AWS or AZURE)\nSoft\n\nSkills:\nStrong analytical abilities to assess and improve master data processes and solutions.\nExcellent verbal and written communication skills, with the ability to convey complex data concepts clearly to technical and non-technical stakeholders.\nEffective problem-solving skills to address data-related issues and implement scalable solutions.\nAbility to work effectively with global, virtual teams",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['MDM', 'advance sql', 'master data', 'reltio', 'data mapping', 'informatica', 'sql', 'data profiling']",2025-06-12 06:24:41
Database Engineer-Architect 7+ Years C2H,Greetings from BCforward INDIA TECHNOLOG...,7 - 12 years,15-30 Lacs P.A.,"['Bangalore Rural', 'Bengaluru']","Greetings from BCforward INDIA TECHNOLOGIES PRIVATE LIMITED.\n\nContract To Hire(C2H) Role\nLocation: Bangalore\nPayroll: BCforward\nWork Mode: Hybrid\n\nJD\n\nPreferred Skills: 5+ years relevant experience into Database Engineer-Architect(Postgres / Postgresql; Oracle; Linux)\n\nDescription:\nSkills: Postgres / Postgresql; Oracle; Linux\n\nDesirable attributes\nSkill at the Unix command line\nThe ability to write code or script, e.g. for test harnesses or other database-related tools and utilities\nExperience as a database administrator and/or a Unix system administrator\nKnowledge of virtualization and cloud infrastructures, and of implementations such as VMWare, OpenShift, Kubernetes and Docker\nKnowledge of AWS, Google Cloud, Azure or other public cloud offerings\nKnowledge of modern storage and compute technologies, including hyper-converged infrastructures\nBachelors degree preferred.\n\nPlease share your Updated Resume, PAN card soft copy, Passport size Photo & UAN History.\n\nInterested applicants can share updated resume to g.sreekanth@bcforward.com\n\nNote: Looking for Immediate to 15-Days joiners at most.\n\nAll the best",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Database Engineer', 'Postgresql', 'Oracle', 'database Architect', 'Linux', 'Unix command']",2025-06-12 06:24:44
Business Analyst - Claims,Thryve Digital,6 - 9 years,Not Disclosed,['Chennai'],"Role Summary:\nThis job provides expertise for standard to moderately complex problem solving and in-depth understanding of system functionality. The incumbent reviews significant amounts of information and analyzes processes to support business unit needs. May troubleshoot errors, conduct impact analyses, and/or solve data rejection. Performs business analyses in one or more operational areas. Identifies process gaps and recommends process improvements for efficiencies. May provide guidance to Associate level employees.\nEssential Responsibilities\nAnalyze Claims Tickets -\nResearch/analyzes provider/Claims issue at hand\nDetermines if provider/claim specific or global issue\nActions taken could be ticket submissions to HMHS, pricing updates, provider file updates, collaboration with various internal stakeholders or Provider Relations, communications sent to Operations on global issues\nRequests cleanup report once issue is corrected, if required\nFollows cleanup through completion and notifies Provider Relations\nFacilitate process improvement meetings and/or discussions.\nAnalyze the functions and operations of a business area/function and identify problem areas. Create process mapping and document current and future state business processes.\nRecommend process efficiencies, strategies for improvement, and/or solutions to align technology with business strategies\nAssist in the development of desktop procedures and/or training material.\nCoordinate, monitor, and report on the progress of clean-up projects to ensure adherence to defined project schedule\nCommunicate effectively with customers and colleagues. Successfully articulate issues, problems, and solutions.\n\n\nThe experience we are looking to add to our team require:\n6-8 years experience in Claims and Adjustments in Federal Employee Program (FEP) business\nSkills:\nBusiness Analysis skills\nClaims and Adjustment subject matter expertise\nCan adjudicate and adjust the claims\nBluecard Home and Host knowledge\nStrong claims research skills are a must\nHigh level of systems and business knowledge\nKnowledge of INSINQ, Oscar, OCWA, CPBRE (Oscar Benefits), FEP Direct\nBusiness Process Improvement\nCollaborative Problem Solving\nExcellent analytical and problem-solving skills\nBachelors or masters degree in any discipline\nGood verbal and written skills\nGood analytical and interpersonal skills\nExceptional people management\nGood to have:\nAHM or any equivalent certification\nAdditional quality/operational certifications\nBusiness acumen on Adjustments and Offset/Recovery",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Business Analysis', 'process mapping', 'Claims management', 'desktop procedures', 'Federal Employee Program', 'OCWA', 'INSINQ', 'CPBRE', 'FBOA', 'Oscar']",2025-06-12 06:24:46
Director - BIM,Axtria,5 - 10 years,Not Disclosed,['Noida'],"Position Summary \nThis position is part of the technical leadership in data warehousing and Business Intelligence areas. Someone who can work on multiple project streams and clients for better business decision making especially in the area of Lifesciences/ Pharmaceutical domain.\n\n Job Responsibilities \n\no Technology Leadership – Lead guide the team independently or with little support to design, implement deliver complex cloud data management and BI project assignments. o Technical portfolio – Expertise in a range of BI and data hosting technologies like the AWS stack (Redshift, EC2), Snowflake, Spark, Full Stack, Qlik, Tableau, Microstrategy o Project Management – Get accurate briefs from the Client and translate into tasks for team members with priorities and timeline plans. Must maintain high standards of quality and thoroughness. Should be able to monitor accuracy and quality of others' work. Ability to think in advance about potential risks and mitigation plans. o Logical Thinking – Able to think analytically, use a systematic and logical approach to analyze data, problems, and situations. Must be able to guide team members in analysis. o Handle Client Relationship, P&L – Manage client relationship and client expectations independently. Should be able to deliver results back to the Client independently. Should have excellent communication skills.\n\n Education \n\nBE/B.Tech\nMaster of Computer Application\n\n Work Experience \n\nMinimum of 5 years of relevant experience in Pharma domain. TechnicalShould have 15 years of hands on experience in the following tools\nMust have working knowledge of toolsAtleast 2 of the following – Qlikview, QlikSense, Tableau, Microstrategy, Spotfire Aware of techniques such asUI design, Report modeling, performance tuning and regression testing Basic expertise with MS excel Advanced expertise with SQL FunctionalShould have experience in following concepts and technologies\nSpecifics\nPharma data sources like IMS, Veeva, Symphony, Cegedim etc.\nBusiness processes like alignment, market definition, segmentation, sales crediting, activity metrics calculation 0-2 years of relevant experience in a large/midsize IT services/Consulting/Analytics Company1-3 years of relevant experience in a large/midsize IT services/Consulting/Analytics Company3-5 years of relevant experience in a large/midsize IT services/Consulting/Analytics Company3-5 years of relevant experience in a large/midsize IT services/Consulting/Analytics Company\n\n Behavioural Competencies \n\nProject Management\nCommunication\nAttention to P&L Impact\nTeamwork & Leadership\nMotivation to Learn and Grow\nLifescience Knowledge\nOwnership\nCultural Fit\nScale of resources managed\nScale of revenues managed / delivered\nProblem solving\nTalent Management\nCapability Building / Thought Leadership\n\n Technical Competencies \n\nAWS KnowHow\nFormal Industry Certification AWS Certified Cloud Practitioner\nSnowflake\nData Engineering\nData Governance\nData Modelling\nData Operations (Service Management)\nData Warehousing & Data Lake\nDatabricks\nDataiku\nFormal Industry Certification Informatica_Cloud Data Warehouse & Data Lake Modernization\nMaster Data Management\nPatient Data Analytics Know How\nPharma Commercial Data - US\nPharma Commercial Data - EU",Industry Type: Analytics / KPO / Research,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['spotfire', 'sql', 'qlikview', 'microstrategy', 'tableau', 'snowflake', 'pharmaceutical', 'project management', 'performance tuning', 'regression testing', 'hosting', 'amazon redshift', 'bi', 'bim', 'sales', 'veeva', 'advanced ms excel', 'talent management', 'amazon ec2', 'spark', 'full stack', 'aws']",2025-06-12 06:24:48
Onix is Hiring Hadoop GCP Engineers!!!,Datametica,4 - 8 years,Not Disclosed,"['Pune', 'Bengaluru']","We are looking for skilled Hadoop and Google Cloud Platform (GCP) Engineers to join our dynamic team. If you have hands-on experience with Big Data technologies and cloud ecosystems, we want to hear from you!\nKey Skills:\nHadoop Ecosystem (HDFS, MapReduce, YARN, Hive, Spark)\nGoogle Cloud Platform (BigQuery, DataProc, Cloud Composer)\nData Ingestion & ETL pipelines\nStrong programming skills (Java, Python, Scala)\nExperience with real-time data processing (Kafka, Spark Streaming)\nWhy Join Us?\nWork on cutting-edge Big Data projects\nCollaborate with a passionate and innovative team\nOpportunities for growth and learning\nInterested candidates, please share your updated resume or connect with us directly!",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['gcp', 'Big Data', 'pyspark', 'Hive', 'Sqoop', 'mapreduce', 'Bigquery', 'Hadoop', 'Spark', 'YARN', 'pig', 'bigq']",2025-06-12 06:24:51
MLOps Engineer,Affine Analytics,4 - 8 years,Not Disclosed,['Bengaluru'],"Machine Learning & Data Pipelines\nStrong understanding of Machine Learning principles, lifecycle, and deployment practices\nExperience in designing and building ML pipelines\nKnowledge of deploying ML models on AWS Lambda, EKS, or other relevant services\nWorking knowledge of Apache Airflow for orchestration of data workflows\nProficiency in Python for scripting, automation, and ML model development with Data Scientists",,,,"['Machine Learning', 'S3', 'AWS Lambda', 'MLOps', 'SQS', 'AWS Glue', 'SNS', 'EKS', 'Lambda', 'Python']",2025-06-12 06:24:53
MDM Associate Analyst,Amgen Inc,1 - 6 years,Not Disclosed,['Hyderabad'],"ABOUT THE ROLE\nRole Description:\nWe are seeking an MDM Associate Analystwith 25 years of development experience to support and enhance our enterprise MDM (Master Data Management) platforms using Informatica/Reltio. This role is critical in delivering high-quality master data solutions across the organization, utilizing modern tools like Databricks and AWS to drive insights and ensure data reliability.\nThe ideal candidate will have strong SQL, data profiling, and experience working with cross-functional teams in a pharma environment.To succeed in this role, the candidate must have strong experience on MDM (Master Data Management) on configuration (L3 Configuration, Assets creation, Data modeling etc), ETL and data mappings (CAI, CDI) , data mastering (Match/Merge and Survivorship rules), source and target integrations (RestAPI, Batch integration, Integration with Databricks tables etc)\nRoles & Responsibilities:\nAnalyze and manage customer master data using Reltio or Informatica MDM solutions.\nPerform advanced SQL queries and data analysis to validate and ensure master data integrity.\nLeverage Python, PySpark, and Databricks for scalable data processing and automation.\nCollaborate with business and data engineering teams for continuous improvement in MDM solutions.\nImplement data stewardship processes and workflows, including approval and DCR mechanisms.\nUtilize AWS cloud services for data storage and compute processes related to MDM.\nContribute to metadata and data modeling activities.\nTrack and manage data issues using tools such as JIRA and document processes in Confluence.\nApply Life Sciences/Pharma industry context to ensure data standards and compliance.\nBasic Qualifications and Experience:\nMasters degree with 1 - 3 years of experience in Business, Engineering, IT or related field OR\nBachelors degree with 2 - 5 years of experience in Business, Engineering, IT or related field OR\nDiploma with 6 - 8 years of experience in Business, Engineering, IT or related field\nFunctional Skills:\nMust-Have Skills:\nStrong experience with Informatica or Reltio MDM platforms in building configurations from scratch (Like L3 configuration or Data modeling, Assets creations, Setting up API integrations, Orchestration)\nStrong experience in building data mappings, data profiling, creating and implementation business rules for data quality and data transformation\nStrong experience in implementing match and merge rules and survivorship of golden records\nExpertise in integrating master data records with downstream systems\nVery good understanding of DWH basics and good knowledge on data modeling\nExperience with IDQ, data modeling and approval workflow/DCR.\nAdvanced SQL expertise and data wrangling.\nExposure to Python and PySpark for data transformation workflows.\nKnowledge of MDM, data governance, stewardship, and profiling practices.\nGood-to-Have Skills:\nFamiliarity with Databricks and AWS architecture.\nBackground in Life Sciences/Pharma industries.\nFamiliarity with project tools like JIRA and Confluence.\nBasics of data engineering concepts.\nProfessional Certifications:\nAny ETL certification (e.g. Informatica)\nAny Data Analysis certification (SQL, Python, Databricks)\nAny cloud certification (AWS or AZURE)\nSoft Skills:\nStrong analytical abilities to assess and improve master data processes and solutions.\nExcellent verbal and written communication skills, with the ability to convey complex data concepts clearly to technical and non-technical stakeholders.\nEffective problem-solving skills to address data-related issues and implement scalable solutions.\nAbility to work effectively with global, virtual teams.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Reltio MDM', 'Data modeling', 'PySpark', 'data governance', 'Informatica', 'API integration', 'AWS', 'data profiling', 'SQL', 'Python']",2025-06-12 06:24:56
AI Engineer,Shashwath Solution,5 - 8 years,Not Disclosed,['Bengaluru'],"Job Summary:\nWe are seeking a highly skilled and hands-on AI Engineer with 3+ years of proven experience in building and deploying solutions using Generative AI and Large Language Models (LLMs). You will work on cutting-edge applications leveraging transformer-based architectures, fine-tuning, prompt engineering, and scalable AI deployments.\n\nThis role is ideal for engineers passionate about AI research and real-world productization of generative AI technologies.\n\nKey Responsibilities:\nDesign, develop, and deploy solutions using LLMs (e.g., GPT, LLaMA, Mistral, Claude, PaLM, etc.) for various NLP and content generation tasks.\n\nWork on fine-tuning, prompt engineering, and retrieval-augmented generation (RAG) pipelines.\n\nIntegrate LLMs into enterprise applications with APIs and orchestrate workflows using Python, LangChain, or similar frameworks.\n\nOptimize model performance, latency, and cost for production use.\n\nCollaborate with data scientists, MLOps engineers, and product managers to deliver scalable AI features.\n\nConduct experiments, analyze results, and publish internal findings or contribute to whitepapers.\n\nEnsure ethical, secure, and responsible use of AI technologies in all implementations.\n\nRequired Skills & Experience:\n3+ years of hands-on experience working with Generative AI, LLMs, and NLP technologies.\n\nStrong programming skills in Python and experience with libraries like Transformers (Hugging Face), LangChain, PyTorch, TensorFlow, etc.\n\nProven track record of fine-tuning LLMs, developing embeddings, and working with vector databases (e.g., FAISS, Pinecone, Weaviate).\n\nExperience deploying models on cloud platforms (AWS, Azure, GCP) and using ML pipelines or MLOps tools.\n\nSolid understanding of deep learning, NLP architectures, tokenization, and evaluation metrics for generative models.\n\nExperience in API development and integration of LLMs into user-facing applications.\n\nPreferred Qualifications:\nMasters or PhD in Computer Science, AI/ML, Data Science, or related field.\n\nExperience with OpenAI APIs, Anthropic, Cohere, or open-source LLMs (e.g., Mistral, Falcon, LLaMA 3).\n\nUnderstanding of RLHF (Reinforcement Learning from Human Feedback) and model alignment techniques.\n\nContributions to open-source AI projects or publications in GenAI/LLM.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Generative AI', 'LLaMA', 'GPT', 'Large Language Models', 'Azure', 'LangChain', 'Hugging Face', 'deep learning', 'OpenAI API', 'NLP', 'PyTorch', 'MLOps', 'GCP', 'PaLM', 'AWS', 'Python', 'TensorFlow']",2025-06-12 06:24:58
Snowflake Architect,Allegis Global Solutions (AGS),9 - 14 years,Not Disclosed,[],"Snowflake Architect\nJob Location: Hyderabad / Bangalore / Chennai / Kolkata / Noida/ Gurgaon / Pune / Indore / Mumbai\n\nJob Details\n\nTechnical Expertise:\nStrong proficiency in Snowflake architecture, including data sharing, partitioning, clustering, and materialized views.\nAdvanced experience with DBT for data transformations and workflow management.\nExpertise in Azure services, including Azure Data Factory, Azure Data Lake, Azure Synapse, and Azure Functions.\nData Engineering:\nProficiency in SQL, Python, or other relevant programming languages.\nStrong understanding of data modeling concepts, including star schema and normalization.\nHands-on experience with ETL/ELT pipelines and data integration tools.\nSoft Skills:\nExcellent problem-solving and analytical skills.\nStrong communication and stakeholder management abilities.\nAbility to work in agile teams and handle multiple priorities.\nPreferred Qualifications:\nCertifications in Snowflake, DBT, or Azure Data Engineering.\nFamiliar with data visualization tools like Power BI or Tableau.\nKnowledge of CI/CD pipelines and DevOps practices for data workflows.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure', 'Snowflake', 'Data Build Tool', 'SQL']",2025-06-12 06:25:00
.Net Fullstack Developer,S&P Global Market Intelligence,3 - 8 years,Not Disclosed,['Hyderabad'],"The Team:\nOur team is responsible for the design, architecture, and development of our client facing applications using MI Platform and Office Add-Ins that are regularly updated as new technologies emerge. You will have the opportunity every day to work with people from a wide variety of backgrounds and will be able to develop a close team dynamic with coworkers from around the globe.\nThe Impact:\nAs a member of the Team, you will be responsible for analysis, design, architecture, development, and support several applications.The ideal candidate should have expertise with cutting edge technologies and a desire to drive change through all alignment across the enterprise. The role requires the candidate to be a hands-on problem solver and developer helping to extend and manage the applications. The work you do will be used every single day, its the essential code youll write that provides the data and analytics required for crucial, daily decisions in the capital and commodities markets.\nWhats in it for you:\nBuild a career with a global company\nExposure to work on Latest cutting-edge Technologies and Grow and improve your skills by working on enterprise level products.\nOpportunity to grow personally and professionally.\nWork on code that fuels the global financial markets.\nResponsibilities:\nDemonstrate a strong sense of ownership and responsibility with release goals. This includes understanding requirements, technical specifications, design, architecture, implementation, unit testing, builds/deployments, and code management.\nEnsure compliance through the adoption of enterprise standards and promotion of best practice guiding principles aligned with organization standards.\nHands-on position requiring strong analytical, architecture, development and debugging skills that includes both development and operations.\nAttaint in-depth Functional knowledge of the domain that we are working on.\nUnderstand Incident Management, Problem Management and able to do root cause analysis.\nEnsure data governance principles adopted, data quality checks and data lineage implemented in each hop of the Data.\nProvide technical guidance and mentoring to the team and help them adopt change as new processes are introduced\nChampion best practices and serve as a subject matter authority\nDevelop solutions to develop/support key business needs\nEngineer components and common services based on standard development models, languages and tools\nProduce system design documents and lead technical walkthroughs\nRegularly evaluate cloud applications, hardware, and software.\nProduce high quality code\nCollaborate effectively with technical and non-technical partners\nAs a team-member should continuously improve the architecture\nWhat We Are Looking For:\nBasic Qualifications\nBachelor'smasters degree in computer science, Information Systems or equivalent.\n3-10 years of experience in application development using Microsoft Technologies.\nKnowledge of object-oriented design, .NET framework and design patterns.\nCommand of essential technologies: HTML, Single Page Application (SPA) frameworks, JavaScript Frameworks (jQuery, KnockoutJS, TypeScript, Durandal, React JS, Require), C#, .NET Framework, CSS, LESS, SQL Server, Web API, Web services\nGood to have: ASP.Net\nExperience with developing solutions involving relational database technologies on SQL Server platform, stored procedure programming experience using Transact SQL.\nExperience deploying data engineering solutions in public clouds like AWS, GCP, or Azure, leveraging cloud power to its fullest.\nProficient with software development lifecycle (SDLC) methodologies like Agile, Test- driven development.\nExperience working with any relational Databases preferably SQL Server.\nExperience in continuous delivery through CI/CD pipelines, containers, and orchestration technologies.\nExperience working with cross functional teams, with strong interpersonal and written communication skills.\nCandidate must have the desire and ability to quickly understand and work within new technologies.\nGood communication and collaboration skills and Ability to work as team player, train and mentor when needed.",Industry Type: Banking,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['.Net', 'C#', 'CSS', 'Web API', 'KnockoutJS', 'HTML', 'SQL Server', '.NET Framework', 'React JS', 'Single Page Application', 'TypeScript', 'software development lifecycle', 'jQuery', 'Web services', 'ASP.Net', 'Durandal', 'LESS']",2025-06-12 06:25:03
Etl Developer,Tata Communications,2 - 4 years,Not Disclosed,['Chennai'],"This is an operational role responsible for providing data analysis & management support. The incumbent may seek appropriate level of guidance and advice to ensure delivery of quality outcomes.\nResponsibilities\nGathering and preparing relevant data to use in analytics applications.\nAcquiring data from primary or secondary data sources and support in maintaining databases\nIdentify, analyze, and interpret trends or patterns in data sets.\nFilter and clean data by reviewing computer reports, printouts, and performance indicators to locate and correct code problems.\nDevelop and Support ETL Jobs, Schedule batch jobs via CRON, Database modeling for RDBMS\nGather data requirements, follow Scrum methodology, ownership from development to deployment",,,,"['Etl Development', 'Informatica', 'Informatica Powercenter', 'ETL']",2025-06-12 06:25:05
Machine Learning Engineer,Draft N Craft Legal Outsourcing,2 - 3 years,10-15 Lacs P.A.,['New Delhi'],"Role Overview:\n\nAs an ML Engineer you will embark on a wonderful journey of developing and implementing various Machine Learning models that will ultimately act as an enabler in the companys growth.\nSince Draft Craft is a legal services-oriented company in which we serve clients from across the border, your work will be majorly aimed towards creating ML workflows that will serve the legal industry and help improve efficiency of the in-house teams. Draft n Craft offers you the perfect opportunity to grow and hone your ML/Data Engineering skills while contributing towards building a worthwhile product.\n\nKey Responsibilities:\nDeveloping data ingestion & data preprocessing pipelines for transforming data presented for legal requirements to extract key and actionable insights. \nData cleaning for supplying accurate, consistent & relevant content to ML models.\nExploring and experimenting with different ML models and architectures that can be used with data from the legal industry in a safe and compliant manner.\nDeveloping and deploying ML models that can function in production environments for the use-cases required by the company.\nAnalyzing key metrics for model performance and devising methods to improve efficiencies of the models.\nDocument the steps involved in data preprocessing, model development, and optimizations undertaken.\nExplore techniques for feature extraction, transformation, and selection to improve model performance.\nUnderstanding software development related terminologies to collaborate with the existing team of software engineers within the company.\nDevise methods of integration of the ML models within the company’s already developed software solutions and employee workflows.\n\nRequired Qualifications:\nDegree holder from Computer Science Engineering, Data Science or related fields.\nMinimum experience of 2 years working as an ML engineer or Data Scientist in a professional capacity.\nStrong programming skills including python and familiarity with related libraries Tensorflow, PyTorch, Pandas etc.\nDatabase Querying in terms of SQL/NoSQL.\nWorking with data extracting and ETL pipelines for pre-processing of data/documents.\nRelevant experience working in NLP, Neural Networks, and Gen AI technologies.\nFamiliarity with working or applying transfer-learning on LLM models like Llama, etc.\nSome experience in software development is preferred.\nKnowledge of deploying ML models and data pipelines to cloud services like AWS/Azure.",Industry Type: Legal,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Generative Ai', 'Retrieval Augmented Generation', 'Python', 'NoSQL', 'Large Language Model', 'Data Extraction', 'Machine Learning', 'SQL']",2025-06-12 06:25:08
Artificial Intelligence Architect,Ltimindtree,12 - 16 years,Not Disclosed,"['Hyderabad', 'Bengaluru', 'Mumbai (All Areas)']",We are looking for an experienced AI ML Developers experience in data science specializing in machine learning python statistical modelling and big data technologies pyspark sql.\n\nThe ideal candidate will have a strong background in developing and deploying machine learning models optimizing ML pipelines and handling largescale structured and unstructured data to drive business impact.\n\nDeep understanding of supervised and unsupervised learning including regression classification Multiclass classification clustering and NLP Proficiency in statistical analysis AB testing and causal inference techniques Experience with model deployment and MLOps in cloud environments AWS GCP \n\nKey Responsibilities\n\nDevelop and deploy machine learning models and predictive analytics solutions for business impact\nWork with largescale structured and unstructured data to extract insights and build scalable models\nDesign implement and optimize ML pipelines for realtime and batch processing\nCollaborate with engineering product and business stakeholders to translate business problems into data science solutions\nApply statistical modeling AB testing and causal inference techniques to evaluate business performance\nApply machine learning and statistical techniques for audience segmentation helping to identify patterns and optimise business strategies\nDrive research and innovation by staying updated with cuttingedge MLAI advancements and incorporating them into our solutions\nOptimize data science models for performance scalability and interpretability in production environments\nMentor junior data scientists and contribute to best practices in data science and engineering,Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Architect', 'MLOps', 'Machine Learning', 'Ai Solutions', 'Aiml', 'Ml']",2025-06-12 06:25:11
Machine Learning Engineer,CADFEM India,1 - 4 years,4-9 Lacs P.A.,['Hyderabad'],"Job Description\nDesign and develop machine learning models tailored to mechanical engineering challenges, including predictive modelling, simulation optimisation, and failure analysis.\nUtilise deep learning and other advanced ML techniques to improve the accuracy and efficiency of CAE simulations.\nPreprocess and analyse large datasets from CAE simulations, experimental tests, and manufacturing processes for modelling.\nTrain, validate, and fine-tune machine learning models using real-world engineering data.\nOptimise models for performance, scalability, and robustness in production environments.\nCollaborate with CAE engineers to integrate ML models into existing simulation workflows (e.g., FEA, CFD, structural analysis).\nAutomate repetitive simulation tasks and enable predictive analytics for design optimisation.\nWork closely with mechanical engineers, data scientists, and software developers to identify business challenges and develop data-driven solutions.\nDeploy machine learning models into production environments and monitor their performance.\nMaintain and update models to ensure reliability and continuous improvement.\nStay abreast of the latest advancements in machine learning, AI, and CAE technologies.\nApply innovative approaches to solve complex engineering problems.\nRequirements\nBachelors or Master’s degree in Mechanical Engineering, Computer Science, or a related field\nProven 2-3 years of experience in developing and deploying machine learning models, preferably in mechanical engineering or CAE domain\nHands-on experience with CAE tools such as ANSYS, Abaqus, or similar FEA/CFD software\nStrong programming skills in Python, R, or Java\nProficiency in machine learning frameworks (TensorFlow, PyTorch, scikit-learn)\nExperience with data preprocessing, feature engineering, and statistical analysis\nSolid understanding of mathematics, statistics, and problem-solving skills\nExcellent analytical thinking and ability to tackle complex engineering challenges\nStrong communication and teamwork skills to collaborate across disciplines\nPreferred: Experience with physics-informed machine learning and digital twin technologies\nPreferred: Familiarity with automation of CAE workflows and predictive modelling for product design\n\nBenefits\nChallenging job and a chance to team up with a young and dynamic professional group\nChance to build yourself as WE grow.\nRemuneration that stays competitive and attractive to retain the best.\nOpportunity to join an organization experiencing year on year growth.",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Scientist', 'ML/DL Engineer', 'Applied Machine Learning', 'AI Engineer', 'Machine Learning Engineer', 'F1-score', 'FastAPI', 'RMSE', 'Flask']",2025-06-12 06:25:13
F2F Weekend Drive - Bangalore- 14th June - DS Gen AI,Ltimindtree,6 - 11 years,Not Disclosed,['Bengaluru'],"Job description\nWe are having a F2F weekend drive for the requirement of a Data Scientist + Gen AI at our LTIM Bangalore Whitefield office.\nDate - 14th June 2025\nExperience - 6+ Years\nMandatory Skills - Data Science, Gen AI, Python, RAG and Azure/AWS, AI/ML, NLP\nLocation - LTIMindtree Bangalore Whitefield Office\nSecondary - (Any) Machine Learning, Deep Learning, ChatGPT, Langchain, Prompt, vector stores, RAG, llama, Computer vision, Deep learning, Machine learning, OCR, Transformer, regression, forecasting, classification, hyper parameter tunning, MLOps, Inference, Model training, Model Deployment\n\nGeneric JD-\nMore than 6 years of experience in Data Engineering, Data Science and AI / ML domain\nExcellent understanding of machine learning techniques and algorithms, such as GPTs, CNN, RNN, k-NN, Naive Bayes, SVM, Decision Forests, etc.\nExperience using business intelligence tools (e.g. Tableau, PowerBI) and data frameworks (e.g. Hadoop)\nExperience in Cloud native skills.\nKnowledge of SQL and Python; familiarity with Scala, Java or C++ is an asset\nAnalytical mind and business acumen and Strong math skills (e.g. statistics, algebra)\nExperience with common data science toolkits, such as TensorFlow, KERAs, PyTorch, PANDAs, Microsoft CNTK, NumPy etc. Deep expertise in at least one of these is highly desirable.\nExperience with NLP, NLG and Large Language Models like BERT, LLaMa, LaMDA, GPT, BLOOM, PaLM, DALL-E, etc.\nGreat communication and presentation skills. Should have experience in working in a fast-paced team culture.\nExperience with AIML and Big Data technologies like AWS SageMaker, Azure Cognitive Services, Google Colab, Jupyter Notebook, Hadoop, PySpark, HIVE, AWS EMR etc.\nExperience with NoSQL databases, such as MongoDB, Cassandra, HBase, Vector databases\nGood understanding of applied statistics skills, such as distributions, statistical testing, regression, etc.\nShould be a data-oriented person with analytical mind and business acumen.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Generative AI', 'Machine Learning', 'Deep Learning', 'Python', 'Azure']",2025-06-12 06:25:16
Hiring Machine Learning Engineer,Motivity Labs,5 - 10 years,14-22.5 Lacs P.A.,['Hyderabad'],"Role - Machine Learning Engineer\nRequired Skills & Experience\n\n5+ years of hands-on experience in building, training, and deploying machine learning models in a professional, production-oriented setting.\nDemonstrable experience with database creation and advanced querying (e.g., SQL, NoSQL), with a strong understanding of data warehousing concepts.\nProven expertise in data blending, transformation, and feature engineering, adept at integrating and harmonizing both structured (e.g., relational databases, CSVs) and unstructured (e.g., text, logs, images) data.\nStrong practical experience with cloud platforms for machine learning development and deployment; significant experience with Google Cloud Platform (GCP) services (e.g., Vertex AI, BigQuery, Dataflow) is highly desirable.\nProficiency in programming languages commonly used in data science (e.g., Python is preferred, R).\nSolid understanding of various machine learning algorithms (e.g., regression, classification, clustering, dimensionality reduction) and experience with advanced techniques like Deep Learning, Natural Language Processing (NLP), or Computer Vision.\nExperience with machine learning libraries and frameworks (e.g., scikit-learn, TensorFlow, PyTorch).\nFamiliarity with MLOps tools and practices, including model versioning, monitoring, A/B testing, and continuous integration/continuous deployment (CI/CD) pipelines.\nExperience with containerization technologies like Docker and orchestration tools like Kubernetes for deploying ML models as REST APIs.\nProficiency with version control systems (e.g., Git, GitHub/GitLab) for collaborative development.\n\nInterested candidates share cv to dikshith.nalapatla@motivitylabs.com",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['GCP', 'Machine Learning', 'Deep Learning', 'Python', 'BigQuery', 'MLOps', 'Git', 'Vertex AI', 'GitHub/GitLab', 'A/B testing', 'Dataflow', 'Kubernetes']",2025-06-12 06:25:18
Power Bi & SQL Developer,Sonata Software,3 - 6 years,Not Disclosed,['Chennai'],"Role & responsibilities\n\nWe are seeking a talented and detail-oriented Power BI Developer with strong skills in SQL and experience working with Azure Databricks. The ideal candidate will be responsible for transforming raw data into meaningful insights using business intelligence tools and data engineering practices. This role involves building dashboards, writing optimized queries, and working with large-scale data platforms to support business decision-making.\nKey Responsibilities:",,,,"['Power BI', 'SQL', 'Data Bricks']",2025-06-12 06:25:20
Gen AI Architect,Alphacom Systems And Solutions,8 - 12 years,20-30 Lacs P.A.,"['Hyderabad', 'Bengaluru', 'Mumbai (All Areas)']","Dear Candidate,\n\nWe are actively hiring for GEN AI position open for PAN India location.\n\nRole Summary\nWe are seeking a seasoned Generative AI Technology Architect to lead the design and implementation of enterprise grade Gen AI solutions This role demands deep technical expertise in AIML a strong architectural mindset and the ability to translate business needs into scalable secure and responsible AI systems You will serve as a thought leader mentor and strategic advisor across AI initiatives\n\nKey Responsibilities\nArchitect end-to-end Gen AI solutions including LLM based applications RAG pipelines and multimodal systems\nDefine and enforce architectural standards design patterns and best practices for Gen AI systems\nLead the evaluation and selection of Gen AI platforms frameworks and infrastructure eg vector DBs orchestration layers model hubs\nCollaborate with data scientists MLOps engineers and business stakeholders to align AI capabilities with enterprise goals\nDrive PoCs and pilots to validate Gen AI use cases and scale successful patterns into production\nEnsure compliance with data privacy security and responsible AI principles\nContribute to internal knowledge assets such as reference architectures whitepapers and reusable components\n\nRequired Skills\nProven experience in designing and deploying AIML systems at scale including Gen AI applications\nDeep understanding of LLMs prompt engineering finetuning and RAG workflows\nProficiency in Python and frameworks like PyTorch TensorFlow Hugging Face LangChain or LlamaIndex\nStrong grasp of cloudnative architectures AWS Azure GCP and containerization Docker Kubernetes\nExperience with MLOps tools MLflow SageMaker Vertex AI etc and CICD for AI pipelines\nFamiliarity with vector databases eg FAISS Pinecone Weaviate and embedding strategies\nShould have understanding of LLM models like Vision MultiModal etc\nStrong Experience in Python programming and related frameworks eg Django Flask\nExtensive experience in building scalable and robust applications using Python\nHandson Experience to all SDLC Phases\nKnowledge of cloud platforms eg AWS Azure GCP and their AIML services\nExperience with database systems eg SQL NoSQL and data modeling\nStrong problem-solving and analytical skills with the ability to translate business requirements into technical solutions\nExcellent leadership and team management skills with the ability to motivate and develop a high performing team Strong communication and collaboration skills with the ability to work effectively in cross functional teams\nSelf-motivated and proactive with a passion for learning and staying up to date with emerging technologies and industry trends\n\nPreferred Qualifications\nBachelors or Masters in Computer Science AI or related field\nExperience integrating Gen AI into enterprise platforms eg ERP CRM HRMS\nExposure to multimodal AI text image audio and agentbased architectures\nStrong communication and stakeholder management skills\nContributions to opensource AI projects or published research\n\nIf interested, kindly share your resume with below details to Richa.Rane@alphacom.in\nFull Name:\nPan Card No:\nDOB:\nTotal Experience:\nRelevant Experience:\nCurrent/Last Company: \nCurrent CTC:\nExpected CTC:\nNotice Period:\nCurrent Location:\nPreferred Location:\nReason for job change:\nShare your passport size photo:\n\nThanks & Regards,\nRicha Rane\nTalent Acquisition\nRicha.Rane@alphacom.in",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Gen AI', 'Artificial Intelligence', 'AI Architect', 'Aiml', 'Tensorflow', 'Pytorch', 'Ai Algorithms', 'Ai Techniques', 'Ai Solutions', 'ai', 'Ai Platform']",2025-06-12 06:25:23
Business Intelligence Lead,Trantor,8 - 10 years,Not Disclosed,[],"We are seeking a highly skilled Lead BI Developer with deep expertise in Tableau and Business\nIntelligence solutions to join our growing team. In this role, you will design and develop end-to-end BI solutions that empower data-driven decisions across the organization. You will collaborate closely with cross-functional teams, manage complex BI environments, and ensure seamless data visualization and reporting.\n1. Key Responsibilities\n\nBI Development & Data Visualization",,,,"['Business Intelligence', 'Tableau', 'ETL', 'Domo', 'SQL', 'Data Visualization']",2025-06-12 06:25:25
Principal Architect-Platform & Applications @ Bangalore_Urgent,"A leader in this space, we deliver world...",13 - 20 years,Not Disclosed,['Bengaluru'],"Principal Architect - Platform & Application Architect\n\nExperience\n15+ years in software/data platform architecture\n5+ years in architectural leadership roles\nArchitecture & Data Platform Expertise\n\nEducation\nBachelors/Master’s in CS, Engineering, or related field\n\n\nTitle: Principal Architect\n\nLocation: Onsite Bangalore\n\nExperience: 15+ years in software & data platform architecture and technology strategy\n\nRole Overview\n\nWe are seeking a Platform & Application Architect to lead the design and implementation of a next-generation, multi-domain data platform and its ecosystem of applications. In this strategic and hands-on role, you will define the overall architecture, select and evolve the technology stack, and establish best practices for governance, scalability, and performance. Your responsibilities will span across the full data lifecycle—ingestion, processing, storage, and analytics—while ensuring the platform is adaptable to diverse and evolving customer needs. This role requires close collaboration with product and business teams to translate strategy into actionable, high-impact platform & products.\n\nKey Responsibilities\n\n1. Architecture & Strategy\nDesign the end-to-end architecture for a On-prem / hybrid data platform (data lake/lakehouse, data warehouse, streaming, and analytics components).\nDefine and document data blueprints, data domain models, and architectural standards.\nLead build vs. buy evaluations for platform components and recommend best-fit tools and technologies.\n2. Data Ingestion & Processing\nArchitect batch and real-time ingestion pipelines using tools like Kafka, Apache NiFi, Flink, or Airbyte.\nOversee scalable ETL/ELT processes and orchestrators (Airflow, dbt, Dagster).\nSupport diverse data sources: IoT, operational databases, APIs, flat files, unstructured data.\n3. Storage & Modeling\nDefine strategies for data storage and partitioning (data lakes, warehouses, Delta Lake, Iceberg, or Hudi).\nDevelop efficient data strategies for both OLAP and OLTP workloads.\nGuide schema evolution, data versioning, and performance tuning.\n4. Governance, Security, and Compliance\nEstablish data governance, cataloging, and lineage tracking frameworks.\nImplement access controls, encryption, and audit trails to ensure compliance with DPDPA, GDPR, HIPAA, etc.\nPromote standardization and best practices across business units.\n5. Platform Engineering & DevOps\nCollaborate with infrastructure and DevOps teams to define CI/CD, monitoring, and DataOps pipelines.\nEnsure observability, reliability, and cost efficiency of the platform.\nDefine SLAs, capacity planning, and disaster recovery plans.\n6. Collaboration & Mentorship\nWork closely with data engineers, scientists, analysts, and product owners to align platform capabilities with business goals.\nMentor teams on architecture principles, technology choices, and operational excellence.\nSkills & Qualifications\n\nBachelor’s or Master’s degree in Computer Science, Engineering, or a related field.\n12+ years of experience in software engineering, including 5+ years in architectural leadership roles.\nProven expertise in designing and scaling distributed systems, microservices, APIs, and event-driven architectures using Java, Python, or Node.js.\nStrong hands-on experience with building scalable data platforms on premise/Hybrid/cloud environments.\nDeep knowledge of modern data lake and warehouse technologies (e.g., Snowflake, BigQuery, Redshift) and table formats like Delta Lake or Iceberg.\nFamiliarity with data mesh, data fabric, and lakehouse paradigms.\nStrong understanding of system reliability, observability, DevSecOps practices, and platform engineering principles.\nDemonstrated success in leading large-scale architectural initiatives across enterprise-grade or consumer-facing platforms.\nExcellent communication, documentation, and presentation skills, with the ability to simplify complex concepts and influence at executive levels.\nCertifications such as TOGAF or AWS Solutions Architect (Professional) and experience in regulated domains (e.g., finance, healthcare, aviation) are desirable.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Architecture', 'Principal Architect', 'on-prem data platforms data lakes', 'Data Platform', 'Designed hybrid', 'Airflow', 'Airbyte', 'architectural leadership', 'Kafka', 'lakehouses', 'Flink', 'or Node.js', 'microservices', 'Hudi', 'streaming', 'DWH', 'dbt', 'Dagster', 'Delta Lake', 'Iceberg', 'distributed systems', 'NiFi', 'Python']",2025-06-12 06:25:27
Azure and GenAi,Sightspectrum,5 - 10 years,20-30 Lacs P.A.,"['Hyderabad', 'Ahmedabad', 'Bengaluru']",Role & responsibilities\n\nGenAI\nAzure\nAI ML,Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['GenAi', 'Azure', 'AIML']",2025-06-12 06:25:29
Business Analyst,Shri Ramswaroop Digital Technology,1 - 6 years,Not Disclosed,['Lucknow'],"Job Responsibilities: • Independently consults with users regarding business needs and how business systems can support those needs\n• Conducting Product Demonstration\nLead efforts to gather and analyze data required to prepare business process and procedural documentation\nCapturing business requirements, processes and objectives, translating business requirements into a solution design\nEnsures activities required to change, maintain and administer applications meet system administration standards\nLead effort to develop Application test cases and test scripts for the preparation and execution of system testing, system integration testing, and user acceptance testing according to test plans. Validates detailed test cases are properly executed to ensure software and reports are functioning properly.\n• Conduct CRP, UAT and obtain necessary Sign off\n• Work with Project Management Office to execute consulting and project planning methodologies.\nManaging Cloud/SaaS-based ERP system\nProduce functional documentation for assigned projects. (BRDs, Fit-Gap Documents, Configuration Docs , Visio etc.)\n• Coordinating with teams which includes Developers, Testers & Designers working in a distributed environment\nProvide end user knowledge transfer and training as needed\nEscalating work priorities among team to Ensure Go-Live\nPost-go-live support\n\nMust Have\n\n• Knowledge of Microsoft Office (Word, Excel, PPT, Visio Creations)\n• Very good communication skills preferably English\n• Able to work under pressure and minimal supervision\n• Must have a streak of managing\n• Self-Motivated & Decision-Making Abilities",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Brd', 'Requirement Gathering', 'FRD', 'UAT']",2025-06-12 06:25:32
"AI Technical Lead Openings at Advantum Health, Hyderabad",Advantum Health,7 - 8 years,Not Disclosed,['Hyderabad'],"AI Technical Lead openings at Advantum Health Pvt Ltd, Hyderabad.\nOverview:\nWe are seeking an experienced AI Technical Lead to drive the architecture, development, and deployment of advanced AI and machine learning solutions, with a strong focus on healthcare and revenue cycle automation. You will oversee a cross-functional team to design intelligent systems, guide technical decisions, and ensure successful delivery of AI-enabled products.\nKey Responsibilities:\nLead the design and development of scalable AI/ML solutions for healthcare automation and RCM workflows.\nCollaborate with stakeholders to translate business requirements into machine learning problems.\nProvide technical leadership on model selection, training, evaluation, and deployment.\nMentor a team of data scientists, engineers, and developers.\nEnsure model governance, version control, auditability, and ethical use of AI.\nEvaluate emerging technologies and make recommendations for adoption.\nQualifications:\nBachelors or Masters in Computer Science, AI, or related field.\n7+ years of experience in software development; 3+ in AI/ML leadership.\nProven experience deploying AI models in production (e.g., NLP, computer vision, anomaly detection).\nProficiency in Python, Tensorflow, Pytorch, and cloud-based AI platforms.\nStrong knowledge of RCM or healthcare data preferred.\nPh: 9177078628\nEmail id: jobs@advantumhealth.com\nAddress: Advantum Health Private Limited, Cyber gateway, Block C, 4th floor Hitech City, Hyderabad.\nDo follow us on LinkedIn, Facebook, Instagram, YouTube and Threads\nAdvantum Health LinkedIn Page:\nhttps://lnkd.in/gVcQAXK3\n\nAdvantum Health Facebook Page:\nhttps://lnkd.in/g7ARQ378\n\nAdvantum Health Instagram Page:\nhttps://lnkd.in/gtQnB_Gc\n\nAdvantum Health India YouTube link:\nhttps://lnkd.in/g_AxPaPp\n\nAdvantum Health Threads link:\nhttps://lnkd.in/gyq73iQ6",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Pytorch', 'Tensorflow', 'Python']",2025-06-12 06:25:34
Gen AI Architect,client of edge,9 - 12 years,Not Disclosed,"['Pune', 'Chennai', 'Bengaluru']","The Company\nIndia's marquee global technology & consulting company. They are an international flag-bearer of technical and managerial excellence. With offices around the globe, the company has a comprehensive presence across multiple segments of the technology product and service industries as well as a blue-chip roster of clients for their Consulting engagements. They are a respected career company and a long-term wealth creator.\nThe Job\nWe are seeking a highly skilled Generative AI Architect with 10+ Years of IT industry experience in which 5/6 years should be in AI/ML/DS domain, including Gen AI technologies to lead the design and deployment of cutting-edge AI solutions. The ideal candidate will possess deep expertise in Generative AI, strong analytical skills, and excellent communication abilities to engage with clients and internal teams. This role involves designing AI architectures, implementing scalable models, and ensuring solutions align with business objectives.\nKey Responsibilities\nCollaborate with clients to understand AI project needs and define measurable success metrics\nDesign end-to-end Generative AI solutions with a focus on performance, security, and ethical AI\nDevelop models using GANs, VAEs, NLP transformers, and other generative techniques\nImplement AI solutions utilizing cloud platforms (AWS, Azure, GCP) and ML frameworks (TensorFlow, PyTorch, LangChain, Semantic Kernels)\nEnsure compliance with Responsible AI and Data Privacy principles, mitigating bias and ensuring ethical AI practices\nWork alongside engineers and data scientists to integrate and optimize AI models into real-world applications\nConduct performance monitoring, troubleshooting, and continuous improvements for AI systems\nStay updated with AI advancements and share knowledge with internal teams\nYour Profile\nPrimary Skills\nExpertise in Generative AI techniques, including image generation, text synthesis, and function calling\nStrong understanding of AI/ML algorithms, cloud computing, NLP, and computer vision\nProficiency in data science tools (NumPy, SciPy, Pandas, Matplotlib, TensorFlow, Keras)\nSolid grasp of Responsible AI, fairness in algorithms, and bias mitigation strategies\nExcellent communication and solution design skills, capable of translating business needs into technical AI architectures.\nSecondary Skills\nKnowledge of industry-specific AI applications (healthcare, finance, manufacturing, etc.)\nBasic project management abilities to oversee timelines and deliverables\nFoundational data preprocessing and quality assurance expertise",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Genrative Ai', 'Natural Language Processing', 'Python', 'Computer Vision', 'Solution Design', 'Ml']",2025-06-12 06:25:36
Senior Manager Information Systems,Amgen Inc,8 - 13 years,Not Disclosed,['Hyderabad'],"What you will do\nIn this vital role you will develop an insight driven sensing capability with a focus on revolutionizing decision making. In this role you will lead the technical delivery for this capability as part of a team data engineers and software engineers. The team will rely on your leadership to own and refine the vision, feature prioritization, partner alignment, and experience leading solution delivery while building this ground-breaking new capability for Amgen. You will drive the software engineering side of the product release and will deliver for the outcomes.\nRoles & Responsibilities:\nLead delivery of overall product and product features from concept to end of life management of the product team comprising of technical engineers, product owners and data scientists to ensure that business, quality, and functional goals are met with each product release\nDrives excellence and quality for the respective product releases, collaborating with Partner teams.\nImpacts quality, efficiency and effectiveness of own team. Has significant input into priorities.\nIncorporate and prioritize feature requests into product roadmap; Able to translate roadmap into execution\nDesign and implement usability, quality, and delivery of a product or feature\nPlan releases and upgrades with no impacts to business\nHands on expertise in driving quality and best in class Agile engineering practices\nEncourage and motivate the product team to deliver innovative and exciting solutions with an appropriate sense of urgency\nManages progress of work and addresses production issues during sprints\nCommunication with partners to make sure goals are clear and the vision is aligned with business objectives\nDirect management and staff development of team members\nWhat we expect of you\nWe are all different, yet we all use our unique contributions to serve patients.\nBasic Qualifications:\nMasters degree and 8 to 10 years of Information Systems experience OR\nBachelors degree and 10 to 14 years ofInformation Systems experience OR\nDiploma and 14 to 18 years of Information Systems experience\nThorough understanding of modern web application development and delivery, Gen AI applications development, Data integration and enterprise data fabric concepts, methodologies, and technologies e.g. AWS technologies, Databricks\nDemonstrated experience in building strong teams with consistent practices.\nDemonstrated experience in navigating matrix organization and leading change.\nPrior experience writing business case documents and securing funding for product team delivery; Financial/Spend management for small to medium product teams is a plus.\nIn-depth knowledge of Agile process and principles.\nDefine success metrics for developer productivity metrics; on a monthly/quarterly basis analyze how the product team is performing against established KPIs.\nFunctional Skills:\nLeadership:\nInfluences through Collaboration: Builds direct and behind-the-scenes support for ideas by working collaboratively with others.\nStrategic Thinking: Anticipates downstream consequences and tailors influencing strategies to achieve positive outcomes.\nTransparent Decision-Making: Clearly articulates the rationale behind decisions and their potential implications, continuously reflecting on successes and failures to enhance performance and decision-making.\nAdaptive Leadership: Recognizes the need for change and actively participates in technical strategy planning.\nPreferred Qualifications:\nStrong influencing skills, influence stakeholders and be able to balance priorities.\nPrior experience in vendor management.\nPrior hands-on experience leading full stack development using infrastructure cloud services (AWS preferred) and cloud-native tools and design patterns (Containers, Serverless, Docker, etc.)\nExperience with developing solutions on AWS technologies such as S3, EMR, Spark,\nAthena, Redshift and others\nFamiliarity with cloud security (AWS /Azure/ GCP)\nConceptual understanding of DevOps tools (Ansible/ Chef / Puppet / Docker /Jenkins)\nProfessional Certifications\nAWS Certified Solutions Architect (preferred)\nCertified DevOps Engineer (preferred)\nCertified Agile Leader or similar (preferred)\nSoft Skills:\nStrong desire for continuous learning to pick new tools/technologies.\nHigh attention to detail is essential with critical thinking ability.\nShould be an active contributor on technological communities/forums\nProactively engages with cross-functional teams to resolve issues and design solutions using critical thinking and analysis skills and best practices.\nInfluences and energizes others toward the common vision and goal. Maintains excitement for a process and drives to new directions of meeting the goal even when odds and setbacks render one path impassable\nEstablished habit of proactive thinking and behavior and the desire and ability to self-start/learn and apply new technologies\nExcellent organizational and time-management skills\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nTeam-oriented, with a focus on achieving team goals\nStrong presentation and public speaking skills.\nShift Information:\nThis position requires you to work a later shift and may be assigned a second or third shift schedule. Candidates must be willing and able to work during evening or night shifts, as required based on business requirements.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Information Systems', 'Azure', 'DevOps', 'Gen AI application development', 'GCP', 'Data integration', 'AWS', 'web application development']",2025-06-12 06:25:39
AI/ML Architect @ Bangalore_Urgent,"A leader in this space, we deliver world...",10 - 15 years,Not Disclosed,['Bengaluru'],"AI/ML Architect\n\nExperience\n10+ years in total, 8+ years in AI/ML development\n3+ years in AI/ML architecture\nEducation\nBachelors/Masters in CS, AI/ML, Engineering, or similar\n\nTitle: AI/ML Architect\nLocation: Onsite Bangalore\nExperience: 10+ years\nPosition Summary:\nWe are seeking an experienced AI/ML Architect to lead the design and deployment of scalable AI solutions. This role requires a strong blend of technical depth, systems thinking, and leadership in machine learning, computer vision, and real-time analytics. You will drive the architecture for edge, on-prem, and cloud-based AI systems, integrating 3rd party data sources, sensor and vision data to enable predictive, prescriptive, and autonomous operations across industrial environments.\nKey Responsibilities:\nArchitecture & Strategy\nDefine the end-to-end architecture for AI/ML systems including time series forecasting, computer vision, and real-time classification.\nDesign scalable ML pipelines (training, validation, deployment, retraining) using MLOps best practices.\nArchitect hybrid deployment models supporting both cloud and edge inference for low-latency processing.\nModel Integration\nGuide the integration of ML models into the IIoT platform for real-time insights, alerting, and decision support.\nSupport model fusion strategies combining disparate data sources, sensor streams with visual data (e.g., object detection + telemetry + 3rd party data ingestion).\nMLOps & Engineering\nDefine and implement ML lifecycle tooling, including version control, CI/CD, experiment tracking, and drift detection.\nEnsure compliance, security, and auditability of deployed ML models.\nCollaboration & Leadership\nCollaborate with Data Scientists, ML Engineers, DevOps, Platform, and Product teams to align AI efforts with business goals.\nMentor engineering and data teams in AI system design, optimization, and deployment strategies.\nStay ahead of AI research and industrial best practices; evaluate and recommend emerging technologies (e.g., LLMs, vision transformers, foundation models).\nMust-Have Qualifications:\nBachelors or Master’s degree in Computer Science, AI/ML, Engineering, or a related technical field.\n8+ years of experience in AI/ML development, with 3+ years in architecting AI solutions at scale.\nDeep understanding of ML frameworks (TensorFlow, PyTorch), time series modeling, and computer vision.\nProven experience with object detection, facial recognition, intrusion detection, and anomaly detection in video or sensor environments.\nExperience in MLOps (MLflow, TFX, Kubeflow, SageMaker, etc.) and model deployment on Kubernetes/Docker.\nProficiency in edge AI (Jetson, Coral TPU, OpenVINO) and cloud platforms (AWS, Azure, GCP).\nNice-to-Have Skills:\nKnowledge of stream processing (Kafka, Spark Streaming, Flink).\nFamiliarity with OT systems and IIoT protocols (MQTT, OPC-UA).\nUnderstanding of regulatory and safety compliance in AI/vision for industrial settings.\nExperience with charts, dashboards, and integrating AI with front-end systems (e.g., alerts, maps, command center UIs).\nRole Impact:\nAs AI/ML Architect, you will shape the intelligence layer of our IIoT platform — enabling smarter, safer, and more efficient industrial operations through AI. You will bridge research and real-world impact, ensuring our AI stack is scalable, explainable, and production-grade from day one.",Industry Type: Emerging Technologies (AI/ML),Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['AI/ML architecture', 'cloud', 'PyTorch', 'TensorFlow', 'time series modeling', 'MLflow', 'object detection', 'anomaly detection', 'Kubeflow', 'Docker', 'OpenVINO', 'MLOps tools', 'AWS', 'facial', 'Azure', 'TFX', 'IIoT platforms', 'SageMaker', 'intrusion', 'CI/CD pipelines', 'GCP', 'Jetson', 'Coral', 'computer vision', 'edge AI', 'Kubernetes']",2025-06-12 06:25:42
Technical Lead (Java),Prodapt Solutions,6 - 10 years,Not Disclosed,"['Chennai', 'Guindy']","Overview\n\n:\nWe are seeking an experienced Technical Lead with a strong background in Java software development . The ideal candidate should possess hands-on coding expertise , architectural understanding , and leadership abilities to drive development teams in building scalable and high-performance applications . This role involves technical mentorship, solution architecture, and ensuring best development practices are followed.\n\nResponsibilities\n\nRoles and Responsibilities:\n1. Technical Leadership & Solution Architecture\nDefine technical architecture and design for Java-based applications.\nProvide technical direction and mentor software engineers.\nConduct code reviews to ensure high-quality standards.\nDefine best practices for development, security, and performance optimization .\nGuide the migration of legacy applications to modern frameworks .\n\n2. Software Development & Deployment\nDesign, develop, and maintain scalable Java microservices .\nWork on database architecture & optimization .\nImplement automated CI/CD pipelines for seamless deployments.\nOptimize backend performance, caching, and data processing .\n\n3. Cross-Team Collaboration\nWork closely with Product Owners, UX/UI Designers, and DevOps .\nCollaborate with Cloud, Security, and Data Engineering Teams .\nEnsure alignment with business goals & technical feasibility .\n\n4. Cloud & DevOps Implementation\nDeploy applications to AWS, Azure, or GCP using containerization (Docker, Kubernetes) .\nManage scalability, monitoring, and logging (Azure Monitor, AWS CloudWatch, Prometheus, ELK Stack) .\nAutomate infrastructure provisioning & cloud resource management .\n\n5. Agile & Team Management\nParticipate in sprint planning, standups, retrospectives .\nTrack and manage work using JIRA, Trello, or Azure DevOps .\nTrain and mentor junior developers and ensure knowledge sharing.\n\nPrimary\n\nSkills:\nCore Java, Java 8+ (or latest version)\nSpring Boot, Spring Framework (Spring MVC, Spring Security, Spring Cloud)\nMicroservices Architecture & API Development\nRESTful Web Services, GraphQL (optional but preferred)\nDatabase Management (MySQL, PostgreSQL, MongoDB)\nMessage Brokers (Kafka, RabbitMQ)\nCloud Services (AWS, Azure, GCP Any one preferred)\nDevOps & CI/CD (Docker, Kubernetes, Jenkins, GitHub Actions, Terraform)\nSecurity & Authentication (OAuth2, JWT, SSO, OpenID)\nPerformance Optimization & System Scalability\n\nSecondary\n\nSkills:\nFrontend Framework Knowledge (React.js, Angular, or Vue.js)\nContainerization & Orchestration (Docker, Kubernetes)\nEvent-Driven Architecture (Kafka, RabbitMQ, ActiveMQ)\nInfrastructure as Code (Terraform, CloudFormation)\nUnit Testing & Automation (JUnit, Mockito, Cypress)\nAgile & Scrum Practices (JIRA, Confluence, Standups, Sprint Planning)\nTechnical Documentation & Architectural Design Patterns\nAI & Machine Learning Basics (Optional but good to have)",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['microservices', 'spring', 'java', 'spring cloud', 'spring boot', 'continuous integration', 'kubernetes', 'ci/cd', 'mockito', 'docker', 'react.js', 'spring mvc', 'devops', 'jenkins', 'api', 'jira', 'rest', 'junit', 'software development', 'microsoft azure', 'rabbitmq', 'spring security', 'kafka', 'terraform', 'agile', 'aws']",2025-06-12 06:25:44
Sr Manager Information Security,Amgen Inc,8 - 10 years,Not Disclosed,['Hyderabad'],"What you will do\nIn this vital role As a Data Security Senior Manager, you will responsible to lead, operate, manage and improve Amgens Data Loss Prevention (DLP) , Cloud Access Security Broker (CASB), and Data Classification services. This position will be responsible for delivering data protection services across Amgens global enterprise. The role will work with architects, engineers and business units to help design, build, and implement critical preventive and detective security controls. This role will lead the team responsible for the protection of Amgen data in a rapidly changing security sector.\nRoles & Responsibilities:\nMaintain the service delivery and working order of Amgen Data Protection solutions across Amgens global enterprise by leading the distributed team of data security analysts and engineers\nExecute Amgen service management processes such as Incident Management, Organisational Change, Service Requests, etc. for Amgens DLP / CASB solutions\nAdvise and consult to business domain experts to collect, analyze, create, tune and automate DLP /CASB policy sets\nTrain and manage the team, including other leaders to analyze events and logs for opportunities to improve SaaS, Classification, and DLP policies\nSynthesize evolving business ecosystem changes to proactively identify new controls to and opportunities to improve data protection practices\nAs needed, support Legal, Human Resources, and Incident Response teams in investigations related to data usage incidents\nMaintain the needed subject matter expertise to keep current, make recommendations, and lead or participate in the implementation and continuous improvement of technologies and services in assigned information security domains\nAct as main contact in audits covering information security services and technologies\nAdvise on cryptographic services to protect the confidentiality and integrity of data at rest and in transit\nCollaborates multi-functionally with analysts, engineers, data scientists to deliver continuous improvement in cyber defense/resilience.\n\nBasic Qualifications:\nMasters degree and 8 to 10 years of experience OR\nBachelors degree and 10 to 14 years of experience OR\nDiploma and 14 to 18 years of experience\n\nFunctional Skills:\nMust-Have Skills:\nTrack record of leading multi-level and matrixed teams in the operations of security services at a large enterprise.\nKnowledge of Cloud Access Security Platforms (Elastica, Netskope, SkyHigh,etc)\nUnderstanding of cloud environment (AWS, O365, Box, Salesforce, etc)\nExperience with Data Protection Technologies for a global enterprise\nSolid knowledge of core cryptographic services (Confidentiality, Data Integrity Verification, Authentication, Non-repudiation) and their applications\nCompetent understanding on how security technologies and data flows (on-prem / cloud) integrate\n\nGood-to-Have Skills:\nExperience and ability to mentor and train others\nService delivery experience including headcount and budgetary planning\nStrong effective verbal and written communication skills including a mastery of Standard American Business English and experience with both technical and persuasive writing\nBasic experience with ITIL processes such as Incident/Problem/Configuration/Change management with a focus on metric-driven delivery\nProfessional Certifications (please mention if the certification is preferred or mandatory for the role):\nCISSP or equivalent preferred\n\nSoft Skills:\nEstablished analytical and gap/fit assessment skills.\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nTeam-oriented, with a focus on achieving team goals\nEffective presentation and public speaking skills.",Industry Type: Pharmaceutical & Life Sciences,Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Information Security', 'Change management', 'Configuration', 'service delivery', 'SaaS', 'Problem management', 'Incident management', 'ITIL processes']",2025-06-12 06:25:47
Java Backend Developer - Kotlin,Sadup Soft,5 - 10 years,Not Disclosed,"['Chennai', 'Delhi / NCR', 'Bengaluru']","We are seeking a talented Java backend Software Engineer with expertise in software engineering to join our team. As a Full Stack Software Engineer, your primary responsibility will be to develop & integrate Generative AI solutions that focus on technology improvements. Specifically, you will be working on projects involving Generative AI solutions for Technology Assistants & Data Management Efficiencies and will contribute for Java backend development such as IDE plugins Data Connections & Integrators.\n\nResponsibilities :\n\n- Collaborate with cross-functional teams such as Data Scientists, Product Partners and Partner Team Developers to identify opportunities for process improvements that can be solved using machine learning and generative AI.\n\n- Write clean, high-performance, high-quality, maintainable code.\n\n- Create connectors to various Content and Collaboration tools using tool specific API's such as Jira, Slack, Git etc\n\n- Create backend applications using Java, docker & in-house frameworks to orchestrate AI applications\n\n- Design and develop Engineering Solutions & generative AI Applications for above ensuring scalability, efficiency, and maintainability of such solutions.\n\n- Implement prompt engineering techniques to fine-tune and enhance LLMs for better performance and application-specific needs.\n\n- Stay abreast of the latest advancements in the field of Generative AI Application Development and actively contribute to the research and development of new Generative AI Applications.\n\nRequirements:\n\n- A Master's or Ph.D. degree in Computer Science or a related field.\n\n- Proven experience working as a Software Engineer, with a focus on Java, and exposure to Generative AI Applications such as chatGPT.\n\n- Strong proficiency in programming languages such as Java, Kotlin, Scala etc ( Mandatory)\n\n- Experience or Exposure in creating IDE Plugins for PyCharm, VS Code, IntelliJ & Web Consoles ( Nice to have )\n\n- Experience in python and data pipelines ( nice to have)\n\n- Solid knowledge of software engineering best practices, including version control systems (e.g., Git), code reviews, and testing methodologies.\n\n- Experience with large language models (LLMs) & prompt engineering techniques, vector DB's ( Nice to have )\n\n- Strong communication skills to effectively collaborate and present findings to both technical and non-technical stakeholders.\n\n- Proven ability to adapt and learn new technologies and frameworks quickly.\n\n- A proactive mindset with a passion for continuous learning and research.\nLocation: Delhi NCR,Bangalore,Chennai,Pune,Kolkata,Ahmedabad,Mumbai,Hyderabad",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Java', 'Application Designing', 'Prompt Engineering', 'Generative AI', 'ChatGPT', 'Full Stack', 'LLM', 'Kotlin', 'Python']",2025-06-12 06:25:50
o9 SSIS integration Consultant,Thoucentric,3 - 8 years,Not Disclosed,"['Hyderabad', 'Chennai', 'Bengaluru']","At Thoucentric, we work on various problem statements.\nThe most popular ones are -\nBuilding capabilities that address a market need, basis our ongoing research efforts\nSolving a specific use case for a current or potential client based on challenges on-ground\nDeveloping new systems that help be a better employer and a better partner to clients",,,,"['O9', 'SSIS', 'SQL']",2025-06-12 06:25:53
Jr.AI Engineer,Tekone It Services,1 - 3 years,1.5-6.5 Lacs P.A.,['Hyderabad'],"Position Overview\nWe are hiring five AI Engineers with 12 years of experience to join our dynamic team in Hyderabad. The ideal candidates will have a solid foundation in Large Language Models (LLMs), LangChain, and Generative AI (GenAI) frameworks. This is a great opportunity to work on innovative AI solutions, contributing to projects that integrate LLMs, prompt engineering, RAG pipelines, and cloud-based deployments.\nKey Responsibilities\nContribute to the design and development of AI-powered applications utilizing LLMs (GPT-3.5, GPT-4, Gemini).\nAssist in building LangChain-based pipelines and workflows, including LangSmith and LangGraph.\nSupport the implementation of Retrieval-Augmented Generation (RAG) frameworks using vector databases such as ChromaDB.\nApply prompt engineering techniques to optimize model responses and improve contextual accuracy.\nDevelop RESTful APIs using Flask or FastAPI to enable model consumption in production environments.\nWrite and manage data workflows using SQL, PySpark, and Spark SQL.\nDeploy and monitor models on Azure Machine Learning or AWS Bedrock platforms.\nCollaborate with cross-functional teams, including data scientists, engineers, and business stakeholders.\nRequired Skills\nProficiency in Python, SQL, PySpark, and Spark SQL\nHands-on experience with LLMs: GPT-3.5, GPT-4, Gemini\nKnowledge of LangChain, LangSmith, LangGraph\nFamiliarity with Vector Databases (e.g., ChromaDB) and embeddings\nExperience with prompt engineering and RAG-based architectures\nExposure to cloud platforms such as Azure ML or AWS Bedrock\nStrong understanding of REST APIs and version control systems (Git/GitHub)\nPreferred Qualifications\nBachelor's degree in Computer Science, Artificial Intelligence, Data Science, or a related field\nInternship or academic project experience in NLP, LLMs, or GenAI technologies\nFamiliarity with MLOps tools and practices (e.g., CI/CD, Airflow)\nStrong problem-solving abilities, attention to detail, and a collaborative mindset",Industry Type: Software Product,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['python', 'Prompt Engineering', 'Artificial Intelligence', 'llm']",2025-06-12 06:25:55
Artificial Intelligence Engineer,Winningstrategy Consulting,2 - 4 years,6-9 Lacs P.A.,['Mumbai'],"Seeking AI Engineer to build intelligent, task-driven agents using React & FastAPI. Must blend AI/ML expertise with software skills to create scalable, modular systems for API/UI interaction.\n\nRequired Candidate profile\n1. 2+ yrs in AI dev\n2. Strong in FastAPI, React, Python\n3. Built LLM-based agent workflows\n4. Used vector DB, LangChain, OpenAI\n5. Deployed on cloud(Azure, AWS, GCP)\n6. Scalable, UI-integrated systems",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Artificial Intelligence', 'Machine Learning', 'Cloud Deployment', 'Vector Db', 'Langchain', 'Natural Language Processing', 'Python Development']",2025-06-12 06:25:57
"Product Manager, AI",Conga,3 - 5 years,Not Disclosed,['Bengaluru( Kadubeesanahalli )'],"Job Title: Product Manager\nLocation: Bangalore\nReports to: Manager, Product Management\n\nA quick snapshot\n\nAs a Product Manager on the Conga Discovery AI team, you will help define, build, and launch AI-driven metadata extraction solutions on top of the Conga platform. Youll bring your experience in AIand ideally Contract Lifecycle Management (CLM)—to shape products that help enterprises discover, process, and analyze critical business data. This role is hands-on, requiring you to be comfortable demonstrating features, collaborating on implementations, and working closely with scrum teams to drive value for our customers.\n\nWhy it’s a big deal\n\nMetadata extraction is at the core of how businesses understand their documents and processes. By leveraging Discovery AI, you’ll help enterprises transform manual, time-consuming tasks into automated workflows that reduce errors, improve compliance, and deliver actionable insights. Your role will be central to creating and refining solutions that can scale to handle the most complex enterprise needs.\n\nAre you the person we’re looking for?\n\nRelevant Experience. You should have more than 3 years of experience in Product Management in B2B SaaS, preferably with data extraction, AI, or document automation products.\n\nDemonstrate. A success in conceptualizing and launching new product features from initial idea to market adoption.\n\nAI or machine learning. Knowledge of fundamentals and an interest in applying them to solve real-world business challenges.\n\nCLM. Exposure or understanding in CLM is a strong plus, as it ties closely into many metadata extraction use cases.\n\nResearch and Creativity. Conduct market and user research to identify new opportunities for AI-driven features.\n\nCustomer feedback. You will gather continuous customer feedback to iterate and prioritize feature development that delivers tangible customer value.\n\nMaintain strong partnerships. With professional services and support teams to address implementation details and customer escalations.\n\nDemo. You should confidently demo features to internal stakeholders, customers, and prospects to showcase Discovery AI capabilities.\n\nAnalyze and prioritize. You will use data analytics, user feedback, and market insights to guide product decisions and roadmap priorities.\n\nBalance. customer requirements, technical feasibility, and time-to-market considerations in a fast-paced environment.\n\nCustomer experience. You will regularly engage with customers to understand their needs and pain points, ensuring Discovery AI addresses real-world challenges.\n\nDocument. New workflows, provide training materials or guidelines, and gather post-launch feedback.\n\nEducation. Bachelor’s degree in Engineering or equivalent; a higher degree is a plus.\n\nHere’s what will give you an edge\n\nChampion the Customer. You appreciate that customers are the heart of the business, and you’re dedicated to delivering solutions that solve their problems while meeting them where they are.\n\nNatural collaborator. You thrive in an Agile, cross-functional setting, seeking input from engineers, designers, data scientists, and peers to make well-rounded product decisions.\n\nPassion. Your genuine enthusiasm for AI and data-driven solutions is evident in your work. You love delving into technical details, exploring new possibilities, and shaping products that redefine how companies operate.\n\nStrong Communication. You will communicate releases, risks, and timelines effectively to leadership and cross-functional stakeholders.\n\nCollaborate. With marketing to position and promote new features that drive adoption and user satisfaction.\n\nLeadership skills. You will work closely with scrum teams to ensure clear backlog priorities, smooth sprint planning, and timely delivery.",Industry Type: Software Product,Department: Product Management,"Employment Type: Full Time, Permanent","['Product Strategy', 'Product Management', 'Product Portfolio', 'Product Life Cycle Management', 'Product Planning']",2025-06-12 06:26:00
AI Conversational Chatbot Developer,Benovymed Healthcare,1 - 4 years,Not Disclosed,['Lucknow'],"Develop and enhance AI-driven chatbot solutions to automate customer interactions and optimize response accuracy.\nDesign and implement NLP algorithms using frameworks like Rasa, Dialogflow, or IBM Watson.\nEnsure seamless chatbot integration with existing platforms and CRM systems.\nWork closely with data scientists to improve machine learning models.\nMust have strong proficiency in Python, deep learning techniques, and chatbot analytics.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Chatbot Development', 'deep learning', 'chatbot', 'python', 'r', 'data analytics', 'predictive modeling', 'machine learning', 'artificial intelligence', 'chatbot analytics']",2025-06-12 06:26:02
AI Conversational Chatbot Developer,Benovymed Healthcare,1 - 4 years,Not Disclosed,['Agra'],"Develop and enhance AI-driven chatbot solutions to automate customer interactions and optimize response accuracy.\nDesign and implement NLP algorithms using frameworks like Rasa, Dialogflow, or IBM Watson.\nEnsure seamless chatbot integration with existing platforms and CRM systems.\nWork closely with data scientists to improve machine learning models.\nMust have strong proficiency in Python, deep learning techniques, and chatbot analytics.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Chatbot Development', 'deep learning', 'chatbot', 'python', 'r', 'data analytics', 'predictive modeling', 'machine learning', 'artificial intelligence', 'chatbot analytics']",2025-06-12 06:26:04
Software Engineer-8910,WebMD,3 - 8 years,Not Disclosed,['Navi Mumbai'],"Position: Software Engineer\npositions: 1\nNo.of\nAbout Company:\nHeadquartered in El Segundo, Calif., Internet Brands is a fully integrated online media and\nsoftware services organization focused on four high-value vertical categories: Health,\nAutomotive, Legal, and Home/Travel. The company's award-winning consumer websites\nlead their categories and serve more than 250 million monthly visitors, while a full range of\nweb presence offerings has established deep, long-term relationships with SMB and\nenterprise clients. Internet Brands' powerful, proprietary operating platform provides the\nflexibility and scalability to fuel the company's continued growth. Internet Brands is a portfolio\ncompany of KKR and Temasek.\nWebMD Health Corp., an Internet Brands Company, is the leading provider of health\ninformation services, serving patients, physicians, health care professionals, employers, and\nhealth plans through our public and private online portals, mobile platforms, and health-\nfocused publications. The WebMD Health Network includes WebMD Health, Medscape,\nJobson Healthcare Information, prIME Oncology, MediQuality, Frontline, QxMD, Vitals\nConsumer Services, MedicineNet, eMedicineHealth, RxList, OnHealth, Medscape\nEducation, and other owned WebMD sites. WebMD, Medscape, CME Circle,\nMedpulse®, eMedicine®, MedicineNet®, theheart.org®, and RxList® are among the\ntrademarks of WebMD Health Corp. or its subsidiaries.\nAll qualified applicants will receive consideration for employment without regard to race,\ncolor, religion, sex, sexual orientation, gender identity, national origin, disability, or veteran\nstatus.\nFor Company details, visit our website: www.webmd.com and www.internetbrands.com\nAll qualified applicants will receive consideration for employment without regard to\nrace, color, religion, sex, sexual orientation, gender identity, national origin, disability,\nor veteran status.\nFor Company details, visit our website: www.webmd.com\nhttps://www.webmd.com/corporate/physicians-interactive",,,,"['ETL', 'SQL', 'Data Warehousing']",2025-06-12 06:26:06
TA Manager- Only IT Hiring Experts- Immediate Joiners,Mascot E Services,9 - 14 years,9-15 Lacs P.A.,['Bengaluru'],"Read Carefully:-\n\nRecruitment (IC & Lead ) Mandatory Skills in hiring- Full stack , Data scientist , AI and Data Analytics(Data Mining & Data Governance), sourcing and has hands on stakeholder mgmt exp\n\nWhatsapp CV to Saurabh 9818385050@ Write TA\n\nRequired Candidate profile\nRecruitment (IC & Lead ) Skills – Full stack , Data scientist, AI and Data Analytics(Data Mining & Governance), sourcing, hands on stakeholder management\n\nWhatsapp CV to Saurabh 9818385050 @ Write TA",Industry Type: Analytics / KPO / Research,Department: Human Resources,"Employment Type: Full Time, Permanent","['Talent Acquisition', 'Technical Hiring', 'IT Staffing', 'IT Recruitment', 'Technology Recruitment', 'Technology Hiring', 'It Hiring', 'Technical Recruitment', 'TA']",2025-06-12 06:26:08
AI Conversational Chatbot Developer,Benovymed Healthcare,1 - 4 years,Not Disclosed,['Chandigarh'],"Develop and enhance AI-driven chatbot solutions to automate customer interactions and optimize response accuracy.\nDesign and implement NLP algorithms using frameworks like Rasa, Dialogflow, or IBM Watson.\nEnsure seamless chatbot integration with existing platforms and CRM systems.\nWork closely with data scientists to improve machine learning models.\nMust have strong proficiency in Python, deep learning techniques, and chatbot analytics.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['AI', 'deep learning techniques', 'Rasa', 'NLP algorithms', 'Dialogflow', 'Python', 'chatbot analytics']",2025-06-12 06:26:10
AI/ML Engineer,Oak Tree Cloud Software,3 - 5 years,Not Disclosed,['Indore'],"Job Title: Python Developer AI/ML & Generative AI\nExperience: 3+ Years\nLocation: Indore (WFO)\nEmployment Type: Full-Time\nIndustry: AI / Technology / Software Development\n\nJob Description\nWe are seeking a skilled Python Developer with 3+ years of hands-on experience in Artificial Intelligence (AI), Machine Learning (ML), Deep Learning (DL) and Generative AI (GenAI). The ideal candidate will be passionate about building intelligent systems and creating real-world applications using modern AI tools and frameworks.\n\nKey Responsibilities:\nDevelop and deploy AI/ML models using Python for real-world use cases including classification, regression, clustering, NLP, and computer vision.\nDesign and fine-tune Generative AI models, including transformers, large language models (LLMs), and text-to-image/audio tools.\nImplement data preprocessing pipelines, feature engineering, and model evaluation metrics.\nCollaborate with cross-functional teams (data scientists, backend developers, and product managers) to integrate models into applications or APIs.\nOptimize and scale ML/AI pipelines for performance and accuracy using techniques like model compression or distributed training.\nWork with GenAI frameworks/tools such as Hugging Face Transformers, LangChain, OpenAI API, or LLaMA.\nPerform research and experimentation with state-of-the-art models and suggest improvements for production use.\nMaintain clear documentation for models, datasets, experiments, and deployment procedures.\nRequired Skills & Qualifications:\nStrong proficiency in Python with focus on data structures, OOPs, and libraries like NumPy, Pandas, Scikit-learn, and Matplotlib.\nExperience with AI/ML frameworks such as TensorFlow, PyTorch, or Keras.\nPractical knowledge of Gen AI tools, including Hugging Face Transformers, OpenAI GPT models, or LLM fine-tuning.\nUnderstanding of ML lifecycle, from data cleaning to model deployment and monitoring.\nDevelop and implement data extraction pipelines for unstructured documents using OCR techniques and libraries (e.g., Tesseract, EasyOCR) to extract text and structured data from PDFs (PyMuPDF), scanned images, and DOCX files.\nExperience with NLP techniques, text generation, summarization, or vector embeddings.\nHands-on experience with REST APIs, FastAPI or Flask to deploy and serve models.\nFamiliarity with version control (Git), CI/CD pipelines, and containerization (Docker).\nBachelor's/Master’s in Computer Science, Artificial Intelligence, Data Science, or related field.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Generative Ai', 'Machine Learning', 'Deep Learning', 'Python']",2025-06-12 06:26:12
Power Bi Engineer,Scalable Systems,4 - 6 years,10-15 Lacs P.A.,['Kochi'],Power Bi Engineer\nLocation : Kochi\nFulltime,Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Power Bi', 'Data Visualization', 'Power System', 'Dashboard Development']",2025-06-12 06:26:15
AI/ML Associate Architect,Resources Valley,8 - 13 years,Not Disclosed,"['Indore', 'Gurugram', 'Jaipur']","experience in Designing and architecting solutions with artifacts and technical documents using LLM, Generative AI, RAG, and Agentic AI applications\nStrong understanding of LLM, Generative AI, RAG, and Agentic AI app.\nExp working with large datasets\n\nRequired Candidate profile\nWorking with data scientists, engineers, and other stakeholders to understand and implement ml algorithms such as regression, classification, and clustering.\nLeading teams on the latest AI tools .",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Aiml', 'Artificial Intelligence', 'Machine Learning', 'Python']",2025-06-12 06:26:17
BI Engineer,Amgen Inc,2 - 5 years,Not Disclosed,['Hyderabad'],"Role Description:\nLets do this. We are seeking an experienced Senior BI Engineer to lead the design, development, and optimization of scalable business intelligence (BI) solutions that empower data-driven decision-making across the organization. The ideal candidate is highly skilled in data modeling, dashboard development, ETL design, and cloud-based BI platforms, with a passion for turning complex data into clear, actionable insights. As a senior member of the BI team, you will work closely with data engineers, analysts, business stakeholders, and product teams to deliver robust, user-friendly analytics solutions that support strategic and operational goals.\nRoles & Responsibilities:\nDesign, develop, and maintain enterprise-grade BI dashboards and reports using tools like Power BI, Tableau, or Looker.\nBuild and optimize semantic models, tabular data structures, and reusable datasets for self-service BI users.\nPartner with business stakeholders to translate requirements into technical solutions, delivering accurate, relevant, and timely insights.\nWork closely with data engineering teams to integrate BI solutions into data lake, warehouse, or lakehouse architectures (e.g., Snowflake, Redshift, Databricks, BigQuery).\nImplement best practices for BI development, including version control, performance optimization, and data governance.\nEnsure BI solutions are secure, scalable, and aligned with enterprise data governance standards.\nMentor junior BI developers and analysts, setting standards for dashboard usability, data visualization, and design consistency.\nCollaborate with cross-functional teams to promote self-service BI adoption and data literacy throughout the organization.\nMonitor BI performance, usage, and adoption, providing continuous improvements and training to enhance impact.\nMust-Have Skills:\n5 - 8 years of experience in BI development and data visualization, with deep expertise in tools such as Power BI, Tableau, or Looker.\nStrong knowledge of SQL, data modeling techniques, and BI architecture best practices.\nExperience working with data warehouses and cloud data platforms\nProficiency in building dashboards, KPIs, and executive-level reporting that align with business priorities.\nSolid understanding of ETL/ELT processes, data pipelines, and integration with BI tools.\nStrong collaboration skills with the ability to work effectively across engineering, product, finance, and business teams.\nExcellent communication skills, with a proven ability to translate technical concepts into business value.\nGood-to-Have Skills:\nExperience in cloud platforms (AWS, Azure, or GCP) and modern data stack environments.\nFamiliarity with data governance, data cataloging, metadata management, and access control.\nExposure to Agile methodologies, CI/CD for BI, and DevOps practices.\nBI or data certifications (e.g., Microsoft Certified: Power BI Data Analyst, Tableau Certified Professional).\nEducation and Professional Certifications\nMasters degree and 3 to 4 + years of Computer Science, IT or related field experience\nOR\nBachelors degree and 5 to 8 + years of Computer Science, IT or related field experience\nPowerBI / Tableau certifications preferred\nScaled Agile SAFe certification preferred\nSoft Skills:\nExcellent analytical and troubleshooting skills.\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nTeam-oriented, with a focus on achieving team goals.\nAbility to learn quickly, be organized and detail oriented.\nStrong presentation and public speaking skills.",Industry Type: Pharmaceutical & Life Sciences,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['BI development', 'Azure', 'Power BI', 'BI tools', 'Agile methodologies', 'data modeling techniques', 'Tableau', 'SQL', 'BI architecture', 'GCP', 'CI/CD', 'cloud data platforms', 'data visualization', 'data warehouses', 'AWS']",2025-06-12 06:26:19
Position For Business Intelligence Developer with ( DBT & Snowflake ),Synergy Technologies,5 - 9 years,Not Disclosed,[],"Hi ,  \nSynergy Technologies is a leader in technology services and consulting. We enable clients across the world to create and execute strategies .We help our clients find the right problems to solve, and to solve these effectively. We bring our expertise and innovation to every project we undertake\n\nPosition: Business Intelligence Developer\nDuration :  Contract to Full Time",,,,"['BI tools', 'DBT', 'Snowflake', 'DAX calculations', 'Business Intelligence Developer', 'AWS']",2025-06-12 06:26:22
Engineering Manager,Amgen Inc,10 - 14 years,Not Disclosed,['Hyderabad'],"Role Description:\nWe are seeking a seasoned Engineering Manager (Data Engineering) to lead the end-to-end management of enterprise data assets and operational data workflows. This role is critical in ensuring the availability, quality, consistency, and timeliness of data across platforms and functions, supporting analytics, reporting, compliance, and digital transformation initiatives. You will be responsible for the day-to-day data operations, manage a team of data professionals, and drive process excellence in data intake, transformation, validation, and delivery. You will work closely with cross-functional teams including data engineering, analytics, IT, governance, and business stakeholders to align operational data capabilities with enterprise needs.\nRoles & Responsibilities:\nLead and manage the enterprise data operations team, responsible for data ingestion, processing, validation, quality control, and publishing to various downstream systems.\nDefine and implement standard operating procedures for data lifecycle management, ensuring accuracy, completeness, and integrity of critical data assets.\nOversee and continuously improve daily operational workflows, including scheduling, monitoring, and troubleshooting data jobs across cloud and on-premise environments.\nEstablish and track key data operations metrics (SLAs, throughput, latency, data quality, incident resolution) and drive continuous improvements.\nPartner with data engineering and platform teams to optimize pipelines, support new data integrations, and ensure scalability and resilience of operational data flows.\nCollaborate with data governance, compliance, and security teams to maintain regulatory compliance, data privacy, and access controls.\nServe as the primary escalation point for data incidents and outages, ensuring rapid response and root cause analysis.\nBuild strong relationships with business and analytics teams to understand data consumption patterns, prioritize operational needs, and align with business objectives.\nDrive adoption of best practices for documentation, metadata, lineage, and change management across data operations processes.\nMentor and develop a high-performing team of data operations analysts and leads.\nFunctional Skills:\nMust-Have Skills:\nExperience managing a team of data engineers in biotech/pharma domain companies.\nExperience in designing and maintaining data pipelines and analytics solutions that extract, transform, and load data from multiple source systems.\nDemonstrated hands-on experience with cloud platforms (AWS) and the ability to architect cost-effective and scalable data solutions.\nExperience managing data workflows in cloud environments such as AWS, Azure, or GCP.\nStrong problem-solving skills with the ability to analyze complex data flow issues and implement sustainable solutions.\nWorking knowledge of SQL, Python, or scripting languages for process monitoring and automation.\nExperience collaborating with data engineering, analytics, IT operations, and business teams in a matrixed organization.\nFamiliarity with data governance, metadata management, access control, and regulatory requirements (e.g., GDPR, HIPAA, SOX).\nExcellent leadership, communication, and stakeholder engagement skills.\nWell versed with full stack development & DataOps automation, logging frameworks, and pipeline orchestration tools.\nStrong analytical and problem-solving skills to address complex data challenges.\nEffective communication and interpersonal skills to collaborate with cross-functional teams.\nGood-to-Have Skills:\nData Engineering Management experience in Biotech/Life Sciences/Pharma\nExperience using graph databases such as Stardog or Marklogic or Neo4J or Allegrograph, etc.\nEducation and Professional Certifications\n9 to 12 years of experience in Computer Science, IT or related field\nAWS Certified Data Engineer preferred\nDatabricks Certificate preferred\nScaled Agile SAFe certification preferred\nSoft Skills:\nExcellent analytical and troubleshooting skills\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation\nAbility to manage multiple priorities successfully\nTeam-oriented, with a focus on achieving team goals\nStrong presentation and public speaking skills",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['data operations', 'fullstack development', 'GCP', 'stakeholder engagement', 'troubleshooting', 'cloud platforms', 'AWS']",2025-06-12 06:26:24
Oracle GRC(PAN INDIA) -8+ Yrs -Hybrid- Immediate joiners,Databuzzltd,8 - 12 years,Not Disclosed,"['Hyderabad', 'Bengaluru', 'Mumbai (All Areas)']","Databuzz is Hiring for Oracle GRC(PAN INDIA) -8+ Yrs -Hybrid- Immediate joiners\n\nPlease mail your profile to alekya.chebrolu@databuzzltd.com with the below details, If\nyou are Interested.\n\nAbout DatabuzzLTD:\n\nDatabuzz is One stop shop for data analytics specialized in Data Science, Big Data, Data Engineering, AI & ML, Cloud Infrastructure and Devops. We are an MNC based in both UK and INDIA. We are a ISO 27001 & GDPR complaint company.\n\nCTC -\nECTC -\nNotice Period/LWD - (Candidate serving notice period will be preferred)\n\nPosition: Oracle GRC(PAN INDIA) -8+ Yrs -Hybrid\nExp -8+ yrs\n\nMandatory Skills:\nShould have Oracle GRC, E Business Suite Governance, Risks and Compliance\nDevelop and Implement GRC Programs and Policies Create and enforce governance risk management and compliance programs to ensure the organization adheres to regulatory requirements and internal policies\nShould have Knowledge of software engineering methodologies reporting tools modeling and testing\nShould have Lean Six Sigma and Business Process Modelling Understanding of Lean Six Sigma and Business Process Modelling and Notation\n\n\nRegards,\nAlekya\nTalent Acquisition Specialist\nalekya.chebrolu@databuzzltd.com",Industry Type: IT Services & Consulting,Department: Other,"Employment Type: Full Time, Permanent","['Oracle Grc', 'Risk Management', 'E Business Suite Governance']",2025-06-12 06:26:27
Etl Engineer,Global Energy,4 - 9 years,Not Disclosed,[],"Role & responsibilities\n\nWe are looking for 5 years of experience in ETL\n\nWork mode :Remote\n\nMandatory skills : CData, ETl\n\nNeed a dedicated ETL engineer to manage all Extract, Transform, Load (ETL) processes.\nStrong expertise in ETL workflows is required.\nMust have hands-on experience with C-Data, as it is the primary tool used by the client.\nPreferably someone with advanced or in-depth experience in C-Data, not just basic knowledge.\nSome exposure or understanding of Salesforce is expected, since it is the client's main system.\nFamiliarity with complex healthcare data is preferred, as the data can be intricate and challenging to work with.\nData handling with C-Data drivers & knowledge should be there",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Cdata sync', 'ETL', 'Salesforce Integration', 'Etl Process', 'Informatica']",2025-06-12 06:26:29
Specialist Software Engineer,Amgen Inc,4 - 6 years,Not Disclosed,['Hyderabad'],"What you will do\nYou will play a key role as part of Operations Generative AI (GenAI) Product team to deliver cutting edge innovative GEN AI solutions across various Process Development functions (Drug Substance, Drug Product, Attribute Sciences & Combination Products) in Operations functions. The role involves developing, implementing and sustaining GEN AI solutions to help find relevant, actionable information quickly and accurately.\nRole Description:\nThe Specialist Software Engineer is responsible for designing, developing, and maintaining GEN AI solutions software applications and solutions that meet business needs and ensure high availability and performance of critical systems and applications in Process development under Operation. This role involves working closely with Data Scientists, business SMEs, and other engineers to create high-quality, scalable GEN AI software solutions to help find relevant, actionable information quickly and accurately, monitoring system health, and responding to incidents to minimize downtime.\nRoles & Responsibilities:\nTake ownership of complex software projects from conception to deployment, Manage software delivery scope, risk, and timeline.\nRapidly prototype concepts into working code.\nProvide technical guidance and mentorship to junior developers.\nContribute to front-end and back-end development using cloud technology.\nDevelop innovative solutions using generative AI technologies.\nIntegrate with other systems and platforms to ensure seamless data flow and functionality.\nConduct code reviews to ensure code quality and adherence to best practices.\nCreate and maintain documentation on software architecture, design, deployment, disaster recovery, and operations.\nAnalyze and understand the functional and technical requirements of applications, solutions, and systems and translate them into software architecture and design specifications.\nWork closely with product team, cross-functional teams, enterprise technology teams and QA, to deliver high-quality and compliant software on time.\nEnsure high quality software deliverables free of bugs and performance issues through proper design and comprehensive testing strategies.\nProvide ongoing support and maintenance for applications, ensuring that they operate smoothly and efficiently.\nArchitect and lead the development of scalable, intelligent search systems leveraging NLP, embeddings, LLMs, and vector search\nOwn the end-to-end lifecycle of search solutions, from ingestion and indexing to ranking, relevancy tuning, and UI integration\nIntegrate AI models that improve search precision, query understanding, and result summarization (e.g., generative answers via LLMs).\nDevelop solutions for handling structured/unstructured data in AI pipelines.\nPartner with platform teams to deploy search solutions on scalable infrastructure (e.g., Kubernetes, Databricks).\nExperience in integrating Generative AI capabilities and Vision Models to enrich content quality and user engagement.\nBasic Qualifications:\nMasters degree with 4 - 6 years of experience in Computer Science, IT or related field OR\nBachelors degree with 6 - 8 years of experience in Computer Science, IT or related field OR\nDiploma with 10 - 12 years of experience in Computer Science, IT or related field\nExperience in Python, Java, AI/ML based Python libraries(PyTorch),\nExperienced with Web frameworks like Flask, Django, Fast API\nExperience with design patterns, data structures, data modelling, data algorithms\nFamiliarity with MLOps, CI/CD for ML, and monitoring of AI models in production.\nExperienced with AWS /Azure Platform, building and deploying the code\nExperience in PostgreSQL /Mongo DB SQL database, vector database for large language models, Databricks or RDS, S3 Buckets\nExperience with popular large language models\nExperience with Retrieval-augmented generation (RAG) framework, AI Agents, Vector stores, AI/ML platforms, embedding models ex Open AI, Langchain, Redis, pgvector\nExperience with prompt engineering, model fine tuning\nExperience with generative AI or retrieval-augmented generation (RAG) frameworks\nExperience in Agile software development methodologies\nExperience in End-to-End testing as part of Test-Driven Development\nPreferred Qualifications:\nStrong understanding of cloud platforms (e.g., AWS, GCP, Azure) and containerization technologies (e.g., Docker, Kubernetes).\nExperience with monitoring and logging tools (e.g., Prometheus, Grafana, Splunk).\nExperience with data processing tools like Hadoop, Spark, or similar.\nExperience with Langchain or llamaIndex framework for language models; Experience with prompt engineering, model fine-tuning.\nExperience working on Full stack Applications\nProfessional Certifications:\nAWS, Data Science Certifications(preferred)\nSoft Skills:\nExcellent analytical and troubleshooting skills.\nStrong verbal and written communication skills.\nAbility to work effectively with global, virtual teams.\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nTeam-oriented, with a focus on achieving team goals.\nStrong presentation and public speaking skills.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Software engineering', 'kubernetes', 'algorithms', 'microsoft azure', 'cloud platforms', 'sql', 'django', 'spark', 'grafana', 'gcp', 'troubleshooting', 'data structures', 'hadoop', 'flask']",2025-06-12 06:26:31
Oracle Fusion SCM with Procurement(PAN INDIA) -8+ Yrs -Hybrid,Databuzzltd,8 - 12 years,Not Disclosed,"['Hyderabad', 'Bengaluru', 'Mumbai (All Areas)']","Databuzz is Hiring for Oracle Fusion SCM with Procurement(PAN INDIA) -8+ Yrs -Hybrid- Immediate joiners\n\nPlease mail your profile to alekya.chebrolu@databuzzltd.com with the below details, If\nyou are Interested.\n\nAbout DatabuzzLTD:\n\nDatabuzz is One stop shop for data analytics specialized in Data Science, Big Data, Data Engineering, AI & ML, Cloud Infrastructure and Devops. We are an MNC based in both UK and INDIA. We are a ISO 27001 & GDPR complaint company.\n\nCTC -\nECTC -\nNotice Period/LWD - (Candidate serving notice period will be preferred)\n\nPosition: Oracle Fusion SCM with Procurement(PAN INDIA) -8+ Yrs -Hybrid(PAN INDIA)\nExp -8+ yrs\n\nMandatory Skills:\nShould have Fusion SCM, Procurement\nShould have experience with 7 years on Fusion Finance implementation and support\nImplementation and Configuration Lead the implementation of Oracle Fusion Financials modules including General Ledger Accounts Payable Account Receivable Cash Management and Fixed Assets\nTesting and Quality Assurance Develop and execute test plans to ensure the successful implementation of Oracle Fusion Financials\nHands-on experience in configuring and customizing Oracle Fusion applications\n\n\nRegards,\nAlekya\nTalent Acquisition Specialist\nalekya.chebrolu@databuzzltd.com",Industry Type: IT Services & Consulting,Department: Other,"Employment Type: Full Time, Permanent","['Oracle Fusion SCM', 'Oracle Fusion Financials']",2025-06-12 06:26:33
.NET developer(Azure)-7+yrs-Pan India-Hybrid,Databuzz ltd,7 - 12 years,Not Disclosed,"['Pune', 'Bengaluru', 'Mumbai (All Areas)']","Databuzz is Hiring for .NET developer(Azure)-7+yrs-Pan India-Hybrid\n\nPlease mail your profile to haritha.jaddu@databuzzltd.com with the below details, If\nyou are Interested.\n\nAbout DatabuzzLTD:\nDatabuzz is One stop shop for data analytics specialized in Data Science, Big Data, Data Engineering, AI & ML, Cloud Infrastructure and Devops. We are an MNC based in both UK and INDIA. We are a ISO 27001 & GDPR complaint company.\n\nCTC -\nECTC -\nNotice Period/LWD - (Candidate serving notice period will be preferred)\n\nPosition: .NET developer(Azure)\nLocation: Pan India\nExp -7+ yrs\n\nMandatory skills :\nCandidate should have strong .NET development experience\nShould have Cloud services for application deployment and integration, including components such as databases, message queuing, secret management storage & retrieval (any cloud provider is acceptable; AWS experience is a plus).\nShould have Message Queuing services like Kafka, RabbitMQ etc\nShould have TDD, Unit & Integration testing experience\nExperience on SQL and NOSQL datastores\nSOLID principles and hands on experience applying them\nCI/CD processes (No experience required but at least understanding on delivering code from development to PROD is required)\n\n\nRegards,\nHaritha\nTalent Acquisition specialist\nharitha.jaddu@databuzzltd.com",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['sql/nosql', 'TDD', '.Net', 'Messaging Queue', 'azure', 'Unit Integration Testing', 'Microservices']",2025-06-12 06:26:35
Denodo Administrator,Pracemo Global Solutions,3 - 8 years,11-21 Lacs P.A.,['Pune'],"Hiring for Denodo Admin with 3+ years experience with below skills:\n\nMust Have:\n- Denodo admin logical data models, views & caching\n- ETL pipelines (Informatica/Talend) for EDW/data lakes, performance issues\n- SQL, Informatica, Talend, Big Data, Hive\n\nRequired Candidate profile\n- Design, develop & maintain ETL pipelines using Informatica PowerCenter or Talend to extract, Hive\n- Optimize & troubleshoot complex SQL queries\n- Immediate Joiner is plus\n- Work from office is must",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Denodo', 'Hive', 'Administrator', 'Informatica', 'SQL', 'Administration', 'Data Engineering', 'Snowflake', 'Big Data', 'Talend', 'ETL', 'Big Data Administration']",2025-06-12 06:26:37
Optimization Specialist,Ltimindtree,6 - 11 years,Not Disclosed,['Navi Mumbai'],"JD Optimization Specialist\n\nJob Summary: We are seeking an experienced Data Scientist with a strong background in optimization to join our team. The ideal candidate will have 12-17 years of experience in data science, machine learning, optimization techniques and Gen AI. This role involves leading complex data science and Gen AI projects ( with a focus in solving optimization problems), mentoring junior data scientists, and driving data-driven decision-making.\n\nRoles and Responsibilities:\n\n12-17 years of overall experience in the design, implementation, or consulting within the data science and AI space.\nOversee the design, development, and implementation of advanced optimization models to solve complex business problems, across domains ( manufacturing, hitech, retail , oil & gas, etc.)\nApply optimization techniques such as linear programming, integer programming, mixed integer linear programming, nonlinear optimization, and heuristic and meta heuristic algorithms ( like Genetic Algorithms ) to solve business problems.\nAnalyzing and understanding use cases to verify their fitment and to determine the best solution approach for data science and generative AI applications.\nWork closely with cross-functional teams including pre-sales, sales, hyper scalar platforms, to integrate the AI solutions and drive business outcomes\nDocumenting comprehensive technical artifacts and establishing best practices for implementing and deploying AI models on various cloud platforms.\nLeading a team of leads and developers in building Proof of Concepts (POCs) for various customers.\nProvide guidance and mentorship to junior data scientists, and sharing knowledge in data science, optimization and AI technologies, fostering a culture of continuous learning and improvements\nEnsuring AI solutions adhere to quality and ethical standards while optimizing their performance in production for reliability and efficiency.\n\nTechnical Skills\n\nProficiency in optimization tools, libraries & products such as CPLEX, Gurobi, PuLP, Llamasoft, etc.\nExperience with building and implementing optimization techniques like linear programming, Mixed Integer Linear Programming, nonlinear optimization, Genetic Algorithms, and heuristic and meta – heuristic algorithms.\nProficiency in programming languages like Python, R, and SQL.\nExpertise in data manipulation and analysis using libraries such as Pandas, NumPy, and Scikit-learn, SciPy\nStrong experience with machine learning frameworks like TensorFlow, Keras, and PyTorch\nProficiency in using LLM, and LLM – based frameworks like RAG, agentic framework to develop AI solutions\nStrong communication skills and experience in managing various stakeholder relationships to gain consensus on complex technical solutions.\nDeep experience in architecting, designing, and implementing solutions on-premises, in the cloud, and using hybrid models, and mastery of the latest AI frameworks.\nIn-depth experience in fine-tuning and customizing pretrained LLMs and AI models, with good understanding of various patterns and practices in AI, data engineering, and large data processing.\nProficiency with a variety of tools and platforms, including but not limited to Amazon SageMaker, Azure ML Studio, Azure Data Lake, Google BigQuery, Vertex AI, AWS S3, Databricks and Snowflake.\nExperience with relational and non-relational databases (e.g., MySQL, MongoDB).\nRelevant certifications in data science, optimization, operations research, Gen AI at the Architect/Lead level will be an advantage.\n\nBehavioral Skills:\n\nExcellent verbal and written communication skills to articulate complex data insights to non-technical stakeholders.\nProblem-Solving: Strong analytical and problem-solving abilities to tackle challenging data and optimization issues.\nCollaboration: Ability to work effectively in a collaborative environment and build strong relationships with cross-functional teams.\nAdaptability: Flexibility to adapt to changing business needs and technological advancements.\nAttention to Detail: High level of attention to detail and commitment to delivering high-quality work.\n\nOther Skills:\nProject Management: Experience in managing large-scale data science, optimization and Gen AI projects and ensuring timely delivery.\nBusiness Acumen: Understanding of business processes and the ability to align data science, optimization and AI initiatives with organizational goals.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Optimization Techniques', 'Data Science', 'Aiml']",2025-06-12 06:26:39
Job Opportunity_Data Science_Noida,Tradeindia,1 - 5 years,Not Disclosed,['Noida( Noida-Greater Noida Expressway )'],"About the Role:\nWe are seeking a highly skilled and innovative AI/ML Engineer with a strong background in Generative AI (LLMs, RAG, Multimodal models, Agentic AI), core Machine Learning (including Deep Learning, NLP, and Vision), and hands-on experience with Data Visualization tools like Tableau. The ideal candidate should also be familiar with deploying solutions on cloud platforms, particularly AWS.\n\nKey Responsibilities:\nDesign, develop, and fine-tune Large Language Models (LLMs) for diverse use cases using prompt engineering, fine-tuning, and embeddings.\nImplement Retrieval-Augmented Generation (RAG) pipelines with semantic search and vector databases (e.g., FAISS, Pinecone).\nWork on multimodal AI models involving text, image, and audio data for advanced AI capabilities.\nBuild agentic AI systems using frameworks like LangChain, OpenAgents, or similar libraries.\nDevelop, train, and evaluate Machine Learning /Deep Learning models for NLP and Computer Vision (object detection, OCR, etc.).\nCreate insightful data dashboards and visualizations using Tableau to support business decision-making.\nDeploy ML pipelines and models on AWS cloud services (e.g., S3, SageMaker, Airflow, Glue).\nFamiliarity with Microsoft Azure especially for OpenAI\nCollaborate with cross-functional teams to understand business problems and translate them into AI/ML solutions.\nKeep up-to-date with the latest advancements in Gen-AI, open-source tools, and cloud technologies.\nPreferred Qualifications:\nBachelors or Masters in Computer Science, Data Science, or related field\n\nRequired Skills:\n\nGen-AI & LLMs:\nExperience with OpenAI, HuggingFace Transformers, LangChain, LlamaIndex, RAG architectures\nExperience in LLM evaluation, prompt optimization, fine-tuning/customizing base models\nFamiliarity with agentic workflows, function calling, and orchestration tools\n\nMachine Learning:\nStrong fundamentals in deep learning, NLP, and computer vision\nExperience with TensorFlow, PyTorch, Scikit-learn\nGood grasp of model deployment, monitoring, and evaluation metrics\n\nVisualization & Storytelling:\nProficient in Tableau for data visualization, dashboards, and storytelling with data\n\nCloud & DevOps:\nFamiliarity with AWS services (S3, Lambda, SageMaker, Glue , MWAA)\nComfortable with Bitbucket (Git) and CI/CD pipelines",Industry Type: Internet (E-Commerce),Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Data Science', 'Python', 'SQL', 'Machine Learning', 'AWS', 'Large Language Models', 'Deep Learning']",2025-06-12 06:26:41
Python and Machine Learning Programmer,Panacorp Software Solutions,1 - 3 years,Not Disclosed,['Nagercoil'],"Job Overview:\nWe are looking for a skilled Python and Data Science Programmer to develop and implement data-driven solutions. The ideal candidate should have strong expertise in Python, machine learning, data analysis, and statistical modeling.\n\nKey Responsibilities:\nData Analysis & Processing: Collect, clean, and preprocess large datasets for analysis.\nMachine Learning: Build, train, and optimize machine learning models for predictive analytics.\nAlgorithm Development: Implement data science algorithms and statistical models for problem-solving.\nAutomation & Scripting: Develop Python scripts and automation tools for data processing and reporting.\nData Visualization: Create dashboards and visual reports using Matplotlib, Seaborn, Plotly, or Power BI/Tableau.\nDatabase Management: Work with SQL and NoSQL databases for data retrieval and storage.\nCollaboration: Work with cross-functional teams, including data engineers, business analysts, and software developers.\nResearch & Innovation: Stay updated with the latest trends in AI, ML, and data science to improve existing models.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Python', 'tableau', 'data analysis', 'data science', 'predictive analytics', 'statistical modeling', 'machine learning', 'sql', 'nosql']",2025-06-12 06:26:44
Python and Machine Learning Programmer,Panacorp Software Solutions,1 - 3 years,Not Disclosed,['Nagercoil'],"Job Overview:\nWe are looking for a skilled Python and Data Science Programmer to develop and implement data-driven solutions. The ideal candidate should have strong expertise in Python, machine learning, data analysis, and statistical modeling.\n\nKey Responsibilities:\nData Analysis & Processing: Collect, clean, and preprocess large datasets for analysis.\nMachine Learning: Build, train, and optimize machine learning models for predictive analytics.\nAlgorithm Development: Implement data science algorithms and statistical models for problem-solving.\nAutomation & Scripting: Develop Python scripts and automation tools for data processing and reporting.\nData Visualization: Create dashboards and visual reports using Matplotlib, Seaborn, Plotly, or Power BI/Tableau.\nDatabase Management: Work with SQL and NoSQL databases for data retrieval and storage.\nCollaboration: Work with cross-functional teams, including data engineers, business analysts, and software developers.\nResearch & Innovation: Stay updated with the latest trends in AI, ML, and data science to improve existing models.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Python', 'NoSQL', 'Power BI', 'Database Management', 'Plotly', 'Data Analysis', 'Seaborn', 'Tableau', 'SQL']",2025-06-12 06:26:46
AI Conversational Chatbot Developer,Benovymed Healthcare,1 - 4 years,Not Disclosed,['Visakhapatnam'],"Develop and enhance AI-driven chatbot solutions to automate customer interactions and optimize response accuracy.\nDesign and implement NLP algorithms using frameworks like Rasa, Dialogflow, or IBM Watson.\nEnsure seamless chatbot integration with existing platforms and CRM systems.\nWork closely with data scientists to improve machine learning models.\nMust have strong proficiency in Python, deep learning techniques, and chatbot analytics.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Chatbot Development', 'deep learning', 'chatbot', 'python', 'r', 'data analytics', 'predictive modeling', 'machine learning', 'artificial intelligence', 'chatbot analytics']",2025-06-12 06:26:48
AI Conversational Chatbot Developer,Benovymed Healthcare,1 - 4 years,Not Disclosed,['Surat'],"Develop and enhance AI-driven chatbot solutions to automate customer interactions and optimize response accuracy. Design and implement NLP algorithms using frameworks like Rasa, Dialogflow, or IBM Watson. Ensure seamless chatbot integration with existing platforms and CRM systems. Work closely with data scientists to improve machine learning models. Must have strong proficiency in Python, deep learning techniques, and chatbot analytics.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['artificial intelligence', 'deep learning', 'chatbot integration', 'Python', 'CRM systems', 'chatbot analytics']",2025-06-12 06:26:51
AI Conversational Chatbot Developer,Benovymed Healthcare,1 - 4 years,Not Disclosed,['Nagpur'],"Develop and enhance AI-driven chatbot solutions to automate customer interactions and optimize response accuracy.\nDesign and implement NLP algorithms using frameworks like Rasa, Dialogflow, or IBM Watson.\nEnsure seamless chatbot integration with existing platforms and CRM systems.\nWork closely with data scientists to improve machine learning models.\nMust have strong proficiency in Python, deep learning techniques, and chatbot analytics.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Chatbot Development', 'deep learning', 'chatbot', 'python', 'r', 'data analytics', 'predictive modeling', 'machine learning', 'artificial intelligence', 'chatbot analytics']",2025-06-12 06:26:53
Snowflake Developer | HYD | Walkin | TCS-C2H | 14th June | 6+,Coventine digital Pvt Ltd,6 - 8 years,6-12 Lacs P.A.,['Hyderabad'],"Key Responsibilities:\n\nDesign, develop, and maintain scalable data pipelines using Snowflake.\nDevelop and optimize complex SQL queries, views, and stored procedures.\nMigrate data from legacy systems to Snowflake using ETL tools like Informatica, Talend, dbt, or Matillion.\nImplement data modeling techniques (Star, Snowflake schemas) and maintain data dictionary.\nEnsure performance tuning, data quality, and security across all Snowflake objects.\nIntegrate Snowflake with BI tools like Tableau, Power BI, or Looker.\nCollaborate with data analysts, data scientists, and business teams to understand requirements and deliver solutions.\nMonitor and manage Snowflake environments using tools like SnowSight, Snowsql, or CloudWatch.\nParticipate in code reviews and enforce best practices for data governance and security.\nDevelop automation scripts using Python, Shell, or Airflow for data workflows.\nRequired Skills:\n\n6+ years of experience in data engineering / data warehousing.\n3+ years hands-on experience with Snowflake Cloud Data Platform.\nStrong expertise in SQL, performance tuning, data modeling, and query optimization.\nExperience with ETL tools like Informatica, Talend, Apache NiFi, or dbt.\nProficient in cloud platforms: AWS / Azure / GCP (preferably AWS).\nGood understanding of DevOps/CI-CD principles for Snowflake deployments.\nHands-on experience with scripting languages: Python, Bash, etc.\nKnowledge of RBAC, masking policies, row access policies in Snowflake.",Industry Type: IT Services & Consulting,Department: Other,"Employment Type: Full Time, Permanent","['performance tuning', 'SQL queries', 'Snowflake', 'AWS / Azure / GCP', 'Python', 'masking policies', 'Informatica', 'Bash', 'Apache NiFi', 'DevOps/CI-CD', 'and query optimization', 'data modeling', 'RBAC', 'row access policies', 'Talend']",2025-06-12 06:26:55
AI Conversational Chatbot Developer,Benovymed Healthcare,1 - 4 years,Not Disclosed,['Patna'],"Develop and enhance AI-driven chatbot solutions to automate customer interactions and optimize response accuracy. Design and implement NLP algorithms using frameworks like Rasa, Dialogflow, or IBM Watson. Ensure seamless chatbot integration with existing platforms and CRM systems. Work closely with data scientists to improve machine learning models. Must have strong proficiency in Python, deep learning techniques, and chatbot analytics.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['artificial intelligence', 'deep learning', 'chatbot integration', 'Python', 'CRM systems', 'chatbot analytics']",2025-06-12 06:26:58
AI Conversational Chatbot Developer,Benovymed Healthcare,1 - 4 years,Not Disclosed,['Mysuru'],"Develop and enhance AI-driven chatbot solutions to automate customer interactions and optimize response accuracy. Design and implement NLP algorithms using frameworks like Rasa, Dialogflow, or IBM Watson. Ensure seamless chatbot integration with existing platforms and CRM systems. Work closely with data scientists to improve machine learning models. Must have strong proficiency in Python, deep learning techniques, and chatbot analytics.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['artificial intelligence', 'deep learning', 'chatbot integration', 'Python', 'CRM systems', 'chatbot analytics']",2025-06-12 06:27:00
Technical Manager - AI/ML,Thryve Digital,12 - 15 years,Not Disclosed,"['Hyderabad', 'Chennai']","Role Summary:\nThis job involvesmanaging the day to day delivery of AI ML team, which will be responsible forbuilding, deploying and maintaining robust, scalable, and efficient ML models\nEssentialResponsibilities\nLead and mentor team of data engineers, MLOps engineers, and machine learning engineers, to achieve project objectives & deliverables (development and deployment of ML models for AI use cases across the platform)\nCollaborate with business, engineering, infrastructure and data science teams to translate their needs or challenges into production-grade Artificial Intelligence and Machine Learning models for batch and real-time requirements\nPrimary point of contact for US & Thryve leaders, ensuring clear communication and aligning with project goals and objectives\nGuide team of AI ML engineers with respect to business objectives and connecting the dots, helping to team to come up with optimal ML models based on use cases\nManage day to day operation and ensure adherence to process, scope, quality and timelines\nIncumbent to assume role of an engagement coordinator between Thryve India team and US counterparts (techno functional)\nHighlight risks and concerns to leadership team, work with multiple stakeholders to establish alignment, contingency and or mitigation plans as required\nExperienced and proactive in tracking delivery statuses, project progress and take ownership of all deliverables from Thryve\nEffective communicator with team members, US stakeholders and management\nStay up-to-date with the latest trends andadvancements in AI and ML and identify opportunities for the team to implementnew models and technologies\nOther duties as assigned or requested approximately 50% technical and 50% management\nProposeand implement best engineering and research practices for scaling ML-poweredfeatures, with a goal to enable fast iteration of and efficient experimentationwith novel features\nTheexperience we are looking to add to our team\nRequired\nBachelor's degree in engineering or computerScience or related field\n12 to 15 years of total experience with atleast 1+ year experience in leading / managing AI - ML projects achieving clear& measurable business objectives\nGood understanding of US Health insurance withat least 3+ years experience in the same\nGood experience / understanding of the entire MLOps from data ingestion to production\nGood understanding of AI-ML concepts &algorithms,including supervised, unsupervised, and reinforcement learning, as well as MLOps\nAbility to take successful, complex ideas fromexperimentation to production\nKnowledge / experience in Google cloud [GCP]or any public cloud platforms\nExcellent communication, presentation, andinterpersonal skills\nProven experience in leading, motivating &inspiring teams to foster collaboration\nProficient in identifying, assessing andcalling out risks and mitigation plans to stakeholders\nExperience in implementing processimprovements and learning from past projects to enhance future performance\nAble to analyze external and internalprocesses, and creating strategies for service delivery optimization\nExperience in working with diverse teams andstakeholders, understanding their needs and managing expectations",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['AI/ML', 'Technical Management', 'MLOps', 'ML models', 'US Health insurance', 'Google cloud']",2025-06-12 06:27:02
AI Conversational Chatbot Developer,Benovymed Healthcare,1 - 4 years,Not Disclosed,['Indore'],"Develop and enhance AI-driven chatbot solutions to automate customer interactions and optimize response accuracy.\nDesign and implement NLP algorithms using frameworks like Rasa, Dialogflow, or IBM Watson.\nEnsure seamless chatbot integration with existing platforms and CRM systems.\nWork closely with data scientists to improve machine learning models.\nMust have strong proficiency in Python, deep learning techniques, and chatbot analytics.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['AI', 'deep learning techniques', 'Rasa', 'NLP algorithms', 'Dialogflow', 'Python', 'chatbot analytics']",2025-06-12 06:27:05
AI Conversational Chatbot Developer,Benovymed Healthcare,1 - 4 years,Not Disclosed,['Bhubaneswar'],"Develop and enhance AI-driven chatbot solutions to automate customer interactions and optimize response accuracy.\nDesign and implement NLP algorithms using frameworks like Rasa, Dialogflow, or IBM Watson.\nEnsure seamless chatbot integration with existing platforms and CRM systems.\nWork closely with data scientists to improve machine learning models.\nMust have strong proficiency in Python, deep learning techniques, and chatbot analytics.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['AI', 'deep learning techniques', 'Rasa', 'NLP algorithms', 'Dialogflow', 'Python', 'chatbot analytics']",2025-06-12 06:27:07
Generative AI Developer,Prodapt Solutions,2 - 5 years,Not Disclosed,['Chennai'],"Overview\n\nWe are looking for a talented Generative AI Developer to join our dynamic team and contribute to our projects in transforming data into actionable insights.\n\nResponsibilities\n\nDesign, develop, and implement generative AI models using Python and relevant libraries.\nCollaborate with data scientists to analyze data and improve model performance.\nDevelop and maintain Flask APIs to serve AI models and facilitate integration with frontend applications.\nUtilize SQL for data manipulation and retrieval to support AI model training and evaluation.\nEngage in code reviews and contribute to best practices for software development.\nStay updated with the latest trends and advancements in AI and machine learning technologies.\n\n\n Primary\n\nSkills:\n \nProficient in Python (including libraries such as TensorFlow, PyTorch, or similar frameworks).\nStrong experience with SQL for database management and data querying.\nFamiliarity with Generative AI tools and platforms (e.g., Gemini Pro).\nExperience in developing RESTful APIs using Flask.\n\n Secondary\n\nSkills:\n \nUnderstanding of machine learning concepts and algorithms.\nFamiliarity with cloud computing services (e.g., AWS, Google Cloud) is a plus.\nKnowledge of data preprocessing and feature engineering techniques.\nExcellent problem-solving skills and ability to work in a collaborative team environment.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['python', 'machine learning', 'sql', 'database management', 'tensorflow', 'rest', 'algorithms', 'software development', 'natural language processing', 'numpy', 'artificial intelligence', 'deep learning', 'data science', 'gcp', 'computer vision', 'pytorch', 'keras', 'flask', 'aws', 'cloud computing']",2025-06-12 06:27:10
Assistant Manager Biostatistician - Hyderabad,Hetero,4 - 9 years,Not Disclosed,['Hyderabad( Sanath Nagar )'],"Role & responsibilities :\n\nCollaborating with cross-functional teams (e.g., clinical researchers, medical experts, data scientists) to design clinical studies and research protocols.\nDeveloping statistical analysis plans (SAP), including the identification of primary and secondary endpoints, sample size calculations, and statistical methodologies.\nEnsuring statistical methods are aligned with regulatory requirements and industry standards, especially in clinical trials.",,,,"['Biostatistician', 'SAS', 'STATA', 'Statistical Programming', 'Biostatistics', 'R', 'Clinical SAS Programming', 'Clinical Trials']",2025-06-12 06:27:45
Azure Databricks Administrator,Lance Labs,10 - 14 years,Not Disclosed,"['Noida', 'Chennai']","Core Responsibilities:-\nPlatform Management: Oversee the deployment, configuration, and maintenance of Databricks clusters and workspaces.\nSecurity & Access Control: Implement role-based access controls (RBAC), manage Unity Catalog, and configure service principals and access tokens.\nPerformance Monitoring: Monitor cluster health, optimize resource utilization, and troubleshoot performance issues.\nAutomation & Scripting: Automate administrative tasks using tools like Python, PowerShell, and Terraform.\nIntegration & Deployment: Manage integrations with Azure Data Lake, Key Vault, and implement CI/CD pipelines using Azure DevOps.\nCompliance & Governance: Ensure data governance, implement backup and disaster recovery strategies, and adhere to security best practices.\nUser Support & Training: Provide technical support, conduct training sessions, and maintain documentation.\nRequired Skills & Experience\nCloud Platforms: Proficiency in Azure, AWS, or GCP; Azure experience is often preferred.\nProgramming Languages: Strong skills in Python, PySpark, PowerShell, and SQL.\nInfrastructure as Code: Experience with Terraform for provisioning and managing cloud resources.\nData Engineering: Knowledge of ETL processes, data pipelines, and big data technologies.\nSecurity & Compliance: Understanding of data security principles, compliance requirements, and best practices.\nExperience: Typically, 5-10 years in IT administration, with at least 25 years focused on Databricks administration.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Terraform', 'Powershell', 'Azure Cloud', 'Databrick Administrator', 'ETL', 'Azure Databricks', 'Iac', 'AWS', 'SQL', 'Python']",2025-06-12 06:27:47
ETL Developer II - Google Cloud Platform/Teradata,Sadup Soft,4 - 6 years,Not Disclosed,['Chennai'],"Must have skills :\n\n- Bachelor's/master's in engineering, Computer Science, or equivalent experience\n\n- 5-6 years of experience in the IT industry, experience in Data space is preferred.\n\n- Working experience in GCP-BQ.\n\n- Good knowledge in Teradata or Oracle.\n\n- Experience in Data modelling.\n\n- Advanced scripting experience - Python, Shell, etc.\n\n- Strong analytical skills including the ability to define problems, collect data, establish facts, and draw valid conclusions\n\n- Knowledge of Scheduling Tools (preferably Airflow, UC4) is a plus\n\n- Working knowledge on any ETL tool (i.e, Informatica) is a plus.\n\n- Excellent written and oral communication skills\n\n- Familiarity with data movement techniques and best practices to handle large volumes of data\n\n- Strong communication skills and willingness to take initiative to contribute beyond core-responsibilities\n\nResponsibilities :\n\nIn this role, the individual will be part of the Credit data engineering team within Credit Platform organization and have the following responsibilities :\n\n- Design and implement an integrated credit data platform that is extremely high-volume, fault-tolerant, scalable backend systems that process and manage petabytes of customer data.\n\n- Should adopt long term/strategic thought process during the entire project life cycle.\n\n- Participating and collaborating with cross functional teams in the organization to understand the business requirements and to deliver solutions that can scale.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Google Cloud Platform', 'Data Engineering', 'Data Modeling', 'Informatica', 'ETL', 'Teradata']",2025-06-12 06:27:50
"Manager, CSAR – Custom Function Programming",Amgen Inc,9 - 12 years,Not Disclosed,['Hyderabad'],"We are looking for a talented individual to join us as a Study Designer and Edit Check Programmer, reporting directly to the Sr. Manager, Clinical Systems and Analytical Reporting.\nThe successful candidate will play a crucial role in adhering to Amgen standards, procedures, and best practices to build and program studies in our clinical trial database. This position will require effective partnership with other CSAR Operations and cross-functional staff to ensure seamless, high-quality deliverables and activities related to the use of electronic data capture technology.\nWe are seeking a strong leader who can confidently influence stakeholders and contribute individually to study-specific and general CSAR/GDO projects or operational work. The ideal candidate will have proven experience in partnering effectively with cross-functional teams to deliver systems support and study deliverables. Additionally, they should have operational experience with clinical database management systems and allied technologies (e.g., Rave EDC, Veeva EDC).\nResponsibilities include, but are not limited to, the following:\nSupport of clinical trial platform technologies\nSupport decision-making by acting as a data scientist bringing awareness to patterns and analytical insight.\nCoordinating and providing programming support to Clinical Study Teams\nEnsure efficient and consistent use of EDC system and ensure the use is complied with the established procedures or standards.\nActing as a technical point of contact for systems deliverables on defined programs\nIdentify, recommend or implement system enhancements, new tools or emerging technologies to decrease database development cycle times and foster a collaborative working environment.\nProviding technical and business process input / expertise on new and emerging technologies\nDevelop, review and implement policies, SOPs and associated documents\nEnsure documentation supports CSAR operational or technical activities is in a complete manner and consistent with regulatory and the established processes.\nAssist in preparing for and responding to audit findings (internal or external).\nKnowledge\nGood Clinical Practice\nStrong understanding and experience in the use of performance management techniques, measures, problem-solving and analytical thinking\nDrug development and clinical trials processes\nData management processes\nClinical trial databases and applications\nEdit check development and Custom function programing\nProgramming Languages\nSystems development lifecycle\nProject planning and management\nCollaborating with global cross-functional teams (team/matrix environment)\nQuality management and Risk Analysis\nRegulatory filings and inspections\nProcess improvement methodologies\nPreferred Qualifications\nAdvanced degree or equivalent in life science, computer science, math, statistics, business administration or related discipline\nBroad knowledge / work experience in data management / programming in the Pharmaceutical or Biotech arena\nGeneral project management and planning experience\nExperience in oversight of outside vendors (CROs, central labs, imaging vendors, IRT vendors, etc.)\nBasic Qualifications\nBachelors degree or equivalent in life science, computer science, business administration or related discipline with 9 to 12 years of experience\nSpecialist knowledge / experience in life sciences or a medically related field\nGeneral biopharmaceutical clinical research experience (clinical research experience obtained working on clinical trials at a biotech, pharmaceutical or CRO company)",Industry Type: Pharmaceutical & Life Sciences,Department: Research & Development,"Employment Type: Full Time, Permanent","['Programming', 'documentation', 'Clinical trial databases', 'performance management', 'Data management processes', 'Systems development lifecycle', 'Quality management']",2025-06-12 06:27:52
Healthcare Integration Developer - Mirth Connect,Veersa Technologies India,4 - 8 years,Not Disclosed,['Noida'],"Job Description\nPosition: Healthcare Integration Developer Mirth Connect\nLocation: Noida\nExperience: 4 to 8 years\n\nKey Responsibilities\n\nDevelop and manage healthcare integration interfaces using Mirth Connect (NextGen Connect).\nDesign, implement, and maintain X12 EDI transactions, particularly:\n837 (Healthcare Claim)\n835 (Remittance Advice)\n270/271 (Eligibility Inquiry/Response)\n276/277 (Claim Status Inquiry/Response)\nWork with HL7 v2.x and FHIR message formats to integrate with EHRs and external systems.\nPerform data mapping, transformation (using JavaScript) and validation of healthcare messages.\nCollaborate with stakeholders and system analysts to gather requirements and design scalable interfaces.\nUtilize Python or .NET (C#) to build supplemental tools, services, or automations for parsing, validation, and backend processing.\nManage Mirth Connect deployments, monitor channel performance, and debug interface errors.\n\nSkills and Qualifications\n\nRequired Skills\n\nPrimary: Mirth Connect\nHands-on experience with Mirth Connect and creation of complex channels.\nSolid knowledge of X12 EDI transactions, especially 837, 835, 270/271, 276/277.\nProficient in working with HL7 and FHIR standards and message structures.\nExperience with JavaScript-based transformation and filters within Mirth.\nStrong command of Mirth components like connectors (File, TCP, SFTP), message transformers, and custom scripting.\n\nSecondary: Python \nProficiency in Python or .NET (C#) for building backend utilities, REST APIs, and data transformation tools.\nExperience consuming and building RESTful APIs.\nFamiliarity with working alongside integration platforms or microservices.\n\nSoft Skills:\nStrong analytical and problem-solving skills with attention to detail.\nExcellent communication and ability to articulate complex technical concepts to non-technical stakeholders.\nLeadership capabilities with experience mentoring and guiding junior developers.\nAdaptability to work in Agile/Scrum environments and deliver under tight deadlines.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['HL7', 'Mirth Nextgen', 'Python', 'X12', 'Javascript', '.Net', 'Fhir']",2025-06-12 06:27:55
"AI Ops / Monitoring Specialist Openings at Advantum Health, Hyderabad",Advantum Health,2 - 3 years,Not Disclosed,['Hyderabad'],"AI Ops/Monitoring Specialist openings at Advantum Health Pvt Ltd, Hyderabad.\nOverview:\nWere seeking an AI Ops/Monitoring Specialist to ensure the stability, transparency, and performance of AI systems in production. You will monitor, log, and troubleshoot AI and RPA models to ensure continuous reliability and compliance.\nKey Responsibilities:\nMonitor AI model health (drift, performance, latency, bias).\nBuild dashboards and alerts using tools like Prometheus, Grafana, or Datadog.\nEstablish SLAs and SLOs for AI/RPA models and pipelines.\nCollaborate with AI teams to integrate observability into model lifecycles.\nDocument anomalies and assist in root cause analysis and mitigation.\nQualifications:\nBachelors in Data Science, IT, or a related field.\n2+ years in systems monitoring, SRE, or MLOps.\nExperience with model monitoring tools (e.g., MLflow, Arize, WhyLabs).\nFamiliarity with AI/ML lifecycles and performance metrics.\nBackground in healthcare or compliance-heavy environments is ideal.\nPh: 9177078628\nEmail id: jobs@advantumhealth.com\nAddress: Advantum Health Private Limited, Cyber gateway, Block C, 4th floor Hitech City, Hyderabad.\nDo follow us on LinkedIn, Facebook, Instagram, YouTube and Threads\nAdvantum Health LinkedIn Page:\nhttps://lnkd.in/gVcQAXK3\n\nAdvantum Health Facebook Page:\nhttps://lnkd.in/g7ARQ378\n\nAdvantum Health Instagram Page:\nhttps://lnkd.in/gtQnB_Gc\n\nAdvantum Health India YouTube link:\nhttps://lnkd.in/g_AxPaPp\n\nAdvantum Health Threads link:\nhttps://lnkd.in/gyq73iQ6",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Sre', 'Datadog', 'Grafana', 'MLOPS', 'Arize', 'Monitoring Tools', 'Whylabs']",2025-06-12 06:27:57
IT/HR Recruiter,NxDT,1 - 2 years,"50,000-2 Lacs P.A.",[],"Job Description\nWe are looking for a passionate and detail-oriented IT Recruiter to join our HR team. The ideal candidate will be responsible for identifying, sourcing, and hiring top IT talent across various domains and technologies, ensuring a smooth and effective recruitment process aligned with the company workforce needs.\n\nKey Responsibilities:\nUnderstand job requirements from hiring managers and create clear and attractive job descriptions.\nSource and screen potential candidates through job portals (e.g., Naukri, Monster, LinkedIn), social media, employee referrals, and internal databases.\nConduct preliminary interviews to assess technical skills and cultural fit.\nCoordinate and schedule technical interviews with hiring teams.\nManage the end-to-end recruitment process including sourcing, screening, interviewing, feedback collection, offer negotiation, and onboarding.\nMaintain accurate and updated records of candidate pipelines and recruitment status in ATS/HRMS systems.\nBuild and maintain a strong candidate pipeline for recurring technical positions.\nPartner with business stakeholders to understand hiring needs and workforce planning.\n\nKey Skills & Qualifications:\nBachelors degree in Human Resources, IT, Business Administration, or a related field.\nProven experience in IT recruitment; knowledge of tech stacks like Java, Python, AWS, DevOps, Data Engineering, etc., is a strong advantage.\nFamiliarity with various sourcing techniques, screening tools, and interview formats.\nStrong communication and interpersonal skills.\nAbility to manage multiple positions simultaneously in a fast-paced environment.\nProficient in MS Office and Applicant Tracking Systems (ATS).",Industry Type: IT Services & Consulting,Department: Human Resources,"Employment Type: Full Time, Permanent","['Team Management', 'IT Recruitment', 'End To End Recruitment', 'It Sourcing', 'Sourcing Profiles', 'Screening', 'IT Staffing', 'Interview Scheduling', 'Mass Mailing', 'Shortlisting', 'Technical Recruitment']",2025-06-12 06:27:59
Walk in Power BI Developer with SQL Expertise,Net Connect,6 - 10 years,5-11 Lacs P.A.,['Hyderabad( Madhapur )'],"Greetings from NCG!\n\nWe have a opening for Power BI Developer role in Hyderabad office!\nBelow JD for your reference\n\nJob Description:\nWe are seeking an experienced Power BI Developer with strong SQL skills to join our analytics team in Hyderabad. The ideal candidate will be responsible for building interactive dashboards and reports, performing data analysis, and collaborating with business teams to deliver insightful visualizations that drive decision-making.",,,,"['Power Bi', 'SQL', 'Microsoft Power Bi']",2025-06-12 06:28:01
Tech Consultant,Software Company,5 - 8 years,Not Disclosed,['Gurugram'],"Urgent Requirement for Tech Consultant Data Engg with 5+yrs Exp Strong knowledge of Big Data technologies (Hadoop, Spark, Snowflake, Databricks, Airflow, AWS), Python, SQL & cloud platforms (AWS, Azure)",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Azure Data Factory', 'Databricks Engineer', 'AWS', 'Python', 'SQL', 'Airflow', 'Snowflake', 'Hadoop', 'Spark', 'ETL Tool']",2025-06-12 06:28:03
Power BI Developer,Ignitho,3 - 5 years,Not Disclosed,['Chennai( Sholinganallur )'],"Position: Senior Power BI Developer\nLocation: Chennai, Tamil Nadu\nExperience Required: 3 to 5 Years\nAbout the Company:\nIgnitho Inc. is a leading AI and data engineering company with a global presence, including US, UK, India, and Costa Rica offices.\nVisit our website to learn more about our work and culture: www.ignitho.com.\nIgnitho is a portfolio company of Nuivio Ventures Inc., a venture builder dedicated to developing Enterprise AI product companies across various domains, including AI, Data Engineering, and IoT.\nLearn more about Nuivio at: www.nuivio.com.\nJob Overview:\nWe are seeking a highly skilled and experienced Senior Power BI Developer to join our dynamic and growing team. In this role, you will be responsible for designing, developing, and maintaining interactive reports and dashboards that empower business users to make informed, data-driven decisions. The ideal candidate will possess a deep understanding of Power BI, data modelling, and business intelligence best practices.\nKey Responsibilities:\nDesign and develop robust Power BI reports and dashboards aligned with business objectives.\nBuild complex semantic models, including composite models.\nUtilise DAX and Power Query to create high-performance BI solutions.\nIntegrate data from multiple on-premises and cloud-based databases.\nDevelop reports with advanced custom visuals and interactive elements.\nImplement data loading through XMLA Endpoints.\nDesign and manage Power Automate flows for process automation.\nCreate and maintain Paginated Reports.\nIntegrate advanced analytics tools (e.g., Python, R) within Power BI.\nApply strong SQL skills and ETL processes for data transformation and warehousing.\nFollow Agile methodologies for BI solution delivery.\nManage deployment pipelines and version control.\nAdminister Power BI environments, including workspace management, security, and content sharing.\nLeverage Power BI Embedded or REST API for advanced integration and automation.\nRequired Qualifications:\nBachelors degree in computer science, Information Systems, Data Science, or a related field (or equivalent professional experience).\n3 to 5 years of hands-on experience in developing Power BI reports and dashboards.\nProven expertise in DAX, Power Query, and data modelling.\nPreferred Skills:\nExperience with Databricks.\nFamiliarity with Python, R, or other data analysis tools.\nPower BI certification is a strong advantage.",Industry Type: Emerging Technologies (AI/ML),Department: IT & Information Security,"Employment Type: Full Time, Permanent","['Power Bi', 'Databricks Unified Data Analytics Platform', 'Dax', 'Power Query', 'Python', 'Xmla', 'Power Automate', 'Data Modeling', 'ETL', 'SQL']",2025-06-12 06:28:05
Solution Architect,Amgen Inc,9 - 14 years,Not Disclosed,['Hyderabad'],"Role Description:\nWe are seeking a seasoned Solution Architect to drive the architecture, development and implementation of data solutions to Amgen functional groups. The ideal candidate able to work in large scale Data Analytic initiatives, engage and work along with Business, Program Management, Data Engineering and Analytic Engineering teams. Be champions of enterprise data analytic strategy, data architecture blueprints and architectural guidelines. As a Solution Architect, you will play a crucial role in designing, building, and optimizing data solutions to Amgen functional groups such as R&D, Operations and GCO.\nRoles & Responsibilities:\nImplement and manage large scale data analytic solutions to Amgen functional groups that align with the Amgen Data strategy\nCollaborate with Business, Program Management, Data Engineering and Analytic Engineering teams to deliver data solutions\nResponsible for design, develop, optimize, delivery and support of Data solutions on AWS and Databricks architecture\nLeverage cloud platforms (AWS preferred) to build scalable and efficient data solutions.\nProvide expert guidance and mentorship to the team members, fostering a culture of innovation and best practices.\nBe passionate and hands-on to quickly experiment with new data related technologies\nDefine guidelines, standards, strategies, security policies and change management policies to support the Enterprise Data platform.\nCollaborate and align with EARB, Cloud Infrastructure, Security and other technology leaders on Enterprise Data Architecture changes\nWork with different project and application groups to drive growth of the Enterprise Data Platform using effective written/verbal communication skills, and lead demos at different roadmap sessions\nOverall management of the Enterprise Data Platform on AWS environment to ensure that the service delivery is cost effective and business SLAs around uptime, performance and capacity are met\nEnsure scalability, reliability, and performance of data platforms by implementing best practices for architecture, cloud resource optimization, and system tuning.\nCollaboration with RunOps engineers to continuously increase our ability to push changes into production with as little manual overhead and as much speed as possible.\nMaintain knowledge of market trends and developments in data integration, data management and analytics software/tools\nWork as part of team in a SAFe Agile/Scrum model\nBasic Qualifications and Experience:\nMasters degree with 6 - 8 years of experience in Computer Science, IT or related field OR\nBachelors degree with 9 - 12 years of experience in Computer Science, IT or related field OR\nFunctional Skills:\nMust-Have Skills:\n7+ years of hands-on experience in Data integrations, Data Management and BI technology stack.\nStrong experience with one or more Data Management tools such as AWS data lake, Snowflake or Azure Data Fabric\nExpert-level proficiency with Databricks and experience in optimizing data pipelines and workflows in Databricks environments.\nStrong experience with Python, PySpark, and SQL for building scalable data workflows and pipelines.\nExperience with Apache Spark, Delta Lake, and other relevant technologies for large-scale data processing.\nFamiliarity with BI tools including Tableau and PowerBI\nDemonstrated ability to enhance cost-efficiency, scalability, and performance for data solutions\nStrong analytical and problem-solving skills to address complex data solutions\nGood-to-Have Skills:\nPreferred to have experience in life science or tech or consultative solution architecture roles\nExperience working with agile development methodologies such as Scaled Agile.\nProfessional Certifications\nAWS Certified Data Engineer preferred\nDatabricks Certificate preferred\nSoft Skills:\nExcellent analytical and troubleshooting skills.\nStrong verbal and written communication skills\nAbility to work effectively with global, virtual teams\nHigh degree of initiative and self-motivation.\nAbility to manage multiple priorities successfully.\nTeam-oriented, with a focus on achieving team goals\nStrong presentation and public speaking skills.",Industry Type: Pharmaceutical & Life Sciences,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['Solution architecture', 'Data Engineering', 'PySpark', 'Tableau', 'SQL', 'Apache Spark', 'Enterprise Data platform', 'AWS data lake', 'Program Management', 'Databricks architecture', 'Azure Data Fabric', 'PowerBI', 'Snowflake', 'Delta Lake', 'Scaled Agile', 'AWS', 'Python']",2025-06-12 06:28:08
Job Opening For Microsoft Fabric + ADF,Bct,11 - 17 years,Not Disclosed,"['Indore', 'Hyderabad']","Greetings of the Day !!\n\nWe have job opening for Microsoft Fabric + ADF with one of our clients. If you are interested in this position, please share update resume in this email id : shaswati.m@bct-consulting.com.\n\n\n* Primary Skill\n\nMicrosoft Fabric\nSecondary Skill 1\n\nAzure Data Factory (ADF)\n12+ years of experience in Microsoft Azure Data Engineering for analytical projects.\nProven expertise in designing, developing, and deploying high-volume, end-to-end ETL pipelines for complex models, including batch, and real-time data integration frameworks using Azure, Microsoft Fabric and Databricks.\nExtensive hands-on experience with Azure Data Factory, Databricks (with Unity Catalog), Azure Functions, Synapse Analytics, Data Lake, Delta Lake, and Azure SQL Database for managing and processing large-scale data integrations.\nExperience in Databricks cluster optimization and workflow management to ensure cost-effective and high-performance processing.\nSound knowledge of data modelling, data governance, data quality management, and data modernization processes.\nDevelop architecture blueprints and technical design documentation for Azure-based data solutions.\nProvide technical leadership and guidance on cloud architecture best practices, ensuring scalable and secure solutions.\nKeep abreast of emerging Azure technologies and recommend enhancements to existing systems.\nLead proof of concepts (PoCs) and adopt agile delivery methodologies for solution development and delivery.",Industry Type: IT Services & Consulting,Department: Data Science & Analytics,"Employment Type: Full Time, Permanent","['Azure Data Factory', 'Azure', 'Microsoft Fabric', 'Databricks', 'ETL pipelines', 'Delta Lake', 'Synapse Analytics', 'Data Lake', 'and Azure SQL Database', 'Microsoft Azure Data Engineering']",2025-06-12 06:28:11
Onix is Hiring MDM Informatica Developer,Onix,3 - 8 years,Not Disclosed,"['Hyderabad', 'Pune']","Job Summary:\nWe are seeking a highly skilled Informatica MDM Developer to join our data integration and management team. The ideal candidate will have extensive experience in Informatica Master Data Management (MDM) solutions and a deep understanding of data quality, data governance, and master data modeling.\nKey Responsibilities:\nDesign, develop, and deploy Informatica MDM solutions (including Hub, IDD, SIF, and MDM Hub configurations).\nWork closely with data architects, business analysts, and stakeholders to understand master data requirements.\nConfigure and manage Trust, Merge, Survivorship rules, and Match/Merge logic.\nImplement data quality (DQ) checks and profiling using Informatica DQ tools.\nDevelop batch and real-time integration using Informatica MDM SIF APIs and ETL tools (e.g., Informatica PowerCenter).\nMonitor and optimize MDM performance and data processing.\nDocument MDM architecture, data flows, and integration touchpoints.\nTroubleshoot and resolve MDM issues across environments (Dev, Test, UAT, Prod).\nSupport data governance and metadata management initiatives.\nRequired Skills:\nStrong hands-on experience with Informatica MDM (10.x or later).\nProficient in match/merge rules, data stewardship, hierarchy management, and SIF APIs.\nExperience with Informatica Data Quality (IDQ) is a plus.\nSolid understanding of data modeling, relational databases, and SQL.\nFamiliarity with REST/SOAP APIs, web services, and real-time data integration.\nExperience in Agile/Scrum environments.\nExcellent problem-solving and communication skills.",Industry Type: IT Services & Consulting,Department: Engineering - Software & QA,"Employment Type: Full Time, Permanent","['informatica', 'Informatica Mdm', 'Informatica Master Data Management', 'Mdm Informatica', 'Etl Informatica', 'MDM']",2025-06-12 06:28:13
